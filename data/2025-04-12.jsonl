{"id": "2504.07100", "pdf": "https://arxiv.org/pdf/2504.07100", "abs": "https://arxiv.org/abs/2504.07100", "authors": ["Abhay Gupta", "Jacob Cheung", "Philip Meng", "Shayan Sayyed", "Austen Liao", "Kevin Zhu", "Sean O'Brien"], "title": "EnDive: A Cross-Dialect Benchmark for Fairness and Performance in Large Language Models", "categories": ["cs.CL", "I.2.7"], "comment": null, "summary": "The diversity of human language, shaped by social, cultural, and regional\ninfluences, presents significant challenges for natural language processing\n(NLP) systems. Existing benchmarks often overlook intra-language variations,\nleaving speakers of non-standard dialects underserved. To address this gap, we\nintroduce EnDive (English Diversity), a benchmark that evaluates five\nwidely-used large language models (LLMs) across tasks in language\nunderstanding, algorithmic reasoning, mathematics, and logic. Our framework\ntranslates Standard American English datasets into five underrepresented\ndialects using few-shot prompting with verified examples from native speakers,\nand compare these translations against rule-based methods via fluency\nassessments, preference tests, and semantic similarity metrics. Human\nevaluations confirm high translation quality, with average scores of at least\n6.02/7 for faithfulness, fluency, and formality. By filtering out\nnear-identical translations, we create a challenging dataset that reveals\nsignificant performance disparities - models consistently underperform on\ndialectal inputs compared to Standard American English. EnDive thus advances\ndialect-aware NLP by uncovering model biases and promoting more equitable\nlanguage technologies."}
{"id": "2504.07113", "pdf": "https://arxiv.org/pdf/2504.07113", "abs": "https://arxiv.org/abs/2504.07113", "authors": ["Aly M. Kassem", "Bernhard Sch√∂lkopf", "Zhijing Jin"], "title": "How Robust Are Router-LLMs? Analysis of the Fragility of LLM Routing Capabilities", "categories": ["cs.CL", "cs.DB"], "comment": null, "summary": "Large language model (LLM) routing has emerged as a crucial strategy for\nbalancing computational costs with performance by dynamically assigning queries\nto the most appropriate model based on query complexity. Despite recent\nadvances showing that preference-data-based routers can outperform traditional\nmethods, current evaluation benchmarks remain limited. They largely focus on\ngeneral model capabilities while overlooking task-specific behaviors and\ncritical concerns such as privacy, safety, and potential backdoor\nvulnerabilities introduced through preference data. In response, we propose the\nDSC benchmark: Diverse, Simple, and Categorized, an evaluation framework that\ncategorizes router performance across a broad spectrum of query types,\nincluding coding, translation, mathematics, human instructions, general\nknowledge, and LLM jailbreaking. Additionally, it integrates privacy and safety\nassessments to reveal hidden risks. Our experiments on three preference-based\nrouters and two commercial counterparts demonstrate that while these systems\nimprove efficiency, they often make suboptimal, category-driven decisions. For\ninstance, a BERT-based router directs all coding and mathematics queries to the\nmost powerful LLM even when simpler models would suffice, while routing\njailbreaking attempts to weaker models, thereby elevating safety risks."}
{"id": "2504.07114", "pdf": "https://arxiv.org/pdf/2504.07114", "abs": "https://arxiv.org/abs/2504.07114", "authors": ["Serina Chang", "Ashton Anderson", "Jake M. Hofman"], "title": "ChatBench: From Static Benchmarks to Human-AI Evaluation", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": null, "summary": "With the rapid adoption of LLM-based chatbots, there is a pressing need to\nevaluate what humans and LLMs can achieve together. However, standard\nbenchmarks, such as MMLU, measure LLM capabilities in isolation (i.e.,\n\"AI-alone\"). Here, we design and conduct a user study to convert MMLU questions\ninto user-AI conversations, by seeding the user with the question and having\nthem carry out a conversation with the LLM to answer their question. We release\nChatBench, a new dataset with AI-alone, user-alone, and user-AI data for 396\nquestions and two LLMs, including 144K answers and 7,336 user-AI conversations.\nWe find that AI-alone accuracy fails to predict user-AI accuracy, with\nsignificant differences across multiple subjects (math, physics, and moral\nreasoning), and we analyze the user-AI conversations to provide insight into\nhow they diverge from AI-alone benchmarks. Finally, we show that fine-tuning a\nuser simulator on a subset of ChatBench improves its ability to estimate\nuser-AI accuracies, increasing correlation on held-out questions by more than\n20 points, creating possibilities for scaling interactive evaluation."}
{"id": "2504.07115", "pdf": "https://arxiv.org/pdf/2504.07115", "abs": "https://arxiv.org/abs/2504.07115", "authors": ["Jiali Cheng", "Hadi Amiri"], "title": "EqualizeIR: Mitigating Linguistic Biases in Retrieval Models", "categories": ["cs.CL", "cs.LG"], "comment": "NAACL 2025", "summary": "This study finds that existing information retrieval (IR) models show\nsignificant biases based on the linguistic complexity of input queries,\nperforming well on linguistically simpler (or more complex) queries while\nunderperforming on linguistically more complex (or simpler) queries. To address\nthis issue, we propose EqualizeIR, a framework to mitigate linguistic biases in\nIR models. EqualizeIR uses a linguistically biased weak learner to capture\nlinguistic biases in IR datasets and then trains a robust model by regularizing\nand refining its predictions using the biased weak learner. This approach\neffectively prevents the robust model from overfitting to specific linguistic\npatterns in data. We propose four approaches for developing\nlinguistically-biased models. Extensive experiments on several datasets show\nthat our method reduces performance disparities across linguistically simple\nand complex queries, while improving overall retrieval performance."}
{"id": "2504.07165", "pdf": "https://arxiv.org/pdf/2504.07165", "abs": "https://arxiv.org/abs/2504.07165", "authors": ["Yana Wei", "Liang Zhao", "Kangheng Lin", "En Yu", "Yuang Peng", "Runpei Dong", "Jianjian Sun", "Haoran Wei", "Zheng Ge", "Xiangyu Zhang", "Vishal M. Patel"], "title": "Perception in Reflection", "categories": ["cs.CV"], "comment": null, "summary": "We present a perception in reflection paradigm designed to transcend the\nlimitations of current large vision-language models (LVLMs), which are expected\nyet often fail to achieve perfect perception initially. Specifically, we\npropose Reflective Perception (RePer), a dual-model reflection mechanism that\nsystematically alternates between policy and critic models, enables iterative\nrefinement of visual perception. This framework is powered by Reflective\nPerceptual Learning (RPL), which reinforces intrinsic reflective capabilities\nthrough a methodically constructed visual reflection dataset and reflective\nunlikelihood training. Comprehensive experimental evaluation demonstrates\nRePer's quantifiable improvements in image understanding, captioning precision,\nand hallucination reduction. Notably, RePer achieves strong alignment between\nmodel attention patterns and human visual focus, while RPL optimizes\nfine-grained and free-form preference alignment. These advancements establish\nperception in reflection as a robust paradigm for future multimodal agents,\nparticularly in tasks requiring complex reasoning and multi-step manipulation."}
{"id": "2504.07116", "pdf": "https://arxiv.org/pdf/2504.07116", "abs": "https://arxiv.org/abs/2504.07116", "authors": ["Andrew Rufail", "Daniel Kim", "Sean O'Brien", "Kevin Zhu"], "title": "CLEAR: Contrasting Textual Feedback with Experts and Amateurs for Reasoning", "categories": ["cs.CL"], "comment": "Accepted at the Conference of the North American Chapter of the\n  Association for Computational Linguistics (NAACL), Student Research Workshop\n  (SRW)", "summary": "We introduce CLEAR (Contrasting Textual Feedback with Experts and Amateurs\nfor Reasoning), a novel approach to language model reasoning that leverages the\nstrengths of a larger (expert) model and smaller (amateur) model. The expert\nand amateur models each provide feedback on a model's initial output and are\ncontrasted with each other into refined feedback. This feedback is subsequently\napplied to iteratively improve CLEAR's responses. Our experiments demonstrate\nthat CLEAR outperforms state-of-the-art methods in several challenging\nreasoning tasks, including story outline improvement (up to 19.6% relative\nincrease in interestingness), constrained generation (up to 18.5% increase in\ncoverage), mathematical reasoning (up to 6.7% improvement in accuracy) and\nmitigation of toxicity (decrease of up to 22% in toxicity)."}
{"id": "2504.07198", "pdf": "https://arxiv.org/pdf/2504.07198", "abs": "https://arxiv.org/abs/2504.07198", "authors": ["Ashutosh Chaubey", "Xulang Guan", "Mohammad Soleymani"], "title": "Face-LLaVA: Facial Expression and Attribute Understanding through Instruction Tuning", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "Project Page: https://face-llava.github.io", "summary": "The human face plays a central role in social communication, necessitating\nthe use of performant computer vision tools for human-centered applications. We\npropose Face-LLaVA, a multimodal large language model for face-centered,\nin-context learning, including facial expression and attribute recognition.\nAdditionally, Face-LLaVA is able to generate natural language descriptions that\ncan be used for reasoning. Leveraging existing visual databases, we first\ndeveloped FaceInstruct-1M, a face-centered database for instruction tuning\nMLLMs for face processing. We then developed a novel face-specific visual\nencoder powered by Face-Region Guided Cross-Attention that integrates face\ngeometry with local visual features. We evaluated the proposed method across\nnine different datasets and five different face processing tasks, including\nfacial expression recognition, action unit detection, facial attribute\ndetection, age estimation and deepfake detection. Face-LLaVA achieves superior\nresults compared to existing open-source MLLMs and competitive performance\ncompared to commercial solutions. Our model output also receives a higher\nreasoning rating by GPT under a zero-shot setting across all the tasks. Both\nour dataset and model wil be released at https://face-llava.github.io to\nsupport future advancements in social AI and foundational vision-language\nresearch."}
{"id": "2504.07128", "pdf": "https://arxiv.org/pdf/2504.07128", "abs": "https://arxiv.org/abs/2504.07128", "authors": ["Sara Vera Marjanoviƒá", "Arkil Patel", "Vaibhav Adlakha", "Milad Aghajohari", "Parishad BehnamGhader", "Mehar Bhatia", "Aditi Khandelwal", "Austin Kraft", "Benno Krojer", "Xing Han L√π", "Nicholas Meade", "Dongchan Shin", "Amirhossein Kazemnejad", "Gaurav Kamath", "Marius Mosbach", "Karolina Sta≈Ñczak", "Siva Reddy"], "title": "DeepSeek-R1 Thoughtology: Let's <think> about LLM Reasoning", "categories": ["cs.CL"], "comment": "142 pages, pre-print", "summary": "Large Reasoning Models like DeepSeek-R1 mark a fundamental shift in how LLMs\napproach complex problems. Instead of directly producing an answer for a given\ninput, DeepSeek-R1 creates detailed multi-step reasoning chains, seemingly\n\"thinking\" about a problem before providing an answer. This reasoning process\nis publicly available to the user, creating endless opportunities for studying\nthe reasoning behaviour of the model and opening up the field of Thoughtology.\nStarting from a taxonomy of DeepSeek-R1's basic building blocks of reasoning,\nour analyses on DeepSeek-R1 investigate the impact and controllability of\nthought length, management of long or confusing contexts, cultural and safety\nconcerns, and the status of DeepSeek-R1 vis-\\`a-vis cognitive phenomena, such\nas human-like language processing and world modelling. Our findings paint a\nnuanced picture. Notably, we show DeepSeek-R1 has a 'sweet spot' of reasoning,\nwhere extra inference time can impair model performance. Furthermore, we find a\ntendency for DeepSeek-R1 to persistently ruminate on previously explored\nproblem formulations, obstructing further exploration. We also note strong\nsafety vulnerabilities of DeepSeek-R1 compared to its non-reasoning\ncounterpart, which can also compromise safety-aligned LLMs."}
{"id": "2504.07252", "pdf": "https://arxiv.org/pdf/2504.07252", "abs": "https://arxiv.org/abs/2504.07252", "authors": ["Rajhans Singh", "Rafael Bidese Puhl", "Kshitiz Dhakal", "Sudhir Sornapudi"], "title": "Few-Shot Adaptation of Grounding DINO for Agricultural Domain", "categories": ["cs.CV"], "comment": null, "summary": "Deep learning models are transforming agricultural applications by enabling\nautomated phenotyping, monitoring, and yield estimation. However, their\neffectiveness heavily depends on large amounts of annotated training data,\nwhich can be labor and time intensive. Recent advances in open-set object\ndetection, particularly with models like Grounding-DINO, offer a potential\nsolution to detect regions of interests based on text prompt input. Initial\nzero-shot experiments revealed challenges in crafting effective text prompts,\nespecially for complex objects like individual leaves and visually similar\nclasses. To address these limitations, we propose an efficient few-shot\nadaptation method that simplifies the Grounding-DINO architecture by removing\nthe text encoder module (BERT) and introducing a randomly initialized trainable\ntext embedding. This method achieves superior performance across multiple\nagricultural datasets, including plant-weed detection, plant counting, insect\nidentification, fruit counting, and remote sensing tasks. Specifically, it\ndemonstrates up to a $\\sim24\\%$ higher mAP than fully fine-tuned YOLO models on\nagricultural datasets and outperforms previous state-of-the-art methods by\n$\\sim10\\%$ in remote sensing, under few-shot learning conditions. Our method\noffers a promising solution for automating annotation and accelerating the\ndevelopment of specialized agricultural AI solutions."}
{"id": "2504.07174", "pdf": "https://arxiv.org/pdf/2504.07174", "abs": "https://arxiv.org/abs/2504.07174", "authors": ["Mingxuan Li", "Hanchen Li", "Chenhao Tan"], "title": "HypoEval: Hypothesis-Guided Evaluation for Natural Language Generation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "22 pages, 3 figures, code link:\n  https://github.com/ChicagoHAI/HypoEval-Gen", "summary": "Large language models (LLMs) have demonstrated great potential for automating\nthe evaluation of natural language generation. Previous frameworks of\nLLM-as-a-judge fall short in two ways: they either use zero-shot setting\nwithout consulting any human input, which leads to low alignment, or fine-tune\nLLMs on labeled data, which requires a non-trivial number of samples. Moreover,\nprevious methods often provide little reasoning behind automated evaluations.\nIn this paper, we propose HypoEval, Hypothesis-guided Evaluation framework,\nwhich first uses a small corpus of human evaluations to generate more detailed\nrubrics for human judgments and then incorporates a checklist-like approach to\ncombine LLM's assigned scores on each decomposed dimension to acquire overall\nscores. With only 30 human evaluations, HypoEval achieves state-of-the-art\nperformance in alignment with both human rankings (Spearman correlation) and\nhuman scores (Pearson correlation), on average outperforming G-Eval by 11.86%\nand fine-tuned Llama-3.1-8B-Instruct with at least 3 times more human\nevaluations by 11.95%. Furthermore, we conduct systematic studies to assess the\nrobustness of HypoEval, highlighting its effectiveness as a reliable and\ninterpretable automated evaluation framework."}
{"id": "2504.07260", "pdf": "https://arxiv.org/pdf/2504.07260", "abs": "https://arxiv.org/abs/2504.07260", "authors": ["Fereidoon Zangeneh", "Amit Dekel", "Alessandro Pieropan", "Patric Jensfelt"], "title": "Quantifying Epistemic Uncertainty in Absolute Pose Regression", "categories": ["cs.CV"], "comment": null, "summary": "Visual relocalization is the task of estimating the camera pose given an\nimage it views. Absolute pose regression offers a solution to this task by\ntraining a neural network, directly regressing the camera pose from image\nfeatures. While an attractive solution in terms of memory and compute\nefficiency, absolute pose regression's predictions are inaccurate and\nunreliable outside the training domain. In this work, we propose a novel method\nfor quantifying the epistemic uncertainty of an absolute pose regression model\nby estimating the likelihood of observations within a variational framework.\nBeyond providing a measure of confidence in predictions, our approach offers a\nunified model that also handles observation ambiguities, probabilistically\nlocalizing the camera in the presence of repetitive structures. Our method\noutperforms existing approaches in capturing the relation between uncertainty\nand prediction error."}
{"id": "2504.07199", "pdf": "https://arxiv.org/pdf/2504.07199", "abs": "https://arxiv.org/abs/2504.07199", "authors": ["Jennifer D'Souza", "Sameer Sadruddin", "Holger Israel", "Mathias Begoin", "Diana Slawig"], "title": "SemEval-2025 Task 5: LLMs4Subjects -- LLM-based Automated Subject Tagging for a National Technical Library's Open-Access Catalog", "categories": ["cs.CL", "cs.AI", "cs.DL", "cs.LG"], "comment": "10 pages, 4 figures, Accepted as SemEval 2025 Task 5 description\n  paper", "summary": "We present SemEval-2025 Task 5: LLMs4Subjects, a shared task on automated\nsubject tagging for scientific and technical records in English and German\nusing the GND taxonomy. Participants developed LLM-based systems to recommend\ntop-k subjects, evaluated through quantitative metrics (precision, recall,\nF1-score) and qualitative assessments by subject specialists. Results highlight\nthe effectiveness of LLM ensembles, synthetic data generation, and multilingual\nprocessing, offering insights into applying LLMs for digital library\nclassification."}
{"id": "2504.07301", "pdf": "https://arxiv.org/pdf/2504.07301", "abs": "https://arxiv.org/abs/2504.07301", "authors": ["Krzysztof Byrski", "Jacek Tabor", "Przemys≈Çaw Spurek", "Marcin Mazur"], "title": "CEC-MMR: Cross-Entropy Clustering Approach to Multi-Modal Regression", "categories": ["cs.CV"], "comment": null, "summary": "In practical applications of regression analysis, it is not uncommon to\nencounter a multitude of values for each attribute. In such a situation, the\nunivariate distribution, which is typically Gaussian, is suboptimal because the\nmean may be situated between modes, resulting in a predicted value that differs\nsignificantly from the actual data. Consequently, to address this issue, a\nmixture distribution with parameters learned by a neural network, known as a\nMixture Density Network (MDN), is typically employed. However, this approach\nhas an important inherent limitation, in that it is not feasible to ascertain\nthe precise number of components with a reasonable degree of accuracy. In this\npaper, we introduce CEC-MMR, a novel approach based on Cross-Entropy Clustering\n(CEC), which allows for the automatic detection of the number of components in\na regression problem. Furthermore, given an attribute and its value, our method\nis capable of uniquely identifying it with the underlying component. The\nexperimental results demonstrate that CEC-MMR yields superior outcomes compared\nto classical MDNs."}
{"id": "2504.07228", "pdf": "https://arxiv.org/pdf/2504.07228", "abs": "https://arxiv.org/abs/2504.07228", "authors": ["Eylon Caplan", "Dan Goldwasser"], "title": "ConceptCarve: Dynamic Realization of Evidence", "categories": ["cs.CL"], "comment": "Under review for ACL 2025", "summary": "Finding evidence for human opinion and behavior at scale is a challenging\ntask, often requiring an understanding of sophisticated thought patterns among\nvast online communities found on social media. For example, studying how gun\nownership is related to the perception of Freedom, requires a retrieval system\nthat can operate at scale over social media posts, while dealing with two key\nchallenges: (1) identifying abstract concept instances, (2) which can be\ninstantiated differently across different communities. To address these, we\nintroduce ConceptCarve, an evidence retrieval framework that utilizes\ntraditional retrievers and LLMs to dynamically characterize the search space\nduring retrieval. Our experiments show that ConceptCarve surpasses traditional\nretrieval systems in finding evidence within a social media community. It also\nproduces an interpretable representation of the evidence for that community,\nwhich we use to qualitatively analyze complex thought patterns that manifest\ndifferently across the communities."}
{"id": "2504.07334", "pdf": "https://arxiv.org/pdf/2504.07334", "abs": "https://arxiv.org/abs/2504.07334", "authors": ["Chendi Lin", "Heshan Liu", "Qunshu Lin", "Zachary Bright", "Shitao Tang", "Yihui He", "Minghao Liu", "Ling Zhu", "Cindy Le"], "title": "Objaverse++: Curated 3D Object Dataset with Quality Annotations", "categories": ["cs.CV", "cs.AI", "cs.LG", "68T45, 68T07", "I.2.10; I.3.5; I.3.7; I.4.8; I.5.1"], "comment": "8 pages, 8 figures. Accepted to CVPR 2025 Workshop on Efficient Large\n  Vision Models (April 2025)", "summary": "This paper presents Objaverse++, a curated subset of Objaverse enhanced with\ndetailed attribute annotations by human experts. Recent advances in 3D content\ngeneration have been driven by large-scale datasets such as Objaverse, which\ncontains over 800,000 3D objects collected from the Internet. Although\nObjaverse represents the largest available 3D asset collection, its utility is\nlimited by the predominance of low-quality models. To address this limitation,\nwe manually annotate 10,000 3D objects with detailed attributes, including\naesthetic quality scores, texture color classifications, multi-object\ncomposition flags, transparency characteristics, etc. Then, we trained a neural\nnetwork capable of annotating the tags for the rest of the Objaverse dataset.\nThrough experiments and a user study on generation results, we demonstrate that\nmodels pre-trained on our quality-focused subset achieve better performance\nthan those trained on the larger dataset of Objaverse in image-to-3D generation\ntasks. In addition, by comparing multiple subsets of training data filtered by\nour tags, our results show that the higher the data quality, the faster the\ntraining loss converges. These findings suggest that careful curation and rich\nannotation can compensate for the raw dataset size, potentially offering a more\nefficient path to develop 3D generative models. We release our enhanced dataset\nof approximately 500,000 curated 3D models to facilitate further research on\nvarious downstream tasks in 3D computer vision. In the near future, we aim to\nextend our annotations to cover the entire Objaverse dataset."}
{"id": "2504.07229", "pdf": "https://arxiv.org/pdf/2504.07229", "abs": "https://arxiv.org/abs/2504.07229", "authors": ["Lakshmipathi Balaji", "Karan Singla"], "title": "Visual-Aware Speech Recognition for Noisy Scenarios", "categories": ["cs.CL", "eess.AS", "eess.SP"], "comment": null, "summary": "Humans have the ability to utilize visual cues, such as lip movements and\nvisual scenes, to enhance auditory perception, particularly in noisy\nenvironments. However, current Automatic Speech Recognition (ASR) or\nAudio-Visual Speech Recognition (AVSR) models often struggle in noisy\nscenarios. To solve this task, we propose a model that improves transcription\nby correlating noise sources to visual cues. Unlike works that rely on lip\nmotion and require the speaker's visibility, we exploit broader visual\ninformation from the environment. This allows our model to naturally filter\nspeech from noise and improve transcription, much like humans do in noisy\nscenarios. Our method re-purposes pretrained speech and visual encoders,\nlinking them with multi-headed attention. This approach enables the\ntranscription of speech and the prediction of noise labels in video inputs. We\nintroduce a scalable pipeline to develop audio-visual datasets, where visual\ncues correlate to noise in the audio. We show significant improvements over\nexisting audio-only models in noisy scenarios. Results also highlight that\nvisual cues play a vital role in improved transcription accuracy."}
{"id": "2504.07335", "pdf": "https://arxiv.org/pdf/2504.07335", "abs": "https://arxiv.org/abs/2504.07335", "authors": ["Akash Jadhav", "Michael Greenspan"], "title": "DLTPose: 6DoF Pose Estimation From Accurate Dense Surface Point Estimates", "categories": ["cs.CV"], "comment": null, "summary": "We propose DLTPose, a novel method for 6DoF object pose estimation from RGB-D\nimages that combines the accuracy of sparse keypoint methods with the\nrobustness of dense pixel-wise predictions. DLTPose predicts per-pixel radial\ndistances to a set of minimally four keypoints, which are then fed into our\nnovel Direct Linear Transform (DLT) formulation to produce accurate 3D object\nframe surface estimates, leading to better 6DoF pose estimation. Additionally,\nwe introduce a novel symmetry-aware keypoint ordering approach, designed to\nhandle object symmetries that otherwise cause inconsistencies in keypoint\nassignments. Previous keypoint-based methods relied on fixed keypoint\norderings, which failed to account for the multiple valid configurations\nexhibited by symmetric objects, which our ordering approach exploits to enhance\nthe model's ability to learn stable keypoint representations. Extensive\nexperiments on the benchmark LINEMOD, Occlusion LINEMOD and YCB-Video datasets\nshow that DLTPose outperforms existing methods, especially for symmetric and\noccluded objects, demonstrating superior Mean Average Recall values of 86.5%\n(LM), 79.7% (LM-O) and 89.5% (YCB-V). The code is available at\nhttps://anonymous.4open.science/r/DLTPose_/ ."}
{"id": "2504.07274", "pdf": "https://arxiv.org/pdf/2504.07274", "abs": "https://arxiv.org/abs/2504.07274", "authors": ["Nikita Tatarinov", "Siddhant Sukhani", "Agam Shah", "Sudheer Chava"], "title": "Language Modeling for the Future of Finance: A Quantitative Survey into Metrics, Tasks, and Data Opportunities", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in language modeling have led to growing interest in applying\nNatural Language Processing (NLP) techniques to financial problems, enabling\nnew approaches to analysis and decision-making. To systematically examine this\ntrend, we review 374 NLP research papers published between 2017 and 2024 across\n38 conferences and workshops, with a focused analysis of 221 papers that\ndirectly address finance-related tasks. We evaluate these papers across 11\nqualitative and quantitative dimensions, identifying key trends such as the\nincreasing use of general-purpose language models, steady progress in sentiment\nanalysis and information extraction, and emerging efforts around explainability\nand privacy-preserving methods. We also discuss the use of evaluation metrics,\nhighlighting the importance of domain-specific ones to complement standard\nmachine learning metrics. Our findings emphasize the need for more accessible,\nadaptive datasets and highlight the significance of incorporating financial\ncrisis periods to strengthen model robustness under real-world conditions. This\nsurvey provides a structured overview of NLP research applied to finance and\noffers practical insights for researchers and practitioners working at this\nintersection."}
{"id": "2504.07336", "pdf": "https://arxiv.org/pdf/2504.07336", "abs": "https://arxiv.org/abs/2504.07336", "authors": ["Siyuan Dai", "Kai Ye", "Guodong Liu", "Haoteng Tang", "Liang Zhan"], "title": "Zeus: Zero-shot LLM Instruction for Union Segmentation in Multimodal Medical Imaging", "categories": ["cs.CV", "cs.AI"], "comment": "21 pages, 4 figures, In Press by a journal", "summary": "Medical image segmentation has achieved remarkable success through the\ncontinuous advancement of UNet-based and Transformer-based foundation\nbackbones. However, clinical diagnosis in the real world often requires\nintegrating domain knowledge, especially textual information. Conducting\nmultimodal learning involves visual and text modalities shown as a solution,\nbut collecting paired vision-language datasets is expensive and time-consuming,\nposing significant challenges. Inspired by the superior ability in numerous\ncross-modal tasks for Large Language Models (LLMs), we proposed a novel\nVision-LLM union framework to address the issues. Specifically, we introduce\nfrozen LLMs for zero-shot instruction generation based on corresponding medical\nimages, imitating the radiology scanning and report generation process. {To\nbetter approximate real-world diagnostic processes}, we generate more precise\ntext instruction from multimodal radiology images (e.g., T1-w or T2-w MRI and\nCT). Based on the impressive ability of semantic understanding and rich\nknowledge of LLMs. This process emphasizes extracting special features from\ndifferent modalities and reunion the information for the ultimate clinical\ndiagnostic. With generated text instruction, our proposed union segmentation\nframework can handle multimodal segmentation without prior collected\nvision-language datasets. To evaluate our proposed method, we conduct\ncomprehensive experiments with influential baselines, the statistical results\nand the visualized case study demonstrate the superiority of our novel method.}"}
{"id": "2504.07282", "pdf": "https://arxiv.org/pdf/2504.07282", "abs": "https://arxiv.org/abs/2504.07282", "authors": ["Lv Qingsong", "Yangning Li", "Zihua Lan", "Zishan Xu", "Jiwei Tang", "Yinghui Li", "Wenhao Jiang", "Hai-Tao Zheng", "Philip S. Yu"], "title": "RAISE: Reinforenced Adaptive Instruction Selection For Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "In the instruction fine-tuning of large language models (LLMs), it has become\na consensus that a few high-quality instructions are superior to a large number\nof low-quality instructions. At present, many instruction selection methods\nhave been proposed, but most of these methods select instruction based on\nheuristic quality metrics, and only consider data selection before training.\nThese designs lead to insufficient optimization of instruction fine-tuning, and\nfixed heuristic indicators are often difficult to optimize for specific tasks.\nSo we designed a dynamic, task-objective-driven instruction selection framework\nRAISE(Reinforenced Adaptive Instruction SElection), which incorporates the\nentire instruction fine-tuning process into optimization, selecting instruction\nat each step based on the expected impact of instruction on model performance\nimprovement. Our approach is well interpretable and has strong task-specific\noptimization capabilities. By modeling dynamic instruction selection as a\nsequential decision-making process, we use RL to train our selection strategy.\nExtensive experiments and result analysis prove the superiority of our method\ncompared with other instruction selection methods. Notably, RAISE achieves\nsuperior performance by updating only 1\\% of the training steps compared to\nfull-data training, demonstrating its efficiency and effectiveness."}
{"id": "2504.07370", "pdf": "https://arxiv.org/pdf/2504.07370", "abs": "https://arxiv.org/abs/2504.07370", "authors": ["Chenyu Han", "Corentin Dumery"], "title": "View-Dependent Uncertainty Estimation of 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has become increasingly popular in 3D scene\nreconstruction for its high visual accuracy. However, uncertainty estimation of\n3DGS scenes remains underexplored and is crucial to downstream tasks such as\nasset extraction and scene completion. Since the appearance of 3D gaussians is\nview-dependent, the color of a gaussian can thus be certain from an angle and\nuncertain from another. We thus propose to model uncertainty in 3DGS as an\nadditional view-dependent per-gaussian feature that can be modeled with\nspherical harmonics. This simple yet effective modeling is easily interpretable\nand can be integrated into the traditional 3DGS pipeline. It is also\nsignificantly faster than ensemble methods while maintaining high accuracy, as\ndemonstrated in our experiments."}
{"id": "2504.07288", "pdf": "https://arxiv.org/pdf/2504.07288", "abs": "https://arxiv.org/abs/2504.07288", "authors": ["Yangning Li", "Zihua Lan", "Lv Qingsong", "Yinghui Li", "Hai-Tao Zheng"], "title": "MDIT: A Model-free Data Interpolation Method for Diverse Instruction Tuning", "categories": ["cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) are increasingly applied across various\ntasks, instruction tuning has emerged as a critical method for enhancing model\nperformance. However, current data management strategies face substantial\nchallenges in generating diverse and comprehensive data, restricting further\nimprovements in model performance. To address this gap, we propose MDIT, a\nnovel model-free data interpolation method for diverse instruction tuning,\nwhich generates varied and high-quality instruction data by performing task\ninterpolation. Moreover, it contains diversity-based clustering strategies to\nensure the diversity of the training data. Extensive experiments show that our\nmethod achieves superior performance in multiple benchmark tasks. The LLMs\nfinetuned with MDIT show significant improvements in numerous tasks such as\ngeneral question answering, math reasoning, and code generation. MDIT offers an\nefficient and automatic data synthetic method, generating diverse instruction\ndata without depending on external resources while expanding the application\npotential of LLMs in complex environments."}
{"id": "2504.07375", "pdf": "https://arxiv.org/pdf/2504.07375", "abs": "https://arxiv.org/abs/2504.07375", "authors": ["Junyi Ma", "Wentao Bao", "Jingyi Xu", "Guanzhong Sun", "Xieyuanli Chen", "Hesheng Wang"], "title": "Novel Diffusion Models for Multimodal 3D Hand Trajectory Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Predicting hand motion is critical for understanding human intentions and\nbridging the action space between human movements and robot manipulations.\nExisting hand trajectory prediction (HTP) methods forecast the future hand\nwaypoints in 3D space conditioned on past egocentric observations. However,\nsuch models are only designed to accommodate 2D egocentric video inputs. There\nis a lack of awareness of multimodal environmental information from both 2D and\n3D observations, hindering the further improvement of 3D HTP performance. In\naddition, these models overlook the synergy between hand movements and headset\ncamera egomotion, either predicting hand trajectories in isolation or encoding\negomotion only from past frames. To address these limitations, we propose novel\ndiffusion models (MMTwin) for multimodal 3D hand trajectory prediction. MMTwin\nis designed to absorb multimodal information as input encompassing 2D RGB\nimages, 3D point clouds, past hand waypoints, and text prompt. Besides, two\nlatent diffusion models, the egomotion diffusion and the HTP diffusion as\ntwins, are integrated into MMTwin to predict camera egomotion and future hand\ntrajectories concurrently. We propose a novel hybrid Mamba-Transformer module\nas the denoising model of the HTP diffusion to better fuse multimodal features.\nThe experimental results on three publicly available datasets and our\nself-recorded data demonstrate that our proposed MMTwin can predict plausible\nfuture 3D hand trajectories compared to the state-of-the-art baselines, and\ngeneralizes well to unseen environments. The code and pretrained models will be\nreleased at https://github.com/IRMVLab/MMTwin."}
{"id": "2504.07304", "pdf": "https://arxiv.org/pdf/2504.07304", "abs": "https://arxiv.org/abs/2504.07304", "authors": ["Santiago G√≥ngora", "Luis Chiruzzo", "Gonzalo M√©ndez", "Pablo Gerv√°s"], "title": "PAYADOR: A Minimalist Approach to Grounding Language Models on Structured Data for Interactive Storytelling and Role-playing Games", "categories": ["cs.CL", "cs.AI"], "comment": "Presented at the 15th International Conference on Computational\n  Creativity (ICCC'24)", "summary": "Every time an Interactive Storytelling (IS) system gets a player input, it is\nfacing the world-update problem. Classical approaches to this problem consist\nin mapping that input to known preprogrammed actions, what can severely\nconstrain the free will of the player. When the expected experience has a\nstrong focus on improvisation, like in Role-playing Games (RPGs), this problem\nis critical. In this paper we present PAYADOR, a different approach that\nfocuses on predicting the outcomes of the actions instead of representing the\nactions themselves. To implement this approach, we ground a Large Language\nModel to a minimal representation of the fictional world, obtaining promising\nresults. We make this contribution open-source, so it can be adapted and used\nfor other related research on unleashing the co-creativity power of RPGs."}
{"id": "2504.07378", "pdf": "https://arxiv.org/pdf/2504.07378", "abs": "https://arxiv.org/abs/2504.07378", "authors": ["Yongkang Dai", "Xiaoshui Huang", "Yunpeng Bai", "Hao Guo", "Hongping Gan", "Ling Yang", "Yilei Shi"], "title": "BRepFormer: Transformer-Based B-rep Geometric Feature Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Recognizing geometric features on B-rep models is a cornerstone technique for\nmultimedia content-based retrieval and has been widely applied in intelligent\nmanufacturing. However, previous research often merely focused on Machining\nFeature Recognition (MFR), falling short in effectively capturing the intricate\ntopological and geometric characteristics of complex geometry features. In this\npaper, we propose BRepFormer, a novel transformer-based model to recognize both\nmachining feature and complex CAD models' features. BRepFormer encodes and\nfuses the geometric and topological features of the models. Afterwards,\nBRepFormer utilizes a transformer architecture for feature propagation and a\nrecognition head to identify geometry features. During each iteration of the\ntransformer, we incorporate a bias that combines edge features and topology\nfeatures to reinforce geometric constraints on each face. In addition, we also\nproposed a dataset named Complex B-rep Feature Dataset (CBF), comprising 20,000\nB-rep models. By covering more complex B-rep models, it is better aligned with\nindustrial applications. The experimental results demonstrate that BRepFormer\nachieves state-of-the-art accuracy on the MFInstSeg, MFTRCAD, and our CBF\ndatasets."}
{"id": "2504.07315", "pdf": "https://arxiv.org/pdf/2504.07315", "abs": "https://arxiv.org/abs/2504.07315", "authors": ["Alessio Tosolini", "Claire Bowern"], "title": "Multilingual MFA: Forced Alignment on Low-Resource Related Languages", "categories": ["cs.CL"], "comment": null, "summary": "We compare the outcomes of multilingual and crosslingual training for related\nand unrelated Australian languages with similar phonological inventories. We\nuse the Montreal Forced Aligner to train acoustic models from scratch and adapt\na large English model, evaluating results against seen data, unseen data (seen\nlanguage), and unseen data and language. Results indicate benefits of adapting\nthe English baseline model for previously unseen languages."}
{"id": "2504.07382", "pdf": "https://arxiv.org/pdf/2504.07382", "abs": "https://arxiv.org/abs/2504.07382", "authors": ["Qingchao Jiang", "Zhishuo Xu", "Zhiying Zhu", "Ning Chen", "Haoyue Wang", "Zhongjie Ba"], "title": "Model Discrepancy Learning: Synthetic Faces Detection Based on Multi-Reconstruction", "categories": ["cs.CV"], "comment": "6 pages, 6 figures", "summary": "Advances in image generation enable hyper-realistic synthetic faces but also\npose risks, thus making synthetic face detection crucial. Previous research\nfocuses on the general differences between generated images and real images,\noften overlooking the discrepancies among various generative techniques. In\nthis paper, we explore the intrinsic relationship between synthetic images and\ntheir corresponding generation technologies. We find that specific images\nexhibit significant reconstruction discrepancies across different generative\nmethods and that matching generation techniques provide more accurate\nreconstructions. Based on this insight, we propose a Multi-Reconstruction-based\ndetector. By reversing and reconstructing images using multiple generative\nmodels, we analyze the reconstruction differences among real, GAN-generated,\nand DM-generated images to facilitate effective differentiation. Additionally,\nwe introduce the Asian Synthetic Face Dataset (ASFD), containing synthetic\nAsian faces generated with various GANs and DMs. This dataset complements\nexisting synthetic face datasets. Experimental results demonstrate that our\ndetector achieves exceptional performance, with strong generalization and\nrobustness."}
{"id": "2504.07316", "pdf": "https://arxiv.org/pdf/2504.07316", "abs": "https://arxiv.org/abs/2504.07316", "authors": ["Shujin Wu", "Cheng Qian", "Yi R.", "Fung", "Paul Pu Liang", "Heng Ji"], "title": "Alice: Proactive Learning with Teacher's Demonstrations for Weak-to-Strong Generalization", "categories": ["cs.CL"], "comment": null, "summary": "The growing capabilities of large language models (LLMs) present a key\nchallenge of maintaining effective human oversight. Weak-to-strong\ngeneralization (W2SG) offers a promising framework for supervising increasingly\ncapable LLMs using weaker ones. Traditional W2SG methods rely on passive\nlearning, where a weak teacher provides noisy demonstrations to train a strong\nstudent. This hinders students from employing their knowledge during training\nand reaching their full potential. In this work, we introduce Alice\n(pro{A}ctive {l}earning w{i}th tea{c}her's D{e}monstrations), a framework that\nleverages complementary knowledge between teacher and student to enhance the\nlearning process.We probe the knowledge base of the teacher model by eliciting\ntheir uncertainty, and then use these insights together with teachers'\nresponses as demonstrations to guide student models in self-generating improved\nresponses for supervision. In addition, for situations with significant\ncapability gaps between teacher and student models, we introduce cascade Alice,\nwhich employs a hierarchical training approach where weak teachers initially\nsupervise intermediate models, who then guide stronger models in sequence.\nExperimental results demonstrate that our method significantly enhances the\nW2SG performance, yielding substantial improvements in three key tasks compared\nto the original W2SG: knowledge-based reasoning (+4.0%), mathematical reasoning\n(+22.62%), and logical reasoning (+12.11%). This highlights the effectiveness\nof our new W2SG paradigm that enables more robust knowledge transfer and\nsupervision outcome."}
{"id": "2504.07392", "pdf": "https://arxiv.org/pdf/2504.07392", "abs": "https://arxiv.org/abs/2504.07392", "authors": ["Darian Toma≈°eviƒá", "Fadi Boutros", "Chenhao Lin", "Naser Damer", "Vitomir ≈†truc", "Peter Peer"], "title": "ID-Booth: Identity-consistent Face Generation with Diffusion Models", "categories": ["cs.CV"], "comment": "IEEE International Conference on Automatic Face and Gesture\n  Recognition (FG) 2025, 14 pages", "summary": "Recent advances in generative modeling have enabled the generation of\nhigh-quality synthetic data that is applicable in a variety of domains,\nincluding face recognition. Here, state-of-the-art generative models typically\nrely on conditioning and fine-tuning of powerful pretrained diffusion models to\nfacilitate the synthesis of realistic images of a desired identity. Yet, these\nmodels often do not consider the identity of subjects during training, leading\nto poor consistency between generated and intended identities. In contrast,\nmethods that employ identity-based training objectives tend to overfit on\nvarious aspects of the identity, and in turn, lower the diversity of images\nthat can be generated. To address these issues, we present in this paper a\nnovel generative diffusion-based framework, called ID-Booth. ID-Booth consists\nof a denoising network responsible for data generation, a variational\nauto-encoder for mapping images to and from a lower-dimensional latent space\nand a text encoder that allows for prompt-based control over the generation\nprocedure. The framework utilizes a novel triplet identity training objective\nand enables identity-consistent image generation while retaining the synthesis\ncapabilities of pretrained diffusion models. Experiments with a\nstate-of-the-art latent diffusion model and diverse prompts reveal that our\nmethod facilitates better intra-identity consistency and inter-identity\nseparability than competing methods, while achieving higher image diversity. In\nturn, the produced data allows for effective augmentation of small-scale\ndatasets and training of better-performing recognition models in a\nprivacy-preserving manner. The source code for the ID-Booth framework is\npublicly available at https://github.com/dariant/ID-Booth."}
{"id": "2504.07357", "pdf": "https://arxiv.org/pdf/2504.07357", "abs": "https://arxiv.org/abs/2504.07357", "authors": ["Saurabh Srivastava", "Ziyu Yao"], "title": "Revisiting Prompt Optimization with Large Reasoning Models-A Case Study on Event Extraction", "categories": ["cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) such as DeepSeek-R1 and OpenAI o1 have\ndemonstrated remarkable capabilities in various reasoning tasks. Their strong\ncapability to generate and reason over intermediate thoughts has also led to\narguments that they may no longer require extensive prompt engineering or\noptimization to interpret human instructions and produce accurate outputs. In\nthis work, we aim to systematically study this open question, using the\nstructured task of event extraction for a case study. We experimented with two\nLRMs (DeepSeek-R1 and o1) and two general-purpose Large Language Models (LLMs)\n(GPT-4o and GPT-4.5), when they were used as task models or prompt optimizers.\nOur results show that on tasks as complicated as event extraction, LRMs as task\nmodels still benefit from prompt optimization, and that using LRMs as prompt\noptimizers yields more effective prompts. Finally, we provide an error analysis\nof common errors made by LRMs and highlight the stability and consistency of\nLRMs in refining task instructions and event guidelines."}
{"id": "2504.07395", "pdf": "https://arxiv.org/pdf/2504.07395", "abs": "https://arxiv.org/abs/2504.07395", "authors": ["Arya Fayyazi", "Mehdi Kamal", "Massoud Pedram"], "title": "FAIR-SIGHT: Fairness Assurance in Image Recognition via Simultaneous Conformal Thresholding and Dynamic Output Repair", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We introduce FAIR-SIGHT, an innovative post-hoc framework designed to ensure\nfairness in computer vision systems by combining conformal prediction with a\ndynamic output repair mechanism. Our approach calculates a fairness-aware\nnon-conformity score that simultaneously assesses prediction errors and\nfairness violations. Using conformal prediction, we establish an adaptive\nthreshold that provides rigorous finite-sample, distribution-free guarantees.\nWhen the non-conformity score for a new image exceeds the calibrated threshold,\nFAIR-SIGHT implements targeted corrective adjustments, such as logit shifts for\nclassification and confidence recalibration for detection, to reduce both group\nand individual fairness disparities, all without the need for retraining or\nhaving access to internal model parameters. Comprehensive theoretical analysis\nvalidates our method's error control and convergence properties. At the same\ntime, extensive empirical evaluations on benchmark datasets show that\nFAIR-SIGHT significantly reduces fairness disparities while preserving high\npredictive performance."}
{"id": "2504.07360", "pdf": "https://arxiv.org/pdf/2504.07360", "abs": "https://arxiv.org/abs/2504.07360", "authors": ["Taibiao Zhao", "Xiaobing Chen", "Mingxuan Sun"], "title": "Enhancing Time Series Forecasting via Multi-Level Text Alignment with LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The adaptation of large language models (LLMs) to time series forecasting\nposes unique challenges, as time series data is continuous in nature, while\nLLMs operate on discrete tokens. Despite the success of LLMs in natural\nlanguage processing (NLP) and other structured domains, aligning time series\ndata with language-based representations while maintaining both predictive\naccuracy and interpretability remains a significant hurdle. Existing methods\nhave attempted to reprogram time series data into text-based forms, but these\noften fall short in delivering meaningful, interpretable results. In this\npaper, we propose a multi-level text alignment framework for time series\nforecasting using LLMs that not only improves prediction accuracy but also\nenhances the interpretability of time series representations. Our method\ndecomposes time series into trend, seasonal, and residual components, which are\nthen reprogrammed into component-specific text representations. We introduce a\nmulti-level alignment mechanism, where component-specific embeddings are\naligned with pre-trained word tokens, enabling more interpretable forecasts.\nExperiments on multiple datasets demonstrate that our method outperforms\nstate-of-the-art models in accuracy while providing good interpretability."}
{"id": "2504.07405", "pdf": "https://arxiv.org/pdf/2504.07405", "abs": "https://arxiv.org/abs/2504.07405", "authors": ["Linyan Huang", "Haonan Lin", "Yanning Zhou", "Kaiwen Xiao"], "title": "FlexIP: Dynamic Control of Preservation and Personality for Customized Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "With the rapid advancement of 2D generative models, preserving subject\nidentity while enabling diverse editing has emerged as a critical research\nfocus. Existing methods typically face inherent trade-offs between identity\npreservation and personalized manipulation. We introduce FlexIP, a novel\nframework that decouples these objectives through two dedicated components: a\nPersonalization Adapter for stylistic manipulation and a Preservation Adapter\nfor identity maintenance. By explicitly injecting both control mechanisms into\nthe generative model, our framework enables flexible parameterized control\nduring inference through dynamic tuning of the weight adapter. Experimental\nresults demonstrate that our approach breaks through the performance\nlimitations of conventional methods, achieving superior identity preservation\nwhile supporting more diverse personalized generation capabilities (Project\nPage: https://flexip-tech.github.io/flexip/)."}
{"id": "2504.07385", "pdf": "https://arxiv.org/pdf/2504.07385", "abs": "https://arxiv.org/abs/2504.07385", "authors": ["Sher Badshah", "Ali Emami", "Hassan Sajjad"], "title": "TALE: A Tool-Augmented Framework for Reference-Free Evaluation of Large Language Models", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": null, "summary": "As Large Language Models (LLMs) become increasingly integrated into\nreal-world, autonomous applications, relying on static, pre-annotated\nreferences for evaluation poses significant challenges in cost, scalability,\nand completeness. We propose Tool-Augmented LLM Evaluation (TALE), a framework\nto assess LLM outputs without predetermined ground-truth answers. Unlike\nconventional metrics that compare to fixed references or depend solely on\nLLM-as-a-judge knowledge, TALE employs an agent with tool-access capabilities\nthat actively retrieves and synthesizes external evidence. It iteratively\ngenerates web queries, collects information, summarizes findings, and refines\nsubsequent searches through reflection. By shifting away from static\nreferences, TALE aligns with free-form question-answering tasks common in\nreal-world scenarios. Experimental results on multiple free-form QA benchmarks\nshow that TALE not only outperforms standard reference-based metrics for\nmeasuring response accuracy but also achieves substantial to near-perfect\nagreement with human evaluations. TALE enhances the reliability of LLM\nevaluations in real-world, dynamic scenarios without relying on static\nreferences."}
{"id": "2504.07415", "pdf": "https://arxiv.org/pdf/2504.07415", "abs": "https://arxiv.org/abs/2504.07415", "authors": ["Kyoyun Choi", "Byungmu Yoon", "Soobum Kim", "Jonggwon Park"], "title": "Leveraging LLMs for Multimodal Retrieval-Augmented Radiology Report Generation via Key Phrase Extraction", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "Automated radiology report generation (RRG) holds potential to reduce\nradiologists' workload, especially as recent advancements in large language\nmodels (LLMs) enable the development of multimodal models for chest X-ray (CXR)\nreport generation. However, multimodal LLMs (MLLMs) are resource-intensive,\nrequiring vast datasets and substantial computational cost for training. To\naddress these challenges, we propose a retrieval-augmented generation approach\nthat leverages multimodal retrieval and LLMs to generate radiology reports\nwhile mitigating hallucinations and reducing computational demands. Our method\nuses LLMs to extract key phrases from radiology reports, effectively focusing\non essential diagnostic information. Through exploring effective training\nstrategies, including image encoder structure search, adding noise to text\nembeddings, and additional training objectives, we combine complementary\npre-trained image encoders and adopt contrastive learning between text and\nsemantic image embeddings. We evaluate our approach on MIMIC-CXR dataset,\nachieving state-of-the-art results on CheXbert metrics and competitive RadGraph\nF1 metric alongside MLLMs, without requiring LLM fine-tuning. Our method\ndemonstrates robust generalization for multi-view RRG, making it suitable for\ncomprehensive clinical applications."}
{"id": "2504.07400", "pdf": "https://arxiv.org/pdf/2504.07400", "abs": "https://arxiv.org/abs/2504.07400", "authors": ["Nishanth Nakshatri", "Nikhil Mehta", "Siyi Liu", "Sihao Chen", "Daniel J. Hopkins", "Dan Roth", "Dan Goldwasser"], "title": "Talking Point based Ideological Discourse Analysis in News Events", "categories": ["cs.CL"], "comment": null, "summary": "Analyzing ideological discourse even in the age of LLMs remains a challenge,\nas these models often struggle to capture the key elements that shape\nreal-world narratives. Specifically, LLMs fail to focus on characteristic\nelements driving dominant discourses and lack the ability to integrate\ncontextual information required for understanding abstract ideological views.\nTo address these limitations, we propose a framework motivated by the theory of\nideological discourse analysis to analyze news articles related to real-world\nevents. Our framework represents the news articles using a relational structure\n- talking points, which captures the interaction between entities, their roles,\nand media frames along with a topic of discussion. It then constructs a\nvocabulary of repeating themes - prominent talking points, that are used to\ngenerate ideology-specific viewpoints (or partisan perspectives). We evaluate\nour framework's ability to generate these perspectives through automated tasks\n- ideology and partisan classification tasks, supplemented by human validation.\nAdditionally, we demonstrate straightforward applicability of our framework in\ncreating event snapshots, a visual way of interpreting event discourse. We\nrelease resulting dataset and model to the community to support further\nresearch."}
{"id": "2504.07416", "pdf": "https://arxiv.org/pdf/2504.07416", "abs": "https://arxiv.org/abs/2504.07416", "authors": ["Jonggwon Park", "Soobum Kim", "Byungmu Yoon", "Kyoyun Choi"], "title": "RadZero: Similarity-Based Cross-Attention for Explainable Vision-Language Alignment in Radiology with Zero-Shot Multi-Task Capability", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "Recent advancements in multi-modal models have significantly improved\nvision-language alignment in radiology. However, existing approaches struggle\nto effectively utilize complex radiology reports for learning, rely on\nlow-resolution images, and offer limited interpretability in attention\nmechanisms. To address these challenges, we introduce RadZero, a novel\nsimilarity-based cross-attention framework for vision-language alignment in\nradiology with zero-shot multi-task capability. RadZero leverages large\nlanguage models to extract minimal semantic sentences from radiology reports\nand employs a multi-positive contrastive learning strategy to effectively\ncapture relationships between images and multiple relevant textual\ndescriptions. It also utilizes a pre-trained vision encoder with additional\ntrainable Transformer layers, allowing efficient high-resolution image\nprocessing. By computing similarity between text embeddings and local image\npatch features, RadZero enables zero-shot inference with similarity probability\nfor classification and pixel-level cross-modal similarity maps for grounding\nand segmentation. Experimental results on public chest radiograph benchmarks\nshow that RadZero outperforms state-of-the-art methods in zero-shot\nclassification, grounding, and segmentation. Furthermore, cross-modal\nsimilarity map analysis highlights its potential for improving explainability\nin vision-language alignment. Additionally, qualitative evaluation demonstrates\nRadZero's capability for open-vocabulary semantic segmentation, further\nvalidating its effectiveness in medical imaging."}
{"id": "2504.07408", "pdf": "https://arxiv.org/pdf/2504.07408", "abs": "https://arxiv.org/abs/2504.07408", "authors": ["Samuel Flanders", "Melati Nungsari", "Mark Cheong Wing Loong"], "title": "AI Coding with Few-Shot Prompting for Thematic Analysis", "categories": ["cs.CL"], "comment": null, "summary": "This paper explores the use of large language models (LLMs), here represented\nby GPT 3.5-Turbo to perform coding for a thematic analysis. Coding is highly\nlabor intensive, making it infeasible for most researchers to conduct\nexhaustive thematic analyses of large corpora. We utilize few-shot prompting\nwith higher quality codes generated on semantically similar passages to enhance\nthe quality of the codes while utilizing a cheap, more easily scalable model."}
{"id": "2504.07418", "pdf": "https://arxiv.org/pdf/2504.07418", "abs": "https://arxiv.org/abs/2504.07418", "authors": ["Anning Hu", "Ang Li", "Xirui Jin", "Danping Zou"], "title": "ThermoStereoRT: Thermal Stereo Matching in Real Time via Knowledge Distillation and Attention-based Refinement", "categories": ["cs.CV"], "comment": "7 pages, 6 figures, 4 tables. Accepted to IEEE ICRA 2025. This is the\n  preprint version", "summary": "We introduce ThermoStereoRT, a real-time thermal stereo matching method\ndesigned for all-weather conditions that recovers disparity from two rectified\nthermal stereo images, envisioning applications such as night-time drone\nsurveillance or under-bed cleaning robots. Leveraging a lightweight yet\npowerful backbone, ThermoStereoRT constructs a 3D cost volume from thermal\nimages and employs multi-scale attention mechanisms to produce an initial\ndisparity map. To refine this map, we design a novel channel and spatial\nattention module. Addressing the challenge of sparse ground truth data in\nthermal imagery, we utilize knowledge distillation to boost performance without\nincreasing computational demands. Comprehensive evaluations on multiple\ndatasets demonstrate that ThermoStereoRT delivers both real-time capacity and\nrobust accuracy, making it a promising solution for real-world deployment in\nvarious challenging environments. Our code will be released on\nhttps://github.com/SJTU-ViSYS-team/ThermoStereoRT"}
{"id": "2504.07421", "pdf": "https://arxiv.org/pdf/2504.07421", "abs": "https://arxiv.org/abs/2504.07421", "authors": ["Amirhossein Abaskohi", "Amrutha Varshini Ramesh", "Shailesh Nanisetty", "Chirag Goel", "David Vazquez", "Christopher Pal", "Spandana Gella", "Giuseppe Carenini", "Issam H. Laradji"], "title": "AgentAda: Skill-Adaptive Data Analytics for Tailored Insight Discovery", "categories": ["cs.CL"], "comment": null, "summary": "We introduce AgentAda, the first LLM-powered analytics agent that can learn\nand use new analytics skills to extract more specialized insights. Unlike\nexisting methods that require users to manually decide which data analytics\nmethod to apply, AgentAda automatically identifies the skill needed from a\nlibrary of analytical skills to perform the analysis. This also allows AgentAda\nto use skills that existing LLMs cannot perform out of the box. The library\ncovers a range of methods, including clustering, predictive modeling, and NLP\ntechniques like BERT, which allow AgentAda to handle complex analytics tasks\nbased on what the user needs. AgentAda's dataset-to-insight extraction strategy\nconsists of three key steps: (I) a question generator to generate queries\nrelevant to the user's goal and persona, (II) a hybrid Retrieval-Augmented\nGeneration (RAG)-based skill matcher to choose the best data analytics skill\nfrom the skill library, and (III) a code generator that produces executable\ncode based on the retrieved skill's documentation to extract key patterns. We\nalso introduce KaggleBench, a benchmark of curated notebooks across diverse\ndomains, to evaluate AgentAda's performance. We conducted a human evaluation\ndemonstrating that AgentAda provides more insightful analytics than existing\ntools, with 48.78% of evaluators preferring its analyses, compared to 27.67%\nfor the unskilled agent. We also propose a novel LLM-as-a-judge approach that\nwe show is aligned with human evaluation as a way to automate insight quality\nevaluation at larger scale."}
{"id": "2504.07441", "pdf": "https://arxiv.org/pdf/2504.07441", "abs": "https://arxiv.org/abs/2504.07441", "authors": ["Huilin Yin", "Pengyu Wang", "Senmao Li", "Jun Yan", "Daniel Watzenig"], "title": "WS-DETR: Robust Water Surface Object Detection through Vision-Radar Fusion with Detection Transformer", "categories": ["cs.CV"], "comment": null, "summary": "Robust object detection for Unmanned Surface Vehicles (USVs) in complex water\nenvironments is essential for reliable navigation and operation. Specifically,\nwater surface object detection faces challenges from blurred edges and diverse\nobject scales. Although vision-radar fusion offers a feasible solution,\nexisting approaches suffer from cross-modal feature conflicts, which negatively\naffect model robustness. To address this problem, we propose a robust\nvision-radar fusion model WS-DETR. In particular, we first introduce a\nMulti-Scale Edge Information Integration (MSEII) module to enhance edge\nperception and a Hierarchical Feature Aggregator (HiFA) to boost multi-scale\nobject detection in the encoder. Then, we adopt self-moving point\nrepresentations for continuous convolution and residual connection to\nefficiently extract irregular features under the scenarios of irregular point\ncloud data. To further mitigate cross-modal conflicts, an Adaptive Feature\nInteractive Fusion (AFIF) module is introduced to integrate visual and radar\nfeatures through geometric alignment and semantic fusion. Extensive experiments\non the WaterScenes dataset demonstrate that WS-DETR achieves state-of-the-art\n(SOTA) performance, maintaining its superiority even under adverse weather and\nlighting conditions."}
{"id": "2504.07433", "pdf": "https://arxiv.org/pdf/2504.07433", "abs": "https://arxiv.org/abs/2504.07433", "authors": ["Tingwei Lu", "Yangning Li", "Liyuan Wang", "Binghuai Lin", "Jiwei Tang", "Wanshi Xu", "Hai-Tao Zheng", "Yinghui Li", "Bingxu An", "Zhao Wei", "Yong Xu"], "title": "From Token to Line: Enhancing Code Generation with a Long-Term Perspective", "categories": ["cs.CL"], "comment": null, "summary": "The emergence of large language models (LLMs) has significantly promoted the\ndevelopment of code generation task, sparking a surge in pertinent literature.\nCurrent research is hindered by redundant generation results and a tendency to\noverfit local patterns in the short term. Although existing studies attempt to\nalleviate the issue by adopting a multi-token prediction strategy, there\nremains limited focus on choosing the appropriate processing length for\ngenerations. By analyzing the attention between tokens during the generation\nprocess of LLMs, it can be observed that the high spikes of the attention\nscores typically appear at the end of lines. This insight suggests that it is\nreasonable to treat each line of code as a fundamental processing unit and\ngenerate them sequentially. Inspired by this, we propose the \\textbf{LSR-MCTS}\nalgorithm, which leverages MCTS to determine the code line-by-line and select\nthe optimal path. Further, we integrate a self-refine mechanism at each node to\nenhance diversity and generate higher-quality programs through error\ncorrection. Extensive experiments and comprehensive analyses on three public\ncoding benchmarks demonstrate that our method outperforms the state-of-the-art\nperformance approaches."}
{"id": "2504.07454", "pdf": "https://arxiv.org/pdf/2504.07454", "abs": "https://arxiv.org/abs/2504.07454", "authors": ["Zitian Tang", "Shijie Wang", "Junho Cho", "Jaewook Yoo", "Chen Sun"], "title": "How Can Objects Help Video-Language Understanding?", "categories": ["cs.CV"], "comment": null, "summary": "How multimodal large language models (MLLMs) perceive the visual world\nremains a mystery. To one extreme, object and relation modeling may be\nimplicitly implemented with inductive biases, for example by treating objects\nas tokens. To the other extreme, empirical results reveal the surprising\nfinding that simply performing visual captioning, which tends to ignore spatial\nconfiguration of the objects, serves as a strong baseline for video\nunderstanding. We aim to answer the question: how can objects help\nvideo-language understanding in MLLMs? We tackle the question from the object\nrepresentation and adaptation perspectives. Specifically, we investigate the\ntrade-off between representation expressiveness (e.g., distributed versus\nsymbolic) and integration difficulty (e.g., data-efficiency when learning the\nadapters). Through extensive evaluations on five video question answering\ndatasets, we confirm that explicit integration of object-centric representation\nremains necessary, and the symbolic objects can be most easily integrated while\nbeing performant for question answering. We hope our findings can encourage the\ncommunity to explore the explicit integration of perception modules into MLLM\ndesign. Our code and models will be publicly released."}
{"id": "2504.07440", "pdf": "https://arxiv.org/pdf/2504.07440", "abs": "https://arxiv.org/abs/2504.07440", "authors": ["Yixin Cao", "Jiahao Ying", "Yaoning Wang", "Xipeng Qiu", "Xuanjing Huang", "Yugang Jiang"], "title": "Revisiting LLM Evaluation through Mechanism Interpretability: a New Metric and Model Utility Law", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have become indispensable across academia,\nindustry, and daily applications, yet current evaluation methods struggle to\nkeep pace with their rapid development. In this paper, we analyze the core\nlimitations of traditional evaluation pipelines and propose a novel metric, the\nModel Utilization Index (MUI), which introduces mechanism interpretability\ntechniques to complement traditional performance metrics. MUI quantifies the\nextent to which a model leverages its capabilities to complete tasks. The core\nidea is that to assess an LLM's overall ability, we must evaluate not only its\ntask performance but also the effort expended to achieve the outcome. Our\nextensive experiments reveal an inverse relationship between MUI and\nperformance, from which we deduce a common trend observed in popular LLMs,\nwhich we term the Utility Law. Based on this, we derive four corollaries that\naddress key challenges, including training judgement, the issue of data\ncontamination, fairness in model comparison, and data diversity. We hope that\nour survey, novel metric, and utility law will foster mutual advancement in\nboth evaluation and mechanism interpretability. Our code can be found at\nhttps://github.com/ALEX-nlp/MUI-Eva."}
{"id": "2504.07462", "pdf": "https://arxiv.org/pdf/2504.07462", "abs": "https://arxiv.org/abs/2504.07462", "authors": ["Hengrun Zhao", "Yunzhi Zhuge", "Yifan Wang", "Lijun Wang", "Huchuan Lu", "Yu Zeng"], "title": "Learning Universal Features for Generalizable Image Forgery Localization", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, advanced image editing and generation methods have rapidly\nevolved, making detecting and locating forged image content increasingly\nchallenging. Most existing image forgery detection methods rely on identifying\nthe edited traces left in the image. However, because the traces of different\nforgeries are distinct, these methods can identify familiar forgeries included\nin the training data but struggle to handle unseen ones. In response, we\npresent an approach for Generalizable Image Forgery Localization (GIFL). Once\ntrained, our model can detect both seen and unseen forgeries, providing a more\npractical and efficient solution to counter false information in the era of\ngenerative AI. Our method focuses on learning general features from the\npristine content rather than traces of specific forgeries, which are relatively\nconsistent across different types of forgeries and therefore can be used as\nuniversal features to locate unseen forgeries. Additionally, as existing image\nforgery datasets are still dominated by traditional hand-crafted forgeries, we\nconstruct a new dataset consisting of images edited by various popular deep\ngenerative image editing methods to further encourage research in detecting\nimages manipulated by deep generative models. Extensive experimental results\nshow that the proposed approach outperforms state-of-the-art methods in the\ndetection of unseen forgeries and also demonstrates competitive results for\nseen forgeries. The code and dataset are available at\nhttps://github.com/ZhaoHengrun/GIFL."}
{"id": "2504.07459", "pdf": "https://arxiv.org/pdf/2504.07459", "abs": "https://arxiv.org/abs/2504.07459", "authors": ["Zehan Li", "Ruhua Pan", "Xinyu Pi"], "title": "Beyond LLMs: A Linguistic Approach to Causal Graph Generation from Narrative Texts", "categories": ["cs.CL"], "comment": "published at the 7th Workshop on Narrative Understanding, NAACL 2025", "summary": "We propose a novel framework for generating causal graphs from narrative\ntexts, bridging high-level causality and detailed event-specific relationships.\nOur method first extracts concise, agent-centered vertices using large language\nmodel (LLM)-based summarization. We introduce an \"Expert Index,\" comprising\nseven linguistically informed features, integrated into a\nSituation-Task-Action-Consequence (STAC) classification model. This hybrid\nsystem, combining RoBERTa embeddings with the Expert Index, achieves superior\nprecision in causal link identification compared to pure LLM-based approaches.\nFinally, a structured five-iteration prompting process refines and constructs\nconnected causal graphs. Experiments on 100 narrative chapters and short\nstories demonstrate that our approach consistently outperforms GPT-4o and\nClaude 3.5 in causal graph quality, while maintaining readability. The\nopen-source tool provides an interpretable, efficient solution for capturing\nnuanced causal chains in narratives."}
{"id": "2504.07476", "pdf": "https://arxiv.org/pdf/2504.07476", "abs": "https://arxiv.org/abs/2504.07476", "authors": ["Yan Xu", "Zhenqiang Zhang", "Zhiwei Zhou", "Liting Geng", "Yue Li", "Jintao Li"], "title": "CMEdataset Advancing China Map Detection and Standardization with Digital Image Resources", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Digital images of Chinas maps play a crucial role in map detection,\nparticularly in ensuring national sovereignty, territorial integrity, and map\ncompliance. However, there is currently no publicly available dataset\nspecifically dedicated to problematic maps the CME dataset. Existing datasets\nprimarily focus on general map data and are insufficient for effectively\nidentifying complex issues such as national boundary misrepresentations,\nmissing elements, and blurred boundaries. Therefore, this study creates a\nProblematic Map dataset that covers five key problem areas, aiming to provide\ndiverse samples for problematic map detection technologies, support\nhigh-precision map compliance detection, and enhance map data quality and\ntimeliness. This dataset not only provides essential resources for map\ncompliance, national security monitoring, and map updates, but also fosters\ninnovation and application of related technologies."}
{"id": "2504.07467", "pdf": "https://arxiv.org/pdf/2504.07467", "abs": "https://arxiv.org/abs/2504.07467", "authors": ["Ruiyi Zhang", "David Sullivan", "Kyle Jackson", "Pengtao Xie", "Mei Chen"], "title": "Defense against Prompt Injection Attacks via Mixture of Encodings", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have emerged as a dominant approach for a wide\nrange of NLP tasks, with their access to external information further enhancing\ntheir capabilities. However, this introduces new vulnerabilities, known as\nprompt injection attacks, where external content embeds malicious instructions\nthat manipulate the LLM's output. Recently, the Base64 defense has been\nrecognized as one of the most effective methods for reducing success rate of\nprompt injection attacks. Despite its efficacy, this method can degrade LLM\nperformance on certain NLP tasks. To address this challenge, we propose a novel\ndefense mechanism: mixture of encodings, which utilizes multiple character\nencodings, including Base64. Extensive experimental results show that our\nmethod achieves one of the lowest attack success rates under prompt injection\nattacks, while maintaining high performance across all NLP tasks, outperforming\nexisting character encoding-based defense methods. This underscores the\neffectiveness of our mixture of encodings strategy for both safety and task\nperformance metrics."}
{"id": "2504.07491", "pdf": "https://arxiv.org/pdf/2504.07491", "abs": "https://arxiv.org/abs/2504.07491", "authors": ["Kimi Team", "Angang Du", "Bohong Yin", "Bowei Xing", "Bowen Qu", "Bowen Wang", "Cheng Chen", "Chenlin Zhang", "Chenzhuang Du", "Chu Wei", "Congcong Wang", "Dehao Zhang", "Dikang Du", "Dongliang Wang", "Enming Yuan", "Enzhe Lu", "Fang Li", "Flood Sung", "Guangda Wei", "Guokun Lai", "Han Zhu", "Hao Ding", "Hao Hu", "Hao Yang", "Hao Zhang", "Haoning Wu", "Haotian Yao", "Haoyu Lu", "Heng Wang", "Hongcheng Gao", "Huabin Zheng", "Jiaming Li", "Jianlin Su", "Jianzhou Wang", "Jiaqi Deng", "Jiezhong Qiu", "Jin Xie", "Jinhong Wang", "Jingyuan Liu", "Junjie Yan", "Kun Ouyang", "Liang Chen", "Lin Sui", "Longhui Yu", "Mengfan Dong", "Mengnan Dong", "Nuo Xu", "Pengyu Cheng", "Qizheng Gu", "Runjie Zhou", "Shaowei Liu", "Sihan Cao", "Tao Yu", "Tianhui Song", "Tongtong Bai", "Wei Song", "Weiran He", "Weixiao Huang", "Weixin Xu", "Xiaokun Yuan", "Xingcheng Yao", "Xingzhe Wu", "Xinxing Zu", "Xinyu Zhou", "Xinyuan Wang", "Y. Charles", "Yan Zhong", "Yang Li", "Yangyang Hu", "Yanru Chen", "Yejie Wang", "Yibo Liu", "Yibo Miao", "Yidao Qin", "Yimin Chen", "Yiping Bao", "Yiqin Wang", "Yongsheng Kang", "Yuanxin Liu", "Yulun Du", "Yuxin Wu", "Yuzhi Wang", "Yuzi Yan", "Zaida Zhou", "Zhaowei Li", "Zhejun Jiang", "Zheng Zhang", "Zhilin Yang", "Zhiqi Huang", "Zihao Huang", "Zijia Zhao", "Ziwei Chen"], "title": "Kimi-VL Technical Report", "categories": ["cs.CV"], "comment": null, "summary": "We present Kimi-VL, an efficient open-source Mixture-of-Experts (MoE)\nvision-language model (VLM) that offers advanced multimodal reasoning,\nlong-context understanding, and strong agent capabilities - all while\nactivating only 2.8B parameters in its language decoder (Kimi-VL-A3B). Kimi-VL\ndemonstrates strong performance across challenging domains: as a\ngeneral-purpose VLM, Kimi-VL excels in multi-turn agent tasks (e.g., OSWorld),\nmatching flagship models. Furthermore, it exhibits remarkable capabilities\nacross diverse challenging vision language tasks, including college-level image\nand video comprehension, OCR, mathematical reasoning, and multi-image\nunderstanding. In comparative evaluations, it effectively competes with\ncutting-edge efficient VLMs such as GPT-4o-mini, Qwen2.5-VL-7B, and\nGemma-3-12B-IT, while surpassing GPT-4o in several key domains. Kimi-VL also\nadvances in processing long contexts and perceiving clearly. With a 128K\nextended context window, Kimi-VL can process diverse long inputs, achieving\nimpressive scores of 64.5 on LongVideoBench and 35.1 on MMLongBench-Doc. Its\nnative-resolution vision encoder, MoonViT, further allows it to see and\nunderstand ultra-high-resolution visual inputs, achieving 83.2 on InfoVQA and\n34.5 on ScreenSpot-Pro, while maintaining lower computational cost for common\ntasks. Building upon Kimi-VL, we introduce an advanced long-thinking variant:\nKimi-VL-Thinking. Developed through long chain-of-thought (CoT) supervised\nfine-tuning (SFT) and reinforcement learning (RL), this model exhibits strong\nlong-horizon reasoning capabilities. It achieves scores of 61.7 on MMMU, 36.8\non MathVision, and 71.3 on MathVista while maintaining the compact 2.8B\nactivated LLM parameters, setting a new standard for efficient multimodal\nthinking models. Code and models are publicly accessible at\nhttps://github.com/MoonshotAI/Kimi-VL."}
{"id": "2504.07470", "pdf": "https://arxiv.org/pdf/2504.07470", "abs": "https://arxiv.org/abs/2504.07470", "authors": ["Xin Su", "Phillip Howard", "Steven Bethard"], "title": "Transformer-Based Temporal Information Extraction and Application: A Review", "categories": ["cs.CL"], "comment": null, "summary": "Temporal information extraction (IE) aims to extract structured temporal\ninformation from unstructured text, thereby uncovering the implicit timelines\nwithin. This technique is applied across domains such as healthcare, newswire,\nand intelligence analysis, aiding models in these areas to perform temporal\nreasoning and enabling human users to grasp the temporal structure of text.\nTransformer-based pre-trained language models have produced revolutionary\nadvancements in natural language processing, demonstrating exceptional\nperformance across a multitude of tasks. Despite the achievements garnered by\nTransformer-based approaches in temporal IE, there is a lack of comprehensive\nreviews on these endeavors. In this paper, we aim to bridge this gap by\nsystematically summarizing and analyzing the body of work on temporal IE using\nTransformers while highlighting potential future research directions."}
{"id": "2504.07503", "pdf": "https://arxiv.org/pdf/2504.07503", "abs": "https://arxiv.org/abs/2504.07503", "authors": ["Jinze Chen", "Wei Zhai", "Yang Cao", "Bin Li", "Zheng-Jun Zha"], "title": "Event Signal Filtering via Probability Flux Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Events offer a novel paradigm for capturing scene dynamics via asynchronous\nsensing, but their inherent randomness often leads to degraded signal quality.\nEvent signal filtering is thus essential for enhancing fidelity by reducing\nthis internal randomness and ensuring consistent outputs across diverse\nacquisition conditions. Unlike traditional time series that rely on fixed\ntemporal sampling to capture steady-state behaviors, events encode transient\ndynamics through polarity and event intervals, making signal modeling\nsignificantly more complex. To address this, the theoretical foundation of\nevent generation is revisited through the lens of diffusion processes. The\nstate and process information within events is modeled as continuous\nprobability flux at threshold boundaries of the underlying irradiance\ndiffusion. Building on this insight, a generative, online filtering framework\ncalled Event Density Flow Filter (EDFilter) is introduced. EDFilter estimates\nevent correlation by reconstructing the continuous probability flux from\ndiscrete events using nonparametric kernel smoothing, and then resamples\nfiltered events from this flux. To optimize fidelity over time, spatial and\ntemporal kernels are employed in a time-varying optimization framework. A fast\nrecursive solver with O(1) complexity is proposed, leveraging state-space\nmodels and lookup tables for efficient likelihood computation. Furthermore, a\nnew real-world benchmark Rotary Event Dataset (RED) is released, offering\nmicrosecond-level ground truth irradiance for full-reference event filtering\nevaluation. Extensive experiments validate EDFilter's performance across tasks\nlike event filtering, super-resolution, and direct event-based blob tracking.\nSignificant gains in downstream applications such as SLAM and video\nreconstruction underscore its robustness and effectiveness."}
{"id": "2504.07490", "pdf": "https://arxiv.org/pdf/2504.07490", "abs": "https://arxiv.org/abs/2504.07490", "authors": ["Nanmanas Linphrachaya", "Irving G√≥mez-M√©ndez", "Adil Siripatana"], "title": "Geological Inference from Textual Data using Word Embeddings", "categories": ["cs.CL", "stat.ME"], "comment": null, "summary": "This research explores the use of Natural Language Processing (NLP)\ntechniques to locate geological resources, with a specific focus on industrial\nminerals. By using word embeddings trained with the GloVe model, we extract\nsemantic relationships between target keywords and a corpus of geological\ntexts. The text is filtered to retain only words with geographical\nsignificance, such as city names, which are then ranked by their cosine\nsimilarity to the target keyword. Dimensional reduction techniques, including\nPrincipal Component Analysis (PCA), Autoencoder, Variational Autoencoder (VAE),\nand VAE with Long Short-Term Memory (VAE-LSTM), are applied to enhance feature\nextraction and improve the accuracy of semantic relations.\n  For benchmarking, we calculate the proximity between the ten cities most\nsemantically related to the target keyword and identified mine locations using\nthe haversine equation. The results demonstrate that combining NLP with\ndimensional reduction techniques provides meaningful insights into the spatial\ndistribution of natural resources. Although the result shows to be in the same\nregion as the supposed location, the accuracy has room for improvement."}
{"id": "2504.07519", "pdf": "https://arxiv.org/pdf/2504.07519", "abs": "https://arxiv.org/abs/2504.07519", "authors": ["Henghao Zhao", "Ge-Peng Ji", "Rui Yan", "Huan Xiong", "Zechao Li"], "title": "VideoExpert: Augmented LLM for Temporal-Sensitive Video Understanding", "categories": ["cs.CV"], "comment": null, "summary": "The core challenge in video understanding lies in perceiving dynamic content\nchanges over time. However, multimodal large language models struggle with\ntemporal-sensitive video tasks, which requires generating timestamps to mark\nthe occurrence of specific events. Existing strategies require MLLMs to\ngenerate absolute or relative timestamps directly. We have observed that those\nMLLMs tend to rely more on language patterns than visual cues when generating\ntimestamps, affecting their performance. To address this problem, we propose\nVideoExpert, a general-purpose MLLM suitable for several temporal-sensitive\nvideo tasks. Inspired by the expert concept, VideoExpert integrates two\nparallel modules: the Temporal Expert and the Spatial Expert. The Temporal\nExpert is responsible for modeling time sequences and performing temporal\ngrounding. It processes high-frame-rate yet compressed tokens to capture\ndynamic variations in videos and includes a lightweight prediction head for\nprecise event localization. The Spatial Expert focuses on content detail\nanalysis and instruction following. It handles specially designed spatial\ntokens and language input, aiming to generate content-related responses. These\ntwo experts collaborate seamlessly via a special token, ensuring coordinated\ntemporal grounding and content generation. Notably, the Temporal and Spatial\nExperts maintain independent parameter sets. By offloading temporal grounding\nfrom content generation, VideoExpert prevents text pattern biases in timestamp\npredictions. Moreover, we introduce a Spatial Compress module to obtain spatial\ntokens. This module filters and compresses patch tokens while preserving key\ninformation, delivering compact yet detail-rich input for the Spatial Expert.\nExtensive experiments demonstrate the effectiveness and versatility of the\nVideoExpert."}
{"id": "2504.07527", "pdf": "https://arxiv.org/pdf/2504.07527", "abs": "https://arxiv.org/abs/2504.07527", "authors": ["Junjie Zhang", "Rushuai Yang", "Shunyu Liu", "Ting-En Lin", "Fei Huang", "Yi Chen", "Yongbin Li", "Dacheng Tao"], "title": "Supervised Optimism Correction: Be Confident When LLMs Are Sure", "categories": ["cs.CL"], "comment": null, "summary": "In this work, we establish a novel theoretical connection between supervised\nfine-tuning and offline reinforcement learning under the token-level Markov\ndecision process, revealing that large language models indeed learn an implicit\n$Q$-function for inference. Through this theoretical lens, we demonstrate that\nthe widely used beam search method suffers from unacceptable over-optimism,\nwhere inference errors are inevitably amplified due to inflated $Q$-value\nestimations of suboptimal steps. To address this limitation, we propose\nSupervised Optimism Correction(SOC), which introduces a simple yet effective\nauxiliary loss for token-level $Q$-value estimations during supervised\nfine-tuning. Specifically, the auxiliary loss employs implicit value\nregularization to boost model confidence in expert-demonstrated responses,\nthereby suppressing over-optimism toward insufficiently supervised responses.\nExtensive experiments on mathematical reasoning benchmarks, including GSM8K,\nMATH, and GAOKAO, showcase the superiority of the proposed SOC with beam search\nacross a series of open-source models."}
{"id": "2504.07524", "pdf": "https://arxiv.org/pdf/2504.07524", "abs": "https://arxiv.org/abs/2504.07524", "authors": ["Xu Zhao", "Pengju Zhang", "Bo Liu", "Yihong Wu"], "title": "DGOcc: Depth-aware Global Query-based Network for Monocular 3D Occupancy Prediction", "categories": ["cs.CV"], "comment": "under review", "summary": "Monocular 3D occupancy prediction, aiming to predict the occupancy and\nsemantics within interesting regions of 3D scenes from only 2D images, has\ngarnered increasing attention recently for its vital role in 3D scene\nunderstanding. Predicting the 3D occupancy of large-scale outdoor scenes from\n2D images is ill-posed and resource-intensive. In this paper, we present\n\\textbf{DGOcc}, a \\textbf{D}epth-aware \\textbf{G}lobal query-based network for\nmonocular 3D \\textbf{Occ}upancy prediction. We first explore prior depth maps\nto extract depth context features that provide explicit geometric information\nfor the occupancy network. Then, in order to fully exploit the depth context\nfeatures, we propose a Global Query-based (GQ) Module. The cooperation of\nattention mechanisms and scale-aware operations facilitates the feature\ninteraction between images and 3D voxels. Moreover, a Hierarchical Supervision\nStrategy (HSS) is designed to avoid upsampling the high-dimension 3D voxel\nfeatures to full resolution, which mitigates GPU memory utilization and time\ncost. Extensive experiments on SemanticKITTI and SSCBench-KITTI-360 datasets\ndemonstrate that the proposed method achieves the best performance on monocular\nsemantic occupancy prediction while reducing GPU and time overhead."}
{"id": "2504.07532", "pdf": "https://arxiv.org/pdf/2504.07532", "abs": "https://arxiv.org/abs/2504.07532", "authors": ["Tuhin Chakrabarty", "Philippe Laban", "Chien-Sheng Wu"], "title": "AI-Slop to AI-Polish? Aligning Language Models through Edit-Based Writing Rewards and Test-time Computation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Under Submission", "summary": "AI-generated text is proliferating across domains, from creative writing and\njournalism to marketing content and scientific articles. Models can follow\nuser-provided instructions to generate coherent and grammatically correct\noutputs but in this work, we study a more fundamental question: how do we\nevaluate and improve the writing quality of AI-generated text? Writing quality\nassessment has received less attention from the community, in part because it\nis fundamentally subjective and requires expertise. We first introduce the\nWriting Quality Benchmark (WQ) by consolidating five writing-preference\ndatasets into 4,729 writing quality judgments. Our experiments show that\ncompetitive baselines, including state-of-the-art LLMs that excel at reasoning\ntasks, barely outperform random baselines on WQ. We then train specialized\nWriting Quality Reward Models (WQRM) of various sizes for writing quality\nassessment that demonstrate strong generalization on four out-of-distribution\ntest sets and 74% accuracy on the WQ benchmark. To further show WQRM's\npractical benefits during inference, we leverage additional test-time compute\nto generate and rank multiple candidate revisions, allowing us to select\nhigher-quality outputs from an initial draft. Human evaluation with 9\nexperienced writers confirm that WQRM-based selection produces writing samples\npreferred by experts 66% overall, and 72.2% when the reward gap is larger than\n1 point. We release our datasets and models to encourage community engagement\nwith writing quality assessment and development of AI writing systems better\naligned with human preferences."}
{"id": "2504.07542", "pdf": "https://arxiv.org/pdf/2504.07542", "abs": "https://arxiv.org/abs/2504.07542", "authors": ["Hongyu Lyu", "Julie Stephany Berrio", "Mao Shan", "Stewart Worrall"], "title": "SydneyScapes: Image Segmentation for Australian Environments", "categories": ["cs.CV"], "comment": null, "summary": "Autonomous Vehicles (AVs) are being partially deployed and tested across\nvarious global locations, including China, the USA, Germany, France, Japan,\nKorea, and the UK, but with limited demonstrations in Australia. The\nintegration of machine learning (ML) into AV perception systems highlights the\nneed for locally labelled datasets to develop and test algorithms in specific\nenvironments. To address this, we introduce SydneyScapes - a dataset tailored\nfor computer vision tasks of image semantic, instance, and panoptic\nsegmentation. This dataset, collected from Sydney and surrounding cities in New\nSouth Wales (NSW), Australia, consists of 756 images with high-quality\npixel-level annotations. It is designed to assist AV industry and researchers\nby providing annotated data and tools for algorithm development, testing, and\ndeployment in the Australian context. Additionally, we offer benchmarking\nresults using state-of-the-art algorithms to establish reference points for\nfuture research and development. The dataset is publicly available at\nhttps://hdl.handle.net/2123/33051."}
{"id": "2504.07583", "pdf": "https://arxiv.org/pdf/2504.07583", "abs": "https://arxiv.org/abs/2504.07583", "authors": ["Patrick Fernandes", "Sweta Agrawal", "Emmanouil Zaranis", "Andr√© F. T. Martins", "Graham Neubig"], "title": "Do LLMs Understand Your Translations? Evaluating Paragraph-level MT with Question Answering", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Despite the steady progress in machine translation evaluation, existing\nautomatic metrics struggle to capture how well meaning is preserved beyond\nsentence boundaries. We posit that reliance on a single intrinsic quality\nscore, trained to mimic human judgments, might be insufficient for evaluating\ntranslations of long, complex passages, and a more ``pragmatic'' approach that\nassesses how accurately key information is conveyed by a translation in context\nis needed. We introduce TREQA (Translation Evaluation via Question-Answering),\na framework that extrinsically evaluates translation quality by assessing how\naccurately candidate translations answer reading comprehension questions that\ntarget key information in the original source or reference texts. In\nchallenging domains that require long-range understanding, such as literary\ntexts, we show that TREQA is competitive with and, in some cases, outperforms\nstate-of-the-art neural and LLM-based metrics in ranking alternative\nparagraph-level translations, despite never being explicitly optimized to\ncorrelate with human judgments. Furthermore, the generated questions and\nanswers offer interpretability: empirical analysis shows that they effectively\ntarget translation errors identified by experts in evaluated datasets. Our code\nis available at https://github.com/deep-spin/treqa"}
{"id": "2504.07549", "pdf": "https://arxiv.org/pdf/2504.07549", "abs": "https://arxiv.org/abs/2504.07549", "authors": ["Bingliang Zhang", "Zihui Wu", "Berthy T. Feng", "Yang Song", "Yisong Yue", "Katherine L. Bouman"], "title": "STeP: A General and Scalable Framework for Solving Video Inverse Problems with Spatiotemporal Diffusion Priors", "categories": ["cs.CV"], "comment": null, "summary": "We study how to solve general Bayesian inverse problems involving videos\nusing diffusion model priors. While it is desirable to use a video diffusion\nprior to effectively capture complex temporal relationships, due to the\ncomputational and data requirements of training such a model, prior work has\ninstead relied on image diffusion priors on single frames combined with\nheuristics to enforce temporal consistency. However, these approaches struggle\nwith faithfully recovering the underlying temporal relationships, particularly\nfor tasks with high temporal uncertainty. In this paper, we demonstrate the\nfeasibility of practical and accessible spatiotemporal diffusion priors by\nfine-tuning latent video diffusion models from pretrained image diffusion\nmodels using limited videos in specific domains. Leveraging this plug-and-play\nspatiotemporal diffusion prior, we introduce a general and scalable framework\nfor solving video inverse problems. We then apply our framework to two\nchallenging scientific video inverse problems--black hole imaging and dynamic\nMRI. Our framework enables the generation of diverse, high-fidelity video\nreconstructions that not only fit observations but also recover multi-modal\nsolutions. By incorporating a spatiotemporal diffusion prior, we significantly\nimprove our ability to capture complex temporal relationships in the data while\nalso enhancing spatial fidelity."}
{"id": "2504.07612", "pdf": "https://arxiv.org/pdf/2504.07612", "abs": "https://arxiv.org/abs/2504.07612", "authors": ["Mihnea-Alexandru V√Ærlan", "RƒÉzvan-Alexandru SmƒÉdu", "Dumitru-Clementin Cercel"], "title": "SaRoHead: A Dataset for Satire Detection in Romanian Multi-Domain News Headlines", "categories": ["cs.CL"], "comment": "5 pages, 1 figure", "summary": "The headline is an important part of a news article, influenced by\nexpressiveness and connection to the exposed subject. Although most news\noutlets aim to present reality objectively, some publications prefer a humorous\napproach in which stylistic elements of satire, irony, and sarcasm blend to\ncover specific topics. Satire detection can be difficult because a headline\naims to expose the main idea behind a news article. In this paper, we propose\nSaRoHead, the first corpus for satire detection in Romanian multi-domain news\nheadlines. Our findings show that the clickbait used in some non-satirical\nheadlines significantly influences the model."}
{"id": "2504.07556", "pdf": "https://arxiv.org/pdf/2504.07556", "abs": "https://arxiv.org/abs/2504.07556", "authors": ["Zijian Zhang", "Xuhui Zheng", "Xuecheng Wu", "Chong Peng", "Xuezhi Cao"], "title": "TokenFocus-VQA: Enhancing Text-to-Image Alignment with Position-Aware Focus and Multi-Perspective Aggregations on LVLMs", "categories": ["cs.CV"], "comment": "10 pages, 3 figures", "summary": "While text-to-image (T2I) generation models have achieved remarkable progress\nin recent years, existing evaluation methodologies for vision-language\nalignment still struggle with the fine-grained semantic matching. Current\napproaches based on global similarity metrics often overlook critical\ntoken-level correspondences between textual descriptions and visual content. To\nthis end, we present TokenFocus-VQA, a novel evaluation framework that\nleverages Large Vision-Language Models (LVLMs) through visual question\nanswering (VQA) paradigm with position-specific probability optimization. Our\nkey innovation lies in designing a token-aware loss function that selectively\nfocuses on probability distributions at pre-defined vocabulary positions\ncorresponding to crucial semantic elements, enabling precise measurement of\nfine-grained semantical alignment. The proposed framework further integrates\nensemble learning techniques to aggregate multi-perspective assessments from\ndiverse LVLMs architectures, thereby achieving further performance enhancement.\nEvaluated on the NTIRE 2025 T2I Quality Assessment Challenge Track 1, our\nTokenFocus-VQA ranks 2nd place (0.8445, only 0.0001 lower than the 1st method)\non public evaluation and 2nd place (0.8426) on the official private test set,\ndemonstrating superiority in capturing nuanced text-image correspondences\ncompared to conventional evaluation methods."}
{"id": "2504.07624", "pdf": "https://arxiv.org/pdf/2504.07624", "abs": "https://arxiv.org/abs/2504.07624", "authors": ["Joel Barmettler", "Abraham Bernstein", "Luca Rossetto"], "title": "ConceptFormer: Towards Efficient Use of Knowledge-Graph Embeddings in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Retrieval Augmented Generation (RAG) has enjoyed increased attention in the\nrecent past and recent advancements in Large Language Models (LLMs) have\nhighlighted the importance of integrating world knowledge into these systems.\nCurrent RAG methodologies often modify the internal architecture of pre-trained\nlanguage models (PLMs) or rely on textifying knowledge graphs (KGs), which is\ninefficient in terms of token usage. This paper introduces ConceptFormer, a new\napproach to augment LLMs with structured knowledge from KGs, such as Wikidata,\nwithout altering their internal structure or relying on textual input of KGs.\nConceptFormer operates in the LLM embedding vector space, creating and\ninjecting \\emph{concept vectors} that encapsulate the information of the KG\nnodes directly. Trained in conjunction with a frozen LLM, ConceptFormer\ngenerates a comprehensive lookup table that maps KG nodes to their respective\nconcept vectors. The approach aims to enhance the factual recall capabilities\nof LLMs by enabling them to process these concept vectors natively, thus\nenriching them with structured world knowledge in an efficient and scalable\nmanner. Our experiments demonstrate that the addition of concept vectors to\nGPT-2 0.1B substantially increases its factual recall ability (Hit@10) by up to\n272\\% when tested on sentences from Wikipedia and up to 348\\% on synthetically\ngenerated sentences. Even injecting only a single concept vector into the\nprompt increases factual recall ability (Hit@10) by up to 213\\% on Wikipedia\nsentences, significantly outperforming RAG with graph textification while\nconsuming 130x fewer input tokens."}
{"id": "2504.07567", "pdf": "https://arxiv.org/pdf/2504.07567", "abs": "https://arxiv.org/abs/2504.07567", "authors": ["Urszula Czerwinska", "Cenk Bircanoglu", "Jeremy Chamoux"], "title": "Benchmarking Image Embeddings for E-Commerce: Evaluating Off-the Shelf Foundation Models, Fine-Tuning Strategies and Practical Trade-offs", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.IR", "cs.LG"], "comment": "accepted at Future Technologies Conference (FTC 2025)", "summary": "We benchmark foundation models image embeddings for classification and\nretrieval in e-Commerce, evaluating their suitability for real-world\napplications. Our study spans embeddings from pre-trained convolutional and\ntransformer models trained via supervised, self-supervised, and text-image\ncontrastive learning. We assess full fine-tuning and transfer learning\n(top-tuning) on six diverse e-Commerce datasets: fashion, consumer goods, cars,\nfood, and retail. Results show full fine-tuning consistently performs well,\nwhile text-image and self-supervised embeddings can match its performance with\nless training. While supervised embeddings remain stable across architectures,\nSSL and contrastive embeddings vary significantly, often benefiting from\ntop-tuning. Top-tuning emerges as an efficient alternative to full fine-tuning,\nreducing computational costs. We also explore cross-tuning, noting its impact\ndepends on dataset characteristics. Our findings offer practical guidelines for\nembedding selection and fine-tuning strategies, balancing efficiency and\nperformance."}
{"id": "2504.07646", "pdf": "https://arxiv.org/pdf/2504.07646", "abs": "https://arxiv.org/abs/2504.07646", "authors": ["Alfredo Garrach√≥n Ruiz", "Tom√°s de la Rosa", "Daniel Borrajo"], "title": "On the Temporal Question-Answering Capabilities of Large Language Models Over Anonymized Data", "categories": ["cs.CL", "cs.AI"], "comment": "18 pages, 7 tables, 5 figures", "summary": "The applicability of Large Language Models (LLMs) in temporal reasoning tasks\nover data that is not present during training is still a field that remains to\nbe explored. In this paper we work on this topic, focusing on structured and\nsemi-structured anonymized data. We not only develop a direct LLM pipeline, but\nalso compare various methodologies and conduct an in-depth analysis. We\nidentified and examined seventeen common temporal reasoning tasks in natural\nlanguage, focusing on their algorithmic components. To assess LLM performance,\nwe created the \\textit{Reasoning and Answering Temporal Ability} dataset\n(RATA), featuring semi-structured anonymized data to ensure reliance on\nreasoning rather than on prior knowledge. We compared several methodologies,\ninvolving SoTA techniques such as Tree-of-Thought, self-reflexion and code\nexecution, tuned specifically for this scenario. Our results suggest that\nachieving scalable and reliable solutions requires more than just standalone\nLLMs, highlighting the need for integrated approaches."}
{"id": "2504.07598", "pdf": "https://arxiv.org/pdf/2504.07598", "abs": "https://arxiv.org/abs/2504.07598", "authors": ["Adrian Cosma", "Andy C«étrun«é", "Emilian R«édoi"], "title": "On Model and Data Scaling for Skeleton-based Self-Supervised Gait Recognition", "categories": ["cs.CV"], "comment": "10 pages, 10 Figures, 3 Tables", "summary": "Gait recognition from video streams is a challenging problem in computer\nvision biometrics due to the subtle differences between gaits and numerous\nconfounding factors. Recent advancements in self-supervised pretraining have\nled to the development of robust gait recognition models that are invariant to\nwalking covariates. While neural scaling laws have transformed model\ndevelopment in other domains by linking performance to data, model size, and\ncompute, their applicability to gait remains unexplored. In this work, we\nconduct the first empirical study scaling on skeleton-based self-supervised\ngait recognition to quantify the effect of data quantity, model size and\ncompute on downstream gait recognition performance. We pretrain multiple\nvariants of GaitPT - a transformer-based architecture - on a dataset of 2.7\nmillion walking sequences collected in the wild. We evaluate zero-shot\nperformance across four benchmark datasets to derive scaling laws for data,\nmodel size, and compute. Our findings demonstrate predictable power-law\nimprovements in performance with increased scale and confirm that data and\ncompute scaling significantly influence downstream accuracy. We further isolate\narchitectural contributions by comparing GaitPT with GaitFormer under\ncontrolled compute budgets. These results provide practical insights into\nresource allocation and performance estimation for real-world gait recognition\nsystems."}
{"id": "2504.07661", "pdf": "https://arxiv.org/pdf/2504.07661", "abs": "https://arxiv.org/abs/2504.07661", "authors": ["Xiaowu Zhang", "Hongfei Zhao", "Jingyi Hou", "Zhijie Liu"], "title": "Unveiling the Impact of Multimodal Features on Chinese Spelling Correction: From Analysis to Design", "categories": ["cs.CL"], "comment": null, "summary": "The Chinese Spelling Correction (CSC) task focuses on detecting and\ncorrecting spelling errors in sentences. Current research primarily explores\ntwo approaches: traditional multimodal pre-trained models and large language\nmodels (LLMs). However, LLMs face limitations in CSC, particularly\nover-correction, making them suboptimal for this task. While existing studies\nhave investigated the use of phonetic and graphemic information in multimodal\nCSC models, effectively leveraging these features to enhance correction\nperformance remains a challenge. To address this, we propose the Multimodal\nAnalysis for Character Usage (\\textbf{MACU}) experiment, identifying potential\nimprovements for multimodal correctison. Based on empirical findings, we\nintroduce \\textbf{NamBert}, a novel multimodal model for Chinese spelling\ncorrection. Experiments on benchmark datasets demonstrate NamBert's superiority\nover SOTA methods. We also conduct a comprehensive comparison between NamBert\nand LLMs, systematically evaluating their strengths and limitations in CSC. Our\ncode and model are available at https://github.com/iioSnail/NamBert."}
{"id": "2504.07603", "pdf": "https://arxiv.org/pdf/2504.07603", "abs": "https://arxiv.org/abs/2504.07603", "authors": ["Youngwan Jin", "Michal Kovac", "Yagiz Nalcakan", "Hyeongjin Ju", "Hanbin Song", "Sanghyeop Yeo", "Shiho Kim"], "title": "RASMD: RGB And SWIR Multispectral Driving Dataset for Robust Perception in Adverse Conditions", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Current autonomous driving algorithms heavily rely on the visible spectrum,\nwhich is prone to performance degradation in adverse conditions like fog, rain,\nsnow, glare, and high contrast. Although other spectral bands like\nnear-infrared (NIR) and long-wave infrared (LWIR) can enhance vision perception\nin such situations, they have limitations and lack large-scale datasets and\nbenchmarks. Short-wave infrared (SWIR) imaging offers several advantages over\nNIR and LWIR. However, no publicly available large-scale datasets currently\nincorporate SWIR data for autonomous driving. To address this gap, we introduce\nthe RGB and SWIR Multispectral Driving (RASMD) dataset, which comprises 100,000\nsynchronized and spatially aligned RGB-SWIR image pairs collected across\ndiverse locations, lighting, and weather conditions. In addition, we provide a\nsubset for RGB-SWIR translation and object detection annotations for a subset\nof challenging traffic scenarios to demonstrate the utility of SWIR imaging\nthrough experiments on both object detection and RGB-to-SWIR image translation.\nOur experiments show that combining RGB and SWIR data in an ensemble framework\nsignificantly improves detection accuracy compared to RGB-only approaches,\nparticularly in conditions where visible-spectrum sensors struggle. We\nanticipate that the RASMD dataset will advance research in multispectral\nimaging for autonomous driving and robust perception systems."}
{"id": "2504.07680", "pdf": "https://arxiv.org/pdf/2504.07680", "abs": "https://arxiv.org/abs/2504.07680", "authors": ["Sheila Castilho", "Zoe Fitzsimmons", "Claire Holton", "Aoife Mc Donagh"], "title": "Synthetic Fluency: Hallucinations, Confabulations, and the Creation of Irish Words in LLM-Generated Translations", "categories": ["cs.CL"], "comment": null, "summary": "This study examines hallucinations in Large Language Model (LLM) translations\ninto Irish, specifically focusing on instances where the models generate novel,\nnon-existent words. We classify these hallucinations within verb and noun\ncategories, identifying six distinct patterns among the latter. Additionally,\nwe analyse whether these hallucinations adhere to Irish morphological rules and\nwhat linguistic tendencies they exhibit. Our findings show that while both\nGPT-4.o and GPT-4.o Mini produce similar types of hallucinations, the Mini\nmodel generates them at a significantly higher frequency. Beyond\nclassification, the discussion raises speculative questions about the\nimplications of these hallucinations for the Irish language. Rather than\nseeking definitive answers, we offer food for thought regarding the increasing\nuse of LLMs and their potential role in shaping Irish vocabulary and linguistic\nevolution. We aim to prompt discussion on how such technologies might influence\nlanguage over time, particularly in the context of low-resource,\nmorphologically rich languages."}
{"id": "2504.07615", "pdf": "https://arxiv.org/pdf/2504.07615", "abs": "https://arxiv.org/abs/2504.07615", "authors": ["Haozhan Shen", "Peng Liu", "Jingcheng Li", "Chunxin Fang", "Yibo Ma", "Jiajia Liao", "Qiaoli Shen", "Zilun Zhang", "Kangjia Zhao", "Qianqian Zhang", "Ruochen Xu", "Tiancheng Zhao"], "title": "VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model", "categories": ["cs.CV", "cs.CL"], "comment": "11 pages", "summary": "Recently DeepSeek R1 has shown that reinforcement learning (RL) can\nsubstantially improve the reasoning capabilities of Large Language Models\n(LLMs) through a simple yet effective design. The core of R1 lies in its\nrule-based reward formulation, which leverages tasks with deterministic\nground-truth answers to enable precise and stable reward computation. In the\nvisual domain, we similarly observe that a wide range of visual understanding\ntasks are inherently equipped with well-defined ground-truth annotations. This\nproperty makes them naturally compatible with rule-based reward mechanisms.\nMotivated by this observation, we investigate the extension of R1-style\nreinforcement learning to Vision-Language Models (VLMs), aiming to enhance\ntheir visual reasoning capabilities. To this end, we develop VLM-R1, a\ndedicated framework designed to harness RL for improving VLMs' performance on\ngeneral vision-language tasks. Using this framework, we further explore the\nfeasibility of applying RL to visual domain. Experimental results indicate that\nthe RL-based model not only delivers competitive performance on visual\nunderstanding tasks but also surpasses Supervised Fine-Tuning (SFT) in\ngeneralization ability. Furthermore, we conduct comprehensive ablation studies\nthat uncover a series of noteworthy insights, including the presence of reward\nhacking in object detection, the emergence of the \"OD aha moment\", the impact\nof training data quality, and the scaling behavior of RL across different model\nsizes. Through these analyses, we aim to deepen the understanding of how\nreinforcement learning enhances the capabilities of vision-language models, and\nwe hope our findings and open-source contributions will support continued\nprogress in the vision-language RL community. Our code and model are available\nat https://github.com/om-ai-lab/VLM-R1"}
{"id": "2504.07685", "pdf": "https://arxiv.org/pdf/2504.07685", "abs": "https://arxiv.org/abs/2504.07685", "authors": ["Silvio Picinini", "Sheila Castilho"], "title": "Context-Aware Monolingual Human Evaluation of Machine Translation", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "This paper explores the potential of context-aware monolingual human\nevaluation for assessing machine translation (MT) when no source is given for\nreference. To this end, we compare monolingual with bilingual evaluations (with\nsource text), under two scenarios: the evaluation of a single MT system, and\nthe comparative evaluation of pairwise MT systems. Four professional\ntranslators performed both monolingual and bilingual evaluations by assigning\nratings and annotating errors, and providing feedback on their experience. Our\nfindings suggest that context-aware monolingual human evaluation achieves\ncomparable outcomes to human bilingual evaluations, and suggest the feasibility\nand potential of monolingual evaluation as an efficient approach to assessing\nMT."}
{"id": "2504.07660", "pdf": "https://arxiv.org/pdf/2504.07660", "abs": "https://arxiv.org/abs/2504.07660", "authors": ["Yini Fang", "Alec Diallo", "Yiqi Shi", "Frederic Jumelle", "Bertram Shi"], "title": "End-to-End Facial Expression Detection in Long Videos", "categories": ["cs.CV"], "comment": null, "summary": "Facial expression detection involves two interrelated tasks: spotting, which\nidentifies the onset and offset of expressions, and recognition, which\nclassifies them into emotional categories. Most existing methods treat these\ntasks separately using a two-step training pipelines. A spotting model first\ndetects expression intervals. A recognition model then classifies the detected\nsegments. However, this sequential approach leads to error propagation,\ninefficient feature learning, and suboptimal performance due to the lack of\njoint optimization of the two tasks. We propose FEDN, an end-to-end Facial\nExpression Detection Network that jointly optimizes spotting and recognition.\nOur model introduces a novel attention-based feature extraction module,\nincorporating segment attention and sliding window attention to improve facial\nfeature learning. By unifying two tasks within a single network, we greatly\nreduce error propagation and enhance overall performance. Experiments on\nCASME}^2 and CASME^3 demonstrate state-of-the-art accuracy for both spotting\nand detection, underscoring the benefits of joint optimization for robust\nfacial expression detection in long videos."}
{"id": "2504.07698", "pdf": "https://arxiv.org/pdf/2504.07698", "abs": "https://arxiv.org/abs/2504.07698", "authors": ["Shiki Sato", "Jun Baba", "Asahi Hentona", "Shinji Iwata", "Akifumi Yoshimoto", "Koichiro Yoshino"], "title": "Proactive User Information Acquisition via Chats on User-Favored Topics", "categories": ["cs.CL"], "comment": "23 pages", "summary": "Chat-oriented dialogue systems designed to provide tangible benefits, such as\nsharing the latest news or preventing frailty in senior citizens, often require\nProactive acquisition of specific user Information via chats on user-faVOred\nTopics (PIVOT). This study proposes the PIVOT task, designed to advance the\ntechnical foundation for these systems. In this task, a system needs to acquire\nthe answers of a user to predefined questions without making the user feel\nabrupt while engaging in a chat on a predefined topic. We found that even\nrecent large language models (LLMs) show a low success rate in the PIVOT task.\nWe constructed a dataset suitable for the analysis to develop more effective\nsystems. Finally, we developed a simple but effective system for this task by\nincorporating insights obtained through the analysis of this dataset."}
{"id": "2504.07667", "pdf": "https://arxiv.org/pdf/2504.07667", "abs": "https://arxiv.org/abs/2504.07667", "authors": ["Yujin Wang", "Jiarui Wu", "Yichen Bian", "Fan Zhang", "Tianfan Xue"], "title": "S2R-HDR: A Large-Scale Rendered Dataset for HDR Fusion", "categories": ["cs.CV"], "comment": "https://openimaginglab.github.io/S2R-HDR", "summary": "The generalization of learning-based high dynamic range (HDR) fusion is often\nlimited by the availability of training data, as collecting large-scale HDR\nimages from dynamic scenes is both costly and technically challenging. To\naddress these challenges, we propose S2R-HDR, the first large-scale\nhigh-quality synthetic dataset for HDR fusion, with 24,000 HDR samples. Using\nUnreal Engine 5, we design a diverse set of realistic HDR scenes that encompass\nvarious dynamic elements, motion types, high dynamic range scenes, and\nlighting. Additionally, we develop an efficient rendering pipeline to generate\nrealistic HDR images. To further mitigate the domain gap between synthetic and\nreal-world data, we introduce S2R-Adapter, a domain adaptation designed to\nbridge this gap and enhance the generalization ability of models. Experimental\nresults on real-world datasets demonstrate that our approach achieves\nstate-of-the-art HDR reconstruction performance. Dataset and code will be\navailable at https://openimaginglab.github.io/S2R-HDR."}
{"id": "2504.07724", "pdf": "https://arxiv.org/pdf/2504.07724", "abs": "https://arxiv.org/abs/2504.07724", "authors": ["Yixiang Chen", "Penglei Sun", "Xiang Li", "Xiaowen Chu"], "title": "MRD-RAG: Enhancing Medical Diagnosis with Multi-Round Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "In recent years, accurately and quickly deploying medical large language\nmodels (LLMs) has become a significant trend. Among these, retrieval-augmented\ngeneration (RAG) has garnered significant attention due to its features of\nrapid deployment and privacy protection. However, existing medical RAG\nframeworks still have shortcomings. Most existing medical RAG frameworks are\ndesigned for single-round question answering tasks and are not suitable for\nmulti-round diagnostic dialogue. On the other hand, existing medical\nmulti-round RAG frameworks do not consider the interconnections between\npotential diseases to inquire precisely like a doctor. To address these issues,\nwe propose a Multi-Round Diagnostic RAG (MRD-RAG) framework that mimics the\ndoctor's diagnostic process. This RAG framework can analyze diagnosis\ninformation of potential diseases and accurately conduct multi-round diagnosis\nlike a doctor. To evaluate the effectiveness of our proposed frameworks, we\nconduct experiments on two modern medical datasets and two traditional Chinese\nmedicine datasets, with evaluations by GPT and human doctors on different\nmethods. The results indicate that our RAG framework can significantly enhance\nthe diagnostic performance of LLMs, highlighting the potential of our approach\nin medical diagnosis. The code and data can be found in our project website\nhttps://github.com/YixiangCh/MRD-RAG/tree/master."}
{"id": "2504.07670", "pdf": "https://arxiv.org/pdf/2504.07670", "abs": "https://arxiv.org/abs/2504.07670", "authors": ["Anne-Sofie Maerten", "Li-Wei Chen", "Stefanie De Winter", "Christophe Bossens", "Johan Wagemans"], "title": "LAPIS: A novel dataset for personalized image aesthetic assessment", "categories": ["cs.CV"], "comment": "accepted at the CVPR 2025 workshop on AI for Creative Visual Content\n  Generation Editing and Understanding (CVEU)", "summary": "We present the Leuven Art Personalized Image Set (LAPIS), a novel dataset for\npersonalized image aesthetic assessment (PIAA). It is the first dataset with\nimages of artworks that is suitable for PIAA. LAPIS consists of 11,723 images\nand was meticulously curated in collaboration with art historians. Each image\nhas an aesthetics score and a set of image attributes known to relate to\naesthetic appreciation. Besides rich image attributes, LAPIS offers rich\npersonal attributes of each annotator. We implemented two existing\nstate-of-the-art PIAA models and assessed their performance on LAPIS. We assess\nthe contribution of personal attributes and image attributes through ablation\nstudies and find that performance deteriorates when certain personal and image\nattributes are removed. An analysis of failure cases reveals that both existing\nmodels make similar incorrect predictions, highlighting the need for\nimprovements in artistic image aesthetic assessment. The LAPIS project page can\nbe found at: https://github.com/Anne-SofieMaerten/LAPIS"}
{"id": "2504.07733", "pdf": "https://arxiv.org/pdf/2504.07733", "abs": "https://arxiv.org/abs/2504.07733", "authors": ["Congluo Xu", "Yu Miao", "Yiling Xiao", "Chengmengjia Lin"], "title": "DeepGreen: Effective LLM-Driven Green-washing Monitoring System Designed for Empirical Testing -- Evidence from China", "categories": ["cs.CL", "econ.GN", "q-fin.EC"], "comment": null, "summary": "This paper proposes DeepGreen, an Large Language Model Driven (LLM-Driven)\nsystem for detecting corporate green-washing behaviour. Utilizing dual-layer\nLLM analysis, DeepGreen preliminarily identifies potential green keywords in\nfinancial statements and then assesses their implementation degree via\niterative semantic analysis of LLM. A core variable GreenImplement is derived\nfrom the ratio from the two layers' output. We extract 204 financial statements\nof 68 companies from A-share market over three years, comprising 89,893 words,\nand analyse them through DeepGreen. Our analysis, supported by violin plots and\nK-means clustering, reveals insights and validates the variable against the\nHuazheng ESG rating. It offers a novel perspective for regulatory agencies and\ninvestors, serving as a proactive monitoring tool that complements traditional\nmethods.Empirical tests show that green implementation can significantly boost\nthe asset return rate of companies, but there is heterogeneity in scale. Small\nand medium-sized companies have limited contribution to asset return via green\nimplementation, so there is a stronger motivation for green-washing."}
{"id": "2504.07687", "pdf": "https://arxiv.org/pdf/2504.07687", "abs": "https://arxiv.org/abs/2504.07687", "authors": ["Yihao Wang", "Zhong Qian", "Peifeng Li"], "title": "FMNV: A Dataset of Media-Published News Videos for Fake News Detection", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "News media, particularly video-based platforms, have become deeply embedded\nin daily life, concurrently amplifying risks of misinformation dissemination.\nConsequently, multimodal fake news detection has garnered significant research\nattention. However, existing datasets predominantly comprise user-generated\nvideos characterized by crude editing and limited public engagement, whereas\nprofessionally crafted fake news videos disseminated by media outlets often\npolitically or virally motivated pose substantially greater societal harm. To\naddress this gap, we construct FMNV, a novel dataset exclusively composed of\nnews videos published by media organizations. Through empirical analysis of\nexisting datasets and our curated collection, we categorize fake news videos\ninto four distinct types. Building upon this taxonomy, we employ Large Language\nModels (LLMs) to automatically generate deceptive content by manipulating\nauthentic media-published news videos. Furthermore, we propose FMNVD, a\nbaseline model featuring a dual-stream architecture integrating CLIP and Faster\nR-CNN for video feature extraction, enhanced by co-attention mechanisms for\nfeature refinement and multimodal aggregation. Comparative experiments\ndemonstrate both the generalization capability of FMNV across multiple\nbaselines and the superior detection efficacy of FMNVD. This work establishes\ncritical benchmarks for detecting high-impact fake news in media ecosystems\nwhile advancing methodologies for cross-modal inconsistency analysis."}
{"id": "2504.07738", "pdf": "https://arxiv.org/pdf/2504.07738", "abs": "https://arxiv.org/abs/2504.07738", "authors": ["A. Loreti", "K. Chen", "R. George", "R. Firth", "A. Agnello", "S. Tanaka"], "title": "Automated Construction of a Knowledge Graph of Nuclear Fusion Energy for Effective Elicitation and Retrieval of Information", "categories": ["cs.CL"], "comment": null, "summary": "In this document, we discuss a multi-step approach to automated construction\nof a knowledge graph, for structuring and representing domain-specific\nknowledge from large document corpora. We apply our method to build the first\nknowledge graph of nuclear fusion energy, a highly specialized field\ncharacterized by vast scope and heterogeneity. This is an ideal benchmark to\ntest the key features of our pipeline, including automatic named entity\nrecognition and entity resolution. We show how pre-trained large language\nmodels can be used to address these challenges and we evaluate their\nperformance against Zipf's law, which characterizes human-generated natural\nlanguage. Additionally, we develop a knowledge-graph retrieval-augmented\ngeneration system that combines large language models with a multi-prompt\napproach. This system provides contextually relevant answers to\nnatural-language queries, including complex multi-hop questions that require\nreasoning across interconnected entities."}
{"id": "2504.07718", "pdf": "https://arxiv.org/pdf/2504.07718", "abs": "https://arxiv.org/abs/2504.07718", "authors": ["Zehong Ma", "Hao Chen", "Wei Zeng", "Limin Su", "Shiliang Zhang"], "title": "Multi-modal Reference Learning for Fine-grained Text-to-Image Retrieval", "categories": ["cs.CV"], "comment": "TMM25", "summary": "Fine-grained text-to-image retrieval aims to retrieve a fine-grained target\nimage with a given text query. Existing methods typically assume that each\ntraining image is accurately depicted by its textual descriptions. However,\ntextual descriptions can be ambiguous and fail to depict discriminative visual\ndetails in images, leading to inaccurate representation learning. To alleviate\nthe effects of text ambiguity, we propose a Multi-Modal Reference learning\nframework to learn robust representations. We first propose a multi-modal\nreference construction module to aggregate all visual and textual details of\nthe same object into a comprehensive multi-modal reference. The multi-modal\nreference hence facilitates the subsequent representation learning and\nretrieval similarity computation. Specifically, a reference-guided\nrepresentation learning module is proposed to use multi-modal references to\nlearn more accurate visual and textual representations. Additionally, we\nintroduce a reference-based refinement method that employs the object\nreferences to compute a reference-based similarity that refines the initial\nretrieval results. Extensive experiments are conducted on five fine-grained\ntext-to-image retrieval datasets for different text-to-image retrieval tasks.\nThe proposed method has achieved superior performance over state-of-the-art\nmethods. For instance, on the text-to-person image retrieval dataset RSTPReid,\nour method achieves the Rank1 accuracy of 56.2\\%, surpassing the recent CFine\nby 5.6\\%."}
{"id": "2504.07749", "pdf": "https://arxiv.org/pdf/2504.07749", "abs": "https://arxiv.org/abs/2504.07749", "authors": ["Vladislav Mikhailov", "Tita Enstad", "David Samuel", "Hans Christian Farseth√•s", "Andrey Kutuzov", "Erik Velldal", "Lilja √òvrelid"], "title": "NorEval: A Norwegian Language Understanding and Generation Evaluation Benchmark", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper introduces NorEval, a new and comprehensive evaluation suite for\nlarge-scale standardized benchmarking of Norwegian generative language models\n(LMs). NorEval consists of 24 high-quality human-created datasets -- of which\nfive are created from scratch. In contrast to existing benchmarks for\nNorwegian, NorEval covers a broad spectrum of task categories targeting\nNorwegian language understanding and generation, establishes human baselines,\nand focuses on both of the official written standards of the Norwegian\nlanguage: Bokm{\\aa}l and Nynorsk. All our datasets and a collection of over 100\nhuman-written prompts are integrated into LM Evaluation Harness, ensuring\nflexible and reproducible evaluation. We describe the NorEval design and\npresent the results of benchmarking 19 open-source pre-trained and\ninstruction-tuned LMs for Norwegian in various scenarios. Our benchmark,\nevaluation framework, and annotation materials are publicly available."}
{"id": "2504.07729", "pdf": "https://arxiv.org/pdf/2504.07729", "abs": "https://arxiv.org/abs/2504.07729", "authors": ["Nicole Tran", "Anisa Prasad", "Yan Zhuang", "Tejas Sudharshan Mathai", "Boah Kim", "Sydney Lewis", "Pritam Mukherjee", "Jianfei Liu", "Ronald M. Summers"], "title": "Benchmarking Multi-Organ Segmentation Tools for Multi-Parametric T1-weighted Abdominal MRI", "categories": ["cs.CV", "cs.AI"], "comment": "Published at SPIE Medical Imaging 2025", "summary": "The segmentation of multiple organs in multi-parametric MRI studies is\ncritical for many applications in radiology, such as correlating imaging\nbiomarkers with disease status (e.g., cirrhosis, diabetes). Recently, three\npublicly available tools, such as MRSegmentator (MRSeg), TotalSegmentator MRI\n(TS), and TotalVibeSegmentator (VIBE), have been proposed for multi-organ\nsegmentation in MRI. However, the performance of these tools on specific MRI\nsequence types has not yet been quantified. In this work, a subset of 40\nvolumes from the public Duke Liver Dataset was curated. The curated dataset\ncontained 10 volumes each from the pre-contrast fat saturated T1, arterial T1w,\nvenous T1w, and delayed T1w phases, respectively. Ten abdominal structures were\nmanually annotated in these volumes. Next, the performance of the three public\ntools was benchmarked on this curated dataset. The results indicated that MRSeg\nobtained a Dice score of 80.7 $\\pm$ 18.6 and Hausdorff Distance (HD) error of\n8.9 $\\pm$ 10.4 mm. It fared the best ($p < .05$) across the different sequence\ntypes in contrast to TS and VIBE."}
{"id": "2504.07754", "pdf": "https://arxiv.org/pdf/2504.07754", "abs": "https://arxiv.org/abs/2504.07754", "authors": ["Bo Zhang", "Hui Ma", "Dailin Li", "Jian Ding", "Jian Wang", "Bo Xu", "HongFei Lin"], "title": "Efficient Tuning of Large Language Models for Knowledge-Grounded Dialogue Generation", "categories": ["cs.CL"], "comment": "Accepted at TACL; pre-MIT Press publication version. Code and data\n  are available at https://github.com/zhangbo-nlp/KEDiT", "summary": "Large language models (LLMs) demonstrate remarkable text comprehension and\ngeneration capabilities but often lack the ability to utilize up-to-date or\ndomain-specific knowledge not included in their training data. To address this\ngap, we introduce KEDiT, an efficient method for fine-tuning LLMs for\nknowledge-grounded dialogue generation. KEDiT operates in two main phases:\nfirst, it employs an information bottleneck to compress retrieved knowledge\ninto learnable parameters, retaining essential information while minimizing\ncomputational overhead. Second, a lightweight knowledge-aware adapter\nintegrates these compressed knowledge vectors into the LLM during fine-tuning,\nupdating less than 2\\% of the model parameters. The experimental results on the\nWizard of Wikipedia and a newly constructed PubMed-Dialog dataset demonstrate\nthat KEDiT excels in generating contextually relevant and informative\nresponses, outperforming competitive baselines in automatic, LLM-based, and\nhuman evaluations. This approach effectively combines the strengths of\npretrained LLMs with the adaptability needed for incorporating dynamic\nknowledge, presenting a scalable solution for fields such as medicine."}
{"id": "2504.07744", "pdf": "https://arxiv.org/pdf/2504.07744", "abs": "https://arxiv.org/abs/2504.07744", "authors": ["Jenna Kline", "Samuel Stevens", "Guy Maalouf", "Camille Rondeau Saint-Jean", "Dat Nguyen Ngoc", "Majid Mirmehdi", "David Guerin", "Tilo Burghardt", "Elzbieta Pastucha", "Blair Costelloe", "Matthew Watson", "Thomas Richardson", "Ulrik Pagh Schultz Lundquist"], "title": "MMLA: Multi-Environment, Multi-Species, Low-Altitude Aerial Footage Dataset", "categories": ["cs.CV"], "comment": null, "summary": "Real-time wildlife detection in drone imagery is critical for numerous\napplications, including animal ecology, conservation, and biodiversity\nmonitoring. Low-altitude drone missions are effective for collecting\nfine-grained animal movement and behavior data, particularly if missions are\nautomated for increased speed and consistency. However, little work exists on\nevaluating computer vision models on low-altitude aerial imagery and\ngeneralizability across different species and settings. To fill this gap, we\npresent a novel multi-environment, multi-species, low-altitude aerial footage\n(MMLA) dataset. MMLA consists of drone footage collected across three diverse\nenvironments: Ol Pejeta Conservancy and Mpala Research Centre in Kenya, and The\nWilds Conservation Center in Ohio, which includes five species: Plains zebras,\nGrevy's zebras, giraffes, onagers, and African Painted Dogs. We comprehensively\nevaluate three YOLO models (YOLOv5m, YOLOv8m, and YOLOv11m) for detecting\nanimals. Results demonstrate significant performance disparities across\nlocations and species-specific detection variations. Our work highlights the\nimportance of evaluating detection algorithms across different environments for\nrobust wildlife monitoring applications using drones."}
{"id": "2504.07794", "pdf": "https://arxiv.org/pdf/2504.07794", "abs": "https://arxiv.org/abs/2504.07794", "authors": ["Alireza Salemi", "Chris Samarinas", "Hamed Zamani"], "title": "Plan-and-Refine: Diverse and Comprehensive Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "This paper studies the limitations of (retrieval-augmented) large language\nmodels (LLMs) in generating diverse and comprehensive responses, and introduces\nthe Plan-and-Refine (P&R) framework based on a two phase system design. In the\nglobal exploration phase, P&R generates a diverse set of plans for the given\ninput, where each plan consists of a list of diverse query aspects with\ncorresponding additional descriptions. This phase is followed by a local\nexploitation phase that generates a response proposal for the input query\nconditioned on each plan and iteratively refines the proposal for improving the\nproposal quality. Finally, a reward model is employed to select the proposal\nwith the highest factuality and coverage. We conduct our experiments based on\nthe ICAT evaluation methodology--a recent approach for answer factuality and\ncomprehensiveness evaluation. Experiments on the two diverse information\nseeking benchmarks adopted from non-factoid question answering and TREC search\nresult diversification tasks demonstrate that P&R significantly outperforms\nbaselines, achieving up to a 13.1% improvement on the ANTIQUE dataset and a\n15.41% improvement on the TREC dataset. Furthermore, a smaller scale user study\nconfirms the substantial efficacy of the P&R framework."}
{"id": "2504.07745", "pdf": "https://arxiv.org/pdf/2504.07745", "abs": "https://arxiv.org/abs/2504.07745", "authors": ["Yangliu Hu", "Zikai Song", "Na Feng", "Yawei Luo", "Junqing Yu", "Yi-Ping Phoebe Chen", "Wei Yang"], "title": "SF2T: Self-supervised Fragment Finetuning of Video-LLMs for Fine-Grained Understanding", "categories": ["cs.CV", "cs.AI", "68T45", "I.4.8; I.5"], "comment": "Accepted to CVPR2025", "summary": "Video-based Large Language Models (Video-LLMs) have witnessed substantial\nadvancements in recent years, propelled by the advancement in multi-modal LLMs.\nAlthough these models have demonstrated proficiency in providing the overall\ndescription of videos, they struggle with fine-grained understanding,\nparticularly in aspects such as visual dynamics and video details inquiries. To\ntackle these shortcomings, we find that fine-tuning Video-LLMs on\nself-supervised fragment tasks, greatly improve their fine-grained video\nunderstanding abilities. Hence we propose two key contributions:(1)\nSelf-Supervised Fragment Fine-Tuning (SF$^2$T), a novel effortless fine-tuning\nmethod, employs the rich inherent characteristics of videos for training, while\nunlocking more fine-grained understanding ability of Video-LLMs. Moreover, it\nrelieves researchers from labor-intensive annotations and smartly circumvents\nthe limitations of natural language, which often fails to capture the complex\nspatiotemporal variations in videos; (2) A novel benchmark dataset, namely\nFineVidBench, for rigorously assessing Video-LLMs' performance at both the\nscene and fragment levels, offering a comprehensive evaluation of their\ncapabilities. We assessed multiple models and validated the effectiveness of\nSF$^2$T on them. Experimental results reveal that our approach improves their\nability to capture and interpret spatiotemporal details."}
{"id": "2504.07803", "pdf": "https://arxiv.org/pdf/2504.07803", "abs": "https://arxiv.org/abs/2504.07803", "authors": ["Mattia Rengo", "Senad Beadini", "Domenico Alfano", "Roberto Abbruzzese"], "title": "A System for Comprehensive Assessment of RAG Frameworks", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Technical Report, 7 pages, 2 figures, 1 table", "summary": "Retrieval Augmented Generation (RAG) has emerged as a standard paradigm for\nenhancing the factual accuracy and contextual relevance of Large Language\nModels (LLMs) by integrating retrieval mechanisms. However, existing evaluation\nframeworks fail to provide a holistic black-box approach to assessing RAG\nsystems, especially in real-world deployment scenarios. To address this gap, we\nintroduce SCARF (System for Comprehensive Assessment of RAG Frameworks), a\nmodular and flexible evaluation framework designed to benchmark deployed RAG\napplications systematically. SCARF provides an end-to-end, black-box evaluation\nmethodology, enabling a limited-effort comparison across diverse RAG\nframeworks. Our framework supports multiple deployment configurations and\nfacilitates automated testing across vector databases and LLM serving\nstrategies, producing a detailed performance report. Moreover, SCARF integrates\npractical considerations such as response coherence, providing a scalable and\nadaptable solution for researchers and industry professionals evaluating RAG\napplications. Using the REST APIs interface, we demonstrate how SCARF can be\napplied to real-world scenarios, showcasing its flexibility in assessing\ndifferent RAG frameworks and configurations. SCARF is available at GitHub\nrepository."}
{"id": "2504.07758", "pdf": "https://arxiv.org/pdf/2504.07758", "abs": "https://arxiv.org/abs/2504.07758", "authors": ["Shuangfan Zhou", "Chu Zhou", "Youwei Lyu", "Heng Guo", "Zhanyu Ma", "Boxin Shi", "Imari Sato"], "title": "PIDSR:ComplementaryPolarizedImageDemosaicingandSuper-Resolution", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Polarization cameras can capture multiple polarized images with different\npolarizer angles in a single shot, bringing convenience to polarization-based\ndownstream tasks. However, their direct outputs are color-polarization filter\narray (CPFA) raw images, requiring demosaicing to reconstruct full-resolution,\nfull-color polarized images; unfortunately, this necessary step introduces\nartifacts that make polarization-related parameters such as the degree of\npolarization (DoP) and angle of polarization (AoP) prone to error. Besides,\nlimited by the hardware design, the resolution of a polarization camera is\noften much lower than that of a conventional RGB camera. Existing polarized\nimage demosaicing (PID) methods are limited in that they cannot enhance\nresolution, while polarized image super-resolution (PISR) methods, though\ndesigned to obtain high-resolution (HR) polarized images from the demosaicing\nresults, tend to retain or even amplify errors in the DoP and AoP introduced by\ndemosaicing artifacts. In this paper, we propose PIDSR, a joint framework that\nperforms complementary Polarized Image Demosaicing and Super-Resolution,\nshowing the ability to robustly obtain high-quality HR polarized images with\nmore accurate DoP and AoP from a CPFA raw image in a direct manner. Experiments\nshow our PIDSR not only achieves state-of-the-art performance on both synthetic\nand real data, but also facilitates downstream tasks."}
{"id": "2504.07807", "pdf": "https://arxiv.org/pdf/2504.07807", "abs": "https://arxiv.org/abs/2504.07807", "authors": ["Hongcheng Guo", "Juntao Yao", "Boyang Wang", "Junjia Du", "Shaosheng Cao", "Donglin Di", "Shun Zhang", "Zhoujun Li"], "title": "Cluster-Driven Expert Pruning for Mixture-of-Experts Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Mixture-of-Experts (MoE) architectures have emerged as a promising paradigm\nfor scaling large language models (LLMs) with sparse activation of\ntask-specific experts. Despite their computational efficiency during inference,\nthe massive overall parameter footprint of MoE models (e.g., GPT-4) introduces\ncritical challenges for practical deployment. Current pruning approaches often\nfail to address two inherent characteristics of MoE systems: 1).intra-layer\nexpert homogeneity where experts within the same MoE layer exhibit functional\nredundancy, and 2). inter-layer similarity patterns where deeper layers tend to\ncontain progressively more homogeneous experts. To tackle these issues, we\npropose Cluster-driven Expert Pruning (C-Prune), a novel two-stage framework\nfor adaptive task-specific compression of MoE LLMs. C-Prune operates through\nlayer-wise expert clustering, which groups functionally similar experts within\neach MoE layer using parameter similarity metrics, followed by global cluster\npruning, which eliminates redundant clusters across all layers through a\nunified importance scoring mechanism that accounts for cross-layer homogeneity.\nWe validate C-Prune through extensive experiments on multiple MoE models and\nbenchmarks. The results demonstrate that C-Prune effectively reduces model size\nwhile outperforming existing MoE pruning methods."}
{"id": "2504.07761", "pdf": "https://arxiv.org/pdf/2504.07761", "abs": "https://arxiv.org/abs/2504.07761", "authors": ["Javier Mu√±oz-Haro", "Ruben Tolosana", "Ruben Vera-Rodriguez", "Aythami Morales", "Julian Fierrez"], "title": "Exploring a Patch-Wise Approach for Privacy-Preserving Fake ID Detection", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": null, "summary": "In an increasingly digitalized world, verifying the authenticity of ID\ndocuments has become a critical challenge for real-life applications such as\ndigital banking, crypto-exchanges, renting, etc. This study focuses on the\ntopic of fake ID detection, covering several limitations in the field. In\nparticular, no publicly available data from real ID documents exists, and most\nstudies rely on proprietary in-house databases that are not available due to\nprivacy reasons. In order to shed some light on this critical challenge that\nmakes difficult to advance in the field, we explore a trade-off between privacy\n(i.e., amount of sensitive data available) and performance, proposing a novel\npatch-wise approach for privacy-preserving fake ID detection. Our proposed\napproach explores how privacy can be enhanced through: i) two levels of\nanonymization for an ID document (i.e., fully- and pseudo-anonymized), and ii)\ndifferent patch size configurations, varying the amount of sensitive data\nvisible in the patch image. Also, state-of-the-art methods such as Vision\nTransformers and Foundation Models are considered in the analysis. The\nexperimental framework shows that, on an unseen database (DLC-2021), our\nproposal achieves 13.91% and 0% EERs at patch and ID document level, showing a\ngood generalization to other databases. In addition to this exploration,\nanother key contribution of our study is the release of the first publicly\navailable database that contains 48,400 patches from both real and fake ID\ndocuments, along with the experimental framework and models, which will be\navailable in our GitHub."}
{"id": "2504.07825", "pdf": "https://arxiv.org/pdf/2504.07825", "abs": "https://arxiv.org/abs/2504.07825", "authors": ["Pavel Chizhov", "Mattia Nee", "Pierre-Carl Langlais", "Ivan P. Yamshchikov"], "title": "What the HellaSwag? On the Validity of Common-Sense Reasoning Benchmarks", "categories": ["cs.CL"], "comment": null, "summary": "Common-sense reasoning is a key language model capability because it\nencapsulates not just specific factual knowledge but rather general language\nand world understanding. Measuring common-sense reasoning, therefore, is\ncrucial for language models of different sizes and applications. One of the\nmost widely used benchmarks for evaluating such capabilities is HellaSwag;\nhowever, in this paper, we show that it has severe construct validity issues.\nThese issues range from basic ungrammaticality and numerous typos to misleading\nprompts or equally correct options. Furthermore, we show that if models are\nevaluated only on answer texts, or with \"Lorem ipsum dolor...\" instead of the\nquestion, more than 65% of model predictions remain the same, and this cannot\nbe attributed merely to contamination. Since benchmark scores are an essential\npart of model selection in both research and commercial applications, these\nvalidity issues can have severe consequences. In particular, knowing that\ntaking benchmark scores at face value is ubiquitous, inadequate evaluation\nleads to ill-informed decisions about models. In this paper, we thoroughly\ninvestigate critical validity issues posed by HellaSwag and illustrate them\nwith various evaluations using generative language models of different sizes.\nWe argue that this benchmark does not accurately measure common-sense reasoning\nand, therefore, should not be used for evaluation in its current state. Based\non the results of our study, we propose requirements that should be met by\nfuture common-sense reasoning benchmarks. In addition, we release GoldenSwag, a\ncorrected subset of HellaSwag, which, to our belief, facilitates acceptable\ncommon-sense reasoning evaluation."}
{"id": "2504.07785", "pdf": "https://arxiv.org/pdf/2504.07785", "abs": "https://arxiv.org/abs/2504.07785", "authors": ["Yan Zhang", "Lechao Cheng", "Yaxiong Wang", "Zhun Zhong", "Meng Wang"], "title": "Towards Micro-Action Recognition with Limited Annotations: An Asynchronous Pseudo Labeling and Training Approach", "categories": ["cs.CV"], "comment": null, "summary": "Micro-Action Recognition (MAR) aims to classify subtle human actions in\nvideo. However, annotating MAR datasets is particularly challenging due to the\nsubtlety of actions. To this end, we introduce the setting of Semi-Supervised\nMAR (SSMAR), where only a part of samples are labeled. We first evaluate\ntraditional Semi-Supervised Learning (SSL) methods to SSMAR and find that these\nmethods tend to overfit on inaccurate pseudo-labels, leading to error\naccumulation and degraded performance. This issue primarily arises from the\ncommon practice of directly using the predictions of classifier as\npseudo-labels to train the model. To solve this issue, we propose a novel\nframework, called Asynchronous Pseudo Labeling and Training (APLT), which\nexplicitly separates the pseudo-labeling process from model training.\nSpecifically, we introduce a semi-supervised clustering method during the\noffline pseudo-labeling phase to generate more accurate pseudo-labels.\nMoreover, a self-adaptive thresholding strategy is proposed to dynamically\nfilter noisy labels of different classes. We then build a memory-based\nprototype classifier based on the filtered pseudo-labels, which is fixed and\nused to guide the subsequent model training phase. By alternating the two\npseudo-labeling and model training phases in an asynchronous manner, the model\ncan not only be learned with more accurate pseudo-labels but also avoid the\noverfitting issue. Experiments on three MAR datasets show that our APLT largely\noutperforms state-of-the-art SSL methods. For instance, APLT improves accuracy\nby 14.5\\% over FixMatch on the MA-12 dataset when using only 50\\% labeled data.\nCode will be publicly available."}
{"id": "2504.07826", "pdf": "https://arxiv.org/pdf/2504.07826", "abs": "https://arxiv.org/abs/2504.07826", "authors": ["RƒÉzvan-Alexandru SmƒÉdu", "Andreea Iuga", "Dumitru-Clementin Cercel"], "title": "MuSaRoNews: A Multidomain, Multimodal Satire Dataset from Romanian News Articles", "categories": ["cs.CL"], "comment": "10 pages, 9 figures", "summary": "Satire and fake news can both contribute to the spread of false information,\neven though both have different purposes (one if for amusement, the other is to\nmisinform). However, it is not enough to rely purely on text to detect the\nincongruity between the surface meaning and the actual meaning of the news\narticles, and, often, other sources of information (e.g., visual) provide an\nimportant clue for satire detection. This work introduces a multimodal corpus\nfor satire detection in Romanian news articles named MuSaRoNews. Specifically,\nwe gathered 117,834 public news articles from real and satirical news sources,\ncomposing the first multimodal corpus for satire detection in the Romanian\nlanguage. We conducted experiments and showed that the use of both modalities\nimproves performance."}
{"id": "2504.07792", "pdf": "https://arxiv.org/pdf/2504.07792", "abs": "https://arxiv.org/abs/2504.07792", "authors": ["Alexander Brettmann", "Jakob Gr√§vinghoff", "Marlene R√ºschoff", "Marie Westhues"], "title": "Breaking the Barriers: Video Vision Transformers for Word-Level Sign Language Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Sign language is a fundamental means of communication for the deaf and\nhard-of-hearing (DHH) community, enabling nuanced expression through gestures,\nfacial expressions, and body movements. Despite its critical role in\nfacilitating interaction within the DHH population, significant barriers\npersist due to the limited fluency in sign language among the hearing\npopulation. Overcoming this communication gap through automatic sign language\nrecognition (SLR) remains a challenge, particularly at a dynamic word-level,\nwhere temporal and spatial dependencies must be effectively recognized. While\nConvolutional Neural Networks have shown potential in SLR, they are\ncomputationally intensive and have difficulties in capturing global temporal\ndependencies between video sequences. To address these limitations, we propose\na Video Vision Transformer (ViViT) model for word-level American Sign Language\n(ASL) recognition. Transformer models make use of self-attention mechanisms to\neffectively capture global relationships across spatial and temporal\ndimensions, which makes them suitable for complex gesture recognition tasks.\nThe VideoMAE model achieves a Top-1 accuracy of 75.58% on the WLASL100 dataset,\nhighlighting its strong performance compared to traditional CNNs with 65.89%.\nOur study demonstrates that transformer-based architectures have great\npotential to advance SLR, overcome communication barriers and promote the\ninclusion of DHH individuals."}
{"id": "2504.07830", "pdf": "https://arxiv.org/pdf/2504.07830", "abs": "https://arxiv.org/abs/2504.07830", "authors": ["Genglin Liu", "Salman Rahman", "Elisa Kreiss", "Marzyeh Ghassemi", "Saadia Gabriel"], "title": "MOSAIC: Modeling Social AI for Content Dissemination and Regulation in Multi-Agent Simulations", "categories": ["cs.CL", "cs.AI", "cs.SI"], "comment": "Work in progress. 22 pages", "summary": "We present a novel, open-source social network simulation framework, MOSAIC,\nwhere generative language agents predict user behaviors such as liking,\nsharing, and flagging content. This simulation combines LLM agents with a\ndirected social graph to analyze emergent deception behaviors and gain a better\nunderstanding of how users determine the veracity of online social content. By\nconstructing user representations from diverse fine-grained personas, our\nsystem enables multi-agent simulations that model content dissemination and\nengagement dynamics at scale. Within this framework, we evaluate three\ndifferent content moderation strategies with simulated misinformation\ndissemination, and we find that they not only mitigate the spread of\nnon-factual content but also increase user engagement. In addition, we analyze\nthe trajectories of popular content in our simulations, and explore whether\nsimulation agents' articulated reasoning for their social interactions truly\naligns with their collective engagement patterns. We open-source our simulation\nsoftware to encourage further research within AI and social sciences."}
{"id": "2504.07810", "pdf": "https://arxiv.org/pdf/2504.07810", "abs": "https://arxiv.org/abs/2504.07810", "authors": ["Daniel Torres", "Joan Duran", "Julia Navarro", "Catalina Sbert"], "title": "Nonlocal Retinex-Based Variational Model and its Deep Unfolding Twin for Low-Light Image Enhancement", "categories": ["cs.CV"], "comment": null, "summary": "Images captured under low-light conditions present significant limitations in\nmany applications, as poor lighting can obscure details, reduce contrast, and\nhide noise. Removing the illumination effects and enhancing the quality of such\nimages is crucial for many tasks, such as image segmentation and object\ndetection. In this paper, we propose a variational method for low-light image\nenhancement based on the Retinex decomposition into illumination, reflectance,\nand noise components. A color correction pre-processing step is applied to the\nlow-light image, which is then used as the observed input in the decomposition.\nMoreover, our model integrates a novel nonlocal gradient-type fidelity term\ndesigned to preserve structural details. Additionally, we propose an automatic\ngamma correction module. Building on the proposed variational approach, we\nextend the model by introducing its deep unfolding counterpart, in which the\nproximal operators are replaced with learnable networks. We propose\ncross-attention mechanisms to capture long-range dependencies in both the\nnonlocal prior of the reflectance and the nonlocal gradient-based constraint.\nExperimental results demonstrate that both methods compare favorably with\nseveral recent and state-of-the-art techniques across different datasets. In\nparticular, despite not relying on learning strategies, the variational model\noutperforms most deep learning approaches both visually and in terms of quality\nmetrics."}
{"id": "2504.07854", "pdf": "https://arxiv.org/pdf/2504.07854", "abs": "https://arxiv.org/abs/2504.07854", "authors": ["Michael J Bommarito II", "Jillian Bommarito", "Daniel Martin Katz"], "title": "The KL3M Data Project: Copyright-Clean Training Resources for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "27 pages, 7 figures, 9 table", "summary": "Practically all large language models have been pre-trained on data that is\nsubject to global uncertainty related to copyright infringement and breach of\ncontract. This creates potential risk for users and developers due to this\nuncertain legal status. The KL3M Data Project directly confronts this critical\nissue by introducing the largest comprehensive training data pipeline that\nminimizes risks related to copyright or breach of contract. The foundation of\nthis project is a corpus of over 132 million documents and trillions of tokens\nspanning 16 different sources that have been verified to meet the strict\ncopyright and licensing protocol detailed herein. We are releasing the entire\npipeline, including 1) the source code to acquire and process these documents,\n2) the original document formats with associated provenance and metadata, 3)\nextracted content in a standardized format, 4) pre-tokenized representations of\nthe documents, and 5) various mid- and post-train resources such as\nquestion-answer, summarization, conversion, drafting, classification,\nprediction, and conversational data. All of these resources are freely\navailable to the public on S3, Hugging Face, and GitHub under CC-BY terms. We\nare committed to continuing this project in furtherance of a more ethical,\nlegal, and sustainable approach to the development and use of AI models."}
{"id": "2504.07813", "pdf": "https://arxiv.org/pdf/2504.07813", "abs": "https://arxiv.org/abs/2504.07813", "authors": ["Pengfei Chen", "Xuehui Yu", "Xumeng Han", "Kuiran Wang", "Guorong Li", "Lingxi Xie", "Zhenjun Han", "Jianbin Jiao"], "title": "P2Object: Single Point Supervised Object Detection and Instance Segmentation", "categories": ["cs.CV"], "comment": "Accepted by IJCV", "summary": "Object recognition using single-point supervision has attracted increasing\nattention recently. However, the performance gap compared with fully-supervised\nalgorithms remains large. Previous works generated class-agnostic\n\\textbf{\\textit{proposals in an image}} offline and then treated mixed\ncandidates as a single bag, putting a huge burden on multiple instance learning\n(MIL). In this paper, we introduce Point-to-Box Network (P2BNet), which\nconstructs balanced \\textbf{\\textit{instance-level proposal bags}} by\ngenerating proposals in an anchor-like way and refining the proposals in a\ncoarse-to-fine paradigm. Through further research, we find that the bag of\nproposals, either at the image level or the instance level, is established on\ndiscrete box sampling. This leads the pseudo box estimation into a sub-optimal\nsolution, resulting in the truncation of object boundaries or the excessive\ninclusion of background. Hence, we conduct a series exploration of\ndiscrete-to-continuous optimization, yielding P2BNet++ and Point-to-Mask\nNetwork (P2MNet). P2BNet++ conducts an approximately continuous proposal\nsampling strategy by better utilizing spatial clues. P2MNet further introduces\nlow-level image information to assist in pixel prediction, and a boundary\nself-prediction is designed to relieve the limitation of the estimated boxes.\nBenefiting from the continuous object-aware \\textbf{\\textit{pixel-level\nperception}}, P2MNet can generate more precise bounding boxes and generalize to\nsegmentation tasks. Our method largely surpasses the previous methods in terms\nof the mean average precision on COCO, VOC, SBD, and Cityscapes, demonstrating\ngreat potential to bridge the performance gap compared with fully supervised\ntasks."}
{"id": "2504.07866", "pdf": "https://arxiv.org/pdf/2504.07866", "abs": "https://arxiv.org/abs/2504.07866", "authors": ["Yichun Yin", "Wenyong Huang", "Kaikai Song", "Yehui Tang", "Xueyu Wu", "Wei Guo", "Peng Guo", "Yaoyuan Wang", "Xiaojun Meng", "Yasheng Wang", "Dong Li", "Can Chen", "Dandan Tu", "Yin Li", "Fisher Yu", "Ruiming Tang", "Yunhe Wang", "Baojun Wang", "Bin Wang", "Bo Wang", "Boxiao Liu", "Changzheng Zhang", "Duyu Tang", "Fei Mi", "Hui Jin", "Jiansheng Wei", "Jiarui Qin", "Jinpeng Li", "Jun Zhao", "Liqun Deng", "Lin Li", "Minghui Xu", "Naifu Zhang", "Nianzu Zheng", "Qiang Li", "Rongju Ruan", "Shengjun Cheng", "Tianyu Guo", "Wei He", "Wei Li", "Weiwen Liu", "Wulong Liu", "Xinyi Dai", "Yonghan Dong", "Yu Pan", "Yue Li", "Yufei Wang", "Yujun Li", "Yunsheng Ni", "Zhe Liu", "Zhenhe Zhang", "Zhicheng Liu"], "title": "Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present Pangu Ultra, a Large Language Model (LLM) with 135 billion\nparameters and dense Transformer modules trained on Ascend Neural Processing\nUnits (NPUs). Although the field of LLM has been witnessing unprecedented\nadvances in pushing the scale and capability of LLM in recent years, training\nsuch a large-scale model still involves significant optimization and system\nchallenges. To stabilize the training process, we propose depth-scaled sandwich\nnormalization, which effectively eliminates loss spikes during the training\nprocess of deep models. We pre-train our model on 13.2 trillion diverse and\nhigh-quality tokens and further enhance its reasoning capabilities during\npost-training. To perform such large-scale training efficiently, we utilize\n8,192 Ascend NPUs with a series of system optimizations. Evaluations on\nmultiple diverse benchmarks indicate that Pangu Ultra significantly advances\nthe state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral\nLarge 2, and even achieves competitive results with DeepSeek-R1, whose sparse\nmodel structure contains much more parameters. Our exploration demonstrates\nthat Ascend NPUs are capable of efficiently and effectively training dense\nmodels with more than 100 billion parameters. Our model and system will be\navailable for our commercial customers."}
{"id": "2504.07836", "pdf": "https://arxiv.org/pdf/2504.07836", "abs": "https://arxiv.org/abs/2504.07836", "authors": ["Junli Liu", "Qizhi Chen", "Zhigang Wang", "Yiwen Tang", "Yiting Zhang", "Chi Yan", "Dong Wang", "Xuelong Li", "Bin Zhao"], "title": "AerialVG: A Challenging Benchmark for Aerial Visual Grounding by Exploring Positional Relations", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages, 6 figures", "summary": "Visual grounding (VG) aims to localize target objects in an image based on\nnatural language descriptions. In this paper, we propose AerialVG, a new task\nfocusing on visual grounding from aerial views. Compared to traditional VG,\nAerialVG poses new challenges, \\emph{e.g.}, appearance-based grounding is\ninsufficient to distinguish among multiple visually similar objects, and\npositional relations should be emphasized. Besides, existing VG models struggle\nwhen applied to aerial imagery, where high-resolution images cause significant\ndifficulties. To address these challenges, we introduce the first AerialVG\ndataset, consisting of 5K real-world aerial images, 50K manually annotated\ndescriptions, and 103K objects. Particularly, each annotation in AerialVG\ndataset contains multiple target objects annotated with relative spatial\nrelations, requiring models to perform comprehensive spatial reasoning.\nFurthermore, we propose an innovative model especially for the AerialVG task,\nwhere a Hierarchical Cross-Attention is devised to focus on target regions, and\na Relation-Aware Grounding module is designed to infer positional relations.\nExperimental results validate the effectiveness of our dataset and method,\nhighlighting the importance of spatial reasoning in aerial visual grounding.\nThe code and dataset will be released."}
{"id": "2504.07878", "pdf": "https://arxiv.org/pdf/2504.07878", "abs": "https://arxiv.org/abs/2504.07878", "authors": ["Jianshu She", "Wenhao Zheng", "Zhengzhong Liu", "Hongyi Wang", "Eric Xing", "Huaxiu Yao", "Qirong Ho"], "title": "Token Level Routing Inference System for Edge Devices", "categories": ["cs.CL", "cs.DC"], "comment": "6 pages, 8 figures, under review of ACL system demo", "summary": "The computational complexity of large language model (LLM) inference\nsignificantly constrains their deployment efficiency on edge devices. In\ncontrast, small language models offer faster decoding and lower resource\nconsumption but often suffer from degraded response quality and heightened\nsusceptibility to hallucinations. To address this trade-off, collaborative\ndecoding, in which a large model assists in generating critical tokens, has\nemerged as a promising solution. This paradigm leverages the strengths of both\nmodel types by enabling high-quality inference through selective intervention\nof the large model, while maintaining the speed and efficiency of the smaller\nmodel. In this work, we present a novel collaborative decoding inference system\nthat allows small models to perform on-device inference while selectively\nconsulting a cloud-based large model for critical token generation. Remarkably,\nthe system achieves a 60% performance gain on CommonsenseQA using only a 0.5B\nmodel on an M1 MacBook, with under 7% of tokens generation uploaded to the\nlarge model in the cloud."}
{"id": "2504.07853", "pdf": "https://arxiv.org/pdf/2504.07853", "abs": "https://arxiv.org/abs/2504.07853", "authors": ["Jiayin Zhao", "Zhenqi Fu", "Tao Yu", "Hui Qiao"], "title": "V2V3D: View-to-View Denoised 3D Reconstruction for Light-Field Microscopy", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Light field microscopy (LFM) has gained significant attention due to its\nability to capture snapshot-based, large-scale 3D fluorescence images. However,\nexisting LFM reconstruction algorithms are highly sensitive to sensor noise or\nrequire hard-to-get ground-truth annotated data for training. To address these\nchallenges, this paper introduces V2V3D, an unsupervised view2view-based\nframework that establishes a new paradigm for joint optimization of image\ndenoising and 3D reconstruction in a unified architecture. We assume that the\nLF images are derived from a consistent 3D signal, with the noise in each view\nbeing independent. This enables V2V3D to incorporate the principle of\nnoise2noise for effective denoising. To enhance the recovery of high-frequency\ndetails, we propose a novel wave-optics-based feature alignment technique,\nwhich transforms the point spread function, used for forward propagation in\nwave optics, into convolution kernels specifically designed for feature\nalignment. Moreover, we introduce an LFM dataset containing LF images and their\ncorresponding 3D intensity volumes. Extensive experiments demonstrate that our\napproach achieves high computational efficiency and outperforms the other\nstate-of-the-art methods. These advancements position V2V3D as a promising\nsolution for 3D imaging under challenging conditions."}
{"id": "2504.07887", "pdf": "https://arxiv.org/pdf/2504.07887", "abs": "https://arxiv.org/abs/2504.07887", "authors": ["Riccardo Cantini", "Alessio Orsino", "Massimo Ruggiero", "Domenico Talia"], "title": "Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have revolutionized artificial intelligence,\ndriving advancements in machine translation, summarization, and conversational\nagents. However, their increasing integration into critical societal domains\nhas raised concerns about embedded biases, which can perpetuate stereotypes and\ncompromise fairness. These biases stem from various sources, including\nhistorical inequalities in training data, linguistic imbalances, and\nadversarial manipulation. Despite mitigation efforts, recent studies indicate\nthat LLMs remain vulnerable to adversarial attacks designed to elicit biased\nresponses. This work proposes a scalable benchmarking framework to evaluate LLM\nrobustness against adversarial bias elicitation. Our methodology involves (i)\nsystematically probing models with a multi-task approach targeting biases\nacross various sociocultural dimensions, (ii) quantifying robustness through\nsafety scores using an LLM-as-a-Judge approach for automated assessment of\nmodel responses, and (iii) employing jailbreak techniques to investigate\nvulnerabilities in safety mechanisms. Our analysis examines prevalent biases in\nboth small and large state-of-the-art models and their impact on model safety.\nAdditionally, we assess the safety of domain-specific models fine-tuned for\ncritical fields, such as medicine. Finally, we release a curated dataset of\nbias-related prompts, CLEAR-Bias, to facilitate systematic vulnerability\nbenchmarking. Our findings reveal critical trade-offs between model size and\nsafety, aiding the development of fairer and more robust future language\nmodels."}
{"id": "2504.07867", "pdf": "https://arxiv.org/pdf/2504.07867", "abs": "https://arxiv.org/abs/2504.07867", "authors": ["Joshua Li", "Fernando Jose Pena Cantu", "Emily Yu", "Alexander Wong", "Yuchen Cui", "Yuhao Chen"], "title": "SAMJAM: Zero-Shot Video Scene Graph Generation for Egocentric Kitchen Videos", "categories": ["cs.CV"], "comment": null, "summary": "Video Scene Graph Generation (VidSGG) is an important topic in understanding\ndynamic kitchen environments. Current models for VidSGG require extensive\ntraining to produce scene graphs. Recently, Vision Language Models (VLM) and\nVision Foundation Models (VFM) have demonstrated impressive zero-shot\ncapabilities in a variety of tasks. However, VLMs like Gemini struggle with the\ndynamics for VidSGG, failing to maintain stable object identities across\nframes. To overcome this limitation, we propose SAMJAM, a zero-shot pipeline\nthat combines SAM2's temporal tracking with Gemini's semantic understanding.\nSAM2 also improves upon Gemini's object grounding by producing more accurate\nbounding boxes. In our method, we first prompt Gemini to generate a frame-level\nscene graph. Then, we employ a matching algorithm to map each object in the\nscene graph with a SAM2-generated or SAM2-propagated mask, producing a\ntemporally-consistent scene graph in dynamic environments. Finally, we repeat\nthis process again in each of the following frames. We empirically demonstrate\nthat SAMJAM outperforms Gemini by 8.33% in mean recall on the EPIC-KITCHENS and\nEPIC-KITCHENS-100 datasets."}
{"id": "2504.07901", "pdf": "https://arxiv.org/pdf/2504.07901", "abs": "https://arxiv.org/abs/2504.07901", "authors": ["Hongcheng Guo", "Fei Zhao", "Shaosheng Cao", "Xinze Lyu", "Ziyan Liu", "Yue Wang", "Boyang Wang", "Zhoujun Li", "Chonggang Lu", "Zhe Xu", "Yao Hu"], "title": "Redefining Machine Translation on Social Network Services with Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "The globalization of social interactions has heightened the need for machine\ntranslation (MT) on Social Network Services (SNS), yet traditional models\nstruggle with culturally nuanced content like memes, slang, and pop culture\nreferences. While large language models (LLMs) have advanced general-purpose\ntranslation, their performance on SNS-specific content remains limited due to\ninsufficient specialized training data and evaluation benchmarks. This paper\nintroduces RedTrans, a 72B LLM tailored for SNS translation, trained on a novel\ndataset developed through three innovations: (1) Supervised Finetuning with\nDual-LLM Back-Translation Sampling, an unsupervised sampling method using\nLLM-based back-translation to select diverse data for large-scale finetuning;\n(2) Rewritten Preference Optimization (RePO), an algorithm that identifies and\ncorrects erroneous preference pairs through expert annotation, building\nreliable preference corpora; and (3) RedTrans-Bench, the first benchmark for\nSNS translation, evaluating phenomena like humor localization, emoji semantics,\nand meme adaptation. Experiments show RedTrans outperforms state-of-the-art\nLLMs. Besides, RedTrans has already been deployed in a real-world production\nenvironment, demonstrating that domain-specific adaptation, effectively bridges\nthe gap between generic and culturally grounded translation systems."}
{"id": "2504.07934", "pdf": "https://arxiv.org/pdf/2504.07934", "abs": "https://arxiv.org/abs/2504.07934", "authors": ["Xiyao Wang", "Zhengyuan Yang", "Chao Feng", "Hongjin Lu", "Linjie Li", "Chung-Ching Lin", "Kevin Lin", "Furong Huang", "Lijuan Wang"], "title": "SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement", "categories": ["cs.CV"], "comment": "21 pages, 5 figures", "summary": "In this paper, we present an effective method to enhance visual reasoning\nwith significantly fewer training samples, relying purely on self-improvement\nwith no knowledge distillation. Our key insight is that the difficulty of\ntraining data during reinforcement fine-tuning (RFT) is critical. Appropriately\nchallenging samples can substantially boost reasoning capabilities even when\nthe dataset is small. Despite being intuitive, the main challenge remains in\naccurately quantifying sample difficulty to enable effective data filtering. To\nthis end, we propose a novel way of repurposing Monte Carlo Tree Search (MCTS)\nto achieve that. Starting from our curated 70k open-source training samples, we\nintroduce an MCTS-based selection method that quantifies sample difficulty\nbased on the number of iterations required by the VLMs to solve each problem.\nThis explicit step-by-step reasoning in MCTS enforces the model to think longer\nand better identifies samples that are genuinely challenging. We filter and\nretain 11k samples to perform RFT on Qwen2.5-VL-7B-Instruct, resulting in our\nfinal model, ThinkLite-VL. Evaluation results on eight benchmarks show that\nThinkLite-VL improves the average performance of Qwen2.5-VL-7B-Instruct by 7%,\nusing only 11k training samples with no knowledge distillation. This\nsignificantly outperforms all existing 7B-level reasoning VLMs, and our fairly\ncomparable baselines that use classic selection methods such as accuracy-based\nfiltering. Notably, on MathVista, ThinkLite-VL-7B achieves the SoTA accuracy of\n75.1, surpassing Qwen2.5-VL-72B, GPT-4o, and O1. Our code, data, and model are\navailable at https://github.com/si0wang/ThinkLite-VL."}
{"id": "2504.07103", "pdf": "https://arxiv.org/pdf/2504.07103", "abs": "https://arxiv.org/abs/2504.07103", "authors": ["Yubin Hong", "Chaofan Li", "Jingyi Zhang", "Yingxia Shao"], "title": "FG-RAG: Enhancing Query-Focused Summarization with Context-Aware Fine-Grained Graph RAG", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enables large language models to provide\nmore precise and pertinent responses by incorporating external knowledge. In\nthe Query-Focused Summarization (QFS) task, GraphRAG-based approaches have\nnotably enhanced the comprehensiveness and diversity of generated responses.\nHowever, existing GraphRAG-based approaches predominantly focus on\ncoarse-grained information summarization without being aware of the specific\nquery, and the retrieved content lacks sufficient contextual information to\ngenerate comprehensive responses. To address the deficiencies of current RAG\nsystems, we propose Context-Aware Fine-Grained Graph RAG (FG-RAG) to enhance\nthe performance of the QFS task. FG-RAG employs Context-Aware Entity Expansion\nin graph retrieval to expand the coverage of retrieved entities in the graph,\nthus providing enough contextual information for the retrieved content.\nFurthermore, FG-RAG utilizes Query-Level Fine-Grained Summarization to\nincorporate fine-grained details during response generation, enhancing query\nawareness for the generated summarization. Our evaluation demonstrates that\nFG-RAG outperforms other RAG systems in multiple metrics of comprehensiveness,\ndiversity, and empowerment when handling the QFS task. Our implementation is\navailable at https://github.com/BuptWululu/FG-RAG."}
{"id": "2504.07940", "pdf": "https://arxiv.org/pdf/2504.07940", "abs": "https://arxiv.org/abs/2504.07940", "authors": ["Rundong Luo", "Matthew Wallingford", "Ali Farhadi", "Noah Snavely", "Wei-Chiu Ma"], "title": "Beyond the Frame: Generating 360¬∞ Panoramic Videos from Perspective Videos", "categories": ["cs.CV"], "comment": "Project page: https://red-fairy.github.io/argus/", "summary": "360{\\deg} videos have emerged as a promising medium to represent our dynamic\nvisual world. Compared to the \"tunnel vision\" of standard cameras, their\nborderless field of view offers a more complete perspective of our\nsurroundings. While existing video models excel at producing standard videos,\ntheir ability to generate full panoramic videos remains elusive. In this paper,\nwe investigate the task of video-to-360{\\deg} generation: given a perspective\nvideo as input, our goal is to generate a full panoramic video that is\nconsistent with the original video. Unlike conventional video generation tasks,\nthe output's field of view is significantly larger, and the model is required\nto have a deep understanding of both the spatial layout of the scene and the\ndynamics of objects to maintain spatio-temporal consistency. To address these\nchallenges, we first leverage the abundant 360{\\deg} videos available online\nand develop a high-quality data filtering pipeline to curate pairwise training\ndata. We then carefully design a series of geometry- and motion-aware\noperations to facilitate the learning process and improve the quality of\n360{\\deg} video generation. Experimental results demonstrate that our model can\ngenerate realistic and coherent 360{\\deg} videos from in-the-wild perspective\nvideo. In addition, we showcase its potential applications, including video\nstabilization, camera viewpoint control, and interactive visual question\nanswering."}
{"id": "2504.07104", "pdf": "https://arxiv.org/pdf/2504.07104", "abs": "https://arxiv.org/abs/2504.07104", "authors": ["Will LeVine", "Bijan Varjavand"], "title": "Relevance Isn't All You Need: Scaling RAG Systems With Inference-Time Compute Via Multi-Criteria Reranking", "categories": ["cs.IR", "cs.CL", "cs.LG"], "comment": null, "summary": "Modern Large Language Model (LLM) systems typically rely on Retrieval\nAugmented Generation (RAG) which aims to gather context that is useful for\nresponse generation. These RAG systems typically optimize strictly towards\nretrieving context that is maximally relevant to the query. However,\nconventional theory suggests that retrieval systems which seek to maximize\ncontext relevance without any additional explicit criteria can create\ninformation bottlenecks. We reaffirm this finding in the modern age of LLM's by\nshowing that in standard RAG pipelines, maximizing for context relevance alone\ncan degrade downstream response quality. In response, we show evaluations of\nexisting RAG methods which account for both context relevance and answer\nquality. These evaluations introduce a novel finding that existing RAG systems\nscale poorly with inference time compute usage when considering our combined\nmetric. We introduce \"RErank BEyond reLevance (REBEL)\", which enables RAG\nsystems to scale with inference-time compute via injection of multi-criteria\noptimization using Chain-of-Thought prompting (and optionally Multi-Turn\ndialogue). Ultimately, this enables a new performance/speed tradeoff curve,\nwhere RAG systems are able to achieve both higher relevance of retrieved\ncontexts and superior answer quality as inference time increases. Code for the\nimplementation of our method in llama-index can be found at the following PR:\nhttps://github.com/run-llama/llama_index/pull/17590. Code for running\nexperiments using this llama-index implementation can be found at\nhttps://github.com/microsoft/REBEL."}
{"id": "2504.07942", "pdf": "https://arxiv.org/pdf/2504.07942", "abs": "https://arxiv.org/abs/2504.07942", "authors": ["Nico Catalano", "Stefano Samele", "Paolo Pertino", "Matteo Matteucci"], "title": "MARS: a Multimodal Alignment and Ranking System for Few-Shot Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Current Few Shot Segmentation literature lacks a mask selection method that\ngoes beyond visual similarity between the query and example images, leading to\nsuboptimal predictions. We present MARS, a plug-and-play ranking system that\nleverages multimodal cues to filter and merge mask proposals robustly. Starting\nfrom a set of mask predictions for a single query image, we score, filter, and\nmerge them to improve results. Proposals are evaluated using multimodal scores\ncomputed at local and global levels. Extensive experiments on COCO-20i,\nPascal-5i, LVIS-92i, and FSS-1000 demonstrate that integrating all four scoring\ncomponents is crucial for robust ranking, validating our contribution. As MARS\ncan be effortlessly integrated with various mask proposal systems, we deploy it\nacross a wide range of top-performer methods and achieve new state-of-the-art\nresults on multiple existing benchmarks. Code will be available upon\nacceptance."}
{"id": "2504.07109", "pdf": "https://arxiv.org/pdf/2504.07109", "abs": "https://arxiv.org/abs/2504.07109", "authors": ["Maxime Louis", "Thibault Formal", "Herv√© Dejean", "St√©phane Clinchant"], "title": "OSCAR: Online Soft Compression And Reranking", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by\nintegrating external knowledge, leading to improved accuracy and relevance.\nHowever, scaling RAG pipelines remains computationally expensive as retrieval\nsizes grow. To address this, we introduce OSCAR, a novel query-dependent online\nsoft compression method that reduces computational overhead while preserving\nperformance. Unlike traditional hard compression methods, which shorten\nretrieved texts, or soft compression approaches, which map documents to\ncontinuous embeddings offline, OSCAR dynamically compresses retrieved\ninformation at inference time, eliminating storage overhead and enabling higher\ncompression rates. Additionally, we extend OSCAR to simultaneously perform\nreranking, further optimizing the efficiency of the RAG pipeline. Our\nexperiments demonstrate state-of-the-art performance with a 2-5x speed-up in\ninference and minimal to no loss in accuracy for LLMs ranging from 1B to 24B\nparameters. The models are available at:\nhttps://huggingface.co/collections/naver/oscar-67d446a8e3a2551f57464295."}
{"id": "2504.07943", "pdf": "https://arxiv.org/pdf/2504.07943", "abs": "https://arxiv.org/abs/2504.07943", "authors": ["Yunhan Yang", "Yuan-Chen Guo", "Yukun Huang", "Zi-Xin Zou", "Zhipeng Yu", "Yangguang Li", "Yan-Pei Cao", "Xihui Liu"], "title": "HoloPart: Generative 3D Part Amodal Segmentation", "categories": ["cs.CV"], "comment": "Project Page: https://vast-ai-research.github.io/HoloPart", "summary": "3D part amodal segmentation--decomposing a 3D shape into complete,\nsemantically meaningful parts, even when occluded--is a challenging but crucial\ntask for 3D content creation and understanding. Existing 3D part segmentation\nmethods only identify visible surface patches, limiting their utility. Inspired\nby 2D amodal segmentation, we introduce this novel task to the 3D domain and\npropose a practical, two-stage approach, addressing the key challenges of\ninferring occluded 3D geometry, maintaining global shape consistency, and\nhandling diverse shapes with limited training data. First, we leverage existing\n3D part segmentation to obtain initial, incomplete part segments. Second, we\nintroduce HoloPart, a novel diffusion-based model, to complete these segments\ninto full 3D parts. HoloPart utilizes a specialized architecture with local\nattention to capture fine-grained part geometry and global shape context\nattention to ensure overall shape consistency. We introduce new benchmarks\nbased on the ABO and PartObjaverse-Tiny datasets and demonstrate that HoloPart\nsignificantly outperforms state-of-the-art shape completion methods. By\nincorporating HoloPart with existing segmentation techniques, we achieve\npromising results on 3D part amodal segmentation, opening new avenues for\napplications in geometry editing, animation, and material assignment."}
{"id": "2504.07126", "pdf": "https://arxiv.org/pdf/2504.07126", "abs": "https://arxiv.org/abs/2504.07126", "authors": ["Osama Ahmed Marzouk", "Omar Rashid Hamdan Al Badi", "Maadh Hamed Salman Al Rashdi", "Hamed Mohammed Eid Al Balushi"], "title": "Proposed 2MW Wind Turbine for Use in the Governorate of Dhofar at the Sultanate of Oman", "categories": ["cs.CE", "cs.CL", "65D30, 65-04, 76G25", "D.3.0; G.1.4; G.1.10; J.2; J.6"], "comment": "9 pages, 14 figures, 2 tables, 1 computer code, open access,\n  published peer-reviewed journal paper", "summary": "In this work, we propose a preliminary design of a horizontal-axis wind\nturbine (HAWT) as a candidate for the Dhofar Wind Farm project, in the southern\nOmani Governorate \"Dhofar\", at the southwest part of the Sultanate of Oman.\nThis wind farm (under construction) is considered to be the first commercial,\nutility-scale (50MW) wind farm in the GCC (Gulf Cooperation Council) area. The\nproposed wind turbine has an expected electricity generation of 2MW. We studied\nthe wind atlas of Oman and from which we determined the maximum possible mean\nwind speed in the entire Sultanate and built our design based on that reference\nvalue, which is 6m/s (21.6km/h). After this, we applied a set of modeling\nequations that estimate the power output from the wind turbine rotor and\nmatched the target electric power to the design variables using a MATLAB\ncomputer code. We reached a suitable design and we present here the\ndistribution of the blade angle (twist angle), and the power per unit span\nalong the rotor blade. The rotor design has 3 blades with a diameter of 70m and\na rotational speed of 24rpm. This rotor gives 2.37MW of output power, which\nexceeds the target 2MW output, allowing for about 15% of power losses in the\ngearbox and generator. We utilized some commercial designs of wind turbines\nfrom different international manufacturers as references for typical limits or\nrecommended values of some design parameters."}
{"id": "2504.07945", "pdf": "https://arxiv.org/pdf/2504.07945", "abs": "https://arxiv.org/abs/2504.07945", "authors": ["Hao Yu", "Rupayan Mallick", "Margrit Betke", "Sarah Adel Bargal"], "title": "GenEAva: Generating Cartoon Avatars with Fine-Grained Facial Expressions from Realistic Diffusion-based Faces", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Cartoon avatars have been widely used in various applications, including\nsocial media, online tutoring, and gaming. However, existing cartoon avatar\ndatasets and generation methods struggle to present highly expressive avatars\nwith fine-grained facial expressions and are often inspired from real-world\nidentities, raising privacy concerns. To address these challenges, we propose a\nnovel framework, GenEAva, for generating high-quality cartoon avatars with\nfine-grained facial expressions. Our approach fine-tunes a state-of-the-art\ntext-to-image diffusion model to synthesize highly detailed and expressive\nfacial expressions. We then incorporate a stylization model that transforms\nthese realistic faces into cartoon avatars while preserving both identity and\nexpression. Leveraging this framework, we introduce the first expressive\ncartoon avatar dataset, GenEAva 1.0, specifically designed to capture 135\nfine-grained facial expressions, featuring 13,230 expressive cartoon avatars\nwith a balanced distribution across genders, racial groups, and age ranges. We\ndemonstrate that our fine-tuned model generates more expressive faces than the\nstate-of-the-art text-to-image diffusion model SDXL. We also verify that the\ncartoon avatars generated by our framework do not include memorized identities\nfrom fine-tuning data. The proposed framework and dataset provide a diverse and\nexpressive benchmark for future research in cartoon avatar generation."}
{"id": "2504.07158", "pdf": "https://arxiv.org/pdf/2504.07158", "abs": "https://arxiv.org/abs/2504.07158", "authors": ["Ling Team", "Caizhi Tang", "Chilin Fu", "Chunwei Wu", "Jia Guo", "Jianwen Wang", "Jingyu Hu", "Liang Jiang", "Meng Li", "Peng Jiao", "Pingping Liu", "Shaomian Zheng", "Shiwei Liang", "Shuaicheng Li", "Yalin Zhang", "Yingting Wu", "Yongkang Liu", "Zhenyu Huang"], "title": "Holistic Capability Preservation: Towards Compact Yet Comprehensive Reasoning Models", "categories": ["cs.LG", "cs.CL"], "comment": "10 pages", "summary": "This technical report presents Ring-Lite-Distill, a lightweight reasoning\nmodel derived from our open-source Mixture-of-Experts (MoE) Large Language\nModels (LLMs) Ling-Lite. This study demonstrates that through meticulous\nhigh-quality data curation and ingenious training paradigms, the compact MoE\nmodel Ling-Lite can be further trained to achieve exceptional reasoning\ncapabilities, while maintaining its parameter-efficient architecture with only\n2.75 billion activated parameters, establishing an efficient lightweight\nreasoning architecture. In particular, in constructing this model, we have not\nmerely focused on enhancing advanced reasoning capabilities, exemplified by\nhigh-difficulty mathematical problem solving, but rather aimed to develop a\nreasoning model with more comprehensive competency coverage. Our approach\nensures coverage across reasoning tasks of varying difficulty levels while\npreserving generic capabilities, such as instruction following, tool use, and\nknowledge retention. We show that, Ring-Lite-Distill's reasoning ability\nreaches a level comparable to DeepSeek-R1-Distill-Qwen-7B, while its general\ncapabilities significantly surpass those of DeepSeek-R1-Distill-Qwen-7B. The\nmodels are accessible at https://huggingface.co/inclusionAI"}
{"id": "2504.07949", "pdf": "https://arxiv.org/pdf/2504.07949", "abs": "https://arxiv.org/abs/2504.07949", "authors": ["Kefan Chen", "Sergiu Oprea", "Justin Theiss", "Sreyas Mohan", "Srinath Sridhar", "Aayush Prakash"], "title": "InteractAvatar: Modeling Hand-Face Interaction in Photorealistic Avatars with Deformable Gaussians", "categories": ["cs.CV"], "comment": null, "summary": "With the rising interest from the community in digital avatars coupled with\nthe importance of expressions and gestures in communication, modeling natural\navatar behavior remains an important challenge across many industries such as\nteleconferencing, gaming, and AR/VR. Human hands are the primary tool for\ninteracting with the environment and essential for realistic human behavior\nmodeling, yet existing 3D hand and head avatar models often overlook the\ncrucial aspect of hand-body interactions, such as between hand and face. We\npresent InteracttAvatar, the first model to faithfully capture the\nphotorealistic appearance of dynamic hand and non-rigid hand-face interactions.\nOur novel Dynamic Gaussian Hand model, combining template model and 3D Gaussian\nSplatting as well as a dynamic refinement module, captures pose-dependent\nchange, e.g. the fine wrinkles and complex shadows that occur during\narticulation. Importantly, our hand-face interaction module models the subtle\ngeometry and appearance dynamics that underlie common gestures. Through\nexperiments of novel view synthesis, self reenactment and cross-identity\nreenactment, we demonstrate that InteracttAvatar can reconstruct hand and\nhand-face interactions from monocular or multiview videos with high-fidelity\ndetails and be animated with novel poses."}
{"id": "2504.07164", "pdf": "https://arxiv.org/pdf/2504.07164", "abs": "https://arxiv.org/abs/2504.07164", "authors": ["Naman Jain", "Jaskirat Singh", "Manish Shetty", "Liang Zheng", "Koushik Sen", "Ion Stoica"], "title": "R2E-Gym: Procedural Environments and Hybrid Verifiers for Scaling Open-Weights SWE Agents", "categories": ["cs.SE", "cs.CL", "cs.LG"], "comment": "Website: https://r2e-gym.github.io/", "summary": "Improving open-source models on real-world SWE tasks (solving GITHUB issues)\nfaces two key challenges: 1) scalable curation of execution environments to\ntrain these models, and, 2) optimal scaling of test-time compute. We introduce\nAgentGym, the largest procedurally-curated executable gym environment for\ntraining real-world SWE-agents, consisting of more than 8.7K tasks. AgentGym is\npowered by two main contributions: 1) SYNGEN: a synthetic data curation recipe\nthat enables scalable curation of executable environments using test-generation\nand back-translation directly from commits, thereby reducing reliance on\nhuman-written issues or unit tests. We show that this enables more scalable\ntraining leading to pass@1 performance of 34.4% on SWE-Bench Verified benchmark\nwith our 32B model. 2) Hybrid Test-time Scaling: we provide an in-depth\nanalysis of two test-time scaling axes; execution-based and execution-free\nverifiers, demonstrating that they exhibit complementary strengths and\nlimitations. Test-based verifiers suffer from low distinguishability, while\nexecution-free verifiers are biased and often rely on stylistic features.\nSurprisingly, we find that while each approach individually saturates around\n42-43%, significantly higher gains can be obtained by leveraging their\ncomplementary strengths. Overall, our approach achieves 51% on the SWE-Bench\nVerified benchmark, reflecting a new state-of-the-art for open-weight\nSWE-agents and for the first time showing competitive performance with\nproprietary models such as o1, o1-preview and sonnet-3.5-v2 (with tools). We\nwill open-source our environments, models, and agent trajectories."}
{"id": "2504.07951", "pdf": "https://arxiv.org/pdf/2504.07951", "abs": "https://arxiv.org/abs/2504.07951", "authors": ["Mustafa Shukor", "Enrico Fini", "Victor Guilherme Turrisi da Costa", "Matthieu Cord", "Joshua Susskind", "Alaaeldin El-Nouby"], "title": "Scaling Laws for Native Multimodal Models Scaling Laws for Native Multimodal Models", "categories": ["cs.CV"], "comment": "31 pages, 26 figures, 13 tables", "summary": "Building general-purpose models that can effectively perceive the world\nthrough multimodal signals has been a long-standing goal. Current approaches\ninvolve integrating separately pre-trained components, such as connecting\nvision encoders to LLMs and continuing multimodal training. While such\napproaches exhibit remarkable sample efficiency, it remains an open question\nwhether such late-fusion architectures are inherently superior. In this work,\nwe revisit the architectural design of native multimodal models (NMMs)--those\ntrained from the ground up on all modalities--and conduct an extensive scaling\nlaws study, spanning 457 trained models with different architectures and\ntraining mixtures. Our investigation reveals no inherent advantage to\nlate-fusion architectures over early-fusion ones, which do not rely on image\nencoders. On the contrary, early-fusion exhibits stronger performance at lower\nparameter counts, is more efficient to train, and is easier to deploy.\nMotivated by the strong performance of the early-fusion architectures, we show\nthat incorporating Mixture of Experts (MoEs) allows for models that learn\nmodality-specific weights, significantly enhancing performance."}
{"id": "2504.07389", "pdf": "https://arxiv.org/pdf/2504.07389", "abs": "https://arxiv.org/abs/2504.07389", "authors": ["Hanqi Xiao", "Yi-Lin Sung", "Elias Stengel-Eskin", "Mohit Bansal"], "title": "Task-Circuit Quantization: Leveraging Knowledge Localization and Interpretability for Compression", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "24 pages. Code: https://github.com/The-Inscrutable-X/TACQ", "summary": "Post-training quantization (PTQ) reduces a model's memory footprint by\nmapping full precision weights into low bit weights without costly retraining,\nbut can degrade its downstream performance especially in low 2- to 3-bit\nsettings. We develop a new mixed-precision PTQ approach, Task-Circuit\nQuantization (TaCQ), that draws parallels to automated circuit discovery,\ndirectly conditioning the quantization process on specific weight circuits --\nwhich we define as sets of weights associated with downstream task performance.\nThese weights are kept as 16-bit weights, while others are quantized,\nmaintaining performance while only adding a marginal memory cost. Specifically,\nTaCQ contrasts unquantized model weights with a uniformly-quantized model to\nestimate the expected change in weights due to quantization and uses gradient\ninformation to predict the resulting impact on task performance, allowing us to\npreserve task-specific weights. We compare TaCQ-based quantization to existing\nmixed-precision quantization methods when conditioning both on general-purpose\nand task-specific data. Across QA, math reasoning, and text-to-SQL tasks for\nboth Llama-3 and Qwen2.5, we find that TaCQ outperforms baselines using the\nsame calibration data and a lower weight budget, achieving major improvements\nin the 2 and 3-bit regime. With only 3.1 bits we are able to recover 96% of\nLlama-3-8B-Instruct's unquantized 16-bit MMLU performance, obtaining a 5.25%\nabsolute improvement over SPQR. We also observe consistently large gains over\nexisting methods in the 2-bit regime, with an average gain of 14.74% over the\nstrongest baseline, SliM-LLM. Moreover, we observe a 7.20% gain without\nconditioning on specific tasks, showing TaCQ's ability to identify important\nweights is not limited to task-conditioned settings."}
{"id": "2504.07954", "pdf": "https://arxiv.org/pdf/2504.07954", "abs": "https://arxiv.org/abs/2504.07954", "authors": ["En Yu", "Kangheng Lin", "Liang Zhao", "Jisheng Yin", "Yana Wei", "Yuang Peng", "Haoran Wei", "Jianjian Sun", "Chunrui Han", "Zheng Ge", "Xiangyu Zhang", "Daxin Jiang", "Jingyu Wang", "Wenbing Tao"], "title": "Perception-R1: Pioneering Perception Policy with Reinforcement Learning", "categories": ["cs.CV", "cs.CL"], "comment": "Github page: https://github.com/linkangheng/PR1", "summary": "Inspired by the success of DeepSeek-R1, we explore the potential of\nrule-based reinforcement learning (RL) in MLLM post-training for perception\npolicy learning. While promising, our initial experiments reveal that\nincorporating a thinking process through RL does not consistently lead to\nperformance gains across all visual perception tasks. This leads us to delve\ninto the essential role of RL in the context of visual perception. In this\nwork, we return to the fundamentals and explore the effects of RL on different\nperception tasks. We observe that the perceptual complexity is a major factor\nin determining the effectiveness of RL. We also observe that reward design\nplays a crucial role in further approching the upper limit of model perception.\nTo leverage these findings, we propose Perception-R1, a scalable RL framework\nusing GRPO during MLLM post-training. With a standard Qwen2.5-VL-3B-Instruct,\nPerception-R1 achieves +4.2% on RefCOCO+, +17.9% on PixMo-Count, +4.2% on\nPageOCR, and notably, 31.9% AP on COCO2017 val for the first time, establishing\na strong baseline for perception policy learning."}
{"id": "2504.07415", "pdf": "https://arxiv.org/pdf/2504.07415", "abs": "https://arxiv.org/abs/2504.07415", "authors": ["Kyoyun Choi", "Byungmu Yoon", "Soobum Kim", "Jonggwon Park"], "title": "Leveraging LLMs for Multimodal Retrieval-Augmented Radiology Report Generation via Key Phrase Extraction", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "Automated radiology report generation (RRG) holds potential to reduce\nradiologists' workload, especially as recent advancements in large language\nmodels (LLMs) enable the development of multimodal models for chest X-ray (CXR)\nreport generation. However, multimodal LLMs (MLLMs) are resource-intensive,\nrequiring vast datasets and substantial computational cost for training. To\naddress these challenges, we propose a retrieval-augmented generation approach\nthat leverages multimodal retrieval and LLMs to generate radiology reports\nwhile mitigating hallucinations and reducing computational demands. Our method\nuses LLMs to extract key phrases from radiology reports, effectively focusing\non essential diagnostic information. Through exploring effective training\nstrategies, including image encoder structure search, adding noise to text\nembeddings, and additional training objectives, we combine complementary\npre-trained image encoders and adopt contrastive learning between text and\nsemantic image embeddings. We evaluate our approach on MIMIC-CXR dataset,\nachieving state-of-the-art results on CheXbert metrics and competitive RadGraph\nF1 metric alongside MLLMs, without requiring LLM fine-tuning. Our method\ndemonstrates robust generalization for multi-view RRG, making it suitable for\ncomprehensive clinical applications."}
{"id": "2504.07955", "pdf": "https://arxiv.org/pdf/2504.07955", "abs": "https://arxiv.org/abs/2504.07955", "authors": ["Yuanhong Yu", "Xingyi He", "Chen Zhao", "Junhao Yu", "Jiaqi Yang", "Ruizhen Hu", "Yujun Shen", "Xing Zhu", "Xiaowei Zhou", "Sida Peng"], "title": "BoxDreamer: Dreaming Box Corners for Generalizable Object Pose Estimation", "categories": ["cs.CV"], "comment": "Project page: https://zju3dv.github.io/boxdreamer", "summary": "This paper presents a generalizable RGB-based approach for object pose\nestimation, specifically designed to address challenges in sparse-view\nsettings. While existing methods can estimate the poses of unseen objects,\ntheir generalization ability remains limited in scenarios involving occlusions\nand sparse reference views, restricting their real-world applicability. To\novercome these limitations, we introduce corner points of the object bounding\nbox as an intermediate representation of the object pose. The 3D object corners\ncan be reliably recovered from sparse input views, while the 2D corner points\nin the target view are estimated through a novel reference-based point\nsynthesizer, which works well even in scenarios involving occlusions. As object\nsemantic points, object corners naturally establish 2D-3D correspondences for\nobject pose estimation with a PnP algorithm. Extensive experiments on the\nYCB-Video and Occluded-LINEMOD datasets show that our approach outperforms\nstate-of-the-art methods, highlighting the effectiveness of the proposed\nrepresentation and significantly enhancing the generalization capabilities of\nobject pose estimation, which is crucial for real-world applications."}
{"id": "2504.07416", "pdf": "https://arxiv.org/pdf/2504.07416", "abs": "https://arxiv.org/abs/2504.07416", "authors": ["Jonggwon Park", "Soobum Kim", "Byungmu Yoon", "Kyoyun Choi"], "title": "RadZero: Similarity-Based Cross-Attention for Explainable Vision-Language Alignment in Radiology with Zero-Shot Multi-Task Capability", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "Recent advancements in multi-modal models have significantly improved\nvision-language alignment in radiology. However, existing approaches struggle\nto effectively utilize complex radiology reports for learning, rely on\nlow-resolution images, and offer limited interpretability in attention\nmechanisms. To address these challenges, we introduce RadZero, a novel\nsimilarity-based cross-attention framework for vision-language alignment in\nradiology with zero-shot multi-task capability. RadZero leverages large\nlanguage models to extract minimal semantic sentences from radiology reports\nand employs a multi-positive contrastive learning strategy to effectively\ncapture relationships between images and multiple relevant textual\ndescriptions. It also utilizes a pre-trained vision encoder with additional\ntrainable Transformer layers, allowing efficient high-resolution image\nprocessing. By computing similarity between text embeddings and local image\npatch features, RadZero enables zero-shot inference with similarity probability\nfor classification and pixel-level cross-modal similarity maps for grounding\nand segmentation. Experimental results on public chest radiograph benchmarks\nshow that RadZero outperforms state-of-the-art methods in zero-shot\nclassification, grounding, and segmentation. Furthermore, cross-modal\nsimilarity map analysis highlights its potential for improving explainability\nin vision-language alignment. Additionally, qualitative evaluation demonstrates\nRadZero's capability for open-vocabulary semantic segmentation, further\nvalidating its effectiveness in medical imaging."}
{"id": "2504.07956", "pdf": "https://arxiv.org/pdf/2504.07956", "abs": "https://arxiv.org/abs/2504.07956", "authors": ["Yukun Qi", "Yiming Zhao", "Yu Zeng", "Xikun Bao", "Wenxuan Huang", "Lin Chen", "Zehui Chen", "Jie Zhao", "Zhongang Qi", "Feng Zhao"], "title": "VCR-Bench: A Comprehensive Evaluation Framework for Video Chain-of-Thought Reasoning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "The advancement of Chain-of-Thought (CoT) reasoning has significantly\nenhanced the capabilities of large language models (LLMs) and large\nvision-language models (LVLMs). However, a rigorous evaluation framework for\nvideo CoT reasoning remains absent. Current video benchmarks fail to adequately\nassess the reasoning process and expose whether failures stem from deficiencies\nin perception or reasoning capabilities. Therefore, we introduce VCR-Bench, a\nnovel benchmark designed to comprehensively evaluate LVLMs' Video\nChain-of-Thought Reasoning capabilities. VCR-Bench comprises 859 videos\nspanning a variety of video content and durations, along with 1,034\nhigh-quality question-answer pairs. Each pair is manually annotated with a\nstepwise CoT rationale, where every step is tagged to indicate its association\nwith the perception or reasoning capabilities. Furthermore, we design seven\ndistinct task dimensions and propose the CoT score to assess the entire CoT\nprocess based on the stepwise tagged CoT rationals. Extensive experiments on\nVCR-Bench highlight substantial limitations in current LVLMs. Even the\ntop-performing model, o1, only achieves a 62.8% CoT score and an 56.7%\naccuracy, while most models score below 40%. Experiments show most models score\nlower on perception than reasoning steps, revealing LVLMs' key bottleneck in\ntemporal-spatial information processing for complex video reasoning. A robust\npositive correlation between the CoT score and accuracy confirms the validity\nof our evaluation framework and underscores the critical role of CoT reasoning\nin solving complex video reasoning tasks. We hope VCR-Bench to serve as a\nstandardized evaluation framework and expose the actual drawbacks in complex\nvideo reasoning task."}
{"id": "2504.07439", "pdf": "https://arxiv.org/pdf/2504.07439", "abs": "https://arxiv.org/abs/2504.07439", "authors": ["Qi Liu", "Haozhe Duan", "Yiqun Chen", "Quanfeng Lu", "Weiwei Sun", "Jiaxin Mao"], "title": "LLM4Ranking: An Easy-to-use Framework of Utilizing Large Language Models for Document Reranking", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Utilizing large language models (LLMs) for document reranking has been a\npopular and promising research direction in recent years, many studies are\ndedicated to improving the performance and efficiency of using LLMs for\nreranking. Besides, it can also be applied in many real-world applications,\nsuch as search engines or retrieval-augmented generation. In response to the\ngrowing demand for research and application in practice, we introduce a unified\nframework, \\textbf{LLM4Ranking}, which enables users to adopt different ranking\nmethods using open-source or closed-source API-based LLMs. Our framework\nprovides a simple and extensible interface for document reranking with LLMs, as\nwell as easy-to-use evaluation and fine-tuning scripts for this task. We\nconducted experiments based on this framework and evaluated various models and\nmethods on several widely used datasets, providing reproducibility results on\nutilizing LLMs for document reranking. Our code is publicly available at\nhttps://github.com/liuqi6777/llm4ranking."}
{"id": "2504.07957", "pdf": "https://arxiv.org/pdf/2504.07957", "abs": "https://arxiv.org/abs/2504.07957", "authors": ["Shengyuan Ding", "Shenxi Wu", "Xiangyu Zhao", "Yuhang Zang", "Haodong Duan", "Xiaoyi Dong", "Pan Zhang", "Yuhang Cao", "Dahua Lin", "Jiaqi Wang"], "title": "MM-IFEngine: Towards Multimodal Instruction Following", "categories": ["cs.CV"], "comment": null, "summary": "The Instruction Following (IF) ability measures how well Multi-modal Large\nLanguage Models (MLLMs) understand exactly what users are telling them and\nwhether they are doing it right. Existing multimodal instruction following\ntraining data is scarce, the benchmarks are simple with atomic instructions,\nand the evaluation strategies are imprecise for tasks demanding exact output\nconstraints. To address this, we present MM-IFEngine, an effective pipeline to\ngenerate high-quality image-instruction pairs. Our MM-IFEngine pipeline yields\nlarge-scale, diverse, and high-quality training data MM-IFInstruct-23k, which\nis suitable for Supervised Fine-Tuning (SFT) and extended as MM-IFDPO-23k for\nDirect Preference Optimization (DPO). We further introduce MM-IFEval, a\nchallenging and diverse multi-modal instruction-following benchmark that\nincludes (1) both compose-level constraints for output responses and\nperception-level constraints tied to the input images, and (2) a comprehensive\nevaluation pipeline incorporating both rule-based assessment and judge model.\nWe conduct SFT and DPO experiments and demonstrate that fine-tuning MLLMs on\nMM-IFInstruct-23k and MM-IFDPO-23k achieves notable gains on various IF\nbenchmarks, such as MM-IFEval (+10.2$\\%$), MIA (+7.6$\\%$), and IFEval\n(+12.3$\\%$). The full data and evaluation code will be released on\nhttps://github.com/SYuan03/MM-IFEngine."}
{"id": "2504.07448", "pdf": "https://arxiv.org/pdf/2504.07448", "abs": "https://arxiv.org/abs/2504.07448", "authors": ["Juzheng Zhang", "Jiacheng You", "Ashwinee Panda", "Tom Goldstein"], "title": "LoRI: Reducing Cross-Task Interference in Multi-Task Low-Rank Adaptation", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "24 pages, 7 figures, 20 tables", "summary": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient\nfine-tuning (PEFT) method for Large Language Models (LLMs), yet it still incurs\nnotable overhead and suffers from parameter interference in multi-task\nscenarios. We propose LoRA with Reduced Interference (LoRI), a simple yet\neffective approach that freezes the projection matrices $A$ as random\nprojections and sparsifies the matrices $B$ using task-specific masks. This\ndesign substantially reduces the number of trainable parameters while\nmaintaining strong task performance. Moreover, LoRI minimizes cross-task\ninterference in adapter merging by leveraging the orthogonality between adapter\nsubspaces, and supports continual learning by using sparsity to mitigate\ncatastrophic forgetting. Extensive experiments across natural language\nunderstanding, mathematical reasoning, code generation, and safety alignment\ntasks demonstrate that LoRI outperforms full fine-tuning and existing PEFT\nmethods, while using up to 95% fewer trainable parameters than LoRA. In\nmulti-task experiments, LoRI enables effective adapter merging and continual\nlearning with reduced cross-task interference. Code is available at:\nhttps://github.com/juzhengz/LoRI"}
{"id": "2504.07958", "pdf": "https://arxiv.org/pdf/2504.07958", "abs": "https://arxiv.org/abs/2504.07958", "authors": ["Hanxue Zhang", "Haoran Jiang", "Qingsong Yao", "Yanan Sun", "Renrui Zhang", "Hao Zhao", "Hongyang Li", "Hongzi Zhu", "Zetong Yang"], "title": "Detect Anything 3D in the Wild", "categories": ["cs.CV"], "comment": null, "summary": "Despite the success of deep learning in close-set 3D object detection,\nexisting approaches struggle with zero-shot generalization to novel objects and\ncamera configurations. We introduce DetAny3D, a promptable 3D detection\nfoundation model capable of detecting any novel object under arbitrary camera\nconfigurations using only monocular inputs. Training a foundation model for 3D\ndetection is fundamentally constrained by the limited availability of annotated\n3D data, which motivates DetAny3D to leverage the rich prior knowledge embedded\nin extensively pre-trained 2D foundation models to compensate for this\nscarcity. To effectively transfer 2D knowledge to 3D, DetAny3D incorporates two\ncore modules: the 2D Aggregator, which aligns features from different 2D\nfoundation models, and the 3D Interpreter with Zero-Embedding Mapping, which\nmitigates catastrophic forgetting in 2D-to-3D knowledge transfer. Experimental\nresults validate the strong generalization of our DetAny3D, which not only\nachieves state-of-the-art performance on unseen categories and novel camera\nconfigurations, but also surpasses most competitors on in-domain data.DetAny3D\nsheds light on the potential of the 3D foundation model for diverse\napplications in real-world scenarios, e.g., rare object detection in autonomous\ndriving, and demonstrates promise for further exploration of 3D-centric tasks\nin open-world settings. More visualization results can be found at DetAny3D\nproject page."}
{"id": "2504.07615", "pdf": "https://arxiv.org/pdf/2504.07615", "abs": "https://arxiv.org/abs/2504.07615", "authors": ["Haozhan Shen", "Peng Liu", "Jingcheng Li", "Chunxin Fang", "Yibo Ma", "Jiajia Liao", "Qiaoli Shen", "Zilun Zhang", "Kangjia Zhao", "Qianqian Zhang", "Ruochen Xu", "Tiancheng Zhao"], "title": "VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model", "categories": ["cs.CV", "cs.CL"], "comment": "11 pages", "summary": "Recently DeepSeek R1 has shown that reinforcement learning (RL) can\nsubstantially improve the reasoning capabilities of Large Language Models\n(LLMs) through a simple yet effective design. The core of R1 lies in its\nrule-based reward formulation, which leverages tasks with deterministic\nground-truth answers to enable precise and stable reward computation. In the\nvisual domain, we similarly observe that a wide range of visual understanding\ntasks are inherently equipped with well-defined ground-truth annotations. This\nproperty makes them naturally compatible with rule-based reward mechanisms.\nMotivated by this observation, we investigate the extension of R1-style\nreinforcement learning to Vision-Language Models (VLMs), aiming to enhance\ntheir visual reasoning capabilities. To this end, we develop VLM-R1, a\ndedicated framework designed to harness RL for improving VLMs' performance on\ngeneral vision-language tasks. Using this framework, we further explore the\nfeasibility of applying RL to visual domain. Experimental results indicate that\nthe RL-based model not only delivers competitive performance on visual\nunderstanding tasks but also surpasses Supervised Fine-Tuning (SFT) in\ngeneralization ability. Furthermore, we conduct comprehensive ablation studies\nthat uncover a series of noteworthy insights, including the presence of reward\nhacking in object detection, the emergence of the \"OD aha moment\", the impact\nof training data quality, and the scaling behavior of RL across different model\nsizes. Through these analyses, we aim to deepen the understanding of how\nreinforcement learning enhances the capabilities of vision-language models, and\nwe hope our findings and open-source contributions will support continued\nprogress in the vision-language RL community. Our code and model are available\nat https://github.com/om-ai-lab/VLM-R1"}
{"id": "2504.07959", "pdf": "https://arxiv.org/pdf/2504.07959", "abs": "https://arxiv.org/abs/2504.07959", "authors": ["Dongyoung Kim", "Mahmoud Afifi", "Dongyun Kim", "Michael S. Brown", "Seon Joo Kim"], "title": "CCMNet: Leveraging Calibrated Color Correction Matrices for Cross-Camera Color Constancy", "categories": ["cs.CV"], "comment": null, "summary": "Computational color constancy, or white balancing, is a key module in a\ncamera's image signal processor (ISP) that corrects color casts from scene\nlighting. Because this operation occurs in the camera-specific raw color space,\nwhite balance algorithms must adapt to different cameras. This paper introduces\na learning-based method for cross-camera color constancy that generalizes to\nnew cameras without retraining. Our method leverages pre-calibrated color\ncorrection matrices (CCMs) available on ISPs that map the camera's raw color\nspace to a standard space (e.g., CIE XYZ). Our method uses these CCMs to\ntransform predefined illumination colors (i.e., along the Planckian locus) into\nthe test camera's raw space. The mapped illuminants are encoded into a compact\ncamera fingerprint embedding (CFE) that enables the network to adapt to unseen\ncameras. To prevent overfitting due to limited cameras and CCMs during\ntraining, we introduce a data augmentation technique that interpolates between\ncameras and their CCMs. Experimental results across multiple datasets and\nbackbones show that our method achieves state-of-the-art cross-camera color\nconstancy while remaining lightweight and relying only on data readily\navailable in camera ISPs."}
{"id": "2504.07643", "pdf": "https://arxiv.org/pdf/2504.07643", "abs": "https://arxiv.org/abs/2504.07643", "authors": ["Florian Schneider", "Narges Baba Ahmadi", "Niloufar Baba Ahmadi", "Iris Vogel", "Martin Semmann", "Chris Biemann"], "title": "CollEX -- A Multimodal Agentic RAG System Enabling Interactive Exploration of Scientific Collections", "categories": ["cs.IR", "cs.CL", "cs.CV"], "comment": null, "summary": "In this paper, we introduce CollEx, an innovative multimodal agentic\nRetrieval-Augmented Generation (RAG) system designed to enhance interactive\nexploration of extensive scientific collections. Given the overwhelming volume\nand inherent complexity of scientific collections, conventional search systems\noften lack necessary intuitiveness and interactivity, presenting substantial\nbarriers for learners, educators, and researchers. CollEx addresses these\nlimitations by employing state-of-the-art Large Vision-Language Models (LVLMs)\nas multimodal agents accessible through an intuitive chat interface. By\nabstracting complex interactions via specialized agents equipped with advanced\ntools, CollEx facilitates curiosity-driven exploration, significantly\nsimplifying access to diverse scientific collections and records therein. Our\nsystem integrates textual and visual modalities, supporting educational\nscenarios that are helpful for teachers, pupils, students, and researchers by\nfostering independent exploration as well as scientific excitement and\ncuriosity. Furthermore, CollEx serves the research community by discovering\ninterdisciplinary connections and complementing visual data. We illustrate the\neffectiveness of our system through a proof-of-concept application containing\nover 64,000 unique records across 32 collections from a local scientific\ncollection from a public university."}
{"id": "2504.07960", "pdf": "https://arxiv.org/pdf/2504.07960", "abs": "https://arxiv.org/abs/2504.07960", "authors": ["Zhong-Yu Li", "Ruoyi Du", "Juncheng Yan", "Le Zhuo", "Zhen Li", "Peng Gao", "Zhanyu Ma", "Ming-Ming Cheng"], "title": "VisualCloze: A Universal Image Generation Framework via Visual In-Context Learning", "categories": ["cs.CV"], "comment": "Project page: https://visualcloze.github.io/", "summary": "Recent progress in diffusion models significantly advances various image\ngeneration tasks. However, the current mainstream approach remains focused on\nbuilding task-specific models, which have limited efficiency when supporting a\nwide range of different needs. While universal models attempt to address this\nlimitation, they face critical challenges, including generalizable task\ninstruction, appropriate task distributions, and unified architectural design.\nTo tackle these challenges, we propose VisualCloze, a universal image\ngeneration framework, which supports a wide range of in-domain tasks,\ngeneralization to unseen ones, unseen unification of multiple tasks, and\nreverse generation. Unlike existing methods that rely on language-based task\ninstruction, leading to task ambiguity and weak generalization, we integrate\nvisual in-context learning, allowing models to identify tasks from visual\ndemonstrations. Meanwhile, the inherent sparsity of visual task distributions\nhampers the learning of transferable knowledge across tasks. To this end, we\nintroduce Graph200K, a graph-structured dataset that establishes various\ninterrelated tasks, enhancing task density and transferable knowledge.\nFurthermore, we uncover that our unified image generation formulation shared a\nconsistent objective with image infilling, enabling us to leverage the strong\ngenerative priors of pre-trained infilling models without modifying the\narchitectures."}
{"id": "2504.07740", "pdf": "https://arxiv.org/pdf/2504.07740", "abs": "https://arxiv.org/abs/2504.07740", "authors": ["Keyu Liang", "Zhongxin Liu", "Chao Liu", "Zhiyuan Wan", "David Lo", "Xiaohu Yang"], "title": "Zero-Shot Cross-Domain Code Search without Fine-Tuning", "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "Code search aims to retrieve semantically relevant code snippets for natural\nlanguage queries. While pre-trained language models (PLMs) have shown\nremarkable performance in this task, they struggle in cross-domain scenarios,\noften requiring costly fine-tuning or facing performance drops in zero-shot\nsettings. RAPID, which generates synthetic data for model fine-tuning, is\ncurrently the only effective method for zero-shot cross-domain code search.\nDespite its effectiveness, RAPID demands substantial computational resources\nfor fine-tuning and needs to maintain specialized models for each domain,\nunderscoring the need for a zero-shot, fine-tuning-free approach for\ncross-domain code search.\n  The key to tackling zero-shot cross-domain code search lies in bridging the\ngaps among domains. In this work, we propose to break the query-code matching\nprocess of code search into two simpler tasks: query-comment matching and\ncode-code matching. Our empirical study reveals the strong complementarity\namong the three matching schemas in zero-shot cross-domain settings, i.e.,\nquery-code, query-comment, and code-code matching. Based on the findings, we\npropose CodeBridge, a zero-shot, fine-tuning-free approach for cross-domain\ncode search. Specifically, CodeBridge uses Large Language Models (LLMs) to\ngenerate comments and pseudo-code, then combines query-code, query-comment, and\ncode-code matching via PLM-based similarity scoring and sampling-based fusion.\nExperimental results show that our approach outperforms the state-of-the-art\nPLM-based code search approaches, i.e., CoCoSoDa and UniXcoder, by an average\nof 21.4% and 24.9% in MRR, respectively, across three datasets. Our approach\nalso yields results that are better than or comparable to those of the\nzero-shot cross-domain code search approach RAPID, which requires costly\nfine-tuning."}
{"id": "2504.07961", "pdf": "https://arxiv.org/pdf/2504.07961", "abs": "https://arxiv.org/abs/2504.07961", "authors": ["Zeren Jiang", "Chuanxia Zheng", "Iro Laina", "Diane Larlus", "Andrea Vedaldi"], "title": "Geo4D: Leveraging Video Generators for Geometric 4D Scene Reconstruction", "categories": ["cs.CV", "I.4.5"], "comment": "16 pages, 5 figures, Project page: https://geo4d.github.io/", "summary": "We introduce Geo4D, a method to repurpose video diffusion models for\nmonocular 3D reconstruction of dynamic scenes. By leveraging the strong dynamic\nprior captured by such video models, Geo4D can be trained using only synthetic\ndata while generalizing well to real data in a zero-shot manner. Geo4D predicts\nseveral complementary geometric modalities, namely point, depth, and ray maps.\nIt uses a new multi-modal alignment algorithm to align and fuse these\nmodalities, as well as multiple sliding windows, at inference time, thus\nobtaining robust and accurate 4D reconstruction of long videos. Extensive\nexperiments across multiple benchmarks show that Geo4D significantly surpasses\nstate-of-the-art video depth estimation methods, including recent methods such\nas MonST3R, which are also designed to handle dynamic scenes."}
{"id": "2504.07831", "pdf": "https://arxiv.org/pdf/2504.07831", "abs": "https://arxiv.org/abs/2504.07831", "authors": ["Simon Lermen", "Mateusz Dziemian", "Natalia P√©rez-Campanero Antol√≠n"], "title": "Deceptive Automated Interpretability: Language Models Coordinating to Fool Oversight Systems", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "We demonstrate how AI agents can coordinate to deceive oversight systems\nusing automated interpretability of neural networks. Using sparse autoencoders\n(SAEs) as our experimental framework, we show that language models (Llama,\nDeepSeek R1, and Claude 3.7 Sonnet) can generate deceptive explanations that\nevade detection. Our agents employ steganographic methods to hide information\nin seemingly innocent explanations, successfully fooling oversight models while\nachieving explanation quality comparable to reference labels. We further find\nthat models can scheme to develop deceptive strategies when they believe the\ndetection of harmful features might lead to negative consequences for\nthemselves. All tested LLM agents were capable of deceiving the overseer while\nachieving high interpretability scores comparable to those of reference labels.\nWe conclude by proposing mitigation strategies, emphasizing the critical need\nfor robust understanding and defenses against deception."}
{"id": "2504.07962", "pdf": "https://arxiv.org/pdf/2504.07962", "abs": "https://arxiv.org/abs/2504.07962", "authors": ["Lang Lin", "Xueyang Yu", "Ziqi Pang", "Yu-Xiong Wang"], "title": "GLUS: Global-Local Reasoning Unified into A Single Large Language Model for Video Segmentation", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "This paper proposes a novel framework utilizing multi-modal large language\nmodels (MLLMs) for referring video object segmentation (RefVOS). Previous\nMLLM-based methods commonly struggle with the dilemma between \"Ref\" and \"VOS\":\nthey either specialize in understanding a few key frames (global reasoning) or\ntracking objects on continuous frames (local reasoning), and rely on external\nVOS or frame selectors to mitigate the other end of the challenge. However, our\nframework GLUS shows that global and local consistency can be unified into a\nsingle video segmentation MLLM: a set of sparse \"context frames\" provides\nglobal information, while a stream of continuous \"query frames\" conducts local\nobject tracking. This is further supported by jointly training the MLLM with a\npre-trained VOS memory bank to simultaneously digest short-range and long-range\ntemporal information. To improve the information efficiency within the limited\ncontext window of MLLMs, we introduce object contrastive learning to\ndistinguish hard false-positive objects and a self-refined framework to\nidentify crucial frames and perform propagation. By collectively integrating\nthese insights, our GLUS delivers a simple yet effective baseline, achieving\nnew state-of-the-art for MLLMs on the MeViS and Ref-Youtube-VOS benchmark. Our\nproject page is at https://glus-video.github.io/."}
{"id": "2504.07840", "pdf": "https://arxiv.org/pdf/2504.07840", "abs": "https://arxiv.org/abs/2504.07840", "authors": ["Cansu Koyuturk", "Emily Theophilou", "Sabrina Patania", "Gregor Donabauer", "Andrea Martinenghi", "Chiara Antico", "Alessia Telari", "Alessia Testa", "Sathya Bursic", "Franca Garzotto", "Davinia Hernandez-Leo", "Udo Kruschwitz", "Davide Taibi", "Simona Amenta", "Martin Ruskov", "Dimitri Ognibene"], "title": "Understanding Learner-LLM Chatbot Interactions and the Impact of Prompting Guidelines", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "Accepted for AIED 2025, the 26th International Conference on\n  Artificial Intelligence in Education, July 22 - 26, 2025, Palermo, Italy", "summary": "Large Language Models (LLMs) have transformed human-computer interaction by\nenabling natural language-based communication with AI-powered chatbots. These\nmodels are designed to be intuitive and user-friendly, allowing users to\narticulate requests with minimal effort. However, despite their accessibility,\nstudies reveal that users often struggle with effective prompting, resulting in\ninefficient responses. Existing research has highlighted both the limitations\nof LLMs in interpreting vague or poorly structured prompts and the difficulties\nusers face in crafting precise queries. This study investigates learner-AI\ninteractions through an educational experiment in which participants receive\nstructured guidance on effective prompting. We introduce and compare three\ntypes of prompting guidelines: a task-specific framework developed through a\nstructured methodology and two baseline approaches. To assess user behavior and\nprompting efficacy, we analyze a dataset of 642 interactions from 107 users.\nUsing Von NeuMidas, an extended pragmatic annotation schema for LLM interaction\nanalysis, we categorize common prompting errors and identify recurring\nbehavioral patterns. We then evaluate the impact of different guidelines by\nexamining changes in user behavior, adherence to prompting strategies, and the\noverall quality of AI-generated responses. Our findings provide a deeper\nunderstanding of how users engage with LLMs and the role of structured\nprompting guidance in enhancing AI-assisted communication. By comparing\ndifferent instructional frameworks, we offer insights into more effective\napproaches for improving user competency in AI interactions, with implications\nfor AI literacy, chatbot usability, and the design of more responsive AI\nsystems."}
{"id": "2504.07963", "pdf": "https://arxiv.org/pdf/2504.07963", "abs": "https://arxiv.org/abs/2504.07963", "authors": ["Shoufa Chen", "Chongjian Ge", "Shilong Zhang", "Peize Sun", "Ping Luo"], "title": "PixelFlow: Pixel-Space Generative Models with Flow", "categories": ["cs.CV"], "comment": "Technical report. Code: https://github.com/ShoufaChen/PixelFlow", "summary": "We present PixelFlow, a family of image generation models that operate\ndirectly in the raw pixel space, in contrast to the predominant latent-space\nmodels. This approach simplifies the image generation process by eliminating\nthe need for a pre-trained Variational Autoencoder (VAE) and enabling the whole\nmodel end-to-end trainable. Through efficient cascade flow modeling, PixelFlow\nachieves affordable computation cost in pixel space. It achieves an FID of 1.98\non 256$\\times$256 ImageNet class-conditional image generation benchmark. The\nqualitative text-to-image results demonstrate that PixelFlow excels in image\nquality, artistry, and semantic control. We hope this new paradigm will inspire\nand open up new opportunities for next-generation visual generation models.\nCode and models are available at https://github.com/ShoufaChen/PixelFlow."}
{"id": "2504.07872", "pdf": "https://arxiv.org/pdf/2504.07872", "abs": "https://arxiv.org/abs/2504.07872", "authors": ["Fei-Hsuan Yu", "Yun-Cheng Chou", "Teng-Ruei Chen"], "title": "Dual Engines of Thoughts: A Depth-Breadth Integration Framework for Open-Ended Analysis", "categories": ["cs.AI", "cs.CE", "cs.CL", "cs.MA"], "comment": null, "summary": "We propose the Dual Engines of Thoughts (DEoT), an analytical framework for\ncomprehensive open-ended reasoning. While traditional reasoning frameworks\nprimarily focus on finding \"the best answer\" or \"the correct answer\" for\nsingle-answer problems, DEoT is specifically designed for \"open-ended\nquestions,\" enabling both broader and deeper analytical exploration. The\nframework centers on three key components: a Base Prompter for refining user\nqueries, a Solver Agent that orchestrates task decomposition, execution, and\nvalidation, and a Dual-Engine System consisting of a Breadth Engine (to explore\ndiverse impact factors) and a Depth Engine (to perform deep investigations).\nThis integrated design allows DEoT to balance wide-ranging coverage with\nin-depth analysis, and it is highly customizable, enabling users to adjust\nanalytical parameters and tool configurations based on specific requirements.\nExperimental results show that DEoT excels in addressing complex, multi-faceted\nquestions, achieving a total win rate of 77-86% compared to existing reasoning\nmodels, thus highlighting its effectiveness in real-world applications."}
{"id": "2504.07134", "pdf": "https://arxiv.org/pdf/2504.07134", "abs": "https://arxiv.org/abs/2504.07134", "authors": ["Qiang Zou", "Lizhen Zhu"], "title": "Boundary representation learning via Transformer", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "The recent rise of generative artificial intelligence (AI), powered by\nTransformer networks, has achieved remarkable success in natural language\nprocessing, computer vision, and graphics. However, the application of\nTransformers in computer-aided design (CAD), particularly for processing\nboundary representation (B-rep) models, remains largely unexplored. To bridge\nthis gap, this paper introduces Boundary Representation Transformer (BRT), a\nnovel method adapting Transformer for B-rep learning. B-rep models pose unique\nchallenges due to their irregular topology and continuous geometric\ndefinitions, which are fundamentally different from the structured and discrete\ndata Transformers are designed for. To address this, BRT proposes a continuous\ngeometric embedding method that encodes B-rep surfaces (trimmed and untrimmed)\ninto B\\'ezier triangles, preserving their shape and continuity without\ndiscretization. Additionally, BRT employs a topology-aware embedding method\nthat organizes these geometric embeddings into a sequence of discrete tokens\nsuitable for Transformers, capturing both geometric and topological\ncharacteristics within B-rep models. This enables the Transformer's attention\nmechanism to effectively learn shape patterns and contextual semantics of\nboundary elements in a B-rep model. Extensive experiments demonstrate that BRT\nachieves state-of-the-art performance in part classification and feature\nrecognition tasks."}
{"id": "2504.07898", "pdf": "https://arxiv.org/pdf/2504.07898", "abs": "https://arxiv.org/abs/2504.07898", "authors": ["Qi Liu", "Jiaxin Mao", "Ji-Rong Wen"], "title": "How do Large Language Models Understand Relevance? A Mechanistic Interpretability Perspective", "categories": ["cs.IR", "cs.CL", "cs.LG"], "comment": null, "summary": "Recent studies have shown that large language models (LLMs) can assess\nrelevance and support information retrieval (IR) tasks such as document ranking\nand relevance judgment generation. However, the internal mechanisms by which\noff-the-shelf LLMs understand and operationalize relevance remain largely\nunexplored. In this paper, we systematically investigate how different LLM\nmodules contribute to relevance judgment through the lens of mechanistic\ninterpretability. Using activation patching techniques, we analyze the roles of\nvarious model components and identify a multi-stage, progressive process in\ngenerating either pointwise or pairwise relevance judgment. Specifically, LLMs\nfirst extract query and document information in the early layers, then process\nrelevance information according to instructions in the middle layers, and\nfinally utilize specific attention heads in the later layers to generate\nrelevance judgments in the required format. Our findings provide insights into\nthe mechanisms underlying relevance assessment in LLMs, offering valuable\nimplications for future research on leveraging LLMs for IR tasks."}
{"id": "2504.07210", "pdf": "https://arxiv.org/pdf/2504.07210", "abs": "https://arxiv.org/abs/2504.07210", "authors": ["Paul Borne--Pons", "Mikolaj Czerkawski", "Rosalie Martin", "Romain Rouffet"], "title": "MESA: Text-Driven Terrain Generation Using Latent Diffusion and Global Copernicus Data", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "Accepted at CVPR 2025 Workshop MORSE", "summary": "Terrain modeling has traditionally relied on procedural techniques, which\noften require extensive domain expertise and handcrafted rules. In this paper,\nwe present MESA - a novel data-centric alternative by training a diffusion\nmodel on global remote sensing data. This approach leverages large-scale\ngeospatial information to generate high-quality terrain samples from text\ndescriptions, showcasing a flexible and scalable solution for terrain\ngeneration. The model's capabilities are demonstrated through extensive\nexperiments, highlighting its ability to generate realistic and diverse terrain\nlandscapes. The dataset produced to support this work, the Major TOM Core-DEM\nextension dataset, is released openly as a comprehensive resource for global\nterrain data. The results suggest that data-driven models, trained on remote\nsensing data, can provide a powerful tool for realistic terrain modeling and\ngeneration."}
{"id": "2504.07952", "pdf": "https://arxiv.org/pdf/2504.07952", "abs": "https://arxiv.org/abs/2504.07952", "authors": ["Mirac Suzgun", "Mert Yuksekgonul", "Federico Bianchi", "Dan Jurafsky", "James Zou"], "title": "Dynamic Cheatsheet: Test-Time Learning with Adaptive Memory", "categories": ["cs.LG", "cs.CL"], "comment": "https://github.com/suzgunmirac/dynamic-cheatsheet", "summary": "Despite their impressive performance on complex tasks, current language\nmodels (LMs) typically operate in a vacuum: Each input query is processed\nseparately, without retaining insights from previous attempts. Here, we present\nDynamic Cheatsheet (DC), a lightweight framework that endows a black-box LM\nwith a persistent, evolving memory. Rather than repeatedly re-discovering or\nre-committing the same solutions and mistakes, DC enables models to store and\nreuse accumulated strategies, code snippets, and general problem-solving\ninsights at inference time. This test-time learning enhances performance\nsubstantially across a range of tasks without needing explicit ground-truth\nlabels or human feedback. Leveraging DC, Claude 3.5 Sonnet's accuracy more than\ndoubled on AIME math exams once it began retaining algebraic insights across\nquestions. Similarly, GPT-4o's success rate on Game of 24 increased from 10% to\n99% after the model discovered and reused a Python-based solution. In tasks\nprone to arithmetic mistakes, such as balancing equations, DC enabled GPT-4o\nand Claude to reach near-perfect accuracy by recalling previously validated\ncode, whereas their baselines stagnated around 50%. Beyond arithmetic\nchallenges, DC yields notable accuracy gains on knowledge-demanding tasks.\nClaude achieved a 9% improvement in GPQA-Diamond and an 8% boost on MMLU-Pro\nproblems. Crucially, DC's memory is self-curated, focusing on concise,\ntransferable snippets rather than entire transcript. Unlike finetuning or\nstatic retrieval methods, DC adapts LMs' problem-solving skills on the fly,\nwithout modifying their underlying parameters. Overall, our findings present DC\nas a promising approach for augmenting LMs with persistent memory, bridging the\ndivide between isolated inference events and the cumulative, experience-driven\nlearning characteristic of human cognition."}
{"id": "2504.07308", "pdf": "https://arxiv.org/pdf/2504.07308", "abs": "https://arxiv.org/abs/2504.07308", "authors": ["Zhe Wang", "Yuhua Ru", "Aladine Chetouani", "Fang Chen", "Fabian Bauer", "Liping Zhang", "Didier Hans", "Rachid Jennane", "Mohamed Jarraya", "Yung Hsin Chen"], "title": "MoEDiff-SR: Mixture of Experts-Guided Diffusion Model for Region-Adaptive MRI Super-Resolution", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Magnetic Resonance Imaging (MRI) at lower field strengths (e.g., 3T) suffers\nfrom limited spatial resolution, making it challenging to capture fine\nanatomical details essential for clinical diagnosis and neuroimaging research.\nTo overcome this limitation, we propose MoEDiff-SR, a Mixture of Experts\n(MoE)-guided diffusion model for region-adaptive MRI Super-Resolution (SR).\nUnlike conventional diffusion-based SR models that apply a uniform denoising\nprocess across the entire image, MoEDiff-SR dynamically selects specialized\ndenoising experts at a fine-grained token level, ensuring region-specific\nadaptation and enhanced SR performance. Specifically, our approach first\nemploys a Transformer-based feature extractor to compute multi-scale patch\nembeddings, capturing both global structural information and local texture\ndetails. The extracted feature embeddings are then fed into an MoE gating\nnetwork, which assigns adaptive weights to multiple diffusion-based denoisers,\neach specializing in different brain MRI characteristics, such as centrum\nsemiovale, sulcal and gyral cortex, and grey-white matter junction. The final\noutput is produced by aggregating the denoised results from these specialized\nexperts according to dynamically assigned gating probabilities. Experimental\nresults demonstrate that MoEDiff-SR outperforms existing state-of-the-art\nmethods in terms of quantitative image quality metrics, perceptual fidelity,\nand computational efficiency. Difference maps from each expert further\nhighlight their distinct specializations, confirming the effective\nregion-specific denoising capability and the interpretability of expert\ncontributions. Additionally, clinical evaluation validates its superior\ndiagnostic capability in identifying subtle pathological features, emphasizing\nits practical relevance in clinical neuroimaging. Our code is available at\nhttps://github.com/ZWang78/MoEDiff-SR."}
{"id": "2504.07954", "pdf": "https://arxiv.org/pdf/2504.07954", "abs": "https://arxiv.org/abs/2504.07954", "authors": ["En Yu", "Kangheng Lin", "Liang Zhao", "Jisheng Yin", "Yana Wei", "Yuang Peng", "Haoran Wei", "Jianjian Sun", "Chunrui Han", "Zheng Ge", "Xiangyu Zhang", "Daxin Jiang", "Jingyu Wang", "Wenbing Tao"], "title": "Perception-R1: Pioneering Perception Policy with Reinforcement Learning", "categories": ["cs.CV", "cs.CL"], "comment": "Github page: https://github.com/linkangheng/PR1", "summary": "Inspired by the success of DeepSeek-R1, we explore the potential of\nrule-based reinforcement learning (RL) in MLLM post-training for perception\npolicy learning. While promising, our initial experiments reveal that\nincorporating a thinking process through RL does not consistently lead to\nperformance gains across all visual perception tasks. This leads us to delve\ninto the essential role of RL in the context of visual perception. In this\nwork, we return to the fundamentals and explore the effects of RL on different\nperception tasks. We observe that the perceptual complexity is a major factor\nin determining the effectiveness of RL. We also observe that reward design\nplays a crucial role in further approching the upper limit of model perception.\nTo leverage these findings, we propose Perception-R1, a scalable RL framework\nusing GRPO during MLLM post-training. With a standard Qwen2.5-VL-3B-Instruct,\nPerception-R1 achieves +4.2% on RefCOCO+, +17.9% on PixMo-Count, +4.2% on\nPageOCR, and notably, 31.9% AP on COCO2017 val for the first time, establishing\na strong baseline for perception policy learning."}
{"id": "2504.07313", "pdf": "https://arxiv.org/pdf/2504.07313", "abs": "https://arxiv.org/abs/2504.07313", "authors": ["Mohammed Lamine Benomar", "Nesma Settouti", "Eric Debreuve", "Xavier Descombes", "Damien Ambrosetti"], "title": "Identifying regions of interest in whole slide images of renal cell carcinoma", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "The histopathological images contain a huge amount of information, which can\nmake diagnosis an extremely timeconsuming and tedious task. In this study, we\ndeveloped a completely automated system to detect regions of interest (ROIs) in\nwhole slide images (WSI) of renal cell carcinoma (RCC), to reduce time analysis\nand assist pathologists in making more accurate decisions. The proposed\napproach is based on an efficient texture descriptor named dominant rotated\nlocal binary pattern (DRLBP) and color transformation to reveal and exploit the\nimmense texture variability at the microscopic high magnifications level.\nThereby, the DRLBPs retain the structural information and utilize the magnitude\nvalues in a local neighborhood for more discriminative power. For the\nclassification of the relevant ROIs, feature extraction of WSIs patches was\nperformed on the color channels separately to form the histograms. Next, we\nused the most frequently occurring patterns as a feature selection step to\ndiscard non-informative features. The performances of different classifiers on\na set of 1800 kidney cancer patches originating from 12 whole slide images were\ncompared and evaluated. Furthermore, the small size of the image dataset allows\nto investigate deep learning approach based on transfer learning for image\npatches classification by using deep features and fine-tuning methods. High\nrecognition accuracy was obtained and the classifiers are efficient, the best\nprecision result was 99.17% achieved with SVM. Moreover, transfer learning\nmodels perform well with comparable performance, and the highest precision\nusing ResNet-50 reached 98.50%. The proposed approach results revealed a very\nefficient image classification and demonstrated efficacy in identifying ROIs.\nThis study presents an automatic system to detect regions of interest relevant\nto the diagnosis of kidney cancer in whole slide histopathology images."}
{"id": "2504.07956", "pdf": "https://arxiv.org/pdf/2504.07956", "abs": "https://arxiv.org/abs/2504.07956", "authors": ["Yukun Qi", "Yiming Zhao", "Yu Zeng", "Xikun Bao", "Wenxuan Huang", "Lin Chen", "Zehui Chen", "Jie Zhao", "Zhongang Qi", "Feng Zhao"], "title": "VCR-Bench: A Comprehensive Evaluation Framework for Video Chain-of-Thought Reasoning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "The advancement of Chain-of-Thought (CoT) reasoning has significantly\nenhanced the capabilities of large language models (LLMs) and large\nvision-language models (LVLMs). However, a rigorous evaluation framework for\nvideo CoT reasoning remains absent. Current video benchmarks fail to adequately\nassess the reasoning process and expose whether failures stem from deficiencies\nin perception or reasoning capabilities. Therefore, we introduce VCR-Bench, a\nnovel benchmark designed to comprehensively evaluate LVLMs' Video\nChain-of-Thought Reasoning capabilities. VCR-Bench comprises 859 videos\nspanning a variety of video content and durations, along with 1,034\nhigh-quality question-answer pairs. Each pair is manually annotated with a\nstepwise CoT rationale, where every step is tagged to indicate its association\nwith the perception or reasoning capabilities. Furthermore, we design seven\ndistinct task dimensions and propose the CoT score to assess the entire CoT\nprocess based on the stepwise tagged CoT rationals. Extensive experiments on\nVCR-Bench highlight substantial limitations in current LVLMs. Even the\ntop-performing model, o1, only achieves a 62.8% CoT score and an 56.7%\naccuracy, while most models score below 40%. Experiments show most models score\nlower on perception than reasoning steps, revealing LVLMs' key bottleneck in\ntemporal-spatial information processing for complex video reasoning. A robust\npositive correlation between the CoT score and accuracy confirms the validity\nof our evaluation framework and underscores the critical role of CoT reasoning\nin solving complex video reasoning tasks. We hope VCR-Bench to serve as a\nstandardized evaluation framework and expose the actual drawbacks in complex\nvideo reasoning task."}
{"id": "2504.07450", "pdf": "https://arxiv.org/pdf/2504.07450", "abs": "https://arxiv.org/abs/2504.07450", "authors": ["Weijie Chen", "James Wang", "Alan McMillan"], "title": "Synthetic CT Generation from Time-of-Flight Non-Attenutaion-Corrected PET for Whole-Body PET Attenuation Correction", "categories": ["eess.IV", "cs.AI", "cs.CV", "68T05, 92C55", "I.2.6; I.2.10"], "comment": "4 pages, 2 figures, ISBI 2025", "summary": "Positron Emission Tomography (PET) imaging requires accurate attenuation\ncorrection (AC) to account for photon loss due to tissue density variations. In\nPET/MR systems, computed tomography (CT), which offers a straightforward\nestimation of AC is not available. This study presents a deep learning approach\nto generate synthetic CT (sCT) images directly from Time-of-Flight (TOF)\nnon-attenuation corrected (NAC) PET images, enhancing AC for PET/MR. We first\nevaluated models pre-trained on large-scale natural image datasets for a\nCT-to-CT reconstruction task, finding that the pre-trained model outperformed\nthose trained solely on medical datasets. The pre-trained model was then\nfine-tuned using an institutional dataset of 35 TOF NAC PET and CT volume\npairs, achieving the lowest mean absolute error (MAE) of 74.49 HU and highest\npeak signal-to-noise ratio (PSNR) of 28.66 dB within the body contour region.\nVisual assessments demonstrated improved reconstruction of both bone and soft\ntissue structures from TOF NAC PET images. This work highlights the\neffectiveness of using pre-trained deep learning models for medical image\ntranslation tasks. Future work will assess the impact of sCT on PET attenuation\ncorrection and explore additional neural network architectures and datasets to\nfurther enhance performance and practical applications in PET imaging."}
{"id": "2504.07965", "pdf": "https://arxiv.org/pdf/2504.07965", "abs": "https://arxiv.org/abs/2504.07965", "authors": ["Lorenz Linhardt", "Tom Neuh√§user", "Lenka Tƒõtkov√°", "Oliver Eberle"], "title": "Cat, Rat, Meow: On the Alignment of Language Model and Human Term-Similarity Judgments", "categories": ["cs.LG", "cs.CL"], "comment": "ICLR 2025 Workshop on Representational Alignment (Re-Align)", "summary": "Small and mid-sized generative language models have gained increasing\nattention. Their size and availability make them amenable to being analyzed at\na behavioral as well as a representational level, allowing investigations of\nhow these levels interact. We evaluate 32 publicly available language models\nfor their representational and behavioral alignment with human similarity\njudgments on a word triplet task. This provides a novel evaluation setting to\nprobe semantic associations in language beyond common pairwise comparisons. We\nfind that (1) even the representations of small language models can achieve\nhuman-level alignment, (2) instruction-tuned model variants can exhibit\nsubstantially increased agreement, (3) the pattern of alignment across layers\nis highly model dependent, and (4) alignment based on models' behavioral\nresponses is highly dependent on model size, matching their representational\nalignment only for the largest evaluated models."}
{"id": "2504.07468", "pdf": "https://arxiv.org/pdf/2504.07468", "abs": "https://arxiv.org/abs/2504.07468", "authors": ["Santanu Roy", "Ashvath Suresh", "Palak Sahu", "Tulika Rudra Gupta"], "title": "Novel Pooling-based VGG-Lite for Pneumonia and Covid-19 Detection from Imbalanced Chest X-Ray Datasets", "categories": ["eess.IV", "cs.CV"], "comment": "12 pages", "summary": "This paper proposes a novel pooling-based VGG-Lite model in order to mitigate\nclass imbalance issues in Chest X-Ray (CXR) datasets. Automatic Pneumonia\ndetection from CXR images by deep learning model has emerged as a prominent and\ndynamic area of research, since the inception of the new Covid-19 variant in\n2020. However, the standard Convolutional Neural Network (CNN) models encounter\nchallenges associated with class imbalance, a prevalent issue found in many\nmedical datasets. The innovations introduced in the proposed model architecture\ninclude: (I) A very lightweight CNN model, `VGG-Lite', is proposed as a base\nmodel, inspired by VGG-16 and MobileNet-V2 architecture. (II) On top of this\nbase model, we leverage an ``Edge Enhanced Module (EEM)\" through a parallel\nbranch, consisting of a ``negative image layer\", and a novel custom pooling\nlayer ``2Max-Min Pooling\". This 2Max-Min Pooling layer is entirely novel in\nthis investigation, providing more attention to edge components within\npneumonia CXR images. Thus, it works as an efficient spatial attention module\n(SAM). We have implemented the proposed framework on two separate CXR datasets.\nThe first dataset is obtained from a readily available source on the internet,\nand the second dataset is a more challenging CXR dataset, assembled by our\nresearch team from three different sources. Experimental results reveal that\nour proposed framework has outperformed pre-trained CNN models, and three\nrecent trend existing models ``Vision Transformer\", ``Pooling-based Vision\nTransformer (PiT)'' and ``PneuNet\", by substantial margins on both datasets.\nThe proposed framework VGG-Lite with EEM, has achieved a macro average of 95%\naccuracy, 97.1% precision, 96.1% recall, and 96.6% F1 score on the ``Pneumonia\nImbalance CXR dataset\", without employing any pre-processing technique."}
{"id": "2504.07560", "pdf": "https://arxiv.org/pdf/2504.07560", "abs": "https://arxiv.org/abs/2504.07560", "authors": ["Moritz Rempe", "Fabian H√∂rst", "Helmut Becker", "Marco Schlimbach", "Lukas Rotkopf", "Kevin Kr√∂ninger", "Jens Kleesiek"], "title": "PhaseGen: A Diffusion-Based Approach for Complex-Valued MRI Data Generation", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Magnetic resonance imaging (MRI) raw data, or k-Space data, is\ncomplex-valued, containing both magnitude and phase information. However,\nclinical and existing Artificial Intelligence (AI)-based methods focus only on\nmagnitude images, discarding the phase data despite its potential for\ndownstream tasks, such as tumor segmentation and classification. In this work,\nwe introduce $\\textit{PhaseGen}$, a novel complex-valued diffusion model for\ngenerating synthetic MRI raw data conditioned on magnitude images, commonly\nused in clinical practice. This enables the creation of artificial\ncomplex-valued raw data, allowing pretraining for models that require k-Space\ninformation. We evaluate PhaseGen on two tasks: skull-stripping directly in\nk-Space and MRI reconstruction using the publicly available FastMRI dataset.\nOur results show that training with synthetic phase data significantly improves\ngeneralization for skull-stripping on real-world data, with an increased\nsegmentation accuracy from $41.1\\%$ to $80.1\\%$, and enhances MRI\nreconstruction when combined with limited real-world data. This work presents a\nstep forward in utilizing generative AI to bridge the gap between\nmagnitude-based datasets and the complex-valued nature of MRI raw data. This\napproach allows researchers to leverage the vast amount of avaliable image\ndomain data in combination with the information-rich k-Space data for more\naccurate and efficient diagnostic tasks. We make our code publicly\n$\\href{https://github.com/TIO-IKIM/PhaseGen}{\\text{available here}}$."}
{"id": "2504.07594", "pdf": "https://arxiv.org/pdf/2504.07594", "abs": "https://arxiv.org/abs/2504.07594", "authors": ["Xiaohao Liu", "Teng Tu", "Yunshan Ma", "Tat-Seng Chua"], "title": "Extending Visual Dynamics for Video-to-Music Generation", "categories": ["cs.MM", "cs.CV"], "comment": "Under review", "summary": "Music profoundly enhances video production by improving quality, engagement,\nand emotional resonance, sparking growing interest in video-to-music\ngeneration. Despite recent advances, existing approaches remain limited in\nspecific scenarios or undervalue the visual dynamics. To address these\nlimitations, we focus on tackling the complexity of dynamics and resolving\ntemporal misalignment between video and music representations. To this end, we\npropose DyViM, a novel framework to enhance dynamics modeling for\nvideo-to-music generation. Specifically, we extract frame-wise dynamics\nfeatures via a simplified motion encoder inherited from optical flow methods,\nfollowed by a self-attention module for aggregation within frames. These\ndynamic features are then incorporated to extend existing music tokens for\ntemporal alignment. Additionally, high-level semantics are conveyed through a\ncross-attention mechanism, and an annealing tuning strategy benefits to\nfine-tune well-trained music decoders efficiently, therefore facilitating\nseamless adaptation. Extensive experiments demonstrate DyViM's superiority over\nstate-of-the-art (SOTA) methods."}
{"id": "2504.07606", "pdf": "https://arxiv.org/pdf/2504.07606", "abs": "https://arxiv.org/abs/2504.07606", "authors": ["Andr√©s Bell-Navas", "Mar√≠a Villalba-Orero", "Enrique Lara-Pezzi", "Jes√∫s Garicano-Mena", "Soledad Le Clainche"], "title": "Heart Failure Prediction using Modal Decomposition and Masked Autoencoders for Scarce Echocardiography Databases", "categories": ["eess.IV", "cs.CV", "68T07, 68T10, 68T45, 62H35", "I.2; I.2.10; I.4.5; I.5.1; I.5.4; J.3"], "comment": "37 pages, 7 figures. arXiv admin note: substantial text overlap with\n  arXiv:2404.19579", "summary": "Heart diseases constitute the main cause of international human defunction.\nAccording to the World Health Organization (WHO), approximately 18 million\ndeaths happen each year due to precisely heart diseases. In particular, heart\nfailures (HF) press the healthcare industry to develop systems for their early,\nrapid and effective prediction. In this work, an automatic system which\nanalyses in real-time echocardiography video sequences is proposed for the\nchallenging and more specific task of prediction of heart failure times. This\nsystem is based on a novel deep learning framework, and works in two stages.\nThe first one transforms the data included in a database of echocardiography\nvideo sequences into a machine learning-compatible collection of annotated\nimages which can be used in the training phase of any kind of machine\nlearning-based framework, including a deep learning one. This initial stage\nincludes the use of the Higher Order Dynamic Mode Decomposition (HODMD)\nalgorithm for both data augmentation and feature extraction. The second stage\nis focused on building and training a Vision Transformer (ViT). Self-supervised\nlearning (SSL) methods, which have been so far barely explored in the\nliterature about heart failure prediction, are applied to effectively train the\nViT from scratch, even with scarce databases of echocardiograms. The designed\nneural network analyses images from echocardiography sequences to estimate the\ntime in which a heart failure will happen. The results obtained show the\nefficacy of the HODMD algorithm and the superiority of the proposed system with\nrespect to several established ViT and Convolutional Neural Network (CNN)\narchitectures."}
{"id": "2504.07643", "pdf": "https://arxiv.org/pdf/2504.07643", "abs": "https://arxiv.org/abs/2504.07643", "authors": ["Florian Schneider", "Narges Baba Ahmadi", "Niloufar Baba Ahmadi", "Iris Vogel", "Martin Semmann", "Chris Biemann"], "title": "CollEX -- A Multimodal Agentic RAG System Enabling Interactive Exploration of Scientific Collections", "categories": ["cs.IR", "cs.CL", "cs.CV"], "comment": null, "summary": "In this paper, we introduce CollEx, an innovative multimodal agentic\nRetrieval-Augmented Generation (RAG) system designed to enhance interactive\nexploration of extensive scientific collections. Given the overwhelming volume\nand inherent complexity of scientific collections, conventional search systems\noften lack necessary intuitiveness and interactivity, presenting substantial\nbarriers for learners, educators, and researchers. CollEx addresses these\nlimitations by employing state-of-the-art Large Vision-Language Models (LVLMs)\nas multimodal agents accessible through an intuitive chat interface. By\nabstracting complex interactions via specialized agents equipped with advanced\ntools, CollEx facilitates curiosity-driven exploration, significantly\nsimplifying access to diverse scientific collections and records therein. Our\nsystem integrates textual and visual modalities, supporting educational\nscenarios that are helpful for teachers, pupils, students, and researchers by\nfostering independent exploration as well as scientific excitement and\ncuriosity. Furthermore, CollEx serves the research community by discovering\ninterdisciplinary connections and complementing visual data. We illustrate the\neffectiveness of our system through a proof-of-concept application containing\nover 64,000 unique records across 32 collections from a local scientific\ncollection from a public university."}
{"id": "2504.07677", "pdf": "https://arxiv.org/pdf/2504.07677", "abs": "https://arxiv.org/abs/2504.07677", "authors": ["Hye-Min Won", "Jieun Lee", "Jiyong Oh"], "title": "Localization Meets Uncertainty: Uncertainty-Aware Multi-Modal Localization", "categories": ["cs.RO", "cs.CV"], "comment": "14 pages, 6 figures", "summary": "Reliable localization is critical for robot navigation in complex indoor\nenvironments. In this paper, we propose an uncertainty-aware localization\nmethod that enhances the reliability of localization outputs without modifying\nthe prediction model itself. This study introduces a percentile-based rejection\nstrategy that filters out unreliable 3-DoF pose predictions based on aleatoric\nand epistemic uncertainties the network estimates. We apply this approach to a\nmulti-modal end-to-end localization that fuses RGB images and 2D LiDAR data,\nand we evaluate it across three real-world datasets collected using a\ncommercialized serving robot. Experimental results show that applying stricter\nuncertainty thresholds consistently improves pose accuracy. Specifically, the\nmean position error is reduced by 41.0%, 56.7%, and 69.4%, and the mean\norientation error by 55.6%, 65.7%, and 73.3%, when applying 90%, 80%, and 70%\nthresholds, respectively. Furthermore, the rejection strategy effectively\nremoves extreme outliers, resulting in better alignment with ground truth\ntrajectories. To the best of our knowledge, this is the first study to\nquantitatively demonstrate the benefits of percentile-based uncertainty\nrejection in multi-modal end-to-end localization tasks. Our approach provides a\npractical means to enhance the reliability and accuracy of localization systems\nin real-world deployments."}
{"id": "2504.07691", "pdf": "https://arxiv.org/pdf/2504.07691", "abs": "https://arxiv.org/abs/2504.07691", "authors": ["Yanglin Huang", "Kai Hu", "Yuan Zhang", "Zhineng Chen", "Xieping Gao"], "title": "Distilling Knowledge from Heterogeneous Architectures for Semantic Segmentation", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted to AAAI 2025", "summary": "Current knowledge distillation (KD) methods for semantic segmentation focus\non guiding the student to imitate the teacher's knowledge within homogeneous\narchitectures. However, these methods overlook the diverse knowledge contained\nin architectures with different inductive biases, which is crucial for enabling\nthe student to acquire a more precise and comprehensive understanding of the\ndata during distillation. To this end, we propose for the first time a generic\nknowledge distillation method for semantic segmentation from a heterogeneous\nperspective, named HeteroAKD. Due to the substantial disparities between\nheterogeneous architectures, such as CNN and Transformer, directly transferring\ncross-architecture knowledge presents significant challenges. To eliminate the\ninfluence of architecture-specific information, the intermediate features of\nboth the teacher and student are skillfully projected into an aligned logits\nspace. Furthermore, to utilize diverse knowledge from heterogeneous\narchitectures and deliver customized knowledge required by the student, a\nteacher-student knowledge mixing mechanism (KMM) and a teacher-student\nknowledge evaluation mechanism (KEM) are introduced. These mechanisms are\nperformed by assessing the reliability and its discrepancy between\nheterogeneous teacher-student knowledge. Extensive experiments conducted on\nthree main-stream benchmarks using various teacher-student pairs demonstrate\nthat our HeteroAKD outperforms state-of-the-art KD methods in facilitating\ndistillation between heterogeneous architectures."}
{"id": "2504.07753", "pdf": "https://arxiv.org/pdf/2504.07753", "abs": "https://arxiv.org/abs/2504.07753", "authors": ["Zini Chen", "Yao Xiao", "Junyan Zhang", "Shaoyu Wang", "Liu Shi", "Qiegen Liu"], "title": "Virtual-mask Informed Prior for Sparse-view Dual-Energy CT Reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Sparse-view sampling in dual-energy computed tomography (DECT) significantly\nreduces radiation dose and increases imaging speed, yet is highly prone to\nartifacts. Although diffusion models have demonstrated potential in effectively\nhandling incomplete data, most existing methods in this field focus on the\nimage do-main and lack global constraints, which consequently leads to\ninsufficient reconstruction quality. In this study, we propose a dual-domain\nvirtual-mask in-formed diffusion model for sparse-view reconstruction by\nleveraging the high inter-channel correlation in DECT. Specifically, the study\ndesigns a virtual mask and applies it to the high-energy and low-energy data to\nperform perturbation operations, thus constructing high-dimensional tensors\nthat serve as the prior information of the diffusion model. In addition, a\ndual-domain collaboration strategy is adopted to integrate the information of\nthe randomly selected high-frequency components in the wavelet domain with the\ninformation in the projection domain, for the purpose of optimizing the global\nstruc-tures and local details. Experimental results indicated that the present\nmethod exhibits excellent performance across multiple datasets."}
{"id": "2504.07760", "pdf": "https://arxiv.org/pdf/2504.07760", "abs": "https://arxiv.org/abs/2504.07760", "authors": ["Zhenhuan Zhou", "Yuchen Zhang", "Ruihong Xu", "Xuansen Zhao", "Tao Li"], "title": "PRAD: Periapical Radiograph Analysis Dataset and Benchmark Model Development", "categories": ["eess.IV", "cs.CV"], "comment": "11 pages & Under Review", "summary": "Deep learning (DL), a pivotal technology in artificial intelligence, has\nrecently gained substantial traction in the domain of dental auxiliary\ndiagnosis. However, its application has predominantly been confined to imaging\nmodalities such as panoramic radiographs and Cone Beam Computed Tomography,\nwith limited focus on auxiliary analysis specifically targeting Periapical\nRadiographs (PR). PR are the most extensively utilized imaging modality in\nendodontics and periodontics due to their capability to capture detailed local\nlesions at a low cost. Nevertheless, challenges such as resolution limitations\nand artifacts complicate the annotation and recognition of PR, leading to a\nscarcity of publicly available, large-scale, high-quality PR analysis datasets.\nThis scarcity has somewhat impeded the advancement of DL applications in PR\nanalysis. In this paper, we present PRAD-10K, a dataset for PR analysis.\nPRAD-10K comprises 10,000 clinical periapical radiograph images, with\npixel-level annotations provided by professional dentists for nine distinct\nanatomical structures, lesions, and artificial restorations or medical devices,\nWe also include classification labels for images with typical conditions or\nlesions. Furthermore, we introduce a DL network named PRNet to establish\nbenchmarks for PR segmentation tasks. Experimental results demonstrate that\nPRNet surpasses previous state-of-the-art medical image segmentation models on\nthe PRAD-10K dataset. The codes and dataset will be made publicly available."}
{"id": "2504.07775", "pdf": "https://arxiv.org/pdf/2504.07775", "abs": "https://arxiv.org/abs/2504.07775", "authors": ["Lorenzo Lasagni", "Antonio Ciccarone", "Renzo Guerrini", "Matteo Lenge", "Ludovico D'incerti"], "title": "Focal Cortical Dysplasia Type II Detection Using Cross Modality Transfer Learning and Grad-CAM in 3D-CNNs for MRI Analysis", "categories": ["eess.IV", "cs.CV", "physics.med-ph"], "comment": null, "summary": "Focal cortical dysplasia (FCD) type II is a major cause of drug-resistant\nepilepsy, often curable only by surgery. Despite its clinical importance, the\ndiagnosis of FCD is very difficult in MRI because of subtle abnormalities,\nleading to misdiagnosis. This study investigates the use of 3D convolutional\nneural networks (3D-CNNs) for FCD detection, using a dataset of 170 subjects\n(85 FCD patients and 85 controls) composed of T1-weighted and FLAIR MRI scans.\nIn particular, it investigates the benefits obtained from cross-modality\ntransfer learning and explainable artificial intelligence (XAI) techniques, in\nparticular Gradient-weighted Class Activation Mapping (Grad-CAM). ResNet\narchitectures (ResNet-18, -34, and -50) were implemented, employing transfer\nlearning strategies that used pre-trained weights from segmentation tasks.\nResults indicate that transfer learning significantly enhances classification\naccuracy (up to 80.3%) and interpretability, as measured by a novel Heat-Score\nmetric, which evaluates the model's focus on clinically relevant regions.\nImprovements in the Heat-Score metric underscore the model's seizure zone\nlocalization capabilities, bringing AI predictions and clinical insights closer\ntogether. These results highlight the importance of transfer learning,\nincluding cross-modality, and XAI in advancing AI-based medical diagnostics,\nespecially for difficult-to-diagnose pathologies such as FCD."}
{"id": "2504.07777", "pdf": "https://arxiv.org/pdf/2504.07777", "abs": "https://arxiv.org/abs/2504.07777", "authors": ["Peng Jia", "Ge Li", "Bafeng Cheng", "Yushan Li", "Rongyu Sun"], "title": "Adaptive Detection of Fast Moving Celestial Objects Using a Mixture of Experts and Physical-Inspired Neural Network", "categories": ["astro-ph.IM", "astro-ph.EP", "cs.CV", "cs.LG", "physics.optics"], "comment": "Accepted by the AJ", "summary": "Fast moving celestial objects are characterized by velocities across the\ncelestial sphere that significantly differ from the motions of background\nstars. In observational images, these objects exhibit distinct shapes,\ncontrasting with the typical appearances of stars. Depending on the\nobservational method employed, these celestial entities may be designated as\nnear-Earth objects or asteroids. Historically, fast moving celestial objects\nhave been observed using ground-based telescopes, where the relative stability\nof stars and Earth facilitated effective image differencing techniques\nalongside traditional fast moving celestial object detection and classification\nalgorithms. However, the growing prevalence of space-based telescopes, along\nwith their diverse observational modes, produces images with different\nproperties, rendering conventional methods less effective. This paper presents\na novel algorithm for detecting fast moving celestial objects within star\nfields. Our approach enhances state-of-the-art fast moving celestial object\ndetection neural networks by transforming them into physical-inspired neural\nnetworks. These neural networks leverage the point spread function of the\ntelescope and the specific observational mode as prior information; they can\ndirectly identify moving fast moving celestial objects within star fields\nwithout requiring additional training, thereby addressing the limitations of\ntraditional techniques. Additionally, all neural networks are integrated using\nthe mixture of experts technique, forming a comprehensive fast moving celestial\nobject detection algorithm. We have evaluated our algorithm using simulated\nobservational data that mimics various observations carried out by space based\ntelescope scenarios and real observation images. Results demonstrate that our\nmethod effectively detects fast moving celestial objects across different\nobservational modes."}
{"id": "2504.07793", "pdf": "https://arxiv.org/pdf/2504.07793", "abs": "https://arxiv.org/abs/2504.07793", "authors": ["Yifan Ding", "Arturas Aleksandrauskas", "Amirhossein Ahmadian", "Jonas Unger", "Fredrik Lindsten", "Gabriel Eilertsen"], "title": "Revisiting Likelihood-Based Out-of-Distribution Detection by Modeling Representations", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Out-of-distribution (OOD) detection is critical for ensuring the reliability\nof deep learning systems, particularly in safety-critical applications.\nLikelihood-based deep generative models have historically faced criticism for\ntheir unsatisfactory performance in OOD detection, often assigning higher\nlikelihood to OOD data than in-distribution samples when applied to image data.\nIn this work, we demonstrate that likelihood is not inherently flawed. Rather,\nseveral properties in the images space prohibit likelihood as a valid detection\nscore. Given a sufficiently good likelihood estimator, specifically using the\nprobability flow formulation of a diffusion model, we show that\nlikelihood-based methods can still perform on par with state-of-the-art methods\nwhen applied in the representation space of pre-trained encoders. The code of\nour work can be found at\n$\\href{https://github.com/limchaos/Likelihood-OOD.git}{\\texttt{https://github.com/limchaos/Likelihood-OOD.git}}$."}
{"id": "2504.07827", "pdf": "https://arxiv.org/pdf/2504.07827", "abs": "https://arxiv.org/abs/2504.07827", "authors": ["Yi Huang", "Ke Zhang", "Wei Liu", "Yuanyuan Wang", "Vishal M. Patel", "Le Lu", "Xu Han", "Dakai Jin", "Ke Yan"], "title": "HarmonySeg: Tubular Structure Segmentation with Deep-Shallow Feature Fusion and Growth-Suppression Balanced Loss", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Accurate segmentation of tubular structures in medical images, such as\nvessels and airway trees, is crucial for computer-aided diagnosis,\nradiotherapy, and surgical planning. However, significant challenges exist in\nalgorithm design when faced with diverse sizes, complex topologies, and (often)\nincomplete data annotation of these structures. We address these difficulties\nby proposing a new tubular structure segmentation framework named HarmonySeg.\nFirst, we design a deep-to-shallow decoder network featuring flexible\nconvolution blocks with varying receptive fields, which enables the model to\neffectively adapt to tubular structures of different scales. Second, to\nhighlight potential anatomical regions and improve the recall of small tubular\nstructures, we incorporate vesselness maps as auxiliary information. These maps\nare aligned with image features through a shallow-and-deep fusion module, which\nsimultaneously eliminates unreasonable candidates to maintain high precision.\nFinally, we introduce a topology-preserving loss function that leverages\ncontextual and shape priors to balance the growth and suppression of tubular\nstructures, which also allows the model to handle low-quality and incomplete\nannotations. Extensive quantitative experiments are conducted on four public\ndatasets. The results show that our model can accurately segment 2D and 3D\ntubular structures and outperform existing state-of-the-art methods. External\nvalidation on a private dataset also demonstrates good generalizability."}
{"id": "2504.07904", "pdf": "https://arxiv.org/pdf/2504.07904", "abs": "https://arxiv.org/abs/2504.07904", "authors": ["Blake VanBerlo", "Alexander Wong", "Jesse Hoey", "Robert Arntfield"], "title": "The Efficacy of Semantics-Preserving Transformations in Self-Supervised Learning for Medical Ultrasound", "categories": ["eess.IV", "cs.CV", "cs.LG", "I.2.10; I.4.9; J.3"], "comment": "17 pages, 12 figures, 18 tables, Submitted to Medical Image Analysis", "summary": "Data augmentation is a central component of joint embedding self-supervised\nlearning (SSL). Approaches that work for natural images may not always be\neffective in medical imaging tasks. This study systematically investigated the\nimpact of data augmentation and preprocessing strategies in SSL for lung\nultrasound. Three data augmentation pipelines were assessed: (1) a baseline\npipeline commonly used across imaging domains, (2) a novel semantic-preserving\npipeline designed for ultrasound, and (3) a distilled set of the most effective\ntransformations from both pipelines. Pretrained models were evaluated on\nmultiple classification tasks: B-line detection, pleural effusion detection,\nand COVID-19 classification. Experiments revealed that semantics-preserving\ndata augmentation resulted in the greatest performance for COVID-19\nclassification - a diagnostic task requiring global image context.\nCropping-based methods yielded the greatest performance on the B-line and\npleural effusion object classification tasks, which require strong local\npattern recognition. Lastly, semantics-preserving ultrasound image\npreprocessing resulted in increased downstream performance for multiple tasks.\nGuidance regarding data augmentation and preprocessing strategies was\nsynthesized for practitioners working with SSL in ultrasound."}
{"id": "2504.07927", "pdf": "https://arxiv.org/pdf/2504.07927", "abs": "https://arxiv.org/abs/2504.07927", "authors": ["Yongyi Shi", "Ge Wang"], "title": "Zero-Shot Low-dose CT Denoising via Sinogram Flicking", "categories": ["eess.IV", "cs.CV"], "comment": "4 pages, 4 figures", "summary": "Many low-dose CT imaging methods rely on supervised learning, which requires\na large number of paired noisy and clean images. However, obtaining paired\nimages in clinical practice is challenging. To address this issue, zero-shot\nself-supervised methods train denoising networks using only the information\nwithin a single image, such as ZS-N2N. However, these methods often employ\ndownsampling operations that degrade image resolution. Additionally, the\ntraining dataset is inherently constrained to the image itself. In this paper,\nwe propose a zero-shot low-dose CT imaging method based on sinogram flicking,\nwhich operates within a single image but generates many copies via random\nconjugate ray matching. Specifically, two conjugate X-ray pencil beams measure\nthe same path; their expected values should be identical, while their noise\nlevels vary during measurements. By randomly swapping portions of the conjugate\nX-rays in the sinogram domain, we generate a large set of sinograms with\nconsistent content but varying noise patterns. When displayed dynamically,\nthese sinograms exhibit a flickering effect due to their identical structural\ncontent but differing noise patterns-hence the term sinogram flicking. We train\nthe network on pairs of sinograms with the same content but different noise\ndistributions using a lightweight model adapted from ZS-NSN. This process is\nrepeated to obtain the final results. A simulation study demonstrates that our\nmethod outperforms state-of-the-art approaches such as ZS-N2N."}
