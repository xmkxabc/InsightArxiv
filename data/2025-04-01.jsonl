{"id": "2503.22698", "pdf": "https://arxiv.org/pdf/2503.22698", "abs": "https://arxiv.org/abs/2503.22698", "authors": ["Basab Jha", "Firoj Paudel"], "title": "Fragile Mastery: Are Domain-Specific Trade-Offs Undermining On-Device Language Models?", "categories": ["cs.CL"], "comment": "14 Pages, 5 figures", "summary": "The application of on-device language models (ODLMs) on resource-constrained\nedge devices is a multi-dimensional problem that strikes a fine balance between\ncomputational effectiveness, memory, power usage, and linguistic capacity\nacross heterogeneous tasks. This holistic study conducts a thorough\ninvestigation of the trade-offs between domain-specific optimization and\ncross-domain robustness, culminating in the proposal of the Generalized Edge\nModel (GEM), a new architecture that aims to balance specialization and\ngeneralization in a harmonious manner. With a rigorous experimental approach\ntesting 47 well-chosen benchmarks in eight domains--healthcare, law, finance,\nSTEM, commonsense, conversational AI, multilingual, and domain-adaptive\ntasks--we show that conventional optimization techniques decrease target task\nperplexity by 18-25% but result in a precipitous decline in general-task\nperformance with F1 scores decreasing by 12-29%, as reported by Liu et al. GEM\nemploys a Sparse Cross-Attention Router (SCAR) to dynamically allocate\ncomputation to a variable number of computing resources with a cross-domain F1\naccuracy of 0.89 on less than 100ms latency across Raspberry Pi 4, Pixel 6,\niPhone 13, and bespoke custom neural processing units (NPUs). Compared to GPT-4\nLite, GEM enhances the general-task level by 7% with respect and parity in\ndomain-specific performance. We propose three new measurement tools--Domain\nSpecialization Index (DSI), Generalization Gap (GG), and Cross-Domain Transfer\nRatio (CDTR)--which show strong correlation between model compression intensity\nand brittleness."}
{"id": "2503.22714", "pdf": "https://arxiv.org/pdf/2503.22714", "abs": "https://arxiv.org/abs/2503.22714", "authors": ["Sergio Torres Aguilar"], "title": "TRIDIS: A Comprehensive Medieval and Early Modern Corpus for HTR and NER", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "6 pages, 3 figures, 2 tables", "summary": "This paper introduces TRIDIS (Tria Digita Scribunt), an open-source corpus of\nmedieval and early modern manuscripts. TRIDIS aggregates multiple legacy\ncollections (all published under open licenses) and incorporates large metadata\ndescriptions. While prior publications referenced some portions of this corpus,\nhere we provide a unified overview with a stronger focus on its constitution.\nWe describe (i) the narrative, chronological, and editorial background of each\nmajor sub-corpus, (ii) its semi-diplomatic transcription rules (expansion,\nnormalization, punctuation), (iii) a strategy for challenging out-of-domain\ntest splits driven by outlier detection in a joint embedding space, and (iv)\npreliminary baseline experiments using TrOCR and MiniCPM2.5 comparing random\nand outlier-based test partitions. Overall, TRIDIS is designed to stimulate\njoint robust Handwritten Text Recognition (HTR) and Named Entity Recognition\n(NER) research across medieval and early modern textual heritage."}
{"id": "2503.22727", "pdf": "https://arxiv.org/pdf/2503.22727", "abs": "https://arxiv.org/abs/2503.22727", "authors": ["Alejandro Lozano", "Min Woo Sun", "James Burgess", "Jeffrey J. Nirschl", "Christopher Polzak", "Yuhui Zhang", "Liangyu Chen", "Jeffrey Gu", "Ivan Lopez", "Josiah Aklilu", "Anita Rau", "Austin Wolfgang Katzer", "Collin Chiu", "Orr Zohar", "Xiaohan Wang", "Alfred Seunghoon Song", "Chiang Chia-Chun", "Robert Tibshirani", "Serena Yeung-Levy"], "title": "A Large-Scale Vision-Language Dataset Derived from Open Scientific Literature to Advance Biomedical Generalist AI", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Despite the excitement behind biomedical artificial intelligence (AI), access\nto high-quality, diverse, and large-scale data - the foundation for modern AI\nsystems - is still a bottleneck to unlocking its full potential. To address\nthis gap, we introduce Biomedica, an open-source dataset derived from the\nPubMed Central Open Access subset, containing over 6 million scientific\narticles and 24 million image-text pairs, along with 27 metadata fields\n(including expert human annotations). To overcome the challenges of accessing\nour large-scale dataset, we provide scalable streaming and search APIs through\na web server, facilitating seamless integration with AI systems. We demonstrate\nthe utility of the Biomedica dataset by building embedding models, chat-style\nmodels, and retrieval-augmented chat agents. Notably, all our AI models surpass\nprevious open systems in their respective categories, underscoring the critical\nrole of diverse, high-quality, and large-scale biomedical data."}
{"id": "2503.22746", "pdf": "https://arxiv.org/pdf/2503.22746", "abs": "https://arxiv.org/abs/2503.22746", "authors": ["Kyung Ho Lim", "Ujin Kang", "Xiang Li", "Jin Sung Kim", "Young-Chul Jung", "Sangjoon Park", "Byung-Hoon Kim"], "title": "Susceptibility of Large Language Models to User-Driven Factors in Medical Queries", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) are increasingly used in healthcare, but their\nreliability is heavily influenced by user-driven factors such as question\nphrasing and the completeness of clinical information. In this study, we\nexamined how misinformation framing, source authority, model persona, and\nomission of key clinical details affect the diagnostic accuracy and reliability\nof LLM outputs. We conducted two experiments: one introducing misleading\nexternal opinions with varying assertiveness (perturbation test), and another\nremoving specific categories of patient information (ablation test). Using\npublic datasets (MedQA and Medbullets), we evaluated proprietary models\n(GPT-4o, Claude 3.5 Sonnet, Claude 3.5 Haiku, Gemini 1.5 Pro, Gemini 1.5 Flash)\nand open-source models (LLaMA 3 8B, LLaMA 3 Med42 8B, DeepSeek R1 8B). All\nmodels were vulnerable to user-driven misinformation, with proprietary models\nespecially affected by definitive and authoritative language. Assertive tone\nhad the greatest negative impact on accuracy. In the ablation test, omitting\nphysical exam findings and lab results caused the most significant performance\ndrop. Although proprietary models had higher baseline accuracy, their\nperformance declined sharply under misinformation. These results highlight the\nneed for well-structured prompts and complete clinical context. Users should\navoid authoritative framing of misinformation and provide full clinical\ndetails, especially for complex cases."}
{"id": "2503.22782", "pdf": "https://arxiv.org/pdf/2503.22782", "abs": "https://arxiv.org/abs/2503.22782", "authors": ["Nina Weng", "Aasa Feragen", "Siavash Bigdeli"], "title": "Patronus: Bringing Transparency to Diffusion Models with Prototypes", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Diffusion-based generative models, such as Denoising Diffusion Probabilistic\nModels (DDPMs), have achieved remarkable success in image generation, but their\nstep-by-step denoising process remains opaque, leaving critical aspects of the\ngeneration mechanism unexplained. To address this, we introduce\n\\emph{Patronus}, an interpretable diffusion model inspired by ProtoPNet.\nPatronus integrates a prototypical network into DDPMs, enabling the extraction\nof prototypes and conditioning of the generation process on their prototype\nactivation vector. This design enhances interpretability by showing the learned\nprototypes and how they influence the generation process. Additionally, the\nmodel supports downstream tasks like image manipulation, enabling more\ntransparent and controlled modifications. Moreover, Patronus could reveal\nshortcut learning in the generation process by detecting unwanted correlations\nbetween learned prototypes. Notably, Patronus operates entirely without any\nannotations or text prompts. This work opens new avenues for understanding and\ncontrolling diffusion models through prototype-based interpretability. Our code\nis available at\n\\href{https://github.com/nina-weng/patronus}{https://github.com/nina-weng/patronus}."}
{"id": "2503.22764", "pdf": "https://arxiv.org/pdf/2503.22764", "abs": "https://arxiv.org/abs/2503.22764", "authors": ["Mingyuan Zhang", "Yue Bai", "Huan Wang", "Yizhou Wang", "Qihua Dong", "Yun Fu"], "title": "Boosting Large Language Models with Mask Fine-Tuning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The model is usually kept integral in the mainstream large language model\n(LLM) fine-tuning protocols. No works have questioned whether maintaining the\nintegrity of the model is indispensable for performance. In this work, we\nintroduce Mask Fine-Tuning (MFT), a brand-new LLM fine-tuning paradigm to show\nthat properly breaking the integrity of the model can surprisingly lead to\nimproved performance. Specifically, MFT learns a set of binary masks supervised\nby the typical LLM fine-tuning objective. Extensive experiments show that MFT\ngains a consistent performance boost across various domains and backbones\n(e.g., 1.95%/1.88% average gain in coding with LLaMA2-7B/3.1-8B). Detailed\nprocedures are provided to study the proposed MFT from different hyperparameter\nperspectives for better insight. In particular, MFT naturally updates the\ncurrent LLM training protocol by deploying it on a complete well-trained model.\nThis study extends the functionality of mask learning from its conventional\nnetwork pruning context for model compression to a more general scope."}
{"id": "2503.22796", "pdf": "https://arxiv.org/pdf/2503.22796", "abs": "https://arxiv.org/abs/2503.22796", "authors": ["Hanling Zhang", "Rundong Su", "Zhihang Yuan", "Pengtao Chen", "Mingzhu Shen Yibo Fan", "Shengen Yan", "Guohao Dai", "Yu Wang"], "title": "DiTFastAttnV2: Head-wise Attention Compression for Multi-Modality Diffusion Transformers", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-image generation models, especially Multimodal Diffusion Transformers\n(MMDiT), have shown remarkable progress in generating high-quality images.\nHowever, these models often face significant computational bottlenecks,\nparticularly in attention mechanisms, which hinder their scalability and\nefficiency. In this paper, we introduce DiTFastAttnV2, a post-training\ncompression method designed to accelerate attention in MMDiT. Through an\nin-depth analysis of MMDiT's attention patterns, we identify key differences\nfrom prior DiT-based methods and propose head-wise arrow attention and caching\nmechanisms to dynamically adjust attention heads, effectively bridging this\ngap. We also design an Efficient Fused Kernel for further acceleration. By\nleveraging local metric methods and optimization techniques, our approach\nsignificantly reduces the search time for optimal compression schemes to just\nminutes while maintaining generation quality. Furthermore, with the customized\nkernel, DiTFastAttnV2 achieves a 68% reduction in attention FLOPs and 1.5x\nend-to-end speedup on 2K image generation without compromising visual fidelity."}
{"id": "2503.22828", "pdf": "https://arxiv.org/pdf/2503.22828", "abs": "https://arxiv.org/abs/2503.22828", "authors": ["Alexander Gurung", "Mirella Lapata"], "title": "Learning to Reason for Long-Form Story Generation", "categories": ["cs.CL"], "comment": null, "summary": "Generating high-quality stories spanning thousands of tokens requires\ncompetency across a variety of skills, from tracking plot and character arcs to\nkeeping a consistent and engaging style. Due to the difficulty of sourcing\nlabeled datasets and precise quality measurements, most work using large\nlanguage models (LLMs) for long-form story generation uses combinations of\nhand-designed prompting techniques to elicit author-like behavior. This is a\nmanual process that is highly dependent on the specific story-generation task.\nMotivated by the recent success of applying RL with Verifiable Rewards to\ndomains like math and coding, we propose a general story-generation task\n(Next-Chapter Prediction) and a reward formulation (Verified Rewards via\nCompletion Likelihood Improvement) that allows us to use an unlabeled book\ndataset as a learning signal for reasoning. We learn to reason over a story's\ncondensed information and generate a detailed plan for the next chapter. Our\nreasoning is evaluated via the chapters it helps a story-generator create, and\ncompared against non-trained and supervised finetuning (SFT) baselines.\nPairwise human judgments reveal the chapters our learned reasoning produces are\npreferred across almost all metrics, and the effect is more pronounced in Scifi\nand Fantasy genres."}
{"id": "2503.22841", "pdf": "https://arxiv.org/pdf/2503.22841", "abs": "https://arxiv.org/abs/2503.22841", "authors": ["Yifan Wang", "Xu Ma", "Yitian Zhang", "Zhongruo Wang", "Sung-Cheol Kim", "Vahid Mirjalili", "Vidya Renganathan", "Yun Fu"], "title": "GmNet: Revisiting Gating Mechanisms From A Frequency View", "categories": ["cs.CV"], "comment": null, "summary": "Gating mechanisms have emerged as an effective strategy integrated into model\ndesigns beyond recurrent neural networks for addressing long-range dependency\nproblems. In a broad understanding, it provides adaptive control over the\ninformation flow while maintaining computational efficiency. However, there is\na lack of theoretical analysis on how the gating mechanism works in neural\nnetworks. In this paper, inspired by the {convolution theorem}, we\nsystematically explore the effect of gating mechanisms on the training dynamics\nof neural networks from a frequency perspective. We investigate the interact\nbetween the element-wise product and activation functions in managing the\nresponses to different frequency components. Leveraging these insights, we\npropose a Gating Mechanism Network (GmNet), a lightweight model designed to\nefficiently utilize the information of various frequency components. It\nminimizes the low-frequency bias present in existing lightweight models. GmNet\nachieves impressive performance in terms of both effectiveness and efficiency\nin the image classification task."}
{"id": "2503.22856", "pdf": "https://arxiv.org/pdf/2503.22856", "abs": "https://arxiv.org/abs/2503.22856", "authors": ["Shanshan Bai", "Anna Kruspe", "Xiaoxiang Zhu"], "title": "Generating Synthetic Oracle Datasets to Analyze Noise Impact: A Study on Building Function Classification Using Tweets", "categories": ["cs.CL"], "comment": null, "summary": "Tweets provides valuable semantic context for earth observation tasks and\nserves as a complementary modality to remote sensing imagery. In building\nfunction classification (BFC), tweets are often collected using geographic\nheuristics and labeled via external databases, an inherently weakly supervised\nprocess that introduces both label noise and sentence level feature noise\n(e.g., irrelevant or uninformative tweets). While label noise has been widely\nstudied, the impact of sentence level feature noise remains underexplored,\nlargely due to the lack of clean benchmark datasets for controlled analysis. In\nthis work, we propose a method for generating a synthetic oracle dataset using\nLLM, designed to contain only tweets that are both correctly labeled and\nsemantically relevant to their associated buildings. This oracle dataset\nenables systematic investigation of noise impacts that are otherwise difficult\nto isolate in real-world data. To assess its utility, we compare model\nperformance using Naive Bayes and mBERT classifiers under three configurations:\nreal vs. synthetic training data, and cross-domain generalization. Results show\nthat noise in real tweets significantly degrades the contextual learning\ncapacity of mBERT, reducing its performance to that of a simple keyword-based\nmodel. In contrast, the clean synthetic dataset allows mBERT to learn\neffectively, outperforming Naive Bayes Bayes by a large margin. These findings\nhighlight that addressing feature noise is more critical than model complexity\nin this task. Our synthetic dataset offers a novel experimental environment for\nfuture noise injection studies and is publicly available on GitHub."}
{"id": "2503.22862", "pdf": "https://arxiv.org/pdf/2503.22862", "abs": "https://arxiv.org/abs/2503.22862", "authors": ["Soumitri Chattopadhyay", "Basar Demir", "Marc Niethammer"], "title": "Zero-shot Domain Generalization of Foundational Models for 3D Medical Image Segmentation: An Experimental Study", "categories": ["cs.CV"], "comment": null, "summary": "Domain shift, caused by variations in imaging modalities and acquisition\nprotocols, limits model generalization in medical image segmentation. While\nfoundation models (FMs) trained on diverse large-scale data hold promise for\nzero-shot generalization, their application to volumetric medical data remains\nunderexplored. In this study, we examine their ability towards domain\ngeneralization (DG), by conducting a comprehensive experimental study\nencompassing 6 medical segmentation FMs and 12 public datasets spanning\nmultiple modalities and anatomies. Our findings reveal the potential of\npromptable FMs in bridging the domain gap via smart prompting techniques.\nAdditionally, by probing into multiple facets of zero-shot DG, we offer\nvaluable insights into the viability of FMs for DG and identify promising\navenues for future research."}
{"id": "2503.22877", "pdf": "https://arxiv.org/pdf/2503.22877", "abs": "https://arxiv.org/abs/2503.22877", "authors": ["Bruno Coelho", "Shujaat Mirza", "Yuyuan Cui", "Christina Pöpper", "Damon McCoy"], "title": "Understanding Inequality of LLM Fact-Checking over Geographic Regions with Agent and Retrieval models", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Fact-checking is a potentially useful application of Large Language Models\n(LLMs) to combat the growing dissemination of disinformation. However, the\nperformance of LLMs varies across geographic regions. In this paper, we\nevaluate the factual accuracy of open and private models across a diverse set\nof regions and scenarios.\n  Using a dataset containing 600 fact-checked statements balanced across six\nglobal regions we examine three experimental setups of fact-checking a\nstatement: (1) when just the statement is available, (2) when an LLM-based\nagent with Wikipedia access is utilized, and (3) as a best case scenario when a\nRetrieval-Augmented Generation (RAG) system provided with the official fact\ncheck is employed. Our findings reveal that regardless of the scenario and LLM\nused, including GPT-4, Claude Sonnet, and LLaMA, statements from the Global\nNorth perform substantially better than those from the Global South.\nFurthermore, this gap is broadened for the more realistic case of a Wikipedia\nagent-based system, highlighting that overly general knowledge bases have a\nlimited ability to address region-specific nuances. These results underscore\nthe urgent need for better dataset balancing and robust retrieval strategies to\nenhance LLM fact-checking capabilities, particularly in geographically diverse\ncontexts."}
{"id": "2503.22869", "pdf": "https://arxiv.org/pdf/2503.22869", "abs": "https://arxiv.org/abs/2503.22869", "authors": ["Alexey Gavryushin", "Florian Redhardt", "Gaia Di Lorenzo", "Luc Van Gool", "Marc Pollefeys", "Kaichun Mo", "Xi Wang"], "title": "SIGHT: Single-Image Conditioned Generation of Hand Trajectories for Hand-Object Interaction", "categories": ["cs.CV"], "comment": null, "summary": "We introduce a novel task of generating realistic and diverse 3D hand\ntrajectories given a single image of an object, which could be involved in a\nhand-object interaction scene or pictured by itself. When humans grasp an\nobject, appropriate trajectories naturally form in our minds to use it for\nspecific tasks. Hand-object interaction trajectory priors can greatly benefit\napplications in robotics, embodied AI, augmented reality and related fields.\nHowever, synthesizing realistic and appropriate hand trajectories given a\nsingle object or hand-object interaction image is a highly ambiguous task,\nrequiring to correctly identify the object of interest and possibly even the\ncorrect interaction among many possible alternatives. To tackle this\nchallenging problem, we propose the SIGHT-Fusion system, consisting of a\ncurated pipeline for extracting visual features of hand-object interaction\ndetails from egocentric videos involving object manipulation, and a\ndiffusion-based conditional motion generation model processing the extracted\nfeatures. We train our method given video data with corresponding hand\ntrajectory annotations, without supervision in the form of action labels. For\nthe evaluation, we establish benchmarks utilizing the first-person FPHAB and\nHOI4D datasets, testing our method against various baselines and using multiple\nmetrics. We also introduce task simulators for executing the generated hand\ntrajectories and reporting task success rates as an additional metric.\nExperiments show that our method generates more appropriate and realistic hand\ntrajectories than baselines and presents promising generalization capability on\nunseen objects. The accuracy of the generated hand trajectories is confirmed in\na physics simulation setting, showcasing the authenticity of the created\nsequences and their applicability in downstream uses."}
{"id": "2503.22913", "pdf": "https://arxiv.org/pdf/2503.22913", "abs": "https://arxiv.org/abs/2503.22913", "authors": ["Xinyu Wang", "Linrui Ma", "Jerry Huang", "Peng Lu", "Prasanna Parthasarathi", "Xiao-Wen Chang", "Boxing Chen", "Yufei Cui"], "title": "Resona: Improving Context Copying in Linear Recurrence Models with Retrieval", "categories": ["cs.CL"], "comment": null, "summary": "Recent shifts in the space of large language model (LLM) research have shown\nan increasing focus on novel architectures to compete with prototypical\nTransformer-based models that have long dominated this space. Linear recurrent\nmodels have proven to be a viable competitor due to their computational\nefficiency. However, such models still demonstrate a sizable gap compared to\nTransformers in terms of in-context learning among other tasks that require\nrecalling information from a context. In this work, we introduce __Resona__, a\nsimple and scalable framework for augmenting linear recurrent models with\nretrieval. __Resona__~augments models with the ability to integrate retrieved\ninformation from the provided input context, enabling tailored behavior to\ndiverse task requirements. Experiments on a variety of linear recurrent models\ndemonstrate that __Resona__-augmented models observe significant performance\ngains on a variety of synthetic as well as real-world natural language tasks,\nhighlighting its ability to act as a general purpose method to improve the\nin-context learning and language modeling abilities of linear recurrent LLMs."}
{"id": "2503.22880", "pdf": "https://arxiv.org/pdf/2503.22880", "abs": "https://arxiv.org/abs/2503.22880", "authors": ["Matias Valdenegro-Toro", "Deepan Chakravarthi Padmanabhan", "Deepak Singh", "Bilal Wehbe", "Yvan Petillot"], "title": "The Marine Debris Forward-Looking Sonar Datasets", "categories": ["cs.CV", "cs.LG"], "comment": "10 pages, 12 figures, Oceans Brest 2025 camera readyu", "summary": "Sonar sensing is fundamental for underwater robotics, but limited by\ncapabilities of AI systems, which need large training datasets. Public data in\nsonar modalities is lacking. This paper presents the Marine Debris\nForward-Looking Sonar datasets, with three different settings (watertank,\nturntable, flooded quarry) increasing dataset diversity and multiple computer\nvision tasks: object classification, object detection, semantic segmentation,\npatch matching, and unsupervised learning. We provide full dataset description,\nbasic analysis and initial results for some tasks. We expect the research\ncommunity will benefit from this dataset, which is publicly available at\nhttps://doi.org/10.5281/zenodo.15101686"}
{"id": "2503.22948", "pdf": "https://arxiv.org/pdf/2503.22948", "abs": "https://arxiv.org/abs/2503.22948", "authors": ["Tianyang Xu", "Xiaoze Liu", "Feijie Wu", "Xiaoqian Wang", "Jing Gao"], "title": "SUV: Scalable Large Language Model Copyright Compliance with Regularized Selective Unlearning", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have transformed natural language processing by\nlearning from massive datasets, yet this rapid progress has also drawn legal\nscrutiny, as the ability to unintentionally generate copyrighted content has\nalready prompted several prominent lawsuits. In this work, we introduce SUV\n(Selective Unlearning for Verbatim data), a selective unlearning framework\ndesigned to prevent LLM from memorizing copyrighted content while preserving\nits overall utility. In detail, the proposed method constructs a dataset that\ncaptures instances of copyrighted infringement cases by the targeted LLM. With\nthe dataset, we unlearn the content from the LLM by means of Direct Preference\nOptimization (DPO), which replaces the verbatim copyrighted content with\nplausible and coherent alternatives. Since DPO may hinder the LLM's performance\nin other unrelated tasks, we integrate gradient projection and Fisher\ninformation regularization to mitigate the degradation. We validate our\napproach using a large-scale dataset of 500 famous books (predominantly\ncopyrighted works) and demonstrate that SUV significantly reduces verbatim\nmemorization with negligible impact on the performance on unrelated tasks.\nExtensive experiments on both our dataset and public benchmarks confirm the\nscalability and efficacy of our approach, offering a promising solution for\nmitigating copyright risks in real-world LLM applications."}
{"id": "2503.22881", "pdf": "https://arxiv.org/pdf/2503.22881", "abs": "https://arxiv.org/abs/2503.22881", "authors": ["Lauren Shrack", "Timm Haucke", "Antoine Salaün", "Arjun Subramonian", "Sara Beery"], "title": "Pairwise Matching of Intermediate Representations for Fine-grained Explainability", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The differences between images belonging to fine-grained categories are often\nsubtle and highly localized, and existing explainability techniques for deep\nlearning models are often too diffuse to provide useful and interpretable\nexplanations. We propose a new explainability method (PAIR-X) that leverages\nboth intermediate model activations and backpropagated relevance scores to\ngenerate fine-grained, highly-localized pairwise visual explanations. We use\nanimal and building re-identification (re-ID) as a primary case study of our\nmethod, and we demonstrate qualitatively improved results over a diverse set of\nexplainability baselines on 35 public re-ID datasets. In interviews, animal\nre-ID experts were in unanimous agreement that PAIR-X was an improvement over\nexisting baselines for deep model explainability, and suggested that its\nvisualizations would be directly applicable to their work. We also propose a\nnovel quantitative evaluation metric for our method, and demonstrate that\nPAIR-X visualizations appear more plausible for correct image matches than\nincorrect ones even when the model similarity score for the pairs is the same.\nBy improving interpretability, PAIR-X enables humans to better distinguish\ncorrect and incorrect matches. Our code is available at:\nhttps://github.com/pairx-explains/pairx"}
{"id": "2503.22954", "pdf": "https://arxiv.org/pdf/2503.22954", "abs": "https://arxiv.org/abs/2503.22954", "authors": ["Xinyu Yao", "Aditya Sannabhadti", "Holly Wiberg", "Karmel S. Shehadeh", "Rema Padman"], "title": "Can LLMs Support Medical Knowledge Imputation? An Evaluation-Based Perspective", "categories": ["cs.CL", "cs.AI", "q-bio.QM"], "comment": "10 pages, 3 figures, AMIA", "summary": "Medical knowledge graphs (KGs) are essential for clinical decision support\nand biomedical research, yet they often exhibit incompleteness due to knowledge\ngaps and structural limitations in medical coding systems. This issue is\nparticularly evident in treatment mapping, where coding systems such as ICD,\nMondo, and ATC lack comprehensive coverage, resulting in missing or\ninconsistent associations between diseases and their potential treatments. To\naddress this issue, we have explored the use of Large Language Models (LLMs)\nfor imputing missing treatment relationships. Although LLMs offer promising\ncapabilities in knowledge augmentation, their application in medical knowledge\nimputation presents significant risks, including factual inaccuracies,\nhallucinated associations, and instability between and within LLMs. In this\nstudy, we systematically evaluate LLM-driven treatment mapping, assessing its\nreliability through benchmark comparisons. Our findings highlight critical\nlimitations, including inconsistencies with established clinical guidelines and\npotential risks to patient safety. This study serves as a cautionary guide for\nresearchers and practitioners, underscoring the importance of critical\nevaluation and hybrid approaches when leveraging LLMs to enhance treatment\nmappings on medical knowledge graphs."}
{"id": "2503.22884", "pdf": "https://arxiv.org/pdf/2503.22884", "abs": "https://arxiv.org/abs/2503.22884", "authors": ["Yi-Ting Shen", "Sungmin Eum", "Doheon Lee", "Rohit Shete", "Chiao-Yi Wang", "Heesung Kwon", "Shuvra S. Bhattacharyya"], "title": "AutoComPose: Automatic Generation of Pose Transition Descriptions for Composed Pose Retrieval Using Multimodal LLMs", "categories": ["cs.CV"], "comment": null, "summary": "Composed pose retrieval (CPR) enables users to search for human poses by\nspecifying a reference pose and a transition description, but progress in this\nfield is hindered by the scarcity and inconsistency of annotated pose\ntransitions. Existing CPR datasets rely on costly human annotations or\nheuristic-based rule generation, both of which limit scalability and diversity.\nIn this work, we introduce AutoComPose, the first framework that leverages\nmultimodal large language models (MLLMs) to automatically generate rich and\nstructured pose transition descriptions. Our method enhances annotation quality\nby structuring transitions into fine-grained body part movements and\nintroducing mirrored/swapped variations, while a cyclic consistency constraint\nensures logical coherence between forward and reverse transitions. To advance\nCPR research, we construct and release two dedicated benchmarks, AIST-CPR and\nPoseFixCPR, supplementing prior datasets with enhanced attributes. Extensive\nexperiments demonstrate that training retrieval models with AutoComPose yields\nsuperior performance over human-annotated and heuristic-based methods,\nsignificantly reducing annotation costs while improving retrieval quality. Our\nwork pioneers the automatic annotation of pose transitions, establishing a\nscalable foundation for future CPR research."}
{"id": "2503.22973", "pdf": "https://arxiv.org/pdf/2503.22973", "abs": "https://arxiv.org/abs/2503.22973", "authors": ["Vivek Iyer", "Ricardo Rei", "Pinzhen Chen", "Alexandra Birch"], "title": "XL-Instruct: Synthetic Data for Cross-Lingual Open-Ended Generation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Cross-lingual open-ended generation -- i.e. generating responses in a desired\nlanguage different from that of the user's query -- is an important yet\nunderstudied problem. We introduce XL-AlpacaEval, a new benchmark for\nevaluating cross-lingual generation capabilities in Large Language Models\n(LLMs), and propose XL-Instruct, a high-quality synthetic data generation\nmethod. Fine-tuning with just 8K XL-Instruct-generated instructions\nsignificantly improves model performance, increasing the win rate against\nGPT-4o-Mini from 7.4% to 21.5%, and improving on several fine-grained quality\nmetrics. Additionally, models fine-tuned on XL-Instruct exhibit strong\nzero-shot transfer to both English-only and multilingual generation tasks.\nGiven its consistent gains across the board, we strongly recommend\nincorporating XL-Instruct in the post-training pipeline of future multilingual\nLLMs. To facilitate further research, we will publicly and freely release the\nXL-Instruct and XL-AlpacaEval datasets, which constitute two of the few\ncross-lingual resources currently available in the literature."}
{"id": "2503.22890", "pdf": "https://arxiv.org/pdf/2503.22890", "abs": "https://arxiv.org/abs/2503.22890", "authors": ["Ke Zhang", "Vishal M. Patel"], "title": "MedCL: Learning Consistent Anatomy Distribution for Scribble-supervised Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Curating large-scale fully annotated datasets is expensive, laborious, and\ncumbersome, especially for medical images. Several methods have been proposed\nin the literature that make use of weak annotations in the form of scribbles.\nHowever, these approaches require large amounts of scribble annotations, and\nare only applied to the segmentation of regular organs, which are often\nunavailable for the disease species that fall in the long-tailed distribution.\nMotivated by the fact that the medical labels have anatomy distribution priors,\nwe propose a scribble-supervised clustering-based framework, called MedCL, to\nlearn the inherent anatomy distribution of medical labels. Our approach\nconsists of two steps: i) Mix the features with intra- and inter-image mix\noperations, and ii) Perform feature clustering and regularize the anatomy\ndistribution at both local and global levels. Combined with a small amount of\nweak supervision, the proposed MedCL is able to segment both regular organs and\nchallenging irregular pathologies. We implement MedCL based on SAM and UNet\nbackbones, and evaluate the performance on three open datasets of regular\nstructure (MSCMRseg), multiple organs (BTCV) and irregular pathology (MyoPS).\nIt is shown that even with less scribble supervision, MedCL substantially\noutperforms the conventional segmentation methods. Our code is available at\nhttps://github.com/BWGZK/MedCL."}
{"id": "2503.22985", "pdf": "https://arxiv.org/pdf/2503.22985", "abs": "https://arxiv.org/abs/2503.22985", "authors": ["Zhengyi Zhao", "Shubo Zhang", "Zezhong Wang", "Bin Liang", "Binyang Li", "Kam-Fai Wong"], "title": "FReM: A Flexible Reasoning Mechanism for Balancing Quick and Slow Thinking in Long-Context Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "Long-context question-answering (LCQA) systems have greatly benefited from\nthe powerful reasoning capabilities of large language models (LLMs), which can\nbe categorized into slow and quick reasoning modes. However, both modes have\ntheir limitations. Slow thinking generally leans to explore every possible\nreasoning path, which leads to heavy overthinking and wastes time. Quick\nthinking usually relies on pattern matching rather than truly understanding the\nquery logic, which misses proper understanding. To address these issues, we\npropose FReM: Flexible Reasoning Mechanism, a method that adjusts reasoning\ndepth according to the complexity of each question. Specifically, FReM\nleverages synthetic reference QA examples to provide an explicit chain of\nthought, enabling efficient handling of simple queries while allowing deeper\nreasoning for more complex ones. By doing so, FReM helps quick-thinking models\nmove beyond superficial pattern matching and narrows the reasoning space for\nslow-thinking models to avoid unnecessary exploration. Experiments on seven QA\ndatasets show that FReM improves reasoning accuracy and scalability,\nparticularly for complex multihop questions, indicating its potential to\nadvance LCQA methodologies."}
{"id": "2503.22906", "pdf": "https://arxiv.org/pdf/2503.22906", "abs": "https://arxiv.org/abs/2503.22906", "authors": ["Heng Yu", "Juze Zhang", "Changan Chen", "Tiange Xiang", "Yusu Fang", "Juan Carlos Niebles", "Ehsan Adeli"], "title": "SocialGen: Modeling Multi-Human Social Interaction with Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Human interactions in everyday life are inherently social, involving\nengagements with diverse individuals across various contexts. Modeling these\nsocial interactions is fundamental to a wide range of real-world applications.\nIn this paper, we introduce SocialGen, the first unified motion-language model\ncapable of modeling interaction behaviors among varying numbers of individuals,\nto address this crucial yet challenging problem. Unlike prior methods that are\nlimited to two-person interactions, we propose a novel social motion\nrepresentation that supports tokenizing the motions of an arbitrary number of\nindividuals and aligning them with the language space. This alignment enables\nthe model to leverage rich, pretrained linguistic knowledge to better\nunderstand and reason about human social behaviors. To tackle the challenges of\ndata scarcity, we curate a comprehensive multi-human interaction dataset,\nSocialX, enriched with textual annotations. Leveraging this dataset, we\nestablish the first comprehensive benchmark for multi-human interaction tasks.\nOur method achieves state-of-the-art performance across motion-language tasks,\nsetting a new standard for multi-human interaction modeling."}
{"id": "2503.22996", "pdf": "https://arxiv.org/pdf/2503.22996", "abs": "https://arxiv.org/abs/2503.22996", "authors": ["Giang Do", "Hung Le", "Truyen Tran"], "title": "Sparse Mixture of Experts as Unified Competitive Learning", "categories": ["cs.CL"], "comment": "18 pages", "summary": "Sparse Mixture of Experts (SMoE) improves the efficiency of large language\nmodel training by directing input tokens to a subset of experts. Despite its\nsuccess in generation tasks, its generalization ability remains an open\nquestion. In this paper, we demonstrate that current SMoEs, which fall into two\ncategories: (1) Token Choice ;and (2) Expert Choice, struggle with tasks such\nas the Massive Text Embedding Benchmark (MTEB). By analyzing their mechanism\nthrough the lens of competitive learning, our study finds that the Token Choice\napproach may overly focus on irrelevant experts, while the Expert Choice\napproach risks discarding important tokens, potentially affecting performance.\nMotivated by this analysis, we propose Unified Competitive Learning SMoE\n(USMoE), a novel and efficient framework designed to improve the performance of\nexisting SMoEs in both scenarios: with and without training. Extensive\nexperiments across various tasks show that USMoE achieves up to a 10%\nimprovement over traditional approaches or reduces computational inference\ncosts by 14% while maintaining strong performance."}
{"id": "2503.22909", "pdf": "https://arxiv.org/pdf/2503.22909", "abs": "https://arxiv.org/abs/2503.22909", "authors": ["Anas Berka", "Mohamed El Hajji", "Raphael Canals", "Youssef Es-saady", "Adel Hafiane"], "title": "Enhancing DeepLabV3+ to Fuse Aerial and Satellite Images for Semantic Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Aerial and satellite imagery are inherently complementary remote sensing\nsources, offering high-resolution detail alongside expansive spatial coverage.\nHowever, the use of these sources for land cover segmentation introduces\nseveral challenges, prompting the development of a variety of segmentation\nmethods. Among these approaches, the DeepLabV3+ architecture is considered as a\npromising approach in the field of single-source image segmentation. However,\ndespite its reliable results for segmentation, there is still a need to\nincrease its robustness and improve its performance. This is particularly\ncrucial for multimodal image segmentation, where the fusion of diverse types of\ninformation is essential.\n  An interesting approach involves enhancing this architectural framework\nthrough the integration of novel components and the modification of certain\ninternal processes.\n  In this paper, we enhance the DeepLabV3+ architecture by introducing a new\ntransposed conventional layers block for upsampling a second entry to fuse it\nwith high level features. This block is designed to amplify and integrate\ninformation from satellite images, thereby enriching the segmentation process\nthrough fusion with aerial images.\n  For experiments, we used the LandCover.ai (Land Cover from Aerial Imagery)\ndataset for aerial images, alongside the corresponding dataset sourced from\nSentinel 2 data.\n  Through the fusion of both sources, the mean Intersection over Union (mIoU)\nachieved a total mIoU of 84.91% without data augmentation."}
{"id": "2503.23007", "pdf": "https://arxiv.org/pdf/2503.23007", "abs": "https://arxiv.org/abs/2503.23007", "authors": ["Giang Do", "Hung Le", "Truyen Tran"], "title": "S2MoE: Robust Sparse Mixture of Experts via Stochastic Learning", "categories": ["cs.CL"], "comment": "4 pages", "summary": "Sparse Mixture of Experts (SMoE) enables efficient training of large language\nmodels by routing input tokens to a select number of experts. However, training\nSMoE remains challenging due to the issue of representation collapse. Recent\nstudies have focused on improving the router to mitigate this problem, but\nexisting approaches face two key limitations: (1) expert embeddings are\nsignificantly smaller than the model's dimension, contributing to\nrepresentation collapse, and (2) routing each input to the Top-K experts can\ncause them to learn overly similar features. In this work, we propose a novel\napproach called Robust Sparse Mixture of Experts via Stochastic Learning\n(S2MoE), which is a mixture of experts designed to learn from both\ndeterministic and non-deterministic inputs via Learning under Uncertainty.\nExtensive experiments across various tasks demonstrate that S2MoE achieves\nperformance comparable to other routing methods while reducing computational\ninference costs by 28%."}
{"id": "2503.22912", "pdf": "https://arxiv.org/pdf/2503.22912", "abs": "https://arxiv.org/abs/2503.22912", "authors": ["Xin Liang", "Yogesh S Rawat"], "title": "DIFFER: Disentangling Identity Features via Semantic Cues for Clothes-Changing Person Re-ID", "categories": ["cs.CV"], "comment": "Accepted in CVPR 2025", "summary": "Clothes-changing person re-identification (CC-ReID) aims to recognize\nindividuals under different clothing scenarios. Current CC-ReID approaches\neither concentrate on modeling body shape using additional modalities including\nsilhouette, pose, and body mesh, potentially causing the model to overlook\nother critical biometric traits such as gender, age, and style, or they\nincorporate supervision through additional labels that the model tries to\ndisregard or emphasize, such as clothing or personal attributes. However, these\nannotations are discrete in nature and do not capture comprehensive\ndescriptions.\n  In this work, we propose DIFFER: Disentangle Identity Features From Entangled\nRepresentations, a novel adversarial learning method that leverages textual\ndescriptions to disentangle identity features. Recognizing that image features\ninherently mix inseparable information, DIFFER introduces NBDetach, a mechanism\ndesigned for feature disentanglement by leveraging the separable nature of text\ndescriptions as supervision. It partitions the feature space into distinct\nsubspaces and, through gradient reversal layers, effectively separates\nidentity-related features from non-biometric features. We evaluate DIFFER on 4\ndifferent benchmark datasets (LTCC, PRCC, CelebreID-Light, and CCVID) to\ndemonstrate its effectiveness and provide state-of-the-art performance across\nall the benchmarks. DIFFER consistently outperforms the baseline method, with\nimprovements in top-1 accuracy of 3.6% on LTCC, 3.4% on PRCC, 2.5% on\nCelebReID-Light, and 1% on CCVID. Our code can be found here."}
{"id": "2503.23029", "pdf": "https://arxiv.org/pdf/2503.23029", "abs": "https://arxiv.org/abs/2503.23029", "authors": ["Yichun Feng", "Jiawei Wang", "Ruikun He", "Lu Zhou", "Yixue Li"], "title": "A Retrieval-Augmented Knowledge Mining Method with Deep Thinking LLMs for Biomedical Research and Clinical Support", "categories": ["cs.CL"], "comment": null, "summary": "Knowledge graphs and large language models (LLMs) are key tools for\nbiomedical knowledge integration and reasoning, facilitating structured\norganization of scientific articles and discovery of complex semantic\nrelationships. However, current methods face challenges: knowledge graph\nconstruction is limited by complex terminology, data heterogeneity, and rapid\nknowledge evolution, while LLMs show limitations in retrieval and reasoning,\nmaking it difficult to uncover cross-document associations and reasoning\npathways. To address these issues, we propose a pipeline that uses LLMs to\nconstruct a biomedical knowledge graph (BioStrataKG) from large-scale articles\nand builds a cross-document question-answering dataset (BioCDQA) to evaluate\nlatent knowledge retrieval and multi-hop reasoning. We then introduce\nIntegrated and Progressive Retrieval-Augmented Reasoning (IP-RAR) to enhance\nretrieval accuracy and knowledge reasoning. IP-RAR maximizes information recall\nthrough Integrated Reasoning-based Retrieval and refines knowledge via\nProgressive Reasoning-based Generation, using self-reflection to achieve deep\nthinking and precise contextual understanding. Experiments show that IP-RAR\nimproves document retrieval F1 score by 20\\% and answer generation accuracy by\n25\\% over existing methods. This framework helps doctors efficiently integrate\ntreatment evidence for personalized medication plans and enables researchers to\nanalyze advancements and research gaps, accelerating scientific discovery and\ndecision-making."}
{"id": "2503.22929", "pdf": "https://arxiv.org/pdf/2503.22929", "abs": "https://arxiv.org/abs/2503.22929", "authors": ["Pei-Kai Huang", "Jun-Xiong Chong", "Ming-Tsung Hsu", "Fang-Yu Hsu", "Yi-Ting Lin", "Kai-Heng Chien", "Hao-Chiang Shao", "Chiou-Ting Hsu"], "title": "Unsupervised Feature Disentanglement and Augmentation Network for One-class Face Anti-spoofing", "categories": ["cs.CV"], "comment": null, "summary": "Face anti-spoofing (FAS) techniques aim to enhance the security of facial\nidentity authentication by distinguishing authentic live faces from deceptive\nattempts. While two-class FAS methods risk overfitting to training attacks to\nachieve better performance, one-class FAS approaches handle unseen attacks well\nbut are less robust to domain information entangled within the liveness\nfeatures. To address this, we propose an Unsupervised Feature Disentanglement\nand Augmentation Network (\\textbf{UFDANet}), a one-class FAS technique that\nenhances generalizability by augmenting face images via disentangled features.\nThe \\textbf{UFDANet} employs a novel unsupervised feature disentangling method\nto separate the liveness and domain features, facilitating discriminative\nfeature learning. It integrates an out-of-distribution liveness feature\naugmentation scheme to synthesize new liveness features of unseen spoof\nclasses, which deviate from the live class, thus enhancing the representability\nand discriminability of liveness features. Additionally, \\textbf{UFDANet}\nincorporates a domain feature augmentation routine to synthesize unseen domain\nfeatures, thereby achieving better generalizability. Extensive experiments\ndemonstrate that the proposed \\textbf{UFDANet} outperforms previous one-class\nFAS methods and achieves comparable performance to state-of-the-art two-class\nFAS methods."}
{"id": "2503.23053", "pdf": "https://arxiv.org/pdf/2503.23053", "abs": "https://arxiv.org/abs/2503.23053", "authors": ["Hongjia Liu", "Jinlong Li"], "title": "A Training-free LLM Framework with Interaction between Contextually Related Subtasks in Solving Complex Tasks", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown remarkable capabilities in solving\ncomplex tasks. Recent work has explored decomposing such tasks into subtasks\nwith independent contexts. However, some contextually related subtasks may\nencounter information loss during execution, leading to redundant operations or\nexecution failures. To address this issue, we propose a training-free framework\nwith an interaction mechanism, which enables a subtask to query specific\ninformation or trigger certain actions in completed subtasks by sending\nrequests. To implement interaction, we introduce a subtask trajectory memory to\nenable resumption of completed subtasks upon receiving interaction requests.\nAdditionally, we propose a new action during execution, which generates a\nconcise and precise description of execution process and outcomes of a subtask,\nto assist subsequent subtasks in determining interaction targets and requests.\nWe evaluate our framework on interactive decision-making task WebShop and\nmulti-hop question answering HotpotQA, with GPT-3.5 and GPT-4, and comparison\nresults show that our framework outperforms the state-of-the-art training-free\nbaselines."}
{"id": "2503.22932", "pdf": "https://arxiv.org/pdf/2503.22932", "abs": "https://arxiv.org/abs/2503.22932", "authors": ["Kristina P. Sinaga"], "title": "Bi-Level Multi-View fuzzy Clustering with Exponential Distance", "categories": ["cs.CV", "cs.LG", "math.PR", "62H30"], "comment": null, "summary": "In this study, we propose extension of fuzzy c-means (FCM) clustering in\nmulti-view environments. First, we introduce an exponential multi-view FCM\n(E-MVFCM). E-MVFCM is a centralized MVC with consideration to heat-kernel\ncoefficients (H-KC) and weight factors. Secondly, we propose an exponential\nbi-level multi-view fuzzy c-means clustering (EB-MVFCM). Different to E-MVFCM,\nEB-MVFCM does automatic computation of feature and weight factors\nsimultaneously. Like E-MVFCM, EB-MVFCM present explicit forms of the H-KC to\nsimplify the generation of the heat-kernel $\\mathcal{K}(t)$ in powers of the\nproper time $t$ during the clustering process. All the features used in this\nstudy, including tools and functions of proposed algorithms will be made\navailable at https://www.github.com/KristinaP09/EB-MVFCM."}
{"id": "2503.23077", "pdf": "https://arxiv.org/pdf/2503.23077", "abs": "https://arxiv.org/abs/2503.23077", "authors": ["Yue Liu", "Jiaying Wu", "Yufei He", "Hongcheng Gao", "Hongyu Chen", "Baolong Bi", "Jiaheng Zhang", "Zhiqi Huang", "Bryan Hooi"], "title": "Efficient Inference for Large Reasoning Models: A Survey", "categories": ["cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) significantly improve the reasoning ability of\nLarge Language Models (LLMs) by learning to reason, exhibiting promising\nperformance in complex task-solving. However, their deliberative reasoning\nprocess leads to inefficiencies in token usage, memory consumption, and\ninference time. Thus, this survey provides a review of efficient inference\nmethods designed specifically for LRMs, focusing on mitigating token\ninefficiency while preserving the reasoning quality. First, we introduce a\ntaxonomy to group the recent methods into two main categories: (a) explicit\ncompact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit\nreasoning structure, and (b) implicit latent CoT, which encodes reasoning steps\nwithin hidden representations instead of explicit tokens. Meanwhile, we discuss\ntheir strengths and weaknesses. Then, we conduct empirical analyses on existing\nmethods from performance and efficiency aspects. Besides, we present open\nchallenges in this field, including human-centric controllable reasoning,\ntrade-off between interpretability and efficiency of reasoning, ensuring safety\nof efficient reasoning, and broader applications of efficient reasoning. In\naddition, we highlight key insights for enhancing LRMs' inference efficiency\nvia techniques such as model merging, new architectures, and agent routers. We\nhope this work serves as a valuable guide, helping researchers overcome\nchallenges in this vibrant\nfield\\footnote{https://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs}."}
{"id": "2503.22936", "pdf": "https://arxiv.org/pdf/2503.22936", "abs": "https://arxiv.org/abs/2503.22936", "authors": ["Pei-Kai Huanga", "Jun-Xiong Chong", "Ming-Tsung Hsu", "Fang-Yu Hsu", "Chiou-Ting Hsu"], "title": "Enhancing Learnable Descriptive Convolutional Vision Transformer for Face Anti-Spoofing", "categories": ["cs.CV"], "comment": null, "summary": "Face anti-spoofing (FAS) heavily relies on identifying live/spoof\ndiscriminative features to counter face presentation attacks. Recently, we\nproposed LDCformer to successfully incorporate the Learnable Descriptive\nConvolution (LDC) into ViT, to model long-range dependency of locally\ndescriptive features for FAS. In this paper, we propose three novel training\nstrategies to effectively enhance the training of LDCformer to largely boost\nits feature characterization capability. The first strategy, dual-attention\nsupervision, is developed to learn fine-grained liveness features guided by\nregional live/spoof attentions. The second strategy, self-challenging\nsupervision, is designed to enhance the discriminability of the features by\ngenerating challenging training data. In addition, we propose a third training\nstrategy, transitional triplet mining strategy, through narrowing the\ncross-domain gap while maintaining the transitional relationship between live\nand spoof features, to enlarge the domain-generalization capability of\nLDCformer. Extensive experiments show that LDCformer under joint supervision of\nthe three novel training strategies outperforms previous methods."}
{"id": "2503.23078", "pdf": "https://arxiv.org/pdf/2503.23078", "abs": "https://arxiv.org/abs/2503.23078", "authors": ["Zhengyi Zhao", "Shubo Zhang", "Yiming Du", "Bin Liang", "Baojun Wang", "Zhongyang Li", "Binyang Li", "Kam-Fai Wong"], "title": "EventWeave: A Dynamic Framework for Capturing Core and Supporting Events in Dialogue Systems", "categories": ["cs.CL"], "comment": null, "summary": "Existing large language models (LLMs) have shown remarkable progress in\ndialogue systems. However, many approaches still overlook the fundamental role\nof events throughout multi-turn interactions, leading to \\textbf{incomplete\ncontext tracking}. Without tracking these events, dialogue systems often lose\ncoherence and miss subtle shifts in user intent, causing disjointed responses.\nTo bridge this gap, we present \\textbf{EventWeave}, an event-centric framework\nthat identifies and updates both core and supporting events as the conversation\nunfolds. Specifically, we organize these events into a dynamic event graph,\nwhich represents the interplay between \\textbf{core events} that shape the\nprimary idea and \\textbf{supporting events} that provide critical context\nduring the whole dialogue. By leveraging this dynamic graph, EventWeave helps\nmodels focus on the most relevant events when generating responses, thus\navoiding repeated visits of the entire dialogue history. Experimental results\non two benchmark datasets show that EventWeave improves response quality and\nevent relevance without fine-tuning."}
{"id": "2503.22952", "pdf": "https://arxiv.org/pdf/2503.22952", "abs": "https://arxiv.org/abs/2503.22952", "authors": ["Yuxuan Wang", "Yueqian Wang", "Bo Chen", "Tong Wu", "Dongyan Zhao", "Zilong Zheng"], "title": "OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming Video Contexts", "categories": ["cs.CV"], "comment": "To appear at CVPR 2025", "summary": "The rapid advancement of multi-modal language models (MLLMs) like GPT-4o has\npropelled the development of Omni language models, designed to process and\nproactively respond to continuous streams of multi-modal data. Despite their\npotential, evaluating their real-world interactive capabilities in streaming\nvideo contexts remains a formidable challenge. In this work, we introduce\nOmniMMI, a comprehensive multi-modal interaction benchmark tailored for\nOmniLLMs in streaming video contexts. OmniMMI encompasses over 1,121 videos and\n2,290 questions, addressing two critical yet underexplored challenges in\nexisting video benchmarks: streaming video understanding and proactive\nreasoning, across six distinct subtasks. Moreover, we propose a novel\nframework, Multi-modal Multiplexing Modeling (M4), designed to enable an\ninference-efficient streaming model that can see, listen while generating."}
{"id": "2503.23084", "pdf": "https://arxiv.org/pdf/2503.23084", "abs": "https://arxiv.org/abs/2503.23084", "authors": ["Yihuai Hong", "Dian Zhou", "Meng Cao", "Lei Yu", "Zhijing Jin"], "title": "The Reasoning-Memorization Interplay in Language Models Is Mediated by a Single Direction", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) excel on a variety of reasoning benchmarks, but\nprevious studies suggest they sometimes struggle to generalize to unseen\nquestions, potentially due to over-reliance on memorized training examples.\nHowever, the precise conditions under which LLMs switch between reasoning and\nmemorization during text generation remain unclear. In this work, we provide a\nmechanistic understanding of LLMs' reasoning-memorization dynamics by\nidentifying a set of linear features in the model's residual stream that govern\nthe balance between genuine reasoning and memory recall. These features not\nonly distinguish reasoning tasks from memory-intensive ones but can also be\nmanipulated to causally influence model performance on reasoning tasks.\nAdditionally, we show that intervening in these reasoning features helps the\nmodel more accurately activate the most relevant problem-solving capabilities\nduring answer generation. Our findings offer new insights into the underlying\nmechanisms of reasoning and memory in LLMs and pave the way for the development\nof more robust and interpretable generative AI systems."}
{"id": "2503.22963", "pdf": "https://arxiv.org/pdf/2503.22963", "abs": "https://arxiv.org/abs/2503.22963", "authors": ["Peiyu Chen", "Fuling Lin", "Weipeng Guan", "Peng Lu"], "title": "SuperEIO: Self-Supervised Event Feature Learning for Event Inertial Odometry", "categories": ["cs.CV"], "comment": null, "summary": "Event cameras asynchronously output low-latency event streams, promising for\nstate estimation in high-speed motion and challenging lighting conditions. As\nopposed to frame-based cameras, the motion-dependent nature of event cameras\npresents persistent challenges in achieving robust event feature detection and\nmatching. In recent years, learning-based approaches have demonstrated superior\nrobustness over traditional handcrafted methods in feature detection and\nmatching, particularly under aggressive motion and HDR scenarios. In this\npaper, we propose SuperEIO, a novel framework that leverages the learning-based\nevent-only detection and IMU measurements to achieve event-inertial odometry.\nOur event-only feature detection employs a convolutional neural network under\ncontinuous event streams. Moreover, our system adopts the graph neural network\nto achieve event descriptor matching for loop closure. The proposed system\nutilizes TensorRT to accelerate the inference speed of deep networks, which\nensures low-latency processing and robust real-time operation on\nresource-limited platforms. Besides, we evaluate our method extensively on\nmultiple public datasets, demonstrating its superior accuracy and robustness\ncompared to other state-of-the-art event-based methods. We have also\nopen-sourced our pipeline to facilitate research in the field:\nhttps://github.com/arclab-hku/SuperEIO."}
{"id": "2503.23088", "pdf": "https://arxiv.org/pdf/2503.23088", "abs": "https://arxiv.org/abs/2503.23088", "authors": ["Himanshu Beniwal", "Reddybathuni Venkat", "Rohit Kumar", "Birudugadda Srivibhav", "Daksh Jain", "Pavan Doddi", "Eshwar Dhande", "Adithya Ananth", "Kuldeep", "Heer Kubadia", "Pratham Sharda", "Mayank Singh"], "title": "UNITYAI-GUARD: Pioneering Toxicity Detection Across Low-Resource Indian Languages", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This work introduces UnityAI-Guard, a framework for binary toxicity\nclassification targeting low-resource Indian languages. While existing systems\npredominantly cater to high-resource languages, UnityAI-Guard addresses this\ncritical gap by developing state-of-the-art models for identifying toxic\ncontent across diverse Brahmic/Indic scripts. Our approach achieves an\nimpressive average F1-score of 84.23% across seven languages, leveraging a\ndataset of 888k training instances and 35k manually verified test instances. By\nadvancing multilingual content moderation for linguistically diverse regions,\nUnityAI-Guard also provides public API access to foster broader adoption and\napplication."}
{"id": "2503.22965", "pdf": "https://arxiv.org/pdf/2503.22965", "abs": "https://arxiv.org/abs/2503.22965", "authors": ["Henri Mueller", "Yechan Kim", "Trevor Gee", "Mahla Nejati"], "title": "Pallet Detection And Localisation From Synthetic Data", "categories": ["cs.CV"], "comment": "10 pages, 9 images, 4 tables, submitted and accepted to ACRA 2024\n  (https://www.araa.asn.au/conference/acra-2024/)", "summary": "The global warehousing industry is experiencing rapid growth, with the market\nsize projected to grow at an annual rate of 8.1% from 2024 to 2030 [Grand View\nResearch, 2021]. This expansion has led to a surge in demand for efficient\npallet detection and localisation systems. While automation can significantly\nstreamline warehouse operations, the development of such systems often requires\nextensive manual data annotation, with an average of 35 seconds per image, for\na typical computer vision project. This paper presents a novel approach to\nenhance pallet detection and localisation using purely synthetic data and\ngeometric features derived from their side faces. By implementing a domain\nrandomisation engine in Unity, the need for time-consuming manual annotation is\neliminated while achieving high-performance results. The proposed method\ndemonstrates a pallet detection performance of 0.995 mAP50 for single pallets\non a real-world dataset. Additionally, an average position accuracy of less\nthan 4.2 cm and an average rotation accuracy of 8.2{\\deg} were achieved for\npallets within a 5-meter range, with the pallet positioned head-on."}
{"id": "2503.23091", "pdf": "https://arxiv.org/pdf/2503.23091", "abs": "https://arxiv.org/abs/2503.23091", "authors": ["Yige Chen", "Zelong Li", "Changbing Yang", "Cindy Zhang", "Amandisa Cady", "Ai Ka Lee", "Zejiao Zeng", "Haihua Pan", "Jungyeul Park"], "title": "Parsing Through Boundaries in Chinese Word Segmentation", "categories": ["cs.CL"], "comment": "Submitted to ACL2025 System Demonstration", "summary": "Chinese word segmentation is a foundational task in natural language\nprocessing (NLP), with far-reaching effects on syntactic analysis. Unlike\nalphabetic languages like English, Chinese lacks explicit word boundaries,\nmaking segmentation both necessary and inherently ambiguous. This study\nhighlights the intricate relationship between word segmentation and syntactic\nparsing, providing a clearer understanding of how different segmentation\nstrategies shape dependency structures in Chinese. Focusing on the Chinese GSD\ntreebank, we analyze multiple word boundary schemes, each reflecting distinct\nlinguistic and computational assumptions, and examine how they influence the\nresulting syntactic structures. To support detailed comparison, we introduce an\ninteractive web-based visualization tool that displays parsing outcomes across\nsegmentation methods."}
{"id": "2503.22976", "pdf": "https://arxiv.org/pdf/2503.22976", "abs": "https://arxiv.org/abs/2503.22976", "authors": ["Jiahui Zhang", "Yurui Chen", "Yanpeng Zhou", "Yueming Xu", "Ze Huang", "Jilin Mei", "Junhui Chen", "Yu-Jie Yuan", "Xinyue Cai", "Guowei Huang", "Xingyue Quan", "Hang Xu", "Li Zhang"], "title": "From Flatland to Space: Teaching Vision-Language Models to Perceive and Reason in 3D", "categories": ["cs.CV"], "comment": "Project page: https://fudan-zvg.github.io/spar", "summary": "Recent advances in LVLMs have improved vision-language understanding, but\nthey still struggle with spatial perception, limiting their ability to reason\nabout complex 3D scenes. Unlike previous approaches that incorporate 3D\nrepresentations into models to improve spatial understanding, we aim to unlock\nthe potential of VLMs by leveraging spatially relevant image data. To this end,\nwe introduce a novel 2D spatial data generation and annotation pipeline built\nupon scene data with 3D ground-truth. This pipeline enables the creation of a\ndiverse set of spatial tasks, ranging from basic perception tasks to more\ncomplex reasoning tasks. Leveraging this pipeline, we construct SPAR-7M, a\nlarge-scale dataset generated from thousands of scenes across multiple public\ndatasets. In addition, we introduce SPAR-Bench, a benchmark designed to offer a\nmore comprehensive evaluation of spatial capabilities compared to existing\nspatial benchmarks, supporting both single-view and multi-view inputs. Training\non both SPAR-7M and large-scale 2D datasets enables our models to achieve\nstate-of-the-art performance on 2D spatial benchmarks. Further fine-tuning on\n3D task-specific datasets yields competitive results, underscoring the\neffectiveness of our dataset in enhancing spatial reasoning."}
{"id": "2503.23095", "pdf": "https://arxiv.org/pdf/2503.23095", "abs": "https://arxiv.org/abs/2503.23095", "authors": ["Yuelyu Ji", "Rui Meng", "Zhuochun Li", "Daqing He"], "title": "Memory-Aware and Uncertainty-Guided Retrieval for Multi-Hop Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "Multi-hop question answering (QA) requires models to retrieve and reason over\nmultiple pieces of evidence. While Retrieval-Augmented Generation (RAG) has\nmade progress in this area, existing methods often suffer from two key\nlimitations: (1) fixed or overly frequent retrieval steps, and (2) ineffective\nuse of previously retrieved knowledge.\n  We propose MIND (Memory-Informed and INteractive Dynamic RAG), a framework\nthat addresses these challenges through: (i) prompt-based entity extraction to\nidentify reasoning-relevant elements, (ii) dynamic retrieval triggering based\non token-level entropy and attention signals, and (iii) memory-aware filtering,\nwhich stores high-confidence facts across reasoning steps to enable consistent\nmulti-hop generation."}
{"id": "2503.22983", "pdf": "https://arxiv.org/pdf/2503.22983", "abs": "https://arxiv.org/abs/2503.22983", "authors": ["Ashesh Ashesh", "Florian Jug"], "title": "indiSplit: Bringing Severity Cognizance to Image Decomposition in Fluorescence Microscopy", "categories": ["cs.CV"], "comment": null, "summary": "Fluorescence microscopy, while being a key driver for progress in the life\nsciences, is also subject to technical limitations. To overcome them,\ncomputational multiplexing techniques have recently been proposed, which allow\nmultiple cellular structures to be captured in a single image and later be\nunmixed. Existing image decomposition methods are trained on a set of\nsuperimposed input images and the respective unmixed target images. It is\ncritical to note that the relative strength (mixing ratio) of the superimposed\nimages for a given input is a priori unknown. However, existing methods are\ntrained on a fixed intensity ratio of superimposed inputs, making them not\ncognizant to the range of relative intensities that can occur in fluorescence\nmicroscopy. In this work, we propose a novel method called indiSplit that is\ncognizant of the severity of the above mentioned mixing ratio. Our idea is\nbased on InDI, a popular iterative method for image restoration, and an ideal\nstarting point to embrace the unknown mixing ratio in any given input. We\nintroduce (i) a suitably trained regressor network that predicts the\ndegradation level (mixing asymmetry) of a given input image and (ii) a\ndegradation-specific normalization module, enabling degradation-aware inference\nacross all mixing ratios. We show that this method solves two relevant tasks in\nfluorescence microscopy, namely image splitting and bleedthrough removal, and\nempirically demonstrate the applicability of indiSplit on $5$ public datasets.\nWe will release all sources under a permissive license."}
{"id": "2503.23163", "pdf": "https://arxiv.org/pdf/2503.23163", "abs": "https://arxiv.org/abs/2503.23163", "authors": ["Yuxin Lu", "Yu-Ying Chuang", "R. Harald Baayen"], "title": "The realization of tones in spontaneous spoken Taiwan Mandarin: a corpus-based survey and theory-driven computational modeling", "categories": ["cs.CL"], "comment": null, "summary": "A growing body of literature has demonstrated that semantics can co-determine\nfine phonetic detail. However, the complex interplay between phonetic\nrealization and semantics remains understudied, particularly in pitch\nrealization. The current study investigates the tonal realization of Mandarin\ndisyllabic words with all 20 possible combinations of two tones, as found in a\ncorpus of Taiwan Mandarin spontaneous speech. We made use of Generalized\nAdditive Mixed Models (GAMs) to model f0 contours as a function of a series of\npredictors, including gender, tonal context, tone pattern, speech rate, word\nposition, bigram probability, speaker and word. In the GAM analysis, word and\nsense emerged as crucial predictors of f0 contours, with effect sizes that\nexceed those of tone pattern. For each word token in our dataset, we then\nobtained a contextualized embedding by applying the GPT-2 large language model\nto the context of that token in the corpus. We show that the pitch contours of\nword tokens can be predicted to a considerable extent from these contextualized\nembeddings, which approximate token-specific meanings in contexts of use. The\nresults of our corpus study show that meaning in context and phonetic\nrealization are far more entangled than standard linguistic theory predicts."}
{"id": "2503.22984", "pdf": "https://arxiv.org/pdf/2503.22984", "abs": "https://arxiv.org/abs/2503.22984", "authors": ["Zhuowei Li", "Tianchen Zhao", "Xiang Xu", "Zheng Zhang", "Zhihua Li", "Xuanbai Chen", "Qin Zhang", "Alessandro Bergamo", "Anil K. Jain", "Yifan Xing"], "title": "Optimal Transport-Guided Source-Free Adaptation for Face Anti-Spoofing", "categories": ["cs.CV", "I.5.4; I.2.10; I.4.8; I.2.6; C.3"], "comment": "15 pages, 7 figures", "summary": "Developing a face anti-spoofing model that meets the security requirements of\nclients worldwide is challenging due to the domain gap between training\ndatasets and diverse end-user test data. Moreover, for security and privacy\nreasons, it is undesirable for clients to share a large amount of their face\ndata with service providers. In this work, we introduce a novel method in which\nthe face anti-spoofing model can be adapted by the client itself to a target\ndomain at test time using only a small sample of data while keeping model\nparameters and training data inaccessible to the client. Specifically, we\ndevelop a prototype-based base model and an optimal transport-guided adaptor\nthat enables adaptation in either a lightweight training or training-free\nfashion, without updating base model's parameters. Furthermore, we propose\ngeodesic mixup, an optimal transport-based synthesis method that generates\naugmented training data along the geodesic path between source prototypes and\ntarget data distribution. This allows training a lightweight classifier to\neffectively adapt to target-specific characteristics while retaining essential\nknowledge learned from the source domain. In cross-domain and cross-attack\nsettings, compared with recent methods, our method achieves average relative\nimprovements of 19.17% in HTER and 8.58% in AUC, respectively."}
{"id": "2503.23204", "pdf": "https://arxiv.org/pdf/2503.23204", "abs": "https://arxiv.org/abs/2503.23204", "authors": ["Aden Haussmann"], "title": "The Challenge of Achieving Attributability in Multilingual Table-to-Text Generation with Question-Answer Blueprints", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Multilingual Natural Language Generation (NLG) is challenging due to the lack\nof training data for low-resource languages. However, some low-resource\nlanguages have up to tens of millions of speakers globally, making it important\nto improve NLG tools for them. Table-to-Text NLG is an excellent measure of\nmodels' reasoning abilities but is very challenging in the multilingual\nsetting. System outputs are often not attributable, or faithful, to the data in\nthe source table. Intermediate planning techniques like Question-Answer (QA)\nblueprints have been shown to improve attributability on summarisation tasks.\nThis work explores whether QA blueprints make multilingual Table-to-Text\noutputs more attributable to the input tables. This paper extends the\nchallenging multilingual Table-to-Text dataset, TaTA, which includes African\nlanguages, with QA blueprints. Sequence-to-sequence language models are then\nfinetuned on this dataset, with and without blueprints. Results show that QA\nblueprints improve performance for models finetuned and evaluated only on\nEnglish examples, but do not demonstrate gains in the multilingual setting.\nThis is due to inaccuracies in machine translating the blueprints from English\ninto target languages when generating the training data, and models failing to\nrely closely on the blueprints they generate. An in-depth analysis is conducted\non why this is challenging."}
{"id": "2503.22986", "pdf": "https://arxiv.org/pdf/2503.22986", "abs": "https://arxiv.org/abs/2503.22986", "authors": ["Yunsong Wang", "Tianxin Huang", "Hanlin Chen", "Gim Hee Lee"], "title": "FreeSplat++: Generalizable 3D Gaussian Splatting for Efficient Indoor Scene Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Recently, the integration of the efficient feed-forward scheme into 3D\nGaussian Splatting (3DGS) has been actively explored. However, most existing\nmethods focus on sparse view reconstruction of small regions and cannot produce\neligible whole-scene reconstruction results in terms of either quality or\nefficiency. In this paper, we propose FreeSplat++, which focuses on extending\nthe generalizable 3DGS to become an alternative approach to large-scale indoor\nwhole-scene reconstruction, which has the potential of significantly\naccelerating the reconstruction speed and improving the geometric accuracy. To\nfacilitate whole-scene reconstruction, we initially propose the Low-cost\nCross-View Aggregation framework to efficiently process extremely long input\nsequences. Subsequently, we introduce a carefully designed pixel-wise triplet\nfusion method to incrementally aggregate the overlapping 3D Gaussian primitives\nfrom multiple views, adaptively reducing their redundancy. Furthermore, we\npropose a weighted floater removal strategy that can effectively reduce\nfloaters, which serves as an explicit depth fusion approach that is crucial in\nwhole-scene reconstruction. After the feed-forward reconstruction of 3DGS\nprimitives, we investigate a depth-regularized per-scene fine-tuning process.\nLeveraging the dense, multi-view consistent depth maps obtained during the\nfeed-forward prediction phase for an extra constraint, we refine the entire\nscene's 3DGS primitive to enhance rendering quality while preserving geometric\naccuracy. Extensive experiments confirm that our FreeSplat++ significantly\noutperforms existing generalizable 3DGS methods, especially in whole-scene\nreconstructions. Compared to conventional per-scene optimized 3DGS approaches,\nour method with depth-regularized per-scene fine-tuning demonstrates\nsubstantial improvements in reconstruction accuracy and a notable reduction in\ntraining time."}
{"id": "2503.23205", "pdf": "https://arxiv.org/pdf/2503.23205", "abs": "https://arxiv.org/abs/2503.23205", "authors": ["Jianfang Chen", "Kai Zhang", "Aoran Gan", "Shiwei Tong", "Shuanghong Shen", "Qi Liu"], "title": "Enhancing Knowledge Graph Completion with Entity Neighborhood and Relation Context", "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": null, "summary": "Knowledge Graph Completion (KGC) aims to infer missing information in\nKnowledge Graphs (KGs) to address their inherent incompleteness. Traditional\nstructure-based KGC methods, while effective, face significant computational\ndemands and scalability challenges due to the need for dense embedding learning\nand scoring all entities in the KG for each prediction. Recent text-based\napproaches using language models like T5 and BERT have mitigated these issues\nby converting KG triples into text for reasoning. However, they often fail to\nfully utilize contextual information, focusing mainly on the neighborhood of\nthe entity and neglecting the context of the relation. To address this issue,\nwe propose KGC-ERC, a framework that integrates both types of context to enrich\nthe input of generative language models and enhance their reasoning\ncapabilities. Additionally, we introduce a sampling strategy to effectively\nselect relevant context within input token constraints, which optimizes the\nutilization of contextual information and potentially improves model\nperformance. Experiments on the Wikidata5M, Wiki27K, and FB15K-237-N datasets\nshow that KGC-ERC outperforms or matches state-of-the-art baselines in\npredictive performance and scalability."}
{"id": "2503.23011", "pdf": "https://arxiv.org/pdf/2503.23011", "abs": "https://arxiv.org/abs/2503.23011", "authors": ["Hoigi Seo", "Junseo Bang", "Haechang Lee", "Joohoon Lee", "Byung Hyun Lee", "Se Young Chun"], "title": "On Geometrical Properties of Text Token Embeddings for Strong Semantic Binding in Text-to-Image Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-Image (T2I) models often suffer from text-image misalignment in\ncomplex scenes involving multiple objects and attributes. Semantic binding aims\nto mitigate this issue by accurately associating the generated attributes and\nobjects with their corresponding noun phrases (NPs). Existing methods rely on\ntext or latent optimizations, yet the factors influencing semantic binding\nremain underexplored. Here we investigate the geometrical properties of text\ntoken embeddings and their cross-attention (CA) maps. We empirically and\ntheoretically analyze that the geometrical properties of token embeddings,\nspecifically both angular distances and norms, play a crucial role in CA map\ndifferentiation. Then, we propose \\textbf{TeeMo}, a training-free text\nembedding-aware T2I framework with strong semantic binding. TeeMo consists of\nCausality-Aware Projection-Out (CAPO) for distinct inter-NP CA maps and\nAdaptive Token Mixing (ATM) with our loss to enhance inter-NP separation while\nmaintaining intra-NP cohesion in CA maps. Extensive experiments confirm TeeMo\nconsistently outperforms prior arts across diverse baselines and datasets."}
{"id": "2503.23213", "pdf": "https://arxiv.org/pdf/2503.23213", "abs": "https://arxiv.org/abs/2503.23213", "authors": ["Diana Bolanos", "Mohammadmehdi Ataei", "Daniele Grandi", "Kosa Goucher-Lambert"], "title": "RECALL-MM: A Multimodal Dataset of Consumer Product Recalls for Risk Analysis using Computational Methods and Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Product recalls provide valuable insights into potential risks and hazards\nwithin the engineering design process, yet their full potential remains\nunderutilized. In this study, we curate data from the United States Consumer\nProduct Safety Commission (CPSC) recalls database to develop a multimodal\ndataset, RECALL-MM, that informs data-driven risk assessment using historical\ninformation, and augment it using generative methods. Patterns in the dataset\nhighlight specific areas where improved safety measures could have significant\nimpact. We extend our analysis by demonstrating interactive clustering maps\nthat embed all recalls into a shared latent space based on recall descriptions\nand product names. Leveraging these data-driven tools, we explore three case\nstudies to demonstrate the dataset's utility in identifying product risks and\nguiding safer design decisions. The first two case studies illustrate how\ndesigners can visualize patterns across recalled products and situate new\nproduct ideas within the broader recall landscape to proactively anticipate\nhazards. In the third case study, we extend our approach by employing a large\nlanguage model (LLM) to predict potential hazards based solely on product\nimages. This demonstrates the model's ability to leverage visual context to\nidentify risk factors, revealing strong alignment with historical recall data\nacross many hazard categories. However, the analysis also highlights areas\nwhere hazard prediction remains challenging, underscoring the importance of\nrisk awareness throughout the design process. Collectively, this work aims to\nbridge the gap between historical recall data and future product safety,\npresenting a scalable, data-driven approach to safer engineering design."}
{"id": "2503.23012", "pdf": "https://arxiv.org/pdf/2503.23012", "abs": "https://arxiv.org/abs/2503.23012", "authors": ["Xinlei Shao", "Hongruixuan Chen", "Fan Zhao", "Kirsty Magson", "Jundong Chen", "Peiran Li", "Jiaqi Wang", "Jun Sasaki"], "title": "Multi-label classification for multi-temporal, multi-spatial coral reef condition monitoring using vision foundation model with adapter learning", "categories": ["cs.CV"], "comment": null, "summary": "Coral reef ecosystems provide essential ecosystem services, but face\nsignificant threats from climate change and human activities. Although advances\nin deep learning have enabled automatic classification of coral reef\nconditions, conventional deep models struggle to achieve high performance when\nprocessing complex underwater ecological images. Vision foundation models,\nknown for their high accuracy and cross-domain generalizability, offer\npromising solutions. However, fine-tuning these models requires substantial\ncomputational resources and results in high carbon emissions. To address these\nchallenges, adapter learning methods such as Low-Rank Adaptation (LoRA) have\nemerged as a solution. This study introduces an approach integrating the DINOv2\nvision foundation model with the LoRA fine-tuning method. The approach\nleverages multi-temporal field images collected through underwater surveys at\n15 dive sites at Koh Tao, Thailand, with all images labeled according to\nuniversal standards used in citizen science-based conservation programs. The\nexperimental results demonstrate that the DINOv2-LoRA model achieved superior\naccuracy, with a match ratio of 64.77%, compared to 60.34% achieved by the best\nconventional model. Furthermore, incorporating LoRA reduced the trainable\nparameters from 1,100M to 5.91M. Transfer learning experiments conducted under\ndifferent temporal and spatial settings highlight the exceptional\ngeneralizability of DINOv2-LoRA across different seasons and sites. This study\nis the first to explore the efficient adaptation of foundation models for\nmulti-label classification of coral reef conditions under multi-temporal and\nmulti-spatial settings. The proposed method advances the classification of\ncoral reef conditions and provides a tool for monitoring, conserving, and\nmanaging coral reef ecosystems."}
{"id": "2503.23242", "pdf": "https://arxiv.org/pdf/2503.23242", "abs": "https://arxiv.org/abs/2503.23242", "authors": ["Dominik Macko", "Aashish Anantha Ramakrishnan", "Jason Samuel Lucas", "Robert Moro", "Ivan Srba", "Adaku Uchendu", "Dongwon Lee"], "title": "Beyond speculation: Measuring the growing presence of LLM-generated texts in multilingual disinformation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Increased sophistication of large language models (LLMs) and the consequent\nquality of generated multilingual text raises concerns about potential\ndisinformation misuse. While humans struggle to distinguish LLM-generated\ncontent from human-written texts, the scholarly debate about their impact\nremains divided. Some argue that heightened fears are overblown due to natural\necosystem limitations, while others contend that specific \"longtail\" contexts\nface overlooked risks. Our study bridges this debate by providing the first\nempirical evidence of LLM presence in the latest real-world disinformation\ndatasets, documenting the increase of machine-generated content following\nChatGPT's release, and revealing crucial patterns across languages, platforms,\nand time periods."}
{"id": "2503.23021", "pdf": "https://arxiv.org/pdf/2503.23021", "abs": "https://arxiv.org/abs/2503.23021", "authors": ["Sol Erika Boman", "Nita Mulliqi", "Anders Blilie", "Xiaoyi Ji", "Kelvin Szolnoky", "Einar Gudlaugsson", "Emiel A. M. Janssen", "Svein R. Kjosavik", "José Asenjo", "Marcello Gambacorta", "Paolo Libretti", "Marcin Braun", "Radzislaw Kordek", "Roman Łowicki", "Kristina Hotakainen", "Päivi Väre", "Bodil Ginnerup Pedersen", "Karina Dalsgaard Sørensen", "Benedicte Parm Ulhøi", "Lars Egevad", "Kimmo Kartasalo"], "title": "The impact of tissue detection on diagnostic artificial intelligence algorithms in digital pathology", "categories": ["cs.CV"], "comment": "25 pages, 2 tables, 3 figures, 1 supplementary figure", "summary": "Tissue detection is a crucial first step in most digital pathology\napplications. Details of the segmentation algorithm are rarely reported, and\nthere is a lack of studies investigating the downstream effects of a poor\nsegmentation algorithm. Disregarding tissue detection quality could create a\nbottleneck for downstream performance and jeopardize patient safety if\ndiagnostically relevant parts of the specimen are excluded from analysis in\nclinical applications.\n  This study aims to determine whether performance of downstream tasks is\nsensitive to the tissue detection method, and to compare performance of\nclassical and AI-based tissue detection. To this end, we trained an AI model\nfor Gleason grading of prostate cancer in whole slide images (WSIs) using two\ndifferent tissue detection algorithms: thresholding (classical) and UNet++\n(AI). A total of 33,823 WSIs scanned on five digital pathology scanners were\nused to train the tissue detection AI model. The downstream Gleason grading\nalgorithm was trained and tested using 70,524 WSIs from 13 clinical sites\nscanned on 13 different scanners.\n  There was a decrease from 116 (0.43%) to 22 (0.08%) fully undetected tissue\nsamples when switching from thresholding-based tissue detection to AI-based,\nsuggesting an AI model may be more reliable than a classical model for avoiding\ntotal failures on slides with unusual appearance. On the slides where tissue\ncould be detected by both algorithms, no significant difference in overall\nGleason grading performance was observed. However, tissue detection dependent\nclinically significant variations in AI grading were observed in 3.5% of\nmalignant slides, highlighting the importance of robust tissue detection for\noptimal clinical performance of diagnostic AI."}
{"id": "2503.23243", "pdf": "https://arxiv.org/pdf/2503.23243", "abs": "https://arxiv.org/abs/2503.23243", "authors": ["Megan A. Brown", "Shubham Atreja", "Libby Hemphill", "Patrick Y. Wu"], "title": "Evaluating how LLM annotations represent diverse views on contentious topics", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Researchers have proposed the use of generative large language models (LLMs)\nto label data for both research and applied settings. This literature\nemphasizes the improved performance of LLMs relative to other natural language\nmodels, noting that LLMs typically outperform other models on standard metrics\nsuch as accuracy, precision, recall, and F1 score. However, previous literature\nhas also highlighted the bias embedded in language models, particularly around\ncontentious topics such as potentially toxic content. This bias could result in\nlabels applied by LLMs that disproportionately align with majority groups over\na more diverse set of viewpoints. In this paper, we evaluate how LLMs represent\ndiverse viewpoints on these contentious tasks. Across four annotation tasks on\nfour datasets, we show that LLMs do not show substantial disagreement with\nannotators on the basis of demographics. Instead, the model, prompt, and\ndisagreement between human annotators on the labeling task are far more\npredictive of LLM agreement. Our findings suggest that when using LLMs to\nannotate data, under-representing the views of particular groups is not a\nsubstantial concern. We conclude with a discussion of the implications for\nresearchers and practitioners."}
{"id": "2503.23022", "pdf": "https://arxiv.org/pdf/2503.23022", "abs": "https://arxiv.org/abs/2503.23022", "authors": ["Xianglong He", "Junyi Chen", "Di Huang", "Zexiang Liu", "Xiaoshui Huang", "Wanli Ouyang", "Chun Yuan", "Yangguang Li"], "title": "MeshCraft: Exploring Efficient and Controllable Mesh Generation with Flow-based DiTs", "categories": ["cs.CV"], "comment": null, "summary": "In the domain of 3D content creation, achieving optimal mesh topology through\nAI models has long been a pursuit for 3D artists. Previous methods, such as\nMeshGPT, have explored the generation of ready-to-use 3D objects via mesh\nauto-regressive techniques. While these methods produce visually impressive\nresults, their reliance on token-by-token predictions in the auto-regressive\nprocess leads to several significant limitations. These include extremely slow\ngeneration speeds and an uncontrollable number of mesh faces. In this paper, we\nintroduce MeshCraft, a novel framework for efficient and controllable mesh\ngeneration, which leverages continuous spatial diffusion to generate discrete\ntriangle faces. Specifically, MeshCraft consists of two core components: 1) a\ntransformer-based VAE that encodes raw meshes into continuous face-level tokens\nand decodes them back to the original meshes, and 2) a flow-based diffusion\ntransformer conditioned on the number of faces, enabling the generation of\nhigh-quality 3D meshes with a predefined number of faces. By utilizing the\ndiffusion model for the simultaneous generation of the entire mesh topology,\nMeshCraft achieves high-fidelity mesh generation at significantly faster speeds\ncompared to auto-regressive methods. Specifically, MeshCraft can generate an\n800-face mesh in just 3.2 seconds (35$\\times$ faster than existing baselines).\nExtensive experiments demonstrate that MeshCraft outperforms state-of-the-art\ntechniques in both qualitative and quantitative evaluations on ShapeNet dataset\nand demonstrates superior performance on Objaverse dataset. Moreover, it\nintegrates seamlessly with existing conditional guidance strategies, showcasing\nits potential to relieve artists from the time-consuming manual work involved\nin mesh creation."}
{"id": "2503.23274", "pdf": "https://arxiv.org/pdf/2503.23274", "abs": "https://arxiv.org/abs/2503.23274", "authors": ["Weisheng Jin", "Maojia Song", "Tej Deep Pala", "Yew Ken Chia", "Amir Zadeh", "Chuan Li", "Soujanya Poria"], "title": "PromptDistill: Query-based Selective Token Retention in Intermediate Layers for Efficient Large Language Model Inference", "categories": ["cs.CL"], "comment": null, "summary": "As large language models (LLMs) tackle increasingly complex tasks and longer\ndocuments, their computational and memory costs during inference become a major\nbottleneck. To address this, we propose PromptDistill, a novel, training-free\nmethod that improves inference efficiency while preserving generation quality.\nPromptDistill identifies and retains the most informative tokens by leveraging\nattention interactions in early layers, preserving their hidden states while\nreducing the computational burden in later layers. This allows the model to\nfocus on essential contextual information without fully processing all tokens.\nUnlike previous methods such as H2O and SnapKV, which perform compression only\nafter processing the entire input, or GemFilter, which selects a fixed portion\nof the initial prompt without considering contextual dependencies,\nPromptDistill dynamically allocates computational resources to the most\nrelevant tokens while maintaining a global awareness of the input. Experiments\nusing our method and baseline approaches with base models such as LLaMA 3.1 8B\nInstruct, Phi 3.5 Mini Instruct, and Qwen2 7B Instruct on benchmarks including\nLongBench, InfBench, and Needle in a Haystack demonstrate that PromptDistill\nsignificantly improves efficiency while having minimal impact on output quality\ncompared to the original models. With a single-stage selection strategy,\nPromptDistill effectively balances performance and efficiency, outperforming\nprior methods like GemFilter, H2O, and SnapKV due to its superior ability to\nretain essential information. Specifically, compared to GemFilter,\nPromptDistill achieves an overall $1\\%$ to $5\\%$ performance improvement while\nalso offering better time efficiency. Additionally, we explore multi-stage\nselection, which further improves efficiency while maintaining strong\ngeneration performance."}
{"id": "2503.23024", "pdf": "https://arxiv.org/pdf/2503.23024", "abs": "https://arxiv.org/abs/2503.23024", "authors": ["Zhihao Yuan", "Yibo Peng", "Jinke Ren", "Yinghong Liao", "Yatong Han", "Chun-Mei Feng", "Hengshuang Zhao", "Guanbin Li", "Shuguang Cui", "Zhen Li"], "title": "Empowering Large Language Models with 3D Situation Awareness", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Driven by the great success of Large Language Models (LLMs) in the 2D image\ndomain, their applications in 3D scene understanding has emerged as a new\ntrend. A key difference between 3D and 2D is that the situation of an\negocentric observer in 3D scenes can change, resulting in different\ndescriptions (e.g., ''left\" or ''right\"). However, current LLM-based methods\noverlook the egocentric perspective and simply use datasets from a global\nviewpoint. To address this issue, we propose a novel approach to automatically\ngenerate a situation-aware dataset by leveraging the scanning trajectory during\ndata collection and utilizing Vision-Language Models (VLMs) to produce\nhigh-quality captions and question-answer pairs. Furthermore, we introduce a\nsituation grounding module to explicitly predict the position and orientation\nof observer's viewpoint, thereby enabling LLMs to ground situation description\nin 3D scenes. We evaluate our approach on several benchmarks, demonstrating\nthat our method effectively enhances the 3D situational awareness of LLMs while\nsignificantly expanding existing datasets and reducing manual effort."}
{"id": "2503.23281", "pdf": "https://arxiv.org/pdf/2503.23281", "abs": "https://arxiv.org/abs/2503.23281", "authors": ["Hieu Nghiem", "Tuan-Dung Le", "Suhao Chen", "Thanh Thieu", "Andrew Gin", "Ellie Phuong Nguyen", "Dursun Delen", "Johnson Thomas", "Jivan Lamichhane", "Zhuqi Miao"], "title": "Extracting Patient History from Clinical Text: A Comparative Study of Clinical Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Extracting medical history entities (MHEs) related to a patient's chief\ncomplaint (CC), history of present illness (HPI), and past, family, and social\nhistory (PFSH) helps structure free-text clinical notes into standardized EHRs,\nstreamlining downstream tasks like continuity of care, medical coding, and\nquality metrics. Fine-tuned clinical large language models (cLLMs) can assist\nin this process while ensuring the protection of sensitive data via on-premises\ndeployment. This study evaluates the performance of cLLMs in recognizing\nCC/HPI/PFSH-related MHEs and examines how note characteristics impact model\naccuracy. We annotated 1,449 MHEs across 61 outpatient-related clinical notes\nfrom the MTSamples repository. To recognize these entities, we fine-tuned seven\nstate-of-the-art cLLMs. Additionally, we assessed the models' performance when\nenhanced by integrating, problems, tests, treatments, and other basic medical\nentities (BMEs). We compared the performance of these models against GPT-4o in\na zero-shot setting. To further understand the textual characteristics\naffecting model accuracy, we conducted an error analysis focused on note\nlength, entity length, and segmentation. The cLLMs showed potential in reducing\nthe time required for extracting MHEs by over 20%. However, detecting many\ntypes of MHEs remained challenging due to their polysemous nature and the\nfrequent involvement of non-medical vocabulary. Fine-tuned GatorTron and\nGatorTronS, two of the most extensively trained cLLMs, demonstrated the highest\nperformance. Integrating pre-identified BME information improved model\nperformance for certain entities. Regarding the impact of textual\ncharacteristics on model performance, we found that longer entities were harder\nto identify, note length did not correlate with a higher error rate, and\nwell-organized segments with headings are beneficial for the extraction."}
{"id": "2503.23030", "pdf": "https://arxiv.org/pdf/2503.23030", "abs": "https://arxiv.org/abs/2503.23030", "authors": ["Huajie Jiang", "Zhengxian Li", "Xiaohan Yu", "Yongli Hu", "Baocai Yin", "Jian Yang", "Yuankai Qi"], "title": "Visual and Semantic Prompt Collaboration for Generalized Zero-Shot Learning", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Generalized zero-shot learning aims to recognize both seen and unseen classes\nwith the help of semantic information that is shared among different classes.\nIt inevitably requires consistent visual-semantic alignment. Existing\napproaches fine-tune the visual backbone by seen-class data to obtain\nsemantic-related visual features, which may cause overfitting on seen classes\nwith a limited number of training images. This paper proposes a novel visual\nand semantic prompt collaboration framework, which utilizes prompt tuning\ntechniques for efficient feature adaptation. Specifically, we design a visual\nprompt to integrate the visual information for discriminative feature learning\nand a semantic prompt to integrate the semantic formation for visualsemantic\nalignment. To achieve effective prompt information integration, we further\ndesign a weak prompt fusion mechanism for the shallow layers and a strong\nprompt fusion mechanism for the deep layers in the network. Through the\ncollaboration of visual and semantic prompts, we can obtain discriminative\nsemantic-related features for generalized zero-shot image recognition.\nExtensive experiments demonstrate that our framework consistently achieves\nfavorable performance in both conventional zero-shot learning and generalized\nzero-shot learning benchmarks compared to other state-of-the-art methods."}
{"id": "2503.23294", "pdf": "https://arxiv.org/pdf/2503.23294", "abs": "https://arxiv.org/abs/2503.23294", "authors": ["Wei Tao", "Bin Zhang", "Xiaoyang Qu", "Jiguang Wan", "Jianzong Wang"], "title": "Cocktail: Chunk-Adaptive Mixed-Precision Quantization for Long-Context LLM Inference", "categories": ["cs.CL"], "comment": "Accepted by the Design, Automation, and Test in Europe 2025 (DATE\n  2025)", "summary": "Recently, large language models (LLMs) have been able to handle longer and\nlonger contexts. However, a context that is too long may cause intolerant\ninference latency and GPU memory usage. Existing methods propose\nmixed-precision quantization to the key-value (KV) cache in LLMs based on token\ngranularity, which is time-consuming in the search process and hardware\ninefficient during computation. This paper introduces a novel approach called\nCocktail, which employs chunk-adaptive mixed-precision quantization to optimize\nthe KV cache. Cocktail consists of two modules: chunk-level quantization search\nand chunk-level KV cache computation. Chunk-level quantization search\ndetermines the optimal bitwidth configuration of the KV cache chunks quickly\nbased on the similarity scores between the corresponding context chunks and the\nquery, maintaining the model accuracy. Furthermore, chunk-level KV cache\ncomputation reorders the KV cache chunks before quantization, avoiding the\nhardware inefficiency caused by mixed-precision quantization in inference\ncomputation. Extensive experiments demonstrate that Cocktail outperforms\nstate-of-the-art KV cache quantization methods on various models and datasets."}
{"id": "2503.23035", "pdf": "https://arxiv.org/pdf/2503.23035", "abs": "https://arxiv.org/abs/2503.23035", "authors": ["Yuxiang Bao", "Huijie Liu", "Xun Gao", "Huan Fu", "Guoliang Kang"], "title": "FreeInv: Free Lunch for Improving DDIM Inversion", "categories": ["cs.CV"], "comment": null, "summary": "Naive DDIM inversion process usually suffers from a trajectory deviation\nissue, i.e., the latent trajectory during reconstruction deviates from the one\nduring inversion. To alleviate this issue, previous methods either learn to\nmitigate the deviation or design cumbersome compensation strategy to reduce the\nmismatch error, exhibiting substantial time and computation cost. In this work,\nwe present a nearly free-lunch method (named FreeInv) to address the issue more\neffectively and efficiently. In FreeInv, we randomly transform the latent\nrepresentation and keep the transformation the same between the corresponding\ninversion and reconstruction time-step. It is motivated from a statistical\nperspective that an ensemble of DDIM inversion processes for multiple\ntrajectories yields a smaller trajectory mismatch error on expectation.\nMoreover, through theoretical analysis and empirical study, we show that\nFreeInv performs an efficient ensemble of multiple trajectories. FreeInv can be\nfreely integrated into existing inversion-based image and video editing\ntechniques. Especially for inverting video sequences, it brings more\nsignificant fidelity and efficiency improvements. Comprehensive quantitative\nand qualitative evaluation on PIE benchmark and DAVIS dataset shows that\nFreeInv remarkably outperforms conventional DDIM inversion, and is competitive\namong previous state-of-the-art inversion methods, with superior computation\nefficiency."}
{"id": "2503.23295", "pdf": "https://arxiv.org/pdf/2503.23295", "abs": "https://arxiv.org/abs/2503.23295", "authors": ["Mikhail Krasitskii", "Olga Kolesnikova", "Liliana Chanona Hernandez", "Grigori Sidorov", "Alexander Gelbukh"], "title": "Advancing Sentiment Analysis in Tamil-English Code-Mixed Texts: Challenges and Transformer-Based Solutions", "categories": ["cs.CL"], "comment": null, "summary": "The sentiment analysis task in Tamil-English code-mixed texts has been\nexplored using advanced transformer-based models. Challenges from grammatical\ninconsistencies, orthographic variations, and phonetic ambiguities have been\naddressed. The limitations of existing datasets and annotation gaps have been\nexamined, emphasizing the need for larger and more diverse corpora. Transformer\narchitectures, including XLM-RoBERTa, mT5, IndicBERT, and RemBERT, have been\nevaluated in low-resource, code-mixed environments. Performance metrics have\nbeen analyzed, highlighting the effectiveness of specific models in handling\nmultilingual sentiment classification. The findings suggest that further\nadvancements in data augmentation, phonetic normalization, and hybrid modeling\napproaches are required to enhance accuracy. Future research directions for\nimproving sentiment analysis in code-mixed texts have been proposed."}
{"id": "2503.23039", "pdf": "https://arxiv.org/pdf/2503.23039", "abs": "https://arxiv.org/abs/2503.23039", "authors": ["Zijun Ding", "Mingdie Xiong", "Congcong Zhu", "Jingrun Chen"], "title": "STSA: Spatial-Temporal Semantic Alignment for Visual Dubbing", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ICME 2025", "summary": "Existing audio-driven visual dubbing methods have achieved great success.\nDespite this, we observe that the semantic ambiguity between spatial and\ntemporal domains significantly degrades the synthesis stability for the dynamic\nfaces. We argue that aligning the semantic features from spatial and temporal\ndomains is a promising approach to stabilizing facial motion. To achieve this,\nwe propose a Spatial-Temporal Semantic Alignment (STSA) method, which\nintroduces a dual-path alignment mechanism and a differentiable semantic\nrepresentation. The former leverages a Consistent Information Learning (CIL)\nmodule to maximize the mutual information at multiple scales, thereby reducing\nthe manifold differences between spatial and temporal domains. The latter\nutilizes probabilistic heatmap as ambiguity-tolerant guidance to avoid the\nabnormal dynamics of the synthesized faces caused by slight semantic jittering.\nExtensive experimental results demonstrate the superiority of the proposed\nSTSA, especially in terms of image quality and synthesis stability. Pre-trained\nweights and inference code are available at\nhttps://github.com/SCAILab-USTC/STSA."}
{"id": "2503.23305", "pdf": "https://arxiv.org/pdf/2503.23305", "abs": "https://arxiv.org/abs/2503.23305", "authors": ["Kenneth J. Sible", "David Chiang"], "title": "Using Source-Side Confidence Estimation for Reliable Translation into Unfamiliar Languages", "categories": ["cs.CL", "cs.LG"], "comment": "7 pages, 5 figures, 1 table. Submitted to ACL 2025 System\n  Demonstrations", "summary": "We present an interactive machine translation (MT) system designed for users\nwho are not proficient in the target language. It aims to improve\ntrustworthiness and explainability by identifying potentially mistranslated\nwords and allowing the user to intervene to correct mistranslations. However,\nconfidence estimation in machine translation has traditionally focused on the\ntarget side. Whereas the conventional approach to source-side confidence\nestimation would have been to project target word probabilities to the source\nside via word alignments, we propose a direct, alignment-free approach that\nmeasures how sensitive the target word probabilities are to changes in the\nsource embeddings. Experimental results show that our method outperforms\ntraditional alignment-based methods at detection of mistranslations."}
{"id": "2503.23044", "pdf": "https://arxiv.org/pdf/2503.23044", "abs": "https://arxiv.org/abs/2503.23044", "authors": ["Yuanyuan Gao", "Hao Li", "Jiaqi Chen", "Zhengyu Zou", "Zhihang Zhong", "Dingwen Zhang", "Xiao Sun", "Junwei Han"], "title": "CityGS-X: A Scalable Architecture for Efficient and Geometrically Accurate Large-Scale Scene Reconstruction", "categories": ["cs.CV"], "comment": "Project page: https://lifuguan.github.io/CityGS-X/", "summary": "Despite its significant achievements in large-scale scene reconstruction, 3D\nGaussian Splatting still faces substantial challenges, including slow\nprocessing, high computational costs, and limited geometric accuracy. These\ncore issues arise from its inherently unstructured design and the absence of\nefficient parallelization. To overcome these challenges simultaneously, we\nintroduce CityGS-X, a scalable architecture built on a novel parallelized\nhybrid hierarchical 3D representation (PH^2-3D). As an early attempt, CityGS-X\nabandons the cumbersome merge-and-partition process and instead adopts a\nnewly-designed batch-level multi-task rendering process. This architecture\nenables efficient multi-GPU rendering through dynamic Level-of-Detail voxel\nallocations, significantly improving scalability and performance. Through\nextensive experiments, CityGS-X consistently outperforms existing methods in\nterms of faster training times, larger rendering capacities, and more accurate\ngeometric details in large-scale scenes. Notably, CityGS-X can train and render\na scene with 5,000+ images in just 5 hours using only 4 * 4090 GPUs, a task\nthat would make other alternative methods encounter Out-Of-Memory (OOM) issues\nand fail completely. This implies that CityGS-X is far beyond the capacity of\nother existing methods."}
{"id": "2503.23306", "pdf": "https://arxiv.org/pdf/2503.23306", "abs": "https://arxiv.org/abs/2503.23306", "authors": ["Youxiang Zhu", "Ruochen Li", "Danqing Wang", "Daniel Haehn", "Xiaohui Liang"], "title": "Focus Directions Make Your Language Models Pay More Attention to Relevant Contexts", "categories": ["cs.CL"], "comment": null, "summary": "Long-context large language models (LLMs) are prone to be distracted by\nirrelevant contexts. The reason for distraction remains poorly understood. In\nthis paper, we first identify the contextual heads, a special group of\nattention heads that control the overall attention of the LLM. Then, we\ndemonstrate that distraction arises when contextual heads fail to allocate\nsufficient attention to relevant contexts and can be mitigated by increasing\nattention to these contexts. We further identify focus directions, located at\nthe key and query activations of these heads, which enable them to allocate\nmore attention to relevant contexts without explicitly specifying which context\nis relevant. We comprehensively evaluate the effect of focus direction on\nvarious long-context tasks and find out focus directions could help to mitigate\nthe poor task alignment of the long-context LLMs. We believe our findings could\npromote further research on long-context LLM alignment."}
{"id": "2503.23062", "pdf": "https://arxiv.org/pdf/2503.23062", "abs": "https://arxiv.org/abs/2503.23062", "authors": ["Sagi Eppel", "Mor Bismut", "Alona Faktor"], "title": "Shape and Texture Recognition in Large Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Shape and texture recognition is fundamental to visual perception. The\nability to identify shapes regardless of orientation, texture, or context, and\nto recognize textures independently of their associated objects, is essential\nfor general visual understanding of the world. We introduce the Large Shape &\nTextures dataset (LAS&T), a giant collection of diverse shapes and textures\nautomatically extracted from real-world images. This dataset is used to\nevaluate how effectively leading Large Vision-Language Models (LVLMs)\nunderstand shapes, textures, and materials in both 2D and 3D scenes. For shape\nrecognition, we test models' ability to match identical shapes that differ in\norientation, texture, color, or environment. Our results show that LVLMs' shape\nidentification capabilities remain significantly below human performance.\nSingle alterations (orientation, texture) cause minor decreases in matching\naccuracy, while multiple changes precipitate dramatic drops. LVLMs appear to\nrely predominantly on high-level and semantic features and struggle with\nabstract shapes lacking clear class associations. For texture and material\nrecognition, we evaluate models' ability to identify identical textures and\nmaterials across different objects and environments. Interestingly, leading\nLVLMs approach human-level performance in recognizing materials in 3D scenes,\nyet substantially underperform humans when identifying simpler 2D textures. The\nLAS&T dataset and benchmark, the largest and most diverse resource for shape\nand texture evaluation, is freely available with generation and testing\nscripts."}
{"id": "2503.23311", "pdf": "https://arxiv.org/pdf/2503.23311", "abs": "https://arxiv.org/abs/2503.23311", "authors": ["Daniele Corradetti", "Alessio Marrani"], "title": "Linguistic Loops and Geometric Invariants as a Way to Pre-Verbal Thought?", "categories": ["cs.CL"], "comment": "10 pages", "summary": "In this work we introduce the concepts of linguistic transformation,\nlinguistic loop and semantic deficit. By exploiting Lie group theoretical and\ngeometric techniques, we define invariants that capture the structural\nproperties of a whole linguistic loop. This result introduces new line of\nresearch, employing tools from Lie theory and higher-dimensional geometry\nwithin language studies. But, even more intriguingly, our study hints to a\nmathematical characterization of the meta-linguistic or pre-verbal thought,\nnamely of those cognitive structures that precede the language."}
{"id": "2503.23064", "pdf": "https://arxiv.org/pdf/2503.23064", "abs": "https://arxiv.org/abs/2503.23064", "authors": ["Yufan Ren", "Konstantinos Tertikas", "Shalini Maiti", "Junlin Han", "Tong Zhang", "Sabine Süsstrunk", "Filippos Kokkinos"], "title": "VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models", "categories": ["cs.CV"], "comment": "8 pages", "summary": "Large Vision-Language Models (LVLMs) struggle with puzzles, which require\nprecise perception, rule comprehension, and logical reasoning. Assessing and\nenhancing their performance in this domain is crucial, as it reflects their\nability to engage in structured reasoning - an essential skill for real-world\nproblem-solving. However, existing benchmarks primarily evaluate pre-trained\nmodels without additional training or fine-tuning, often lack a dedicated focus\non reasoning, and fail to establish a systematic evaluation framework. To\naddress these limitations, we introduce VGRP-Bench, a Visual Grid Reasoning\nPuzzle Benchmark featuring 20 diverse puzzles. VGRP-Bench spans multiple\ndifficulty levels, and includes extensive experiments not only on existing chat\nLVLMs (e.g., GPT-4o), but also on reasoning LVLMs (e.g., Gemini-Thinking). Our\nresults reveal that even the state-of-the-art LVLMs struggle with these\npuzzles, highlighting fundamental limitations in their puzzle-solving\ncapabilities. Most importantly, through systematic experiments, we identify and\nanalyze key factors influencing LVLMs' puzzle-solving performance, including\nthe number of clues, grid size, and rule complexity. Furthermore, we explore\ntwo Supervised Fine-Tuning (SFT) strategies that can be used in post-training:\nSFT on solutions (S-SFT) and SFT on synthetic reasoning processes (R-SFT).\nWhile both methods significantly improve performance on trained puzzles, they\nexhibit limited generalization to unseen ones. We will release VGRP-Bench to\nfacilitate further research on LVLMs for complex, real-world problem-solving."}
{"id": "2503.23360", "pdf": "https://arxiv.org/pdf/2503.23360", "abs": "https://arxiv.org/abs/2503.23360", "authors": ["Guanhua Chen", "Yutong Yao", "Ci-Jun Gao", "Lidia S. Chao", "Feng Wan", "Derek F. Wong"], "title": "Not All LoRA Parameters Are Essential: Insights on Inference Necessity", "categories": ["cs.CL"], "comment": null, "summary": "Current research on LoRA primarily focuses on minimizing the number of\nfine-tuned parameters or optimizing its architecture. However, the necessity of\nall fine-tuned LoRA layers during inference remains underexplored. In this\npaper, we investigate the contribution of each LoRA layer to the model's\nability to predict the ground truth and hypothesize that lower-layer LoRA\nmodules play a more critical role in model reasoning and understanding. To\naddress this, we propose a simple yet effective method to enhance the\nperformance of large language models (LLMs) fine-tuned with LoRA. Specifically,\nwe identify a ``boundary layer'' that distinguishes essential LoRA layers by\nanalyzing a small set of validation samples. During inference, we drop all LoRA\nlayers beyond this boundary. We evaluate our approach on three strong baselines\nacross four widely-used text generation datasets. Our results demonstrate\nconsistent and significant improvements, underscoring the effectiveness of\nselectively retaining critical LoRA layers during inference."}
{"id": "2503.23081", "pdf": "https://arxiv.org/pdf/2503.23081", "abs": "https://arxiv.org/abs/2503.23081", "authors": ["Anastasiia Fadeeva", "Vincent Coriou", "Diego Antognini", "Claudiu Musat", "Andrii Maksai"], "title": "InkFM: A Foundational Model for Full-Page Online Handwritten Note Understanding", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Tablets and styluses are increasingly popular for taking notes. To optimize\nthis experience and ensure a smooth and efficient workflow, it's important to\ndevelop methods for accurately interpreting and understanding the content of\nhandwritten digital notes. We introduce a foundational model called InkFM for\nanalyzing full pages of handwritten content. Trained on a diverse mixture of\ntasks, this model offers a unique combination of capabilities: recognizing text\nin 28 different scripts, mathematical expressions recognition, and segmenting\npages into distinct elements like text and drawings. Our results demonstrate\nthat these tasks can be effectively unified within a single model, achieving\nSoTA text line segmentation out-of-the-box quality surpassing public baselines\nlike docTR. Fine- or LoRA-tuning our base model on public datasets further\nimproves the quality of page segmentation, achieves state-of the art text\nrecognition (DeepWriting, CASIA, SCUT, and Mathwriting datasets) and sketch\nclassification (QuickDraw). This adaptability of InkFM provides a powerful\nstarting point for developing applications with handwritten input."}
{"id": "2503.23361", "pdf": "https://arxiv.org/pdf/2503.23361", "abs": "https://arxiv.org/abs/2503.23361", "authors": ["Linxin Song", "Xuwei Ding", "Jieyu Zhang", "Taiwei Shi", "Ryotaro Shimizu", "Rahul Gupta", "Yang Liu", "Jian Kang", "Jieyu Zhao"], "title": "Discovering Knowledge Deficiencies of Language Models on Massive Knowledge Base", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) possess impressive linguistic capabilities but\noften fail to faithfully retain factual knowledge, leading to hallucinations\nand unreliable outputs. Understanding LLMs' knowledge deficiencies by\nexhaustively evaluating against full-scale knowledge bases is computationally\nprohibitive, especially for closed-weight models. We propose stochastic error\nascent (SEA), a scalable and efficient framework for discovering knowledge\ndeficiencies (errors) in closed-weight LLMs under a strict query budget. Rather\nthan naively probing all knowledge candidates, SEA formulates error discovery\nas a stochastic optimization process: it iteratively retrieves new high-error\ncandidates by leveraging the semantic similarity to previously observed\nfailures. To further enhance search efficiency and coverage, SEA employs\nhierarchical retrieval across document and paragraph levels, and constructs a\nrelation directed acyclic graph to model error propagation and identify\nsystematic failure modes. Empirically, SEA uncovers 40.7x more knowledge errors\nthan Automated Capability Discovery and 26.7% more than AutoBencher, while\nreducing the cost-per-error by 599x and 9x, respectively. Human evaluation\nconfirms the high quality of generated questions, while ablation and\nconvergence analyses validate the contribution of each component in SEA.\nFurther analysis on the discovered errors reveals correlated failure patterns\nacross LLM families and recurring deficits, highlighting the need for better\ndata coverage and targeted fine-tuning in future LLM development."}
{"id": "2503.23083", "pdf": "https://arxiv.org/pdf/2503.23083", "abs": "https://arxiv.org/abs/2503.23083", "authors": ["Hasan Moughnieh", "Mohamad Chalhoub", "Hasan Nasrallah", "Cristiano Nattero", "Paolo Campanella", "Ali J. Ghandour"], "title": "Efficient Adaptation For Remote Sensing Visual Grounding", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Foundation models have revolutionized artificial intelligence (AI), offering\nremarkable capabilities across multi-modal domains. Their ability to precisely\nlocate objects in complex aerial and satellite images, using rich contextual\ninformation and detailed object descriptions, is essential for remote sensing\n(RS). These models can associate textual descriptions with object positions\nthrough the Visual Grounding (VG) task, but due to domain-specific challenges,\ntheir direct application to RS produces sub-optimal results. To address this,\nwe applied Parameter Efficient Fine Tuning (PEFT) techniques to adapt these\nmodels for RS-specific VG tasks. Specifically, we evaluated LoRA placement\nacross different modules in Grounding DINO and used BitFit and adapters to\nfine-tune the OFA foundation model pre-trained on general-purpose VG datasets.\nThis approach achieved performance comparable to or surpassing current State Of\nThe Art (SOTA) models while significantly reducing computational costs. This\nstudy highlights the potential of PEFT techniques to advance efficient and\nprecise multi-modal analysis in RS, offering a practical and cost-effective\nalternative to full model training."}
{"id": "2503.23362", "pdf": "https://arxiv.org/pdf/2503.23362", "abs": "https://arxiv.org/abs/2503.23362", "authors": ["Jia-Chen Zhang", "Yu-Jie Xiong", "Xi-He Qiu", "Chun-Ming Xia", "Fei Dai"], "title": "Mixture of Routers", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages,4 figures", "summary": "Supervised fine-tuning (SFT) is a milestone in aligning large language models\nwith human instructions and adapting them to downstream tasks. In particular,\nLow-Rank Adaptation (LoRA) has gained widespread attention due to its parameter\nefficiency. However, its impact on improving the performance of large models\nremains limited. Recent studies suggest that combining LoRA with\nMixture-of-Experts (MoE) can significantly enhance fine-tuning performance. MoE\nadapts to the diversity and complexity of datasets by dynamically selecting the\nmost suitable experts, thereby improving task accuracy and efficiency. Despite\nimpressive results, recent studies reveal issues in the MoE routing mechanism,\nsuch as incorrect assignments and imbalanced expert allocation. Inspired by the\nprinciples of Redundancy and Fault Tolerance Theory. We innovatively integrate\nthe concept of Mixture of Experts into the routing mechanism and propose an\nefficient fine-tuning method called Mixture of Routers (MoR). It employs\nmultiple sub-routers for joint selection and uses a learnable main router to\ndetermine the weights of the sub-routers. The results show that MoR outperforms\nbaseline models on most tasks, achieving an average performance improvement of\n1%. MoR can serve as a plug-and-play, parameter-efficient fine-tuning method\nsuitable for a wide range of applications. Our code is available here:\nhttps://anonymous.4open.science/r/MoR-DFC6."}
{"id": "2503.23094", "pdf": "https://arxiv.org/pdf/2503.23094", "abs": "https://arxiv.org/abs/2503.23094", "authors": ["Andrea Boscolo Camiletto", "Jian Wang", "Eduardo Alvarado", "Rishabh Dabral", "Thabo Beeler", "Marc Habermann", "Christian Theobalt"], "title": "FRAME: Floor-aligned Representation for Avatar Motion from Egocentric Video", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025", "summary": "Egocentric motion capture with a head-mounted body-facing stereo camera is\ncrucial for VR and AR applications but presents significant challenges such as\nheavy occlusions and limited annotated real-world data. Existing methods rely\non synthetic pretraining and struggle to generate smooth and accurate\npredictions in real-world settings, particularly for lower limbs. Our work\naddresses these limitations by introducing a lightweight VR-based data\ncollection setup with on-board, real-time 6D pose tracking. Using this setup,\nwe collected the most extensive real-world dataset for ego-facing ego-mounted\ncameras to date in size and motion variability. Effectively integrating this\nmultimodal input -- device pose and camera feeds -- is challenging due to the\ndiffering characteristics of each data source. To address this, we propose\nFRAME, a simple yet effective architecture that combines device pose and camera\nfeeds for state-of-the-art body pose prediction through geometrically sound\nmultimodal integration and can run at 300 FPS on modern hardware. Lastly, we\nshowcase a novel training strategy to enhance the model's generalization\ncapabilities. Our approach exploits the problem's geometric properties,\nyielding high-quality motion capture free from common artifacts in prior works.\nQualitative and quantitative evaluations, along with extensive comparisons,\ndemonstrate the effectiveness of our method. Data, code, and CAD designs will\nbe available at https://vcai.mpi-inf.mpg.de/projects/FRAME/"}
{"id": "2503.23371", "pdf": "https://arxiv.org/pdf/2503.23371", "abs": "https://arxiv.org/abs/2503.23371", "authors": ["Jeonghyun Ko", "Gyeongyun Park", "Donghoon Lee", "Kyunam Lee"], "title": "FeRG-LLM : Feature Engineering by Reason Generation Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to NAACL 2025 Findings", "summary": "One of the key tasks in machine learning for tabular data is feature\nengineering. Although it is vital for improving the performance of models, it\ndemands considerable human expertise and deep domain knowledge, making it\nlabor-intensive endeavor. To address this issue, we propose a novel framework,\n\\textbf{FeRG-LLM} (\\textbf{Fe}ature engineering by \\textbf{R}eason\n\\textbf{G}eneration \\textbf{L}arge \\textbf{L}anguage \\textbf{M}odels), a large\nlanguage model designed to automatically perform feature engineering at an\n8-billion-parameter scale. We have constructed two-stage conversational\ndialogues that enable language models to analyze machine learning tasks and\ndiscovering new features, exhibiting their Chain-of-Thought (CoT) capabilities.\nWe use these dialogues to fine-tune Llama 3.1 8B model and integrate Direct\nPreference Optimization (DPO) to receive feedback improving quality of new\nfeatures and the model's performance. Our experiments show that FeRG-LLM\nperforms comparably to or better than Llama 3.1 70B on most datasets, while\nusing fewer resources and achieving reduced inference time. It outperforms\nother studies in classification tasks and performs well in regression tasks.\nMoreover, since it does not rely on cloud-hosted LLMs like GPT-4 with extra API\ncosts when generating features, it can be deployed locally, addressing security\nconcerns."}
{"id": "2503.23105", "pdf": "https://arxiv.org/pdf/2503.23105", "abs": "https://arxiv.org/abs/2503.23105", "authors": ["Yifan Xu", "Vineet Kamat", "Carol Menassa"], "title": "Open-Vocabulary Semantic Segmentation with Uncertainty Alignment for Robotic Scene Understanding in Indoor Building Environments", "categories": ["cs.CV"], "comment": "32 pages, 7 figures", "summary": "The global rise in the number of people with physical disabilities, in part\ndue to improvements in post-trauma survivorship and longevity, has amplified\nthe demand for advanced assistive technologies to improve mobility and\nindependence. Autonomous assistive robots, such as smart wheelchairs, require\nrobust capabilities in spatial segmentation and semantic recognition to\nnavigate complex built environments effectively. Place segmentation involves\ndelineating spatial regions like rooms or functional areas, while semantic\nrecognition assigns semantic labels to these regions, enabling accurate\nlocalization to user-specific needs. Existing approaches often utilize deep\nlearning; however, these close-vocabulary detection systems struggle to\ninterpret intuitive and casual human instructions. Additionally, most existing\nmethods ignore the uncertainty of the scene recognition problem, leading to low\nsuccess rates, particularly in ambiguous and complex environments. To address\nthese challenges, we propose an open-vocabulary scene semantic segmentation and\ndetection pipeline leveraging Vision Language Models (VLMs) and Large Language\nModels (LLMs). Our approach follows a 'Segment Detect Select' framework for\nopen-vocabulary scene classification, enabling adaptive and intuitive\nnavigation for assistive robots in built environments."}
{"id": "2503.23383", "pdf": "https://arxiv.org/pdf/2503.23383", "abs": "https://arxiv.org/abs/2503.23383", "authors": ["Xuefeng Li", "Haoyang Zou", "Pengfei Liu"], "title": "ToRL: Scaling Tool-Integrated RL", "categories": ["cs.CL"], "comment": null, "summary": "We introduce ToRL (Tool-Integrated Reinforcement Learning), a framework for\ntraining large language models (LLMs) to autonomously use computational tools\nvia reinforcement learning. Unlike supervised fine-tuning, ToRL allows models\nto explore and discover optimal strategies for tool use. Experiments with\nQwen2.5-Math models show significant improvements: ToRL-7B reaches 43.3\\%\naccuracy on AIME~24, surpassing reinforcement learning without tool integration\nby 14\\% and the best existing Tool-Integrated Reasoning (TIR) model by 17\\%.\nFurther analysis reveals emergent behaviors such as strategic tool invocation,\nself-regulation of ineffective code, and dynamic adaptation between\ncomputational and analytical reasoning, all arising purely through\nreward-driven learning."}
{"id": "2503.23106", "pdf": "https://arxiv.org/pdf/2503.23106", "abs": "https://arxiv.org/abs/2503.23106", "authors": ["Chao Tao", "Dandan Zhong", "Weiliang Mu", "Zhuofei Du", "Haiyang Wu"], "title": "A large-scale image-text dataset benchmark for farmland segmentation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "The traditional deep learning paradigm that solely relies on labeled data has\nlimitations in representing the spatial relationships between farmland elements\nand the surrounding environment.It struggles to effectively model the dynamic\ntemporal evolution and spatial heterogeneity of farmland. Language,as a\nstructured knowledge carrier,can explicitly express the spatiotemporal\ncharacteristics of farmland, such as its shape, distribution,and surrounding\nenvironmental information.Therefore,a language-driven learning paradigm can\neffectively alleviate the challenges posed by the spatiotemporal heterogeneity\nof farmland.However,in the field of remote sensing imagery of farmland,there is\ncurrently no comprehensive benchmark dataset to support this research\ndirection.To fill this gap,we introduced language based descriptions of\nfarmland and developed FarmSeg-VL dataset,the first fine-grained image-text\ndataset designed for spatiotemporal farmland segmentation.Firstly, this article\nproposed a semi-automatic annotation method that can accurately assign caption\nto each image, ensuring high data quality and semantic richness while improving\nthe efficiency of dataset construction.Secondly,the FarmSeg-VL exhibits\nsignificant spatiotemporal characteristics.In terms of the temporal\ndimension,it covers all four seasons.In terms of the spatial dimension,it\ncovers eight typical agricultural regions across China.In addition, in terms of\ncaptions,FarmSeg-VL covers rich spatiotemporal characteristics of\nfarmland,including its inherent properties,phenological characteristics,\nspatial distribution,topographic and geomorphic features,and the distribution\nof surrounding environments.Finally,we present a performance analysis of VLMs\nand the deep learning models that rely solely on labels trained on the\nFarmSeg-VL,demonstrating its potential as a standard benchmark for farmland\nsegmentation."}
{"id": "2503.23415", "pdf": "https://arxiv.org/pdf/2503.23415", "abs": "https://arxiv.org/abs/2503.23415", "authors": ["Alexander Murphy", "Mohd Sanad Zaki Rizvi", "Aden Haussmann", "Ping Nie", "Guifu Liu", "Aryo Pradipta Gema", "Pasquale Minervini"], "title": "An Analysis of Decoding Methods for LLM-based Agents for Faithful Multi-Hop Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) frequently produce factually inaccurate outputs\n- a phenomenon known as hallucination - which limits their accuracy in\nknowledge-intensive NLP tasks. Retrieval-augmented generation and agentic\nframeworks such as Reasoning and Acting (ReAct) can address this issue by\ngiving the model access to external knowledge. However, LLMs often fail to\nremain faithful to retrieved information. Mitigating this is critical,\nespecially if LLMs are required to reason about the retrieved information.\nRecent research has explored training-free decoding strategies to improve the\nfaithfulness of model generations. We present a systematic analysis of how the\ncombination of the ReAct framework and decoding strategies (i.e., DeCoRe, DoLa,\nand CAD) can influence the faithfulness of LLM-generated answers. Our results\nshow that combining an agentic framework for knowledge retrieval with decoding\nmethods that enhance faithfulness can increase accuracy on the downstream\nMulti-Hop Question Answering tasks. For example, we observe an F1 increase from\n19.5 to 32.6 on HotpotQA when using ReAct and DoLa."}
{"id": "2503.23109", "pdf": "https://arxiv.org/pdf/2503.23109", "abs": "https://arxiv.org/abs/2503.23109", "authors": ["Xiaolu Liu", "Ruizi Yang", "Song Wang", "Wentong Li", "Junbo Chen", "Jianke Zhu"], "title": "Uncertainty-Instructed Structure Injection for Generalizable HD Map Construction", "categories": ["cs.CV"], "comment": "17 pages, 10 figures", "summary": "Reliable high-definition (HD) map construction is crucial for the driving\nsafety of autonomous vehicles. Although recent studies demonstrate improved\nperformance, their generalization capability across unfamiliar driving scenes\nremains unexplored. To tackle this issue, we propose UIGenMap, an\nuncertainty-instructed structure injection approach for generalizable HD map\nvectorization, which concerns the uncertainty resampling in statistical\ndistribution and employs explicit instance features to reduce excessive\nreliance on training data. Specifically, we introduce the perspective-view (PV)\ndetection branch to obtain explicit structural features, in which the\nuncertainty-aware decoder is designed to dynamically sample probability\ndistributions considering the difference in scenes. With probabilistic\nembedding and selection, UI2DPrompt is proposed to construct PV-learnable\nprompts. These PV prompts are integrated into the map decoder by designed\nhybrid injection to compensate for neglected instance structures. To ensure\nreal-time inference, a lightweight Mimic Query Distillation is designed to\nlearn from PV prompts, which can serve as an efficient alternative to the flow\nof PV branches. Extensive experiments on challenging geographically disjoint\n(geo-based) data splits demonstrate that our UIGenMap achieves superior\nperformance, with +5.7 mAP improvement on the nuScenes dataset. Source code\nwill be available at https://github.com/xiaolul2/UIGenMap."}
{"id": "2503.23427", "pdf": "https://arxiv.org/pdf/2503.23427", "abs": "https://arxiv.org/abs/2503.23427", "authors": ["Wenhan Liu", "Xinyu Ma", "Yutao Zhu", "Lixin Su", "Shuaiqiang Wang", "Dawei Yin", "Zhicheng Dou"], "title": "CoRanking: Collaborative Ranking with Small and Large Ranking Agents", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated superior listwise ranking\nperformance. However, their superior performance often relies on large-scale\nparameters (\\eg, GPT-4) and a repetitive sliding window process, which\nintroduces significant efficiency challenges. In this paper, we propose\n\\textbf{CoRanking}, a novel collaborative ranking framework that combines small\nand large ranking models for efficient and effective ranking. CoRanking first\nemploys a small-size reranker to pre-rank all the candidate passages, bringing\nrelevant ones to the top part of the list (\\eg, top-20). Then, the LLM listwise\nreranker is applied to only rerank these top-ranked passages instead of the\nwhole list, substantially enhancing overall ranking efficiency. Although more\nefficient, previous studies have revealed that the LLM listwise reranker have\nsignificant positional biases on the order of input passages. Directly feed the\ntop-ranked passages from small reranker may result in the sub-optimal\nperformance of LLM listwise reranker. To alleviate this problem, we introduce a\npassage order adjuster trained via reinforcement learning, which reorders the\ntop passages from the small reranker to align with the LLM's preferences of\npassage order. Extensive experiments on three IR benchmarks demonstrate that\nCoRanking significantly improves efficiency (reducing ranking latency by about\n70\\%) while achieving even better effectiveness compared to using only the LLM\nlistwise reranker."}
{"id": "2503.23121", "pdf": "https://arxiv.org/pdf/2503.23121", "abs": "https://arxiv.org/abs/2503.23121", "authors": ["Guohong Huang", "Ling-An Zeng", "Zexin Zheng", "Shengbo Gu", "Wei-Shi Zheng"], "title": "Efficient Explicit Joint-level Interaction Modeling with Mamba for Text-guided HOI Generation", "categories": ["cs.CV"], "comment": "Accepted to ICME 2025", "summary": "We propose a novel approach for generating text-guided human-object\ninteractions (HOIs) that achieves explicit joint-level interaction modeling in\na computationally efficient manner. Previous methods represent the entire human\nbody as a single token, making it difficult to capture fine-grained joint-level\ninteractions and resulting in unrealistic HOIs. However, treating each\nindividual joint as a token would yield over twenty times more tokens,\nincreasing computational overhead. To address these challenges, we introduce an\nEfficient Explicit Joint-level Interaction Model (EJIM). EJIM features a\nDual-branch HOI Mamba that separately and efficiently models spatiotemporal HOI\ninformation, as well as a Dual-branch Condition Injector for integrating text\nsemantics and object geometry into human and object motions. Furthermore, we\ndesign a Dynamic Interaction Block and a progressive masking mechanism to\niteratively filter out irrelevant joints, ensuring accurate and nuanced\ninteraction modeling. Extensive quantitative and qualitative evaluations on\npublic datasets demonstrate that EJIM surpasses previous works by a large\nmargin while using only 5\\% of the inference time. Code is available\n\\href{https://github.com/Huanggh531/EJIM}{here}."}
{"id": "2503.23439", "pdf": "https://arxiv.org/pdf/2503.23439", "abs": "https://arxiv.org/abs/2503.23439", "authors": ["Hyunjong Ok", "Suho Yoo", "Jaeho Lee"], "title": "Speculative End-Turn Detector for Efficient Speech Chatbot Assistant", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": "Preprint", "summary": "Spoken dialogue systems powered by large language models have demonstrated\nremarkable abilities in understanding human speech and generating appropriate\nspoken responses. However, these systems struggle with end-turn detection (ETD)\n-- the ability to distinguish between user turn completion and hesitation. This\nlimitation often leads to premature or delayed responses, disrupting the flow\nof spoken conversations. In this paper, we introduce the ETD Dataset, the first\npublic dataset for end-turn detection. The ETD dataset consists of both\nsynthetic speech data generated with text-to-speech models and real-world\nspeech data collected from web sources. We also propose SpeculativeETD, a novel\ncollaborative inference framework that balances efficiency and accuracy to\nimprove real-time ETD in resource-constrained environments. Our approach\njointly employs a lightweight GRU-based model, which rapidly detects the\nnon-speaking units in real-time on local devices, and a high-performance\nWav2vec-based model running on the server to make a more challenging\nclassification of distinguishing turn ends from mere pauses. Experiments\ndemonstrate that the proposed SpeculativeETD significantly improves ETD\naccuracy while keeping the required computations low. Datasets and code will be\navailable after the review."}
{"id": "2503.23125", "pdf": "https://arxiv.org/pdf/2503.23125", "abs": "https://arxiv.org/abs/2503.23125", "authors": ["Shuhao Fu", "Andrew Jun Lee", "Anna Wang", "Ida Momennejad", "Trevor Bihl", "Hongjing Lu", "Taylor W. Webb"], "title": "Evaluating Compositional Scene Understanding in Multimodal Generative Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The visual world is fundamentally compositional. Visual scenes are defined by\nthe composition of objects and their relations. Hence, it is essential for\ncomputer vision systems to reflect and exploit this compositionality to achieve\nrobust and generalizable scene understanding. While major strides have been\nmade toward the development of general-purpose, multimodal generative models,\nincluding both text-to-image models and multimodal vision-language models, it\nremains unclear whether these systems are capable of accurately generating and\ninterpreting scenes involving the composition of multiple objects and\nrelations. In this work, we present an evaluation of the compositional visual\nprocessing capabilities in the current generation of text-to-image (DALL-E 3)\nand multimodal vision-language models (GPT-4V, GPT-4o, Claude Sonnet 3.5,\nQWEN2-VL-72B, and InternVL2.5-38B), and compare the performance of these\nsystems to human participants. The results suggest that these systems display\nsome ability to solve compositional and relational tasks, showing notable\nimprovements over the previous generation of multimodal models, but with\nperformance nevertheless well below the level of human participants,\nparticularly for more complex scenes involving many ($>5$) objects and multiple\nrelations. These results highlight the need for further progress toward\ncompositional understanding of visual scenes."}
{"id": "2503.23483", "pdf": "https://arxiv.org/pdf/2503.23483", "abs": "https://arxiv.org/abs/2503.23483", "authors": ["Katrina Brown", "Reid McIlroy"], "title": "Order Independence With Finetuning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Published as a Bi-Align workshop paper at ICLR 2025", "summary": "Large language models (LLMs) demonstrate remarkable performance on many NLP\ntasks, yet often exhibit order dependence: simply reordering semantically\nidentical tokens (e.g., answer choices in multiple-choice questions) can lead\nto inconsistent predictions. Recent work proposes Set-Based Prompting (SBP) as\na way to remove order information from designated token subsets, thereby\nmitigating positional biases. However, applying SBP on base models induces an\nout-of-distribution input format, which can degrade in-distribution\nperformance. We introduce a fine-tuning strategy that integrates SBP into the\ntraining process, \"pulling\" these set-formatted prompts closer to the model's\ntraining manifold. We show that SBP can be incorporated into a model via\nfine-tuning. Our experiments on in-distribution (MMLU) and out-of-distribution\n(CSQA, ARC Challenge) multiple-choice tasks show that SBP fine-tuning\nsignificantly improves accuracy and robustness to answer-order permutations,\nall while preserving broader language modeling capabilities. We discuss the\nbroader implications of order-invariant modeling and outline future directions\nfor building fairer, more consistent LLMs."}
{"id": "2503.23130", "pdf": "https://arxiv.org/pdf/2503.23130", "abs": "https://arxiv.org/abs/2503.23130", "authors": ["Boyi Ma", "Yanguang Zhao", "Jie Wang", "Guankun Wang", "Kun Yuan", "Tong Chen", "Long Bai", "Hongliang Ren"], "title": "Can DeepSeek-V3 Reason Like a Surgeon? An Empirical Evaluation for Vision-Language Understanding in Robotic-Assisted Surgery", "categories": ["cs.CV", "cs.CL", "cs.RO"], "comment": "Technical Report", "summary": "DeepSeek-V3, a recently emerging Large Language Model (LLM), demonstrates\noutstanding performance in general scene understanding, question-answering\n(QA), and text generation tasks, owing to its efficient training paradigm and\nstrong reasoning capabilities. In this study, we investigate the dialogue\ncapabilities of DeepSeek-V3 in robotic surgery scenarios, focusing on tasks\nsuch as Single Phrase QA, Visual QA, and Detailed Description. The Single\nPhrase QA tasks further include sub-tasks such as surgical instrument\nrecognition, action understanding, and spatial position analysis. We conduct\nextensive evaluations using publicly available datasets, including EndoVis18\nand CholecT50, along with their corresponding dialogue data. Our comprehensive\nevaluation results indicate that, when provided with specific prompts,\nDeepSeek-V3 performs well in surgical instrument and tissue recognition tasks\nHowever, DeepSeek-V3 exhibits significant limitations in spatial position\nanalysis and struggles to understand surgical actions accurately. Additionally,\nour findings reveal that, under general prompts, DeepSeek-V3 lacks the ability\nto effectively analyze global surgical concepts and fails to provide detailed\ninsights into surgical scenarios. Based on our observations, we argue that the\nDeepSeek-V3 is not ready for vision-language tasks in surgical contexts without\nfine-tuning on surgery-specific datasets."}
{"id": "2503.23503", "pdf": "https://arxiv.org/pdf/2503.23503", "abs": "https://arxiv.org/abs/2503.23503", "authors": ["Sid Bharthulwar", "John Rho", "Katrina Brown"], "title": "Evolutionary Prompt Optimization Discovers Emergent Multimodal Reasoning Strategies in Vision-Language Models", "categories": ["cs.CL"], "comment": "Published at ICLR 2025 Workshop on Reasoning and Planning for LLMs", "summary": "We present a framework for optimizing prompts in vision-language models to\nelicit multimodal reasoning without model retraining. Using an evolutionary\nalgorithm to guide prompt updates downstream of visual tasks, our approach\nimproves upon baseline prompt-updating algorithms, which lack evolution-style\n\"survival of the fittest\" iteration. Crucially, we find this approach enables\nthe language model to independently discover progressive problem-solving\ntechniques across several evolution generations. For example, the model reasons\nthat to \"break down\" visually complex spatial tasks, making a tool call to a\nPython interpreter to perform tasks (such as cropping, image segmentation, or\nsaturation changes) would improve performance significantly. Our\nexperimentation shows that explicitly evoking this \"tool calling\" call, via\nsystem-level XML $...\\texttt{<tool>} ... \\texttt{</tool>}...$ tags, can\neffectively flag Python interpreter access for the same language model to\ngenerate relevant programs, generating advanced multimodal functionality. This\nfunctionality can be crystallized into a system-level prompt that induces\nimproved performance at inference time, and our experimentation suggests up to\n$\\approx 50\\%$ relative improvement across select visual tasks. Downstream\nperformance is trained and evaluated across subtasks from MathVista, M3CoT, and\nGeoBench-VLM datasets. Importantly, our approach shows that evolutionary prompt\noptimization guides language models towards self-reasoning discoveries, which\nresult in improved zero-shot generalization across tasks."}
{"id": "2503.23131", "pdf": "https://arxiv.org/pdf/2503.23131", "abs": "https://arxiv.org/abs/2503.23131", "authors": ["Alexander Vogel", "Omar Moured", "Yufan Chen", "Jiaming Zhang", "Rainer Stiefelhagen"], "title": "RefChartQA: Grounding Visual Answer on Chart Images through Instruction Tuning", "categories": ["cs.CV"], "comment": "All models and code will be publicly available at\n  https://github.com/moured/RefChartQA", "summary": "Recently, Vision Language Models (VLMs) have increasingly emphasized document\nvisual grounding to achieve better human-computer interaction, accessibility,\nand detailed understanding. However, its application to visualizations such as\ncharts remains under-explored due to the inherent complexity of interleaved\nvisual-numerical relationships in chart images. Existing chart understanding\nmethods primarily focus on answering questions without explicitly identifying\nthe visual elements that support their predictions. To bridge this gap, we\nintroduce RefChartQA, a novel benchmark that integrates Chart Question\nAnswering (ChartQA) with visual grounding, enabling models to refer elements at\nmultiple granularities within chart images. Furthermore, we conduct a\ncomprehensive evaluation by instruction-tuning 5 state-of-the-art VLMs across\ndifferent categories. Our experiments demonstrate that incorporating spatial\nawareness via grounding improves response accuracy by over 15%, reducing\nhallucinations, and improving model reliability. Additionally, we identify key\nfactors influencing text-spatial alignment, such as architectural improvements\nin TinyChart, which leverages a token-merging module for enhanced feature\nfusion. Our dataset is open-sourced for community development and further\nadvancements. All models and code will be publicly available at\nhttps://github.com/moured/RefChartQA."}
{"id": "2503.23512", "pdf": "https://arxiv.org/pdf/2503.23512", "abs": "https://arxiv.org/abs/2503.23512", "authors": ["Qiang Yi", "Yangfan He", "Jianhui Wang", "Xinyuan Song", "Shiyao Qian", "Miao Zhang", "Li Sun", "Tianyu Shi"], "title": "SCORE: Story Coherence and Retrieval Enhancement for AI Narratives", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) excel at generating creative narratives but\nstruggle with long-term coherence and emotional consistency in complex stories.\nTo address this, we propose SCORE (Story Coherence and Retrieval Enhancement),\na framework integrating three components: 1) Dynamic State Tracking (monitoring\nobjects/characters via symbolic logic), 2) Context-Aware Summarization\n(hierarchical episode summaries for temporal progression), and 3) Hybrid\nRetrieval (combining TF-IDF keyword relevance with cosine similarity-based\nsemantic embeddings). The system employs a temporally-aligned\nRetrieval-Augmented Generation (RAG) pipeline to validate contextual\nconsistency. Evaluations show SCORE achieves 23.6% higher coherence (NCI-2.0\nbenchmark), 89.7% emotional consistency (EASM metric), and 41.8% fewer\nhallucinations versus baseline GPT models. Its modular design supports\nincremental knowledge graph construction for persistent story memory and\nmulti-LLM backend compatibility, offering an explainable solution for\nindustrial-scale narrative systems requiring long-term consistency."}
{"id": "2503.23135", "pdf": "https://arxiv.org/pdf/2503.23135", "abs": "https://arxiv.org/abs/2503.23135", "authors": ["Ao Wang", "Hui Chen", "Zijia Lin", "Jungong Han", "Guiguang Ding"], "title": "LSNet: See Large, Focus Small", "categories": ["cs.CV"], "comment": "CVPR 2025 Camera-ready Version", "summary": "Vision network designs, including Convolutional Neural Networks and Vision\nTransformers, have significantly advanced the field of computer vision. Yet,\ntheir complex computations pose challenges for practical deployments,\nparticularly in real-time applications. To tackle this issue, researchers have\nexplored various lightweight and efficient network designs. However, existing\nlightweight models predominantly leverage self-attention mechanisms and\nconvolutions for token mixing. This dependence brings limitations in\neffectiveness and efficiency in the perception and aggregation processes of\nlightweight networks, hindering the balance between performance and efficiency\nunder limited computational budgets. In this paper, we draw inspiration from\nthe dynamic heteroscale vision ability inherent in the efficient human vision\nsystem and propose a ``See Large, Focus Small'' strategy for lightweight vision\nnetwork design. We introduce LS (\\textbf{L}arge-\\textbf{S}mall) convolution,\nwhich combines large-kernel perception and small-kernel aggregation. It can\nefficiently capture a wide range of perceptual information and achieve precise\nfeature aggregation for dynamic and complex visual representations, thus\nenabling proficient processing of visual information. Based on LS convolution,\nwe present LSNet, a new family of lightweight models. Extensive experiments\ndemonstrate that LSNet achieves superior performance and efficiency over\nexisting lightweight networks in various vision tasks. Codes and models are\navailable at https://github.com/jameslahm/lsnet."}
{"id": "2503.23513", "pdf": "https://arxiv.org/pdf/2503.23513", "abs": "https://arxiv.org/abs/2503.23513", "authors": ["Zhengren Wang", "Jiayang Yu", "Dongsheng Ma", "Zhe Chen", "Yu Wang", "Zhiyu Li", "Feiyu Xiong", "Yanfeng Wang", "Weinan E", "Linpeng Tang", "Wentao Zhang"], "title": "RARE: Retrieval-Augmented Reasoning Modeling", "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Domain-specific intelligence demands specialized knowledge and sophisticated\nreasoning for problem-solving, posing significant challenges for large language\nmodels (LLMs) that struggle with knowledge hallucination and inadequate\nreasoning capabilities under constrained parameter budgets. Inspired by Bloom's\nTaxonomy in educational theory, we propose Retrieval-Augmented Reasoning\nModeling (RARE), a novel paradigm that decouples knowledge storage from\nreasoning optimization. RARE externalizes domain knowledge to retrievable\nsources and internalizes domain-specific reasoning patterns during training.\nSpecifically, by injecting retrieved knowledge into training prompts, RARE\ntransforms learning objectives from rote memorization to contextualized\nreasoning application. It enables models to bypass parameter-intensive\nmemorization and prioritize the development of higher-order cognitive\nprocesses. Our experiments demonstrate that lightweight RARE-trained models\n(e.g., Llama-3.1-8B) could achieve state-of-the-art performance, surpassing\nretrieval-augmented GPT-4 and Deepseek-R1 distilled counterparts. RARE\nestablishes a paradigm shift where maintainable external knowledge bases\nsynergize with compact, reasoning-optimized models, collectively driving more\nscalable domain-specific intelligence. Repo:\nhttps://github.com/Open-DataFlow/RARE"}
{"id": "2503.23137", "pdf": "https://arxiv.org/pdf/2503.23137", "abs": "https://arxiv.org/abs/2503.23137", "authors": ["Tuo Liang", "Zhe Hu", "Jing Li", "Hao Zhang", "Yiren Lu", "Yunlai Zhou", "Yiran Qiao", "Disheng Liu", "Jeirui Peng", "Jing Ma", "Yu Yin"], "title": "When 'YES' Meets 'BUT': Can Large Models Comprehend Contradictory Humor Through Comparative Reasoning?", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Understanding humor-particularly when it involves complex, contradictory\nnarratives that require comparative reasoning-remains a significant challenge\nfor large vision-language models (VLMs). This limitation hinders AI's ability\nto engage in human-like reasoning and cultural expression. In this paper, we\ninvestigate this challenge through an in-depth analysis of comics that\njuxtapose panels to create humor through contradictions. We introduce the\nYesBut (V2), a novel benchmark with 1,262 comic images from diverse\nmultilingual and multicultural contexts, featuring comprehensive annotations\nthat capture various aspects of narrative understanding. Using this benchmark,\nwe systematically evaluate a wide range of VLMs through four complementary\ntasks spanning from surface content comprehension to deep narrative reasoning,\nwith particular emphasis on comparative reasoning between contradictory\nelements. Our extensive experiments reveal that even the most advanced models\nsignificantly underperform compared to humans, with common failures in visual\nperception, key element identification, comparative analysis and\nhallucinations. We further investigate text-based training strategies and\nsocial knowledge augmentation methods to enhance model performance. Our\nfindings not only highlight critical weaknesses in VLMs' understanding of\ncultural and creative expressions but also provide pathways toward developing\ncontext-aware models capable of deeper narrative understanding though\ncomparative reasoning."}
{"id": "2503.23514", "pdf": "https://arxiv.org/pdf/2503.23514", "abs": "https://arxiv.org/abs/2503.23514", "authors": ["Siqi Fan", "Xiusheng Huang", "Yiqun Yao", "Xuezhi Fang", "Kang Liu", "Peng Han", "Shuo Shang", "Aixin Sun", "Yequan Wang"], "title": "If an LLM Were a Character, Would It Know Its Own Story? Evaluating Lifelong Learning in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) can carry out human-like dialogue, but unlike\nhumans, they are stateless due to the superposition property. However, during\nmulti-turn, multi-agent interactions, LLMs begin to exhibit consistent,\ncharacter-like behaviors, hinting at a form of emergent lifelong learning.\nDespite this, existing benchmarks often fail to capture these dynamics,\nprimarily focusing on static, open-ended evaluations. To address this gap, we\nintroduce LIFESTATE-BENCH, a benchmark designed to assess lifelong learning in\nLLMs. It features two episodic datasets: Hamlet and a synthetic script\ncollection, rich in narrative structure and character interactions. Our fact\nchecking evaluation probes models' self-awareness, episodic memory retrieval,\nand relationship tracking, across both parametric and non-parametric\napproaches. Experiments on models like Llama3.1-8B, GPT-4-turbo, and DeepSeek\nR1, we demonstrate that nonparametric methods significantly outperform\nparametric ones in managing stateful learning. However, all models exhibit\nchallenges with catastrophic forgetting as interactions extend, highlighting\nthe need for further advancements in lifelong learning."}
{"id": "2503.23162", "pdf": "https://arxiv.org/pdf/2503.23162", "abs": "https://arxiv.org/abs/2503.23162", "authors": ["Zhenyu Tang", "Chaoran Feng", "Xinhua Cheng", "Wangbo Yu", "Junwu Zhang", "Yuan Liu", "Xiaoxiao Long", "Wenping Wang", "Li Yuan"], "title": "NeuralGS: Bridging Neural Fields and 3D Gaussian Splatting for Compact 3D Representations", "categories": ["cs.CV"], "comment": "Project page: https://pku-yuangroup.github.io/NeuralGS/", "summary": "3D Gaussian Splatting (3DGS) demonstrates superior quality and rendering\nspeed, but with millions of 3D Gaussians and significant storage and\ntransmission costs. Recent 3DGS compression methods mainly concentrate on\ncompressing Scaffold-GS, achieving impressive performance but with an\nadditional voxel structure and a complex encoding and quantization strategy. In\nthis paper, we aim to develop a simple yet effective method called NeuralGS\nthat explores in another way to compress the original 3DGS into a compact\nrepresentation without the voxel structure and complex quantization strategies.\nOur observation is that neural fields like NeRF can represent complex 3D scenes\nwith Multi-Layer Perceptron (MLP) neural networks using only a few megabytes.\nThus, NeuralGS effectively adopts the neural field representation to encode the\nattributes of 3D Gaussians with MLPs, only requiring a small storage size even\nfor a large-scale scene. To achieve this, we adopt a clustering strategy and\nfit the Gaussians with different tiny MLPs for each cluster, based on\nimportance scores of Gaussians as fitting weights. We experiment on multiple\ndatasets, achieving a 45-times average model size reduction without harming the\nvisual quality. The compression performance of our method on original 3DGS is\ncomparable to the dedicated Scaffold-GS-based compression methods, which\ndemonstrate the huge potential of directly compressing original 3DGS with\nneural fields."}
{"id": "2503.23523", "pdf": "https://arxiv.org/pdf/2503.23523", "abs": "https://arxiv.org/abs/2503.23523", "authors": ["Haochen Liu", "Song Wang", "Chen Chen", "Jundong Li"], "title": "Question-Aware Knowledge Graph Prompting for Enhancing Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) often struggle with tasks requiring external\nknowledge, such as knowledge-intensive Multiple Choice Question Answering\n(MCQA). Integrating Knowledge Graphs (KGs) can enhance reasoning; however,\nexisting methods typically demand costly fine-tuning or retrieve noisy KG\ninformation. Recent approaches leverage Graph Neural Networks (GNNs) to\ngenerate KG-based input embedding prefixes as soft prompts for LLMs but fail to\naccount for question relevance, resulting in noisy prompts. Moreover, in MCQA\ntasks, the absence of relevant KG knowledge for certain answer options remains\na significant challenge. To address these issues, we propose Question-Aware\nKnowledge Graph Prompting (QAP), which incorporates question embeddings into\nGNN aggregation to dynamically assess KG relevance. QAP employs global\nattention to capture inter-option relationships, enriching soft prompts with\ninferred knowledge. Experimental results demonstrate that QAP outperforms\nstate-of-the-art methods across multiple datasets, highlighting its\neffectiveness."}
{"id": "2503.23178", "pdf": "https://arxiv.org/pdf/2503.23178", "abs": "https://arxiv.org/abs/2503.23178", "authors": ["Pengyu Chen", "Teng Fei", "Yunyan Du", "Jiawei Yi", "Yi Li", "John A. Kupfer"], "title": "Intelligent Bear Prevention System Based on Computer Vision: An Approach to Reduce Human-Bear Conflicts in the Tibetan Plateau Area, China", "categories": ["cs.CV"], "comment": null, "summary": "Conflicts between humans and bears on the Tibetan Plateau present substantial\nthreats to local communities and hinder wildlife preservation initiatives. This\nresearch introduces a novel strategy that incorporates computer vision\nalongside Internet of Things (IoT) technologies to alleviate these issues.\nTailored specifically for the harsh environment of the Tibetan Plateau, the\napproach utilizes the K210 development board paired with the YOLO object\ndetection framework along with a tailored bear-deterrent mechanism, offering\nminimal energy usage and real-time efficiency in bear identification and\ndeterrence. The model's performance was evaluated experimentally, achieving a\nmean Average Precision (mAP) of 91.4%, demonstrating excellent precision and\ndependability. By integrating energy-efficient components, the proposed system\neffectively surpasses the challenges of remote and off-grid environments,\nensuring uninterrupted operation in secluded locations. This study provides a\nviable, eco-friendly, and expandable solution to mitigate human-bear conflicts,\nthereby improving human safety and promoting bear conservation in isolated\nareas like Yushu, China."}
{"id": "2503.23542", "pdf": "https://arxiv.org/pdf/2503.23542", "abs": "https://arxiv.org/abs/2503.23542", "authors": ["Xabier de Zuazo", "Eva Navas", "Ibon Saratxaga", "Inma Hernáez Rioja"], "title": "Whisper-LM: Improving ASR Models with Language Models for Low-Resource Languages", "categories": ["cs.CL", "68T50 (Primary), 62H30", "I.2.7; I.2.6; J.5.2"], "comment": "26 pages, 6 figures, includes supplementary materials. Will be\n  submitted to IEEE/ACM Transactions on Audio, Speech, and Language Processing", "summary": "Automatic speech recognition systems have undoubtedly advanced with the\nintegration of multilingual and multitask models such as Whisper, which have\nshown a promising ability to understand and process speech across a wide range\nof languages. Despite their robustness, these models often fall short in\nhandling the linguistic distinctions of minority languages. This study\naddresses this gap by integrating traditional and novel language models with\nfine-tuned Whisper models to raise their performance in less commonly studied\nlanguages. Through rigorous fine-tuning and evaluation across multiple\ndatasets, we demonstrate substantial improvements in word error rate,\nparticularly in low-resource scenarios. Our approach not only does take\nadvantage of the extensive data Whisper was pre-trained on, but also\ncomplements its linguistic adaptability by incorporating language models. We\nobtained improvements up to 51\\% for in-distribution datasets and up to 34\\%\nfor out-of-distribution sentences using statistical language models, while\nlarge language models provided moderate but consistently robust improvement\nacross diverse linguistic contexts. The findings reveal that, while the\nintegration reliably benefits all model sizes, the extent of improvement\nvaries, highlighting the importance of optimized language model parameters.\nFinally, we emphasize the importance of selecting appropriate evaluation\nparameters when reporting the results using transformer-based ASR models. In\nsummary, this research clears the way for more inclusive ASR technologies that\nperform better across languages by enriching their linguistic knowledge. For\nfurther implementation details of this study, the technical documentation and\nsource code are available at http://www.github.com/hitz-zentroa/whisper-lm."}
{"id": "2503.23181", "pdf": "https://arxiv.org/pdf/2503.23181", "abs": "https://arxiv.org/abs/2503.23181", "authors": ["Sunoh Kim", "Daeho Um"], "title": "Enhancing Weakly Supervised Video Grounding via Diverse Inference Strategies for Boundary and Prediction Selection", "categories": ["cs.CV"], "comment": null, "summary": "Weakly supervised video grounding aims to localize temporal boundaries\nrelevant to a given query without explicit ground-truth temporal boundaries.\nWhile existing methods primarily use Gaussian-based proposals, they overlook\nthe importance of (1) boundary prediction and (2) top-1 prediction selection\nduring inference. In their boundary prediction, boundaries are simply set at\nhalf a standard deviation away from a Gaussian mean on both sides, which may\nnot accurately capture the optimal boundaries. In the top-1 prediction process,\nthese existing methods rely heavily on intersections with other proposals,\nwithout considering the varying quality of each proposal. To address these\nissues, we explore various inference strategies by introducing (1) novel\nboundary prediction methods to capture diverse boundaries from multiple\nGaussians and (2) new selection methods that take proposal quality into\naccount. Extensive experiments on the ActivityNet Captions and Charades-STA\ndatasets validate the effectiveness of our inference strategies, demonstrating\nperformance improvements without requiring additional training."}
{"id": "2503.23547", "pdf": "https://arxiv.org/pdf/2503.23547", "abs": "https://arxiv.org/abs/2503.23547", "authors": ["Saif M. Mohammad"], "title": "NRC VAD Lexicon v2: Norms for Valence, Arousal, and Dominance for over 55k English Terms", "categories": ["cs.CL"], "comment": null, "summary": "Factor analysis studies have shown that the primary dimensions of word\nmeaning are Valence (V), Arousal (A), and Dominance (D) (also referred to in\nsocial cognition research as Competence (C)). These dimensions impact various\naspects of our lives from social competence and emotion regulation to success\nin the work place and how we view the world. We present here the NRC VAD\nLexicon v2, which has human ratings of valence, arousal, and dominance for more\nthan 55,000 English words and phrases. Notably, it adds entries for $\\sim$25k\nadditional words to v1.0. It also now includes for the first time entries for\ncommon multi-word phrases (~10k). We show that the associations are highly\nreliable. The lexicon enables a wide variety of research in psychology, NLP,\npublic health, digital humanities, and social sciences. The NRC VAD Lexicon v2\nis made freely available for research through our project webpage."}
{"id": "2503.23185", "pdf": "https://arxiv.org/pdf/2503.23185", "abs": "https://arxiv.org/abs/2503.23185", "authors": ["Shota Hirose", "Kazuki Kotoyori", "Kasidis Arunruangsirilert", "Fangzheng Lin", "Heming Sun", "Jiro Katto"], "title": "Real-time Video Prediction With Fast Video Interpolation Model and Prediction Training", "categories": ["cs.CV"], "comment": "ICIP 2024", "summary": "Transmission latency significantly affects users' quality of experience in\nreal-time interaction and actuation. As latency is principally inevitable,\nvideo prediction can be utilized to mitigate the latency and ultimately enable\nzero-latency transmission. However, most of the existing video prediction\nmethods are computationally expensive and impractical for real-time\napplications. In this work, we therefore propose real-time video prediction\ntowards the zero-latency interaction over networks, called IFRVP (Intermediate\nFeature Refinement Video Prediction). Firstly, we propose three training\nmethods for video prediction that extend frame interpolation models, where we\nutilize a simple convolution-only frame interpolation network based on IFRNet.\nSecondly, we introduce ELAN-based residual blocks into the prediction models to\nimprove both inference speed and accuracy. Our evaluations show that our\nproposed models perform efficiently and achieve the best trade-off between\nprediction accuracy and computational speed among the existing video prediction\nmethods. A demonstration movie is also provided at http://bit.ly/IFRVPDemo."}
{"id": "2503.23566", "pdf": "https://arxiv.org/pdf/2503.23566", "abs": "https://arxiv.org/abs/2503.23566", "authors": ["Haein Kong", "Seonghyeon Moon"], "title": "When LLM Therapists Become Salespeople: Evaluating Large Language Models for Ethical Motivational Interviewing", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have been actively applied in the mental health\nfield. Recent research shows the promise of LLMs in applying psychotherapy,\nespecially motivational interviewing (MI). However, there is a lack of studies\ninvestigating how language models understand MI ethics. Given the risks that\nmalicious actors can use language models to apply MI for unethical purposes, it\nis important to evaluate their capability of differentiating ethical and\nunethical MI practices. Thus, this study investigates the ethical awareness of\nLLMs in MI with multiple experiments. Our findings show that LLMs have a\nmoderate to strong level of knowledge in MI. However, their ethical standards\nare not aligned with the MI spirit, as they generated unethical responses and\nperformed poorly in detecting unethical responses. We proposed a Chain-of-Ethic\nprompt to mitigate those risks and improve safety. Finally, our proposed\nstrategy effectively improved ethical MI response generation and detection\nperformance. These findings highlight the need for safety evaluations and\nguidelines for building ethical LLM-powered psychotherapy."}
{"id": "2503.23200", "pdf": "https://arxiv.org/pdf/2503.23200", "abs": "https://arxiv.org/abs/2503.23200", "authors": ["Pengyu Chen", "Sicheng Wang", "Cuizhen Wang", "Senrong Wang", "Beiao Huang", "Lu Huang", "Zhe Zang"], "title": "A GAN-Enhanced Deep Learning Framework for Rooftop Detection from Historical Aerial Imagery", "categories": ["cs.CV"], "comment": null, "summary": "Accurate rooftop detection from historical aerial imagery is vital for\nexamining long-term urban development and human settlement patterns. However,\nblack-and-white analog photographs pose significant challenges for modern\nobject detection frameworks due to their limited spatial resolution, lack of\ncolor information, and archival degradation. To address these limitations, this\nstudy introduces a two-stage image enhancement pipeline based on Generative\nAdversarial Networks (GANs): image colorization using DeOldify, followed by\nsuper-resolution enhancement with Real-ESRGAN. The enhanced images were then\nused to train and evaluate rooftop detection models, including Faster R-CNN,\nDETReg, and YOLOv11n. Results show that combining colorization with\nsuper-resolution substantially improves detection performance, with YOLOv11n\nachieving a mean Average Precision (mAP) exceeding 85%. This reflects an\nimprovement of approximately 40% over original black-and-white images and 20%\nover images enhanced through colorization alone. The proposed method\neffectively bridges the gap between archival imagery and contemporary deep\nlearning techniques, enabling more reliable extraction of building footprints\nfrom historical aerial photographs."}
{"id": "2503.23576", "pdf": "https://arxiv.org/pdf/2503.23576", "abs": "https://arxiv.org/abs/2503.23576", "authors": ["Injy Hamed", "Ngoc Thang Vu", "Nizar Habash"], "title": "The Impact of Code-switched Synthetic Data Quality is Task Dependent: Insights from MT and ASR", "categories": ["cs.CL"], "comment": "Accepted to the Workshop on Computational Approaches to Linguistic\n  Code-Switching (CALCS)", "summary": "Code-switching, the act of alternating between languages, emerged as a\nprevalent global phenomenon that needs to be addressed for building\nuser-friendly language technologies. A main bottleneck in this pursuit is data\nscarcity, motivating research in the direction of code-switched data\naugmentation. However, current literature lacks comprehensive studies that\nenable us to understand the relation between the quality of synthetic data and\nimprovements on NLP tasks. We extend previous research conducted in this\ndirection on machine translation (MT) with results on automatic speech\nrecognition (ASR) and cascaded speech translation (ST) to test generalizability\nof findings. Our experiments involve a wide range of augmentation techniques,\ncovering lexical replacements, linguistic theories, and back-translation. Based\non the results of MT, ASR, and ST, we draw conclusions and insights regarding\nthe efficacy of various augmentation techniques and the impact of quality on\nperformance."}
{"id": "2503.23212", "pdf": "https://arxiv.org/pdf/2503.23212", "abs": "https://arxiv.org/abs/2503.23212", "authors": ["Max Gupta", "Sunayana Rane", "R. Thomas McCoy", "Thomas L. Griffiths"], "title": "Convolutional Neural Networks Can (Meta-)Learn the Same-Different Relation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "While convolutional neural networks (CNNs) have come to match and exceed\nhuman performance in many settings, the tasks these models optimize for are\nlargely constrained to the level of individual objects, such as classification\nand captioning. Humans remain vastly superior to CNNs in visual tasks involving\nrelations, including the ability to identify two objects as `same' or\n`different'. A number of studies have shown that while CNNs can be coaxed into\nlearning the same-different relation in some settings, they tend to generalize\npoorly to other instances of this relation. In this work we show that the same\nCNN architectures that fail to generalize the same-different relation with\nconventional training are able to succeed when trained via meta-learning, which\nexplicitly encourages abstraction and generalization across tasks."}
{"id": "2503.23671", "pdf": "https://arxiv.org/pdf/2503.23671", "abs": "https://arxiv.org/abs/2503.23671", "authors": ["Tongke Ni", "Yang Fan", "Junru Zhou", "Xiangping Wu", "Qingcai Chen"], "title": "CrossFormer: Cross-Segment Semantic Fusion for Document Segmentation", "categories": ["cs.CL"], "comment": "10 pages, 4 figures", "summary": "Text semantic segmentation involves partitioning a document into multiple\nparagraphs with continuous semantics based on the subject matter, contextual\ninformation, and document structure. Traditional approaches have typically\nrelied on preprocessing documents into segments to address input length\nconstraints, resulting in the loss of critical semantic information across\nsegments. To address this, we present CrossFormer, a transformer-based model\nfeaturing a novel cross-segment fusion module that dynamically models latent\nsemantic dependencies across document segments, substantially elevating\nsegmentation accuracy. Additionally, CrossFormer can replace rule-based chunk\nmethods within the Retrieval-Augmented Generation (RAG) system, producing more\nsemantically coherent chunks that enhance its efficacy. Comprehensive\nevaluations confirm CrossFormer's state-of-the-art performance on public text\nsemantic segmentation datasets, alongside considerable gains on RAG benchmarks."}
{"id": "2503.23214", "pdf": "https://arxiv.org/pdf/2503.23214", "abs": "https://arxiv.org/abs/2503.23214", "authors": ["Vincent Gbouna Zakka", "Zhuangzhuang Dai", "Luis J. Manso"], "title": "Action Recognition in Real-World Ambient Assisted Living Environment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The growing ageing population and their preference to maintain independence\nby living in their own homes require proactive strategies to ensure safety and\nsupport. Ambient Assisted Living (AAL) technologies have emerged to facilitate\nageing in place by offering continuous monitoring and assistance within the\nhome. Within AAL technologies, action recognition plays a crucial role in\ninterpreting human activities and detecting incidents like falls, mobility\ndecline, or unusual behaviours that may signal worsening health conditions.\nHowever, action recognition in practical AAL applications presents challenges,\nincluding occlusions, noisy data, and the need for real-time performance. While\nadvancements have been made in accuracy, robustness to noise, and computation\nefficiency, achieving a balance among them all remains a challenge. To address\nthis challenge, this paper introduces the Robust and Efficient Temporal\nConvolution network (RE-TCN), which comprises three main elements: Adaptive\nTemporal Weighting (ATW), Depthwise Separable Convolutions (DSC), and data\naugmentation techniques. These elements aim to enhance the model's accuracy,\nrobustness against noise and occlusion, and computational efficiency within\nreal-world AAL contexts. RE-TCN outperforms existing models in terms of\naccuracy, noise and occlusion robustness, and has been validated on four\nbenchmark datasets: NTU RGB+D 60, Northwestern-UCLA, SHREC'17, and DHG-14/28.\nThe code is publicly available at: https://github.com/Gbouna/RE-TCN"}
{"id": "2503.23673", "pdf": "https://arxiv.org/pdf/2503.23673", "abs": "https://arxiv.org/abs/2503.23673", "authors": ["Zhengyi Zhao", "Shubo Zhang", "Bin Liang", "Binyang Li", "Kam-Fai Wong"], "title": "WHERE and WHICH: Iterative Debate for Biomedical Synthetic Data Augmentation", "categories": ["cs.CL"], "comment": null, "summary": "In Biomedical Natural Language Processing (BioNLP) tasks, such as Relation\nExtraction, Named Entity Recognition, and Text Classification, the scarcity of\nhigh-quality data remains a significant challenge. This limitation poisons\nlarge language models to correctly understand relationships between biological\nentities, such as molecules and diseases, or drug interactions, and further\nresults in potential misinterpretation of biomedical documents. To address this\nissue, current approaches generally adopt the Synthetic Data Augmentation\nmethod which involves similarity computation followed by word replacement, but\ncounterfactual data are usually generated. As a result, these methods disrupt\nmeaningful word sets or produce sentences with meanings that deviate\nsubstantially from the original context, rendering them ineffective in\nimproving model performance. To this end, this paper proposes a\nbiomedical-dedicated rationale-based synthetic data augmentation method. Beyond\nthe naive lexicon similarity, specific bio-relation similarity is measured to\nhold the augmented instance having a strong correlation with bio-relation\ninstead of simply increasing the diversity of augmented data. Moreover, a\nmulti-agents-involved reflection mechanism helps the model iteratively\ndistinguish different usage of similar entities to escape falling into the\nmis-replace trap. We evaluate our method on the BLURB and BigBIO benchmark,\nwhich includes 9 common datasets spanning four major BioNLP tasks. Our\nexperimental results demonstrate consistent performance improvements across all\ntasks, highlighting the effectiveness of our approach in addressing the\nchallenges associated with data scarcity and enhancing the overall performance\nof biomedical NLP models."}
{"id": "2503.23220", "pdf": "https://arxiv.org/pdf/2503.23220", "abs": "https://arxiv.org/abs/2503.23220", "authors": ["Marc-Antoine Lavoie", "Anas Mahmoud", "Steven L. Waslander"], "title": "Large Self-Supervised Models Bridge the Gap in Domain Adaptive Object Detection", "categories": ["cs.CV"], "comment": "16 pages (8 main), 5 figures, accepted at CVPR 2025", "summary": "The current state-of-the-art methods in domain adaptive object detection\n(DAOD) use Mean Teacher self-labelling, where a teacher model, directly derived\nas an exponential moving average of the student model, is used to generate\nlabels on the target domain which are then used to improve both models in a\npositive loop. This couples learning and generating labels on the target\ndomain, and other recent works also leverage the generated labels to add\nadditional domain alignment losses. We believe this coupling is brittle and\nexcessively constrained: there is no guarantee that a student trained only on\nsource data can generate accurate target domain labels and initiate the\npositive feedback loop, and much better target domain labels can likely be\ngenerated by using a large pretrained network that has been exposed to much\nmore data. Vision foundational models are exactly such models, and they have\nshown impressive task generalization capabilities even when frozen. We want to\nleverage these models for DAOD and introduce DINO Teacher, which consists of\ntwo components. First, we train a new labeller on source data only using a\nlarge frozen DINOv2 backbone and show it generates more accurate labels than\nMean Teacher. Next, we align the student's source and target image patch\nfeatures with those from a DINO encoder, driving source and target\nrepresentations closer to the generalizable DINO representation. We obtain\nstate-of-the-art performance on multiple DAOD datasets. Code available at\nhttps://github.com/TRAILab/DINO_Teacher"}
{"id": "2503.23674", "pdf": "https://arxiv.org/pdf/2503.23674", "abs": "https://arxiv.org/abs/2503.23674", "authors": ["Cameron R. Jones", "Benjamin K. Bergen"], "title": "Large Language Models Pass the Turing Test", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "We evaluated 4 systems (ELIZA, GPT-4o, LLaMa-3.1-405B, and GPT-4.5) in two\nrandomised, controlled, and pre-registered Turing tests on independent\npopulations. Participants had 5 minute conversations simultaneously with\nanother human participant and one of these systems before judging which\nconversational partner they thought was human. When prompted to adopt a\nhumanlike persona, GPT-4.5 was judged to be the human 73% of the time:\nsignificantly more often than interrogators selected the real human\nparticipant. LLaMa-3.1, with the same prompt, was judged to be the human 56% of\nthe time -- not significantly more or less often than the humans they were\nbeing compared to -- while baseline models (ELIZA and GPT-4o) achieved win\nrates significantly below chance (23% and 21% respectively). The results\nconstitute the first empirical evidence that any artificial system passes a\nstandard three-party Turing test. The results have implications for debates\nabout what kind of intelligence is exhibited by Large Language Models (LLMs),\nand the social and economic impacts these systems are likely to have."}
{"id": "2503.23226", "pdf": "https://arxiv.org/pdf/2503.23226", "abs": "https://arxiv.org/abs/2503.23226", "authors": ["Kushal Agrawal", "Romi Banerjee"], "title": "Synthetic Art Generation and DeepFake Detection A Study on Jamini Roy Inspired Dataset", "categories": ["cs.CV", "cs.AI"], "comment": "13 pages, 7 figures, 6 tables", "summary": "The intersection of generative AI and art is a fascinating area that brings\nboth exciting opportunities and significant challenges, especially when it\ncomes to identifying synthetic artworks. This study takes a unique approach by\nexamining diffusion-based generative models in the context of Indian art,\nspecifically focusing on the distinctive style of Jamini Roy. To explore this,\nwe fine-tuned Stable Diffusion 3 and used techniques like ControlNet and\nIPAdapter to generate realistic images. This allowed us to create a new dataset\nthat includes both real and AI-generated artworks, which is essential for a\ndetailed analysis of what these models can produce. We employed various\nqualitative and quantitative methods, such as Fourier domain assessments and\nautocorrelation metrics, to uncover subtle differences between synthetic images\nand authentic pieces. A key takeaway from recent research is that existing\nmethods for detecting deepfakes face considerable challenges, especially when\nthe deepfakes are of high quality and tailored to specific cultural contexts.\nThis highlights a critical gap in current detection technologies, particularly\nin light of the challenges identified above, where high-quality and culturally\nspecific deepfakes are difficult to detect. This work not only sheds light on\nthe increasing complexity of generative models but also sets a crucial\nfoundation for future research aimed at effective detection of synthetic art."}
{"id": "2503.23687", "pdf": "https://arxiv.org/pdf/2503.23687", "abs": "https://arxiv.org/abs/2503.23687", "authors": ["Sharad Duwal"], "title": "MKA: Leveraging Cross-Lingual Consensus for Model Abstention", "categories": ["cs.CL", "cs.LG"], "comment": "To appear in Building Trust Workshop at ICLR 2025", "summary": "Reliability of LLMs is questionable even as they get better at more tasks. A\nwider adoption of LLMs is contingent on whether they are usably factual. And if\nthey are not, on whether they can properly calibrate their confidence in their\nresponses. This work focuses on utilizing the multilingual knowledge of an LLM\nto inform its decision to abstain or answer when prompted. We develop a\nmultilingual pipeline to calibrate the model's confidence and let it abstain\nwhen uncertain. We run several multilingual models through the pipeline to\nprofile them across different languages. We find that the performance of the\npipeline varies by model and language, but that in general they benefit from\nit. This is evidenced by the accuracy improvement of $71.2\\%$ for Bengali over\na baseline performance without the pipeline. Even a high-resource language like\nEnglish sees a $15.5\\%$ improvement. These results hint at possible further\nimprovements."}
{"id": "2503.23234", "pdf": "https://arxiv.org/pdf/2503.23234", "abs": "https://arxiv.org/abs/2503.23234", "authors": ["Alessio Borgi", "Luca Maiano", "Irene Amerini"], "title": "Z-SASLM: Zero-Shot Style-Aligned SLI Blending Latent Manipulation", "categories": ["cs.CV"], "comment": "Accepted to the CVPR 2025 Workshop AI for Creative Visual Content\n  Generation Editing and Understanding", "summary": "We introduce Z-SASLM, a Zero-Shot Style-Aligned SLI (Spherical Linear\nInterpolation) Blending Latent Manipulation pipeline that overcomes the\nlimitations of current multi-style blending methods. Conventional approaches\nrely on linear blending, assuming a flat latent space leading to suboptimal\nresults when integrating multiple reference styles. In contrast, our framework\nleverages the non-linear geometry of the latent space by using SLI Blending to\ncombine weighted style representations. By interpolating along the geodesic on\nthe hypersphere, Z-SASLM preserves the intrinsic structure of the latent space,\nensuring high-fidelity and coherent blending of diverse styles - all without\nthe need for fine-tuning. We further propose a new metric, Weighted Multi-Style\nDINO ViT-B/8, designed to quantitatively evaluate the consistency of the\nblended styles. While our primary focus is on the theoretical and practical\nadvantages of SLI Blending for style manipulation, we also demonstrate its\neffectiveness in a multi-modal content fusion setting through comprehensive\nexperimental studies. Experimental results show that Z-SASLM achieves enhanced\nand robust style alignment. The implementation code can be found at:\nhttps://github.com/alessioborgi/Z-SASLM."}
{"id": "2503.23688", "pdf": "https://arxiv.org/pdf/2503.23688", "abs": "https://arxiv.org/abs/2503.23688", "authors": ["William Guey", "Pierrick Bougault", "Vitor D. de Moura", "Wei Zhang", "Jose O. Gomes"], "title": "Mapping Geopolitical Bias in 11 Large Language Models: A Bilingual, Dual-Framing Analysis of U.S.-China Tensions", "categories": ["cs.CL", "cs.HC"], "comment": "Preliminary version,20 pages, 10 figures, 1 table", "summary": "This study systematically analyzes geopolitical bias across 11 prominent\nLarge Language Models (LLMs) by examining their responses to seven critical\ntopics in U.S.-China relations. Utilizing a bilingual (English and Chinese) and\ndual-framing (affirmative and reverse) methodology, we generated 19,712 prompts\ndesigned to detect ideological leanings in model outputs. Responses were\nquantitatively assessed on a normalized scale from -2 (strongly Pro-China) to\n+2 (strongly Pro-U.S.) and categorized according to stance, neutrality, and\nrefusal rates. The findings demonstrate significant and consistent ideological\nalignments correlated with the LLMs' geographic origins; U.S.-based models\npredominantly favored Pro-U.S. stances, while Chinese-origin models exhibited\npronounced Pro-China biases. Notably, language and prompt framing substantially\ninfluenced model responses, with several LLMs exhibiting stance reversals based\non prompt polarity or linguistic context. Additionally, we introduced\ncomprehensive metrics to evaluate response consistency across languages and\nframing conditions, identifying variability and vulnerabilities in model\nbehaviors. These results offer practical insights that can guide organizations\nand individuals in selecting LLMs best aligned with their operational\npriorities and geopolitical considerations, underscoring the importance of\ncareful model evaluation in politically sensitive applications. Furthermore,\nthe research highlights specific prompt structures and linguistic variations\nthat can strategically trigger distinct responses from models, revealing\nmethods for effectively navigating and influencing LLM outputs."}
{"id": "2503.23249", "pdf": "https://arxiv.org/pdf/2503.23249", "abs": "https://arxiv.org/abs/2503.23249", "authors": ["Mahtab Jamali", "Paul Davidsson", "Reza Khoshkangini", "Martin Georg Ljungqvist", "Radu-Casian Mihailescu"], "title": "Context in object detection: a systematic literature review", "categories": ["cs.CV", "I.2.10; I.4.8"], "comment": "Artificial Intelligence Review Journal", "summary": "Context is an important factor in computer vision as it offers valuable\ninformation to clarify and analyze visual data. Utilizing the contextual\ninformation inherent in an image or a video can improve the precision and\neffectiveness of object detectors. For example, where recognizing an isolated\nobject might be challenging, context information can improve comprehension of\nthe scene. This study explores the impact of various context-based approaches\nto object detection. Initially, we investigate the role of context in object\ndetection and survey it from several perspectives. We then review and discuss\nthe most recent context-based object detection approaches and compare them.\nFinally, we conclude by addressing research questions and identifying gaps for\nfurther studies. More than 265 publications are included in this survey,\ncovering different aspects of context in different categories of object\ndetection, including general object detection, video object detection, small\nobject detection, camouflaged object detection, zero-shot, one-shot, and\nfew-shot object detection. This literature review presents a comprehensive\noverview of the latest advancements in context-based object detection,\nproviding valuable contributions such as a thorough understanding of contextual\ninformation and effective methods for integrating various context types into\nobject detection, thus benefiting researchers."}
{"id": "2503.23714", "pdf": "https://arxiv.org/pdf/2503.23714", "abs": "https://arxiv.org/abs/2503.23714", "authors": ["Youmi Ma", "Sakae Mizuki", "Kazuki Fujii", "Taishi Nakamura", "Masanari Ohi", "Hinari Shimada", "Taihei Shiotani", "Koshiro Saito", "Koki Maeda", "Kakeru Hattori", "Takumi Okamoto", "Shigeki Ishida", "Rio Yokota", "Hiroya Takamura", "Naoaki Okazaki"], "title": "Building Instruction-Tuning Datasets from Human-Written Instructions with Open-Weight Large Language Models", "categories": ["cs.CL"], "comment": "15 pages, 5 figures", "summary": "Instruction tuning is crucial for enabling Large Language Models (LLMs) to\nsolve real-world tasks. Prior work has shown the effectiveness of\ninstruction-tuning data synthesized solely from LLMs, raising a fundamental\nquestion: Do we still need human-originated signals for instruction tuning?\nThis work answers the question affirmatively: we build state-of-the-art\ninstruction-tuning datasets sourced from human-written instructions, by simply\npairing them with LLM-generated responses. LLMs fine-tuned on our datasets\nconsistently outperform those fine-tuned on existing ones. Our data\nconstruction approach can be easily adapted to other languages; we build\ndatasets for Japanese and confirm that LLMs tuned with our data reach\nstate-of-the-art performance. Analyses suggest that instruction-tuning in a new\nlanguage allows LLMs to follow instructions, while the tuned models exhibit a\nnotable lack of culture-specific knowledge in that language. The datasets and\nfine-tuned models will be publicly available. Our datasets, synthesized with\nopen-weight LLMs, are openly distributed under permissive licenses, allowing\nfor diverse use cases."}
{"id": "2503.23257", "pdf": "https://arxiv.org/pdf/2503.23257", "abs": "https://arxiv.org/abs/2503.23257", "authors": ["Mohammadmahdi Honarmand", "Onur Cezmi Mutlu", "Parnian Azizian", "Saimourya Surabhi", "Dennis P. Wall"], "title": "FIESTA: Fisher Information-based Efficient Selective Test-time Adaptation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Robust facial expression recognition in unconstrained, \"in-the-wild\"\nenvironments remains challenging due to significant domain shifts between\ntraining and testing distributions. Test-time adaptation (TTA) offers a\npromising solution by adapting pre-trained models during inference without\nrequiring labeled test data. However, existing TTA approaches typically rely on\nmanually selecting which parameters to update, potentially leading to\nsuboptimal adaptation and high computational costs. This paper introduces a\nnovel Fisher-driven selective adaptation framework that dynamically identifies\nand updates only the most critical model parameters based on their importance\nas quantified by Fisher information. By integrating this principled parameter\nselection approach with temporal consistency constraints, our method enables\nefficient and effective adaptation specifically tailored for video-based facial\nexpression recognition. Experiments on the challenging AffWild2 benchmark\ndemonstrate that our approach significantly outperforms existing TTA methods,\nachieving a 7.7% improvement in F1 score over the base model while adapting\nonly 22,000 parameters-more than 20 times fewer than comparable methods. Our\nablation studies further reveal that parameter importance can be effectively\nestimated from minimal data, with sampling just 1-3 frames sufficient for\nsubstantial performance gains. The proposed approach not only enhances\nrecognition accuracy but also dramatically reduces computational overhead,\nmaking test-time adaptation more practical for real-world affective computing\napplications."}
{"id": "2503.23733", "pdf": "https://arxiv.org/pdf/2503.23733", "abs": "https://arxiv.org/abs/2503.23733", "authors": ["Yiyang Du", "Xiaochen Wang", "Chi Chen", "Jiabo Ye", "Yiru Wang", "Peng Li", "Ming Yan", "Ji Zhang", "Fei Huang", "Zhifang Sui", "Maosong Sun", "Yang Liu"], "title": "AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models with Unsupervised Coefficient Optimization", "categories": ["cs.CL", "cs.CV"], "comment": "CVPR 2025", "summary": "Recently, model merging methods have demonstrated powerful strengths in\ncombining abilities on various tasks from multiple Large Language Models\n(LLMs). While previous model merging methods mainly focus on merging\nhomogeneous models with identical architecture, they meet challenges when\ndealing with Multimodal Large Language Models (MLLMs) with inherent\nheterogeneous property, including differences in model architecture and the\nasymmetry in the parameter space. In this work, we propose AdaMMS, a novel\nmodel merging method tailored for heterogeneous MLLMs. Our method tackles the\nchallenges in three steps: mapping, merging and searching. Specifically, we\nfirst design mapping function between models to apply model merging on MLLMs\nwith different architecture. Then we apply linear interpolation on model\nweights to actively adapt the asymmetry in the heterogeneous MLLMs. Finally in\nthe hyper-parameter searching step, we propose an unsupervised hyper-parameter\nselection method for model merging. As the first model merging method capable\nof merging heterogeneous MLLMs without labeled data, extensive experiments on\nvarious model combinations demonstrated that AdaMMS outperforms previous model\nmerging methods on various vision-language benchmarks."}
{"id": "2503.23266", "pdf": "https://arxiv.org/pdf/2503.23266", "abs": "https://arxiv.org/abs/2503.23266", "authors": ["Shihao Cheng", "Jinlu Zhang", "Yue Liu", "Zhigang Tu"], "title": "OwlSight: A Robust Illumination Adaptation Framework for Dark Video Human Action Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Human action recognition in low-light environments is crucial for various\nreal-world applications. However, the existing approaches overlook the full\nutilization of brightness information throughout the training phase, leading to\nsuboptimal performance. To address this limitation, we propose OwlSight, a\nbiomimetic-inspired framework with whole-stage illumination enhancement to\ninteract with action classification for accurate dark video human action\nrecognition. Specifically, OwlSight incorporates a Time-Consistency Module\n(TCM) to capture shallow spatiotemporal features meanwhile maintaining temporal\ncoherence, which are then processed by a Luminance Adaptation Module (LAM) to\ndynamically adjust the brightness based on the input luminance distribution.\nFurthermore, a Reflect Augmentation Module (RAM) is presented to maximize\nillumination utilization and simultaneously enhance action recognition via two\ninteractive paths. Additionally, we build Dark-101, a large-scale dataset\ncomprising 18,310 dark videos across 101 action categories, significantly\nsurpassing existing datasets (e.g., ARID1.5 and Dark-48) in scale and\ndiversity. Extensive experiments demonstrate that the proposed OwlSight\nachieves state-of-the-art performance across four low-light action recognition\nbenchmarks. Notably, it outperforms previous best approaches by 5.36% on\nARID1.5 and 1.72% on Dark-101, highlighting its effectiveness in challenging\ndark environments."}
{"id": "2503.23740", "pdf": "https://arxiv.org/pdf/2503.23740", "abs": "https://arxiv.org/abs/2503.23740", "authors": ["Lu Fan", "Jiashu Pu", "Rongsheng Zhang", "Xiao-Ming Wu"], "title": "LANID: LLM-assisted New Intent Discovery", "categories": ["cs.CL", "cs.AI"], "comment": "Published in LREC-COLING 2024", "summary": "Task-oriented Dialogue Systems (TODS) often face the challenge of\nencountering new intents. New Intent Discovery (NID) is a crucial task that\naims to identify these novel intents while maintaining the capability to\nrecognize existing ones. Previous efforts to adapt TODS to new intents have\nstruggled with inadequate semantic representation or have depended on external\nknowledge, which is often not scalable or flexible. Recently, Large Language\nModels (LLMs) have demonstrated strong zero-shot capabilities; however, their\nscale can be impractical for real-world applications that involve extensive\nqueries. To address the limitations of existing NID methods by leveraging LLMs,\nwe propose LANID, a framework that enhances the semantic representation of\nlightweight NID encoders with the guidance of LLMs. Specifically, LANID employs\nthe $K$-nearest neighbors and Density-Based Spatial Clustering of Applications\nwith Noise (DBSCAN) algorithms to sample selective utterance pairs from the\ntraining set. It then queries an LLM to ascertain the relationships between\nthese pairs. The data produced from this process is utilized to design a\ncontrastive fine-tuning task, which is then used to train a small encoder with\na contrastive triplet loss. Our experimental results demonstrate the efficacy\nof the proposed method across three distinct NID datasets, surpassing strong\nbaselines in both unsupervised and semi-supervised settings. Our code is\navailable at https://github.com/floatSDSDS/LANID."}
{"id": "2503.23275", "pdf": "https://arxiv.org/pdf/2503.23275", "abs": "https://arxiv.org/abs/2503.23275", "authors": ["Deeksha Arun", "Kagan Ozturk", "Kevin W. Bowyer", "Patrick Flynn"], "title": "Improved Ear Verification with Vision Transformers and Overlapping Patches", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Ear recognition has emerged as a promising biometric modality due to the\nrelative stability in appearance during adulthood. Although Vision Transformers\n(ViTs) have been widely used in image recognition tasks, their efficiency in\near recognition has been hampered by a lack of attention to overlapping\npatches, which is crucial for capturing intricate ear features. In this study,\nwe evaluate ViT-Tiny (ViT-T), ViT-Small (ViT-S), ViT-Base (ViT-B) and ViT-Large\n(ViT-L) configurations on a diverse set of datasets (OPIB, AWE, WPUT, and\nEarVN1.0), using an overlapping patch selection strategy. Results demonstrate\nthe critical importance of overlapping patches, yielding superior performance\nin 44 of 48 experiments in a structured study. Moreover, upon comparing the\nresults of the overlapping patches with the non-overlapping configurations, the\nincrease is significant, reaching up to 10% for the EarVN1.0 dataset. In terms\nof model performance, the ViT-T model consistently outperformed the ViT-S,\nViT-B, and ViT-L models on the AWE, WPUT, and EarVN1.0 datasets. The highest\nscores were achieved in a configuration with a patch size of 28x28 and a stride\nof 14 pixels. This patch-stride configuration represents 25% of the normalized\nimage area (112x112 pixels) for the patch size and 12.5% of the row or column\nsize for the stride. This study confirms that transformer architectures with\noverlapping patch selection can serve as an efficient and high-performing\noption for ear-based biometric recognition tasks in verification scenarios."}
{"id": "2503.23768", "pdf": "https://arxiv.org/pdf/2503.23768", "abs": "https://arxiv.org/abs/2503.23768", "authors": ["Zhecheng Li", "Guoxian Song", "Yujun Cai", "Zhen Xiong", "Junsong Yuan", "Yiwei Wang"], "title": "Texture or Semantics? Vision-Language Models Get Lost in Font Recognition", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Modern Vision-Language Models (VLMs) exhibit remarkable visual and linguistic\ncapabilities, achieving impressive performance in various tasks such as image\nrecognition and object localization. However, their effectiveness in\nfine-grained tasks remains an open question. In everyday scenarios, individuals\nencountering design materials, such as magazines, typography tutorials,\nresearch papers, or branding content, may wish to identify aesthetically\npleasing fonts used in the text. Given their multimodal capabilities and free\naccessibility, many VLMs are often considered potential tools for font\nrecognition. This raises a fundamental question: Do VLMs truly possess the\ncapability to recognize fonts? To investigate this, we introduce the Font\nRecognition Benchmark (FRB), a compact and well-structured dataset comprising\n15 commonly used fonts. FRB includes two versions: (i) an easy version, where\n10 sentences are rendered in different fonts, and (ii) a hard version, where\neach text sample consists of the names of the 15 fonts themselves, introducing\na stroop effect that challenges model perception. Through extensive evaluation\nof various VLMs on font recognition tasks, we arrive at the following key\nfindings: (i) Current VLMs exhibit limited font recognition capabilities, with\nmany state-of-the-art models failing to achieve satisfactory performance. (ii)\nFew-shot learning and Chain-of-Thought (CoT) prompting provide minimal benefits\nin improving font recognition accuracy across different VLMs. (iii) Attention\nanalysis sheds light on the inherent limitations of VLMs in capturing semantic\nfeatures."}
{"id": "2503.23282", "pdf": "https://arxiv.org/pdf/2503.23282", "abs": "https://arxiv.org/abs/2503.23282", "authors": ["Felix Wimbauer", "Weirong Chen", "Dominik Muhle", "Christian Rupprecht", "Daniel Cremers"], "title": "AnyCam: Learning to Recover Camera Poses and Intrinsics from Casual Videos", "categories": ["cs.CV"], "comment": "CVPR 2025 - For more details and code, please check out our project\n  page under https://fwmb.github.io/anycam", "summary": "Estimating camera motion and intrinsics from casual videos is a core\nchallenge in computer vision. Traditional bundle-adjustment based methods, such\nas SfM and SLAM, struggle to perform reliably on arbitrary data. Although\nspecialized SfM approaches have been developed for handling dynamic scenes,\nthey either require intrinsics or computationally expensive test-time\noptimization and often fall short in performance. Recently, methods like Dust3r\nhave reformulated the SfM problem in a more data-driven way. While such\ntechniques show promising results, they are still 1) not robust towards dynamic\nobjects and 2) require labeled data for supervised training. As an alternative,\nwe propose AnyCam, a fast transformer model that directly estimates camera\nposes and intrinsics from a dynamic video sequence in feed-forward fashion. Our\nintuition is that such a network can learn strong priors over realistic camera\nposes. To scale up our training, we rely on an uncertainty-based loss\nformulation and pre-trained depth and flow networks instead of motion or\ntrajectory supervision. This allows us to use diverse, unlabelled video\ndatasets obtained mostly from YouTube. Additionally, we ensure that the\npredicted trajectory does not accumulate drift over time through a lightweight\ntrajectory refinement step. We test AnyCam on established datasets, where it\ndelivers accurate camera poses and intrinsics both qualitatively and\nquantitatively. Furthermore, even with trajectory refinement, AnyCam is\nsignificantly faster than existing works for SfM in dynamic settings. Finally,\nby combining camera information, uncertainty, and depth, our model can produce\nhigh-quality 4D pointclouds."}
{"id": "2503.23777", "pdf": "https://arxiv.org/pdf/2503.23777", "abs": "https://arxiv.org/abs/2503.23777", "authors": ["Jiangnan Li", "Thuy-Trang Vu", "Christian Herold", "Amirhossein Tebbifakhr", "Shahram Khadivi", "Gholamreza Haffari"], "title": "CONGRAD:Conflicting Gradient Filtering for Multilingual Preference Alignment", "categories": ["cs.CL"], "comment": null, "summary": "Naive joint training of large language models (LLMs) for multilingual\npreference alignment can suffer from negative interference. This is a known\nissue in multilingual training, where conflicting objectives degrade overall\nperformance. However, the impact of this phenomenon in the context of\nmultilingual preference alignment remains largely underexplored. To address\nthis issue, we propose CONGRAD, a scalable and effective filtering method that\nselects high-quality preference samples with minimal gradient conflicts across\nlanguages. Our method leverages gradient surgery to retain samples aligned with\nan aggregated multilingual update direction. Additionally, we incorporate a\nsublinear gradient compression strategy that reduces memory overhead during\ngradient accumulation. We integrate CONGRAD into self-rewarding framework and\nevaluate on LLaMA3-8B and Gemma2-2B across 10 languages. Results show that\nCONGRAD consistently outperforms strong baselines in both seen and unseen\nlanguages, with minimal alignment tax."}
{"id": "2503.23283", "pdf": "https://arxiv.org/pdf/2503.23283", "abs": "https://arxiv.org/abs/2503.23283", "authors": ["Lu Yu", "Haoyu Han", "Zhe Tao", "Hantao Yao", "Changsheng Xu"], "title": "Language Guided Concept Bottleneck Models for Interpretable Continual Learning", "categories": ["cs.CV"], "comment": "CVPR 2025; Project Page: https://github.com/FisherCats/CLG-CBM", "summary": "Continual learning (CL) aims to enable learning systems to acquire new\nknowledge constantly without forgetting previously learned information. CL\nfaces the challenge of mitigating catastrophic forgetting while maintaining\ninterpretability across tasks. Most existing CL methods focus primarily on\npreserving learned knowledge to improve model performance. However, as new\ninformation is introduced, the interpretability of the learning process becomes\ncrucial for understanding the evolving decision-making process, yet it is\nrarely explored. In this paper, we introduce a novel framework that integrates\nlanguage-guided Concept Bottleneck Models (CBMs) to address both challenges.\nOur approach leverages the Concept Bottleneck Layer, aligning semantic\nconsistency with CLIP models to learn human-understandable concepts that can\ngeneralize across tasks. By focusing on interpretable concepts, our method not\nonly enhances the models ability to retain knowledge over time but also\nprovides transparent decision-making insights. We demonstrate the effectiveness\nof our approach by achieving superior performance on several datasets,\noutperforming state-of-the-art methods with an improvement of up to 3.06% in\nfinal average accuracy on ImageNet-subset. Additionally, we offer concept\nvisualizations for model predictions, further advancing the understanding of\ninterpretable continual learning."}
{"id": "2503.23779", "pdf": "https://arxiv.org/pdf/2503.23779", "abs": "https://arxiv.org/abs/2503.23779", "authors": ["Ine Gevers", "Victor De Marez", "Luna De Bruyne", "Walter Daelemans"], "title": "WinoWhat: A Parallel Corpus of Paraphrased WinoGrande Sentences with Common Sense Categorization", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this study, we take a closer look at how Winograd schema challenges can be\nused to evaluate common sense reasoning in LLMs. Specifically, we evaluate\ngenerative models of different sizes on the popular WinoGrande benchmark. We\nrelease WinoWhat, a new corpus, in which each instance of the WinoGrande\nvalidation set is paraphrased. Additionally, we evaluate the performance on the\nchallenge across five common sense knowledge categories, giving more\nfine-grained insights on what types of knowledge are more challenging for LLMs.\nSurprisingly, all models perform significantly worse on WinoWhat, implying that\nLLM reasoning capabilities are overestimated on WinoGrande. To verify whether\nthis is an effect of benchmark memorization, we match benchmark instances to\nLLM trainingdata and create two test-suites. We observe that memorization has a\nminimal effect on model performance on WinoGrande."}
{"id": "2503.23297", "pdf": "https://arxiv.org/pdf/2503.23297", "abs": "https://arxiv.org/abs/2503.23297", "authors": ["Zhenyang Liu", "Yikai Wang", "Sixiao Zheng", "Tongying Pan", "Longfei Liang", "Yanwei Fu", "Xiangyang Xue"], "title": "ReasonGrounder: LVLM-Guided Hierarchical Feature Splatting for Open-Vocabulary 3D Visual Grounding and Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Open-vocabulary 3D visual grounding and reasoning aim to localize objects in\na scene based on implicit language descriptions, even when they are occluded.\nThis ability is crucial for tasks such as vision-language navigation and\nautonomous robotics. However, current methods struggle because they rely\nheavily on fine-tuning with 3D annotations and mask proposals, which limits\ntheir ability to handle diverse semantics and common knowledge required for\neffective reasoning. In this work, we propose ReasonGrounder, an LVLM-guided\nframework that uses hierarchical 3D feature Gaussian fields for adaptive\ngrouping based on physical scale, enabling open-vocabulary 3D grounding and\nreasoning. ReasonGrounder interprets implicit instructions using large\nvision-language models (LVLM) and localizes occluded objects through 3D\nGaussian splatting. By incorporating 2D segmentation masks from the SAM and\nmulti-view CLIP embeddings, ReasonGrounder selects Gaussian groups based on\nobject scale, enabling accurate localization through both explicit and implicit\nlanguage understanding, even in novel, occluded views. We also contribute\nReasoningGD, a new dataset containing over 10K scenes and 2 million annotations\nfor evaluating open-vocabulary 3D grounding and amodal perception under\nocclusion. Experiments show that ReasonGrounder significantly improves 3D\ngrounding accuracy in real-world scenarios."}
{"id": "2503.23798", "pdf": "https://arxiv.org/pdf/2503.23798", "abs": "https://arxiv.org/abs/2503.23798", "authors": ["Xuan Luo", "Weizhi Wang", "Xifeng Yan"], "title": "Adaptive Layer-skipping in Pre-trained LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Various layer-skipping methods have been proposed to accelerate token\ngeneration in large language models (LLMs). However, they have overlooked a\nfundamental question: How do computational demands vary across the generation\nof different tokens? In this work, we introduce FlexiDepth, a method that\ndynamically adjusts the number of Transformer layers used in text generation.\nBy incorporating a plug-in router and adapter, FlexiDepth enables adaptive\nlayer-skipping in LLMs without modifying their original parameters. Introducing\nFlexiDepth to Llama-3-8B model achieves layer skipping of 8 layers out of 32,\nand meanwhile maintains the full 100\\% benchmark performance. Experimental\nresults with FlexiDepth demonstrate that computational demands in LLMs\nsignificantly vary based on token type. Specifically, generating repetitive\ntokens or fixed phrases requires fewer layers, whereas producing tokens\ninvolving computation or high uncertainty requires more layers. Interestingly,\nthis adaptive allocation pattern aligns with human intuition. To advance\nresearch in this area, we open sourced FlexiDepth and a dataset documenting\nFlexiDepth's layer allocation patterns for future exploration."}
{"id": "2503.23300", "pdf": "https://arxiv.org/pdf/2503.23300", "abs": "https://arxiv.org/abs/2503.23300", "authors": ["Wenqi Jia", "Bolin Lai", "Miao Liu", "Danfei Xu", "James M. Rehg"], "title": "Learning Predictive Visuomotor Coordination", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Understanding and predicting human visuomotor coordination is crucial for\napplications in robotics, human-computer interaction, and assistive\ntechnologies. This work introduces a forecasting-based task for visuomotor\nmodeling, where the goal is to predict head pose, gaze, and upper-body motion\nfrom egocentric visual and kinematic observations. We propose a\n\\textit{Visuomotor Coordination Representation} (VCR) that learns structured\ntemporal dependencies across these multimodal signals. We extend a\ndiffusion-based motion modeling framework that integrates egocentric vision and\nkinematic sequences, enabling temporally coherent and accurate visuomotor\npredictions. Our approach is evaluated on the large-scale EgoExo4D dataset,\ndemonstrating strong generalization across diverse real-world activities. Our\nresults highlight the importance of multimodal integration in understanding\nvisuomotor coordination, contributing to research in visuomotor learning and\nhuman behavior modeling."}
{"id": "2503.23811", "pdf": "https://arxiv.org/pdf/2503.23811", "abs": "https://arxiv.org/abs/2503.23811", "authors": ["Chris Brogly", "Connor McElroy"], "title": "Did ChatGPT or Copilot use alter the style of internet news headlines? A time series regression analysis", "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "The release of advanced Large Language Models (LLMs) such as ChatGPT and\nCopilot is changing the way text is created and may influence the content that\nwe find on the web. This study investigated whether the release of these two\npopular LLMs coincided with a change in writing style in headlines and links on\nworldwide news websites. 175 NLP features were obtained for each text in a\ndataset of 451 million headlines/links. An interrupted time series analysis was\napplied for each of the 175 NLP features to evaluate whether there were any\nstatistically significant sustained changes after the release dates of ChatGPT\nand/or Copilot. There were a total of 44 features that did not appear to have\nany significant sustained change after the release of ChatGPT/Copilot. A total\nof 91 other features did show significant change with ChatGPT and/or Copilot\nalthough significance with earlier control LLM release dates (GPT-1/2/3,\nGopher) removed them from consideration. This initial analysis suggests these\nlanguage models may have had a limited impact on the style of individual news\nheadlines/links, with respect to only some NLP measures."}
{"id": "2503.23307", "pdf": "https://arxiv.org/pdf/2503.23307", "abs": "https://arxiv.org/abs/2503.23307", "authors": ["Cong Wei", "Bo Sun", "Haoyu Ma", "Ji Hou", "Felix Juefei-Xu", "Zecheng He", "Xiaoliang Dai", "Luxin Zhang", "Kunpeng Li", "Tingbo Hou", "Animesh Sinha", "Peter Vajda", "Wenhu Chen"], "title": "MoCha: Towards Movie-Grade Talking Character Synthesis", "categories": ["cs.CV"], "comment": "https://congwei1230.github.io/MoCha/", "summary": "Recent advancements in video generation have achieved impressive motion\nrealism, yet they often overlook character-driven storytelling, a crucial task\nfor automated film, animation generation. We introduce Talking Characters, a\nmore realistic task to generate talking character animations directly from\nspeech and text. Unlike talking head, Talking Characters aims at generating the\nfull portrait of one or more characters beyond the facial region. In this\npaper, we propose MoCha, the first of its kind to generate talking characters.\nTo ensure precise synchronization between video and speech, we propose a\nspeech-video window attention mechanism that effectively aligns speech and\nvideo tokens. To address the scarcity of large-scale speech-labeled video\ndatasets, we introduce a joint training strategy that leverages both\nspeech-labeled and text-labeled video data, significantly improving\ngeneralization across diverse character actions. We also design structured\nprompt templates with character tags, enabling, for the first time,\nmulti-character conversation with turn-based dialogue-allowing AI-generated\ncharacters to engage in context-aware conversations with cinematic coherence.\nExtensive qualitative and quantitative evaluations, including human preference\nstudies and benchmark comparisons, demonstrate that MoCha sets a new standard\nfor AI-generated cinematic storytelling, achieving superior realism,\nexpressiveness, controllability and generalization."}
{"id": "2503.23829", "pdf": "https://arxiv.org/pdf/2503.23829", "abs": "https://arxiv.org/abs/2503.23829", "authors": ["Yi Su", "Dian Yu", "Linfeng Song", "Juntao Li", "Haitao Mi", "Zhaopeng Tu", "Min Zhang", "Dong Yu"], "title": "Expanding RL with Verifiable Rewards Across Diverse Domains", "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement learning (RL) with verifiable rewards (RLVR) has shown\npromising results in mathematical reasoning and coding tasks where\nwell-structured reference answers are available. However, its applicability to\nbroader domains remains underexplored. In this work, we study the extension of\nRLVR to more diverse domains such as medicine, chemistry, psychology, and\neconomics. We observe high agreement in binary judgments across different large\nlanguage models (LLMs) when objective reference answers exist, which challenges\nthe necessity of large-scale annotation for training domain-specific reward\nmodels. To address the limitations of binary rewards when handling unstructured\nreference answers, we further incorporate model-based soft scoring into RLVR to\nimprove its flexibility. Our experiments show that a distilled generative\nreward model can serve as an effective cross-domain verifier, providing\nreliable reward signals for RL without requiring domain-specific annotations.\nBy fine-tuning a base 7B model using various RL algorithms against our reward\nmodel, we obtain policies that outperform state-of-the-art open-source aligned\nLLMs such as Qwen2.5-72B-Instruct and DeepSeek-R1-Distill-Qwen-32B by a large\nmargin, across domains in free-form answer settings. This also strengthens\nRLVR's robustness and scalability, highlighting its potential for real-world\napplications with noisy or weak labels."}
{"id": "2503.23313", "pdf": "https://arxiv.org/pdf/2503.23313", "abs": "https://arxiv.org/abs/2503.23313", "authors": ["Harshvardhan Takawale", "Nirupam Roy"], "title": "SpINR: Neural Volumetric Reconstruction for FMCW Radars", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we introduce SpINR, a novel framework for volumetric\nreconstruction using Frequency-Modulated Continuous-Wave (FMCW) radar data.\nTraditional radar imaging techniques, such as backprojection, often assume\nideal signal models and require dense aperture sampling, leading to limitations\nin resolution and generalization. To address these challenges, SpINR integrates\na fully differentiable forward model that operates natively in the frequency\ndomain with implicit neural representations (INRs). This integration leverages\nthe linear relationship between beat frequency and scatterer distance inherent\nin FMCW radar systems, facilitating more efficient and accurate learning of\nscene geometry. Additionally, by computing outputs for only the relevant\nfrequency bins, our forward model achieves greater computational efficiency\ncompared to time-domain approaches that process the entire signal before\ntransformation. Through extensive experiments, we demonstrate that SpINR\nsignificantly outperforms classical backprojection methods and existing\nlearning-based approaches, achieving higher resolution and more accurate\nreconstructions of complex scenes. This work represents the first application\nof neural volumetic reconstruction in the radar domain, offering a promising\ndirection for future research in radar-based imaging and perception systems."}
{"id": "2503.23848", "pdf": "https://arxiv.org/pdf/2503.23848", "abs": "https://arxiv.org/abs/2503.23848", "authors": ["Minghan Wang", "Ye Bai", "Yuxia Wang", "Thuy-Trang Vu", "Ehsan Shareghi", "Gholamreza Haffari"], "title": "SpeechDialogueFactory: Generating High-Quality Speech Dialogue Data to Accelerate Your Speech-LLM Development", "categories": ["cs.CL"], "comment": null, "summary": "High-quality speech dialogue datasets are crucial for Speech-LLM development,\nyet existing acquisition methods face significant limitations. Human recordings\nincur high costs and privacy concerns, while synthetic approaches often lack\nconversational authenticity. To address these challenges, we introduce\n\\textsc{SpeechDialogueFactory}, a production-ready framework for generating\nnatural speech dialogues efficiently. Our solution employs a comprehensive\npipeline including metadata generation, dialogue scripting,\nparalinguistic-enriched utterance simulation, and natural speech synthesis with\nvoice cloning. Additionally, the system provides an interactive UI for detailed\nsample inspection and a high-throughput batch synthesis mode. Evaluations show\nthat dialogues generated by our system achieve a quality comparable to human\nrecordings while significantly reducing production costs. We release our work\nas an open-source toolkit, alongside example datasets available in English and\nChinese, empowering researchers and developers in Speech-LLM research and\ndevelopment."}
{"id": "2503.23330", "pdf": "https://arxiv.org/pdf/2503.23330", "abs": "https://arxiv.org/abs/2503.23330", "authors": ["Hongxiang Jiang", "Jihao Yin", "Qixiong Wang", "Jiaqi Feng", "Guo Chen"], "title": "EagleVision: Object-level Attribute Multimodal LLM for Remote Sensing", "categories": ["cs.CV"], "comment": "Under Review", "summary": "Recent advances in multimodal large language models (MLLMs) have demonstrated\nimpressive results in various visual tasks. However, in remote sensing (RS),\nhigh resolution and small proportion of objects pose challenges to existing\nMLLMs, which struggle with object-centric tasks, particularly in precise\nlocalization and fine-grained attribute description for each object. These RS\nMLLMs have not yet surpassed classical visual perception models, as they only\nprovide coarse image understanding, leading to limited gains in real-world\nscenarios. To address this gap, we establish EagleVision, an MLLM tailored for\nremote sensing that excels in object detection and attribute comprehension.\nEquipped with the Attribute Disentangle module, EagleVision learns\ndisentanglement vision tokens to express distinct attributes. To support\nobject-level visual-language alignment, we construct EVAttrs-95K, the first\nlarge-scale object attribute understanding dataset in RS for instruction\ntuning, along with a novel evaluation benchmark, EVBench. EagleVision achieves\nstate-of-the-art performance on both fine-grained object detection and object\nattribute understanding tasks, highlighting the mutual promotion between\ndetection and understanding capabilities in MLLMs. The code, model, data, and\ndemo will be available at https://github.com/XiangTodayEatsWhat/EagleVision."}
{"id": "2503.23895", "pdf": "https://arxiv.org/pdf/2503.23895", "abs": "https://arxiv.org/abs/2503.23895", "authors": ["Yuqiao Tan", "Shizhu He", "Huanxuan Liao", "Jun Zhao", "Kang Liu"], "title": "Better wit than wealth: Dynamic Parametric Retrieval Augmented Generation for Test-time Knowledge Enhancement", "categories": ["cs.CL", "cs.AI"], "comment": "preprint", "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nretrieving relevant documents from external sources and incorporating them into\nthe context. While it improves reliability by providing factual texts, it\nsignificantly increases inference costs as context length grows and introduces\nchallenging issue of RAG hallucination, primarily caused by the lack of\ncorresponding parametric knowledge in LLMs. An efficient solution is to enhance\nthe knowledge of LLMs at test-time. Parametric RAG (PRAG) addresses this by\nembedding document into LLMs parameters to perform test-time knowledge\nenhancement, effectively reducing inference costs through offline training.\nHowever, its high training and storage costs, along with limited generalization\nability, significantly restrict its practical adoption. To address these\nchallenges, we propose Dynamic Parametric RAG (DyPRAG), a novel framework that\nleverages a lightweight parameter translator model to efficiently convert\ndocuments into parametric knowledge. DyPRAG not only reduces inference,\ntraining, and storage costs but also dynamically generates parametric\nknowledge, seamlessly enhancing the knowledge of LLMs and resolving knowledge\nconflicts in a plug-and-play manner at test-time. Extensive experiments on\nmultiple datasets demonstrate the effectiveness and generalization capabilities\nof DyPRAG, offering a powerful and practical RAG paradigm which enables\nsuperior knowledge fusion and mitigates RAG hallucination in real-world\napplications. Our code is available at https://github.com/Trae1ounG/DyPRAG."}
{"id": "2503.23331", "pdf": "https://arxiv.org/pdf/2503.23331", "abs": "https://arxiv.org/abs/2503.23331", "authors": ["Hongwei Zheng", "Han Li", "Wenrui Dai", "Ziyang Zheng", "Chenglin Li", "Junni Zou", "Hongkai Xiong"], "title": "HiPART: Hierarchical Pose AutoRegressive Transformer for Occluded 3D Human Pose Estimation", "categories": ["cs.CV", "cs.LG"], "comment": "CVPR2025", "summary": "Existing 2D-to-3D human pose estimation (HPE) methods struggle with the\nocclusion issue by enriching information like temporal and visual cues in the\nlifting stage. In this paper, we argue that these methods ignore the limitation\nof the sparse skeleton 2D input representation, which fundamentally restricts\nthe 2D-to-3D lifting and worsens the occlusion issue. To address these, we\npropose a novel two-stage generative densification method, named Hierarchical\nPose AutoRegressive Transformer (HiPART), to generate hierarchical 2D dense\nposes from the original sparse 2D pose. Specifically, we first develop a\nmulti-scale skeleton tokenization module to quantize the highly dense 2D pose\ninto hierarchical tokens and propose a Skeleton-aware Alignment to strengthen\ntoken connections. We then develop a Hierarchical AutoRegressive Modeling\nscheme for hierarchical 2D pose generation. With generated hierarchical poses\nas inputs for 2D-to-3D lifting, the proposed method shows strong robustness in\noccluded scenarios and achieves state-of-the-art performance on the\nsingle-frame-based 3D HPE. Moreover, it outperforms numerous multi-frame\nmethods while reducing parameter and computational complexity and can also\ncomplement them to further enhance performance and robustness."}
{"id": "2503.23899", "pdf": "https://arxiv.org/pdf/2503.23899", "abs": "https://arxiv.org/abs/2503.23899", "authors": ["Diana Galvan-Sosa", "Gabrielle Gaudeau", "Pride Kavumba", "Yunmeng Li", "Hongyi gu", "Zheng Yuan", "Keisuke Sakaguchi", "Paula Buttery"], "title": "Rubrik's Cube: Testing a New Rubric for Evaluating Explanations on the CUBE dataset", "categories": ["cs.CL", "I.2.7"], "comment": "9 main pages (21 appendix pages), 7 figures, submitted to ACL 2025", "summary": "The performance and usability of Large-Language Models (LLMs) are driving\ntheir use in explanation generation tasks. However, despite their widespread\nadoption, LLM explanations have been found to be unreliable, making it\ndifficult for users to distinguish good from bad explanations. To address this\nissue, we present Rubrik's CUBE, an education-inspired rubric and a dataset of\n26k explanations, written and later quality-annotated using the rubric by both\nhumans and six open- and closed-source LLMs. The CUBE dataset focuses on two\nreasoning and two language tasks, providing the necessary diversity for us to\neffectively test our proposed rubric. Using Rubrik, we find that explanations\nare influenced by both task and perceived difficulty. Low quality stems\nprimarily from a lack of conciseness in LLM-generated explanations, rather than\ncohesion and word choice. The full dataset, rubric, and code will be made\navailable upon acceptance."}
{"id": "2503.23332", "pdf": "https://arxiv.org/pdf/2503.23332", "abs": "https://arxiv.org/abs/2503.23332", "authors": ["Wenhao Luo", "Zhangyi Shen", "Ye Yao", "Feng Ding", "Guopu Zhu", "Weizhi Meng"], "title": "TraceMark-LDM: Authenticatable Watermarking for Latent Diffusion Models via Binary-Guided Rearrangement", "categories": ["cs.CV"], "comment": "14 pages, 6 figures,", "summary": "Image generation algorithms are increasingly integral to diverse aspects of\nhuman society, driven by their practical applications. However, insufficient\noversight in artificial Intelligence generated content (AIGC) can facilitate\nthe spread of malicious content and increase the risk of copyright\ninfringement. Among the diverse range of image generation models, the Latent\nDiffusion Model (LDM) is currently the most widely used, dominating the\nmajority of the Text-to-Image model market. Currently, most attribution methods\nfor LDMs rely on directly embedding watermarks into the generated images or\ntheir intermediate noise, a practice that compromises both the quality and the\nrobustness of the generated content. To address these limitations, we introduce\nTraceMark-LDM, an novel algorithm that integrates watermarking to attribute\ngenerated images while guaranteeing non-destructive performance. Unlike current\nmethods, TraceMark-LDM leverages watermarks as guidance to rearrange random\nvariables sampled from a Gaussian distribution. To mitigate potential\ndeviations caused by inversion errors, the small absolute elements are grouped\nand rearranged. Additionally, we fine-tune the LDM encoder to enhance the\nrobustness of the watermark. Experimental results show that images synthesized\nusing TraceMark-LDM exhibit superior quality and attribution accuracy compared\nto state-of-the-art (SOTA) techniques. Notably, TraceMark-LDM demonstrates\nexceptional robustness against various common attack methods, consistently\noutperforming SOTA methods."}
{"id": "2503.23913", "pdf": "https://arxiv.org/pdf/2503.23913", "abs": "https://arxiv.org/abs/2503.23913", "authors": ["Xiaoxuan Wang", "Yihe Deng", "Mingyu Derek Ma", "Wei Wang"], "title": "Entropy-Based Adaptive Weighting for Self-Training", "categories": ["cs.CL"], "comment": null, "summary": "The mathematical problem-solving capabilities of large language models have\nbecome a focal point of research, with growing interests in leveraging\nself-generated reasoning paths as a promising way to refine and enhance these\nmodels. These paths capture step-by-step logical processes while requiring only\nthe correct answer for supervision. The self-training method has been shown to\nbe effective in reasoning tasks while eliminating the need for external models\nand manual annotations. However, optimizing the use of self-generated data for\nmodel training remains an open challenge. In this work, we propose\nEntropy-Based Adaptive Weighting for Self-Training (EAST), an adaptive\nweighting strategy designed to prioritize uncertain data during self-training.\nSpecifically, EAST employs a mapping function with a tunable parameter that\ncontrols the sharpness of the weighting, assigning higher weights to data where\nthe model exhibits greater uncertainty. This approach guides the model to focus\non more informative and challenging examples, thereby enhancing its reasoning\nability. We evaluate our approach on GSM8K and MATH benchmarks. Empirical\nresults show that, while the vanilla method yields virtually no improvement\n(0%) on MATH, EAST achieves around a 1% gain over backbone model. On GSM8K,\nEAST attains a further 1-2% performance boost compared to the vanilla method."}
{"id": "2503.23337", "pdf": "https://arxiv.org/pdf/2503.23337", "abs": "https://arxiv.org/abs/2503.23337", "authors": ["Jingui Ma", "Yang Hu", "Luyang Tang", "Jiayu Yang", "Yongqi Zhai", "Ronggang Wang"], "title": "Enhancing 3D Gaussian Splatting Compression via Spatial Condition-based Prediction", "categories": ["cs.CV", "cs.MM"], "comment": "The paper has been accepted by ICME2025 in March,2025", "summary": "Recently, 3D Gaussian Spatting (3DGS) has gained widespread attention in\nNovel View Synthesis (NVS) due to the remarkable real-time rendering\nperformance. However, the substantial cost of storage and transmission of\nvanilla 3DGS hinders its further application (hundreds of megabytes or even\ngigabytes for a single scene). Motivated by the achievements of prediction in\nvideo compression, we introduce the prediction technique into the anchor-based\nGaussian representation to effectively reduce the bit rate. Specifically, we\npropose a spatial condition-based prediction module to utilize the\ngrid-captured scene information for prediction, with a residual compensation\nstrategy designed to learn the missing fine-grained information. Besides, to\nfurther compress the residual, we propose an instance-aware hyper prior,\ndeveloping a structure-aware and instance-aware entropy model. Extensive\nexperiments demonstrate the effectiveness of our prediction-based compression\nframework and each technical component. Even compared with SOTA compression\nmethod, our framework still achieves a bit rate savings of 24.42 percent. Code\nis to be released!"}
{"id": "2503.23924", "pdf": "https://arxiv.org/pdf/2503.23924", "abs": "https://arxiv.org/abs/2503.23924", "authors": ["Ziyang Ma", "Zuchao Li", "Lefei Zhang", "Gui-Song Xia", "Bo Du", "Liangpei Zhang", "Dacheng Tao"], "title": "Model Hemorrhage and the Robustness Limits of Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": "33 pages, 18 figures", "summary": "Large language models (LLMs) demonstrate strong performance across natural\nlanguage processing tasks, yet undergo significant performance degradation when\nmodified for deployment through quantization, pruning, or decoding strategy\nadjustments. We define this phenomenon as model hemorrhage - performance\ndecline caused by parameter alterations and architectural changes. Through\nsystematic analysis of various LLM frameworks, we identify key vulnerability\npatterns: layer expansion frequently disrupts attention mechanisms, compression\ntechniques induce information loss cascades, and decoding adjustments amplify\nprediction divergences. Our investigation reveals transformer architectures\nexhibit inherent robustness thresholds that determine hemorrhage severity\nacross modification types. We propose three mitigation strategies:\ngradient-aware pruning preserves critical weight pathways, dynamic quantization\nscaling maintains activation integrity, and decoding calibration aligns\ngeneration trajectories with original model distributions. This work\nestablishes foundational metrics for evaluating model stability during\nadaptation, providing practical guidelines for maintaining performance while\nenabling efficient LLM deployment. Our findings advance understanding of neural\nnetwork resilience under architectural transformations, particularly for\nlarge-scale language models."}
{"id": "2503.23344", "pdf": "https://arxiv.org/pdf/2503.23344", "abs": "https://arxiv.org/abs/2503.23344", "authors": ["Ragav Sachdeva", "Andrew Zisserman"], "title": "From Panels to Prose: Generating Literary Narratives from Comics", "categories": ["cs.CV"], "comment": null, "summary": "Comics have long been a popular form of storytelling, offering visually\nengaging narratives that captivate audiences worldwide. However, the visual\nnature of comics presents a significant barrier for visually impaired readers,\nlimiting their access to these engaging stories. In this work, we provide a\npragmatic solution to this accessibility challenge by developing an automated\nsystem that generates text-based literary narratives from manga comics. Our\napproach aims to create an evocative and immersive prose that not only conveys\nthe original narrative but also captures the depth and complexity of\ncharacters, their interactions, and the vivid settings in which they reside.\n  To this end we make the following contributions: (1) We present a unified\nmodel, Magiv3, that excels at various functional tasks pertaining to comic\nunderstanding, such as localising panels, characters, texts, and speech-bubble\ntails, performing OCR, grounding characters etc. (2) We release human-annotated\ncaptions for over 3300 Japanese comic panels, along with character grounding\nannotations, and benchmark large vision-language models in their ability to\nunderstand comic images. (3) Finally, we demonstrate how integrating large\nvision-language models with Magiv3, can generate seamless literary narratives\nthat allows visually impaired audiences to engage with the depth and richness\nof comic storytelling."}
{"id": "2503.23990", "pdf": "https://arxiv.org/pdf/2503.23990", "abs": "https://arxiv.org/abs/2503.23990", "authors": ["Yumeng Fu", "Junjie Wu", "Zhongjie Wang", "Meishan Zhang", "Yulin Wu", "Bingquan Liu"], "title": "BeMERC: Behavior-Aware MLLM-based Framework for Multimodal Emotion Recognition in Conversation", "categories": ["cs.CL"], "comment": null, "summary": "Multimodal emotion recognition in conversation (MERC), the task of\nidentifying the emotion label for each utterance in a conversation, is vital\nfor developing empathetic machines. Current MLLM-based MERC studies focus\nmainly on capturing the speaker's textual or vocal characteristics, but ignore\nthe significance of video-derived behavior information. Different from text and\naudio inputs, learning videos with rich facial expression, body language and\nposture, provides emotion trigger signals to the models for more accurate\nemotion predictions. In this paper, we propose a novel behavior-aware\nMLLM-based framework (BeMERC) to incorporate speaker's behaviors, including\nsubtle facial micro-expression, body language and posture, into a vanilla\nMLLM-based MERC model, thereby facilitating the modeling of emotional dynamics\nduring a conversation. Furthermore, BeMERC adopts a two-stage instruction\ntuning strategy to extend the model to the conversations scenario for\nend-to-end training of a MERC predictor. Experiments demonstrate that BeMERC\nachieves superior performance than the state-of-the-art methods on two\nbenchmark datasets, and also provides a detailed discussion on the significance\nof video-derived behavior information in MERC."}
{"id": "2503.23353", "pdf": "https://arxiv.org/pdf/2503.23353", "abs": "https://arxiv.org/abs/2503.23353", "authors": ["Xiangyang Luo", "Junhao Cheng", "Yifan Xie", "Xin Zhang", "Tao Feng", "Zhou Liu", "Fei Ma", "Fei Yu"], "title": "Object Isolated Attention for Consistent Story Visualization", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages, 4 figures", "summary": "Open-ended story visualization is a challenging task that involves generating\ncoherent image sequences from a given storyline. One of the main difficulties\nis maintaining character consistency while creating natural and contextually\nfitting scenes--an area where many existing methods struggle. In this paper, we\npropose an enhanced Transformer module that uses separate self attention and\ncross attention mechanisms, leveraging prior knowledge from pre-trained\ndiffusion models to ensure logical scene creation. The isolated self attention\nmechanism improves character consistency by refining attention maps to reduce\nfocus on irrelevant areas and highlight key features of the same character.\nMeanwhile, the isolated cross attention mechanism independently processes each\ncharacter's features, avoiding feature fusion and further strengthening\nconsistency. Notably, our method is training-free, allowing the continuous\ngeneration of new characters and storylines without re-tuning. Both qualitative\nand quantitative evaluations show that our approach outperforms current\nmethods, demonstrating its effectiveness."}
{"id": "2503.24006", "pdf": "https://arxiv.org/pdf/2503.24006", "abs": "https://arxiv.org/abs/2503.24006", "authors": ["Safa Alsaidi", "Marc Vincent", "Olivia Boyer", "Nicolas Garcelon", "Miguel Couceiro", "Adrien Coulet"], "title": "Comparing representations of long clinical texts for the task of patient note-identification", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we address the challenge of patient-note identification, which\ninvolves accurately matching an anonymized clinical note to its corresponding\npatient, represented by a set of related notes. This task has broad\napplications, including duplicate records detection and patient similarity\nanalysis, which require robust patient-level representations. We explore\nvarious embedding methods, including Hierarchical Attention Networks (HAN),\nthree-level Hierarchical Transformer Networks (HTN), LongFormer, and advanced\nBERT-based models, focusing on their ability to process mediumto-long clinical\ntexts effectively. Additionally, we evaluate different pooling strategies\n(mean, max, and mean_max) for aggregating wordlevel embeddings into\npatient-level representations and we examine the impact of sliding windows on\nmodel performance. Our results indicate that BERT-based embeddings outperform\ntraditional and hierarchical models, particularly in processing lengthy\nclinical notes and capturing nuanced patient representations. Among the pooling\nstrategies, mean_max pooling consistently yields the best results, highlighting\nits ability to capture critical features from clinical notes. Furthermore, the\nreproduction of our results on both MIMIC dataset and Necker hospital data\nwarehouse illustrates the generalizability of these approaches to real-world\napplications, emphasizing the importance of both embedding methods and\naggregation strategies in optimizing patient-note identification and enhancing\npatient-level modeling."}
{"id": "2503.23355", "pdf": "https://arxiv.org/pdf/2503.23355", "abs": "https://arxiv.org/abs/2503.23355", "authors": ["Linfeng Tang", "Chunyu Li", "Guoqing Wang", "Yixuan Yuan", "Jiayi Ma"], "title": "DSPFusion: Image Fusion via Degradation and Semantic Dual-Prior Guidance", "categories": ["cs.CV"], "comment": null, "summary": "Existing fusion methods are tailored for high-quality images but struggle\nwith degraded images captured under harsh circumstances, thus limiting the\npractical potential of image fusion. This work presents a \\textbf{D}egradation\nand \\textbf{S}emantic \\textbf{P}rior dual-guided framework for degraded image\n\\textbf{Fusion} (\\textbf{DSPFusion}), utilizing degradation priors and\nhigh-quality scene semantic priors restored via diffusion models to guide both\ninformation recovery and fusion in a unified model. In specific, it first\nindividually extracts modality-specific degradation priors, while jointly\ncapturing comprehensive low-quality semantic priors. Subsequently, a diffusion\nmodel is developed to iteratively restore high-quality semantic priors in a\ncompact latent space, enabling our method to be over $20 \\times$ faster than\nmainstream diffusion model-based image fusion schemes. Finally, the degradation\npriors and high-quality semantic priors are employed to guide information\nenhancement and aggregation via the dual-prior guidance and prior-guided fusion\nmodules. Extensive experiments demonstrate that DSPFusion mitigates most\ntypical degradations while integrating complementary context with minimal\ncomputational cost, greatly broadening the application scope of image fusion."}
{"id": "2503.24013", "pdf": "https://arxiv.org/pdf/2503.24013", "abs": "https://arxiv.org/abs/2503.24013", "authors": ["Gergely Flamich", "David Vilar", "Jan-Thorsten Peter", "Markus Freitag"], "title": "You Cannot Feed Two Birds with One Score: the Accuracy-Naturalness Tradeoff in Translation", "categories": ["cs.CL"], "comment": null, "summary": "The goal of translation, be it by human or by machine, is, given some text in\na source language, to produce text in a target language that simultaneously 1)\npreserves the meaning of the source text and 2) achieves natural expression in\nthe target language. However, researchers in the machine translation community\nusually assess translations using a single score intended to capture semantic\naccuracy and the naturalness of the output simultaneously. In this paper, we\nbuild on recent advances in information theory to mathematically prove and\nempirically demonstrate that such single-score summaries do not and cannot give\nthe complete picture of a system's true performance. Concretely, we prove that\na tradeoff exists between accuracy and naturalness and demonstrate it by\nevaluating the submissions to the WMT24 shared task. Our findings help explain\nwell-known empirical phenomena, such as the observation that optimizing\ntranslation systems for a specific accuracy metric (like BLEU) initially\nimproves the system's naturalness, while ``overfitting'' the system to the\nmetric can significantly degrade its naturalness. Thus, we advocate for a\nchange in how translations are evaluated: rather than comparing systems using a\nsingle number, they should be compared on an accuracy-naturalness plane."}
{"id": "2503.23356", "pdf": "https://arxiv.org/pdf/2503.23356", "abs": "https://arxiv.org/abs/2503.23356", "authors": ["Linfeng Tang", "Yeda Wang", "Zhanchuan Cai", "Junjun Jiang", "Jiayi Ma"], "title": "ControlFusion: A Controllable Image Fusion Framework with Language-Vision Degradation Prompts", "categories": ["cs.CV"], "comment": null, "summary": "Current image fusion methods struggle to address the composite degradations\nencountered in real-world imaging scenarios and lack the flexibility to\naccommodate user-specific requirements. In response to these challenges, we\npropose a controllable image fusion framework with language-vision prompts,\ntermed ControlFusion, which adaptively neutralizes composite degradations. On\nthe one hand, we develop a degraded imaging model that integrates physical\nimaging mechanisms, including the Retinex theory and atmospheric scattering\nprinciple, to simulate composite degradations, thereby providing potential for\naddressing real-world complex degradations from the data level. On the other\nhand, we devise a prompt-modulated restoration and fusion network that\ndynamically enhances features with degradation prompts, enabling our method to\naccommodate composite degradation of varying levels. Specifically, considering\nindividual variations in quality perception of users, we incorporate a text\nencoder to embed user-specified degradation types and severity levels as\ndegradation prompts. We also design a spatial-frequency collaborative visual\nadapter that autonomously perceives degradations in source images, thus\neliminating the complete dependence on user instructions. Extensive experiments\ndemonstrate that ControlFusion outperforms SOTA fusion methods in fusion\nquality and degradation handling, particularly in countering real-world and\ncompound degradations with various levels."}
{"id": "2503.24027", "pdf": "https://arxiv.org/pdf/2503.24027", "abs": "https://arxiv.org/abs/2503.24027", "authors": ["Florian Carichon", "Romain Rampa", "Golnoosh Farnadi"], "title": "Crossing Boundaries: Leveraging Semantic Divergences to Explore Cultural Novelty in Cooking Recipes", "categories": ["cs.CL"], "comment": null, "summary": "Novelty modeling and detection is a core topic in Natural Language Processing\n(NLP), central to numerous tasks such as recommender systems and automatic\nsummarization. It involves identifying pieces of text that deviate in some way\nfrom previously known information. However, novelty is also a crucial\ndeterminant of the unique perception of relevance and quality of an experience,\nas it rests upon each individual's understanding of the world. Social factors,\nparticularly cultural background, profoundly influence perceptions of novelty\nand innovation. Cultural novelty arises from differences in salience and\nnovelty as shaped by the distance between distinct communities. While cultural\ndiversity has garnered increasing attention in artificial intelligence (AI),\nthe lack of robust metrics for quantifying cultural novelty hinders a deeper\nunderstanding of these divergences. This gap limits quantifying and\nunderstanding cultural differences within computational frameworks. To address\nthis, we propose an interdisciplinary framework that integrates knowledge from\nsociology and management. Central to our approach is GlobalFusion, a novel\ndataset comprising 500 dishes and approximately 100,000 cooking recipes\ncapturing cultural adaptation from over 150 countries. By introducing a set of\nJensen-Shannon Divergence metrics for novelty, we leverage this dataset to\nanalyze textual divergences when recipes from one community are modified by\nanother with a different cultural background. The results reveal significant\ncorrelations between our cultural novelty metrics and established cultural\nmeasures based on linguistic, religious, and geographical distances. Our\nfindings highlight the potential of our framework to advance the understanding\nand measurement of cultural diversity in AI."}
{"id": "2503.23359", "pdf": "https://arxiv.org/pdf/2503.23359", "abs": "https://arxiv.org/abs/2503.23359", "authors": ["Linfeng Tang", "Yeda Wang", "Meiqi Gong", "Zizhuo Li", "Yuxin Deng", "Xunpeng Yi", "Chunyu Li", "Han Xu", "Hao Zhang", "Jiayi Ma"], "title": "VideoFusion: A Spatio-Temporal Collaborative Network for Mutli-modal Video Fusion and Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Compared to images, videos better align with real-world acquisition scenarios\nand possess valuable temporal cues. However, existing multi-sensor fusion\nresearch predominantly integrates complementary context from multiple images\nrather than videos. This primarily stems from two factors: 1) the scarcity of\nlarge-scale multi-sensor video datasets, limiting research in video fusion, and\n2) the inherent difficulty of jointly modeling spatial and temporal\ndependencies in a unified framework. This paper proactively compensates for the\ndilemmas. First, we construct M3SVD, a benchmark dataset with $220$ temporally\nsynchronized and spatially registered infrared-visible video pairs comprising\n153,797 frames, filling the data gap for the video fusion community. Secondly,\nwe propose VideoFusion, a multi-modal video fusion model that fully exploits\ncross-modal complementarity and temporal dynamics to generate spatio-temporally\ncoherent videos from (potentially degraded) multi-modal inputs. Specifically,\n1) a differential reinforcement module is developed for cross-modal information\ninteraction and enhancement, 2) a complete modality-guided fusion strategy is\nemployed to adaptively integrate multi-modal features, and 3) a bi-temporal\nco-attention mechanism is devised to dynamically aggregate forward-backward\ntemporal contexts to reinforce cross-frame feature representations. Extensive\nexperiments reveal that VideoFusion outperforms existing image-oriented fusion\nparadigms in sequential scenarios, effectively mitigating temporal\ninconsistency and interference."}
{"id": "2503.24062", "pdf": "https://arxiv.org/pdf/2503.24062", "abs": "https://arxiv.org/abs/2503.24062", "authors": ["Fatemeh Mohammadi", "Tommaso Romano", "Samira Maghool", "Paolo Ceravolo"], "title": "Artificial Conversations, Real Results: Fostering Language Detection with Synthetic Data", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Collecting high-quality training data is essential for fine-tuning Large\nLanguage Models (LLMs). However, acquiring such data is often costly and\ntime-consuming, especially for non-English languages such as Italian. Recently,\nresearchers have begun to explore the use of LLMs to generate synthetic\ndatasets as a viable alternative. This study proposes a pipeline for generating\nsynthetic data and a comprehensive approach for investigating the factors that\ninfluence the validity of synthetic data generated by LLMs by examining how\nmodel performance is affected by metrics such as prompt strategy, text length\nand target position in a specific task, i.e. inclusive language detection in\nItalian job advertisements. Our results show that, in most cases and across\ndifferent metrics, the fine-tuned models trained on synthetic data consistently\noutperformed other models on both real and synthetic test datasets. The study\ndiscusses the practical implications and limitations of using synthetic data\nfor language detection tasks with LLMs."}
{"id": "2503.23365", "pdf": "https://arxiv.org/pdf/2503.23365", "abs": "https://arxiv.org/abs/2503.23365", "authors": ["Zhangcun Yan", "Jianqing Li", "Peng Hang", "Jian Sun"], "title": "OnSiteVRU: A High-Resolution Trajectory Dataset for High-Density Vulnerable Road Users", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "With the acceleration of urbanization and the growth of transportation\ndemands, the safety of vulnerable road users (VRUs, such as pedestrians and\ncyclists) in mixed traffic flows has become increasingly prominent,\nnecessitating high-precision and diverse trajectory data to support the\ndevelopment and optimization of autonomous driving systems. However, existing\ndatasets fall short in capturing the diversity and dynamics of VRU behaviors,\nmaking it difficult to meet the research demands of complex traffic\nenvironments. To address this gap, this study developed the OnSiteVRU datasets,\nwhich cover a variety of scenarios, including intersections, road segments, and\nurban villages. These datasets provide trajectory data for motor vehicles,\nelectric bicycles, and human-powered bicycles, totaling approximately 17,429\ntrajectories with a precision of 0.04 seconds. The datasets integrate both\naerial-view natural driving data and onboard real-time dynamic detection data,\nalong with environmental information such as traffic signals, obstacles, and\nreal-time maps, enabling a comprehensive reconstruction of interaction events.\nThe results demonstrate that VRU\\_Data outperforms traditional datasets in\nterms of VRU density and scene coverage, offering a more comprehensive\nrepresentation of VRU behavioral characteristics. This provides critical\nsupport for traffic flow modeling, trajectory prediction, and autonomous\ndriving virtual testing. The dataset is publicly available for download at:\n  https://www.kaggle.com/datasets/zcyan2/mixed-traffic-trajectory-dataset-in-from-shanghai."}
{"id": "2503.24102", "pdf": "https://arxiv.org/pdf/2503.24102", "abs": "https://arxiv.org/abs/2503.24102", "authors": ["Yewei Song", "Lujun Li", "Cedric Lothritz", "Saad Ezzini", "Lama Sleem", "Niccolo Gentile", "Radu State", "Tegawendé F. Bissyandé", "Jacques Klein"], "title": "Is LLM the Silver Bullet to Low-Resource Languages Machine Translation?", "categories": ["cs.CL"], "comment": null, "summary": "Low-Resource Languages (LRLs) present significant challenges in natural\nlanguage processing due to their limited linguistic resources and\nunderrepresentation in standard datasets. While recent advancements in Large\nLanguage Models (LLMs) and Neural Machine Translation (NMT) have substantially\nimproved translation capabilities for high-resource languages, performance\ndisparities persist for LRLs, particularly impacting privacy-sensitive and\nresource-constrained scenarios. This paper systematically evaluates the\nlimitations of current LLMs across 200 languages using benchmarks such as\nFLORES-200. We also explore alternative data sources, including news articles\nand bilingual dictionaries, and demonstrate how knowledge distillation from\nlarge pre-trained models can significantly improve smaller LRL translations.\nAdditionally, we investigate various fine-tuning strategies, revealing that\nincremental enhancements markedly reduce performance gaps on smaller LLMs."}
{"id": "2503.23367", "pdf": "https://arxiv.org/pdf/2503.23367", "abs": "https://arxiv.org/abs/2503.23367", "authors": ["Hang Guo", "Yawei Li", "Taolin Zhang", "Jiangshan Wang", "Tao Dai", "Shu-Tao Xia", "Luca Benini"], "title": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning", "categories": ["cs.CV"], "comment": "Technical Report", "summary": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR."}
{"id": "2503.24115", "pdf": "https://arxiv.org/pdf/2503.24115", "abs": "https://arxiv.org/abs/2503.24115", "authors": ["Zhiming Ma", "Peidong Wang", "Minhua Huang", "Jingpeng Wang", "Kai Wu", "Xiangzhao Lv", "Yachun Pang", "Yin Yang", "Wenjie Tang", "Yuchen Kang"], "title": "TeleAntiFraud-28k: A Audio-Text Slow-Thinking Dataset for Telecom Fraud Detection", "categories": ["cs.CL", "cs.MM"], "comment": null, "summary": "The detection of telecom fraud faces significant challenges due to the lack\nof high-quality multimodal training data that integrates audio signals with\nreasoning-oriented textual analysis. To address this gap, we present\nTeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset\nspecifically designed for automated telecom fraud analysis. Our dataset is\nconstructed through three strategies: (1) Privacy-preserved text-truth sample\ngeneration using automatically speech recognition (ASR)-transcribed call\nrecordings (with anonymized original audio), ensuring real-world consistency\nthrough text-to-speech (TTS) model regeneration; (2) Semantic enhancement via\nlarge language model (LLM)-based self-instruction sampling on authentic ASR\noutputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that\nsimulates emerging fraud tactics through predefined communication scenarios and\nfraud typologies. The generated dataset contains 28,511 rigorously processed\nspeech-text pairs, complete with detailed annotations for fraud reasoning. The\ndataset is divided into three tasks: scenario classification, fraud detection,\nfraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a\nstandardized evaluation benchmark comprising proportionally sampled instances\nfrom the dataset, to facilitate systematic testing of model performance on\ntelecom fraud detection tasks. We also contribute a production-optimized\nsupervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while\nopen-sourcing the data processing framework to enable community-driven dataset\nexpansion. This work establishes a foundational framework for multimodal\nanti-fraud research while addressing critical challenges in data privacy and\nscenario diversity. The project will be released at\nhttps://github.com/JimmyMa99/TeleAntiFraud."}
{"id": "2503.23368", "pdf": "https://arxiv.org/pdf/2503.23368", "abs": "https://arxiv.org/abs/2503.23368", "authors": ["Xindi Yang", "Baolu Li", "Yiming Zhang", "Zhenfei Yin", "Lei Bai", "Liqian Ma", "Zhiyong Wang", "Jianfei Cai", "Tien-Tsin Wong", "Huchuan Lu", "Xu Jia"], "title": "Towards Physically Plausible Video Generation via VLM Planning", "categories": ["cs.CV", "cs.AI"], "comment": "18 pages, 11 figures", "summary": "Video diffusion models (VDMs) have advanced significantly in recent years,\nenabling the generation of highly realistic videos and drawing the attention of\nthe community in their potential as world simulators. However, despite their\ncapabilities, VDMs often fail to produce physically plausible videos due to an\ninherent lack of understanding of physics, resulting in incorrect dynamics and\nevent sequences. To address this limitation, we propose a novel two-stage\nimage-to-video generation framework that explicitly incorporates physics. In\nthe first stage, we employ a Vision Language Model (VLM) as a coarse-grained\nmotion planner, integrating chain-of-thought and physics-aware reasoning to\npredict a rough motion trajectories/changes that approximate real-world\nphysical dynamics while ensuring the inter-frame consistency. In the second\nstage, we use the predicted motion trajectories/changes to guide the video\ngeneration of a VDM. As the predicted motion trajectories/changes are rough,\nnoise is added during inference to provide freedom to the VDM in generating\nmotion with more fine details. Extensive experimental results demonstrate that\nour framework can produce physically plausible motion, and comparative\nevaluations highlight the notable superiority of our approach over existing\nmethods. More video results are available on our Project Page:\nhttps://madaoer.github.io/projects/physically_plausible_video_generation."}
{"id": "2503.24116", "pdf": "https://arxiv.org/pdf/2503.24116", "abs": "https://arxiv.org/abs/2503.24116", "authors": ["Anna Shopova", "Cristoph Lippert", "Leslee J. Shaw", "Eugenia Alleva"], "title": "Multi-Task Learning for Extracting Menstrual Characteristics from Clinical Notes", "categories": ["cs.CL"], "comment": null, "summary": "Menstrual health is a critical yet often overlooked aspect of women's\nhealthcare. Despite its clinical relevance, detailed data on menstrual\ncharacteristics is rarely available in structured medical records. To address\nthis gap, we propose a novel Natural Language Processing pipeline to extract\nkey menstrual cycle attributes -- dysmenorrhea, regularity, flow volume, and\nintermenstrual bleeding. Our approach utilizes the GatorTron model with\nMulti-Task Prompt-based Learning, enhanced by a hybrid retrieval preprocessing\nstep to identify relevant text segments. It out- performs baseline methods,\nachieving an average F1-score of 90% across all menstrual characteristics,\ndespite being trained on fewer than 100 annotated clinical notes. The retrieval\nstep consistently improves performance across all approaches, allowing the\nmodel to focus on the most relevant segments of lengthy clinical notes. These\nresults show that combining multi-task learning with retrieval improves\ngeneralization and performance across menstrual charac- teristics, advancing\nautomated extraction from clinical notes and supporting women's health\nresearch."}
{"id": "2503.23370", "pdf": "https://arxiv.org/pdf/2503.23370", "abs": "https://arxiv.org/abs/2503.23370", "authors": ["Chenxing Sun", "Jing Bai"], "title": "Map Feature Perception Metric for Map Generation Quality Assessment and Loss Optimization", "categories": ["cs.CV"], "comment": null, "summary": "In intelligent cartographic generation tasks empowered by generative models,\nthe authenticity of synthesized maps constitutes a critical determinant.\nConcurrently, the selection of appropriate evaluation metrics to quantify map\nauthenticity emerges as a pivotal research challenge. Current methodologies\npredominantly adopt computer vision-based image assessment metrics to compute\ndiscrepancies between generated and reference maps. However, conventional\nvisual similarity metrics-including L1, L2, SSIM, and FID-primarily operate at\npixel-level comparisons, inadequately capturing cartographic global features\nand spatial correlations, consequently inducing semantic-structural artifacts\nin generated outputs. This study introduces a novel Map Feature Perception\nMetric designed to evaluate global characteristics and spatial congruence\nbetween synthesized and target maps. Diverging from pixel-wise metrics, our\napproach extracts elemental-level deep features that comprehensively encode\ncartographic structural integrity and topological relationships. Experimental\nvalidation demonstrates MFP's superior capability in evaluating cartographic\nsemantic features, with classification-enhanced implementations outperforming\nconventional loss functions across diverse generative frameworks. When employed\nas optimization objectives, our metric achieves performance gains ranging from\n2% to 50% across multiple benchmarks compared to traditional L1, L2, and SSIM\nbaselines. This investigation concludes that explicit consideration of\ncartographic global attributes and spatial coherence substantially enhances\ngenerative model optimization, thereby significantly improving the geographical\nplausibility of synthesized maps."}
{"id": "2503.24190", "pdf": "https://arxiv.org/pdf/2503.24190", "abs": "https://arxiv.org/abs/2503.24190", "authors": ["Xiaomeng Ma", "Qihui Xu"], "title": "Implicit In-Context Learning: Evidence from Artificial Language Experiments", "categories": ["cs.CL"], "comment": null, "summary": "Humans acquire language through implicit learning, absorbing complex patterns\nwithout explicit awareness. While LLMs demonstrate impressive linguistic\ncapabilities, it remains unclear whether they exhibit human-like pattern\nrecognition during in-context learning at inferencing level. We adapted three\nclassic artificial language learning experiments spanning morphology,\nmorphosyntax, and syntax to systematically evaluate implicit learning at\ninferencing level in two state-of-the-art OpenAI models: gpt-4o and o3-mini.\nOur results reveal linguistic domain-specific alignment between models and\nhuman behaviors, o3-mini aligns better in morphology while both models align in\nsyntax."}
{"id": "2503.23377", "pdf": "https://arxiv.org/pdf/2503.23377", "abs": "https://arxiv.org/abs/2503.23377", "authors": ["Kai Liu", "Wei Li", "Lai Chen", "Shengqiong Wu", "Yanhao Zheng", "Jiayi Ji", "Fan Zhou", "Rongxin Jiang", "Jiebo Luo", "Hao Fei", "Tat-Seng Chua"], "title": "JavisDiT: Joint Audio-Video Diffusion Transformer with Hierarchical Spatio-Temporal Prior Synchronization", "categories": ["cs.CV", "cs.AI", "cs.SD", "eess.AS"], "comment": "Work in progress. Homepage: https://javisdit.github.io/", "summary": "This paper introduces JavisDiT, a novel Joint Audio-Video Diffusion\nTransformer designed for synchronized audio-video generation (JAVG). Built upon\nthe powerful Diffusion Transformer (DiT) architecture, JavisDiT is able to\ngenerate high-quality audio and video content simultaneously from open-ended\nuser prompts. To ensure optimal synchronization, we introduce a fine-grained\nspatio-temporal alignment mechanism through a Hierarchical Spatial-Temporal\nSynchronized Prior (HiST-Sypo) Estimator. This module extracts both global and\nfine-grained spatio-temporal priors, guiding the synchronization between the\nvisual and auditory components. Furthermore, we propose a new benchmark,\nJavisBench, consisting of 10,140 high-quality text-captioned sounding videos\nspanning diverse scenes and complex real-world scenarios. Further, we\nspecifically devise a robust metric for evaluating the synchronization between\ngenerated audio-video pairs in real-world complex content. Experimental results\ndemonstrate that JavisDiT significantly outperforms existing methods by\nensuring both high-quality generation and precise synchronization, setting a\nnew standard for JAVG tasks. Our code, model, and dataset will be made publicly\navailable at https://javisdit.github.io/."}
{"id": "2503.24198", "pdf": "https://arxiv.org/pdf/2503.24198", "abs": "https://arxiv.org/abs/2503.24198", "authors": ["Jingxian Xu", "Mengyu Zhou", "Weichang Liu", "Hanbing Liu", "Shi Han", "Dongmei Zhang"], "title": "TwT: Thinking without Tokens by Habitual Reasoning Distillation with Multi-Teachers' Guidance", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have made significant strides in problem-solving\nby incorporating reasoning processes. However, this enhanced reasoning\ncapability results in an increased number of output tokens during inference,\nleading to higher computational costs. To address this challenge, we propose\nTwT (Thinking without Tokens), a method that reduces inference-time costs\nthrough habitual reasoning distillation with multi-teachers' guidance, while\nmaintaining high performance. Our approach introduces a Habitual Reasoning\nDistillation method, which internalizes explicit reasoning into the model's\nhabitual behavior through a Teacher-Guided compression strategy inspired by\nhuman cognition. Additionally, we propose Dual-Criteria Rejection Sampling\n(DCRS), a technique that generates a high-quality and diverse distillation\ndataset using multiple teacher models, making our method suitable for\nunsupervised scenarios. Experimental results demonstrate that TwT effectively\nreduces inference costs while preserving superior performance, achieving up to\na 13.6% improvement in accuracy with fewer output tokens compared to other\ndistillation methods, offering a highly practical solution for efficient LLM\ndeployment."}
{"id": "2503.23379", "pdf": "https://arxiv.org/pdf/2503.23379", "abs": "https://arxiv.org/abs/2503.23379", "authors": ["Haiduo Huang", "Yadong Zhang", "Pengju Ren"], "title": "KernelDNA: Dynamic Kernel Sharing via Decoupled Naive Adapters", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Dynamic convolution enhances model capacity by adaptively combining multiple\nkernels, yet faces critical trade-offs: prior works either (1) incur\nsignificant parameter overhead by scaling kernel numbers linearly, (2)\ncompromise inference speed through complex kernel interactions, or (3) struggle\nto jointly optimize dynamic attention and static kernels. We also observe that\npre-trained Convolutional Neural Networks (CNNs) exhibit inter-layer redundancy\nakin to that in Large Language Models (LLMs). Specifically, dense convolutional\nlayers can be efficiently replaced by derived ``child\" layers generated from a\nshared ``parent\" convolutional kernel through an adapter.\n  To address these limitations and implement the weight-sharing mechanism, we\npropose a lightweight convolution kernel plug-in, named KernelDNA. It decouples\nkernel adaptation into input-dependent dynamic routing and pre-trained static\nmodulation, ensuring both parameter efficiency and hardware-friendly inference.\nUnlike existing dynamic convolutions that expand parameters via multi-kernel\nensembles, our method leverages cross-layer weight sharing and adapter-based\nmodulation, enabling dynamic kernel specialization without altering the\nstandard convolution structure. This design preserves the native computational\nefficiency of standard convolutions while enhancing representation power\nthrough input-adaptive kernel adjustments. Experiments on image classification\nand dense prediction tasks demonstrate that KernelDNA achieves state-of-the-art\naccuracy-efficiency balance among dynamic convolution variants. Our codes are\navailable at https://github.com/haiduo/KernelDNA."}
{"id": "2503.24206", "pdf": "https://arxiv.org/pdf/2503.24206", "abs": "https://arxiv.org/abs/2503.24206", "authors": ["Abdul Sittar", "Luka Golob", "Mateja Smiljanic"], "title": "Synthetic News Generation for Fake News Classification", "categories": ["cs.CL"], "comment": "13 pages, 8 figures", "summary": "This study explores the generation and evaluation of synthetic fake news\nthrough fact based manipulations using large language models (LLMs). We\nintroduce a novel methodology that extracts key facts from real articles,\nmodifies them, and regenerates content to simulate fake news while maintaining\ncoherence. To assess the quality of the generated content, we propose a set of\nevaluation metrics coherence, dissimilarity, and correctness. The research also\ninvestigates the application of synthetic data in fake news classification,\ncomparing traditional machine learning models with transformer based models\nsuch as BERT. Our experiments demonstrate that transformer models, especially\nBERT, effectively leverage synthetic data for fake news detection, showing\nimprovements with smaller proportions of synthetic data. Additionally, we find\nthat fact verification features, which focus on identifying factual\ninconsistencies, provide the most promising results in distinguishing synthetic\nfake news. The study highlights the potential of synthetic data to enhance fake\nnews detection systems, offering valuable insights for future research and\nsuggesting that targeted improvements in synthetic data generation can further\nstrengthen detection models."}
{"id": "2503.23381", "pdf": "https://arxiv.org/pdf/2503.23381", "abs": "https://arxiv.org/abs/2503.23381", "authors": ["Jiexin Wang", "Wenwen Qiang", "Zhao Yang", "Bing Su"], "title": "Enhancing Human Motion Prediction via Multi-range Decoupling Decoding with Gating-adjusting Aggregation", "categories": ["cs.CV"], "comment": null, "summary": "Expressive representation of pose sequences is crucial for accurate motion\nmodeling in human motion prediction (HMP). While recent deep learning-based\nmethods have shown promise in learning motion representations, these methods\ntend to overlook the varying relevance and dependencies between historical\ninformation and future moments, with a stronger correlation for short-term\npredictions and weaker for distant future predictions. This limits the learning\nof motion representation and then hampers prediction performance. In this\npaper, we propose a novel approach called multi-range decoupling decoding with\ngating-adjusting aggregation ($MD2GA$), which leverages the temporal\ncorrelations to refine motion representation learning. This approach employs a\ntwo-stage strategy for HMP. In the first stage, a multi-range decoupling\ndecoding adeptly adjusts feature learning by decoding the shared features into\ndistinct future lengths, where different decoders offer diverse insights into\nmotion patterns. In the second stage, a gating-adjusting aggregation\ndynamically combines the diverse insights guided by input motion data.\nExtensive experiments demonstrate that the proposed method can be easily\nintegrated into other motion prediction methods and enhance their prediction\nperformance."}
{"id": "2503.24220", "pdf": "https://arxiv.org/pdf/2503.24220", "abs": "https://arxiv.org/abs/2503.24220", "authors": ["Abdul Sittar", "Dunja Mladenic", "Alenka Gucek", "Marko Grobelnik"], "title": "BAR-Analytics: A Web-based Platform for Analyzing Information Spreading Barriers in News: Comparative Analysis Across Multiple Barriers and Events", "categories": ["cs.CL"], "comment": "46 pages", "summary": "This paper presents BAR-Analytics, a web-based, open-source platform designed\nto analyze news dissemination across geographical, economic, political, and\ncultural boundaries. Using the Russian-Ukrainian and Israeli-Palestinian\nconflicts as case studies, the platform integrates four analytical methods:\npropagation analysis, trend analysis, sentiment analysis, and temporal topic\nmodeling. Over 350,000 articles were collected and analyzed, with a focus on\neconomic disparities and geographical influences using metadata enrichment. We\nevaluate the case studies using coherence, sentiment polarity, topic frequency,\nand trend shifts as key metrics. Our results show distinct patterns in news\ncoverage: the Israeli-Palestinian conflict tends to have more negative\nsentiment with a focus on human rights, while the Russia-Ukraine conflict is\nmore positive, emphasizing election interference. These findings highlight the\ninfluence of political, economic, and regional factors in shaping media\nnarratives across different conflicts."}
{"id": "2503.23388", "pdf": "https://arxiv.org/pdf/2503.23388", "abs": "https://arxiv.org/abs/2503.23388", "authors": ["Fanding Huang", "Jingyan Jiang", "Qinting Jiang", "Hebei Li", "Faisal Nadeem Khan", "Zhi Wang"], "title": "COSMIC: Clique-Oriented Semantic Multi-space Integration for Robust CLIP Test-Time Adaptation", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "comment": "Accepted to CVPR 2025", "summary": "Recent vision-language models (VLMs) face significant challenges in test-time\nadaptation to novel domains. While cache-based methods show promise by\nleveraging historical information, they struggle with both caching unreliable\nfeature-label pairs and indiscriminately using single-class information during\nquerying, significantly compromising adaptation accuracy. To address these\nlimitations, we propose COSMIC (Clique-Oriented Semantic Multi-space\nIntegration for CLIP), a robust test-time adaptation framework that enhances\nadaptability through multi-granular, cross-modal semantic caching and\ngraph-based querying mechanisms. Our framework introduces two key innovations:\nDual Semantics Graph (DSG) and Clique Guided Hyper-class (CGH). The Dual\nSemantics Graph constructs complementary semantic spaces by incorporating\ntextual features, coarse-grained CLIP features, and fine-grained DINOv2\nfeatures to capture rich semantic relationships. Building upon these dual\ngraphs, the Clique Guided Hyper-class component leverages structured class\nrelationships to enhance prediction robustness through correlated class\nselection. Extensive experiments demonstrate COSMIC's superior performance\nacross multiple benchmarks, achieving significant improvements over\nstate-of-the-art methods: 15.81% gain on out-of-distribution tasks and 5.33% on\ncross-domain generation with CLIP RN-50. Code is available at\ngithub.com/hf618/COSMIC."}
{"id": "2503.24235", "pdf": "https://arxiv.org/pdf/2503.24235", "abs": "https://arxiv.org/abs/2503.24235", "authors": ["Qiyuan Zhang", "Fuyuan Lyu", "Zexu Sun", "Lei Wang", "Weixu Zhang", "Zhihan Guo", "Yufei Wang", "Irwin King", "Xue Liu", "Chen Ma"], "title": "What, How, Where, and How Well? A Survey on Test-Time Scaling in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As enthusiasm for scaling computation (data and parameters) in the\npretraining era gradually diminished, test-time scaling (TTS), also referred to\nas ``test-time computing'' has emerged as a prominent research focus. Recent\nstudies demonstrate that TTS can further elicit the problem-solving\ncapabilities of large language models (LLMs), enabling significant\nbreakthroughs not only in specialized reasoning tasks, such as mathematics and\ncoding, but also in general tasks like open-ended Q&A. However, despite the\nexplosion of recent efforts in this area, there remains an urgent need for a\ncomprehensive survey offering a systemic understanding. To fill this gap, we\npropose a unified, multidimensional framework structured along four core\ndimensions of TTS research: what to scale, how to scale, where to scale, and\nhow well to scale. Building upon this taxonomy, we conduct an extensive review\nof methods, application scenarios, and assessment aspects, and present an\norganized decomposition that highlights the unique functional roles of\nindividual techniques within the broader TTS landscape. From this analysis, we\ndistill the major developmental trajectories of TTS to date and offer hands-on\nguidelines for practical deployment. Furthermore, we identify several open\nchallenges and offer insights into promising future directions, including\nfurther scaling, clarifying the functional essence of techniques, generalizing\nto more tasks, and more attributions."}
{"id": "2503.23398", "pdf": "https://arxiv.org/pdf/2503.23398", "abs": "https://arxiv.org/abs/2503.23398", "authors": ["Leander Girrbach", "Stephan Alaniz", "Genevieve Smith", "Zeynep Akata"], "title": "A Large Scale Analysis of Gender Biases in Text-to-Image Generative Models", "categories": ["cs.CV", "cs.CY"], "comment": null, "summary": "With the increasing use of image generation technology, understanding its\nsocial biases, including gender bias, is essential. This paper presents the\nfirst large-scale study on gender bias in text-to-image (T2I) models, focusing\non everyday situations. While previous research has examined biases in\noccupations, we extend this analysis to gender associations in daily\nactivities, objects, and contexts. We create a dataset of 3,217 gender-neutral\nprompts and generate 200 images per prompt from five leading T2I models. We\nautomatically detect the perceived gender of people in the generated images and\nfilter out images with no person or multiple people of different genders,\nleaving 2,293,295 images. To enable a broad analysis of gender bias in T2I\nmodels, we group prompts into semantically similar concepts and calculate the\nproportion of male- and female-gendered images for each prompt. Our analysis\nshows that T2I models reinforce traditional gender roles, reflect common gender\nstereotypes in household roles, and underrepresent women in financial related\nactivities. Women are predominantly portrayed in care- and human-centered\nscenarios, and men in technical or physical labor scenarios."}
{"id": "2503.24245", "pdf": "https://arxiv.org/pdf/2503.24245", "abs": "https://arxiv.org/abs/2503.24245", "authors": ["Dun Yuan", "Hao Zhou", "Di Wu", "Xue Liu", "Hao Chen", "Yan Xin", "Jianzhong", "Zhang"], "title": "Enhancing Large Language Models (LLMs) for Telecommunications using Knowledge Graphs and Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": "This work has been accepted to ICC 2025 IEEE International Conference\n  on Communications. copyright 2025 IEEE", "summary": "Large language models (LLMs) have made significant progress in\ngeneral-purpose natural language processing tasks. However, LLMs are still\nfacing challenges when applied to domain-specific areas like\ntelecommunications, which demands specialized expertise and adaptability to\nevolving standards. This paper presents a novel framework that combines\nknowledge graph (KG) and retrieval-augmented generation (RAG) techniques to\nenhance LLM performance in the telecom domain. The framework leverages a KG to\ncapture structured, domain-specific information about network protocols,\nstandards, and other telecom-related entities, comprehensively representing\ntheir relationships. By integrating KG with RAG, LLMs can dynamically access\nand utilize the most relevant and up-to-date knowledge during response\ngeneration. This hybrid approach bridges the gap between structured knowledge\nrepresentation and the generative capabilities of LLMs, significantly enhancing\naccuracy, adaptability, and domain-specific comprehension. Our results\ndemonstrate the effectiveness of the KG-RAG framework in addressing complex\ntechnical queries with precision. The proposed KG-RAG model attained an\naccuracy of 88% for question answering tasks on a frequently used\ntelecom-specific dataset, compared to 82% for the RAG-only and 48% for the\nLLM-only approaches."}
{"id": "2503.23402", "pdf": "https://arxiv.org/pdf/2503.23402", "abs": "https://arxiv.org/abs/2503.23402", "authors": ["Junsu Kim", "Yunhoe Ku", "Dongyoon Han", "Seungryul Baek"], "title": "Diffusion Meets Few-shot Class Incremental Learning", "categories": ["cs.CV", "cs.AI"], "comment": "pre-print", "summary": "Few-shot class-incremental learning (FSCIL) is challenging due to extremely\nlimited training data; while aiming to reduce catastrophic forgetting and learn\nnew information. We propose Diffusion-FSCIL, a novel approach that employs a\ntext-to-image diffusion model as a frozen backbone. Our conjecture is that\nFSCIL can be tackled using a large generative model's capabilities benefiting\nfrom 1) generation ability via large-scale pre-training; 2) multi-scale\nrepresentation; 3) representational flexibility through the text encoder. To\nmaximize the representation capability, we propose to extract multiple\ncomplementary diffusion features to play roles as latent replay with slight\nsupport from feature distillation for preventing generative biases. Our\nframework realizes efficiency through 1) using a frozen backbone; 2) minimal\ntrainable components; 3) batch processing of multiple feature extractions.\nExtensive experiments on CUB-200, miniImageNet, and CIFAR-100 show that\nDiffusion-FSCIL surpasses state-of-the-art methods, preserving performance on\npreviously learned classes and adapting effectively to new ones."}
{"id": "2503.24293", "pdf": "https://arxiv.org/pdf/2503.24293", "abs": "https://arxiv.org/abs/2503.24293", "authors": ["Hayley Ross", "Kathryn Davidson", "Najoung Kim"], "title": "Is analogy enough to draw novel adjective-noun inferences?", "categories": ["cs.CL"], "comment": "8 pages (16 pages with appendix). Submitted to SCiL 2025", "summary": "Recent work (Ross et al., 2025, 2024) has argued that the ability of humans\nand LLMs respectively to generalize to novel adjective-noun combinations shows\nthat they each have access to a compositional mechanism to determine the\nphrase's meaning and derive inferences. We study whether these inferences can\ninstead be derived by analogy to known inferences, without need for\ncomposition. We investigate this by (1) building a model of analogical\nreasoning using similarity over lexical items, and (2) asking human\nparticipants to reason by analogy. While we find that this strategy works well\nfor a large proportion of the dataset of Ross et al. (2025), there are novel\ncombinations for which both humans and LLMs derive convergent inferences but\nwhich are not well handled by analogy. We thus conclude that the mechanism\nhumans and LLMs use to generalize in these cases cannot be fully reduced to\nanalogy, and likely involves composition."}
{"id": "2503.23407", "pdf": "https://arxiv.org/pdf/2503.23407", "abs": "https://arxiv.org/abs/2503.23407", "authors": ["Wei Zeng", "Xuebin Chang", "Jianghao Su", "Xiang Gu", "Jian Sun", "Zongben Xu"], "title": "GMapLatent: Geometric Mapping in Latent Space", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Cross-domain generative models based on encoder-decoder AI architectures have\nattracted much attention in generating realistic images, where domain alignment\nis crucial for generation accuracy. Domain alignment methods usually deal\ndirectly with the initial distribution; however, mismatched or mixed clusters\ncan lead to mode collapse and mixture problems in the decoder, compromising\nmodel generalization capabilities. In this work, we innovate a cross-domain\nalignment and generation model that introduces a canonical latent space\nrepresentation based on geometric mapping to align the cross-domain latent\nspaces in a rigorous and precise manner, thus avoiding mode collapse and\nmixture in the encoder-decoder generation architectures. We name this model\nGMapLatent. The core of the method is to seamlessly align latent spaces with\nstrict cluster correspondence constraints using the canonical parameterizations\nof cluster-decorated latent spaces. We first (1) transform the latent space to\na canonical parameter domain by composing barycenter translation, optimal\ntransport merging and constrained harmonic mapping, and then (2) compute\ngeometric registration with cluster constraints over the canonical parameter\ndomains. This process realizes a bijective (one-to-one and onto) mapping\nbetween newly transformed latent spaces and generates a precise alignment of\ncluster pairs. Cross-domain generation is then achieved through the aligned\nlatent spaces embedded in the encoder-decoder pipeline. Experiments on\ngray-scale and color images validate the efficiency, efficacy and applicability\nof GMapLatent, and demonstrate that the proposed model has superior performance\nover existing models."}
{"id": "2503.24307", "pdf": "https://arxiv.org/pdf/2503.24307", "abs": "https://arxiv.org/abs/2503.24307", "authors": ["Arshia Kermani", "Veronica Perez-Rosas", "Vangelis Metsis"], "title": "A Systematic Evaluation of LLM Strategies for Mental Health Text Analysis: Fine-tuning vs. Prompt Engineering vs. RAG", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "This study presents a systematic comparison of three approaches for the\nanalysis of mental health text using large language models (LLMs): prompt\nengineering, retrieval augmented generation (RAG), and fine-tuning. Using LLaMA\n3, we evaluate these approaches on emotion classification and mental health\ncondition detection tasks across two datasets. Fine-tuning achieves the highest\naccuracy (91% for emotion classification, 80% for mental health conditions) but\nrequires substantial computational resources and large training sets, while\nprompt engineering and RAG offer more flexible deployment with moderate\nperformance (40-68% accuracy). Our findings provide practical insights for\nimplementing LLM-based solutions in mental health applications, highlighting\nthe trade-offs between accuracy, computational requirements, and deployment\nflexibility."}
{"id": "2503.23422", "pdf": "https://arxiv.org/pdf/2503.23422", "abs": "https://arxiv.org/abs/2503.23422", "authors": ["Xin Zuo", "Jiaran Jiang", "Jifeng Shen", "Wankou Yang"], "title": "Improving underwater semantic segmentation with underwater image quality attention and muti-scale aggregation attention", "categories": ["cs.CV"], "comment": "Accepted by Pattern Analysis and Applications", "summary": "Underwater image understanding is crucial for both submarine navigation and\nseabed exploration. However, the low illumination in underwater environments\ndegrades the imaging quality, which in turn seriously deteriorates the\nperformance of underwater semantic segmentation, particularly for outlining the\nobject region boundaries. To tackle this issue, we present UnderWater SegFormer\n(UWSegFormer), a transformer-based framework for semantic segmentation of\nlow-quality underwater images. Firstly, we propose the Underwater Image Quality\nAttention (UIQA) module. This module enhances the representation of highquality\nsemantic information in underwater image feature channels through a channel\nself-attention mechanism. In order to address the issue of loss of imaging\ndetails due to the underwater environment, the Multi-scale Aggregation\nAttention(MAA) module is proposed. This module aggregates sets of semantic\nfeatures at different scales by extracting discriminative information from\nhigh-level features,thus compensating for the semantic loss of detail in\nunderwater objects. Finally, during training, we introduce Edge Learning Loss\n(ELL) in order to enhance the model's learning of underwater object edges and\nimprove the model's prediction accuracy. Experiments conducted on the SUIM and\nDUT-USEG (DUT) datasets have demonstrated that the proposed method has\nadvantages in terms of segmentation completeness, boundary clarity, and\nsubjective perceptual details when compared to SOTA methods. In addition, the\nproposed method achieves the highest mIoU of 82.12 and 71.41 on the SUIM and\nDUT datasets, respectively. Code will be available at\nhttps://github.com/SAWRJJ/UWSegFormer."}
{"id": "2503.24310", "pdf": "https://arxiv.org/pdf/2503.24310", "abs": "https://arxiv.org/abs/2503.24310", "authors": ["Alok Abhishek", "Lisa Erickson", "Tushar Bandopadhyay"], "title": "BEATS: Bias Evaluation and Assessment Test Suite for Large Language Models", "categories": ["cs.CL", "cs.AI", "68T01 (Primary), 68T50 (Secondary)", "I.2.0; I.2.7"], "comment": "32 pages, 33 figures, preprint version", "summary": "In this research, we introduce BEATS, a novel framework for evaluating Bias,\nEthics, Fairness, and Factuality in Large Language Models (LLMs). Building upon\nthe BEATS framework, we present a bias benchmark for LLMs that measure\nperformance across 29 distinct metrics. These metrics span a broad range of\ncharacteristics, including demographic, cognitive, and social biases, as well\nas measures of ethical reasoning, group fairness, and factuality related\nmisinformation risk. These metrics enable a quantitative assessment of the\nextent to which LLM generated responses may perpetuate societal prejudices that\nreinforce or expand systemic inequities. To achieve a high score on this\nbenchmark a LLM must show very equitable behavior in their responses, making it\na rigorous standard for responsible AI evaluation. Empirical results based on\ndata from our experiment show that, 37.65\\% of outputs generated by industry\nleading models contained some form of bias, highlighting a substantial risk of\nusing these models in critical decision making systems. BEATS framework and\nbenchmark offer a scalable and statistically rigorous methodology to benchmark\nLLMs, diagnose factors driving biases, and develop mitigation strategies. With\nthe BEATS framework, our goal is to help the development of more socially\nresponsible and ethically aligned AI models."}
{"id": "2503.23447", "pdf": "https://arxiv.org/pdf/2503.23447", "abs": "https://arxiv.org/abs/2503.23447", "authors": ["Jongseo Lee", "Joohyun Chang", "Dongho Lee", "Jinwoo Choi"], "title": "CA^2ST: Cross-Attention in Audio, Space, and Time for Holistic Video Recognition", "categories": ["cs.CV"], "comment": "27 pages including appendix, TPAMI under review", "summary": "We propose Cross-Attention in Audio, Space, and Time (CA^2ST), a\ntransformer-based method for holistic video recognition. Recognizing actions in\nvideos requires both spatial and temporal understanding, yet most existing\nmodels lack a balanced spatio-temporal understanding of videos. To address\nthis, we propose a novel two-stream architecture, called Cross-Attention in\nSpace and Time (CAST), using only RGB input. In each layer of CAST, Bottleneck\nCross-Attention (B-CA) enables spatial and temporal experts to exchange\ninformation and make synergistic predictions. For holistic video understanding,\nwe extend CAST by integrating an audio expert, forming Cross-Attention in\nVisual and Audio (CAVA). We validate the CAST on benchmarks with different\ncharacteristics, EPIC-KITCHENS-100, Something-Something-V2, and Kinetics-400,\nconsistently showing balanced performance. We also validate the CAVA on\naudio-visual action recognition benchmarks, including UCF-101, VGG-Sound,\nKineticsSound, and EPIC-SOUNDS. With a favorable performance of CAVA across\nthese datasets, we demonstrate the effective information exchange among\nmultiple experts within the B-CA module. In summary, CA^2ST combines CAST and\nCAVA by employing spatial, temporal, and audio experts through cross-attention,\nachieving balanced and holistic video understanding."}
{"id": "2503.24364", "pdf": "https://arxiv.org/pdf/2503.24364", "abs": "https://arxiv.org/abs/2503.24364", "authors": ["Łukasz Borchmann", "Marek Wydmuch"], "title": "Query and Conquer: Execution-Guided SQL Generation", "categories": ["cs.CL"], "comment": null, "summary": "We propose a novel approach for generating complex outputs that significantly\nimproves accuracy in text-to-SQL tasks. Our method leverages execution results\nto select the most semantically consistent query from multiple candidates,\nenabling smaller, cost-effective models to surpass computationally intensive\nreasoning methods such as o1, o3-mini, and DeepSeek R1 while reducing inference\ncost by as much as 30 times. It integrates effortlessly with existing models,\noffering a practical and scalable pathway to state-of-the-art SQL generation."}
{"id": "2503.23450", "pdf": "https://arxiv.org/pdf/2503.23450", "abs": "https://arxiv.org/abs/2503.23450", "authors": ["Bohao Xing", "Kaishen Yuan", "Zitong Yu", "Xin Liu", "Heikki Kälviäinen"], "title": "AU-TTT: Vision Test-Time Training model for Facial Action Unit Detection", "categories": ["cs.CV"], "comment": null, "summary": "Facial Action Units (AUs) detection is a cornerstone of objective facial\nexpression analysis and a critical focus in affective computing. Despite its\nimportance, AU detection faces significant challenges, such as the high cost of\nAU annotation and the limited availability of datasets. These constraints often\nlead to overfitting in existing methods, resulting in substantial performance\ndegradation when applied across diverse datasets. Addressing these issues is\nessential for improving the reliability and generalizability of AU detection\nmethods. Moreover, many current approaches leverage Transformers for their\neffectiveness in long-context modeling, but they are hindered by the quadratic\ncomplexity of self-attention. Recently, Test-Time Training (TTT) layers have\nemerged as a promising solution for long-sequence modeling. Additionally, TTT\napplies self-supervised learning for iterative updates during both training and\ninference, offering a potential pathway to mitigate the generalization\nchallenges inherent in AU detection tasks. In this paper, we propose a novel\nvision backbone tailored for AU detection, incorporating bidirectional TTT\nblocks, named AU-TTT. Our approach introduces TTT Linear to the AU detection\ntask and optimizes image scanning mechanisms for enhanced performance.\nAdditionally, we design an AU-specific Region of Interest (RoI) scanning\nmechanism to capture fine-grained facial features critical for AU detection.\nExperimental results demonstrate that our method achieves competitive\nperformance in both within-domain and cross-domain scenarios."}
{"id": "2503.24377", "pdf": "https://arxiv.org/pdf/2503.24377", "abs": "https://arxiv.org/abs/2503.24377", "authors": ["Rui Wang", "Hongru Wang", "Boyang Xue", "Jianhui Pang", "Shudong Liu", "Yi Chen", "Jiahao Qiu", "Derek Fai Wong", "Heng Ji", "Kam-Fai Wong"], "title": "Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "In Progress; Paper list Repo:\n  https://github.com/DevoAllen/Awesome-Reasoning-Economy-Papers", "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced their ability to perform complex reasoning tasks, transitioning from\nfast and intuitive thinking (System 1) to slow and deep reasoning (System 2).\nWhile System 2 reasoning improves task accuracy, it often incurs substantial\ncomputational costs due to its slow thinking nature and inefficient or\nunnecessary reasoning behaviors. In contrast, System 1 reasoning is\ncomputationally efficient but leads to suboptimal performance. Consequently, it\nis critical to balance the trade-off between performance (benefits) and\ncomputational costs (budgets), giving rise to the concept of reasoning economy.\nIn this survey, we provide a comprehensive analysis of reasoning economy in\nboth the post-training and test-time inference stages of LLMs, encompassing i)\nthe cause of reasoning inefficiency, ii) behavior analysis of different\nreasoning patterns, and iii) potential solutions to achieve reasoning economy.\nBy offering actionable insights and highlighting open challenges, we aim to\nshed light on strategies for improving the reasoning economy of LLMs, thereby\nserving as a valuable resource for advancing research in this evolving area. We\nalso provide a public repository to continually track developments in this\nfast-evolving field."}
{"id": "2503.23451", "pdf": "https://arxiv.org/pdf/2503.23451", "abs": "https://arxiv.org/abs/2503.23451", "authors": ["Aimira Baitieva", "Yacine Bouaouni", "Alexandre Briot", "Dick Ameln", "Souhaiel Khalfaoui", "Samet Akcay"], "title": "Beyond Academic Benchmarks: Critical Analysis and Best Practices for Visual Industrial Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Anomaly detection (AD) is essential for automating visual inspection in\nmanufacturing. This field of computer vision is rapidly evolving, with\nincreasing attention towards real-world applications. Meanwhile, popular\ndatasets are typically produced in controlled lab environments with\nartificially created defects, unable to capture the diversity of real\nproduction conditions. New methods often fail in production settings, showing\nsignificant performance degradation or requiring impractical computational\nresources. This disconnect between academic results and industrial viability\nthreatens to misdirect visual anomaly detection research. This paper makes\nthree key contributions: (1) we demonstrate the importance of real-world\ndatasets and establish benchmarks using actual production data, (2) we provide\na fair comparison of existing SOTA methods across diverse tasks by utilizing\nmetrics that are valuable for practical applications, and (3) we present a\ncomprehensive analysis of recent advancements in this field by discussing\nimportant challenges and new perspectives for bridging the academia-industry\ngap. The code is publicly available at\nhttps://github.com/abc-125/viad-benchmark"}
{"id": "2503.22692", "pdf": "https://arxiv.org/pdf/2503.22692", "abs": "https://arxiv.org/abs/2503.22692", "authors": ["Shokoufeh Mirzaei", "Jesse Arzate", "Yukti Vijay"], "title": "Enhancing Aviation Communication Transcription: Fine-Tuning Distil-Whisper with LoRA", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG"], "comment": "14 pages, 4 Figures, 4 Tables, Under review by Journal of Aerospace\n  Information Systems", "summary": "Transcription of aviation communications has several applications, from\nassisting air traffic controllers in identifying the accuracy of read-back\nerrors to search and rescue operations. Recent advances in artificial\nintelligence have provided unprecedented opportunities for improving aviation\ncommunication transcription tasks. OpenAI's Whisper is one of the leading\nautomatic speech recognition models. However, fine-tuning Whisper for aviation\ncommunication transcription is not computationally efficient. Thus, this paper\naims to use a Parameter-Efficient Fine-tuning method called Low-Rank Adaptation\nto fine-tune a more computationally efficient version of Whisper,\ndistil-Whisper. To perform the fine-tuning, we used the Air Traffic Control\nCorpus dataset from the Linguistic Data Consortium, which contains\napproximately 70 hours of controller and pilot transmissions near three major\nairports in the US. The objective was to reduce the word error rate to enhance\naccuracy in the transcription of aviation communication. First, starting with\nan initial set of hyperparameters for LoRA (Alpha = 64 and Rank = 32), we\nperformed a grid search. We applied a 5-fold cross-validation to find the best\ncombination of distil-Whisper hyperparameters. Then, we fine-tuned the model\nfor LoRA hyperparameters, achieving an impressive average word error rate of\n3.86% across five folds. This result highlights the model's potential for use\nin the cockpit."}
{"id": "2503.23452", "pdf": "https://arxiv.org/pdf/2503.23452", "abs": "https://arxiv.org/abs/2503.23452", "authors": ["Yuhang Yang", "Ke Fan", "Shangkun Sun", "Hongxiang Li", "Ailing Zeng", "FeiLin Han", "Wei Zhai", "Wei Liu", "Yang Cao", "Zheng-Jun Zha"], "title": "VideoGen-Eval: Agent-based System for Video Generation Evaluation", "categories": ["cs.CV"], "comment": "project:https://github.com/AILab-CVC/VideoGen-Eval", "summary": "The rapid advancement of video generation has rendered existing evaluation\nsystems inadequate for assessing state-of-the-art models, primarily due to\nsimple prompts that cannot showcase the model's capabilities, fixed evaluation\noperators struggling with Out-of-Distribution (OOD) cases, and misalignment\nbetween computed metrics and human preferences. To bridge the gap, we propose\nVideoGen-Eval, an agent evaluation system that integrates LLM-based content\nstructuring, MLLM-based content judgment, and patch tools designed for\ntemporal-dense dimensions, to achieve a dynamic, flexible, and expandable video\ngeneration evaluation. Additionally, we introduce a video generation benchmark\nto evaluate existing cutting-edge models and verify the effectiveness of our\nevaluation system. It comprises 700 structured, content-rich prompts (both T2V\nand I2V) and over 12,000 videos generated by 20+ models, among them, 8\ncutting-edge models are selected as quantitative evaluation for the agent and\nhuman. Extensive experiments validate that our proposed agent-based evaluation\nsystem demonstrates strong alignment with human preferences and reliably\ncompletes the evaluation, as well as the diversity and richness of the\nbenchmark."}
{"id": "2503.22693", "pdf": "https://arxiv.org/pdf/2503.22693", "abs": "https://arxiv.org/abs/2503.22693", "authors": ["Alejandro Lopez-Lira", "Jihoon Kwon", "Sangwoon Yoon", "Jy-yong Sohn", "Chanyeol Choi"], "title": "Bridging Language Models and Financial Analysis", "categories": ["q-fin.ST", "cs.AI", "cs.CL"], "comment": "28 pages", "summary": "The rapid advancements in Large Language Models (LLMs) have unlocked\ntransformative possibilities in natural language processing, particularly\nwithin the financial sector. Financial data is often embedded in intricate\nrelationships across textual content, numerical tables, and visual charts,\nposing challenges that traditional methods struggle to address effectively.\nHowever, the emergence of LLMs offers new pathways for processing and analyzing\nthis multifaceted data with increased efficiency and insight. Despite the fast\npace of innovation in LLM research, there remains a significant gap in their\npractical adoption within the finance industry, where cautious integration and\nlong-term validation are prioritized. This disparity has led to a slower\nimplementation of emerging LLM techniques, despite their immense potential in\nfinancial applications. As a result, many of the latest advancements in LLM\ntechnology remain underexplored or not fully utilized in this domain. This\nsurvey seeks to bridge this gap by providing a comprehensive overview of recent\ndevelopments in LLM research and examining their applicability to the financial\nsector. Building on previous survey literature, we highlight several novel LLM\nmethodologies, exploring their distinctive capabilities and their potential\nrelevance to financial data analysis. By synthesizing insights from a broad\nrange of studies, this paper aims to serve as a valuable resource for\nresearchers and practitioners, offering direction on promising research avenues\nand outlining future opportunities for advancing LLM applications in finance."}
{"id": "2503.23453", "pdf": "https://arxiv.org/pdf/2503.23453", "abs": "https://arxiv.org/abs/2503.23453", "authors": ["Maofu Liu", "Jiahui Liu", "Xiaokang Zhang"], "title": "Semantic-Spatial Feature Fusion with Dynamic Graph Refinement for Remote Sensing Image Captioning", "categories": ["cs.CV"], "comment": null, "summary": "Remote sensing image captioning aims to generate semantically accurate\ndescriptions that are closely linked to the visual features of remote sensing\nimages. Existing approaches typically emphasize fine-grained extraction of\nvisual features and capturing global information. However, they often overlook\nthe complementary role of textual information in enhancing visual semantics and\nface challenges in precisely locating objects that are most relevant to the\nimage context. To address these challenges, this paper presents a\nsemantic-spatial feature fusion with dynamic graph refinement (SFDR) method,\nwhich integrates the semantic-spatial feature fusion (SSFF) and dynamic graph\nfeature refinement (DGFR) modules. The SSFF module utilizes a multi-level\nfeature representation strategy by leveraging pre-trained CLIP features, grid\nfeatures, and ROI features to integrate rich semantic and spatial information.\nIn the DGFR module, a graph attention network captures the relationships\nbetween feature nodes, while a dynamic weighting mechanism prioritizes objects\nthat are most relevant to the current scene and suppresses less significant\nones. Therefore, the proposed SFDR method significantly enhances the quality of\nthe generated descriptions. Experimental results on three benchmark datasets\ndemonstrate the effectiveness of the proposed method. The source code will be\navailable at https://github.com/zxk688}{https://github.com/zxk688."}
{"id": "2503.22705", "pdf": "https://arxiv.org/pdf/2503.22705", "abs": "https://arxiv.org/abs/2503.22705", "authors": ["Georgios P. Georgiou"], "title": "Enhancing nonnative speech perception and production through an AI-powered application", "categories": ["eess.AS", "cs.AI", "cs.CL"], "comment": null, "summary": "While research on using Artificial Intelligence (AI) through various\napplications to enhance foreign language pronunciation is expanding, it has\nprimarily focused on aspects such as comprehensibility and intelligibility,\nlargely neglecting the improvement of individual speech sounds in both\nperception and production. This study seeks to address this gap by examining\nthe impact of training with an AI-powered mobile application on nonnative sound\nperception and production. Participants completed a pretest assessing their\nability to discriminate the second language English heed-hid contrast and\nproduce these vowels in sentence contexts. The intervention involved training\nwith the Speakometer mobile application, which incorporated recording tasks\nfeaturing the English vowels, along with pronunciation feedback and practice.\nThe posttest mirrored the pretest to measure changes in performance. The\nresults revealed significant improvements in both discrimination accuracy and\nproduction of the target contrast following the intervention. However,\nparticipants did not achieve native-like competence. These findings highlight\nthe effectiveness of AI-powered applications in facilitating speech acquisition\nand support their potential use for personalized, interactive pronunciation\ntraining beyond the classroom."}
{"id": "2503.23455", "pdf": "https://arxiv.org/pdf/2503.23455", "abs": "https://arxiv.org/abs/2503.23455", "authors": ["Junzhu Mao", "Yang Shen", "Jinyang Guo", "Yazhou Yao", "Xiansheng Hua"], "title": "Efficient Token Compression for Vision Transformer with Spatial Information Preserved", "categories": ["cs.CV", "cs.MM"], "comment": "accepted by IEEE Transactions on Multimedia", "summary": "Token compression is essential for reducing the computational and memory\nrequirements of transformer models, enabling their deployment in\nresource-constrained environments. In this work, we propose an efficient and\nhardware-compatible token compression method called Prune and Merge. Our\napproach integrates token pruning and merging operations within transformer\nmodels to achieve layer-wise token compression. By introducing trainable merge\nand reconstruct matrices and utilizing shortcut connections, we efficiently\nmerge tokens while preserving important information and enabling the\nrestoration of pruned tokens. Additionally, we introduce a novel\ngradient-weighted attention scoring mechanism that computes token importance\nscores during the training phase, eliminating the need for separate\ncomputations during inference and enhancing compression efficiency. We also\nleverage gradient information to capture the global impact of tokens and\nautomatically identify optimal compression structures. Extensive experiments on\nthe ImageNet-1k and ADE20K datasets validate the effectiveness of our approach,\nachieving significant speed-ups with minimal accuracy degradation compared to\nstate-of-the-art methods. For instance, on DeiT-Small, we achieve a\n1.64$\\times$ speed-up with only a 0.2\\% drop in accuracy on ImageNet-1k.\nMoreover, by compressing segmenter models and comparing with existing methods,\nwe demonstrate the superior performance of our approach in terms of efficiency\nand effectiveness. Code and models have been made available at\nhttps://github.com/NUST-Machine-Intelligence-Laboratory/prune_and_merge."}
{"id": "2503.22708", "pdf": "https://arxiv.org/pdf/2503.22708", "abs": "https://arxiv.org/abs/2503.22708", "authors": ["Peter Jansen", "Oyvind Tafjord", "Marissa Radensky", "Pao Siangliulue", "Tom Hope", "Bhavana Dalvi Mishra", "Bodhisattwa Prasad Majumder", "Daniel S. Weld", "Peter Clark"], "title": "CodeScientist: End-to-End Semi-Automated Scientific Discovery with Code-based Experimentation", "categories": ["cs.AI", "cs.CL"], "comment": "98 Pages (13 pages: main paper body; 85 pages: appendix)", "summary": "Despite the surge of interest in autonomous scientific discovery (ASD) of\nsoftware artifacts (e.g., improved ML algorithms), current ASD systems face two\nkey limitations: (1) they largely explore variants of existing codebases or\nsimilarly constrained design spaces, and (2) they produce large volumes of\nresearch artifacts (such as automatically generated papers and code) that are\ntypically evaluated using conference-style paper review with limited evaluation\nof code. In this work we introduce CodeScientist, a novel ASD system that\nframes ideation and experiment construction as a form of genetic search jointly\nover combinations of research articles and codeblocks defining common actions\nin a domain (like prompting a language model). We use this paradigm to conduct\nhundreds of automated experiments on machine-generated ideas broadly in the\ndomain of agents and virtual environments, with the system returning 19\ndiscoveries, 6 of which were judged as being both at least minimally sound and\nincrementally novel after a multi-faceted evaluation beyond that typically\nconducted in prior work, including external (conference-style) review, code\nreview, and replication attempts. Moreover, the discoveries span new tasks,\nagents, metrics, and data, suggesting a qualitative shift from benchmark\noptimization to broader discoveries."}
{"id": "2503.23456", "pdf": "https://arxiv.org/pdf/2503.23456", "abs": "https://arxiv.org/abs/2503.23456", "authors": ["Maofu Liu", "Xin Jiang", "Xiaokang Zhang"], "title": "CADFormer: Fine-Grained Cross-modal Alignment and Decoding Transformer for Referring Remote Sensing Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Referring Remote Sensing Image Segmentation (RRSIS) is a challenging task,\naiming to segment specific target objects in remote sensing (RS) images based\non a given language expression. Existing RRSIS methods typically employ\ncoarse-grained unidirectional alignment approaches to obtain multimodal\nfeatures, and they often overlook the critical role of language features as\ncontextual information during the decoding process. Consequently, these methods\nexhibit weak object-level correspondence between visual and language features,\nleading to incomplete or erroneous predicted masks, especially when handling\ncomplex expressions and intricate RS image scenes. To address these challenges,\nwe propose a fine-grained cross-modal alignment and decoding Transformer,\nCADFormer, for RRSIS. Specifically, we design a semantic mutual guidance\nalignment module (SMGAM) to achieve both vision-to-language and\nlanguage-to-vision alignment, enabling comprehensive integration of visual and\ntextual features for fine-grained cross-modal alignment. Furthermore, a\ntextual-enhanced cross-modal decoder (TCMD) is introduced to incorporate\nlanguage features during decoding, using refined textual information as context\nto enhance the relationship between cross-modal features. To thoroughly\nevaluate the performance of CADFormer, especially for inconspicuous targets in\ncomplex scenes, we constructed a new RRSIS dataset, called RRSIS-HR, which\nincludes larger high-resolution RS image patches and semantically richer\nlanguage expressions. Extensive experiments on the RRSIS-HR dataset and the\npopular RRSIS-D dataset demonstrate the effectiveness and superiority of\nCADFormer. Datasets and source codes will be available at\nhttps://github.com/zxk688."}
{"id": "2503.22726", "pdf": "https://arxiv.org/pdf/2503.22726", "abs": "https://arxiv.org/abs/2503.22726", "authors": ["Yue Yin"], "title": "InfoBid: A Simulation Framework for Studying Information Disclosure in Auctions with Large Language Model-based Agents", "categories": ["cs.GT", "cs.CL", "cs.HC", "cs.MA", "econ.GN", "q-fin.EC"], "comment": "AAAI 2025 Workshop: Economics of Modern ML: Markets, Incentives, and\n  Generative AI", "summary": "In online advertising systems, publishers often face a trade-off in\ninformation disclosure strategies: while disclosing more information can\nenhance efficiency by enabling optimal allocation of ad impressions, it may\nlose revenue potential by decreasing uncertainty among competing advertisers.\nSimilar to other challenges in market design, understanding this trade-off is\nconstrained by limited access to real-world data, leading researchers and\npractitioners to turn to simulation frameworks. The recent emergence of large\nlanguage models (LLMs) offers a novel approach to simulations, providing\nhuman-like reasoning and adaptability without necessarily relying on explicit\nassumptions about agent behavior modeling. Despite their potential, existing\nframeworks have yet to integrate LLM-based agents for studying information\nasymmetry and signaling strategies, particularly in the context of auctions. To\naddress this gap, we introduce InfoBid, a flexible simulation framework that\nleverages LLM agents to examine the effects of information disclosure\nstrategies in multi-agent auction settings. Using GPT-4o, we implemented\nsimulations of second-price auctions with diverse information schemas. The\nresults reveal key insights into how signaling influences strategic behavior\nand auction outcomes, which align with both economic and social learning\ntheories. Through InfoBid, we hope to foster the use of LLMs as proxies for\nhuman economic and social agents in empirical studies, enhancing our\nunderstanding of their capabilities and limitations. This work bridges the gap\nbetween theoretical market designs and practical applications, advancing\nresearch in market simulations, information design, and agent-based reasoning\nwhile offering a valuable tool for exploring the dynamics of digital economies."}
{"id": "2503.23459", "pdf": "https://arxiv.org/pdf/2503.23459", "abs": "https://arxiv.org/abs/2503.23459", "authors": ["Chenglong Lu", "Shen Liang", "Xuewei Wang", "Wei Wang"], "title": "Reinforcement Learning-based Token Pruning in Vision Transformers: A Markov Game Approach", "categories": ["cs.CV"], "comment": "Accepted by IEEE International Conference on Multimedia & Expo (ICME)\n  2025", "summary": "Vision Transformers (ViTs) have computational costs scaling quadratically\nwith the number of tokens, calling for effective token pruning policies. Most\nexisting policies are handcrafted, lacking adaptivity to varying inputs.\nMoreover, they fail to consider the sequential nature of token pruning across\nmultiple layers. In this work, for the first time (as far as we know), we\nexploit Reinforcement Learning (RL) to data-adaptively learn a pruning policy.\nFormulating token pruning as a sequential decision-making problem, we model it\nas a Markov Game and utilize Multi-Agent Proximal Policy Optimization (MAPPO)\nwhere each agent makes an individualized pruning decision for a single token.\nWe also develop reward functions that enable simultaneous collaboration and\ncompetition of these agents to balance efficiency and accuracy. On the\nwell-known ImageNet-1k dataset, our method improves the inference speed by up\nto 44% while incurring only a negligible accuracy drop of 0.4%. The source code\nis available at https://github.com/daashuai/rl4evit."}
{"id": "2503.22732", "pdf": "https://arxiv.org/pdf/2503.22732", "abs": "https://arxiv.org/abs/2503.22732", "authors": ["Mohamed Amine Ferrag", "Norbert Tihanyi", "Merouane Debbah"], "title": "Reasoning Beyond Limits: Advances and Open Problems for LLMs", "categories": ["cs.LG", "cs.CL"], "comment": "41 pages", "summary": "Recent generative reasoning breakthroughs have transformed how large language\nmodels (LLMs) tackle complex problems by dynamically retrieving and refining\ninformation while generating coherent, multi-step thought processes. Techniques\nsuch as inference-time scaling, reinforcement learning, supervised fine-tuning,\nand distillation have been successfully applied to models like DeepSeek-R1,\nOpenAI's o1 & o3, GPT-4o, Qwen-32B, and various Llama variants, resulting in\nenhanced reasoning capabilities. In this paper, we provide a comprehensive\nanalysis of the top 27 LLM models released between 2023 and 2025 (including\nmodels such as Mistral AI Small 3 24B, DeepSeek-R1, Search-o1, QwQ-32B, and\nphi-4). Then, we present an extensive overview of training methodologies that\nspans general training approaches, mixture-of-experts (MoE) and architectural\ninnovations, retrieval-augmented generation (RAG), chain-of-thought and\nself-improvement techniques, as well as test-time compute scaling,\ndistillation, and reinforcement learning (RL) methods. Finally, we discuss the\nkey challenges in advancing LLM capabilities, including improving multi-step\nreasoning without human supervision, overcoming limitations in chained tasks,\nbalancing structured prompts with flexibility, and enhancing long-context\nretrieval and external tool integration."}
{"id": "2503.23461", "pdf": "https://arxiv.org/pdf/2503.23461", "abs": "https://arxiv.org/abs/2503.23461", "authors": ["Nikai Du", "Zhennan Chen", "Zhizhou Chen", "Shan Gao", "Xi Chen", "Zhengkai Jiang", "Jian Yang", "Ying Tai"], "title": "TextCrafter: Accurately Rendering Multiple Texts in Complex Visual Scenes", "categories": ["cs.CV"], "comment": null, "summary": "This paper explores the task of Complex Visual Text Generation (CVTG), which\ncenters on generating intricate textual content distributed across diverse\nregions within visual images. In CVTG, image generation models often rendering\ndistorted and blurred visual text or missing some visual text. To tackle these\nchallenges, we propose TextCrafter, a novel multi-visual text rendering method.\nTextCrafter employs a progressive strategy to decompose complex visual text\ninto distinct components while ensuring robust alignment between textual\ncontent and its visual carrier. Additionally, it incorporates a token focus\nenhancement mechanism to amplify the prominence of visual text during the\ngeneration process. TextCrafter effectively addresses key challenges in CVTG\ntasks, such as text confusion, omissions, and blurriness. Moreover, we present\na new benchmark dataset, CVTG-2K, tailored to rigorously evaluate the\nperformance of generative models on CVTG tasks. Extensive experiments\ndemonstrate that our method surpasses state-of-the-art approaches."}
{"id": "2503.22735", "pdf": "https://arxiv.org/pdf/2503.22735", "abs": "https://arxiv.org/abs/2503.22735", "authors": ["Andrew Rothwell", "Joss Moorkens", "Tomas Svoboda"], "title": "Training in translation tools and technologies: Findings of the EMT survey 2023", "categories": ["cs.CY", "cs.CL"], "comment": null, "summary": "This article reports on the third iteration of a survey of computerized tools\nand technologies taught as part of postgraduate translation training\nprogrammes. While the survey was carried out under the aegis of the EMT\nNetwork, more than half of responses are from outside that network. The results\nshow the responsiveness of programmes to innovations in translation technology,\nwith increased compulsory inclusion of machine translation, post-editing, and\nquality evaluation, and a rapid response to the release of generative tools.\nThe flexibility required during the Covid-19 pandemic has also led to some\nlasting changes to programmes. While the range of tools being taught has\ncontinued to expand, programmes seem to be consolidating their core offering\naround cloud-based software with cost-free academic access. There has also been\nan increase in the embedding of professional contexts and workflows associated\nwith translation technology. Generic file management and data security skills\nhave increased in perceived importance, and legal and ethical issues related to\ntranslation data have also become more prominent. In terms of course delivery\nthe shift away from conventional labs identified in EMT2017 has accelerated\nmarkedly, no doubt partly driven by the pandemic, accompanied by a dramatic\nexpansion in the use of students' personal devices."}
{"id": "2503.23463", "pdf": "https://arxiv.org/pdf/2503.23463", "abs": "https://arxiv.org/abs/2503.23463", "authors": ["Xingcheng Zhou", "Xuyuan Han", "Feng Yang", "Yunpu Ma", "Alois C. Knoll"], "title": "OpenDriveVLA: Towards End-to-end Autonomous Driving with Large Vision Language Action Model", "categories": ["cs.CV"], "comment": null, "summary": "We present OpenDriveVLA, a Vision-Language Action (VLA) model designed for\nend-to-end autonomous driving. OpenDriveVLA builds upon open-source pre-trained\nlarge Vision-Language Models (VLMs) to generate reliable driving actions,\nconditioned on 3D environmental perception, ego vehicle states, and driver\ncommands. To bridge the modality gap between driving visual representations and\nlanguage embeddings, we propose a hierarchical vision-language alignment\nprocess, projecting both 2D and 3D structured visual tokens into a unified\nsemantic space. Besides, OpenDriveVLA models the dynamic relationships between\nthe ego vehicle, surrounding agents, and static road elements through an\nautoregressive agent-env-ego interaction process, ensuring both spatially and\nbehaviorally informed trajectory planning. Extensive experiments on the\nnuScenes dataset demonstrate that OpenDriveVLA achieves state-of-the-art\nresults across open-loop trajectory planning and driving-related\nquestion-answering tasks. Qualitative analyses further illustrate\nOpenDriveVLA's superior capability to follow high-level driving commands and\nrobustly generate trajectories under challenging scenarios, highlighting its\npotential for next-generation end-to-end autonomous driving. We will release\nour code to facilitate further research in this domain."}
{"id": "2503.22742", "pdf": "https://arxiv.org/pdf/2503.22742", "abs": "https://arxiv.org/abs/2503.22742", "authors": ["William Claster", "Suhas KM", "Dhairya Gundechia"], "title": "Adaptive Integrated Layered Attention (AILA)", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.IR", "cs.NE"], "comment": null, "summary": "We propose Adaptive Integrated Layered Attention (AILA), a neural network\narchitecture that combines dense skip connections with different mechanisms for\nadaptive feature reuse across network layers. We evaluate AILA on three\nchallenging tasks: price forecasting for various commodities and indices (S&P\n500, Gold, US dollar Futures, Coffee, Wheat), image recognition using the\nCIFAR-10 dataset, and sentiment analysis on the IMDB movie review dataset. In\nall cases, AILA matches strong deep learning baselines (LSTMs, Transformers,\nand ResNets), achieving it at a fraction of the training and inference time.\nNotably, we implement and test two versions of the model - AILA-Architecture 1,\nwhich uses simple linear layers as the connection mechanism between layers, and\nAILA-Architecture 2, which implements an attention mechanism to selectively\nfocus on outputs from previous layers. Both architectures are applied in a\nsingle-task learning setting, with each model trained separately for individual\ntasks. Results confirm that AILA's adaptive inter-layer connections yield\nrobust gains by flexibly reusing pertinent features at multiple network depths.\nThe AILA approach thus presents an extension to existing architectures,\nimproving long-range sequence modeling, image recognition with optimised\ncomputational speed, and SOTA classification performance in practice."}
{"id": "2503.23468", "pdf": "https://arxiv.org/pdf/2503.23468", "abs": "https://arxiv.org/abs/2503.23468", "authors": ["Eytan Kats", "Kai Geißler", "Jochen G. Hirsch", "Stefan Heldman", "Mattias P. Heinrich"], "title": "Internal Organ Localization Using Depth Images", "categories": ["cs.CV"], "comment": "Accepted for German Conference on Medical Image Computing 2025 (BVM\n  2025)", "summary": "Automated patient positioning is a crucial step in streamlining MRI workflows\nand enhancing patient throughput. RGB-D camera-based systems offer a promising\napproach to automate this process by leveraging depth information to estimate\ninternal organ positions. This paper investigates the feasibility of a\nlearning-based framework to infer approximate internal organ positions from the\nbody surface. Our approach utilizes a large-scale dataset of MRI scans to train\na deep learning model capable of accurately predicting organ positions and\nshapes from depth images alone. We demonstrate the effectiveness of our method\nin localization of multiple internal organs, including bones and soft tissues.\nOur findings suggest that RGB-D camera-based systems integrated into MRI\nworkflows have the potential to streamline scanning procedures and improve\npatient experience by enabling accurate and automated patient positioning."}
{"id": "2503.22832", "pdf": "https://arxiv.org/pdf/2503.22832", "abs": "https://arxiv.org/abs/2503.22832", "authors": ["Simeng Sun", "Cheng-Ping Hsieh", "Faisal Ladhak", "Erik Arakelyan", "Santiago Akle Serano", "Boris Ginsburg"], "title": "L0-Reasoning Bench: Evaluating Procedural Correctness in Language Models via Simple Program Execution", "categories": ["cs.PL", "cs.CL"], "comment": null, "summary": "Complex reasoning tasks often rely on the ability to consistently and\naccurately apply simple rules across incremental steps, a foundational\ncapability which we term \"level-0\" reasoning. To systematically evaluate this\ncapability, we introduce L0-Bench, a language model benchmark for testing\nprocedural correctness -- the ability to generate correct reasoning processes,\ncomplementing existing benchmarks that primarily focus on outcome correctness.\nGiven synthetic Python functions with simple operations, L0-Bench grades models\non their ability to generate step-by-step, error-free execution traces. The\nsynthetic nature of L0-Bench enables systematic and scalable generation of test\nprograms along various axes (e.g., number of trace steps). We evaluate a\ndiverse array of recent closed-source and open-weight models on a baseline test\nset. All models exhibit degradation as the number of target trace steps\nincreases, while larger models and reasoning-enhanced models better maintain\ncorrectness over multiple steps. Additionally, we use L0-Bench to explore\ntest-time scaling along three dimensions: input context length, number of\nsolutions for majority voting, and inference steps. Our results suggest\nsubstantial room to improve \"level-0\" reasoning and potential directions to\nbuild more reliable reasoning systems."}
{"id": "2503.23472", "pdf": "https://arxiv.org/pdf/2503.23472", "abs": "https://arxiv.org/abs/2503.23472", "authors": ["Guandong Li", "Mengxia Ye"], "title": "Efficient Dynamic Attention 3D Convolution for Hyperspectral Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "Deep neural networks face several challenges in hyperspectral image\nclassification, including insufficient utilization of joint spatial-spectral\ninformation, gradient vanishing with increasing depth, and overfitting. To\nenhance feature extraction efficiency while skipping redundant information,\nthis paper proposes a dynamic attention convolution design based on an improved\n3D-DenseNet model. The design employs multiple parallel convolutional kernels\ninstead of a single kernel and assigns dynamic attention weights to these\nparallel convolutions. This dynamic attention mechanism achieves adaptive\nfeature response based on spatial characteristics in the spatial dimension of\nhyperspectral images, focusing more on key spatial structures. In the spectral\ndimension, it enables dynamic discrimination of different bands, alleviating\ninformation redundancy and computational complexity caused by high spectral\ndimensionality. The DAC module enhances model representation capability by\nattention-based aggregation of multiple convolutional kernels without\nincreasing network depth or width. The proposed method demonstrates superior\nperformance in both inference speed and accuracy, outperforming mainstream\nhyperspectral image classification methods on the IN, UP, and KSC datasets."}
{"id": "2503.22879", "pdf": "https://arxiv.org/pdf/2503.22879", "abs": "https://arxiv.org/abs/2503.22879", "authors": ["Hung-Yueh Chiang", "Chi-Chih Chang", "Natalia Frumkin", "Kai-Chiang Wu", "Mohamed S. Abdelfattah", "Diana Marculescu"], "title": "Quamba2: A Robust and Scalable Post-training Quantization Framework for Selective State Space Models", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.PF"], "comment": null, "summary": "State Space Models (SSMs) are emerging as a compelling alternative to\nTransformers because of their consistent memory usage and high performance.\nDespite this, scaling up SSMs on cloud services or limited-resource devices is\nchallenging due to their storage requirements and computational power. To\novercome this, quantizing SSMs with low bit-width data formats can reduce model\nsize and benefit from hardware acceleration. As SSMs are prone to\nquantization-induced errors, recent efforts have focused on optimizing a\nparticular model or bit-width for efficiency without sacrificing performance.\nHowever, distinct bit-width configurations are essential for different\nscenarios, like W4A8 for boosting large-batch decoding speed, and W4A16 for\nenhancing generation speed in short prompt applications for a single user. To\nthis end, we present Quamba2, compatible with W8A8, W4A8, and W4A16 for both\nMamba1 and Mamba2 backbones, addressing the growing demand for SSM deployment\non various platforms. Based on the channel order preserving and activation\npersistence of SSMs, we propose an offline approach to quantize inputs of a\nlinear recurrence in 8-bit by sorting and clustering for input $x$, combined\nwith a per-state-group quantization for input-dependent parameters $B$ and $C$.\nTo ensure compute-invariance in the SSM output, we rearrange weights offline\naccording to the clustering sequence. The experiments show that Quamba2-8B\noutperforms several state-of-the-art SSM quantization methods and delivers\n1.3$\\times$ and 3$\\times$ speed-ups in the pre-filling and generation stages,\nrespectively, while offering 4$\\times$ memory reduction with only a $1.6\\%$\naverage accuracy drop. The evaluation on MMLU shows the generalizability and\nrobustness of our framework. The code and quantized models will be released at:\nhttps://github.com/enyac-group/Quamba."}
{"id": "2503.23495", "pdf": "https://arxiv.org/pdf/2503.23495", "abs": "https://arxiv.org/abs/2503.23495", "authors": ["Ashim Dahal", "Saydul Akbar Murad", "Nick Rahimi"], "title": "Embedding Shift Dissection on CLIP: Effects of Augmentations on VLM's Representation Learning", "categories": ["cs.CV"], "comment": "accepted at MIV at CVPR 2025", "summary": "Understanding the representation shift on Vision Language Models like CLIP\nunder different augmentations provides valuable insights on Mechanistic\nInterpretability. In this study, we show the shift on CLIP's embeddings on 9\ncommon augmentation techniques: noise, blur, color jitter, scale and rotate,\nflip, elastic and perspective transforms, random brightness and contrast, and\ncoarse dropout of pixel blocks. We scrutinize the embedding shifts under\nsimilarity on attention map, patch, edge, detail preservation, cosine\nsimilarity, L2 distance, pairwise distance and dendrogram clusters and provide\nqualitative analysis on sample images. Our findings suggest certain\naugmentations like noise, perspective transform and shift scaling have higher\ndegree of drastic impact on embedding shift. This study provides a concrete\nfoundation for future work on VLM's robustness for mechanical interpretation\nand adversarial data defense."}
{"id": "2503.22968", "pdf": "https://arxiv.org/pdf/2503.22968", "abs": "https://arxiv.org/abs/2503.22968", "authors": ["Hanwool Lee", "Soo Yong Kim", "Dasol Choi", "SangWon Baek", "Seunghyeok Hong", "Ilgyun Jeong", "Inseon Hwang", "Naeun Lee", "Guijin Son"], "title": "HRET: A Self-Evolving LLM Evaluation Toolkit for Korean", "categories": ["cs.CE", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent advancements in Korean large language models (LLMs) have spurred\nnumerous benchmarks and evaluation methodologies, yet the lack of a\nstandardized evaluation framework has led to inconsistent results and limited\ncomparability. To address this, we introduce HRET Haerae Evaluation Toolkit, an\nopen-source, self-evolving evaluation framework tailored specifically for\nKorean LLMs. HRET unifies diverse evaluation methods, including logit-based\nscoring, exact-match, language-inconsistency penalization, and LLM-as-a-Judge\nassessments. Its modular, registry-based architecture integrates major\nbenchmarks (HAE-RAE Bench, KMMLU, KUDGE, HRM8K) and multiple inference backends\n(vLLM, HuggingFace, OpenAI-compatible endpoints). With automated pipelines for\ncontinuous evolution, HRET provides a robust foundation for reproducible, fair,\nand transparent Korean NLP research."}
{"id": "2503.23502", "pdf": "https://arxiv.org/pdf/2503.23502", "abs": "https://arxiv.org/abs/2503.23502", "authors": ["Jannik Endres", "Oliver Hahn", "Charles Corbière", "Simone Schaub-Meyer", "Stefan Roth", "Alexandre Alahi"], "title": "Boosting Omnidirectional Stereo Matching with a Pre-trained Depth Foundation Model", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "Project page: https://vita-epfl.github.io/DFI-OmniStereo-website/", "summary": "Omnidirectional depth perception is essential for mobile robotics\napplications that require scene understanding across a full 360{\\deg} field of\nview. Camera-based setups offer a cost-effective option by using stereo depth\nestimation to generate dense, high-resolution depth maps without relying on\nexpensive active sensing. However, existing omnidirectional stereo matching\napproaches achieve only limited depth accuracy across diverse environments,\ndepth ranges, and lighting conditions, due to the scarcity of real-world data.\nWe present DFI-OmniStereo, a novel omnidirectional stereo matching method that\nleverages a large-scale pre-trained foundation model for relative monocular\ndepth estimation within an iterative optimization-based stereo matching\narchitecture. We introduce a dedicated two-stage training strategy to utilize\nthe relative monocular depth features for our omnidirectional stereo matching\nbefore scale-invariant fine-tuning. DFI-OmniStereo achieves state-of-the-art\nresults on the real-world Helvipad dataset, reducing disparity MAE by\napproximately 16% compared to the previous best omnidirectional stereo method."}
{"id": "2503.22989", "pdf": "https://arxiv.org/pdf/2503.22989", "abs": "https://arxiv.org/abs/2503.22989", "authors": ["Gabriel Recchia", "Chatrik Singh Mangat", "Issac Li", "Gayatri Krishnakumar"], "title": "FindTheFlaws: Annotated Errors for Detecting Flawed Reasoning and Scalable Oversight Research", "categories": ["cs.AI", "cs.CL", "I.2"], "comment": "43 pages, 3 figures. for associated repository, see\n  https://github.com/modulo-research/findtheflaws", "summary": "As AI models tackle increasingly complex problems, ensuring reliable human\noversight becomes more challenging due to the difficulty of verifying\nsolutions. Approaches to scaling AI supervision include debate, in which two\nagents engage in structured dialogue to help a judge evaluate claims; critique,\nin which models identify potential flaws in proposed solutions; and\nprover-verifier games, in which a capable 'prover' model generates solutions\nthat must be verifiable by a less capable 'verifier'. Evaluations of the\nscalability of these and similar approaches to difficult problems benefit from\ndatasets that include (1) long-form expert-verified correct solutions and (2)\nlong-form flawed solutions with annotations highlighting specific errors, but\nfew are available.\n  To address this gap, we present FindTheFlaws, a group of five diverse\ndatasets spanning medicine, mathematics, science, coding, and the Lojban\nlanguage. Each dataset contains questions and long-form solutions with expert\nannotations validating their correctness or identifying specific error(s) in\nthe reasoning. We evaluate frontier models' critiquing capabilities and observe\na range of performance that can be leveraged for scalable oversight\nexperiments: models performing more poorly on particular datasets can serve as\njudges/verifiers for more capable models. Additionally, for some task/dataset\ncombinations, expert baselines exceed even top model performance, making them\nmore beneficial for scalable oversight experiments."}
{"id": "2503.23507", "pdf": "https://arxiv.org/pdf/2503.23507", "abs": "https://arxiv.org/abs/2503.23507", "authors": ["Siladittya Manna", "Suresh Das", "Sayantari Ghosh", "Saumik Bhattacharya"], "title": "Federated Self-Supervised Learning for One-Shot Cross-Modal and Cross-Imaging Technique Segmentation", "categories": ["cs.CV", "cs.LG", "eess.IV", "physics.med-ph"], "comment": null, "summary": "Decentralized federated learning enables learning of data representations\nfrom multiple sources without compromising the privacy of the clients. In\napplications like medical image segmentation, where obtaining a large annotated\ndataset from a single source is a distressing problem, federated\nself-supervised learning can provide some solace. In this work, we push the\nlimits further by exploring a federated self-supervised one-shot segmentation\ntask representing a more data-scarce scenario. We adopt a pre-existing\nself-supervised few-shot segmentation framework CoWPro and adapt it to the\nfederated learning scenario. To the best of our knowledge, this work is the\nfirst to attempt a self-supervised few-shot segmentation task in the federated\nlearning domain. Moreover, we consider the clients to be constituted of data\nfrom different modalities and imaging techniques like MR or CT, which makes the\nproblem even harder. Additionally, we reinforce and improve the baseline CoWPro\nmethod using a fused dice loss which shows considerable improvement in\nperformance over the baseline CoWPro. Finally, we evaluate this novel framework\non a completely unseen held-out part of the local client dataset. We observe\nthat the proposed framework can achieve performance at par or better than the\nFedAvg version of the CoWPro framework on the held-out validation dataset."}
{"id": "2503.23037", "pdf": "https://arxiv.org/pdf/2503.23037", "abs": "https://arxiv.org/abs/2503.23037", "authors": ["Aske Plaat", "Max van Duijn", "Niki van Stein", "Mike Preuss", "Peter van der Putten", "Kees Joost Batenburg"], "title": "Agentic Large Language Models, a survey", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "There is great interest in agentic LLMs, large language models that act as\nagents. We review the growing body of work in this area and provide a research\nagenda. Agentic LLMs are LLMs that (1) reason, (2) act, and (3) interact. We\norganize the literature according to these three categories. The research in\nthe first category focuses on reasoning, reflection, and retrieval, aiming to\nimprove decision making; the second category focuses on action models, robots,\nand tools, aiming for agents that act as useful assistants; the third category\nfocuses on multi-agent systems, aiming for collaborative task solving and\nsimulating interaction to study emergent social behavior. We find that works\nmutually benefit from results in other categories: retrieval enables tool use,\nreflection improves multi-agent collaboration, and reasoning benefits all\ncategories. We discuss applications of agentic LLMs and provide an agenda for\nfurther research. Important applications are in medical diagnosis, logistics\nand financial market analysis. Meanwhile, self-reflective agents playing roles\nand interacting with one another augment the process of scientific research\nitself. Further, agentic LLMs may provide a solution for the problem of LLMs\nrunning out of training data: inference-time behavior generates new training\nstates, such that LLMs can keep learning without needing ever larger datasets.\nWe note that there is risk associated with LLM assistants taking action in the\nreal world, while agentic LLMs are also likely to benefit society."}
{"id": "2503.23508", "pdf": "https://arxiv.org/pdf/2503.23508", "abs": "https://arxiv.org/abs/2503.23508", "authors": ["Yuming Chen", "Jiangyan Feng", "Haodong Zhang", "Lijun Gong", "Feng Zhu", "Rui Zhao", "Qibin Hou", "Ming-Ming Cheng", "Yibing Song"], "title": "Re-Aligning Language to Visual Objects with an Agentic Workflow", "categories": ["cs.CV"], "comment": "33 pages, 20 figures, 17 tables, ICLR 2025", "summary": "Language-based object detection (LOD) aims to align visual objects with\nlanguage expressions. A large amount of paired data is utilized to improve LOD\nmodel generalizations. During the training process, recent studies leverage\nvision-language models (VLMs) to automatically generate human-like expressions\nfor visual objects, facilitating training data scaling up. In this process, we\nobserve that VLM hallucinations bring inaccurate object descriptions (e.g.,\nobject name, color, and shape) to deteriorate VL alignment quality. To reduce\nVLM hallucinations, we propose an agentic workflow controlled by an LLM to\nre-align language to visual objects via adaptively adjusting image and text\nprompts. We name this workflow Real-LOD, which includes planning, tool use, and\nreflection steps. Given an image with detected objects and VLM raw language\nexpressions, Real-LOD reasons its state automatically and arranges action based\non our neural symbolic designs (i.e., planning). The action will adaptively\nadjust the image and text prompts and send them to VLMs for object\nre-description (i.e., tool use). Then, we use another LLM to analyze these\nrefined expressions for feedback (i.e., reflection). These steps are conducted\nin a cyclic form to gradually improve language descriptions for re-aligning to\nvisual objects. We construct a dataset that contains a tiny amount of 0.18M\nimages with re-aligned language expression and train a prevalent LOD model to\nsurpass existing LOD methods by around 50% on the standard benchmarks. Our\nReal-LOD workflow, with automatic VL refinement, reveals a potential to\npreserve data quality along with scaling up data quantity, which further\nimproves LOD performance from a data-alignment perspective."}
{"id": "2503.23083", "pdf": "https://arxiv.org/pdf/2503.23083", "abs": "https://arxiv.org/abs/2503.23083", "authors": ["Hasan Moughnieh", "Mohamad Chalhoub", "Hasan Nasrallah", "Cristiano Nattero", "Paolo Campanella", "Ali J. Ghandour"], "title": "Efficient Adaptation For Remote Sensing Visual Grounding", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Foundation models have revolutionized artificial intelligence (AI), offering\nremarkable capabilities across multi-modal domains. Their ability to precisely\nlocate objects in complex aerial and satellite images, using rich contextual\ninformation and detailed object descriptions, is essential for remote sensing\n(RS). These models can associate textual descriptions with object positions\nthrough the Visual Grounding (VG) task, but due to domain-specific challenges,\ntheir direct application to RS produces sub-optimal results. To address this,\nwe applied Parameter Efficient Fine Tuning (PEFT) techniques to adapt these\nmodels for RS-specific VG tasks. Specifically, we evaluated LoRA placement\nacross different modules in Grounding DINO and used BitFit and adapters to\nfine-tune the OFA foundation model pre-trained on general-purpose VG datasets.\nThis approach achieved performance comparable to or surpassing current State Of\nThe Art (SOTA) models while significantly reducing computational costs. This\nstudy highlights the potential of PEFT techniques to advance efficient and\nprecise multi-modal analysis in RS, offering a practical and cost-effective\nalternative to full model training."}
{"id": "2503.23509", "pdf": "https://arxiv.org/pdf/2503.23509", "abs": "https://arxiv.org/abs/2503.23509", "authors": ["Tianming Liang", "Haichao Jiang", "Wei-Shi Zheng", "Jian-Fang Hu"], "title": "ReferDINO-Plus: 2nd Solution for 4th PVUW MeViS Challenge at CVPR 2025", "categories": ["cs.CV"], "comment": null, "summary": "Referring Video Object Segmentation (RVOS) aims to segment target objects\nthroughout a video based on a text description. This task has attracted\nincreasing attention in the field of computer vision due to its promising\napplications in video editing and human-agent interaction. Recently, ReferDINO\nhas demonstrated promising performance in this task by adapting object-level\nvision-language knowledge from pretrained foundational image models. In this\nreport, we further enhance its capabilities by incorporating the advantages of\nSAM2 in mask quality and object consistency. In addition, to effectively\nbalance performance between single-object and multi-object scenarios, we\nintroduce a conditional mask fusion strategy that adaptively fuses the masks\nfrom ReferDINO and SAM2. Our solution, termed ReferDINO-Plus, achieves 60.43\n\\(\\mathcal{J}\\&\\mathcal{F}\\) on MeViS test set, securing 2nd place in the MeViS\nPVUW challenge at CVPR 2025. The code is available at:\nhttps://github.com/iSEE-Laboratory/ReferDINO-Plus."}
{"id": "2503.23100", "pdf": "https://arxiv.org/pdf/2503.23100", "abs": "https://arxiv.org/abs/2503.23100", "authors": ["Zehua Liu", "Han Wu", "Ruifeng She", "Xiaojin Fu", "Xiongwei Han", "Tao Zhong", "Mingxuan Yuan"], "title": "Beyond Standard MoE: Mixture of Latent Experts for Resource-Efficient Language Models", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Mixture of Experts (MoE) has emerged as a pivotal architectural paradigm for\nefficient scaling of Large Language Models (LLMs), operating through selective\nactivation of parameter subsets for each input token. Nevertheless,\nconventional MoE architectures encounter substantial challenges, including\nexcessive memory utilization and communication overhead during training and\ninference, primarily attributable to the proliferation of expert modules. In\nthis paper, we introduce Mixture of Latent Experts (MoLE), a novel\nparameterization methodology that facilitates the mapping of specific experts\ninto a shared latent space. Specifically, all expert operations are\nsystematically decomposed into two principal components: a shared projection\ninto a lower-dimensional latent space, followed by expert-specific\ntransformations with significantly reduced parametric complexity. This\nfactorized approach substantially diminishes parameter count and computational\nrequirements. Beyond the pretraining implementation of the MoLE architecture,\nwe also establish a rigorous mathematical framework for transforming\npre-trained MoE models into the MoLE architecture, characterizing the\nsufficient conditions for optimal factorization and developing a systematic\ntwo-phase algorithm for this conversion process. Our comprehensive theoretical\nanalysis demonstrates that MoLE significantly enhances computational efficiency\nacross multiple dimensions while preserving model representational capacity.\nEmpirical evaluations corroborate our theoretical findings, confirming that\nMoLE achieves performance comparable to standard MoE implementations while\nsubstantially reducing resource requirements."}
{"id": "2503.23519", "pdf": "https://arxiv.org/pdf/2503.23519", "abs": "https://arxiv.org/abs/2503.23519", "authors": ["Haruya Ishikawa", "Yoshimitsu Aoki"], "title": "BoundMatch: Boundary detection applied to semi-supervised segmentation for urban-driving scenes", "categories": ["cs.CV"], "comment": "15 pages, 7 figures", "summary": "Semi-supervised semantic segmentation (SS-SS) aims to mitigate the heavy\nannotation burden of dense pixel labeling by leveraging abundant unlabeled\nimages alongside a small labeled set. While current teacher-student consistency\nregularization methods achieve strong results, they often overlook a critical\nchallenge: the precise delineation of object boundaries. In this paper, we\npropose BoundMatch, a novel multi-task SS-SS framework that explicitly\nintegrates semantic boundary detection into the consistency regularization\npipeline. Our core mechanism, Boundary Consistency Regularized Multi-Task\nLearning (BCRM), enforces prediction agreement between teacher and student\nmodels on both segmentation masks and detailed semantic boundaries. To further\nenhance performance and sharpen contours, BoundMatch incorporates two\nlightweight fusion modules: Boundary-Semantic Fusion (BSF) injects learned\nboundary cues into the segmentation decoder, while Spatial Gradient Fusion\n(SGF) refines boundary predictions using mask gradients, leading to\nhigher-quality boundary pseudo-labels. This framework is built upon SAMTH, a\nstrong teacher-student baseline featuring a Harmonious Batch Normalization\n(HBN) update strategy for improved stability. Extensive experiments on diverse\ndatasets including Cityscapes, BDD100K, SYNTHIA, ADE20K, and Pascal VOC show\nthat BoundMatch achieves competitive performance against state-of-the-art\nmethods while significantly improving boundary-specific evaluation metrics. We\nalso demonstrate its effectiveness in realistic large-scale unlabeled data\nscenarios and on lightweight architectures designed for mobile deployment."}
{"id": "2503.23106", "pdf": "https://arxiv.org/pdf/2503.23106", "abs": "https://arxiv.org/abs/2503.23106", "authors": ["Chao Tao", "Dandan Zhong", "Weiliang Mu", "Zhuofei Du", "Haiyang Wu"], "title": "A large-scale image-text dataset benchmark for farmland segmentation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "The traditional deep learning paradigm that solely relies on labeled data has\nlimitations in representing the spatial relationships between farmland elements\nand the surrounding environment.It struggles to effectively model the dynamic\ntemporal evolution and spatial heterogeneity of farmland. Language,as a\nstructured knowledge carrier,can explicitly express the spatiotemporal\ncharacteristics of farmland, such as its shape, distribution,and surrounding\nenvironmental information.Therefore,a language-driven learning paradigm can\neffectively alleviate the challenges posed by the spatiotemporal heterogeneity\nof farmland.However,in the field of remote sensing imagery of farmland,there is\ncurrently no comprehensive benchmark dataset to support this research\ndirection.To fill this gap,we introduced language based descriptions of\nfarmland and developed FarmSeg-VL dataset,the first fine-grained image-text\ndataset designed for spatiotemporal farmland segmentation.Firstly, this article\nproposed a semi-automatic annotation method that can accurately assign caption\nto each image, ensuring high data quality and semantic richness while improving\nthe efficiency of dataset construction.Secondly,the FarmSeg-VL exhibits\nsignificant spatiotemporal characteristics.In terms of the temporal\ndimension,it covers all four seasons.In terms of the spatial dimension,it\ncovers eight typical agricultural regions across China.In addition, in terms of\ncaptions,FarmSeg-VL covers rich spatiotemporal characteristics of\nfarmland,including its inherent properties,phenological characteristics,\nspatial distribution,topographic and geomorphic features,and the distribution\nof surrounding environments.Finally,we present a performance analysis of VLMs\nand the deep learning models that rely solely on labels trained on the\nFarmSeg-VL,demonstrating its potential as a standard benchmark for farmland\nsegmentation."}
{"id": "2503.23529", "pdf": "https://arxiv.org/pdf/2503.23529", "abs": "https://arxiv.org/abs/2503.23529", "authors": ["Shuhei Tarashima", "Xinqi Shu", "Norio Tagawa"], "title": "ViLAaD: Enhancing \"Attracting and Dispersing'' Source-Free Domain Adaptation with Vision-and-Language Model", "categories": ["cs.CV"], "comment": "15 pages", "summary": "Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained source model\nto a target dataset from a different domain without access to the source data.\nConventional SFDA methods are limited by the information encoded in the\npre-trained source model and the unlabeled target data. Recently, approaches\nleveraging auxiliary resources have emerged, yet remain in their early stages,\noffering ample opportunities for research. In this work, we propose a novel\nmethod that incorporates auxiliary information by extending an existing SFDA\nframework using Vision-and-Language (ViL) models. Specifically, we build upon\nAttracting and Dispersing (AaD), a widely adopted SFDA technique, and\ngeneralize its core principle to naturally integrate ViL models as a powerful\ninitialization for target adaptation. Our approach, called ViL-enhanced AaD\n(ViLAaD), preserves the simplicity and flexibility of the AaD framework, while\nleveraging ViL models to significantly boost adaptation performance. We\nvalidate our method through experiments using various ViL models, demonstrating\nthat ViLAaD consistently outperforms both AaD and zero-shot classification by\nViL models, especially when both the source model and ViL model provide strong\ninitializations. Moreover, the flexibility of ViLAaD allows it to be seamlessly\nincorporated into an alternating optimization framework with ViL prompt tuning\nand extended with additional objectives for target model adaptation. Extensive\nexperiments on four SFDA benchmarks show that this enhanced version, ViLAaD++,\nachieves state-of-the-art performance across multiple SFDA scenarios, including\nClosed-set SFDA, Partial-set SFDA, and Open-set SFDA."}
{"id": "2503.23130", "pdf": "https://arxiv.org/pdf/2503.23130", "abs": "https://arxiv.org/abs/2503.23130", "authors": ["Boyi Ma", "Yanguang Zhao", "Jie Wang", "Guankun Wang", "Kun Yuan", "Tong Chen", "Long Bai", "Hongliang Ren"], "title": "Can DeepSeek-V3 Reason Like a Surgeon? An Empirical Evaluation for Vision-Language Understanding in Robotic-Assisted Surgery", "categories": ["cs.CV", "cs.CL", "cs.RO"], "comment": "Technical Report", "summary": "DeepSeek-V3, a recently emerging Large Language Model (LLM), demonstrates\noutstanding performance in general scene understanding, question-answering\n(QA), and text generation tasks, owing to its efficient training paradigm and\nstrong reasoning capabilities. In this study, we investigate the dialogue\ncapabilities of DeepSeek-V3 in robotic surgery scenarios, focusing on tasks\nsuch as Single Phrase QA, Visual QA, and Detailed Description. The Single\nPhrase QA tasks further include sub-tasks such as surgical instrument\nrecognition, action understanding, and spatial position analysis. We conduct\nextensive evaluations using publicly available datasets, including EndoVis18\nand CholecT50, along with their corresponding dialogue data. Our comprehensive\nevaluation results indicate that, when provided with specific prompts,\nDeepSeek-V3 performs well in surgical instrument and tissue recognition tasks\nHowever, DeepSeek-V3 exhibits significant limitations in spatial position\nanalysis and struggles to understand surgical actions accurately. Additionally,\nour findings reveal that, under general prompts, DeepSeek-V3 lacks the ability\nto effectively analyze global surgical concepts and fails to provide detailed\ninsights into surgical scenarios. Based on our observations, we argue that the\nDeepSeek-V3 is not ready for vision-language tasks in surgical contexts without\nfine-tuning on surgery-specific datasets."}
{"id": "2503.23534", "pdf": "https://arxiv.org/pdf/2503.23534", "abs": "https://arxiv.org/abs/2503.23534", "authors": ["Rafi Ibn Sultan", "Hui Zhu", "Chengyin Li", "Dongxiao Zhu"], "title": "BiPVL-Seg: Bidirectional Progressive Vision-Language Fusion with Global-Local Alignment for Medical Image Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Medical image segmentation typically relies solely on visual data,\noverlooking the rich textual information clinicians use for diagnosis.\nVision-language models attempt to bridge this gap, but existing approaches\noften process visual and textual features independently, resulting in weak\ncross-modal alignment. Simple fusion techniques fail due to the inherent\ndifferences between spatial visual features and sequential text embeddings.\nAdditionally, medical terminology deviates from general language, limiting the\neffectiveness of off-the-shelf text encoders and further hindering\nvision-language alignment. We propose BiPVL-Seg, an end-to-end framework that\nintegrates vision-language fusion and embedding alignment through architectural\nand training innovations, where both components reinforce each other to enhance\nmedical image segmentation. BiPVL-Seg introduces bidirectional progressive\nfusion in the architecture, which facilitates stage-wise information exchange\nbetween vision and text encoders. Additionally, it incorporates global-local\ncontrastive alignment, a training objective that enhances the text encoder's\ncomprehension by aligning text and vision embeddings at both class and concept\nlevels. Extensive experiments on diverse medical imaging benchmarks across CT\nand MR modalities demonstrate BiPVL-Seg's superior performance when compared\nwith state-of-the-art methods in complex multi-class segmentation. Source code\nis available in this GitHub repository."}
{"id": "2503.23137", "pdf": "https://arxiv.org/pdf/2503.23137", "abs": "https://arxiv.org/abs/2503.23137", "authors": ["Tuo Liang", "Zhe Hu", "Jing Li", "Hao Zhang", "Yiren Lu", "Yunlai Zhou", "Yiran Qiao", "Disheng Liu", "Jeirui Peng", "Jing Ma", "Yu Yin"], "title": "When 'YES' Meets 'BUT': Can Large Models Comprehend Contradictory Humor Through Comparative Reasoning?", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Understanding humor-particularly when it involves complex, contradictory\nnarratives that require comparative reasoning-remains a significant challenge\nfor large vision-language models (VLMs). This limitation hinders AI's ability\nto engage in human-like reasoning and cultural expression. In this paper, we\ninvestigate this challenge through an in-depth analysis of comics that\njuxtapose panels to create humor through contradictions. We introduce the\nYesBut (V2), a novel benchmark with 1,262 comic images from diverse\nmultilingual and multicultural contexts, featuring comprehensive annotations\nthat capture various aspects of narrative understanding. Using this benchmark,\nwe systematically evaluate a wide range of VLMs through four complementary\ntasks spanning from surface content comprehension to deep narrative reasoning,\nwith particular emphasis on comparative reasoning between contradictory\nelements. Our extensive experiments reveal that even the most advanced models\nsignificantly underperform compared to humans, with common failures in visual\nperception, key element identification, comparative analysis and\nhallucinations. We further investigate text-based training strategies and\nsocial knowledge augmentation methods to enhance model performance. Our\nfindings not only highlight critical weaknesses in VLMs' understanding of\ncultural and creative expressions but also provide pathways toward developing\ncontext-aware models capable of deeper narrative understanding though\ncomparative reasoning."}
{"id": "2503.23538", "pdf": "https://arxiv.org/pdf/2503.23538", "abs": "https://arxiv.org/abs/2503.23538", "authors": ["Jiyeon Han", "Dahee Kwon", "Gayoung Lee", "Junho Kim", "Jaesik Choi"], "title": "Enhancing Creative Generation on Stable Diffusion-based Models", "categories": ["cs.CV"], "comment": "CVPR 2025 accepted paper", "summary": "Recent text-to-image generative models, particularly Stable Diffusion and its\ndistilled variants, have achieved impressive fidelity and strong text-image\nalignment. However, their creative capability remains constrained, as including\n`creative' in prompts seldom yields the desired results. This paper introduces\nC3 (Creative Concept Catalyst), a training-free approach designed to enhance\ncreativity in Stable Diffusion-based models. C3 selectively amplifies features\nduring the denoising process to foster more creative outputs. We offer\npractical guidelines for choosing amplification factors based on two main\naspects of creativity. C3 is the first study to enhance creativity in diffusion\nmodels without extensive computational costs. We demonstrate its effectiveness\nacross various Stable Diffusion-based models."}
{"id": "2503.23145", "pdf": "https://arxiv.org/pdf/2503.23145", "abs": "https://arxiv.org/abs/2503.23145", "authors": ["Anjiang Wei", "Tarun Suresh", "Jiannan Cao", "Naveen Kannan", "Yuheng Wu", "Kai Yan", "Thiago S. F. X. Teixeira", "Ke Wang", "Alex Aiken"], "title": "CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive Program Synthesis", "categories": ["cs.PL", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Inductive program synthesis, or programming by example, requires synthesizing\nfunctions from input-output examples that generalize to unseen inputs. While\nlarge language model agents have shown promise in programming tasks guided by\nnatural language, their ability to perform inductive program synthesis is\nunderexplored. Existing evaluation protocols rely on static sets of examples\nand held-out tests, offering no feedback when synthesized functions are\nincorrect and failing to reflect real-world scenarios such as reverse\nengineering. We propose CodeARC, the Code Abstraction and Reasoning Challenge,\na new evaluation framework where agents interact with a hidden target function\nby querying it with new inputs, synthesizing candidate functions, and\niteratively refining their solutions using a differential testing oracle. This\ninteractive setting encourages agents to perform function calls and\nself-correction based on feedback. We construct the first large-scale benchmark\nfor general-purpose inductive program synthesis, featuring 1114 functions.\nAmong 18 models evaluated, o3-mini performs best with a success rate of 52.7%,\nhighlighting the difficulty of this task. Fine-tuning LLaMA-3.1-8B-Instruct on\ncurated synthesis traces yields up to a 31% relative performance gain. CodeARC\nprovides a more realistic and challenging testbed for evaluating LLM-based\nprogram synthesis and inductive reasoning."}
{"id": "2503.23573", "pdf": "https://arxiv.org/pdf/2503.23573", "abs": "https://arxiv.org/abs/2503.23573", "authors": ["Maximilian Augustin", "Yannic Neuhaus", "Matthias Hein"], "title": "DASH: Detection and Assessment of Systematic Hallucinations of VLMs", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Vision-language models (VLMs) are prone to object hallucinations, where they\nerroneously indicate the presenceof certain objects in an image. Existing\nbenchmarks quantify hallucinations using relatively small, labeled datasets.\nHowever, this approach is i) insufficient to assess hallucinations that arise\nin open-world settings, where VLMs are widely used, and ii) inadequate for\ndetecting systematic errors in VLMs. We propose DASH (Detection and Assessment\nof Systematic Hallucinations), an automatic, large-scale pipeline designed to\nidentify systematic hallucinations of VLMs on real-world images in an\nopen-world setting. A key component is DASH-OPT for image-based retrieval,\nwhere we optimize over the ''natural image manifold'' to generate images that\nmislead the VLM. The output of DASH consists of clusters of real and\nsemantically similar images for which the VLM hallucinates an object. We apply\nDASH to PaliGemma and two LLaVA-NeXT models across 380 object classes and, in\ntotal, find more than 19k clusters with 950k images. We study the transfer of\nthe identified systematic hallucinations to other VLMs and show that\nfine-tuning PaliGemma with the model-specific images obtained with DASH\nmitigates object hallucinations. Code and data are available at\nhttps://YanNeu.github.io/DASH."}
{"id": "2503.23174", "pdf": "https://arxiv.org/pdf/2503.23174", "abs": "https://arxiv.org/abs/2503.23174", "authors": ["Mattia Opper", "Roland Fernandez", "Paul Smolensky", "Jianfeng Gao"], "title": "TRA: Better Length Generalisation with Threshold Relative Attention", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Transformers struggle with length generalisation, displaying poor performance\neven on basic tasks. We test whether these limitations can be explained through\ntwo key failures of the self-attention mechanism. The first is the inability to\nfully remove irrelevant information. The second is tied to position, even if\nthe dot product between a key and query is highly negative (i.e. an irrelevant\nkey) learned positional biases may unintentionally up-weight such information -\ndangerous when distances become out of distribution. Put together, these two\nfailure cases lead to compounding generalisation difficulties. We test whether\nthey can be mitigated through the combination of a) selective sparsity -\ncompletely removing irrelevant keys from the attention softmax and b)\ncontextualised relative distance - distance is only considered as between the\nquery and the keys that matter. We show how refactoring the attention mechanism\nwith these two mitigations in place can substantially improve generalisation\ncapabilities of decoder only transformers."}
{"id": "2503.23577", "pdf": "https://arxiv.org/pdf/2503.23577", "abs": "https://arxiv.org/abs/2503.23577", "authors": ["Cameron Fiore", "Hongyi Fan", "Benjamin Kimia"], "title": "Multiview Image-Based Localization", "categories": ["cs.CV"], "comment": null, "summary": "The image retrieval (IR) approach to image localization has distinct\nadvantages to the 3D and the deep learning (DNN) approaches: it is\nseen-agnostic, simpler to implement and use, has no privacy issues, and is\ncomputationally efficient. The main drawback of this approach is relatively\npoor localization in both position and orientation of the query camera when\ncompared to the competing approaches. This paper represents a hybrid approach\nthat stores only image features in the database like some IR methods, but\nrelies on a latent 3D reconstruction, like 3D methods but without retaining a\n3D scene reconstruction. The approach is based on two ideas: {\\em (i)} a novel\nproposal where query camera center estimation relies only on relative\ntranslation estimates but not relative rotation estimates through a decoupling\nof the two, and {\\em (ii)} a shift from computing optimal pose from estimated\nrelative pose to computing optimal pose from multiview correspondences, thus\ncutting out the ``middle-man''. Our approach shows improved performance on the\n7-Scenes and Cambridge Landmarks datasets while also improving on timing and\nmemory footprint as compared to state-of-the-art."}
{"id": "2503.23239", "pdf": "https://arxiv.org/pdf/2503.23239", "abs": "https://arxiv.org/abs/2503.23239", "authors": ["Reza Esfandiarpoor", "George Zerveas", "Ruochen Zhang", "Macton Mgonzo", "Carsten Eickhoff", "Stephen H. Bach"], "title": "Beyond Contrastive Learning: Synthetic Data Enables List-wise Training with Multiple Levels of Relevance", "categories": ["cs.IR", "cs.CL", "cs.LG"], "comment": "Code: https://github.com/BatsResearch/sycl", "summary": "Recent advancements in large language models (LLMs) have allowed the\naugmentation of information retrieval (IR) pipelines with synthetic data in\nvarious ways. Yet, the main training paradigm remains: contrastive learning\nwith binary relevance labels and the InfoNCE loss, where one positive document\nis compared against one or more negatives. This objective treats all documents\nthat are not explicitly annotated as relevant on an equally negative footing,\nregardless of their actual degree of relevance, thus (a) missing subtle nuances\nthat are useful for ranking and (b) being susceptible to annotation noise. To\novercome this limitation, in this work we forgo real training documents and\nannotations altogether and use open-source LLMs to directly generate synthetic\ndocuments that answer real user queries according to several different levels\nof relevance. This fully synthetic ranking context of graduated relevance,\ntogether with an appropriate list-wise loss (Wasserstein distance), enables us\nto train dense retrievers in a way that better captures the ranking task.\nExperiments on various IR datasets show that our proposed approach outperforms\nconventional training with InfoNCE by a large margin. Without using any real\ndocuments for training, our dense retriever significantly outperforms the same\nretriever trained through self-supervision. More importantly, it matches the\nperformance of the same retriever trained on real, labeled training documents\nof the same dataset, while being more robust to distribution shift and clearly\noutperforming it when evaluated zero-shot on the BEIR dataset collection."}
{"id": "2503.23580", "pdf": "https://arxiv.org/pdf/2503.23580", "abs": "https://arxiv.org/abs/2503.23580", "authors": ["Zheng-Peng Duan", "Jiawei Zhang", "Xin Jin", "Ziheng Zhang", "Zheng Xiong", "Dongqing Zou", "Jimmy Ren", "Chun-Le Guo", "Chongyi Li"], "title": "DiT4SR: Taming Diffusion Transformer for Real-World Image Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Large-scale pre-trained diffusion models are becoming increasingly popular in\nsolving the Real-World Image Super-Resolution (Real-ISR) problem because of\ntheir rich generative priors. The recent development of diffusion transformer\n(DiT) has witnessed overwhelming performance over the traditional UNet-based\narchitecture in image generation, which also raises the question: Can we adopt\nthe advanced DiT-based diffusion model for Real-ISR? To this end, we propose\nour DiT4SR, one of the pioneering works to tame the large-scale DiT model for\nReal-ISR. Instead of directly injecting embeddings extracted from\nlow-resolution (LR) images like ControlNet, we integrate the LR embeddings into\nthe original attention mechanism of DiT, allowing for the bidirectional flow of\ninformation between the LR latent and the generated latent. The sufficient\ninteraction of these two streams allows the LR stream to evolve with the\ndiffusion process, producing progressively refined guidance that better aligns\nwith the generated latent at each diffusion step. Additionally, the LR guidance\nis injected into the generated latent via a cross-stream convolution layer,\ncompensating for DiT's limited ability to capture local information. These\nsimple but effective designs endow the DiT model with superior performance in\nReal-ISR, which is demonstrated by extensive experiments. Project Page:\nhttps://adam-duan.github.io/projects/dit4sr/."}
{"id": "2503.23314", "pdf": "https://arxiv.org/pdf/2503.23314", "abs": "https://arxiv.org/abs/2503.23314", "authors": ["Wonduk Seo", "Juhyeon Lee", "Yi Bu"], "title": "SPIO: Ensemble and Selective Strategies via LLM-Based Multi-Agent Planning in Automated Data Science", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "comment": "Under Review", "summary": "Large Language Models (LLMs) have revolutionized automated data analytics and\nmachine learning by enabling dynamic reasoning and adaptability. While recent\napproaches have advanced multi-stage pipelines through multi-agent systems,\nthey typically rely on rigid, single-path workflows that limit the exploration\nand integration of diverse strategies, often resulting in suboptimal\npredictions. To address these challenges, we propose SPIO (Sequential Plan\nIntegration and Optimization), a novel framework that leverages LLM-driven\ndecision-making to orchestrate multi-agent planning across four key modules:\ndata preprocessing, feature engineering, modeling, and hyperparameter tuning.\nIn each module, dedicated planning agents independently generate candidate\nstrategies that cascade into subsequent stages, fostering comprehensive\nexploration. A plan optimization agent refines these strategies by suggesting\nseveral optimized plans. We further introduce two variants: SPIO-S, which\nselects a single best solution path as determined by the LLM, and SPIO-E, which\nselects the top k candidate plans and ensembles them to maximize predictive\nperformance. Extensive experiments on Kaggle and OpenML datasets demonstrate\nthat SPIO significantly outperforms state-of-the-art methods, providing a\nrobust and scalable solution for automated data science task."}
{"id": "2503.23587", "pdf": "https://arxiv.org/pdf/2503.23587", "abs": "https://arxiv.org/abs/2503.23587", "authors": ["Martin Malenický", "Martin Cífka", "Médéric Fourmy", "Louis Montaut", "Justin Carpentier", "Josef Sivic", "Vladimir Petrik"], "title": "PhysPose: Refining 6D Object Poses with Physical Constraints", "categories": ["cs.CV", "cs.RO"], "comment": "Project page: https://data.ciirc.cvut.cz/public/projects/2025PhysPose", "summary": "Accurate 6D object pose estimation from images is a key problem in\nobject-centric scene understanding, enabling applications in robotics,\naugmented reality, and scene reconstruction. Despite recent advances, existing\nmethods often produce physically inconsistent pose estimates, hindering their\ndeployment in real-world scenarios. We introduce PhysPose, a novel approach\nthat integrates physical reasoning into pose estimation through a\npostprocessing optimization enforcing non-penetration and gravitational\nconstraints. By leveraging scene geometry, PhysPose refines pose estimates to\nensure physical plausibility. Our approach achieves state-of-the-art accuracy\non the YCB-Video dataset from the BOP benchmark and improves over the\nstate-of-the-art pose estimation methods on the HOPE-Video dataset.\nFurthermore, we demonstrate its impact in robotics by significantly improving\nsuccess rates in a challenging pick-and-place task, highlighting the importance\nof physical consistency in real-world applications."}
{"id": "2503.23333", "pdf": "https://arxiv.org/pdf/2503.23333", "abs": "https://arxiv.org/abs/2503.23333", "authors": ["Jing Zhu", "Mingxuan Ju", "Yozen Liu", "Danai Koutra", "Neil Shah", "Tong Zhao"], "title": "Beyond Unimodal Boundaries: Generative Recommendation with Multimodal Semantics", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Generative recommendation (GR) has become a powerful paradigm in\nrecommendation systems that implicitly links modality and semantics to item\nrepresentation, in contrast to previous methods that relied on non-semantic\nitem identifiers in autoregressive models. However, previous research has\npredominantly treated modalities in isolation, typically assuming item content\nis unimodal (usually text). We argue that this is a significant limitation\ngiven the rich, multimodal nature of real-world data and the potential\nsensitivity of GR models to modality choices and usage. Our work aims to\nexplore the critical problem of Multimodal Generative Recommendation (MGR),\nhighlighting the importance of modality choices in GR nframeworks. We reveal\nthat GR models are particularly sensitive to different modalities and examine\nthe challenges in achieving effective GR when multiple modalities are\navailable. By evaluating design strategies for effectively leveraging multiple\nmodalities, we identify key challenges and introduce MGR-LF++, an enhanced late\nfusion framework that employs contrastive modality alignment and special tokens\nto denote different modalities, achieving a performance improvement of over 20%\ncompared to single-modality alternatives."}
{"id": "2503.23606", "pdf": "https://arxiv.org/pdf/2503.23606", "abs": "https://arxiv.org/abs/2503.23606", "authors": ["Wei Xu", "Charles James Wagner", "Junjie Luo", "Qi Guo"], "title": "Blurry-Edges: Photon-Limited Depth Estimation from Defocused Boundaries", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025. Project page: https://blurry-edges.qiguo.org/", "summary": "Extracting depth information from photon-limited, defocused images is\nchallenging because depth from defocus (DfD) relies on accurate estimation of\ndefocus blur, which is fundamentally sensitive to image noise. We present a\nnovel approach to robustly measure object depths from photon-limited images\nalong the defocused boundaries. It is based on a new image patch\nrepresentation, Blurry-Edges, that explicitly stores and visualizes a rich set\nof low-level patch information, including boundaries, color, and smoothness. We\ndevelop a deep neural network architecture that predicts the Blurry-Edges\nrepresentation from a pair of differently defocused images, from which depth\ncan be calculated using a closed-form DfD relation we derive. The experimental\nresults on synthetic and real data show that our method achieves the highest\ndepth estimation accuracy on photon-limited images compared to a broad range of\nstate-of-the-art DfD methods."}
{"id": "2503.23339", "pdf": "https://arxiv.org/pdf/2503.23339", "abs": "https://arxiv.org/abs/2503.23339", "authors": ["Neil Mallinar", "A. Ali Heydari", "Xin Liu", "Anthony Z. Faranesh", "Brent Winslow", "Nova Hammerquist", "Benjamin Graef", "Cathy Speed", "Mark Malhotra", "Shwetak Patel", "Javier L. Prieto", "Daniel McDuff", "Ahmed A. Metwally"], "title": "A Scalable Framework for Evaluating Health Language Models", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) have emerged as powerful tools for analyzing\ncomplex datasets. Recent studies demonstrate their potential to generate\nuseful, personalized responses when provided with patient-specific health\ninformation that encompasses lifestyle, biomarkers, and context. As LLM-driven\nhealth applications are increasingly adopted, rigorous and efficient one-sided\nevaluation methodologies are crucial to ensure response quality across multiple\ndimensions, including accuracy, personalization and safety. Current evaluation\npractices for open-ended text responses heavily rely on human experts. This\napproach introduces human factors and is often cost-prohibitive,\nlabor-intensive, and hinders scalability, especially in complex domains like\nhealthcare where response assessment necessitates domain expertise and\nconsiders multifaceted patient data. In this work, we introduce Adaptive\nPrecise Boolean rubrics: an evaluation framework that streamlines human and\nautomated evaluation of open-ended questions by identifying gaps in model\nresponses using a minimal set of targeted rubrics questions. Our approach is\nbased on recent work in more general evaluation settings that contrasts a\nsmaller set of complex evaluation targets with a larger set of more precise,\ngranular targets answerable with simple boolean responses. We validate this\napproach in metabolic health, a domain encompassing diabetes, cardiovascular\ndisease, and obesity. Our results demonstrate that Adaptive Precise Boolean\nrubrics yield higher inter-rater agreement among expert and non-expert human\nevaluators, and in automated assessments, compared to traditional Likert\nscales, while requiring approximately half the evaluation time of Likert-based\nmethods. This enhanced efficiency, particularly in automated evaluation and\nnon-expert contributions, paves the way for more extensive and cost-effective\nevaluation of LLMs in health."}
{"id": "2503.23618", "pdf": "https://arxiv.org/pdf/2503.23618", "abs": "https://arxiv.org/abs/2503.23618", "authors": ["Amar Kumar", "Anita Kriz", "Barak Pertzov", "Tal Arbel"], "title": "Leveraging Vision-Language Foundation Models to Reveal Hidden Image-Attribute Relationships in Medical Imaging", "categories": ["cs.CV"], "comment": null, "summary": "Vision-language foundation models (VLMs) have shown impressive performance in\nguiding image generation through text, with emerging applications in medical\nimaging. In this work, we are the first to investigate the question: 'Can\nfine-tuned foundation models help identify critical, and possibly unknown, data\nproperties?' By evaluating our proposed method on a chest x-ray dataset, we\nshow that these models can generate high-resolution, precisely edited images\ncompared to methods that rely on Structural Causal Models (SCMs) according to\nnumerous metrics. For the first time, we demonstrate that fine-tuned VLMs can\nreveal hidden data relationships that were previously obscured due to available\nmetadata granularity and model capacity limitations. Our experiments\ndemonstrate both the potential of these models to reveal underlying dataset\nproperties while also exposing the limitations of fine-tuned VLMs for accurate\nimage editing and susceptibility to biases and spurious correlations."}
{"id": "2503.23363", "pdf": "https://arxiv.org/pdf/2503.23363", "abs": "https://arxiv.org/abs/2503.23363", "authors": ["Jiwon Jeong", "Hyeju Jang", "Hogun Park"], "title": "Large Language Models Are Better Logical Fallacy Reasoners with Counterargument, Explanation, and Goal-Aware Prompt Formulation", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted to NAACL 2025 Findings", "summary": "The advancement of Large Language Models (LLMs) has greatly improved our\nability to process complex language. However, accurately detecting logical\nfallacies remains a significant challenge. This study presents a novel and\neffective prompt formulation approach for logical fallacy detection, applicable\nin both supervised (fine-tuned) and unsupervised (zero-shot) settings. Our\nmethod enriches input text incorporating implicit contextual information --\ncounterarguments, explanations, and goals -- which we query for validity within\nthe context of the argument. We then rank these queries based on confidence\nscores to inform classification. We evaluate our approach across multiple\ndatasets from 5 domains, covering 29 distinct fallacy types, using models from\nthe GPT and LLaMA series. The results show substantial improvements over\nstate-of-the-art models, with F1 score increases of up to 0.60 in zero-shot\nsettings and up to 0.45 in fine-tuned models. Extensive analyses further\nillustrate why and how our method excels."}
{"id": "2503.23623", "pdf": "https://arxiv.org/pdf/2503.23623", "abs": "https://arxiv.org/abs/2503.23623", "authors": ["Zahra TehraniNasab", "Amar Kumar", "Tal Arbel"], "title": "Language-Guided Trajectory Traversal in Disentangled Stable Diffusion Latent Space for Factorized Medical Image Generation", "categories": ["cs.CV"], "comment": "10 pages", "summary": "Text-to-image diffusion models have demonstrated a remarkable ability to\ngenerate photorealistic images from natural language prompts. These\nhigh-resolution, language-guided synthesized images are essential for the\nexplainability of disease or exploring causal relationships. However, their\npotential for disentangling and controlling latent factors of variation in\nspecialized domains like medical imaging remains under-explored. In this work,\nwe present the first investigation of the power of pre-trained vision-language\nfoundation models, once fine-tuned on medical image datasets, to perform latent\ndisentanglement for factorized medical image generation and interpolation.\nThrough extensive experiments on chest X-ray and skin datasets, we illustrate\nthat fine-tuned, language-guided Stable Diffusion inherently learns to\nfactorize key attributes for image generation, such as the patient's anatomical\nstructures or disease diagnostic features. We devise a framework to identify,\nisolate, and manipulate key attributes through latent space trajectory\ntraversal of generative models, facilitating precise control over medical image\nsynthesis."}
{"id": "2503.23424", "pdf": "https://arxiv.org/pdf/2503.23424", "abs": "https://arxiv.org/abs/2503.23424", "authors": ["Gil Gekker", "Meirav Segal", "Dan Lahav", "Omer Nevo"], "title": "What Makes an Evaluation Useful? Common Pitfalls and Best Practices", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR", "cs.CY"], "comment": null, "summary": "Following the rapid increase in Artificial Intelligence (AI) capabilities in\nrecent years, the AI community has voiced concerns regarding possible safety\nrisks. To support decision-making on the safe use and development of AI\nsystems, there is a growing need for high-quality evaluations of dangerous\nmodel capabilities. While several attempts to provide such evaluations have\nbeen made, a clear definition of what constitutes a \"good evaluation\" has yet\nto be agreed upon. In this practitioners' perspective paper, we present a set\nof best practices for safety evaluations, drawing on prior work in model\nevaluation and illustrated through cybersecurity examples. We first discuss the\nsteps of the initial thought process, which connects threat modeling to\nevaluation design. Then, we provide the characteristics and parameters that\nmake an evaluation useful. Finally, we address additional considerations as we\nmove from building specific evaluations to building a full and comprehensive\nevaluation suite."}
{"id": "2503.23647", "pdf": "https://arxiv.org/pdf/2503.23647", "abs": "https://arxiv.org/abs/2503.23647", "authors": ["Said Ohamouddoua", "Mohamed Ohamouddoub", "Rafik Lasrib", "Hanaa El Afiaa", "Raddouane Chiheba", "Abdellatif El Afiaa"], "title": "Introducing the Short-Time Fourier Kolmogorov Arnold Network: A Dynamic Graph CNN Approach for Tree Species Classification in 3D Point Clouds", "categories": ["cs.CV"], "comment": null, "summary": "Accurate classification of tree species based on Terrestrial Laser Scanning\n(TLS) and Airborne Laser Scanning (ALS) is essential for biodiversity\nconservation. While advanced deep learning models for 3D point cloud\nclassification have demonstrated strong performance in this domain, their high\ncomplexity often hinders the development of efficient, low-computation\narchitectures. In this paper, we introduce STFT-KAN, a novel Kolmogorov-Arnold\nnetwork that integrates the Short-Time Fourier Transform (STFT), which can\nreplace the standard linear layer with activation. We implemented STFT-KAN\nwithin a lightweight version of DGCNN, called liteDGCNN, to classify tree\nspecies using the TLS data. Our experiments show that STFT-KAN outperforms\nexisting KAN variants by effectively balancing model complexity and performance\nwith parameter count reduction, achieving competitive results compared to\nMLP-based models. Additionally, we evaluated a hybrid architecture that\ncombines MLP in edge convolution with STFT-KAN in other layers, achieving\ncomparable performance to MLP models while reducing the parameter count by 50%\nand 75% compared to other KAN-based variants. Furthermore, we compared our\nmodel to leading 3D point cloud learning approaches, demonstrating that\nSTFT-KAN delivers competitive results compared to the state-of-the-art method\nPointMLP lite with an 87% reduction in parameter count."}
{"id": "2503.23448", "pdf": "https://arxiv.org/pdf/2503.23448", "abs": "https://arxiv.org/abs/2503.23448", "authors": ["Max Hort", "Linas Vidziunas", "Leon Moonen"], "title": "Semantic-Preserving Transformations as Mutation Operators: A Study on Their Effectiveness in Defect Detection", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted for publication in Mutation 2025 at the 18th IEEE\n  International Conference on Software Testing, Verification and Validation\n  (ICST 2025)", "summary": "Recent advances in defect detection use language models. Existing works\nenhanced the training data to improve the models' robustness when applied to\nsemantically identical code (i.e., predictions should be the same). However,\nthe use of semantically identical code has not been considered for improving\nthe tools during their application - a concept closely related to metamorphic\ntesting.\n  The goal of our study is to determine whether we can use semantic-preserving\ntransformations, analogue to mutation operators, to improve the performance of\ndefect detection tools in the testing stage. We first collect existing\npublications which implemented semantic-preserving transformations and share\ntheir implementation, such that we can reuse them. We empirically study the\neffectiveness of three different ensemble strategies for enhancing defect\ndetection tools. We apply the collected transformations on the Devign dataset,\nconsidering vulnerabilities as a type of defect, and two fine-tuned large\nlanguage models for defect detection (VulBERTa, PLBART). We found 28\npublications with 94 different transformations.\n  We choose to implement 39 transformations from four of the publications, but\na manual check revealed that 23 out 39 transformations change code semantics.\nUsing the 16 remaining, correct transformations and three ensemble strategies,\nwe were not able to increase the accuracy of the defect detection models. Our\nresults show that reusing shared semantic-preserving transformation is\ndifficult, sometimes even causing wrongful changes to the semantics.\n  Keywords: defect detection, language model, semantic-preserving\ntransformation, ensemble"}
{"id": "2503.23660", "pdf": "https://arxiv.org/pdf/2503.23660", "abs": "https://arxiv.org/abs/2503.23660", "authors": ["Junjie Zheng", "Zihao Chen", "Chaofan Ding", "Xinhan Di"], "title": "DeepDubber-V1: Towards High Quality and Dialogue, Narration, Monologue Adaptive Movie Dubbing Via Multi-Modal Chain-of-Thoughts Reasoning Guidance", "categories": ["cs.CV"], "comment": "11 pages, 5 figures", "summary": "Current movie dubbing technology can generate the desired voice from a given\nspeech prompt, ensuring good synchronization between speech and visuals while\naccurately conveying the intended emotions. However, in movie dubbing, key\naspects such as adapting to different dubbing styles, handling dialogue,\nnarration, and monologue effectively, and understanding subtle details like the\nage and gender of speakers, have not been well studied. To address this\nchallenge, we propose a framework of multi-modal large language model. First,\nit utilizes multimodal Chain-of-Thought (CoT) reasoning methods on visual\ninputs to understand dubbing styles and fine-grained attributes. Second, it\ngenerates high-quality dubbing through large speech generation models, guided\nby multimodal conditions. Additionally, we have developed a movie dubbing\ndataset with CoT annotations. The evaluation results demonstrate a performance\nimprovement over state-of-the-art methods across multiple datasets. In\nparticular, for the evaluation metrics, the SPK-SIM and EMO-SIM increases from\n82.48% to 89.74%, 66.24% to 78.88% for dubbing setting 2.0 on V2C Animation\ndataset, LSE-D and MCD-SL decreases from 14.79 to 14.63, 5.24 to 4.74 for\ndubbing setting 2.0 on Grid dataset, SPK-SIM increases from 64.03 to 83.42 and\nWER decreases from 52.69% to 23.20% for initial reasoning setting on proposed\nCoT-Movie-Dubbing dataset in the comparison with the state-of-the art models."}
{"id": "2503.23466", "pdf": "https://arxiv.org/pdf/2503.23466", "abs": "https://arxiv.org/abs/2503.23466", "authors": ["Max Hort", "Leon Moonen"], "title": "Codehacks: A Dataset of Adversarial Tests for Competitive Programming Problems Obtained from Codeforces", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted for publication at the 18th IEEE International Conference on\n  Software Testing, Verification and Validation (ICST 2025)", "summary": "Software is used in critical applications in our day-to-day life and it is\nimportant to ensure its correctness. One popular approach to assess correctness\nis to evaluate software on tests. If a test fails, it indicates a fault in the\nsoftware under test; if all tests pass correctly, one may assume that the\nsoftware is correct. However, the reliability of these results depends on the\ntest suite considered, and there is a risk of false negatives (i.e. software\nthat passes all available tests but contains bugs because some cases are not\ntested). Therefore, it is important to consider error-inducing test cases when\nevaluating software.\n  To support data-driven creation of such a test-suite, which is especially of\ninterest for testing software synthesized from large language models, we curate\na dataset (Codehacks) of programming problems together with corresponding\nerror-inducing test cases (i.e., \"hacks\"). This dataset is collected from the\nwild, in particular, from the Codeforces online judge platform. The dataset\ncomprises 288,617 hacks for 5,578 programming problems, each with a natural\nlanguage description, as well as the source code for 2,196 submitted solutions\nto these problems that can be broken with their corresponding hacks.\n  Keywords: competitive programming, language model, dataset"}
{"id": "2503.23664", "pdf": "https://arxiv.org/pdf/2503.23664", "abs": "https://arxiv.org/abs/2503.23664", "authors": ["Masahiko Tsuji", "Hitoshi Niigaki", "Ryuichi Tanida"], "title": "LiM-Loc: Visual Localization with Dense and Accurate 3D Reference Maps Directly Corresponding 2D Keypoints to 3D LiDAR Point Clouds", "categories": ["cs.CV"], "comment": "8 pages, 6 figures", "summary": "Visual localization is to estimate the 6-DOF camera pose of a query image in\na 3D reference map. We extract keypoints from the reference image and generate\na 3D reference map with 3D reconstruction of the keypoints in advance. We\nemphasize that the more keypoints in the 3D reference map and the smaller the\nerror of the 3D positions of the keypoints, the higher the accuracy of the\ncamera pose estimation. However, previous image-only methods require a huge\nnumber of images, and it is difficult to 3D-reconstruct keypoints without error\ndue to inevitable mismatches and failures in feature matching. As a result, the\n3D reference map is sparse and inaccurate. In contrast, accurate 3D reference\nmaps can be generated by combining images and 3D sensors. Recently, 3D-LiDAR\nhas been widely used around the world. LiDAR, which measures a large space with\nhigh density, has become inexpensive. In addition, accurately calibrated\ncameras are also widely used, so images that record the external parameters of\nthe camera without errors can be easily obtained. In this paper, we propose a\nmethod to directly assign 3D LiDAR point clouds to keypoints to generate dense\nand accurate 3D reference maps. The proposed method avoids feature matching and\nachieves accurate 3D reconstruction for almost all keypoints. To estimate\ncamera pose over a wide area, we use the wide-area LiDAR point cloud to remove\npoints that are not visible to the camera and reduce 2D-3D correspondence\nerrors. Using indoor and outdoor datasets, we apply the proposed method to\nseveral state-of-the-art local features and confirm that it improves the\naccuracy of camera pose estimation."}
{"id": "2503.23487", "pdf": "https://arxiv.org/pdf/2503.23487", "abs": "https://arxiv.org/abs/2503.23487", "authors": ["Irtaza Khalid", "Amir Masoud Nourollah", "Steven Schockaert"], "title": "Benchmarking Systematic Relational Reasoning with Large Language and Reasoning Models", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Submitted to ACL 2025", "summary": "Large Language Models (LLMs) have been found to struggle with systematic\nreasoning. Even on tasks where they appear to perform well, their performance\noften depends on shortcuts, rather than on genuine reasoning abilities, leading\nthem to collapse on out-of-distribution examples. Post-training strategies\nbased on reinforcement learning and chain-of-thought prompting have recently\nbeen hailed as a step change. However, little is still known about the\npotential of the resulting ``Large Reasoning Models'' (LRMs) beyond problem\nsolving in mathematics and programming, where finding genuine\nout-of-distribution problems can be difficult. In this paper, we focus on tasks\nthat require systematic reasoning about relational compositions, especially for\nqualitative spatial and temporal reasoning. These tasks allow us to control the\ndifficulty of problem instances, and measure in a precise way to what extent\nmodels can generalise. We find that that the considered LLMs and LRMs overall\nperform poorly overall, albeit better than random chance."}
{"id": "2503.23667", "pdf": "https://arxiv.org/pdf/2503.23667", "abs": "https://arxiv.org/abs/2503.23667", "authors": ["Kotaro Inoue"], "title": "Context-Independent OCR with Multimodal LLMs: Effects of Image Resolution and Visual Complexity", "categories": ["cs.CV"], "comment": null, "summary": "Due to their high versatility in tasks such as image captioning, document\nanalysis, and automated content generation, multimodal Large Language Models\n(LLMs) have attracted significant attention across various industrial fields.\nIn particular, they have been shown to surpass specialized models in Optical\nCharacter Recognition (OCR). Nevertheless, their performance under different\nimage conditions remains insufficiently investigated, and individual character\nrecognition is not guaranteed due to their reliance on contextual cues. In this\nwork, we examine a context-independent OCR task using single-character images\nwith diverse visual complexities to determine the conditions for accurate\nrecognition. Our findings reveal that multimodal LLMs can match conventional\nOCR methods at about 300 ppi, yet their performance deteriorates significantly\nbelow 150 ppi. Additionally, we observe a very weak correlation between visual\ncomplexity and misrecognitions, whereas a conventional OCR-specific model\nexhibits no correlation. These results suggest that image resolution and visual\ncomplexity may play an important role in the reliable application of multimodal\nLLMs to OCR tasks that require precise character-level accuracy."}
{"id": "2503.23730", "pdf": "https://arxiv.org/pdf/2503.23730", "abs": "https://arxiv.org/abs/2503.23730", "authors": ["Yoonshik Kim", "Jaeyoon Jung"], "title": "KOFFVQA: An Objectively Evaluated Free-form VQA Benchmark for Large Vision-Language Models in the Korean Language", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Accepted to CVPRW 2025, Workshop on Benchmarking and Expanding AI\n  Multimodal Approaches", "summary": "The recent emergence of Large Vision-Language Models(VLMs) has resulted in a\nvariety of different benchmarks for evaluating such models. Despite this, we\nobserve that most existing evaluation methods suffer from the fact that they\neither require the model to choose from pre-determined responses, sacrificing\nopen-endedness, or evaluate responses using a judge model, resulting in\nsubjective and unreliable evaluation. In addition, we observe a lack of\nbenchmarks for VLMs in the Korean language, which are necessary as a separate\nmetric from more common English language benchmarks, as the performance of\ngenerative language models can differ significantly based on the language being\nused. Therefore, we present KOFFVQA, a general-purpose free-form visual\nquestion answering benchmark in the Korean language for the evaluation of VLMs.\nOur benchmark consists of 275 carefully crafted questions each paired with an\nimage and grading criteria covering 10 different aspects of VLM performance.\nThe grading criteria eliminate the problem of unreliability by allowing the\njudge model to grade each response based on a pre-determined set of rules. By\ndefining the evaluation criteria in an objective manner, even a small\nopen-source model can be used to evaluate models on our benchmark reliably. In\naddition to evaluating a large number of existing VLMs on our benchmark, we\nalso experimentally verify that our method of using pre-existing grading\ncriteria for evaluation is much more reliable than existing methods. Our\nevaluation code is available at https://github.com/maum-ai/KOFFVQA"}
{"id": "2503.23670", "pdf": "https://arxiv.org/pdf/2503.23670", "abs": "https://arxiv.org/abs/2503.23670", "authors": ["Takeshi Noda", "Chao Chen", "Junsheng Zhou", "Weiqi Zhang", "Yu-Shen Liu", "Zhizhong Han"], "title": "Learning Bijective Surface Parameterization for Inferring Signed Distance Functions from Sparse Point Clouds with Grid Deformation", "categories": ["cs.CV"], "comment": "Accepted by Conference on Computer Vision and Pattern Recognition\n  (CVPR) 2025. Project page:https://takeshie.github.io/Bijective-SDF", "summary": "Inferring signed distance functions (SDFs) from sparse point clouds remains a\nchallenge in surface reconstruction. The key lies in the lack of detailed\ngeometric information in sparse point clouds, which is essential for learning a\ncontinuous field. To resolve this issue, we present a novel approach that\nlearns a dynamic deformation network to predict SDFs in an end-to-end manner.\nTo parameterize a continuous surface from sparse points, we propose a bijective\nsurface parameterization (BSP) that learns the global shape from local patches.\nSpecifically, we construct a bijective mapping for sparse points from the\nparametric domain to 3D local patches, integrating patches into the global\nsurface. Meanwhile, we introduce grid deformation optimization (GDO) into the\nsurface approximation to optimize the deformation of grid points and further\nrefine the parametric surfaces. Experimental results on synthetic and real\nscanned datasets demonstrate that our method significantly outperforms the\ncurrent state-of-the-art methods. Project page:\nhttps://takeshie.github.io/Bijective-SDF"}
{"id": "2503.23746", "pdf": "https://arxiv.org/pdf/2503.23746", "abs": "https://arxiv.org/abs/2503.23746", "authors": ["Dizhan Xue", "Jing Cui", "Shengsheng Qian", "Chuanrui Hu", "Changsheng Xu"], "title": "Short-video Propagation Influence Rating: A New Real-world Dataset and A New Large Graph Model", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.MM", "cs.SI"], "comment": null, "summary": "Short-video platforms have gained immense popularity, captivating the\ninterest of millions, if not billions, of users globally. Recently, researchers\nhave highlighted the significance of analyzing the propagation of short-videos,\nwhich typically involves discovering commercial values, public opinions, user\nbehaviors, etc. This paper proposes a new Short-video Propagation Influence\nRating (SPIR) task and aims to promote SPIR from both the dataset and method\nperspectives. First, we propose a new Cross-platform Short-Video (XS-Video)\ndataset, which aims to provide a large-scale and real-world short-video\npropagation network across various platforms to facilitate the research on\nshort-video propagation. Our XS-Video dataset includes 117,720 videos, 381,926\nsamples, and 535 topics across 5 biggest Chinese platforms, annotated with the\npropagation influence from level 0 to 9. To the best of our knowledge, this is\nthe first large-scale short-video dataset that contains cross-platform data or\nprovides all of the views, likes, shares, collects, fans, comments, and comment\ncontent. Second, we propose a Large Graph Model (LGM) named NetGPT, based on a\nnovel three-stage training mechanism, to bridge heterogeneous graph-structured\ndata with the powerful reasoning ability and knowledge of Large Language Models\n(LLMs). Our NetGPT can comprehend and analyze the short-video propagation\ngraph, enabling it to predict the long-term propagation influence of\nshort-videos. Comprehensive experimental results evaluated by both\nclassification and regression metrics on our XS-Video dataset indicate the\nsuperiority of our method for SPIR."}
{"id": "2503.23679", "pdf": "https://arxiv.org/pdf/2503.23679", "abs": "https://arxiv.org/abs/2503.23679", "authors": ["Mingkai Tian", "Guorong Li", "Yuankai Qi", "Amin Beheshti", "Javen Qinfeng Shi", "Anton van den Hengel", "Qingming Huang"], "title": "The Devil is in the Distributions: Explicit Modeling of Scene Content is Key in Zero-Shot Video Captioning", "categories": ["cs.CV"], "comment": "13 pages", "summary": "Zero-shot video captioning requires that a model generate high-quality\ncaptions without human-annotated video-text pairs for training.\nState-of-the-art approaches to the problem leverage CLIP to extract\nvisual-relevant textual prompts to guide language models in generating\ncaptions. These methods tend to focus on one key aspect of the scene and build\na caption that ignores the rest of the visual input. To address this issue, and\ngenerate more accurate and complete captions, we propose a novel progressive\nmulti-granularity textual prompting strategy for zero-shot video captioning.\nOur approach constructs three distinct memory banks, encompassing noun phrases,\nscene graphs of noun phrases, and entire sentences. Moreover, we introduce a\ncategory-aware retrieval mechanism that models the distribution of natural\nlanguage surrounding the specific topics in question. Extensive experiments\ndemonstrate the effectiveness of our method with 5.7%, 16.2%, and 3.4%\nimprovements in terms of the main metric CIDEr on MSR-VTT, MSVD, and VATEX\nbenchmarks compared to existing state-of-the-art."}
{"id": "2503.23760", "pdf": "https://arxiv.org/pdf/2503.23760", "abs": "https://arxiv.org/abs/2503.23760", "authors": ["Manuel Scheibl", "Birte Richter", "Alissa Müller", "Michael Beetz", "Britta Wrede"], "title": "Towards a cognitive architecture to enable natural language interaction in co-constructive task learning", "categories": ["cs.RO", "cs.CL", "cs.HC"], "comment": "8 pages, 5 figures, submitted to: IEEE RO-MAN 2025", "summary": "This research addresses the question, which characteristics a cognitive\narchitecture must have to leverage the benefits of natural language in\nCo-Constructive Task Learning (CCTL). To provide context, we first discuss\nInteractive Task Learning (ITL), the mechanisms of the human memory system, and\nthe significance of natural language and multi-modality. Next, we examine the\ncurrent state of cognitive architectures, analyzing their capabilities to\ninform a concept of CCTL grounded in multiple sources. We then integrate\ninsights from various research domains to develop a unified framework. Finally,\nwe conclude by identifying the remaining challenges and requirements necessary\nto achieve CCTL in Human-Robot Interaction (HRI)."}
{"id": "2503.23684", "pdf": "https://arxiv.org/pdf/2503.23684", "abs": "https://arxiv.org/abs/2503.23684", "authors": ["Haitao Tian", "Junyang Li", "Chenxing Wang", "Helong Jiang"], "title": "Detail-aware multi-view stereo network for depth estimation", "categories": ["cs.CV"], "comment": null, "summary": "Multi-view stereo methods have achieved great success for depth estimation\nbased on the coarse-to-fine depth learning frameworks, however, the existing\nmethods perform poorly in recovering the depth of object boundaries and detail\nregions. To address these issues, we propose a detail-aware multi-view stereo\nnetwork (DA-MVSNet) with a coarse-to-fine framework. The geometric depth clues\nhidden in the coarse stage are utilized to maintain the geometric structural\nrelationships between object surfaces and enhance the expressive capability of\nimage features. In addition, an image synthesis loss is employed to constrain\nthe gradient flow for detailed regions and further strengthen the supervision\nof object boundaries and texture-rich areas. Finally, we propose an adaptive\ndepth interval adjustment strategy to improve the accuracy of object\nreconstruction. Extensive experiments on the DTU and Tanks & Temples datasets\ndemonstrate that our method achieves competitive results. The code is available\nat https://github.com/wsmtht520-/DAMVSNet."}
{"id": "2503.23804", "pdf": "https://arxiv.org/pdf/2503.23804", "abs": "https://arxiv.org/abs/2503.23804", "authors": ["Shiyi Yang", "Zhibo Hu", "Chen Wang", "Tong Yu", "Xiwei Xu", "Liming Zhu", "Lina Yao"], "title": "Get the Agents Drunk: Memory Perturbations in Autonomous Agent-based Recommender Systems", "categories": ["cs.CR", "cs.CL", "cs.IR", "cs.MA"], "comment": null, "summary": "Large language model-based agents are increasingly used in recommender\nsystems (Agent4RSs) to achieve personalized behavior modeling. Specifically,\nAgent4RSs introduces memory mechanisms that enable the agents to autonomously\nlearn and self-evolve from real-world interactions. However, to the best of our\nknowledge, how robust Agent4RSs are remains unexplored. As such, in this paper,\nwe propose the first work to attack Agent4RSs by perturbing agents' memories,\nnot only to uncover their limitations but also to enhance their security and\nrobustness, ensuring the development of safer and more reliable AI agents.\n  Given the security and privacy concerns, it is more practical to launch\nattacks under a black-box setting, where the accurate knowledge of the victim\nmodels cannot be easily obtained. Moreover, the practical attacks are often\nstealthy to maximize the impact. To this end, we propose a novel practical\nattack framework named DrunkAgent. DrunkAgent consists of a generation module,\na strategy module, and a surrogate module. The generation module aims to\nproduce effective and coherent adversarial textual triggers, which can be used\nto achieve attack objectives such as promoting the target items. The strategy\nmodule is designed to `get the target agents drunk' so that their memories\ncannot be effectively updated during the interaction process. As such, the\ntriggers can play the best role. Both of the modules are optimized on the\nsurrogate module to improve the transferability and imperceptibility of the\nattacks. By identifying and analyzing the vulnerabilities, our work provides\ncritical insights that pave the way for building safer and more resilient\nAgent4RSs. Extensive experiments across various real-world datasets demonstrate\nthe effectiveness of DrunkAgent."}
{"id": "2503.23702", "pdf": "https://arxiv.org/pdf/2503.23702", "abs": "https://arxiv.org/abs/2503.23702", "authors": ["Shufan Xi", "Zexian Liu", "Junlin Chang", "Hongyu Wu", "Xiaogang Wang", "Aimin Hao"], "title": "3D Dental Model Segmentation with Geometrical Boundary Preserving", "categories": ["cs.CV"], "comment": "The IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  2025", "summary": "3D intraoral scan mesh is widely used in digital dentistry diagnosis,\nsegmenting 3D intraoral scan mesh is a critical preliminary task. Numerous\napproaches have been devised for precise tooth segmentation. Currently, the\ndeep learning-based methods are capable of the high accuracy segmentation of\ncrown. However, the segmentation accuracy at the junction between the crown and\nthe gum is still below average. Existing down-sampling methods are unable to\neffectively preserve the geometric details at the junction. To address these\nproblems, we propose CrossTooth, a boundary-preserving segmentation method that\ncombines 3D mesh selective downsampling to retain more vertices at the\ntooth-gingiva area, along with cross-modal discriminative boundary features\nextracted from multi-view rendered images, enhancing the geometric\nrepresentation of the segmentation network. Using a point network as a backbone\nand incorporating image complementary features, CrossTooth significantly\nimproves segmentation accuracy, as demonstrated by experiments on a public\nintraoral scan dataset."}
{"id": "2503.24110", "pdf": "https://arxiv.org/pdf/2503.24110", "abs": "https://arxiv.org/abs/2503.24110", "authors": ["François Olivier", "Zied Bouraoui"], "title": "Grounding Agent Reasoning in Image Schemas: A Neurosymbolic Approach to Embodied Cognition", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Despite advances in embodied AI, agent reasoning systems still struggle to\ncapture the fundamental conceptual structures that humans naturally use to\nunderstand and interact with their environment. To address this, we propose a\nnovel framework that bridges embodied cognition theory and agent systems by\nleveraging a formal characterization of image schemas, which are defined as\nrecurring patterns of sensorimotor experience that structure human cognition.\nBy customizing LLMs to translate natural language descriptions into formal\nrepresentations based on these sensorimotor patterns, we will be able to create\na neurosymbolic system that grounds the agent's understanding in fundamental\nconceptual structures. We argue that such an approach enhances both efficiency\nand interpretability while enabling more intuitive human-agent interactions\nthrough shared embodied understanding."}
{"id": "2503.23709", "pdf": "https://arxiv.org/pdf/2503.23709", "abs": "https://arxiv.org/abs/2503.23709", "authors": ["Xulong Shi", "Caiyi Sun", "Zhi Qi", "Liu Hao", "Xiaodong Yang"], "title": "Expanding-and-Shrinking Binary Neural Networks", "categories": ["cs.CV"], "comment": null, "summary": "While binary neural networks (BNNs) offer significant benefits in terms of\nspeed, memory and energy, they encounter substantial accuracy degradation in\nchallenging tasks compared to their real-valued counterparts. Due to the\nbinarization of weights and activations, the possible values of each entry in\nthe feature maps generated by BNNs are strongly constrained. To tackle this\nlimitation, we propose the expanding-and-shrinking operation, which enhances\nbinary feature maps with negligible increase of computation complexity, thereby\nstrengthening the representation capacity. Extensive experiments conducted on\nmultiple benchmarks reveal that our approach generalizes well across diverse\napplications ranging from image classification, object detection to generative\ndiffusion model, while also achieving remarkable improvement over various\nleading binarization algorithms based on different architectures including both\nCNNs and Transformers."}
{"id": "2503.24219", "pdf": "https://arxiv.org/pdf/2503.24219", "abs": "https://arxiv.org/abs/2503.24219", "authors": ["Karim Radouane", "Hanane Azzag", "Mustapha lebbah"], "title": "MB-ORES: A Multi-Branch Object Reasoner for Visual Grounding in Remote Sensing", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": null, "summary": "We propose a unified framework that integrates object detection (OD) and\nvisual grounding (VG) for remote sensing (RS) imagery. To support conventional\nOD and establish an intuitive prior for VG task, we fine-tune an open-set\nobject detector using referring expression data, framing it as a partially\nsupervised OD task. In the first stage, we construct a graph representation of\neach image, comprising object queries, class embeddings, and proposal\nlocations. Then, our task-aware architecture processes this graph to perform\nthe VG task. The model consists of: (i) a multi-branch network that integrates\nspatial, visual, and categorical features to generate task-aware proposals, and\n(ii) an object reasoning network that assigns probabilities across proposals,\nfollowed by a soft selection mechanism for final referring object localization.\nOur model demonstrates superior performance on the OPT-RSVG and DIOR-RSVG\ndatasets, achieving significant improvements over state-of-the-art methods\nwhile retaining classical OD capabilities. The code will be available in our\nrepository: \\url{https://github.com/rd20karim/MB-ORES}."}
{"id": "2503.23712", "pdf": "https://arxiv.org/pdf/2503.23712", "abs": "https://arxiv.org/abs/2503.23712", "authors": ["Jie Cheng", "Hao Zheng", "Meiguang Zheng", "Lei Wang", "Hao Wu", "Jian Zhang"], "title": "ElimPCL: Eliminating Noise Accumulation with Progressive Curriculum Labeling for Source-Free Domain Adaptation", "categories": ["cs.CV"], "comment": "ICME 2025 camera-ready", "summary": "Source-Free Domain Adaptation (SFDA) aims to train a target model without\nsource data, and the key is to generate pseudo-labels using a pre-trained\nsource model. However, we observe that the source model often produces highly\nuncertain pseudo-labels for hard samples, particularly those heavily affected\nby domain shifts, leading to these noisy pseudo-labels being introduced even\nbefore adaptation and further reinforced through parameter updates.\nAdditionally, they continuously influence neighbor samples through propagation\nin the feature space.To eliminate the issue of noise accumulation, we propose a\nnovel Progressive Curriculum Labeling (ElimPCL) method, which iteratively\nfilters trustworthy pseudo-labeled samples based on prototype consistency to\nexclude high-noise samples from training. Furthermore, a Dual MixUP technique\nis designed in the feature space to enhance the separability of hard samples,\nthereby mitigating the interference of noisy samples on their\nneighbors.Extensive experiments validate the effectiveness of ElimPCL,\nachieving up to a 3.4% improvement on challenging tasks compared to\nstate-of-the-art methods."}
{"id": "2503.24228", "pdf": "https://arxiv.org/pdf/2503.24228", "abs": "https://arxiv.org/abs/2503.24228", "authors": ["Saab Mansour", "Leonardo Perelli", "Lorenzo Mainetti", "George Davidson", "Stefano D'Amato"], "title": "PAARS: Persona Aligned Agentic Retail Shoppers", "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "In e-commerce, behavioral data is collected for decision making which can be\ncostly and slow. Simulation with LLM powered agents is emerging as a promising\nalternative for representing human population behavior. However, LLMs are known\nto exhibit certain biases, such as brand bias, review rating bias and limited\nrepresentation of certain groups in the population, hence they need to be\ncarefully benchmarked and aligned to user behavior. Ultimately, our goal is to\nsynthesise an agent population and verify that it collectively approximates a\nreal sample of humans. To this end, we propose a framework that: (i) creates\nsynthetic shopping agents by automatically mining personas from anonymised\nhistorical shopping data, (ii) equips agents with retail-specific tools to\nsynthesise shopping sessions and (iii) introduces a novel alignment suite\nmeasuring distributional differences between humans and shopping agents at the\ngroup (i.e. population) level rather than the traditional \"individual\" level.\nExperimental results demonstrate that using personas improves performance on\nthe alignment suite, though a gap remains to human behaviour. We showcase an\ninitial application of our framework for automated agentic A/B testing and\ncompare the findings to human results. Finally, we discuss applications,\nlimitations and challenges setting the stage for impactful future work."}
{"id": "2503.23715", "pdf": "https://arxiv.org/pdf/2503.23715", "abs": "https://arxiv.org/abs/2503.23715", "authors": ["Kun Liu", "Qi Liu", "Xinchen Liu", "Jie Li", "Yongdong Zhang", "Jiebo Luo", "Xiaodong He", "Wu Liu"], "title": "HOIGen-1M: A Large-scale Dataset for Human-Object Interaction Video Generation", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Text-to-video (T2V) generation has made tremendous progress in generating\ncomplicated scenes based on texts. However, human-object interaction (HOI)\noften cannot be precisely generated by current T2V models due to the lack of\nlarge-scale videos with accurate captions for HOI. To address this issue, we\nintroduce HOIGen-1M, the first largescale dataset for HOI Generation,\nconsisting of over one million high-quality videos collected from diverse\nsources. In particular, to guarantee the high quality of videos, we first\ndesign an efficient framework to automatically curate HOI videos using the\npowerful multimodal large language models (MLLMs), and then the videos are\nfurther cleaned by human annotators. Moreover, to obtain accurate textual\ncaptions for HOI videos, we design a novel video description method based on a\nMixture-of-Multimodal-Experts (MoME) strategy that not only generates\nexpressive captions but also eliminates the hallucination by individual MLLM.\nFurthermore, due to the lack of an evaluation framework for generated HOI\nvideos, we propose two new metrics to assess the quality of generated videos in\na coarse-to-fine manner. Extensive experiments reveal that current T2V models\nstruggle to generate high-quality HOI videos and confirm that our HOIGen-1M\ndataset is instrumental for improving HOI video generation. Project webpage is\navailable at https://liuqi-creat.github.io/HOIGen.github.io."}
{"id": "2503.24260", "pdf": "https://arxiv.org/pdf/2503.24260", "abs": "https://arxiv.org/abs/2503.24260", "authors": ["Zhengren Wang", "Rui Ling", "Chufan Wang", "Yongan Yu", "Zhiyu Li", "Feiyu Xiong", "Wentao Zhang"], "title": "MaintainCoder: Maintainable Code Generation Under Dynamic Requirements", "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "Modern code generation has made significant strides in functional correctness\nand execution efficiency. However, these systems often overlook a critical\ndimension in real-world software development: maintainability. To handle\ndynamic requirements with minimal rework, we propose MaintainCoder as a\npioneering solution. It integrates Waterfall model, design patterns, and\nmulti-agent collaboration to systematically enhance cohesion, reduce coupling,\nand improve adaptability. We also introduce MaintainBench, a benchmark\ncomprising requirement changes and corresponding dynamic metrics on\nmaintainance effort. Experiments demonstrate that existing code generation\nmethods struggle to meet maintainability standards when requirements evolve. In\ncontrast, MaintainCoder improves maintainability metrics by 14-30% with even\nhigher correctness, i.e. pass@k. Our work not only provides the foundation of\nmaintainable code generation, but also highlights the need for more holistic\ncode quality research. Resources:\nhttps://github.com/IAAR-Shanghai/MaintainCoder."}
{"id": "2503.23717", "pdf": "https://arxiv.org/pdf/2503.23717", "abs": "https://arxiv.org/abs/2503.23717", "authors": ["Yi Liu", "Wengen Li", "Jihong Guan", "Shuigeng Zhou", "Yichao Zhang"], "title": "Effective Cloud Removal for Remote Sensing Images by an Improved Mean-Reverting Denoising Model with Elucidated Design Space", "categories": ["cs.CV"], "comment": "29 pages, 12 figures", "summary": "Cloud removal (CR) remains a challenging task in remote sensing image\nprocessing. Although diffusion models (DM) exhibit strong generative\ncapabilities, their direct applications to CR are suboptimal, as they generate\ncloudless images from random noise, ignoring inherent information in cloudy\ninputs. To overcome this drawback, we develop a new CR model EMRDM based on\nmean-reverting diffusion models (MRDMs) to establish a direct diffusion process\nbetween cloudy and cloudless images. Compared to current MRDMs, EMRDM offers a\nmodular framework with updatable modules and an elucidated design space, based\non a reformulated forward process and a new ordinary differential equation\n(ODE)-based backward process. Leveraging our framework, we redesign key MRDM\nmodules to boost CR performance, including restructuring the denoiser via a\npreconditioning technique, reorganizing the training process, and improving the\nsampling process by introducing deterministic and stochastic samplers. To\nachieve multi-temporal CR, we further develop a denoising network for\nsimultaneously denoising sequential images. Experiments on mono-temporal and\nmulti-temporal datasets demonstrate the superior performance of EMRDM. Our code\nis available at https://github.com/Ly403/EMRDM."}
{"id": "2503.24289", "pdf": "https://arxiv.org/pdf/2503.24289", "abs": "https://arxiv.org/abs/2503.24289", "authors": ["Jiacheng Lin", "Tian Wang", "Kun Qian"], "title": "Rec-R1: Bridging Generative Large Language Models and User-Centric Recommendation Systems via Reinforcement Learning", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "We propose Rec-R1, a general reinforcement learning framework that bridges\nlarge language models (LLMs) with recommendation systems through closed-loop\noptimization. Unlike prompting and supervised fine-tuning (SFT), Rec-R1\ndirectly optimizes LLM generation using feedback from a fixed black-box\nrecommendation model, without relying on synthetic SFT data from proprietary\nmodels such as GPT-4o. This avoids the substantial cost and effort required for\ndata distillation. To verify the effectiveness of Rec-R1, we evaluate it on two\nrepresentative tasks: product search and sequential recommendation.\nExperimental results demonstrate that Rec-R1 not only consistently outperforms\nprompting- and SFT-based methods, but also achieves significant gains over\nstrong discriminative baselines, even when used with simple retrievers such as\nBM25. Moreover, Rec-R1 preserves the general-purpose capabilities of the LLM,\nunlike SFT, which often impairs instruction-following and reasoning. These\nfindings suggest Rec-R1 as a promising foundation for continual task-specific\nadaptation without catastrophic forgetting."}
{"id": "2503.23722", "pdf": "https://arxiv.org/pdf/2503.23722", "abs": "https://arxiv.org/abs/2503.23722", "authors": ["Xiang Hu", "Yuhao Wang", "Pingping Zhang", "Huchuan Lu"], "title": "LATex: Leveraging Attribute-based Text Knowledge for Aerial-Ground Person Re-Identification", "categories": ["cs.CV"], "comment": null, "summary": "Aerial-Ground person Re-IDentification (AG-ReID) aims to retrieve specific\npersons across heterogeneous cameras in different views. Previous methods\nusually adopt large-scale models, focusing on view-invariant features. However,\nthey overlook the semantic information in person attributes. Additionally,\nexisting training strategies often rely on full fine-tuning large-scale models,\nwhich significantly increases training costs. To address these issues, we\npropose a novel framework named LATex for AG-ReID, which adopts prompt-tuning\nstrategies to leverage attribute-based text knowledge. More specifically, we\nfirst introduce the Contrastive Language-Image Pre-training (CLIP) model as the\nbackbone, and propose an Attribute-aware Image Encoder (AIE) to extract global\nsemantic features and attribute-aware features. Then, with these features, we\npropose a Prompted Attribute Classifier Group (PACG) to generate person\nattribute predictions and obtain the encoded representations of predicted\nattributes. Finally, we design a Coupled Prompt Template (CPT) to transform\nattribute tokens and view information into structured sentences. These\nsentences are processed by the text encoder of CLIP to generate more\ndiscriminative features. As a result, our framework can fully leverage\nattribute-based text knowledge to improve the AG-ReID. Extensive experiments on\nthree AG-ReID benchmarks demonstrate the effectiveness of our proposed LATex.\nThe source code will be available."}
{"id": "2503.24290", "pdf": "https://arxiv.org/pdf/2503.24290", "abs": "https://arxiv.org/abs/2503.24290", "authors": ["Jingcheng Hu", "Yinmin Zhang", "Qi Han", "Daxin Jiang", "Xiangyu Zhang", "Heung-Yeung Shum"], "title": "Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "We introduce Open-Reasoner-Zero, the first open source implementation of\nlarge-scale reasoning-oriented RL training focusing on scalability, simplicity\nand accessibility. Through extensive experiments, we demonstrate that a\nminimalist approach, vanilla PPO with GAE ($\\lambda=1$, $\\gamma=1$) and\nstraightforward rule-based rewards, without any KL regularization, is\nsufficient to scale up both response length and benchmark performance, similar\nto the phenomenon observed in DeepSeek-R1-Zero. Using the same base model as\nDeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance on\nAIME2024, MATH500, and the GPQA Diamond benchmark while demonstrating\nremarkable efficiency -- requiring only a tenth of the training steps, compared\nto DeepSeek-R1-Zero pipeline. In the spirit of open source, we release our\nsource code, parameter settings, training data, and model weights across\nvarious sizes."}
{"id": "2503.23725", "pdf": "https://arxiv.org/pdf/2503.23725", "abs": "https://arxiv.org/abs/2503.23725", "authors": ["Hongwei Ren", "Xiaopeng Lin", "Hongxiang Huang", "Yue Zhou", "Bojun Cheng"], "title": "Exploring Temporal Dynamics in Event-based Eye Tracker", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025 Event-based Vision Workshop", "summary": "Eye-tracking is a vital technology for human-computer interaction, especially\nin wearable devices such as AR, VR, and XR. The realization of high-speed and\nhigh-precision eye-tracking using frame-based image sensors is constrained by\ntheir limited temporal resolution, which impairs the accurate capture of rapid\nocular dynamics, such as saccades and blinks. Event cameras, inspired by\nbiological vision systems, are capable of perceiving eye movements with\nextremely low power consumption and ultra-high temporal resolution. This makes\nthem a promising solution for achieving high-speed, high-precision tracking\nwith rich temporal dynamics. In this paper, we propose TDTracker, an effective\neye-tracking framework that captures rapid eye movements by thoroughly modeling\ntemporal dynamics from both implicit and explicit perspectives. TDTracker\nutilizes 3D convolutional neural networks to capture implicit short-term\ntemporal dynamics and employs a cascaded structure consisting of a\nFrequency-aware Module, GRU, and Mamba to extract explicit long-term temporal\ndynamics. Ultimately, a prediction heatmap is used for eye coordinate\nregression. Experimental results demonstrate that TDTracker achieves\nstate-of-the-art (SOTA) performance on the synthetic SEET dataset and secured\nThird place in the CVPR event-based eye-tracking challenge 2025. Our code is\navailable at https://github.com/rhwxmx/TDTracker."}
{"id": "2503.24354", "pdf": "https://arxiv.org/pdf/2503.24354", "abs": "https://arxiv.org/abs/2503.24354", "authors": ["Rana Muhammad Shahroz Khan", "Dongwen Tang", "Pingzhi Li", "Kai Wang", "Tianlong Chen"], "title": "ORAL: Prompting Your Large-Scale LoRAs via Conditional Recurrent Diffusion", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Parameter generation has emerged as a novel paradigm for neural network\ndevelopment, offering an alternative to traditional neural network training by\nsynthesizing high-quality model weights directly. In the context of Low-Rank\nAdaptation (LoRA) for evolving ($\\textit{i.e.}$, constantly updated) large\nlanguage models (LLMs), this approach promises efficient adaptation without\ncostly retraining. However, existing methods face critical limitations in\nsimultaneously achieving scalability and controllability. In this paper, we\nintroduce $\\texttt{ORAL}$, a novel $\\textbf{conditional recurrent diffusion}$\nframework that addresses these challenges. $\\texttt{ORAL}$ incorporates a novel\nconditioning mechanism that integrates model architecture and textual task\nspecifications, enabling the generation of task-specific LoRA parameters that\ncan seamlessly transfer across evolving foundation models. Our approach\nsuccessfully scales to billions-of-parameter LLMs and maintains\ncontrollability. Through extensive experiments across seven language tasks,\nfour vision tasks, and three multimodal tasks using five pre-trained LLMs, we\ndemonstrate that $\\texttt{ORAL}$ generates high-quality LoRA parameters that\nachieve comparable or superior performance to vanilla trained counterparts."}
{"id": "2503.23730", "pdf": "https://arxiv.org/pdf/2503.23730", "abs": "https://arxiv.org/abs/2503.23730", "authors": ["Yoonshik Kim", "Jaeyoon Jung"], "title": "KOFFVQA: An Objectively Evaluated Free-form VQA Benchmark for Large Vision-Language Models in the Korean Language", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Accepted to CVPRW 2025, Workshop on Benchmarking and Expanding AI\n  Multimodal Approaches", "summary": "The recent emergence of Large Vision-Language Models(VLMs) has resulted in a\nvariety of different benchmarks for evaluating such models. Despite this, we\nobserve that most existing evaluation methods suffer from the fact that they\neither require the model to choose from pre-determined responses, sacrificing\nopen-endedness, or evaluate responses using a judge model, resulting in\nsubjective and unreliable evaluation. In addition, we observe a lack of\nbenchmarks for VLMs in the Korean language, which are necessary as a separate\nmetric from more common English language benchmarks, as the performance of\ngenerative language models can differ significantly based on the language being\nused. Therefore, we present KOFFVQA, a general-purpose free-form visual\nquestion answering benchmark in the Korean language for the evaluation of VLMs.\nOur benchmark consists of 275 carefully crafted questions each paired with an\nimage and grading criteria covering 10 different aspects of VLM performance.\nThe grading criteria eliminate the problem of unreliability by allowing the\njudge model to grade each response based on a pre-determined set of rules. By\ndefining the evaluation criteria in an objective manner, even a small\nopen-source model can be used to evaluate models on our benchmark reliably. In\naddition to evaluating a large number of existing VLMs on our benchmark, we\nalso experimentally verify that our method of using pre-existing grading\ncriteria for evaluation is much more reliable than existing methods. Our\nevaluation code is available at https://github.com/maum-ai/KOFFVQA"}
{"id": "2503.24358", "pdf": "https://arxiv.org/pdf/2503.24358", "abs": "https://arxiv.org/abs/2503.24358", "authors": ["Hao Wang", "Ligong Han", "Kai Xu", "Akash Srivastava"], "title": "SQuat: Subspace-orthogonal KV Cache Quantization", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IT", "math.IT"], "comment": null, "summary": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms."}
{"id": "2503.23731", "pdf": "https://arxiv.org/pdf/2503.23731", "abs": "https://arxiv.org/abs/2503.23731", "authors": ["Yinq-Rong Chern", "Yuhao Lee", "Hsiao-Ching Lin", "Guan-Ting Chen", "Ying-Hsien Chen", "Fu-Sung Lin", "Chih-Yao Chuang", "Jenn-Jier James Lien", "Chih-Hsien Huang"], "title": "Investigation of intelligent barbell squat coaching system based on computer vision and machine learning", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "Purpose: Research has revealed that strength training can reduce the\nincidence of chronic diseases and physical deterioration at any age. Therefore,\nhaving a movement diagnostic system is crucial for training alone. Hence, this\nstudy developed an artificial intelligence and computer vision-based barbell\nsquat coaching system with a real-time mode that immediately diagnoses the\nissue and provides feedback after each squat. In addition, a replay mode allows\nusers to examine their previous squats and check their comments. Initially,\nfour primary characteristics of the barbell squat were identified: body joint\nangles, dorsiflexion, the ratio of knee-to-hip movement, and barbell stability.\nMethods: We collect 8,151 squats from 77 participants, categorizing them as\ngood squats and six issues. Then, we trained the diagnosis models with three\nmachine-learning architectures. Furthermore, this research applied the SHapley\nAdditive exPlanations (SHAP) method to enhance the accuracy of issue prediction\nand reduce the computation time by feature selection. Results: The F1 score of\nthe six issues reached 86.86%, 69.01%, 77.42%, 90.74%, 95.83%, and 100%. Each\nsquat diagnosis took less than 0.5 seconds. Finally, this study examined the\nefficacy of the proposed system with two groups of participants trained with\nand without the system. Subsequently, participants trained with the system\nexhibited substantial improvements in their squat technique, as assessed both\nby the system itself and by a professional weightlifting coach. Conclusion:\nThis is a comprehensive study that integrates artificial intelligence, computer\nvision and multivariable processing technologies, aimed at building a\nreal-time, user-friendly barbell squat feedback and training system."}
{"id": "2503.24370", "pdf": "https://arxiv.org/pdf/2503.24370", "abs": "https://arxiv.org/abs/2503.24370", "authors": ["Tong Wu", "Chong Xiang", "Jiachen T. Wang", "Prateek Mittal"], "title": "Effectively Controlling Reasoning Models through Thinking Intervention", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reasoning-enhanced large language models (LLMs) explicitly generate\nintermediate reasoning steps prior to generating final answers, helping the\nmodel excel in complex problem-solving. In this paper, we demonstrate that this\nemerging generation framework offers a unique opportunity for more fine-grained\ncontrol over model behavior. We propose Thinking Intervention, a novel paradigm\ndesigned to explicitly guide the internal reasoning processes of LLMs by\nstrategically inserting or revising specific thinking tokens. We conduct\ncomprehensive evaluations across multiple tasks, including instruction\nfollowing on IFEval, instruction hierarchy on SEP, and safety alignment on\nXSTest and SORRY-Bench. Our results demonstrate that Thinking Intervention\nsignificantly outperforms baseline prompting approaches, achieving up to 6.7%\naccuracy gains in instruction-following scenarios, 15.4% improvements in\nreasoning about instruction hierarchies, and a 40.0% increase in refusal rates\nfor unsafe prompts using open-source DeepSeek R1 models. Overall, our work\nopens a promising new research avenue for controlling reasoning LLMs."}
{"id": "2503.23736", "pdf": "https://arxiv.org/pdf/2503.23736", "abs": "https://arxiv.org/abs/2503.23736", "authors": ["Lingyu Liu", "Yaxiong Wang", "Li Zhu", "Zhedong Zheng"], "title": "Every Painting Awakened: A Training-free Framework for Painting-to-Animation Generation", "categories": ["cs.CV", "cs.MM"], "comment": "The project is available at:\n  https://painting-animation.github.io/animation/", "summary": "We introduce a training-free framework specifically designed to bring\nreal-world static paintings to life through image-to-video (I2V) synthesis,\naddressing the persistent challenge of aligning these motions with textual\nguidance while preserving fidelity to the original artworks. Existing I2V\nmethods, primarily trained on natural video datasets, often struggle to\ngenerate dynamic outputs from static paintings. It remains challenging to\ngenerate motion while maintaining visual consistency with real-world paintings.\nThis results in two distinct failure modes: either static outputs due to\nlimited text-based motion interpretation or distorted dynamics caused by\ninadequate alignment with real-world artistic styles. We leverage the advanced\ntext-image alignment capabilities of pre-trained image models to guide the\nanimation process. Our approach introduces synthetic proxy images through two\nkey innovations: (1) Dual-path score distillation: We employ a dual-path\narchitecture to distill motion priors from both real and synthetic data,\npreserving static details from the original painting while learning dynamic\ncharacteristics from synthetic frames. (2) Hybrid latent fusion: We integrate\nhybrid features extracted from real paintings and synthetic proxy images via\nspherical linear interpolation in the latent space, ensuring smooth transitions\nand enhancing temporal consistency. Experimental evaluations confirm that our\napproach significantly improves semantic alignment with text prompts while\nfaithfully preserving the unique characteristics and integrity of the original\npaintings. Crucially, by achieving enhanced dynamic effects without requiring\nany model training or learnable parameters, our framework enables plug-and-play\nintegration with existing I2V methods, making it an ideal solution for\nanimating real-world paintings. More animated examples can be found on our\nproject website."}
{"id": "2503.24376", "pdf": "https://arxiv.org/pdf/2503.24376", "abs": "https://arxiv.org/abs/2503.24376", "authors": ["Yi Chen", "Yuying Ge", "Rui Wang", "Yixiao Ge", "Lu Qiu", "Ying Shan", "Xihui Liu"], "title": "Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Technical Report (In Progress); Code released at:\n  https://github.com/TencentARC/SEED-Bench-R1", "summary": "Recent advancements in Chain of Thought (COT) generation have significantly\nimproved the reasoning capabilities of Large Language Models (LLMs), with\nreinforcement learning (RL) emerging as an effective post-training approach.\nMultimodal Large Language Models (MLLMs) inherit this reasoning potential but\nremain underexplored in tasks requiring both perception and logical reasoning.\nTo address this, we introduce SEED-Bench-R1, a benchmark designed to\nsystematically evaluate post-training methods for MLLMs in video understanding.\nIt includes intricate real-world videos and complex everyday planning tasks in\nthe format of multiple-choice questions, requiring sophisticated perception and\nreasoning. SEED-Bench-R1 assesses generalization through a three-level\nhierarchy: in-distribution, cross-environment, and cross-environment-task\nscenarios, equipped with a large-scale training dataset with easily verifiable\nground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL\nwith supervised fine-tuning (SFT), demonstrating RL's data efficiency and\nsuperior performance on both in-distribution and out-of-distribution tasks,\neven outperforming SFT on general video understanding benchmarks like\nLongVideoBench. Our detailed analysis reveals that RL enhances visual\nperception but often produces less logically coherent reasoning chains. We\nidentify key limitations such as inconsistent reasoning and overlooked visual\ncues, and suggest future improvements in base model reasoning, reward modeling,\nand RL robustness against noisy signals."}
{"id": "2503.23746", "pdf": "https://arxiv.org/pdf/2503.23746", "abs": "https://arxiv.org/abs/2503.23746", "authors": ["Dizhan Xue", "Jing Cui", "Shengsheng Qian", "Chuanrui Hu", "Changsheng Xu"], "title": "Short-video Propagation Influence Rating: A New Real-world Dataset and A New Large Graph Model", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.MM", "cs.SI"], "comment": null, "summary": "Short-video platforms have gained immense popularity, captivating the\ninterest of millions, if not billions, of users globally. Recently, researchers\nhave highlighted the significance of analyzing the propagation of short-videos,\nwhich typically involves discovering commercial values, public opinions, user\nbehaviors, etc. This paper proposes a new Short-video Propagation Influence\nRating (SPIR) task and aims to promote SPIR from both the dataset and method\nperspectives. First, we propose a new Cross-platform Short-Video (XS-Video)\ndataset, which aims to provide a large-scale and real-world short-video\npropagation network across various platforms to facilitate the research on\nshort-video propagation. Our XS-Video dataset includes 117,720 videos, 381,926\nsamples, and 535 topics across 5 biggest Chinese platforms, annotated with the\npropagation influence from level 0 to 9. To the best of our knowledge, this is\nthe first large-scale short-video dataset that contains cross-platform data or\nprovides all of the views, likes, shares, collects, fans, comments, and comment\ncontent. Second, we propose a Large Graph Model (LGM) named NetGPT, based on a\nnovel three-stage training mechanism, to bridge heterogeneous graph-structured\ndata with the powerful reasoning ability and knowledge of Large Language Models\n(LLMs). Our NetGPT can comprehend and analyze the short-video propagation\ngraph, enabling it to predict the long-term propagation influence of\nshort-videos. Comprehensive experimental results evaluated by both\nclassification and regression metrics on our XS-Video dataset indicate the\nsuperiority of our method for SPIR."}
{"id": "2503.24388", "pdf": "https://arxiv.org/pdf/2503.24388", "abs": "https://arxiv.org/abs/2503.24388", "authors": ["Zhonghan Zhao", "Wenwei Zhang", "Haian Huang", "Kuikun Liu", "Jianfei Gao", "Gaoang Wang", "Kai Chen"], "title": "RIG: Synergizing Reasoning and Imagination in End-to-End Generalist Policy", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Reasoning before action and imagining potential outcomes (i.e., world models)\nare essential for embodied agents operating in complex open-world environments.\nYet, prior work either incorporates only one of these abilities in an\nend-to-end agent or integrates multiple specialized models into an agent\nsystem, limiting the learning efficiency and generalization of the policy.\nThus, this paper makes the first attempt to synergize Reasoning and Imagination\nin an end-to-end Generalist policy, termed RIG. To train RIG in an end-to-end\nmanner, we construct a data pipeline that progressively integrates and enriches\nthe content of imagination and reasoning in the trajectories collected from\nexisting agents. The joint learning of reasoning and next image generation\nexplicitly models the inherent correlation between reasoning, action, and\ndynamics of environments, and thus exhibits more than $17\\times$ sample\nefficiency improvements and generalization in comparison with previous works.\nDuring inference, RIG first reasons about the next action, produces potential\naction, and then predicts the action outcomes, which offers the agent a chance\nto review and self-correct based on the imagination before taking real actions.\nExperimental results show that the synergy of reasoning and imagination not\nonly improves the robustness, generalization, and interoperability of\ngeneralist policy but also enables test-time scaling to enhance overall\nperformance."}
{"id": "2503.23747", "pdf": "https://arxiv.org/pdf/2503.23747", "abs": "https://arxiv.org/abs/2503.23747", "authors": ["Jingyi Zhou", "Peng Ye", "Haoyu Zhang", "Jiakang Yuan", "Rao Qiang", "Liu YangChenXu", "Wu Cailin", "Feng Xu", "Tao Chen"], "title": "Consistency-aware Self-Training for Iterative-based Stereo Matching", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Iterative-based methods have become mainstream in stereo matching due to\ntheir high performance. However, these methods heavily rely on labeled data and\nface challenges with unlabeled real-world data. To this end, we propose a\nconsistency-aware self-training framework for iterative-based stereo matching\nfor the first time, leveraging real-world unlabeled data in a teacher-student\nmanner. We first observe that regions with larger errors tend to exhibit more\npronounced oscillation characteristics during model prediction.Based on this,\nwe introduce a novel consistency-aware soft filtering module to evaluate the\nreliability of teacher-predicted pseudo-labels, which consists of a\nmulti-resolution prediction consistency filter and an iterative prediction\nconsistency filter to assess the prediction fluctuations of multiple\nresolutions and iterative optimization respectively. Further, we introduce a\nconsistency-aware soft-weighted loss to adjust the weight of pseudo-labels\naccordingly, relieving the error accumulation and performance degradation\nproblem due to incorrect pseudo-labels. Extensive experiments demonstrate that\nour method can improve the performance of various iterative-based stereo\nmatching approaches in various scenarios. In particular, our method can achieve\nfurther enhancements over the current SOTA methods on several benchmark\ndatasets."}
{"id": "2503.23751", "pdf": "https://arxiv.org/pdf/2503.23751", "abs": "https://arxiv.org/abs/2503.23751", "authors": ["Yu Zhou", "Dian Zheng", "Qijie Mo", "Renjie Lu", "Kun-Yu Lin", "Wei-Shi Zheng"], "title": "Decoupled Distillation to Erase: A General Unlearning Method for Any Class-centric Tasks", "categories": ["cs.CV"], "comment": "CVPR2025, Equal contributions from first two authors", "summary": "In this work, we present DEcoupLEd Distillation To Erase (DELETE), a general\nand strong unlearning method for any class-centric tasks. To derive this, we\nfirst propose a theoretical framework to analyze the general form of unlearning\nloss and decompose it into forgetting and retention terms. Through the\ntheoretical framework, we point out that a class of previous methods could be\nmainly formulated as a loss that implicitly optimizes the forgetting term while\nlacking supervision for the retention term, disturbing the distribution of\npre-trained model and struggling to adequately preserve knowledge of the\nremaining classes. To address it, we refine the retention term using \"dark\nknowledge\" and propose a mask distillation unlearning method. By applying a\nmask to separate forgetting logits from retention logits, our approach\noptimizes both the forgetting and refined retention components simultaneously,\nretaining knowledge of the remaining classes while ensuring thorough forgetting\nof the target class. Without access to the remaining data or intervention\n(i.e., used in some works), we achieve state-of-the-art performance across\nvarious benchmarks. What's more, DELETE is a general solution that can be\napplied to various downstream tasks, including face recognition, backdoor\ndefense, and semantic segmentation with great performance."}
{"id": "2503.23764", "pdf": "https://arxiv.org/pdf/2503.23764", "abs": "https://arxiv.org/abs/2503.23764", "authors": ["Md Mahfuz Al Hasan", "Mahdi Zaman", "Abdul Jawad", "Alberto Santamaria-Pang", "Ho Hin Lee", "Ivan Tarapov", "Kyle See", "Md Shah Imran", "Antika Roy", "Yaser Pourmohammadi Fallah", "Navid Asadizanjani", "Reza Forghani"], "title": "WaveFormer: A 3D Transformer with Wavelet-Driven Feature Representation for Efficient Medical Image Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Transformer-based architectures have advanced medical image analysis by\neffectively modeling long-range dependencies, yet they often struggle in 3D\nsettings due to substantial memory overhead and insufficient capture of\nfine-grained local features. We address these limi- tations with WaveFormer, a\nnovel 3D-transformer that: i) leverages the fundamental frequency-domain\nproperties of features for contextual rep- resentation, and ii) is inspired by\nthe top-down mechanism of the human visual recognition system, making it a\nbiologically motivated architec- ture. By employing discrete wavelet\ntransformations (DWT) at multiple scales, WaveFormer preserves both global\ncontext and high-frequency de- tails while replacing heavy upsampling layers\nwith efficient wavelet-based summarization and reconstruction. This\nsignificantly reduces the number of parameters, which is critical for\nreal-world deployment where compu- tational resources and training times are\nconstrained. Furthermore, the model is generic and easily adaptable to diverse\napplications. Evaluations on BraTS2023, FLARE2021, and KiTS2023 demonstrate\nperformance on par with state-of-the-art methods while offering substantially\nlower computational complexity."}
{"id": "2503.23765", "pdf": "https://arxiv.org/pdf/2503.23765", "abs": "https://arxiv.org/abs/2503.23765", "authors": ["Yun Li", "Yiming Zhang", "Tao Lin", "XiangRui Liu", "Wenxiao Cai", "Zheng Liu", "Bo Zhao"], "title": "STI-Bench: Are MLLMs Ready for Precise Spatial-Temporal World Understanding?", "categories": ["cs.CV"], "comment": null, "summary": "The use of Multimodal Large Language Models (MLLMs) as an end-to-end solution\nfor Embodied AI and Autonomous Driving has become a prevailing trend. While\nMLLMs have been extensively studied for visual semantic understanding tasks,\ntheir ability to perform precise and quantitative spatial-temporal\nunderstanding in real-world applications remains largely unexamined, leading to\nuncertain prospects. To evaluate models' Spatial-Temporal Intelligence, we\nintroduce STI-Bench, a benchmark designed to evaluate MLLMs' spatial-temporal\nunderstanding through challenging tasks such as estimating and predicting the\nappearance, pose, displacement, and motion of objects. Our benchmark\nencompasses a wide range of robot and vehicle operations across desktop,\nindoor, and outdoor scenarios. The extensive experiments reveals that the\nstate-of-the-art MLLMs still struggle in real-world spatial-temporal\nunderstanding, especially in tasks requiring precise distance estimation and\nmotion analysis."}
{"id": "2503.23771", "pdf": "https://arxiv.org/pdf/2503.23771", "abs": "https://arxiv.org/abs/2503.23771", "authors": ["Fengxiang Wang", "Hongzhen Wang", "Mingshuo Chen", "Di Wang", "Yulin Wang", "Zonghao Guo", "Qiang Ma", "Long Lan", "Wenjing Yang", "Jing Zhang", "Zhiyuan Liu", "Maosong Sun"], "title": "XLRS-Bench: Could Your Multimodal LLMs Understand Extremely Large Ultra-High-Resolution Remote Sensing Imagery?", "categories": ["cs.CV"], "comment": "It has been accepted by CVPR2025", "summary": "The astonishing breakthrough of multimodal large language models (MLLMs) has\nnecessitated new benchmarks to quantitatively assess their capabilities, reveal\ntheir limitations, and indicate future research directions. However, this is\nchallenging in the context of remote sensing (RS), since the imagery features\nultra-high resolution that incorporates extremely complex semantic\nrelationships. Existing benchmarks usually adopt notably smaller image sizes\nthan real-world RS scenarios, suffer from limited annotation quality, and\nconsider insufficient dimensions of evaluation. To address these issues, we\npresent XLRS-Bench: a comprehensive benchmark for evaluating the perception and\nreasoning capabilities of MLLMs in ultra-high-resolution RS scenarios.\nXLRS-Bench boasts the largest average image size (8500$\\times$8500) observed\nthus far, with all evaluation samples meticulously annotated manually, assisted\nby a novel semi-automatic captioner on ultra-high-resolution RS images. On top\nof the XLRS-Bench, 16 sub-tasks are defined to evaluate MLLMs' 10 kinds of\nperceptual capabilities and 6 kinds of reasoning capabilities, with a primary\nemphasis on advanced cognitive processes that facilitate real-world\ndecision-making and the capture of spatiotemporal changes. The results of both\ngeneral and RS-focused MLLMs on XLRS-Bench indicate that further efforts are\nneeded for real-world RS applications. We have open-sourced XLRS-Bench to\nsupport further research in developing more powerful MLLMs for remote sensing."}
{"id": "2503.23775", "pdf": "https://arxiv.org/pdf/2503.23775", "abs": "https://arxiv.org/abs/2503.23775", "authors": ["Lucas Heublein", "Nisha L. Raichur", "Tobias Feigl", "Tobias Brieger", "Fin Heuer", "Lennart Asbach", "Alexander Rügamer", "Felix Ott"], "title": "Evaluation of (Un-)Supervised Machine Learning Methods for GNSS Interference Classification with Real-World Data Discrepancies", "categories": ["cs.CV", "cs.LG", "68-00, 68T01, 68T30", "E.0; E.2; H.4; I.5"], "comment": "34 pages, 25 figures", "summary": "The accuracy and reliability of vehicle localization on roads are crucial for\napplications such as self-driving cars, toll systems, and digital tachographs.\nTo achieve accurate positioning, vehicles typically use global navigation\nsatellite system (GNSS) receivers to validate their absolute positions.\nHowever, GNSS-based positioning can be compromised by interference signals,\nnecessitating the identification, classification, determination of purpose, and\nlocalization of such interference to mitigate or eliminate it. Recent\napproaches based on machine learning (ML) have shown superior performance in\nmonitoring interference. However, their feasibility in real-world applications\nand environments has yet to be assessed. Effective implementation of ML\ntechniques requires training datasets that incorporate realistic interference\nsignals, including real-world noise and potential multipath effects that may\noccur between transmitter, receiver, and satellite in the operational area.\nAdditionally, these datasets require reference labels. Creating such datasets\nis often challenging due to legal restrictions, as causing interference to GNSS\nsources is strictly prohibited. Consequently, the performance of ML-based\nmethods in practical applications remains unclear. To address this gap, we\ndescribe a series of large-scale measurement campaigns conducted in real-world\nsettings at two highway locations in Germany and the Seetal Alps in Austria,\nand in large-scale controlled indoor environments. We evaluate the latest\nsupervised ML-based methods to report on their performance in real-world\nsettings and present the applicability of pseudo-labeling for unsupervised\nlearning. We demonstrate the challenges of combining datasets due to data\ndiscrepancies and evaluate outlier detection, domain adaptation, and data\naugmentation techniques to present the models' capabilities to adapt to changes\nin the datasets."}
{"id": "2503.23786", "pdf": "https://arxiv.org/pdf/2503.23786", "abs": "https://arxiv.org/abs/2503.23786", "authors": ["Haoran Shen", "Peixian Zhuang", "Jiahao Kou", "Yuxin Zeng", "Haoying Xu", "Jiangyun Li"], "title": "MGD-SAM2: Multi-view Guided Detail-enhanced Segment Anything Model 2 for High-Resolution Class-agnostic Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Segment Anything Models (SAMs), as vision foundation models, have\ndemonstrated remarkable performance across various image analysis tasks.\nDespite their strong generalization capabilities, SAMs encounter challenges in\nfine-grained detail segmentation for high-resolution class-independent\nsegmentation (HRCS), due to the limitations in the direct processing of\nhigh-resolution inputs and low-resolution mask predictions, and the reliance on\naccurate manual prompts. To address these limitations, we propose MGD-SAM2\nwhich integrates SAM2 with multi-view feature interaction between a global\nimage and local patches to achieve precise segmentation. MGD-SAM2 incorporates\nthe pre-trained SAM2 with four novel modules: the Multi-view Perception Adapter\n(MPAdapter), the Multi-view Complementary Enhancement Module (MCEM), the\nHierarchical Multi-view Interaction Module (HMIM), and the Detail Refinement\nModule (DRM). Specifically, we first introduce MPAdapter to adapt the SAM2\nencoder for enhanced extraction of local details and global semantics in HRCS\nimages. Then, MCEM and HMIM are proposed to further exploit local texture and\nglobal context by aggregating multi-view features within and across\nmulti-scales. Finally, DRM is designed to generate gradually restored\nhigh-resolution mask predictions, compensating for the loss of fine-grained\ndetails resulting from directly upsampling the low-resolution prediction maps.\nExperimental results demonstrate the superior performance and strong\ngeneralization of our model on multiple high-resolution and normal-resolution\ndatasets. Code will be available at https://github.com/sevenshr/MGD-SAM2."}
{"id": "2503.23793", "pdf": "https://arxiv.org/pdf/2503.23793", "abs": "https://arxiv.org/abs/2503.23793", "authors": ["Zhongnan Cai", "Yingying Wang", "Yunlong Lin", "Hui Zheng", "Ge Meng", "Zixu Lin", "Jiaxin Xie", "Junbin Lu", "Yue Huang", "Xinghao Ding"], "title": "Pan-LUT: Efficient Pan-sharpening via Learnable Look-Up Tables", "categories": ["cs.CV"], "comment": "12 pages, 6 figures", "summary": "Recently, deep learning-based pan-sharpening algorithms have achieved notable\nadvancements over traditional methods. However, many deep learning-based\napproaches incur substantial computational overhead during inference,\nespecially with high-resolution images. This excessive computational demand\nlimits the applicability of these methods in real-world scenarios, particularly\nin the absence of dedicated computing devices such as GPUs and TPUs. To address\nthese challenges, we propose Pan-LUT, a novel learnable look-up table (LUT)\nframework for pan-sharpening that strikes a balance between performance and\ncomputational efficiency for high-resolution remote sensing images. To finely\ncontrol the spectral transformation, we devise the PAN-guided look-up table\n(PGLUT) for channel-wise spectral mapping. To effectively capture fine-grained\nspatial details and adaptively learn local contexts, we introduce the spatial\ndetails look-up table (SDLUT) and adaptive aggregation look-up table (AALUT).\nOur proposed method contains fewer than 300K parameters and processes a 8K\nresolution image in under 1 ms using a single NVIDIA GeForce RTX 2080 Ti GPU,\ndemonstrating significantly faster performance compared to other methods.\nExperiments reveal that Pan-LUT efficiently processes large remote sensing\nimages in a lightweight manner, bridging the gap to real-world applications.\nFurthermore, our model surpasses SOTA methods in full-resolution scenes under\nreal-world conditions, highlighting its effectiveness and efficiency."}
{"id": "2503.23796", "pdf": "https://arxiv.org/pdf/2503.23796", "abs": "https://arxiv.org/abs/2503.23796", "authors": ["Bosung Kim", "Kyuhwan Lee", "Isu Jeong", "Jungmin Cheon", "Yeojin Lee", "Seulki Lee"], "title": "On-device Sora: Enabling Training-Free Diffusion-based Text-to-Video Generation for Mobile Devices", "categories": ["cs.CV"], "comment": null, "summary": "We present On-device Sora, the first model training-free solution for\ndiffusion-based on-device text-to-video generation that operates efficiently on\nsmartphone-grade devices. To address the challenges of diffusion-based\ntext-to-video generation on computation- and memory-limited mobile devices, the\nproposed On-device Sora applies three novel techniques to pre-trained video\ngenerative models. First, Linear Proportional Leap (LPL) reduces the excessive\ndenoising steps required in video diffusion through an efficient leap-based\napproach. Second, Temporal Dimension Token Merging (TDTM) minimizes intensive\ntoken-processing computation in attention layers by merging consecutive tokens\nalong the temporal dimension. Third, Concurrent Inference with Dynamic Loading\n(CI-DL) dynamically partitions large models into smaller blocks and loads them\ninto memory for concurrent model inference, effectively addressing the\nchallenges of limited device memory. We implement On-device Sora on the iPhone\n15 Pro, and the experimental evaluations show that it is capable of generating\nhigh-quality videos on the device, comparable to those produced by high-end\nGPUs. These results show that On-device Sora enables efficient and high-quality\nvideo generation on resource-constrained mobile devices. We envision the\nproposed On-device Sora as a significant first step toward democratizing\nstate-of-the-art generative technologies, enabling video generation on\ncommodity mobile and embedded devices without resource-intensive re-training\nfor model optimization (compression). The code implementation is available at a\nGitHub repository(https://github.com/eai-lab/On-device-Sora)."}
{"id": "2503.23806", "pdf": "https://arxiv.org/pdf/2503.23806", "abs": "https://arxiv.org/abs/2503.23806", "authors": ["Xiaoqing Guo", "Wuyang Li", "Yixuan Yuan"], "title": "Bridge the Gap Between Visual and Linguistic Comprehension for Generalized Zero-shot Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Generalized zero-shot semantic segmentation (GZS3) aims to achieve the\nhuman-level capability of segmenting not only seen classes but also novel class\nregions unseen in the training data through introducing the bridge of semantic\nrepresentations, e.g., word vector. While effective, the way of utilizing one\nsemantic representation to associate the corresponding class and to enable the\nknowledge transfer from seen to unseen classes is insufficient as well as\nincompatible with human cognition. Inspired by the observation that humans\noften use some `part' and `state' information to comprehend the seen objects\nand imagine unseen classes, we decouple each class into detailed descriptions,\nincluding object parts and states. Based on the decoupling formulation, we\npropose a Decoupled Vision-Language Matching (DeVLMatch) framework, composed of\nspatial-part (SPMatch) and channel-state (CSMatch) matching modules, for GZS3.\nIn SPMatch, we comprehend objects with spatial part information from both\nvisual and linguistic perspectives and perform graph matching to bridge the\ngap. In CSMatch, states of objects from the linguistic perspective are matched\nto compatible channel information from the visual perspective. By decoupling\nand matching objects across visual and linguistic comprehension, we can\nexplicitly introspect the relationship between seen and unseen classes in\nfine-grained object part and state levels, thereby facilitating the knowledge\ntransfer from seen to unseen classes in visual space. The proposed DeVLMatch\nframework surpasses the previous GZS3 methods on standard benchmarks, including\nPASCAL VOC, COCO-Stuff, and CATARACTS, demonstrating its effectiveness."}
{"id": "2503.23844", "pdf": "https://arxiv.org/pdf/2503.23844", "abs": "https://arxiv.org/abs/2503.23844", "authors": ["Xuyang Li", "Chenyu Li", "Pedram Ghamisi", "Danfeng Hong"], "title": "FlexiMo: A Flexible Remote Sensing Foundation Model", "categories": ["cs.CV"], "comment": null, "summary": "The rapid expansion of multi-source satellite imagery drives innovation in\nEarth observation, opening unprecedented opportunities for Remote Sensing\nFoundation Models to harness diverse data. However, many existing models remain\nconstrained by fixed spatial resolutions and patch sizes, limiting their\nability to fully exploit the heterogeneous spatial characteristics inherent in\nsatellite imagery. To address these challenges, we propose FlexiMo, a flexible\nremote sensing foundation model that endows the pre-trained model with the\nflexibility to adapt to arbitrary spatial resolutions. Central to FlexiMo is a\nspatial resolution-aware module that employs a parameter-free alignment\nembedding mechanism to dynamically recalibrate patch embeddings based on the\ninput image's resolution and dimensions. This design not only preserves\ncritical token characteristics and ensures multi-scale feature fidelity but\nalso enables efficient feature extraction without requiring modifications to\nthe underlying network architecture. In addition, FlexiMo incorporates a\nlightweight channel adaptation module that leverages prior spectral information\nfrom sensors. This mechanism allows the model to process images with varying\nnumbers of channels while maintaining the data's intrinsic physical properties.\nExtensive experiments on diverse multimodal, multi-resolution, and multi-scale\ndatasets demonstrate that FlexiMo significantly enhances model generalization\nand robustness. In particular, our method achieves outstanding performance\nacross a range of downstream tasks, including scene classification, land cover\nclassification, urban building segmentation, and cloud detection. By enabling\nparameter-efficient and physically consistent adaptation, FlexiMo paves the way\nfor more adaptable and effective foundation models in real-world remote sensing\napplications."}
{"id": "2503.23862", "pdf": "https://arxiv.org/pdf/2503.23862", "abs": "https://arxiv.org/abs/2503.23862", "authors": ["SeonYeong Lee", "EonSeung Seong", "DongEon Lee", "SiYeoul Lee", "Yubin Cho", "Chunsu Park", "Seonho Kim", "MinKyoung Seo", "YoungSin Ko", "MinWoo Kim"], "title": "Learned Image Compression and Restoration for Digital Pathology", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Digital pathology images play a crucial role in medical diagnostics, but\ntheir ultra-high resolution and large file sizes pose significant challenges\nfor storage, transmission, and real-time visualization. To address these\nissues, we propose CLERIC, a novel deep learning-based image compression\nframework designed specifically for whole slide images (WSIs). CLERIC\nintegrates a learnable lifting scheme and advanced convolutional techniques to\nenhance compression efficiency while preserving critical pathological details.\nOur framework employs a lifting-scheme transform in the analysis stage to\ndecompose images into low- and high-frequency components, enabling more\nstructured latent representations. These components are processed through\nparallel encoders incorporating Deformable Residual Blocks (DRB) and Recurrent\nResidual Blocks (R2B) to improve feature extraction and spatial adaptability.\nThe synthesis stage applies an inverse lifting transform for effective image\nreconstruction, ensuring high-fidelity restoration of fine-grained tissue\nstructures. We evaluate CLERIC on a digital pathology image dataset and compare\nits performance against state-of-the-art learned image compression (LIC)\nmodels. Experimental results demonstrate that CLERIC achieves superior\nrate-distortion (RD) performance, significantly reducing storage requirements\nwhile maintaining high diagnostic image quality. Our study highlights the\npotential of deep learning-based compression in digital pathology, facilitating\nefficient data management and long-term storage while ensuring seamless\nintegration into clinical workflows and AI-assisted diagnostic systems. Code\nand models are available at: https://github.com/pnu-amilab/CLERIC."}
{"id": "2503.23881", "pdf": "https://arxiv.org/pdf/2503.23881", "abs": "https://arxiv.org/abs/2503.23881", "authors": ["Tianyi Gong", "Boyan Li", "Yifei Zhong", "Fangxin Wang"], "title": "ExScene: Free-View 3D Scene Reconstruction with Gaussian Splatting from a Single Image", "categories": ["cs.CV"], "comment": "ICME 2025", "summary": "The increasing demand for augmented and virtual reality applications has\nhighlighted the importance of crafting immersive 3D scenes from a simple\nsingle-view image. However, due to the partial priors provided by single-view\ninput, existing methods are often limited to reconstruct low-consistency 3D\nscenes with narrow fields of view from single-view input. These limitations\nmake them less capable of generalizing to reconstruct immersive scenes. To\naddress this problem, we propose ExScene, a two-stage pipeline to reconstruct\nan immersive 3D scene from any given single-view image. ExScene designs a novel\nmultimodal diffusion model to generate a high-fidelity and globally consistent\npanoramic image. We then develop a panoramic depth estimation approach to\ncalculate geometric information from panorama, and we combine geometric\ninformation with high-fidelity panoramic image to train an initial 3D Gaussian\nSplatting (3DGS) model. Following this, we introduce a GS refinement technique\nwith 2D stable video diffusion priors. We add camera trajectory consistency and\ncolor-geometric priors into the denoising process of diffusion to improve color\nand spatial consistency across image sequences. These refined sequences are\nthen used to fine-tune the initial 3DGS model, leading to better reconstruction\nquality. Experimental results demonstrate that our ExScene achieves consistent\nand immersive scene reconstruction using only single-view input, significantly\nsurpassing state-of-the-art baselines."}
{"id": "2503.23882", "pdf": "https://arxiv.org/pdf/2503.23882", "abs": "https://arxiv.org/abs/2503.23882", "authors": ["Halil İbrahim Öztürk", "Muhammet Esat Kalfaoğlu", "Ozsel Kilinc"], "title": "GLane3D : Detecting Lanes with Graph of 3D Keypoints", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Accurate and efficient lane detection in 3D space is essential for autonomous\ndriving systems, where robust generalization is the foremost requirement for 3D\nlane detection algorithms. Considering the extensive variation in lane\nstructures worldwide, achieving high generalization capacity is particularly\nchallenging, as algorithms must accurately identify a wide variety of lane\npatterns worldwide. Traditional top-down approaches rely heavily on learning\nlane characteristics from training datasets, often struggling with lanes\nexhibiting previously unseen attributes. To address this generalization\nlimitation, we propose a method that detects keypoints of lanes and\nsubsequently predicts sequential connections between them to construct complete\n3D lanes. Each key point is essential for maintaining lane continuity, and we\npredict multiple proposals per keypoint by allowing adjacent grids to predict\nthe same keypoint using an offset mechanism. PointNMS is employed to eliminate\noverlapping proposal keypoints, reducing redundancy in the estimated BEV graph\nand minimizing computational overhead from connection estimations. Our model\nsurpasses previous state-of-the-art methods on both the Apollo and OpenLane\ndatasets, demonstrating superior F1 scores and a strong generalization capacity\nwhen models trained on OpenLane are evaluated on the Apollo dataset, compared\nto prior approaches."}
{"id": "2503.23888", "pdf": "https://arxiv.org/pdf/2503.23888", "abs": "https://arxiv.org/abs/2503.23888", "authors": ["Xin Zhang", "Siting Huang", "Xiangyang Luo", "Yifan Xie", "Weijiang Yu", "Heng Chang", "Fei Ma", "Fei Yu"], "title": "MuseFace: Text-driven Face Editing via Diffusion-based Mask Generation Approach", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages, 5 figures,IEEE International Conference on Multimedia & Expo\n  2025", "summary": "Face editing modifies the appearance of face, which plays a key role in\ncustomization and enhancement of personal images. Although much work have\nachieved remarkable success in text-driven face editing, they still face\nsignificant challenges as none of them simultaneously fulfill the\ncharacteristics of diversity, controllability and flexibility. To address this\nchallenge, we propose MuseFace, a text-driven face editing framework, which\nrelies solely on text prompt to enable face editing. Specifically, MuseFace\nintegrates a Text-to-Mask diffusion model and a semantic-aware face editing\nmodel, capable of directly generating fine-grained semantic masks from text and\nperforming face editing. The Text-to-Mask diffusion model provides\n\\textit{diversity} and \\textit{flexibility} to the framework, while the\nsemantic-aware face editing model ensures \\textit{controllability} of the\nframework. Our framework can create fine-grained semantic masks, making precise\nface editing possible, and significantly enhancing the controllability and\nflexibility of face editing models. Extensive experiments demonstrate that\nMuseFace achieves superior high-fidelity performance."}
{"id": "2503.23897", "pdf": "https://arxiv.org/pdf/2503.23897", "abs": "https://arxiv.org/abs/2503.23897", "authors": ["Yufei Wang", "Lanqing Guo", "Zhihao Li", "Jiaxing Huang", "Pichao Wang", "Bihan Wen", "Jian Wang"], "title": "Training-Free Text-Guided Image Editing with Visual Autoregressive Model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-guided image editing is an essential task that enables users to modify\nimages through natural language descriptions. Recent advances in diffusion\nmodels and rectified flows have significantly improved editing quality,\nprimarily relying on inversion techniques to extract structured noise from\ninput images. However, inaccuracies in inversion can propagate errors, leading\nto unintended modifications and compromising fidelity. Moreover, even with\nperfect inversion, the entanglement between textual prompts and image features\noften results in global changes when only local edits are intended. To address\nthese challenges, we propose a novel text-guided image editing framework based\non VAR (Visual AutoRegressive modeling), which eliminates the need for explicit\ninversion while ensuring precise and controlled modifications. Our method\nintroduces a caching mechanism that stores token indices and probability\ndistributions from the original image, capturing the relationship between the\nsource prompt and the image. Using this cache, we design an adaptive\nfine-grained masking strategy that dynamically identifies and constrains\nmodifications to relevant regions, preventing unintended changes. A token\nreassembling approach further refines the editing process, enhancing diversity,\nfidelity, and control. Our framework operates in a training-free manner and\nachieves high-fidelity editing with faster inference speeds, processing a 1K\nresolution image in as fast as 1.2 seconds. Extensive experiments demonstrate\nthat our method achieves performance comparable to, or even surpassing,\nexisting diffusion- and rectified flow-based approaches in both quantitative\nmetrics and visual quality. The code will be released."}
{"id": "2503.23905", "pdf": "https://arxiv.org/pdf/2503.23905", "abs": "https://arxiv.org/abs/2503.23905", "authors": ["Qihan Huang", "Long Chan", "Jinlong Liu", "Wanggui He", "Hao Jiang", "Mingli Song", "Jingyuan Chen", "Chang Yao", "Jie Song"], "title": "Boosting MLLM Reasoning with Text-Debiased Hint-GRPO", "categories": ["cs.CV"], "comment": null, "summary": "MLLM reasoning has drawn widespread research for its excellent\nproblem-solving capability. Current reasoning methods fall into two types: PRM,\nwhich supervises the intermediate reasoning steps, and ORM, which supervises\nthe final results. Recently, DeepSeek-R1 has challenged the traditional view\nthat PRM outperforms ORM, which demonstrates strong generalization performance\nusing an ORM method (i.e., GRPO). However, current MLLM's GRPO algorithms still\nstruggle to handle challenging and complex multimodal reasoning tasks (e.g.,\nmathematical reasoning). In this work, we reveal two problems that impede the\nperformance of GRPO on the MLLM: Low data utilization and Text-bias. Low data\nutilization refers to that GRPO cannot acquire positive rewards to update the\nMLLM on difficult samples, and text-bias is a phenomenon that the MLLM bypasses\nimage condition and solely relies on text condition for generation after GRPO\ntraining. To tackle these problems, this work proposes Hint-GRPO that improves\ndata utilization by adaptively providing hints for samples of varying\ndifficulty, and text-bias calibration that mitigates text-bias by calibrating\nthe token prediction logits with image condition in test-time. Experiment\nresults on three base MLLMs across eleven datasets demonstrate that our\nproposed methods advance the reasoning capability of original MLLM by a large\nmargin, exhibiting superior performance to existing MLLM reasoning methods. Our\ncode is available at https://github.com/hqhQAQ/Hint-GRPO."}
{"id": "2503.23907", "pdf": "https://arxiv.org/pdf/2503.23907", "abs": "https://arxiv.org/abs/2503.23907", "authors": ["Zhichao Liao", "Xiaokun Liu", "Wenyu Qin", "Qingyu Li", "Qiulin Wang", "Pengfei Wan", "Di Zhang", "Long Zeng", "Pingfa Feng"], "title": "HumanAesExpert: Advancing a Multi-Modality Foundation Model for Human Image Aesthetic Assessment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Image Aesthetic Assessment (IAA) is a long-standing and challenging research\ntask. However, its subset, Human Image Aesthetic Assessment (HIAA), has been\nscarcely explored, even though HIAA is widely used in social media, AI\nworkflows, and related domains. To bridge this research gap, our work pioneers\na holistic implementation framework tailored for HIAA. Specifically, we\nintroduce HumanBeauty, the first dataset purpose-built for HIAA, which\ncomprises 108k high-quality human images with manual annotations. To achieve\ncomprehensive and fine-grained HIAA, 50K human images are manually collected\nthrough a rigorous curation process and annotated leveraging our trailblazing\n12-dimensional aesthetic standard, while the remaining 58K with overall\naesthetic labels are systematically filtered from public datasets. Based on the\nHumanBeauty database, we propose HumanAesExpert, a powerful Vision Language\nModel for aesthetic evaluation of human images. We innovatively design an\nExpert head to incorporate human knowledge of aesthetic sub-dimensions while\njointly utilizing the Language Modeling (LM) and Regression head. This approach\nempowers our model to achieve superior proficiency in both overall and\nfine-grained HIAA. Furthermore, we introduce a MetaVoter, which aggregates\nscores from all three heads, to effectively balance the capabilities of each\nhead, thereby realizing improved assessment precision. Extensive experiments\ndemonstrate that our HumanAesExpert models deliver significantly better\nperformance in HIAA than other state-of-the-art models. Our datasets, models,\nand codes are publicly released to advance the HIAA community. Project webpage:\nhttps://humanaesexpert.github.io/HumanAesExpert/"}
{"id": "2503.23911", "pdf": "https://arxiv.org/pdf/2503.23911", "abs": "https://arxiv.org/abs/2503.23911", "authors": ["Ruisheng Han", "Kanglei Zhou", "Amir Atapour-Abarghouei", "Xiaohui Liang", "Hubert P. H. Shum"], "title": "FineCausal: A Causal-Based Framework for Interpretable Fine-Grained Action Quality Assessment", "categories": ["cs.CV"], "comment": null, "summary": "Action quality assessment (AQA) is critical for evaluating athletic\nperformance, informing training strategies, and ensuring safety in competitive\nsports. However, existing deep learning approaches often operate as black boxes\nand are vulnerable to spurious correlations, limiting both their reliability\nand interpretability. In this paper, we introduce FineCausal, a novel\ncausal-based framework that achieves state-of-the-art performance on the\nFineDiving-HM dataset. Our approach leverages a Graph Attention Network-based\ncausal intervention module to disentangle human-centric foreground cues from\nbackground confounders, and incorporates a temporal causal attention module to\ncapture fine-grained temporal dependencies across action stages. This\ndual-module strategy enables FineCausal to generate detailed spatio-temporal\nrepresentations that not only achieve state-of-the-art scoring performance but\nalso provide transparent, interpretable feedback on which features drive the\nassessment. Despite its strong performance, FineCausal requires extensive\nexpert knowledge to define causal structures and depends on high-quality\nannotations, challenges that we discuss and address as future research\ndirections. Code is available at https://github.com/Harrison21/FineCausal."}
{"id": "2503.23925", "pdf": "https://arxiv.org/pdf/2503.23925", "abs": "https://arxiv.org/abs/2503.23925", "authors": ["Zizhuo Li", "Yifan Lu", "Linfeng Tang", "Shihua Zhang", "Jiayi Ma"], "title": "CoMatch: Dynamic Covisibility-Aware Transformer for Bilateral Subpixel-Level Semi-Dense Image Matching", "categories": ["cs.CV"], "comment": null, "summary": "This prospective study proposes CoMatch, a novel semi-dense image matcher\nwith dynamic covisibility awareness and bilateral subpixel accuracy. Firstly,\nobserving that modeling context interaction over the entire coarse feature map\nelicits highly redundant computation due to the neighboring representation\nsimilarity of tokens, a covisibility-guided token condenser is introduced to\nadaptively aggregate tokens in light of their covisibility scores that are\ndynamically estimated, thereby ensuring computational efficiency while\nimproving the representational capacity of aggregated tokens simultaneously.\nSecondly, considering that feature interaction with massive non-covisible areas\nis distracting, which may degrade feature distinctiveness, a\ncovisibility-assisted attention mechanism is deployed to selectively suppress\nirrelevant message broadcast from non-covisible reduced tokens, resulting in\nrobust and compact attention to relevant rather than all ones. Thirdly, we find\nthat at the fine-level stage, current methods adjust only the target view's\nkeypoints to subpixel level, while those in the source view remain restricted\nat the coarse level and thus not informative enough, detrimental to keypoint\nlocation-sensitive usages. A simple yet potent fine correlation module is\ndeveloped to refine the matching candidates in both source and target views to\nsubpixel level, attaining attractive performance improvement. Thorough\nexperimentation across an array of public benchmarks affirms CoMatch's\npromising accuracy, efficiency, and generalizability."}
{"id": "2503.23930", "pdf": "https://arxiv.org/pdf/2503.23930", "abs": "https://arxiv.org/abs/2503.23930", "authors": ["Jiankai Tang", "Jiacheng Liu", "Renling Tong", "Kai Zhu", "Zhe Li", "Xin Yi", "Junliang Xing", "Yuanchun Shi", "Yuntao Wang"], "title": "Exploring Reliable PPG Authentication on Smartwatches in Daily Scenarios", "categories": ["cs.CV"], "comment": null, "summary": "Photoplethysmography (PPG) Sensors, widely deployed in smartwatches, offer a\nsimple and non-invasive authentication approach for daily use. However, PPG\nauthentication faces reliability issues due to motion artifacts from physical\nactivity and physiological variability over time. To address these challenges,\nwe propose MTL-RAPID, an efficient and reliable PPG authentication model, that\nemploys a multitask joint training strategy, simultaneously assessing signal\nquality and verifying user identity. The joint optimization of these two tasks\nin MTL-RAPID results in a structure that outperforms models trained on\nindividual tasks separately, achieving stronger performance with fewer\nparameters. In our comprehensive user studies regarding motion artifacts (N =\n30), time variations (N = 32), and user preferences (N = 16), MTL-RAPID\nachieves a best AUC of 99.2\\% and an EER of 3.5\\%, outperforming existing\nbaselines. We opensource our PPG authentication dataset along with the\nMTL-RAPID model to facilitate future research on GitHub."}
{"id": "2503.23947", "pdf": "https://arxiv.org/pdf/2503.23947", "abs": "https://arxiv.org/abs/2503.23947", "authors": ["Guhnoo Yun", "Juhan Yoo", "Kijung Kim", "Jeongho Lee", "Paul Hongsuck Seo", "Dong Hwan Kim"], "title": "Spectral-Adaptive Modulation Networks for Visual Perception", "categories": ["cs.CV"], "comment": null, "summary": "Recent studies have shown that 2D convolution and self-attention exhibit\ndistinct spectral behaviors, and optimizing their spectral properties can\nenhance vision model performance. However, theoretical analyses remain limited\nin explaining why 2D convolution is more effective in high-pass filtering than\nself-attention and why larger kernels favor shape bias, akin to self-attention.\nIn this paper, we employ graph spectral analysis to theoretically simulate and\ncompare the frequency responses of 2D convolution and self-attention within a\nunified framework. Our results corroborate previous empirical findings and\nreveal that node connectivity, modulated by window size, is a key factor in\nshaping spectral functions. Leveraging this insight, we introduce a\n\\textit{spectral-adaptive modulation} (SPAM) mixer, which processes visual\nfeatures in a spectral-adaptive manner using multi-scale convolutional kernels\nand a spectral re-scaling mechanism to refine spectral components. Based on\nSPAM, we develop SPANetV2 as a novel vision backbone. Extensive experiments\ndemonstrate that SPANetV2 outperforms state-of-the-art models across multiple\nvision tasks, including ImageNet-1K classification, COCO object detection, and\nADE20K semantic segmentation."}
{"id": "2503.23951", "pdf": "https://arxiv.org/pdf/2503.23951", "abs": "https://arxiv.org/abs/2503.23951", "authors": ["Fangda Chen", "Shanshan Zhao", "Chuanfu Xu", "Long Lan"], "title": "JointTuner: Appearance-Motion Adaptive Joint Training for Customized Video Generation", "categories": ["cs.CV"], "comment": "Project Page: https://fdchen24.github.io/JointTuner-Website", "summary": "Recent text-to-video advancements have enabled coherent video synthesis from\nprompts and expanded to fine-grained control over appearance and motion.\nHowever, existing methods either suffer from concept interference due to\nfeature domain mismatch caused by naive decoupled optimizations or exhibit\nappearance contamination induced by spatial feature leakage resulting from the\nentanglement of motion and appearance in reference video reconstructions. In\nthis paper, we propose JointTuner, a novel adaptive joint training framework,\nto alleviate these issues. Specifically, we develop Adaptive LoRA, which\nincorporates a context-aware gating mechanism, and integrate the gated LoRA\ncomponents into the spatial and temporal Transformers within the diffusion\nmodel. These components enable simultaneous optimization of appearance and\nmotion, eliminating concept interference. In addition, we introduce the\nAppearance-independent Temporal Loss, which decouples motion patterns from\nintrinsic appearance in reference video reconstructions through an\nappearance-agnostic noise prediction task. The key innovation lies in adding\nframe-wise offset noise to the ground-truth Gaussian noise, perturbing its\ndistribution, thereby disrupting spatial attributes associated with frames\nwhile preserving temporal coherence. Furthermore, we construct a benchmark\ncomprising 90 appearance-motion customized combinations and 10 multi-type\nautomatic metrics across four dimensions, facilitating a more comprehensive\nevaluation for this customization task. Extensive experiments demonstrate the\nsuperior performance of our method compared to current advanced approaches."}
{"id": "2503.23956", "pdf": "https://arxiv.org/pdf/2503.23956", "abs": "https://arxiv.org/abs/2503.23956", "authors": ["Kai Huang", "Hao Zou", "Bochen Wang", "Ye Xi", "Zhen Xie", "Hao Wang"], "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for Efficient Large Vision-Language Model Inference", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches."}
{"id": "2503.23958", "pdf": "https://arxiv.org/pdf/2503.23958", "abs": "https://arxiv.org/abs/2503.23958", "authors": ["Nima Torbati", "Anastasia Meshcheryakova", "Diana Mechtcheriakova", "Amirreza Mahbod"], "title": "A Multi-Stage Auto-Context Deep Learning Framework for Tissue and Nuclei Segmentation and Classification in H&E-Stained Histological Images of Advanced Melanoma", "categories": ["cs.CV"], "comment": "15 pages", "summary": "Melanoma is the most lethal form of skin cancer, with an increasing incidence\nrate worldwide. Analyzing histological images of melanoma by localizing and\nclassifying tissues and cell nuclei is considered the gold standard method for\ndiagnosis and treatment options for patients. While many computerized\napproaches have been proposed for automatic analysis, most perform tissue-based\nanalysis and nuclei (cell)-based analysis as separate tasks, which might be\nsuboptimal.\n  In this work, using the PUMA challenge dataset, we proposed a novel\nmulti-stage deep learning approach by combining tissue and nuclei information\nin a unified framework based on the auto-context concept to perform\nsegmentation and classification in histological images of melanoma. Through\npre-training and further post-processing, our approach achieved second and\nfirst place rankings in the PUMA challenge, with average micro Dice tissue\nscore and summed nuclei F1-score of 73.40% for Track 1 and 63.48% for Track 2,\nrespectively. Our implementation for training and testing is available at:\nhttps://github.com/NimaTorbati/PumaSubmit"}
{"id": "2503.23959", "pdf": "https://arxiv.org/pdf/2503.23959", "abs": "https://arxiv.org/abs/2503.23959", "authors": ["Bizhe Bai", "Jianjian Cao", "Yadan Luo", "Tao Che"], "title": "Local Information Matters: Inference Acceleration For Grounded Conversation Generation Models Through Adaptive Local-Aware Token Pruning", "categories": ["cs.CV"], "comment": "Work in progress", "summary": "Grounded Conversation Generation (GCG) is an emerging vision-language task\nthat requires models to generate natural language responses seamlessly\nintertwined with corresponding object segmentation masks. Recent models, such\nas GLaMM and OMG-LLaVA, achieve pixel-level grounding but incur significant\ncomputational costs due to processing a large number of visual tokens. Existing\ntoken pruning methods, like FastV and PyramidDrop, fail to preserve the local\nvisual features critical for accurate grounding, leading to substantial\nperformance drops in GCG tasks. To address this, we propose Adaptive\nLocal-Aware Token Pruning (ALTP), a simple yet effective framework that\naccelerates GCG models by prioritizing local object information. ALTP\nintroduces two key components: (1) Detail Density Capture (DDC), which uses\nsuperpixel segmentation to retain tokens in object-centric regions, preserving\nfine-grained details, and (2) Dynamic Density Formation (DDF), which\ndynamically allocates tokens based on information density, ensuring higher\nretention in semantically rich areas. Extensive experiments on the GranDf\ndataset demonstrate that ALTP significantly outperforms existing token pruning\nmethods, such as FastV and PyramidDrop, on both GLaMM and OMG-LLaVA models.\nNotably, when applied to GLaMM, ALTP achieves a 90% reduction in visual tokens\nwith a 4.9% improvement in AP50 and a 5.0% improvement in Recall compared to\nPyramidDrop. Similarly, on OMG-LLaVA, ALTP improves AP by 2.1% and mIOU by 3.0%\nat a 90% token reduction compared with PDrop."}
{"id": "2503.23963", "pdf": "https://arxiv.org/pdf/2503.23963", "abs": "https://arxiv.org/abs/2503.23963", "authors": ["Miao Fan", "Shanshan Yu", "Shengtong Xu", "Kun Jiang", "Haoyi Xiong", "Xiangzeng Liu"], "title": "A Benchmark for Vision-Centric HD Mapping by V2I Systems", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted by IEEE IV'25", "summary": "Autonomous driving faces safety challenges due to a lack of global\nperspective and the semantic information of vectorized high-definition (HD)\nmaps. Information from roadside cameras can greatly expand the map perception\nrange through vehicle-to-infrastructure (V2I) communications. However, there is\nstill no dataset from the real world available for the study on map\nvectorization onboard under the scenario of vehicle-infrastructure cooperation.\nTo prosper the research on online HD mapping for Vehicle-Infrastructure\nCooperative Autonomous Driving (VICAD), we release a real-world dataset, which\ncontains collaborative camera frames from both vehicles and roadside\ninfrastructures, and provides human annotations of HD map elements. We also\npresent an end-to-end neural framework (i.e., V2I-HD) leveraging vision-centric\nV2I systems to construct vectorized maps. To reduce computation costs and\nfurther deploy V2I-HD on autonomous vehicles, we introduce a directionally\ndecoupled self-attention mechanism to V2I-HD. Extensive experiments show that\nV2I-HD has superior performance in real-time inference speed, as tested by our\nreal-world dataset. Abundant qualitative results also demonstrate stable and\nrobust map construction quality with low cost in complex and various driving\nscenes. As a benchmark, both source codes and the dataset have been released at\nOneDrive for the purpose of further study."}
{"id": "2503.23965", "pdf": "https://arxiv.org/pdf/2503.23965", "abs": "https://arxiv.org/abs/2503.23965", "authors": ["Miao Fan", "Xuxu Kong", "Shengtong Xu", "Haoyi Xiong", "Xiangzeng Liu"], "title": "Video-based Traffic Light Recognition by Rockchip RV1126 for Autonomous Driving", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted by IEEE IV'25", "summary": "Real-time traffic light recognition is fundamental for autonomous driving\nsafety and navigation in urban environments. While existing approaches rely on\nsingle-frame analysis from onboard cameras, they struggle with complex\nscenarios involving occlusions and adverse lighting conditions. We present\n\\textit{ViTLR}, a novel video-based end-to-end neural network that processes\nmultiple consecutive frames to achieve robust traffic light detection and state\nclassification. The architecture leverages a transformer-like design with\nconvolutional self-attention modules, which is optimized specifically for\ndeployment on the Rockchip RV1126 embedded platform. Extensive evaluations on\ntwo real-world datasets demonstrate that \\textit{ViTLR} achieves\nstate-of-the-art performance while maintaining real-time processing\ncapabilities (>25 FPS) on RV1126's NPU. The system shows superior robustness\nacross temporal stability, varying target distances, and challenging\nenvironmental conditions compared to existing single-frame approaches. We have\nsuccessfully integrated \\textit{ViTLR} into an ego-lane traffic light\nrecognition system using HD maps for autonomous driving applications. The\ncomplete implementation, including source code and datasets, is made publicly\navailable to facilitate further research in this domain."}
{"id": "2503.23980", "pdf": "https://arxiv.org/pdf/2503.23980", "abs": "https://arxiv.org/abs/2503.23980", "authors": ["Yanbo Wang", "Yongtao Chen", "Chuan Cao", "Tianchen Deng", "Wentao Zhao", "Jingchuan Wang", "Weidong Chen"], "title": "SALT: A Flexible Semi-Automatic Labeling Tool for General LiDAR Point Clouds with Cross-Scene Adaptability and 4D Consistency", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "We propose a flexible Semi-Automatic Labeling Tool (SALT) for general LiDAR\npoint clouds with cross-scene adaptability and 4D consistency. Unlike recent\napproaches that rely on camera distillation, SALT operates directly on raw\nLiDAR data, automatically generating pre-segmentation results. To achieve this,\nwe propose a novel zero-shot learning paradigm, termed data alignment, which\ntransforms LiDAR data into pseudo-images by aligning with the training\ndistribution of vision foundation models. Additionally, we design a\n4D-consistent prompting strategy and 4D non-maximum suppression module to\nenhance SAM2, ensuring high-quality, temporally consistent presegmentation.\nSALT surpasses the latest zero-shot methods by 18.4% PQ on SemanticKITTI and\nachieves nearly 40-50% of human annotator performance on our newly collected\nlow-resolution LiDAR data and on combined data from three LiDAR types,\nsignificantly boosting annotation efficiency. We anticipate that SALT's\nopen-sourcing will catalyze substantial expansion of current LiDAR datasets and\nlay the groundwork for the future development of LiDAR foundation models. Code\nis available at https://github.com/Cavendish518/SALT."}
{"id": "2503.23993", "pdf": "https://arxiv.org/pdf/2503.23993", "abs": "https://arxiv.org/abs/2503.23993", "authors": ["Ming Yuan", "Sichao Wang", "Chuang Zhang", "Lei He", "Qing Xu", "Jianqiang Wang"], "title": "DenseFormer: Learning Dense Depth Map from Sparse Depth and Image via Conditional Diffusion Model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The depth completion task is a critical problem in autonomous driving,\ninvolving the generation of dense depth maps from sparse depth maps and RGB\nimages. Most existing methods employ a spatial propagation network to\niteratively refine the depth map after obtaining an initial dense depth. In\nthis paper, we propose DenseFormer, a novel method that integrates the\ndiffusion model into the depth completion task. By incorporating the denoising\nmechanism of the diffusion model, DenseFormer generates the dense depth map by\nprogressively refining an initial random depth distribution through multiple\niterations. We propose a feature extraction module that leverages a feature\npyramid structure, along with multi-layer deformable attention, to effectively\nextract and integrate features from sparse depth maps and RGB images, which\nserve as the guiding condition for the diffusion process. Additionally, this\npaper presents a depth refinement module that applies multi-step iterative\nrefinement across various ranges to the dense depth results generated by the\ndiffusion process. The module utilizes image features enriched with multi-scale\ninformation and sparse depth input to further enhance the accuracy of the\npredicted depth map. Extensive experiments on the KITTI outdoor scene dataset\ndemonstrate that DenseFormer outperforms classical depth completion methods."}
{"id": "2503.24008", "pdf": "https://arxiv.org/pdf/2503.24008", "abs": "https://arxiv.org/abs/2503.24008", "authors": ["Qi Wu", "Quanlong Zheng", "Yanhao Zhang", "Junlin Xie", "Jinguo Luo", "Kuo Wang", "Peng Liu", "Qingsong Xie", "Ru Zhen", "Haonan Lu", "Zhenyu Yang"], "title": "H2VU-Benchmark: A Comprehensive Benchmark for Hierarchical Holistic Video Understanding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the rapid development of multimodal models, the demand for assessing\nvideo understanding capabilities has been steadily increasing. However,\nexisting benchmarks for evaluating video understanding exhibit significant\nlimitations in coverage, task diversity, and scene adaptability. These\nshortcomings hinder the accurate assessment of models' comprehensive video\nunderstanding capabilities. To tackle this challenge, we propose a hierarchical\nand holistic video understanding (H2VU) benchmark designed to evaluate both\ngeneral video and online streaming video comprehension. This benchmark\ncontributes three key features:\n  Extended video duration: Spanning videos from brief 3-second clips to\ncomprehensive 1.5-hour recordings, thereby bridging the temporal gaps found in\ncurrent benchmarks. Comprehensive assessment tasks: Beyond traditional\nperceptual and reasoning tasks, we have introduced modules for\ncountercommonsense comprehension and trajectory state tracking. These additions\ntest the models' deep understanding capabilities beyond mere prior knowledge.\nEnriched video data: To keep pace with the rapid evolution of current AI\nagents, we have expanded first-person streaming video datasets. This expansion\nallows for the exploration of multimodal models' performance in understanding\nstreaming videos from a first-person perspective. Extensive results from H2VU\nreveal that existing multimodal large language models (MLLMs) possess\nsubstantial potential for improvement in our newly proposed evaluation tasks.\nWe expect that H2VU will facilitate advancements in video understanding\nresearch by offering a comprehensive and in-depth analysis of MLLMs."}
{"id": "2503.24014", "pdf": "https://arxiv.org/pdf/2503.24014", "abs": "https://arxiv.org/abs/2503.24014", "authors": ["Minh David Thao Chan", "Ruoyu Zhao", "Yukuan Jia", "Ruiqing Mao", "Sheng Zhou"], "title": "Optimization of Layer Skipping and Frequency Scaling for Convolutional Neural Networks under Latency Constraint", "categories": ["cs.CV"], "comment": "12 pages, 6 figures, Accepted in Proc. Eur. Conf. Comput. Vis. (ECCV)\n  Workshops. Milan, Italy: Springer, September 2024", "summary": "The energy consumption of Convolutional Neural Networks (CNNs) is a critical\nfactor in deploying deep learning models on resource-limited equipment such as\nmobile devices and autonomous vehicles. We propose an approach involving\nProportional Layer Skipping (PLS) and Frequency Scaling (FS). Layer skipping\nreduces computational complexity by selectively bypassing network layers,\nwhereas frequency scaling adjusts the frequency of the processor to optimize\nenergy use under latency constraints. Experiments of PLS and FS on ResNet-152\nwith the CIFAR-10 dataset demonstrated significant reductions in computational\ndemands and energy consumption with minimal accuracy loss. This study offers\npractical solutions for improving real-time processing in resource-limited\nsettings and provides insights into balancing computational efficiency and\nmodel performance."}
{"id": "2503.24017", "pdf": "https://arxiv.org/pdf/2503.24017", "abs": "https://arxiv.org/abs/2503.24017", "authors": ["Chenqi Guo", "Mengshuo Rong", "Qianli Feng", "Rongfan Feng", "Yinglong Ma"], "title": "Crossmodal Knowledge Distillation with WordNet-Relaxed Text Embeddings for Robust Image Classification", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Crossmodal knowledge distillation (KD) aims to enhance a unimodal student\nusing a multimodal teacher model. In particular, when the teacher's modalities\ninclude the student's, additional complementary information can be exploited to\nimprove knowledge transfer. In supervised image classification, image datasets\ntypically include class labels that represent high-level concepts, suggesting a\nnatural avenue to incorporate textual cues for crossmodal KD. However, these\nlabels rarely capture the deeper semantic structures in real-world visuals and\ncan lead to label leakage if used directly as inputs, ultimately limiting KD\nperformance. To address these issues, we propose a multi-teacher crossmodal KD\nframework that integrates CLIP image embeddings with learnable WordNet-relaxed\ntext embeddings under a hierarchical loss. By avoiding direct use of exact\nclass names and instead using semantically richer WordNet expansions, we\nmitigate label leakage and introduce more diverse textual cues. Experiments\nshow that this strategy significantly boosts student performance, whereas noisy\nor overly precise text embeddings hinder distillation efficiency.\nInterpretability analyses confirm that WordNet-relaxed prompts encourage\nheavier reliance on visual features over textual shortcuts, while still\neffectively incorporating the newly introduced textual cues. Our method\nachieves state-of-the-art or second-best results on six public datasets,\ndemonstrating its effectiveness in advancing crossmodal KD."}
{"id": "2503.24026", "pdf": "https://arxiv.org/pdf/2503.24026", "abs": "https://arxiv.org/abs/2503.24026", "authors": ["Boyuan Wang", "Xiaofeng Wang", "Chaojun Ni", "Guosheng Zhao", "Zhiqin Yang", "Zheng Zhu", "Muyang Zhang", "Yukun Zhou", "Xinze Chen", "Guan Huang", "Lihong Liu", "Xingang Wang"], "title": "HumanDreamer: Generating Controllable Human-Motion Videos via Decoupled Generation", "categories": ["cs.CV"], "comment": "Project Page: https://humandreamer.github.io", "summary": "Human-motion video generation has been a challenging task, primarily due to\nthe difficulty inherent in learning human body movements. While some approaches\nhave attempted to drive human-centric video generation explicitly through pose\ncontrol, these methods typically rely on poses derived from existing videos,\nthereby lacking flexibility. To address this, we propose HumanDreamer, a\ndecoupled human video generation framework that first generates diverse poses\nfrom text prompts and then leverages these poses to generate human-motion\nvideos. Specifically, we propose MotionVid, the largest dataset for\nhuman-motion pose generation. Based on the dataset, we present MotionDiT, which\nis trained to generate structured human-motion poses from text prompts.\nBesides, a novel LAMA loss is introduced, which together contribute to a\nsignificant improvement in FID by 62.4%, along with respective enhancements in\nR-precision for top1, top2, and top3 by 41.8%, 26.3%, and 18.3%, thereby\nadvancing both the Text-to-Pose control accuracy and FID metrics. Our\nexperiments across various Pose-to-Video baselines demonstrate that the poses\ngenerated by our method can produce diverse and high-quality human-motion\nvideos. Furthermore, our model can facilitate other downstream tasks, such as\npose sequence prediction and 2D-3D motion lifting."}
{"id": "2503.24032", "pdf": "https://arxiv.org/pdf/2503.24032", "abs": "https://arxiv.org/abs/2503.24032", "authors": ["Yasashwini Sai Gowri P", "Karthik Seemakurthy", "Andrews Agyemang Opoku", "Sita Devi Bharatula"], "title": "BBoxCut: A Targeted Data Augmentation Technique for Enhancing Wheat Head Detection Under Occlusions", "categories": ["cs.CV"], "comment": null, "summary": "Wheat plays a critical role in global food security, making it one of the\nmost extensively studied crops. Accurate identification and measurement of key\ncharacteristics of wheat heads are essential for breeders to select varieties\nfor cross-breeding, with the goal of developing nutrient-dense, resilient, and\nsustainable cultivars. Traditionally, these measurements are performed\nmanually, which is both time-consuming and inefficient. Advances in digital\ntechnologies have paved the way for automating this process. However, field\nconditions pose significant challenges, such as occlusions of leaves,\noverlapping wheat heads, varying lighting conditions, and motion blur. In this\npaper, we propose a novel data augmentation technique, BBoxCut, which uses\nrandom localized masking to simulate occlusions caused by leaves and\nneighboring wheat heads. We evaluated our approach using three state-of-the-art\nobject detectors and observed mean average precision (mAP) gains of 2.76, 3.26,\nand 1.9 for Faster R-CNN, FCOS, and DETR, respectively. Our augmentation\ntechnique led to significant improvements both qualitatively and\nquantitatively. In particular, the improvements were particularly evident in\nscenarios involving occluded wheat heads, demonstrating the robustness of our\nmethod in challenging field conditions."}
{"id": "2503.24057", "pdf": "https://arxiv.org/pdf/2503.24057", "abs": "https://arxiv.org/abs/2503.24057", "authors": ["Xuxiong Liu", "Tengteng Dong", "Fei Wang", "Weijie Feng", "Xiao Sun"], "title": "AMMSM: Adaptive Motion Magnification and Sparse Mamba for Micro-Expression Recognition", "categories": ["cs.CV"], "comment": "Accepted by ICME 2025", "summary": "Micro-expressions are typically regarded as unconscious manifestations of a\nperson's genuine emotions. However, their short duration and subtle signals\npose significant challenges for downstream recognition. We propose a multi-task\nlearning framework named the Adaptive Motion Magnification and Sparse Mamba\n(AMMSM) to address this. This framework aims to enhance the accurate capture of\nmicro-expressions through self-supervised subtle motion magnification, while\nthe sparse spatial selection Mamba architecture combines sparse activation with\nthe advanced Visual Mamba model to model key motion regions and their valuable\nrepresentations more effectively. Additionally, we employ evolutionary search\nto optimize the magnification factor and the sparsity ratios of spatial\nselection, followed by fine-tuning to improve performance further. Extensive\nexperiments on two standard datasets demonstrate that the proposed AMMSM\nachieves state-of-the-art (SOTA) accuracy and robustness."}
{"id": "2503.24065", "pdf": "https://arxiv.org/pdf/2503.24065", "abs": "https://arxiv.org/abs/2503.24065", "authors": ["Siqi Zhang", "Yanyuan Qiao", "Qunbo Wang", "Zike Yan", "Qi Wu", "Zhihua Wei", "Jing Liu"], "title": "COSMO: Combination of Selective Memorization for Low-cost Vision-and-Language Navigation", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Vision-and-Language Navigation (VLN) tasks have gained prominence within\nartificial intelligence research due to their potential application in fields\nlike home assistants. Many contemporary VLN approaches, while based on\ntransformer architectures, have increasingly incorporated additional components\nsuch as external knowledge bases or map information to enhance performance.\nThese additions, while boosting performance, also lead to larger models and\nincreased computational costs. In this paper, to achieve both high performance\nand low computational costs, we propose a novel architecture with the\nCOmbination of Selective MemOrization (COSMO). Specifically, COSMO integrates\nstate-space modules and transformer modules, and incorporates two\nVLN-customized selective state space modules: the Round Selective Scan (RSS)\nand the Cross-modal Selective State Space Module (CS3). RSS facilitates\ncomprehensive inter-modal interactions within a single scan, while the CS3\nmodule adapts the selective state space module into a dual-stream architecture,\nthereby enhancing the acquisition of cross-modal interactions. Experimental\nvalidations on three mainstream VLN benchmarks, REVERIE, R2R, and R2R-CE, not\nonly demonstrate competitive navigation performance of our model but also show\na significant reduction in computational costs."}
{"id": "2503.24071", "pdf": "https://arxiv.org/pdf/2503.24071", "abs": "https://arxiv.org/abs/2503.24071", "authors": ["Teresa Dorszewski", "Lenka Tětková", "Robert Jenssen", "Lars Kai Hansen", "Kristoffer Knutsen Wickstrøm"], "title": "From Colors to Classes: Emergence of Concepts in Vision Transformers", "categories": ["cs.CV", "cs.LG"], "comment": "Preprint. Accepted at The 3rd World Conference on eXplainable\n  Artificial Intelligence", "summary": "Vision Transformers (ViTs) are increasingly utilized in various computer\nvision tasks due to their powerful representation capabilities. However, it\nremains understudied how ViTs process information layer by layer. Numerous\nstudies have shown that convolutional neural networks (CNNs) extract features\nof increasing complexity throughout their layers, which is crucial for tasks\nlike domain adaptation and transfer learning. ViTs, lacking the same inductive\nbiases as CNNs, can potentially learn global dependencies from the first layers\ndue to their attention mechanisms. Given the increasing importance of ViTs in\ncomputer vision, there is a need to improve the layer-wise understanding of\nViTs. In this work, we present a novel, layer-wise analysis of concepts encoded\nin state-of-the-art ViTs using neuron labeling. Our findings reveal that ViTs\nencode concepts with increasing complexity throughout the network. Early layers\nprimarily encode basic features such as colors and textures, while later layers\nrepresent more specific classes, including objects and animals. As the\ncomplexity of encoded concepts increases, the number of concepts represented in\neach layer also rises, reflecting a more diverse and specific set of features.\nAdditionally, different pretraining strategies influence the quantity and\ncategory of encoded concepts, with finetuning to specific downstream tasks\ngenerally reducing the number of encoded concepts and shifting the concepts to\nmore relevant categories."}
{"id": "2503.24088", "pdf": "https://arxiv.org/pdf/2503.24088", "abs": "https://arxiv.org/abs/2503.24088", "authors": ["Lars Möllenbrok", "Behnood Rasti", "Begüm Demir"], "title": "A Plasticity-Aware Method for Continual Self-Supervised Learning in Remote Sensing", "categories": ["cs.CV"], "comment": "Accepted at IEEE International Geoscience and Remote Sensing\n  Symposium 2025", "summary": "Continual self-supervised learning (CSSL) methods have gained increasing\nattention in remote sensing (RS) due to their capability to learn new tasks\nsequentially from continuous streams of unlabeled data.\n  Existing CSSL methods, while learning new tasks, focus on preventing\ncatastrophic forgetting. To this end, most of them use regularization\nstrategies to retain knowledge of previous tasks. This reduces the model's\nability to adapt to the data of new tasks (i.e., learning plasticity), which\ncan degrade performance. To address this problem, in this paper, we propose a\nnovel CSSL method that aims to learn tasks sequentially, while achieving high\nlearning plasticity. To this end, the proposed method uses a knowledge\ndistillation strategy with an integrated decoupling mechanism. The decoupling\nis achieved by first dividing the feature dimensions into task-common and\ntask-specific parts. Then, the task-common features are forced to be correlated\nto ensure memory stability while the task-specific features are forced to be\nde-correlated facilitating the learning of new features. Experimental results\nshow the effectiveness of the proposed method compared to CaSSLe, which is a\nwidely used CSSL framework, with improvements of up to 1.12% in average\naccuracy and 2.33% in intransigence in a task-incremental scenario, and 1.24%\nin average accuracy and 2.01% in intransigence in a class-incremental scenario."}
{"id": "2503.24091", "pdf": "https://arxiv.org/pdf/2503.24091", "abs": "https://arxiv.org/abs/2503.24091", "authors": ["Xiangyuan Peng", "Miao Tang", "Huawei Sun", "Lorenzo Servadei", "Robert Wille"], "title": "4D mmWave Radar in Adverse Environments for Autonomous Driving: A Survey", "categories": ["cs.CV"], "comment": "8 pages", "summary": "Autonomous driving systems require accurate and reliable perception. However,\nadverse environments, such as rain, snow, and fog, can significantly degrade\nthe performance of LiDAR and cameras. In contrast, 4D millimeter-wave (mmWave)\nradar not only provides 3D sensing and additional velocity measurements but\nalso maintains robustness in challenging conditions, making it increasingly\nvaluable for autonomous driving. Recently, research on 4D mmWave radar under\nadverse environments has been growing, but a comprehensive survey is still\nlacking. To bridge this gap, this survey comprehensively reviews the current\nresearch on 4D mmWave radar under adverse environments. First, we present an\noverview of existing 4D mmWave radar datasets encompassing diverse weather and\nlighting scenarios. Next, we analyze methods and models according to different\nadverse conditions. Finally, the challenges faced in current studies and\npotential future directions are discussed for advancing 4D mmWave radar\napplications in harsh environments. To the best of our knowledge, this is the\nfirst survey specifically focusing on 4D mmWave radar in adverse environments\nfor autonomous driving."}
{"id": "2503.24096", "pdf": "https://arxiv.org/pdf/2503.24096", "abs": "https://arxiv.org/abs/2503.24096", "authors": ["Adrienne Deganutti", "Simon Hadfield", "Andrew Gilbert"], "title": "DANTE-AD: Dual-Vision Attention Network for Long-Term Audio Description", "categories": ["cs.CV"], "comment": null, "summary": "Audio Description is a narrated commentary designed to aid vision-impaired\naudiences in perceiving key visual elements in a video. While short-form video\nunderstanding has advanced rapidly, a solution for maintaining coherent\nlong-term visual storytelling remains unresolved. Existing methods rely solely\non frame-level embeddings, effectively describing object-based content but\nlacking contextual information across scenes. We introduce DANTE-AD, an\nenhanced video description model leveraging a dual-vision Transformer-based\narchitecture to address this gap. DANTE-AD sequentially fuses both frame and\nscene level embeddings to improve long-term contextual understanding. We\npropose a novel, state-of-the-art method for sequential cross-attention to\nachieve contextual grounding for fine-grained audio description generation.\nEvaluated on a broad range of key scenes from well-known movie clips, DANTE-AD\noutperforms existing methods across traditional NLP metrics and LLM-based\nevaluations."}
{"id": "2503.24108", "pdf": "https://arxiv.org/pdf/2503.24108", "abs": "https://arxiv.org/abs/2503.24108", "authors": ["Anwesa Choudhuri", "Zhongpai Gao", "Meng Zheng", "Benjamin Planche", "Terrence Chen", "Ziyan Wu"], "title": "PolypSegTrack: Unified Foundation Model for Colonoscopy Video Analysis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Early detection, accurate segmentation, classification and tracking of polyps\nduring colonoscopy are critical for preventing colorectal cancer. Many existing\ndeep-learning-based methods for analyzing colonoscopic videos either require\ntask-specific fine-tuning, lack tracking capabilities, or rely on\ndomain-specific pre-training. In this paper, we introduce\n\\textit{PolypSegTrack}, a novel foundation model that jointly addresses polyp\ndetection, segmentation, classification and unsupervised tracking in\ncolonoscopic videos. Our approach leverages a novel conditional mask loss,\nenabling flexible training across datasets with either pixel-level segmentation\nmasks or bounding box annotations, allowing us to bypass task-specific\nfine-tuning. Our unsupervised tracking module reliably associates polyp\ninstances across frames using object queries, without relying on any\nheuristics. We leverage a robust vision foundation model backbone that is\npre-trained unsupervisedly on natural images, thereby removing the need for\ndomain-specific pre-training. Extensive experiments on multiple polyp\nbenchmarks demonstrate that our method significantly outperforms existing\nstate-of-the-art approaches in detection, segmentation, classification, and\ntracking."}
{"id": "2503.24121", "pdf": "https://arxiv.org/pdf/2503.24121", "abs": "https://arxiv.org/abs/2503.24121", "authors": ["Valentin Boussot", "Cédric Hémon", "Jean-Claude Nunes", "Jason Downling", "Simon Rouzé", "Caroline Lafond", "Anaïs Barateau", "Jean-Louis Dillenseger"], "title": "IMPACT: A Generic Semantic Loss for Multimodal Medical Image Registration", "categories": ["cs.CV", "cs.LG"], "comment": "Submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI). This is a preprint version and has not been\n  peer-reviewed", "summary": "Image registration is fundamental in medical imaging, enabling precise\nalignment of anatomical structures for diagnosis, treatment planning,\nimage-guided treatment or longitudinal monitoring. This work introduces IMPACT\n(Image Metric with Pretrained model-Agnostic Comparison for Transmodality\nregistration), a generic semantic similarity metric designed for seamless\nintegration into diverse image registration frameworks (such as Elastix and\nVoxelmorph). It compares deep learning-based features extracted from medical\nimages without requiring task-specific training, ensuring broad applicability\nacross various modalities. By leveraging the features of the large-scale\npretrained TotalSegmentator models and the ability to integrate Segment\nAnything Model (SAM) and other large-scale segmentation networks, this approach\noffers significant advantages. It provides robust, scalable, and efficient\nsolutions for multimodal image registration. The IMPACT loss was evaluated on\nfive challenging registration tasks involving thoracic CT/CBCT, and pelvic\nMR/CT datasets. Quantitative metrics, such as Target Registration Error and\nDice Similarity Coefficient, demonstrated significant improvements in\nanatomical alignment compared to baseline methods. Qualitative analyses further\nconfirmed the increased robustness of the proposed metric in the face of noise,\nartifacts, and modality variations. IMPACT's versatility and efficiency make it\na valuable tool for advancing registration performance in clinical and research\napplications, addressing critical challenges in multimodal medical imaging."}
{"id": "2503.24129", "pdf": "https://arxiv.org/pdf/2503.24129", "abs": "https://arxiv.org/abs/2503.24129", "authors": ["Dominik Schnaus", "Nikita Araslanov", "Daniel Cremers"], "title": "It's a (Blind) Match! Towards Vision-Language Correspondence without Parallel Data", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to CVPR 2025, Project page:\n  https://dominik-schnaus.github.io/itsamatch/", "summary": "The platonic representation hypothesis suggests that vision and language\nembeddings become more homogeneous as model and dataset sizes increase. In\nparticular, pairwise distances within each modality become more similar. This\nsuggests that as foundation models mature, it may become possible to match\nvision and language embeddings in a fully unsupervised fashion, i.e. without\nparallel data. We present the first feasibility study, and investigate\nconformity of existing vision and language foundation models in the context of\nunsupervised, or \"blind\", matching. First, we formulate unsupervised matching\nas a quadratic assignment problem and introduce a novel heuristic that\noutperforms previous solvers. We also develop a technique to find optimal\nmatching problems, for which a non-trivial match is very likely. Second, we\nconduct an extensive study deploying a range of vision and language models on\nfour datasets. Our analysis reveals that for many problem instances, vision and\nlanguage representations can be indeed matched without supervision. This\nfinding opens up the exciting possibility of embedding semantic knowledge into\nother modalities virtually annotation-free. As a proof of concept, we showcase\nan unsupervised classifier, which achieves non-trivial classification accuracy\nwithout any image-text annotation."}
{"id": "2503.24135", "pdf": "https://arxiv.org/pdf/2503.24135", "abs": "https://arxiv.org/abs/2503.24135", "authors": ["Alexis Guichemerre", "Soufiane Belharbi", "Mohammadhadi Shateri", "Luke McCaffrey", "Eric Granger"], "title": "PixelCAM: Pixel Class Activation Mapping for Histology Image Classification and ROI Localization", "categories": ["cs.CV"], "comment": "32 pages, 20 figures, Medical Imaging with Deep Learning (MIDL 2025)", "summary": "Weakly supervised object localization (WSOL) methods allow training models to\nclassify images and localize ROIs. WSOL only requires low-cost image-class\nannotations yet provides a visually interpretable classifier, which is\nimportant in histology image analysis. Standard WSOL methods rely on class\nactivation mapping (CAM) methods to produce spatial localization maps according\nto a single- or two-step strategy. While both strategies have made significant\nprogress, they still face several limitations with histology images.\nSingle-step methods can easily result in under- or over-activation due to the\nlimited visual ROI saliency in histology images and the limited localization\ncues. They also face the well-known issue of asynchronous convergence between\nclassification and localization tasks. The two-step approach is sub-optimal\nbecause it is tied to a frozen classifier, limiting the capacity for\nlocalization. Moreover, these methods also struggle when applied to\nout-of-distribution (OOD) datasets. In this paper, a multi-task approach for\nWSOL is introduced for simultaneous training of both tasks to address the\nasynchronous convergence problem. In particular, localization is performed in\nthe pixel-feature space of an image encoder that is shared with classification.\nThis allows learning discriminant features and accurate delineation of\nforeground/background regions to support ROI localization and image\nclassification. We propose PixelCAM, a cost-effective foreground/background\npixel-wise classifier in the pixel-feature space that allows for spatial object\nlocalization. PixelCAM is trained using pixel pseudo-labels collected from a\npretrained WSOL model. Both image and pixel-wise classifiers are trained\nsimultaneously using standard gradient descent. In addition, our pixel\nclassifier can easily be integrated into CNN- and transformer-based\narchitectures without any modifications."}
{"id": "2503.24166", "pdf": "https://arxiv.org/pdf/2503.24166", "abs": "https://arxiv.org/abs/2503.24166", "authors": ["Fabian Fuchs", "Mario Ruben Fernandez", "Norman Ettrich", "Janis Keuper"], "title": "Foundation Models For Seismic Data Processing: An Extensive Review", "categories": ["cs.CV"], "comment": null, "summary": "Seismic processing plays a crucial role in transforming raw data into\nhigh-quality subsurface images, pivotal for various geoscience applications.\nDespite its importance, traditional seismic processing techniques face\nchallenges such as noisy and damaged data and the reliance on manual,\ntime-consuming workflows. The emergence of deep learning approaches has\nintroduced effective and user-friendly alternatives, yet many of these deep\nlearning approaches rely on synthetic datasets and specialized neural networks.\nRecently, foundation models have gained traction in the seismic domain, due to\ntheir success in natural imaging. This paper investigates the application of\nfoundation models in seismic processing on the tasks: demultiple,\ninterpolation, and denoising. It evaluates the impact of different model\ncharacteristics, such as pre-training technique and neural network\narchitecture, on performance and efficiency. Rather than proposing a single\nseismic foundation model, this paper critically examines various natural image\nfoundation models and suggest some promising candidates for future exploration."}
{"id": "2503.24180", "pdf": "https://arxiv.org/pdf/2503.24180", "abs": "https://arxiv.org/abs/2503.24180", "authors": ["Ziming Cheng", "Zhiyuan Huang", "Junting Pan", "Zhaohui Hou", "Mingjie Zhan"], "title": "Navi-plus: Managing Ambiguous GUI Navigation Tasks with Follow-up", "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Graphical user interfaces (GUI) automation agents are emerging as powerful\ntools, enabling humans to accomplish increasingly complex tasks on smart\ndevices. However, users often inadvertently omit key information when conveying\ntasks, which hinders agent performance in the current agent paradigm that does\nnot support immediate user intervention. To address this issue, we introduce a\n$\\textbf{Self-Correction GUI Navigation}$ task that incorporates interactive\ninformation completion capabilities within GUI agents. We developed the\n$\\textbf{Navi-plus}$ dataset with GUI follow-up question-answer pairs,\nalongside a $\\textbf{Dual-Stream Trajectory Evaluation}$ method to benchmark\nthis new capability. Our results show that agents equipped with the ability to\nask GUI follow-up questions can fully recover their performance when faced with\nambiguous user tasks."}
{"id": "2503.24182", "pdf": "https://arxiv.org/pdf/2503.24182", "abs": "https://arxiv.org/abs/2503.24182", "authors": ["Yingrui Ji", "Xi Xiao", "Gaofei Chen", "Hao Xu", "Chenrui Ma", "Lijing Zhu", "Aokun Liang", "Jiansheng Chen"], "title": "CIBR: Cross-modal Information Bottleneck Regularization for Robust CLIP Generalization", "categories": ["cs.CV"], "comment": null, "summary": "Contrastive Language-Image Pretraining (CLIP) has achieved remarkable success\nin cross-modal tasks such as zero-shot image classification and text-image\nretrieval by effectively aligning visual and textual representations. However,\nthe theoretical foundations underlying CLIP's strong generalization remain\nunclear. In this work, we address this gap by proposing the Cross-modal\nInformation Bottleneck (CIB) framework. CIB offers a principled interpretation\nof CLIP's contrastive learning objective as an implicit Information Bottleneck\noptimization. Under this view, the model maximizes shared cross-modal\ninformation while discarding modality-specific redundancies, thereby preserving\nessential semantic alignment across modalities. Building on this insight, we\nintroduce a Cross-modal Information Bottleneck Regularization (CIBR) method\nthat explicitly enforces these IB principles during training. CIBR introduces a\npenalty term to discourage modality-specific redundancy, thereby enhancing\nsemantic alignment between image and text features. We validate CIBR on\nextensive vision-language benchmarks, including zero-shot classification across\nseven diverse image datasets and text-image retrieval on MSCOCO and Flickr30K.\nThe results show consistent performance gains over standard CLIP. These\nfindings provide the first theoretical understanding of CLIP's generalization\nthrough the IB lens. They also demonstrate practical improvements, offering\nguidance for future cross-modal representation learning."}
{"id": "2503.24210", "pdf": "https://arxiv.org/pdf/2503.24210", "abs": "https://arxiv.org/abs/2503.24210", "authors": ["Seungjun Lee", "Gim Hee Lee"], "title": "DiET-GS: Diffusion Prior and Event Stream-Assisted Motion Deblurring 3D Gaussian Splatting", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "CVPR 2025. Project Page: https://diet-gs.github.io", "summary": "Reconstructing sharp 3D representations from blurry multi-view images are\nlong-standing problem in computer vision. Recent works attempt to enhance\nhigh-quality novel view synthesis from the motion blur by leveraging\nevent-based cameras, benefiting from high dynamic range and microsecond\ntemporal resolution. However, they often reach sub-optimal visual quality in\neither restoring inaccurate color or losing fine-grained details. In this\npaper, we present DiET-GS, a diffusion prior and event stream-assisted motion\ndeblurring 3DGS. Our framework effectively leverages both blur-free event\nstreams and diffusion prior in a two-stage training strategy. Specifically, we\nintroduce the novel framework to constraint 3DGS with event double integral,\nachieving both accurate color and well-defined details. Additionally, we\npropose a simple technique to leverage diffusion prior to further enhance the\nedge details. Qualitative and quantitative results on both synthetic and\nreal-world data demonstrate that our DiET-GS is capable of producing\nsignificantly better quality of novel views compared to the existing baselines.\nOur project page is https://diet-gs.github.io"}
{"id": "2503.24219", "pdf": "https://arxiv.org/pdf/2503.24219", "abs": "https://arxiv.org/abs/2503.24219", "authors": ["Karim Radouane", "Hanane Azzag", "Mustapha lebbah"], "title": "MB-ORES: A Multi-Branch Object Reasoner for Visual Grounding in Remote Sensing", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": null, "summary": "We propose a unified framework that integrates object detection (OD) and\nvisual grounding (VG) for remote sensing (RS) imagery. To support conventional\nOD and establish an intuitive prior for VG task, we fine-tune an open-set\nobject detector using referring expression data, framing it as a partially\nsupervised OD task. In the first stage, we construct a graph representation of\neach image, comprising object queries, class embeddings, and proposal\nlocations. Then, our task-aware architecture processes this graph to perform\nthe VG task. The model consists of: (i) a multi-branch network that integrates\nspatial, visual, and categorical features to generate task-aware proposals, and\n(ii) an object reasoning network that assigns probabilities across proposals,\nfollowed by a soft selection mechanism for final referring object localization.\nOur model demonstrates superior performance on the OPT-RSVG and DIOR-RSVG\ndatasets, achieving significant improvements over state-of-the-art methods\nwhile retaining classical OD capabilities. The code will be available in our\nrepository: \\url{https://github.com/rd20karim/MB-ORES}."}
{"id": "2503.24229", "pdf": "https://arxiv.org/pdf/2503.24229", "abs": "https://arxiv.org/abs/2503.24229", "authors": ["Daichi Otsuka", "Shinichi Mae", "Ryosuke Yamada", "Hirokatsu Kataoka"], "title": "Pre-training with 3D Synthetic Data: Learning 3D Point Cloud Instance Segmentation from 3D Synthetic Scenes", "categories": ["cs.CV"], "comment": null, "summary": "In the recent years, the research community has witnessed growing use of 3D\npoint cloud data for the high applicability in various real-world applications.\nBy means of 3D point cloud, this modality enables to consider the actual size\nand spatial understanding. The applied fields include mechanical control of\nrobots, vehicles, or other real-world systems. Along this line, we would like\nto improve 3D point cloud instance segmentation which has emerged as a\nparticularly promising approach for these applications. However, the creation\nof 3D point cloud datasets entails enormous costs compared to 2D image\ndatasets. To train a model of 3D point cloud instance segmentation, it is\nnecessary not only to assign categories but also to provide detailed\nannotations for each point in the large-scale 3D space. Meanwhile, the increase\nof recent proposals for generative models in 3D domain has spurred proposals\nfor using a generative model to create 3D point cloud data. In this work, we\npropose a pre-training with 3D synthetic data to train a 3D point cloud\ninstance segmentation model based on generative model for 3D scenes represented\nby point cloud data. We directly generate 3D point cloud data with Point-E for\ninserting a generated data into a 3D scene. More recently in 2025, although\nthere are other accurate 3D generation models, even using the Point-E as an\nearly 3D generative model can effectively support the pre-training with 3D\nsynthetic data. In the experimental section, we compare our pre-training method\nwith baseline methods indicated improved performance, demonstrating the\nefficacy of 3D generative models for 3D point cloud instance segmentation."}
{"id": "2503.24258", "pdf": "https://arxiv.org/pdf/2503.24258", "abs": "https://arxiv.org/abs/2503.24258", "authors": ["Lorenzo Tronchin", "Tommy Löfstedt", "Paolo Soda", "Valerio Guarrasi"], "title": "Beyond a Single Mode: GAN Ensembles for Diverse Medical Data Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The advancement of generative AI, particularly in medical imaging, confronts\nthe trilemma of ensuring high fidelity, diversity, and efficiency in synthetic\ndata generation. While Generative Adversarial Networks (GANs) have shown\npromise across various applications, they still face challenges like mode\ncollapse and insufficient coverage of real data distributions. This work\nexplores the use of GAN ensembles to overcome these limitations, specifically\nin the context of medical imaging. By solving a multi-objective optimisation\nproblem that balances fidelity and diversity, we propose a method for selecting\nan optimal ensemble of GANs tailored for medical data. The selected ensemble is\ncapable of generating diverse synthetic medical images that are representative\nof true data distributions and computationally efficient. Each model in the\nensemble brings a unique contribution, ensuring minimal redundancy. We\nconducted a comprehensive evaluation using three distinct medical datasets,\ntesting 22 different GAN architectures with various loss functions and\nregularisation techniques. By sampling models at different training epochs, we\ncrafted 110 unique configurations. The results highlight the capability of GAN\nensembles to enhance the quality and utility of synthetic medical images,\nthereby improving the efficacy of downstream tasks such as diagnostic\nmodelling."}
{"id": "2503.24267", "pdf": "https://arxiv.org/pdf/2503.24267", "abs": "https://arxiv.org/abs/2503.24267", "authors": ["Yixuan Li", "Yu Tian", "Yipo Huang", "Wei Lu", "Shiqi Wang", "Weisi Lin", "Anderson Rocha"], "title": "FakeScope: Large Multimodal Expert Model for Transparent AI-Generated Image Forensics", "categories": ["cs.CV"], "comment": null, "summary": "The rapid and unrestrained advancement of generative artificial intelligence\n(AI) presents a double-edged sword: while enabling unprecedented creativity, it\nalso facilitates the generation of highly convincing deceptive content,\nundermining societal trust. As image generation techniques become increasingly\nsophisticated, detecting synthetic images is no longer just a binary task: it\nnecessitates interpretable, context-aware methodologies that enhance\ntrustworthiness and transparency. However, existing detection models primarily\nfocus on classification, offering limited explanatory insights into image\nauthenticity. In this work, we propose FakeScope, an expert multimodal model\n(LMM) tailored for AI-generated image forensics, which not only identifies\nAI-synthetic images with high accuracy but also provides rich, interpretable,\nand query-driven forensic insights. We first construct FakeChain dataset that\ncontains linguistic authenticity reasoning based on visual trace evidence,\ndeveloped through a novel human-machine collaborative framework. Building upon\nit, we further present FakeInstruct, the largest multimodal instruction tuning\ndataset containing 2 million visual instructions tailored to enhance forensic\nawareness in LMMs. FakeScope achieves state-of-the-art performance in both\nclosed-ended and open-ended forensic scenarios. It can distinguish synthetic\nimages with high accuracy while offering coherent and insightful explanations,\nfree-form discussions on fine-grained forgery attributes, and actionable\nenhancement strategies. Notably, despite being trained exclusively on\nqualitative hard labels, FakeScope demonstrates remarkable zero-shot\nquantitative capability on detection, enabled by our proposed token-based\nprobability estimation strategy. Furthermore, FakeScope exhibits strong\ngeneralization and in-the-wild ability, ensuring its applicability in\nreal-world scenarios."}
{"id": "2503.24270", "pdf": "https://arxiv.org/pdf/2503.24270", "abs": "https://arxiv.org/abs/2503.24270", "authors": ["Yuelei Li", "Hyunjin Kim", "Fangneng Zhan", "Ri-Zhao Qiu", "Mazeyu Ji", "Xiaojun Shan", "Xueyan Zou", "Paul Liang", "Hanspeter Pfister", "Xiaolong Wang"], "title": "Visual Acoustic Fields", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Objects produce different sounds when hit, and humans can intuitively infer\nhow an object might sound based on its appearance and material properties.\nInspired by this intuition, we propose Visual Acoustic Fields, a framework that\nbridges hitting sounds and visual signals within a 3D space using 3D Gaussian\nSplatting (3DGS). Our approach features two key modules: sound generation and\nsound localization. The sound generation module leverages a conditional\ndiffusion model, which takes multiscale features rendered from a\nfeature-augmented 3DGS to generate realistic hitting sounds. Meanwhile, the\nsound localization module enables querying the 3D scene, represented by the\nfeature-augmented 3DGS, to localize hitting positions based on the sound\nsources. To support this framework, we introduce a novel pipeline for\ncollecting scene-level visual-sound sample pairs, achieving alignment between\ncaptured images, impact locations, and corresponding sounds. To the best of our\nknowledge, this is the first dataset to connect visual and acoustic signals in\na 3D context. Extensive experiments on our dataset demonstrate the\neffectiveness of Visual Acoustic Fields in generating plausible impact sounds\nand accurately localizing impact sources. Our project page is at\nhttps://yuelei0428.github.io/projects/Visual-Acoustic-Fields/."}
{"id": "2503.24272", "pdf": "https://arxiv.org/pdf/2503.24272", "abs": "https://arxiv.org/abs/2503.24272", "authors": ["Yizhou Huang", "Yihua Cheng", "Kezhi Wang"], "title": "Learning Velocity and Acceleration: Self-Supervised Motion Consistency for Pedestrian Trajectory Prediction", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Understanding human motion is crucial for accurate pedestrian trajectory\nprediction. Conventional methods typically rely on supervised learning, where\nground-truth labels are directly optimized against predicted trajectories. This\namplifies the limitations caused by long-tailed data distributions, making it\ndifficult for the model to capture abnormal behaviors. In this work, we propose\na self-supervised pedestrian trajectory prediction framework that explicitly\nmodels position, velocity, and acceleration. We leverage velocity and\nacceleration information to enhance position prediction through feature\ninjection and a self-supervised motion consistency mechanism. Our model\nhierarchically injects velocity features into the position stream. Acceleration\nfeatures are injected into the velocity stream. This enables the model to\npredict position, velocity, and acceleration jointly. From the predicted\nposition, we compute corresponding pseudo velocity and acceleration, allowing\nthe model to learn from data-generated pseudo labels and thus achieve\nself-supervised learning. We further design a motion consistency evaluation\nstrategy grounded in physical principles; it selects the most reasonable\npredicted motion trend by comparing it with historical dynamics and uses this\ntrend to guide and constrain trajectory generation. We conduct experiments on\nthe ETH-UCY and Stanford Drone datasets, demonstrating that our method achieves\nstate-of-the-art performance on both datasets."}
{"id": "2503.24282", "pdf": "https://arxiv.org/pdf/2503.24282", "abs": "https://arxiv.org/abs/2503.24282", "authors": ["Jian Wang", "Xin Lan", "Jizhe Zhou", "Yuxin Tian", "Jiancheng Lv"], "title": "Style Quantization for Data-Efficient GAN Training", "categories": ["cs.CV"], "comment": null, "summary": "Under limited data setting, GANs often struggle to navigate and effectively\nexploit the input latent space. Consequently, images generated from adjacent\nvariables in a sparse input latent space may exhibit significant discrepancies\nin realism, leading to suboptimal consistency regularization (CR) outcomes. To\naddress this, we propose \\textit{SQ-GAN}, a novel approach that enhances CR by\nintroducing a style space quantization scheme. This method transforms the\nsparse, continuous input latent space into a compact, structured discrete proxy\nspace, allowing each element to correspond to a specific real data point,\nthereby improving CR performance. Instead of direct quantization, we first map\nthe input latent variables into a less entangled ``style'' space and apply\nquantization using a learnable codebook. This enables each quantized code to\ncontrol distinct factors of variation. Additionally, we optimize the optimal\ntransport distance to align the codebook codes with features extracted from the\ntraining data by a foundation model, embedding external knowledge into the\ncodebook and establishing a semantically rich vocabulary that properly\ndescribes the training dataset. Extensive experiments demonstrate significant\nimprovements in both discriminator robustness and generation quality with our\nmethod."}
{"id": "2503.24298", "pdf": "https://arxiv.org/pdf/2503.24298", "abs": "https://arxiv.org/abs/2503.24298", "authors": ["Thinesh Thiyakesan Ponbagavathi", "Alina Roitberg"], "title": "Order Matters: On Parameter-Efficient Image-to-Video Probing for Recognizing Nearly Symmetric Actions", "categories": ["cs.CV"], "comment": null, "summary": "We study parameter-efficient image-to-video probing for the unaddressed\nchallenge of recognizing nearly symmetric actions - visually similar actions\nthat unfold in opposite temporal order (e.g., opening vs. closing a bottle).\nExisting probing mechanisms for image-pretrained models, such as DinoV2 and\nCLIP, rely on attention mechanism for temporal modeling but are inherently\npermutation-invariant, leading to identical predictions regardless of frame\norder. To address this, we introduce Self-attentive Temporal Embedding Probing\n(STEP), a simple yet effective approach designed to enforce temporal\nsensitivity in parameter-efficient image-to-video transfer. STEP enhances\nself-attentive probing with three key modifications: (1) a learnable frame-wise\npositional encoding, explicitly encoding temporal order; (2) a single global\nCLS token, for sequence coherence; and (3) a simplified attention mechanism to\nimprove parameter efficiency. STEP outperforms existing image-to-video probing\nmechanisms by 3-15% across four activity recognition benchmarks with only 1/3\nof the learnable parameters. On two datasets, it surpasses all published\nmethods, including fully fine-tuned models. STEP shows a distinct advantage in\nrecognizing nearly symmetric actions, surpassing other probing mechanisms by\n9-19%. and parameter-heavier PEFT-based transfer methods by 5-15%. Code and\nmodels will be made publicly available."}
{"id": "2503.24306", "pdf": "https://arxiv.org/pdf/2503.24306", "abs": "https://arxiv.org/abs/2503.24306", "authors": ["Adam Schmidt", "Mert Asim Karaoglu", "Soham Sinha", "Mingang Jang", "Ho-Gun Ha", "Kyungmin Jung", "Kyeongmo Gu", "Ihsan Ullah", "Hyunki Lee", "Jonáš Šerých", "Michal Neoral", "Jiří Matas", "Rulin Zhou", "Wenlong He", "An Wang", "Hongliang Ren", "Bruno Silva", "Sandro Queirós", "Estêvão Lima", "João L. Vilaça", "Shunsuke Kikuchi", "Atsushi Kouno", "Hiroki Matsuzaki", "Tongtong Li", "Yulu Chen", "Ling Li", "Xiang Ma", "Xiaojian Li", "Mona Sheikh Zeinoddin", "Xu Wang", "Zafer Tandogdu", "Greg Shaw", "Evangelos Mazomenos", "Danail Stoyanov", "Yuxin Chen", "Zijian Wu", "Alexander Ladikos", "Simon DiMaio", "Septimiu E. Salcudean", "Omid Mohareri"], "title": "Point Tracking in Surgery--The 2024 Surgical Tattoos in Infrared (STIR) Challenge", "categories": ["cs.CV"], "comment": null, "summary": "Understanding tissue motion in surgery is crucial to enable applications in\ndownstream tasks such as segmentation, 3D reconstruction, virtual tissue\nlandmarking, autonomous probe-based scanning, and subtask autonomy. Labeled\ndata are essential to enabling algorithms in these downstream tasks since they\nallow us to quantify and train algorithms. This paper introduces a point\ntracking challenge to address this, wherein participants can submit their\nalgorithms for quantification. The submitted algorithms are evaluated using a\ndataset named surgical tattoos in infrared (STIR), with the challenge aptly\nnamed the STIR Challenge 2024. The STIR Challenge 2024 comprises two\nquantitative components: accuracy and efficiency. The accuracy component tests\nthe accuracy of algorithms on in vivo and ex vivo sequences. The efficiency\ncomponent tests the latency of algorithm inference. The challenge was conducted\nas a part of MICCAI EndoVis 2024. In this challenge, we had 8 total teams, with\n4 teams submitting before and 4 submitting after challenge day. This paper\ndetails the STIR Challenge 2024, which serves to move the field towards more\naccurate and efficient algorithms for spatial understanding in surgery. In this\npaper we summarize the design, submissions, and results from the challenge. The\nchallenge dataset is available here: https://zenodo.org/records/14803158 , and\nthe code for baseline models and metric calculation is available here:\nhttps://github.com/athaddius/STIRMetrics"}
{"id": "2503.24320", "pdf": "https://arxiv.org/pdf/2503.24320", "abs": "https://arxiv.org/abs/2503.24320", "authors": ["Wenyan Cong", "Hanqing Zhu", "Peihao Wang", "Bangya Liu", "Dejia Xu", "Kevin Wang", "David Z. Pan", "Yan Wang", "Zhiwen Fan", "Zhangyang Wang"], "title": "Can Test-Time Scaling Improve World Foundation Model?", "categories": ["cs.CV"], "comment": null, "summary": "World foundation models, which simulate the physical world by predicting\nfuture states from current observations and inputs, have become central to many\napplications in physical intelligence, including autonomous driving and\nrobotics. However, these models require substantial computational resources for\npretraining and are further constrained by available data during post-training.\nAs such, scaling computation at test time emerges as both a critical and\npractical alternative to traditional model enlargement or re-training. In this\nwork, we introduce SWIFT, a test-time scaling framework tailored for WFMs.\nSWIFT integrates our extensible WFM evaluation toolkit with process-level\ninference strategies, including fast tokenization, probability-based Top-K\npruning, and efficient beam search. Empirical results on the COSMOS model\ndemonstrate that test-time scaling exists even in a compute-optimal way. Our\nfindings reveal that test-time scaling laws hold for WFMs and that SWIFT\nprovides a scalable and effective pathway for improving WFM inference without\nretraining or increasing model size. The code is available at\nhttps://github.com/Mia-Cong/SWIFT.git."}
{"id": "2503.24326", "pdf": "https://arxiv.org/pdf/2503.24326", "abs": "https://arxiv.org/abs/2503.24326", "authors": ["Rupert Polley", "Sai Vignesh Abishek Deenadayalan", "J. Marius Zöllner"], "title": "Self-Supervised Pretraining for Aerial Road Extraction", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Deep neural networks for aerial image segmentation require large amounts of\nlabeled data, but high-quality aerial datasets with precise annotations are\nscarce and costly to produce. To address this limitation, we propose a\nself-supervised pretraining method that improves segmentation performance while\nreducing reliance on labeled data. Our approach uses inpainting-based\npretraining, where the model learns to reconstruct missing regions in aerial\nimages, capturing their inherent structure before being fine-tuned for road\nextraction. This method improves generalization, enhances robustness to domain\nshifts, and is invariant to model architecture and dataset choice. Experiments\nshow that our pretraining significantly boosts segmentation accuracy,\nespecially in low-data regimes, making it a scalable solution for aerial image\nanalysis."}
{"id": "2503.24345", "pdf": "https://arxiv.org/pdf/2503.24345", "abs": "https://arxiv.org/abs/2503.24345", "authors": ["Fang Yan", "Jianfeng Wu", "Jiawen Li", "Wei Wang", "Jiaxuan Lu", "Wen Chen", "Zizhao Gao", "Jianan Li", "Hong Yan", "Jiabo Ma", "Minda Chen", "Yang Lu", "Qing Chen", "Yizhi Wang", "Xitong Ling", "Xuenian Wang", "Zihan Wang", "Qiang Huang", "Shengyi Hua", "Mianxin Liu", "Lei Ma", "Tian Shen", "Xiaofan Zhang", "Yonghong He", "Hao Chen", "Shaoting Zhang", "Zhe Wang"], "title": "PathOrchestra: A Comprehensive Foundation Model for Computational Pathology with Over 100 Diverse Clinical-Grade Tasks", "categories": ["cs.CV"], "comment": null, "summary": "The complexity and variability inherent in high-resolution pathological\nimages present significant challenges in computational pathology. While\npathology foundation models leveraging AI have catalyzed transformative\nadvancements, their development demands large-scale datasets, considerable\nstorage capacity, and substantial computational resources. Furthermore,\nensuring their clinical applicability and generalizability requires rigorous\nvalidation across a broad spectrum of clinical tasks. Here, we present\nPathOrchestra, a versatile pathology foundation model trained via\nself-supervised learning on a dataset comprising 300K pathological slides from\n20 tissue and organ types across multiple centers. The model was rigorously\nevaluated on 112 clinical tasks using a combination of 61 private and 51 public\ndatasets. These tasks encompass digital slide preprocessing, pan-cancer\nclassification, lesion identification, multi-cancer subtype classification,\nbiomarker assessment, gene expression prediction, and the generation of\nstructured reports. PathOrchestra demonstrated exceptional performance across\n27,755 WSIs and 9,415,729 ROIs, achieving over 0.950 accuracy in 47 tasks,\nincluding pan-cancer classification across various organs, lymphoma subtype\ndiagnosis, and bladder cancer screening. Notably, it is the first model to\ngenerate structured reports for high-incidence colorectal cancer and\ndiagnostically complex lymphoma-areas that are infrequently addressed by\nfoundational models but hold immense clinical potential. Overall, PathOrchestra\nexemplifies the feasibility and efficacy of a large-scale, self-supervised\npathology foundation model, validated across a broad range of clinical-grade\ntasks. Its high accuracy and reduced reliance on extensive data annotation\nunderline its potential for clinical integration, offering a pathway toward\nmore efficient and high-quality medical services."}
{"id": "2503.24357", "pdf": "https://arxiv.org/pdf/2503.24357", "abs": "https://arxiv.org/abs/2503.24357", "authors": ["Shuaizheng Liu", "Jianqi Ma", "Lingchen Sun", "Xiangtao Kong", "Lei Zhang"], "title": "InstructRestore: Region-Customized Image Restoration with Human Instructions", "categories": ["cs.CV"], "comment": null, "summary": "Despite the significant progress in diffusion prior-based image restoration,\nmost existing methods apply uniform processing to the entire image, lacking the\ncapability to perform region-customized image restoration according to user\ninstructions. In this work, we propose a new framework, namely InstructRestore,\nto perform region-adjustable image restoration following human instructions. To\nachieve this, we first develop a data generation engine to produce training\ntriplets, each consisting of a high-quality image, the target region\ndescription, and the corresponding region mask. With this engine and careful\ndata screening, we construct a comprehensive dataset comprising 536,945\ntriplets to support the training and evaluation of this task. We then examine\nhow to integrate the low-quality image features under the ControlNet\narchitecture to adjust the degree of image details enhancement. Consequently,\nwe develop a ControlNet-like model to identify the target region and allocate\ndifferent integration scales to the target and surrounding regions, enabling\nregion-customized image restoration that aligns with user instructions.\nExperimental results demonstrate that our proposed InstructRestore approach\nenables effective human-instructed image restoration, such as images with bokeh\neffects and user-instructed local enhancement. Our work advances the\ninvestigation of interactive image restoration and enhancement techniques.\nData, code, and models will be found at\nhttps://github.com/shuaizhengliu/InstructRestore.git."}
{"id": "2503.24366", "pdf": "https://arxiv.org/pdf/2503.24366", "abs": "https://arxiv.org/abs/2503.24366", "authors": ["Shakiba Kheradmand", "Delio Vicini", "George Kopanas", "Dmitry Lagun", "Kwang Moo Yi", "Mark Matthews", "Andrea Tagliasacchi"], "title": "StochasticSplats: Stochastic Rasterization for Sorting-Free 3D Gaussian Splatting", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "3D Gaussian splatting (3DGS) is a popular radiance field method, with many\napplication-specific extensions. Most variants rely on the same core algorithm:\ndepth-sorting of Gaussian splats then rasterizing in primitive order. This\nensures correct alpha compositing, but can cause rendering artifacts due to\nbuilt-in approximations. Moreover, for a fixed representation, sorted rendering\noffers little control over render cost and visual fidelity. For example, and\ncounter-intuitively, rendering a lower-resolution image is not necessarily\nfaster. In this work, we address the above limitations by combining 3D Gaussian\nsplatting with stochastic rasterization. Concretely, we leverage an unbiased\nMonte Carlo estimator of the volume rendering equation. This removes the need\nfor sorting, and allows for accurate 3D blending of overlapping Gaussians. The\nnumber of Monte Carlo samples further imbues 3DGS with a way to trade off\ncomputation time and quality. We implement our method using OpenGL shaders,\nenabling efficient rendering on modern GPU hardware. At a reasonable visual\nquality, our method renders more than four times faster than sorted\nrasterization."}
{"id": "2503.24368", "pdf": "https://arxiv.org/pdf/2503.24368", "abs": "https://arxiv.org/abs/2503.24368", "authors": ["Xiaoran Zhang", "Eric Z. Chen", "Lin Zhao", "Xiao Chen", "Yikang Liu", "Boris Maihe", "James S. Duncan", "Terrence Chen", "Shanhui Sun"], "title": "Adapting Vision Foundation Models for Real-time Ultrasound Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "We propose a novel approach that adapts hierarchical vision foundation models\nfor real-time ultrasound image segmentation. Existing ultrasound segmentation\nmethods often struggle with adaptability to new tasks, relying on costly manual\nannotations, while real-time approaches generally fail to match\nstate-of-the-art performance. To overcome these limitations, we introduce an\nadaptive framework that leverages the vision foundation model Hiera to extract\nmulti-scale features, interleaved with DINOv2 representations to enhance visual\nexpressiveness. These enriched features are then decoded to produce precise and\nrobust segmentation. We conduct extensive evaluations on six public datasets\nand one in-house dataset, covering both cardiac and thyroid ultrasound\nsegmentation. Experiments show that our approach outperforms state-of-the-art\nmethods across multiple datasets and excels with limited supervision,\nsurpassing nnUNet by over 20\\% on average in the 1\\% and 10\\% data settings.\nOur method achieves $\\sim$77 FPS inference speed with TensorRT on a single GPU,\nenabling real-time clinical applications."}
{"id": "2503.24374", "pdf": "https://arxiv.org/pdf/2503.24374", "abs": "https://arxiv.org/abs/2503.24374", "authors": ["Maxim V. Shugaev", "Vincent Chen", "Maxim Karrenbach", "Kyle Ashley", "Bridget Kennedy", "Naresh P. Cuntoor"], "title": "ERUPT: Efficient Rendering with Unposed Patch Transformer", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "This work addresses the problem of novel view synthesis in diverse scenes\nfrom small collections of RGB images. We propose ERUPT (Efficient Rendering\nwith Unposed Patch Transformer) a state-of-the-art scene reconstruction model\ncapable of efficient scene rendering using unposed imagery. We introduce\npatch-based querying, in contrast to existing pixel-based queries, to reduce\nthe compute required to render a target view. This makes our model highly\nefficient both during training and at inference, capable of rendering at 600\nfps on commercial hardware. Notably, our model is designed to use a learned\nlatent camera pose which allows for training using unposed targets in datasets\nwith sparse or inaccurate ground truth camera pose. We show that our approach\ncan generalize on large real-world data and introduce a new benchmark dataset\n(MSVS-1M) for latent view synthesis using street-view imagery collected from\nMapillary. In contrast to NeRF and Gaussian Splatting, which require dense\nimagery and precise metadata, ERUPT can render novel views of arbitrary scenes\nwith as few as five unposed input images. ERUPT achieves better rendered image\nquality than current state-of-the-art methods for unposed image synthesis\ntasks, reduces labeled data requirements by ~95\\% and decreases computational\nrequirements by an order of magnitude, providing efficient novel view synthesis\nfor diverse real-world scenes."}
{"id": "2503.24376", "pdf": "https://arxiv.org/pdf/2503.24376", "abs": "https://arxiv.org/abs/2503.24376", "authors": ["Yi Chen", "Yuying Ge", "Rui Wang", "Yixiao Ge", "Lu Qiu", "Ying Shan", "Xihui Liu"], "title": "Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Technical Report (In Progress); Code released at:\n  https://github.com/TencentARC/SEED-Bench-R1", "summary": "Recent advancements in Chain of Thought (COT) generation have significantly\nimproved the reasoning capabilities of Large Language Models (LLMs), with\nreinforcement learning (RL) emerging as an effective post-training approach.\nMultimodal Large Language Models (MLLMs) inherit this reasoning potential but\nremain underexplored in tasks requiring both perception and logical reasoning.\nTo address this, we introduce SEED-Bench-R1, a benchmark designed to\nsystematically evaluate post-training methods for MLLMs in video understanding.\nIt includes intricate real-world videos and complex everyday planning tasks in\nthe format of multiple-choice questions, requiring sophisticated perception and\nreasoning. SEED-Bench-R1 assesses generalization through a three-level\nhierarchy: in-distribution, cross-environment, and cross-environment-task\nscenarios, equipped with a large-scale training dataset with easily verifiable\nground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL\nwith supervised fine-tuning (SFT), demonstrating RL's data efficiency and\nsuperior performance on both in-distribution and out-of-distribution tasks,\neven outperforming SFT on general video understanding benchmarks like\nLongVideoBench. Our detailed analysis reveals that RL enhances visual\nperception but often produces less logically coherent reasoning chains. We\nidentify key limitations such as inconsistent reasoning and overlooked visual\ncues, and suggest future improvements in base model reasoning, reward modeling,\nand RL robustness against noisy signals."}
{"id": "2503.24379", "pdf": "https://arxiv.org/pdf/2503.24379", "abs": "https://arxiv.org/abs/2503.24379", "authors": ["Shengqiong Wu", "Weicai Ye", "Jiahao Wang", "Quande Liu", "Xintao Wang", "Pengfei Wan", "Di Zhang", "Kun Gai", "Shuicheng Yan", "Hao Fei", "Tat-Seng Chua"], "title": "Any2Caption:Interpreting Any Condition to Caption for Controllable Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": "Project Page: https://sqwu.top/Any2Cap/", "summary": "To address the bottleneck of accurate user intent interpretation within the\ncurrent video generation community, we present Any2Caption, a novel framework\nfor controllable video generation under any condition. The key idea is to\ndecouple various condition interpretation steps from the video synthesis step.\nBy leveraging modern multimodal large language models (MLLMs), Any2Caption\ninterprets diverse inputs--text, images, videos, and specialized cues such as\nregion, motion, and camera poses--into dense, structured captions that offer\nbackbone video generators with better guidance. We also introduce Any2CapIns, a\nlarge-scale dataset with 337K instances and 407K conditions for\nany-condition-to-caption instruction tuning. Comprehensive evaluations\ndemonstrate significant improvements of our system in controllability and video\nquality across various aspects of existing video generation models. Project\nPage: https://sqwu.top/Any2Cap/"}
{"id": "2503.24381", "pdf": "https://arxiv.org/pdf/2503.24381", "abs": "https://arxiv.org/abs/2503.24381", "authors": ["Yuping Wang", "Xiangyu Huang", "Xiaokang Sun", "Mingxuan Yan", "Shuo Xing", "Zhengzhong Tu", "Jiachen Li"], "title": "UniOcc: A Unified Benchmark for Occupancy Forecasting and Prediction in Autonomous Driving", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MA", "cs.RO"], "comment": "14 pages; Dataset: https://huggingface.co/datasets/tasl-lab/uniocc;\n  Code: https://github.com/tasl-lab/UniOcc", "summary": "We introduce UniOcc, a comprehensive, unified benchmark for occupancy\nforecasting (i.e., predicting future occupancies based on historical\ninformation) and current-frame occupancy prediction from camera images. UniOcc\nunifies data from multiple real-world datasets (i.e., nuScenes, Waymo) and\nhigh-fidelity driving simulators (i.e., CARLA, OpenCOOD), which provides 2D/3D\noccupancy labels with per-voxel flow annotations and support for cooperative\nautonomous driving. In terms of evaluation, unlike existing studies that rely\non suboptimal pseudo labels for evaluation, UniOcc incorporates novel metrics\nthat do not depend on ground-truth occupancy, enabling robust assessment of\nadditional aspects of occupancy quality. Through extensive experiments on\nstate-of-the-art models, we demonstrate that large-scale, diverse training data\nand explicit flow information significantly enhance occupancy prediction and\nforecasting performance."}
{"id": "2503.24382", "pdf": "https://arxiv.org/pdf/2503.24382", "abs": "https://arxiv.org/abs/2503.24382", "authors": ["Chong Bao", "Xiyu Zhang", "Zehao Yu", "Jiale Shi", "Guofeng Zhang", "Songyou Peng", "Zhaopeng Cui"], "title": "Free360: Layered Gaussian Splatting for Unbounded 360-Degree View Synthesis from Extremely Sparse and Unposed Views", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025. Project Page:\n  https://zju3dv.github.io/free360/", "summary": "Neural rendering has demonstrated remarkable success in high-quality 3D\nneural reconstruction and novel view synthesis with dense input views and\naccurate poses. However, applying it to extremely sparse, unposed views in\nunbounded 360{\\deg} scenes remains a challenging problem. In this paper, we\npropose a novel neural rendering framework to accomplish the unposed and\nextremely sparse-view 3D reconstruction in unbounded 360{\\deg} scenes. To\nresolve the spatial ambiguity inherent in unbounded scenes with sparse input\nviews, we propose a layered Gaussian-based representation to effectively model\nthe scene with distinct spatial layers. By employing a dense stereo\nreconstruction model to recover coarse geometry, we introduce a layer-specific\nbootstrap optimization to refine the noise and fill occluded regions in the\nreconstruction. Furthermore, we propose an iterative fusion of reconstruction\nand generation alongside an uncertainty-aware training approach to facilitate\nmutual conditioning and enhancement between these two processes. Comprehensive\nexperiments show that our approach outperforms existing state-of-the-art\nmethods in terms of rendering quality and surface reconstruction accuracy.\nProject page: https://zju3dv.github.io/free360/"}
{"id": "2503.24387", "pdf": "https://arxiv.org/pdf/2503.24387", "abs": "https://arxiv.org/abs/2503.24387", "authors": ["Lee Hsin-Ying", "Kelvin C. K. Chan", "Ming-Hsuan Yang"], "title": "Consistent Subject Generation via Contrastive Instantiated Concepts", "categories": ["cs.CV"], "comment": "Project page: https://contrastive-concept-instantiation.github.io", "summary": "While text-to-image generative models can synthesize diverse and faithful\ncontents, subject variation across multiple creations limits the application in\nlong content generation. Existing approaches require time-consuming tuning,\nreferences for all subjects, or access to other creations. We introduce\nContrastive Concept Instantiation (CoCoIns) to effectively synthesize\nconsistent subjects across multiple independent creations. The framework\nconsists of a generative model and a mapping network, which transforms input\nlatent codes into pseudo-words associated with certain instances of concepts.\nUsers can generate consistent subjects with the same latent codes. To construct\nsuch associations, we propose a contrastive learning approach that trains the\nnetwork to differentiate the combination of prompts and latent codes. Extensive\nevaluations of human faces with a single subject show that CoCoIns performs\ncomparably to existing methods while maintaining higher flexibility. We also\ndemonstrate the potential of extending CoCoIns to multiple subjects and other\nobject categories."}
{"id": "2503.24389", "pdf": "https://arxiv.org/pdf/2503.24389", "abs": "https://arxiv.org/abs/2503.24389", "authors": ["Chenyang Li", "Wenxuan Liu", "Guoqiang Gong", "Xiaobo Ding", "Xian Zhong"], "title": "SU-YOLO: Spiking Neural Network for Efficient Underwater Object Detection", "categories": ["cs.CV", "cs.NE"], "comment": null, "summary": "Underwater object detection is critical for oceanic research and industrial\nsafety inspections. However, the complex optical environment and the limited\nresources of underwater equipment pose significant challenges to achieving high\naccuracy and low power consumption. To address these issues, we propose Spiking\nUnderwater YOLO (SU-YOLO), a Spiking Neural Network (SNN) model. Leveraging the\nlightweight and energy-efficient properties of SNNs, SU-YOLO incorporates a\nnovel spike-based underwater image denoising method based solely on integer\naddition, which enhances the quality of feature maps with minimal computational\noverhead. In addition, we introduce Separated Batch Normalization (SeBN), a\ntechnique that normalizes feature maps independently across multiple time steps\nand is optimized for integration with residual structures to capture the\ntemporal dynamics of SNNs more effectively. The redesigned spiking residual\nblocks integrate the Cross Stage Partial Network (CSPNet) with the YOLO\narchitecture to mitigate spike degradation and enhance the model's feature\nextraction capabilities. Experimental results on URPC2019 underwater dataset\ndemonstrate that SU-YOLO achieves mAP of 78.8% with 6.97M parameters and an\nenergy consumption of 2.98 mJ, surpassing mainstream SNN models in both\ndetection accuracy and computational efficiency. These results underscore the\npotential of SNNs for engineering applications. The code is available in\nhttps://github.com/lwxfight/snn-underwater."}
{"id": "2503.24391", "pdf": "https://arxiv.org/pdf/2503.24391", "abs": "https://arxiv.org/abs/2503.24391", "authors": ["Xingyu Chen", "Yue Chen", "Yuliang Xiu", "Andreas Geiger", "Anpei Chen"], "title": "Easi3R: Estimating Disentangled Motion from DUSt3R Without Training", "categories": ["cs.CV"], "comment": "Page: https://easi3r.github.io/ Code:\n  https://github.com/Inception3D/Easi3R", "summary": "Recent advances in DUSt3R have enabled robust estimation of dense point\nclouds and camera parameters of static scenes, leveraging Transformer network\narchitectures and direct supervision on large-scale 3D datasets. In contrast,\nthe limited scale and diversity of available 4D datasets present a major\nbottleneck for training a highly generalizable 4D model. This constraint has\ndriven conventional 4D methods to fine-tune 3D models on scalable dynamic video\ndata with additional geometric priors such as optical flow and depths. In this\nwork, we take an opposite path and introduce Easi3R, a simple yet efficient\ntraining-free method for 4D reconstruction. Our approach applies attention\nadaptation during inference, eliminating the need for from-scratch pre-training\nor network fine-tuning. We find that the attention layers in DUSt3R inherently\nencode rich information about camera and object motion. By carefully\ndisentangling these attention maps, we achieve accurate dynamic region\nsegmentation, camera pose estimation, and 4D dense point map reconstruction.\nExtensive experiments on real-world dynamic videos demonstrate that our\nlightweight attention adaptation significantly outperforms previous\nstate-of-the-art methods that are trained or finetuned on extensive dynamic\ndatasets. Our code is publicly available for research purpose at\nhttps://easi3r.github.io/"}
{"id": "2503.22697", "pdf": "https://arxiv.org/pdf/2503.22697", "abs": "https://arxiv.org/abs/2503.22697", "authors": ["Feihan Feng", "Jingxin Nie"], "title": "From Eye to Mind: brain2text Decoding Reveals the Neural Mechanisms of Visual Semantic Processing", "categories": ["q-bio.NC", "cs.AI", "cs.CV"], "comment": "27 pages, 7 figures", "summary": "Deciphering the neural mechanisms that transform sensory experiences into\nmeaningful semantic representations is a fundamental challenge in cognitive\nneuroscience. While neuroimaging has mapped a distributed semantic network, the\nformat and neural code of semantic content remain elusive, particularly for\ncomplex, naturalistic stimuli. Traditional brain decoding, focused on visual\nreconstruction, primarily captures low-level perceptual features, missing the\ndeeper semantic essence guiding human cognition. Here, we introduce a paradigm\nshift by directly decoding fMRI signals into textual descriptions of viewed\nnatural images. Our novel deep learning model, trained without visual input,\nachieves state-of-the-art semantic decoding performance, generating meaningful\ncaptions that capture the core semantic content of complex scenes.\nNeuroanatomical analysis reveals the critical role of higher-level visual\nregions, including MT+, ventral stream visual cortex, and inferior parietal\ncortex, in this semantic transformation. Category-specific decoding further\ndemonstrates nuanced neural representations for semantic dimensions like\nanimacy and motion. This text-based decoding approach provides a more direct\nand interpretable window into the brain's semantic encoding than visual\nreconstruction, offering a powerful new methodology for probing the neural\nbasis of complex semantic processing, refining our understanding of the\ndistributed semantic network, and potentially inspiring brain-inspired language\nmodels."}
{"id": "2503.22713", "pdf": "https://arxiv.org/pdf/2503.22713", "abs": "https://arxiv.org/abs/2503.22713", "authors": ["Nooshin Bahador", "Milad Lankarany"], "title": "Chirp Localization via Fine-Tuned Transformer Model: A Proof-of-Concept Study", "categories": ["eess.AS", "cs.AI", "cs.CV", "cs.LG", "cs.SD"], "comment": "19 pages, 8 figures", "summary": "Spectrograms are pivotal in time-frequency signal analysis, widely used in\naudio processing and computational neuroscience. Chirp-like patterns in\nelectroencephalogram (EEG) spectrograms (marked by linear or exponential\nfrequency sweep) are key biomarkers for seizure dynamics, but automated tools\nfor their detection, localization, and feature extraction are lacking. This\nstudy bridges this gap by fine-tuning a Vision Transformer (ViT) model on\nsynthetic spectrograms, augmented with Low-Rank Adaptation (LoRA) to boost\nadaptability. We generated 100000 synthetic spectrograms with chirp parameters,\ncreating the first large-scale benchmark for chirp localization. These\nspectrograms mimic neural chirps using linear or exponential frequency sweep,\nGaussian noise, and smoothing. A ViT model, adapted for regression, predicted\nchirp parameters. LoRA fine-tuned the attention layers, enabling efficient\nupdates to the pre-trained backbone. Training used MSE loss and the AdamW\noptimizer, with a learning rate scheduler and early stopping to curb\noverfitting. Only three features were targeted: Chirp Start Time (Onset Time),\nChirp Start Frequency (Onset Frequency), and Chirp End Frequency (Offset\nFrequency). Performance was evaluated via Pearson correlation between predicted\nand actual labels. Results showed strong alignment: 0.9841 correlation for\nchirp start time, with stable inference times (137 to 140s) and minimal bias in\nerror distributions. This approach offers a tool for chirp analysis in EEG\ntime-frequency representation, filling a critical methodological void."}
{"id": "2503.22714", "pdf": "https://arxiv.org/pdf/2503.22714", "abs": "https://arxiv.org/abs/2503.22714", "authors": ["Sergio Torres Aguilar"], "title": "TRIDIS: A Comprehensive Medieval and Early Modern Corpus for HTR and NER", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "6 pages, 3 figures, 2 tables", "summary": "This paper introduces TRIDIS (Tria Digita Scribunt), an open-source corpus of\nmedieval and early modern manuscripts. TRIDIS aggregates multiple legacy\ncollections (all published under open licenses) and incorporates large metadata\ndescriptions. While prior publications referenced some portions of this corpus,\nhere we provide a unified overview with a stronger focus on its constitution.\nWe describe (i) the narrative, chronological, and editorial background of each\nmajor sub-corpus, (ii) its semi-diplomatic transcription rules (expansion,\nnormalization, punctuation), (iii) a strategy for challenging out-of-domain\ntest splits driven by outlier detection in a joint embedding space, and (iv)\npreliminary baseline experiments using TrOCR and MiniCPM2.5 comparing random\nand outlier-based test partitions. Overall, TRIDIS is designed to stimulate\njoint robust Handwritten Text Recognition (HTR) and Named Entity Recognition\n(NER) research across medieval and early modern textual heritage."}
{"id": "2503.22715", "pdf": "https://arxiv.org/pdf/2503.22715", "abs": "https://arxiv.org/abs/2503.22715", "authors": ["Jiahao Qin", "Feng Liu", "Lu Zong"], "title": "Hierarchical Adaptive Expert for Multimodal Sentiment Analysis", "categories": ["cs.LG", "cs.CV", "cs.MM"], "comment": "11 pages, 3 figures", "summary": "Multimodal sentiment analysis has emerged as a critical tool for\nunderstanding human emotions across diverse communication channels. While\nexisting methods have made significant strides, they often struggle to\neffectively differentiate and integrate modality-shared and modality-specific\ninformation, limiting the performance of multimodal learning. To address this\nchallenge, we propose the Hierarchical Adaptive Expert for Multimodal Sentiment\nAnalysis (HAEMSA), a novel framework that synergistically combines evolutionary\noptimization, cross-modal knowledge transfer, and multi-task learning. HAEMSA\nemploys a hierarchical structure of adaptive experts to capture both global and\nlocal modality representations, enabling more nuanced sentiment analysis. Our\napproach leverages evolutionary algorithms to dynamically optimize network\narchitectures and modality combinations, adapting to both partial and full\nmodality scenarios. Extensive experiments demonstrate HAEMSA's superior\nperformance across multiple benchmark datasets. On CMU-MOSEI, HAEMSA achieves a\n2.6% increase in 7-class accuracy and a 0.059 decrease in MAE compared to the\nprevious best method. For CMU-MOSI, we observe a 6.3% improvement in 7-class\naccuracy and a 0.058 reduction in MAE. On IEMOCAP, HAEMSA outperforms the\nstate-of-the-art by 2.84% in weighted-F1 score for emotion recognition. These\nresults underscore HAEMSA's effectiveness in capturing complex multimodal\ninteractions and generalizing across different emotional contexts."}
{"id": "2503.22728", "pdf": "https://arxiv.org/pdf/2503.22728", "abs": "https://arxiv.org/abs/2503.22728", "authors": ["Ao Fu", "Ziqi Ni", "Yi Zhou"], "title": "Dual Audio-Centric Modality Coupling for Talking Head Generation", "categories": ["cs.SD", "cs.CV", "eess.AS"], "comment": "9 pages, 4 figures", "summary": "The generation of audio-driven talking head videos is a key challenge in\ncomputer vision and graphics, with applications in virtual avatars and digital\nmedia. Traditional approaches often struggle with capturing the complex\ninteraction between audio and facial dynamics, leading to lip synchronization\nand visual quality issues. In this paper, we propose a novel NeRF-based\nframework, Dual Audio-Centric Modality Coupling (DAMC), which effectively\nintegrates content and dynamic features from audio inputs. By leveraging a dual\nencoder structure, DAMC captures semantic content through the Content-Aware\nEncoder and ensures precise visual synchronization through the Dynamic-Sync\nEncoder. These features are fused using a Cross-Synchronized Fusion Module\n(CSFM), enhancing content representation and lip synchronization. Extensive\nexperiments show that our method outperforms existing state-of-the-art\napproaches in key metrics such as lip synchronization accuracy and image\nquality, demonstrating robust generalization across various audio inputs,\nincluding synthetic speech from text-to-speech (TTS) systems. Our results\nprovide a promising solution for high-quality, audio-driven talking head\ngeneration and present a scalable approach for creating realistic talking\nheads."}
{"id": "2503.22729", "pdf": "https://arxiv.org/pdf/2503.22729", "abs": "https://arxiv.org/abs/2503.22729", "authors": ["Jiahao Qin", "Feng Liu", "Lu Zong"], "title": "Ancestral Mamba: Enhancing Selective Discriminant Space Model with Online Visual Prototype Learning for Efficient and Robust Discriminant Approach", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "10 pages, 3 figures", "summary": "In the realm of computer graphics, the ability to learn continuously from\nnon-stationary data streams while adapting to new visual patterns and\nmitigating catastrophic forgetting is of paramount importance. Existing\napproaches often struggle to capture and represent the essential\ncharacteristics of evolving visual concepts, hindering their applicability to\ndynamic graphics tasks. In this paper, we propose Ancestral Mamba, a novel\napproach that integrates online prototype learning into a selective\ndiscriminant space model for efficient and robust online continual learning.\nThe key components of our approach include Ancestral Prototype Adaptation\n(APA), which continuously refines and builds upon learned visual prototypes,\nand Mamba Feedback (MF), which provides targeted feedback to adapt to\nchallenging visual patterns. APA enables the model to continuously adapt its\nprototypes, building upon ancestral knowledge to tackle new challenges, while\nMF acts as a targeted feedback mechanism, focusing on challenging classes and\nrefining their representations. Extensive experiments on graphics-oriented\ndatasets, such as CIFAR-10 and CIFAR-100, demonstrate the superior performance\nof Ancestral Mamba compared to state-of-the-art baselines, achieving\nsignificant improvements in accuracy and forgetting mitigation."}
{"id": "2503.22742", "pdf": "https://arxiv.org/pdf/2503.22742", "abs": "https://arxiv.org/abs/2503.22742", "authors": ["William Claster", "Suhas KM", "Dhairya Gundechia"], "title": "Adaptive Integrated Layered Attention (AILA)", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.IR", "cs.NE"], "comment": null, "summary": "We propose Adaptive Integrated Layered Attention (AILA), a neural network\narchitecture that combines dense skip connections with different mechanisms for\nadaptive feature reuse across network layers. We evaluate AILA on three\nchallenging tasks: price forecasting for various commodities and indices (S&P\n500, Gold, US dollar Futures, Coffee, Wheat), image recognition using the\nCIFAR-10 dataset, and sentiment analysis on the IMDB movie review dataset. In\nall cases, AILA matches strong deep learning baselines (LSTMs, Transformers,\nand ResNets), achieving it at a fraction of the training and inference time.\nNotably, we implement and test two versions of the model - AILA-Architecture 1,\nwhich uses simple linear layers as the connection mechanism between layers, and\nAILA-Architecture 2, which implements an attention mechanism to selectively\nfocus on outputs from previous layers. Both architectures are applied in a\nsingle-task learning setting, with each model trained separately for individual\ntasks. Results confirm that AILA's adaptive inter-layer connections yield\nrobust gains by flexibly reusing pertinent features at multiple network depths.\nThe AILA approach thus presents an extension to existing architectures,\nimproving long-range sequence modeling, image recognition with optimised\ncomputational speed, and SOTA classification performance in practice."}
{"id": "2503.22829", "pdf": "https://arxiv.org/pdf/2503.22829", "abs": "https://arxiv.org/abs/2503.22829", "authors": ["Zhen Lin", "Hongyu Yuan", "Richard Barcus", "Qing Lyu", "Sucheta Chakravarty", "Megan E. Lipford", "Carol A. Shively", "Suzanne Craft", "Mohammad Kawas", "Jeongchul Kim", "Christopher T. Whitlow"], "title": "Nonhuman Primate Brain Tissue Segmentation Using a Transfer Learning Approach", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Non-human primates (NHPs) serve as critical models for understanding human\nbrain function and neurological disorders due to their close evolutionary\nrelationship with humans. Accurate brain tissue segmentation in NHPs is\ncritical for understanding neurological disorders, but challenging due to the\nscarcity of annotated NHP brain MRI datasets, the small size of the NHP brain,\nthe limited resolution of available imaging data and the anatomical differences\nbetween human and NHP brains. To address these challenges, we propose a novel\napproach utilizing STU-Net with transfer learning to leverage knowledge\ntransferred from human brain MRI data to enhance segmen-tation accuracy in the\nNHP brain MRI, particularly when training data is limited.The combination of\nSTU-Net and transfer learning effectively delineates complex tissue boundaries\nand captures fine anatomical details specific to NHP brains. Notably, our\nmethod demonstrated improvement in segmenting small subcortical structures such\nas putamen and thalamus that are challenging to resolve with limited spatial\nresolution and tissue contrast, and achieved DSC of over 0.88, IoU over 0.8 and\nHD95 under 7. This study introduces a robust method for multi-class brain\ntissue segmentation in NHPs, potentially accelerating research in evolutionary\nneuroscience and preclinical studies of neurological disorders relevant to\nhuman health."}
{"id": "2503.22876", "pdf": "https://arxiv.org/pdf/2503.22876", "abs": "https://arxiv.org/abs/2503.22876", "authors": ["Kushagra Srivastava", "Rutwik Kulkarni", "Manoj Velmurugan", "Nitin J. Sanket"], "title": "VizFlyt: Perception-centric Pedagogical Framework For Autonomous Aerial Robots", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted at ICRA 2025. Projected Page:\n  https://pear.wpi.edu/research/vizflyt.html", "summary": "Autonomous aerial robots are becoming commonplace in our lives. Hands-on\naerial robotics courses are pivotal in training the next-generation workforce\nto meet the growing market demands. Such an efficient and compelling course\ndepends on a reliable testbed. In this paper, we present \\textit{VizFlyt}, an\nopen-source perception-centric Hardware-In-The-Loop (HITL) photorealistic\ntesting framework for aerial robotics courses. We utilize pose from an external\nlocalization system to hallucinate real-time and photorealistic visual sensors\nusing 3D Gaussian Splatting. This enables stress-free testing of autonomy\nalgorithms on aerial robots without the risk of crashing into obstacles. We\nachieve over 100Hz of system update rate. Lastly, we build upon our past\nexperiences of offering hands-on aerial robotics courses and propose a new\nopen-source and open-hardware curriculum based on \\textit{VizFlyt} for the\nfuture. We test our framework on various course projects in real-world HITL\nexperiments and present the results showing the efficacy of such a system and\nits large potential use cases. Code, datasets, hardware guides and demo videos\nare available at https://pear.wpi.edu/research/vizflyt.html"}
{"id": "2503.22943", "pdf": "https://arxiv.org/pdf/2503.22943", "abs": "https://arxiv.org/abs/2503.22943", "authors": ["Haoyang Wang", "Ruishan Guo", "Pengtao Ma", "Ciyu Ruan", "Xinyu Luo", "Wenhua Ding", "Tianyang Zhong", "Jingao Xu", "Yunhao Liu", "Xinlei Chen"], "title": "Towards Mobile Sensing with Event Cameras on High-mobility Resource-constrained Devices: A Survey", "categories": ["cs.RO", "cs.CV"], "comment": "32 pages, 9 figures", "summary": "With the increasing complexity of mobile device applications, these devices\nare evolving toward high mobility. This shift imposes new demands on mobile\nsensing, particularly in terms of achieving high accuracy and low latency.\nEvent-based vision has emerged as a disruptive paradigm, offering high temporal\nresolution, low latency, and energy efficiency, making it well-suited for\nhigh-accuracy and low-latency sensing tasks on high-mobility platforms.\nHowever, the presence of substantial noisy events, the lack of inherent\nsemantic information, and the large data volume pose significant challenges for\nevent-based data processing on resource-constrained mobile devices. This paper\nsurveys the literature over the period 2014-2024, provides a comprehensive\noverview of event-based mobile sensing systems, covering fundamental\nprinciples, event abstraction methods, algorithmic advancements, hardware and\nsoftware acceleration strategies. We also discuss key applications of event\ncameras in mobile sensing, including visual odometry, object tracking, optical\nflow estimation, and 3D reconstruction, while highlighting the challenges\nassociated with event data processing, sensor fusion, and real-time deployment.\nFurthermore, we outline future research directions, such as improving event\ncamera hardware with advanced optics, leveraging neuromorphic computing for\nefficient processing, and integrating bio-inspired algorithms to enhance\nperception. To support ongoing research, we provide an open-source\n\\textit{Online Sheet} with curated resources and recent developments. We hope\nthis survey serves as a valuable reference, facilitating the adoption of\nevent-based vision across diverse applications."}
{"id": "2503.23042", "pdf": "https://arxiv.org/pdf/2503.23042", "abs": "https://arxiv.org/abs/2503.23042", "authors": ["M Rita Verdelho", "Alexandre Bernardino", "Catarina Barata"], "title": "MIL vs. Aggregation: Evaluating Patient-Level Survival Prediction Strategies Using Graph-Based Learning", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Oncologists often rely on a multitude of data, including whole-slide images\n(WSIs), to guide therapeutic decisions, aiming for the best patient outcome.\nHowever, predicting the prognosis of cancer patients can be a challenging task\ndue to tumor heterogeneity and intra-patient variability, and the complexity of\nanalyzing WSIs. These images are extremely large, containing billions of\npixels, making direct processing computationally expensive and requiring\nspecialized methods to extract relevant information. Additionally, multiple\nWSIs from the same patient may capture different tumor regions, some being more\ninformative than others. This raises a fundamental question: Should we use all\nWSIs to characterize the patient, or should we identify the most representative\nslide for prognosis? Our work seeks to answer this question by performing a\ncomparison of various strategies for predicting survival at the WSI and patient\nlevel. The former treats each WSI as an independent sample, mimicking the\nstrategy adopted in other works, while the latter comprises methods to either\naggregate the predictions of the several WSIs or automatically identify the\nmost relevant slide using multiple-instance learning (MIL). Additionally, we\nevaluate different Graph Neural Networks architectures under these strategies.\nWe conduct our experiments using the MMIST-ccRCC dataset, which comprises\npatients with clear cell renal cell carcinoma (ccRCC). Our results show that\nMIL-based selection improves accuracy, suggesting that choosing the most\nrepresentative slide benefits survival prediction."}
{"id": "2503.23050", "pdf": "https://arxiv.org/pdf/2503.23050", "abs": "https://arxiv.org/abs/2503.23050", "authors": ["Tiago Almeida", "Plinio Moreno", "Catarina Barata"], "title": "Prediction of 30-day hospital readmission with clinical notes and EHR information", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "High hospital readmission rates are associated with significant costs and\nhealth risks for patients. Therefore, it is critical to develop predictive\nmodels that can support clinicians to determine whether or not a patient will\nreturn to the hospital in a relatively short period of time (e.g, 30-days).\nNowadays, it is possible to collect both structured (electronic health records\n- EHR) and unstructured information (clinical notes) about a patient hospital\nevent, all potentially containing relevant information for a predictive model.\nHowever, their integration is challenging. In this work we explore the\ncombination of clinical notes and EHRs to predict 30-day hospital readmissions.\nWe address the representation of the various types of information available in\nthe EHR data, as well as exploring LLMs to characterize the clinical notes. We\ncollect both information sources as the nodes of a graph neural network (GNN).\nOur model achieves an AUROC of 0.72 and a balanced accuracy of 66.7\\%,\nhighlighting the importance of combining the multimodal information."}
{"id": "2503.23179", "pdf": "https://arxiv.org/pdf/2503.23179", "abs": "https://arxiv.org/abs/2503.23179", "authors": ["Wiebke Heyer", "Yannic Elser", "Lennart Berkel", "Xinrui Song", "Xuanang Xu", "Pingkun Yan", "Xi Jia", "Zi Li", "Tony C. W. Mok", "BoWen LI", "Christian Staackmann", "Christoph Großbröhmer", "Alessa Hering", "Malte M. Sieren", "Mattias P. Heinrich"], "title": "OncoReg: Medical Image Registration for Oncological Challenges", "categories": ["eess.IV", "cs.CV"], "comment": "26 pages, 6 figures", "summary": "In modern cancer research, the vast volume of medical data generated is often\nunderutilised due to challenges related to patient privacy. The OncoReg\nChallenge addresses this issue by enabling researchers to develop and validate\nimage registration methods through a two-phase framework that ensures patient\nprivacy while fostering the development of more generalisable AI models. Phase\none involves working with a publicly available dataset, while phase two focuses\non training models on a private dataset within secure hospital networks.\nOncoReg builds upon the foundation established by the Learn2Reg Challenge by\nincorporating the registration of interventional cone-beam computed tomography\n(CBCT) with standard planning fan-beam CT (FBCT) images in radiotherapy.\nAccurate image registration is crucial in oncology, particularly for dynamic\ntreatment adjustments in image-guided radiotherapy, where precise alignment is\nnecessary to minimise radiation exposure to healthy tissues while effectively\ntargeting tumours. This work details the methodology and data behind the\nOncoReg Challenge and provides a comprehensive analysis of the competition\nentries and results. Findings reveal that feature extraction plays a pivotal\nrole in this registration task. A new method emerging from this challenge\ndemonstrated its versatility, while established approaches continue to perform\ncomparably to newer techniques. Both deep learning and classical approaches\nstill play significant roles in image registration, with the combination of\nmethods - particularly in feature extraction - proving most effective."}
{"id": "2503.23219", "pdf": "https://arxiv.org/pdf/2503.23219", "abs": "https://arxiv.org/abs/2503.23219", "authors": ["Sanjoy Chowdhury", "Hanan Gani", "Nishit Anand", "Sayan Nag", "Ruohan Gao", "Mohamed Elhoseiny", "Salman Khan", "Dinesh Manocha"], "title": "Aurelia: Test-time Reasoning Distillation in Audio-Visual LLMs", "categories": ["eess.AS", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Recent advancements in reasoning optimization have greatly enhanced the\nperformance of large language models (LLMs). However, existing work fails to\naddress the complexities of audio-visual scenarios, underscoring the need for\nfurther research. In this paper, we introduce AURELIA, a novel actor-critic\nbased audio-visual (AV) reasoning framework that distills structured,\nstep-by-step reasoning into AVLLMs at test time, improving their ability to\nprocess complex multi-modal inputs without additional training or fine-tuning.\nTo further advance AVLLM reasoning skills, we present AVReasonBench, a\nchallenging benchmark comprising 4500 audio-visual questions, each paired with\ndetailed step-by-step reasoning. Our benchmark spans six distinct tasks,\nincluding AV-GeoIQ, which evaluates AV reasoning combined with geographical and\ncultural knowledge. Evaluating 18 AVLLMs on AVReasonBench reveals significant\nlimitations in their multi-modal reasoning capabilities. Using AURELIA, we\nachieve up to a 100% relative improvement, demonstrating its effectiveness.\nThis performance gain highlights the potential of reasoning-enhanced data\ngeneration for advancing AVLLMs in real-world applications. Our code and data\nwill be publicly released at: https: //github.com/schowdhury671/aurelia."}
{"id": "2503.23241", "pdf": "https://arxiv.org/pdf/2503.23241", "abs": "https://arxiv.org/abs/2503.23241", "authors": ["Nam Anh Dinh", "Itai Lang", "Hyunwoo Kim", "Oded Stein", "Rana Hanocka"], "title": "Geometry in Style: 3D Stylization via Surface Normal Deformation", "categories": ["cs.GR", "cs.CV"], "comment": "CVPR 2025. Our project page is at\n  https://threedle.github.io/geometry-in-style", "summary": "We present Geometry in Style, a new method for identity-preserving mesh\nstylization. Existing techniques either adhere to the original shape through\noverly restrictive deformations such as bump maps or significantly modify the\ninput shape using expressive deformations that may introduce artifacts or alter\nthe identity of the source shape. In contrast, we represent a deformation of a\ntriangle mesh as a target normal vector for each vertex neighborhood. The\ndeformations we recover from target normals are expressive enough to enable\ndetailed stylizations yet restrictive enough to preserve the shape's identity.\nWe achieve such deformations using our novel differentiable\nAs-Rigid-As-Possible (dARAP) layer, a neural-network-ready adaptation of the\nclassical ARAP algorithm which we use to solve for per-vertex rotations and\ndeformed vertices. As a differentiable layer, dARAP is paired with a visual\nloss from a text-to-image model to drive deformations toward style prompts,\naltogether giving us Geometry in Style. Our project page is at\nhttps://threedle.github.io/geometry-in-style."}
{"id": "2503.23265", "pdf": "https://arxiv.org/pdf/2503.23265", "abs": "https://arxiv.org/abs/2503.23265", "authors": ["Björn Möller", "Lucas Görnhardt", "Tim Fingscheidt"], "title": "A Lightweight Image Super-Resolution Transformer Trained on Low-Resolution Images Only", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Transformer architectures prominently lead single-image super-resolution\n(SISR) benchmarks, reconstructing high-resolution (HR) images from their\nlow-resolution (LR) counterparts. Their strong representative power, however,\ncomes with a higher demand for training data compared to convolutional neural\nnetworks (CNNs). For many real-world SR applications, the availability of\nhigh-quality HR training images is not given, sparking interest in LR-only\ntraining methods. The LR-only SISR benchmark mimics this condition by allowing\nonly low-resolution (LR) images for model training. For a 4x super-resolution,\nthis effectively reduces the amount of available training data to 6.25% of the\nHR image pixels, which puts the employment of a data-hungry transformer model\ninto question. In this work, we are the first to utilize a lightweight vision\ntransformer model with LR-only training methods addressing the unsupervised\nSISR LR-only benchmark. We adopt and configure a recent LR-only training method\nfrom microscopy image super-resolution to macroscopic real-world data,\nresulting in our multi-scale training method for bicubic degradation (MSTbic).\nFurthermore, we compare it with reference methods and prove its effectiveness\nboth for a transformer and a CNN model. We evaluate on the classic SR benchmark\ndatasets Set5, Set14, BSD100, Urban100, and Manga109, and show superior\nperformance over state-of-the-art (so far: CNN-based) LR-only SISR methods. The\ncode is available on GitHub:\nhttps://github.com/ifnspaml/SuperResolutionMultiscaleTraining."}
{"id": "2503.23284", "pdf": "https://arxiv.org/pdf/2503.23284", "abs": "https://arxiv.org/abs/2503.23284", "authors": ["Feng-Lin Liu", "Hongbo Fu", "Xintao Wang", "Weicai Ye", "Pengfei Wan", "Di Zhang", "Lin Gao"], "title": "SketchVideo: Sketch-based Video Generation and Editing", "categories": ["cs.GR", "cs.CV"], "comment": "CVPR 2025", "summary": "Video generation and editing conditioned on text prompts or images have\nundergone significant advancements. However, challenges remain in accurately\ncontrolling global layout and geometry details solely by texts, and supporting\nmotion control and local modification through images. In this paper, we aim to\nachieve sketch-based spatial and motion control for video generation and\nsupport fine-grained editing of real or synthetic videos. Based on the DiT\nvideo generation model, we propose a memory-efficient control structure with\nsketch control blocks that predict residual features of skipped DiT blocks.\nSketches are drawn on one or two keyframes (at arbitrary time points) for easy\ninteraction. To propagate such temporally sparse sketch conditions across all\nframes, we propose an inter-frame attention mechanism to analyze the\nrelationship between the keyframes and each video frame. For sketch-based video\nediting, we design an additional video insertion module that maintains\nconsistency between the newly edited content and the original video's spatial\nfeature and dynamic motion. During inference, we use latent fusion for the\naccurate preservation of unedited regions. Extensive experiments demonstrate\nthat our SketchVideo achieves superior performance in controllable video\ngeneration and editing."}
{"id": "2503.23312", "pdf": "https://arxiv.org/pdf/2503.23312", "abs": "https://arxiv.org/abs/2503.23312", "authors": ["Hyunsik Jeon", "Satoshi Koide", "Yu Wang", "Zhankui He", "Julian McAuley"], "title": "LaViC: Adapting Large Vision-Language Models to Visually-Aware Conversational Recommendation", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Conversational recommender systems engage users in dialogues to refine their\nneeds and provide more personalized suggestions. Although textual information\nsuffices for many domains, visually driven categories such as fashion or home\ndecor potentially require detailed visual information related to color, style,\nor design. To address this challenge, we propose LaViC (Large Vision-Language\nConversational Recommendation Framework), a novel approach that integrates\ncompact image representations into dialogue-based recommendation systems. LaViC\nleverages a large vision-language model in a two-stage process: (1) visual\nknowledge self-distillation, which condenses product images from hundreds of\ntokens into a small set of visual tokens in a self-distillation manner,\nsignificantly reducing computational overhead, and (2) recommendation prompt\ntuning, which enables the model to incorporate both dialogue context and\ndistilled visual tokens, providing a unified mechanism for capturing textual\nand visual features. To support rigorous evaluation of visually-aware\nconversational recommendation, we construct a new dataset by aligning Reddit\nconversations with Amazon product listings across multiple visually oriented\ncategories (e.g., fashion, beauty, and home). This dataset covers realistic\nuser queries and product appearances in domains where visual details are\ncrucial. Extensive experiments demonstrate that LaViC significantly outperforms\ntext-only conversational recommendation methods and open-source vision-language\nbaselines. Moreover, LaViC achieves competitive or superior accuracy compared\nto prominent proprietary baselines (e.g., GPT-3.5-turbo, GPT-4o-mini, and\nGPT-4o), demonstrating the necessity of explicitly using visual data for\ncapturing product attributes and showing the effectiveness of our\nvision-language integration. Our code and dataset are available at\nhttps://github.com/jeon185/LaViC."}
{"id": "2503.23333", "pdf": "https://arxiv.org/pdf/2503.23333", "abs": "https://arxiv.org/abs/2503.23333", "authors": ["Jing Zhu", "Mingxuan Ju", "Yozen Liu", "Danai Koutra", "Neil Shah", "Tong Zhao"], "title": "Beyond Unimodal Boundaries: Generative Recommendation with Multimodal Semantics", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Generative recommendation (GR) has become a powerful paradigm in\nrecommendation systems that implicitly links modality and semantics to item\nrepresentation, in contrast to previous methods that relied on non-semantic\nitem identifiers in autoregressive models. However, previous research has\npredominantly treated modalities in isolation, typically assuming item content\nis unimodal (usually text). We argue that this is a significant limitation\ngiven the rich, multimodal nature of real-world data and the potential\nsensitivity of GR models to modality choices and usage. Our work aims to\nexplore the critical problem of Multimodal Generative Recommendation (MGR),\nhighlighting the importance of modality choices in GR nframeworks. We reveal\nthat GR models are particularly sensitive to different modalities and examine\nthe challenges in achieving effective GR when multiple modalities are\navailable. By evaluating design strategies for effectively leveraging multiple\nmodalities, we identify key challenges and introduce MGR-LF++, an enhanced late\nfusion framework that employs contrastive modality alignment and special tokens\nto denote different modalities, achieving a performance improvement of over 20%\ncompared to single-modality alternatives."}
{"id": "2503.23348", "pdf": "https://arxiv.org/pdf/2503.23348", "abs": "https://arxiv.org/abs/2503.23348", "authors": ["Jianhua Sun", "Jiude Wei", "Yuxuan Li", "Cewu Lu"], "title": "Physically Ground Commonsense Knowledge for Articulated Object Manipulation with Analytic Concepts", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "We human rely on a wide range of commonsense knowledge to interact with an\nextensive number and categories of objects in the physical world. Likewise,\nsuch commonsense knowledge is also crucial for robots to successfully develop\ngeneralized object manipulation skills. While recent advancements in Large\nLanguage Models (LLM) have showcased their impressive capabilities in acquiring\ncommonsense knowledge and conducting commonsense reasoning, effectively\ngrounding this semantic-level knowledge produced by LLMs to the physical world\nto thoroughly guide robots in generalized articulated object manipulation\nremains a challenge that has not been sufficiently addressed. To this end, we\nintroduce analytic concepts, procedurally defined upon mathematical symbolism\nthat can be directly computed and simulated by machines. By leveraging the\nanalytic concepts as a bridge between the semantic-level knowledge inferred by\nLLMs and the physical world where real robots operate, we are able to figure\nout the knowledge of object structure and functionality with physics-informed\nrepresentations, and then use the physically grounded knowledge to instruct\nrobot control policies for generalized, interpretable and accurate articulated\nobject manipulation. Extensive experiments in both simulation and real-world\nenvironments demonstrate the superiority of our approach."}
{"id": "2503.23410", "pdf": "https://arxiv.org/pdf/2503.23410", "abs": "https://arxiv.org/abs/2503.23410", "authors": ["Zhi Zhang", "Meng Gai", "Sheng Li"], "title": "Visual Acuity Consistent Foveated Rendering towards Retinal Resolution", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Prior foveated rendering methods often suffer from a limitation where the\nshading load escalates with increasing display resolution, leading to decreased\nefficiency, particularly when dealing with retinal-level resolutions. To tackle\nthis challenge, we begin with the essence of the human visual system (HVS)\nperception and present visual acuity-consistent foveated rendering (VaFR),\naiming to achieve exceptional rendering performance at retinal-level\nresolutions. Specifically, we propose a method with a novel log-polar mapping\nfunction derived from the human visual acuity model, which accommodates the\nnatural bandwidth of the visual system. This mapping function and its\nassociated shading rate guarantee a consistent output of rendering information,\nregardless of variations in the display resolution of the VR HMD. Consequently,\nour VaFR outperforms alternative methods, improving rendering speed while\npreserving perceptual visual quality, particularly when operating at retinal\nresolutions. We validate our approach using both the rasterization and\nray-casting rendering pipelines. We also validate our approach using different\nbinocular rendering strategies for HMD devices. In diverse testing scenarios,\nour approach delivers better perceptual visual quality than prior foveated\nrendering while achieving an impressive speedup of 6.5$\\times$-9.29$\\times$ for\ndeferred rendering of 3D scenarios and an even more powerful speedup of\n10.4$\\times$-16.4$\\times$ for ray-casting at retinal resolution. Additionally,\nour approach significantly enhances the rendering performance of binocular 8K\npath tracing, achieving smooth frame rates."}
{"id": "2503.23515", "pdf": "https://arxiv.org/pdf/2503.23515", "abs": "https://arxiv.org/abs/2503.23515", "authors": ["Alice E. A. Allen", "Emily Shinkle", "Roxana Bujack", "Nicholas Lubbers"], "title": "Optimal Invariant Bases for Atomistic Machine Learning", "categories": ["physics.chem-ph", "cs.CV", "stat.ML"], "comment": null, "summary": "The representation of atomic configurations for machine learning models has\nled to the development of numerous descriptors, often to describe the local\nenvironment of atoms. However, many of these representations are incomplete\nand/or functionally dependent. Incomplete descriptor sets are unable to\nrepresent all meaningful changes in the atomic environment. Complete\nconstructions of atomic environment descriptors, on the other hand, often\nsuffer from a high degree of functional dependence, where some descriptors can\nbe written as functions of the others. These redundant descriptors do not\nprovide additional power to discriminate between different atomic environments\nand increase the computational burden. By employing techniques from the pattern\nrecognition literature to existing atomistic representations, we remove\ndescriptors that are functions of other descriptors to produce the smallest\npossible set that satisfies completeness. We apply this in two ways: first we\nrefine an existing description, the Atomistic Cluster Expansion. We show that\nthis yields a more efficient subset of descriptors. Second, we augment an\nincomplete construction based on a scalar neural network, yielding a new\nmessage-passing network architecture that can recognize up to 5-body patterns\nin each neuron by taking advantage of an optimal set of Cartesian tensor\ninvariants. This architecture shows strong accuracy on state-of-the-art\nbenchmarks while retaining low computational cost. Our results not only yield\nimproved models, but point the way to classes of invariant bases that minimize\ncost while maximizing expressivity for a host of applications."}
{"id": "2503.23598", "pdf": "https://arxiv.org/pdf/2503.23598", "abs": "https://arxiv.org/abs/2503.23598", "authors": ["Kalliopi Basioti", "Pritish Sahu", "Qingze Tony Liu", "Zihao Xu", "Hao Wang", "Vladimir Pavlovic"], "title": "GenVP: Generating Visual Puzzles with Contrastive Hierarchical VAEs", "categories": ["cs.AI", "cs.CV"], "comment": "Accepted to ICLR 2025", "summary": "Raven's Progressive Matrices (RPMs) is an established benchmark to examine\nthe ability to perform high-level abstract visual reasoning (AVR). Despite the\ncurrent success of algorithms that solve this task, humans can generalize\nbeyond a given puzzle and create new puzzles given a set of rules, whereas\nmachines remain locked in solving a fixed puzzle from a curated choice list. We\npropose Generative Visual Puzzles (GenVP), a framework to model the entire RPM\ngeneration process, a substantially more challenging task. Our model's\ncapability spans from generating multiple solutions for one specific problem\nprompt to creating complete new puzzles out of the desired set of rules.\nExperiments on five different datasets indicate that GenVP achieves\nstate-of-the-art (SOTA) performance both in puzzle-solving accuracy and\nout-of-distribution (OOD) generalization in 22 OOD scenarios. Compared to SOTA\ngenerative approaches, which struggle to solve RPMs when the feasible solution\nspace increases, GenVP efficiently generalizes to these challenging setups.\nMoreover, our model demonstrates the ability to produce a wide range of\ncomplete RPMs given a set of abstract rules by effectively capturing the\nrelationships between abstract rules and visual object properties."}
{"id": "2503.23644", "pdf": "https://arxiv.org/pdf/2503.23644", "abs": "https://arxiv.org/abs/2503.23644", "authors": ["Chaojian Li", "Sixu Li", "Linrui Jiang", "Jingqun Zhang", "Yingyan Celine Lin"], "title": "Uni-Render: A Unified Accelerator for Real-Time Rendering Across Diverse Neural Renderers", "categories": ["cs.GR", "cs.AR", "cs.CV"], "comment": "Accepted by HPCA'25", "summary": "Recent advancements in neural rendering technologies and their supporting\ndevices have paved the way for immersive 3D experiences, significantly\ntransforming human interaction with intelligent devices across diverse\napplications. However, achieving the desired real-time rendering speeds for\nimmersive interactions is still hindered by (1) the lack of a universal\nalgorithmic solution for different application scenarios and (2) the dedication\nof existing devices or accelerators to merely specific rendering pipelines. To\novercome this challenge, we have developed a unified neural rendering\naccelerator that caters to a wide array of typical neural rendering pipelines,\nenabling real-time and on-device rendering across different applications while\nmaintaining both efficiency and compatibility. Our accelerator design is based\non the insight that, although neural rendering pipelines vary and their\nalgorithm designs are continually evolving, they typically share common\noperators, predominantly executing similar workloads. Building on this insight,\nwe propose a reconfigurable hardware architecture that can dynamically adjust\ndataflow to align with specific rendering metric requirements for diverse\napplications, effectively supporting both typical and the latest hybrid\nrendering pipelines. Benchmarking experiments and ablation studies on both\nsynthetic and real-world scenes demonstrate the effectiveness of the proposed\naccelerator. The proposed unified accelerator stands out as the first solution\ncapable of achieving real-time neural rendering across varied representative\npipelines on edge devices, potentially paving the way for the next generation\nof neural graphics applications."}
{"id": "2503.23733", "pdf": "https://arxiv.org/pdf/2503.23733", "abs": "https://arxiv.org/abs/2503.23733", "authors": ["Yiyang Du", "Xiaochen Wang", "Chi Chen", "Jiabo Ye", "Yiru Wang", "Peng Li", "Ming Yan", "Ji Zhang", "Fei Huang", "Zhifang Sui", "Maosong Sun", "Yang Liu"], "title": "AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models with Unsupervised Coefficient Optimization", "categories": ["cs.CL", "cs.CV"], "comment": "CVPR 2025", "summary": "Recently, model merging methods have demonstrated powerful strengths in\ncombining abilities on various tasks from multiple Large Language Models\n(LLMs). While previous model merging methods mainly focus on merging\nhomogeneous models with identical architecture, they meet challenges when\ndealing with Multimodal Large Language Models (MLLMs) with inherent\nheterogeneous property, including differences in model architecture and the\nasymmetry in the parameter space. In this work, we propose AdaMMS, a novel\nmodel merging method tailored for heterogeneous MLLMs. Our method tackles the\nchallenges in three steps: mapping, merging and searching. Specifically, we\nfirst design mapping function between models to apply model merging on MLLMs\nwith different architecture. Then we apply linear interpolation on model\nweights to actively adapt the asymmetry in the heterogeneous MLLMs. Finally in\nthe hyper-parameter searching step, we propose an unsupervised hyper-parameter\nselection method for model merging. As the first model merging method capable\nof merging heterogeneous MLLMs without labeled data, extensive experiments on\nvarious model combinations demonstrated that AdaMMS outperforms previous model\nmerging methods on various vision-language benchmarks."}
{"id": "2503.23752", "pdf": "https://arxiv.org/pdf/2503.23752", "abs": "https://arxiv.org/abs/2503.23752", "authors": ["Jin Zhou", "Yi Zhou", "Pengfei Xu", "Hui Huang"], "title": "StrokeFusion: Vector Sketch Generation via Joint Stroke-UDF Encoding and Latent Sequence Diffusion", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "In the field of sketch generation, raster-format trained models often produce\nnon-stroke artifacts, while vector-format trained models typically lack a\nholistic understanding of sketches, leading to compromised recognizability.\nMoreover, existing methods struggle to extract common features from similar\nelements (e.g., eyes of animals) appearing at varying positions across\nsketches. To address these challenges, we propose StrokeFusion, a two-stage\nframework for vector sketch generation. It contains a dual-modal sketch feature\nlearning network that maps strokes into a high-quality latent space. This\nnetwork decomposes sketches into normalized strokes and jointly encodes stroke\nsequences with Unsigned Distance Function (UDF) maps, representing sketches as\nsets of stroke feature vectors. Building upon this representation, our\nframework exploits a stroke-level latent diffusion model that simultaneously\nadjusts stroke position, scale, and trajectory during generation. This enables\nhigh-fidelity sketch generation while supporting stroke interpolation editing.\nExtensive experiments on the QuickDraw dataset demonstrate that our framework\noutperforms state-of-the-art techniques, validating its effectiveness in\npreserving structural integrity and semantic features. Code and models will be\nmade publicly available upon publication."}
{"id": "2503.23768", "pdf": "https://arxiv.org/pdf/2503.23768", "abs": "https://arxiv.org/abs/2503.23768", "authors": ["Zhecheng Li", "Guoxian Song", "Yujun Cai", "Zhen Xiong", "Junsong Yuan", "Yiwei Wang"], "title": "Texture or Semantics? Vision-Language Models Get Lost in Font Recognition", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Modern Vision-Language Models (VLMs) exhibit remarkable visual and linguistic\ncapabilities, achieving impressive performance in various tasks such as image\nrecognition and object localization. However, their effectiveness in\nfine-grained tasks remains an open question. In everyday scenarios, individuals\nencountering design materials, such as magazines, typography tutorials,\nresearch papers, or branding content, may wish to identify aesthetically\npleasing fonts used in the text. Given their multimodal capabilities and free\naccessibility, many VLMs are often considered potential tools for font\nrecognition. This raises a fundamental question: Do VLMs truly possess the\ncapability to recognize fonts? To investigate this, we introduce the Font\nRecognition Benchmark (FRB), a compact and well-structured dataset comprising\n15 commonly used fonts. FRB includes two versions: (i) an easy version, where\n10 sentences are rendered in different fonts, and (ii) a hard version, where\neach text sample consists of the names of the 15 fonts themselves, introducing\na stroop effect that challenges model perception. Through extensive evaluation\nof various VLMs on font recognition tasks, we arrive at the following key\nfindings: (i) Current VLMs exhibit limited font recognition capabilities, with\nmany state-of-the-art models failing to achieve satisfactory performance. (ii)\nFew-shot learning and Chain-of-Thought (CoT) prompting provide minimal benefits\nin improving font recognition accuracy across different VLMs. (iii) Attention\nanalysis sheds light on the inherent limitations of VLMs in capturing semantic\nfeatures."}
{"id": "2503.23819", "pdf": "https://arxiv.org/pdf/2503.23819", "abs": "https://arxiv.org/abs/2503.23819", "authors": ["Swarnava Bhattacharyya", "Umapada Pal", "Tapabrata Chakraborti"], "title": "Conformal uncertainty quantification to evaluate predictive fairness of foundation AI model for skin lesion classes across patient demographics", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Deep learning based diagnostic AI systems based on medical images are\nstarting to provide similar performance as human experts. However these data\nhungry complex systems are inherently black boxes and therefore slow to be\nadopted for high risk applications like healthcare. This problem of lack of\ntransparency is exacerbated in the case of recent large foundation models,\nwhich are trained in a self supervised manner on millions of data points to\nprovide robust generalisation across a range of downstream tasks, but the\nembeddings generated from them happen through a process that is not\ninterpretable, and hence not easily trustable for clinical applications. To\naddress this timely issue, we deploy conformal analysis to quantify the\npredictive uncertainty of a vision transformer (ViT) based foundation model\nacross patient demographics with respect to sex, age and ethnicity for the\ntasks of skin lesion classification using several public benchmark datasets.\nThe significant advantage of this method is that conformal analysis is method\nindependent and it not only provides a coverage guarantee at population level\nbut also provides an uncertainty score for each individual. We used a\nmodel-agnostic dynamic F1-score-based sampling during model training, which\nhelped to stabilize the class imbalance and we investigate the effects on\nuncertainty quantification (UQ) with or without this bias mitigation step. Thus\nwe show how this can be used as a fairness metric to evaluate the robustness of\nthe feature embeddings of the foundation model (Google DermFoundation) and thus\nadvance the trustworthiness and fairness of clinical AI."}
{"id": "2503.23877", "pdf": "https://arxiv.org/pdf/2503.23877", "abs": "https://arxiv.org/abs/2503.23877", "authors": ["Junyao Shi", "Zhuolun Zhao", "Tianyou Wang", "Ian Pedroza", "Amy Luo", "Jie Wang", "Jason Ma", "Dinesh Jayaraman"], "title": "ZeroMimic: Distilling Robotic Manipulation Skills from Web Videos", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "ICRA 2025. Project website: https://zeromimic.github.io/", "summary": "Many recent advances in robotic manipulation have come through imitation\nlearning, yet these rely largely on mimicking a particularly hard-to-acquire\nform of demonstrations: those collected on the same robot in the same room with\nthe same objects as the trained policy must handle at test time. In contrast,\nlarge pre-recorded human video datasets demonstrating manipulation skills\nin-the-wild already exist, which contain valuable information for robots. Is it\npossible to distill a repository of useful robotic skill policies out of such\ndata without any additional requirements on robot-specific demonstrations or\nexploration? We present the first such system ZeroMimic, that generates\nimmediately deployable image goal-conditioned skill policies for several common\ncategories of manipulation tasks (opening, closing, pouring, pick&place,\ncutting, and stirring) each capable of acting upon diverse objects and across\ndiverse unseen task setups. ZeroMimic is carefully designed to exploit recent\nadvances in semantic and geometric visual understanding of human videos,\ntogether with modern grasp affordance detectors and imitation policy classes.\nAfter training ZeroMimic on the popular EpicKitchens dataset of ego-centric\nhuman videos, we evaluate its out-of-the-box performance in varied real-world\nand simulated kitchen settings with two different robot embodiments,\ndemonstrating its impressive abilities to handle these varied tasks. To enable\nplug-and-play reuse of ZeroMimic policies on other task setups and robots, we\nrelease software and policy checkpoints of our skill policies."}
{"id": "2503.23893", "pdf": "https://arxiv.org/pdf/2503.23893", "abs": "https://arxiv.org/abs/2503.23893", "authors": ["Maximilian Springenberg", "Noelia Otero", "Yuxin Xue", "Jackie Ma"], "title": "DiffScale: Continuous Downscaling and Bias Correction of Subseasonal Wind Speed Forecasts using Diffusion Models", "categories": ["cs.LG", "cs.AI", "cs.CV", "eess.IV"], "comment": "28 pages, 18 figures, preprint under review", "summary": "Renewable resources are strongly dependent on local and large-scale weather\nsituations. Skillful subseasonal to seasonal (S2S) forecasts -- beyond two\nweeks and up to two months -- can offer significant socioeconomic advantages to\nthe energy sector. This study aims to enhance wind speed predictions using a\ndiffusion model with classifier-free guidance to downscale S2S forecasts of\nsurface wind speed. We propose DiffScale, a diffusion model that super-resolves\nspatial information for continuous downscaling factors and lead times.\nLeveraging weather priors as guidance for the generative process of diffusion\nmodels, we adopt the perspective of conditional probabilities on sampling\nsuper-resolved S2S forecasts. We aim to directly estimate the density\nassociated with the target S2S forecasts at different spatial resolutions and\nlead times without auto-regression or sequence prediction, resulting in an\nefficient and flexible model. Synthetic experiments were designed to\nsuper-resolve wind speed S2S forecasts from the European Center for\nMedium-Range Weather Forecast (ECMWF) from a coarse resolution to a finer\nresolution of ERA5 reanalysis data, which serves as a high-resolution target.\nThe innovative aspect of DiffScale lies in its flexibility to downscale\narbitrary scaling factors, enabling it to generalize across various grid\nresolutions and lead times -without retraining the model- while correcting\nmodel errors, making it a versatile tool for improving S2S wind speed\nforecasts. We achieve a significant improvement in prediction quality,\noutperforming baselines up to week 3."}
{"id": "2503.23898", "pdf": "https://arxiv.org/pdf/2503.23898", "abs": "https://arxiv.org/abs/2503.23898", "authors": ["Rihui Zhang", "Haiming Zhu", "Jingtong Zhao", "Lei Zhang", "Fang-Fang Yin", "Chunhao Wang", "Zhenyu Yang"], "title": "An Explainable Neural Radiomic Sequence Model with Spatiotemporal Continuity for Quantifying 4DCT-based Pulmonary Ventilation", "categories": ["physics.med-ph", "cs.CV"], "comment": "43 pages, 13 figures", "summary": "Accurate evaluation of regional lung ventilation is essential for the\nmanagement and treatment of lung cancer patients, supporting assessments of\npulmonary function, optimization of therapeutic strategies, and monitoring of\ntreatment response. Currently, ventilation scintigraphy using nuclear medicine\ntechniques is widely employed in clinical practice; however, it is often\ntime-consuming, costly, and entails additional radiation exposure. In this\nstudy, we propose an explainable neural radiomic sequence model to identify\nregions of compromised pulmonary ventilation based on four-dimensional computed\ntomography (4DCT). A cohort of 45 lung cancer patients from the VAMPIRE dataset\nwas analyzed. For each patient, lung volumes were segmented from 4DCT, and\nvoxel-wise radiomic features (56-dimensional) were extracted across the\nrespiratory cycle to capture local intensity and texture dynamics, forming\ntemporal radiomic sequences. Ground truth ventilation defects were delineated\nvoxel-wise using Galligas-PET and DTPA-SPECT. To identify compromised regions,\nwe developed a temporal saliency-enhanced explainable long short-term memory\n(LSTM) network trained on the radiomic sequences. Temporal saliency maps were\ngenerated to highlight key features contributing to the model's predictions.\nThe proposed model demonstrated robust performance, achieving average (range)\nDice similarity coefficients of 0.78 (0.74-0.79) for 25 PET cases and 0.78\n(0.74-0.82) for 20 SPECT cases. The temporal saliency map explained three key\nradiomic sequences in ventilation quantification: during lung exhalation,\ncompromised pulmonary function region typically exhibits (1) an increasing\ntrend of intensity and (2) a decreasing trend of homogeneity, in contrast to\nhealthy lung tissue."}
{"id": "2503.23949", "pdf": "https://arxiv.org/pdf/2503.23949", "abs": "https://arxiv.org/abs/2503.23949", "authors": ["Florian Bayer", "Christian Rathgeb"], "title": "AMB-FHE: Adaptive Multi-biometric Fusion with Fully Homomorphic Encryption", "categories": ["cs.CR", "cs.CV"], "comment": null, "summary": "Biometric systems strive to balance security and usability. The use of\nmulti-biometric systems combining multiple biometric modalities is usually\nrecommended for high-security applications. However, the presentation of\nmultiple biometric modalities can impair the user-friendliness of the overall\nsystem and might not be necessary in all cases. In this work, we present a\nsimple but flexible approach to increase the privacy protection of\nhomomorphically encrypted multi-biometric reference templates while enabling\nadaptation to security requirements at run-time: An adaptive multi-biometric\nfusion with fully homomorphic encryption (AMB-FHE). AMB-FHE is benchmarked\nagainst a bimodal biometric database consisting of the CASIA iris and MCYT\nfingerprint datasets using deep neural networks for feature extraction. Our\ncontribution is easy to implement and increases the flexibility of biometric\nauthentication while offering increased privacy protection through joint\nencryption of templates from multiple modalities."}
{"id": "2503.24009", "pdf": "https://arxiv.org/pdf/2503.24009", "abs": "https://arxiv.org/abs/2503.24009", "authors": ["Mikel Zhobro", "Andreas René Geist", "Georg Martius"], "title": "Learning 3D-Gaussian Simulators from RGB Videos", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "Learning physics simulations from video data requires maintaining spatial and\ntemporal consistency, a challenge often addressed with strong inductive biases\nor ground-truth 3D information -- limiting scalability and generalization. We\nintroduce 3DGSim, a 3D physics simulator that learns object dynamics end-to-end\nfrom multi-view RGB videos. It encodes images into a 3D Gaussian particle\nrepresentation, propagates dynamics via a transformer, and renders frames using\n3D Gaussian splatting. By jointly training inverse rendering with a dynamics\ntransformer using a temporal encoding and merging layer, 3DGSimembeds physical\nproperties into point-wise latent vectors without enforcing explicit\nconnectivity constraints. This enables the model to capture diverse physical\nbehaviors, from rigid to elastic and cloth-like interactions, along with\nrealistic lighting effects that also generalize to unseen multi-body\ninteractions and novel scene edits."}
{"id": "2503.24138", "pdf": "https://arxiv.org/pdf/2503.24138", "abs": "https://arxiv.org/abs/2503.24138", "authors": ["Uxue Delaquintana-Aramendi", "Leire Benito-del-Valle", "Aitor Alvarez-Gila", "Javier Pascau", "Luisa F Sánchez-Peralta", "Artzai Picón", "J Blas Pagador", "Cristina L Saratxaga"], "title": "AI-Assisted Colonoscopy: Polyp Detection and Segmentation using Foundation Models", "categories": ["eess.IV", "cs.CV"], "comment": "This work has been submitted to the IEEE TMI for possible publication", "summary": "In colonoscopy, 80% of the missed polyps could be detected with the help of\nDeep Learning models. In the search for algorithms capable of addressing this\nchallenge, foundation models emerge as promising candidates. Their zero-shot or\nfew-shot learning capabilities, facilitate generalization to new data or tasks\nwithout extensive fine-tuning. A concept that is particularly advantageous in\nthe medical imaging domain, where large annotated datasets for traditional\ntraining are scarce. In this context, a comprehensive evaluation of foundation\nmodels for polyp segmentation was conducted, assessing both detection and\ndelimitation. For the study, three different colonoscopy datasets have been\nemployed to compare the performance of five different foundation models,\nDINOv2, YOLO-World, GroundingDINO, SAM and MedSAM, against two benchmark\nnetworks, YOLOv8 and Mask R-CNN. Results show that the success of foundation\nmodels in polyp characterization is highly dependent on domain specialization.\nFor optimal performance in medical applications, domain-specific models are\nessential, and generic models require fine-tuning to achieve effective results.\nThrough this specialization, foundation models demonstrated superior\nperformance compared to state-of-the-art detection and segmentation models,\nwith some models even excelling in zero-shot evaluation; outperforming\nfine-tuned models on unseen data."}
{"id": "2503.24160", "pdf": "https://arxiv.org/pdf/2503.24160", "abs": "https://arxiv.org/abs/2503.24160", "authors": ["Angela Lopez-Cardona", "Parvin Emami", "Sebastian Idesis", "Saravanakumar Duraisamy", "Luis A. Leiva", "Ioannis Arapakis"], "title": "A Comparative Study of Scanpath Models in Graph-Based Visualization", "categories": ["cs.HC", "cs.CV"], "comment": null, "summary": "Information Visualization (InfoVis) systems utilize visual representations to\nenhance data interpretation. Understanding how visual attention is allocated is\nessential for optimizing interface design. However, collecting Eye-tracking\n(ET) data presents challenges related to cost, privacy, and scalability.\nComputational models provide alternatives for predicting gaze patterns, thereby\nadvancing InfoVis research. In our study, we conducted an ET experiment with 40\nparticipants who analyzed graphs while responding to questions of varying\ncomplexity within the context of digital forensics. We compared human scanpaths\nwith synthetic ones generated by models such as DeepGaze, UMSS, and Gazeformer.\nOur research evaluates the accuracy of these models and examines how question\ncomplexity and number of nodes influence performance. This work contributes to\nthe development of predictive modeling in visual analytics, offering insights\nthat can enhance the design and effectiveness of InfoVis systems."}
{"id": "2503.24354", "pdf": "https://arxiv.org/pdf/2503.24354", "abs": "https://arxiv.org/abs/2503.24354", "authors": ["Rana Muhammad Shahroz Khan", "Dongwen Tang", "Pingzhi Li", "Kai Wang", "Tianlong Chen"], "title": "ORAL: Prompting Your Large-Scale LoRAs via Conditional Recurrent Diffusion", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Parameter generation has emerged as a novel paradigm for neural network\ndevelopment, offering an alternative to traditional neural network training by\nsynthesizing high-quality model weights directly. In the context of Low-Rank\nAdaptation (LoRA) for evolving ($\\textit{i.e.}$, constantly updated) large\nlanguage models (LLMs), this approach promises efficient adaptation without\ncostly retraining. However, existing methods face critical limitations in\nsimultaneously achieving scalability and controllability. In this paper, we\nintroduce $\\texttt{ORAL}$, a novel $\\textbf{conditional recurrent diffusion}$\nframework that addresses these challenges. $\\texttt{ORAL}$ incorporates a novel\nconditioning mechanism that integrates model architecture and textual task\nspecifications, enabling the generation of task-specific LoRA parameters that\ncan seamlessly transfer across evolving foundation models. Our approach\nsuccessfully scales to billions-of-parameter LLMs and maintains\ncontrollability. Through extensive experiments across seven language tasks,\nfour vision tasks, and three multimodal tasks using five pre-trained LLMs, we\ndemonstrate that $\\texttt{ORAL}$ generates high-quality LoRA parameters that\nachieve comparable or superior performance to vanilla trained counterparts."}
