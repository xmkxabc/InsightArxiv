{"id": "2504.07981", "pdf": "https://arxiv.org/pdf/2504.07981", "abs": "https://arxiv.org/abs/2504.07981", "authors": ["Kaixin Li", "Ziyang Meng", "Hongzhan Lin", "Ziyang Luo", "Yuchen Tian", "Jing Ma", "Zhiyong Huang", "Tat-Seng Chua"], "title": "ScreenSpot-Pro: GUI Grounding for Professional High-Resolution Computer Use", "categories": ["cs.CV", "cs.HC", "cs.MM", "68-11 68-04", "I.2.7; I.2.10"], "comment": "13pages", "summary": "Recent advancements in Multi-modal Large Language Models (MLLMs) have led to\nsignificant progress in developing GUI agents for general tasks such as web\nbrowsing and mobile phone use. However, their application in professional\ndomains remains under-explored. These specialized workflows introduce unique\nchallenges for GUI perception models, including high-resolution displays,\nsmaller target sizes, and complex environments. In this paper, we introduce\nScreenSpot-Pro, a new benchmark designed to rigorously evaluate the grounding\ncapabilities of MLLMs in high-resolution professional settings. The benchmark\ncomprises authentic high-resolution images from a variety of professional\ndomains with expert annotations. It spans 23 applications across five\nindustries and three operating systems. Existing GUI grounding models perform\npoorly on this dataset, with the best model achieving only 18.9%. Our\nexperiments reveal that strategically reducing the search area enhances\naccuracy. Based on this insight, we propose ScreenSeekeR, a visual search\nmethod that utilizes the GUI knowledge of a strong planner to guide a cascaded\nsearch, achieving state-of-the-art performance with 48.1% without any\nadditional training. We hope that our benchmark and findings will advance the\ndevelopment of GUI agents for professional applications. Code, data and\nleaderboard can be found at https://gui-agent.github.io/grounding-leaderboard."}
{"id": "2504.08003", "pdf": "https://arxiv.org/pdf/2504.08003", "abs": "https://arxiv.org/abs/2504.08003", "authors": ["Ning Li", "Jingran Zhang", "Justin Cui"], "title": "Have we unified image generation and understanding yet? An empirical study of GPT-4o's image generation ability", "categories": ["cs.CV"], "comment": "Early work, technical report", "summary": "OpenAI's multimodal GPT-4o has demonstrated remarkable capabilities in image\ngeneration and editing, yet its ability to achieve world knowledge-informed\nsemantic synthesis--seamlessly integrating domain knowledge, contextual\nreasoning, and instruction adherence--remains unproven. In this study, we\nsystematically evaluate these capabilities across three critical dimensions:\n(1) Global Instruction Adherence, (2) Fine-Grained Editing Precision, and (3)\nPost-Generation Reasoning. While existing benchmarks highlight GPT-4o's strong\ncapabilities in image generation and editing, our evaluation reveals GPT-4o's\npersistent limitations: the model frequently defaults to literal\ninterpretations of instructions, inconsistently applies knowledge constraints,\nand struggles with conditional reasoning tasks. These findings challenge\nprevailing assumptions about GPT-4o's unified understanding and generation\ncapabilities, exposing significant gaps in its dynamic knowledge integration.\nOur study calls for the development of more robust benchmarks and training\nstrategies that go beyond surface-level alignment, emphasizing context-aware\nand reasoning-grounded multimodal generation."}
{"id": "2504.08010", "pdf": "https://arxiv.org/pdf/2504.08010", "abs": "https://arxiv.org/abs/2504.08010", "authors": ["Shuaicheng Niu", "Guohao Chen", "Peilin Zhao", "Tianyi Wang", "Pengcheng Wu", "Zhiqi Shen"], "title": "Self-Bootstrapping for Versatile Test-Time Adaptation", "categories": ["cs.CV", "cs.LG"], "comment": "16 pages, 10 tables, 4 figures", "summary": "In this paper, we seek to develop a versatile test-time adaptation (TTA)\nobjective for a variety of tasks - classification and regression across image-,\nobject-, and pixel-level predictions. We achieve this through a\nself-bootstrapping scheme that optimizes prediction consistency between the\ntest image (as target) and its deteriorated view. The key challenge lies in\ndevising effective augmentations/deteriorations that: i) preserve the image's\ngeometric information, e.g., object sizes and locations, which is crucial for\nTTA on object/pixel-level tasks, and ii) provide sufficient learning signals\nfor TTA. To this end, we analyze how common distribution shifts affect the\nimage's information power across spatial frequencies in the Fourier domain, and\nreveal that low-frequency components carry high power and masking these\ncomponents supplies more learning signals, while masking high-frequency\ncomponents can not. In light of this, we randomly mask the low-frequency\namplitude of an image in its Fourier domain for augmentation. Meanwhile, we\nalso augment the image with noise injection to compensate for missing learning\nsignals at high frequencies, by enhancing the information power there.\nExperiments show that, either independently or as a plug-and-play module, our\nmethod achieves superior results across classification, segmentation, and 3D\nmonocular detection tasks with both transformer and CNN models."}
{"id": "2504.08012", "pdf": "https://arxiv.org/pdf/2504.08012", "abs": "https://arxiv.org/abs/2504.08012", "authors": ["Yuseon Kim", "Kyongseok Park"], "title": "SRVP: Strong Recollection Video Prediction Model Using Attention-Based Spatiotemporal Correlation Fusion", "categories": ["cs.CV"], "comment": "This paper has been accepted to CVPR 2025 Precognition Workshop", "summary": "Video prediction (VP) generates future frames by leveraging spatial\nrepresentations and temporal context from past frames. Traditional recurrent\nneural network (RNN)-based models enhance memory cell structures to capture\nspatiotemporal states over extended durations but suffer from gradual loss of\nobject appearance details. To address this issue, we propose the strong\nrecollection VP (SRVP) model, which integrates standard attention (SA) and\nreinforced feature attention (RFA) modules. Both modules employ scaled\ndot-product attention to extract temporal context and spatial correlations,\nwhich are then fused to enhance spatiotemporal representations. Experiments on\nthree benchmark datasets demonstrate that SRVP mitigates image quality\ndegradation in RNN-based models while achieving predictive performance\ncomparable to RNN-free architectures."}
{"id": "2504.07982", "pdf": "https://arxiv.org/pdf/2504.07982", "abs": "https://arxiv.org/abs/2504.07982", "authors": ["Harishwar Reddy", "Madhusudan Srinivasan", "Upulee Kanewala"], "title": "Metamorphic Testing for Fairness Evaluation in Large Language Models: Identifying Intersectional Bias in LLaMA and GPT", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have made significant strides in Natural\nLanguage Processing but remain vulnerable to fairness-related issues, often\nreflecting biases inherent in their training data. These biases pose risks,\nparticularly when LLMs are deployed in sensitive areas such as healthcare,\nfinance, and law. This paper introduces a metamorphic testing approach to\nsystematically identify fairness bugs in LLMs. We define and apply a set of\nfairness-oriented metamorphic relations (MRs) to assess the LLaMA and GPT\nmodel, a state-of-the-art LLM, across diverse demographic inputs. Our\nmethodology includes generating source and follow-up test cases for each MR and\nanalyzing model responses for fairness violations. The results demonstrate the\neffectiveness of MT in exposing bias patterns, especially in relation to tone\nand sentiment, and highlight specific intersections of sensitive attributes\nthat frequently reveal fairness faults. This research improves fairness testing\nin LLMs, providing a structured approach to detect and mitigate biases and\nimprove model robustness in fairness-sensitive applications."}
{"id": "2504.08019", "pdf": "https://arxiv.org/pdf/2504.08019", "abs": "https://arxiv.org/abs/2504.08019", "authors": ["Qi Bi", "Jingjun Yi", "Hao Zheng", "Haolan Zhan", "Wei Ji", "Yawen Huang", "Yuexiang Li"], "title": "DGFamba: Learning Flow Factorized State Space for Visual Domain Generalization", "categories": ["cs.CV", "cs.AI"], "comment": "accepted by AAAI2025", "summary": "Domain generalization aims to learn a representation from the source domain,\nwhich can be generalized to arbitrary unseen target domains. A fundamental\nchallenge for visual domain generalization is the domain gap caused by the\ndramatic style variation whereas the image content is stable. The realm of\nselective state space, exemplified by VMamba, demonstrates its global receptive\nfield in representing the content. However, the way exploiting the\ndomain-invariant property for selective state space is rarely explored. In this\npaper, we propose a novel Flow Factorized State Space model, dubbed as\nDG-Famba, for visual domain generalization. To maintain domain consistency, we\ninnovatively map the style-augmented and the original state embeddings by flow\nfactorization. In this latent flow space, each state embedding from a certain\nstyle is specified by a latent probability path. By aligning these probability\npaths in the latent space, the state embeddings are able to represent the same\ncontent distribution regardless of the style differences. Extensive experiments\nconducted on various visual domain generalization settings show its\nstate-of-the-art performance."}
{"id": "2504.07983", "pdf": "https://arxiv.org/pdf/2504.07983", "abs": "https://arxiv.org/abs/2504.07983", "authors": ["Shurui Wu", "Xinyi Huang", "Dingxin Lu"], "title": "Psychological Health Knowledge-Enhanced LLM-based Social Network Crisis Intervention Text Transfer Recognition Method", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As the prevalence of mental health crises increases on social media\nplatforms, identifying and preventing potential harm has become an urgent\nchallenge. This study introduces a large language model (LLM)-based text\ntransfer recognition method for social network crisis intervention, enhanced\nwith domain-specific mental health knowledge. We propose a multi-level\nframework that incorporates transfer learning using BERT, and integrates mental\nhealth knowledge, sentiment analysis, and behavior prediction techniques. The\nframework includes a crisis annotation tool trained on social media datasets\nfrom real-world events, enabling the model to detect nuanced emotional cues and\nidentify psychological crises. Experimental results show that the proposed\nmethod outperforms traditional models in crisis detection accuracy and exhibits\ngreater sensitivity to subtle emotional and contextual variations."}
{"id": "2504.08020", "pdf": "https://arxiv.org/pdf/2504.08020", "abs": "https://arxiv.org/abs/2504.08020", "authors": ["Qi Bi", "Jingjun Yi", "Haolan Zhan", "Wei Ji", "Gui-Song Xia"], "title": "Learning Fine-grained Domain Generalization via Hyperbolic State Space Hallucination", "categories": ["cs.CV", "cs.AI"], "comment": "accepted by AAAI2025", "summary": "Fine-grained domain generalization (FGDG) aims to learn a fine-grained\nrepresentation that can be well generalized to unseen target domains when only\ntrained on the source domain data. Compared with generic domain generalization,\nFGDG is particularly challenging in that the fine-grained category can be only\ndiscerned by some subtle and tiny patterns. Such patterns are particularly\nfragile under the cross-domain style shifts caused by illumination, color and\netc. To push this frontier, this paper presents a novel Hyperbolic State Space\nHallucination (HSSH) method. It consists of two key components, namely, state\nspace hallucination (SSH) and hyperbolic manifold consistency (HMC). SSH\nenriches the style diversity for the state embeddings by firstly extrapolating\nand then hallucinating the source images. Then, the pre- and post- style\nhallucinate state embeddings are projected into the hyperbolic manifold. The\nhyperbolic state space models the high-order statistics, and allows a better\ndiscernment of the fine-grained patterns. Finally, the hyperbolic distance is\nminimized, so that the impact of style variation on fine-grained patterns can\nbe eliminated. Experiments on three FGDG benchmarks demonstrate its\nstate-of-the-art performance."}
{"id": "2504.07984", "pdf": "https://arxiv.org/pdf/2504.07984", "abs": "https://arxiv.org/abs/2504.07984", "authors": ["Jianheng Li", "Lirong Chen"], "title": "Topic mining based on fine-tuning Sentence-BERT and LDA", "categories": ["cs.CL", "cs.IR"], "comment": "11 pages, 7 Postscript figures", "summary": "Research background: With the continuous development of society, consumers\npay more attention to the key information of product fine-grained attributes\nwhen shopping. Research purposes: This study will fine tune the Sentence-BERT\nword embedding model and LDA model, mine the subject characteristics in online\nreviews of goods, and show consumers the details of various aspects of goods.\nResearch methods: First, the Sentence-BERT model was fine tuned in the field of\ne-commerce online reviews, and the online review text was converted into a word\nvector set with richer semantic information; Secondly, the vectorized word set\nis input into the LDA model for topic feature extraction; Finally, focus on the\nkey functions of the product through keyword analysis under the theme. Results:\nThis study compared this model with other word embedding models and LDA models,\nand compared it with common topic extraction methods. The theme consistency of\nthis model is 0.5 higher than that of other models, which improves the accuracy\nof theme extraction"}
{"id": "2504.08046", "pdf": "https://arxiv.org/pdf/2504.08046", "abs": "https://arxiv.org/abs/2504.08046", "authors": ["Mia Chiquier", "Orr Avrech", "Yossi Gandelsman", "Berthy Feng", "Katherine Bouman", "Carl Vondrick"], "title": "Teaching Humans Subtle Differences with DIFFusion", "categories": ["cs.CV"], "comment": null, "summary": "Human expertise depends on the ability to recognize subtle visual\ndifferences, such as distinguishing diseases, species, or celestial phenomena.\nWe propose a new method to teach novices how to differentiate between nuanced\ncategories in specialized domains. Our method uses generative models to\nvisualize the minimal change in features to transition between classes, i.e.,\ncounterfactuals, and performs well even in domains where data is sparse,\nexamples are unpaired, and category boundaries are not easily explained by\ntext. By manipulating the conditioning space of diffusion models, our proposed\nmethod DIFFusion disentangles category structure from instance identity,\nenabling high-fidelity synthesis even in challenging domains. Experiments\nacross six domains show accurate transitions even with limited and unpaired\nexamples across categories. User studies confirm that our generated\ncounterfactuals outperform unpaired examples in teaching perceptual expertise,\nshowing the potential of generative models for specialized visual learning."}
{"id": "2504.07986", "pdf": "https://arxiv.org/pdf/2504.07986", "abs": "https://arxiv.org/abs/2504.07986", "authors": ["Runjin Chen", "Zhenyu Zhang", "Junyuan Hong", "Souvik Kundu", "Zhangyang Wang"], "title": "SEAL: Steerable Reasoning Calibration of Large Language Models for Free", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs), such as OpenAI's o1-series have demonstrated\ncompelling capabilities for complex reasoning tasks via the extended\nchain-of-thought (CoT) reasoning mechanism. However, recent studies reveal\nsubstantial redundancy in the CoT reasoning traces, which not only increases\ninference latency but also negatively impacts model performance by diverting\nattention to unnecessary reasoning paths. To address this issue, we investigate\nthe internal reasoning structures of LLMs and categorize them into three\nprimary thought types: execution, reflection, and transition thoughts.\nMoreover, our analysis reveals that excessive reflection and transition\nthoughts are strongly correlated with failure cases and these thought\ncategories exhibit clear separation in the latent space. Based on these, we\nintroduce SEAL (Steerable reasoning calibration), a training-free approach that\nseamlessly calibrates the CoT process, improving accuracy while demonstrating\nsignificant efficiency gains. SEAL consists of an offline stage for extracting\nthe reasoning steering vector in the latent space, followed by an on-the-fly\ncalibration of the reasoning trace through representation intervention using\nthe steering vector. Notably, the steering vector exhibits strong\ntransferability across various tasks. Extensive experiments across multiple\nmodels (DeepSeek-R1-Distill and QwQ-32B-Preview) and benchmarks (Math500,\nGSM8K, LiveCodeBench) validate the effectiveness of SEAL, up to a 11%\nimprovement in accuracy while reducing reasoning tokens by 11.8% to 50.4%. Our\ncode is publicly available at https://github.com/VITA-Group/SEAL."}
{"id": "2504.08049", "pdf": "https://arxiv.org/pdf/2504.08049", "abs": "https://arxiv.org/abs/2504.08049", "authors": ["Angelina Ibarra", "Joshua Peeples"], "title": "Patch distribution modeling framework adaptive cosine estimator (PaDiM-ACE) for anomaly detection and localization in synthetic aperture radar imagery", "categories": ["cs.CV"], "comment": "Accepted to SPIE, Defense and Commercial Sensing, Algorithms for\n  Synthetic Aperture Radar Imagery XXXII (April 2025)", "summary": "This work presents a new approach to anomaly detection and localization in\nsynthetic aperture radar imagery (SAR), expanding upon the existing patch\ndistribution modeling framework (PaDiM). We introduce the adaptive cosine\nestimator (ACE) detection statistic. PaDiM uses the Mahalanobis distance at\ninference, an unbounded metric. ACE instead uses the cosine similarity metric,\nproviding bounded anomaly detection scores. The proposed method is evaluated\nacross multiple SAR datasets, with performance metrics including the area under\nthe receiver operating curve (AUROC) at the image and pixel level, aiming for\nincreased performance in anomaly detection and localization of SAR imagery. The\ncode is publicly available:\nhttps://github.com/Advanced-Vision-and-Learning-Lab/PaDiM-LACE."}
{"id": "2504.07989", "pdf": "https://arxiv.org/pdf/2504.07989", "abs": "https://arxiv.org/abs/2504.07989", "authors": ["Nirvan Patil", "Malhar Abhay Inamdar", "Agnivo Gosai", "Guruprasad Pathak", "Anish Joshi", "Aryan Sagavekar", "Anish Joshirao", "Raj Dandekar", "Rajat Dandekar", "Sreedath Panat"], "title": "Regional Tiny Stories: Using Small Models to Compare Language Learning and Tokenizer Performance", "categories": ["cs.CL", "cs.AI"], "comment": "34 pages, 24 figures, 16 tables", "summary": "Small Language Models (SLMs) offer efficient alternatives to LLMs for\nspecific domains. The 2023 TinyStories study developed an English dataset that\nallows SLMs with 1 to 10 million parameters to produce coherent outputs. Our\nresearch expands this framework by translating the original dataset into Indian\nlanguages and creating synthetic data using LLMs. We focus on Hindi, Marathi,\nand Bengali, evaluating SLMs for regional language processing and understanding\nlinguistic complexity. We show that SLMs efficiently process regional languages\nwith significantly fewer parameters than LLMs, providing a complementary\nframework for ``inference based evaluation\" of tokenization strategies and\nlinguistic complexity. Our analysis shows that language-specific tokenizers\noutperform general-purpose ones for Indian languages. Empirical validations,\nsupported by information-theoretic and morphological analyses, provides\nfundamental understanding behind the better performance of Hindi models over\nMarathi and Bengali. Additionally, we show that synthetic datasets outperform\ntranslated content for training SLMs. Correlation analyses reveal\ncross-linguistic patterns and language-specific relationships between\ncreativity, grammatical precision, and narrative completeness. These findings\nadvance both the practical application of SLMs to underserved languages and our\ntheoretical understanding of neural language development."}
{"id": "2504.08054", "pdf": "https://arxiv.org/pdf/2504.08054", "abs": "https://arxiv.org/abs/2504.08054", "authors": ["Meilun Zhou", "Aditya Dutt", "Alina Zare"], "title": "Multi-Task Learning with Multi-Annotation Triplet Loss for Improved Object Detection", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted for Oral Presentation at the 45th IEEE International\n  Geoscience and Remote Sensing Symposium (IGARSS), 2025, Brisbane, Australia.\n  4 pages and 4 figures", "summary": "Triplet loss traditionally relies only on class labels and does not use all\navailable information in multi-task scenarios where multiple types of\nannotations are available. This paper introduces a Multi-Annotation Triplet\nLoss (MATL) framework that extends triplet loss by incorporating additional\nannotations, such as bounding box information, alongside class labels in the\nloss formulation. By using these complementary annotations, MATL improves\nmulti-task learning for tasks requiring both classification and localization.\nExperiments on an aerial wildlife imagery dataset demonstrate that MATL\noutperforms conventional triplet loss in both classification and localization.\nThese findings highlight the benefit of using all available annotations for\ntriplet loss in multi-task learning frameworks."}
{"id": "2504.07992", "pdf": "https://arxiv.org/pdf/2504.07992", "abs": "https://arxiv.org/abs/2504.07992", "authors": ["Seth Drake"], "title": "'Neural howlround' in large language models: a self-reinforcing bias phenomenon, and a dynamic attenuation solution", "categories": ["cs.CL", "cs.AI", "cs.NE"], "comment": "27 pages, 3 figures, 2 tables,", "summary": "Large language model (LLM)-driven AI systems may exhibit an inference failure\nmode we term `neural howlround,' a self-reinforcing cognitive loop where\ncertain highly weighted inputs become dominant, leading to entrenched response\npatterns resistant to correction. This paper explores the mechanisms underlying\nthis phenomenon, which is distinct from model collapse and biased salience\nweighting. We propose an attenuation-based correction mechanism that\ndynamically introduces counterbalancing adjustments and can restore adaptive\nreasoning, even in `locked-in' AI systems. Additionally, we discuss some other\nrelated effects arising from improperly managed reinforcement. Finally, we\noutline potential applications of this mitigation strategy for improving AI\nrobustness in real-world decision-making tasks."}
{"id": "2504.08061", "pdf": "https://arxiv.org/pdf/2504.08061", "abs": "https://arxiv.org/abs/2504.08061", "authors": ["Kai Hu", "Zhidan Zhao", "Zhifeng Hao"], "title": "STEI-PCN: an efficient pure convolutional network for traffic prediction via spatial-temporal encoding and inferring", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Traffic data exhibits complex temporal, spatial, and spatial-temporal\ncorrelations. Most of models use either independent modules to separately\nextract temporal and spatial correlations or joint modules to synchronously\nextract them, without considering the spatial-temporal correlations. Moreover,\nmodels that consider joint spatial-temporal correlations (temporal, spatial,\nand spatial-temporal correlations) often encounter significant challenges in\naccuracy and computational efficiency which prevent such models from\ndemonstrating the expected advantages of a joint spatial-temporal correlations\narchitecture. To address these issues, this paper proposes an efficient pure\nconvolutional network for traffic prediction via spatial-temporal encoding and\ninferring (STEI-PCN). The model introduces and designs a dynamic adjacency\nmatrix inferring module based on absolute spatial and temporal coordinates, as\nwell as relative spatial and temporal distance encoding, using a graph\nconvolutional network combined with gating mechanism to capture local\nsynchronous joint spatial-temporal correlations. Additionally, three layers of\ntemporal dilated causal convolutional network are used to capture long-range\ntemporal correlations. Finally, through multi-view collaborative prediction\nmodule, the model integrates the gated-activated original, local synchronous\njoint spatial-temporal, and long-range temporal features to achieve\ncomprehensive prediction. This study conducts extensive experiments on flow\ndatasets (PeMS03/04/07/08) and speed dataset (PeMS-Bay), covering multiple\nprediction horizons. The results show that STEI-PCN demonstrates competitive\ncomputational efficiency in both training and inference speeds, and achieves\nsuperior or slightly inferior to state-of-the-art (SOTA) models on most\nevaluation metrics."}
{"id": "2504.07994", "pdf": "https://arxiv.org/pdf/2504.07994", "abs": "https://arxiv.org/abs/2504.07994", "authors": ["Samah Alkhuzaey", "Floriana Grasso", "Terry R. Payne", "Valentina Tamma"], "title": "Evaluating the Fitness of Ontologies for the Task of Question Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Ontology-based question generation is an important application of\nsemantic-aware systems that enables the creation of large question banks for\ndiverse learning environments. The effectiveness of these systems, both in\nterms of the calibre and cognitive difficulty of the resulting questions,\ndepends heavily on the quality and modelling approach of the underlying\nontologies, making it crucial to assess their fitness for this task. To date,\nthere has been no comprehensive investigation into the specific ontology\naspects or characteristics that affect the question generation process.\nTherefore, this paper proposes a set of requirements and task-specific metrics\nfor evaluating the fitness of ontologies for question generation tasks in\npedagogical settings. Using the ROMEO methodology, a structured framework for\nderiving task-specific metrics, an expert-based approach is employed to assess\nthe performance of various ontologies in Automatic Question Generation (AQG)\ntasks, which is then evaluated over a set of ontologies. Our results\ndemonstrate that ontology characteristics significantly impact the\neffectiveness of question generation, with different ontologies exhibiting\nvarying performance levels. This highlights the importance of assessing\nontology quality with respect to AQG tasks."}
{"id": "2504.08072", "pdf": "https://arxiv.org/pdf/2504.08072", "abs": "https://arxiv.org/abs/2504.08072", "authors": ["Sushant Gautam", "Jingdao Chen"], "title": "X-DECODE: EXtreme Deblurring with Curriculum Optimization and Domain Equalization", "categories": ["cs.CV"], "comment": null, "summary": "Restoring severely blurred images remains a significant challenge in computer\nvision, impacting applications in autonomous driving, medical imaging, and\nphotography. This paper introduces a novel training strategy based on\ncurriculum learning to improve the robustness of deep learning models for\nextreme image deblurring. Unlike conventional approaches that train on only low\nto moderate blur levels, our method progressively increases the difficulty by\nintroducing images with higher blur severity over time, allowing the model to\nadapt incrementally. Additionally, we integrate perceptual and hinge loss\nduring training to enhance fine detail restoration and improve training\nstability. We experimented with various curriculum learning strategies and\nexplored the impact of the train-test domain gap on the deblurring performance.\nExperimental results on the Extreme-GoPro dataset showed that our method\noutperforms the next best method by 14% in SSIM, whereas experiments on the\nExtreme-KITTI dataset showed that our method outperforms the next best by 18%\nin SSIM. Ablation studies showed that a linear curriculum progression\noutperforms step-wise, sigmoid, and exponential progressions, while\nhyperparameter settings such as the training blur percentage and loss function\nformulation all play important roles in addressing extreme blur artifacts.\nDatasets and code are available at https://github.com/RAPTOR-MSSTATE/XDECODE"}
{"id": "2504.07995", "pdf": "https://arxiv.org/pdf/2504.07995", "abs": "https://arxiv.org/abs/2504.07995", "authors": ["Biplav Srivastava", "Kausik Lakkaraju", "Nitin Gupta", "Vansh Nagpal", "Bharath C. Muppasani", "Sara E. Jones"], "title": "SafeChat: A Framework for Building Trustworthy Collaborative Assistants and a Case Study of its Usefulness", "categories": ["cs.CL"], "comment": null, "summary": "Collaborative assistants, or chatbots, are data-driven decision support\nsystems that enable natural interaction for task completion. While they can\nmeet critical needs in modern society, concerns about their reliability and\ntrustworthiness persist. In particular, Large Language Model (LLM)-based\nchatbots like ChatGPT, Gemini, and DeepSeek are becoming more accessible.\nHowever, such chatbots have limitations, including their inability to explain\nresponse generation, the risk of generating problematic content, the lack of\nstandardized testing for reliability, and the need for deep AI expertise and\nextended development times. These issues make chatbots unsuitable for\ntrust-sensitive applications like elections or healthcare. To address these\nconcerns, we introduce SafeChat, a general architecture for building safe and\ntrustworthy chatbots, with a focus on information retrieval use cases. Key\nfeatures of SafeChat include: (a) safety, with a domain-agnostic design where\nresponses are grounded and traceable to approved sources (provenance), and\n'do-not-respond' strategies to prevent harmful answers; (b) usability, with\nautomatic extractive summarization of long responses, traceable to their\nsources, and automated trust assessments to communicate expected chatbot\nbehavior, such as sentiment; and (c) fast, scalable development, including a\nCSV-driven workflow, automated testing, and integration with various devices.\nWe implemented SafeChat in an executable framework using the open-source\nchatbot platform Rasa. A case study demonstrates its application in building\nElectionBot-SC, a chatbot designed to safely disseminate official election\ninformation. SafeChat is being used in many domains, validating its potential,\nand is available at: https://github.com/ai4society/trustworthy-chatbot."}
{"id": "2504.08100", "pdf": "https://arxiv.org/pdf/2504.08100", "abs": "https://arxiv.org/abs/2504.08100", "authors": ["Junbang Liu", "Enpei Huang", "Dongxing Mao", "Hui Zhang", "Xinyuan Song", "Yongxin Ni"], "title": "ContrastiveGaussian: High-Fidelity 3D Generation with Contrastive Learning and Gaussian Splatting", "categories": ["cs.CV"], "comment": "Code will be available at\n  https://github.com/YaNLlan-ljb/ContrastiveGaussian", "summary": "Creating 3D content from single-view images is a challenging problem that has\nattracted considerable attention in recent years. Current approaches typically\nutilize score distillation sampling (SDS) from pre-trained 2D diffusion models\nto generate multi-view 3D representations. Although some methods have made\nnotable progress by balancing generation speed and model quality, their\nperformance is often limited by the visual inconsistencies of the diffusion\nmodel outputs. In this work, we propose ContrastiveGaussian, which integrates\ncontrastive learning into the generative process. By using a perceptual loss,\nwe effectively differentiate between positive and negative samples, leveraging\nthe visual inconsistencies to improve 3D generation quality. To further enhance\nsample differentiation and improve contrastive learning, we incorporate a\nsuper-resolution model and introduce another Quantity-Aware Triplet Loss to\naddress varying sample distributions during training. Our experiments\ndemonstrate that our approach achieves superior texture fidelity and improved\ngeometric consistency."}
{"id": "2504.07997", "pdf": "https://arxiv.org/pdf/2504.07997", "abs": "https://arxiv.org/abs/2504.07997", "authors": ["Tian Xie", "Tongxin Yin", "Vaishakh Keshava", "Xueru Zhang", "Siddhartha Reddy Jonnalagadda"], "title": "BiasCause: Evaluate Socially Biased Causal Reasoning of Large Language Models", "categories": ["cs.CL"], "comment": "This work has been done when the first author is at Google. The first\n  author is a student at the Ohio State University", "summary": "While large language models (LLMs) already play significant roles in society,\nresearch has shown that LLMs still generate content including social bias\nagainst certain sensitive groups. While existing benchmarks have effectively\nidentified social biases in LLMs, a critical gap remains in our understanding\nof the underlying reasoning that leads to these biased outputs. This paper goes\none step further to evaluate the causal reasoning process of LLMs when they\nanswer questions eliciting social biases. We first propose a novel conceptual\nframework to classify the causal reasoning produced by LLMs. Next, we use LLMs\nto synthesize $1788$ questions covering $8$ sensitive attributes and manually\nvalidate them. The questions can test different kinds of causal reasoning by\nletting LLMs disclose their reasoning process with causal graphs. We then test\n4 state-of-the-art LLMs. All models answer the majority of questions with\nbiased causal reasoning, resulting in a total of $4135$ biased causal graphs.\nMeanwhile, we discover $3$ strategies for LLMs to avoid biased causal reasoning\nby analyzing the \"bias-free\" cases. Finally, we reveal that LLMs are also prone\nto \"mistaken-biased\" causal reasoning, where they first confuse correlation\nwith causality to infer specific sensitive group names and then incorporate\nbiased causal reasoning."}
{"id": "2504.08110", "pdf": "https://arxiv.org/pdf/2504.08110", "abs": "https://arxiv.org/abs/2504.08110", "authors": ["Muhammad Saif Ullah Khan", "Stephan Krauß", "Didier Stricker"], "title": "Towards Unconstrained 2D Pose Estimation of the Human Spine", "categories": ["cs.CV"], "comment": "Accepted for publication in CVPRW 2025", "summary": "We present SpineTrack, the first comprehensive dataset for 2D spine pose\nestimation in unconstrained settings, addressing a crucial need in sports\nanalytics, healthcare, and realistic animation. Existing pose datasets often\nsimplify the spine to a single rigid segment, overlooking the nuanced\narticulation required for accurate motion analysis. In contrast, SpineTrack\nannotates nine detailed spinal keypoints across two complementary subsets: a\nsynthetic set comprising 25k annotations created using Unreal Engine with\nbiomechanical alignment through OpenSim, and a real-world set comprising over\n33k annotations curated via an active learning pipeline that iteratively\nrefines automated annotations with human feedback. This integrated approach\nensures anatomically consistent labels at scale, even for challenging,\nin-the-wild images. We further introduce SpinePose, extending state-of-the-art\nbody pose estimators using knowledge distillation and an anatomical\nregularization strategy to jointly predict body and spine keypoints. Our\nexperiments in both general and sports-specific contexts validate the\neffectiveness of SpineTrack for precise spine pose estimation, establishing a\nrobust foundation for future research in advanced biomechanical analysis and 3D\nspine reconstruction in the wild."}
{"id": "2504.08001", "pdf": "https://arxiv.org/pdf/2504.08001", "abs": "https://arxiv.org/abs/2504.08001", "authors": ["Miguel López-Otal", "Jorge Gracia", "Jordi Bernad", "Carlos Bobed", "Lucía Pitarch-Ballesteros", "Emma Anglés-Herrero"], "title": "Linguistic Interpretability of Transformer-based Language Models: a systematic review", "categories": ["cs.CL", "I.2.7"], "comment": "Supplementary material:\n  https://github.com/sid-unizar/ling-int-survey/blob/main/table.pdf", "summary": "Language models based on the Transformer architecture achieve excellent\nresults in many language-related tasks, such as text classification or\nsentiment analysis. However, despite the architecture of these models being\nwell-defined, little is known about how their internal computations help them\nachieve their results. This renders these models, as of today, a type of 'black\nbox' systems. There is, however, a line of research -- 'interpretability' --\naiming to learn how information is encoded inside these models. More\nspecifically, there is work dedicated to studying whether Transformer-based\nmodels possess knowledge of linguistic phenomena similar to human speakers --\nan area we call 'linguistic interpretability' of these models. In this survey\nwe present a comprehensive analysis of 160 research works, spread across\nmultiple languages and models -- including multilingual ones -- that attempt to\ndiscover linguistic information from the perspective of several traditional\nLinguistics disciplines: Syntax, Morphology, Lexico-Semantics and Discourse.\nOur survey fills a gap in the existing interpretability literature, which\neither not focus on linguistic knowledge in these models or present some\nlimitations -- e.g. only studying English-based models. Our survey also focuses\non Pre-trained Language Models not further specialized for a downstream task,\nwith an emphasis on works that use interpretability techniques that explore\nmodels' internal representations."}
{"id": "2504.08111", "pdf": "https://arxiv.org/pdf/2504.08111", "abs": "https://arxiv.org/abs/2504.08111", "authors": ["Marco Schouten", "Mehmet Onurcan Kaya", "Serge Belongie", "Dim P. Papadopoulos"], "title": "POEM: Precise Object-level Editing via MLLM control", "categories": ["cs.CV"], "comment": "Accepted to SCIA 2025", "summary": "Diffusion models have significantly improved text-to-image generation,\nproducing high-quality, realistic images from textual descriptions. Beyond\ngeneration, object-level image editing remains a challenging problem, requiring\nprecise modifications while preserving visual coherence. Existing text-based\ninstructional editing methods struggle with localized shape and layout\ntransformations, often introducing unintended global changes. Image\ninteraction-based approaches offer better accuracy but require manual human\neffort to provide precise guidance. To reduce this manual effort while\nmaintaining a high image editing accuracy, in this paper, we propose POEM, a\nframework for Precise Object-level Editing using Multimodal Large Language\nModels (MLLMs). POEM leverages MLLMs to analyze instructional prompts and\ngenerate precise object masks before and after transformation, enabling\nfine-grained control without extensive user input. This structured reasoning\nstage guides the diffusion-based editing process, ensuring accurate object\nlocalization and transformation. To evaluate our approach, we introduce\nVOCEdits, a benchmark dataset based on PASCAL VOC 2012, augmented with\ninstructional edit prompts, ground-truth transformations, and precise object\nmasks. Experimental results show that POEM outperforms existing text-based\nimage editing approaches in precision and reliability while reducing manual\neffort compared to interaction-based methods."}
{"id": "2504.08002", "pdf": "https://arxiv.org/pdf/2504.08002", "abs": "https://arxiv.org/abs/2504.08002", "authors": ["Tong Piao", "Pei Tang", "Zhipeng Zhang", "Jiaqi Li", "Qiao Liu", "Zufeng Wu"], "title": "More diverse more adaptive: Comprehensive Multi-task Learning for Improved LLM Domain Adaptation in E-commerce", "categories": ["cs.CL"], "comment": "Accepted by KDD workshop 2024", "summary": "In recent years, Large Language Models (LLMs) have been widely applied across\nvarious domains due to their powerful domain adaptation capabilities. Previous\nstudies have suggested that diverse, multi-modal data can enhance LLMs' domain\nadaptation performance. However, this hypothesis remains insufficiently\nvalidated in the e-commerce sector. To address this gap, we propose a\ncomprehensive e-commerce multi-task framework and design empirical experiments\nto examine the impact of diverse data and tasks on LLMs from two perspectives:\n\"capability comprehensiveness\" and \"task comprehensiveness.\" Specifically, we\nobserve significant improvements in LLM performance by progressively\nintroducing tasks related to new major capability areas and by continuously\nadding subtasks within different major capability domains. Furthermore, we\nobserve that increasing model capacity amplifies the benefits of diversity,\nsuggesting a synergistic relationship between model capacity and data\ndiversity. Finally, we validate the best-performing model from our empirical\nexperiments in the KDD Cup 2024, achieving a rank 5 in Task 1. This outcome\ndemonstrates the significance of our research for advancing LLMs in the\ne-commerce domain."}
{"id": "2504.08115", "pdf": "https://arxiv.org/pdf/2504.08115", "abs": "https://arxiv.org/abs/2504.08115", "authors": ["Lucian Chauvina", "Somil Guptac", "Angelina Ibarrac", "Joshua Peeples"], "title": "Benchmarking Suite for Synthetic Aperture Radar Imagery Anomaly Detection (SARIAD) Algorithms", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted to SPIE at:\n  https://spie.org/defense-commercial-sensing/presentation/Benchmarking-suite-for-synthetic-aperture-radar-imagery-anomaly-detection-SARIAD/13456-3", "summary": "Anomaly detection is a key research challenge in computer vision and machine\nlearning with applications in many fields from quality control to radar\nimaging. In radar imaging, specifically synthetic aperture radar (SAR), anomaly\ndetection can be used for the classification, detection, and segmentation of\nobjects of interest. However, there is no method for developing and\nbenchmarking these methods on SAR imagery. To address this issue, we introduce\nSAR imagery anomaly detection (SARIAD). In conjunction with Anomalib, a\ndeep-learning library for anomaly detection, SARIAD provides a comprehensive\nsuite of algorithms and datasets for assessing and developing anomaly detection\napproaches on SAR imagery. SARIAD specifically integrates multiple SAR datasets\nalong with tools to effectively apply various anomaly detection algorithms to\nSAR imagery. Several anomaly detection metrics and visualizations are\navailable. Overall, SARIAD acts as a central package for benchmarking SAR\nmodels and datasets to allow for reproducible research in the field of anomaly\ndetection in SAR imagery. This package is publicly available:\nhttps://github.com/Advanced-Vision-and-Learning-Lab/SARIAD."}
{"id": "2504.08024", "pdf": "https://arxiv.org/pdf/2504.08024", "abs": "https://arxiv.org/abs/2504.08024", "authors": ["Fabian Retkowski", "Maike Züfle", "Andreas Sudmann", "Dinah Pfau", "Jan Niehues", "Alexander Waibel"], "title": "From Speech to Summary: A Comprehensive Survey of Speech Summarization", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Speech summarization has become an essential tool for efficiently managing\nand accessing the growing volume of spoken and audiovisual content. However,\ndespite its increasing importance, speech summarization is still not clearly\ndefined and intersects with several research areas, including speech\nrecognition, text summarization, and specific applications like meeting\nsummarization. This survey not only examines existing datasets and evaluation\nmethodologies, which are crucial for assessing the effectiveness of\nsummarization approaches but also synthesizes recent developments in the field,\nhighlighting the shift from traditional systems to advanced models like\nfine-tuned cascaded architectures and end-to-end solutions."}
{"id": "2504.08125", "pdf": "https://arxiv.org/pdf/2504.08125", "abs": "https://arxiv.org/abs/2504.08125", "authors": ["Shalini Maiti", "Lourdes Agapito", "Filippos Kokkinos"], "title": "Gen3DEval: Using vLLMs for Automatic Evaluation of Generated 3D Objects", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Rapid advancements in text-to-3D generation require robust and scalable\nevaluation metrics that align closely with human judgment, a need unmet by\ncurrent metrics such as PSNR and CLIP, which require ground-truth data or focus\nonly on prompt fidelity. To address this, we introduce Gen3DEval, a novel\nevaluation framework that leverages vision large language models (vLLMs)\nspecifically fine-tuned for 3D object quality assessment. Gen3DEval evaluates\ntext fidelity, appearance, and surface quality by analyzing 3D surface normals,\nwithout requiring ground-truth comparisons, bridging the gap between automated\nmetrics and user preferences. Compared to state-of-the-art task-agnostic\nmodels, Gen3DEval demonstrates superior performance in user-aligned\nevaluations, placing it as a comprehensive and accessible benchmark for future\nresearch on text-to-3D generation. The project page can be found here:\n\\href{https://shalini-maiti.github.io/gen3deval.github.io/}{https://shalini-maiti.github.io/gen3deval.github.io/}."}
{"id": "2504.08040", "pdf": "https://arxiv.org/pdf/2504.08040", "abs": "https://arxiv.org/abs/2504.08040", "authors": ["Akram Mustafa", "Usman Naseem", "Mostafa Rahimi Azghadi"], "title": "Can Reasoning LLMs Enhance Clinical Document Classification?", "categories": ["cs.CL", "cs.AI"], "comment": "28 pages, 13 tables, 12 figures", "summary": "Clinical document classification is essential for converting unstructured\nmedical texts into standardised ICD-10 diagnoses, yet it faces challenges due\nto complex medical language, privacy constraints, and limited annotated\ndatasets. Large Language Models (LLMs) offer promising improvements in accuracy\nand efficiency for this task. This study evaluates the performance and\nconsistency of eight LLMs; four reasoning (Qwen QWQ, Deepseek Reasoner, GPT o3\nMini, Gemini 2.0 Flash Thinking) and four non-reasoning (Llama 3.3, GPT 4o\nMini, Gemini 2.0 Flash, Deepseek Chat); in classifying clinical discharge\nsummaries using the MIMIC-IV dataset. Using cTAKES to structure clinical\nnarratives, models were assessed across three experimental runs, with majority\nvoting determining final predictions. Results showed that reasoning models\noutperformed non-reasoning models in accuracy (71% vs 68%) and F1 score (67% vs\n60%), with Gemini 2.0 Flash Thinking achieving the highest accuracy (75%) and\nF1 score (76%). However, non-reasoning models demonstrated greater stability\n(91% vs 84% consistency). Performance varied across ICD-10 codes, with\nreasoning models excelling in complex cases but struggling with abstract\ncategories. Findings indicate a trade-off between accuracy and consistency,\nsuggesting that a hybrid approach could optimise clinical coding. Future\nresearch should explore multi-label classification, domain-specific\nfine-tuning, and ensemble methods to enhance model reliability in real-world\napplications."}
{"id": "2504.08140", "pdf": "https://arxiv.org/pdf/2504.08140", "abs": "https://arxiv.org/abs/2504.08140", "authors": ["Cherish Puniani", "Advika Sinha", "Shree Singhi", "Aayan Yadav"], "title": "Impact of Language Guidance: A Reproducibility Study", "categories": ["cs.CV"], "comment": null, "summary": "Modern deep-learning architectures need large amounts of data to produce\nstate-of-the-art results. Annotating such huge datasets is time-consuming,\nexpensive, and prone to human error. Recent advances in self-supervised\nlearning allow us to train huge models without explicit annotation. Contrastive\nlearning is a popular paradigm in self-supervised learning. Recent works like\nSimCLR and CLIP rely on image augmentations or directly minimizing cross-modal\nloss between image and text. Banani et al. (2023) propose to use language\nguidance to sample view pairs. They claim that language enables better\nconceptual similarity, eliminating the effects of visual variability. We\nreproduce their experiments to verify their claims and find that their dataset,\nRedCaps, contains low-quality captions. We use an off-the-shelf image\ncaptioning model, BLIP-2, to replace the captions and improve performance, and\nwe also devise a new metric to evaluate the semantic capabilities of\nself-supervised models based on interpretability methods."}
{"id": "2504.08102", "pdf": "https://arxiv.org/pdf/2504.08102", "abs": "https://arxiv.org/abs/2504.08102", "authors": ["Ingryd V. S. T. Pereira", "George D. C. Cavalcanti", "Rafael M. O. Cruz"], "title": "Multi-view autoencoders for Fake News Detection", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by IEEE Symposium Series on Computational Intelligence -\n  IEEE SSCI 2025", "summary": "Given the volume and speed at which fake news spreads across social media,\nautomatic fake news detection has become a highly important task. However, this\ntask presents several challenges, including extracting textual features that\ncontain relevant information about fake news. Research about fake news\ndetection shows that no single feature extraction technique consistently\noutperforms the others across all scenarios. Nevertheless, different feature\nextraction techniques can provide complementary information about the textual\ndata and enable a more comprehensive representation of the content. This paper\nproposes using multi-view autoencoders to generate a joint feature\nrepresentation for fake news detection by integrating several feature\nextraction techniques commonly used in the literature. Experiments on fake news\ndatasets show a significant improvement in classification performance compared\nto individual views (feature representations). We also observed that selecting\na subset of the views instead of composing a latent space with all the views\ncan be advantageous in terms of accuracy and computational effort. For further\ndetails, including source codes, figures, and datasets, please refer to the\nproject's repository: https://github.com/ingrydpereira/multiview-fake-news."}
{"id": "2504.08149", "pdf": "https://arxiv.org/pdf/2504.08149", "abs": "https://arxiv.org/abs/2504.08149", "authors": ["Danielle Sullivan-Pao", "Nicole Tian", "Pooya Khorrami"], "title": "LoRAX: LoRA eXpandable Networks for Continual Synthetic Image Attribution", "categories": ["cs.CV"], "comment": null, "summary": "As generative AI image technologies become more widespread and advanced,\nthere is a growing need for strong attribution models. These models are crucial\nfor verifying the authenticity of images and identifying the architecture of\ntheir originating generative models-key to maintaining media integrity.\nHowever, attribution models struggle to generalize to unseen models, and\ntraditional fine-tuning methods for updating these models have shown to be\nimpractical in real-world settings. To address these challenges, we propose\nLoRA eXpandable Networks (LoRAX), a parameter-efficient class incremental\nalgorithm that adapts to novel generative image models without the need for\nfull retraining. Our approach trains an extremely parameter-efficient feature\nextractor per continual learning task via Low Rank Adaptation. Each\ntask-specific feature extractor learns distinct features while only requiring a\nsmall fraction of the parameters present in the underlying feature extractor's\nbackbone model. Our extensive experimentation shows LoRAX outperforms or\nremains competitive with state-of-the-art class incremental learning algorithms\non the Continual Deepfake Detection benchmark across all training scenarios and\nmemory settings, while requiring less than 3% of the number of trainable\nparameters per feature extractor compared to the full-rank implementation.\nLoRAX code is available at: https://github.com/mit-ll/lorax_cil."}
{"id": "2504.08120", "pdf": "https://arxiv.org/pdf/2504.08120", "abs": "https://arxiv.org/abs/2504.08120", "authors": ["Daniil Larionov", "Sotaro Takeshita", "Ran Zhang", "Yanran Chen", "Christoph Leiter", "Zhipin Wang", "Christian Greisinger", "Steffen Eger"], "title": "DeepSeek vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and Summarization?", "categories": ["cs.CL"], "comment": null, "summary": "Reasoning-enabled large language models (LLMs) have recently demonstrated\nimpressive performance in complex logical and mathematical tasks, yet their\neffectiveness in evaluating natural language generation remains unexplored.\nThis study systematically compares reasoning-based LLMs (DeepSeek-R1 and OpenAI\no3) with their non-reasoning counterparts across machine translation (MT) and\ntext summarization (TS) evaluation tasks. We evaluate eight models across three\narchitectural categories, including state-of-the-art reasoning models, their\ndistilled variants (ranging from 8B to 70B parameters), and equivalent\nconventional, non-reasoning LLMs. Our experiments on WMT23 and SummEval\nbenchmarks reveal that the benefits of reasoning capabilities are highly model\nand task-dependent: while OpenAI o3-mini models show consistent performance\nimprovements with increased reasoning intensity, DeepSeek-R1 underperforms\ncompared to its non-reasoning variant, with exception to certain aspects of TS\nevaluation. Correlation analysis demonstrates that increased reasoning token\nusage positively correlates with evaluation quality in o3-mini models.\nFurthermore, our results show that distillation of reasoning capabilities\nmaintains reasonable performance in medium-sized models (32B) but degrades\nsubstantially in smaller variants (8B). This work provides the first\ncomprehensive assessment of reasoning LLMs for NLG evaluation and offers\ninsights into their practical use."}
{"id": "2504.08154", "pdf": "https://arxiv.org/pdf/2504.08154", "abs": "https://arxiv.org/abs/2504.08154", "authors": ["Yiqiao Li", "Jie Wei", "Camille Kamga"], "title": "Investigating Vision-Language Model for Point Cloud-based Vehicle Classification", "categories": ["cs.CV"], "comment": "5 pages,3 figures, 1 table, CVPR DriveX workshop", "summary": "Heavy-duty trucks pose significant safety challenges due to their large size\nand limited maneuverability compared to passenger vehicles. A deeper\nunderstanding of truck characteristics is essential for enhancing the safety\nperspective of cooperative autonomous driving. Traditional LiDAR-based truck\nclassification methods rely on extensive manual annotations, which makes them\nlabor-intensive and costly. The rapid advancement of large language models\n(LLMs) trained on massive datasets presents an opportunity to leverage their\nfew-shot learning capabilities for truck classification. However, existing\nvision-language models (VLMs) are primarily trained on image datasets, which\nmakes it challenging to directly process point cloud data. This study\nintroduces a novel framework that integrates roadside LiDAR point cloud data\nwith VLMs to facilitate efficient and accurate truck classification, which\nsupports cooperative and safe driving environments. This study introduces three\nkey innovations: (1) leveraging real-world LiDAR datasets for model\ndevelopment, (2) designing a preprocessing pipeline to adapt point cloud data\nfor VLM input, including point cloud registration for dense 3D rendering and\nmathematical morphological techniques to enhance feature representation, and\n(3) utilizing in-context learning with few-shot prompting to enable vehicle\nclassification with minimally labeled training data. Experimental results\ndemonstrate encouraging performance of this method and present its potential to\nreduce annotation efforts while improving classification accuracy."}
{"id": "2504.08165", "pdf": "https://arxiv.org/pdf/2504.08165", "abs": "https://arxiv.org/abs/2504.08165", "authors": ["Alex Warstadt", "Aaron Mueller", "Leshem Choshen", "Ethan Wilcox", "Chengxu Zhuang", "Juan Ciro", "Rafael Mosquera", "Bhargavi Paranjape", "Adina Williams", "Tal Linzen", "Ryan Cotterell"], "title": "Findings of the BabyLM Challenge: Sample-Efficient Pretraining on Developmentally Plausible Corpora", "categories": ["cs.CL"], "comment": "Published in Proceedings of BabyLM. Please cite the published version\n  on ACL anthology: http://aclanthology.org/2023.conll-babylm.1/", "summary": "Children can acquire language from less than 100 million words of input.\nLarge language models are far less data-efficient: they typically require 3 or\n4 orders of magnitude more data and still do not perform as well as humans on\nmany evaluations. These intensive resource demands limit the ability of\nresearchers to train new models and use existing models as developmentally\nplausible cognitive models. The BabyLM Challenge is a communal effort in which\nparticipants compete to optimize language model training on a fixed data\nbudget. Submissions are compared on various evaluation tasks targeting\ngrammatical ability, downstream task performance, and generalization.\nParticipants can submit to up to three tracks with progressively looser data\nrestrictions. From over 30 submissions, we extract concrete recommendations on\nhow best to train data-efficient language models, and on where future efforts\nshould (and perhaps should not) focus. The winning submissions using the\nLTG-BERT architecture (Samuel et al., 2023) outperformed models trained on\ntrillions of words. Other submissions achieved strong results through training\non shorter input sequences or training a student model on a pretrained teacher.\nCurriculum learning attempts, which accounted for a large number of\nsubmissions, were largely unsuccessful, though some showed modest improvements."}
{"id": "2504.08166", "pdf": "https://arxiv.org/pdf/2504.08166", "abs": "https://arxiv.org/abs/2504.08166", "authors": ["Vivek Trivedy", "Amani Almalki", "Longin Jan Latecki"], "title": "Learning Object Focused Attention", "categories": ["cs.CV"], "comment": null, "summary": "We propose an adaptation to the training of Vision Transformers (ViTs) that\nallows for an explicit modeling of objects during the attention computation.\nThis is achieved by adding a new branch to selected attention layers that\ncomputes an auxiliary loss which we call the object-focused attention (OFA)\nloss. We restrict the attention to image patches that belong to the same object\nclass, which allows ViTs to gain a better understanding of configural (or\nholistic) object shapes by focusing on intra-object patches instead of other\npatches such as those in the background. Our proposed inductive bias fits\neasily into the attention framework of transformers since it only adds an\nauxiliary loss over selected attention layers. Furthermore, our approach has no\nadditional overhead during inference. We also experiment with multiscale\nmasking to further improve the performance of our OFA model and give a path\nforward for self-supervised learning with our method. Our experimental results\ndemonstrate that ViTs with OFA achieve better classification results than their\nbase models, exhibit a stronger generalization ability to out-of-distribution\n(OOD) and adversarially corrupted images, and learn representations based on\nobject shapes rather than spurious correlations via general textures. For our\nOOD setting, we generate a novel dataset using the COCO dataset and Stable\nDiffusion inpainting which we plan to share with the community."}
{"id": "2504.08202", "pdf": "https://arxiv.org/pdf/2504.08202", "abs": "https://arxiv.org/abs/2504.08202", "authors": ["Yu Fu", "Haz Sameen Shahgir", "Hui Liu", "Xianfeng Tang", "Qi He", "Yue Dong"], "title": "Harnessing the Unseen: The Hidden Influence of Intrinsic Knowledge in Long-Context Language Models", "categories": ["cs.CL"], "comment": "21 pages,11figures", "summary": "Recent advances in long-context models (LCMs), designed to handle extremely\nlong input contexts, primarily focus on utilizing external contextual\ninformation, often leaving the influence of large language models' intrinsic\nknowledge underexplored. In this work, we investigate how this intrinsic\nknowledge affects content generation and demonstrate that its impact becomes\nincreasingly pronounced as context length extends. Furthermore, we show that\nthe model's ability to utilize intrinsic knowledge, which we call intrinsic\nretrieval ability, does not improve simultaneously with its ability to leverage\ncontextual knowledge through extrinsic retrieval ability. Moreover, better\nextrinsic retrieval can interfere with the model's ability to use its own\nknowledge effectively, limiting its full potential. To bridge this gap, we\ndesign a simple yet effective Hybrid Needle-in-a-Haystack test that evaluates\nmodels based on their capabilities across both retrieval abilities, rather than\nsolely emphasizing extrinsic retrieval ability. Our experimental results reveal\nthat Qwen-2.5 models significantly outperform Llama-3.1 models, demonstrating\nsuperior intrinsic retrieval ability. Moreover, even the more powerful\nLlama-3.1-70B-Instruct model fails to exhibit better performance under LCM\nconditions, highlighting the importance of evaluating models from a\ndual-retrieval perspective."}
{"id": "2504.08175", "pdf": "https://arxiv.org/pdf/2504.08175", "abs": "https://arxiv.org/abs/2504.08175", "authors": ["Hossein Feiz", "David Labbé", "Thomas Romeas", "Jocelyn Faubert", "Sheldon Andrews"], "title": "Multi-person Physics-based Pose Estimation for Combat Sports", "categories": ["cs.CV"], "comment": null, "summary": "We propose a novel framework for accurate 3D human pose estimation in combat\nsports using sparse multi-camera setups. Our method integrates robust\nmulti-view 2D pose tracking via a transformer-based top-down approach,\nemploying epipolar geometry constraints and long-term video object segmentation\nfor consistent identity tracking across views. Initial 3D poses are obtained\nthrough weighted triangulation and spline smoothing, followed by kinematic\noptimization to refine pose accuracy. We further enhance pose realism and\nrobustness by introducing a multi-person physics-based trajectory optimization\nstep, effectively addressing challenges such as rapid motions, occlusions, and\nclose interactions. Experimental results on diverse datasets, including a new\nbenchmark of elite boxing footage, demonstrate state-of-the-art performance.\nAdditionally, we release comprehensive annotated video datasets to advance\nfuture research in multi-person pose estimation for combat sports."}
{"id": "2504.08211", "pdf": "https://arxiv.org/pdf/2504.08211", "abs": "https://arxiv.org/abs/2504.08211", "authors": ["Leo Kampen", "Carlos Rabat Villarreal", "Louis Yu", "Santu Karmaker", "Dongji Feng"], "title": "LLM for Comparative Narrative Analysis", "categories": ["cs.CL", "cs.AI"], "comment": "5 pages, 4 figures, Appendix included", "summary": "In this paper, we conducted a Multi-Perspective Comparative Narrative\nAnalysis (CNA) on three prominent LLMs: GPT-3.5, PaLM2, and Llama2. We applied\nidentical prompts and evaluated their outputs on specific tasks, ensuring an\nequitable and unbiased comparison between various LLMs. Our study revealed that\nthe three LLMs generated divergent responses to the same prompt, indicating\nnotable discrepancies in their ability to comprehend and analyze the given\ntask. Human evaluation was used as the gold standard, evaluating four\nperspectives to analyze differences in LLM performance."}
{"id": "2504.08181", "pdf": "https://arxiv.org/pdf/2504.08181", "abs": "https://arxiv.org/abs/2504.08181", "authors": ["Ruineng Li", "Daitao Xing", "Huiming Sun", "Yuanzhou Ha", "Jinglin Shen", "Chiuman Ho"], "title": "TokenMotion: Decoupled Motion Control via Token Disentanglement for Human-centric Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Human-centric motion control in video generation remains a critical\nchallenge, particularly when jointly controlling camera movements and human\nposes in scenarios like the iconic Grammy Glambot moment. While recent video\ndiffusion models have made significant progress, existing approaches struggle\nwith limited motion representations and inadequate integration of camera and\nhuman motion controls. In this work, we present TokenMotion, the first\nDiT-based video diffusion framework that enables fine-grained control over\ncamera motion, human motion, and their joint interaction. We represent camera\ntrajectories and human poses as spatio-temporal tokens to enable local control\ngranularity. Our approach introduces a unified modeling framework utilizing a\ndecouple-and-fuse strategy, bridged by a human-aware dynamic mask that\neffectively handles the spatially-and-temporally varying nature of combined\nmotion signals. Through extensive experiments, we demonstrate TokenMotion's\neffectiveness across both text-to-video and image-to-video paradigms,\nconsistently outperforming current state-of-the-art methods in human-centric\nmotion control tasks. Our work represents a significant advancement in\ncontrollable video generation, with particular relevance for creative\nproduction applications."}
{"id": "2504.08213", "pdf": "https://arxiv.org/pdf/2504.08213", "abs": "https://arxiv.org/abs/2504.08213", "authors": ["Samuel Flanders", "Melati Nungsari", "Mark Cheong Wing Loong"], "title": "Big Meaning: Qualitative Analysis on Large Bodies of Data Using AI", "categories": ["cs.CL"], "comment": "arXiv admin note: text overlap with arXiv:2504.07408", "summary": "This study introduces a framework that leverages AI-generated descriptive\ncodes to indicate a text's fecundity--the density of unique human-generated\ncodes--in thematic analysis. Rather than replacing human interpretation,\nAI-generated codes guide the selection of texts likely to yield richer\nqualitative insights. Using a dataset of 2,530 Malaysian news articles on\nrefugee attitudes, we compare AI-selected documents to randomly chosen ones by\nhaving three human coders independently derive codes. The results demonstrate\nthat AI-selected texts exhibit approximately twice the fecundity. Our findings\nsupport the use of AI-generated codes as an effective proxy for identifying\ndocuments with a high potential for meaning-making in thematic analysis."}
{"id": "2504.08186", "pdf": "https://arxiv.org/pdf/2504.08186", "abs": "https://arxiv.org/abs/2504.08186", "authors": ["Fahd Baba", "Devon Mack"], "title": "Comparative Analysis of Different Methods for Classifying Polychromatic Sketches", "categories": ["cs.CV"], "comment": null, "summary": "Image classification is a significant challenge in computer vision,\nparticularly in domains humans are not accustomed to. As machine learning and\nartificial intelligence become more prominent, it is crucial these algorithms\ndevelop a sense of sight that is on par with or exceeds human ability. For this\nreason, we have collected, cleaned, and parsed a large dataset of hand-drawn\ndoodles and compared multiple machine learning solutions to classify these\nimages into 170 distinct categories. The best model we found achieved a Top-1\naccuracy of 47.5%, significantly surpassing human performance on the dataset,\nwhich stands at 41%."}
{"id": "2504.08231", "pdf": "https://arxiv.org/pdf/2504.08231", "abs": "https://arxiv.org/abs/2504.08231", "authors": ["Tianyu Cao", "Neel Bhandari", "Akhila Yerukola", "Akari Asai", "Maarten Sap"], "title": "Out of Style: RAG's Fragility to Linguistic Variation", "categories": ["cs.CL"], "comment": null, "summary": "Despite the impressive performance of Retrieval-augmented Generation (RAG)\nsystems across various NLP benchmarks, their robustness in handling real-world\nuser-LLM interaction queries remains largely underexplored. This presents a\ncritical gap for practical deployment, where user queries exhibit greater\nlinguistic variations and can trigger cascading errors across interdependent\nRAG components. In this work, we systematically analyze how varying four\nlinguistic dimensions (formality, readability, politeness, and grammatical\ncorrectness) impact RAG performance. We evaluate two retrieval models and nine\nLLMs, ranging from 3 to 72 billion parameters, across four information-seeking\nQuestion Answering (QA) datasets. Our results reveal that linguistic\nreformulations significantly impact both retrieval and generation stages,\nleading to a relative performance drop of up to 40.41% in Recall@5 scores for\nless formal queries and 38.86% in answer match scores for queries containing\ngrammatical errors. Notably, RAG systems exhibit greater sensitivity to such\nvariations compared to LLM-only generations, highlighting their vulnerability\nto error propagation due to linguistic shifts. These findings highlight the\nneed for improved robustness techniques to enhance reliability in diverse user\ninteractions."}
{"id": "2504.08205", "pdf": "https://arxiv.org/pdf/2504.08205", "abs": "https://arxiv.org/abs/2504.08205", "authors": ["Minjae Seo", "Myoungsung You", "Junhee Lee", "Jaehan Kim", "Hwanjo Heo", "Jintae Oh", "Jinwoo Kim"], "title": "EO-VLM: VLM-Guided Energy Overload Attacks on Vision Models", "categories": ["cs.CV", "cs.CR"], "comment": "Presented as a poster at ACSAC 2024", "summary": "Vision models are increasingly deployed in critical applications such as\nautonomous driving and CCTV monitoring, yet they remain susceptible to\nresource-consuming attacks. In this paper, we introduce a novel\nenergy-overloading attack that leverages vision language model (VLM) prompts to\ngenerate adversarial images targeting vision models. These images, though\nimperceptible to the human eye, significantly increase GPU energy consumption\nacross various vision models, threatening the availability of these systems.\nOur framework, EO-VLM (Energy Overload via VLM), is model-agnostic, meaning it\nis not limited by the architecture or type of the target vision model. By\nexploiting the lack of safety filters in VLMs like DALL-E 3, we create\nadversarial noise images without requiring prior knowledge or internal\nstructure of the target vision models. Our experiments demonstrate up to a 50%\nincrease in energy consumption, revealing a critical vulnerability in current\nvision models."}
{"id": "2504.08260", "pdf": "https://arxiv.org/pdf/2504.08260", "abs": "https://arxiv.org/abs/2504.08260", "authors": ["Yonchanok Khaokaew", "Flora D. Salim", "Andreas Züfle", "Hao Xue", "Taylor Anderson", "Matthew Scotch", "David J Heslop"], "title": "Evaluating the Bias in LLMs for Surveying Opinion and Decision Making in Healthcare", "categories": ["cs.CL"], "comment": null, "summary": "Generative agents have been increasingly used to simulate human behaviour in\nsilico, driven by large language models (LLMs). These simulacra serve as\nsandboxes for studying human behaviour without compromising privacy or safety.\nHowever, it remains unclear whether such agents can truly represent real\nindividuals. This work compares survey data from the Understanding America\nStudy (UAS) on healthcare decision-making with simulated responses from\ngenerative agents. Using demographic-based prompt engineering, we create\ndigital twins of survey respondents and analyse how well different LLMs\nreproduce real-world behaviours. Our findings show that some LLMs fail to\nreflect realistic decision-making, such as predicting universal vaccine\nacceptance. However, Llama 3 captures variations across race and Income more\naccurately but also introduces biases not present in the UAS data. This study\nhighlights the potential of generative agents for behavioural research while\nunderscoring the risks of bias from both LLMs and prompting strategies."}
{"id": "2504.08212", "pdf": "https://arxiv.org/pdf/2504.08212", "abs": "https://arxiv.org/abs/2504.08212", "authors": ["Guangcong Zheng", "Teng Li", "Xianpan Zhou", "Xi Li"], "title": "RealCam-Vid: High-resolution Video Dataset with Dynamic Scenes and Metric-scale Camera Movements", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in camera-controllable video generation have been constrained\nby the reliance on static-scene datasets with relative-scale camera\nannotations, such as RealEstate10K. While these datasets enable basic viewpoint\ncontrol, they fail to capture dynamic scene interactions and lack metric-scale\ngeometric consistency-critical for synthesizing realistic object motions and\nprecise camera trajectories in complex environments. To bridge this gap, we\nintroduce the first fully open-source, high-resolution dynamic-scene dataset\nwith metric-scale camera annotations in https://github.com/ZGCTroy/RealCam-Vid."}
{"id": "2504.08281", "pdf": "https://arxiv.org/pdf/2504.08281", "abs": "https://arxiv.org/abs/2504.08281", "authors": ["Vishal Gandhi", "Sagar Gandhi"], "title": "ELSA: A Style Aligned Dataset for Emotionally Intelligent Language Generation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "8 pages", "summary": "Advancements in emotion aware language processing increasingly shape vital\nNLP applications ranging from conversational AI and affective computing to\ncomputational psychology and creative content generation. Existing emotion\ndatasets either lack emotional granularity or fail to capture necessary\nstylistic diversity, limiting the advancement of effective emotion conditioned\ntext generation systems. Seeking to bridge this crucial gap between granularity\nand style diversity, this paper introduces a novel systematically constructed\ndataset named ELSA Emotion and Language Style Alignment Dataset leveraging fine\ngrained emotion taxonomies adapted from existing sources such as dair ai\nemotion dataset and GoEmotions taxonomy. This dataset comprises multiple\nemotionally nuanced variations of original sentences regenerated across\ndistinct contextual styles such as conversational, formal, poetic, and\nnarrative, using advanced Large Language Models LLMs. Rigorous computational\nevaluation using metrics such as perplexity, embedding variance, readability,\nlexical diversity, and semantic coherence measures validates the datasets\nemotional authenticity, linguistic fluency, and textual diversity.\nComprehensive metric analyses affirm its potential to support deeper\nexplorations into emotion conditioned style adaptive text generation. By\nenabling precision tuned emotionally nuanced language modeling, our dataset\ncreates fertile ground for research on fine grained emotional control, prompt\ndriven explanation, interpretability, and style adaptive expressive language\ngeneration with LLMs."}
{"id": "2504.08219", "pdf": "https://arxiv.org/pdf/2504.08219", "abs": "https://arxiv.org/abs/2504.08219", "authors": ["Ziyan Liu", "Yuxu Lu", "Huashan Yu", "Dong yang"], "title": "VL-UR: Vision-Language-guided Universal Restoration of Images Degraded by Adverse Weather Conditions", "categories": ["cs.CV"], "comment": null, "summary": "Image restoration is critical for improving the quality of degraded images,\nwhich is vital for applications like autonomous driving, security surveillance,\nand digital content enhancement. However, existing methods are often tailored\nto specific degradation scenarios, limiting their adaptability to the diverse\nand complex challenges in real-world environments. Moreover, real-world\ndegradations are typically non-uniform, highlighting the need for adaptive and\nintelligent solutions. To address these issues, we propose a novel\nvision-language-guided universal restoration (VL-UR) framework. VL-UR leverages\na zero-shot contrastive language-image pre-training (CLIP) model to enhance\nimage restoration by integrating visual and semantic information. A scene\nclassifier is introduced to adapt CLIP, generating high-quality language\nembeddings aligned with degraded images while predicting degraded types for\ncomplex scenarios. Extensive experiments across eleven diverse degradation\nsettings demonstrate VL-UR's state-of-the-art performance, robustness, and\nadaptability. This positions VL-UR as a transformative solution for modern\nimage restoration challenges in dynamic, real-world environments."}
{"id": "2504.08300", "pdf": "https://arxiv.org/pdf/2504.08300", "abs": "https://arxiv.org/abs/2504.08300", "authors": ["Yuyang Xu", "Renjun Hu", "Haochao Ying", "Jian Wu", "Xing Shi", "Wei Lin"], "title": "Large language models could be rote learners", "categories": ["cs.CL", "cs.AI"], "comment": "Work in Progress", "summary": "Multiple-choice question (MCQ) benchmarks are widely used for evaluating\nLarge Language Models (LLMs), yet their reliability is undermined by benchmark\ncontamination. In this study, we reframe contamination as an inherent aspect of\nlearning and seek to disentangle genuine capability acquisition from\nsuperficial memorization in LLM evaluation. First, by analyzing model\nperformance under different memorization conditions, we uncover a\ncounterintuitive trend: LLMs perform worse on memorized MCQs than on\nnon-memorized ones, indicating the coexistence of two distinct learning\nphenomena, i.e., rote memorization and genuine capability learning. To\ndisentangle them, we propose TrinEval, a novel evaluation framework that\nreformulates MCQs into an alternative trinity format, reducing memorization\nwhile preserving knowledge assessment. Experiments validate TrinEval's\neffectiveness in reformulation, and its evaluation reveals that common LLMs may\nmemorize by rote 20.5% of knowledge points (in MMLU on average)."}
{"id": "2504.08222", "pdf": "https://arxiv.org/pdf/2504.08222", "abs": "https://arxiv.org/abs/2504.08222", "authors": ["Zhaoyu Liu", "Kan Jiang", "Murong Ma", "Zhe Hou", "Yun Lin", "Jin Song Dong"], "title": "F$^3$Set: Towards Analyzing Fast, Frequent, and Fine-grained Events from Videos", "categories": ["cs.CV", "cs.AI"], "comment": "The Thirteenth International Conference on Learning Representations\n  (ICLR 2025)", "summary": "Analyzing Fast, Frequent, and Fine-grained (F$^3$) events presents a\nsignificant challenge in video analytics and multi-modal LLMs. Current methods\nstruggle to identify events that satisfy all the F$^3$ criteria with high\naccuracy due to challenges such as motion blur and subtle visual discrepancies.\nTo advance research in video understanding, we introduce F$^3$Set, a benchmark\nthat consists of video datasets for precise F$^3$ event detection. Datasets in\nF$^3$Set are characterized by their extensive scale and comprehensive detail,\nusually encompassing over 1,000 event types with precise timestamps and\nsupporting multi-level granularity. Currently, F$^3$Set contains several sports\ndatasets, and this framework may be extended to other applications as well. We\nevaluated popular temporal action understanding methods on F$^3$Set, revealing\nsubstantial challenges for existing techniques. Additionally, we propose a new\nmethod, F$^3$ED, for F$^3$ event detections, achieving superior performance.\nThe dataset, model, and benchmark code are available at\nhttps://github.com/F3Set/F3Set."}
{"id": "2504.08385", "pdf": "https://arxiv.org/pdf/2504.08385", "abs": "https://arxiv.org/abs/2504.08385", "authors": ["Markus Flicke", "Glenn Angrabeit", "Madhav Iyengar", "Vitalii Protsenko", "Illia Shakun", "Jovan Cicvaric", "Bora Kargi", "Haoyu He", "Lukas Schuler", "Lewin Scholz", "Kavyanjali Agnihotri", "Yong Cao", "Andreas Geiger"], "title": "Scholar Inbox: Personalized Paper Recommendations for Scientists", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "https://www.scholar-inbox.com/", "summary": "Scholar Inbox is a new open-access platform designed to address the\nchallenges researchers face in staying current with the rapidly expanding\nvolume of scientific literature. We provide personalized recommendations,\ncontinuous updates from open-access archives (arXiv, bioRxiv, etc.), visual\npaper summaries, semantic search, and a range of tools to streamline research\nworkflows and promote open research access. The platform's personalized\nrecommendation system is trained on user ratings, ensuring that recommendations\nare tailored to individual researchers' interests. To further enhance the user\nexperience, Scholar Inbox also offers a map of science that provides an\noverview of research across domains, enabling users to easily explore specific\ntopics. We use this map to address the cold start problem common in recommender\nsystems, as well as an active learning strategy that iteratively prompts users\nto rate a selection of papers, allowing the system to learn user preferences\nquickly. We evaluate the quality of our recommendation system on a novel\ndataset of 800k user ratings, which we make publicly available, as well as via\nan extensive user study. https://www.scholar-inbox.com/"}
{"id": "2504.08252", "pdf": "https://arxiv.org/pdf/2504.08252", "abs": "https://arxiv.org/abs/2504.08252", "authors": ["Travis Driver", "Andrew Vaughan", "Yang Cheng", "Adnan Ansar", "John Christian", "Panagiotis Tsiotras"], "title": "Stereophotoclinometry Revisited", "categories": ["cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2312.06865", "summary": "Image-based surface reconstruction and characterization is crucial for\nmissions to small celestial bodies, as it informs mission planning, navigation,\nand scientific analysis. However, current state-of-the-practice methods, such\nas stereophotoclinometry (SPC), rely heavily on human-in-the-loop verification\nand high-fidelity a priori information. This paper proposes\nPhotoclinometry-from-Motion (PhoMo), a novel framework that incorporates\nphotoclinometry techniques into a keypoint-based structure-from-motion (SfM)\nsystem to estimate the surface normal and albedo at detected landmarks to\nimprove autonomous surface and shape characterization of small celestial bodies\nfrom in-situ imagery. In contrast to SPC, we forego the expensive maplet\nestimation step and instead use dense keypoint measurements and correspondences\nfrom an autonomous keypoint detection and matching method based on deep\nlearning. Moreover, we develop a factor graph-based approach allowing for\nsimultaneous optimization of the spacecraft's pose, landmark positions,\nSun-relative direction, and surface normals and albedos via fusion of Sun\nvector measurements and image keypoint measurements. The proposed framework is\nvalidated on real imagery taken by the Dawn mission to the asteroid 4 Vesta and\nthe minor planet 1 Ceres and compared against an SPC reconstruction, where we\ndemonstrate superior rendering performance compared to an SPC solution and\nprecise alignment to a stereophotogrammetry (SPG) solution without relying on\nany a priori camera pose and topography information or humans-in-the-loop."}
{"id": "2504.08399", "pdf": "https://arxiv.org/pdf/2504.08399", "abs": "https://arxiv.org/abs/2504.08399", "authors": ["Yin Jou Huang", "Rafik Hadfi"], "title": "Beyond Self-Reports: Multi-Observer Agents for Personality Assessment in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 5 figures, 2 tables", "summary": "There is a growing interest in assessing the personality traits of Large\nlanguage models (LLMs). However, traditional personality assessments based on\nself-report questionnaires may fail to capture their true behavioral nuances\ndue to inherent biases and meta-knowledge contamination. This paper introduces\na novel multi-observer framework for LLM personality assessment that draws\ninspiration from informant-report methods in psychology. Instead of relying\nsolely on self-assessments, our approach employs multiple observer agents\nconfigured with a specific relationship context (e.g., family, friend, or\nworkplace) to simulate interactive scenarios with a subject LLM. These\nobservers engage in dialogues and subsequently provide ratings across the Big\nFive personality dimensions. Our experiments reveal that LLMs possess\nsystematic biases in self-report personality ratings. Moreover, aggregating\nobserver ratings effectively reduces non-systematic biases and achieves optimal\nreliability with 5-7 observers. The findings highlight the significant impact\nof relationship context on personality perception and demonstrate that a\nmulti-observer paradigm yields a more robust and context-sensitive evaluation\nof LLM personality traits."}
{"id": "2504.08253", "pdf": "https://arxiv.org/pdf/2504.08253", "abs": "https://arxiv.org/abs/2504.08253", "authors": ["Jinghe Yang", "Mingming Gong", "Ye Pu"], "title": "Knowledge Distillation for Underwater Feature Extraction and Matching via GAN-synthesized Images", "categories": ["cs.CV"], "comment": null, "summary": "Autonomous Underwater Vehicles (AUVs) play a crucial role in underwater\nexploration. Vision-based methods offer cost-effective solutions for\nlocalization and mapping in the absence of conventional sensors like GPS and\nLIDAR. However, underwater environments present significant challenges for\nfeature extraction and matching due to image blurring and noise caused by\nattenuation, scattering, and the interference of \\textit{marine snow}. In this\npaper, we aim to improve the robustness of the feature extraction and matching\nin the turbid underwater environment using the cross-modal knowledge\ndistillation method that transfers the in-air feature extraction models to\nunderwater settings using synthetic underwater images as the medium. We first\npropose a novel adaptive GAN-synthesis method to estimate water parameters and\nunderwater noise distribution, to generate environment-specific synthetic\nunderwater images. We then introduce a general knowledge distillation framework\ncompatible with different teacher models. The evaluation of GAN-based synthesis\nhighlights the significance of the new components, i.e. GAN-synthesized noise\nand forward scattering, in the proposed model. Additionally, the downstream\napplication of feature extraction and matching (VSLAM) on real underwater\nsequences validates the effectiveness of the transferred model."}
{"id": "2504.08527", "pdf": "https://arxiv.org/pdf/2504.08527", "abs": "https://arxiv.org/abs/2504.08527", "authors": ["Taisei Kanda", "Mingzhe Jin", "Wataru Zaitsu"], "title": "Integrated ensemble of BERT- and features-based models for authorship attribution in Japanese literary works", "categories": ["cs.CL"], "comment": null, "summary": "Traditionally, authorship attribution (AA) tasks relied on statistical data\nanalysis and classification based on stylistic features extracted from texts.\nIn recent years, pre-trained language models (PLMs) have attracted significant\nattention in text classification tasks. However, although they demonstrate\nexcellent performance on large-scale short-text datasets, their effectiveness\nremains under-explored for small samples, particularly in AA tasks.\nAdditionally, a key challenge is how to effectively leverage PLMs in\nconjunction with traditional feature-based methods to advance AA research. In\nthis study, we aimed to significantly improve performance using an integrated\nintegrative ensemble of traditional feature-based and modern PLM-based methods\non an AA task in a small sample. For the experiment, we used two corpora of\nliterary works to classify 10 authors each. The results indicate that BERT is\neffective, even for small-sample AA tasks. Both BERT-based and classifier\nensembles outperformed their respective stand-alone models, and the integrated\nensemble approach further improved the scores significantly. For the corpus\nthat was not included in the pre-training data, the integrated ensemble\nimproved the F1 score by approximately 14 points, compared to the\nbest-performing single model. Our methodology provides a viable solution for\nthe efficient use of the ever-expanding array of data processing tools in the\nforeseeable future."}
{"id": "2504.08259", "pdf": "https://arxiv.org/pdf/2504.08259", "abs": "https://arxiv.org/abs/2504.08259", "authors": ["Ruohao Zhan", "Yijin Li", "Yisheng He", "Shuo Chen", "Yichen Shen", "Xinyu Chen", "Zilong Dong", "Zhaoyang Huang", "Guofeng Zhang"], "title": "CoProSketch: Controllable and Progressive Sketch Generation with Diffusion Model", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 9 figures", "summary": "Sketches serve as fundamental blueprints in artistic creation because sketch\nediting is easier and more intuitive than pixel-level RGB image editing for\npainting artists, yet sketch generation remains unexplored despite advancements\nin generative models. We propose a novel framework CoProSketch, providing\nprominent controllability and details for sketch generation with diffusion\nmodels. A straightforward method is fine-tuning a pretrained image generation\ndiffusion model with binarized sketch images. However, we find that the\ndiffusion models fail to generate clear binary images, which makes the produced\nsketches chaotic. We thus propose to represent the sketches by unsigned\ndistance field (UDF), which is continuous and can be easily decoded to sketches\nthrough a lightweight network. With CoProSketch, users generate a rough sketch\nfrom a bounding box and a text prompt. The rough sketch can be manually edited\nand fed back into the model for iterative refinement and will be decoded to a\ndetailed sketch as the final result. Additionally, we curate the first\nlarge-scale text-sketch paired dataset as the training data. Experiments\ndemonstrate superior semantic consistency and controllability over baselines,\noffering a practical solution for integrating user feedback into generative\nworkflows."}
{"id": "2504.08528", "pdf": "https://arxiv.org/pdf/2504.08528", "abs": "https://arxiv.org/abs/2504.08528", "authors": ["Siddhant Arora", "Kai-Wei Chang", "Chung-Ming Chien", "Yifan Peng", "Haibin Wu", "Yossi Adi", "Emmanuel Dupoux", "Hung-Yi Lee", "Karen Livescu", "Shinji Watanabe"], "title": "On The Landscape of Spoken Language Models: A Comprehensive Survey", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "The field of spoken language processing is undergoing a shift from training\ncustom-built, task-specific models toward using and optimizing spoken language\nmodels (SLMs) which act as universal speech processing systems. This trend is\nsimilar to the progression toward universal language models that has taken\nplace in the field of (text) natural language processing. SLMs include both\n\"pure\" language models of speech -- models of the distribution of tokenized\nspeech sequences -- and models that combine speech encoders with text language\nmodels, often including both spoken and written input or output. Work in this\narea is very diverse, with a range of terminology and evaluation settings. This\npaper aims to contribute an improved understanding of SLMs via a unifying\nliterature survey of recent work in the context of the evolution of the field.\nOur survey categorizes the work in this area by model architecture, training,\nand evaluation choices, and describes some key challenges and directions for\nfuture work."}
{"id": "2504.08269", "pdf": "https://arxiv.org/pdf/2504.08269", "abs": "https://arxiv.org/abs/2504.08269", "authors": ["Qi Zhi Lim", "Chin Poo Lee", "Kian Ming Lim", "Kalaiarasi Sonai Muthu Anbananthen"], "title": "VLMT: Vision-Language Multimodal Transformer for Multimodal Multi-hop Question Answering", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "The increasing availability of multimodal data across text, tables, and\nimages presents new challenges for developing models capable of complex\ncross-modal reasoning. Existing methods for Multimodal Multi-hop Question\nAnswering (MMQA) often suffer from limited reasoning capabilities, reliance on\nmodality conversion, and inadequate alignment between visual and textual\nrepresentations. To address these limitations, this paper introduces\nVision-Language Multimodal Transformer (VLMT), a unified architecture that\nintegrates a transformer-based vision encoder with a sequence-to-sequence\nlanguage model. VLMT employs a direct token-level injection mechanism to fuse\nvisual and textual inputs within a shared embedding space, eliminating the need\nfor intermediate projection layers. To enhance cross-modal alignment and\nreasoning, a three-stage pretraining strategy is proposed to progressively\nalign vision-language representations and improve the model's capacity for\nmultimodal understanding. Based on the pretrained backbone, two task-specific\nmodules are instantiated to form a two-stage MMQA framework: a multimodal\nreranker that predicts document relevance scores and utilizes a relative\nthreshold with top-k strategy for context retrieval, and a multimodal question\nanswering model that generates contextually grounded answers based on the\nretrieved evidence. Comprehensive experiments on two benchmark datasets\ndemonstrate the effectiveness of the proposed approach. On MultimodalQA\nvalidation set, VLMT-Large achieves 76.5% Exact Match and 80.1% F1,\noutperforming the previous state-of-the-art by +9.1% in Exact Match and +8.8%\nin F1. On WebQA, it attains a QA score of 47.6, surpassing prior models such as\nPERQA by +3.2. These results highlight VLMT's strong capabilities in multimodal\nreasoning and its potential to advance real-world information retrieval and\nquestion answering systems."}
{"id": "2504.08537", "pdf": "https://arxiv.org/pdf/2504.08537", "abs": "https://arxiv.org/abs/2504.08537", "authors": ["Burak Senel"], "title": "Lexical Bundle Frequency as a Construct-Relevant Candidate Feature in Automated Scoring of L2 Academic Writing", "categories": ["cs.CL"], "comment": null, "summary": "Automated scoring (AS) systems are increasingly used for evaluating L2\nwriting, but require ongoing refinement for construct validity. While prior\nwork suggested lexical bundles (LBs) - recurrent multi-word sequences\nsatisfying certain frequency criteria - could inform assessment, their\nempirical integration into AS models needs further investigation. This study\ntested the impact of incorporating LB frequency features into an AS model for\nTOEFL independent writing tasks. Analyzing a sampled subcorpus (N=1,225 essays,\n9 L1s) from the TOEFL11 corpus, scored by ETS-trained raters (Low, Medium,\nHigh), 3- to 9-word LBs were extracted, distinguishing prompt-specific from\nnon-prompt types. A baseline Support Vector Machine (SVM) scoring model using\nestablished linguistic features (e.g., mechanics, cohesion, sophistication) was\ncompared against an extended model including three aggregate LB frequency\nfeatures (total prompt, total non-prompt, overall total). Results revealed\nsignificant, though generally small-effect, relationships between LB frequency\n(especially non-prompt bundles) and proficiency (p < .05). Mean frequencies\nsuggested lower proficiency essays used more LBs overall. Critically, the\nLB-enhanced model improved agreement with human raters (Quadratic Cohen's Kappa\n+2.05%, overall Cohen's Kappa +5.63%), with notable gains for low (+10.1% exact\nagreement) and medium (+14.3% Cohen's Kappa) proficiency essays. These findings\ndemonstrate that integrating aggregate LB frequency offers potential for\ndeveloping more linguistically informed and accurate AS systems, particularly\nfor differentiating developing L2 writers."}
{"id": "2504.08272", "pdf": "https://arxiv.org/pdf/2504.08272", "abs": "https://arxiv.org/abs/2504.08272", "authors": ["Licheng Yan", "Bob Zhang", "Andrew Beng Jin Teoh", "Lu Leng", "Shuyi Li", "Yuqi Wang", "Ziyuan Yang"], "title": "Palmprint De-Identification Using Diffusion Model for High-Quality and Diverse Synthesis", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Palmprint recognition techniques have advanced significantly in recent years,\nenabling reliable recognition even when palmprints are captured in uncontrolled\nor challenging environments. However, this strength also introduces new risks,\nas publicly available palmprint images can be misused by adversaries for\nmalicious activities. Despite this growing concern, research on methods to\nobscure or anonymize palmprints remains largely unexplored. Thus, it is\nessential to develop a palmprint de-identification technique capable of\nremoving identity-revealing features while retaining the image's utility and\npreserving non-sensitive information. In this paper, we propose a training-free\nframework that utilizes pre-trained diffusion models to generate diverse,\nhigh-quality palmprint images that conceal identity features for\nde-identification purposes. To ensure greater stability and controllability in\nthe synthesis process, we incorporate a semantic-guided embedding fusion\nalongside a prior interpolation mechanism. We further propose the\nde-identification ratio, a novel metric for intuitive de-identification\nassessment. Extensive experiments across multiple palmprint datasets and\nrecognition methods demonstrate that our method effectively conceals\nidentity-related traits with significant diversity across de-identified\nsamples. The de-identified samples preserve high visual fidelity and maintain\nexcellent usability, achieving a balance between de-identification and\nretaining non-identity information."}
{"id": "2504.08543", "pdf": "https://arxiv.org/pdf/2504.08543", "abs": "https://arxiv.org/abs/2504.08543", "authors": ["Frances Laureano De Leon", "Yixiao Wang", "Yue Feng", "Mark G. Lee"], "title": "UoB-NLP at SemEval-2025 Task 11: Leveraging Adapters for Multilingual and Cross-Lingual Emotion Detection", "categories": ["cs.CL"], "comment": "Accepted to appear in Proceedings of the 19th International Workshop\n  on Semantic Evaluation (SemEval-2025)", "summary": "Emotion detection in natural language processing is a challenging task due to\nthe complexity of human emotions and linguistic diversity. While significant\nprogress has been made in high-resource languages, emotion detection in\nlow-resource languages remains underexplored. In this work, we address\nmultilingual and cross-lingual emotion detection by leveraging adapter-based\nfine-tuning with multilingual pre-trained language models. Adapters introduce a\nsmall number of trainable parameters while keeping the pre-trained model\nweights fixed, offering a parameter-efficient approach to adaptation. We\nexperiment with different adapter tuning strategies, including task-only\nadapters, target-language-ready task adapters, and language-family-based\nadapters. Our results show that target-language-ready task adapters achieve the\nbest overall performance, particularly for low-resource African languages with\nour team ranking 7th for Tigrinya, and 8th for Kinyarwanda in Track A. In Track\nC, our system ranked 3rd for Amharic, and 4th for Oromo, Tigrinya, Kinyarwanda,\nHausa, and Igbo. Our approach outperforms large language models in 11 languages\nand matches their performance in four others, despite our models having\nsignificantly fewer parameters. Furthermore, we find that adapter-based models\nretain cross-linguistic transfer capabilities while requiring fewer\ncomputational resources compared to full fine-tuning for each language."}
{"id": "2504.08280", "pdf": "https://arxiv.org/pdf/2504.08280", "abs": "https://arxiv.org/abs/2504.08280", "authors": ["Xiong Li", "Shulei Liu", "Xingning Chen", "Yisong Wu", "Dong Zhu"], "title": "PNE-SGAN: Probabilistic NDT-Enhanced Semantic Graph Attention Network for LiDAR Loop Closure Detection", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "LiDAR loop closure detection (LCD) is crucial for consistent Simultaneous\nLocalization and Mapping (SLAM) but faces challenges in robustness and\naccuracy. Existing methods, including semantic graph approaches, often suffer\nfrom coarse geometric representations and lack temporal robustness against\nnoise, dynamics, and viewpoint changes. We introduce PNE-SGAN, a Probabilistic\nNDT-Enhanced Semantic Graph Attention Network, to overcome these limitations.\nPNE-SGAN enhances semantic graphs by using Normal Distributions Transform (NDT)\ncovariance matrices as rich, discriminative geometric node features, processed\nvia a Graph Attention Network (GAT). Crucially, it integrates graph similarity\nscores into a probabilistic temporal filtering framework (modeled as an\nHMM/Bayes filter), incorporating uncertain odometry for motion modeling and\nutilizing forward-backward smoothing to effectively handle ambiguities.\nEvaluations on challenging KITTI sequences (00 and 08) demonstrate\nstate-of-the-art performance, achieving Average Precision of 96.2\\% and 95.1\\%,\nrespectively. PNE-SGAN significantly outperforms existing methods, particularly\nin difficult bidirectional loop scenarios where others falter. By synergizing\ndetailed NDT geometry with principled probabilistic temporal reasoning,\nPNE-SGAN offers a highly accurate and robust solution for LiDAR LCD, enhancing\nSLAM reliability in complex, large-scale environments."}
{"id": "2504.08590", "pdf": "https://arxiv.org/pdf/2504.08590", "abs": "https://arxiv.org/abs/2504.08590", "authors": ["Nicola Horst", "Davide Mazzaccara", "Antonia Schmidt", "Michael Sullivan", "Filippo Momentè", "Luca Franceschetti", "Philipp Sadler", "Sherzod Hakimov", "Alberto Testoni", "Raffaella Bernardi", "Raquel Fernández", "Alexander Koller", "Oliver Lemon", "David Schlangen", "Mario Giulianelli", "Alessandro Suglia"], "title": "Playpen: An Environment for Exploring Learning Through Conversational Interaction", "categories": ["cs.CL"], "comment": "Source code: https://github.com/lm-playpen/playpen Please send\n  correspodence to: lm-playschool@googlegroups.com", "summary": "Are we running out of learning signal? Predicting the next word in an\nexisting text has turned out to be a powerful signal, at least at scale. But\nthere are signs that we are running out of this resource. In recent months,\ninteraction between learner and feedback-giver has come into focus, both for\n\"alignment\" (with a reward model judging the quality of instruction following\nattempts) and for improving \"reasoning\" (process- and outcome-based verifiers\njudging reasoning steps). In this paper, we explore to what extent synthetic\ninteraction in what we call Dialogue Games -- goal-directed and rule-governed\nactivities driven predominantly by verbal actions -- can provide a learning\nsignal, and how this signal can be used. We introduce an environment for\nproducing such interaction data (with the help of a Large Language Model as\ncounterpart to the learner model), both offline and online. We investigate the\neffects of supervised fine-tuning on this data, as well as reinforcement\nlearning setups such as DPO, and GRPO; showing that all of these approaches\nachieve some improvements in in-domain games, but only GRPO demonstrates the\nability to generalise to out-of-domain games as well as retain competitive\nperformance in reference-based tasks. We release the framework and the baseline\ntraining setups in the hope that this can foster research in this promising new\ndirection."}
{"id": "2504.08291", "pdf": "https://arxiv.org/pdf/2504.08291", "abs": "https://arxiv.org/abs/2504.08291", "authors": ["Junjia Huang", "Pengxiang Yan", "Jiyang Liu", "Jie Wu", "Zhao Wang", "Yitong Wang", "Liang Lin", "Guanbin Li"], "title": "DreamFuse: Adaptive Image Fusion with Diffusion Transformer", "categories": ["cs.CV"], "comment": "under review", "summary": "Image fusion seeks to seamlessly integrate foreground objects with background\nscenes, producing realistic and harmonious fused images. Unlike existing\nmethods that directly insert objects into the background, adaptive and\ninteractive fusion remains a challenging yet appealing task. It requires the\nforeground to adjust or interact with the background context, enabling more\ncoherent integration. To address this, we propose an iterative\nhuman-in-the-loop data generation pipeline, which leverages limited initial\ndata with diverse textual prompts to generate fusion datasets across various\nscenarios and interactions, including placement, holding, wearing, and style\ntransfer. Building on this, we introduce DreamFuse, a novel approach based on\nthe Diffusion Transformer (DiT) model, to generate consistent and harmonious\nfused images with both foreground and background information. DreamFuse employs\na Positional Affine mechanism to inject the size and position of the foreground\ninto the background, enabling effective foreground-background interaction\nthrough shared attention. Furthermore, we apply Localized Direct Preference\nOptimization guided by human feedback to refine DreamFuse, enhancing background\nconsistency and foreground harmony. DreamFuse achieves harmonious fusion while\ngeneralizing to text-driven attribute editing of the fused results.\nExperimental results demonstrate that our method outperforms state-of-the-art\napproaches across multiple metrics."}
{"id": "2504.08596", "pdf": "https://arxiv.org/pdf/2504.08596", "abs": "https://arxiv.org/abs/2504.08596", "authors": ["Gaya Mehenni", "Amal Zouaq"], "title": "MedHal: An Evaluation Dataset for Medical Hallucination Detection", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": null, "summary": "We present MedHal, a novel large-scale dataset specifically designed to\nevaluate if models can detect hallucinations in medical texts. Current\nhallucination detection methods face significant limitations when applied to\nspecialized domains like medicine, where they can have disastrous consequences.\nExisting medical datasets are either too small, containing only a few hundred\nsamples, or focus on a single task like Question Answering or Natural Language\nInference. MedHal addresses these gaps by: (1) incorporating diverse medical\ntext sources and tasks; (2) providing a substantial volume of annotated samples\nsuitable for training medical hallucination detection models; and (3) including\nexplanations for factual inconsistencies to guide model learning. We\ndemonstrate MedHal's utility by training and evaluating a baseline medical\nhallucination detection model, showing improvements over general-purpose\nhallucination detection approaches. This resource enables more efficient\nevaluation of medical text generation systems while reducing reliance on costly\nexpert review, potentially accelerating the development of medical AI research."}
{"id": "2504.08296", "pdf": "https://arxiv.org/pdf/2504.08296", "abs": "https://arxiv.org/abs/2504.08296", "authors": ["Ruihan Zhang", "Borou Yu", "Jiajian Min", "Yetong Xin", "Zheng Wei", "Juncheng Nemo Shi", "Mingzhen Huang", "Xianghao Kong", "Nix Liu Xin", "Shanshan Jiang", "Praagya Bahuguna", "Mark Chan", "Khushi Hora", "Lijian Yang", "Yongqi Liang", "Runhe Bian", "Yunlei Liu", "Isabela Campillo Valencia", "Patricia Morales Tredinick", "Ilia Kozlov", "Sijia Jiang", "Peiwen Huang", "Na Chen", "Xuanxuan Liu", "Anyi Rao"], "title": "Generative AI for Film Creation: A Survey of Recent Advances", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025 CVEU workshop: AI for Creative Visual Content\n  Generation Editing and Understanding", "summary": "Generative AI (GenAI) is transforming filmmaking, equipping artists with\ntools like text-to-image and image-to-video diffusion, neural radiance fields,\navatar generation, and 3D synthesis. This paper examines the adoption of these\ntechnologies in filmmaking, analyzing workflows from recent AI-driven films to\nunderstand how GenAI contributes to character creation, aesthetic styling, and\nnarration. We explore key strategies for maintaining character consistency,\nachieving stylistic coherence, and ensuring motion continuity. Additionally, we\nhighlight emerging trends such as the growing use of 3D generation and the\nintegration of real footage with AI-generated elements.\n  Beyond technical advancements, we examine how GenAI is enabling new artistic\nexpressions, from generating hard-to-shoot footage to dreamlike diffusion-based\nmorphing effects, abstract visuals, and unworldly objects. We also gather\nartists' feedback on challenges and desired improvements, including\nconsistency, controllability, fine-grained editing, and motion refinement. Our\nstudy provides insights into the evolving intersection of AI and filmmaking,\noffering a roadmap for researchers and artists navigating this rapidly\nexpanding field."}
{"id": "2504.08609", "pdf": "https://arxiv.org/pdf/2504.08609", "abs": "https://arxiv.org/abs/2504.08609", "authors": ["Julian Bäumler", "Louis Blöcher", "Lars-Joel Frey", "Xian Chen", "Markus Bayer", "Christian Reuter"], "title": "A Survey of Machine Learning Models and Datasets for the Multi-label Classification of Textual Hate Speech in English", "categories": ["cs.CL", "cs.AI"], "comment": "35 pages, 4 figures, 4 tables", "summary": "The dissemination of online hate speech can have serious negative\nconsequences for individuals, online communities, and entire societies. This\nand the large volume of hateful online content prompted both practitioners',\ni.e., in content moderation or law enforcement, and researchers' interest in\nmachine learning models to automatically classify instances of hate speech.\nWhereas most scientific works address hate speech classification as a binary\ntask, practice often requires a differentiation into sub-types, e.g., according\nto target, severity, or legality, which may overlap for individual content.\nHence, researchers created datasets and machine learning models that approach\nhate speech classification in textual data as a multi-label problem. This work\npresents the first systematic and comprehensive survey of scientific literature\non this emerging research landscape in English (N=46). We contribute with a\nconcise overview of 28 datasets suited for training multi-label classification\nmodels that reveals significant heterogeneity regarding label-set, size,\nmeta-concept, annotation process, and inter-annotator agreement. Our analysis\nof 24 publications proposing suitable classification models further establishes\ninconsistency in evaluation and a preference for architectures based on\nBidirectional Encoder Representation from Transformers (BERT) and Recurrent\nNeural Networks (RNNs). We identify imbalanced training data, reliance on\ncrowdsourcing platforms, small and sparse datasets, and missing methodological\nalignment as critical open issues and formulate ten recommendations for\nresearch."}
{"id": "2504.08306", "pdf": "https://arxiv.org/pdf/2504.08306", "abs": "https://arxiv.org/abs/2504.08306", "authors": ["Kehuan Song", "Xinglin Xie", "Kexin Zhang", "Licheng Jiao", "Lingling Li", "Shuyuan Yang"], "title": "STSeg-Complex Video Object Segmentation: The 1st Solution for 4th PVUW MOSE Challenge", "categories": ["cs.CV"], "comment": null, "summary": "Segmentation of video objects in complex scenarios is highly challenging, and\nthe MOSE dataset has significantly contributed to the development of this\nfield. This technical report details the STSeg solution proposed by the\n\"imaplus\" team.By finetuning SAM2 and the unsupervised model TMO on the MOSE\ndataset, the STSeg solution demonstrates remarkable advantages in handling\ncomplex object motions and long-video sequences. In the inference phase, an\nAdaptive Pseudo-labels Guided Model Refinement Pipeline is adopted to\nintelligently select appropriate models for processing each video. Through\nfinetuning the models and employing the Adaptive Pseudo-labels Guided Model\nRefinement Pipeline in the inference phase, the STSeg solution achieved a J&F\nscore of 87.26% on the test set of the 2025 4th PVUW Challenge MOSE Track,\nsecuring the 1st place and advancing the technology for video object\nsegmentation in complex scenarios."}
{"id": "2504.08672", "pdf": "https://arxiv.org/pdf/2504.08672", "abs": "https://arxiv.org/abs/2504.08672", "authors": ["Fangzhi Xu", "Hang Yan", "Chang Ma", "Haiteng Zhao", "Qiushi Sun", "Kanzhi Cheng", "Junxian He", "Jun Liu", "Zhiyong Wu"], "title": "Genius: A Generalizable and Purely Unsupervised Self-Training Framework For Advanced Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "14 pages, 7 figures", "summary": "Advancing LLM reasoning skills has captivated wide interest. However, current\npost-training techniques rely heavily on supervisory signals, such as outcome\nsupervision or auxiliary reward models, which face the problem of scalability\nand high annotation costs. This motivates us to enhance LLM reasoning without\nthe need for external supervision. We introduce a generalizable and purely\nunsupervised self-training framework, named Genius. Without external auxiliary,\nGenius requires to seek the optimal response sequence in a stepwise manner and\noptimize the LLM. To explore the potential steps and exploit the optimal ones,\nGenius introduces a stepwise foresight re-sampling strategy to sample and\nestimate the step value by simulating future outcomes. Further, we recognize\nthat the unsupervised setting inevitably induces the intrinsic noise and\nuncertainty. To provide a robust optimization, we propose an\nadvantage-calibrated optimization (ACO) loss function to mitigate estimation\ninconsistencies. Combining these techniques together, Genius provides an\nadvanced initial step towards self-improve LLM reasoning with general queries\nand without supervision, revolutionizing reasoning scaling laws given the vast\navailability of general queries. The code will be released at\nhttps://github.com/xufangzhi/Genius."}
{"id": "2504.08307", "pdf": "https://arxiv.org/pdf/2504.08307", "abs": "https://arxiv.org/abs/2504.08307", "authors": ["Qinghongbing Xie", "Zijian Liang", "Long Zeng"], "title": "DSM: Building A Diverse Semantic Map for 3D Visual Grounding", "categories": ["cs.CV", "cs.RO"], "comment": "8 pages, 6 figures, submitted to IROS, Project Page:\n  https://binicey.github.io/DSM", "summary": "In recent years, with the growing research and application of multimodal\nlarge language models (VLMs) in robotics, there has been an increasing trend of\nutilizing VLMs for robotic scene understanding tasks. Existing approaches that\nuse VLMs for 3D Visual Grounding tasks often focus on obtaining scene\ninformation through geometric and visual information, overlooking the\nextraction of diverse semantic information from the scene and the understanding\nof rich implicit semantic attributes, such as appearance, physics, and\naffordance. The 3D scene graph, which combines geometry and language, is an\nideal representation method for environmental perception and is an effective\ncarrier for language models in 3D Visual Grounding tasks. To address these\nissues, we propose a diverse semantic map construction method specifically\ndesigned for robotic agents performing 3D Visual Grounding tasks. This method\nleverages VLMs to capture the latent semantic attributes and relations of\nobjects within the scene and creates a Diverse Semantic Map (DSM) through a\ngeometry sliding-window map construction strategy. We enhance the understanding\nof grounding information based on DSM and introduce a novel approach named\nDSM-Grounding. Experimental results show that our method outperforms current\napproaches in tasks like semantic segmentation and 3D Visual Grounding,\nparticularly excelling in overall metrics compared to the state-of-the-art. In\naddition, we have deployed this method on robots to validate its effectiveness\nin navigation and grasping tasks."}
{"id": "2504.08690", "pdf": "https://arxiv.org/pdf/2504.08690", "abs": "https://arxiv.org/abs/2504.08690", "authors": ["Yiliu Sun", "Yanfang Zhang", "Zicheng Zhao", "Sheng Wan", "Dacheng Tao", "Chen Gong"], "title": "Fast-Slow-Thinking: Complex Task Solving with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "37 pages, 7 figures", "summary": "Nowadays, Large Language Models (LLMs) have been gradually employed to solve\ncomplex tasks. To face the challenge, task decomposition has become an\neffective way, which proposes to divide a complex task into multiple simpler\nsubtasks and then solve them separately so that the difficulty of the original\ntask can be reduced. However, the performance of existing task decomposition\nmethods can be suboptimal when the task contains overly complex logic and\nconstraints. In this situation, the solution generated by LLMs may deviate from\nthe original purpose of the task, or contain redundant or even erroneous\ncontent. Therefore, inspired by the fact that humans possess two thinking\nsystems including fast thinking and slow thinking, this paper introduces a new\ntask decomposition method termed ``Fast-Slow-Thinking'' (FST), which stimulates\nLLMs to solve tasks through the cooperation of Fast Thinking (FT) and Slow\nThinking (ST) steps. Here FT focuses more on the general and concise aspect of\nthe task, and ST focuses more on the details of the task. In FT, LLMs are\nprompted to remove the constraints of the original task, therefore simplifying\nit to a general and concise one. In ST, we recall the constraints removed in\nFT, so that LLMs can improve the answer generated in FT to meet the\nrequirements of the original task. Therefore, our FST method enables LLMs to\nconsider a complex problem via a human-like cognition process from coarse to\nfine, the effectiveness of which has been well demonstrated by the experiments\non three types of tasks."}
{"id": "2504.08344", "pdf": "https://arxiv.org/pdf/2504.08344", "abs": "https://arxiv.org/abs/2504.08344", "authors": ["Renda Li", "Xiaohua Qi", "Qiang Ling", "Jun Yu", "Ziyi Chen", "Peng Chang", "Mei HanJing Xiao"], "title": "EasyGenNet: An Efficient Framework for Audio-Driven Gesture Video Generation Based on Diffusion Model", "categories": ["cs.CV"], "comment": null, "summary": "Audio-driven cospeech video generation typically involves two stages:\nspeech-to-gesture and gesture-to-video. While significant advances have been\nmade in speech-to-gesture generation, synthesizing natural expressions and\ngestures remains challenging in gesture-to-video systems. In order to improve\nthe generation effect, previous works adopted complex input and training\nstrategies and required a large amount of data sets for pre-training, which\nbrought inconvenience to practical applications. We propose a simple one-stage\ntraining method and a temporal inference method based on a diffusion model to\nsynthesize realistic and continuous gesture videos without the need for\nadditional training of temporal modules.The entire model makes use of existing\npre-trained weights, and only a few thousand frames of data are needed for each\ncharacter at a time to complete fine-tuning. Built upon the video generator, we\nintroduce a new audio-to-video pipeline to synthesize co-speech videos, using\n2D human skeleton as the intermediate motion representation. Our experiments\nshow that our method outperforms existing GAN-based and diffusion-based\nmethods."}
{"id": "2504.08694", "pdf": "https://arxiv.org/pdf/2504.08694", "abs": "https://arxiv.org/abs/2504.08694", "authors": ["Hang Ni", "Fan Liu", "Xinyu Ma", "Lixin Su", "Shuaiqiang Wang", "Dawei Yin", "Hui Xiong", "Hao Liu"], "title": "TP-RAG: Benchmarking Retrieval-Augmented Large Language Model Agents for Spatiotemporal-Aware Travel Planning", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown promise in automating travel\nplanning, yet they often fall short in addressing nuanced spatiotemporal\nrationality. While existing benchmarks focus on basic plan validity, they\nneglect critical aspects such as route efficiency, POI appeal, and real-time\nadaptability. This paper introduces TP-RAG, the first benchmark tailored for\nretrieval-augmented, spatiotemporal-aware travel planning. Our dataset includes\n2,348 real-world travel queries, 85,575 fine-grain annotated POIs, and 18,784\nhigh-quality travel trajectory references sourced from online tourist\ndocuments, enabling dynamic and context-aware planning. Through extensive\nexperiments, we reveal that integrating reference trajectories significantly\nimproves spatial efficiency and POI rationality of the travel plan, while\nchallenges persist in universality and robustness due to conflicting references\nand noisy data. To address these issues, we propose EvoRAG, an evolutionary\nframework that potently synergizes diverse retrieved trajectories with LLMs'\nintrinsic reasoning. EvoRAG achieves state-of-the-art performance, improving\nspatiotemporal compliance and reducing commonsense violation compared to\nground-up and retrieval-augmented baselines. Our work underscores the potential\nof hybridizing Web knowledge with LLM-driven optimization, paving the way for\nmore reliable and adaptive travel planning agents."}
{"id": "2504.08348", "pdf": "https://arxiv.org/pdf/2504.08348", "abs": "https://arxiv.org/abs/2504.08348", "authors": ["Josef Bengtson", "David Nilsson", "Fredrik Kahl"], "title": "Geometric Consistency Refinement for Single Image Novel View Synthesis via Test-Time Adaptation of Diffusion Models", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025 EDGE Workshop. Project page:\n  https://gc-ref.github.io/", "summary": "Diffusion models for single image novel view synthesis (NVS) can generate\nhighly realistic and plausible images, but they are limited in the geometric\nconsistency to the given relative poses. The generated images often show\nsignificant errors with respect to the epipolar constraints that should be\nfulfilled, as given by the target pose. In this paper we address this issue by\nproposing a methodology to improve the geometric correctness of images\ngenerated by a diffusion model for single image NVS. We formulate a loss\nfunction based on image matching and epipolar constraints, and optimize the\nstarting noise in a diffusion sampling process such that the generated image\nshould both be a realistic image and fulfill geometric constraints derived from\nthe given target pose. Our method does not require training data or fine-tuning\nof the diffusion models, and we show that we can apply it to multiple\nstate-of-the-art models for single image NVS. The method is evaluated on the\nMegaScenes dataset and we show that geometric consistency is improved compared\nto the baseline models while retaining the quality of the generated images."}
{"id": "2504.08697", "pdf": "https://arxiv.org/pdf/2504.08697", "abs": "https://arxiv.org/abs/2504.08697", "authors": ["Zdeněk Kasner", "Vilém Zouhar", "Patrícia Schmidtová", "Ivan Kartáč", "Kristýna Onderková", "Ondřej Plátek", "Dimitra Gkatzia", "Saad Mahamood", "Ondřej Dušek", "Simone Balloccu"], "title": "Large Language Models as Span Annotators", "categories": ["cs.CL"], "comment": null, "summary": "For high-quality texts, single-score metrics seldom provide actionable\nfeedback. In contrast, span annotation - pointing out issues in the text by\nannotating their spans - can guide improvements and provide insights. Until\nrecently, span annotation was limited to human annotators or fine-tuned encoder\nmodels. In this study, we automate span annotation with large language models\n(LLMs). We compare expert or skilled crowdworker annotators with open and\nproprietary LLMs on three tasks: data-to-text generation evaluation, machine\ntranslation evaluation, and propaganda detection in human-written texts. In our\nexperiments, we show that LLMs as span annotators are straightforward to\nimplement and notably more cost-efficient than human annotators. The LLMs\nachieve moderate agreement with skilled human annotators, in some scenarios\ncomparable to the average agreement among the annotators themselves.\nQualitative analysis shows that reasoning models outperform their\ninstruction-tuned counterparts and provide more valid explanations for\nannotations. We release the dataset of more than 40k model and human\nannotations for further research."}
{"id": "2504.08358", "pdf": "https://arxiv.org/pdf/2504.08358", "abs": "https://arxiv.org/abs/2504.08358", "authors": ["Jiarui Wang", "Huiyu Duan", "Yu Zhao", "Juntong Wang", "Guangtao Zhai", "Xiongkuo Min"], "title": "LMM4LMM: Benchmarking and Evaluating Large-multimodal Image Generation with LMMs", "categories": ["cs.CV"], "comment": null, "summary": "Recent breakthroughs in large multimodal models (LMMs) have significantly\nadvanced both text-to-image (T2I) generation and image-to-text (I2T)\ninterpretation. However, many generated images still suffer from issues related\nto perceptual quality and text-image alignment. Given the high cost and\ninefficiency of manual evaluation, an automatic metric that aligns with human\npreferences is desirable. To this end, we present EvalMi-50K, a comprehensive\ndataset and benchmark for evaluating large-multimodal image generation, which\nfeatures (i) comprehensive tasks, encompassing 2,100 extensive prompts across\n20 fine-grained task dimensions, and (ii) large-scale human-preference\nannotations, including 100K mean-opinion scores (MOSs) and 50K\nquestion-answering (QA) pairs annotated on 50,400 images generated from 24 T2I\nmodels. Based on EvalMi-50K, we propose LMM4LMM, an LMM-based metric for\nevaluating large multimodal T2I generation from multiple dimensions including\nperception, text-image correspondence, and task-specific accuracy. Extensive\nexperimental results show that LMM4LMM achieves state-of-the-art performance on\nEvalMi-50K, and exhibits strong generalization ability on other AI-generated\nimage evaluation benchmark datasets, manifesting the generality of both the\nEvalMi-50K dataset and LMM4LMM metric. Both EvalMi-50K and LMM4LMM will be\nreleased at https://github.com/IntMeGroup/LMM4LMM."}
{"id": "2504.08716", "pdf": "https://arxiv.org/pdf/2504.08716", "abs": "https://arxiv.org/abs/2504.08716", "authors": ["Wissam Antoun", "Benoît Sagot", "Djamé Seddah"], "title": "ModernBERT or DeBERTaV3? Examining Architecture and Data Influence on Transformer Encoder Models Performance", "categories": ["cs.CL"], "comment": "Preprint. Under review", "summary": "Pretrained transformer-encoder models like DeBERTaV3 and ModernBERT introduce\narchitectural advancements aimed at improving efficiency and performance.\nAlthough the authors of ModernBERT report improved performance over DeBERTaV3\non several benchmarks, the lack of disclosed training data and the absence of\ncomparisons using a shared dataset make it difficult to determine whether these\ngains are due to architectural improvements or differences in training data. In\nthis work, we conduct a controlled study by pretraining ModernBERT on the same\ndataset as CamemBERTaV2, a DeBERTaV3 French model, isolating the effect of\nmodel design. Our results show that the previous model generation remains\nsuperior in sample efficiency and overall benchmark performance, with\nModernBERT's primary advantage being faster training and inference speed.\nHowever, the new proposed model still provides meaningful architectural\nimprovements compared to earlier models such as BERT and RoBERTa. Additionally,\nwe observe that high-quality pre-training data accelerates convergence but does\nnot significantly improve final performance, suggesting potential benchmark\nsaturation. These findings show the importance of disentangling pretraining\ndata from architectural innovations when evaluating transformer models."}
{"id": "2504.08361", "pdf": "https://arxiv.org/pdf/2504.08361", "abs": "https://arxiv.org/abs/2504.08361", "authors": ["Yi Chen", "Tianchen Deng", "Wentao Zhao", "Xiaoning Wang", "Wenqian Xi", "Weidong Chen", "Jingchuan Wang"], "title": "SN-LiDAR: Semantic Neural Fields for Novel Space-time View LiDAR Synthesis", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Recent research has begun exploring novel view synthesis (NVS) for LiDAR\npoint clouds, aiming to generate realistic LiDAR scans from unseen viewpoints.\nHowever, most existing approaches do not reconstruct semantic labels, which are\ncrucial for many downstream applications such as autonomous driving and robotic\nperception. Unlike images, which benefit from powerful segmentation models,\nLiDAR point clouds lack such large-scale pre-trained models, making semantic\nannotation time-consuming and labor-intensive. To address this challenge, we\npropose SN-LiDAR, a method that jointly performs accurate semantic\nsegmentation, high-quality geometric reconstruction, and realistic LiDAR\nsynthesis. Specifically, we employ a coarse-to-fine planar-grid feature\nrepresentation to extract global features from multi-frame point clouds and\nleverage a CNN-based encoder to extract local semantic features from the\ncurrent frame point cloud. Extensive experiments on SemanticKITTI and KITTI-360\ndemonstrate the superiority of SN-LiDAR in both semantic and geometric\nreconstruction, effectively handling dynamic objects and large-scale scenes.\nCodes will be available on https://github.com/dtc111111/SN-Lidar."}
{"id": "2504.08719", "pdf": "https://arxiv.org/pdf/2504.08719", "abs": "https://arxiv.org/abs/2504.08719", "authors": ["Krishna C. Puvvada", "Faisal Ladhak", "Santiago Akle Serrano", "Cheng-Ping Hsieh", "Shantanu Acharya", "Somshubra Majumdar", "Fei Jia", "Samuel Kriman", "Simeng Sun", "Dima Rekesh", "Boris Ginsburg"], "title": "SWAN-GPT: An Efficient and Scalable Approach for Long-Context Language Modeling", "categories": ["cs.CL"], "comment": null, "summary": "We present a decoder-only Transformer architecture that robustly generalizes\nto sequence lengths substantially longer than those seen during training. Our\nmodel, SWAN-GPT, interleaves layers without positional encodings (NoPE) and\nsliding-window attention layers equipped with rotary positional encodings\n(SWA-RoPE). Experiments demonstrate strong performance on sequence lengths\nsignificantly longer than the training length without the need for additional\nlong-context training. This robust length extrapolation is achieved through our\nnovel architecture, enhanced by a straightforward dynamic scaling of attention\nscores during inference. In addition, SWAN-GPT is more computationally\nefficient than standard GPT architectures, resulting in cheaper training and\nhigher throughput. Further, we demonstrate that existing pre-trained\ndecoder-only models can be efficiently converted to the SWAN architecture with\nminimal continued training, enabling longer contexts. Overall, our work\npresents an effective approach for scaling language models to longer contexts\nin a robust and efficient manner."}
{"id": "2504.08368", "pdf": "https://arxiv.org/pdf/2504.08368", "abs": "https://arxiv.org/abs/2504.08368", "authors": ["Cheng-Yu Hsieh", "Pavan Kumar Anasosalu Vasu", "Fartash Faghri", "Raviteja Vemulapalli", "Chun-Liang Li", "Ranjay Krishna", "Oncel Tuzel", "Hadi Pouransari"], "title": "FocalLens: Instruction Tuning Enables Zero-Shot Conditional Image Representations", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "Visual understanding is inherently contextual -- what we focus on in an image\ndepends on the task at hand. For instance, given an image of a person holding a\nbouquet of flowers, we may focus on either the person such as their clothing,\nor the type of flowers, depending on the context of interest. Yet, most\nexisting image encoding paradigms represent an image as a fixed, generic\nfeature vector, overlooking the potential needs of prioritizing varying visual\ninformation for different downstream use cases. In this work, we introduce\nFocalLens, a conditional visual encoding method that produces different\nrepresentations for the same image based on the context of interest, expressed\nflexibly through natural language. We leverage vision instruction tuning data\nand contrastively finetune a pretrained vision encoder to take natural language\ninstructions as additional inputs for producing conditional image\nrepresentations. Extensive experiments validate that conditional image\nrepresentation from FocalLens better pronounce the visual features of interest\ncompared to generic features produced by standard vision encoders like CLIP. In\naddition, we show FocalLens further leads to performance improvements on a\nrange of downstream tasks including image-image retrieval, image\nclassification, and image-text retrieval, with an average gain of 5 and 10\npoints on the challenging SugarCrepe and MMVP-VLM benchmarks, respectively."}
{"id": "2504.08044", "pdf": "https://arxiv.org/pdf/2504.08044", "abs": "https://arxiv.org/abs/2504.08044", "authors": ["Tanmay Laud", "Akadia Kacha-Ochana", "Steven A. Sumner", "Vikram Krishnasamy", "Royal Law", "Lyna Schieber", "Munmun De Choudhury", "Mai ElSherief"], "title": "Large-Scale Analysis of Online Questions Related to Opioid Use Disorder on Reddit", "categories": ["cs.SI", "cs.CL"], "comment": "Accepted to ICWSM 2025", "summary": "Opioid use disorder (OUD) is a leading health problem that affects individual\nwell-being as well as general public health. Due to a variety of reasons,\nincluding the stigma faced by people using opioids, online communities for\nrecovery and support were formed on different social media platforms. In these\ncommunities, people share their experiences and solicit information by asking\nquestions to learn about opioid use and recovery. However, these communities do\nnot always contain clinically verified information. In this paper, we study\nnatural language questions asked in the context of OUD-related discourse on\nReddit. We adopt transformer-based question detection along with hierarchical\nclustering across 19 subreddits to identify six coarse-grained categories and\n69 fine-grained categories of OUD-related questions. Our analysis uncovers ten\nareas of information seeking from Reddit users in the context of OUD: drug\nsales, specific drug-related questions, OUD treatment, drug uses, side effects,\nwithdrawal, lifestyle, drug testing, pain management and others, during the\nstudy period of 2018-2021. Our work provides a major step in improving the\nunderstanding of OUD-related questions people ask unobtrusively on Reddit. We\nfinally discuss technological interventions and public health harm reduction\ntechniques based on the topics of these questions."}
{"id": "2504.08384", "pdf": "https://arxiv.org/pdf/2504.08384", "abs": "https://arxiv.org/abs/2504.08384", "authors": ["Huu-Loc Tran", "Tinh-Anh Nguyen-Nhu", "Huu-Phong Phan-Nguyen", "Tien-Huy Nguyen", "Nhat-Minh Nguyen-Dich", "Anh Dao", "Huy-Duc Do", "Quan Nguyen", "Hoang M. Le", "Quang-Vinh Dinh"], "title": "Towards Efficient and Robust Moment Retrieval System: A Unified Framework for Multi-Granularity Models and Temporal Reranking", "categories": ["cs.CV"], "comment": null, "summary": "Long-form video understanding presents significant challenges for interactive\nretrieval systems, as conventional methods struggle to process extensive video\ncontent efficiently. Existing approaches often rely on single models,\ninefficient storage, unstable temporal search, and context-agnostic reranking,\nlimiting their effectiveness. This paper presents a novel framework to enhance\ninteractive video retrieval through four key innovations: (1) an ensemble\nsearch strategy that integrates coarse-grained (CLIP) and fine-grained (BEIT3)\nmodels to improve retrieval accuracy, (2) a storage optimization technique that\nreduces redundancy by selecting representative keyframes via TransNetV2 and\ndeduplication, (3) a temporal search mechanism that localizes video segments\nusing dual queries for start and end points, and (4) a temporal reranking\napproach that leverages neighboring frame context to stabilize rankings.\nEvaluated on known-item search and question-answering tasks, our framework\ndemonstrates substantial improvements in retrieval precision, efficiency, and\nuser interpretability, offering a robust solution for real-world interactive\nvideo retrieval applications."}
{"id": "2504.08066", "pdf": "https://arxiv.org/pdf/2504.08066", "abs": "https://arxiv.org/abs/2504.08066", "authors": ["Yutaro Yamada", "Robert Tjarko Lange", "Cong Lu", "Shengran Hu", "Chris Lu", "Jakob Foerster", "Jeff Clune", "David Ha"], "title": "The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "AI is increasingly playing a pivotal role in transforming how scientific\ndiscoveries are made. We introduce The AI Scientist-v2, an end-to-end agentic\nsystem capable of producing the first entirely AI generated\npeer-review-accepted workshop paper. This system iteratively formulates\nscientific hypotheses, designs and executes experiments, analyzes and\nvisualizes data, and autonomously authors scientific manuscripts. Compared to\nits predecessor (v1, Lu et al., 2024 arXiv:2408.06292), The AI Scientist-v2\neliminates the reliance on human-authored code templates, generalizes\neffectively across diverse machine learning domains, and leverages a novel\nprogressive agentic tree-search methodology managed by a dedicated experiment\nmanager agent. Additionally, we enhance the AI reviewer component by\nintegrating a Vision-Language Model (VLM) feedback loop for iterative\nrefinement of content and aesthetics of the figures. We evaluated The AI\nScientist-v2 by submitting three fully autonomous manuscripts to a\npeer-reviewed ICLR workshop. Notably, one manuscript achieved high enough\nscores to exceed the average human acceptance threshold, marking the first\ninstance of a fully AI-generated paper successfully navigating a peer review.\nThis accomplishment highlights the growing capability of AI in conducting all\naspects of scientific research. We anticipate that further advancements in\nautonomous scientific discovery technologies will profoundly impact human\nknowledge generation, enabling unprecedented scalability in research\nproductivity and significantly accelerating scientific breakthroughs, greatly\nbenefiting society at large. We have open-sourced the code at\nhttps://github.com/SakanaAI/AI-Scientist-v2 to foster the future development of\nthis transformative technology. We also discuss the role of AI in science,\nincluding AI safety."}
{"id": "2504.08388", "pdf": "https://arxiv.org/pdf/2504.08388", "abs": "https://arxiv.org/abs/2504.08388", "authors": ["Junliang Guo", "Yang Ye", "Tianyu He", "Haoyu Wu", "Yushu Jiang", "Tim Pearce", "Jiang Bian"], "title": "MineWorld: a Real-Time and Open-Source Interactive World Model on Minecraft", "categories": ["cs.CV", "cs.AI"], "comment": "Technical report. Project page https://aka.ms/mineworld", "summary": "World modeling is a crucial task for enabling intelligent agents to\neffectively interact with humans and operate in dynamic environments. In this\nwork, we propose MineWorld, a real-time interactive world model on Minecraft,\nan open-ended sandbox game which has been utilized as a common testbed for\nworld modeling. MineWorld is driven by a visual-action autoregressive\nTransformer, which takes paired game scenes and corresponding actions as input,\nand generates consequent new scenes following the actions. Specifically, by\ntransforming visual game scenes and actions into discrete token ids with an\nimage tokenizer and an action tokenizer correspondingly, we consist the model\ninput with the concatenation of the two kinds of ids interleaved. The model is\nthen trained with next token prediction to learn rich representations of game\nstates as well as the conditions between states and actions simultaneously. In\ninference, we develop a novel parallel decoding algorithm that predicts the\nspatial redundant tokens in each frame at the same time, letting models in\ndifferent scales generate $4$ to $7$ frames per second and enabling real-time\ninteractions with game players. In evaluation, we propose new metrics to assess\nnot only visual quality but also the action following capacity when generating\nnew scenes, which is crucial for a world model. Our comprehensive evaluation\nshows the efficacy of MineWorld, outperforming SoTA open-sourced diffusion\nbased world models significantly. The code and model have been released."}
{"id": "2504.08104", "pdf": "https://arxiv.org/pdf/2504.08104", "abs": "https://arxiv.org/abs/2504.08104", "authors": ["Tianyi Wu", "Zhiwei Xue", "Yue Liu", "Jiaheng Zhang", "Bryan Hooi", "See-Kiong Ng"], "title": "Geneshift: Impact of different scenario shift on Jailbreaking LLM", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "Jailbreak attacks, which aim to cause LLMs to perform unrestricted behaviors,\nhave become a critical and challenging direction in AI safety. Despite\nachieving the promising attack success rate using dictionary-based evaluation,\nexisting jailbreak attack methods fail to output detailed contents to satisfy\nthe harmful request, leading to poor performance on GPT-based evaluation. To\nthis end, we propose a black-box jailbreak attack termed GeneShift, by using a\ngenetic algorithm to optimize the scenario shifts. Firstly, we observe that the\nmalicious queries perform optimally under different scenario shifts. Based on\nit, we develop a genetic algorithm to evolve and select the hybrid of scenario\nshifts. It guides our method to elicit detailed and actionable harmful\nresponses while keeping the seemingly benign facade, improving stealthiness.\nExtensive experiments demonstrate the superiority of GeneShift. Notably,\nGeneShift increases the jailbreak success rate from 0% to 60% when direct\nprompting alone would fail."}
{"id": "2504.08389", "pdf": "https://arxiv.org/pdf/2504.08389", "abs": "https://arxiv.org/abs/2504.08389", "authors": ["Jiawei Lan", "Zhibiao Wang", "Haoyang Yu", "Ye Tao", "Wenhua Cui"], "title": "Light-YOLOv8-Flame: A Lightweight High-Performance Flame Detection Algorithm", "categories": ["cs.CV"], "comment": "12 pages, 19 figures, 6 tables. Submitted to Engineering Letters", "summary": "Fire detection algorithms, particularly those based on computer vision,\nencounter significant challenges such as high computational costs and delayed\nresponse times, which hinder their application in real-time systems. To address\nthese limitations, this paper introduces Light-YOLOv8-Flame, a lightweight\nflame detection algorithm specifically designed for fast and efficient\nreal-time deployment. The proposed model enhances the YOLOv8 architecture\nthrough the substitution of the original C2f module with the FasterNet Block\nmodule. This new block combines Partial Convolution (PConv) and Convolution\n(Conv) layers, reducing both computational complexity and model size. A dataset\ncomprising 7,431 images, representing both flame and non-flame scenarios, was\ncollected and augmented for training purposes. Experimental findings indicate\nthat the modified YOLOv8 model achieves a 0.78% gain in mean average precision\n(mAP) and a 2.05% boost in recall, while reducing the parameter count by\n25.34%, with only a marginal decrease in precision by 0.82%. These findings\nhighlight that Light-YOLOv8-Flame offers enhanced detection performance and\nspeed, making it well-suited for real-time fire detection on\nresource-constrained devices."}
{"id": "2504.08192", "pdf": "https://arxiv.org/pdf/2504.08192", "abs": "https://arxiv.org/abs/2504.08192", "authors": ["Aashiq Muhamed", "Jacopo Bonato", "Mona Diab", "Virginia Smith"], "title": "SAEs $\\textit{Can}$ Improve Unlearning: Dynamic Sparse Autoencoder Guardrails for Precision Unlearning in LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "Machine unlearning is a promising approach to improve LLM safety by removing\nunwanted knowledge from the model. However, prevailing gradient-based\nunlearning methods suffer from issues such as high computational costs,\nhyperparameter instability, poor sequential unlearning capability,\nvulnerability to relearning attacks, low data efficiency, and lack of\ninterpretability. While Sparse Autoencoders are well-suited to improve these\naspects by enabling targeted activation-based unlearning, prior approaches\nunderperform gradient-based methods. This work demonstrates that, contrary to\nthese earlier findings, SAEs can significantly improve unlearning when employed\ndynamically. We introduce $\\textbf{Dynamic DAE Guardrails}$ (DSG), a novel\nmethod for precision unlearning that leverages principled feature selection and\na dynamic classifier. Our experiments show DSG substantially outperforms\nleading unlearning methods, achieving superior forget-utility trade-offs. DSG\naddresses key drawbacks of gradient-based approaches for unlearning -- offering\nenhanced computational efficiency and stability, robust performance in\nsequential unlearning, stronger resistance to relearning attacks, better data\nefficiency including zero-shot settings, and more interpretable unlearning."}
{"id": "2504.08410", "pdf": "https://arxiv.org/pdf/2504.08410", "abs": "https://arxiv.org/abs/2504.08410", "authors": ["Mingzhi Pei", "Xu Cao", "Xiangyi Wang", "Heng Guo", "Zhanyu Ma"], "title": "PMNI: Pose-free Multi-view Normal Integration for Reflective and Textureless Surface Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Reflective and textureless surfaces remain a challenge in multi-view 3D\nreconstruction.Both camera pose calibration and shape reconstruction often fail\ndue to insufficient or unreliable cross-view visual features. To address these\nissues, we present PMNI (Pose-free Multi-view Normal Integration), a neural\nsurface reconstruction method that incorporates rich geometric information by\nleveraging surface normal maps instead of RGB images. By enforcing geometric\nconstraints from surface normals and multi-view shape consistency within a\nneural signed distance function (SDF) optimization framework, PMNI\nsimultaneously recovers accurate camera poses and high-fidelity surface\ngeometry. Experimental results on synthetic and real-world datasets show that\nour method achieves state-of-the-art performance in the reconstruction of\nreflective surfaces, even without reliable initial camera poses."}
{"id": "2504.08247", "pdf": "https://arxiv.org/pdf/2504.08247", "abs": "https://arxiv.org/abs/2504.08247", "authors": ["Liu Xiao", "Li Zhiyuan", "Lin Yueyu"], "title": "Millions of States: Designing a Scalable MoE Architecture with RWKV-7 Meta-learner", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "State-based sequence models like RWKV-7 offer a compelling alternative to\nTransformer architectures, achieving linear complexity while demonstrating\ngreater expressive power in short-context scenarios and enabling state tracking\nbeyond the \\(\\text{TC}^0\\) complexity class. However, RWKV-7 lacks mechanisms\nfor token-parameter interactions and native scalability, limiting its\nadaptability and growth without retraining. In this paper, we propose\n\\textbf{Meta-State}, a novel extension to RWKV-7 that replaces attention\nmechanisms with a fully state-driven approach, integrating token-parameter\ninteractions through a \\textbf{Self-State Encoder} (SSE) mechanism. The SSE\nrepurposes a portion of the RWKV-7 Weighted Key-Value (WKV) state as\ntransformation weights to encode token-parameter interactions in a linear,\nstate-driven manner without introducing new trainable matrices or softmax\noperations, while preserving the autoregressive property of token processing.\nMeta-State supports progressive model scaling by expanding the WKV state and\nparameter tokens, reusing existing parameters without retraining. Our approach\nbridges the gap between state-based modeling, token-parameter interactions, and\nscalable architectures, offering a flexible framework for efficient and\nadaptable sequence modeling with linear complexity and constant memory usage."}
{"id": "2504.08411", "pdf": "https://arxiv.org/pdf/2504.08411", "abs": "https://arxiv.org/abs/2504.08411", "authors": ["Dawei Zhou", "Suzhi Gang", "Decheng Liu", "Tongliang Liu", "Nannan Wang", "Xinbo Gao"], "title": "A Knowledge-guided Adversarial Defense for Resisting Malicious Visual Manipulation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Malicious applications of visual manipulation have raised serious threats to\nthe security and reputation of users in many fields. To alleviate these issues,\nadversarial noise-based defenses have been enthusiastically studied in recent\nyears. However, ``data-only\" methods tend to distort fake samples in the\nlow-level feature space rather than the high-level semantic space, leading to\nlimitations in resisting malicious manipulation. Frontier research has shown\nthat integrating knowledge in deep learning can produce reliable and\ngeneralizable solutions. Inspired by these, we propose a knowledge-guided\nadversarial defense (KGAD) to actively force malicious manipulation models to\noutput semantically confusing samples. Specifically, in the process of\ngenerating adversarial noise, we focus on constructing significant semantic\nconfusions at the domain-specific knowledge level, and exploit a metric closely\nrelated to visual perception to replace the general pixel-wise metrics. The\ngenerated adversarial noise can actively interfere with the malicious\nmanipulation model by triggering knowledge-guided and perception-related\ndisruptions in the fake samples. To validate the effectiveness of the proposed\nmethod, we conduct qualitative and quantitative experiments on human perception\nand visual quality assessment. The results on two different tasks both show\nthat our defense provides better protection compared to state-of-the-art\nmethods and achieves great generalizability."}
{"id": "2504.08269", "pdf": "https://arxiv.org/pdf/2504.08269", "abs": "https://arxiv.org/abs/2504.08269", "authors": ["Qi Zhi Lim", "Chin Poo Lee", "Kian Ming Lim", "Kalaiarasi Sonai Muthu Anbananthen"], "title": "VLMT: Vision-Language Multimodal Transformer for Multimodal Multi-hop Question Answering", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "The increasing availability of multimodal data across text, tables, and\nimages presents new challenges for developing models capable of complex\ncross-modal reasoning. Existing methods for Multimodal Multi-hop Question\nAnswering (MMQA) often suffer from limited reasoning capabilities, reliance on\nmodality conversion, and inadequate alignment between visual and textual\nrepresentations. To address these limitations, this paper introduces\nVision-Language Multimodal Transformer (VLMT), a unified architecture that\nintegrates a transformer-based vision encoder with a sequence-to-sequence\nlanguage model. VLMT employs a direct token-level injection mechanism to fuse\nvisual and textual inputs within a shared embedding space, eliminating the need\nfor intermediate projection layers. To enhance cross-modal alignment and\nreasoning, a three-stage pretraining strategy is proposed to progressively\nalign vision-language representations and improve the model's capacity for\nmultimodal understanding. Based on the pretrained backbone, two task-specific\nmodules are instantiated to form a two-stage MMQA framework: a multimodal\nreranker that predicts document relevance scores and utilizes a relative\nthreshold with top-k strategy for context retrieval, and a multimodal question\nanswering model that generates contextually grounded answers based on the\nretrieved evidence. Comprehensive experiments on two benchmark datasets\ndemonstrate the effectiveness of the proposed approach. On MultimodalQA\nvalidation set, VLMT-Large achieves 76.5% Exact Match and 80.1% F1,\noutperforming the previous state-of-the-art by +9.1% in Exact Match and +8.8%\nin F1. On WebQA, it attains a QA score of 47.6, surpassing prior models such as\nPERQA by +3.2. These results highlight VLMT's strong capabilities in multimodal\nreasoning and its potential to advance real-world information retrieval and\nquestion answering systems."}
{"id": "2504.08412", "pdf": "https://arxiv.org/pdf/2504.08412", "abs": "https://arxiv.org/abs/2504.08412", "authors": ["Chao Qi", "Jianqin Yin", "Meng Chen", "Yingchun Niu", "Yuan Sun"], "title": "Boosting the Class-Incremental Learning in 3D Point Clouds via Zero-Collection-Cost Basic Shape Pre-Training", "categories": ["cs.CV"], "comment": null, "summary": "Existing class-incremental learning methods in 3D point clouds rely on\nexemplars (samples of former classes) to resist the catastrophic forgetting of\nmodels, and exemplar-free settings will greatly degrade the performance. For\nexemplar-free incremental learning, the pre-trained model methods have achieved\nstate-of-the-art results in 2D domains. However, these methods cannot be\nmigrated to the 3D domains due to the limited pre-training datasets and\ninsufficient focus on fine-grained geometric details. This paper breaks through\nthese limitations, proposing a basic shape dataset with zero collection cost\nfor model pre-training. It helps a model obtain extensive knowledge of 3D\ngeometries. Based on this, we propose a framework embedded with 3D geometry\nknowledge for incremental learning in point clouds, compatible with\nexemplar-free (-based) settings. In the incremental stage, the geometry\nknowledge is extended to represent objects in point clouds. The class prototype\nis calculated by regularizing the data representation with the same category\nand is kept adjusting in the learning process. It helps the model remember the\nshape features of different categories. Experiments show that our method\noutperforms other baseline methods by a large margin on various benchmark\ndatasets, considering both exemplar-free (-based) settings."}
{"id": "2504.08274", "pdf": "https://arxiv.org/pdf/2504.08274", "abs": "https://arxiv.org/abs/2504.08274", "authors": ["Haowei Lou", "Hye-young Paik", "Sheng Li", "Wen Hu", "Lina Yao"], "title": "Generalized Multilingual Text-to-Speech Generation with Language-Aware Style Adaptation", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "Text-to-Speech (TTS) models can generate natural, human-like speech across\nmultiple languages by transforming phonemes into waveforms. However,\nmultilingual TTS remains challenging due to discrepancies in phoneme\nvocabularies and variations in prosody and speaking style across languages.\nExisting approaches either train separate models for each language, which\nachieve high performance at the cost of increased computational resources, or\nuse a unified model for multiple languages that struggles to capture\nfine-grained, language-specific style variations. In this work, we propose\nLanStyleTTS, a non-autoregressive, language-aware style adaptive TTS framework\nthat standardizes phoneme representations and enables fine-grained,\nphoneme-level style control across languages. This design supports a unified\nmultilingual TTS model capable of producing accurate and high-quality speech\nwithout the need to train language-specific models. We evaluate LanStyleTTS by\nintegrating it with several state-of-the-art non-autoregressive TTS\narchitectures. Results show consistent performance improvements across\ndifferent model backbones. Furthermore, we investigate a range of acoustic\nfeature representations, including mel-spectrograms and autoencoder-derived\nlatent features. Our experiments demonstrate that latent encodings can\nsignificantly reduce model size and computational cost while preserving\nhigh-quality speech generation."}
{"id": "2504.08414", "pdf": "https://arxiv.org/pdf/2504.08414", "abs": "https://arxiv.org/abs/2504.08414", "authors": ["Jun Yan", "Huilin Yin"], "title": "Adversarial Examples in Environment Perception for Automated Driving (Review)", "categories": ["cs.CV"], "comment": "One chapter of upcoming Springer book: Recent Advances in Autonomous\n  Vehicle Technology, 2025", "summary": "The renaissance of deep learning has led to the massive development of\nautomated driving. However, deep neural networks are vulnerable to adversarial\nexamples. The perturbations of adversarial examples are imperceptible to human\neyes but can lead to the false predictions of neural networks. It poses a huge\nrisk to artificial intelligence (AI) applications for automated driving. This\nsurvey systematically reviews the development of adversarial robustness\nresearch over the past decade, including the attack and defense methods and\ntheir applications in automated driving. The growth of automated driving pushes\nforward the realization of trustworthy AI applications. This review lists\nsignificant references in the research history of adversarial examples."}
{"id": "2504.08329", "pdf": "https://arxiv.org/pdf/2504.08329", "abs": "https://arxiv.org/abs/2504.08329", "authors": ["Junmo Kim", "Namkyeong Lee", "Jiwon Kim", "Kwangsoo Kim"], "title": "MedRep: Medical Concept Representation for General Electronic Health Record Foundation Models", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Under review", "summary": "Electronic health record (EHR) foundation models have been an area ripe for\nexploration with their improved performance in various medical tasks. Despite\nthe rapid advances, there exists a fundamental limitation: Processing unseen\nmedical codes out of the vocabulary. This problem limits the generality of EHR\nfoundation models and the integration of models trained with different\nvocabularies. To deal with this problem, we propose MedRep for EHR foundation\nmodels based on the observational medical outcome partnership (OMOP) common\ndata model (CDM), providing the integrated medical concept representations and\nthe basic data augmentation strategy for patient trajectories. For concept\nrepresentation learning, we enrich the information of each concept with a\nminimal definition through large language model (LLM) prompts and enhance the\ntext-based representations through graph ontology of OMOP vocabulary.\nTrajectory augmentation randomly replaces selected concepts with other similar\nconcepts that have closely related representations to let the model practice\nwith the concepts out-of-vocabulary. Finally, we demonstrate that EHR\nfoundation models trained with MedRep better maintain the prediction\nperformance in external datasets. Our code implementation is publicly available\nat https://github.com/kicarussays/MedRep."}
{"id": "2504.08419", "pdf": "https://arxiv.org/pdf/2504.08419", "abs": "https://arxiv.org/abs/2504.08419", "authors": ["Ruizhe Wang", "Junyan Yang", "Qiao Wang"], "title": "GeoTexBuild: 3D Building Model Generation from Map Footprints", "categories": ["cs.CV"], "comment": "16 pages(excluding references), 10 figures", "summary": "We introduce GeoTexBuild, a modular generative framework for creating 3D\nbuilding models from map footprints. The proposed framework employs a\nthree-stage process comprising height map generation, geometry reconstruction,\nand appearance stylization, culminating in building models with intricate\ngeometry and appearance attributes. By integrating customized ControlNet and\nText2Mesh models, we explore effective methods for controlling both geometric\nand visual attributes during the generation process. By this, we eliminate the\nproblem of structural variations behind a single facade photo of the existing\n3D generation techniques. Experimental results at each stage validate the\ncapability of GeoTexBuild to generate detailed and accurate building models\nfrom footprints derived from site planning or map designs. Our framework\nsignificantly reduces manual labor in modeling buildings and can offer\ninspiration for designers."}
{"id": "2504.08368", "pdf": "https://arxiv.org/pdf/2504.08368", "abs": "https://arxiv.org/abs/2504.08368", "authors": ["Cheng-Yu Hsieh", "Pavan Kumar Anasosalu Vasu", "Fartash Faghri", "Raviteja Vemulapalli", "Chun-Liang Li", "Ranjay Krishna", "Oncel Tuzel", "Hadi Pouransari"], "title": "FocalLens: Instruction Tuning Enables Zero-Shot Conditional Image Representations", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "Visual understanding is inherently contextual -- what we focus on in an image\ndepends on the task at hand. For instance, given an image of a person holding a\nbouquet of flowers, we may focus on either the person such as their clothing,\nor the type of flowers, depending on the context of interest. Yet, most\nexisting image encoding paradigms represent an image as a fixed, generic\nfeature vector, overlooking the potential needs of prioritizing varying visual\ninformation for different downstream use cases. In this work, we introduce\nFocalLens, a conditional visual encoding method that produces different\nrepresentations for the same image based on the context of interest, expressed\nflexibly through natural language. We leverage vision instruction tuning data\nand contrastively finetune a pretrained vision encoder to take natural language\ninstructions as additional inputs for producing conditional image\nrepresentations. Extensive experiments validate that conditional image\nrepresentation from FocalLens better pronounce the visual features of interest\ncompared to generic features produced by standard vision encoders like CLIP. In\naddition, we show FocalLens further leads to performance improvements on a\nrange of downstream tasks including image-image retrieval, image\nclassification, and image-text retrieval, with an average gain of 5 and 10\npoints on the challenging SugarCrepe and MMVP-VLM benchmarks, respectively."}
{"id": "2504.08422", "pdf": "https://arxiv.org/pdf/2504.08422", "abs": "https://arxiv.org/abs/2504.08422", "authors": ["Chao Qi", "Jianqin Yin", "Ren Zhang"], "title": "CMIP-CIL: A Cross-Modal Benchmark for Image-Point Class Incremental Learning", "categories": ["cs.CV"], "comment": null, "summary": "Image-point class incremental learning helps the 3D-points-vision robots\ncontinually learn category knowledge from 2D images, improving their perceptual\ncapability in dynamic environments. However, some incremental learning methods\naddress unimodal forgetting but fail in cross-modal cases, while others handle\nmodal differences within training/testing datasets but assume no modal gaps\nbetween them. We first explore this cross-modal task, proposing a benchmark\nCMIP-CIL and relieving the cross-modal catastrophic forgetting problem. It\nemploys masked point clouds and rendered multi-view images within a contrastive\nlearning framework in pre-training, empowering the vision model with the\ngeneralizations of image-point correspondence. In the incremental stage, by\nfreezing the backbone and promoting object representations close to their\nrespective prototypes, the model effectively retains and generalizes knowledge\nacross previously seen categories while continuing to learn new ones. We\nconduct comprehensive experiments on the benchmark datasets. Experiments prove\nthat our method achieves state-of-the-art results, outperforming the baseline\nmethods by a large margin."}
{"id": "2504.08408", "pdf": "https://arxiv.org/pdf/2504.08408", "abs": "https://arxiv.org/abs/2504.08408", "authors": ["Md Abdullah Al Kafi", "Sumit Kumar Banshal", "Md Sadman Shakib", "Showrov Azam", "Tamanna Alam Tabashom"], "title": "BOISHOMMO: Holistic Approach for Bangla Hate Speech", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "One of the most alarming issues in digital society is hate speech (HS) on\nsocial media. The severity is so high that researchers across the globe are\ncaptivated by this domain. A notable amount of work has been conducted to\naddress the identification and alarm system. However, a noticeable gap exists,\nespecially for low-resource languages. Comprehensive datasets are the main\nproblem among the constrained resource languages, such as Bangla.\nInterestingly, hate speech or any particular speech has no single\ndimensionality. Similarly, the hate component can simultaneously have multiple\nabusive attributes, which seems to be missed in the existing datasets. Thus, a\nmulti-label Bangla hate speech dataset named BOISHOMMO has been compiled and\nevaluated in this work. That includes categories of HS across race, gender,\nreligion, politics, and more. With over two thousand annotated examples,\nBOISHOMMO provides a nuanced understanding of hate speech in Bangla and\nhighlights the complexities of processing non-Latin scripts. Apart from\nevaluating with multiple algorithmic approaches, it also highlights the\ncomplexities of processing Bangla text and assesses model performance. This\nunique multi-label approach enriches future hate speech detection and analysis\nstudies for low-resource languages by providing a more nuanced, diverse\ndataset."}
{"id": "2504.08441", "pdf": "https://arxiv.org/pdf/2504.08441", "abs": "https://arxiv.org/abs/2504.08441", "authors": ["Jonathan Prexl", "Michael Recla", "Michael Schmitt"], "title": "SARFormer -- An Acquisition Parameter Aware Vision Transformer for Synthetic Aperture Radar Data", "categories": ["cs.CV"], "comment": null, "summary": "This manuscript introduces SARFormer, a modified Vision Transformer (ViT)\narchitecture designed for processing one or multiple synthetic aperture radar\n(SAR) images. Given the complex image geometry of SAR data, we propose an\nacquisition parameter encoding module that significantly guides the learning\nprocess, especially in the case of multiple images, leading to improved\nperformance on downstream tasks. We further explore self-supervised\npre-training, conduct experiments with limited labeled data, and benchmark our\ncontribution and adaptations thoroughly in ablation experiments against a\nbaseline, where the model is tested on tasks such as height reconstruction and\nsegmentation. Our approach achieves up to 17% improvement in terms of RMSE over\nbaseline models"}
{"id": "2504.08525", "pdf": "https://arxiv.org/pdf/2504.08525", "abs": "https://arxiv.org/abs/2504.08525", "authors": ["Ye Ye"], "title": "Task Memory Engine (TME): Enhancing State Awareness for Multi-Step LLM Agent Tasks", "categories": ["cs.AI", "cs.CL", "68T05", "I.2.6; I.2.8; H.3.3"], "comment": "14 pages, 5 figures. Preprint prepared for future submission.\n  Includes implementation and token-efficiency analysis. Code at\n  https://github.com/biubiutomato/TME-Agent", "summary": "Large Language Models (LLMs) are increasingly used as autonomous agents for\nmulti-step tasks. However, most existing frameworks fail to maintain a\nstructured understanding of the task state, often relying on linear prompt\nconcatenation or shallow memory buffers. This leads to brittle performance,\nfrequent hallucinations, and poor long-range coherence. In this work, we\npropose the Task Memory Engine (TME), a lightweight and structured memory\nmodule that tracks task execution using a hierarchical Task Memory Tree (TMT).\nEach node in the tree corresponds to a task step, storing relevant input,\noutput, status, and sub-task relationships. We introduce a prompt synthesis\nmethod that dynamically generates LLM prompts based on the active node path,\nsignificantly improving execution consistency and contextual grounding. Through\ncase studies and comparative experiments on multi-step agent tasks, we\ndemonstrate that TME leads to better task completion accuracy and more\ninterpretable behavior with minimal implementation overhead. The full\nimplementation of TME is available at\nhttps://github.com/biubiutomato/TME-Agent."}
{"id": "2504.08449", "pdf": "https://arxiv.org/pdf/2504.08449", "abs": "https://arxiv.org/abs/2504.08449", "authors": ["Jian Wang", "Rishabh Dabral", "Diogo Luvizon", "Zhe Cao", "Lingjie Liu", "Thabo Beeler", "Christian Theobalt"], "title": "Ego4o: Egocentric Human Motion Capture and Understanding from Multi-Modal Input", "categories": ["cs.CV"], "comment": null, "summary": "This work focuses on tracking and understanding human motion using consumer\nwearable devices, such as VR/AR headsets, smart glasses, cellphones, and\nsmartwatches. These devices provide diverse, multi-modal sensor inputs,\nincluding egocentric images, and 1-3 sparse IMU sensors in varied combinations.\nMotion descriptions can also accompany these signals. The diverse input\nmodalities and their intermittent availability pose challenges for consistent\nmotion capture and understanding. In this work, we present Ego4o (o for omni),\na new framework for simultaneous human motion capture and understanding from\nmulti-modal egocentric inputs. This method maintains performance with partial\ninputs while achieving better results when multiple modalities are combined.\nFirst, the IMU sensor inputs, the optional egocentric image, and text\ndescription of human motion are encoded into the latent space of a motion\nVQ-VAE. Next, the latent vectors are sent to the VQ-VAE decoder and optimized\nto track human motion. When motion descriptions are unavailable, the latent\nvectors can be input into a multi-modal LLM to generate human motion\ndescriptions, which can further enhance motion capture accuracy. Quantitative\nand qualitative evaluations demonstrate the effectiveness of our method in\npredicting accurate human motion and high-quality motion descriptions."}
{"id": "2504.08619", "pdf": "https://arxiv.org/pdf/2504.08619", "abs": "https://arxiv.org/abs/2504.08619", "authors": ["Zhiqiu Xia", "Lang Zhu", "Bingzhe Li", "Feng Chen", "Qiannan Li", "Hang Liu"], "title": "Analyzing 16,193 LLM Papers for Fun and Profits", "categories": ["cs.DL", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are reshaping the landscape of computer science\nresearch, driving significant shifts in research priorities across diverse\nconferences and fields. This study provides a comprehensive analysis of the\npublication trend of LLM-related papers in 77 top-tier computer science\nconferences over the past six years (2019-2024). We approach this analysis from\nfour distinct perspectives: (1) We investigate how LLM research is driving\ntopic shifts within major conferences. (2) We adopt a topic modeling approach\nto identify various areas of LLM-related topic growth and reveal the topics of\nconcern at different conferences. (3) We explore distinct contribution patterns\nof academic and industrial institutions. (4) We study the influence of national\norigins on LLM development trajectories. Synthesizing the findings from these\ndiverse analytical angles, we derive ten key insights that illuminate the\ndynamics and evolution of the LLM research ecosystem."}
{"id": "2504.08451", "pdf": "https://arxiv.org/pdf/2504.08451", "abs": "https://arxiv.org/abs/2504.08451", "authors": ["Weiye Chen", "Qingen Zhu", "Qian Long"], "title": "Muon-Accelerated Attention Distillation for Real-Time Edge Synthesis via Optimized Latent Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in visual synthesis have leveraged diffusion models and\nattention mechanisms to achieve high-fidelity artistic style transfer and\nphotorealistic text-to-image generation. However, real-time deployment on edge\ndevices remains challenging due to computational and memory constraints. We\npropose Muon-AD, a co-designed framework that integrates the Muon optimizer\nwith attention distillation for real-time edge synthesis. By eliminating\ngradient conflicts through orthogonal parameter updates and dynamic pruning,\nMuon-AD achieves 3.2 times faster convergence compared to Stable\nDiffusion-TensorRT, while maintaining synthesis quality (15% lower FID, 4%\nhigher SSIM). Our framework reduces peak memory to 7GB on Jetson Orin and\nenables 24FPS real-time generation through mixed-precision quantization and\ncurriculum learning. Extensive experiments on COCO-Stuff and ImageNet-Texture\ndemonstrate Muon-AD's Pareto-optimal efficiency-quality trade-offs. Here, we\nshow a 65% reduction in communication overhead during distributed training and\nreal-time 10s/image generation on edge GPUs. These advancements pave the way\nfor democratizing high-quality visual synthesis in resource-constrained\nenvironments."}
{"id": "2504.08641", "pdf": "https://arxiv.org/pdf/2504.08641", "abs": "https://arxiv.org/abs/2504.08641", "authors": ["Jialu Li", "Shoubin Yu", "Han Lin", "Jaemin Cho", "Jaehong Yoon", "Mohit Bansal"], "title": "Training-free Guidance in Text-to-Video Generation via Multimodal Planning and Structured Noise Initialization", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Website: https://video-msg.github.io; The first three authors\n  contributed equally", "summary": "Recent advancements in text-to-video (T2V) diffusion models have\nsignificantly enhanced the visual quality of the generated videos. However,\neven recent T2V models find it challenging to follow text descriptions\naccurately, especially when the prompt requires accurate control of spatial\nlayouts or object trajectories. A recent line of research uses layout guidance\nfor T2V models that require fine-tuning or iterative manipulation of the\nattention map during inference time. This significantly increases the memory\nrequirement, making it difficult to adopt a large T2V model as a backbone. To\naddress this, we introduce Video-MSG, a training-free Guidance method for T2V\ngeneration based on Multimodal planning and Structured noise initialization.\nVideo-MSG consists of three steps, where in the first two steps, Video-MSG\ncreates Video Sketch, a fine-grained spatio-temporal plan for the final video,\nspecifying background, foreground, and object trajectories, in the form of\ndraft video frames. In the last step, Video-MSG guides a downstream T2V\ndiffusion model with Video Sketch through noise inversion and denoising.\nNotably, Video-MSG does not need fine-tuning or attention manipulation with\nadditional memory during inference time, making it easier to adopt large T2V\nmodels. Video-MSG demonstrates its effectiveness in enhancing text alignment\nwith multiple T2V backbones (VideoCrafter2 and CogVideoX-5B) on popular T2V\ngeneration benchmarks (T2VCompBench and VBench). We provide comprehensive\nablation studies about noise inversion ratio, different background generators,\nbackground object detection, and foreground object segmentation."}
{"id": "2504.08452", "pdf": "https://arxiv.org/pdf/2504.08452", "abs": "https://arxiv.org/abs/2504.08452", "authors": ["Jyri Maanpää", "Julius Pesonen", "Iaroslav Melekhov", "Heikki Hyyti", "Juha Hyyppä"], "title": "Road Grip Uncertainty Estimation Through Surface State Segmentation", "categories": ["cs.CV"], "comment": "15 pages, 5 figures (supplementary material 2 pages, 1 figure).\n  Anonymized version submitted to Scandinavian Conference on Image Analysis\n  (SCIA) 2025", "summary": "Slippery road conditions pose significant challenges for autonomous driving.\nBeyond predicting road grip, it is crucial to estimate its uncertainty reliably\nto ensure safe vehicle control. In this work, we benchmark several uncertainty\nprediction methods to assess their effectiveness for grip uncertainty\nestimation. Additionally, we propose a novel approach that leverages road\nsurface state segmentation to predict grip uncertainty. Our method estimates a\npixel-wise grip probability distribution based on inferred road surface\nconditions. Experimental results indicate that the proposed approach enhances\nthe robustness of grip uncertainty prediction."}
{"id": "2504.08714", "pdf": "https://arxiv.org/pdf/2504.08714", "abs": "https://arxiv.org/abs/2504.08714", "authors": ["Xinyi Gu", "Jiayuan Mao"], "title": "Generating Fine Details of Entity Interactions", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "Project Page: https://concepts-ai.com/p/detailscribe/", "summary": "Images not only depict objects but also encapsulate rich interactions between\nthem. However, generating faithful and high-fidelity images involving multiple\nentities interacting with each other, is a long-standing challenge. While\npre-trained text-to-image models are trained on large-scale datasets to follow\ndiverse text instructions, they struggle to generate accurate interactions,\nlikely due to the scarcity of training data for uncommon object interactions.\nThis paper introduces InterActing, an interaction-focused dataset with 1000\nfine-grained prompts covering three key scenarios: (1) functional and\naction-based interactions, (2) compositional spatial relationships, and (3)\nmulti-subject interactions. To address interaction generation challenges, we\npropose a decomposition-augmented refinement procedure. Our approach,\nDetailScribe, built on Stable Diffusion 3.5, leverages LLMs to decompose\ninteractions into finer-grained concepts, uses a VLM to critique generated\nimages, and applies targeted interventions within the diffusion process in\nrefinement. Automatic and human evaluations show significantly improved image\nquality, demonstrating the potential of enhanced inference strategies. Our\ndataset and code are available at https://concepts-ai.com/p/detailscribe/ to\nfacilitate future exploration of interaction-rich image generation."}
{"id": "2504.08473", "pdf": "https://arxiv.org/pdf/2504.08473", "abs": "https://arxiv.org/abs/2504.08473", "authors": ["Bram Vanherle", "Brent Zoomers", "Jeroen Put", "Frank Van Reeth", "Nick Michiels"], "title": "Cut-and-Splat: Leveraging Gaussian Splatting for Synthetic Data Generation", "categories": ["cs.CV"], "comment": "Accepted at the International Conference on Robotics, Computer Vision\n  and Intelligent Systems 2025 (ROBOVIS)", "summary": "Generating synthetic images is a useful method for cheaply obtaining labeled\ndata for training computer vision models. However, obtaining accurate 3D models\nof relevant objects is necessary, and the resulting images often have a gap in\nrealism due to challenges in simulating lighting effects and camera artifacts.\nWe propose using the novel view synthesis method called Gaussian Splatting to\naddress these challenges. We have developed a synthetic data pipeline for\ngenerating high-quality context-aware instance segmentation training data for\nspecific objects. This process is fully automated, requiring only a video of\nthe target object. We train a Gaussian Splatting model of the target object and\nautomatically extract the object from the video. Leveraging Gaussian Splatting,\nwe then render the object on a random background image, and monocular depth\nestimation is employed to place the object in a believable pose. We introduce a\nnovel dataset to validate our approach and show superior performance over other\ndata generation approaches, such as Cut-and-Paste and Diffusion model-based\ngeneration."}
{"id": "2504.08725", "pdf": "https://arxiv.org/pdf/2504.08725", "abs": "https://arxiv.org/abs/2504.08725", "authors": ["Dayu Yang", "Antoine Simoulin", "Xin Qian", "Xiaoyi Liu", "Yuwei Cao", "Zhaopu Teng", "Grey Yang"], "title": "DocAgent: A Multi-Agent System for Automated Code Documentation Generation", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "High-quality code documentation is crucial for software development\nespecially in the era of AI. However, generating it automatically using Large\nLanguage Models (LLMs) remains challenging, as existing approaches often\nproduce incomplete, unhelpful, or factually incorrect outputs. We introduce\nDocAgent, a novel multi-agent collaborative system using topological code\nprocessing for incremental context building. Specialized agents (Reader,\nSearcher, Writer, Verifier, Orchestrator) then collaboratively generate\ndocumentation. We also propose a multi-faceted evaluation framework assessing\nCompleteness, Helpfulness, and Truthfulness. Comprehensive experiments show\nDocAgent significantly outperforms baselines consistently. Our ablation study\nconfirms the vital role of the topological processing order. DocAgent offers a\nrobust approach for reliable code documentation generation in complex and\nproprietary repositories."}
{"id": "2504.08481", "pdf": "https://arxiv.org/pdf/2504.08481", "abs": "https://arxiv.org/abs/2504.08481", "authors": ["Kerol Djoumessi", "Samuel Ofosu Mensah", "Philipp Berens"], "title": "A Hybrid Fully Convolutional CNN-Transformer Model for Inherently Interpretable Medical Image Classification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In many medical imaging tasks, convolutional neural networks (CNNs)\nefficiently extract local features hierarchically. More recently, vision\ntransformers (ViTs) have gained popularity, using self-attention mechanisms to\ncapture global dependencies, but lacking the inherent spatial localization of\nconvolutions. Therefore, hybrid models combining CNNs and ViTs have been\ndeveloped to combine the strengths of both architectures. However, such hybrid\nCNN-ViT models are difficult to interpret, which hinders their application in\nmedical imaging. In this work, we introduce an interpretable-by-design hybrid\nfully convolutional CNN-Transformer architecture for medical image\nclassification. Unlike widely used post-hoc saliency methods for ViTs, our\napproach generates faithful and localized evidence maps that directly reflect\nthe model's decision process. We evaluated our method on two medical image\nclassification tasks using color fundus images. Our model not only achieves\nstate-of-the-art predictive performance compared to both black-box and\ninterpretable models but also provides class-specific sparse evidence maps in a\nsingle forward pass. The code is available at:\nhttps://anonymous.4open.science/r/Expl-CNN-Transformer/."}
{"id": "2504.08734", "pdf": "https://arxiv.org/pdf/2504.08734", "abs": "https://arxiv.org/abs/2504.08734", "authors": ["Yanlin Wang", "Kefeng Duan", "Dewu Zheng", "Ensheng Shi", "Fengji Zhang", "Yanli Wang", "Jiachi Chen", "Xilin Liu", "Yuchi Ma", "Hongyu Zhang", "Qianxiang Wang", "Zibin Zheng"], "title": "Towards an Understanding of Context Utilization in Code Intelligence", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "Code intelligence is an emerging domain in software engineering, aiming to\nimprove the effectiveness and efficiency of various code-related tasks. Recent\nresearch suggests that incorporating contextual information beyond the basic\noriginal task inputs (i.e., source code) can substantially enhance model\nperformance. Such contextual signals may be obtained directly or indirectly\nfrom sources such as API documentation or intermediate representations like\nabstract syntax trees can significantly improve the effectiveness of code\nintelligence. Despite growing academic interest, there is a lack of systematic\nanalysis of context in code intelligence. To address this gap, we conduct an\nextensive literature review of 146 relevant studies published between September\n2007 and August 2024. Our investigation yields four main contributions. (1) A\nquantitative analysis of the research landscape, including publication trends,\nvenues, and the explored domains; (2) A novel taxonomy of context types used in\ncode intelligence; (3) A task-oriented analysis investigating context\nintegration strategies across diverse code intelligence tasks; (4) A critical\nevaluation of evaluation methodologies for context-aware methods. Based on\nthese findings, we identify fundamental challenges in context utilization in\ncurrent code intelligence systems and propose a research roadmap that outlines\nkey opportunities for future research."}
{"id": "2504.08531", "pdf": "https://arxiv.org/pdf/2504.08531", "abs": "https://arxiv.org/abs/2504.08531", "authors": ["Tommaso Galliena", "Tommaso Apicella", "Stefano Rosa", "Pietro Morerio", "Alessio Del Bue", "Lorenzo Natale"], "title": "Embodied Image Captioning: Self-supervised Learning Agents for Spatially Coherent Image Descriptions", "categories": ["cs.CV", "cs.RO"], "comment": "11 pages, 8 figures, 5 tables, code and test set annotations\n  available at https://hsp-iit.github.io/embodied-captioning/", "summary": "We present a self-supervised method to improve an agent's abilities in\ndescribing arbitrary objects while actively exploring a generic environment.\nThis is a challenging problem, as current models struggle to obtain coherent\nimage captions due to different camera viewpoints and clutter. We propose a\nthree-phase framework to fine-tune existing captioning models that enhances\ncaption accuracy and consistency across views via a consensus mechanism. First,\nan agent explores the environment, collecting noisy image-caption pairs. Then,\na consistent pseudo-caption for each object instance is distilled via consensus\nusing a large language model. Finally, these pseudo-captions are used to\nfine-tune an off-the-shelf captioning model, with the addition of contrastive\nlearning. We analyse the performance of the combination of captioning models,\nexploration policies, pseudo-labeling methods, and fine-tuning strategies, on\nour manually labeled test set. Results show that a policy can be trained to\nmine samples with higher disagreement compared to classical baselines. Our\npseudo-captioning method, in combination with all policies, has a higher\nsemantic similarity compared to other existing methods, and fine-tuning\nimproves caption accuracy and consistency by a significant margin. Code and\ntest set annotations available at\nhttps://hsp-iit.github.io/embodied-captioning/"}
{"id": "2504.08540", "pdf": "https://arxiv.org/pdf/2504.08540", "abs": "https://arxiv.org/abs/2504.08540", "authors": ["Jörg Gamerdinger", "Sven Teufel", "Oliver Bringmann"], "title": "Datasets for Lane Detection in Autonomous Driving: A Comprehensive Review", "categories": ["cs.CV"], "comment": null, "summary": "Accurate lane detection is essential for automated driving, enabling safe and\nreliable vehicle navigation in a variety of road scenarios. Numerous datasets\nhave been introduced to support the development and evaluation of lane\ndetection algorithms, each differing in terms of the amount of data, sensor\ntypes, annotation granularity, environmental conditions, and scenario\ndiversity. This paper provides a comprehensive review of over 30 publicly\navailable lane detection datasets, systematically analysing their\ncharacteristics, advantages and limitations. We classify these datasets based\non key factors such as sensor resolution, annotation types and diversity of\nroad and weather conditions. By identifying existing challenges and research\ngaps, we highlight opportunities for future dataset improvements that can\nfurther drive innovation in robust lane detection. This survey serves as a\nresource for researchers seeking appropriate datasets for lane detection, and\ncontributes to the broader goal of advancing autonomous driving."}
{"id": "2504.08542", "pdf": "https://arxiv.org/pdf/2504.08542", "abs": "https://arxiv.org/abs/2504.08542", "authors": ["Haoran Cheng", "Qide Dong", "Liang Peng", "Zhizhou Sha", "Weiguo Feng", "Jinghui Xie", "Zhao Song", "Shilei Wen", "Xiaofei He", "Boxi Wu"], "title": "Discriminator-Free Direct Preference Optimization for Video Diffusion", "categories": ["cs.CV"], "comment": "arXiv admin note: text overlap with arXiv:2412.14167 by other authors", "summary": "Direct Preference Optimization (DPO), which aligns models with human\npreferences through win/lose data pairs, has achieved remarkable success in\nlanguage and image generation. However, applying DPO to video diffusion models\nfaces critical challenges: (1) Data inefficiency. Generating thousands of\nvideos per DPO iteration incurs prohibitive costs; (2) Evaluation uncertainty.\nHuman annotations suffer from subjective bias, and automated discriminators\nfail to detect subtle temporal artifacts like flickering or motion incoherence.\nTo address these, we propose a discriminator-free video DPO framework that: (1)\nUses original real videos as win cases and their edited versions (e.g.,\nreversed, shuffled, or noise-corrupted clips) as lose cases; (2) Trains video\ndiffusion models to distinguish and avoid artifacts introduced by editing. This\napproach eliminates the need for costly synthetic video comparisons, provides\nunambiguous quality signals, and enables unlimited training data expansion\nthrough simple editing operations. We theoretically prove the framework's\neffectiveness even when real videos and model-generated videos follow different\ndistributions. Experiments on CogVideoX demonstrate the efficiency of the\nproposed method."}
{"id": "2504.08550", "pdf": "https://arxiv.org/pdf/2504.08550", "abs": "https://arxiv.org/abs/2504.08550", "authors": ["Alireza Fathalizadeh", "Roozbeh Razavi-Far"], "title": "Proxy-Anchor and EVT-Driven Continual Learning Method for Generalized Category Discovery", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Continual generalized category discovery has been introduced and studied in\nthe literature as a method that aims to continuously discover and learn novel\ncategories in incoming data batches while avoiding catastrophic forgetting of\npreviously learned categories. A key component in addressing this challenge is\nthe model's ability to separate novel samples, where Extreme Value Theory (EVT)\nhas been effectively employed. In this work, we propose a novel method that\nintegrates EVT with proxy anchors to define boundaries around proxies using a\nprobability of inclusion function, enabling the rejection of unknown samples.\nAdditionally, we introduce a novel EVT-based loss function to enhance the\nlearned representation, achieving superior performance compared to other\ndeep-metric learning methods in similar settings. Using the derived probability\nfunctions, novel samples are effectively separated from previously known\ncategories. However, category discovery within these novel samples can\nsometimes overestimate the number of new categories. To mitigate this issue, we\npropose a novel EVT-based approach to reduce the model size and discard\nredundant proxies. We also incorporate experience replay and knowledge\ndistillation mechanisms during the continual learning stage to prevent\ncatastrophic forgetting. Experimental results demonstrate that our proposed\napproach outperforms state-of-the-art methods in continual generalized category\ndiscovery scenarios."}
{"id": "2504.08551", "pdf": "https://arxiv.org/pdf/2504.08551", "abs": "https://arxiv.org/abs/2504.08551", "authors": ["Mohamed Sabry", "Gregory Schroeder", "Joshua Varughese", "Cristina Olaverri-Monreal"], "title": "Shadow Erosion and Nighttime Adaptability for Camera-Based Automated Driving Applications", "categories": ["cs.CV"], "comment": "7 pages", "summary": "Enhancement of images from RGB cameras is of particular interest due to its\nwide range of ever-increasing applications such as medical imaging, satellite\nimaging, automated driving, etc. In autonomous driving, various techniques are\nused to enhance image quality under challenging lighting conditions. These\ninclude artificial augmentation to improve visibility in poor nighttime\nconditions, illumination-invariant imaging to reduce the impact of lighting\nvariations, and shadow mitigation to ensure consistent image clarity in bright\ndaylight. This paper proposes a pipeline for Shadow Erosion and Nighttime\nAdaptability in images for automated driving applications while preserving\ncolor and texture details. The Shadow Erosion and Nighttime Adaptability\npipeline is compared to the widely used CLAHE technique and evaluated based on\nillumination uniformity and visual perception quality metrics. The results also\ndemonstrate a significant improvement over CLAHE, enhancing a YOLO-based\ndrivable area segmentation algorithm."}
{"id": "2504.08568", "pdf": "https://arxiv.org/pdf/2504.08568", "abs": "https://arxiv.org/abs/2504.08568", "authors": ["Luis Chuquimarca", "Boris Vintimilla", "Sergio Velastin"], "title": "Banana Ripeness Level Classification using a Simple CNN Model Trained with Real and Synthetic Datasets", "categories": ["cs.CV", "68T05, 68T07, 68T10", "I.4.7; I.2.10"], "comment": "9 pages, 7 figures, conference", "summary": "The level of ripeness is essential in determining the quality of bananas. To\ncorrectly estimate banana maturity, the metrics of international marketing\nstandards need to be considered. However, the process of assessing the maturity\nof bananas at an industrial level is still carried out using manual methods.\nThe use of CNN models is an attractive tool to solve the problem, but there is\na limitation regarding the availability of sufficient data to train these\nmodels reliably. On the other hand, in the state-of-the-art, existing CNN\nmodels and the available data have reported that the accuracy results are\nacceptable in identifying banana maturity. For this reason, this work presents\nthe generation of a robust dataset that combines real and synthetic data for\ndifferent levels of banana ripeness. In addition, it proposes a simple CNN\narchitecture, which is trained with synthetic data and using the transfer\nlearning technique, the model is improved to classify real data, managing to\ndetermine the level of maturity of the banana. The proposed CNN model is\nevaluated with several architectures, then hyper-parameter configurations are\nvaried, and optimizers are used. The results show that the proposed CNN model\nreaches a high accuracy of 0.917 and a fast execution time."}
{"id": "2504.08578", "pdf": "https://arxiv.org/pdf/2504.08578", "abs": "https://arxiv.org/abs/2504.08578", "authors": ["Maria Santos-Villafranca", "Dustin Carrión-Ojeda", "Alejandro Perez-Yus", "Jesus Bermudez-Cameo", "Jose J. Guerrero", "Simone Schaub-Meyer"], "title": "Knowledge Distillation for Multimodal Egocentric Action Recognition Robust to Missing Modalities", "categories": ["cs.CV"], "comment": "Project Page: https://visinf.github.io/KARMMA", "summary": "Action recognition is an essential task in egocentric vision due to its wide\nrange of applications across many fields. While deep learning methods have been\nproposed to address this task, most rely on a single modality, typically video.\nHowever, including additional modalities may improve the robustness of the\napproaches to common issues in egocentric videos, such as blurriness and\nocclusions. Recent efforts in multimodal egocentric action recognition often\nassume the availability of all modalities, leading to failures or performance\ndrops when any modality is missing. To address this, we introduce an efficient\nmultimodal knowledge distillation approach for egocentric action recognition\nthat is robust to missing modalities (KARMMA) while still benefiting when\nmultiple modalities are available. Our method focuses on resource-efficient\ndevelopment by leveraging pre-trained models as unimodal feature extractors in\nour teacher model, which distills knowledge into a much smaller and faster\nstudent model. Experiments on the Epic-Kitchens and Something-Something\ndatasets demonstrate that our student model effectively handles missing\nmodalities while reducing its accuracy drop in this scenario."}
{"id": "2504.08581", "pdf": "https://arxiv.org/pdf/2504.08581", "abs": "https://arxiv.org/abs/2504.08581", "authors": ["Xin Tan", "Yuzhou Ji", "He Zhu", "Yuan Xie"], "title": "FMLGS: Fast Multilevel Language Embedded Gaussians for Part-level Interactive Agents", "categories": ["cs.CV"], "comment": null, "summary": "The semantically interactive radiance field has long been a promising\nbackbone for 3D real-world applications, such as embodied AI to achieve scene\nunderstanding and manipulation. However, multi-granularity interaction remains\na challenging task due to the ambiguity of language and degraded quality when\nit comes to queries upon object components. In this work, we present FMLGS, an\napproach that supports part-level open-vocabulary query within 3D Gaussian\nSplatting (3DGS). We propose an efficient pipeline for building and querying\nconsistent object- and part-level semantics based on Segment Anything Model 2\n(SAM2). We designed a semantic deviation strategy to solve the problem of\nlanguage ambiguity among object parts, which interpolates the semantic features\nof fine-grained targets for enriched information. Once trained, we can query\nboth objects and their describable parts using natural language. Comparisons\nwith other state-of-the-art methods prove that our method can not only better\nlocate specified part-level targets, but also achieve first-place performance\nconcerning both speed and accuracy, where FMLGS is 98 x faster than LERF, 4 x\nfaster than LangSplat and 2.5 x faster than LEGaussians. Meanwhile, we further\nintegrate FMLGS as a virtual agent that can interactively navigate through 3D\nscenes, locate targets, and respond to user demands through a chat interface,\nwhich demonstrates the potential of our work to be further expanded and applied\nin the future."}
{"id": "2504.08584", "pdf": "https://arxiv.org/pdf/2504.08584", "abs": "https://arxiv.org/abs/2504.08584", "authors": ["Mahshad Lotfinia", "Arash Tayebiarasteh", "Samaneh Samiei", "Mehdi Joodaki", "Soroosh Tayebi Arasteh"], "title": "Boosting multi-demographic federated learning for chest x-ray analysis using general-purpose self-supervised representations", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Reliable artificial intelligence (AI) models for medical image analysis often\ndepend on large and diverse labeled datasets. Federated learning (FL) offers a\ndecentralized and privacy-preserving approach to training but struggles in\nhighly non-independent and identically distributed (non-IID) settings, where\ninstitutions with more representative data may experience degraded performance.\nMoreover, existing large-scale FL studies have been limited to adult datasets,\nneglecting the unique challenges posed by pediatric data, which introduces\nadditional non-IID variability. To address these limitations, we analyzed\nn=398,523 adult chest radiographs from diverse institutions across multiple\ncountries and n=9,125 pediatric images, leveraging transfer learning from\ngeneral-purpose self-supervised image representations to classify pneumonia and\ncases with no abnormality. Using state-of-the-art vision transformers, we found\nthat FL improved performance only for smaller adult datasets (P<0.001) but\ndegraded performance for larger datasets (P<0.064) and pediatric cases\n(P=0.242). However, equipping FL with self-supervised weights significantly\nenhanced outcomes across pediatric cases (P=0.031) and most adult datasets\n(P<0.008), except the largest dataset (P=0.052). These findings underscore the\npotential of easily deployable general-purpose self-supervised image\nrepresentations to address non-IID challenges in clinical FL applications and\nhighlight their promise for enhancing patient outcomes and advancing pediatric\nhealthcare, where data scarcity and variability remain persistent obstacles."}
{"id": "2504.08588", "pdf": "https://arxiv.org/pdf/2504.08588", "abs": "https://arxiv.org/abs/2504.08588", "authors": ["Claudio Cimarelli", "Jose Andres Millan-Romera", "Holger Voos", "Jose Luis Sanchez-Lopez"], "title": "Hardware, Algorithms, and Applications of the Neuromorphic Vision Sensor: a Review", "categories": ["cs.CV"], "comment": "26 pages total, 26 without references, two images and five tables.\n  Submitted to IEEE Sensors", "summary": "Neuromorphic, or event, cameras represent a transformation in the classical\napproach to visual sensing encodes detected instantaneous per-pixel\nillumination changes into an asynchronous stream of event packets. Their\nnovelty compared to standard cameras lies in the transition from capturing full\npicture frames at fixed time intervals to a sparse data format which, with its\ndistinctive qualities, offers potential improvements in various applications.\nHowever, these advantages come at the cost of reinventing algorithmic\nprocedures or adapting them to effectively process the new data format.\n  In this survey, we systematically examine neuromorphic vision along three\nmain dimensions. First, we highlight the technological evolution and\ndistinctive hardware features of neuromorphic cameras from their inception to\nrecent models. Second, we review image processing algorithms developed\nexplicitly for event-based data, covering key works on feature detection,\ntracking, and optical flow -which form the basis for analyzing image elements\nand transformations -as well as depth and pose estimation or object\nrecognition, which interpret more complex scene structures and components.\nThese techniques, drawn from classical computer vision and modern data-driven\napproaches, are examined to illustrate the breadth of applications for\nevent-based cameras. Third, we present practical application case studies\ndemonstrating how event cameras have been successfully used across various\nindustries and scenarios. Finally, we analyze the challenges limiting\nwidespread adoption, identify significant research gaps compared to standard\nimaging techniques, and outline promising future directions and opportunities\nthat neuromorphic vision offers."}
{"id": "2504.08591", "pdf": "https://arxiv.org/pdf/2504.08591", "abs": "https://arxiv.org/abs/2504.08591", "authors": ["Yongsheng Yu", "Haitian Zheng", "Zhifei Zhang", "Jianming Zhang", "Yuqian Zhou", "Connelly Barnes", "Yuchen Liu", "Wei Xiong", "Zhe Lin", "Jiebo Luo"], "title": "ZipIR: Latent Pyramid Diffusion Transformer for High-Resolution Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Recent progress in generative models has significantly improved image\nrestoration capabilities, particularly through powerful diffusion models that\noffer remarkable recovery of semantic details and local fidelity. However,\ndeploying these models at ultra-high resolutions faces a critical trade-off\nbetween quality and efficiency due to the computational demands of long-range\nattention mechanisms. To address this, we introduce ZipIR, a novel framework\nthat enhances efficiency, scalability, and long-range modeling for high-res\nimage restoration. ZipIR employs a highly compressed latent representation that\ncompresses image 32x, effectively reducing the number of spatial tokens, and\nenabling the use of high-capacity models like the Diffusion Transformer (DiT).\nToward this goal, we propose a Latent Pyramid VAE (LP-VAE) design that\nstructures the latent space into sub-bands to ease diffusion training. Trained\non full images up to 2K resolution, ZipIR surpasses existing diffusion-based\nmethods, offering unmatched speed and quality in restoring high-resolution\nimages from severely degraded inputs."}
{"id": "2504.08593", "pdf": "https://arxiv.org/pdf/2504.08593", "abs": "https://arxiv.org/abs/2504.08593", "authors": ["Low Jian He", "Harry Walsh", "Ozge Mercanoglu Sincan", "Richard Bowden"], "title": "Hands-On: Segmenting Individual Signs from Continuous Sequences", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted in the 19th IEEE International Conference on Automatic Face\n  and Gesture Recognition", "summary": "This work tackles the challenge of continuous sign language segmentation, a\nkey task with huge implications for sign language translation and data\nannotation. We propose a transformer-based architecture that models the\ntemporal dynamics of signing and frames segmentation as a sequence labeling\nproblem using the Begin-In-Out (BIO) tagging scheme. Our method leverages the\nHaMeR hand features, and is complemented with 3D Angles. Extensive experiments\nshow that our model achieves state-of-the-art results on the DGS Corpus, while\nour features surpass prior benchmarks on BSLCorpus."}
{"id": "2504.08602", "pdf": "https://arxiv.org/pdf/2504.08602", "abs": "https://arxiv.org/abs/2504.08602", "authors": ["Gesina Schwalbe", "Georgii Mikriukov", "Edgar Heinert", "Stavros Gerolymatos", "Mert Keser", "Alois Knoll", "Matthias Rottmann", "Annika Mütze"], "title": "On Background Bias of Post-Hoc Concept Embeddings in Computer Vision DNNs", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "camera-ready version for 3rd World Conference on eXplainable\n  Artificial Intelligence; 5 figures, 6 tables; code available at:\n  https://github.com/gesina/bg_randomized_loce", "summary": "The thriving research field of concept-based explainable artificial\nintelligence (C-XAI) investigates how human-interpretable semantic concepts\nembed in the latent spaces of deep neural networks (DNNs). Post-hoc approaches\ntherein use a set of examples to specify a concept, and determine its\nembeddings in DNN latent space using data driven techniques. This proved useful\nto uncover biases between different target (foreground or concept) classes.\nHowever, given that the background is mostly uncontrolled during training, an\nimportant question has been left unattended so far: Are/to what extent are\nstate-of-the-art, data-driven post-hoc C-XAI approaches themselves prone to\nbiases with respect to their backgrounds? E.g., wild animals mostly occur\nagainst vegetation backgrounds, and they seldom appear on roads. Even simple\nand robust C-XAI methods might abuse this shortcut for enhanced performance. A\ndangerous performance degradation of the concept-corner cases of animals on the\nroad could thus remain undiscovered. This work validates and thoroughly\nconfirms that established Net2Vec-based concept segmentation techniques\nfrequently capture background biases, including alarming ones, such as\nunderperformance on road scenes. For the analysis, we compare 3 established\ntechniques from the domain of background randomization on >50 concepts from 2\ndatasets, and 7 diverse DNN architectures. Our results indicate that even\nlow-cost setups can provide both valuable insight and improved background\nrobustness."}
{"id": "2504.08613", "pdf": "https://arxiv.org/pdf/2504.08613", "abs": "https://arxiv.org/abs/2504.08613", "authors": ["Mohamed Abbas Hedjazi", "Oussama Hadjerci", "Adel Hafiane"], "title": "Enhancing knowledge retention for continual learning with domain-specific adapters and features gating", "categories": ["cs.CV", "eess.IV"], "comment": "Submitted to Applied Intelligence (Springer), under review since\n  November 26, 2024", "summary": "Continual learning empowers models to learn from a continuous stream of data\nwhile preserving previously acquired knowledge, effectively addressing the\nchallenge of catastrophic forgetting. In this study, we propose a new approach\nthat integrates adapters within the self-attention mechanisms of Vision\nTransformers to enhance knowledge retention when sequentially adding datasets\nfrom different domains. Unlike previous methods that continue learning with\nonly one dataset, our approach introduces domain-specific output heads and\nfeature gating, allowing the model to maintain high accuracy on previously\nlearned tasks while incorporating only the essential information from multiple\ndomains. The proposed method is compared to prominent parameter-efficient\nfine-tuning methods in the current state of the art. The results provide\nevidence that our method effectively alleviates the limitations of previous\nworks. Furthermore, we conduct a comparative analysis using three datasets,\nCIFAR-100, Flowers102, and DTD, each representing a distinct domain, to\ninvestigate the impact of task order on model performance. Our findings\nunderscore the critical role of dataset sequencing in shaping learning\noutcomes, demonstrating that strategic ordering can significantly improve the\nmodel's ability to adapt to evolving data distributions over time while\npreserving the integrity of previously learned knowledge."}
{"id": "2504.08616", "pdf": "https://arxiv.org/pdf/2504.08616", "abs": "https://arxiv.org/abs/2504.08616", "authors": ["Lei Kang", "Xuanshuo Fu", "Lluis Gomez", "Alicia Fornés", "Ernest Valveny", "Dimosthenis Karatzas"], "title": "Preserving Privacy Without Compromising Accuracy: Machine Unlearning for Handwritten Text Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Handwritten Text Recognition (HTR) is essential for document analysis and\ndigitization. However, handwritten data often contains user-identifiable\ninformation, such as unique handwriting styles and personal lexicon choices,\nwhich can compromise privacy and erode trust in AI services. Legislation like\nthe ``right to be forgotten'' underscores the necessity for methods that can\nexpunge sensitive information from trained models. Machine unlearning addresses\nthis by selectively removing specific data from models without necessitating\ncomplete retraining. Yet, it frequently encounters a privacy-accuracy tradeoff,\nwhere safeguarding privacy leads to diminished model performance. In this\npaper, we introduce a novel two-stage unlearning strategy for a multi-head\ntransformer-based HTR model, integrating pruning and random labeling. Our\nproposed method utilizes a writer classification head both as an indicator and\na trigger for unlearning, while maintaining the efficacy of the recognition\nhead. To our knowledge, this represents the first comprehensive exploration of\nmachine unlearning within HTR tasks. We further employ Membership Inference\nAttacks (MIA) to evaluate the effectiveness of unlearning user-identifiable\ninformation. Extensive experiments demonstrate that our approach effectively\npreserves privacy while maintaining model accuracy, paving the way for new\nresearch directions in the document analysis community. Our code will be\npublicly available upon acceptance."}
{"id": "2504.08620", "pdf": "https://arxiv.org/pdf/2504.08620", "abs": "https://arxiv.org/abs/2504.08620", "authors": ["Emmanuel Azuh Mensah", "Joban Mand", "Yueheng Ou", "Min Jang", "Kurtis Heimerl"], "title": "Efficient Mixture of Geographical Species for On Device Wildlife Monitoring", "categories": ["cs.CV"], "comment": null, "summary": "Efficient on-device models have become attractive for near-sensor insight\ngeneration, of particular interest to the ecological conservation community.\nFor this reason, deep learning researchers are proposing more approaches to\ndevelop lower compute models. However, since vision transformers are very new\nto the edge use case, there are still unexplored approaches, most notably\nconditional execution of subnetworks based on input data. In this work, we\nexplore the training of a single species detector which uses conditional\ncomputation to bias structured sub networks in a geographically-aware manner.\nWe propose a method for pruning the expert model per location and demonstrate\nconditional computation performance on two geographically distributed datasets:\niNaturalist and iWildcam."}
{"id": "2504.08635", "pdf": "https://arxiv.org/pdf/2504.08635", "abs": "https://arxiv.org/abs/2504.08635", "authors": ["Gabriele Lozupone", "Alessandro Bria", "Francesco Fontanella", "Frederick J. A. Meijer", "Claudio De Stefano", "Henkjan Huisman"], "title": "Latent Diffusion Autoencoders: Toward Efficient and Meaningful Unsupervised Representation Learning in Medical Imaging", "categories": ["cs.CV", "41A05, 41A10, 65D05, 65D17,"], "comment": "15 pages, 9 figures, 7 tables", "summary": "This study presents Latent Diffusion Autoencoder (LDAE), a novel\nencoder-decoder diffusion-based framework for efficient and meaningful\nunsupervised learning in medical imaging, focusing on Alzheimer disease (AD)\nusing brain MR from the ADNI database as a case study. Unlike conventional\ndiffusion autoencoders operating in image space, LDAE applies the diffusion\nprocess in a compressed latent representation, improving computational\nefficiency and making 3D medical imaging representation learning tractable. To\nvalidate the proposed approach, we explore two key hypotheses: (i) LDAE\neffectively captures meaningful semantic representations on 3D brain MR\nassociated with AD and ageing, and (ii) LDAE achieves high-quality image\ngeneration and reconstruction while being computationally efficient.\nExperimental results support both hypotheses: (i) linear-probe evaluations\ndemonstrate promising diagnostic performance for AD (ROC-AUC: 90%, ACC: 84%)\nand age prediction (MAE: 4.1 years, RMSE: 5.2 years); (ii) the learned semantic\nrepresentations enable attribute manipulation, yielding anatomically plausible\nmodifications; (iii) semantic interpolation experiments show strong\nreconstruction of missing scans, with SSIM of 0.969 (MSE: 0.0019) for a 6-month\ngap. Even for longer gaps (24 months), the model maintains robust performance\n(SSIM > 0.93, MSE < 0.004), indicating an ability to capture temporal\nprogression trends; (iv) compared to conventional diffusion autoencoders, LDAE\nsignificantly increases inference throughput (20x faster) while also enhancing\nreconstruction quality. These findings position LDAE as a promising framework\nfor scalable medical imaging applications, with the potential to serve as a\nfoundation model for medical image analysis. Code available at\nhttps://github.com/GabrieleLozupone/LDAE"}
{"id": "2504.08641", "pdf": "https://arxiv.org/pdf/2504.08641", "abs": "https://arxiv.org/abs/2504.08641", "authors": ["Jialu Li", "Shoubin Yu", "Han Lin", "Jaemin Cho", "Jaehong Yoon", "Mohit Bansal"], "title": "Training-free Guidance in Text-to-Video Generation via Multimodal Planning and Structured Noise Initialization", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Website: https://video-msg.github.io; The first three authors\n  contributed equally", "summary": "Recent advancements in text-to-video (T2V) diffusion models have\nsignificantly enhanced the visual quality of the generated videos. However,\neven recent T2V models find it challenging to follow text descriptions\naccurately, especially when the prompt requires accurate control of spatial\nlayouts or object trajectories. A recent line of research uses layout guidance\nfor T2V models that require fine-tuning or iterative manipulation of the\nattention map during inference time. This significantly increases the memory\nrequirement, making it difficult to adopt a large T2V model as a backbone. To\naddress this, we introduce Video-MSG, a training-free Guidance method for T2V\ngeneration based on Multimodal planning and Structured noise initialization.\nVideo-MSG consists of three steps, where in the first two steps, Video-MSG\ncreates Video Sketch, a fine-grained spatio-temporal plan for the final video,\nspecifying background, foreground, and object trajectories, in the form of\ndraft video frames. In the last step, Video-MSG guides a downstream T2V\ndiffusion model with Video Sketch through noise inversion and denoising.\nNotably, Video-MSG does not need fine-tuning or attention manipulation with\nadditional memory during inference time, making it easier to adopt large T2V\nmodels. Video-MSG demonstrates its effectiveness in enhancing text alignment\nwith multiple T2V backbones (VideoCrafter2 and CogVideoX-5B) on popular T2V\ngeneration benchmarks (T2VCompBench and VBench). We provide comprehensive\nablation studies about noise inversion ratio, different background generators,\nbackground object detection, and foreground object segmentation."}
{"id": "2504.08645", "pdf": "https://arxiv.org/pdf/2504.08645", "abs": "https://arxiv.org/abs/2504.08645", "authors": ["Alessio Lombardi", "Li Duan", "Ahmed Elnagar", "Ahmed Zaalouk", "Khalid Ismail", "Edlira Vakaj"], "title": "Title block detection and information extraction for enhanced building drawings search", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages, 8 figures, 1 table. Accepted for publication in the 2025\n  European Conference on Computing in Construction (EC3,\n  https://ec-3.org/conference2025/)", "summary": "The architecture, engineering, and construction (AEC) industry still heavily\nrelies on information stored in drawings for building construction,\nmaintenance, compliance and error checks. However, information extraction (IE)\nfrom building drawings is often time-consuming and costly, especially when\ndealing with historical buildings. Drawing search can be simplified by\nleveraging the information stored in the title block portion of the drawing,\nwhich can be seen as drawing metadata. However, title block IE can be complex\nespecially when dealing with historical drawings which do not follow existing\nstandards for uniformity. This work performs a comparison of existing methods\nfor this kind of IE task, and then proposes a novel title block detection and\nIE pipeline which outperforms existing methods, in particular when dealing with\ncomplex, noisy historical drawings. The pipeline is obtained by combining a\nlightweight Convolutional Neural Network and GPT-4o, the proposed inference\npipeline detects building engineering title blocks with high accuracy, and then\nextract structured drawing metadata from the title blocks, which can be used\nfor drawing search, filtering and grouping. The work demonstrates high accuracy\nand efficiency in IE for both vector (CAD) and hand-drawn (historical)\ndrawings. A user interface (UI) that leverages the extracted metadata for\ndrawing search is established and deployed on real projects, which demonstrates\nsignificant time savings. Additionally, an extensible domain-expert-annotated\ndataset for title block detection is developed, via an efficient AEC-friendly\nannotation workflow that lays the foundation for future work."}
{"id": "2504.08646", "pdf": "https://arxiv.org/pdf/2504.08646", "abs": "https://arxiv.org/abs/2504.08646", "authors": ["Ian Noronha", "Advait Prasad Jawaji", "Juan Camilo Soto", "Jiajun An", "Yan Gu", "Upinder Kaur"], "title": "MBE-ARI: A Multimodal Dataset Mapping Bi-directional Engagement in Animal-Robot Interaction", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted to ICRA 2025", "summary": "Animal-robot interaction (ARI) remains an unexplored challenge in robotics,\nas robots struggle to interpret the complex, multimodal communication cues of\nanimals, such as body language, movement, and vocalizations. Unlike human-robot\ninteraction, which benefits from established datasets and frameworks,\nanimal-robot interaction lacks the foundational resources needed to facilitate\nmeaningful bidirectional communication. To bridge this gap, we present the\nMBE-ARI (Multimodal Bidirectional Engagement in Animal-Robot Interaction), a\nnovel multimodal dataset that captures detailed interactions between a legged\nrobot and cows. The dataset includes synchronized RGB-D streams from multiple\nviewpoints, annotated with body pose and activity labels across interaction\nphases, offering an unprecedented level of detail for ARI research.\nAdditionally, we introduce a full-body pose estimation model tailored for\nquadruped animals, capable of tracking 39 keypoints with a mean average\nprecision (mAP) of 92.7%, outperforming existing benchmarks in animal pose\nestimation. The MBE-ARI dataset and our pose estimation framework lay a robust\nfoundation for advancing research in animal-robot interaction, providing\nessential tools for developing perception, reasoning, and interaction\nframeworks needed for effective collaboration between robots and animals. The\ndataset and resources are publicly available at\nhttps://github.com/RISELabPurdue/MBE-ARI/, inviting further exploration and\ndevelopment in this critical area."}
{"id": "2504.08654", "pdf": "https://arxiv.org/pdf/2504.08654", "abs": "https://arxiv.org/abs/2504.08654", "authors": ["Masashi Hatano", "Zhifan Zhu", "Hideo Saito", "Dima Damen"], "title": "The Invisible EgoHand: 3D Hand Forecasting through EgoBody Pose Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Forecasting hand motion and pose from an egocentric perspective is essential\nfor understanding human intention. However, existing methods focus solely on\npredicting positions without considering articulation, and only when the hands\nare visible in the field of view. This limitation overlooks the fact that\napproximate hand positions can still be inferred even when they are outside the\ncamera's view. In this paper, we propose a method to forecast the 3D\ntrajectories and poses of both hands from an egocentric video, both in and out\nof the field of view. We propose a diffusion-based transformer architecture for\nEgocentric Hand Forecasting, EgoH4, which takes as input the observation\nsequence and camera poses, then predicts future 3D motion and poses for both\nhands of the camera wearer. We leverage full-body pose information, allowing\nother joints to provide constraints on hand motion. We denoise the hand and\nbody joints along with a visibility predictor for hand joints and a 3D-to-2D\nreprojection loss that minimizes the error when hands are in-view. We evaluate\nEgoH4 on the Ego-Exo4D dataset, combining subsets with body and hand\nannotations. We train on 156K sequences and evaluate on 34K sequences,\nrespectively. EgoH4 improves the performance by 3.4cm and 5.1cm over the\nbaseline in terms of ADE for hand trajectory forecasting and MPJPE for hand\npose forecasting. Project page: https://masashi-hatano.github.io/EgoH4/"}
{"id": "2504.08675", "pdf": "https://arxiv.org/pdf/2504.08675", "abs": "https://arxiv.org/abs/2504.08675", "authors": ["Gokce Guven", "H. Fatih Ugurdag", "Hasan F. Ates"], "title": "X2BR: High-Fidelity 3D Bone Reconstruction from a Planar X-Ray Image with Hybrid Neural Implicit Methods", "categories": ["cs.CV"], "comment": null, "summary": "Accurate 3D bone reconstruction from a single planar X-ray remains a\nchallenge due to anatomical complexity and limited input data. We propose X2BR,\na hybrid neural implicit framework that combines continuous volumetric\nreconstruction with template-guided non-rigid registration. The core network,\nX2B, employs a ConvNeXt-based encoder to extract spatial features from X-rays\nand predict high-fidelity 3D bone occupancy fields without relying on\nstatistical shape models. To further refine anatomical accuracy, X2BR\nintegrates a patient-specific template mesh, constructed using YOLOv9-based\ndetection and the SKEL biomechanical skeleton model. The coarse reconstruction\nis aligned to the template using geodesic-based coherent point drift, enabling\nanatomically consistent 3D bone volumes. Experimental results on a clinical\ndataset show that X2B achieves the highest numerical accuracy, with an IoU of\n0.952 and Chamfer-L1 distance of 0.005, outperforming recent baselines\nincluding X2V and D2IM-Net. Building on this, X2BR incorporates anatomical\npriors via YOLOv9-based bone detection and biomechanical template alignment,\nleading to reconstructions that, while slightly lower in IoU (0.875), offer\nsuperior anatomical realism, especially in rib curvature and vertebral\nalignment. This numerical accuracy vs. visual consistency trade-off between X2B\nand X2BR highlights the value of hybrid frameworks for clinically relevant 3D\nreconstructions."}
{"id": "2504.08685", "pdf": "https://arxiv.org/pdf/2504.08685", "abs": "https://arxiv.org/abs/2504.08685", "authors": ["Team Seawead", "Ceyuan Yang", "Zhijie Lin", "Yang Zhao", "Shanchuan Lin", "Zhibei Ma", "Haoyuan Guo", "Hao Chen", "Lu Qi", "Sen Wang", "Feng Cheng", "Feilong Zuo Xuejiao Zeng", "Ziyan Yang", "Fangyuan Kong", "Zhiwu Qing", "Fei Xiao", "Meng Wei", "Tuyen Hoang", "Siyu Zhang", "Peihao Zhu", "Qi Zhao", "Jiangqiao Yan", "Liangke Gui", "Sheng Bi", "Jiashi Li", "Yuxi Ren", "Rui Wang", "Huixia Li", "Xuefeng Xiao", "Shu Liu", "Feng Ling", "Heng Zhang", "Houmin Wei", "Huafeng Kuang", "Jerry Duncan", "Junda Zhang", "Junru Zheng", "Li Sun", "Manlin Zhang", "Renfei Sun", "Xiaobin Zhuang", "Xiaojie Li", "Xin Xia", "Xuyan Chi", "Yanghua Peng", "Yuping Wang", "Yuxuan Wang", "Zhongkai Zhao", "Zhuo Chen", "Zuquan Song", "Zhenheng Yang", "Jiashi Feng", "Jianchao Yang", "Lu Jiang"], "title": "Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model", "categories": ["cs.CV", "cs.AI"], "comment": "Technical report", "summary": "This technical report presents a cost-efficient strategy for training a video\ngeneration foundation model. We present a mid-sized research model with\napproximately 7 billion parameters (7B) called Seaweed-7B trained from scratch\nusing 665,000 H100 GPU hours. Despite being trained with moderate computational\nresources, Seaweed-7B demonstrates highly competitive performance compared to\ncontemporary video generation models of much larger size. Design choices are\nespecially crucial in a resource-constrained setting. This technical report\nhighlights the key design decisions that enhance the performance of the\nmedium-sized diffusion model. Empirically, we make two observations: (1)\nSeaweed-7B achieves performance comparable to, or even surpasses, larger models\ntrained on substantially greater GPU resources, and (2) our model, which\nexhibits strong generalization ability, can be effectively adapted across a\nwide range of downstream applications either by lightweight fine-tuning or\ncontinue training. See the project page at https://seaweed.video/"}
{"id": "2504.08710", "pdf": "https://arxiv.org/pdf/2504.08710", "abs": "https://arxiv.org/abs/2504.08710", "authors": ["Joshua Fixelle"], "title": "Hypergraph Vision Transformers: Images are More than Nodes, More than Edges", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Recent advancements in computer vision have highlighted the scalability of\nVision Transformers (ViTs) across various tasks, yet challenges remain in\nbalancing adaptability, computational efficiency, and the ability to model\nhigher-order relationships. Vision Graph Neural Networks (ViGs) offer an\nalternative by leveraging graph-based methodologies but are hindered by the\ncomputational bottlenecks of clustering algorithms used for edge generation. To\naddress these issues, we propose the Hypergraph Vision Transformer (HgVT),\nwhich incorporates a hierarchical bipartite hypergraph structure into the\nvision transformer framework to capture higher-order semantic relationships\nwhile maintaining computational efficiency. HgVT leverages population and\ndiversity regularization for dynamic hypergraph construction without\nclustering, and expert edge pooling to enhance semantic extraction and\nfacilitate graph-based image retrieval. Empirical results demonstrate that HgVT\nachieves strong performance on image classification and retrieval, positioning\nit as an efficient framework for semantic-based vision tasks."}
{"id": "2504.08714", "pdf": "https://arxiv.org/pdf/2504.08714", "abs": "https://arxiv.org/abs/2504.08714", "authors": ["Xinyi Gu", "Jiayuan Mao"], "title": "Generating Fine Details of Entity Interactions", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "Project Page: https://concepts-ai.com/p/detailscribe/", "summary": "Images not only depict objects but also encapsulate rich interactions between\nthem. However, generating faithful and high-fidelity images involving multiple\nentities interacting with each other, is a long-standing challenge. While\npre-trained text-to-image models are trained on large-scale datasets to follow\ndiverse text instructions, they struggle to generate accurate interactions,\nlikely due to the scarcity of training data for uncommon object interactions.\nThis paper introduces InterActing, an interaction-focused dataset with 1000\nfine-grained prompts covering three key scenarios: (1) functional and\naction-based interactions, (2) compositional spatial relationships, and (3)\nmulti-subject interactions. To address interaction generation challenges, we\npropose a decomposition-augmented refinement procedure. Our approach,\nDetailScribe, built on Stable Diffusion 3.5, leverages LLMs to decompose\ninteractions into finer-grained concepts, uses a VLM to critique generated\nimages, and applies targeted interventions within the diffusion process in\nrefinement. Automatic and human evaluations show significantly improved image\nquality, demonstrating the potential of enhanced inference strategies. Our\ndataset and code are available at https://concepts-ai.com/p/detailscribe/ to\nfacilitate future exploration of interaction-rich image generation."}
{"id": "2504.08718", "pdf": "https://arxiv.org/pdf/2504.08718", "abs": "https://arxiv.org/abs/2504.08718", "authors": ["Haohang Jian", "Jinlu Zhang", "Junyi Wu", "Zhigang Tu"], "title": "EMO-X: Efficient Multi-Person Pose and Shape Estimation in One-Stage", "categories": ["cs.CV"], "comment": null, "summary": "Expressive Human Pose and Shape Estimation (EHPS) aims to jointly estimate\nhuman pose, hand gesture, and facial expression from monocular images. Existing\nmethods predominantly rely on Transformer-based architectures, which suffer\nfrom quadratic complexity in self-attention, leading to substantial\ncomputational overhead, especially in multi-person scenarios. Recently, Mamba\nhas emerged as a promising alternative to Transformers due to its efficient\nglobal modeling capability. However, it remains limited in capturing\nfine-grained local dependencies, which are essential for precise EHPS. To\naddress these issues, we propose EMO-X, the Efficient Multi-person One-stage\nmodel for multi-person EHPS. Specifically, we explore a Scan-based Global-Local\nDecoder (SGLD) that integrates global context with skeleton-aware local\nfeatures to iteratively enhance human tokens. Our EMO-X leverages the superior\nglobal modeling capability of Mamba and designs a local bidirectional scan\nmechanism for skeleton-aware local refinement. Comprehensive experiments\ndemonstrate that EMO-X strikes an excellent balance between efficiency and\naccuracy. Notably, it achieves a significant reduction in computational\ncomplexity, requiring 69.8% less inference time compared to state-of-the-art\n(SOTA) methods, while outperforming most of them in accuracy."}
{"id": "2504.08727", "pdf": "https://arxiv.org/pdf/2504.08727", "abs": "https://arxiv.org/abs/2504.08727", "authors": ["Boyang Deng", "Songyou Peng", "Kyle Genova", "Gordon Wetzstein", "Noah Snavely", "Leonidas Guibas", "Thomas Funkhouser"], "title": "Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections of Images", "categories": ["cs.CV", "cs.AI", "cs.CY"], "comment": "Project page: https://boyangdeng.com/visual-chronicles; second and\n  third listed authors have equal contributions", "summary": "We present a system using Multimodal LLMs (MLLMs) to analyze a large database\nwith tens of millions of images captured at different times, with the aim of\ndiscovering patterns in temporal changes. Specifically, we aim to capture\nfrequent co-occurring changes (\"trends\") across a city over a certain period.\nUnlike previous visual analyses, our analysis answers open-ended queries (e.g.,\n\"what are the frequent types of changes in the city?\") without any\npredetermined target subjects or training labels. These properties cast prior\nlearning-based or unsupervised visual analysis tools unsuitable. We identify\nMLLMs as a novel tool for their open-ended semantic understanding capabilities.\nYet, our datasets are four orders of magnitude too large for an MLLM to ingest\nas context. So we introduce a bottom-up procedure that decomposes the massive\nvisual analysis problem into more tractable sub-problems. We carefully design\nMLLM-based solutions to each sub-problem. During experiments and ablation\nstudies with our system, we find it significantly outperforms baselines and is\nable to discover interesting trends from images captured in large cities (e.g.,\n\"addition of outdoor dining,\", \"overpass was painted blue,\" etc.). See more\nresults and interactive demos at https://boyangdeng.com/visual-chronicles."}
{"id": "2504.08729", "pdf": "https://arxiv.org/pdf/2504.08729", "abs": "https://arxiv.org/abs/2504.08729", "authors": ["Sonia Joseph", "Praneet Suresh", "Ethan Goldfarb", "Lorenz Hufe", "Yossi Gandelsman", "Robert Graham", "Danilo Bzdok", "Wojciech Samek", "Blake Aaron Richards"], "title": "Steering CLIP's vision transformer with sparse autoencoders", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "8 pages, 7 figures. Accepted to the CVPR 2025 Workshop on Mechanistic\n  Interpretability for Vision (MIV)", "summary": "While vision models are highly capable, their internal mechanisms remain\npoorly understood -- a challenge which sparse autoencoders (SAEs) have helped\naddress in language, but which remains underexplored in vision. We address this\ngap by training SAEs on CLIP's vision transformer and uncover key differences\nbetween vision and language processing, including distinct sparsity patterns\nfor SAEs trained across layers and token types. We then provide the first\nsystematic analysis on the steerability of CLIP's vision transformer by\nintroducing metrics to quantify how precisely SAE features can be steered to\naffect the model's output. We find that 10-15\\% of neurons and features are\nsteerable, with SAEs providing thousands more steerable features than the base\nmodel. Through targeted suppression of SAE features, we then demonstrate\nimproved performance on three vision disentanglement tasks (CelebA, Waterbirds,\nand typographic attacks), finding optimal disentanglement in middle model\nlayers, and achieving state-of-the-art performance on defense against\ntypographic attacks."}
{"id": "2504.08736", "pdf": "https://arxiv.org/pdf/2504.08736", "abs": "https://arxiv.org/abs/2504.08736", "authors": ["Tianwei Xiong", "Jun Hao Liew", "Zilong Huang", "Jiashi Feng", "Xihui Liu"], "title": "GigaTok: Scaling Visual Tokenizers to 3 Billion Parameters for Autoregressive Image Generation", "categories": ["cs.CV"], "comment": "project page: https://silentview.github.io/GigaTok", "summary": "In autoregressive (AR) image generation, visual tokenizers compress images\ninto compact discrete latent tokens, enabling efficient training of downstream\nautoregressive models for visual generation via next-token prediction. While\nscaling visual tokenizers improves image reconstruction quality, it often\ndegrades downstream generation quality -- a challenge not adequately addressed\nin existing literature. To address this, we introduce GigaTok, the first\napproach to simultaneously improve image reconstruction, generation, and\nrepresentation learning when scaling visual tokenizers. We identify the growing\ncomplexity of latent space as the key factor behind the reconstruction vs.\ngeneration dilemma. To mitigate this, we propose semantic regularization, which\naligns tokenizer features with semantically consistent features from a\npre-trained visual encoder. This constraint prevents excessive latent space\ncomplexity during scaling, yielding consistent improvements in both\nreconstruction and downstream autoregressive generation. Building on semantic\nregularization, we explore three key practices for scaling tokenizers:(1) using\n1D tokenizers for better scalability, (2) prioritizing decoder scaling when\nexpanding both encoder and decoder, and (3) employing entropy loss to stabilize\ntraining for billion-scale tokenizers. By scaling to $\\bf{3 \\space billion}$\nparameters, GigaTok achieves state-of-the-art performance in reconstruction,\ndownstream AR generation, and downstream AR representation quality."}
{"id": "2504.07998", "pdf": "https://arxiv.org/pdf/2504.07998", "abs": "https://arxiv.org/abs/2504.07998", "authors": ["Jinming Lu", "Minghao She", "Wendong Mao", "Zhongfeng Wang"], "title": "CDM-QTA: Quantized Training Acceleration for Efficient LoRA Fine-Tuning of Diffusion Model", "categories": ["cs.GR", "cs.AI", "cs.AR", "cs.CV"], "comment": "ISCAS 2025", "summary": "Fine-tuning large diffusion models for custom applications demands\nsubstantial power and time, which poses significant challenges for efficient\nimplementation on mobile devices. In this paper, we develop a novel training\naccelerator specifically for Low-Rank Adaptation (LoRA) of diffusion models,\naiming to streamline the process and reduce computational complexity. By\nleveraging a fully quantized training scheme for LoRA fine-tuning, we achieve\nsubstantial reductions in memory usage and power consumption while maintaining\nhigh model fidelity. The proposed accelerator features flexible dataflow,\nenabling high utilization for irregular and variable tensor shapes during the\nLoRA process. Experimental results show up to 1.81x training speedup and 5.50x\nenergy efficiency improvements compared to the baseline, with minimal impact on\nimage generation quality."}
{"id": "2504.08073", "pdf": "https://arxiv.org/pdf/2504.08073", "abs": "https://arxiv.org/abs/2504.08073", "authors": ["Chengyu Yang", "Chengjun Liu"], "title": "Interpretable Automatic Rosacea Detection with Whitened Cosine Similarity", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "According to the National Rosacea Society, approximately sixteen million\nAmericans suffer from rosacea, a common skin condition that causes flushing or\nlong-term redness on a person's face. To increase rosacea awareness and to\nbetter assist physicians to make diagnosis on this disease, we propose an\ninterpretable automatic rosacea detection method based on whitened cosine\nsimilarity in this paper. The contributions of the proposed methods are\nthree-fold. First, the proposed method can automatically distinguish patients\nsuffering from rosacea from people who are clean of this disease with a\nsignificantly higher accuracy than other methods in unseen test data, including\nboth classical deep learning and statistical methods. Second, the proposed\nmethod addresses the interpretability issue by measuring the similarity between\nthe test sample and the means of two classes, namely the rosacea class versus\nthe normal class, which allows both medical professionals and patients to\nunderstand and trust the results. And finally, the proposed methods will not\nonly help increase awareness of rosacea in the general population, but will\nalso help remind patients who suffer from this disease of possible early\ntreatment, as rosacea is more treatable in its early stages. The code and data\nare available at https://github.com/chengyuyang-njit/ICCRD-2025. The code and\ndata are available at https://github.com/chengyuyang-njit/ICCRD-2025."}
{"id": "2504.08177", "pdf": "https://arxiv.org/pdf/2504.08177", "abs": "https://arxiv.org/abs/2504.08177", "authors": ["Sourya Sengupta", "Satrajit Chakrabarty", "Keerthi Sravan Ravi", "Gopal Avinash", "Ravi Soni"], "title": "SynthFM: Training Modality-agnostic Foundation Models for Medical Image Segmentation without Real Medical Data", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Foundation models like the Segment Anything Model (SAM) excel in zero-shot\nsegmentation for natural images but struggle with medical image segmentation\ndue to differences in texture, contrast, and noise. Annotating medical images\nis costly and requires domain expertise, limiting large-scale annotated data\navailability. To address this, we propose SynthFM, a synthetic data generation\nframework that mimics the complexities of medical images, enabling foundation\nmodels to adapt without real medical data. Using SAM's pretrained encoder and\ntraining the decoder from scratch on SynthFM's dataset, we evaluated our method\non 11 anatomical structures across 9 datasets (CT, MRI, and Ultrasound).\nSynthFM outperformed zero-shot baselines like SAM and MedSAM, achieving\nsuperior results under different prompt settings and on out-of-distribution\ndatasets."}
{"id": "2504.08353", "pdf": "https://arxiv.org/pdf/2504.08353", "abs": "https://arxiv.org/abs/2504.08353", "authors": ["Ren Li", "Cong Cao", "Corentin Dumery", "Yingxuan You", "Hao Li", "Pascal Fua"], "title": "Single View Garment Reconstruction Using Diffusion Mapping Via Pattern Coordinates", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": null, "summary": "Reconstructing 3D clothed humans from images is fundamental to applications\nlike virtual try-on, avatar creation, and mixed reality. While recent advances\nhave enhanced human body recovery, accurate reconstruction of garment geometry\n-- especially for loose-fitting clothing -- remains an open challenge. We\npresent a novel method for high-fidelity 3D garment reconstruction from single\nimages that bridges 2D and 3D representations. Our approach combines Implicit\nSewing Patterns (ISP) with a generative diffusion model to learn rich garment\nshape priors in a 2D UV space. A key innovation is our mapping model that\nestablishes correspondences between 2D image pixels, UV pattern coordinates,\nand 3D geometry, enabling joint optimization of both 3D garment meshes and the\ncorresponding 2D patterns by aligning learned priors with image observations.\nDespite training exclusively on synthetically simulated cloth data, our method\ngeneralizes effectively to real-world images, outperforming existing approaches\non both tight- and loose-fitting garments. The reconstructed garments maintain\nphysical plausibility while capturing fine geometric details, enabling\ndownstream applications including garment retargeting and texture manipulation."}
{"id": "2504.08366", "pdf": "https://arxiv.org/pdf/2504.08366", "abs": "https://arxiv.org/abs/2504.08366", "authors": ["Sauradip Nag", "Daniel Cohen-Or", "Hao Zhang", "Ali Mahdavi-Amiri"], "title": "In-2-4D: Inbetweening from Two Single-View Images to 4D Generation", "categories": ["cs.GR", "cs.CV"], "comment": "Technical Report", "summary": "We propose a new problem, In-2-4D, for generative 4D (i.e., 3D + motion)\ninbetweening from a minimalistic input setting: two single-view images\ncapturing an object in two distinct motion states. Given two images\nrepresenting the start and end states of an object in motion, our goal is to\ngenerate and reconstruct the motion in 4D. We utilize a video interpolation\nmodel to predict the motion, but large frame-to-frame motions can lead to\nambiguous interpretations. To overcome this, we employ a hierarchical approach\nto identify keyframes that are visually close to the input states and show\nsignificant motion, then generate smooth fragments between them. For each\nfragment, we construct the 3D representation of the keyframe using Gaussian\nSplatting. The temporal frames within the fragment guide the motion, enabling\ntheir transformation into dynamic Gaussians through a deformation field. To\nimprove temporal consistency and refine 3D motion, we expand the self-attention\nof multi-view diffusion across timesteps and apply rigid transformation\nregularization. Finally, we merge the independently generated 3D motion\nsegments by interpolating boundary deformation fields and optimizing them to\nalign with the guiding video, ensuring smooth and flicker-free transitions.\nThrough extensive qualitative and quantitiave experiments as well as a user\nstudy, we show the effectiveness of our method and its components. The project\npage is available at https://in-2-4d.github.io/"}
{"id": "2504.08421", "pdf": "https://arxiv.org/pdf/2504.08421", "abs": "https://arxiv.org/abs/2504.08421", "authors": ["Marco Fontana", "Ángel F. García-Fernández", "Simon Maskell"], "title": "Poisson multi-Bernoulli mixture filter for trajectory measurements", "categories": ["eess.SP", "cs.CV", "stat.AP"], "comment": "16 pages, 7 figures, journal paper", "summary": "This paper presents a Poisson multi-Bernoulli mixture (PMBM) filter for\nmulti-target filtering based on sensor measurements that are sets of\ntrajectories in the last two-time step window. The proposed filter, the\ntrajectory measurement PMBM (TM-PMBM) filter, propagates a PMBM density on the\nset of target states. In prediction, the filter obtains the PMBM density on the\nset of trajectories over the last two time steps. This density is then updated\nwith the set of trajectory measurements. After the update step, the PMBM\nposterior on the set of two-step trajectories is marginalised to obtain a PMBM\ndensity on the set of target states. The filter provides a closed-form solution\nfor multi-target filtering based on sets of trajectory measurements, estimating\nthe set of target states at the end of each time window. Additionally, the\npaper proposes computationally lighter alternatives to the TM-PMBM filter by\nderiving a Poisson multi-Bernoulli (PMB) density through Kullback-Leibler\ndivergence minimisation in an augmented space with auxiliary variables. The\nperformance of the proposed filters are evaluated in a simulation study."}
{"id": "2504.08431", "pdf": "https://arxiv.org/pdf/2504.08431", "abs": "https://arxiv.org/abs/2504.08431", "authors": ["Jiafan Lu", "Dongcheng Hu", "Yitian Ye", "Anqi Liu", "Zixian Zhang", "Xin Peng"], "title": "The Composite Visual-Laser Navigation Method Applied in Indoor Poultry Farming Environments", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Indoor poultry farms require inspection robots to maintain precise\nenvironmental control, which is crucial for preventing the rapid spread of\ndisease and large-scale bird mortality. However, the complex conditions within\nthese facilities, characterized by areas of intense illumination and water\naccumulation, pose significant challenges. Traditional navigation methods that\nrely on a single sensor often perform poorly in such environments, resulting in\nissues like laser drift and inaccuracies in visual navigation line extraction.\nTo overcome these limitations, we propose a novel composite navigation method\nthat integrates both laser and vision technologies. This approach dynamically\ncomputes a fused yaw angle based on the real-time reliability of each sensor\nmodality, thereby eliminating the need for physical navigation lines.\nExperimental validation in actual poultry house environments demonstrates that\nour method not only resolves the inherent drawbacks of single-sensor systems,\nbut also significantly enhances navigation precision and operational\nefficiency. As such, it presents a promising solution for improving the\nperformance of inspection robots in complex indoor poultry farming settings."}
{"id": "2504.08541", "pdf": "https://arxiv.org/pdf/2504.08541", "abs": "https://arxiv.org/abs/2504.08541", "authors": ["Zhao Dong", "Ka Chen", "Zhaoyang Lv", "Hong-Xing Yu", "Yunzhi Zhang", "Cheng Zhang", "Yufeng Zhu", "Stephen Tian", "Zhengqin Li", "Geordie Moffatt", "Sean Christofferson", "James Fort", "Xiaqing Pan", "Mingfei Yan", "Jiajun Wu", "Carl Yuheng Ren", "Richard Newcombe"], "title": "Digital Twin Catalog: A Large-Scale Photorealistic 3D Object Digital Twin Dataset", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.RO"], "comment": "accepted to CVPR 2025 highlights", "summary": "We introduce Digital Twin Catalog (DTC), a new large-scale photorealistic 3D\nobject digital twin dataset. A digital twin of a 3D object is a highly\ndetailed, virtually indistinguishable representation of a physical object,\naccurately capturing its shape, appearance, physical properties, and other\nattributes. Recent advances in neural-based 3D reconstruction and inverse\nrendering have significantly improved the quality of 3D object reconstruction.\nDespite these advancements, there remains a lack of a large-scale, digital twin\nquality real-world dataset and benchmark that can quantitatively assess and\ncompare the performance of different reconstruction methods, as well as improve\nreconstruction quality through training or fine-tuning. Moreover, to\ndemocratize 3D digital twin creation, it is essential to integrate creation\ntechniques with next-generation egocentric computing platforms, such as AR\nglasses. Currently, there is no dataset available to evaluate 3D object\nreconstruction using egocentric captured images. To address these gaps, the DTC\ndataset features 2,000 scanned digital twin-quality 3D objects, along with\nimage sequences captured under different lighting conditions using DSLR cameras\nand egocentric AR glasses. This dataset establishes the first comprehensive\nreal-world evaluation benchmark for 3D digital twin creation tasks, offering a\nrobust foundation for comparing and improving existing reconstruction methods.\nThe DTC dataset is already released at\nhttps://www.projectaria.com/datasets/dtc/ and we will also make the baseline\nevaluations open-source."}
{"id": "2504.08548", "pdf": "https://arxiv.org/pdf/2504.08548", "abs": "https://arxiv.org/abs/2504.08548", "authors": ["Miguel Espinosa", "Valerio Marsocci", "Yuru Jia", "Elliot J. Crowley", "Mikolaj Czerkawski"], "title": "COP-GEN-Beta: Unified Generative Modelling of COPernicus Imagery Thumbnails", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted at CVPR 2025 Workshop MORSE", "summary": "In remote sensing, multi-modal data from various sensors capturing the same\nscene offers rich opportunities, but learning a unified representation across\nthese modalities remains a significant challenge. Traditional methods have\noften been limited to single or dual-modality approaches. In this paper, we\nintroduce COP-GEN-Beta, a generative diffusion model trained on optical, radar,\nand elevation data from the Major TOM dataset. What sets COP-GEN-Beta apart is\nits ability to map any subset of modalities to any other, enabling zero-shot\nmodality translation after training. This is achieved through a sequence-based\ndiffusion transformer, where each modality is controlled by its own timestep\nembedding. We extensively evaluate COP-GEN-Beta on thumbnail images from the\nMajor TOM dataset, demonstrating its effectiveness in generating high-quality\nsamples. Qualitative and quantitative evaluations validate the model's\nperformance, highlighting its potential as a powerful pre-trained model for\nfuture remote sensing tasks."}
{"id": "2504.08603", "pdf": "https://arxiv.org/pdf/2504.08603", "abs": "https://arxiv.org/abs/2504.08603", "authors": ["Sebastián Barbas Laina", "Simon Boche", "Sotiris Papatheodorou", "Simon Schaefer", "Jaehyung Jung", "Stefan Leutenegger"], "title": "FindAnything: Open-Vocabulary and Object-Centric Mapping for Robot Exploration in Any Environment", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "11 pages, 5 figures", "summary": "Geometrically accurate and semantically expressive map representations have\nproven invaluable to facilitate robust and safe mobile robot navigation and\ntask planning. Nevertheless, real-time, open-vocabulary semantic understanding\nof large-scale unknown environments is still an open problem. In this paper we\npresent FindAnything, an open-world mapping and exploration framework that\nincorporates vision-language information into dense volumetric submaps. Thanks\nto the use of vision-language features, FindAnything bridges the gap between\npure geometric and open-vocabulary semantic information for a higher level of\nunderstanding while allowing to explore any environment without the help of any\nexternal source of ground-truth pose information. We represent the environment\nas a series of volumetric occupancy submaps, resulting in a robust and accurate\nmap representation that deforms upon pose updates when the underlying SLAM\nsystem corrects its drift, allowing for a locally consistent representation\nbetween submaps. Pixel-wise vision-language features are aggregated from\nefficient SAM (eSAM)-generated segments, which are in turn integrated into\nobject-centric volumetric submaps, providing a mapping from open-vocabulary\nqueries to 3D geometry that is scalable also in terms of memory usage. The\nopen-vocabulary map representation of FindAnything achieves state-of-the-art\nsemantic accuracy in closed-set evaluations on the Replica dataset. This level\nof scene understanding allows a robot to explore environments based on objects\nor areas of interest selected via natural language queries. Our system is the\nfirst of its kind to be deployed on resource-constrained devices, such as MAVs,\nleveraging vision-language information for real-world robotic tasks."}
{"id": "2504.08626", "pdf": "https://arxiv.org/pdf/2504.08626", "abs": "https://arxiv.org/abs/2504.08626", "authors": ["Renu Sharma", "Debasmita Pal", "Arun Ross"], "title": "Task-conditioned Ensemble of Expert Models for Continuous Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "One of the major challenges in machine learning is maintaining the accuracy\nof the deployed model (e.g., a classifier) in a non-stationary environment. The\nnon-stationary environment results in distribution shifts and, consequently, a\ndegradation in accuracy. Continuous learning of the deployed model with new\ndata could be one remedy. However, the question arises as to how we should\nupdate the model with new training data so that it retains its accuracy on the\nold data while adapting to the new data. In this work, we propose a\ntask-conditioned ensemble of models to maintain the performance of the existing\nmodel. The method involves an ensemble of expert models based on task\nmembership information. The in-domain models-based on the local outlier concept\n(different from the expert models) provide task membership information\ndynamically at run-time to each probe sample. To evaluate the proposed method,\nwe experiment with three setups: the first represents distribution shift\nbetween tasks (LivDet-Iris-2017), the second represents distribution shift both\nbetween and within tasks (LivDet-Iris-2020), and the third represents disjoint\ndistribution between tasks (Split MNIST). The experiments highlight the\nbenefits of the proposed method. The source code is available at\nhttps://github.com/iPRoBe-lab/Continuous_Learning_FE_DM."}
