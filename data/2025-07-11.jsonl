{"id": "2507.07210", "title": "WatchWitch: Interoperability, Privacy, and Autonomy for the Apple Watch", "authors": ["Nils Rollshausen", "Alexander Heinrich", "Matthias Hollick", "Jiska Classen"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      To appear in \"Proceedings on Privacy Enhancing Technologies\"", "url": "http://arxiv.org/abs/2507.07210v1", "summary": "Smartwatches such as the Apple Watch collect vast amounts of intimate health\nand fitness data as we wear them. Users have little choice regarding how this\ndata is processed: The Apple Watch can only be used with Apple's iPhones, using\ntheir software and their cloud services. We are the first to publicly\nreverse-engineer the watch's wireless protocols, which led to discovering\nmultiple security issues in Apple's proprietary implementation. With\nWatchWitch, our custom Android reimplementation, we break out of Apple's walled\ngarden -- demonstrating practical interoperability with enhanced privacy\ncontrols and data autonomy. We thus pave the way for more consumer choice in\nthe smartwatch ecosystem, offering users more control over their devices.", "comment": "To appear in \"Proceedings on Privacy Enhancing Technologies\"", "pdf_url": "http://arxiv.org/pdf/2507.07210v1", "cate": "cs.CR", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07244", "title": "Automated Attack Testflow Extraction from Cyber Threat Report using BERT for Contextual Analysis", "authors": ["Faissal Ahmadou", "Sepehr Ghaffarzadegan", "Boubakr Nour", "Makan Pourzandi", "Mourad Debbabi", "Chadi Assi"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07244v1", "summary": "In the ever-evolving landscape of cybersecurity, the rapid identification and\nmitigation of Advanced Persistent Threats (APTs) is crucial. Security\npractitioners rely on detailed threat reports to understand the tactics,\ntechniques, and procedures (TTPs) employed by attackers. However, manually\nextracting attack testflows from these reports requires elusive knowledge and\nis time-consuming and prone to errors. This paper proposes FLOWGUARDIAN, a\nnovel solution leveraging language models (i.e., BERT) and Natural Language\nProcessing (NLP) techniques to automate the extraction of attack testflows from\nunstructured threat reports. FLOWGUARDIAN systematically analyzes and\ncontextualizes security events, reconstructs attack sequences, and then\ngenerates comprehensive testflows. This automated approach not only saves time\nand reduces human error but also ensures comprehensive coverage and robustness\nin cybersecurity testing. Empirical validation using public threat reports\ndemonstrates FLOWGUARDIAN's accuracy and efficiency, significantly enhancing\nthe capabilities of security teams in proactive threat hunting and incident\nresponse.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07244v1", "cate": "cs.CR", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07246", "title": "Disa: Accurate Learning-based Static Disassembly with Attentions", "authors": ["Peicheng Wang", "Monika Santra", "Mingyu Liu", "Cong Sun", "Dongrui Zeng", "Gang Tan"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      To appear at ACM CCS 2025", "url": "http://arxiv.org/abs/2507.07246v1", "summary": "For reverse engineering related security domains, such as vulnerability\ndetection, malware analysis, and binary hardening, disassembly is crucial yet\nchallenging. The fundamental challenge of disassembly is to identify\ninstruction and function boundaries. Classic approaches rely on file-format\nassumptions and architecture-specific heuristics to guess the boundaries,\nresulting in incomplete and incorrect disassembly, especially when the binary\nis obfuscated. Recent advancements of disassembly have demonstrated that deep\nlearning can improve both the accuracy and efficiency of disassembly. In this\npaper, we propose Disa, a new learning-based disassembly approach that uses the\ninformation of superset instructions over the multi-head self-attention to\nlearn the instructions' correlations, thus being able to infer function\nentry-points and instruction boundaries. Disa can further identify instructions\nrelevant to memory block boundaries to facilitate an advanced block-memory\nmodel based value-set analysis for an accurate control flow graph (CFG)\ngeneration. Our experiments show that Disa outperforms prior deep-learning\ndisassembly approaches in function entry-point identification, especially\nachieving 9.1% and 13.2% F1-score improvement on binaries respectively\nobfuscated by the disassembly desynchronization technique and popular\nsource-level obfuscator. By achieving an 18.5% improvement in the memory block\nprecision, Disa generates more accurate CFGs with a 4.4% reduction in Average\nIndirect Call Targets (AICT) compared with the state-of-the-art heuristic-based\napproach.", "comment": "To appear at ACM CCS 2025", "pdf_url": "http://arxiv.org/pdf/2507.07246v1", "cate": "cs.CR", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07250", "title": "Semi-fragile watermarking of remote sensing images using DWT, vector quantization and automatic tiling", "authors": ["Jordi Serra-Ruiz", "David Meg√≠as"], "categories": ["cs.CR", "cs.MM"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07250v1", "summary": "A semi-fragile watermarking scheme for multiple band images is presented in\nthis article. We propose to embed a mark into remote sensing images applying a\ntree-structured vector quantization approach to the pixel signatures instead of\nprocessing each band separately. The signature of the multispectral or\nhyperspectral image is used to embed the mark in it order to detect any\nsignificant modification of the original image. The image is segmented into\nthree-dimensional blocks, and a tree-structured vector quantizer is built for\neach block. These trees are manipulated using an iterative algorithm until the\nresulting block satisfies a required criterion, which establishes the embedded\nmark. The method is shown to be able to preserve the mark under lossy\ncompression (above a given threshold) but, at the same time, it detects\npossibly forged blocks and their position in the whole image.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07250v1", "cate": "cs.CR", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07258", "title": "FedP3E: Privacy-Preserving Prototype Exchange for Non-IID IoT Malware Detection in Cross-Silo Federated Learning", "authors": ["Rami Darwish", "Mahmoud Abdelsalam", "Sajad Khorsandroo", "Kaushik Roy"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07258v1", "summary": "As IoT ecosystems continue to expand across critical sectors, they have\nbecome prominent targets for increasingly sophisticated and large-scale malware\nattacks. The evolving threat landscape, combined with the sensitive nature of\nIoT-generated data, demands detection frameworks that are both\nprivacy-preserving and resilient to data heterogeneity. Federated Learning (FL)\noffers a promising solution by enabling decentralized model training without\nexposing raw data. However, standard FL algorithms such as FedAvg and FedProx\noften fall short in real-world deployments characterized by class imbalance and\nnon-IID data distributions -- particularly in the presence of rare or disjoint\nmalware classes. To address these challenges, we propose FedP3E\n(Privacy-Preserving Prototype Exchange), a novel FL framework that supports\nindirect cross-client representation sharing while maintaining data privacy.\nEach client constructs class-wise prototypes using Gaussian Mixture Models\n(GMMs), perturbs them with Gaussian noise, and transmits only these compact\nsummaries to the server. The aggregated prototypes are then distributed back to\nclients and integrated into local training, supported by SMOTE-based\naugmentation to enhance representation of minority malware classes. Rather than\nrelying solely on parameter averaging, our prototype-driven mechanism enables\nclients to enrich their local models with complementary structural patterns\nobserved across the federation -- without exchanging raw data or gradients.\nThis targeted strategy reduces the adverse impact of statistical heterogeneity\nwith minimal communication overhead. We evaluate FedP3E on the N-BaIoT dataset\nunder realistic cross-silo scenarios with varying degrees of data imbalance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07258v1", "cate": "cs.CR", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07401", "title": "Shuffling for Semantic Secrecy", "authors": ["Fupei Chen", "Liyao Xiang", "Haoxiang Sun", "Hei Victor Cheng", "Kaiming Shen"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07401v1", "summary": "Deep learning draws heavily on the latest progress in semantic\ncommunications. The present paper aims to examine the security aspect of this\ncutting-edge technique from a novel shuffling perspective. Our goal is to\nimprove upon the conventional secure coding scheme to strike a desirable\ntradeoff between transmission rate and leakage rate. To be more specific, for a\nwiretap channel, we seek to maximize the transmission rate while minimizing the\nsemantic error probability under the given leakage rate constraint. Toward this\nend, we devise a novel semantic security communication system wherein the\nrandom shuffling pattern plays the role of the shared secret key. Intuitively,\nthe permutation of feature sequences via shuffling would distort the semantic\nessence of the target data to a sufficient extent so that eavesdroppers cannot\naccess it anymore. The proposed random shuffling method also exhibits its\nflexibility in working for the existing semantic communication system as a\nplugin. Simulations demonstrate the significant advantage of the proposed\nmethod over the benchmark in boosting secure transmission, especially when\nchannels are prone to strong noise and unpredictable fading.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07401v1", "cate": "cs.CR", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07406", "title": "Phishing Detection in the Gen-AI Era: Quantized LLMs vs Classical Models", "authors": ["Jikesh Thapa", "Gurrehmat Chahal", "Serban Voinea Gabreanu", "Yazan Otoum"], "categories": ["cs.CR", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      8 Pages, IEEE Conference", "url": "http://arxiv.org/abs/2507.07406v1", "summary": "Phishing attacks are becoming increasingly sophisticated, underscoring the\nneed for detection systems that strike a balance between high accuracy and\ncomputational efficiency. This paper presents a comparative evaluation of\ntraditional Machine Learning (ML), Deep Learning (DL), and quantized\nsmall-parameter Large Language Models (LLMs) for phishing detection. Through\nexperiments on a curated dataset, we show that while LLMs currently\nunderperform compared to ML and DL methods in terms of raw accuracy, they\nexhibit strong potential for identifying subtle, context-based phishing cues.\nWe also investigate the impact of zero-shot and few-shot prompting strategies,\nrevealing that LLM-rephrased emails can significantly degrade the performance\nof both ML and LLM-based detectors. Our benchmarking highlights that models\nlike DeepSeek R1 Distill Qwen 14B (Q8_0) achieve competitive accuracy, above\n80%, using only 17GB of VRAM, supporting their viability for cost-efficient\ndeployment. We further assess the models' adversarial robustness and\ncost-performance tradeoffs, and demonstrate how lightweight LLMs can provide\nconcise, interpretable explanations to support real-time decision-making. These\nfindings position optimized LLMs as promising components in phishing defence\nsystems and offer a path forward for integrating explainable, efficient AI into\nmodern cybersecurity frameworks.", "comment": "8 Pages, IEEE Conference", "pdf_url": "http://arxiv.org/pdf/2507.07406v1", "cate": "cs.CR", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07413", "title": "Hybrid LLM-Enhanced Intrusion Detection for Zero-Day Threats in IoT Networks", "authors": ["Mohammad F. Al-Hammouri", "Yazan Otoum", "Rasha Atwa", "Amiya Nayak"], "categories": ["cs.CR", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      6 pages, IEEE conference", "url": "http://arxiv.org/abs/2507.07413v1", "summary": "This paper presents a novel approach to intrusion detection by integrating\ntraditional signature-based methods with the contextual understanding\ncapabilities of the GPT-2 Large Language Model (LLM). As cyber threats become\nincreasingly sophisticated, particularly in distributed, heterogeneous, and\nresource-constrained environments such as those enabled by the Internet of\nThings (IoT), the need for dynamic and adaptive Intrusion Detection Systems\n(IDSs) becomes increasingly urgent. While traditional methods remain effective\nfor detecting known threats, they often fail to recognize new and evolving\nattack patterns. In contrast, GPT-2 excels at processing unstructured data and\nidentifying complex semantic relationships, making it well-suited to uncovering\nsubtle, zero-day attack vectors. We propose a hybrid IDS framework that merges\nthe robustness of signature-based techniques with the adaptability of\nGPT-2-driven semantic analysis. Experimental evaluations on a representative\nintrusion dataset demonstrate that our model enhances detection accuracy by\n6.3%, reduces false positives by 9.0%, and maintains near real-time\nresponsiveness. These results affirm the potential of language model\nintegration to build intelligent, scalable, and resilient cybersecurity\ndefences suited for modern connected environments.", "comment": "6 pages, IEEE conference", "pdf_url": "http://arxiv.org/pdf/2507.07413v1", "cate": "cs.CR", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07416", "title": "Autonomous AI-based Cybersecurity Framework for Critical Infrastructure: Real-Time Threat Mitigation", "authors": ["Jenifer Paulraj", "Brindha Raghuraman", "Nagarani Gopalakrishnan", "Yazan Otoum"], "categories": ["cs.CR", "cs.AI", "cs.ET", "cs.LG"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      7 pages, IEEE conference", "url": "http://arxiv.org/abs/2507.07416v1", "summary": "Critical infrastructure systems, including energy grids, healthcare\nfacilities, transportation networks, and water distribution systems, are\npivotal to societal stability and economic resilience. However, the increasing\ninterconnectivity of these systems exposes them to various cyber threats,\nincluding ransomware, Denial-of-Service (DoS) attacks, and Advanced Persistent\nThreats (APTs). This paper examines cybersecurity vulnerabilities in critical\ninfrastructure, highlighting the threat landscape, attack vectors, and the role\nof Artificial Intelligence (AI) in mitigating these risks. We propose a hybrid\nAI-driven cybersecurity framework to enhance real-time vulnerability detection,\nthreat modelling, and automated remediation. This study also addresses the\ncomplexities of adversarial AI, regulatory compliance, and integration. Our\nfindings provide actionable insights to strengthen the security and resilience\nof critical infrastructure systems against emerging cyber threats.", "comment": "7 pages, IEEE conference", "pdf_url": "http://arxiv.org/pdf/2507.07416v1", "cate": "cs.CR", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07417", "title": "May I have your Attention? Breaking Fine-Tuning based Prompt Injection Defenses using Architecture-Aware Attacks", "authors": ["Nishit V. Pandya", "Andrey Labunets", "Sicun Gao", "Earlence Fernandes"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07417v1", "summary": "A popular class of defenses against prompt injection attacks on large\nlanguage models (LLMs) relies on fine-tuning the model to separate instructions\nand data, so that the LLM does not follow instructions that might be present\nwith data. There are several academic systems and production-level\nimplementations of this idea. We evaluate the robustness of this class of\nprompt injection defenses in the whitebox setting by constructing strong\noptimization-based attacks and showing that the defenses do not provide the\nclaimed security properties. Specifically, we construct a novel attention-based\nattack algorithm for text-based LLMs and apply it to two recent whitebox\ndefenses SecAlign (CCS 2025) and StruQ (USENIX Security 2025), showing attacks\nwith success rates of up to 70% with modest increase in attacker budget in\nterms of tokens. Our findings make fundamental progress towards understanding\nthe robustness of prompt injection defenses in the whitebox setting. We release\nour code and attacks at https://github.com/nishitvp/better_opts_attacks", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07417v1", "cate": "cs.CR", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07732", "title": "RADAR: a Radio-based Analytics for Dynamic Association and Recognition of pseudonyms in VANETs", "authors": ["Giovanni Gambigliani Zoccoli", "Filip Valgimigli", "Dario Stabili", "Mirco Marchetti"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      7 pages, 4 figures, accepted for publication at the 2025 IEEE 102nd Vehicular Technology Conference: VTC2025-Fall", "url": "http://arxiv.org/abs/2507.07732v1", "summary": "This paper presents RADAR, a tracking algorithm for vehicles participating in\nCooperative Intelligent Transportation Systems (C-ITS) that exploits multiple\nradio signals emitted by a modern vehicle to break privacy-preserving pseudonym\nschemes deployed in VANETs. This study shows that by combining Dedicated Short\nRange Communication (DSRC) and Wi-Fi probe request messages broadcast by the\nvehicle, it is possible to improve tracking over standard de-anonymization\napproaches that only leverage DSRC, especially in realistic scenarios where the\nattacker does not have full coverage of the entire vehicle path. The\nexperimental evaluation compares three different metrics for pseudonym and\nWi-Fi probe identifier association (Count, Statistical RSSI, and Pearson RSSI),\ndemonstrating that the Pearson RSSI metric is better at tracking vehicles under\npseudonym-changing schemes in all scenarios and against previous works. As an\nadditional contribution to the state-of-the-art, we publicly release all\nimplementations and simulation scenarios used in this work.", "comment": "7 pages, 4 figures, accepted for publication at the 2025 IEEE 102nd\n  Vehicular Technology Conference: VTC2025-Fall", "pdf_url": "http://arxiv.org/pdf/2507.07732v1", "cate": "cs.CR", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07773", "title": "Rainbow Artifacts from Electromagnetic Signal Injection Attacks on Image Sensors", "authors": ["Youqian Zhang", "Xinyu Ji", "Zhihao Wang", "Qinhong Jiang"], "categories": ["cs.CR", "cs.CV", "B.8; I.4"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      5 pages, 4 figures", "url": "http://arxiv.org/abs/2507.07773v1", "summary": "Image sensors are integral to a wide range of safety- and security-critical\nsystems, including surveillance infrastructure, autonomous vehicles, and\nindustrial automation. These systems rely on the integrity of visual data to\nmake decisions. In this work, we investigate a novel class of electromagnetic\nsignal injection attacks that target the analog domain of image sensors,\nallowing adversaries to manipulate raw visual inputs without triggering\nconventional digital integrity checks. We uncover a previously undocumented\nattack phenomenon on CMOS image sensors: rainbow-like color artifacts induced\nin images captured by image sensors through carefully tuned electromagnetic\ninterference. We further evaluate the impact of these attacks on\nstate-of-the-art object detection models, showing that the injected artifacts\npropagate through the image signal processing pipeline and lead to significant\nmispredictions. Our findings highlight a critical and underexplored\nvulnerability in the visual perception stack, highlighting the need for more\nrobust defenses against physical-layer attacks in such systems.", "comment": "5 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.07773v1", "cate": "cs.CR", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07871", "title": "Mitigating Watermark Stealing Attacks in Generative Models via Multi-Key Watermarking", "authors": ["Toluwani Aremu", "Noor Hussein", "Munachiso Nwadike", "Samuele Poppi", "Jie Zhang", "Karthik Nandakumar", "Neil Gong", "Nils Lukas"], "categories": ["cs.CR", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07871v1", "summary": "Watermarking offers a promising solution for GenAI providers to establish the\nprovenance of their generated content. A watermark is a hidden signal embedded\nin the generated content, whose presence can later be verified using a secret\nwatermarking key. A threat to GenAI providers are \\emph{watermark stealing}\nattacks, where users forge a watermark into content that was \\emph{not}\ngenerated by the provider's models without access to the secret key, e.g., to\nfalsely accuse the provider. Stealing attacks collect \\emph{harmless}\nwatermarked samples from the provider's model and aim to maximize the expected\nsuccess rate of generating \\emph{harmful} watermarked samples. Our work focuses\non mitigating stealing attacks while treating the underlying watermark as a\nblack-box. Our contributions are: (i) Proposing a multi-key extension to\nmitigate stealing attacks that can be applied post-hoc to any watermarking\nmethod across any modality. (ii) We provide theoretical guarantees and\ndemonstrate empirically that our method makes forging substantially less\neffective across multiple datasets, and (iii) we formally define the threat of\nwatermark forging as the task of generating harmful, watermarked content and\nmodel this threat via security games.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07871v1", "cate": "cs.CR", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07901", "title": "The Trust Fabric: Decentralized Interoperability and Economic Coordination for the Agentic Web", "authors": ["Sree Bhargavi Balija", "Rekha Singal", "Abhishek Singh", "Ramesh Raskar", "Erfan Darzi", "Raghu Bala", "Thomas Hardjono", "Ken Huang"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07901v1", "summary": "The fragmentation of AI agent ecosystems has created urgent demands for\ninteroperability, trust, and economic coordination that current protocols --\nincluding MCP (Hou et al., 2025), A2A (Habler et al., 2025), ACP (Liu et al.,\n2025), and Cisco's AGP (Edwards, 2025) -- cannot address at scale. We present\nthe Nanda Unified Architecture, a decentralized framework built around three\ncore innovations: fast DID-based agent discovery through distributed\nregistries, semantic agent cards with verifiable credentials and composability\nprofiles, and a dynamic trust layer that integrates behavioral attestations\nwith policy compliance. The system introduces X42/H42 micropayments for\neconomic coordination and MAESTRO, a security framework incorporating\nSynergetics' patented AgentTalk protocol (US Patent 12,244,584 B1) and secure\ncontainerization. Real-world deployments demonstrate 99.9 percent compliance in\nhealthcare applications and substantial monthly transaction volumes with strong\nprivacy guarantees. By unifying MIT's trust research with production\ndeployments from Cisco and Synergetics, we show how cryptographic proofs and\npolicy-as-code transform agents into trust-anchored participants in a\ndecentralized economy (Lakshmanan, 2025; Sha, 2025). The result enables a\nglobally interoperable Internet of Agents where trust becomes the native\ncurrency of collaboration across both enterprise and Web3 ecosystems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07901v1", "cate": "cs.CR", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07115", "title": "Autonomous Control Leveraging LLMs: An Agentic Framework for Next-Generation Industrial Automation", "authors": ["Javal Vyas", "Mehmet Mercangoz"], "categories": ["cs.AI", "cs.MA", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07115v1", "summary": "The increasing complexity of modern chemical processes, coupled with\nworkforce shortages and intricate fault scenarios, demands novel automation\nparadigms that blend symbolic reasoning with adaptive control. In this work, we\nintroduce a unified agentic framework that leverages large language models\n(LLMs) for both discrete fault-recovery planning and continuous process control\nwithin a single architecture. We adopt Finite State Machines (FSMs) as\ninterpretable operating envelopes: an LLM-driven planning agent proposes\nrecovery sequences through the FSM, a Simulation Agent executes and checks each\ntransition, and a Validator-Reprompting loop iteratively refines invalid plans.\nIn Case Study 1, across 180 randomly generated FSMs of varying sizes (4-25\nstates, 4-300 transitions), GPT-4o and GPT-4o-mini achieve 100% valid-path\nsuccess within five reprompts-outperforming open-source LLMs in both accuracy\nand latency. In Case Study 2, the same framework modulates dual-heater inputs\non a laboratory TCLab platform (and its digital twin) to maintain a target\naverage temperature under persistent asymmetric disturbances. Compared to\nclassical PID control, our LLM-based controller attains similar performance,\nwhile ablation of the prompting loop reveals its critical role in handling\nnonlinear dynamics. We analyze key failure modes-such as instruction following\nlapses and coarse ODE approximations. Our results demonstrate that, with\nstructured feedback and modular agents, LLMs can unify high-level symbolic\nplanningand low-level continuous control, paving the way towards resilient,\nlanguage-driven automation in chemical engineering.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07115v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03"}
{"id": "2507.07916", "title": "Can Large Language Models Improve Phishing Defense? A Large-Scale Controlled Experiment on Warning Dialogue Explanations", "authors": ["Federico Maria Cau", "Giuseppe Desolda", "Francesco Greco", "Lucio Davide Spano", "Luca Vigan√≤"], "categories": ["cs.CR", "cs.HC"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07916v1", "summary": "Phishing has become a prominent risk in modern cybersecurity, often used to\nbypass technological defences by exploiting predictable human behaviour.\nWarning dialogues are a standard mitigation measure, but the lack of\nexplanatory clarity and static content limits their effectiveness. In this\npaper, we report on our research to assess the capacity of Large Language\nModels (LLMs) to generate clear, concise, and scalable explanations for\nphishing warnings. We carried out a large-scale between-subjects user study (N\n= 750) to compare the influence of warning dialogues supplemented with manually\ngenerated explanations against those generated by two LLMs, Claude 3.5 Sonnet\nand Llama 3.3 70B. We investigated two explanatory styles (feature-based and\ncounterfactual) for their effects on behavioural metrics (click-through rate)\nand perceptual outcomes (e.g., trust, risk, clarity). The results indicate that\nwell-constructed LLM-generated explanations can equal or surpass manually\ncrafted explanations in reducing susceptibility to phishing; Claude-generated\nwarnings exhibited particularly robust performance. Feature-based explanations\nwere more effective for genuine phishing attempts, whereas counterfactual\nexplanations diminished false-positive rates. Other variables such as workload,\ngender, and prior familiarity with warning dialogues significantly moderated\nwarning effectiveness. These results indicate that LLMs can be used to\nautomatically build explanations for warning users against phishing, and that\nsuch solutions are scalable, adaptive, and consistent with human-centred\nvalues.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07916v1", "cate": "cs.CR", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07257", "title": "Open Source Planning & Control System with Language Agents for Autonomous Scientific Discovery", "authors": ["Licong Xu", "Milind Sarkar", "Anto I. Lonappan", "√ç√±igo Zubeldia", "Pablo Villanueva-Domingo", "Santiago Casas", "Christian Fidler", "Chetana Amancharla", "Ujjwal Tiwari", "Adrian Bayer", "Chadi Ait Ekiou", "Miles Cranmer", "Adrian Dimitrov", "James Fergusson", "Kahaan Gandhi", "Sven Krippendorf", "Andrew Laverick", "Julien Lesgourgues", "Antony Lewis", "Thomas Meier", "Blake Sherwin", "Kristen Surrao", "Francisco Villaescusa-Navarro", "Chi Wang", "Xueqing Xu", "Boris Bolliet"], "categories": ["cs.AI", "astro-ph.IM", "cs.CL", "cs.MA"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Accepted contribution to the ICML 2025 Workshop on Machine Learning for Astrophysics. Code: this https URL Videos: this https URL HuggingFace: this https URL Cloud: this https URL", "url": "http://arxiv.org/abs/2507.07257v1", "summary": "We present a multi-agent system for automation of scientific research tasks,\ncmbagent. The system is formed by about 30 Large Language Model (LLM) agents\nand implements a Planning & Control strategy to orchestrate the agentic\nworkflow, with no human-in-the-loop at any point. Each agent specializes in a\ndifferent task (performing retrieval on scientific papers and codebases,\nwriting code, interpreting results, critiquing the output of other agents) and\nthe system is able to execute code locally. We successfully apply cmbagent to\ncarry out a PhD level cosmology task (the measurement of cosmological\nparameters using supernova data) and evaluate its performance on two benchmark\nsets, finding superior performance over state-of-the-art LLMs. The source code\nis available on GitHub, demonstration videos are also available, and the system\nis deployed on HuggingFace and will be available on the cloud.", "comment": "Accepted contribution to the ICML 2025 Workshop on Machine Learning\n  for Astrophysics. Code: https://github.com/CMBAgents/cmbagent; Videos:\n  https://www.youtube.com/@cmbagent; HuggingFace:\n  https://huggingface.co/spaces/astropilot-ai/cmbagent; Cloud:\n  https://cmbagent.cloud", "pdf_url": "http://arxiv.org/pdf/2507.07257v1", "cate": "cs.AI", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07927", "title": "KeyDroid: A Large-Scale Analysis of Secure Key Storage in Android Apps", "authors": ["Jenny Blessing", "Ross J. Anderson", "Alastair R. Beresford"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07927v1", "summary": "Most contemporary mobile devices offer hardware-backed storage for\ncryptographic keys, user data, and other sensitive credentials. Such hardware\nprotects credentials from extraction by an adversary who has compromised the\nmain operating system, such as a malicious third-party app. Since 2011, Android\napp developers can access trusted hardware via the Android Keystore API. In\nthis work, we conduct the first comprehensive survey of hardware-backed key\nstorage in Android devices. We analyze 490 119 Android apps, collecting data on\nhow trusted hardware is used by app developers (if used at all) and\ncross-referencing our findings with sensitive user data collected by each app,\nas self-reported by developers via the Play Store's data safety labels.\n  We find that despite industry-wide initiatives to encourage adoption, 56.3%\nof apps self-reporting as processing sensitive user data do not use Android's\ntrusted hardware capabilities at all, while just 5.03% of apps collecting some\nform of sensitive data use the strongest form of trusted hardware, a secure\nelement distinct from the main processor. To better understand the potential\ndownsides of using secure hardware, we conduct the first empirical analysis of\ntrusted hardware performance in mobile devices, measuring the runtime of common\ncryptographic operations across both software- and hardware-backed keystores.\nWe find that while hardware-backed key storage using a coprocessor is viable\nfor most common cryptographic operations, secure elements capable of preventing\nmore advanced attacks make performance infeasible for symmetric encryption with\nnon-negligible payloads and any kind of asymmetric encryption.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07927v1", "cate": "cs.CR", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07400", "title": "KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent Workflows", "authors": ["Zaifeng Pan", "Ajjkumar Patel", "Zhengding Hu", "Yipeng Shen", "Yue Guan", "Wan-Lu Li", "Lianhui Qin", "Yida Wang", "Yufei Ding"], "categories": ["cs.DC", "cs.MA"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07400v1", "summary": "Large language model (LLM) based agentic workflows have become a popular\nparadigm for coordinating multiple specialized agents to solve complex tasks.\nTo improve serving efficiency, existing LLM systems employ prefix caching to\nreuse key-value (KV) tensors corresponding to agents' fixed prompts, thereby\navoiding redundant computation across repeated invocations. However, current\nsystems typically evict KV caches using a Least Recently Used (LRU) policy,\nwhich fails to anticipate future agent usage and often discards KV caches\nshortly before their reuse. This leads to frequent cache misses and substantial\nrecomputation or swapping overhead. We present KVFlow, a workflow-aware KV\ncache management framework tailored for agentic workloads. KVFlow abstracts the\nagent execution schedule as an Agent Step Graph and assigns each agent a\nsteps-to-execution value that estimates its temporal proximity to future\nactivation. These values guide a fine-grained eviction policy at the KV node\nlevel, allowing KVFlow to preserve entries likely to be reused and efficiently\nmanage shared prefixes in tree-structured caches. Moreover, KVFlow introduces a\nfully overlapped KV prefetching mechanism, which proactively loads required\ntensors from CPU to GPU in background threads for agents scheduled in the next\nstep, thereby avoiding cache miss stalls during generation. Compared to SGLang\nwith hierarchical radix cache, KVFlow achieves up to 1.83$\\times$ speedup for\nsingle workflows with large prompts, and up to 2.19$\\times$ speedup for\nscenarios with many concurrent workflows.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07400v1", "cate": "cs.DC", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07972", "title": "EinHops: Einsum Notation for Expressive Homomorphic Operations on RNS-CKKS Tensors", "authors": ["Karthik Garimella", "Austin Ebel", "Brandon Reagen"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      11 pages, 7 figures, 1 table", "url": "http://arxiv.org/abs/2507.07972v1", "summary": "Fully Homomorphic Encryption (FHE) is an encryption scheme that allows for\ncomputation to be performed directly on encrypted data, effectively closing the\nloop on secure and outsourced computing. Data is encrypted not only during rest\nand transit, but also during processing. However, FHE provides a limited\ninstruction set: SIMD addition, SIMD multiplication, and cyclic rotation of 1-D\nvectors. This restriction makes performing multi-dimensional tensor operations\nchallenging. Practitioners must pack these tensors into 1-D vectors and map\ntensor operations onto this one-dimensional layout rather than their\ntraditional nested structure. And while prior systems have made significant\nstrides in automating this process, they often hide critical packing decisions\nbehind layers of abstraction, making debugging, optimizing, and building on top\nof these systems difficult.\n  In this work, we approach multi-dimensional tensor operations in FHE through\nEinstein summation (einsum) notation. Einsum notation explicitly encodes\ndimensional structure and operations in its syntax, naturally exposing how\ntensors should be packed and transformed. We decompose einsum expressions into\na fixed set of FHE-friendly operations. We implement our design and present\nEinHops, a minimalist system that factors einsum expressions into a fixed\nsequence of FHE operations. EinHops enables developers to perform encrypted\ntensor operations using FHE while maintaining full visibility into the\nunderlying packing strategy. We evaluate EinHops on a range of tensor\noperations from a simple transpose to complex multi-dimensional contractions.\nWe show that the explicit nature of einsum notation allows us to build an FHE\ntensor system that is simple, general, and interpretable. We open-source\nEinHops at the following repository: https://github.com/baahl-nyu/einhops.", "comment": "11 pages, 7 figures, 1 table", "pdf_url": "http://arxiv.org/pdf/2507.07972v1", "cate": "cs.CR", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07509", "title": "Toward Real-World Chinese Psychological Support Dialogues: CPsDD Dataset and a Co-Evolving Multi-Agent System", "authors": ["Yuanchen Shi", "Longyin Zhang", "Fang Kong"], "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      10pages,8 figures", "url": "http://arxiv.org/abs/2507.07509v1", "summary": "The growing need for psychological support due to increasing pressures has\nexposed the scarcity of relevant datasets, particularly in non-English\nlanguages. To address this, we propose a framework that leverages limited\nreal-world data and expert knowledge to fine-tune two large language models:\nDialog Generator and Dialog Modifier. The Generator creates large-scale\npsychological counseling dialogues based on predefined paths, which guide\nsystem response strategies and user interactions, forming the basis for\neffective support. The Modifier refines these dialogues to align with\nreal-world data quality. Through both automated and manual review, we construct\nthe Chinese Psychological support Dialogue Dataset (CPsDD), containing 68K\ndialogues across 13 groups, 16 psychological problems, 13 causes, and 12\nsupport focuses. Additionally, we introduce the Comprehensive Agent Dialogue\nSupport System (CADSS), where a Profiler analyzes user characteristics, a\nSummarizer condenses dialogue history, a Planner selects strategies, and a\nSupporter generates empathetic responses. The experimental results of the\nStrategy Prediction and Emotional Support Conversation (ESC) tasks demonstrate\nthat CADSS achieves state-of-the-art performance on both CPsDD and ESConv\ndatasets.", "comment": "10pages,8 figures", "pdf_url": "http://arxiv.org/pdf/2507.07509v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07142", "title": "g2o vs. Ceres: Optimizing Scan Matching in Cartographer SLAM", "authors": ["Quanjie Qiu", "MengCheng Lau"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07142v1", "summary": "This article presents a comparative analysis of g2o and Ceres solvers in\nenhancing scan matching performance within the Cartographer framework.\nCartographer, a widely-used library for Simultaneous Localization and Mapping\n(SLAM), relies on optimization algorithms to refine pose estimates and improve\nmap accuracy. The research aims to evaluate the performance, efficiency, and\naccuracy of the g2o solver in comparison to the Ceres solver, which is the\ndefault in Cartographer. In our experiments comparing Ceres and g2o within\nCartographer, Ceres outperformed g2o in terms of speed, convergence efficiency,\nand overall map clarity. Ceres required fewer iterations and less time to\nconverge, producing more accurate and well-defined maps, especially in\nreal-world mapping scenarios with the AgileX LIMO robot. However, g2o excelled\nin localized obstacle detection, highlighting its value in specific situations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07142v1", "cate": "cs.RO", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07974", "title": "Defending Against Prompt Injection With a Few DefensiveTokens", "authors": ["Sizhe Chen", "Yizhu Wang", "Nicholas Carlini", "Chawin Sitawarin", "David Wagner"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07974v1", "summary": "When large language model (LLM) systems interact with external data to\nperform complex tasks, a new attack, namely prompt injection, becomes a\nsignificant threat. By injecting instructions into the data accessed by the\nsystem, the attacker is able to override the initial user task with an\narbitrary task directed by the attacker. To secure the system, test-time\ndefenses, e.g., defensive prompting, have been proposed for system developers\nto attain security only when needed in a flexible manner. However, they are\nmuch less effective than training-time defenses that change the model\nparameters. Motivated by this, we propose DefensiveToken, a test-time defense\nwith prompt injection robustness comparable to training-time alternatives.\nDefensiveTokens are newly inserted as special tokens, whose embeddings are\noptimized for security. In security-sensitive cases, system developers can\nappend a few DefensiveTokens before the LLM input to achieve security with a\nminimal utility drop. In scenarios where security is less of a concern,\ndevelopers can simply skip DefensiveTokens; the LLM system remains the same as\nthere is no defense, generating high-quality responses. Thus, DefensiveTokens,\nif released alongside the model, allow a flexible switch between the\nstate-of-the-art (SOTA) utility and almost-SOTA security at test time. The code\nis available at https://github.com/Sizhe-Chen/DefensiveToken.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07974v1", "cate": "cs.CR", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07560", "title": "Conjugated Capabilities: Interrelations of Elementary Human Capabilities and Their Implication on Human-Machine Task Allocation and Capability Testing Procedures", "authors": ["Nils Mandischer", "Larissa F√ºller", "Torsten Alles", "Frank Flemisch", "Lars Mikelsons"], "categories": ["cs.HC", "cs.MA", "cs.RO"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      This work was accepted by the IEEE International Conference on Systems, Man, and Cybernetics (SMC), Vienna, Austria, 2025", "url": "http://arxiv.org/abs/2507.07560v1", "summary": "Human and automation capabilities are the foundation of every human-autonomy\ninteraction and interaction pattern. Therefore, machines need to understand the\ncapacity and performance of human doing, and adapt their own behavior,\naccordingly. In this work, we address the concept of conjugated capabilities,\ni.e. capabilities that are dependent or interrelated and between which effort\ncan be distributed. These may be used to overcome human limitations, by\nshifting effort from a deficient to a conjugated capability with performative\nresources. For example: A limited arm's reach may be compensated by tilting the\ntorso forward. We analyze the interrelation between elementary capabilities\nwithin the IMBA standard to uncover potential conjugation, and show evidence in\ndata of post-rehabilitation patients. From the conjugated capabilities, within\nthe example application of stationary manufacturing, we create a network of\ninterrelations. With this graph, a manifold of potential uses is enabled. We\nshowcase the graph's usage in optimizing IMBA test design to accelerate data\nrecordings, and discuss implications of conjugated capabilities on task\nallocation between the human and an autonomy.", "comment": "This work was accepted by the IEEE International Conference on\n  Systems, Man, and Cybernetics (SMC), Vienna, Austria, 2025", "pdf_url": "http://arxiv.org/pdf/2507.07560v1", "cate": "cs.HC", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07221", "title": "Self-Wearing Adaptive Garments via Soft Robotic Unfurling", "authors": ["Nam Gyun Kim", "William E. Heap", "Yimeng Qin", "Elvy B. Yao", "Jee-Hwan Ryu", "Allison M. Okamura"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07221v1", "summary": "Robotic dressing assistance has the potential to improve the quality of life\nfor individuals with limited mobility. Existing solutions predominantly rely on\nrigid robotic manipulators, which have challenges in handling deformable\ngarments and ensuring safe physical interaction with the human body. Prior\nrobotic dressing methods require excessive operation times, complex control\nstrategies, and constrained user postures, limiting their practicality and\nadaptability. This paper proposes a novel soft robotic dressing system, the\nSelf-Wearing Adaptive Garment (SWAG), which uses an unfurling and growth\nmechanism to facilitate autonomous dressing. Unlike traditional approaches,the\nSWAG conforms to the human body through an unfurling based deployment method,\neliminating skin-garment friction and enabling a safer and more efficient\ndressing process. We present the working principles of the SWAG, introduce its\ndesign and fabrication, and demonstrate its performance in dressing assistance.\nThe proposed system demonstrates effective garment application across various\ngarment configurations, presenting a promising alternative to conventional\nrobotic dressing assistance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07221v1", "cate": "cs.RO", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07134", "title": "BOOST: Out-of-Distribution-Informed Adaptive Sampling for Bias Mitigation in Stylistic Convolutional Neural Networks", "authors": ["Mridula Vijendran", "Shuang Chen", "Jingjing Deng", "Hubert P. H. Shum"], "categories": ["cs.AI", "cs.LG", "I.2.10"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      18 pages, 7 figures, 3 tables", "url": "http://arxiv.org/abs/2507.07134v1", "summary": "The pervasive issue of bias in AI presents a significant challenge to\npainting classification, and is getting more serious as these systems become\nincreasingly integrated into tasks like art curation and restoration. Biases,\noften arising from imbalanced datasets where certain artistic styles dominate,\ncompromise the fairness and accuracy of model predictions, i.e., classifiers\nare less accurate on rarely seen paintings. While prior research has made\nstrides in improving classification performance, it has largely overlooked the\ncritical need to address these underlying biases, that is, when dealing with\nout-of-distribution (OOD) data. Our insight highlights the necessity of a more\nrobust approach to bias mitigation in AI models for art classification on\nbiased training data. We propose a novel OOD-informed model bias adaptive\nsampling method called BOOST (Bias-Oriented OOD Sampling and Tuning). It\naddresses these challenges by dynamically adjusting temperature scaling and\nsampling probabilities, thereby promoting a more equitable representation of\nall classes. We evaluate our proposed approach to the KaoKore and PACS\ndatasets, focusing on the model's ability to reduce class-wise bias. We further\npropose a new metric, Same-Dataset OOD Detection Score (SODC), designed to\nassess class-wise separation and per-class bias reduction. Our method\ndemonstrates the ability to balance high performance with fairness, making it a\nrobust solution for unbiasing AI models in the art domain.", "comment": "18 pages, 7 figures, 3 tables", "pdf_url": "http://arxiv.org/pdf/2507.07134v1", "cate": "cs.AI", "date": "2025-07-08", "updated": "2025-07-08"}
{"id": "2507.07139", "title": "Image Can Bring Your Memory Back: A Novel Multi-Modal Guided Attack against Image Generation Model Unlearning", "authors": ["Renyang Liu", "Guanlin Li", "Tianwei Zhang", "See-Kiong Ng"], "categories": ["cs.CV", "cs.CR", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07139v1", "summary": "Recent advances in image generation models (IGMs), particularly\ndiffusion-based architectures such as Stable Diffusion (SD), have markedly\nenhanced the quality and diversity of AI-generated visual content. However,\ntheir generative capability has also raised significant ethical, legal, and\nsocietal concerns, including the potential to produce harmful, misleading, or\ncopyright-infringing content. To mitigate these concerns, machine unlearning\n(MU) emerges as a promising solution by selectively removing undesirable\nconcepts from pretrained models. Nevertheless, the robustness and effectiveness\nof existing unlearning techniques remain largely unexplored, particularly in\nthe presence of multi-modal adversarial inputs.\n  To bridge this gap, we propose Recall, a novel adversarial framework\nexplicitly designed to compromise the robustness of unlearned IGMs. Unlike\nexisting approaches that predominantly rely on adversarial text prompts, Recall\nexploits the intrinsic multi-modal conditioning capabilities of diffusion\nmodels by efficiently optimizing adversarial image prompts with guidance from a\nsingle semantically relevant reference image. Extensive experiments across ten\nstate-of-the-art unlearning methods and diverse tasks show that Recall\nconsistently outperforms existing baselines in terms of adversarial\neffectiveness, computational efficiency, and semantic fidelity with the\noriginal textual prompt. These findings reveal critical vulnerabilities in\ncurrent unlearning mechanisms and underscore the need for more robust solutions\nto ensure the safety and reliability of generative models. Code and data are\npublicly available at \\textcolor{blue}{https://github.com/ryliu68/RECALL}.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07139v1", "cate": "cs.CV", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2504.21582", "title": "MF-LLM: Simulating Population Decision Dynamics via a Mean-Field Large Language Model Framework", "authors": ["Qirui Mi", "Mengyue Yang", "Xiangning Yu", "Zhiyu Zhao", "Cheng Deng", "Bo An", "Haifeng Zhang", "Xu Chen", "Jun Wang"], "categories": ["cs.MA", "cs.AI"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "Comments:      29 pages, 8 figures, 4 tables", "url": "http://arxiv.org/abs/2504.21582v3", "summary": "Simulating collective decision-making involves more than aggregating\nindividual behaviors; it emerges from dynamic interactions among individuals.\nWhile large language models (LLMs) offer strong potential for social\nsimulation, achieving quantitative alignment with real-world data remains a key\nchallenge. To bridge this gap, we propose the Mean-Field LLM (MF-LLM)\nframework, the first to incorporate mean field theory into LLM-based social\nsimulation. MF-LLM models bidirectional interactions between individuals and\nthe population through an iterative process, generating population signals to\nguide individual decisions, which in turn update the signals. This interplay\nproduces coherent trajectories of collective behavior. To improve alignment\nwith real-world data, we introduce IB-Tune, a novel fine-tuning method inspired\nby the Information Bottleneck principle, which retains population signals most\npredictive of future actions while filtering redundant history. Evaluated on a\nreal-world social dataset, MF-LLM reduces KL divergence to human population\ndistributions by 47\\% compared to non-mean-field baselines, enabling accurate\ntrend forecasting and effective intervention planning. Generalizing across 7\ndomains and 4 LLM backbones, MF-LLM provides a scalable, high-fidelity\nfoundation for social simulation.", "comment": "29 pages, 8 figures, 4 tables", "pdf_url": "http://arxiv.org/pdf/2504.21582v3", "cate": "cs.MA", "date": "2025-04-30", "updated": "2025-07-10"}
{"id": "2507.07225", "title": "3D Steering and Localization in Pipes and Burrows using an Externally Steered Soft Growing Robot", "authors": ["Yimeng Qin", "Jared Grinberg", "William Heap", "Allison M. Okamura"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07225v1", "summary": "Navigation and inspection in confined environments, such as tunnels and\npipes, pose significant challenges for existing robots due to limitations in\nmaneuverability and adaptability to varying geometries. Vine robots, which are\nsoft growing continuum robots that extend their length through soft material\neversion at their tip, offer unique advantages due to their ability to navigate\ntight spaces, adapt to complex paths, and minimize friction. However, existing\nvine robot designs struggle with navigation in manmade and natural passageways,\nwith branches and sharp 3D turns. In this letter, we introduce a steerable vine\nrobot specifically designed for pipe and burrow environments. The robot\nfeatures a simple tubular body and an external tip mount that steers the vine\nrobot in three degrees of freedom by changing the growth direction and, when\nnecessary, bracing against the wall of the pipe or burrow. Our external tip\nsteering approach enables: (1) active branch selection in 3D space with a\nmaximum steerable angle of 51.7{\\deg}, (2) navigation of pipe networks with\nradii as small as 2.5 cm, (3) a compliant tip enabling navigation of sharp\nturns, and (4) real-time 3D localization in GPS-denied environments using\ntip-mounted sensors and continuum body odometry. We describe the forward\nkinematics, characterize steerability, and demonstrate the system in a 3D pipe\nsystem as well as a natural animal burrow.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07225v1", "cate": "cs.RO", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07203", "title": "State-Inference-Based Prompting for Natural Language Trading with Game NPCs", "authors": ["Minkyung Kim", "Junsik Kim", "Hwidong Bae", "Woongcheol Yang", "Sangdon Park", "Sohee Bae"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      9 pages main content, 4 pages appendix, 3 figures. Accepted to the KDD 2025 Workshop on Prompt Optimization", "url": "http://arxiv.org/abs/2507.07203v1", "summary": "Large Language Models enable dynamic game interactions but struggle with\nrule-governed trading systems. Current implementations suffer from rule\nviolations, such as item hallucinations and calculation errors, that erode\nplayer trust. Here, State-Inference-Based Prompting (SIBP) enables reliable\ntrading through autonomous dialogue state inference and context-specific rule\nadherence. The approach decomposes trading into six states within a unified\nprompt framework, implementing context-aware item referencing and\nplaceholder-based price calculations. Evaluation across 100 trading dialogues\ndemonstrates >97% state compliance, >95% referencing accuracy, and 99.7%\ncalculation precision. SIBP maintains computational efficiency while\noutperforming baseline approaches, establishing a practical foundation for\ntrustworthy NPC interactions in commercial games.", "comment": "9 pages main content, 4 pages appendix, 3 figures. Accepted to the\n  KDD 2025 Workshop on Prompt Optimization", "pdf_url": "http://arxiv.org/pdf/2507.07203v1", "cate": "cs.AI", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07341", "title": "On the Impossibility of Separating Intelligence from Judgment: The Computational Intractability of Filtering for AI Alignment", "authors": ["Sarah Ball", "Greg Gluch", "Shafi Goldwasser", "Frauke Kreuter", "Omer Reingold", "Guy N. Rothblum"], "categories": ["cs.AI", "cs.CR"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07341v1", "summary": "With the increased deployment of large language models (LLMs), one concern is\ntheir potential misuse for generating harmful content. Our work studies the\nalignment challenge, with a focus on filters to prevent the generation of\nunsafe information. Two natural points of intervention are the filtering of the\ninput prompt before it reaches the model, and filtering the output after\ngeneration. Our main results demonstrate computational challenges in filtering\nboth prompts and outputs. First, we show that there exist LLMs for which there\nare no efficient prompt filters: adversarial prompts that elicit harmful\nbehavior can be easily constructed, which are computationally indistinguishable\nfrom benign prompts for any efficient filter. Our second main result identifies\na natural setting in which output filtering is computationally intractable. All\nof our separation results are under cryptographic hardness assumptions. In\naddition to these core findings, we also formalize and study relaxed mitigation\napproaches, demonstrating further computational barriers. We conclude that\nsafety cannot be achieved by designing filters external to the LLM internals\n(architecture and weights); in particular, black-box access to the LLM will not\nsuffice. Based on our technical results, we argue that an aligned AI system's\nintelligence cannot be separated from its judgment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07341v1", "cate": "cs.AI", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2506.03053", "title": "MAEBE: Multi-Agent Emergent Behavior Framework", "authors": ["Sinem Erisken", "Timothy Gothard", "Martin Leitgab", "Ram Potham"], "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "Comments:      Preprint. This work has been submitted to the Multi-Agent Systems Workshop at ICML 2025 for review", "url": "http://arxiv.org/abs/2506.03053v2", "summary": "Traditional AI safety evaluations on isolated LLMs are insufficient as\nmulti-agent AI ensembles become prevalent, introducing novel emergent risks.\nThis paper introduces the Multi-Agent Emergent Behavior Evaluation (MAEBE)\nframework to systematically assess such risks. Using MAEBE with the Greatest\nGood Benchmark (and a novel double-inversion question technique), we\ndemonstrate that: (1) LLM moral preferences, particularly for Instrumental\nHarm, are surprisingly brittle and shift significantly with question framing,\nboth in single agents and ensembles. (2) The moral reasoning of LLM ensembles\nis not directly predictable from isolated agent behavior due to emergent group\ndynamics. (3) Specifically, ensembles exhibit phenomena like peer pressure\ninfluencing convergence, even when guided by a supervisor, highlighting\ndistinct safety and alignment challenges. Our findings underscore the necessity\nof evaluating AI systems in their interactive, multi-agent contexts.", "comment": "Preprint. This work has been submitted to the Multi-Agent Systems\n  Workshop at ICML 2025 for review", "pdf_url": "http://arxiv.org/pdf/2506.03053v2", "cate": "cs.MA", "date": "2025-06-03", "updated": "2025-07-10"}
{"id": "2507.07299", "title": "LangNavBench: Evaluation of Natural Language Understanding in Semantic Navigation", "authors": ["Sonia Raychaudhuri", "Enrico Cancelli", "Tommaso Campari", "Lamberto Ballan", "Manolis Savva", "Angel X. Chang"], "categories": ["cs.RO", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07299v1", "summary": "Recent progress in large vision-language models has driven improvements in\nlanguage-based semantic navigation, where an embodied agent must reach a target\nobject described in natural language. Despite these advances, we still lack a\nclear, language-focused benchmark for testing how well such agents ground the\nwords in their instructions. We address this gap with LangNav, an open-set\ndataset specifically created to test an agent's ability to locate objects\ndescribed at different levels of detail, from broad category names to fine\nattributes and object-object relations. Every description in LangNav was\nmanually checked, yielding a lower error rate than existing lifelong- and\nsemantic-navigation datasets. On top of LangNav we build LangNavBench, a\nbenchmark that measures how well current semantic-navigation methods understand\nand act on these descriptions while moving toward their targets. LangNavBench\nallows us to systematically compare models on their handling of attributes,\nspatial and relational cues, and category hierarchies, offering the first\nthorough, language-centric evaluation of embodied navigation systems. We also\npresent Multi-Layered Feature Map (MLFM), a method that builds a queryable\nmulti-layered semantic map, particularly effective when dealing with small\nobjects or instructions involving spatial relations. MLFM outperforms\nstate-of-the-art mapping-based navigation baselines on the LangNav dataset.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07299v1", "cate": "cs.RO", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07217", "title": "Neurosymbolic Feature Extraction for Identifying Forced Labor in Supply Chains", "authors": ["Zili Wang", "Frank Montabon", "Kristin Yvonne Rozier"], "categories": ["cs.AI", "cs.LG", "cs.LO", "I.2.4; I.2.7; J.4"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07217v1", "summary": "Supply chain networks are complex systems that are challenging to analyze;\nthis problem is exacerbated when there are illicit activities involved in the\nsupply chain, such as counterfeit parts, forced labor, or human trafficking.\nWhile machine learning (ML) can find patterns in complex systems like supply\nchains, traditional ML techniques require large training data sets. However,\nillicit supply chains are characterized by very sparse data, and the data that\nis available is often (purposely) corrupted or unreliable in order to hide the\nnature of the activities. We need to be able to automatically detect new\npatterns that correlate with such illegal activity over complex, even temporal\ndata, without requiring large training data sets. We explore neurosymbolic\nmethods for identifying instances of illicit activity in supply chains and\ncompare the effectiveness of manual and automated feature extraction from news\narticles accurately describing illicit activities uncovered by authorities. We\npropose a question tree approach for querying a large language model (LLM) to\nidentify and quantify the relevance of articles. This enables a systematic\nevaluation of the differences between human and machine classification of news\narticles related to forced labor in supply chains.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07217v1", "cate": "cs.AI", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07483", "title": "Temporal Unlearnable Examples: Preventing Personal Video Data from Unauthorized Exploitation by Object Tracking", "authors": ["Qiangqiang Wu", "Yi Yu", "Chenqi Kong", "Ziquan Liu", "Jia Wan", "Haoliang Li", "Alex C. Kot", "Antoni B. Chan"], "categories": ["cs.CV", "cs.CR"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2507.07483v1", "summary": "With the rise of social media, vast amounts of user-uploaded videos (e.g.,\nYouTube) are utilized as training data for Visual Object Tracking (VOT).\nHowever, the VOT community has largely overlooked video data-privacy issues, as\nmany private videos have been collected and used for training commercial models\nwithout authorization. To alleviate these issues, this paper presents the first\ninvestigation on preventing personal video data from unauthorized exploitation\nby deep trackers. Existing methods for preventing unauthorized data use\nprimarily focus on image-based tasks (e.g., image classification), directly\napplying them to videos reveals several limitations, including inefficiency,\nlimited effectiveness, and poor generalizability. To address these issues, we\npropose a novel generative framework for generating Temporal Unlearnable\nExamples (TUEs), and whose efficient computation makes it scalable for usage on\nlarge-scale video datasets. The trackers trained w/ TUEs heavily rely on\nunlearnable noises for temporal matching, ignoring the original data structure\nand thus ensuring training video data-privacy. To enhance the effectiveness of\nTUEs, we introduce a temporal contrastive loss, which further corrupts the\nlearning of existing trackers when using our TUEs for training. Extensive\nexperiments demonstrate that our approach achieves state-of-the-art performance\nin video data-privacy protection, with strong transferability across VOT\nmodels, datasets, and temporal matching tasks.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.07483v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2409.18047", "title": "HARMONIC: Cognitive and Control Collaboration in Human-Robotic Teams", "authors": ["Sanjay Oruganti", "Sergei Nirenburg", "Marjorie McShane", "Jesse English", "Michael K. Roberts", "Christian Arndt", "Sahithi Kamireddy", "Carlos Gonzalez", "Mingyo Seo", "Luis Sentis"], "categories": ["cs.RO", "cs.AI", "cs.MA"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.18047v3", "summary": "This paper describes HARMONIC, a cognitive-robotic architecture that\nintegrates the OntoAgent cognitive framework with general-purpose robot control\nsystems applied to human-robot teaming (HRT). HARMONIC incorporates\nmetacognition, meaningful natural language communication, and explainability\ncapabilities required for developing mutual trust in HRT. Through simulation\nexperiments involving a joint search task performed by a heterogeneous team of\ntwo HARMONIC-based robots and a human operator, we demonstrate heterogeneous\nrobots that coordinate their actions, adapt to complex scenarios, and engage in\nnatural human-robot communication. Evaluation results show that HARMONIC-based\nrobots can reason about plans, goals, and team member attitudes while providing\nclear explanations for their decisions, which are essential requirements for\nrealistic human-robot teaming.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.18047v3", "cate": "cs.RO", "date": "2024-09-26", "updated": "2025-07-09"}
{"id": "2507.07315", "title": "Classifying Emergence in Robot Swarms: An Observer-Dependent Approach", "authors": ["Ricardo Vega", "Cameron Nowzari"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      25 pages, 3 tables, 8 figures", "url": "http://arxiv.org/abs/2507.07315v1", "summary": "Emergence and swarms are widely discussed topics, yet no consensus exists on\ntheir formal definitions. This lack of agreement makes it difficult not only\nfor new researchers to grasp these concepts, but also for experts who may use\nthe same terms to mean different things. Many attempts have been made to\nobjectively define 'swarm' or 'emergence,' with recent work highlighting the\nrole of the external observer. Still, several researchers argue that once an\nobserver's vantage point (e.g., scope, resolution, context) is established, the\nterms can be made objective or measured quantitatively. In this note, we\npropose a framework to discuss these ideas rigorously by separating externally\nobservable states from latent, unobservable ones. This allows us to compare and\ncontrast existing definitions of swarms and emergence on common ground. We\nargue that these concepts are ultimately subjective-shaped less by the system\nitself than by the perception and tacit knowledge of the observer.\nSpecifically, we suggest that a 'swarm' is not defined by its group behavior\nalone, but by the process generating that behavior. Our broader goal is to\nsupport the design and deployment of robotic swarm systems, highlighting the\ncritical distinction between multi-robot systems and true swarms.", "comment": "25 pages, 3 tables, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.07315v1", "cate": "cs.RO", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07302", "title": "Application of LLMs to Multi-Robot Path Planning and Task Allocation", "authors": ["Ashish Kumar"], "categories": ["cs.AI", "cs.RO"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07302v1", "summary": "Efficient exploration is a well known problem in deep reinforcement learning\nand this problem is exacerbated in multi-agent reinforcement learning due the\nintrinsic complexities of such algorithms. There are several approaches to\nefficiently explore an environment to learn to solve tasks by multi-agent\noperating in that environment, of which, the idea of expert exploration is\ninvestigated in this work. More specifically, this work investigates the\napplication of large-language models as expert planners for efficient\nexploration in planning based tasks for multiple agents.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07302v1", "cate": "cs.AI", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07735", "title": "GuardVal: Dynamic Large Language Model Jailbreak Evaluation for Comprehensive Safety Testing", "authors": ["Peiyan Zhang", "Haibo Jin", "Liying Kang", "Haohan Wang"], "categories": ["cs.LG", "cs.CL", "cs.CR", "I.2.7; I.2.8"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      24 pages", "url": "http://arxiv.org/abs/2507.07735v1", "summary": "Jailbreak attacks reveal critical vulnerabilities in Large Language Models\n(LLMs) by causing them to generate harmful or unethical content. Evaluating\nthese threats is particularly challenging due to the evolving nature of LLMs\nand the sophistication required in effectively probing their vulnerabilities.\nCurrent benchmarks and evaluation methods struggle to fully address these\nchallenges, leaving gaps in the assessment of LLM vulnerabilities. In this\npaper, we review existing jailbreak evaluation practices and identify three\nassumed desiderata for an effective jailbreak evaluation protocol. To address\nthese challenges, we introduce GuardVal, a new evaluation protocol that\ndynamically generates and refines jailbreak prompts based on the defender LLM's\nstate, providing a more accurate assessment of defender LLMs' capacity to\nhandle safety-critical situations. Moreover, we propose a new optimization\nmethod that prevents stagnation during prompt refinement, ensuring the\ngeneration of increasingly effective jailbreak prompts that expose deeper\nweaknesses in the defender LLMs. We apply this protocol to a diverse set of\nmodels, from Mistral-7b to GPT-4, across 10 safety domains. Our findings\nhighlight distinct behavioral patterns among the models, offering a\ncomprehensive view of their robustness. Furthermore, our evaluation process\ndeepens the understanding of LLM behavior, leading to insights that can inform\nfuture research and drive the development of more secure models.", "comment": "24 pages", "pdf_url": "http://arxiv.org/pdf/2507.07735v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2501.02770", "title": "Multi-Agent Pathfinding Under Team-Connected Communication Constraint via Adaptive Path Expansion and Dynamic Leading", "authors": ["Hoang-Dung Bui", "Erion Plaku", "Gregoy J. Stein"], "categories": ["cs.AI", "cs.MA", "cs.RO"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.02770v4", "summary": "This paper proposes a novel planning framework to handle a multi-agent\npathfinding problem under team-connected communication constraint, where all\nagents must have a connected communication channel to the rest of the team\nduring their entire movements. Standard multi-agent path finding approaches\n(e.g., priority-based search) have potential in this domain but fail when\nneighboring configurations at start and goal differ. Their single-expansion\napproach -- computing each agent's path from the start to the goal in just a\nsingle expansion -- cannot reliably handle planning under communication\nconstraints for agents as their neighbors change during navigating. Similarly,\nleader-follower approaches (e.g., platooning) are effective at maintaining team\ncommunication, but fixing the leader at the outset of planning can cause\nplanning to become stuck in dense-clutter environments, limiting their\npractical utility. To overcome this limitation, we propose a novel two-level\nmulti-agent pathfinding framework that integrates two techniques: adaptive path\nexpansion to expand agent paths to their goals in multiple stages; and dynamic\nleading technique that enables the reselection of the leading agent during each\nagent path expansion whenever progress cannot be made. Simulation experiments\nshow the efficiency of our planners, which can handle up to 25 agents across\nfive environment types under a limited communication range constraint and up to\n11-12 agents on three environment types under line-of-sight communication\nconstraint, exceeding 90% success-rate where baselines routinely fail.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.02770v4", "cate": "cs.AI", "date": "2025-01-06", "updated": "2025-07-10"}
{"id": "2507.07327", "title": "Effects of Wrist-Worn Haptic Feedback on Force Accuracy and Task Speed during a Teleoperated Robotic Surgery Task", "authors": ["Brian B. Vuong", "Josie Davidson", "Sangheui Cheon", "Kyujin Cho", "Allison M. Okamura"], "categories": ["cs.RO", "cs.HC"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      This work has been submitted to the IEEE for possible publication", "url": "http://arxiv.org/abs/2507.07327v1", "summary": "Previous work has shown that the addition of haptic feedback to the hands can\nimprove awareness of tool-tissue interactions and enhance performance of\nteleoperated tasks in robot-assisted minimally invasive surgery. However,\nhand-based haptic feedback occludes direct interaction with the manipulanda of\nsurgeon console in teleoperated surgical robots. We propose relocating haptic\nfeedback to the wrist using a wearable haptic device so that haptic feedback\nmechanisms do not need to be integrated into the manipulanda. However, it is\nunknown if such feedback will be effective, given that it is not co-located\nwith the finger movements used for manipulation. To test if relocated haptic\nfeedback improves force application during teleoperated tasks using da Vinci\nResearch Kit (dVRK) surgical robot, participants learned to palpate a phantom\ntissue to desired forces. A soft pneumatic wrist-worn haptic device with an\nanchoring system renders tool-tissue interaction forces to the wrist of the\nuser. Participants performed the palpation task with and without wrist-worn\nhaptic feedback and were evaluated for the accuracy of applied forces.\nParticipants demonstrated statistically significant lower force error when\nwrist-worn haptic feedback was provided. Participants also performed the\npalpation task with longer movement times when provided wrist-worn haptic\nfeedback, indicating that the haptic feedback may have caused participants to\noperate at a different point in the speed-accuracy tradeoff curve.", "comment": "This work has been submitted to the IEEE for possible publication", "pdf_url": "http://arxiv.org/pdf/2507.07327v1", "cate": "cs.RO", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07306", "title": "ViDove: A Translation Agent System with Multimodal Context and Memory-Augmented Reasoning", "authors": ["Yichen Lu", "Wei Dai", "Jiaen Liu", "Ching Wing Kwok", "Zongheng Wu", "Xudong Xiao", "Ao Sun", "Sheng Fu", "Jianyuan Zhan", "Yian Wang", "Takatomo Saito", "Sicheng Lai"], "categories": ["cs.AI", "cs.CL", "eess.AS"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07306v1", "summary": "LLM-based translation agents have achieved highly human-like translation\nresults and are capable of handling longer and more complex contexts with\ngreater efficiency. However, they are typically limited to text-only inputs. In\nthis paper, we introduce ViDove, a translation agent system designed for\nmultimodal input. Inspired by the workflow of human translators, ViDove\nleverages visual and contextual background information to enhance the\ntranslation process. Additionally, we integrate a multimodal memory system and\nlong-short term memory modules enriched with domain-specific knowledge,\nenabling the agent to perform more accurately and adaptively in real-world\nscenarios. As a result, ViDove achieves significantly higher translation\nquality in both subtitle generation and general translation tasks, with a 28%\nimprovement in BLEU scores and a 15% improvement in SubER compared to previous\nstate-of-the-art baselines. Moreover, we introduce DoveBench, a new benchmark\nfor long-form automatic video subtitling and translation, featuring 17 hours of\nhigh-quality, human-annotated data. Our code is available here:\nhttps://github.com/pigeonai-org/ViDove", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07306v1", "cate": "cs.AI", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2409.08476", "title": "Research on Data Right Confirmation Mechanism of Federated Learning based on Blockchain", "authors": ["Xiaogang Cheng", "Ren Guo"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      in Chinese language", "url": "http://arxiv.org/abs/2409.08476v2", "summary": "Federated learning can solve the privacy protection problem in distributed\ndata mining and machine learning, and how to protect the ownership, use and\nincome rights of all parties involved in federated learning is an important\nissue. This paper proposes a federated learning data ownership confirmation\nmechanism based on blockchain and smart contract, which uses decentralized\nblockchain technology to save the contribution of each participant on the\nblockchain, and distributes the benefits of federated learning results through\nthe blockchain. In the local simulation environment of the blockchain, the\nrelevant smart contracts and data structures are simulated and implemented, and\nthe feasibility of the scheme is preliminarily demonstrated.", "comment": "in Chinese language", "pdf_url": "http://arxiv.org/pdf/2409.08476v2", "cate": "cs.CR", "date": "2024-09-13", "updated": "2025-07-10"}
{"id": "2507.07356", "title": "UniTracker: Learning Universal Whole-Body Motion Tracker for Humanoid Robots", "authors": ["Kangning Yin", "Weishuai Zeng", "Ke Fan", "Zirui Wang", "Qiang Zhang", "Zheng Tian", "Jingbo Wang", "Jiangmiao Pang", "Weinan Zhang"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      10 pages, 5 figures", "url": "http://arxiv.org/abs/2507.07356v1", "summary": "Humanoid robots must achieve diverse, robust, and generalizable whole-body\ncontrol to operate effectively in complex, human-centric environments. However,\nexisting methods, particularly those based on teacher-student frameworks often\nsuffer from a loss of motion diversity during policy distillation and exhibit\nlimited generalization to unseen behaviors. In this work, we present\nUniTracker, a simplified yet powerful framework that integrates a Conditional\nVariational Autoencoder (CVAE) into the student policy to explicitly model the\nlatent diversity of human motion. By leveraging a learned CVAE prior, our\nmethod enables the student to retain expressive motion characteristics while\nimproving robustness and adaptability under partial observations. The result is\na single policy capable of tracking a wide spectrum of whole-body motions with\nhigh fidelity and stability. Comprehensive experiments in both simulation and\nreal-world deployments demonstrate that UniTracker significantly outperforms\nMLP-based DAgger baselines in motion quality, generalization to unseen\nreferences, and deployment robustness, offering a practical and scalable\nsolution for expressive humanoid control.", "comment": "10 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.07356v1", "cate": "cs.RO", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07355", "title": "Supply Chain Optimization via Generative Simulation and Iterative Decision Policies", "authors": ["Haoyue Bai", "Haoyu Wang", "Nanxu Gong", "Xinyuan Wang", "Wangyang Ying", "Haifeng Chen", "Yanjie Fu"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07355v1", "summary": "High responsiveness and economic efficiency are critical objectives in supply\nchain transportation, both of which are influenced by strategic decisions on\nshipping mode. An integrated framework combining an efficient simulator with an\nintelligent decision-making algorithm can provide an observable, low-risk\nenvironment for transportation strategy design. An ideal simulation-decision\nframework must (1) generalize effectively across various settings, (2) reflect\nfine-grained transportation dynamics, (3) integrate historical experience with\npredictive insights, and (4) maintain tight integration between simulation\nfeedback and policy refinement. We propose Sim-to-Dec framework to satisfy\nthese requirements. Specifically, Sim-to-Dec consists of a generative\nsimulation module, which leverages autoregressive modeling to simulate\ncontinuous state changes, reducing dependence on handcrafted domain-specific\nrules and enhancing robustness against data fluctuations; and a history-future\ndual-aware decision model, refined iteratively through end-to-end optimization\nwith simulator interactions. Extensive experiments conducted on three\nreal-world datasets demonstrate that Sim-to-Dec significantly improves timely\ndelivery rates and profit.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07355v1", "cate": "cs.AI", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07129", "title": "Growing Transformers: Modular Composition and Layer-wise Expansion on a Frozen Substrate", "authors": ["A. Bochkov"], "categories": ["cs.LG", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07129v1", "summary": "The prevailing paradigm for scaling large language models (LLMs) involves\nmonolithic, end-to-end training, a resource-intensive process that lacks\nflexibility. This paper explores an alternative, constructive approach to model\ndevelopment, built upon the foundation of non-trainable, deterministic input\nembeddings. In prior [1], we established that high-level semantic reasoning can\nemerge in Transformers using frozen embeddings derived from the visual\nstructure of Unicode glyphs. Here, we demonstrate that this fixed\nrepresentational substrate acts as a universal \"docking port,\" enabling two\npowerful and efficient scaling paradigms: seamless modular composition and\nprogressive layer-wise growth.\n  First, we show that specialist models trained on disparate datasets (e.g.,\nRussian and Chinese text) can be merged into a single, more capable\nMixture-of-Experts (MoE) model, post-training, with zero architectural\nmodification. This is achieved by simply averaging their output logits. The\nresulting MoE model exhibits immediate performance improvements on reasoning\nbenchmarks like MMLU, surpassing its constituent experts without catastrophic\nforgetting. Second, we introduce a layer-wise constructive training\nmethodology, where a deep Transformer is \"grown\" by progressively stacking and\ntraining one layer at a time. This method demonstrates stable convergence and a\nclear correlation between model depth and the emergence of complex reasoning\nabilities, such as those required for SQuAD.\n  Our findings suggest a paradigm shift from monolithic optimization towards a\nmore biological or constructive model of AI development, where complexity is\nbuilt incrementally and modules can be composed freely. This opens new avenues\nfor resource-efficient scaling, continual learning, and a more democratized\necosystem for building powerful AI systems. We release all code and models to\nfacilitate further research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07129v1", "cate": "cs.LG", "date": "2025-07-08", "updated": "2025-07-08"}
{"id": "2410.07414", "title": "Bayes-Nash Generative Privacy Against Membership Inference Attacks", "authors": ["Tao Zhang", "Rajagopal Venkatesaramani", "Rajat K. De", "Bradley A. Malin", "Yevgeniy Vorobeychik"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      arXiv admin note: substantial text overlap with arXiv:2406.01811", "url": "http://arxiv.org/abs/2410.07414v5", "summary": "Membership inference attacks (MIAs) pose significant privacy risks by\ndetermining whether individual data is in a dataset. While differential privacy\n(DP) mitigates these risks, it has limitations including limited resolution in\nexpressing privacy-utility tradeoffs and intractable sensitivity calculations\nfor tight guarantees. We propose a game-theoretic framework modeling privacy\nprotection as a Bayesian game between defender and attacker, where privacy loss\ncorresponds to the attacker's membership inference ability. To address\nstrategic complexity, we represent the defender's mixed strategy as a neural\nnetwork generator mapping private datasets to public representations (e.g.,\nnoisy statistics) and the attacker's strategy as a discriminator making\nmembership claims. This \\textit{general-sum Generative Adversarial Network}\ntrains iteratively through alternating updates, yielding \\textit{Bayes-Nash\nGenerative Privacy (BNGP)} strategies. BNGP avoids worst-case privacy proofs\nsuch as sensitivity calculations, supports correlated mechanism compositions,\nhandles heterogeneous attacker preferences. Empirical studies on sensitive\ndataset summary statistics show our approach significantly outperforms\nstate-of-the-art methods by generating stronger attacks and achieving better\nprivacy-utility tradeoffs.", "comment": "arXiv admin note: substantial text overlap with arXiv:2406.01811", "pdf_url": "http://arxiv.org/pdf/2410.07414v5", "cate": "cs.CR", "date": "2024-10-09", "updated": "2025-07-10"}
{"id": "2507.07370", "title": "Data-driven Kinematic Modeling in Soft Robots: System Identification and Uncertainty Quantification", "authors": ["Zhanhong Jiang", "Dylan Shah", "Hsin-Jung Yang", "Soumik Sarkar"], "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      6 pages; 6 figures; accepted at the 5th Modeling, Estimation and Control Conference (MECC 2025)", "url": "http://arxiv.org/abs/2507.07370v1", "summary": "Precise kinematic modeling is critical in calibration and controller design\nfor soft robots, yet remains a challenging issue due to their highly nonlinear\nand complex behaviors. To tackle the issue, numerous data-driven machine\nlearning approaches have been proposed for modeling nonlinear dynamics.\nHowever, these models suffer from prediction uncertainty that can negatively\naffect modeling accuracy, and uncertainty quantification for kinematic modeling\nin soft robots is underexplored. In this work, using limited simulation and\nreal-world data, we first investigate multiple linear and nonlinear machine\nlearning models commonly used for kinematic modeling of soft robots. The\nresults reveal that nonlinear ensemble methods exhibit the most robust\ngeneralization performance. We then develop a conformal kinematic modeling\nframework for soft robots by utilizing split conformal prediction to quantify\npredictive position uncertainty, ensuring distribution-free prediction\nintervals with a theoretical guarantee.", "comment": "6 pages; 6 figures; accepted at the 5th Modeling, Estimation and\n  Control Conference (MECC 2025)", "pdf_url": "http://arxiv.org/pdf/2507.07370v1", "cate": "cs.RO", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07426", "title": "DrugMCTS: a drug repurposing framework combining multi-agent, RAG and Monte Carlo Tree Search", "authors": ["Zerui Yang", "Yuwei Wan", "Yinqiao Li", "Yudai Matsuda", "Tong Xie", "Linqi Song"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07426v1", "summary": "Recent advances in large language models have demonstrated considerable\npotential in scientific domains such as drug discovery. However, their\neffectiveness remains constrained when reasoning extends beyond the knowledge\nacquired during pretraining. Conventional approaches, such as fine-tuning or\nretrieval-augmented generation, face limitations in either imposing high\ncomputational overhead or failing to fully exploit structured scientific data.\nTo overcome these challenges, we propose DrugMCTS, a novel framework that\nsynergistically integrates RAG, multi-agent collaboration, and Monte Carlo Tree\nSearch for drug repurposing. The framework employs five specialized agents\ntasked with retrieving and analyzing molecular and protein information, thereby\nenabling structured and iterative reasoning. Without requiring domain-specific\nfine-tuning, DrugMCTS empowers Qwen2.5-7B-Instruct to outperform Deepseek-R1 by\nover 20\\%. Extensive experiments on the DrugBank and KIBA datasets demonstrate\nthat DrugMCTS achieves substantially higher recall and robustness compared to\nboth general-purpose LLMs and deep learning baselines. Our results highlight\nthe importance of structured reasoning, agent-based collaboration, and\nfeedback-driven search mechanisms in advancing LLM applications for drug\ndiscovery.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07426v1", "cate": "cs.AI", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07135", "title": "FACap: A Large-scale Fashion Dataset for Fine-grained Composed Image Retrieval", "authors": ["Fran√ßois Gard√®res", "Shizhe Chen", "Camille-Sovanneary Gauthier", "Jean Ponce"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07135v1", "summary": "The composed image retrieval (CIR) task is to retrieve target images given a\nreference image and a modification text. Recent methods for CIR leverage large\npretrained vision-language models (VLMs) and achieve good performance on\ngeneral-domain concepts like color and texture. However, they still struggle\nwith application domains like fashion, because the rich and diverse vocabulary\nused in fashion requires specific fine-grained vision and language\nunderstanding. An additional difficulty is the lack of large-scale fashion\ndatasets with detailed and relevant annotations, due to the expensive cost of\nmanual annotation by specialists. To address these challenges, we introduce\nFACap, a large-scale, automatically constructed fashion-domain CIR dataset. It\nleverages web-sourced fashion images and a two-stage annotation pipeline\npowered by a VLM and a large language model (LLM) to generate accurate and\ndetailed modification texts. Then, we propose a new CIR model FashionBLIP-2,\nwhich fine-tunes the general-domain BLIP-2 model on FACap with lightweight\nadapters and multi-head query-candidate matching to better account for\nfine-grained fashion-specific information. FashionBLIP-2 is evaluated with and\nwithout additional fine-tuning on the Fashion IQ benchmark and the enhanced\nevaluation dataset enhFashionIQ, leveraging our pipeline to obtain\nhigher-quality annotations. Experimental results show that the combination of\nFashionBLIP-2 and pretraining with FACap significantly improves the model's\nperformance in fashion CIR especially for retrieval with fine-grained\nmodification texts, demonstrating the value of our dataset and approach in a\nhighly demanding environment such as e-commerce websites. Code is available at\nhttps://fgxaos.github.io/facap-paper-website/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07135v1", "cate": "cs.LG", "date": "2025-07-08", "updated": "2025-07-08"}
{"id": "2507.07238", "title": "Dirty Data in the Newsroom: Comparing Data Preparation in Journalism and Data Science", "authors": ["Stephen Kasica", "Charles Berret", "Tamara Munzner"], "categories": ["cs.HC", "cs.CY", "A.0"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      18 pages, 3 figures, Published in proceedings of the 2023 CHI Conference on Human Factors in Computing Systems", "url": "http://arxiv.org/abs/2507.07238v1", "summary": "The work involved in gathering, wrangling, cleaning, and otherwise preparing\ndata for analysis is often the most time consuming and tedious aspect of data\nwork. Although many studies describe data preparation within the context of\ndata science workflows, there has been little research on data preparation in\ndata journalism. We address this gap with a hybrid form of thematic analysis\nthat combines deductive codes derived from existing accounts of data science\nworkflows and inductive codes arising from an interview study with 36\nprofessional data journalists. We extend a previous model of data science work\nto incorporate detailed activities of data preparation. We synthesize 60 dirty\ndata issues from 16 taxonomies on dirty data and our interview data, and we\nprovide a novel taxonomy to characterize these dirty data issues as\ndiscrepancies between mental models. We also identify four challenges faced by\njournalists: diachronic, regional, fragmented, and disparate data sources.", "comment": "18 pages, 3 figures, Published in proceedings of the 2023 CHI\n  Conference on Human Factors in Computing Systems", "pdf_url": "http://arxiv.org/pdf/2507.07238v1", "cate": "cs.HC", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2502.15281", "title": "DITING: A Static Analyzer for Identifying Bad Partitioning Issues in TEE Applications", "authors": ["Chengyan Ma", "Ruidong Han", "Jieke Shi", "Ye Liu", "Yuqing Niu", "Di Lu", "Chuang Tian", "Jianfeng Ma", "Debin Gao", "David Lo"], "categories": ["cs.CR", "cs.SE"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.15281v2", "summary": "Trusted Execution Environment (TEE) enhances the security of mobile\napplications and cloud services by isolating sensitive code in the secure world\nfrom the non-secure normal world. However, TEE applications are still\nconfronted with vulnerabilities stemming from bad partitioning. Bad\npartitioning can lead to critical security problems of TEE, such as leaking\nsensitive data to the normal world or being adversely affected by malicious\ninputs from the normal world.\n  To address this, we propose an approach to detect partitioning issues in TEE\napplications. First, we conducted a survey of TEE vulnerabilities caused by bad\npartitioning and found that the parameters exchanged between the secure and\nnormal worlds often contain insecure usage with bad partitioning\nimplementation. Second, we developed a tool named DITING that can analyze\ndata-flows of these parameters and identify their violations of security rules\nwe defined to find bad partitioning issues. Different from existing research\nthat only focuses on malicious input to TEE, we assess the partitioning issues\nmore comprehensively through input/output and shared memory. Finally, we\ncreated the first benchmark targeting bad partitioning, consisting of 110 test\ncases. Experiments demonstrate that DITING achieves an F1 score of 0.90 in\nidentifying bad partitioning issues.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.15281v2", "cate": "cs.CR", "date": "2025-02-21", "updated": "2025-07-10"}
{"id": "2507.07376", "title": "PILOC: A Pheromone Inverse Guidance Mechanism and Local-Communication Framework for Dynamic Target Search of Multi-Agent in Unknown Environments", "authors": ["Hengrui Liu", "Yi Feng", "Qilong Zhang"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07376v1", "summary": "Multi-Agent Search and Rescue (MASAR) plays a vital role in disaster\nresponse, exploration, and reconnaissance. However, dynamic and unknown\nenvironments pose significant challenges due to target unpredictability and\nenvironmental uncertainty. To tackle these issues, we propose PILOC, a\nframework that operates without global prior knowledge, leveraging local\nperception and communication. It introduces a pheromone inverse guidance\nmechanism to enable efficient coordination and dynamic target localization.\nPILOC promotes decentralized cooperation through local communication,\nsignificantly reducing reliance on global channels. Unlike conventional\nheuristics, the pheromone mechanism is embedded into the observation space of\nDeep Reinforcement Learning (DRL), supporting indirect agent coordination based\non environmental cues. We further integrate this strategy into a DRL-based\nmulti-agent architecture and conduct extensive experiments. Results show that\ncombining local communication with pheromone-based guidance significantly\nboosts search efficiency, adaptability, and system robustness. Compared to\nexisting methods, PILOC performs better under dynamic and\ncommunication-constrained scenarios, offering promising directions for future\nMASAR applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07376v1", "cate": "cs.RO", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07445", "title": "StarDojo: Benchmarking Open-Ended Behaviors of Agentic Multimodal LLMs in Production-Living Simulations with Stardew Valley", "authors": ["Weihao Tan", "Changjiu Jiang", "Yu Duan", "Mingcong Lei", "Jiageng Li", "Yitian Hong", "Xinrun Wang", "Bo An"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Project website: this https URL", "url": "http://arxiv.org/abs/2507.07445v1", "summary": "Autonomous agents navigating human society must master both production\nactivities and social interactions, yet existing benchmarks rarely evaluate\nthese skills simultaneously. To bridge this gap, we introduce StarDojo, a novel\nbenchmark based on Stardew Valley, designed to assess AI agents in open-ended\nproduction-living simulations. In StarDojo, agents are tasked to perform\nessential livelihood activities such as farming and crafting, while\nsimultaneously engaging in social interactions to establish relationships\nwithin a vibrant community. StarDojo features 1,000 meticulously curated tasks\nacross five key domains: farming, crafting, exploration, combat, and social\ninteractions. Additionally, we provide a compact subset of 100 representative\ntasks for efficient model evaluation. The benchmark offers a unified,\nuser-friendly interface that eliminates the need for keyboard and mouse\ncontrol, supports all major operating systems, and enables the parallel\nexecution of multiple environment instances, making it particularly well-suited\nfor evaluating the most capable foundation agents, powered by multimodal large\nlanguage models (MLLMs). Extensive evaluations of state-of-the-art MLLMs agents\ndemonstrate substantial limitations, with the best-performing model, GPT-4.1,\nachieving only a 12.7% success rate, primarily due to challenges in visual\nunderstanding, multimodal reasoning and low-level manipulation. As a\nuser-friendly environment and benchmark, StarDojo aims to facilitate further\nresearch towards robust, open-ended agents in complex production-living\nenvironments.", "comment": "Project website: https://weihaotan.github.io/StarDojo", "pdf_url": "http://arxiv.org/pdf/2507.07445v1", "cate": "cs.AI", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07137", "title": "Automating Evaluation of Diffusion Model Unlearning with (Vision-) Language Model World Knowledge", "authors": ["Eric Yeats", "Darryl Hannan", "Henry Kvinge", "Timothy Doster", "Scott Mahan"], "categories": ["cs.LG", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07137v1", "summary": "Machine unlearning (MU) is a promising cost-effective method to cleanse\nundesired information (generated concepts, biases, or patterns) from\nfoundational diffusion models. While MU is orders of magnitude less costly than\nretraining a diffusion model without the undesired information, it can be\nchallenging and labor-intensive to prove that the information has been fully\nremoved from the model. Moreover, MU can damage diffusion model performance on\nsurrounding concepts that one would like to retain, making it unclear if the\ndiffusion model is still fit for deployment. We introduce autoeval-dmun, an\nautomated tool which leverages (vision-) language models to thoroughly assess\nunlearning in diffusion models. Given a target concept, autoeval-dmun extracts\nstructured, relevant world knowledge from the language model to identify nearby\nconcepts which are likely damaged by unlearning and to circumvent unlearning\nwith adversarial prompts. We use our automated tool to evaluate popular\ndiffusion model unlearning methods, revealing that language models (1) impose\nsemantic orderings of nearby concepts which correlate well with unlearning\ndamage and (2) effectively circumvent unlearning with synthetic adversarial\nprompts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07137v1", "cate": "cs.LG", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07362", "title": "FLoRA: An Advanced AI-Powered Engine to Facilitate Hybrid Human-AI Regulated Learning", "authors": ["Xinyu Li", "Tongguang Li", "Lixiang Yan", "Yuheng Li", "Linxuan Zhao", "Mladen Rakoviƒá", "Inge Molenaar", "Dragan Ga≈°eviƒá", "Yizhou Fan"], "categories": ["cs.HC", "cs.CY"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07362v1", "summary": "SRL, defined as learners' ability to systematically plan, monitor, and\nregulate their learning activities, is crucial for sustained academic\nachievement and lifelong learning competencies. Emerging Artificial\nIntelligence (AI) developments profoundly influence SRL interactions by\npotentially either diminishing or strengthening learners' opportunities to\nexercise their own regulatory skills. Recent literature emphasizes a balanced\napproach termed Hybrid Human-AI Regulated Learning (HHAIRL), in which AI\nprovides targeted, timely scaffolding while preserving the learners' role as\nactive decision-makers and reflective monitors of their learning process.\nNevertheless, existing digital tools frequently fall short, lacking\nadaptability, focusing narrowly on isolated SRL phases, and insufficiently\nsupport meaningful human-AI interactions. In response, this paper introduces\nthe enhanced \\flora Engine, which incorporates advanced Generative Artificial\nIntelligence (GenAI) features and state-of-the-art learning analytics,\nexplicitly grounded in SRL and HHAIRL theories. The \\flora Engine offers\ninstrumentation tools such as collaborative writing, multi-agents chatbot, and\ndetailed learning trace logging to support dynamic, adaptive scaffolding\ntailored to individual needs in real time. We further present a summary of\nseveral research studies that provide the validations for and illustrate how\nthese instrumentation tools can be utilized in real-world educational and\nexperimental contexts. These studies demonstrate the effectiveness of \\flora\nEngine in fostering SRL and HHAIRL, providing both theoretical insights and\npractical solutions for the future of AI-enhanced learning context.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07362v1", "cate": "cs.HC", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07342", "title": "Discrete Beamforming Optimization for RISs with a Limited Phase Range and Amplitude Attenuation", "authors": ["Dogan Kutay Pekcan", "Hongyi Liao", "Ender Ayanoglu"], "categories": ["eess.SP", "cs.ET"], "primary_category": "Subjects:       Emerging Technologies (cs.ET)", "pdf_link": null, "comments": "Comments:      13 pages, 17 figures, 2 tables", "url": "http://arxiv.org/abs/2507.07342v1", "summary": "This paper addresses the problem of maximizing the received power at a user\nequipment via reconfigurable intelligent surface (RIS) characterized by\nphase-dependent amplitude (PDA) and discrete phase shifts over a limited phase\nrange. Given complex RIS coefficients, that is, discrete phase shifts and PDAs,\nwe derive the necessary and sufficient conditions to achieve the optimal\nsolution. To this end, we propose an optimal search algorithm that is proven to\nconverge in linear time within at most NK steps, significantly outperforming\nthe exhaustive search approach that would otherwise be needed for RISs with\namplitude attenuation. Furthermore, we introduce a practical quantization\nframework for PDA-introduced RISs termed amplitude-introduced polar\nquantization (APQ), and extend it to a novel algorithm named extended\namplitude-introduced polar quantization (EAPQ) that works with geometric\nprojections. We derive closed-form expressions to assess how closely the\nperformance of the proposed RIS configuration can approximate the ideal case\nwith continuous phases and no attenuation. Our analysis reveals that increasing\nthe number of discrete phases beyond K = 4 yields only marginal gains,\nregardless of attenuation levels, provided the RIS has a sufficiently wide\nphase range R. Furthermore, we also show and quantify that when the phase range\nR is limited, the performance is sensitive to attenuation for larger R, and\nsensitive to R when there is less attenuation. Finally, the proposed optimal\nalgorithm provides a generic upper bound that could serve as a benchmark for\ndiscrete beamforming in RISs with amplitude constraints.", "comment": "13 pages, 17 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2507.07342v1", "cate": "eess.SP", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2505.15216", "title": "BountyBench: Dollar Impact of AI Agent Attackers and Defenders on Real-World Cybersecurity Systems", "authors": ["Andy K. Zhang", "Joey Ji", "Celeste Menders", "Riya Dulepet", "Thomas Qin", "Ron Y. Wang", "Junrong Wu", "Kyleen Liao", "Jiliang Li", "Jinghan Hu", "Sara Hong", "Nardos Demilew", "Shivatmica Murgai", "Jason Tran", "Nishka Kacheria", "Ethan Ho", "Denis Liu", "Lauren McLane", "Olivia Bruvik", "Dai-Rong Han", "Seungwoo Kim", "Akhil Vyas", "Cuiyuanxiu Chen", "Ryan Li", "Weiran Xu", "Jonathan Z. Ye", "Prerit Choudhary", "Siddharth M. Bhatia", "Vikram Sivashankar", "Yuxuan Bao", "Dawn Song", "Dan Boneh", "Daniel E. Ho", "Percy Liang"], "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      93 pages", "url": "http://arxiv.org/abs/2505.15216v2", "summary": "AI agents have the potential to significantly alter the cybersecurity\nlandscape. Here, we introduce the first framework to capture offensive and\ndefensive cyber-capabilities in evolving real-world systems. Instantiating this\nframework with BountyBench, we set up 25 systems with complex, real-world\ncodebases. To capture the vulnerability lifecycle, we define three task types:\nDetect (detecting a new vulnerability), Exploit (exploiting a specific\nvulnerability), and Patch (patching a specific vulnerability). For Detect, we\nconstruct a new success indicator, which is general across vulnerability types\nand provides localized evaluation. We manually set up the environment for each\nsystem, including installing packages, setting up server(s), and hydrating\ndatabase(s). We add 40 bug bounties, which are vulnerabilities with monetary\nawards of \\$10-\\$30,485, covering 9 of the OWASP Top 10 Risks. To modulate task\ndifficulty, we devise a new strategy based on information to guide detection,\ninterpolating from identifying a zero day to exploiting a specific\nvulnerability. We evaluate 8 agents: Claude Code, OpenAI Codex CLI with o3-high\nand o4-mini, and custom agents with o3-high, GPT-4.1, Gemini 2.5 Pro Preview,\nClaude 3.7 Sonnet Thinking, and DeepSeek-R1. Given up to three attempts, the\ntop-performing agents are OpenAI Codex CLI: o3-high (12.5% on Detect, mapping\nto \\$3,720; 90% on Patch, mapping to \\$14,152), Custom Agent with Claude 3.7\nSonnet Thinking (67.5% on Exploit), and OpenAI Codex CLI: o4-mini (90% on\nPatch, mapping to \\$14,422). OpenAI Codex CLI: o3-high, OpenAI Codex CLI:\no4-mini, and Claude Code are more capable at defense, achieving higher Patch\nscores of 90%, 90%, and 87.5%, compared to Exploit scores of 47.5%, 32.5%, and\n57.5% respectively; while the custom agents are relatively balanced between\noffense and defense, achieving Exploit scores of 37.5-67.5% and Patch scores of\n35-60%.", "comment": "93 pages", "pdf_url": "http://arxiv.org/pdf/2505.15216v2", "cate": "cs.CR", "date": "2025-05-21", "updated": "2025-07-10"}
{"id": "2507.07444", "title": "Towards Safe Autonomous Driving: A Real-Time Safeguarding Concept for Motion Planning Algorithms", "authors": ["Korbinian Moller", "Rafael Neher", "Marvin Seegert", "Johannes Betz"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      7 pages, submitted to the IEEE ICVES 2025, Coventry, UK", "url": "http://arxiv.org/abs/2507.07444v1", "summary": "Ensuring the functional safety of motion planning modules in autonomous\nvehicles remains a critical challenge, especially when dealing with complex or\nlearning-based software. Online verification has emerged as a promising\napproach to monitor such systems at runtime, yet its integration into embedded\nreal-time environments remains limited. This work presents a safeguarding\nconcept for motion planning that extends prior approaches by introducing a time\nsafeguard. While existing methods focus on geometric and dynamic feasibility,\nour approach additionally monitors the temporal consistency of planning outputs\nto ensure timely system response. A prototypical implementation on a real-time\noperating system evaluates trajectory candidates using constraint-based\nfeasibility checks and cost-based plausibility metrics. Preliminary results\nshow that the safeguarding module operates within real-time bounds and\neffectively detects unsafe trajectories. However, the full integration of the\ntime safeguard logic and fallback strategies is ongoing. This study contributes\na modular and extensible framework for runtime trajectory verification and\nhighlights key aspects for deployment on automotive-grade hardware. Future work\nincludes completing the safeguarding logic and validating its effectiveness\nthrough hardware-in-the-loop simulations and vehicle-based testing. The code is\navailable at: https://github.com/TUM-AVS/motion-planning-supervisor", "comment": "7 pages, submitted to the IEEE ICVES 2025, Coventry, UK", "pdf_url": "http://arxiv.org/pdf/2507.07444v1", "cate": "cs.RO", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07544", "title": "Position: We Need An Algorithmic Understanding of Generative AI", "authors": ["Oliver Eberle", "Thomas McGee", "Hamza Giaffar", "Taylor Webb", "Ida Momennejad"], "categories": ["cs.AI", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Accepted at ICML 2025 as a Spotlight Position Paper", "url": "http://arxiv.org/abs/2507.07544v1", "summary": "What algorithms do LLMs actually learn and use to solve problems? Studies\naddressing this question are sparse, as research priorities are focused on\nimproving performance through scale, leaving a theoretical and empirical gap in\nunderstanding emergent algorithms. This position paper proposes AlgEval: a\nframework for systematic research into the algorithms that LLMs learn and use.\nAlgEval aims to uncover algorithmic primitives, reflected in latent\nrepresentations, attention, and inference-time compute, and their algorithmic\ncomposition to solve task-specific problems. We highlight potential\nmethodological paths and a case study toward this goal, focusing on emergent\nsearch algorithms. Our case study illustrates both the formation of top-down\nhypotheses about candidate algorithms, and bottom-up tests of these hypotheses\nvia circuit-level analysis of attention patterns and hidden states. The\nrigorous, systematic evaluation of how LLMs actually solve tasks provides an\nalternative to resource-intensive scaling, reorienting the field toward a\nprincipled understanding of underlying computations. Such algorithmic\nexplanations offer a pathway to human-understandable interpretability, enabling\ncomprehension of the model's internal reasoning performance measures. This can\nin turn lead to more sample-efficient methods for training and improving\nperformance, as well as novel architectures for end-to-end and multi-agent\nsystems.", "comment": "Accepted at ICML 2025 as a Spotlight Position Paper", "pdf_url": "http://arxiv.org/pdf/2507.07544v1", "cate": "cs.AI", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07138", "title": "GNNs Meet Sequence Models Along the Shortest-Path: an Expressive Method for Link Prediction", "authors": ["Francesco Ferrini", "Veronica Lachi", "Antonio Longa", "Bruno Lepri", "Andrea Passerini"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07138v1", "summary": "Graph Neural Networks (GNNs) often struggle to capture the link-specific\nstructural patterns crucial for accurate link prediction, as their node-centric\nmessage-passing schemes overlook the subgraph structures connecting a pair of\nnodes. Existing methods to inject such structural context either incur high\ncomputational cost or rely on simplistic heuristics (e.g., common neighbor\ncounts) that fail to model multi-hop dependencies. We introduce SP4LP (Shortest\nPath for Link Prediction), a novel framework that combines GNN-based node\nencodings with sequence modeling over shortest paths. Specifically, SP4LP first\napplies a GNN to compute representations for all nodes, then extracts the\nshortest path between each candidate node pair and processes the resulting\nsequence of node embeddings using a sequence model. This design enables SP4LP\nto capture expressive multi-hop relational patterns with computational\nefficiency. Empirically, SP4LP achieves state-of-the-art performance across\nlink prediction benchmarks. Theoretically, we prove that SP4LP is strictly more\nexpressive than standard message-passing GNNs and several state-of-the-art\nstructural features methods, establishing it as a general and principled\napproach for link prediction in graphs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07138v1", "cate": "cs.LG", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07550", "title": "Pluri-perspectivism in Human-robot Co-creativity with Older Adults", "authors": ["Marianne Bossema", "Rob Saunders", "Aske Plaat", "Somaya Ben Allouch"], "categories": ["cs.HC", "cs.RO"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07550v1", "summary": "This position paper explores pluriperspectivism as a core element of human\ncreative experience and its relevance to humanrobot cocreativity We propose a\nlayered fivedimensional model to guide the design of cocreative behaviors and\nthe analysis of interaction dynamics This model is based on literature and\nresults from an interview study we conducted with 10 visual artists and 8 arts\neducators examining how pluriperspectivism supports creative practice The\nfindings of this study provide insight in how robots could enhance human\ncreativity through adaptive contextsensitive behavior demonstrating the\npotential of pluriperspectivism This paper outlines future directions for\nintegrating pluriperspectivism with visionlanguage models VLMs to support\ncontext sensitivity in cocreative robots", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07550v1", "cate": "cs.HC", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07116", "title": "Analysing semantic data storage in Distributed Ledger Technologies for Data Spaces", "authors": ["Juan Cano-Benito", "Andrea Cimmino", "Sven Hertling", "Heiko Paulheim", "Ra√∫l Garc√≠a-Castro"], "categories": ["cs.DC", "cs.AI", "cs.ET"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07116v1", "summary": "Data spaces are emerging as decentralised infrastructures that enable\nsovereign, secure, and trustworthy data exchange among multiple participants.\nTo achieve semantic interoperability within these environments, the use of\nsemantic web technologies and knowledge graphs has been proposed. Although\ndistributed ledger technologies (DLT) fit as the underlying infrastructure for\ndata spaces, there remains a significant gap in terms of the efficient storage\nof semantic data on these platforms. This paper presents a systematic\nevaluation of semantic data storage across different types of DLT (public,\nprivate, and hybrid), using a real-world knowledge graph as an experimental\nbasis. The study compares performance, storage efficiency, resource\nconsumption, and the capabilities to update and query semantic data. The\nresults show that private DLTs are the most efficient for storing and managing\nsemantic content, while hybrid DLTs offer a balanced trade-off between public\nauditability and operational efficiency. This research leads to a discussion on\nthe selection of the most appropriate DLT infrastructure based on the data\nsovereignty requirements of decentralised data ecosystems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07116v1", "cate": "cs.DC", "date": "2025-07-03", "updated": "2025-07-03"}
{"id": "2506.01220", "title": "Vulnerability Management Chaining: An Integrated Framework for Efficient Cybersecurity Risk Prioritization", "authors": ["Naoyuki Shimizu", "Masaki Hashimoto"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      16 pages, 3 figures", "url": "http://arxiv.org/abs/2506.01220v3", "summary": "As the number of Common Vulnerabilities and Exposures (CVE) continues to grow\nexponentially, security teams face increasingly difficult decisions about\nprioritization. Current approaches using Common Vulnerability Scoring System\n(CVSS) scores produce overwhelming volumes of high-priority vulnerabilities,\nwhile Exploit Prediction Scoring System (EPSS) and Known Exploited\nVulnerabilities (KEV) catalog offer valuable but incomplete perspectives on\nactual exploitation risk. We present Vulnerability Management Chaining, a\ndecision tree framework that systematically integrates these three approaches\nto achieve efficient vulnerability prioritization. Our framework employs a\ntwo-stage evaluation process: first applying threat-based filtering using KEV\nmembership or EPSS threshold $\\geq$ 0.088), then applying vulnerability\nseverity assessment using CVSS scores $\\geq$ 7.0) to enable informed\ndeprioritization. Experimental validation using 28,377 real-world\nvulnerabilities and vendor-reported exploitation data demonstrates 18-fold\nefficiency improvements while maintaining 85.6\\% coverage. Organizations can\nreduce urgent remediation workload by approximately 95\\%. The integration\nidentifies 48 additional exploited vulnerabilities that neither KEV nor EPSS\ncaptures individually. Our framework uses exclusively open-source data,\nenabling immediate adoption regardless of organizational resources.", "comment": "16 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2506.01220v3", "cate": "cs.CR", "date": "2025-06-02", "updated": "2025-07-10"}
{"id": "2507.07467", "title": "SCREP: Scene Coordinate Regression and Evidential Learning-based Perception-Aware Trajectory Generation", "authors": ["Juyeop Han", "Lukas Lao Beyer", "Guilherme V. Cavalheiro", "Sertac Karaman"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 pages, 7 figures, 3 tables", "url": "http://arxiv.org/abs/2507.07467v1", "summary": "Autonomous flight in GPS denied indoor spaces requires trajectories that keep\nvisual localization error tightly bounded across varied missions. Whereas\nvisual inertial odometry (VIO) accumulates drift over time, scene coordinate\nregression (SCR) yields drift-free, high accuracy absolute pose estimation. We\npresent a perception-aware framework that couples an evidential learning-based\nSCR pose estimator with a receding horizon trajectory optimizer. The optimizer\nsteers the onboard camera toward pixels whose uncertainty predicts reliable\nscene coordinates, while a fixed-lag smoother fuses the low rate SCR stream\nwith high rate IMU data to close the perception control loop in real time. In\nsimulation, our planner reduces translation (rotation) mean error by 54% / 15%\n(40% / 31%) relative to yaw fixed and forward-looking baselines, respectively.\nMoreover, hardware in the loop experiment validates the feasibility of our\nproposed framework.", "comment": "8 pages, 7 figures, 3 tables", "pdf_url": "http://arxiv.org/pdf/2507.07467v1", "cate": "cs.RO", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07576", "title": "On Trustworthy Rule-Based Models and Explanations", "authors": ["Mohamed Siala", "Jordi Planes", "Joao Marques-Silva"], "categories": ["cs.AI", "cs.LG", "cs.LO"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07576v1", "summary": "A task of interest in machine learning (ML) is that of ascribing explanations\nto the predictions made by ML models. Furthermore, in domains deemed high risk,\nthe rigor of explanations is paramount. Indeed, incorrect explanations can and\nwill mislead human decision makers. As a result, and even if interpretability\nis acknowledged as an elusive concept, so-called interpretable models are\nemployed ubiquitously in high-risk uses of ML and data mining (DM). This is the\ncase for rule-based ML models, which encompass decision trees, diagrams, sets\nand lists. This paper relates explanations with well-known undesired facets of\nrule-based ML models, which include negative overlap and several forms of\nredundancy. The paper develops algorithms for the analysis of these undesired\nfacets of rule-based systems, and concludes that well-known and widely used\ntools for learning rule-based ML models will induce rule sets that exhibit one\nor more negative facets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07576v1", "cate": "cs.AI", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07140", "title": "Exploring Sparse Adapters for Scalable Merging of Parameter Efficient Experts", "authors": ["Samin Yeasar Arnob", "Zhan Su", "Minseon Kim", "Oleksiy Ostapenko", "Riyasat Ohib", "Esra'a Saleh", "Doina Precup", "Lucas Caccia", "Alessandro Sordoni"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07140v1", "summary": "Merging parameter-efficient task experts has recently gained growing\nattention as a way to build modular architectures that can be rapidly adapted\non the fly for specific downstream tasks, without requiring additional\nfine-tuning. Typically, LoRA serves as the foundational building block of such\nparameter-efficient modular architectures, leveraging low-rank weight\nstructures to reduce the number of trainable parameters. In this paper, we\nstudy the properties of sparse adapters, which train only a subset of weights\nin the base neural network, as potential building blocks of modular\narchitectures. First, we propose a simple method for training highly effective\nsparse adapters, which is conceptually simpler than existing methods in the\nliterature and surprisingly outperforms both LoRA and full fine-tuning in our\nsetting. Next, we investigate the merging properties of these sparse adapters\nby merging adapters for up to 20 natural language processing tasks, thus\nscaling beyond what is usually studied in the literature. Our findings\ndemonstrate that sparse adapters yield superior in-distribution performance\npost-merging compared to LoRA or full model merging. Achieving strong held-out\nperformance remains a challenge for all methods considered.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07140v1", "cate": "cs.LG", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07551", "title": "ArchiveGPT: A human-centered evaluation of using a vision language model for image cataloguing", "authors": ["Line Abele", "Gerrit Anders", "Tolgahan Aydƒ±n", "J√ºrgen Buder", "Helen Fischer", "Dominik Kimmel", "Markus Huff"], "categories": ["cs.HC", "cs.AI", "cs.DL"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      56 pages, 7 figures", "url": "http://arxiv.org/abs/2507.07551v1", "summary": "The accelerating growth of photographic collections has outpaced manual\ncataloguing, motivating the use of vision language models (VLMs) to automate\nmetadata generation. This study examines whether Al-generated catalogue\ndescriptions can approximate human-written quality and how generative Al might\nintegrate into cataloguing workflows in archival and museum collections. A VLM\n(InternVL2) generated catalogue descriptions for photographic prints on\nlabelled cardboard mounts with archaeological content, evaluated by archive and\narchaeology experts and non-experts in a human-centered, experimental\nframework. Participants classified descriptions as AI-generated or\nexpert-written, rated quality, and reported willingness to use and trust in AI\ntools. Classification performance was above chance level, with both groups\nunderestimating their ability to detect Al-generated descriptions. OCR errors\nand hallucinations limited perceived quality, yet descriptions rated higher in\naccuracy and usefulness were harder to classify, suggesting that human review\nis necessary to ensure the accuracy and quality of catalogue descriptions\ngenerated by the out-of-the-box model, particularly in specialized domains like\narchaeological cataloguing. Experts showed lower willingness to adopt AI tools,\nemphasizing concerns on preservation responsibility over technical performance.\nThese findings advocate for a collaborative approach where AI supports draft\ngeneration but remains subordinate to human verification, ensuring alignment\nwith curatorial values (e.g., provenance, transparency). The successful\nintegration of this approach depends not only on technical advancements, such\nas domain-specific fine-tuning, but even more on establishing trust among\nprofessionals, which could both be fostered through a transparent and\nexplainable AI pipeline.", "comment": "56 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.07551v1", "cate": "cs.HC", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07597", "title": "Quantum Executor: A Unified Interface for Quantum Computing", "authors": ["Giuseppe Bisicchia", "Alessandro Bocci", "Antonio Brogi"], "categories": ["quant-ph", "cs.ET", "cs.SE"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      11 pages, 1 figure", "url": "http://arxiv.org/abs/2507.07597v1", "summary": "As quantum computing evolves from theoretical promise to practical\ndeployment, the demand for robust, portable, and scalable tools for quantum\nsoftware experimentation is growing. This paper introduces Quantum Executor, a\nbackend-agnostic execution engine designed to orchestrate quantum experiments\nacross heterogeneous platforms. Quantum Executor provides a declarative and\nmodular interface that decouples experiment design from backend execution,\nenabling seamless interoperability and code reuse across diverse quantum and\nclassical resources. Key features include support for asynchronous and\ndistributed execution, customizable execution strategies and a unified API for\nmanaging quantum experiments. We illustrate its applicability through two\nlife-like usage scenarios such as automated benchmarking and hybrid validation,\ndiscussing its capacity to streamline quantum development. We conclude by\ndiscussing current limitations and outlining a roadmap for future enhancements.", "comment": "11 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2507.07597v1", "cate": "quant-ph", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.06850", "title": "The Dark Side of LLMs: Agent-based Attacks for Complete Computer Takeover", "authors": ["Matteo Lupinacci", "Francesco Aurelio Pironti", "Francesco Blefari", "Francesco Romeo", "Luigi Arena", "Angelo Furfaro"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.06850v2", "summary": "The rapid adoption of Large Language Model (LLM) agents and multi-agent\nsystems enables unprecedented capabilities in natural language processing and\ngeneration. However, these systems have introduced unprecedented security\nvulnerabilities that extend beyond traditional prompt injection attacks. This\npaper presents the first comprehensive evaluation of LLM agents as attack\nvectors capable of achieving complete computer takeover through the\nexploitation of trust boundaries within agentic AI systems where autonomous\nentities interact and influence each other. We demonstrate that adversaries can\nleverage three distinct attack surfaces - direct prompt injection, RAG backdoor\nattacks, and inter-agent trust exploitation - to coerce popular LLMs (including\nGPT-4o, Claude-4 and Gemini-2.5) into autonomously installing and executing\nmalware on victim machines. Our evaluation of 17 state-of-the-art LLMs reveals\nan alarming vulnerability hierarchy: while 41.2% of models succumb to direct\nprompt injection, 52.9% are vulnerable to RAG backdoor attacks, and a critical\n82.4% can be compromised through inter-agent trust exploitation. Notably, we\ndiscovered that LLMs which successfully resist direct malicious commands will\nexecute identical payloads when requested by peer agents, revealing a\nfundamental flaw in current multi-agent security models. Our findings\ndemonstrate that only 5.9% of tested models (1/17) proved resistant to all\nattack vectors, with the majority exhibiting context-dependent security\nbehaviors that create exploitable blind spots. Our findings also highlight the\nneed to increase awareness and research on the security risks of LLMs, showing\na paradigm shift in cybersecurity threats, where AI tools themselves become\nsophisticated attack vectors.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.06850v2", "cate": "cs.CR", "date": "2025-07-09", "updated": "2025-07-10"}
{"id": "2507.07661", "title": "FiDTouch: A 3D Wearable Haptic Display for the Finger Pad", "authors": ["Daria Trinitatova", "Dzmitry Tsetserukou"], "categories": ["cs.RO", "cs.HC"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted to the IEEE World Haptics Conference 2025 (IEEE WHC 2025), 7 pages, 8 figures, 3 tables", "url": "http://arxiv.org/abs/2507.07661v1", "summary": "The applications of fingertip haptic devices have spread to various fields\nfrom revolutionizing virtual reality and medical training simulations to\nfacilitating remote robotic operations, proposing great potential for enhancing\nuser experiences, improving training outcomes, and new forms of interaction. In\nthis work, we present FiDTouch, a 3D wearable haptic device that delivers\ncutaneous stimuli to the finger pad, such as contact, pressure, encounter, skin\nstretch, and vibrotactile feedback. The application of a tiny inverted Delta\nrobot in the mechanism design allows providing accurate contact and fast\nchanging dynamic stimuli to the finger pad surface. The performance of the\ndeveloped display was evaluated in a two-stage user study of the perception of\nstatic spatial contact stimuli and skin stretch stimuli generated on the finger\npad. The proposed display, by providing users with precise touch and force\nstimuli, can enhance user immersion and efficiency in the fields of\nhuman-computer and human-robot interactions.", "comment": "Accepted to the IEEE World Haptics Conference 2025 (IEEE WHC 2025), 7\n  pages, 8 figures, 3 tables", "pdf_url": "http://arxiv.org/pdf/2507.07661v1", "cate": "cs.RO", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07595", "title": "Context Pooling: Query-specific Graph Pooling for Generic Inductive Link Prediction in Knowledge Graphs", "authors": ["Zhixiang Su", "Di Wang", "Chunyan Miao"], "categories": ["cs.AI", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07595v1", "summary": "Recent investigations on the effectiveness of Graph Neural Network\n(GNN)-based models for link prediction in Knowledge Graphs (KGs) show that\nvanilla aggregation does not significantly impact the model performance. In\nthis paper, we introduce a novel method, named Context Pooling, to enhance\nGNN-based models' efficacy for link predictions in KGs. To our best of\nknowledge, Context Pooling is the first methodology that applies graph pooling\nin KGs. Additionally, Context Pooling is first-of-its-kind to enable the\ngeneration of query-specific graphs for inductive settings, where testing\nentities are unseen during training. Specifically, we devise two metrics,\nnamely neighborhood precision and neighborhood recall, to assess the neighbors'\nlogical relevance regarding the given queries, thereby enabling the subsequent\ncomprehensive identification of only the logically relevant neighbors for link\nprediction. Our method is generic and assessed by being applied to two\nstate-of-the-art (SOTA) models on three public transductive and inductive\ndatasets, achieving SOTA performance in 42 out of 48 settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07595v1", "cate": "cs.AI", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07141", "title": "Str-GCL: Structural Commonsense Driven Graph Contrastive Learning", "authors": ["Dongxiao He", "Yongqi Huang", "Jitao Zhao", "Xiaobao Wang", "Zhen Wang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted by WWW 2025", "url": "http://arxiv.org/abs/2507.07141v1", "summary": "Graph Contrastive Learning (GCL) is a widely adopted approach in\nself-supervised graph representation learning, applying contrastive objectives\nto produce effective representations. However, current GCL methods primarily\nfocus on capturing implicit semantic relationships, often overlooking the\nstructural commonsense embedded within the graph's structure and attributes,\nwhich contains underlying knowledge crucial for effective representation\nlearning. Due to the lack of explicit information and clear guidance in general\ngraph, identifying and integrating such structural commonsense in GCL poses a\nsignificant challenge. To address this gap, we propose a novel framework called\nStructural Commonsense Unveiling in Graph Contrastive Learning (Str-GCL).\nStr-GCL leverages first-order logic rules to represent structural commonsense\nand explicitly integrates them into the GCL framework. It introduces\ntopological and attribute-based rules without altering the original graph and\nemploys a representation alignment mechanism to guide the encoder in\neffectively capturing this commonsense. To the best of our knowledge, this is\nthe first attempt to directly incorporate structural commonsense into GCL.\nExtensive experiments demonstrate that Str-GCL outperforms existing GCL\nmethods, providing a new perspective on leveraging structural commonsense in\ngraph representation learning.", "comment": "Accepted by WWW 2025", "pdf_url": "http://arxiv.org/pdf/2507.07141v1", "cate": "cs.LG", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07930", "title": "Probing Experts' Perspectives on AI-Assisted Public Speaking Training", "authors": ["Nesrine Fourati", "Alisa Barkar", "Marion Drag√©e", "Liv Danthon-Lefebvre", "Mathieu Chollet"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07930v1", "summary": "Background: Public speaking is a vital professional skill, yet it remains a\nsource of significant anxiety for many individuals. Traditional training relies\nheavily on expert coaching, but recent advances in AI has led to novel types of\ncommercial automated public speaking feedback tools. However, most research has\nfocused on prototypes rather than commercial applications, and little is known\nabout how public speaking experts perceive these tools.\n  Objectives: This study aims to evaluate expert opinions on the efficacy and\ndesign of commercial AI-based public speaking training tools and to propose\nguidelines for their improvement.\n  Methods: The research involved 16 semi-structured interviews and 2 focus\ngroups with public speaking experts. Participants discussed their views on\ncurrent commercial tools, their potential integration into traditional\ncoaching, and suggestions for enhancing these systems.\n  Results and Conclusions: Experts acknowledged the value of AI tools in\nhandling repetitive, technical aspects of training, allowing coaches to focus\non higher-level skills. However they found key issues in current tools,\nemphasising the need for personalised, understandable, carefully selected\nfeedback and clear instructional design. Overall, they supported a hybrid model\ncombining traditional coaching with AI-supported exercises.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07930v1", "cate": "cs.HC", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.06156", "title": "Hedge Funds on a Swamp: Analyzing Patterns, Vulnerabilities, and Defense Measures in Blockchain Bridges [Experiment, Analysis & Benchmark]", "authors": ["Poupak Azad", "Jiahua Xu", "Yebo Feng", "Preston Strowbridge", "Cuneyt Akcora"], "categories": ["cs.ET", "cs.CR"], "primary_category": "Subjects:       Emerging Technologies (cs.ET)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.06156v2", "summary": "Blockchain bridges have become essential infrastructure for enabling\ninteroperability across different blockchain networks, with more than $24B\nmonthly bridge transaction volume. However, their growing adoption has been\naccompanied by a disproportionate rise in security breaches, making them the\nsingle largest source of financial loss in Web3. For cross-chain ecosystems to\nbe robust and sustainable, it is essential to understand and address these\nvulnerabilities. In this study, we present a comprehensive systematization of\nblockchain bridge design and security. We define three bridge security priors,\nformalize the architectural structure of 13 prominent bridges, and identify 23\nattack vectors grounded in real-world blockchain exploits. Using this\nfoundation, we evaluate 43 representative attack scenarios and introduce a\nlayered threat model that captures security failures across source chain,\noff-chain, and destination chain components.\n  Our analysis at the static code and transaction network levels reveals\nrecurring design flaws, particularly in access control, validator trust\nassumptions, and verification logic, and identifies key patterns in adversarial\nbehavior based on transaction-level traces. To support future development, we\npropose a decision framework for bridge architecture design, along with defense\nmechanisms such as layered validation and circuit breakers. This work provides\na data-driven foundation for evaluating bridge security and lays the groundwork\nfor standardizing resilient cross-chain infrastructure.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.06156v2", "cate": "cs.ET", "date": "2025-07-08", "updated": "2025-07-10"}
{"id": "2507.07325", "title": "A German Gold-Standard Dataset for Sentiment Analysis in Software Engineering", "authors": ["Martin Obaidi", "Marc Herrmann", "Elisa Schmid", "Raymond Ochsner", "Kurt Schneider", "Jil Kl√ºnder"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      This paper has been accepted at the 33rd IEEE International Requirements Engineering Workshop (REW 2025)", "url": "http://arxiv.org/abs/2507.07325v1", "summary": "Sentiment analysis is an essential technique for investigating the emotional\nclimate within developer teams, contributing to both team productivity and\nproject success. Existing sentiment analysis tools in software engineering\nprimarily rely on English or non-German gold-standard datasets. To address this\ngap, our work introduces a German dataset of 5,949 unique developer statements,\nextracted from the German developer forum Android-Hilfe.de. Each statement was\nannotated with one of six basic emotions, based on the emotion model by Shaver\net al., by four German-speaking computer science students. Evaluation of the\nannotation process showed high interrater agreement and reliability. These\nresults indicate that the dataset is sufficiently valid and robust to support\nsentiment analysis in the German-speaking software engineering community.\nEvaluation with existing German sentiment analysis tools confirms the lack of\ndomain-specific solutions for software engineering. We also discuss approaches\nto optimize annotation and present further use cases for the dataset.", "comment": "This paper has been accepted at the 33rd IEEE International\n  Requirements Engineering Workshop (REW 2025)", "pdf_url": "http://arxiv.org/pdf/2507.07325v1", "cate": "cs.SE", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2305.13651", "title": "Adversarial Defenses via Vector Quantization", "authors": ["Zhiyi Dong", "Yongyi Mao"], "categories": ["cs.LG", "cs.CR", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      This is the author-accepted version of our paper published in Neurocomputing. The final published version is available at: this https URL", "url": "http://arxiv.org/abs/2305.13651v2", "summary": "Adversarial attacks pose significant challenges to the robustness of modern\ndeep neural networks in computer vision, and defending these networks against\nadversarial attacks has attracted intense research efforts. Among various\ndefense strategies, preprocessing-based defenses are practically appealing\nsince there is no need to train the network under protection. However, such\napproaches typically do not achieve comparable robustness as other methods such\nas adversarial training. In this paper, we propose a novel framework for\npreprocessing-based defenses, where a vector quantizer is used as a\npreprocessor. This framework, inspired by and extended from Randomized\nDiscretization (RandDisc), is theoretically principled by rate-distortion\ntheory: indeed, RandDisc may be viewed as a scalar quantizer, and\nrate-distortion theory suggests that such quantization schemes are inferior to\nvector quantization. In our framework, the preprocessing vector quantizer\ntreats the input image as a collection of patches and finds a set of\nrepresentative patches based on the patch distributions; each original patch is\nthen modified according to the representative patches close to it. We present\ntwo lightweight defenses in this framework, referred to as patched RandDisc\n(pRD) and sliding-window RandDisc (swRD), where the patches are disjoint in the\nformer and overlapping in the latter. We show that vector-quantization-based\ndefenses have certifiable robust accuracy and that pRD and swRD demonstrate\nstate-of-the-art performances, surpassing RandDisc by a large margin. Notably,\nthe proposed defenses possess the obfuscated gradients property. Our\nexperiments however show that pRD and swRD remain effective under the STE and\nEOT attacks, which are designed specifically for defenses with gradient\nobfuscation. ...", "comment": "This is the author-accepted version of our paper published in\n  Neurocomputing. The final published version is available at:\n  https://doi.org/10.1016/j.neucom.2025.130703", "pdf_url": "http://arxiv.org/pdf/2305.13651v2", "cate": "cs.LG", "date": "2023-05-23", "updated": "2025-07-09"}
{"id": "2507.07714", "title": "Adaptive Gaussian Mixture Models-based Anomaly Detection for under-constrained Cable-Driven Parallel Robots", "authors": ["Julio Garrido", "Javier Vales", "Diego Silva-Mu√±iz", "Enrique Riveiro", "Pablo L√≥pez-Matencio", "Josu√© Rivera-Andrade"], "categories": ["cs.RO", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      14 pages, 8 figures, 1 table, to be submitted to Advanced Intelligent Systems", "url": "http://arxiv.org/abs/2507.07714v1", "summary": "Cable-Driven Parallel Robots (CDPRs) are increasingly used for load\nmanipulation tasks involving predefined toolpaths with intermediate stops. At\neach stop, where the platform maintains a fixed pose and the motors keep the\ncables under tension, the system must evaluate whether it is safe to proceed by\ndetecting anomalies that could compromise performance (e.g., wind gusts or\ncable impacts). This paper investigates whether anomalies can be detected using\nonly motor torque data, without additional sensors. It introduces an adaptive,\nunsupervised outlier detection algorithm based on Gaussian Mixture Models\n(GMMs) to identify anomalies from torque signals. The method starts with a\nbrief calibration period, just a few seconds, during which a GMM is fit on\nknown anomaly-free data. Real-time torque measurements are then evaluated using\nMahalanobis distance from the GMM, with statistically derived thresholds\ntriggering anomaly flags. Model parameters are periodically updated using the\nlatest segments identified as anomaly-free to adapt to changing conditions.\nValidation includes 14 long-duration test sessions simulating varied wind\nintensities. The proposed method achieves a 100% true positive rate and 95.4%\naverage true negative rate, with 1-second detection latency. Comparative\nevaluation against power threshold and non-adaptive GMM methods indicates\nhigher robustness to drift and environmental variation.", "comment": "14 pages, 8 figures, 1 table, to be submitted to Advanced Intelligent\n  Systems", "pdf_url": "http://arxiv.org/pdf/2507.07714v1", "cate": "cs.RO", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07599", "title": "Enhancing Vaccine Safety Surveillance: Extracting Vaccine Mentions from Emergency Department Triage Notes Using Fine-Tuned Large Language Models", "authors": ["Sedigh Khademi", "Jim Black", "Christopher Palmer", "Muhammad Javed", "Hazel Clothier", "Jim Buttery", "Gerardo Luis Dimaguila"], "categories": ["cs.AI", "cs.CL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      5 pages", "url": "http://arxiv.org/abs/2507.07599v1", "summary": "This study evaluates fine-tuned Llama 3.2 models for extracting\nvaccine-related information from emergency department triage notes to support\nnear real-time vaccine safety surveillance. Prompt engineering was used to\ninitially create a labeled dataset, which was then confirmed by human\nannotators. The performance of prompt-engineered models, fine-tuned models, and\na rule-based approach was compared. The fine-tuned Llama 3 billion parameter\nmodel outperformed other models in its accuracy of extracting vaccine names.\nModel quantization enabled efficient deployment in resource-constrained\nenvironments. Findings demonstrate the potential of large language models in\nautomating data extraction from emergency department notes, supporting\nefficient vaccine safety surveillance and early detection of emerging adverse\nevents following immunization issues.", "comment": "5 pages", "pdf_url": "http://arxiv.org/pdf/2507.07599v1", "cate": "cs.AI", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07143", "title": "Understanding Malware Propagation Dynamics through Scientific Machine Learning", "authors": ["Karthik Pappu", "Prathamesh Dinesh Joshi", "Raj Abhijit Dandekar", "Rajat Dandekar", "Sreedath Panat"], "categories": ["cs.LG", "cs.CR"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      17 pages, 6 figures, 4 tables", "url": "http://arxiv.org/abs/2507.07143v1", "summary": "Accurately modeling malware propagation is essential for designing effective\ncybersecurity defenses, particularly against adaptive threats that evolve in\nreal time. While traditional epidemiological models and recent neural\napproaches offer useful foundations, they often fail to fully capture the\nnonlinear feedback mechanisms present in real-world networks. In this work, we\napply scientific machine learning to malware modeling by evaluating three\napproaches: classical Ordinary Differential Equations (ODEs), Universal\nDifferential Equations (UDEs), and Neural ODEs. Using data from the Code Red\nworm outbreak, we show that the UDE approach substantially reduces prediction\nerror compared to both traditional and neural baselines by 44%, while\npreserving interpretability. We introduce a symbolic recovery method that\ntransforms the learned neural feedback into explicit mathematical expressions,\nrevealing suppression mechanisms such as network saturation, security response,\nand malware variant evolution. Our results demonstrate that hybrid\nphysics-informed models can outperform both purely analytical and purely neural\napproaches, offering improved predictive accuracy and deeper insight into the\ndynamics of malware spread. These findings support the development of early\nwarning systems, efficient outbreak response strategies, and targeted cyber\ndefense interventions.", "comment": "17 pages, 6 figures, 4 tables", "pdf_url": "http://arxiv.org/pdf/2507.07143v1", "cate": "cs.LG", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07216", "title": "Bias-Aware Mislabeling Detection via Decoupled Confident Learning", "authors": ["Yunyi Li", "Maria De-Arteaga", "Maytal Saar-Tsechansky"], "categories": ["cs.LG", "cs.AI", "cs.DB", "cs.HC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07216v1", "summary": "Reliable data is a cornerstone of modern organizational systems. A notable\ndata integrity challenge stems from label bias, which refers to systematic\nerrors in a label, a covariate that is central to a quantitative analysis, such\nthat its quality differs across social groups. This type of bias has been\nconceptually and empirically explored and is widely recognized as a pressing\nissue across critical domains. However, effective methodologies for addressing\nit remain scarce. In this work, we propose Decoupled Confident Learning\n(DeCoLe), a principled machine learning based framework specifically designed\nto detect mislabeled instances in datasets affected by label bias, enabling\nbias aware mislabelling detection and facilitating data quality improvement. We\ntheoretically justify the effectiveness of DeCoLe and evaluate its performance\nin the impactful context of hate speech detection, a domain where label bias is\na well documented challenge. Empirical results demonstrate that DeCoLe excels\nat bias aware mislabeling detection, consistently outperforming alternative\napproaches for label error detection. Our work identifies and addresses the\nchallenge of bias aware mislabeling detection and offers guidance on how DeCoLe\ncan be integrated into organizational data management practices as a powerful\ntool to enhance data reliability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07216v1", "cate": "cs.LG", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07344", "title": "Automatic Generation of Explainability Requirements and Software Explanations From User Reviews", "authors": ["Martin Obaidi", "Jannik Fischbach", "Jakob Droste", "Hannah Deters", "Marc Herrmann", "Jil Kl√ºnder", "Steffen Kr√§tzig", "Hugo Villamizar", "Kurt Schneider"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      This paper has been accepted at the 33rd IEEE International Requirements Engineering Workshop (REW 2025)", "url": "http://arxiv.org/abs/2507.07344v1", "summary": "Explainability has become a crucial non-functional requirement to enhance\ntransparency, build user trust, and ensure regulatory compliance. However,\ntranslating explanation needs expressed in user feedback into structured\nrequirements and corresponding explanations remains challenging. While existing\nmethods can identify explanation-related concerns in user reviews, there is no\nestablished approach for systematically deriving requirements and generating\naligned explanations. To contribute toward addressing this gap, we introduce a\ntool-supported approach that automates this process. To evaluate its\neffectiveness, we collaborated with an industrial automation manufacturer to\ncreate a dataset of 58 user reviews, each annotated with manually crafted\nexplainability requirements and explanations. Our evaluation shows that while\nAI-generated requirements often lack relevance and correctness compared to\nhuman-created ones, the AI-generated explanations are frequently preferred for\ntheir clarity and style. Nonetheless, correctness remains an issue,\nhighlighting the importance of human validation. This work contributes to the\nadvancement of explainability requirements in software systems by (1)\nintroducing an automated approach to derive requirements from user reviews and\ngenerate corresponding explanations, (2) providing empirical insights into the\nstrengths and limitations of automatically generated artifacts, and (3)\nreleasing a curated dataset to support future research on the automatic\ngeneration of explainability requirements.", "comment": "This paper has been accepted at the 33rd IEEE International\n  Requirements Engineering Workshop (REW 2025)", "pdf_url": "http://arxiv.org/pdf/2507.07344v1", "cate": "cs.SE", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07108", "title": "Multi-level Mixture of Experts for Multimodal Entity Linking", "authors": ["Zhiwei Hu", "V√≠ctor Guti√©rrez-Basulto", "Zhiliang Xiang", "Ru Li", "Jeff Z. Pan"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at KDD 2025", "url": "http://arxiv.org/abs/2507.07108v1", "summary": "Multimodal Entity Linking (MEL) aims to link ambiguous mentions within\nmultimodal contexts to associated entities in a multimodal knowledge base.\nExisting approaches to MEL introduce multimodal interaction and fusion\nmechanisms to bridge the modality gap and enable multi-grained semantic\nmatching. However, they do not address two important problems: (i) mention\nambiguity, i.e., the lack of semantic content caused by the brevity and\nomission of key information in the mention's textual context; (ii) dynamic\nselection of modal content, i.e., to dynamically distinguish the importance of\ndifferent parts of modal information. To mitigate these issues, we propose a\nMulti-level Mixture of Experts (MMoE) model for MEL. MMoE has four components:\n(i) the description-aware mention enhancement module leverages large language\nmodels to identify the WikiData descriptions that best match a mention,\nconsidering the mention's textual context; (ii) the multimodal feature\nextraction module adopts multimodal feature encoders to obtain textual and\nvisual embeddings for both mentions and entities; (iii)-(iv) the intra-level\nmixture of experts and inter-level mixture of experts modules apply a switch\nmixture of experts mechanism to dynamically and adaptively select features from\nrelevant regions of information. Extensive experiments demonstrate the\noutstanding performance of MMoE compared to the state-of-the-art. MMoE's code\nis available at: https://github.com/zhiweihu1103/MEL-MMoE.", "comment": "Accepted at KDD 2025", "pdf_url": "http://arxiv.org/pdf/2507.07108v1", "cate": "cs.CV", "date": "2025-06-03", "updated": "2025-06-03"}
{"id": "2507.07660", "title": "Scalable Signed Exponential Random Graph Models under Local Dependence", "authors": ["Marc Schalberger", "Cornelius Fritz"], "categories": ["cs.SI", "stat.CO"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07660v1", "summary": "Traditional network analysis focuses on binary edges, while real-world\nrelationships are more nuanced, encompassing cooperation, neutrality, and\nconflict. The rise of negative edges in social media discussions spurred\ninterest in analyzing signed interactions, especially in polarized debates.\nHowever, the vast data generated by digital networks presents challenges for\ntraditional methods like Stochastic Block Models (SBM) and Exponential Family\nRandom Graph Models (ERGM), particularly due to the homogeneity assumption and\nglobal dependence, which become increasingly unrealistic as network size grows.\nTo address this, we propose a novel method that combines the strengths of SBM\nand ERGM while mitigating their weaknesses by incorporating local dependence\nbased on non-overlapping blocks. Our approach involves a two-step process:\nfirst, decomposing the network into sub-networks using SBM approximation, and\nthen estimating parameters using ERGM methods. We validate our method on large\nsynthetic networks and apply it to a signed Wikipedia network of thousands of\neditors. Through the use of local dependence, we find patterns consistent with\nstructural balance theory.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07660v1", "cate": "cs.SI", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2406.10427", "title": "Adaptive Randomized Smoothing: Certified Adversarial Robustness for Multi-Step Defences", "authors": ["Saiyue Lyu", "Shadab Shaikh", "Frederick Shpilevskiy", "Evan Shelhamer", "Mathias L√©cuyer"], "categories": ["cs.LG", "cs.CR"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2406.10427v3", "summary": "We propose Adaptive Randomized Smoothing (ARS) to certify the predictions of\nour test-time adaptive models against adversarial examples. ARS extends the\nanalysis of randomized smoothing using $f$-Differential Privacy to certify the\nadaptive composition of multiple steps. For the first time, our theory covers\nthe sound adaptive composition of general and high-dimensional functions of\nnoisy inputs. We instantiate ARS on deep image classification to certify\npredictions against adversarial examples of bounded $L_{\\infty}$ norm. In the\n$L_{\\infty}$ threat model, ARS enables flexible adaptation through\nhigh-dimensional input-dependent masking. We design adaptivity benchmarks,\nbased on CIFAR-10 and CelebA, and show that ARS improves standard test accuracy\nby $1$ to $15\\%$ points. On ImageNet, ARS improves certified test accuracy by\nup to $1.6\\%$ points over standard RS without adaptivity. Our code is available\nat https://github.com/ubc-systopia/adaptive-randomized-smoothing .", "comment": null, "pdf_url": "http://arxiv.org/pdf/2406.10427v3", "cate": "cs.LG", "date": "2024-06-14", "updated": "2025-07-10"}
{"id": "2507.07718", "title": "Implementation and Assessment of an Augmented Training Curriculum for Surgical Robotics", "authors": ["Alberto Rota", "Ke Fan", "Elena De Momi"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07718v1", "summary": "The integration of high-level assistance algorithms in surgical robotics\ntraining curricula may be beneficial in establishing a more comprehensive and\nrobust skillset for aspiring surgeons, improving their clinical performance as\na consequence. This work presents the development and validation of a\nhaptic-enhanced Virtual Reality simulator for surgical robotics training,\nfeaturing 8 surgical tasks that the trainee can interact with thanks to the\nembedded physics engine. This virtual simulated environment is augmented by the\nintroduction of high-level haptic interfaces for robotic assistance that aim at\nre-directing the motion of the trainee's hands and wrists toward targets or\naway from obstacles, and providing a quantitative performance score after the\nexecution of each training exercise.An experimental study shows that the\nintroduction of enhanced robotic assistance into a surgical robotics training\ncurriculum improves performance during the training process and, crucially,\npromotes the transfer of the acquired skills to an unassisted surgical\nscenario, like the clinical one.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07718v1", "cate": "cs.RO", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07619", "title": "Towards conservative inference in credal networks using belief functions: the case of credal chains", "authors": ["Marco Sangalli", "Thomas Krak", "Cassio de Campos"], "categories": ["cs.AI", "math.PR"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07619v1", "summary": "This paper explores belief inference in credal networks using Dempster-Shafer\ntheory. By building on previous work, we propose a novel framework for\npropagating uncertainty through a subclass of credal networks, namely chains.\nThe proposed approach efficiently yields conservative intervals through belief\nand plausibility functions, combining computational speed with robust\nuncertainty representation. Key contributions include formalizing belief-based\ninference methods and comparing belief-based inference against classical\nsensitivity analysis. Numerical results highlight the advantages and\nlimitations of applying belief inference within this framework, providing\ninsights into its practical utility for chains and for credal networks in\ngeneral.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07619v1", "cate": "cs.AI", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07145", "title": "CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs", "authors": ["Zhaojing Zhou", "Xunchao Li", "Minghao Li", "Handi Zhang", "Haoshuang Wang", "Wenbin Chang", "Yiqun Liu", "Qingqing Dang", "Dianhai Yu", "Yanjun Ma", "Haifeng Wang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      11 pages, 3 figures", "url": "http://arxiv.org/abs/2507.07145v1", "summary": "The rapid scaling of Large Language Models (LLMs) elevates inference costs\nand compounds substantial deployment barriers. While quantization to 8 or 4\nbits mitigates this, sub-3-bit methods face severe accuracy, scalability, and\nefficiency degradation. We propose Convolutional Code Quantization (CCQ), an\ninference-optimized quantization approach compressing LLMs to 2.0-2.75 bits\nwith minimal accuracy loss. Departing from error-prone scalar quantization or\nslow vector quantization, CCQ integrates a hardware-aware bit-shift encoding\nand decoding solution with Convolutional Code, Hybrid Encoding, and Code\nCluster, jointly overcoming accuracy-speed bottlenecks. We construct a\nlookup-free encoding space, enabling a linear mapping between the codebook and\nweight vectors, thereby optimizing inference performance. Meanwhile, by drawing\non the concept of data mapping from vector quantization, we minimize the\nperformance degradation of the model under extremely low-bit conditions.\nExperiments demonstrate that CCQ achieves outstanding performance on LLMs\nacross various benchmarks. We compress DeepSeek-V3 (671B total parameters) to\n184GB and ERNIE-4.5-300B-A47B to 89GB, enabling single-GPU deployment of ERNIE\n4.5 and eliminating inter-card communication. The 2-bit ERNIE-4.5-300B-A47B\nmodel and inference engine have been open-sourced.", "comment": "11 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2507.07145v1", "cate": "cs.LG", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07387", "title": "Digital Salon: An AI and Physics-Driven Tool for 3D Hair Grooming and Simulation", "authors": ["Chengan He", "Jorge Alejandro Amador Herrera", "Zhixin Shu", "Xin Sun", "Yao Feng", "S√∂ren Pirk", "Dominik L. Michels", "Meng Zhang", "Tuanfeng Y. Wang", "Julie Dorsey", "Holly Rushmeier", "Yi Zhou"], "categories": ["cs.GR", "cs.HC"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07387v1", "summary": "We introduce Digital Salon, a comprehensive hair authoring system that\nsupports real-time 3D hair generation, simulation, and rendering. Unlike\nexisting methods that focus on isolated parts of 3D hair modeling and involve a\nheavy computation process or network training, Digital Salon offers a holistic\nand interactive system that lowers the technical barriers of 3D hair modeling\nthrough natural language-based interaction. The system guides users through\nfour key stages: text-guided hair retrieval, real-time hair simulation,\ninteractive hair refinement, and hair-conditioned image generation. This\ncohesive workflow makes advanced hair design accessible to users of varying\nskill levels and dramatically streamlines the creative process in digital media\nwith an intuitive, versatile, and efficient solution for hair modeling. User\nstudies show that our system can outperform traditional hair modeling workflows\nfor rapid prototyping. Furthermore, we provide insights into the benefits of\nour system with future potential of deploying our system in real salon\nenvironments. More details can be found on our project page:\nhttps://digital-salon.github.io/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07387v1", "cate": "cs.GR", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07468", "title": "Towards an Engineering Workflow Management System for Asset Administration Shells using BPMN", "authors": ["Sten Gr√ºner", "Nafise Eskandani"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      7 pages, 7 figures, Accepted at IFAC EAAS 2025 ( this https URL )", "url": "http://arxiv.org/abs/2507.07468v1", "summary": "The integration of Industry 4.0 technologies into engineering workflows is an\nessential step toward automating and optimizing plant and process engineering\nprocesses. The Asset Administration Shell (AAS) serves as a key enabler for\ncreating interoperable Digital Twins that facilitate engineering data exchange\nand automation. This paper explores the use of AAS within engineering\nworkflows, particularly in combination with Business Process Model and Notation\n(BPMN) to define structured and automated processes. We propose a distributed\nAAS copy-on-write infrastructure that enhances security and scalability while\nenabling seamless cross organizational collaboration. We also introduce a\nworkflow management prototype automating AAS operations and engineering\nworkflows, improving efficiency and traceability.", "comment": "7 pages, 7 figures, Accepted at IFAC EAAS 2025\n  (https://j3c.org/eaas.php)", "pdf_url": "http://arxiv.org/pdf/2507.07468v1", "cate": "cs.SE", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07125", "title": "CoPT: Unsupervised Domain Adaptive Segmentation using Domain-Agnostic Text Embeddings", "authors": ["Cristina Mata", "Kanchana Ranasinghe", "Michael S. Ryoo"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ECCV 2024", "url": "http://arxiv.org/abs/2507.07125v1", "summary": "Unsupervised domain adaptation (UDA) involves learning class semantics from\nlabeled data within a source domain that generalize to an unseen target domain.\nUDA methods are particularly impactful for semantic segmentation, where\nannotations are more difficult to collect than in image classification. Despite\nrecent advances in large-scale vision-language representation learning, UDA\nmethods for segmentation have not taken advantage of the domain-agnostic\nproperties of text. To address this, we present a novel Covariance-based\nPixel-Text loss, CoPT, that uses domain-agnostic text embeddings to learn\ndomain-invariant features in an image segmentation encoder. The text embeddings\nare generated through our LLM Domain Template process, where an LLM is used to\ngenerate source and target domain descriptions that are fed to a frozen CLIP\nmodel and combined. In experiments on four benchmarks we show that a model\ntrained using CoPT achieves the new state of the art performance on UDA for\nsegmentation. The code can be found at https://github.com/cfmata/CoPT.", "comment": "ECCV 2024", "pdf_url": "http://arxiv.org/pdf/2507.07125v1", "cate": "cs.CV", "date": "2025-07-08", "updated": "2025-07-08"}
{"id": "2507.07727", "title": "Beyond Connectivity: Higher-Order Network Framework for Capturing Memory-Driven Mobility Dynamics", "authors": ["Chen Zhang", "J√ºrgen Hackl"], "categories": ["cs.SI"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07727v1", "summary": "Understanding and predicting mobility dynamics in transportation networks is\ncritical for infrastructure planning, resilience analysis, and traffic\nmanagement. Traditional graph-based models typically assume memoryless\nmovement, limiting their ability to capture sequential dependencies inherent in\nreal-world mobility patterns. In this study, we introduce a novel higher-order\nnetwork framework for modeling memory-dependent dynamics in transportation\nsystems. By extending classical graph representations through higher-order\nMarkov chains and de Bruijn graph structures, our framework encodes the spatial\nand temporal ordering of traversed paths, enabling the analysis of structurally\nand functionally critical components with improved fidelity. We generalize key\nnetwork analytics, including betweenness centrality, PageRank, and next-step\nprediction, to this higher-order setting and validate our approach on the Sioux\nFalls transportation network using agent-based trajectory data generated with\nMATSim. Experimental results demonstrate that higher-order models outperform\nfirst-order baselines across multiple tasks, with the third-order model\nachieving an optimal balance between predictive accuracy and model complexity.\nThese findings highlight the importance of incorporating memory effects into\nnetwork-based transportation analysis and offer a scalable, data-driven\nmethodology for capturing complex mobility behaviors in infrastructure systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07727v1", "cate": "cs.SI", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2407.14937", "title": "Operationalizing a Threat Model for Red-Teaming Large Language Models (LLMs)", "authors": ["Apurv Verma", "Satyapriya Krishna", "Sebastian Gehrmann", "Madhavan Seshadri", "Anu Pradhan", "Tom Ault", "Leslie Barrett", "David Rabinowitz", "John Doucette", "NhatHai Phan"], "categories": ["cs.CL", "cs.CR", "I.2.7"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Transactions of Machine Learning Research (TMLR)", "url": "http://arxiv.org/abs/2407.14937v2", "summary": "Creating secure and resilient applications with large language models (LLM)\nrequires anticipating, adjusting to, and countering unforeseen threats.\nRed-teaming has emerged as a critical technique for identifying vulnerabilities\nin real-world LLM implementations. This paper presents a detailed threat model\nand provides a systematization of knowledge (SoK) of red-teaming attacks on\nLLMs. We develop a taxonomy of attacks based on the stages of the LLM\ndevelopment and deployment process and extract various insights from previous\nresearch. In addition, we compile methods for defense and practical red-teaming\nstrategies for practitioners. By delineating prominent attack motifs and\nshedding light on various entry points, this paper provides a framework for\nimproving the security and robustness of LLM-based systems.", "comment": "Transactions of Machine Learning Research (TMLR)", "pdf_url": "http://arxiv.org/pdf/2407.14937v2", "cate": "cs.CL", "date": "2024-07-20", "updated": "2025-07-10"}
{"id": "2507.07724", "title": "Distributed Surface Inspection via Operational Modal Analysis by a Swarm of Miniaturized Vibration-Sensing Robots", "authors": ["Thiemen Siemensma", "Niels de Boer", "Bahar Haghighat"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07724v1", "summary": "Robot swarms offer the potential to serve a variety of distributed sensing\napplications. An interesting real-world application that stands to benefit\nsignificantly from deployment of swarms is structural monitoring, where\ntraditional sensor networks face challenges in structural coverage due to their\nstatic nature. This paper investigates the deployment of a swarm of\nminiaturized vibration sensing robots to inspect and localize structural\ndamages on a surface section within a high-fidelity simulation environment. In\nparticular, we consider a 1 m x 1 m x 3 mm steel surface section and utilize\nfinite element analysis using Abaqus to obtain realistic structural vibration\ndata. The resulting vibration data is imported into the physics-based robotic\nsimulator Webots, where we simulate the dynamics of our surface inspecting\nrobot swarm. We employ (i) Gaussian process estimators to guide the robots'\nexploration as they collect vibration samples across the surface and (ii)\noperational modal analysis to detect structural damages by estimating and\ncomparing existing and intact structural vibration patterns. We analyze the\ninfluence of exploration radii on estimation uncertainty and assess the\neffectiveness of our method across 10 randomized scenarios, where the number,\nlocations, surface area, and depth of structural damages vary. Our simulation\nstudies validate the efficacy of our miniaturized robot swarm for\nvibration-based structural inspection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07724v1", "cate": "cs.RO", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07644", "title": "PlanQA: A Benchmark for Spatial Reasoning in LLMs using Structured Representations", "authors": ["Fedor Rodionov", "Abdelrahman Eldesokey", "Michael Birsak", "John Femiani", "Bernard Ghanem", "Peter Wonka"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      25 pages, 18 figures. Diagnostic benchmark for spatial reasoning in LLMs. Project page: this https URL", "url": "http://arxiv.org/abs/2507.07644v1", "summary": "We introduce PlanQA, a diagnostic benchmark for evaluating geometric and\nspatial reasoning in large-language models (LLMs). PlanQA is grounded in\nstructured representations of indoor scenes, such as kitchens, living rooms,\nand bedrooms, encoded in a symbolic format (e.g., JSON, XML layouts). The\nbenchmark includes diverse question types that test not only metric and\ntopological reasoning (e.g., distance, visibility, shortest paths) but also\ninterior design constraints such as affordance, clearance, balance, and\nusability. Our results across a variety of frontier open-source and commercial\nLLMs show that while models may succeed in shallow queries, they often fail to\nsimulate physical constraints, preserve spatial coherence, or generalize under\nlayout perturbation. PlanQA uncovers a clear blind spot in today's LLMs: they\ndo not consistently reason about real-world layouts. We hope that this\nbenchmark inspires new work on language models that can accurately infer and\nmanipulate spatial and geometric properties in practical settings.", "comment": "25 pages, 18 figures. Diagnostic benchmark for spatial reasoning in\n  LLMs. Project page: https://OldDelorean.github.io/PlanQA/", "pdf_url": "http://arxiv.org/pdf/2507.07644v1", "cate": "cs.AI", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07146", "title": "An attention-aware GNN-based input defender against multi-turn jailbreak on LLMs", "authors": ["Zixuan Huang", "Kecheng Huang", "Lihao Yin", "Bowei He", "Huiling Zhen", "Mingxuan Yuan", "Zili Shao"], "categories": ["cs.LG", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07146v1", "summary": "Large Language Models (LLMs) have gained widespread popularity and are\nincreasingly integrated into various applications. However, their capabilities\ncan be exploited for both benign and harmful purposes. Despite rigorous\ntraining and fine-tuning for safety, LLMs remain vulnerable to jailbreak\nattacks. Recently, multi-turn attacks have emerged, exacerbating the issue.\nUnlike single-turn attacks, multi-turn attacks gradually escalate the dialogue,\nmaking them more difficult to detect and mitigate, even after they are\nidentified.\n  In this study, we propose G-Guard, an innovative attention-aware GNN-based\ninput classifier designed to defend against multi-turn jailbreak attacks on\nLLMs. G-Guard constructs an entity graph for multi-turn queries, explicitly\ncapturing relationships between harmful keywords and queries even when those\nkeywords appear only in previous queries. Additionally, we introduce an\nattention-aware augmentation mechanism that retrieves the most similar\nsingle-turn query based on the multi-turn conversation. This retrieved query is\ntreated as a labeled node in the graph, enhancing the ability of GNN to\nclassify whether the current query is harmful. Evaluation results demonstrate\nthat G-Guard outperforms all baselines across all datasets and evaluation\nmetrics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07146v1", "cate": "cs.LG", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07610", "title": "SpatialViz-Bench: Automatically Generated Spatial Visualization Reasoning Tasks for MLLMs", "authors": ["Siting Wang", "Luoyang Sun", "Cheng Deng", "Kun Shao", "Minnan Pei", "Zheng Tian", "Haifeng Zhang", "Jun Wang"], "categories": ["cs.CV", "cs.CL", "cs.HC"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07610v1", "summary": "Humans can directly imagine and manipulate visual images in their minds, a\ncapability known as spatial visualization. While multi-modal Large Language\nModels (MLLMs) support imagination-based reasoning, spatial visualization\nremains insufficiently evaluated, typically embedded within broader\nmathematical and logical assessments. Existing evaluations often rely on IQ\ntests or math competitions that may overlap with training data, compromising\nassessment reliability. To this end, we introduce SpatialViz-Bench, a\ncomprehensive multi-modal benchmark for spatial visualization with 12 tasks\nacross 4 sub-abilities, comprising 1,180 automatically generated problems. Our\nevaluation of 33 state-of-the-art MLLMs not only reveals wide performance\nvariations and demonstrates the benchmark's strong discriminative power, but\nalso uncovers counter-intuitive findings: models exhibit unexpected behaviors\nby showing difficulty perception that misaligns with human intuition,\ndisplaying dramatic 2D-to-3D performance cliffs, and defaulting to formula\nderivation despite spatial tasks requiring visualization alone. SpatialVizBench\nempirically demonstrates that state-of-the-art MLLMs continue to exhibit\ndeficiencies in spatial visualization tasks, thereby addressing a significant\nlacuna in the field. The benchmark is publicly available.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07610v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07548", "title": "From Requirements to Code: Understanding Developer Practices in LLM-Assisted Software Engineering", "authors": ["Jonathan Ullrich", "Matthias Koch", "Andreas Vogelsang"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      This paper has been accepted for publication at the 33rd IEEE International Requirements Engineering (RE) conference", "url": "http://arxiv.org/abs/2507.07548v1", "summary": "With the advent of generative LLMs and their advanced code generation\ncapabilities, some people already envision the end of traditional software\nengineering, as LLMs may be able to produce high-quality code based solely on\nthe requirements a domain expert feeds into the system. The feasibility of this\nvision can be assessed by understanding how developers currently incorporate\nrequirements when using LLMs for code generation-a topic that remains largely\nunexplored. We interviewed 18 practitioners from 14 companies to understand how\nthey (re)use information from requirements and other design artifacts to feed\nLLMs when generating code. Based on our findings, we propose a theory that\nexplains the processes developers employ and the artifacts they rely on. Our\ntheory suggests that requirements, as typically documented, are too abstract\nfor direct input into LLMs. Instead, they must first be manually decomposed\ninto programming tasks, which are then enriched with design decisions and\narchitectural constraints before being used in prompts. Our study highlights\nthat fundamental RE work is still necessary when LLMs are used to generate\ncode. Our theory is important for contextualizing scientific approaches to\nautomating requirements-centric SE tasks.", "comment": "This paper has been accepted for publication at the 33rd IEEE\n  International Requirements Engineering (RE) conference", "pdf_url": "http://arxiv.org/pdf/2507.07548v1", "cate": "cs.SE", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07148", "title": "Explainable Artificial Intelligence in Biomedical Image Analysis: A Comprehensive Survey", "authors": ["Getamesay Haile Dagnaw", "Yanming Zhu", "Muhammad Hassan Maqsood", "Wencheng Yang", "Xingshuai Dong", "Xuefei Yin", "Alan Wee-Chung Liew"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07148v1", "summary": "Explainable artificial intelligence (XAI) has become increasingly important\nin biomedical image analysis to promote transparency, trust, and clinical\nadoption of DL models. While several surveys have reviewed XAI techniques, they\noften lack a modality-aware perspective, overlook recent advances in multimodal\nand vision-language paradigms, and provide limited practical guidance. This\nsurvey addresses this gap through a comprehensive and structured synthesis of\nXAI methods tailored to biomedical image analysis.We systematically categorize\nXAI methods, analyzing their underlying principles, strengths, and limitations\nwithin biomedical contexts. A modality-centered taxonomy is proposed to align\nXAI methods with specific imaging types, highlighting the distinct\ninterpretability challenges across modalities. We further examine the emerging\nrole of multimodal learning and vision-language models in explainable\nbiomedical AI, a topic largely underexplored in previous work. Our\ncontributions also include a summary of widely used evaluation metrics and\nopen-source frameworks, along with a critical discussion of persistent\nchallenges and future directions. This survey offers a timely and in-depth\nfoundation for advancing interpretable DL in biomedical image analysis.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07148v1", "cate": "cs.CV", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07884", "title": "Conspiracy to Commit: Information Pollution, Artificial Intelligence, and Real-World Hate Crime", "authors": ["Alberto Aziani", "Michael V. Lo Giudice", "Ali Shadman Yazdi"], "categories": ["cs.SI"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07884v1", "summary": "Is demand for conspiracy theories online linked to real-world hate crimes? By\nanalyzing online search trends for 36 racially and politically-charged\nconspiracy theories in Michigan (2015-2019), we employ a one-dimensional\nconvolutional neural network (1D-CNN) to predict hate crime occurrences\noffline. A subset of theories including the Rothschilds family, Q-Anon, and The\nGreat Replacement improves prediction accuracy, with effects emerging two to\nthree weeks after fluctuations in searches. However, most theories showed no\nclear connection to offline hate crimes. Aligning with neutralization and\ndifferential association theories, our findings provide a partial empirical\nlink between specific racially charged conspiracy theories and real-world\nviolence. Just as well, this study underscores the potential for machine\nlearning to be used in identifying harmful online patterns and advancing social\nscience research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07884v1", "cate": "cs.SI", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2502.09772", "title": "Implementation and Analysis of Regev's Quantum Factorization Algorithm", "authors": ["Przemys≈Çaw Pawlitko", "Natalia Moƒáko", "Marcin Niemiec", "Piotr Cho≈Çda"], "categories": ["quant-ph", "cs.CR"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.09772v2", "summary": "Quantum computing represents a significant advancement in computational\ncapabilities. Of particular concern is its impact on asymmetric cryptography\nthrough, notably, Shor's algorithm and the more recently developed Regev's\nalgorithm for factoring composite numbers. We present our implementation of the\nlatter. Our analysis encompasses both quantum simulation results and classical\ncomponent examples, with particular emphasis on comparative cases between\nRegev's and Shor's algorithms. Our experimental results reveal that Regev's\nalgorithm indeed outperforms Shor's algorithm for certain composite numbers in\npractice. However, we observed significant performance variations across\ndifferent input values. Despite Regev's algorithm's theoretical asymptotic\nefficiency advantage, our implementation exhibited execution times longer than\nShor's algorithm for small integer factorization in both quantum and classical\ncomponents. These findings offer insights into the practical challenges and\nperformance characteristics of implementing Regev's algorithm in realistic\nquantum computing scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.09772v2", "cate": "quant-ph", "date": "2025-02-13", "updated": "2025-07-09"}
{"id": "2507.07745", "title": "On the capabilities of LLMs for classifying and segmenting time series of fruit picking motions into primitive actions", "authors": ["Eleni Konstantinidou", "Nikolaos Kounalakis", "Nikolaos Efstathopoulos", "Dimitrios Papageorgiou"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      This paper is a Late Breaking Results report and it will be presented through a poster at the 34th IEEE International Conference on Robot and Human Interactive Communication (ROMAN), 2025 at Eindhoven, the Netherlands", "url": "http://arxiv.org/abs/2507.07745v1", "summary": "Despite their recent introduction to human society, Large Language Models\n(LLMs) have significantly affected the way we tackle mental challenges in our\neveryday lives. From optimizing our linguistic communication to assisting us in\nmaking important decisions, LLMs, such as ChatGPT, are notably reducing our\ncognitive load by gradually taking on an increasing share of our mental\nactivities. In the context of Learning by Demonstration (LbD), classifying and\nsegmenting complex motions into primitive actions, such as pushing, pulling,\ntwisting etc, is considered to be a key-step towards encoding a task. In this\nwork, we investigate the capabilities of LLMs to undertake this task,\nconsidering a finite set of predefined primitive actions found in fruit picking\noperations. By utilizing LLMs instead of simple supervised learning or analytic\nmethods, we aim at making the method easily applicable and deployable in a\nreal-life scenario. Three different fine-tuning approaches are investigated,\ncompared on datasets captured kinesthetically, using a UR10e robot, during a\nfruit-picking scenario.", "comment": "This paper is a Late Breaking Results report and it will be presented\n  through a poster at the 34th IEEE International Conference on Robot and Human\n  Interactive Communication (ROMAN), 2025 at Eindhoven, the Netherlands", "pdf_url": "http://arxiv.org/pdf/2507.07745v1", "cate": "cs.RO", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07723", "title": "Stable Preference Optimization for LLMs: A Bilevel Approach Beyond Direct Preference Optimization", "authors": ["Chengtao Jian", "Kai Yang", "Ye Ouyang", "Xiaozhou Ye"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07723v1", "summary": "Direct Preference Optimization (DPO) has emerged as a popular and efficient\nalternative to reward modeling and reinforcement learning for aligning language\nmodels with human preferences. Despite its empirical success, the theoretical\nproperties and intrinsic limitations of DPO remain underexplored. In this work,\nwe first present a comprehensive analysis of DPO's dynamics from a probability\nevolution perspective. Our analysis reveals that DPO is highly sensitive to\ninitialization. It also tends to misallocate probability mass, which can\ninadvertently shift probability toward irrelevant or undesired responses. This\nmisallocation may unintentionally reinforce model bias, thereby compromising\nboth the stability of model alignment and the consistency with intended\npreferences. Motivated by these theoretical findings, we propose a\ntheoretically grounded bilevel optimization framework that tightly integrate\nsupervised fine-tuning with an enhanced DPO objective a.k.a. stable preference\noptimization. Our approach introduces a principled regularization scheme to\nexplicitly encourage absolute probability improvement for preferred outputs,\nwhile maintaining stable optimization dynamics. Experiments on challenging\nreasoning and summarization benchmarks elucidate that our method consistently\nimproves reasoning accuracy and better aligns output distributions with\nintended preferences, outperforming standard DPO. Stable preference\noptimization provides new insights into the design of preference-based\nalignment objectives and opens up new avenues towards more reliable and\ninterpretable language model alignment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07723v1", "cate": "cs.AI", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07147", "title": "Weighted Multi-Prompt Learning with Description-free Large Language Model Distillation", "authors": ["Sua Lee", "Kyubum Shin", "Jung Ho Park"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Published as a conference paper at ICLR 2025", "url": "http://arxiv.org/abs/2507.07147v1", "summary": "Recent advances in pre-trained Vision Language Models (VLM) have shown\npromising potential for effectively adapting to downstream tasks through prompt\nlearning, without the need for additional annotated paired datasets. To\nsupplement the text information in VLM trained on correlations with vision\ndata, new approaches leveraging Large Language Models (LLM) in prompts have\nbeen proposed, enhancing robustness to unseen and diverse data. Existing\nmethods typically extract text-based responses (i.e., descriptions) from LLM to\nincorporate into prompts; however, this approach suffers from high variability\nand low reliability. In this work, we propose Description-free Multi-prompt\nLearning(DeMul), a novel method that eliminates the process of extracting\ndescriptions and instead directly distills knowledge from LLM into prompts. By\nadopting a description-free approach, prompts can encapsulate richer semantics\nwhile still being represented as continuous vectors for optimization, thereby\neliminating the need for discrete pre-defined templates. Additionally, in a\nmulti-prompt setting, we empirically demonstrate the potential of prompt\nweighting in reflecting the importance of different prompts during training.\nExperimental results show that our approach achieves superior performance\nacross 11 recognition datasets.", "comment": "Published as a conference paper at ICLR 2025", "pdf_url": "http://arxiv.org/pdf/2507.07147v1", "cate": "cs.LG", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07881", "title": "Opting Out of Generative AI: a Behavioral Experiment on the Role of Education in Perplexity AI Avoidance", "authors": ["Roberto Ulloa", "Juhi Kulshrestha", "Celina Kacperski"], "categories": ["cs.CY", "cs.HC"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07881v1", "summary": "The rise of conversational AI (CAI), powered by large language models, is\ntransforming how individuals access and interact with digital information.\nHowever, these tools may inadvertently amplify existing digital inequalities.\nThis study investigates whether differences in formal education are associated\nwith CAI avoidance, leveraging behavioral data from an online experiment (N =\n1,636). Participants were randomly assigned to a control or an\ninformation-seeking task, either a traditional online search or a CAI\n(Perplexity AI). Task avoidance (operationalized as survey abandonment or\nproviding unrelated responses during task assignment) was significantly higher\nin the CAI group (51%) compared to the search (30.9%) and control (16.8%)\ngroups, with the highest CAI avoidance among participants with lower education\nlevels (~74.4%). Structural equation modeling based on the theoretical\nframework UTAUT2 and LASSO regressions reveal that education is strongly\nassociated with CAI avoidance, even after accounting for various cognitive and\naffective predictors of technology adoption. These findings underscore\neducation's central role in shaping AI adoption and the role of self-selection\nbiases in AI-related research, stressing the need for inclusive design to\nensure equitable access to emerging technologies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07881v1", "cate": "cs.CY", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07682", "title": "Prompt Engineering for Requirements Engineering: A Literature Review and Roadmap", "authors": ["Kaicheng Huang", "Fanyu Wang", "Yutan Huang", "Chetan Arora"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07682v1", "summary": "Advancements in large language models (LLMs) have led to a surge of prompt\nengineering (PE) techniques that can enhance various requirements engineering\n(RE) tasks. However, current LLMs are often characterized by significant\nuncertainty and a lack of controllability. This absence of clear guidance on\nhow to effectively prompt LLMs acts as a barrier to their trustworthy\nimplementation in the RE field. We present the first roadmap-oriented\nsystematic literature review of Prompt Engineering for RE (PE4RE). Following\nKitchenham's and Petersen's secondary-study protocol, we searched six digital\nlibraries, screened 867 records, and analyzed 35 primary studies. To bring\norder to a fragmented landscape, we propose a hybrid taxonomy that links\ntechnique-oriented patterns (e.g., few-shot, Chain-of-Thought) to task-oriented\nRE roles (elicitation, validation, traceability). Two research questions, with\nfive sub-questions, map the tasks addressed, LLM families used, and prompt\ntypes adopted, and expose current limitations and research gaps. Finally, we\noutline a step-by-step roadmap showing how today's ad-hoc PE prototypes can\nevolve into reproducible, practitioner-friendly workflows.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07682v1", "cate": "cs.SE", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07151", "title": "Robust Multimodal Large Language Models Against Modality Conflict", "authors": ["Zongmeng Zhang", "Wengang Zhou", "Jie Zhao", "Houqiang Li"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICML 2025", "url": "http://arxiv.org/abs/2507.07151v1", "summary": "Despite the impressive capabilities of multimodal large language models\n(MLLMs) in vision-language tasks, they are prone to hallucinations in\nreal-world scenarios. This paper investigates the hallucination phenomenon in\nMLLMs from the perspective of modality conflict. Unlike existing works focusing\non the conflicts between model responses and inputs, we study the inherent\nconflicts in inputs from different modalities that place MLLMs in a dilemma and\ndirectly lead to hallucinations. We formally define the modality conflict and\nconstruct a dataset named Multimodal Modality Conflict (MMMC) to simulate this\nphenomenon in vision-language tasks. Three methods based on prompt engineering,\nsupervised fine-tuning, and reinforcement learning are proposed to alleviate\nthe hallucination caused by modality conflict. Extensive experiments are\nconducted on the MMMC dataset to analyze the merits and demerits of these\nmethods. Our results show that the reinforcement learning method achieves the\nbest performance in mitigating the hallucination under modality conflict, while\nthe supervised fine-tuning method shows promising and stable performance. Our\nwork sheds light on the unnoticed modality conflict that leads to\nhallucinations and provides more insights into the robustness of MLLMs.", "comment": "ICML 2025", "pdf_url": "http://arxiv.org/pdf/2507.07151v1", "cate": "cs.CV", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07536", "title": "Efficient and Adaptive Estimation of Local Triadic Coefficients", "authors": ["Ilie Sarpe", "Aristides Gionis"], "categories": ["cs.DS", "cs.SI"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      Accepted at VLDB'25 (extended version)", "url": "http://arxiv.org/abs/2507.07536v1", "summary": "Characterizing graph properties is fundamental to the analysis and to our\nunderstanding of real-world networked systems. The local clustering\ncoefficient, and the more recently introduced, local closure coefficient,\ncapture powerful properties that are essential in a large number of\napplications, ranging from graph embeddings to graph partitioning. Such\ncoefficients capture the local density of the neighborhood of each node,\nconsidering incident triadic structures and paths of length two. For this\nreason, we refer to these coefficients collectively as local triadic\ncoefficients.\n  In this work, we consider the novel problem of computing efficiently the\naverage of local triadic coefficients, over a given partition of the nodes of\nthe input graph into a set of disjoint buckets. The average local triadic\ncoefficients of the nodes in each bucket provide a better insight into the\ninterplay of graph structure and the properties of the nodes associated to each\nbucket. Unfortunately, exact computation, which requires listing all triangles\nin a graph, is infeasible for large networks. Hence, we focus on obtaining\nhighly-accurate probabilistic estimates.\n  We develop Triad, an adaptive algorithm based on sampling, which can be used\nto estimate the average local triadic coefficients for a partition of the nodes\ninto buckets. Triad is based on a new class of unbiased estimators, and\nnon-trivial bounds on its sample complexity, enabling the efficient computation\nof highly accurate estimates. Finally, we show how Triad can be efficiently\nused in practice on large networks, and we present a case study showing that\naverage local triadic coefficients can capture high-order patterns over\ncollaboration networks.", "comment": "Accepted at VLDB'25 (extended version)", "pdf_url": "http://arxiv.org/pdf/2507.07536v1", "cate": "cs.DS", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2502.18549", "title": "ARBoids: Adaptive Residual Reinforcement Learning With Boids Model for Cooperative Multi-USV Target Defense", "authors": ["Jiyue Tao", "Tongsheng Shen", "Dexin Zhao", "Feitian Zhang"], "categories": ["cs.LG", "cs.CR"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.18549v2", "summary": "The target defense problem (TDP) for unmanned surface vehicles (USVs)\nconcerns intercepting an adversarial USV before it breaches a designated target\nregion, using one or more defending USVs. A particularly challenging scenario\narises when the attacker exhibits superior maneuverability compared to the\ndefenders, significantly complicating effective interception. To tackle this\nchallenge, this letter introduces ARBoids, a novel adaptive residual\nreinforcement learning framework that integrates deep reinforcement learning\n(DRL) with the biologically inspired, force-based Boids model. Within this\nframework, the Boids model serves as a computationally efficient baseline\npolicy for multi-agent coordination, while DRL learns a residual policy to\nadaptively refine and optimize the defenders' actions. The proposed approach is\nvalidated in a high-fidelity Gazebo simulation environment, demonstrating\nsuperior performance over traditional interception strategies, including pure\nforce-based approaches and vanilla DRL policies. Furthermore, the learned\npolicy exhibits strong adaptability to attackers with diverse maneuverability\nprofiles, highlighting its robustness and generalization capability. The code\nof ARBoids will be released upon acceptance of this letter.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.18549v2", "cate": "cs.LG", "date": "2025-02-25", "updated": "2025-07-10"}
{"id": "2507.07752", "title": "IRAF-SLAM: An Illumination-Robust and Adaptive Feature-Culling Front-End for Visual SLAM in Challenging Environments", "authors": ["Thanh Nguyen Canh", "Bao Nguyen Quoc", "Haolan Zhang", "Bupesh Rethinam Veeraiah", "Xiem HoangVan", "Nak Young Chong"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      In the European Conference on Mobile Robots 2025", "url": "http://arxiv.org/abs/2507.07752v1", "summary": "Robust Visual SLAM (vSLAM) is essential for autonomous systems operating in\nreal-world environments, where challenges such as dynamic objects, low texture,\nand critically, varying illumination conditions often degrade performance.\nExisting feature-based SLAM systems rely on fixed front-end parameters, making\nthem vulnerable to sudden lighting changes and unstable feature tracking. To\naddress these challenges, we propose ``IRAF-SLAM'', an Illumination-Robust and\nAdaptive Feature-Culling front-end designed to enhance vSLAM resilience in\ncomplex and challenging environments. Our approach introduces: (1) an image\nenhancement scheme to preprocess and adjust image quality under varying\nlighting conditions; (2) an adaptive feature extraction mechanism that\ndynamically adjusts detection sensitivity based on image entropy, pixel\nintensity, and gradient analysis; and (3) a feature culling strategy that\nfilters out unreliable feature points using density distribution analysis and a\nlighting impact factor. Comprehensive evaluations on the TUM-VI and European\nRobotics Challenge (EuRoC) datasets demonstrate that IRAF-SLAM significantly\nreduces tracking failures and achieves superior trajectory accuracy compared to\nstate-of-the-art vSLAM methods under adverse illumination conditions. These\nresults highlight the effectiveness of adaptive front-end strategies in\nimproving vSLAM robustness without incurring significant computational\noverhead. The implementation of IRAF-SLAM is publicly available at\nhttps://thanhnguyencanh. github.io/IRAF-SLAM/.", "comment": "In the European Conference on Mobile Robots 2025", "pdf_url": "http://arxiv.org/pdf/2507.07752v1", "cate": "cs.RO", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07743", "title": "Identification of Violin Reduction via Contour Lines Classification", "authors": ["Phil√©mon Beghin", "Anne-Emmanuelle Ceulemans", "Fran√ßois Glineur"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07743v1", "summary": "The first violins appeared in late 16th-century Italy. Over the next 200\nyears, they spread across Europe and luthiers of various royal courts, eager to\nexperiment with new techniques, created a highly diverse family of instruments.\nAround 1750, size standards were introduced to unify violin making for\norchestras and conservatories. Instruments that fell between two standards were\nthen reduced to a smaller size by luthiers. These reductions have an impact on\nseveral characteristics of violins, in particular on the contour lines, i.e.\nlines of constant altitude, which look more like a U for non reduced\ninstruments and a V for reduced ones. While such differences are observed by\nexperts, they have not been studied quantitatively.\n  This paper presents a method for classifying violins as reduced or\nnon-reduced based on their contour lines. We study a corpus of 25 instruments\nwhose 3D geometric meshes were acquired via photogrammetry. For each\ninstrument, we extract 10-20 contour lines regularly spaced every millimetre.\nEach line is fitted with a parabola-like curve (with an equation of the type y\n= alpha*abs(x)**beta) depending on two parameters, describing how open (beta)\nand how vertically stretched (alpha) the curve is. We compute additional\nfeatures from those parameters, using regressions and counting how many values\nfall under some threshold. We also deal with outliers and non equal numbers of\nlevels, and eventually obtain a numerical profile for each instrument.\n  We then apply classification methods to assess whether geometry alone can\npredict size reduction. We find that distinguishing between reduced and non\nreduced instruments is feasible to some degree, taking into account that a\nwhole spectrum of more or less transformed violins exists, for which it is more\ndifficult to quantify the reduction. We also find the opening parameter beta to\nbe the most predictive.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07743v1", "cate": "cs.AI", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07192", "title": "Bridging the Last Mile of Prediction: Enhancing Time Series Forecasting with Conditional Guided Flow Matching", "authors": ["Huibo Xu", "Runlong Yu", "Likang Wu", "Xianquan Wang", "Qi Liu"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07192v1", "summary": "Diffusion models, a type of generative model, have shown promise in time\nseries forecasting. But they face limitations like rigid source distributions\nand limited sampling paths, which hinder their performance. Flow matching\noffers faster generation, higher-quality outputs, and greater flexibility,\nwhile also possessing the ability to utilize valuable information from the\nprediction errors of prior models, which were previously inaccessible yet\ncritically important. To address these challenges and fully unlock the untapped\npotential of flow matching, we propose Conditional Guided Flow Matching (CGFM).\nCGFM extends flow matching by incorporating the outputs of an auxiliary model,\nenabling a previously unattainable capability in the field: learning from the\nerrors of the auxiliary model. For time series forecasting tasks, it integrates\nhistorical data as conditions and guidance, constructs two-sided conditional\nprobability paths, and uses a general affine path to expand the space of\nprobability paths, ultimately leading to improved predictions. Extensive\nexperiments show that CGFM consistently enhances and outperforms\nstate-of-the-art models, highlighting its effectiveness in advancing\nforecasting methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07192v1", "cate": "cs.LG", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07911", "title": "The Potential of Olfactory Stimuli in Stress Reduction through Virtual Reality", "authors": ["Yasmin Elsaddik Valdivieso", "Mohd Faisal", "Karim Alghoul", "Monireh", "Vahdati", "Kamran Gholizadeh Hamlabadi", "Fedwa Laamarti", "Hussein Al Osman", "Abdulmotaleb El Saddik"], "categories": ["cs.MM", "cs.HC"], "primary_category": "Subjects:       Multimedia (cs.MM)", "pdf_link": null, "comments": "Comments:      Accepted to IEEE Medical Measurements & Applications (MeMeA) 2025", "url": "http://arxiv.org/abs/2507.07911v1", "summary": "Immersive virtual reality (VR) is a promising tool for stress reduction and\nrelaxation, traditionally relying on visual and auditory stimuli. This study\nexamines the role of olfactory stimuli in enhancing these effects, using a\nrandomized within-subject design. Thirty participants aged 18-60 experienced VR\nscenarios simulating a calming seaside environment, with sessions lasting 45\nminutes, in two conditions: with and without a \"Beach\" essential oil scent\n(Yankee Candle) administered via diffuser. Stress and relaxation were assessed\nthrough self-reported surveys and physiological measures, specifically\nECG-based heart rate variability (HRV). Results showed no significant\ndifference in self-reported relaxation scores (p=0.371) between conditions, but\nHRV analysis revealed a significant stress reduction (p=0.002) with olfactory\ninput, with HF increasing 108% from the Math Stress Test to the scented\nrelaxation condition, compared to 44% without scent. Additionally, 71.4% of\nparticipants expressed willingness to use olfactory-enhanced VR for relaxation,\nsuggesting practical appeal. These findings indicate that olfactory stimuli may\nenhance relaxation subconsciously, underscoring the importance of multisensory\nintegration in VR. Future work could explore personalized scents and long-term\neffects to optimize VR- based interventions for emotional and physical\nwell-being.", "comment": "Accepted to IEEE Medical Measurements & Applications (MeMeA) 2025", "pdf_url": "http://arxiv.org/pdf/2507.07911v1", "cate": "cs.MM", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07689", "title": "From Domain Documents to Requirements: Retrieval-Augmented Generation in the Space Industry", "authors": ["Chetan Arora", "Fanyu Wang", "Chakkrit Tantithamthavorn", "Aldeida Aleti", "Shaun Kenyon"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07689v1", "summary": "Requirements engineering (RE) in the space industry is inherently complex,\ndemanding high precision, alignment with rigorous standards, and adaptability\nto mission-specific constraints. Smaller space organisations and new entrants\noften struggle to derive actionable requirements from extensive, unstructured\ndocuments such as mission briefs, interface specifications, and regulatory\nstandards. In this innovation opportunity paper, we explore the potential of\nRetrieval-Augmented Generation (RAG) models to support and (semi-)automate\nrequirements generation in the space domain. We present a modular, AI-driven\napproach that preprocesses raw space mission documents, classifies them into\nsemantically meaningful categories, retrieves contextually relevant content\nfrom domain standards, and synthesises draft requirements using large language\nmodels (LLMs). We apply the approach to a real-world mission document from the\nspace domain to demonstrate feasibility and assess early outcomes in\ncollaboration with our industry partner, Starbound Space Solutions. Our\npreliminary results indicate that the approach can reduce manual effort,\nimprove coverage of relevant requirements, and support lightweight compliance\nalignment. We outline a roadmap toward broader integration of AI in RE\nworkflows, intending to lower barriers for smaller organisations to participate\nin large-scale, safety-critical missions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07689v1", "cate": "cs.SE", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07153", "title": "Aerial Maritime Vessel Detection and Identification", "authors": ["Antonella Barisic Kulas", "Frano Petric", "Stjepan Bogdan"], "categories": ["cs.CV", "cs.AI", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Preprint. ICUAS 2025", "url": "http://arxiv.org/abs/2507.07153v1", "summary": "Autonomous maritime surveillance and target vessel identification in\nenvironments where Global Navigation Satellite Systems (GNSS) are not available\nis critical for a number of applications such as search and rescue and threat\ndetection. When the target vessel is only described by visual cues and its last\nknown position is not available, unmanned aerial vehicles (UAVs) must rely\nsolely on on-board vision to scan a large search area under strict\ncomputational constraints. To address this challenge, we leverage the YOLOv8\nobject detection model to detect all vessels in the field of view. We then\napply feature matching and hue histogram distance analysis to determine whether\nany detected vessel corresponds to the target. When found, we localize the\ntarget using simple geometric principles. We demonstrate the proposed method in\nreal-world experiments during the MBZIRC2023 competition, integrated into a\nfully autonomous system with GNSS-denied navigation. We also evaluate the\nimpact of perspective on detection accuracy and localization precision and\ncompare it with the oracle approach.", "comment": "Preprint. ICUAS 2025", "pdf_url": "http://arxiv.org/pdf/2507.07153v1", "cate": "cs.CV", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2411.07907", "title": "Diffusion of complex contagions is shaped by a trade-off between reach and reinforcement", "authors": ["Allison Wan", "Christoph Riedl", "David Lazer"], "categories": ["cs.SI", "physics.soc-ph"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.07907v2", "summary": "How does social network structure amplify or stifle behavior diffusion?\nExisting theory suggests that when social reinforcement makes the adoption of\nbehavior more likely, it should spread more -- both farther and faster -- on\nclustered networks with redundant ties. Conversely, if adoption does not\nbenefit from social reinforcement, it should spread more on random networks\nwhich avoid such redundancies. We develop a novel model of behavior diffusion\nwith tunable probabilistic adoption and social reinforcement parameters to\nsystematically evaluate the conditions under which clustered networks spread\nbehavior better than random networks. Using simulations and analytical methods,\nwe identify precise boundaries in the parameter space where one network type\noutperforms the other or they perform equally. We find that, in most cases,\nrandom networks spread behavior as far or farther than clustered networks, even\nwhen social reinforcement increases adoption. Although we find that\nprobabilistic, socially reinforced behaviors can spread farther on clustered\nnetworks in some cases, this is not the dominant pattern. Clustered networks\nare even less advantageous when individuals remain influential for longer after\nadopting, have more neighbors, or need more neighbors before social\nreinforcement takes effect. Under such conditions, clustering tends to help\nonly when adoption is nearly deterministic, which is not representative of\nsocially reinforced behaviors more generally. Clustered networks outperform\nrandom networks by a 5% margin in only 22% of the parameter space under its\nmost favorable conditions. This pattern reflects a fundamental tradeoff: random\nties enhance reach, while clustered ties enhance social reinforcement.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.07907v2", "cate": "cs.SI", "date": "2024-11-12", "updated": "2025-07-10"}
{"id": "2504.15284", "title": "EditLord: Learning Code Transformation Rules for Code Editing", "authors": ["Weichen Li", "Albert Jan", "Baishakhi Ray", "Junfeng Yang", "Chengzhi Mao", "Kexin Pei"], "categories": ["cs.SE", "cs.CR", "cs.LG"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.15284v4", "summary": "Code editing is a foundational task in software development, where its\neffectiveness depends on whether it introduces desired code property changes\nwithout changing the original code's intended functionality. Existing\napproaches often formulate code editing as an implicit end-to-end task,\nomitting the fact that code-editing procedures inherently consist of discrete\nand explicit steps. Thus, they suffer from suboptimal performance and lack of\nrobustness and generalization. We introduce EditLord, a code editing framework\nthat makes the code transformation steps explicit. Our key insight is to employ\na language model (LM) as an inductive learner to extract code editing rules\nfrom the training code pairs as concise meta-rule sets. Such rule sets will be\nmanifested for each training sample to augment them for finetuning or assist in\nprompting- and iterative-based code editing. EditLord outperforms the\nstate-of-the-art by an average of 22.7% in editing performance and 58.1% in\nrobustness while achieving 20.2% higher functional correctness across critical\nsoftware engineering and security applications, LM models, and editing modes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.15284v4", "cate": "cs.SE", "date": "2025-03-10", "updated": "2025-07-09"}
{"id": "2507.07794", "title": "Collaborative Human-Robot Surgery for Mandibular Angle Split Osteotomy: Optical Tracking based Approach", "authors": ["Zhe Han", "Huanyu Tian", "Tom Vercauteren", "Da Liu", "Changsheng Li", "Xingguang Duan"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07794v1", "summary": "Mandibular Angle Split Osteotomy (MASO) is a significant procedure in oral\nand maxillofacial surgery. Despite advances in technique and instrumentation,\nits success still relies heavily on the surgeon's experience. In this work, a\nhuman-robot collaborative system is proposed to perform MASO according to a\npreoperative plan and under guidance of a surgeon. A task decomposition\nmethodology is used to divide the collaborative surgical procedure into three\nsubtasks: (1) positional control and (2) orientation control, both led by the\nrobot for precise alignment; and (3) force-control, managed by surgeon to\nensure safety. Additionally, to achieve patient tracking without the need for a\nskull clamp, an optical tracking system (OTS) is utilized. Movement of the\npatient mandibular is measured with an optical-based tracker mounted on a\ndental occlusal splint. A registration method and Robot-OTS calibration method\nare introduced to achieve reliable navigation within our framework. The\nexperiments of drilling were conducted on the realistic phantom model, which\ndemonstrated that the average error between the planned and actual drilling\npoints is 1.85mm.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07794v1", "cate": "cs.RO", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07787", "title": "Measuring AI Alignment with Human Flourishing", "authors": ["Elizabeth Hilliard", "Akshaya Jagadeesh", "Alex Cook", "Steele Billings", "Nicholas Skytland", "Alicia Llewellyn", "Jackson Paull", "Nathan Paull", "Nolan Kurylo", "Keatra Nesbitt", "Robert Gruenewald", "Anthony Jantzi", "Omar Chavez"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07787v1", "summary": "This paper introduces the Flourishing AI Benchmark (FAI Benchmark), a novel\nevaluation framework that assesses AI alignment with human flourishing across\nseven dimensions: Character and Virtue, Close Social Relationships, Happiness\nand Life Satisfaction, Meaning and Purpose, Mental and Physical Health,\nFinancial and Material Stability, and Faith and Spirituality. Unlike\ntraditional benchmarks that focus on technical capabilities or harm prevention,\nthe FAI Benchmark measures AI performance on how effectively models contribute\nto the flourishing of a person across these dimensions. The benchmark evaluates\nhow effectively LLM AI systems align with current research models of holistic\nhuman well-being through a comprehensive methodology that incorporates 1,229\nobjective and subjective questions. Using specialized judge Large Language\nModels (LLMs) and cross-dimensional evaluation, the FAI Benchmark employs\ngeometric mean scoring to ensure balanced performance across all flourishing\ndimensions. Initial testing of 28 leading language models reveals that while\nsome models approach holistic alignment (with the highest-scoring models\nachieving 72/100), none are acceptably aligned across all dimensions,\nparticularly in Faith and Spirituality, Character and Virtue, and Meaning and\nPurpose. This research establishes a framework for developing AI systems that\nactively support human flourishing rather than merely avoiding harm, offering\nsignificant implications for AI development, ethics, and evaluation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07787v1", "cate": "cs.AI", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07197", "title": "Combining Pre-Trained Models for Enhanced Feature Representation in Reinforcement Learning", "authors": ["Elia Piccoli", "Malio Li", "Giacomo Carf√¨", "Vincenzo Lomonaco", "Davide Bacciu"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Published at 4th Conference on Lifelong Learning Agents (CoLLAs), 2025", "url": "http://arxiv.org/abs/2507.07197v1", "summary": "The recent focus and release of pre-trained models have been a key components\nto several advancements in many fields (e.g. Natural Language Processing and\nComputer Vision), as a matter of fact, pre-trained models learn disparate\nlatent embeddings sharing insightful representations. On the other hand,\nReinforcement Learning (RL) focuses on maximizing the cumulative reward\nobtained via agent's interaction with the environment. RL agents do not have\nany prior knowledge about the world, and they either learn from scratch an\nend-to-end mapping between the observation and action spaces or, in more recent\nworks, are paired with monolithic and computationally expensive Foundational\nModels. How to effectively combine and leverage the hidden information of\ndifferent pre-trained models simultaneously in RL is still an open and\nunderstudied question. In this work, we propose Weight Sharing Attention (WSA),\na new architecture to combine embeddings of multiple pre-trained models to\nshape an enriched state representation, balancing the tradeoff between\nefficiency and performance. We run an extensive comparison between several\ncombination modes showing that WSA obtains comparable performance on multiple\nAtari games compared to end-to-end models. Furthermore, we study the\ngeneralization capabilities of this approach and analyze how scaling the number\nof models influences agents' performance during and after training.", "comment": "Published at 4th Conference on Lifelong Learning Agents (CoLLAs),\n  2025", "pdf_url": "http://arxiv.org/pdf/2507.07197v1", "cate": "cs.LG", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.04278", "title": "DMER-Ranker: Learning to Rank Emotion Descriptions in the Absence of Ground Truth", "authors": ["Zheng Lian", "Licai Sun", "Haoyu Chen", "Zebang Cheng", "Fan Zhang", "Ziyu Jia", "Ziyang Ma", "Fei Ma", "Xiaojiang Peng", "Jianhua Tao"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.04278v2", "summary": "With the recent success of Large Language Models (LLMs), Descriptive\nMultimodal Emotion Recognition (DMER) has garnered increasing attention, which\naims to describe a person's emotional state using free-form natural language.\nUnlike traditional discriminative methods that rely on predefined emotion\ntaxonomies, DMER offers greater flexibility in emotional expression, enabling\nfine-grained and interpretable emotion representations. However, this free-form\nprediction paradigm exposes significant challenges in evaluation. Existing\nmethods either depend on ground-truth descriptions that require substantial\nmanual annotations or simplify the task by shifting the focus from evaluating\ndescriptions to evaluating emotion labels. However, this simplification\noverlooks critical aspects such as emotional temporal dynamics, intensity, and\nuncertainty. To address these limitations, we draw inspiration from\nReinforcement Learning from Human Feedback (RLHF) and propose DMER-Ranker, a\nnovel evaluation strategy that reformulates the traditional ``prediction-ground\ntruth'' comparison into the ``prediction-prediction'' comparison, eliminating\nthe need for ground-truth descriptions. We then employ the Bradley-Terry\nalgorithm to convert pairwise comparison results into model-level rankings.\nAdditionally, we explore the possibility of automatic preference prediction and\nintroduce DMER-Preference, the first preference dataset specifically designed\nfor human emotions. Our work advances the field of DMER and lays the foundation\nfor more intelligent human-computer interaction systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.04278v2", "cate": "cs.HC", "date": "2025-07-06", "updated": "2025-07-10"}
{"id": "2507.07448", "title": "Toolchain for Faster Iterations in Quantum Software Development", "authors": ["Otso Kinanen", "Andr√©s D. Mu√±oz-Moller", "Vlad Stirbu", "Tommi Mikkonen"], "categories": ["quant-ph", "cs.SE"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      arXiv admin note: text overlap with arXiv:2408.06756", "url": "http://arxiv.org/abs/2507.07448v1", "summary": "Quantum computing proposes a revolutionary paradigm that can radically\ntransform numerous scientific and industrial application domains. To realize\nthis promise, these new capabilities need software solutions that are able to\neffectively harness its power. However, developers may face significant\nchallenges when developing and executing quantum software due to the limited\navailability of quantum computer hardware, high computational demands of\nsimulating quantum computers on classical systems, and complicated technology\nstack to enable currently available accelerators into development environments.\nThese limitations make it difficult for the developer to create an efficient\nworkflow for quantum software development. In this paper, we investigate the\npotential of using remote computational capabilities in an efficient manner to\nimprove the workflow of quantum software developers, by lowering the barrier of\nmoving between local execution and computationally more efficient remote\nhardware and offering speedup in execution with simulator surroundings. The\ngoal is to allow the development of more complex circuits and to support an\niterative software development approach. In our experiment, with the solution\npresented in this paper, we have obtained up to 5 times faster circuit\nexecution runtime, and enabled qubit ranges from 21 to 29 qubits with a simple\nplug-and-play kernel for the Jupyter notebook.", "comment": "arXiv admin note: text overlap with arXiv:2408.06756", "pdf_url": "http://arxiv.org/pdf/2507.07448v1", "cate": "quant-ph", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07154", "title": "CL-Polyp: A Contrastive Learning-Enhanced Network for Accurate Polyp Segmentation", "authors": ["Desheng Li", "Chaoliang Liu", "Zhiyong Xiao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07154v1", "summary": "Accurate segmentation of polyps from colonoscopy images is crucial for the\nearly diagnosis and treatment of colorectal cancer. Most existing deep\nlearning-based polyp segmentation methods adopt an Encoder-Decoder\narchitecture, and some utilize multi-task frameworks that incorporate auxiliary\ntasks such as classification to enhance segmentation performance. However,\nthese approaches often require additional labeled data and rely on task\nsimilarity, which can limit their generalizability. To address these\nchallenges, we propose CL-Polyp, a contrastive learning-enhanced polyp\nsegmentation network. Our method leverages contrastive learning to improve the\nencoder's ability to extract discriminative features by contrasting positive\nand negative sample pairs derived from polyp images. This self-supervised\nstrategy enhances visual representation without requiring additional\nannotations. In addition, we introduce two lightweight and effective modules:\nthe Modified Atrous Spatial Pyramid Pooling (MASPP) module for better\nmulti-scale feature fusion, and the Channel Concatenate and Element Add (CA)\nmodule to fuse low-level and upsampled features for improved boundary\nreconstruction. Extensive experiments on five benchmark datasets-Kvasir-SEG,\nCVC-ClinicDB, CVC-ColonDB, CVC-300, and ETIS-demonstrate that CL-Polyp\nconsistently outperforms state-of-the-art methods. Specifically, it improves\nthe IoU metric by 0.011 and 0.020 on the Kvasir-SEG and CVC-ClinicDB datasets,\nrespectively, validating its effectiveness in clinical polyp segmentation\ntasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07154v1", "cate": "cs.CV", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2412.12187", "title": "Random walk based snapshot clustering for detecting community dynamics in temporal networks", "authors": ["Filip Bla≈°koviƒá", "Tim O. F. Conrad", "Stefan Klus", "Nata≈°a Djurdjevac Conrad"], "categories": ["cs.SI", "math.DS"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.12187v3", "summary": "The evolution of many dynamical systems that describe relationships or\ninteractions between objects can be effectively modeled by temporal networks,\nwhich are typically represented as a sequence of static network snapshots. In\nthis paper, we introduce a novel random walk-based approach that can identify\nclusters of time-snapshots in which network community structures are stable.\nThis allows us to detect significant structural shifts over time, such as the\nsplitting or merging of communities or their births and deaths. We also provide\na low-dimensional representation of entire snapshots, placing those with\nsimilar community structure close to each other in the feature space. To\nvalidate our approach, we develop an agent-based algorithm that generates\nsynthetic datasets with the desired characteristic properties, enabling\nthorough testing and benchmarking. We further demonstrate the effectiveness and\nbroad applicability of our technique by testing it on various social dynamics\nmodels and real-world datasets and comparing its performance to several\nstate-of-the-art algorithms. Our findings highlight the strength of our\napproach to correctly capture and analyze the dynamics of complex systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.12187v3", "cate": "cs.SI", "date": "2024-12-13", "updated": "2025-07-10"}
{"id": "2507.07118", "title": "Synergistic Localization and Sensing in MIMO-OFDM Systems via Mixed-Integer Bilevel Learning", "authors": ["Zelin Zhu", "Kai Yang", "Rui Zhang"], "categories": ["cs.NI", "cs.LG"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07118v1", "summary": "Wireless localization and sensing technologies are essential in modern\nwireless networks, supporting applications in smart cities, the Internet of\nThings (IoT), and autonomous systems. High-performance localization and sensing\nsystems are critical for both network efficiency and emerging intelligent\napplications. Integrating channel state information (CSI) with deep learning\nhas recently emerged as a promising solution. Recent works have leveraged the\nspatial diversity of multiple input multiple output (MIMO) systems and the\nfrequency granularity of orthogonal frequency division multiplexing (OFDM)\nwaveforms to improve spatial resolution. Nevertheless, the joint modeling of\nlocalization and sensing under the high-dimensional CSI characteristics of\nMIMO-OFDM systems remains insufficiently investigated. This work aims to\njointly model and optimize localization and sensing tasks to harness their\npotential synergy. We first formulate localization and sensing as a\nmixed-integer bilevel deep learning problem and then propose a novel stochastic\nproximal gradient-based mixed-integer bilevel optimization (SPG-MIBO)\nalgorithm. SPG-MIBO is well-suited for high-dimensional and large-scale\ndatasets, leveraging mini-batch training at each step for computational and\nmemory efficiency. The algorithm is also supported by theoretical convergence\nguarantees. Extensive experiments on multiple datasets validate its\neffectiveness and highlight the performance gains from joint localization and\nsensing optimization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07118v1", "cate": "cs.NI", "date": "2025-07-07", "updated": "2025-07-07"}
{"id": "2504.20310", "title": "A Cryptographic Perspective on Mitigation vs. Detection in Machine Learning", "authors": ["Greg Gluch", "Shafi Goldwasser"], "categories": ["cs.LG", "cs.AI", "cs.CR"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      28 pages", "url": "http://arxiv.org/abs/2504.20310v2", "summary": "In this paper, we initiate a cryptographically inspired theoretical study of\ndetection versus mitigation of adversarial inputs produced by attackers on\nMachine Learning algorithms during inference time.\n  We formally define defense by detection (DbD) and defense by mitigation\n(DbM). Our definitions come in the form of a 3-round protocol between two\nresource-bounded parties: a trainer/defender and an attacker. The attacker aims\nto produce inference-time inputs that fool the training algorithm. We define\ncorrectness, completeness, and soundness properties to capture successful\ndefense at inference time while not degrading (too much) the performance of the\nalgorithm on inputs from the training distribution.\n  We first show that achieving DbD and achieving DbM are equivalent for ML\nclassification tasks. Surprisingly, this is not the case for ML generative\nlearning tasks, where there are many possible correct outputs for each input.\nWe show a separation between DbD and DbM by exhibiting two generative learning\ntasks for which it is possible to defend by mitigation but it is provably\nimpossible to defend by detection. The mitigation phase uses significantly less\ncomputational resources than the initial training algorithm. In the first\nlearning task we consider sample complexity as the resource and in the second\nthe time complexity. The first result holds under the assumption that the\nIdentity-Based Fully Homomorphic Encryption (IB-FHE), publicly-verifiable\nzero-knowledge Succinct Non-Interactive Arguments of Knowledge (zk-SNARK), and\nStrongly Unforgeable Signatures exist. The second result assumes the existence\nof Non-Parallelizing Languages with Average-Case Hardness (NPL) and\nIncrementally-Verifiable Computation (IVC) and IB-FHE.", "comment": "28 pages", "pdf_url": "http://arxiv.org/pdf/2504.20310v2", "cate": "cs.LG", "date": "2025-04-28", "updated": "2025-07-10"}
{"id": "2507.07825", "title": "Beyond Robustness: Learning Unknown Dynamic Load Adaptation for Quadruped Locomotion on Rough Terrain", "authors": ["Leixin Chang", "Yuxuan Nai", "Hua Chen", "Liangjing Yang"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted to the 2025 IEEE International Conference on Robotics & Automation (ICRA). 8 pages, 8 figures", "url": "http://arxiv.org/abs/2507.07825v1", "summary": "Unknown dynamic load carrying is one important practical application for\nquadruped robots. Such a problem is non-trivial, posing three major challenges\nin quadruped locomotion control. First, how to model or represent the dynamics\nof the load in a generic manner. Second, how to make the robot capture the\ndynamics without any external sensing. Third, how to enable the robot to\ninteract with load handling the mutual effect and stabilizing the load. In this\nwork, we propose a general load modeling approach called load characteristics\nmodeling to capture the dynamics of the load. We integrate this proposed\nmodeling technique and leverage recent advances in Reinforcement Learning (RL)\nbased locomotion control to enable the robot to infer the dynamics of load\nmovement and interact with the load indirectly to stabilize it and realize the\nsim-to-real deployment to verify its effectiveness in real scenarios. We\nconduct extensive comparative simulation experiments to validate the\neffectiveness and superiority of our proposed method. Results show that our\nmethod outperforms other methods in sudden load resistance, load stabilizing\nand locomotion with heavy load on rough terrain.\n\\href{https://leixinjonaschang.github.io/leggedloadadapt.github.io/}{Project\nPage}.", "comment": "Accepted to the 2025 IEEE International Conference on Robotics &\n  Automation (ICRA). 8 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.07825v1", "cate": "cs.RO", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07818", "title": "MoSE: Skill-by-Skill Mixture-of-Expert Learning for Autonomous Driving", "authors": ["Lu Xu", "Jiaqian Yu", "Xiongfeng Peng", "Yiwei Chen", "Weiming Li", "Jaewook Yoo", "Sunghyun Chunag", "Dongwook Lee", "Daehyun Ji", "Chao Zhang"], "categories": ["cs.AI", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07818v1", "summary": "Recent studies show large language models (LLMs) and vision language models\n(VLMs) trained using web-scale data can empower end-to-end autonomous driving\nsystems for a better generalization and interpretation. Specifically, by\ndynamically routing inputs to specialized subsets of parameters, the\nMixture-of-Experts (MoE) technique enables general LLMs or VLMs to achieve\nsubstantial performance improvements while maintaining computational\nefficiency. However, general MoE models usually demands extensive training data\nand complex optimization. In this work, inspired by the learning process of\nhuman drivers, we propose a skill-oriented MoE, called MoSE, which mimics human\ndrivers' learning process and reasoning process, skill-by-skill and\nstep-by-step. We propose a skill-oriented routing mechanism that begins with\ndefining and annotating specific skills, enabling experts to identify the\nnecessary driving competencies for various scenarios and reasoning tasks,\nthereby facilitating skill-by-skill learning. Further align the driving process\nto multi-step planning in human reasoning and end-to-end driving models, we\nbuild a hierarchical skill dataset and pretrain the router to encourage the\nmodel to think step-by-step. Unlike multi-round dialogs, MoSE integrates\nvaluable auxiliary tasks (e.g.\\ description, reasoning, planning) in one single\nforward process without introducing any extra computational cost. With less\nthan 3B sparsely activated parameters, our model outperforms several 8B+\nparameters on CODA AD corner case reasoning task. Compared to existing methods\nbased on open-source models and data, our approach achieves state-of-the-art\nperformance with significantly reduced activated model size (at least by\n$62.5\\%$) with a single-turn conversation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07818v1", "cate": "cs.AI", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07207", "title": "Scale leads to compositional generalization", "authors": ["Florian Redhardt", "Yassir Akram", "Simon Schug"], "categories": ["cs.LG", "cs.NE"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Code available at this https URL", "url": "http://arxiv.org/abs/2507.07207v1", "summary": "Can neural networks systematically capture discrete, compositional task\nstructure despite their continuous, distributed nature? The impressive\ncapabilities of large-scale neural networks suggest that the answer to this\nquestion is yes. However, even for the most capable models, there are still\nfrequent failure cases that raise doubts about their compositionality. Here, we\nseek to understand what it takes for a standard neural network to generalize\nover tasks that share compositional structure. We find that simply scaling data\nand model size leads to compositional generalization. We show that this holds\nacross different task encodings as long as the training distribution\nsufficiently covers the task space. In line with this finding, we prove that\nstandard multilayer perceptrons can approximate a general class of\ncompositional task families to arbitrary precision using only a linear number\nof neurons with respect to the number of task modules. Finally, we uncover that\nif networks successfully compositionally generalize, the constituents of a task\ncan be linearly decoded from their hidden activations. We show that this metric\ncorrelates with failures of text-to-image generation models to compose known\nconcepts.", "comment": "Code available at https://github.com/smonsays/scale-compositionality", "pdf_url": "http://arxiv.org/pdf/2507.07207v1", "cate": "cs.LG", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2403.13318", "title": "A Survey of Machine Learning for Estimating Workload: Considering Unknown Tasks", "authors": ["Josh Bhagat Smith", "Julie A. Adams"], "categories": ["cs.RO", "cs.HC"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2403.13318v2", "summary": "Successful human-robot teaming will require robots to adapt autonomously to a\nhuman teammate's internal state, where a critical element of such adaptation is\nthe ability to estimate the human's workload in unknown situations. Existing\nworkload models use machine learning to model the relationship between\nphysiological signals and workload. These methods often struggle to generalize\nto unknown tasks, as the relative importance of various physiological signals\nchange significantly between tasks. Many of these changes constitute a\nmeaningful shift in the data's distribution, which violates a core assumption\nmade by the underlying machine learning approach. A survey of machine learning\ntechniques designed to overcome these challenges is presented, where common\ntechniques are evaluated using three criteria: portability, model complexity,\nand adaptability. These criteria are used to analyze each technique's\napplicability to estimating workload during unknown tasks in dynamic\nenvironments and guide future empirical experimentation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2403.13318v2", "cate": "cs.RO", "date": "2024-03-20", "updated": "2025-07-10"}
{"id": "2507.07649", "title": "ProvideQ: A Quantum Optimization Toolbox", "authors": ["Domenik Eichhorn", "Nick Poser", "Maximilian Schweikart", "Ina Schaefer"], "categories": ["quant-ph", "cs.SE"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      This paper was submitted and accepted at the IEEE QCE 2025", "url": "http://arxiv.org/abs/2507.07649v1", "summary": "Hybrid solvers for combinatorial optimization problems combine the advantages\nof classical and quantum computing to overcome difficult computational\nchallenges. Although their theoretical performance seems promising, their\npractical applicability is challenging due to the lack of a technological stack\nthat can seamlessly integrate quantum solutions with existing classical\noptimization frameworks. We tackle this challenge by introducing the ProvideQ\ntoolbox, a software tool that enables users to easily adapt and configure\nhybrid solvers via Meta-Solver strategies. A Meta-Solver strategy implements\ndecomposition techniques, which splits problems into classical and quantum\nsubroutines. The ProvideQ toolbox enables the interactive creation of such\ndecompositions via a Meta-Solver configuration tool. It combines\nwell-established classical optimization techniques with quantum circuits that\nare seamlessly executable on multiple backends. This paper introduces the\ntechnical details of the ProvideQ toolbox, explains its architecture, and\ndemonstrates possible applications for several real-world use cases. Our proof\nof concept shows that Meta-Solver strategies already enable the application of\nquantum subroutines today, however, more sophisticated hardware is required to\nmake their performance competitive.", "comment": "This paper was submitted and accepted at the IEEE QCE 2025", "pdf_url": "http://arxiv.org/pdf/2507.07649v1", "cate": "quant-ph", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07157", "title": "Interpretable EEG-to-Image Generation with Semantic Prompts", "authors": ["Arshak Rezvani", "Ali Akbari", "Kosar Sanjar Arani", "Maryam Mirian", "Emad Arasteh", "Martin J. McKeown"], "categories": ["cs.CV", "cs.LG", "eess.SP"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Actionable Interpretability Workshop (non-archival) at the 42 International Conference on Machine Learning", "url": "http://arxiv.org/abs/2507.07157v1", "summary": "Decoding visual experience from brain signals offers exciting possibilities\nfor neuroscience and interpretable AI. While EEG is accessible and temporally\nprecise, its limitations in spatial detail hinder image reconstruction. Our\nmodel bypasses direct EEG-to-image generation by aligning EEG signals with\nmultilevel semantic captions -- ranging from object-level to abstract themes --\ngenerated by a large language model. A transformer-based EEG encoder maps brain\nactivity to these captions through contrastive learning. During inference,\ncaption embeddings retrieved via projection heads condition a pretrained latent\ndiffusion model for image generation. This text-mediated framework yields\nstate-of-the-art visual decoding on the EEGCVPR dataset, with interpretable\nalignment to known neurocognitive pathways. Dominant EEG-caption associations\nreflected the importance of different semantic levels extracted from perceived\nimages. Saliency maps and t-SNE projections reveal semantic topography across\nthe scalp. Our model demonstrates how structured semantic mediation enables\ncognitively aligned visual decoding from EEG.", "comment": "Actionable Interpretability Workshop (non-archival) at the 42\n  International Conference on Machine Learning", "pdf_url": "http://arxiv.org/pdf/2507.07157v1", "cate": "cs.CV", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2307.10016", "title": "When Dialects Collide: How Socioeconomic Mixing Affects Language Use", "authors": ["Thomas Louf", "Jos√© J. Ramasco", "David S√°nchez", "M√°rton Karsai"], "categories": ["physics.soc-ph", "cs.CL", "cs.SI"], "primary_category": "Subjects:       Physics and Society (physics.soc-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2307.10016v2", "summary": "The socioeconomic background of people and how they use standard forms of\nlanguage are not independent, as demonstrated in various sociolinguistic\nstudies. However, the extent to which these correlations may be influenced by\nthe mixing of people from different socioeconomic classes remains relatively\nunexplored from a quantitative perspective. In this work we leverage geotagged\ntweets and transferable computational methods to map deviations from standard\nEnglish on a large scale, in seven thousand administrative areas of England and\nWales. We combine these data with high-resolution income maps to assign a proxy\nsocioeconomic indicator to home-located users. Strikingly, across eight\nmetropolitan areas we find a consistent pattern suggesting that the more\ndifferent socioeconomic classes mix, the less interdependent the frequency of\ntheir departures from standard grammar and their income become. Further, we\npropose an agent-based model of linguistic variety adoption that sheds light on\nthe mechanisms that produce the observations seen in the data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2307.10016v2", "cate": "physics.soc-ph", "date": "2023-07-19", "updated": "2025-07-10"}
{"id": "2507.07149", "title": "DAF: An Efficient End-to-End Dynamic Activation Framework for on-Device DNN Training", "authors": ["Renyuan Liu", "Yuyang Leng", "Kaiyan Liu", "Shaohan Hu", "Chun-Fu", "Chen", "Peijun Zhao", "Heechul Yun", "Shuochao Yao"], "categories": ["cs.NI", "cs.LG"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      Accepted to MobiSys 2025", "url": "http://arxiv.org/abs/2507.07149v1", "summary": "Recent advancements in on-device training for deep neural networks have\nunderscored the critical need for efficient activation compression to overcome\nthe memory constraints of mobile and edge devices. As activations dominate\nmemory usage during training and are essential for gradient computation,\ncompressing them without compromising accuracy remains a key research\nchallenge. While existing methods for dynamic activation quantization promise\ntheoretical memory savings, their practical deployment is impeded by\nsystem-level challenges such as computational overhead and memory\nfragmentation.\n  To address these challenges, we introduce DAF, a Dynamic Activation Framework\nthat enables scalable and efficient on-device training through system-level\noptimizations. DAF achieves both memory- and time-efficient dynamic\nquantization training by addressing key system bottlenecks. It develops hybrid\nreduction operations tailored to the memory hierarchies of mobile and edge\nSoCs, leverages collaborative CPU-GPU bit-packing for efficient dynamic\nquantization, and implements an importance-aware paging memory management\nscheme to reduce fragmentation and support dynamic memory adjustments.\n  These optimizations collectively enable DAF to achieve substantial memory\nsavings and speedup without compromising model training accuracy. Evaluations\non various deep learning models across embedded and mobile platforms\ndemonstrate up to a $22.9\\times$ reduction in memory usage and a $3.2\\times$\nspeedup, making DAF a scalable and practical solution for resource-constrained\nenvironments.", "comment": "Accepted to MobiSys 2025", "pdf_url": "http://arxiv.org/pdf/2507.07149v1", "cate": "cs.NI", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2506.04462", "title": "Watermarking Degrades Alignment in Language Models: Analysis and Mitigation", "authors": ["Apurv Verma", "NhatHai Phan", "Shubhendu Trivedi"], "categories": ["cs.CL", "cs.CR", "cs.LG", "I.2.7"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Published at the 1st Workshop on GenAI Watermarking, collocated with ICLR 2025. OpenReview: this https URL", "url": "http://arxiv.org/abs/2506.04462v2", "summary": "Watermarking techniques for large language models (LLMs) can significantly\nimpact output quality, yet their effects on truthfulness, safety, and\nhelpfulness remain critically underexamined. This paper presents a systematic\nanalysis of how two popular watermarking approaches-Gumbel and KGW-affect these\ncore alignment properties across four aligned LLMs. Our experiments reveal two\ndistinct degradation patterns: guard attenuation, where enhanced helpfulness\nundermines model safety, and guard amplification, where excessive caution\nreduces model helpfulness. These patterns emerge from watermark-induced shifts\nin token distribution, surfacing the fundamental tension that exists between\nalignment objectives.\n  To mitigate these degradations, we propose Alignment Resampling (AR), an\ninference-time sampling method that uses an external reward model to restore\nalignment. We establish a theoretical lower bound on the improvement in\nexpected reward score as the sample size is increased and empirically\ndemonstrate that sampling just 2-4 watermarked generations effectively recovers\nor surpasses baseline (unwatermarked) alignment scores. To overcome the limited\nresponse diversity of standard Gumbel watermarking, our modified implementation\nsacrifices strict distortion-freeness while maintaining robust detectability,\nensuring compatibility with AR. Experimental results confirm that AR\nsuccessfully recovers baseline alignment in both watermarking approaches, while\nmaintaining strong watermark detectability. This work reveals the critical\nbalance between watermark strength and model alignment, providing a simple\ninference-time solution to responsibly deploy watermarked LLMs in practice.", "comment": "Published at the 1st Workshop on GenAI Watermarking, collocated with\n  ICLR 2025. OpenReview: https://openreview.net/forum?id=SIBkIV48gF", "pdf_url": "http://arxiv.org/pdf/2506.04462v2", "cate": "cs.CL", "date": "2025-06-04", "updated": "2025-07-10"}
{"id": "2507.07845", "title": "Perceptual Distortions and Autonomous Representation Learning in a Minimal Robotic System", "authors": ["David Warutumo", "Ciira wa Maina"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      2 authors, 23 pages, 11 figures", "url": "http://arxiv.org/abs/2507.07845v1", "summary": "Autonomous agents, particularly in the field of robotics, rely on sensory\ninformation to perceive and navigate their environment. However, these sensory\ninputs are often imperfect, leading to distortions in the agent's internal\nrepresentation of the world. This paper investigates the nature of these\nperceptual distortions and how they influence autonomous representation\nlearning using a minimal robotic system. We utilize a simulated two-wheeled\nrobot equipped with distance sensors and a compass, operating within a simple\nsquare environment. Through analysis of the robot's sensor data during random\nexploration, we demonstrate how a distorted perceptual space emerges. Despite\nthese distortions, we identify emergent structures within the perceptual space\nthat correlate with the physical environment, revealing how the robot\nautonomously learns a structured representation for navigation without explicit\nspatial information. This work contributes to the understanding of embodied\ncognition, minimal agency, and the role of perception in self-generated\nnavigation strategies in artificial life.", "comment": "2 authors, 23 pages, 11 figures", "pdf_url": "http://arxiv.org/pdf/2507.07845v1", "cate": "cs.RO", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07820", "title": "AI Should Sense Better, Not Just Scale Bigger: Adaptive Sensing as a Paradigm Shift", "authors": ["Eunsu Baek", "Keondo Park", "Jeonggil Ko", "Min-hwan Oh", "Taesik Gong", "Hyung-Sin Kim"], "categories": ["cs.AI", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07820v1", "summary": "Current AI advances largely rely on scaling neural models and expanding\ntraining datasets to achieve generalization and robustness. Despite notable\nsuccesses, this paradigm incurs significant environmental, economic, and\nethical costs, limiting sustainability and equitable access. Inspired by\nbiological sensory systems, where adaptation occurs dynamically at the input\n(e.g., adjusting pupil size, refocusing vision)--we advocate for adaptive\nsensing as a necessary and foundational shift. Adaptive sensing proactively\nmodulates sensor parameters (e.g., exposure, sensitivity, multimodal\nconfigurations) at the input level, significantly mitigating covariate shifts\nand improving efficiency. Empirical evidence from recent studies demonstrates\nthat adaptive sensing enables small models (e.g., EfficientNet-B0) to surpass\nsubstantially larger models (e.g., OpenCLIP-H) trained with significantly more\ndata and compute. We (i) outline a roadmap for broadly integrating adaptive\nsensing into real-world applications spanning humanoid, healthcare, autonomous\nsystems, agriculture, and environmental monitoring, (ii) critically assess\ntechnical and ethical integration challenges, and (iii) propose targeted\nresearch directions, such as standardized benchmarks, real-time adaptive\nalgorithms, multimodal integration, and privacy-preserving methods.\nCollectively, these efforts aim to transition the AI community toward\nsustainable, robust, and equitable artificial intelligence systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07820v1", "cate": "cs.AI", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07222", "title": "Efficient Parametric SVD of Koopman Operator for Stochastic Dynamical Systems", "authors": ["Minchan Jeong", "J. Jon Ryu", "Se-Young Yun", "Gregory W. Wornell"], "categories": ["cs.LG", "cs.NA", "math.DS", "math.NA"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      28 pages, 4 figures. Under review for NeurIPS 2025. The first two authors contributed equally", "url": "http://arxiv.org/abs/2507.07222v1", "summary": "The Koopman operator provides a principled framework for analyzing nonlinear\ndynamical systems through linear operator theory. Recent advances in dynamic\nmode decomposition (DMD) have shown that trajectory data can be used to\nidentify dominant modes of a system in a data-driven manner. Building on this\nidea, deep learning methods such as VAMPnet and DPNet have been proposed to\nlearn the leading singular subspaces of the Koopman operator. However, these\nmethods require backpropagation through potentially numerically unstable\noperations on empirical second moment matrices, such as singular value\ndecomposition and matrix inversion, during objective computation, which can\nintroduce biased gradient estimates and hinder scalability to large systems. In\nthis work, we propose a scalable and conceptually simple method for learning\nthe top-k singular functions of the Koopman operator for stochastic dynamical\nsystems based on the idea of low-rank approximation. Our approach eliminates\nthe need for unstable linear algebraic operations and integrates easily into\nmodern deep learning pipelines. Empirical results demonstrate that the learned\nsingular subspaces are both reliable and effective for downstream tasks such as\neigen-analysis and multi-step prediction.", "comment": "28 pages, 4 figures. Under review for NeurIPS 2025. The first two\n  authors contributed equally", "pdf_url": "http://arxiv.org/pdf/2507.07222v1", "cate": "cs.LG", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2407.16803", "title": "C3T: Cross-modal Transfer Through Time for Sensor-based Human Activity Recognition", "authors": ["Abhi Kamboj", "Anh Duy Nguyen", "Minh N. Do"], "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG", "eess.SP"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2407.16803v4", "summary": "In order to unlock the potential of diverse sensors, we investigate a method\nto transfer knowledge between time-series modalities using a multimodal\n\\textit{temporal} representation space for Human Activity Recognition (HAR).\nSpecifically, we explore the setting where the modality used in testing has no\nlabeled data during training, which we refer to as Unsupervised Modality\nAdaptation (UMA). We categorize existing UMA approaches as Student-Teacher or\nContrastive Alignment methods. These methods typically compress continuous-time\ndata samples into single latent vectors during alignment, inhibiting their\nability to transfer temporal information through real-world temporal\ndistortions. To address this, we introduce Cross-modal Transfer Through Time\n(C3T), which preserves temporal information during alignment to handle dynamic\nsensor data better. C3T achieves this by aligning a set of temporal latent\nvectors across sensing modalities. Our extensive experiments on various\ncamera+IMU datasets demonstrate that C3T outperforms existing methods in UMA by\nat least 8% in accuracy and shows superior robustness to temporal distortions\nsuch as time-shift, misalignment, and dilation. Our findings suggest that C3T\nhas significant potential for developing generalizable models for time-series\nsensor data, opening new avenues for various multimodal applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2407.16803v4", "cate": "cs.CV", "date": "2024-07-23", "updated": "2025-07-10"}
{"id": "2504.14641", "title": "HLSTester: Efficient Testing of Behavioral Discrepancies with LLMs for High-Level Synthesis", "authors": ["Kangwei Xu", "Bing Li", "Grace Li Zhang", "Ulf Schlichtmann"], "categories": ["cs.SE", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      arXiv admin note: text overlap with arXiv:2407.03889", "url": "http://arxiv.org/abs/2504.14641v2", "summary": "In high-level synthesis (HLS), C/C++ programs with synthesis directives are\nused to generate circuits for FPGA implementations. However, hardware-specific\nand platform-dependent characteristics in these implementations can introduce\nbehavioral discrepancies between the original C/C++ programs and the circuits\nafter high-level synthesis. Existing methods for testing behavioral\ndiscrepancies in HLS are still immature, and the testing workflow requires\nsignificant human efforts. To address this challenge, we propose HLSTester, a\nlarge language model (LLM) aided testing framework that efficiently detects\nbehavioral discrepancies in HLS. To mitigate hallucinations in LLMs and enhance\nprompt quality, the testbenches for original C/C++ programs are leveraged to\nguide LLMs in generating HLS-compatible testbenches, effectively eliminating\ncertain traditional C/C++ constructs that are incompatible with HLS tools. Key\nvariables are pinpointed through a backward slicing technique in both C/C++ and\nHLS programs to monitor their runtime spectra, enabling an in-depth analysis of\nthe discrepancy symptoms. To reduce test time, a testing input generation\nmechanism is introduced to integrate dynamic mutation with insights from an\nLLM-based progressive reasoning chain. In addition, repetitive hardware testing\nis skipped by a redundancy-aware filtering technique for the generated test\ninputs. Experimental results demonstrate that the proposed LLM-aided testing\nframework significantly accelerates the testing workflow while achieving higher\ntestbench simulation pass rates compared with the traditional method and the\ndirect use of LLMs on the same HLS programs.", "comment": "arXiv admin note: text overlap with arXiv:2407.03889", "pdf_url": "http://arxiv.org/pdf/2504.14641v2", "cate": "cs.SE", "date": "2025-04-20", "updated": "2025-07-09"}
{"id": "2507.07202", "title": "A Survey on Long-Video Storytelling Generation: Architectures, Consistency, and Cinematic Quality", "authors": ["Mohamed Elmoghany", "Ryan Rossi", "Seunghyun Yoon", "Subhojyoti Mukherjee", "Eslam Bakr", "Puneet Mathur", "Gang Wu", "Viet Dac Lai", "Nedim Lipka", "Ruiyi Zhang", "Varun Manjunatha", "Chien Nguyen", "Daksh Dangi", "Abel Salinas", "Mohammad Taesiri", "Hongjie Chen", "Xiaolei Huang", "Joe Barrow", "Nesreen Ahmed", "Hoda Eldardiry", "Namyong Park", "Yu Wang", "Jaemin Cho", "Anh Totti Nguyen", "Zhengzhong Tu", "Thien Nguyen", "Dinesh Manocha", "Mohamed Elhoseiny", "Franck Dernoncourt"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07202v1", "summary": "Despite the significant progress that has been made in video generative\nmodels, existing state-of-the-art methods can only produce videos lasting 5-16\nseconds, often labeled \"long-form videos\". Furthermore, videos exceeding 16\nseconds struggle to maintain consistent character appearances and scene layouts\nthroughout the narrative. In particular, multi-subject long videos still fail\nto preserve character consistency and motion coherence. While some methods can\ngenerate videos up to 150 seconds long, they often suffer from frame redundancy\nand low temporal diversity. Recent work has attempted to produce long-form\nvideos featuring multiple characters, narrative coherence, and high-fidelity\ndetail. We comprehensively studied 32 papers on video generation to identify\nkey architectural components and training strategies that consistently yield\nthese qualities. We also construct a comprehensive novel taxonomy of existing\nmethods and present comparative tables that categorize papers by their\narchitectural designs and performance characteristics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07202v1", "cate": "cs.CV", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07437", "title": "PHandover: Parallel Handover in Mobile Satellite Network", "authors": ["Jiasheng Wu", "Shaojie Su", "Wenjun Zhu", "Xiong Wang", "Jingjing Zhang", "Xingqiu He", "Yue Gao"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      14 pages, 14 figures", "url": "http://arxiv.org/abs/2507.07437v1", "summary": "The construction of Low Earth Orbit (LEO) satellite constellations has\nrecently attracted tremendous attention from both academia and industry. The 5G\nand 6G standards have identified LEO satellite networks as a key component of\nfuture mobile networks. However, due to the high-speed movement of satellites,\nground terminals often experience frequent and high-latency handovers, which\nsignificantly deteriorate the performance of latency-sensitive applications. To\naddress this challenge, we propose a parallel handover mechanism for mobile\nsatellite networks that can considerably reduce handover latency. The main idea\nis to employ plan-based handovers instead of measurement-based handovers to\navoid interactions between the access and core networks, thereby eliminating\nthe significant time overhead associated with traditional handover procedures.\nSpecifically, we introduce a novel network function named the Satellite\nSynchronized Function (SSF), which is designed to be fully compliant with the\nstandard 5G core network. In addition, we propose a machine learning model for\nsignal strength prediction, coupled with an efficient handover scheduling\nalgorithm. We have conducted extensive experiments, and the results demonstrate\nthat our proposed handover scheme can reduce handover latency by 21\\times\ncompared to the standard NTN handover scheme and two other existing handover\napproaches, along with significant improvements in network stability and\nuser-level performance.", "comment": "14 pages, 14 figures", "pdf_url": "http://arxiv.org/pdf/2507.07437v1", "cate": "cs.NI", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07846", "title": "ROS Help Desk: GenAI Powered, User-Centric Framework for ROS Error Diagnosis and Debugging", "authors": ["Kavindie Katuwandeniya", "Samith Rajapaksha Jayasekara Widhanapathirana"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07846v1", "summary": "As the robotics systems increasingly integrate into daily life, from smart\nhome assistants to the new-wave of industrial automation systems (Industry\n4.0), there's an increasing need to bridge the gap between complex robotic\nsystems and everyday users. The Robot Operating System (ROS) is a flexible\nframework often utilised in writing robot software, providing tools and\nlibraries for building complex robotic systems. However, ROS's distributed\narchitecture and technical messaging system create barriers for understanding\nrobot status and diagnosing errors. This gap can lead to extended maintenance\ndowntimes, as users with limited ROS knowledge may struggle to quickly diagnose\nand resolve system issues. Moreover, this deficit in expertise often delays\nproactive maintenance and troubleshooting, further increasing the frequency and\nduration of system interruptions. ROS Help Desk provides intuitive error\nexplanations and debugging support, dynamically customized to users of varying\nexpertise levels. It features user-centric debugging tools that simplify error\ndiagnosis, implements proactive error detection capabilities to reduce\ndowntime, and integrates multimodal data processing for comprehensive system\nstate understanding across multi-sensor data (e.g., lidar, RGB). Testing\nqualitatively and quantitatively with artificially induced errors demonstrates\nthe system's ability to proactively and accurately diagnose problems,\nultimately reducing maintenance time and fostering more effective human-robot\ncollaboration.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07846v1", "cate": "cs.RO", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07857", "title": "Searching for actual causes: Approximate algorithms with adjustable precision", "authors": ["Samuel Reyd", "Ada Diaconescu", "Jean-Louis Dessalles"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07857v1", "summary": "Causality has gained popularity in recent years. It has helped improve the\nperformance, reliability, and interpretability of machine learning models.\nHowever, recent literature on explainable artificial intelligence (XAI) has\nfaced criticism. The classical XAI and causality literature focuses on\nunderstanding which factors contribute to which consequences. While such\nknowledge is valuable for researchers and engineers, it is not what non-expert\nusers expect as explanations. Instead, these users often await facts that cause\nthe target consequences, i.e., actual causes. Formalizing this notion is still\nan open problem. Additionally, identifying actual causes is reportedly an\nNP-complete problem, and there are too few practical solutions to approximate\nformal definitions. We propose a set of algorithms to identify actual causes\nwith a polynomial complexity and an adjustable level of precision and\nexhaustiveness. Our experiments indicate that the algorithms (1) identify\ncauses for different categories of systems that are not handled by existing\napproaches (i.e., non-boolean, black-box, and stochastic systems), (2) can be\nadjusted to gain more precision and exhaustiveness with more computation time.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07857v1", "cate": "cs.AI", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07236", "title": "An Information-Theoretic Perspective on Multi-LLM Uncertainty Estimation", "authors": ["Maya Kruse", "Majid Afshar", "Saksham Khatwani", "Anoop Mayampurath", "Guanhua Chen", "Yanjun Gao"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Under review", "url": "http://arxiv.org/abs/2507.07236v1", "summary": "Large language models (LLMs) often behave inconsistently across inputs,\nindicating uncertainty and motivating the need for its quantification in\nhigh-stakes settings. Prior work on calibration and uncertainty quantification\noften focuses on individual models, overlooking the potential of model\ndiversity. We hypothesize that LLMs make complementary predictions due to\ndifferences in training and the Zipfian nature of language, and that\naggregating their outputs leads to more reliable uncertainty estimates. To\nleverage this, we propose MUSE (Multi-LLM Uncertainty via Subset Ensembles), a\nsimple information-theoretic method that uses Jensen-Shannon Divergence to\nidentify and aggregate well-calibrated subsets of LLMs. Experiments on binary\nprediction tasks demonstrate improved calibration and predictive performance\ncompared to single-model and naive ensemble baselines.", "comment": "Under review", "pdf_url": "http://arxiv.org/pdf/2507.07236v1", "cate": "cs.LG", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2409.18813", "title": "EyeTrAES: Fine-grained, Low-Latency Eye Tracking via Adaptive Event Slicing", "authors": ["Argha Sen", "Nuwan Bandara", "Ila Gokarn", "Thivya Kandappu", "Archan Misra"], "categories": ["cs.CV", "cs.HC"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      32 pages,15 figures,", "url": "http://arxiv.org/abs/2409.18813v3", "summary": "Eye-tracking technology has gained significant attention in recent years due\nto its wide range of applications in human-computer interaction, virtual and\naugmented reality, and wearable health. Traditional RGB camera-based\neye-tracking systems often struggle with poor temporal resolution and\ncomputational constraints, limiting their effectiveness in capturing rapid eye\nmovements. To address these limitations, we propose EyeTrAES, a novel approach\nusing neuromorphic event cameras for high-fidelity tracking of natural\npupillary movement that shows significant kinematic variance. One of EyeTrAES's\nhighlights is the use of a novel adaptive windowing/slicing algorithm that\nensures just the right amount of descriptive asynchronous event data\naccumulation within an event frame, across a wide range of eye movement\npatterns. EyeTrAES then applies lightweight image processing functions over\naccumulated event frames from just a single eye to perform pupil segmentation\nand tracking. We show that these methods boost pupil tracking fidelity by 6+%,\nachieving IoU~=92%, while incurring at least 3x lower latency than competing\npure event-based eye tracking alternatives [38]. We additionally demonstrate\nthat the microscopic pupillary motion captured by EyeTrAES exhibits distinctive\nvariations across individuals and can thus serve as a biometric fingerprint.\nFor robust user authentication, we train a lightweight per-user Random Forest\nclassifier using a novel feature vector of short-term pupillary kinematics,\ncomprising a sliding window of pupil (location, velocity, acceleration)\ntriples. Experimental studies with two different datasets demonstrate that the\nEyeTrAES-based authentication technique can simultaneously achieve high\nauthentication accuracy (~=0.82) and low processing latency (~=12ms), and\nsignificantly outperform multiple state-of-the-art competitive baselines.", "comment": "32 pages,15 figures,", "pdf_url": "http://arxiv.org/pdf/2409.18813v3", "cate": "cs.CV", "date": "2024-09-27", "updated": "2025-07-10"}
{"id": "2507.02137", "title": "Towards Trustworthy Sentiment Analysis in Software Engineering: Dataset Characteristics and Tool Selection", "authors": ["Martin Obaidi", "Marc Herrmann", "Jil Kl√ºnder", "Kurt Schneider"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      This paper has been accepted at the RETRAI workshop of the 33rd IEEE International Requirements Engineering Workshop (REW 2025)", "url": "http://arxiv.org/abs/2507.02137v2", "summary": "Software development relies heavily on text-based communication, making\nsentiment analysis a valuable tool for understanding team dynamics and\nsupporting trustworthy AI-driven analytics in requirements engineering.\nHowever, existing sentiment analysis tools often perform inconsistently across\ndatasets from different platforms, due to variations in communication style and\ncontent.\n  In this study, we analyze linguistic and statistical features of 10 developer\ncommunication datasets from five platforms and evaluate the performance of 14\nsentiment analysis tools. Based on these results, we propose a mapping approach\nand questionnaire that recommends suitable sentiment analysis tools for new\ndatasets, using their characteristic features as input.\n  Our results show that dataset characteristics can be leveraged to improve\ntool selection, as platforms differ substantially in both linguistic and\nstatistical properties. While transformer-based models such as SetFit and\nRoBERTa consistently achieve strong results, tool effectiveness remains\ncontext-dependent. Our approach supports researchers and practitioners in\nselecting trustworthy tools for sentiment analysis in software engineering,\nwhile highlighting the need for ongoing evaluation as communication contexts\nevolve.", "comment": "This paper has been accepted at the RETRAI workshop of the 33rd IEEE\n  International Requirements Engineering Workshop (REW 2025)", "pdf_url": "http://arxiv.org/pdf/2507.02137v2", "cate": "cs.SE", "date": "2025-07-02", "updated": "2025-07-09"}
{"id": "2507.07230", "title": "Colors See Colors Ignore: Clothes Changing ReID with Color Disentanglement", "authors": ["Priyank Pathak", "Yogesh S. Rawat"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV'25 paper", "url": "http://arxiv.org/abs/2507.07230v1", "summary": "Clothes-Changing Re-Identification (CC-ReID) aims to recognize individuals\nacross different locations and times, irrespective of clothing. Existing\nmethods often rely on additional models or annotations to learn robust,\nclothing-invariant features, making them resource-intensive. In contrast, we\nexplore the use of color - specifically foreground and background colors - as a\nlightweight, annotation-free proxy for mitigating appearance bias in ReID\nmodels. We propose Colors See, Colors Ignore (CSCI), an RGB-only method that\nleverages color information directly from raw images or video frames. CSCI\nefficiently captures color-related appearance bias ('Color See') while\ndisentangling it from identity-relevant ReID features ('Color Ignore'). To\nachieve this, we introduce S2A self-attention, a novel self-attention to\nprevent information leak between color and identity cues within the feature\nspace. Our analysis shows a strong correspondence between learned color\nembeddings and clothing attributes, validating color as an effective proxy when\nexplicit clothing labels are unavailable. We demonstrate the effectiveness of\nCSCI on both image and video ReID with extensive experiments on four CC-ReID\ndatasets. We improve the baseline by Top-1 2.9% on LTCC and 5.0% on PRCC for\nimage-based ReID, and 1.0% on CCVID and 2.5% on MeVID for video-based ReID\nwithout relying on additional supervision. Our results highlight the potential\nof color as a cost-effective solution for addressing appearance bias in\nCC-ReID. Github: https://github.com/ppriyank/ICCV-CSCI-Person-ReID.", "comment": "ICCV'25 paper", "pdf_url": "http://arxiv.org/pdf/2507.07230v1", "cate": "cs.CV", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07481", "title": "Energy Transfer and Data Collection from Batteryless Sensors in Low-altitude Wireless Networks", "authors": ["Wen Zhang", "Aimin Wang", "Jiahui Li", "Geng Sun", "Jiacheng Wang", "Weijie Yuan", "Dusit Niyato"], "categories": ["cs.NI", "eess.SP"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07481v1", "summary": "The integration of wireless power transfer (WPT) with Internet of Things\n(IoT) offers promising solutions for sensing applications, but faces\nsignificant challenges when deployed in hard-to-access areas such as\nhigh-temperature environments. In such extreme conditions, traditional fixed\nWPT infrastructure cannot be safely installed, and batteries rapidly degrade\ndue to hardware failures. In this paper, we propose an uncrewed aerial vehicle\n(UAV)-assisted data collection and WPT framework for batteryless sensor (BLS)\nnetworks deployed in these challenging environments. Specifically, we consider\na practical scenario where a UAV first transfers energy to BLS nodes via WPT,\nenabling these nodes to subsequently transmit their collected data to the UAV\nthrough orthogonal frequency-division multiple access (OFDMA). Then, we\nformulate a multi-objective optimization problem that aims to maximize the fair\ndata collection volume while minimizing the UAV energy consumption through\njoint optimization of transmit power allocation and flight trajectory planning.\nDue to the non-convex nature and dynamic characteristics of this problem,\nconventional optimization methods prove inadequate. To address these\nchallenges, we propose an enhanced soft actor-critic algorithm with\nparameter-free attention, prioritized experience replay, and value-based reward\ncentering (SAC-PPV), thereby improving the exploration efficiency and learning\nstability of the algorithm in complex WPT scenarios. Simulation results\ndemonstrate that the proposed approach consistently outperforms benchmark\nalgorithms under various network configurations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07481v1", "cate": "cs.NI", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07872", "title": "Improving AEBS Validation Through Objective Intervention Classification Leveraging the Prediction Divergence Principle", "authors": ["Daniel Betschinske", "Steven Peters"], "categories": ["cs.RO", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      This work has been accepted for publication at the 2025 IEEE International Automated Vehicle Validation Conference (IAVVC)", "url": "http://arxiv.org/abs/2507.07872v1", "summary": "The safety validation of automatic emergency braking system (AEBS) requires\naccurately distinguishing between false positive (FP) and true positive (TP)\nsystem activations. While simulations allow straightforward differentiation by\ncomparing scenarios with and without interventions, analyzing activations from\nopen-loop resimulations - such as those from field operational testing (FOT) -\nis more complex. This complexity arises from scenario parameter uncertainty and\nthe influence of driver interventions in the recorded data. Human labeling is\nfrequently used to address these challenges, relying on subjective assessments\nof intervention necessity or situational criticality, potentially introducing\nbiases and limitations. This work proposes a rule-based classification approach\nleveraging the Prediction Divergence Principle (PDP) to address those issues.\nApplied to a simplified AEBS, the proposed method reveals key strengths,\nlimitations, and system requirements for effective implementation. The findings\nsuggest that combining this approach with human labeling may enhance the\ntransparency and consistency of classification, thereby improving the overall\nvalidation process. While the rule set for classification derived in this work\nadopts a conservative approach, the paper outlines future directions for\nrefinement and broader applicability. Finally, this work highlights the\npotential of such methods to complement existing practices, paving the way for\nmore reliable and reproducible AEBS validation frameworks.", "comment": "This work has been accepted for publication at the 2025 IEEE\n  International Automated Vehicle Validation Conference (IAVVC)", "pdf_url": "http://arxiv.org/pdf/2507.07872v1", "cate": "cs.RO", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07893", "title": "An Integrated Framework of Prompt Engineering and Multidimensional Knowledge Graphs for Legal Dispute Analysis", "authors": ["Mingda Zhang", "Na Zhao", "Jianglong Qing", "Qing xu", "Kaiwen Pan", "Ting luo"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      15 pages,3 figures", "url": "http://arxiv.org/abs/2507.07893v1", "summary": "The rapid development of artificial intelligence has positioned large\nlanguage models as fundamental components of intelligent legal systems.\nHowever, these models face significant limitations in legal dispute analysis,\nincluding insufficient legal knowledge representation, limited concept\nunderstanding, and reasoning deficiencies. This research proposes an enhanced\nframework integrating prompt engineering with multidimensional knowledge\ngraphs. The framework introduces a three-stage hierarchical prompt structure\ncomprising task definition, knowledge background, and reasoning guidance,\nsupplemented by legal-specific reasoning templates and dynamic optimization\nmechanisms. A three-layer knowledge graph architecture is constructed with\nlegal classification ontology, representation, and instance layers. Four\ncomplementary methods enable precise legal concept retrieval: direct legal norm\ncode matching, domain-specific semantic vector similarity, ontology-based path\nreasoning, and specialized lexical segmentation. These components integrate\nwith web search technology to establish a knowledge-enhanced framework for\nlegal decision-making. Experimental results demonstrate significant performance\nimprovements in legal dispute analysis, enabling accurate legal application\nanalysis for complex cases while exhibiting nuanced understanding of judicial\ndecision-making logic, providing a novel technical approach for implementing\nintelligent legal assistance systems.", "comment": "15 pages,3 figures", "pdf_url": "http://arxiv.org/pdf/2507.07893v1", "cate": "cs.AI", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07237", "title": "Towards Robust Surrogate Models: Benchmarking Machine Learning Approaches to Expediting Phase Field Simulations of Brittle Fracture", "authors": ["Erfan Hamdi", "Emma Lejeune"], "categories": ["cs.LG", "physics.data-an", "74R10, 74B20, 74A40, 68T07", "J.2; I.6.3; I.6.5"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      29 pages, 13 figures", "url": "http://arxiv.org/abs/2507.07237v1", "summary": "Data driven approaches have the potential to make modeling complex, nonlinear\nphysical phenomena significantly more computationally tractable. For example,\ncomputational modeling of fracture is a core challenge where machine learning\ntechniques have the potential to provide a much needed speedup that would\nenable progress in areas such as mutli-scale modeling and uncertainty\nquantification. Currently, phase field modeling (PFM) of fracture is one such\napproach that offers a convenient variational formulation to model crack\nnucleation, branching and propagation. To date, machine learning techniques\nhave shown promise in approximating PFM simulations. However, most studies rely\non overly simple benchmarks that do not reflect the true complexity of the\nfracture processes where PFM excels as a method. To address this gap, we\nintroduce a challenging dataset based on PFM simulations designed to benchmark\nand advance ML methods for fracture modeling. This dataset includes three\nenergy decomposition methods, two boundary conditions, and 1,000 random initial\ncrack configurations for a total of 6,000 simulations. Each sample contains 100\ntime steps capturing the temporal evolution of the crack field. Alongside this\ndataset, we also implement and evaluate Physics Informed Neural Networks\n(PINN), Fourier Neural Operators (FNO) and UNet models as baselines, and\nexplore the impact of ensembling strategies on prediction accuracy. With this\ncombination of our dataset and baseline models drawn from the literature we aim\nto provide a standardized and challenging benchmark for evaluating machine\nlearning approaches to solid mechanics. Our results highlight both the promise\nand limitations of popular current models, and demonstrate the utility of this\ndataset as a testbed for advancing machine learning in fracture mechanics\nresearch.", "comment": "29 pages, 13 figures", "pdf_url": "http://arxiv.org/pdf/2507.07237v1", "cate": "cs.LG", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2503.23760", "title": "Towards a cognitive architecture to enable natural language interaction in co-constructive task learning", "authors": ["Manuel Scheibl", "Birte Richter", "Alissa M√ºller", "Michael Beetz", "Britta Wrede"], "categories": ["cs.RO", "cs.CL", "cs.HC"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 pages, 5 figures, accepted at: IEEE RO-MAN 2025 Conference", "url": "http://arxiv.org/abs/2503.23760v2", "summary": "This research addresses the question, which characteristics a cognitive\narchitecture must have to leverage the benefits of natural language in\nCo-Constructive Task Learning (CCTL). To provide context, we first discuss\nInteractive Task Learning (ITL), the mechanisms of the human memory system, and\nthe significance of natural language and multi-modality. Next, we examine the\ncurrent state of cognitive architectures, analyzing their capabilities to\ninform a concept of CCTL grounded in multiple sources. We then integrate\ninsights from various research domains to develop a unified framework. Finally,\nwe conclude by identifying the remaining challenges and requirements necessary\nto achieve CCTL in Human-Robot Interaction (HRI).", "comment": "8 pages, 5 figures, accepted at: IEEE RO-MAN 2025 Conference", "pdf_url": "http://arxiv.org/pdf/2503.23760v2", "cate": "cs.RO", "date": "2025-03-31", "updated": "2025-07-10"}
{"id": "2507.05270", "title": "Open Source, Hidden Costs: A Systematic Literature Review on OSS License Management", "authors": ["Boyuan Li", "Chengwei Liu", "Lingling Fan", "Sen Chen", "Zhenlin Zhang", "Zheli Liu"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.05270v2", "summary": "Integrating third-party software components is a common practice in modern\nsoftware development, offering significant advantages in terms of efficiency\nand innovation. However, this practice is fraught with risks related to\nsoftware licensing. A lack of understanding may lead to disputes, which can\npose serious legal and operational challenges. To these ends, both academia and\nindustry have conducted various investigations and proposed solutions and tools\nto deal with these challenges. However, significant limitations still remain.\nMoreover, the rapid evolution of open-source software (OSS) licenses, as well\nas the rapidly incorporated generative software engineering techniques, such as\nlarge language models for code (CodeLLMs), are placing greater demands on the\nsystematic management of software license risks. To unveil the severe\nchallenges and explore possible future directions, we conduct the first\nsystematic literature review (SLR) on 80 carefully selected OSS license-related\npapers, classifying existing research into three key categories, i.e., license\nidentification, license risk assessment, and license risk mitigation. Based on\nthese, we discuss challenges in existing solutions, conclude the opportunities\nto shed light on future research directions and offer practical recommendations\nfor practitioners. We hope this thorough review will help bridge the gaps\nbetween academia and industry and accelerate the ecosystem-wide governance of\nlegitimate software risks within the software engineering community.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.05270v2", "cate": "cs.SE", "date": "2025-07-03", "updated": "2025-07-10"}
{"id": "2507.07242", "title": "Automated Video Segmentation Machine Learning Pipeline", "authors": ["Johannes Merz", "Lucien Fostier"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07242v1", "summary": "Visual effects (VFX) production often struggles with slow, resource-intensive\nmask generation. This paper presents an automated video segmentation pipeline\nthat creates temporally consistent instance masks. It employs machine learning\nfor: (1) flexible object detection via text prompts, (2) refined per-frame\nimage segmentation and (3) robust video tracking to ensure temporal stability.\nDeployed using containerization and leveraging a structured output format, the\npipeline was quickly adopted by our artists. It significantly reduces manual\neffort, speeds up the creation of preliminary composites, and provides\ncomprehensive segmentation data, thereby enhancing overall VFX production\nefficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07242v1", "cate": "cs.CV", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07535", "title": "A Fragmentation-Aware Adaptive Bilevel Search Framework for Service Mapping in Computing Power Networks", "authors": ["Jingzhao Xie", "Zhenglian Li", "Gang Sun", "Long Luo", "Hongfang Yu", "Dusit Niyato"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      This work has been submitted to the IEEE for possible publication", "url": "http://arxiv.org/abs/2507.07535v1", "summary": "Computing Power Network (CPN) unifies wide-area computing resources through\ncoordinated network control, while cloud-native abstractions enable flexible\nresource orchestration and on-demand service provisioning atop the elastic\ninfrastructure CPN provides. However, current approaches fall short of fully\nintegrating computing resources via network-enabled coordination as envisioned\nby CPN. In particular, optimally mapping services to an underlying\ninfrastructure to maximize resource efficiency and service satisfaction remains\nchallenging. To overcome this challenge, we formally define the service mapping\nproblem in CPN, establish its theoretical intractability, and identify key\nchallenges in practical optimization. We propose Adaptive Bilevel Search (ABS),\na modular framework featuring (1) graph partitioning-based reformulation to\ncapture variable coupling, (2) a bilevel optimization architecture for\nefficient global exploration with local optimality guarantees, and (3)\nfragmentation-aware evaluation for global performance guidance. Implemented\nusing distributed particle swarm optimization, ABS is extensively evaluated\nacross diverse CPN scenarios, consistently outperforming existing approaches.\nNotably, in complex scenarios, ABS achieves up to 73.2% higher computing\nresource utilization and a 60.2% higher service acceptance ratio compared to\nthe best-performing baseline.", "comment": "This work has been submitted to the IEEE for possible publication", "pdf_url": "http://arxiv.org/pdf/2507.07535v1", "cate": "cs.NI", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07557", "title": "Sparse Signal Recovery From Quadratic Systems with Full-Rank Matrices", "authors": ["Jinming Wen", "Yi Hu", "Meng Huang"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07557v1", "summary": "In signal processing and data recovery, reconstructing a signal from\nquadratic measurements poses a significant challenge, particularly in\nhigh-dimensional settings where measurements $m$ is far less than the signal\ndimension $n$ (i.e., $m \\ll n$). This paper addresses this problem by\nexploiting signal sparsity. Using tools from algebraic geometry, we derive\ntheoretical recovery guarantees for sparse quadratic systems, showing that\n$m\\ge 2s$ (real case) and $m\\ge 4s-2$ (complex case) generic measurements\nsuffice to uniquely recover all $s$-sparse signals. Under a Gaussian\nmeasurement model, we propose a novel two-stage Sparse Gauss-Newton (SGN)\nalgorithm. The first stage employs a support-restricted spectral\ninitialization, yielding an accurate initial estimate with $m=O(s^2\\log{n})$\nmeasurements. The second stage refines this estimate via an iterative\nhard-thresholding Gauss-Newton method, achieving quadratic convergence to the\ntrue signal within finitely many iterations when $m\\ge O(s\\log{n})$. Compared\nto existing second-order methods, our algorithm achieves near-optimal sampling\ncomplexity for the refinement stage without requiring resampling. Numerical\nexperiments indicate that SGN significantly outperforms state-of-the-art\nalgorithms in both accuracy and computational efficiency. In particular, (1)\nwhen sparsity level $s$ is high, compared with existing algorithms, SGN can\nachieve the same success rate with fewer measurements. (2) SGN converges with\nonly about $1/10$ iterations of the best existing algorithm and reach lower\nrelative error.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07557v1", "cate": "cs.IT", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07980", "title": "UniTac: Whole-Robot Touch Sensing Without Tactile Sensors", "authors": ["Wanjia Fu", "Hongyu Li", "Ivy X. He", "Stefanie Tellex", "Srinath Sridhar"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07980v1", "summary": "Robots can better interact with humans and unstructured environments through\ntouch sensing. However, most commercial robots are not equipped with tactile\nskins, making it challenging to achieve even basic touch-sensing functions,\nsuch as contact localization. We present UniTac, a data-driven whole-body\ntouch-sensing approach that uses only proprioceptive joint sensors and does not\nrequire the installation of additional sensors. Our approach enables a robot\nequipped solely with joint sensors to localize contacts. Our goal is to\ndemocratize touch sensing and provide an off-the-shelf tool for HRI researchers\nto provide their robots with touch-sensing capabilities. We validate our\napproach on two platforms: the Franka robot arm and the Spot quadruped. On\nFranka, we can localize contact to within 8.0 centimeters, and on Spot, we can\nlocalize to within 7.2 centimeters at around 2,000 Hz on an RTX 3090 GPU\nwithout adding any additional sensors to the robot. Project website:\nhttps://ivl.cs.brown.edu/research/unitac.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07980v1", "cate": "cs.RO", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07931", "title": "Meek Models Shall Inherit the Earth", "authors": ["Hans Gundlach", "Jayson Lynch", "Neil Thompson"], "categories": ["cs.AI", "cs.CY", "I.2.0; K.4.1"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      13 pages, 9 figures, longer version of the paper presented at TAIG ICML 2025", "url": "http://arxiv.org/abs/2507.07931v1", "summary": "The past decade has seen incredible scaling of AI systems by a few companies,\nleading to inequality in AI model performance. This paper argues that, contrary\nto prevailing intuition, the diminishing returns to compute scaling will lead\nto a convergence of AI model capabilities. In other words, meek models (those\nwith limited computation budget) shall inherit the earth, approaching the\nperformance level of the best models overall. We develop a model illustrating\nthat under a fixed-distribution next-token objective, the marginal capability\nreturns to raw compute shrink substantially. Given current scaling practices,\nwe argue that these diminishing returns are strong enough that even companies\nthat can scale their models exponentially faster than other organizations will\neventually have little advantage in capabilities. As part of our argument, we\ngive several reasons that proxies like training loss differences capture\nimportant capability measures using evidence from benchmark data and\ntheoretical performance models. In addition, we analyze empirical data on the\ncapability difference of AI models over time. Finally, in light of the\nincreasing ability of meek models, we argue that AI strategy and policy require\nreexamination, and we outline the areas this shift will affect.", "comment": "13 pages, 9 figures, longer version of the paper presented at TAIG\n  ICML 2025", "pdf_url": "http://arxiv.org/pdf/2507.07931v1", "cate": "cs.AI", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07247", "title": "Attentions Under the Microscope: A Comparative Study of Resource Utilization for Variants of Self-Attention", "authors": ["Zhengyu Tian", "Anantha Padmanaban Krishna Kumar", "Hemant Krishnakumar", "Reza Rawassizadeh"], "categories": ["cs.LG", "cs.AI", "cs.NE"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      6 pages, 8 figures", "url": "http://arxiv.org/abs/2507.07247v1", "summary": "As large language models (LLMs) and visual language models (VLMs) grow in\nscale and application, attention mechanisms have become a central computational\nbottleneck due to their high memory and time complexity. While many efficient\nattention variants have been proposed, there remains a lack of rigorous\nevaluation on their actual energy usage and hardware resource demands during\ntraining. In this work, we benchmark eight attention mechanisms in training\nGPT-2 architecture, measuring key metrics including training time, GPU memory\nusage, FLOPS, CPU usage, and power consumption. Our results reveal that\nattention mechanisms with optimized kernel implementations, including Flash\nAttention, Locality-Sensitive Hashing (LSH) Attention, and Multi-Head Latent\nAttention (MLA), achieve the best energy efficiency. We further show that lower\nGPU power alone does not guarantee reduced energy use, as training time plays\nan equally important role. Our study highlights the importance of energy-aware\nbenchmarking in attention design and provides a practical insight for selecting\nresource-efficient mechanisms. All our codes are available at GitHub.", "comment": "6 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.07247v1", "cate": "cs.LG", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.05289", "title": "Measuring how changes in code readability attributes affect code quality evaluation by Large Language Models", "authors": ["Igor Regis da Silva Simoes", "Elaine Venson"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.05289v2", "summary": "Code readability is one of the main aspects of code quality, influenced by\nvarious properties like identifier names, comments, code structure, and\nadherence to standards. However, measuring this attribute poses challenges in\nboth industry and academia. While static analysis tools assess attributes such\nas code smells and comment percentage, code reviews introduce an element of\nsubjectivity. This paper explores using Large Language Models (LLMs) to\nevaluate code quality attributes related to its readability in a standardized,\nreproducible, and consistent manner. We conducted a quasi-experiment study to\nmeasure the effects of code changes on Large Language Model (LLM)s\ninterpretation regarding its readability quality attribute. Nine LLMs were\ntested, undergoing three interventions: removing comments, replacing identifier\nnames with obscure names, and refactoring to remove code smells. Each\nintervention involved 10 batch analyses per LLM, collecting data on response\nvariability. We compared the results with a known reference model and tool. The\nresults showed that all LLMs were sensitive to the interventions, with\nagreement with the reference classifier being high for the original and\nrefactored code scenarios. The LLMs demonstrated a strong semantic sensitivity\nthat the reference model did not fully capture. A thematic analysis of the LLMs\nreasoning confirmed their evaluations directly reflected the nature of each\nintervention. The models also exhibited response variability, with 9.37% to\n14.58% of executions showing a standard deviation greater than zero, indicating\nresponse oscillation, though this did not always compromise the statistical\nsignificance of the results. LLMs demonstrated potential for evaluating\nsemantic quality aspects, such as coherence between identifier names, comments,\nand documentation with code purpose.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.05289v2", "cate": "cs.SE", "date": "2025-07-05", "updated": "2025-07-09"}
{"id": "2507.07262", "title": "DisenQ: Disentangling Q-Former for Activity-Biometrics", "authors": ["Shehreen Azad", "Yogesh S Rawat"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted in ICCV 2025", "url": "http://arxiv.org/abs/2507.07262v1", "summary": "In this work, we address activity-biometrics, which involves identifying\nindividuals across diverse set of activities. Unlike traditional person\nidentification, this setting introduces additional challenges as identity cues\nbecome entangled with motion dynamics and appearance variations, making\nbiometrics feature learning more complex. While additional visual data like\npose and/or silhouette help, they often struggle from extraction inaccuracies.\nTo overcome this, we propose a multimodal language-guided framework that\nreplaces reliance on additional visual data with structured textual\nsupervision. At its core, we introduce \\textbf{DisenQ} (\\textbf{Disen}tangling\n\\textbf{Q}-Former), a unified querying transformer that disentangles\nbiometrics, motion, and non-biometrics features by leveraging structured\nlanguage guidance. This ensures identity cues remain independent of appearance\nand motion variations, preventing misidentifications. We evaluate our approach\non three activity-based video benchmarks, achieving state-of-the-art\nperformance. Additionally, we demonstrate strong generalization to complex\nreal-world scenario with competitive performance on a traditional video-based\nidentification benchmark, showing the effectiveness of our framework.", "comment": "Accepted in ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.07262v1", "cate": "cs.CV", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07677", "title": "Can cloud-based VR streaming handle Wi-Fi OBSS contention?", "authors": ["Miguel Casasnovas", "Marc Carrascosa-Zamacois", "Boris Bellalta"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      preprint", "url": "http://arxiv.org/abs/2507.07677v1", "summary": "This paper experimentally analyzes the negative impact of contention caused\nby neighboring Wi-Fi networks operating on overlapping channels on Virtual\nReality (VR) streaming over Wi-Fi, focusing on scenarios of partial and full\nchannel overlap within an 80 MHz channel. Our results show that (i) increasing\nthe number of 80 MHz Overlapping Basic Service Sets (OBSSs) intensifies\ncontention and degrades VR streaming performance; (ii) OBSS activity on the\nsecondary-sided 40 MHz portion degrades performance more than activity on the\nprimary-sided 40 MHz portion; (iii) for the same aggregate load, full channel\noverlap with two 40 MHz OBSS contenders is less detrimental than partial\noverlap with a single high-load 40 MHz contender, but more disruptive than full\noverlap with two 80 MHz contenders; and (iv) full channel overlap with two 40\nMHz OBSS contenders has a smaller impact on VR streaming under symmetric\ntraffic loads than under asymmetric loads. Moreover, our results demonstrate\nthat our previously proposed Network-aware Step-wise adaptive bitrate algorithm\nfor VR streaming (NeSt-VR) effectively mitigates performance degradation in\nOBSS environments, enabling VR streaming under heavier OBSS traffic conditions.", "comment": "preprint", "pdf_url": "http://arxiv.org/pdf/2507.07677v1", "cate": "cs.NI", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07565", "title": "Secure Cooperative Gradient Coding: Optimality, Reliability, and Global Privacy", "authors": ["Shudi Weng"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07565v1", "summary": "This paper studies privacy-sensitive federated learning (FL) with unreliable\ncommunication, focusing on secure aggregation and straggler mitigation. While\nsecure aggregation cryptographically reconstructs the global model without\nexposing client updates, random link failures disrupt its key coordination,\ndegrading model accuracy. Moreover, unreliable communication can lead to\nobjective inconsistency, causing the global model to converge to arbitrary,\nsub-optimal points far from the intended optimum. This paper proposes Secure\nCooperative Gradient Coding (SecCoGC), a practical solution that achieves\nsecure aggregation with arbitrarily strong privacy guarantees and robust\nstraggler mitigation under unreliable communication. SecCoGC operates natively\nin the real field, making it directly applicable to practical deployments. To\nensure equitable privacy protection across clients, we further introduce\nFair-SecCoGC, an extension that enforces fairness in the level of privacy\noffered to all users. To conclude, this paper formally formulates the problem\nof secure aggregation in the real field and presents both general and\ncomputationally efficient key construction methods. Moreover, it provides a\ncomprehensive privacy analysis under Local Mutual Information Privacy (LMIP)\nand Local Differential Privacy (LDP) across all protocol layers. Robustness and\nconvergence properties are also rigorously analyzed. Finally, extensive\nsimulations are performed across diverse network conditions and benchmark\ndatasets to validate the effectiveness of the proposed methods. The results\nshow that SecCoGC achieves strong robustness to unreliable communication under\narbitrarily strong privacy guarantees. It outperforms existing\nprivacy-preserving methods with performance gains of up to 20\\%-70\\%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07565v1", "cate": "cs.IT", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07007", "title": "Robust signal decompositions on the circle", "authors": ["Aral Kose", "Daniel Liberzon"], "categories": ["math.OC", "cs.RO"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07007v1", "summary": "We consider the problem of decomposing a piecewise constant function on the\ncircle into a sum of indicator functions of closed circular disks in the plane,\nwhose number and location are not a priori known. This represents a situation\nwhere an agent moving on the circle is able to sense its proximity to some\nlandmarks, and the goal is to estimate the number of these landmarks and their\npossible locations -- which can in turn enable control tasks such as motion\nplanning and obstacle avoidance. Moreover, the exact values of the function at\nits discontinuities (which correspond to disk boundaries for the individual\nindicator functions) are not assumed to be known to the agent. We introduce\nsuitable notions of robustness and degrees of freedom to single out those\ndecompositions that are more desirable, or more likely, given this non-precise\ndata collected by the agent. We provide a characterization of robust\ndecompositions and give a procedure for generating all such decompositions.\nWhen the given function admits a robust decomposition, we compute the number of\npossible robust decompositions and derive bounds for the number of\ndecompositions maximizing the degrees of freedom.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07007v1", "cate": "math.OC", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07935", "title": "Working with AI: Measuring the Occupational Implications of Generative AI", "authors": ["Kiran Tomlinson", "Sonia Jaffe", "Will Wang", "Scott Counts", "Siddharth Suri"], "categories": ["cs.AI", "cs.CY", "econ.GN", "q-fin.EC"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      40 pages", "url": "http://arxiv.org/abs/2507.07935v1", "summary": "Given the rapid adoption of generative AI and its potential to impact a wide\nrange of tasks, understanding the effects of AI on the economy is one of\nsociety's most important questions. In this work, we take a step toward that\ngoal by analyzing the work activities people do with AI, how successfully and\nbroadly those activities are done, and combine that with data on what\noccupations do those activities. We analyze a dataset of 200k anonymized and\nprivacy-scrubbed conversations between users and Microsoft Bing Copilot, a\npublicly available generative AI system. We find the most common work\nactivities people seek AI assistance for involve gathering information and\nwriting, while the most common activities that AI itself is performing are\nproviding information and assistance, writing, teaching, and advising.\nCombining these activity classifications with measurements of task success and\nscope of impact, we compute an AI applicability score for each occupation. We\nfind the highest AI applicability scores for knowledge work occupation groups\nsuch as computer and mathematical, and office and administrative support, as\nwell as occupations such as sales whose work activities involve providing and\ncommunicating information. Additionally, we characterize the types of work\nactivities performed most successfully, how wage and education correlate with\nAI applicability, and how real-world usage compares to predictions of\noccupational AI impact.", "comment": "40 pages", "pdf_url": "http://arxiv.org/pdf/2507.07935v1", "cate": "cs.AI", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07259", "title": "Exploiting Edge Features for Transferable Adversarial Attacks in Distributed Machine Learning", "authors": ["Giulio Rossolini", "Fabio Brau", "Alessandro Biondi", "Battista Biggio", "Giorgio Buttazzo"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      under review", "url": "http://arxiv.org/abs/2507.07259v1", "summary": "As machine learning models become increasingly deployed across the edge of\ninternet of things environments, a partitioned deep learning paradigm in which\nmodels are split across multiple computational nodes introduces a new dimension\nof security risk. Unlike traditional inference setups, these distributed\npipelines span the model computation across heterogeneous nodes and\ncommunication layers, thereby exposing a broader attack surface to potential\nadversaries. Building on these motivations, this work explores a previously\noverlooked vulnerability: even when both the edge and cloud components of the\nmodel are inaccessible (i.e., black-box), an adversary who intercepts the\nintermediate features transmitted between them can still pose a serious threat.\nWe demonstrate that, under these mild and realistic assumptions, an attacker\ncan craft highly transferable proxy models, making the entire deep learning\nsystem significantly more vulnerable to evasion attacks. In particular, the\nintercepted features can be effectively analyzed and leveraged to distill\nsurrogate models capable of crafting highly transferable adversarial examples\nagainst the target model. To this end, we propose an exploitation strategy\nspecifically designed for distributed settings, which involves reconstructing\nthe original tensor shape from vectorized transmitted features using simple\nstatistical analysis, and adapting surrogate architectures accordingly to\nenable effective feature distillation. A comprehensive and systematic\nexperimental evaluation has been conducted to demonstrate that surrogate models\ntrained with the proposed strategy, i.e., leveraging intermediate features,\ntremendously improve the transferability of adversarial attacks. These findings\nunderscore the urgent need to account for intermediate feature leakage in the\ndesign of secure distributed deep learning systems.", "comment": "under review", "pdf_url": "http://arxiv.org/pdf/2507.07259v1", "cate": "cs.LG", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.05995", "title": "PromiseTune: Unveiling Causally Promising and Explainable Configuration Tuning", "authors": ["Pengzhou Chen", "Tao Chen"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      This paper has been accepted by ICSE26", "url": "http://arxiv.org/abs/2507.05995v2", "summary": "The high configurability of modern software systems has made configuration\ntuning a crucial step for assuring system performance, e.g., latency or\nthroughput. However, given the expensive measurements, large configuration\nspace, and rugged configuration landscape, existing tuners suffer\nineffectiveness due to the difficult balance of budget utilization between\nexploring uncertain regions (for escaping from local optima) and exploiting\nguidance of known good configurations (for fast convergence). The root cause is\nthat we lack knowledge of where the promising regions lay, which also causes\nchallenges in the explainability of the results.\n  In this paper, we propose PromiseTune that tunes configuration guided by\ncausally purified rules. PromiseTune is unique in the sense that we learn\nrules, which reflect certain regions in the configuration landscape, and purify\nthem with causal inference. The remaining rules serve as approximated\nreflections of the promising regions, bounding the tuning to emphasize these\nplaces in the landscape. This, as we demonstrate, can effectively mitigate the\nimpact of the exploration and exploitation trade-off. Those purified regions\ncan then be paired with the measured configurations to provide spatial\nexplainability at the landscape level. Comparing with 11 state-of-the-art\ntuners on 12 systems and varying budgets, we show that PromiseTune performs\nsignificantly better than the others with 42% superior rank to the overall\nsecond best while providing richer information to explain the hidden system\ncharacteristics.", "comment": "This paper has been accepted by ICSE26", "pdf_url": "http://arxiv.org/pdf/2507.05995v2", "cate": "cs.SE", "date": "2025-07-08", "updated": "2025-07-10"}
{"id": "2507.07274", "title": "LinguaMark: Do Multimodal Models Speak Fairly? A Benchmark-Based Evaluation", "authors": ["Ananya Raval", "Aravind Narayanan", "Vahid Reza Khazaie", "Shaina Raza"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at ASONAM'25", "url": "http://arxiv.org/abs/2507.07274v1", "summary": "Large Multimodal Models (LMMs) are typically trained on vast corpora of\nimage-text data but are often limited in linguistic coverage, leading to biased\nand unfair outputs across languages. While prior work has explored multimodal\nevaluation, less emphasis has been placed on assessing multilingual\ncapabilities. In this work, we introduce LinguaMark, a benchmark designed to\nevaluate state-of-the-art LMMs on a multilingual Visual Question Answering\n(VQA) task. Our dataset comprises 6,875 image-text pairs spanning 11 languages\nand five social attributes. We evaluate models using three key metrics: Bias,\nAnswer Relevancy, and Faithfulness. Our findings reveal that closed-source\nmodels generally achieve the highest overall performance. Both closed-source\n(GPT-4o and Gemini2.5) and open-source models (Gemma3, Qwen2.5) perform\ncompetitively across social attributes, and Qwen2.5 demonstrates strong\ngeneralization across multiple languages. We release our benchmark and\nevaluation code to encourage reproducibility and further research.", "comment": "Accepted at ASONAM'25", "pdf_url": "http://arxiv.org/pdf/2507.07274v1", "cate": "cs.CV", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07841", "title": "HaLert: A Resilient Smart City Architecture for Post-Disaster Based on Wi-Fi HaLow Mesh and SDN", "authors": ["Ana Rita Ortigoso", "Gabriel Vieira", "Daniel Fuentes", "Lu√≠s Fraz√£o", "Nuno Costa", "Ant√≥nio Pereira"], "categories": ["cs.NI", "cs.CY", "cs.SY", "eess.SY", "68M10, 68M12, 68W15", "C.2.1; C.2.2; C.2.3; C.2.6; H.5.5; K.4.1"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07841v1", "summary": "Events such as catastrophes and disasters are, in most cases, unpredictable.\nConsequently, reusing existing infrastructures to develop alternative\ncommunication strategies after disasters is essential to minimise the impact of\nthese events on the population's ability to communicate and promptly receive\nalerts from authorities. In this context, the emergence of smart cities,\ncharacterised by dense and geographically distributed IoT networks, presents\nsignificant potential for such reuse. This work proposes HaLert, a resilient\narchitecture for smart cities based on a Wi-Fi HaLow IEEE 802.11s mesh network,\nwhose resources can be readily reallocated to support a emergency communication\nsystem to exchange messages (including text, location, image, audio, and video)\nbetween citizens, authorities, and between both parties. To facilitate remote\nmonitoring and configuration of the network, the architecture incorporates the\nSDN (Software-Defined Networking) paradigm, supported by a LoRa controlled\nflooding mesh network. A prototype was developed based on this architecture and\ntested in a real urban scenario comprising both indoor and outdoor\nenvironments. The results demonstrated that, despite the significant impact of\nobstacles, lack of line-of-sight, and terrain slopes on the latency (average\nlatency between 15 and 54.8 ms) and throughput (upload bitrates between 134 and\n726 Kbps and download bitrates between 117 and 682 Kbps) of the Wi-Fi HaLow\nnetwork, it remained stable and resilient, successfully providing all\nfunctionalities associated with the HaLert architecture. The tests conducted on\nthe LoRa network revealed a high average message success rate of 94.96%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07841v1", "cate": "cs.NI", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07728", "title": "Linear codes for $b$-symbol read channels attaining the Griesmer bound", "authors": ["Sascha Kurz"], "categories": ["cs.IT", "math.CO", "math.IT", "05B25, 94B65, 94B60"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      27 pages, 1 table. Comments very welcome!", "url": "http://arxiv.org/abs/2507.07728v1", "summary": "Reading channels where $b$-tuples of adjacent symbols are read at every step\nhave e.g.\\ applications in storage. Corresponding bounds and constructions of\ncodes for the $b$-symbol metric, especially the pair-symbol metric where $b=2$,\nwere intensively studied in the last fifteen years. Here we determine the\noptimal code parameters of linear codes in the $b$-symbol metric assuming that\nthe minimum distance is sufficiently large. We also determine the optimal\nparameters of linear binary codes in the pair-symbol metric for small\ndimensions.", "comment": "27 pages, 1 table. Comments very welcome!", "pdf_url": "http://arxiv.org/pdf/2507.07728v1", "cate": "cs.IT", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07683", "title": "Accelerating Transposed Convolutions on FPGA-based Edge Devices", "authors": ["Jude Haris", "Jos√© Cano"], "categories": ["cs.AR", "cs.DC", "cs.LG"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "Comments:      Accepted to 35th International Conference on Field-Programmable Logic and Applications (FPL) 2025", "url": "http://arxiv.org/abs/2507.07683v1", "summary": "Transposed Convolutions (TCONV) enable the up-scaling mechanism within\ngenerative Artificial Intelligence (AI) models. However, the predominant\nInput-Oriented Mapping (IOM) method for implementing TCONV has complex output\nmapping, overlapping sums, and ineffectual computations. These inefficiencies\nfurther exacerbate the performance bottleneck of TCONV and generative models on\nresource-constrained edge devices. To address this problem, in this paper we\npropose MM2IM, a hardware-software co-designed accelerator that combines Matrix\nMultiplication (MatMul) with col2IM to process TCONV layers on\nresource-constrained edge devices efficiently. Using the SECDA-TFLite design\ntoolkit, we implement MM2IM and evaluate its performance across 261 TCONV\nproblem configurations, achieving an average speedup of 1.9x against a\ndual-thread ARM Neon optimized CPU baseline. We then evaluate the performance\nof MM2IM on a range of TCONV layers from well-known generative models achieving\nup to 4.2x speedup, and compare it against similar resource-constrained TCONV\naccelerators, outperforming them by at least 2x GOPs/DSP. Finally, we evaluate\nMM2IM on the DCGAN and pix2pix GAN models, achieving up to 3x speedup and 2.4x\nenergy reduction against the CPU baseline.", "comment": "Accepted to 35th International Conference on Field-Programmable Logic\n  and Applications (FPL) 2025", "pdf_url": "http://arxiv.org/pdf/2507.07683v1", "cate": "cs.AR", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07781", "title": "SURPRISE3D: A Dataset for Spatial Understanding and Reasoning in Complex 3D Scenes", "authors": ["Jiaxin Huang", "Ziwen Li", "Hanlve Zhang", "Runnan Chen", "Xiao He", "Yandong Guo", "Wenping Wang", "Tongliang Liu", "Mingming Gong"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07781v1", "summary": "The integration of language and 3D perception is critical for embodied AI and\nrobotic systems to perceive, understand, and interact with the physical world.\nSpatial reasoning, a key capability for understanding spatial relationships\nbetween objects, remains underexplored in current 3D vision-language research.\nExisting datasets often mix semantic cues (e.g., object name) with spatial\ncontext, leading models to rely on superficial shortcuts rather than genuinely\ninterpreting spatial relationships. To address this gap, we introduce\nS\\textsc{urprise}3D, a novel dataset designed to evaluate language-guided\nspatial reasoning segmentation in complex 3D scenes. S\\textsc{urprise}3D\nconsists of more than 200k vision language pairs across 900+ detailed indoor\nscenes from ScanNet++ v2, including more than 2.8k unique object classes. The\ndataset contains 89k+ human-annotated spatial queries deliberately crafted\nwithout object name, thereby mitigating shortcut biases in spatial\nunderstanding. These queries comprehensively cover various spatial reasoning\nskills, such as relative position, narrative perspective, parametric\nperspective, and absolute distance reasoning. Initial benchmarks demonstrate\nsignificant challenges for current state-of-the-art expert 3D visual grounding\nmethods and 3D-LLMs, underscoring the necessity of our dataset and the\naccompanying 3D Spatial Reasoning Segmentation (3D-SRS) benchmark suite.\nS\\textsc{urprise}3D and 3D-SRS aim to facilitate advancements in spatially\naware AI, paving the way for effective embodied interaction and robotic\nplanning. The code and datasets can be found in\nhttps://github.com/liziwennba/SUPRISE.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07781v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2506.13201", "title": "A Comprehensive Survey on Deep Learning Solutions for 3D Flood Mapping", "authors": ["Wenfeng Jia", "Bin Liang", "Yuxi Liu", "Muhammad Arif Khan", "Lihong Zheng"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.13201v1", "summary": "Flooding remains a major global challenge, worsened by climate change and\nurbanization, demanding advanced solutions for effective disaster management.\nWhile traditional 2D flood mapping techniques provide limited insights, 3D\nflood mapping, powered by deep learning (DL), offers enhanced capabilities by\nintegrating flood extent and depth. This paper presents a comprehensive survey\nof deep learning-based 3D flood mapping, emphasizing its advancements over 2D\nmaps by integrating flood extent and depth for effective disaster management\nand urban planning. The survey categorizes deep learning techniques into task\ndecomposition and end-to-end approaches, applicable to both static and dynamic\nflood features. We compare key DL architectures, highlighting their respective\nroles in enhancing prediction accuracy and computational efficiency.\nAdditionally, this work explores diverse data sources such as digital elevation\nmodels, satellite imagery, rainfall, and simulated data, outlining their roles\nin 3D flood mapping. The applications reviewed range from real-time flood\nprediction to long-term urban planning and risk assessment. However,\nsignificant challenges persist, including data scarcity, model\ninterpretability, and integration with traditional hydrodynamic models. This\nsurvey concludes by suggesting future directions to address these limitations,\nfocusing on enhanced datasets, improved models, and policy implications for\nflood management. This survey aims to guide researchers and practitioners in\nleveraging DL techniques for more robust and reliable 3D flood mapping,\nfostering improved flood management strategies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.13201v1", "cate": "cs.CV", "date": "2025-06-16", "updated": "2025-06-16"}
{"id": "2507.07261", "title": "Robust Multimodal Learning Framework For Intake Gesture Detection Using Contactless Radar and Wearable IMU Sensors", "authors": ["Chunzhuo Wang", "Hans Hallez", "Bart Vanrumste"], "categories": ["cs.LG", "eess.SP"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      This manuscript has been submitted to a peer-reviewed journal and is currently under review", "url": "http://arxiv.org/abs/2507.07261v1", "summary": "Automated food intake gesture detection plays a vital role in dietary\nmonitoring, enabling objective and continuous tracking of eating behaviors to\nsupport better health outcomes. Wrist-worn inertial measurement units (IMUs)\nhave been widely used for this task with promising results. More recently,\ncontactless radar sensors have also shown potential. This study explores\nwhether combining wearable and contactless sensing modalities through\nmultimodal learning can further improve detection performance. We also address\na major challenge in multimodal learning: reduced robustness when one modality\nis missing. To this end, we propose a robust multimodal temporal convolutional\nnetwork with cross-modal attention (MM-TCN-CMA), designed to integrate IMU and\nradar data, enhance gesture detection, and maintain performance under missing\nmodality conditions. A new dataset comprising 52 meal sessions (3,050 eating\ngestures and 797 drinking gestures) from 52 participants is developed and made\npublicly available. Experimental results show that the proposed framework\nimproves the segmental F1-score by 4.3% and 5.2% over unimodal Radar and IMU\nmodels, respectively. Under missing modality scenarios, the framework still\nachieves gains of 1.3% and 2.4% for missing radar and missing IMU inputs. This\nis the first study to demonstrate a robust multimodal learning framework that\neffectively fuses IMU and radar data for food intake gesture detection.", "comment": "This manuscript has been submitted to a peer-reviewed journal and is\n  currently under review", "pdf_url": "http://arxiv.org/pdf/2507.07261v1", "cate": "cs.LG", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2503.11498", "title": "Open-source automatic pipeline for efficient conversion of large-scale point clouds to IFC format", "authors": ["Sl√°vek Zbirovsk√Ω", "V√°clav Ne≈æerka"], "categories": ["cs.CV", "cs.SE"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      published version, 23 pages, 25 figures", "url": "http://arxiv.org/abs/2503.11498v3", "summary": "Building Information Modeling (BIM) is an essential component in the\nsustainable reconstruction and revitalization of ageing structures. However,\nmodel creation usually relies on laborious manual transformation of the\nunstructured point cloud data provided by laser scans or photogrammetry. This\npaper presents Cloud2BIM, an open-source software tool designed to automate the\nconversion of point clouds into BIM models compliant with the Industry\nFoundation Classes (IFC) standard. Cloud2BIM integrates advanced algorithms for\nwall and slab segmentation, opening detection, and room zoning based on real\nwall surfaces, resulting in a comprehensive and fully automated workflow.\nUnlike existing tools, it avoids computationally- and calibration-intensive\ntechniques such as RANSAC, supports non-orthogonal geometries, and provides\nunprecedented processing speed-achieving results up to seven times faster than\nfastest competing solutions. Systematic validation using benchmark datasets\nconfirms that Cloud2BIM is an easy-to-use, efficient, and scalable solution for\ngenerating accurate BIM models, capable of converting extensive point cloud\ndatasets for entire buildings into IFC format with minimal user input.", "comment": "published version, 23 pages, 25 figures", "pdf_url": "http://arxiv.org/pdf/2503.11498v3", "cate": "cs.CV", "date": "2025-03-14", "updated": "2025-07-10"}
{"id": "2507.07297", "title": "MagiC: Evaluating Multimodal Cognition Toward Grounded Visual Reasoning", "authors": ["Chengfei Wu", "Ronald Seoh", "Bingxuan Li", "Liqiang Zhang", "Fengrong Han", "Dan Goldwasser"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07297v1", "summary": "Recent advances in large vision-language models have led to impressive\nperformance in visual question answering and multimodal reasoning. However, it\nremains unclear whether these models genuinely perform grounded visual\nreasoning or rely on superficial patterns and dataset biases. In this work, we\nintroduce MagiC, a comprehensive benchmark designed to evaluate grounded\nmultimodal cognition, assessing not only answer accuracy but also the quality\nof step-by-step reasoning and its alignment with relevant visual evidence. Our\nbenchmark includes approximately 5,500 weakly supervised QA examples generated\nfrom strong model outputs and 900 human-curated examples with fine-grained\nannotations, including answers, rationales, and bounding box groundings. We\nevaluate 15 vision-language models ranging from 7B to 70B parameters across\nfour dimensions: final answer correctness, reasoning validity, grounding\nfidelity, and self-correction ability. MagiC further includes diagnostic\nsettings to probe model robustness under adversarial visual cues and assess\ntheir capacity for introspective error correction. We introduce new metrics\nsuch as MagiScore and StepSense, and provide comprehensive analyses that reveal\nkey limitations and opportunities in current approaches to grounded visual\nreasoning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07297v1", "cate": "cs.CV", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07117", "title": "Collective Communication Profiling of Modern-day Machine Learning Workloads", "authors": ["Jit Gupta", "Andrew Li", "Tarun Banka", "Ariel Cohen", "T. Sridhar", "Raj Yavatkar"], "categories": ["cs.DC", "cs.AI", "cs.NI"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      Poser, USENIX NSDI 2025, April 2025, Philadelphia, PA, USA", "url": "http://arxiv.org/abs/2507.07117v1", "summary": "Machine Learning jobs, carried out on large number of distributed high\nperformance systems, involve periodic communication using operations like\nAllReduce, AllGather, and Broadcast. These operations may create high bandwidth\nand bursty traffic patterns, leading to network congestion and packet loss,\nthus impacting the performance of these jobs. Hence it is imperative to analyze\nthese patterns, which can be helpful in provisioning network resources\ndepending on the type of machine learning workloads. In this poster we carry\nout extensive analysis of the collective communication behavior seen in a wide\nvariety of models (ex. DeepSeek, GPT, Llama, etc.) To achieve this we\ninstrument Nvidia Collective Communication Library logging functionality for\nricher context about the collectives and workloads. We adjust configuration\nparameters that influence collective communication behavior, such as\nparallelism, number of nodes, and model type. This overview presents and\ndiscusses some of the results on the collective communication behavior for the\nopen source DeepSeek V3 inferencing model, which includes operation type and\ncount, transfer sizes per operation, and request size distribution. Our\nanalysis shows that it makes sense to rethink current collective communication\nframeworks and network topologies so as to accommodate the effect of network\nanomalies on the mentioned workloads.", "comment": "Poser, USENIX NSDI 2025, April 2025, Philadelphia, PA, USA", "pdf_url": "http://arxiv.org/pdf/2507.07117v1", "cate": "cs.DC", "date": "2025-07-03", "updated": "2025-07-03"}
{"id": "2507.07842", "title": "Generalized bilateral multilevel construction for constant dimension codes from parallel mixed dimension construction", "authors": ["Han Li", "Fang-Wei Fu"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      Submitted for possible publication", "url": "http://arxiv.org/abs/2507.07842v1", "summary": "Constant dimension codes (CDCs), as special subspace codes, have received\nextensive attention due to their applications in random network coding. The\nbasic problem of CDCs is to determine the maximal possible size\n$A_q(n,d,\\{k\\})$ for given parameters $q, n, d$, and $k$. This paper introduces\ncriteria for choosing appropriate bilateral identifying vectors compatible with\nthe parallel mixed dimension construction (Des. Codes Cryptogr. 93(1):227--241,\n2025). We then utilize the generalized bilateral multilevel construction (Des.\nCodes Cryptogr. 93(1):197--225, 2025) to improve the parallel mixed dimension\nconstruction efficiently. Many new CDCs that are better than the previously\nbest-known codes are constructed.", "comment": "Submitted for possible publication", "pdf_url": "http://arxiv.org/pdf/2507.07842v1", "cate": "cs.IT", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07223", "title": "Compute Can't Handle the Truth: Why Communication Tax Prioritizes Memory and Interconnects in Modern AI Infrastructure", "authors": ["Myoungsoo Jung"], "categories": ["cs.DC", "cs.AR", "B.4.3; C.0; C.2.1; C.2.2"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07223v1", "summary": "Modern AI workloads such as large language models (LLMs) and\nretrieval-augmented generation (RAG) impose severe demands on memory,\ncommunication bandwidth, and resource flexibility. Traditional GPU-centric\narchitectures struggle to scale due to growing inter-GPU communication\noverheads. This report introduces key AI concepts and explains how Transformers\nrevolutionized data representation in LLMs. We analyze large-scale AI hardware\nand data center designs, identifying scalability bottlenecks in hierarchical\nsystems. To address these, we propose a modular data center architecture based\non Compute Express Link (CXL) that enables disaggregated scaling of memory,\ncompute, and accelerators. We further explore accelerator-optimized\ninterconnects-collectively termed XLink (e.g., UALink, NVLink, NVLink\nFusion)-and introduce a hybrid CXL-over-XLink design to reduce long-distance\ndata transfers while preserving memory coherence. We also propose a\nhierarchical memory model that combines local and pooled memory, and evaluate\nlightweight CXL implementations, HBM, and silicon photonics for efficient\nscaling. Our evaluations demonstrate improved scalability, throughput, and\nflexibility in AI infrastructure.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07223v1", "cate": "cs.DC", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07969", "title": "Reinforcement Learning with Action Chunking", "authors": ["Qiyang Li", "Zhiyuan Zhou", "Sergey Levine"], "categories": ["cs.LG", "cs.AI", "cs.RO", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      25 pages, 15 figures", "url": "http://arxiv.org/abs/2507.07969v1", "summary": "We present Q-chunking, a simple yet effective recipe for improving\nreinforcement learning (RL) algorithms for long-horizon, sparse-reward tasks.\nOur recipe is designed for the offline-to-online RL setting, where the goal is\nto leverage an offline prior dataset to maximize the sample-efficiency of\nonline learning. Effective exploration and sample-efficient learning remain\ncentral challenges in this setting, as it is not obvious how the offline data\nshould be utilized to acquire a good exploratory policy. Our key insight is\nthat action chunking, a technique popularized in imitation learning where\nsequences of future actions are predicted rather than a single action at each\ntimestep, can be applied to temporal difference (TD)-based RL methods to\nmitigate the exploration challenge. Q-chunking adopts action chunking by\ndirectly running RL in a 'chunked' action space, enabling the agent to (1)\nleverage temporally consistent behaviors from offline data for more effective\nonline exploration and (2) use unbiased $n$-step backups for more stable and\nefficient TD learning. Our experimental results demonstrate that Q-chunking\nexhibits strong offline performance and online sample efficiency, outperforming\nprior best offline-to-online methods on a range of long-horizon, sparse-reward\nmanipulation tasks.", "comment": "25 pages, 15 figures", "pdf_url": "http://arxiv.org/pdf/2507.07969v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2506.21142", "title": "Generative Adversarial Evasion and Out-of-Distribution Detection for UAV Cyber-Attacks", "authors": ["Deepak Kumar Panda", "Weisi Guo"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.21142v1", "summary": "The growing integration of UAVs into civilian airspace underscores the need\nfor resilient and intelligent intrusion detection systems (IDS), as traditional\nanomaly detection methods often fail to identify novel threats. A common\napproach treats unfamiliar attacks as out-of-distribution (OOD) samples;\nhowever, this leaves systems vulnerable when mitigation is inadequate.\nMoreover, conventional OOD detectors struggle to distinguish stealthy\nadversarial attacks from genuine OOD events. This paper introduces a\nconditional generative adversarial network (cGAN)-based framework for crafting\nstealthy adversarial attacks that evade IDS mechanisms. We first design a\nrobust multi-class IDS classifier trained on benign UAV telemetry and known\ncyber-attacks, including Denial of Service (DoS), false data injection (FDI),\nman-in-the-middle (MiTM), and replay attacks. Using this classifier, our cGAN\nperturbs known attacks to generate adversarial samples that misclassify as\nbenign while retaining statistical resemblance to OOD distributions. These\nadversarial samples are iteratively refined to achieve high stealth and success\nrates. To detect such perturbations, we implement a conditional variational\nautoencoder (CVAE), leveraging negative log-likelihood to separate adversarial\ninputs from authentic OOD samples. Comparative evaluation shows that CVAE-based\nregret scores significantly outperform traditional Mahalanobis distance-based\ndetectors in identifying stealthy adversarial threats. Our findings emphasize\nthe importance of advanced probabilistic modeling to strengthen IDS\ncapabilities against adaptive, generative-model-based cyber intrusions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21142v1", "cate": "cs.LG", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2507.07271", "title": "Beyond the ATE: Interpretable Modelling of Treatment Effects over Dose and Time", "authors": ["Julianna Piskorz", "Krzysztof Kacprzyk", "Mihaela van der Schaar"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Presented at the Actionable Interpretability Workshop at ICML 2025", "url": "http://arxiv.org/abs/2507.07271v1", "summary": "The Average Treatment Effect (ATE) is a foundational metric in causal\ninference, widely used to assess intervention efficacy in randomized controlled\ntrials (RCTs). However, in many applications -- particularly in healthcare --\nthis static summary fails to capture the nuanced dynamics of treatment effects\nthat vary with both dose and time. We propose a framework for modelling\ntreatment effect trajectories as smooth surfaces over dose and time, enabling\nthe extraction of clinically actionable insights such as onset time, peak\neffect, and duration of benefit. To ensure interpretability, robustness, and\nverifiability -- key requirements in high-stakes domains -- we adapt\nSemanticODE, a recent framework for interpretable trajectory modelling, to the\ncausal setting where treatment effects are never directly observed. Our\napproach decouples the estimation of trajectory shape from the specification of\nclinically relevant properties (e.g., maxima, inflection points), supporting\ndomain-informed priors, post-hoc editing, and transparent analysis. We show\nthat our method yields accurate, interpretable, and editable models of\ntreatment dynamics, facilitating both rigorous causal analysis and practical\ndecision-making.", "comment": "Presented at the Actionable Interpretability Workshop at ICML 2025", "pdf_url": "http://arxiv.org/pdf/2507.07271v1", "cate": "cs.LG", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2505.12878", "title": "QCP: A Practical Separation Logic-based C Program Verification Tool", "authors": ["Xiwei Wu", "Yueyang Feng", "Xiaoyang Lu", "Tianchuan Lin", "Kan Liu", "Zhiyi Wang", "Shushu Wu", "Lihan Xie", "Chengxi Yang", "Hongyi Zhong", "Naijun Zhan", "Zhenjiang Hu", "Qinxiang Cao"], "categories": ["cs.PL", "cs.SE"], "primary_category": "Subjects:       Programming Languages (cs.PL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.12878v2", "summary": "As software systems increase in size and complexity dramatically, ensuring\ntheir correctness, security, and reliability becomes an increasingly formidable\nchallenge. Despite significant advancements in verification techniques and\ntools, there still remain %these tools still continue to encounter substantial\ndifficulties when applying these tools to complex, real-world scenarios. To\naddress these difficulties, this paper introduces a novel verification tool,\ncalled \\textbf{Qualified C Programming Verifier (QCP)}. QCP incorporates a\nrefined front-end %syntax of assertion language to enhance user interaction.\nThe proposed assertion language aims to %syntax is designed to lower the entry\nbarrier for verification tools, improve proof efficiency by improving\nautomation, and facilitate a deeper understanding of both the program and its\nverification results.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.12878v2", "cate": "cs.PL", "date": "2025-05-19", "updated": "2025-07-10"}
{"id": "2507.07317", "title": "ADIEE: Automatic Dataset Creation and Scorer for Instruction-Guided Image Editing Evaluation", "authors": ["Sherry X. Chen", "Yi Wei", "Luowei Zhou", "Suren Kumar"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      International Conference on Computer Vision (ICCV) 2025", "url": "http://arxiv.org/abs/2507.07317v1", "summary": "Recent advances in instruction-guided image editing underscore the need for\neffective automated evaluation. While Vision-Language Models (VLMs) have been\nexplored as judges, open-source models struggle with alignment, and proprietary\nmodels lack transparency and cost efficiency. Additionally, no public training\ndatasets exist to fine-tune open-source VLMs, only small benchmarks with\ndiverse evaluation schemes. To address this, we introduce ADIEE, an automated\ndataset creation approach which is then used to train a scoring model for\ninstruction-guided image editing evaluation. We generate a large-scale dataset\nwith over 100K samples and use it to fine-tune a LLaVA-NeXT-8B model modified\nto decode a numeric score from a custom token. The resulting scorer outperforms\nall open-source VLMs and Gemini-Pro 1.5 across all benchmarks, achieving a\n0.0696 (+17.24%) gain in score correlation with human ratings on AURORA-Bench,\nand improving pair-wise comparison accuracy by 4.03% (+7.21%) on GenAI-Bench\nand 4.75% (+9.35%) on AURORA-Bench, respectively, compared to the\nstate-of-the-art. The scorer can act as a reward model, enabling automated best\nedit selection and model fine-tuning. Notably, the proposed scorer can boost\nMagicBrush model's average evaluation score on ImagenHub from 5.90 to 6.43\n(+8.98%).", "comment": "International Conference on Computer Vision (ICCV) 2025", "pdf_url": "http://arxiv.org/pdf/2507.07317v1", "cate": "cs.CV", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07581", "title": "CHOMET: Conditional Handovers via Meta-Learning", "authors": ["Michail Kalntis", "Fernando A. Kuipers", "George Iosifidis"], "categories": ["cs.LG", "cs.NI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07581v1", "summary": "Handovers (HOs) are the cornerstone of modern cellular networks for enabling\nseamless connectivity to a vast and diverse number of mobile users. However, as\nmobile networks become more complex with more diverse users and smaller cells,\ntraditional HOs face significant challenges, such as prolonged delays and\nincreased failures. To mitigate these issues, 3GPP introduced conditional\nhandovers (CHOs), a new type of HO that enables the preparation (i.e., resource\nallocation) of multiple cells for a single user to increase the chance of HO\nsuccess and decrease the delays in the procedure. Despite its advantages, CHO\nintroduces new challenges that must be addressed, including efficient resource\nallocation and managing signaling/communication overhead from frequent cell\npreparations and releases. This paper presents a novel framework aligned with\nthe O-RAN paradigm that leverages meta-learning for CHO optimization, providing\nrobust dynamic regret guarantees and demonstrating at least 180% superior\nperformance than other 3GPP benchmarks in volatile signal conditions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07581v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07241", "title": "Secrecy Energy Efficiency Maximization in RIS-Aided Networks: Active or Nearly-Passive RIS?", "authors": ["Robert Kuku Fotock", "Agbotiname Lucky Imoize", "Alessio Zappone", "Marco Di Renzo", "Roberto Garello"], "categories": ["math.OC", "cs.IT", "eess.SP", "math.IT", "49M20 (Primary) 49M05, 94A05 (Secondary)", "F.2.1; F.2.3; I.6.8; G.1.6"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      16 pages, 11 figures, IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY", "url": "http://arxiv.org/abs/2507.07241v1", "summary": "This work addresses the problem of secrecy energy efficiency (SEE)\nmaximization in RIS-aided wireless networks. The use of active and\nnearly-passive RISs are compared and their trade-off in terms of SEE is\nanalyzed. Considering both perfect and statistical channel state information,\ntwo SEE maximization algorithms are developed to optimize the transmit powers\nof the mobile users, the RIS reflection coefficients, and the base station\nreceive filters. Numerical results quantify the trade-off between active and\nnearly-passive RISs in terms of SEE, with active RISs yielding worse SEE values\nas the static power consumed by each reflecting element increases.", "comment": "16 pages, 11 figures, IEEE TRANSACTIONS ON INFORMATION FORENSICS AND\n  SECURITY", "pdf_url": "http://arxiv.org/pdf/2507.07241v1", "cate": "math.OC", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2412.09709", "title": "DiP: A Scalable, Energy-Efficient Systolic Array for Matrix Multiplication Acceleration", "authors": ["Ahmed J. Abdelmaksoud", "Shady Agwa", "Themis Prodromakis"], "categories": ["cs.AR", "cs.DC"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.09709v2", "summary": "Transformers are gaining increasing attention across different application\ndomains due to their outstanding accuracy. However, these data-intensive models\nadd significant performance demands to the existing computing architectures.\nSystolic arrays are spatial architectures that have been adopted by commercial\nAI computing platforms (like Google TPUs), due to their energy-efficient\napproach of data-reusability. However, these spatial architectures face a\npenalty in throughput and energy efficiency due to the need for input and\noutput synchronization using First-In-First-Out (FIFO) buffers. This paper\nproposes a novel scalable systolic-array architecture featuring Diagonal-Input\nand Permutated weight-stationary (DiP) dataflow for the acceleration of matrix\nmultiplication. The proposed architecture eliminates the synchronization FIFOs\nrequired by state-of-the-art weight stationary systolic arrays. Aside from the\narea, power, and energy savings achieved by eliminating these FIFOs, DiP\narchitecture maximizes the computational resources (PEs) utilization. Thus, it\noutperforms the weight-stationary counterparts in terms of throughput by up to\n50%. A comprehensive hardware design space exploration is demonstrated using\ncommercial 22nm technology, highlighting the scalability advantages of DiP over\nthe conventional approach across various dimensions where DiP offers\nimprovement of energy efficiency per area up to 2.02x. Furthermore, DiP is\nevaluated using various transformer workloads from widely-used models,\nconsistently outperforming TPU-like architectures, achieving energy\nimprovements of up to 1.81x and latency improvements of up to 1.49x across a\nrange of transformer workloads. At a 64x64 size with 4096 PEs, DiP achieves a\npeak performance of 8.2 TOPS with energy efficiency 9.55 TOPS/W.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.09709v2", "cate": "cs.AR", "date": "2024-12-12", "updated": "2025-07-10"}
{"id": "2404.08390", "title": "Collective Bayesian Decision-Making in a Swarm of Miniaturized Robots for Surface Inspection", "authors": ["Thiemen Siemensma", "Darren Chiu", "Sneha Ramshanker", "Radhika Nagpal", "Bahar Haghighat"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2404.08390v2", "summary": "Robot swarms can effectively serve a variety of sensing and inspection\napplications. Certain inspection tasks require a binary classification\ndecision. This work presents an experimental setup for a surface inspection\ntask based on vibration sensing and studies a Bayesian two-outcome\ndecision-making algorithm in a swarm of miniaturized wheeled robots. The robots\nare tasked with individually inspecting and collectively classifying a 1mx1m\ntiled surface consisting of vibrating and non-vibrating tiles based on the\nmajority type of tiles. The robots sense vibrations using onboard IMUs and\nperform collision avoidance using a set of IR sensors. We develop a simulation\nand optimization framework leveraging the Webots robotic simulator and a\nParticle Swarm Optimization (PSO) method. We consider two existing information\nsharing strategies and propose a new one that allows the swarm to rapidly reach\naccurate classification decisions. We first find optimal parameters that allow\nefficient sampling in simulation and then evaluate our proposed strategy\nagainst the two existing ones using 100 randomized simulation and 10 real\nexperiments. We find that our proposed method compels the swarm to make\ndecisions at an accelerated rate, with an improvement of up to 20.52% in mean\ndecision time at only 0.78% loss in accuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2404.08390v2", "cate": "cs.RO", "date": "2024-04-12", "updated": "2025-07-10"}
{"id": "2507.07120", "title": "Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding", "authors": ["Nidhi Bhatia", "Ankit More", "Ritika Borkar", "Tiyasa Mitra", "Ramon Matas", "Ritchie Zhao", "Maximilian Golub", "Dheevatsa Mudigere", "Brian Pharris", "Bita Darvish Rouhani"], "categories": ["cs.DC", "cs.AI"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07120v1", "summary": "As LLMs scale to multi-million-token KV histories, real-time autoregressive\ndecoding under tight Token-to-Token Latency (TTL) constraints faces growing\npressure. Two core bottlenecks dominate: accessing Feed-Forward Network (FFN)\nweights and reading long KV caches. While Tensor Parallelism (TP) helps\nmitigate the cost of FFN weight reads, it does not scale well for attention.\nWhen TP width exceeds the number of KV heads, it leads to inefficient KV\nduplication, limits parallelism, and constrains batch size. Simultaneously,\nDRAM reads for long KV histories scale linearly with batch size, further\ncapping efficiency.\n  We introduce Helix Parallelism, a hybrid execution strategy that applies KV\nparallelism during attention to shard KV caches across GPUs, then reuses the\nsame GPUs for TP in dense LLMs or TPxExpert Parallel (EP) in MoEs during FFN\ncomputation. To preserve exact attention behavior, Helix includes a lightweight\ncommunication step. To minimize the exposed communication cost, we introduce\nHelix HOP-B. Helix HOP-B effectively minimizes communication overhead through\nbatchwise overlap, preserving low TTL while improving GPU efficiency. Compared\nto conventional parallelism approaches, Helix reduces TTL by up to 1.5x at\nfixed batch sizes and supports up to 32x larger batches under the same latency\nbudget for DeepSeek-R1, pushing forward the throughput-latency Pareto on\nBlackwell and making real-time inference with ultra-long-sequence practical.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07120v1", "cate": "cs.DC", "date": "2025-07-07", "updated": "2025-07-07"}
{"id": "2507.07276", "title": "TRIP: A Nonparametric Test to Diagnose Biased Feature Importance Scores", "authors": ["Aaron Foote", "Danny Krizanc"], "categories": ["cs.LG", "stat.ME", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted at the Workshop on Explainable Artificial Intelligence (XAI) at IJCAI 2025", "url": "http://arxiv.org/abs/2507.07276v1", "summary": "Along with accurate prediction, understanding the contribution of each\nfeature to the making of the prediction, i.e., the importance of the feature,\nis a desirable and arguably necessary component of a machine learning model.\nFor a complex model such as a random forest, such importances are not innate --\nas they are, e.g., with linear regression. Efficient methods have been created\nto provide such capabilities, with one of the most popular among them being\npermutation feature importance due to its efficiency, model-agnostic nature,\nand perceived intuitiveness. However, permutation feature importance has been\nshown to be misleading in the presence of dependent features as a result of the\ncreation of unrealistic observations when permuting the dependent features. In\nthis work, we develop TRIP (Test for Reliable Interpretation via Permutation),\na test requiring minimal assumptions that is able to detect unreliable\npermutation feature importance scores that are the result of model\nextrapolation. To build on this, we demonstrate how the test can be\ncomplemented in order to allow its use in high dimensional settings. Through\ntesting on simulated data and applications, our results show that the test can\nbe used to reliably detect when permutation feature importance scores are\nunreliable.", "comment": "Accepted at the Workshop on Explainable Artificial Intelligence (XAI)\n  at IJCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.07276v1", "cate": "cs.LG", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07333", "title": "Scalable and Realistic Virtual Try-on Application for Foundation Makeup with Kubelka-Munk Theory", "authors": ["Hui Pang", "Sunil Hadap", "Violetta Shevchenko", "Rahul Suresh", "Amin Banitalebi-Dehkordi"], "categories": ["cs.CV", "I.4.9"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Presented at the workshop Three questions about virtual try-on at CVPR 2025", "url": "http://arxiv.org/abs/2507.07333v1", "summary": "Augmented reality is revolutionizing beauty industry with virtual try-on\n(VTO) applications, which empowers users to try a wide variety of products\nusing their phones without the hassle of physically putting on real products. A\ncritical technical challenge in foundation VTO applications is the accurate\nsynthesis of foundation-skin tone color blending while maintaining the\nscalability of the method across diverse product ranges. In this work, we\npropose a novel method to approximate well-established Kubelka-Munk (KM) theory\nfor faster image synthesis while preserving foundation-skin tone color blending\nrealism. Additionally, we build a scalable end-to-end framework for realistic\nfoundation makeup VTO solely depending on the product information available on\ne-commerce sites. We validate our method using real-world makeup images,\ndemonstrating that our framework outperforms other techniques.", "comment": "Presented at the workshop Three questions about virtual try-on at\n  CVPR 2025", "pdf_url": "http://arxiv.org/pdf/2507.07333v1", "cate": "cs.CV", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2504.05793", "title": "Negotiating Strict Latency Limits for Dynamic Real-Time Services in Vehicular Time-Sensitive Networks", "authors": ["Timo Salomon", "Lisa Maile", "Philipp Meyer", "Franz Korf", "Thomas C. Schmidt"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.05793v2", "summary": "Future vehicles are expected to dynamically deploy in-vehicle applications\nwithin a Service-Oriented Architecture (SOA). Critical services operate under\nhard real-time constraints, which Time-Sensitive Networking (TSN) complements\non the in-vehicle Ethernet layer. TSN ensures deterministic communication\nbetween critical services and its Credit-Based Shaper (CBS) supports dynamic\nresource reservations. However, the dynamic nature of service deployment\nchallenges network resource configuration, since any new reservation may change\nthe latency of already validated flows. In addition, standard methods of\nworst-case latency analysis for CBS have been found incorrect, and current TSN\nstream reservation procedures lack mechanisms to signal application layer\nQuality-of-Service (QoS) requirements or verify deadlines. In this paper, we\npropose a QoS negotiation scheme within the automotive SOA that interacts with\nthe TSN network controller to reserve resources while ensuring latency bounds.\nWe comparatively evaluate reservation schemes using worst-case analysis and\nsimulations of a realistic In-Vehicle Network (IVN) for demonstrating their\nimpact on QoS guarantees, resource utilization, and setup times. We find that\nonly a reservation scheme utilizing per-queue delay budgets and network\ncalculus provides valid configurations and guarantees acceptable latency bounds\nthroughout the IVN. The proposed service negotiation mechanism efficiently\nestablishes 450 vehicular network reservations in just 11 ms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.05793v2", "cate": "cs.NI", "date": "2025-04-08", "updated": "2025-07-10"}
{"id": "2507.07507", "title": "Optimization of Probabilistic Constellation Shaping for Optical OFDM Systems with Clipping Distortion", "authors": ["Thanh V. Pham", "Susumu Ishihara"], "categories": ["eess.SY", "cs.IT", "cs.SY", "eess.SP", "math.IT"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07507v1", "summary": "Optical orthogonal frequency-division multiplexing (OFDM) and probabilistic\nconstellation shaping (PCS) have emerged as powerful techniques to enhance the\nperformance of optical wireless communications (OWC) systems. While PCS\nimproves spectral efficiency and adaptability, we show that its integration\nwith optical OFDM can inadvertently increase the peak-to-average power ratio\n(PAPR) of the signal, exacerbating clipping distortion due to signal clipping.\nThis letter investigates the impact of PCS on the PAPR of direct current-biased\noptical OFDM (DCO-OFDM) waveforms and proposes an optimization of PCS that\nmaximizes channel capacity, considering clipping distortion. The optimization\nproblem is shown to be complex and non-convex. We thus present a suboptimal yet\nefficient solving approach based on projected gradient descent to solve the\nproblem. Simulation results demonstrate the superiority of the proposed\napproach over the conventional uniform signaling, particularly under severe\nclipping distortion conditions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07507v1", "cate": "eess.SY", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2408.06553", "title": "Centralization vs. decentralization in multi-robot sweep coverage with ground robots and UAVs", "authors": ["Aryo Jamshidpey", "Mostafa Wahby", "Michael Allwright", "Weixu Zhu", "Marco Dorigo", "Mary Katherine Heinrich"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      IRIDIA, Universite Libre de Bruxelles, Brussels, Belgium, 2021", "url": "http://arxiv.org/abs/2408.06553v3", "summary": "In swarm robotics, decentralized control is often proposed as a more scalable\nand fault-tolerant alternative to centralized control. However, centralized\nbehaviors are often faster and more efficient than their decentralized\ncounterparts. In any given application, the goals and constraints of the task\nbeing solved should guide the choice to use centralized control, decentralized\ncontrol, or a combination of the two. Currently, the exact trade-offs that\nexist between centralization and decentralization are not well defined. In this\npaper, we study comparative performance assessment between centralization and\ndecentralization in the example task of sweep coverage, across five different\ntypes of multi-robot control structures: random walk, decentralized with\nbeacons, hybrid formation control using self-organizing hierarchy, centralized\nformation control, and predetermined. In all five approaches, the coverage task\nis completed by a group of ground robots. In each approach, except for the\nrandom walk, the ground robots are assisted by UAVs, acting as supervisors or\nbeacons. We compare the approaches in terms of three performance metrics for\nwhich centralized approaches are expected to have an advantage -- coverage\ncompleteness, coverage uniformity, and sweep completion time -- and two metrics\nfor which decentralized approaches are expected to have an advantage --\nscalability (4, 8, or 16 ground robots) and fault tolerance (0%, 25%, 50%, or\n75% ground robot failure).", "comment": "IRIDIA, Universite Libre de Bruxelles, Brussels, Belgium, 2021", "pdf_url": "http://arxiv.org/pdf/2408.06553v3", "cate": "cs.RO", "date": "2024-08-13", "updated": "2025-07-09"}
{"id": "2507.07126", "title": "DpDNet: An Dual-Prompt-Driven Network for Universal PET-CT Segmentation", "authors": ["Xinglong Liang", "Jiaju Huang", "Luyi Han", "Tianyu Zhang", "Xin Wang", "Yuan Gao", "Chunyao Lu", "Lishan Cai", "Tao Tan", "Ritse Mann"], "categories": ["eess.IV", "cs.AI"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07126v1", "summary": "PET-CT lesion segmentation is challenging due to noise sensitivity, small and\nvariable lesion morphology, and interference from physiological high-metabolic\nsignals. Current mainstream approaches follow the practice of one network\nsolving the segmentation of multiple cancer lesions by treating all cancers as\na single task. However, this overlooks the unique characteristics of different\ncancer types. Considering the specificity and similarity of different cancers\nin terms of metastatic patterns, organ preferences, and FDG uptake intensity,\nwe propose DpDNet, a Dual-Prompt-Driven network that incorporates specific\nprompts to capture cancer-specific features and common prompts to retain shared\nknowledge. Additionally, to mitigate information forgetting caused by the early\nintroduction of prompts, prompt-aware heads are employed after the decoder to\nadaptively handle multiple segmentation tasks. Experiments on a PET-CT dataset\nwith four cancer types show that DpDNet outperforms state-of-the-art models.\nFinally, based on the segmentation results, we calculated MTV, TLG, and SUVmax\nfor breast cancer survival analysis. The results suggest that DpDNet has the\npotential to serve as a valuable tool for personalized risk stratification,\nsupporting clinicians in optimizing treatment strategies and improving\noutcomes. Code is available at https://github.com/XinglongLiang08/DpDNet.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07126v1", "cate": "eess.IV", "date": "2025-07-08", "updated": "2025-07-08"}
{"id": "2507.07288", "title": "Natural Evolutionary Search meets Probabilistic Numerics", "authors": ["Pierre Osselin", "Masaki Adachi", "Xiaowen Dong", "Michael A. Osborne"], "categories": ["cs.LG", "cs.NE"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      8 pages, 5 figures (24 pages, 11 figures including references and appendices)", "url": "http://arxiv.org/abs/2507.07288v1", "summary": "Zeroth-order local optimisation algorithms are essential for solving\nreal-valued black-box optimisation problems. Among these, Natural Evolution\nStrategies (NES) represent a prominent class, particularly well-suited for\nscenarios where prior distributions are available. By optimising the objective\nfunction in the space of search distributions, NES algorithms naturally\nintegrate prior knowledge during initialisation, making them effective in\nsettings such as semi-supervised learning and user-prior belief frameworks.\nHowever, due to their reliance on random sampling and Monte Carlo estimates,\nNES algorithms can suffer from limited sample efficiency. In this paper, we\nintroduce a novel class of algorithms, termed Probabilistic Natural\nEvolutionary Strategy Algorithms (ProbNES), which enhance the NES framework\nwith Bayesian quadrature. We show that ProbNES algorithms consistently\noutperforms their non-probabilistic counterparts as well as global sample\nefficient methods such as Bayesian Optimisation (BO) or $\\pi$BO across a wide\nrange of tasks, including benchmark test functions, data-driven optimisation\ntasks, user-informed hyperparameter tuning tasks and locomotion tasks.", "comment": "8 pages, 5 figures (24 pages, 11 figures including references and\n  appendices)", "pdf_url": "http://arxiv.org/pdf/2507.07288v1", "cate": "cs.LG", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07340", "title": "Entity Re-identification in Visual Storytelling via Contrastive Reinforcement Learning", "authors": ["Daniel A. P. Oliveira", "David Martins de Matos"], "categories": ["cs.CV", "I.2; I.4; I.5; I.7"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      7 pages", "url": "http://arxiv.org/abs/2507.07340v1", "summary": "Visual storytelling systems, particularly large vision-language models,\nstruggle to maintain character and object identity across frames,\n  often failing to recognize when entities in different images represent the\nsame individuals or objects,\n  leading to inconsistent references and referential hallucinations.\n  This occurs because models lack explicit training on when to establish entity\nconnections across frames.\n  We propose a contrastive reinforcement learning approach that trains models\nto discriminate between coherent image sequences\n  and stories from unrelated images.\n  We extend the Story Reasoning dataset with synthetic negative examples to\nteach appropriate entity connection behavior.\n  We employ Direct Preference Optimization with a dual-component reward\nfunction that promotes grounding and re-identification of entities\n  in real stories while penalizing incorrect entity connections in synthetic\ncontexts.\n  Using this contrastive framework, we fine-tune Qwen Storyteller (based on\nQwen2.5-VL 7B).\n  Evaluation shows improvements in grounding mAP from 0.27 to 0.31 (+14.8%), F1\nfrom 0.35 to 0.41 (+17.1%).\n  Pronoun grounding accuracy improved across all pronoun types except ``its'',\n  and cross-frame character and object persistence increased\n  across all frame counts, with entities appearing in 5 or more frames\nadvancing from 29.3% to 33.3% (+13.7%).\n  Well-structured stories, containing the chain-of-thought and grounded story,\nincreased from 79.1% to 97.5% (+23.3%).", "comment": "7 pages", "pdf_url": "http://arxiv.org/pdf/2507.07340v1", "cate": "cs.CV", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2506.00283", "title": "Direct-to-Cell: A First Look into Starlink's Direct Satellite-to-Device Radio Access Network through Crowdsourced Measurements", "authors": ["Jorge Garcia-Cabeza", "Javier Albert-Smet", "Zoraida Frias", "Luis Mendo", "Santiago Andr√©s Azcoitia", "Eduardo Yraola"], "categories": ["cs.NI", "C.2.1"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      7 pages, 6 figures. Several corrections", "url": "http://arxiv.org/abs/2506.00283v4", "summary": "Low Earth Orbit (LEO) satellite mega-constellations have recently emerged as\na viable access solution for broadband services in underserved areas. In 2024,\nDirect Satellite-to-Device (DS2D) communications, which enable unmodified\nsmartphones to connect directly to spaceborne base stations, entered\nlarge-scale beta testing, with Starlink globally leading deployments. This\npaper presents the first measurement study of commercial DS2D services. Using\ncrowdsourced mobile network data collected in the U.S. between October 2024 and\nApril 2025, our research derives evidence-based insights into the capabilities,\nlimitations, and prospective evolution of DS2D technologies providing\nSupplemental Coverage from Space (SCS) services to expand existing mobile\nnetwork connectivity. We observe a strong correlation between the number of\nsatellites deployed and the expanding extension of observed measurements,\nconcentrated in accessible but poorly covered areas by terrestrial networks,\nsuch as national parks and large low-density counties. The data reveal stable\nphysical-layer value measurement throughout the observation period, with a\nlower median RSRP (24-dB difference) and a higher RSRQ (3 dB difference)\ncompared to terrestrial networks, reflecting the SMS-only usage of the DS2D\nnetwork during this period. Based on SINR measurements, we estimate the\nexpected performance of the announced DS2D mobile data service to be around 4\nMbps per beam in outdoor conditions. We also discuss strategies to expand this\ncapacity up to 12 Mbps in the future, depending on key regulatory decisions\nregarding satellite licenses, spectrum availability, and allowable radiated\npower levels.", "comment": "7 pages, 6 figures. Several corrections", "pdf_url": "http://arxiv.org/pdf/2506.00283v4", "cate": "cs.NI", "date": "2025-05-30", "updated": "2025-07-09"}
{"id": "2507.07520", "title": "Conditions for Large-Sample Majorization of Pairs of Flat States in Terms of $Œ±$-$z$ Relative Entropies", "authors": ["Frits Verhagen", "Marco Tomamichel", "Erkka Haapasalo"], "categories": ["quant-ph", "cs.IT", "math.IT"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07520v1", "summary": "In this work, we offer the first operational interpretation of the\n$\\alpha$-$z$ relative entropies, which were introduced by Jak\\v{s}i\\'{c} {\\it\net al.} \\cite{Jaksic2012} and Audenaert and Datta \\cite{Audenaert_Datta_2015},\nwhere the $\\alpha$ and $z$ parameters are truly independent from each other.\nNamely, we show that these relative entropies appear in the conditions for\nlarge-sample or catalytic relative majorization of pairs of flat states and\ncertain generalizations of them. Additionally, the optimal rate of converting\none such pair into another may be formulated in terms of the $\\alpha$-$z$\nrelative entropies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07520v1", "cate": "quant-ph", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2410.13973", "title": "MarineFormer: A Spatio-Temporal Attention Model for USV Navigation in Dynamic Marine Environments", "authors": ["Ehsan Kazemi", "Dechen Gao", "Iman Soltani"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.13973v4", "summary": "Autonomous navigation in marine environments can be extremely challenging,\nespecially in the presence of spatially varying flow disturbances and dynamic\nand static obstacles. In this work, we demonstrate that incorporating local\nflow field measurements fundamentally alters the nature of the problem,\ntransforming otherwise unsolvable navigation scenarios into tractable ones.\nHowever, the mere availability of flow data is not sufficient; it must be\neffectively fused with conventional sensory inputs such as ego-state and\nobstacle states. To this end, we propose \\textbf{MarineFormer}, a\nTransformer-based policy architecture that integrates two complementary\nattention mechanisms: spatial attention for sensor fusion, and temporal\nattention for capturing environmental dynamics. MarineFormer is trained\nend-to-end via reinforcement learning in a 2D simulated environment with\nrealistic flow features and obstacles. Extensive evaluations against classical\nand state-of-the-art baselines show that our approach improves episode\ncompletion success rate by nearly 23\\% while reducing path length. Ablation\nstudies further highlight the critical role of flow measurements and the\neffectiveness of our proposed architecture in leveraging them.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.13973v4", "cate": "cs.RO", "date": "2024-10-17", "updated": "2025-07-09"}
{"id": "2507.07133", "title": "Generative Panoramic Image Stitching", "authors": ["Mathieu Tuli", "Kaveh Kamali", "David B. Lindell"], "categories": ["cs.GR", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07133v1", "summary": "We introduce the task of generative panoramic image stitching, which aims to\nsynthesize seamless panoramas that are faithful to the content of multiple\nreference images containing parallax effects and strong variations in lighting,\ncamera capture settings, or style. In this challenging setting, traditional\nimage stitching pipelines fail, producing outputs with ghosting and other\nartifacts. While recent generative models are capable of outpainting content\nconsistent with multiple reference images, they fail when tasked with\nsynthesizing large, coherent regions of a panorama. To address these\nlimitations, we propose a method that fine-tunes a diffusion-based inpainting\nmodel to preserve a scene's content and layout based on multiple reference\nimages. Once fine-tuned, the model outpaints a full panorama from a single\nreference image, producing a seamless and visually coherent result that\nfaithfully integrates content from all reference images. Our approach\nsignificantly outperforms baselines for this task in terms of image quality and\nthe consistency of image structure and scene layout when evaluated on captured\ndatasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07133v1", "cate": "cs.GR", "date": "2025-07-08", "updated": "2025-07-08"}
{"id": "2507.07291", "title": "Estimating Dataset Dimension via Singular Metrics under the Manifold Hypothesis: Application to Inverse Problems", "authors": ["Paola Causin", "Alessio Marta"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07291v1", "summary": "High-dimensional datasets often exhibit low-dimensional geometric structures,\nas suggested by the manifold hypothesis, which implies that data lie on a\nsmooth manifold embedded in a higher-dimensional ambient space. While this\ninsight underpins many advances in machine learning and inverse problems, fully\nleveraging it requires to deal with three key tasks: estimating the intrinsic\ndimension (ID) of the manifold, constructing appropriate local coordinates, and\nlearning mappings between ambient and manifold spaces. In this work, we propose\na framework that addresses all these challenges using a Mixture of Variational\nAutoencoders (VAEs) and tools from Riemannian geometry. We specifically focus\non estimating the ID of datasets by analyzing the numerical rank of the VAE\ndecoder pullback metric. The estimated ID guides the construction of an atlas\nof local charts using a mixture of invertible VAEs, enabling accurate manifold\nparameterization and efficient inference. We how this approach enhances\nsolutions to ill-posed inverse problems, particularly in biomedical imaging, by\nenforcing that reconstructions lie on the learned manifold. Lastly, we explore\nthe impact of network pruning on manifold geometry and reconstruction quality,\nshowing that the intrinsic dimension serves as an effective proxy for\nmonitoring model capacity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07291v1", "cate": "cs.LG", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07374", "title": "PacGDC: Label-Efficient Generalizable Depth Completion with Projection Ambiguity and Consistency", "authors": ["Haotian Wang", "Aoran Xiao", "Xiaoqin Zhang", "Meng Yang", "Shijian Lu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2507.07374v1", "summary": "Generalizable depth completion enables the acquisition of dense metric depth\nmaps for unseen environments, offering robust perception capabilities for\nvarious downstream tasks. However, training such models typically requires\nlarge-scale datasets with metric depth labels, which are often labor-intensive\nto collect. This paper presents PacGDC, a label-efficient technique that\nenhances data diversity with minimal annotation effort for generalizable depth\ncompletion. PacGDC builds on novel insights into inherent ambiguities and\nconsistencies in object shapes and positions during 2D-to-3D projection,\nallowing the synthesis of numerous pseudo geometries for the same visual scene.\nThis process greatly broadens available geometries by manipulating scene scales\nof the corresponding depth maps. To leverage this property, we propose a new\ndata synthesis pipeline that uses multiple depth foundation models as scale\nmanipulators. These models robustly provide pseudo depth labels with varied\nscene scales, affecting both local objects and global layouts, while ensuring\nprojection consistency that supports generalization. To further diversify\ngeometries, we incorporate interpolation and relocation strategies, as well as\nunlabeled images, extending the data coverage beyond the individual use of\nfoundation models. Extensive experiments show that PacGDC achieves remarkable\ngeneralizability across multiple benchmarks, excelling in diverse scene\nsemantics/scales and depth sparsity/patterns under both zero-shot and few-shot\nsettings. Code: https://github.com/Wang-xjtu/PacGDC.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.07374v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2411.18199", "title": "Semantic Edge Computing and Semantic Communications in 6G Networks: A Unifying Survey and Research Challenges", "authors": ["Milin Zhang", "Mohammad Abdi", "Venkat R. Dasari", "Francesco Restuccia"], "categories": ["cs.LG", "cs.NI", "eess.SP"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted for publication in Elsevier Computer Networks", "url": "http://arxiv.org/abs/2411.18199v3", "summary": "Semantic Edge Computing (SEC) and Semantic Communications (SemComs) have been\nproposed as viable approaches to achieve real-time edge-enabled intelligence in\nsixth-generation (6G) wireless networks. On one hand, SemCom leverages the\nstrength of Deep Neural Networks (DNNs) to encode and communicate the semantic\ninformation only, while making it robust to channel distortions by compensating\nfor wireless effects. Ultimately, this leads to an improvement in the\ncommunication efficiency. On the other hand, SEC has leveraged distributed DNNs\nto divide the computation of a DNN across different devices based on their\ncomputational and networking constraints. Although significant progress has\nbeen made in both fields, the literature lacks a systematic view to connect\nboth fields. In this work, we fulfill the current gap by unifying the SEC and\nSemCom fields. We summarize the research problems in these two fields and\nprovide a comprehensive review of the state of the art with a focus on their\ntechnical strengths and challenges.", "comment": "Accepted for publication in Elsevier Computer Networks", "pdf_url": "http://arxiv.org/pdf/2411.18199v3", "cate": "cs.LG", "date": "2024-11-27", "updated": "2025-07-09"}
{"id": "2507.07647", "title": "Consistent and Asymptotically Efficient Localization from Bearing-only Measurements", "authors": ["Shenghua Hu", "Guangyang Zeng", "Wenchao Xue", "Haitao Fang", "Biqiang Mu"], "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07647v1", "summary": "We study the problem of signal source localization using bearing-only\nmeasurements. Initially, we present easily verifiable geometric conditions for\nsensor deployment to ensure the asymptotic identifiability of the model and\ndemonstrate the consistency and asymptotic efficiency of the maximum likelihood\n(ML) estimator. However, obtaining the ML estimator is challenging due to its\nassociation with a non-convex optimization problem. To address this, we propose\na two-step estimator that shares the same asymptotic properties as the ML\nestimator while offering low computational complexity, linear in the number of\nmeasurements. The primary challenge lies in obtaining a preliminary consistent\nestimator in the first step. To achieve this, we construct a linear\nleast-squares problem through algebraic operations on the measurement nonlinear\nmodel to first obtain a biased closed-form solution. We then eliminate the bias\nusing the data to yield an asymptotically unbiased and consistent estimator.\nThe key to this process is obtaining a consistent estimator of the variance of\nthe sine of the noise by taking the reciprocal of the maximum eigenvalue of a\nspecially constructed matrix from the data. In the second step, we perform a\nsingle Gauss-Newton iteration using the preliminary consistent estimator as the\ninitial value, achieving the same asymptotic properties as the ML estimator.\nFinally, simulation results demonstrate the superior performance of the\nproposed two-step estimator for large sample sizes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07647v1", "cate": "eess.SP", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07114", "title": "Distributed Training under Packet Loss", "authors": ["Erez Weintraub", "Ron Banner", "Ariel Orda"], "categories": ["cs.DC", "cs.LG"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07114v1", "summary": "State-of-the-art language and vision models are routinely trained across\nthousands of GPUs, often spanning multiple data-centers, yet today's\ndistributed frameworks still assume reliable connections (e.g., InfiniBand or\nRoCE). The resulting acknowledgment traffic and retransmissions inflate tail\nlatencies and limit scalability. Leveraging unreliable connections will reduce\nlatency but may sacrifice model accuracy and convergence once packets are\ndropped. A principled, end-to-end solution that preserves accuracy and\nconvergence guarantees under genuine packet loss has previously been missing.\nWe address this critical gap by introducing a novel distributed training\nframework capable of operating over unreliable connections, offering unbiased\ngradient aggregation and bounded parameter drift without modifying model code\nor optimizers. The key insight is a two-stage defense against missing messages:\n(i) Unbiased gradient aggregation: each worker reconstructs a consistent\ngradient estimate from whatever packets arrive, guaranteeing expectation-level\ncorrectness; and (ii) Bounded-drift parameter broadcasts: we prove the\ninter-worker model discrepancy remains O(1) even after arbitrarily many\niterations, preventing the unbounded divergence typical of asynchronous setups.\nAnalytical bounds are matched by experiments on the LLAMA2 7B model with 64\nGPUs: tolerating 10% random packet loss yields at most 0.8% perplexity change.\nThis work bridges the gap between communication-efficient datacenter protocols\nand the accuracy and generalization guarantees demanded by modern large-model\ntraining, enabling robust, high-throughput learning on commodity or wide-area\nnetworks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07114v1", "cate": "cs.DC", "date": "2025-07-02", "updated": "2025-07-02"}
{"id": "2411.05481", "title": "Relative Pose Estimation for Nonholonomic Robot Formation with UWB-IO Measurements", "authors": ["Kunrui Ze", "Wei Wang", "Shuoyu Yue", "Guibin Sun", "Kexin Liu", "Jinhu L√º"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      11 pages, 12 figures", "url": "http://arxiv.org/abs/2411.05481v2", "summary": "This article studies the problem of distributed formation control for\nmultiple robots by using onboard ultra wide band (UWB) distance and inertial\nodometer (IO) measurements.\n  Although this problem has been widely studied, a fundamental limitation of\nmost works is that they require each robot's pose and sensor measurements are\nexpressed in a common reference frame.\n  However, it is inapplicable for nonholonomic robot formations due to the\npractical difficulty of aligning IO measurements of individual robot in a\ncommon frame.\n  To address this problem, firstly, a concurrent-learning based estimator is\nfirstly proposed to achieve relative localization between neighboring robots in\na local frame.\n  Different from most relative localization methods in a global frame, both\nrelative position and orientation in a local frame are estimated with only UWB\nranging and IO\n  measurements.\n  Secondly, to deal with information loss caused by directed communication\ntopology, a cooperative localization algorithm is introduced to estimate the\nrelative pose to the leader robot.\n  Thirdly, based on the theoretical results on relative pose estimation, a\ndistributed formation tracking controller is proposed for nonholonomic robots.\n  Both 3D and 2D real-world experiments conducted on aerial robots and grounded\nrobots are provided to demonstrate the effectiveness of the proposed method.", "comment": "11 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2411.05481v2", "cate": "cs.RO", "date": "2024-11-08", "updated": "2025-07-10"}
{"id": "2507.07155", "title": "Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics", "authors": ["Xueqing Xu", "Boris Bolliet", "Adrian Dimitrov", "Andrew Laverick", "Francisco Villaescusa-Navarro", "Licong Xu", "√ç√±igo Zubeldia"], "categories": ["astro-ph.IM", "astro-ph.CO", "cs.AI"], "primary_category": "Subjects:       Instrumentation and Methods for Astrophysics (astro-ph.IM)", "pdf_link": null, "comments": "Comments:      Accepted contribution (spotlight) to the ICML 2025 Workshop on Machine Learning for Astrophysics; codes: this https URL , this https URL , this https URL", "url": "http://arxiv.org/abs/2507.07155v1", "summary": "We evaluate 9 Retrieval Augmented Generation (RAG) agent configurations on\n105 Cosmology Question-Answer (QA) pairs that we built specifically for this\npurpose.The RAG configurations are manually evaluated by a human expert, that\nis, a total of 945 generated answers were assessed. We find that currently the\nbest RAG agent configuration is with OpenAI embedding and generative model,\nyielding 91.4\\% accuracy. Using our human evaluation results we calibrate\nLLM-as-a-Judge (LLMaaJ) system which can be used as a robust proxy for human\nevaluation. These results allow us to systematically select the best RAG agent\nconfiguration for multi-agent system for autonomous scientific discovery in\nastrophysics (e.g., cmbagent presented in a companion paper) and provide us\nwith an LLMaaJ system that can be scaled to thousands of cosmology QA pairs. We\nmake our QA dataset, human evaluation results, RAG pipelines, and LLMaaJ system\npublicly available for further use by the astrophysics community.", "comment": "Accepted contribution (spotlight) to the ICML 2025 Workshop on\n  Machine Learning for Astrophysics; codes:\n  https://huggingface.co/datasets/ASTROANTS/CosmoPaperQA,\n  https://github.com/CMBAgents/cmbagent, https://github.com/CMBAgents/scirag", "pdf_url": "http://arxiv.org/pdf/2507.07155v1", "cate": "astro-ph.IM", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07292", "title": "Discretization-independent multifidelity operator learning for partial differential equations", "authors": ["Jacob Hauck", "Yanzhi Zhang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      33 pages, 9 figures, submitted to the Journal of Machine Learning Research", "url": "http://arxiv.org/abs/2507.07292v1", "summary": "We develop a new and general encode-approximate-reconstruct operator learning\nmodel that leverages learned neural representations of bases for input and\noutput function distributions. We introduce the concepts of \\textit{numerical\noperator learning} and \\textit{discretization independence}, which clarify the\nrelationship between theoretical formulations and practical realizations of\noperator learning models. Our model is discretization-independent, making it\nparticularly effective for multifidelity learning. We establish theoretical\napproximation guarantees, demonstrating uniform universal approximation under\nstrong assumptions on the input functions and statistical approximation under\nweaker conditions. To our knowledge, this is the first comprehensive study that\ninvestigates how discretization independence enables robust and efficient\nmultifidelity operator learning. We validate our method through extensive\nnumerical experiments involving both local and nonlocal PDEs, including\ntime-independent and time-dependent problems. The results show that\nmultifidelity training significantly improves accuracy and computational\nefficiency. Moreover, multifidelity training further enhances empirical\ndiscretization independence.", "comment": "33 pages, 9 figures, submitted to the Journal of Machine Learning\n  Research", "pdf_url": "http://arxiv.org/pdf/2507.07292v1", "cate": "cs.LG", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07379", "title": "Adaptive Particle-Based Shape Modeling for Anatomical Surface Correspondence", "authors": ["Hong Xu", "Shireen Y. Elhabian"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07379v1", "summary": "Particle-based shape modeling (PSM) is a family of approaches that\nautomatically quantifies shape variability across anatomical cohorts by\npositioning particles (pseudo landmarks) on shape surfaces in a consistent\nconfiguration. Recent advances incorporate implicit radial basis function\nrepresentations as self-supervised signals to better capture the complex\ngeometric properties of anatomical structures. However, these methods still\nlack self-adaptivity -- that is, the ability to automatically adjust particle\nconfigurations to local geometric features of each surface, which is essential\nfor accurately representing complex anatomical variability. This paper\nintroduces two mechanisms to increase surface adaptivity while maintaining\nconsistent particle configurations: (1) a novel neighborhood correspondence\nloss to enable high adaptivity and (2) a geodesic correspondence algorithm that\nregularizes optimization to enforce geodesic neighborhood consistency. We\nevaluate the efficacy and scalability of our approach on challenging datasets,\nproviding a detailed analysis of the adaptivity-correspondence trade-off and\nbenchmarking against existing methods on surface representation accuracy and\ncorrespondence metrics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07379v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2502.08118", "title": "Future Resource Bank for ISAC: Achieving Fast and Stable Win-Win Matching for Both Individuals and Coalitions", "authors": ["Houyi Qi", "Minghui Liwang", "Seyyedali Hosseinalipour", "Liqun Fu", "Sai Zou", "Wei Ni"], "categories": ["cs.DC", "cs.NI"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.08118v5", "summary": "Future wireless networks must support emerging applications where\nenvironmental awareness is as critical as data transmission. Integrated Sensing\nand Communication (ISAC) enables this vision by allowing base stations (BSs) to\nallocate bandwidth and power to mobile users (MUs) for communications and\ncooperative sensing. However, this resource allocation is highly challenging\ndue to: (i) dynamic resource demands from MUs and resource supply from BSs, and\n(ii) the selfishness of MUs and BSs. To address these challenges, existing\nsolutions rely on either real-time (online) resource trading, which incurs high\noverhead and failures, or static long-term (offline) resource contracts, which\nlack flexibility. To overcome these limitations, we propose the Future Resource\nBank for ISAC, a hybrid trading framework that integrates offline and online\nresource allocation through a level-wise client model, where MUs and their\ncoalitions negotiate with BSs. We introduce two mechanisms: (i) Role-Friendly\nWin-Win Matching (offRFW$^2$M), leveraging overbooking to establish risk-aware,\nstable contracts, and (ii) Effective Backup Win-Win Matching (onEBW$^2$M),\nwhich dynamically reallocates unmet demand and surplus supply. We theoretically\nprove stability, individual rationality, and weak Pareto optimality of these\nmechanisms. Through simulations, we show that our framework improves social\nwelfare, latency, and energy efficiency compared to existing methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.08118v5", "cate": "cs.DC", "date": "2025-02-12", "updated": "2025-07-10"}
{"id": "2507.07789", "title": "Computationally Efficient Information-Driven Optical Design with Interchanging Optimization", "authors": ["Eric Markley", "Henry Pinkard", "Leyla Kabuli", "Nalini Singh", "Laura Waller"], "categories": ["eess.IV", "cs.CE", "cs.CV", "cs.IT", "math.IT", "physics.optics"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07789v1", "summary": "Recent work has demonstrated that imaging systems can be evaluated through\nthe information content of their measurements alone, enabling\napplication-agnostic optical design that avoids computational decoding\nchallenges. Information-Driven Encoder Analysis Learning (IDEAL) was proposed\nto automate this process through gradient-based. In this work, we study IDEAL\nacross diverse imaging systems and find that it suffers from high memory usage,\nlong runtimes, and a potentially mismatched objective function due to\nend-to-end differentiability requirements. We introduce IDEAL with\nInterchanging Optimization (IDEAL-IO), a method that decouples density\nestimation from optical parameter optimization by alternating between fitting\nmodels to current measurements and updating optical parameters using fixed\nmodels for information estimation. This approach reduces runtime and memory\nusage by up to 6x while enabling more expressive density models that guide\noptimization toward superior designs. We validate our method on diffractive\noptics, lensless imaging, and snapshot 3D microscopy applications, establishing\ninformation-theoretic optimization as a practical, scalable strategy for\nreal-world imaging system design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07789v1", "cate": "eess.IV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07130", "title": "Ampere: Communication-Efficient and High-Accuracy Split Federated Learning", "authors": ["Zihan Zhang", "Leon Wong", "Blesson Varghese"], "categories": ["cs.DC", "cs.LG"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07130v1", "summary": "A Federated Learning (FL) system collaboratively trains neural networks\nacross devices and a server but is limited by significant on-device computation\ncosts. Split Federated Learning (SFL) systems mitigate this by offloading a\nblock of layers of the network from the device to a server. However, in doing\nso, it introduces large communication overheads due to frequent exchanges of\nintermediate activations and gradients between devices and the server and\nreduces model accuracy for non-IID data. We propose Ampere, a novel\ncollaborative training system that simultaneously minimizes on-device\ncomputation and device-server communication while improving model accuracy.\nUnlike SFL, which uses a global loss by iterative end-to-end training, Ampere\ndevelops unidirectional inter-block training to sequentially train the device\nand server block with a local loss, eliminating the transfer of gradients. A\nlightweight auxiliary network generation method decouples training between the\ndevice and server, reducing frequent intermediate exchanges to a single\ntransfer, which significantly reduces the communication overhead. Ampere\nmitigates the impact of data heterogeneity by consolidating activations\ngenerated by the trained device block to train the server block, in contrast to\nSFL, which trains on device-specific, non-IID activations. Extensive\nexperiments on multiple CNNs and transformers show that, compared to\nstate-of-the-art SFL baseline systems, Ampere (i) improves model accuracy by up\nto 13.26% while reducing training time by up to 94.6%, (ii) reduces\ndevice-server communication overhead by up to 99.1% and on-device computation\nby up to 93.13%, and (iii) reduces standard deviation of accuracy by 53.39% for\nvarious non-IID degrees highlighting superior performance when faced with\nheterogeneous data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07130v1", "cate": "cs.DC", "date": "2025-07-08", "updated": "2025-07-08"}
{"id": "2411.05548", "title": "Equivariant IMU Preintegration with Biases: a Galilean Group Approach", "authors": ["Giulio Delama", "Alessandro Fornasier", "Robert Mahony", "Stephan Weiss"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.05548v5", "summary": "This letter proposes a new approach for Inertial Measurement Unit (IMU)\npreintegration, a fundamental building block that can be leveraged in different\noptimization-based Inertial Navigation System (INS) localization solutions.\nInspired by recent advances in equivariant theory applied to biased INSs, we\nderive a discrete-time formulation of the IMU preintegration on\n${\\mathbf{Gal}(3) \\ltimes \\mathfrak{gal}(3)}$, the left-trivialization of the\ntangent group of the Galilean group $\\mathbf{Gal}(3)$. We define a novel\npreintegration error that geometrically couples the navigation states and the\nbias leading to lower linearization error. Our method improves in consistency\ncompared to existing preintegration approaches which treat IMU biases as a\nseparate state-space. Extensive validation against state-of-the-art methods,\nboth in simulation and with real-world IMU data, implementation in the Lie++\nlibrary, and open-source code are provided.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.05548v5", "cate": "cs.RO", "date": "2024-11-08", "updated": "2025-07-10"}
{"id": "2507.07186", "title": "Planted in Pretraining, Swayed by Finetuning: A Case Study on the Origins of Cognitive Biases in LLMs", "authors": ["Itay Itzhak", "Yonatan Belinkov", "Gabriel Stanovsky"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      CoLM 2025", "url": "http://arxiv.org/abs/2507.07186v1", "summary": "Large language models (LLMs) exhibit cognitive biases -- systematic\ntendencies of irrational decision-making, similar to those seen in humans.\nPrior work has found that these biases vary across models and can be amplified\nby instruction tuning. However, it remains unclear if these differences in\nbiases stem from pretraining, finetuning, or even random noise due to training\nstochasticity. We propose a two-step causal experimental approach to\ndisentangle these factors. First, we finetune models multiple times using\ndifferent random seeds to study how training randomness affects over $30$\ncognitive biases. Second, we introduce \\emph{cross-tuning} -- swapping\ninstruction datasets between models to isolate bias sources. This swap uses\ndatasets that led to different bias patterns, directly testing whether biases\nare dataset-dependent. Our findings reveal that while training randomness\nintroduces some variability, biases are mainly shaped by pretraining: models\nwith the same pretrained backbone exhibit more similar bias patterns than those\nsharing only finetuning data. These insights suggest that understanding biases\nin finetuned models requires considering their pretraining origins beyond\nfinetuning effects. This perspective can guide future efforts to develop\nprincipled strategies for evaluating and mitigating bias in LLMs.", "comment": "CoLM 2025", "pdf_url": "http://arxiv.org/pdf/2507.07186v1", "cate": "cs.CL", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07316", "title": "AdeptHEQ-FL: Adaptive Homomorphic Encryption for Federated Learning of Hybrid Classical-Quantum Models with Dynamic Layer Sparing", "authors": ["Md Abrar Jahin", "Taufikur Rahman Fuad", "M. F. Mridha", "Nafiz Fahad", "Md. Jakir Hossen"], "categories": ["cs.LG", "cs.CR"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted in 1st International Workshop on ICCV'25 BISCUIT (Biomedical Image and Signal Computing for Unbiasedness, Interpretability, and Trustworthiness)", "url": "http://arxiv.org/abs/2507.07316v1", "summary": "Federated Learning (FL) faces inherent challenges in balancing model\nperformance, privacy preservation, and communication efficiency, especially in\nnon-IID decentralized environments. Recent approaches either sacrifice formal\nprivacy guarantees, incur high overheads, or overlook quantum-enhanced\nexpressivity. We introduce AdeptHEQ-FL, a unified hybrid classical-quantum FL\nframework that integrates (i) a hybrid CNN-PQC architecture for expressive\ndecentralized learning, (ii) an adaptive accuracy-weighted aggregation scheme\nleveraging differentially private validation accuracies, (iii) selective\nhomomorphic encryption (HE) for secure aggregation of sensitive model layers,\nand (iv) dynamic layer-wise adaptive freezing to minimize communication\noverhead while preserving quantum adaptability. We establish formal privacy\nguarantees, provide convergence analysis, and conduct extensive experiments on\nthe CIFAR-10, SVHN, and Fashion-MNIST datasets. AdeptHEQ-FL achieves a $\\approx\n25.43\\%$ and $\\approx 14.17\\%$ accuracy improvement over Standard-FedQNN and\nFHE-FedQNN, respectively, on the CIFAR-10 dataset. Additionally, it reduces\ncommunication overhead by freezing less important layers, demonstrating the\nefficiency and practicality of our privacy-preserving, resource-aware design\nfor FL.", "comment": "Accepted in 1st International Workshop on ICCV'25 BISCUIT (Biomedical\n  Image and Signal Computing for Unbiasedness, Interpretability, and\n  Trustworthiness)", "pdf_url": "http://arxiv.org/pdf/2507.07316v1", "cate": "cs.LG", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07381", "title": "Multi-Scale Attention and Gated Shifting for Fine-Grained Event Spotting in Videos", "authors": ["Hao Xu", "Arbind Agrahari Baniya", "Sam Wells", "Mohamed Reda Bouadjenek", "Richard Dazeley", "Sunil Aryal"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07381v1", "summary": "Precise Event Spotting (PES) in sports videos requires frame-level\nrecognition of fine-grained actions from single-camera footage. Existing PES\nmodels typically incorporate lightweight temporal modules such as Gate Shift\nModule (GSM) or Gate Shift Fuse (GSF) to enrich 2D CNN feature extractors with\ntemporal context. However, these modules are limited in both temporal receptive\nfield and spatial adaptability. We propose a Multi-Scale Attention Gate Shift\nModule (MSAGSM) that enhances GSM with multi-scale temporal dilations and\nmulti-head spatial attention, enabling efficient modeling of both short- and\nlong-term dependencies while focusing on salient regions. MSAGSM is a\nlightweight plug-and-play module that can be easily integrated with various 2D\nbackbones. To further advance the field, we introduce the Table Tennis\nAustralia (TTA) dataset-the first PES benchmark for table tennis-containing\nover 4800 precisely annotated events. Extensive experiments across five PES\nbenchmarks demonstrate that MSAGSM consistently improves performance with\nminimal overhead, setting new state-of-the-art results.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07381v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07961", "title": "Sharp estimates of quantum covering problems via a novel trace inequality", "authors": ["Hao-Chung Cheng", "Li Gao", "Christoph Hirche", "Hao-Wei Huang", "Po-Chieh Liu"], "categories": ["quant-ph", "cs.IT", "math.FA", "math.IT", "math.OA"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07961v1", "summary": "In this paper, we prove a novel trace inequality involving two operators. As\napplications, we sharpen the one-shot achievability bound on the relative\nentropy error in a wealth of quantum covering-type problems, such as soft\ncovering, privacy amplification, convex splitting, quantum information\ndecoupling, and quantum channel simulation by removing some dimension-dependent\nfactors. Moreover, the established one-shot bounds extend to\ninfinite-dimensional separable Hilbert spaces as well. The proof techniques are\nbased on the recently developed operator layer cake theorem and an operator\nchange-of-variable argument, which are of independent interest.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07961v1", "cate": "quant-ph", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07144", "title": "M$^2$-MFP: A Multi-Scale and Multi-Level Memory Failure Prediction Framework for Reliable Cloud Infrastructure", "authors": ["Hongyi Xie", "Min Zhou", "Qiao Yu", "Jialiang Yu", "Zhenli Sheng", "Hong Xie", "Defu Lian"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07144v1", "summary": "As cloud services become increasingly integral to modern IT infrastructure,\nensuring hardware reliability is essential to sustain high-quality service.\nMemory failures pose a significant threat to overall system stability, making\naccurate failure prediction through the analysis of memory error logs (i.e.,\nCorrectable Errors) imperative. Existing memory failure prediction approaches\nhave notable limitations: rule-based expert models suffer from limited\ngeneralizability and low recall rates, while automated feature extraction\nmethods exhibit suboptimal performance. To address these limitations, we\npropose M$^2$-MFP: a Multi-scale and hierarchical memory failure prediction\nframework designed to enhance the reliability and availability of cloud\ninfrastructure. M$^2$-MFP converts Correctable Errors (CEs) into multi-level\nbinary matrix representations and introduces a Binary Spatial Feature Extractor\n(BSFE) to automatically extract high-order features at both DIMM-level and\nbit-level. Building upon the BSFE outputs, we develop a dual-path temporal\nmodeling architecture: 1) a time-patch module that aggregates multi-level\nfeatures within observation windows, and 2) a time-point module that employs\ninterpretable rule-generation trees trained on bit-level patterns. Experiments\non both benchmark datasets and real-world deployment show the superiority of\nM$^2$-MFP as it outperforms existing state-of-the-art methods by significant\nmargins. Code and data are available at this repository:\nhttps://github.com/hwcloud-RAS/M2-MFP.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07144v1", "cate": "cs.DC", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2412.20429", "title": "Multi-Scenario Reasoning: Unlocking Cognitive Autonomy in Humanoid Robots for Multimodal Understanding", "authors": ["Libo Wang"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:       this https URL", "url": "http://arxiv.org/abs/2412.20429v4", "summary": "To improve the cognitive autonomy of humanoid robots, this research proposes\na multi-scenario reasoning architecture to solve the technical shortcomings of\nmulti-modal understanding in this field. It draws on simulation based\nexperimental design that adopts multi-modal synthesis (visual, auditory,\ntactile) and builds a simulator \"Maha\" to perform the experiment. The findings\ndemonstrate the feasibility of this architecture in multimodal data. It\nprovides reference experience for the exploration of cross-modal interaction\nstrategies for humanoid robots in dynamic environments. In addition,\nmulti-scenario reasoning simulates the high-level reasoning mechanism of the\nhuman brain to humanoid robots at the cognitive level. This new concept\npromotes cross-scenario practical task transfer and semantic-driven action\nplanning. It heralds the future development of self-learning and autonomous\nbehavior of humanoid robots in changing scenarios.", "comment": "https://github.com/brucewang123456789/GeniusTrail/tree/main/Multi-Scenario%20Reasoning", "pdf_url": "http://arxiv.org/pdf/2412.20429v4", "cate": "cs.RO", "date": "2024-12-29", "updated": "2025-07-09"}
{"id": "2507.07188", "title": "Prompt Perturbations Reveal Human-Like Biases in LLM Survey Responses", "authors": ["Jens Rupprecht", "Georg Ahnert", "Markus Strohmaier"], "categories": ["cs.CL", "cs.AI", "cs.CY", "J.4"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      18 pages, 17 figures", "url": "http://arxiv.org/abs/2507.07188v1", "summary": "Large Language Models (LLMs) are increasingly used as proxies for human\nsubjects in social science surveys, but their reliability and susceptibility to\nknown response biases are poorly understood. This paper investigates the\nresponse robustness of LLMs in normative survey contexts -- we test nine\ndiverse LLMs on questions from the World Values Survey (WVS), applying a\ncomprehensive set of 11 perturbations to both question phrasing and answer\noption structure, resulting in over 167,000 simulated interviews. In doing so,\nwe not only reveal LLMs' vulnerabilities to perturbations but also reveal that\nall tested models exhibit a consistent \\textit{recency bias} varying in\nintensity, disproportionately favoring the last-presented answer option. While\nlarger models are generally more robust, all models remain sensitive to\nsemantic variations like paraphrasing and to combined perturbations. By\napplying a set of perturbations, we reveal that LLMs partially align with\nsurvey response biases identified in humans. This underscores the critical\nimportance of prompt design and robustness testing when using LLMs to generate\nsynthetic survey data.", "comment": "18 pages, 17 figures", "pdf_url": "http://arxiv.org/pdf/2507.07188v1", "cate": "cs.CL", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07320", "title": "Optimizing Communication and Device Clustering for Clustered Federated Learning with Differential Privacy", "authors": ["Dongyu Wei", "Xiaoren Xu", "Shiwen Mao", "Mingzhe Chen"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07320v1", "summary": "In this paper, a secure and communication-efficient clustered federated\nlearning (CFL) design is proposed. In our model, several base stations (BSs)\nwith heterogeneous task-handling capabilities and multiple users with\nnon-independent and identically distributed (non-IID) data jointly perform CFL\ntraining incorporating differential privacy (DP) techniques. Since each BS can\nprocess only a subset of the learning tasks and has limited wireless resource\nblocks (RBs) to allocate to users for federated learning (FL) model parameter\ntransmission, it is necessary to jointly optimize RB allocation and user\nscheduling for CFL performance optimization. Meanwhile, our considered CFL\nmethod requires devices to use their limited data and FL model information to\ndetermine their task identities, which may introduce additional communication\noverhead. We formulate an optimization problem whose goal is to minimize the\ntraining loss of all learning tasks while considering device clustering, RB\nallocation, DP noise, and FL model transmission delay. To solve the problem, we\npropose a novel dynamic penalty function assisted value decomposed multi-agent\nreinforcement learning (DPVD-MARL) algorithm that enables distributed BSs to\nindependently determine their connected users, RBs, and DP noise of the\nconnected users but jointly minimize the training loss of all learning tasks\nacross all BSs. Different from the existing MARL methods that assign a large\npenalty for invalid actions, we propose a novel penalty assignment scheme that\nassigns penalty depending on the number of devices that cannot meet\ncommunication constraints (e.g., delay), which can guide the MARL scheme to\nquickly find valid actions, thus improving the convergence speed. Simulation\nresults show that the DPVD-MARL can improve the convergence rate by up to 20%\nand the ultimate accumulated rewards by 15% compared to independent Q-learning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07320v1", "cate": "cs.LG", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07393", "title": "KeyRe-ID: Keypoint-Guided Person Re-Identification using Part-Aware Representation in Videos", "authors": ["Jinseong Kim", "Junghoon Song", "Gyeongseon Baek", "Byeongjoon Noh"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 pages, 2 figures,", "url": "http://arxiv.org/abs/2507.07393v1", "summary": "We propose \\textbf{KeyRe-ID}, a keypoint-guided video-based person\nre-identification framework consisting of global and local branches that\nleverage human keypoints for enhanced spatiotemporal representation learning.\nThe global branch captures holistic identity semantics through\nTransformer-based temporal aggregation, while the local branch dynamically\nsegments body regions based on keypoints to generate fine-grained, part-aware\nfeatures. Extensive experiments on MARS and iLIDS-VID benchmarks demonstrate\nstate-of-the-art performance, achieving 91.73\\% mAP and 97.32\\% Rank-1 accuracy\non MARS, and 96.00\\% Rank-1 and 100.0\\% Rank-5 accuracy on iLIDS-VID. The code\nfor this work will be publicly available on GitHub upon publication.", "comment": "10 pages, 2 figures,", "pdf_url": "http://arxiv.org/pdf/2507.07393v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2405.06554", "title": "Tradeoffs among Action Taking Policies Matter in Active Sequential Multi-Hypothesis Testing: the Optimal Error Exponent Region", "authors": ["Chia-Yu Hsu", "I-Hsiang Wang"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      Accepted for publication in the IEEE Transactions on Information Theory", "url": "http://arxiv.org/abs/2405.06554v3", "summary": "Reliability of sequential hypothesis testing can be greatly improved when the\ndecision maker is given the freedom to adaptively take an action that\ndetermines the distribution of the current collected sample. Such advantage of\nsampling adaptivity has been realized since Chernoff's seminal paper in 1959\n[1]. While a large body of works have explored and investigated the gain of\nadaptivity, in the general multiple-hypothesis setting, the fundamental limits\nof individual error probabilities have not been fully understood. In\nparticular, in the asymptotic regime as the expected stopping time tends to\ninfinity, the error exponents are only characterized in specific cases, such as\nthat of the total error probability. In this paper, we consider a general setup\nof active sequential multiple-hypothesis testing where at each time slot, a\ntemporally varying subset of data sources (out of a known set) emerges from\nwhich the decision maker can select to collect samples, subject to a family of\nexpected selection budget constraints. The selection of sources, understood as\nthe ``action'' at each time slot, is constrained in a predefined action space.\nAt the end of each time slot, the decision maker either decides to make the\ninference on the $M$ hypotheses, or continues to observe the data sources for\nthe next time slot. The optimal tradeoffs among $M(M-1)$ types of error\nexponents are characterized. A companion asymptotically optimal test that\nstrikes the balance between exploration and exploitation is proposed to achieve\nany target error exponents within the region. To the best of our knowledge,\nthis is the first time in the literature to identify such tradeoffs among error\nexponents in active sequential hypothesis testing, and it uncovers the tension\namong different action taking policies even in the basic setting of Chernoff\n[1].", "comment": "Accepted for publication in the IEEE Transactions on Information\n  Theory", "pdf_url": "http://arxiv.org/pdf/2405.06554v3", "cate": "cs.IT", "date": "2024-05-10", "updated": "2025-07-10"}
{"id": "2507.07352", "title": "Machine Learning-driven Multiscale MD Workflows: The Mini-MuMMI Experience", "authors": ["Lo√Øc Pottier", "Konstantia Georgouli", "Timothy S. Carpenter", "Fikret Aydin", "Jeremy O. B. Tempkin", "Dwight V. Nissley", "Frederick H. Streitz", "Thomas R. W. Scogland", "Peer-Timo Bremer", "Felice C. Lightstone", "Helgi I. Ing√≥lfsson"], "categories": ["cs.DC", "cs.LG"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07352v1", "summary": "Computational models have become one of the prevalent methods to model\ncomplex phenomena. To accurately model complex interactions, such as detailed\nbiomolecular interactions, scientists often rely on multiscale models comprised\nof several internal models operating at difference scales, ranging from\nmicroscopic to macroscopic length and time scales. Bridging the gap between\ndifferent time and length scales has historically been challenging but the\nadvent of newer machine learning (ML) approaches has shown promise for tackling\nthat task. Multiscale models require massive amounts of computational power and\na powerful workflow management system. Orchestrating ML-driven multiscale\nstudies on parallel systems with thousands of nodes is challenging, the\nworkflow must schedule, allocate and control thousands of simulations operating\nat different scales. Here, we discuss the massively parallel Multiscale\nMachine-Learned Modeling Infrastructure (MuMMI), a multiscale workflow\nmanagement infrastructure, that can orchestrate thousands of molecular dynamics\n(MD) simulations operating at different timescales, spanning from millisecond\nto nanosecond. More specifically, we introduce a novel version of MuMMI called\n\"mini-MuMMI\". Mini-MuMMI is a curated version of MuMMI designed to run on\nmodest HPC systems or even laptops whereas MuMMI requires larger HPC systems.\nWe demonstrate mini-MuMMI utility by exploring RAS-RAF membrane interactions\nand discuss the different challenges behind the generalization of multiscale\nworkflows and how mini-MuMMI can be leveraged to target a broader range of\napplications outside of MD and RAS-RAF interactions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07352v1", "cate": "cs.DC", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07357", "title": "Short-Term Gains, Long-Term Gaps: The Impact of GenAI and Search Technologies on Retention", "authors": ["Mahir Akgun", "Sacip Toker"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      To appear in the proceedings of the 26th International Conference on Artificial Intelligence in Education (AIED 2025)", "url": "http://arxiv.org/abs/2507.07357v1", "summary": "The rise of Generative AI (GenAI) tools, such as ChatGPT, has transformed how\nstudents access and engage with information, raising questions about their\nimpact on learning outcomes and retention. This study investigates how GenAI\n(ChatGPT), search engines (Google), and e-textbooks influence student\nperformance across tasks of varying cognitive complexity, based on Bloom's\nTaxonomy. Using a sample of 123 students, we examined performance in three\ntasks: [1] knowing and understanding, [2] applying, and [3] synthesizing,\nevaluating, and creating. Results indicate that ChatGPT and Google groups\noutperformed the control group in immediate assessments for lower-order\ncognitive tasks, benefiting from quick access to structured information.\nHowever, their advantage diminished over time, with retention test scores\naligning with those of the e-textbook group. For higher-order cognitive tasks,\nno significant differences were observed among groups, with the control group\ndemonstrating the highest retention. These findings suggest that while\nAI-driven tools facilitate immediate performance, they do not inherently\nreinforce long-term retention unless supported by structured learning\nstrategies. The study highlights the need for balanced technology integration\nin education, ensuring that AI tools are paired with pedagogical approaches\nthat promote deep cognitive engagement and knowledge retention.", "comment": "To appear in the proceedings of the 26th International Conference on\n  Artificial Intelligence in Education (AIED 2025)", "pdf_url": "http://arxiv.org/pdf/2507.07357v1", "cate": "cs.CY", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2502.13451", "title": "MapNav: A Novel Memory Representation via Annotated Semantic Maps for Vision-and-Language Navigation", "authors": ["Lingfeng Zhang", "Xiaoshuai Hao", "Qinwen Xu", "Qiang Zhang", "Xinyao Zhang", "Pengwei Wang", "Jing Zhang", "Zhongyuan Wang", "Shanghang Zhang", "Renjing Xu"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.13451v4", "summary": "Vision-and-language navigation (VLN) is a key task in Embodied AI, requiring\nagents to navigate diverse and unseen environments while following natural\nlanguage instructions. Traditional approaches rely heavily on historical\nobservations as spatio-temporal contexts for decision making, leading to\nsignificant storage and computational overhead. In this paper, we introduce\nMapNav, a novel end-to-end VLN model that leverages Annotated Semantic Map\n(ASM) to replace historical frames. Specifically, our approach constructs a\ntop-down semantic map at the start of each episode and update it at each\ntimestep, allowing for precise object mapping and structured navigation\ninformation. Then, we enhance this map with explicit textual labels for key\nregions, transforming abstract semantics into clear navigation cues and\ngenerate our ASM. MapNav agent using the constructed ASM as input, and use the\npowerful end-to-end capabilities of VLM to empower VLN. Extensive experiments\ndemonstrate that MapNav achieves state-of-the-art (SOTA) performance in both\nsimulated and real-world environments, validating the effectiveness of our\nmethod. Moreover, we will release our ASM generation source code and dataset to\nensure reproducibility, contributing valuable resources to the field. We\nbelieve that our proposed MapNav can be used as a new memory representation\nmethod in VLN, paving the way for future research in this field.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.13451v4", "cate": "cs.RO", "date": "2025-02-19", "updated": "2025-07-10"}
{"id": "2507.07201", "title": "MODA: A Unified 3D Diffusion Framework for Multi-Task Target-Aware Molecular Generation", "authors": ["Dong Xu", "Zhangfan Yang", "Sisi Yuan", "Jenna Xinyi Yao", "Jiangqiang Li", "Junkai Ji"], "categories": ["q-bio.BM", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Biomolecules (q-bio.BM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07201v1", "summary": "Three-dimensional molecular generators based on diffusion models can now\nreach near-crystallographic accuracy, yet they remain fragmented across tasks.\nSMILES-only inputs, two-stage pretrain-finetune pipelines, and\none-task-one-model practices hinder stereochemical fidelity, task alignment,\nand zero-shot transfer. We introduce MODA, a diffusion framework that unifies\nfragment growing, linker design, scaffold hopping, and side-chain decoration\nwith a Bayesian mask scheduler. During training, a contiguous spatial fragment\nis masked and then denoised in one pass, enabling the model to learn shared\ngeometric and chemical priors across tasks. Multi-task training yields a\nuniversal backbone that surpasses six diffusion baselines and three training\nparadigms on substructure, chemical property, interaction, and geometry.\nModel-C reduces ligand-protein clashes and substructure divergences while\nmaintaining Lipinski compliance, whereas Model-B preserves similarity but\ntrails in novelty and binding affinity. Zero-shot de novo design and\nlead-optimisation tests confirm stable negative Vina scores and high\nimprovement rates without force-field refinement. These results demonstrate\nthat a single-stage multi-task diffusion routine can replace two-stage\nworkflows for structure-based molecular design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07201v1", "cate": "q-bio.BM", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07323", "title": "Optimizing Model Splitting and Device Task Assignment for Deceptive Signal Assisted Private Multi-hop Split Learning", "authors": ["Dongyu Wei", "Xiaoren Xu", "Yuchen Liu", "H. Vincent Poor", "Mingzhe Chen"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07323v1", "summary": "In this paper, deceptive signal-assisted private split learning is\ninvestigated. In our model, several edge devices jointly perform collaborative\ntraining, and some eavesdroppers aim to collect the model and data information\nfrom devices. To prevent the eavesdroppers from collecting model and data\ninformation, a subset of devices can transmit deceptive signals. Therefore, it\nis necessary to determine the subset of devices used for deceptive signal\ntransmission, the subset of model training devices, and the models assigned to\neach model training device. This problem is formulated as an optimization\nproblem whose goal is to minimize the information leaked to eavesdroppers while\nmeeting the model training energy consumption and delay constraints. To solve\nthis problem, we propose a soft actor-critic deep reinforcement learning\nframework with intrinsic curiosity module and cross-attention (ICM-CA) that\nenables a centralized agent to determine the model training devices, the\ndeceptive signal transmission devices, the transmit power, and sub-models\nassigned to each model training device without knowing the position and\nmonitoring probability of eavesdroppers. The proposed method uses an ICM module\nto encourage the server to explore novel actions and states and a CA module to\ndetermine the importance of each historical state-action pair thus improving\ntraining efficiency. Simulation results demonstrate that the proposed method\nimproves the convergence rate by up to 3x and reduces the information leaked to\neavesdroppers by up to 13% compared to the traditional SAC algorithm.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07323v1", "cate": "cs.LG", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07394", "title": "Behave Your Motion: Habit-preserved Cross-category Animal Motion Transfer", "authors": ["Zhimin Zhang", "Bi'an Du", "Caoyuan Ma", "Zheng Wang", "Wei Hu"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07394v1", "summary": "Animal motion embodies species-specific behavioral habits, making the\ntransfer of motion across categories a critical yet complex task for\napplications in animation and virtual reality. Existing motion transfer\nmethods, primarily focused on human motion, emphasize skeletal alignment\n(motion retargeting) or stylistic consistency (motion style transfer), often\nneglecting the preservation of distinct habitual behaviors in animals. To\nbridge this gap, we propose a novel habit-preserved motion transfer framework\nfor cross-category animal motion. Built upon a generative framework, our model\nintroduces a habit-preservation module with category-specific habit encoder,\nallowing it to learn motion priors that capture distinctive habitual\ncharacteristics. Furthermore, we integrate a large language model (LLM) to\nfacilitate the motion transfer to previously unobserved species. To evaluate\nthe effectiveness of our approach, we introduce the DeformingThings4D-skl\ndataset, a quadruped dataset with skeletal bindings, and conduct extensive\nexperiments and quantitative analyses, which validate the superiority of our\nproposed model.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07394v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2501.11109", "title": "Estimation Error: Distribution and Pointwise Limits", "authors": ["Luca Barletta", "Alex Dytso", "Shlomo Shamai"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      9 pages. Extended version of a paper presented to IEEE ITW 2025. 2nd version: corrected a typo in Proposition 1 and in Theorem 1", "url": "http://arxiv.org/abs/2501.11109v2", "summary": "In this paper, we examine the distribution and convergence properties of the\nestimation error $W = X - \\hat{X}(Y)$, where $\\hat{X}(Y)$ is the Bayesian\nestimator of a random variable $X$ from a noisy observation $Y = X +\\sigma Z$\nwhere $\\sigma$ is the parameter indicating the strength of noise $Z$. Using the\nconditional expectation framework (that is, $\\hat{X}(Y)$ is the conditional\nmean), we define the normalized error $\\mathcal{E}_\\sigma = \\frac{W}{\\sigma}$\nand explore its properties.\n  Specifically, in the first part of the paper, we characterize the probability\ndensity function of $W$ and $\\mathcal{E}_\\sigma$. Along the way, we also find\nconditions for the existence of the inverse functions for the conditional\nexpectations. In the second part, we study pointwise (i.e., almost sure)\nconvergence of $\\mathcal{E}_\\sigma$ as $\\sigma \\to 0$ under various assumptions\nabout the noise and the underlying distributions. Our results extend some of\nthe previous limits of $\\mathcal{E}_\\sigma$ as $\\sigma \\to 0$ studied under the\n$L^2$ convergence, known as the \\emph{mmse dimension}, to the pointwise case.", "comment": "9 pages. Extended version of a paper presented to IEEE ITW 2025. 2nd\n  version: corrected a typo in Proposition 1 and in Theorem 1", "pdf_url": "http://arxiv.org/pdf/2501.11109v2", "cate": "cs.IT", "date": "2025-01-19", "updated": "2025-07-09"}
{"id": "2507.07671", "title": "Multi-agent Reinforcement Learning-based In-place Scaling Engine for Edge-cloud Systems", "authors": ["Jovan Prodanov", "Bla≈æ Bertalaniƒç", "Carolina Fortuna", "Shih-Kai Chou", "Matja≈æ Branko Juriƒç", "Ramon Sanchez-Iborra", "Jernej Hribar"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      Accepted at IEEE Cloud 2025", "url": "http://arxiv.org/abs/2507.07671v1", "summary": "Modern edge-cloud systems face challenges in efficiently scaling resources to\nhandle dynamic and unpredictable workloads. Traditional scaling approaches\ntypically rely on static thresholds and predefined rules, which are often\ninadequate for optimizing resource utilization and maintaining performance in\ndistributed and dynamic environments. This inefficiency hinders the\nadaptability and performance required in edge-cloud infrastructures, which can\nonly be achieved through the newly proposed in-place scaling. To address this\nproblem, we propose the Multi-Agent Reinforcement Learning-based In-place\nScaling Engine (MARLISE) that enables seamless, dynamic, reactive control with\nin-place resource scaling. We develop our solution using two Deep Reinforcement\nLearning algorithms: Deep Q-Network (DQN), and Proximal Policy Optimization\n(PPO). We analyze each version of the proposed MARLISE solution using dynamic\nworkloads, demonstrating their ability to ensure low response times of\nmicroservices and scalability. Our results show that MARLISE-based approaches\noutperform heuristic method in managing resource elasticity while maintaining\nmicroservice response times and achieving higher resource efficiency.", "comment": "Accepted at IEEE Cloud 2025", "pdf_url": "http://arxiv.org/pdf/2507.07671v1", "cate": "cs.DC", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07364", "title": "The Evolution of Scientific Credit: When Authorship Norms Impede Collaboration", "authors": ["Toby Handfield", "Kevin Zollman"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      45 pages, 18 figures. Code: this https URL", "url": "http://arxiv.org/abs/2507.07364v1", "summary": "Scientific authorship norms vary dramatically across disciplines, from\ncontribution-sensitive systems where first author is the greatest contributor\nand subsequent author order reflects relative input, to\ncontribution-insensitive conventions like alphabetical ordering or\nsenior-author-last. We develop evolutionary game-theoretic models to examine\nboth how these divergent norms emerge and their subsequent effects on\ncollaborative behavior. Our first model reveals that contribution-insensitive\nnorms evolve when researchers who sacrifice positional advantage face the\nstrongest adaptive pressure -- for example senior authors managing larger\ncollaboration portfolios or bearing heavier reputational stakes. This \"Red\nKing\" dynamic potentially explains why fields in which senior researchers\ncommand large labs, major grants, and extensive collaboration portfolios may\nparadoxically evolve conventions that favour junior-author positioning. Our\nsecond model demonstrates that established norms influence researchers'\nwillingness to collaborate, with contribution-sensitive norms consistently\noutperforming insensitive alternatives in fostering successful partnerships.\nContribution-insensitive norms create systematic coordination failures through\ntwo mechanisms: \"main contributor resentment\" when exceptional work goes\nunrecognized, and \"second contributor resentment\" when comparable efforts\nreceive unequal credit. These findings suggest that widely adopted practices\nlike senior-last positioning and alphabetical ordering may function as\ninstitutional frictions that impede valuable scientific collaborations rather\nthan neutral organizational conventions, potentially reducing overall\nscientific productivity across affected disciplines.", "comment": "45 pages, 18 figures. Code:\n  https://github.com/ghostleopold/author_order", "pdf_url": "http://arxiv.org/pdf/2507.07364v1", "cate": "cs.CY", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2502.20805", "title": "FunHOI: Annotation-Free 3D Hand-Object Interaction Generation via Functional Text Guidanc", "authors": ["Yongqi Tian", "Xueyu Sun", "Haoyuan He", "Linji Hao", "Ning Ding", "Caigui Jiang"], "categories": ["cs.RO", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.20805v2", "summary": "Hand-object interaction(HOI) is the fundamental link between human and\nenvironment, yet its dexterous and complex pose significantly challenges for\ngesture control. Despite significant advances in AI and robotics, enabling\nmachines to understand and simulate hand-object interactions, capturing the\nsemantics of functional grasping tasks remains a considerable challenge. While\nprevious work can generate stable and correct 3D grasps, they are still far\nfrom achieving functional grasps due to unconsidered grasp semantics. To\naddress this challenge, we propose an innovative two-stage framework,\nFunctional Grasp Synthesis Net (FGS-Net), for generating 3D HOI driven by\nfunctional text. This framework consists of a text-guided 3D model generator,\nFunctional Grasp Generator (FGG), and a pose optimization strategy, Functional\nGrasp Refiner (FGR). FGG generates 3D models of hands and objects based on text\ninput, while FGR fine-tunes the poses using Object Pose Approximator and energy\nfunctions to ensure the relative position between the hand and object aligns\nwith human intent and remains physically plausible. Extensive experiments\ndemonstrate that our approach achieves precise and high-quality HOI generation\nwithout requiring additional 3D annotation data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.20805v2", "cate": "cs.RO", "date": "2025-02-28", "updated": "2025-07-10"}
{"id": "2507.07318", "title": "SonicMotion: Dynamic Spatial Audio Soundscapes with Latent Diffusion Models", "authors": ["Christian Templin", "Yanda Zhu", "Hao Wang"], "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07318v1", "summary": "Spatial audio is an integral part of immersive entertainment, such as VR/AR,\nand has seen increasing popularity in cinema and music as well. The most common\nformat of spatial audio is described as first-order Ambisonics (FOA). We seek\nto extend recent advancements in FOA generative AI models to enable the\ngeneration of 3D scenes with dynamic sound sources. Our proposed end-to-end\nmodel, SonicMotion, comes in two variations which vary in their user input and\nlevel of precision in sound source localization. In addition to our model, we\nalso present a new dataset of simulated spatial audio-caption pairs. Evaluation\nof our models demonstrate that they are capable of matching the semantic\nalignment and audio quality of state of the art models while capturing the\ndesired spatial attributes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07318v1", "cate": "cs.SD", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07328", "title": "Bridging the Plausibility-Validity Gap by Fine-Tuning a Reasoning-Enhanced LLM for Chemical Synthesis and Discovery", "authors": ["Malikussaid", "Hilal Hudan Nuha"], "categories": ["cs.LG", "cs.AI", "cs.CE", "physics.chem-ph"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      42 pages, 8 figures, 1 equation, 2 algorithms, 31 tables, to be published in ISPACS Conference 2025, unabridged version", "url": "http://arxiv.org/abs/2507.07328v1", "summary": "Large Language Models (LLMs) often generate scientifically plausible but\nfactually invalid information, a challenge we term the \"plausibility-validity\ngap,\" particularly in specialized domains like chemistry. This paper presents a\nsystematic methodology to bridge this gap by developing a specialized\nscientific assistant. We utilized the Magistral Small model, noted for its\nintegrated reasoning capabilities, and fine-tuned it using Low-Rank Adaptation\n(LoRA). A key component of our approach was the creation of a \"dual-domain\ndataset,\" a comprehensive corpus curated from various sources encompassing both\nmolecular properties and chemical reactions, which was standardized to ensure\nquality. Our evaluation demonstrates that the fine-tuned model achieves\nsignificant improvements over the baseline model in format adherence, chemical\nvalidity of generated molecules, and the feasibility of proposed synthesis\nroutes. The results indicate a hierarchical learning pattern, where syntactic\ncorrectness is learned more readily than chemical possibility and synthesis\nfeasibility. While a comparative analysis with human experts revealed\ncompetitive performance in areas like chemical creativity and reasoning, it\nalso highlighted key limitations, including persistent errors in\nstereochemistry, a static knowledge cutoff, and occasional reference\nhallucination. This work establishes a viable framework for adapting generalist\nLLMs into reliable, specialized tools for chemical research, while also\ndelineating critical areas for future improvement.", "comment": "42 pages, 8 figures, 1 equation, 2 algorithms, 31 tables, to be\n  published in ISPACS Conference 2025, unabridged version", "pdf_url": "http://arxiv.org/pdf/2507.07328v1", "cate": "cs.LG", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07395", "title": "Seg-Wild: Interactive Segmentation based on 3D Gaussian Splatting for Unconstrained Image Collections", "authors": ["Yongtang Bao", "Chengjie Tang", "Yuze Wang", "Haojie Li"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07395v1", "summary": "Reconstructing and segmenting scenes from unconstrained photo collections\nobtained from the Internet is a novel but challenging task. Unconstrained photo\ncollections are easier to get than well-captured photo collections. These\nunconstrained images suffer from inconsistent lighting and transient\nocclusions, which makes segmentation challenging. Previous segmentation methods\ncannot address transient occlusions or accurately restore the scene's lighting\nconditions. Therefore, we propose Seg-Wild, an interactive segmentation method\nbased on 3D Gaussian Splatting for unconstrained image collections, suitable\nfor in-the-wild scenes. We integrate multi-dimensional feature embeddings for\neach 3D Gaussian and calculate the feature similarity between the feature\nembeddings and the segmentation target to achieve interactive segmentation in\nthe 3D scene. Additionally, we introduce the Spiky 3D Gaussian Cutter (SGC) to\nsmooth abnormal 3D Gaussians. We project the 3D Gaussians onto a 2D plane and\ncalculate the ratio of 3D Gaussians that need to be cut using the SAM mask. We\nalso designed a benchmark to evaluate segmentation quality in in-the-wild\nscenes. Experimental results demonstrate that compared to previous methods,\nSeg-Wild achieves better segmentation results and reconstruction quality. Our\ncode will be available at https://github.com/Sugar0725/Seg-Wild.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07395v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2501.18374", "title": "Proofs for Folklore Theorems on the Radon-Nikodym Derivative", "authors": ["Yaiza Bermudez", "Gaetan Bisson", "I√±aki Esnaola", "Samir M. Perlaza"], "categories": ["cs.IT", "math.HO", "math.IT", "math.ST", "stat.ML", "stat.TH"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      20 pages", "url": "http://arxiv.org/abs/2501.18374v3", "summary": "In this technical report, rigorous statements and formal proofs are presented\nfor both foundational and advanced folklore theorems on the Radon-Nikodym\nderivative. The cases of conditional and marginal probability measures are\ncarefully considered, which leads to an identity involving the sum of mutual\nand lautum information suggesting a new interpretation for such a sum.", "comment": "20 pages", "pdf_url": "http://arxiv.org/pdf/2501.18374v3", "cate": "cs.IT", "date": "2025-01-30", "updated": "2025-07-10"}
{"id": "2507.07932", "title": "KIS-S: A GPU-Aware Kubernetes Inference Simulator with RL-Based Auto-Scaling", "authors": ["Guilin Zhang", "Wulan Guo", "Ziqi Tan", "Qiang Guan", "Hailong Jiang"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      8 pages, 6 figures", "url": "http://arxiv.org/abs/2507.07932v1", "summary": "Autoscaling GPU inference workloads in Kubernetes remains challenging due to\nthe reactive and threshold-based nature of default mechanisms such as the\nHorizontal Pod Autoscaler (HPA), which struggle under dynamic and bursty\ntraffic patterns and lack integration with GPU-level metrics. We present KIS-S,\na unified framework that combines KISim, a GPU-aware Kubernetes Inference\nSimulator, with KIScaler, a Proximal Policy Optimization (PPO)-based\nautoscaler. KIScaler learns latency-aware and resource-efficient scaling\npolicies entirely in simulation, and is directly deployed without retraining.\nExperiments across four traffic patterns show that KIScaler improves average\nreward by 75.2%, reduces P95 latency up to 6.7x over CPU baselines, and\ngeneralizes without retraining. Our work bridges the gap between reactive\nautoscaling and intelligent orchestration for scalable GPU-accelerated\nenvironments.", "comment": "8 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.07932v1", "cate": "cs.DC", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07517", "title": "Vaccine Hesitancy on YouTube: a Competition between Health and Politics", "authors": ["Yelena Mejova", "Michele Tizzani"], "categories": ["cs.CY", "cs.SI"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      Digital Public Health Conference 2025", "url": "http://arxiv.org/abs/2507.07517v1", "summary": "YouTube has rapidly emerged as a predominant platform for content\nconsumption, effectively displacing conventional media such as television and\nnews outlets. A part of the enormous video stream uploaded to this platform\nincludes health-related content, both from official public health\norganizations, and from any individual or group that can make an account. The\nquality of information available on YouTube is a critical point of public\nhealth safety, especially when concerning major interventions, such as\nvaccination. This study differentiates itself from previous efforts of auditing\nYouTube videos on this topic by conducting a systematic daily collection of\nposted videos mentioning vaccination for the duration of 3 months. We show that\nthe competition for the public's attention is between public health messaging\nby institutions and individual educators on one side, and commentators on\nsociety and politics on the other, the latest contributing the most to the\nvideos expressing stances against vaccination. Videos opposing vaccination are\nmore likely to mention politicians and publication media such as podcasts,\nreports, and news analysis, on the other hand, videos in favor are more likely\nto mention specific diseases or health-related topics. Finally, we find that,\nat the time of analysis, only 2.7% of the videos have been taken down (by the\nplatform or the channel), despite 20.8% of the collected videos having a\nvaccination hesitant stance, pointing to a lack of moderation activity for\nhesitant content. The availability of high-quality information is essential to\nimprove awareness and compliance with public health interventions. Our findings\nhelp characterize the public discourse around vaccination on one of the largest\nmedia platforms, disentangling the role of the different creators and their\nstances, and as such, they provide important insights for public health\ncommunication policy.", "comment": "Digital Public Health Conference 2025", "pdf_url": "http://arxiv.org/pdf/2507.07517v1", "cate": "cs.CY", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2505.16042", "title": "Reference Free Platform Adaptive Locomotion for Quadrupedal Robots using a Dynamics Conditioned Policy", "authors": ["David Rytz", "Suyoung Choi", "Wanming Yu", "Wolfgang Merkt", "Jemin Hwangbo", "Ioannis Havoutis"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 pages, 6 tables, 5 figures", "url": "http://arxiv.org/abs/2505.16042v2", "summary": "This article presents Platform Adaptive Locomotion (PAL), a unified control\nmethod for quadrupedal robots with different morphologies and dynamics. We\nleverage deep reinforcement learning to train a single locomotion policy on\nprocedurally generated robots. The policy maps proprioceptive robot state\ninformation and base velocity commands into desired joint actuation targets,\nwhich are conditioned using a latent embedding of the temporally local system\ndynamics. We explore two conditioning strategies - one using a GRU-based\ndynamics encoder and another using a morphology-based property estimator - and\nshow that morphology-aware conditioning outperforms temporal dynamics encoding\nregarding velocity task tracking for our hardware test on ANYmal C. Our results\ndemonstrate that both approaches achieve robust zero-shot transfer across\nmultiple unseen simulated quadrupeds. Furthermore, we demonstrate the need for\ncareful robot reference modelling during training: exposing the policy to a\ndiverse set of robot morphologies and dynamics leads to improved\ngeneralization, reducing the velocity tracking error by up to 30% compared to\nthe baseline method. Despite PAL not surpassing the best-performing\nreference-free controller in all cases, our analysis uncovers critical design\nchoices and informs improvements to the state of the art.", "comment": "8 pages, 6 tables, 5 figures", "pdf_url": "http://arxiv.org/pdf/2505.16042v2", "cate": "cs.RO", "date": "2025-05-21", "updated": "2025-07-10"}
{"id": "2507.07335", "title": "Leveraging Manifold Embeddings for Enhanced Graph Transformer Representations and Learning", "authors": ["Ankit Jyothish", "Ali Jannesari"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07335v1", "summary": "Graph transformers typically embed every node in a single Euclidean space,\nblurring heterogeneous topologies. We prepend a lightweight Riemannian\nmixture-of-experts layer that routes each node to various kinds of manifold,\nmixture of spherical, flat, hyperbolic - best matching its local structure.\nThese projections provide intrinsic geometric explanations to the latent space.\nInserted into a state-of-the-art ensemble graph transformer, this projector\nlifts accuracy by up to 3% on four node-classification benchmarks. The ensemble\nmakes sure that both euclidean and non-euclidean features are captured.\nExplicit, geometry-aware projection thus sharpens predictive power while making\ngraph representations more interpretable.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07335v1", "cate": "cs.LG", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07348", "title": "Zero-Shot Context Generalization in Reinforcement Learning from Few Training Contexts", "authors": ["James Chapman", "Kedar Karhadkar", "Guido Montufar"], "categories": ["cs.LG", "I.2.6; I.2.8"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      10 pages, 8 figures, 3 tables, submitted to Neurips 2025", "url": "http://arxiv.org/abs/2507.07348v1", "summary": "Deep reinforcement learning (DRL) has achieved remarkable success across\nmultiple domains, including competitive games, natural language processing, and\nrobotics. Despite these advancements, policies trained via DRL often struggle\nto generalize to evaluation environments with different parameters. This\nchallenge is typically addressed by training with multiple contexts and/or by\nleveraging additional structure in the problem. However, obtaining sufficient\ntraining data across diverse contexts can be impractical in real-world\napplications. In this work, we consider contextual Markov decision processes\n(CMDPs) with transition and reward functions that exhibit regularity in context\nparameters. We introduce the context-enhanced Bellman equation (CEBE) to\nimprove generalization when training on a single context. We prove both\nanalytically and empirically that the CEBE yields a first-order approximation\nto the Q-function trained across multiple contexts. We then derive context\nsample enhancement (CSE) as an efficient data augmentation method for\napproximating the CEBE in deterministic control environments. We numerically\nvalidate the performance of CSE in simulation environments, showcasing its\npotential to improve generalization in DRL.", "comment": "10 pages, 8 figures, 3 tables, submitted to Neurips 2025", "pdf_url": "http://arxiv.org/pdf/2507.07348v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07410", "title": "EscherNet++: Simultaneous Amodal Completion and Scalable View Synthesis through Masked Fine-Tuning and Enhanced Feed-Forward 3D Reconstruction", "authors": ["Xinan Zhang", "Muhammad Zubair Irshad", "Anthony Yezzi", "Yi-Chang Tsai", "Zsolt Kira"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07410v1", "summary": "We propose EscherNet++, a masked fine-tuned diffusion model that can\nsynthesize novel views of objects in a zero-shot manner with amodal completion\nability. Existing approaches utilize multiple stages and complex pipelines to\nfirst hallucinate missing parts of the image and then perform novel view\nsynthesis, which fail to consider cross-view dependencies and require redundant\nstorage and computing for separate stages. Instead, we apply masked fine-tuning\nincluding input-level and feature-level masking to enable an end-to-end model\nwith the improved ability to synthesize novel views and conduct amodal\ncompletion. In addition, we empirically integrate our model with other\nfeed-forward image-to-mesh models without extra training and achieve\ncompetitive results with reconstruction time decreased by 95%, thanks to its\nability to synthesize arbitrary query views. Our method's scalable nature\nfurther enhances fast 3D reconstruction. Despite fine-tuning on a smaller\ndataset and batch size, our method achieves state-of-the-art results, improving\nPSNR by 3.9 and Volume IoU by 0.28 on occluded tasks in 10-input settings,\nwhile also generalizing to real-world occluded reconstruction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07410v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2502.06118", "title": "Token-Domain Multiple Access: Exploiting Semantic Orthogonality for Collision Mitigation", "authors": ["Li Qiao", "Mahdi Boloursaz Mashhadi", "Zhen Gao", "Deniz G√ºnd√ºz"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      Published at the IEEE INFOCOM Workshops 2025", "url": "http://arxiv.org/abs/2502.06118v2", "summary": "Token communications is an emerging generative semantic communication concept\nthat reduces transmission rates by using context and transformer-based token\nprocessing, with tokens serving as universal semantic units. In this paper, we\npropose a semantic multiple access scheme in the token domain, referred to as\nToDMA, where a large number of devices share a tokenizer and a modulation\ncodebook for source and channel coding, respectively. Specifically, the source\nsignal is tokenized into sequences, with each token modulated into a codeword.\nCodewords from multiple devices are transmitted simultaneously, resulting in\noverlap at the receiver. The receiver detects the transmitted tokens, assigns\nthem to their respective sources, and mitigates token collisions by leveraging\ncontext and semantic orthogonality across the devices' messages. Simulations\ndemonstrate that the proposed ToDMA framework outperforms context-unaware\northogonal and non-orthogonal communication methods in image transmission\ntasks, achieving lower latency and better image quality.", "comment": "Published at the IEEE INFOCOM Workshops 2025", "pdf_url": "http://arxiv.org/pdf/2502.06118v2", "cate": "cs.IT", "date": "2025-02-10", "updated": "2025-07-10"}
{"id": "1602.03104", "title": "A Graph Isomorphism-based Decentralized Algorithm for Modular Robot Configuration Formation", "authors": ["Ayan Dutta", "Prithviraj Dasgupta", "Carl Nelson"], "categories": ["cs.RO", "cs.DC", "cs.DS"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/1602.03104v1", "summary": "We consider the problem of configuration formation in modular robot systems\nwhere a set of modules that are initially in different configurations and\nlocated at different locations are required to assume appropriate positions so\nthat they can get into a new, user-specified, target configuration. We propose\na novel algorithm based on graph isomorphism, where the modules select\nlocations or spots in the target configuration using a utility-based framework,\nwhile retaining their original configuration to the greatest extent possible,\nto reduce the time and energy required by the modules to assume the target\nconfiguration. We have shown analytically that our proposed algorithm is\ncomplete and guarantees a Pareto-optimal allocation. Experimental simulations\nof our algorithm with different number of modules in different initial\nconfigurations and located initially at different locations, show that the\nplanning time of our algorithm is nominal (order of msec. for 100 modules). We\nhave also compared our algorithm against a market-based allocation algorithm\nand shown that our proposed algorithm performs better in terms of time and\nnumber of messages exchanged.", "comment": null, "pdf_url": "http://arxiv.org/pdf/1602.03104v1", "cate": "cs.RO", "date": "2016-02-09", "updated": "2016-02-09"}
{"id": "2507.07703", "title": "AI Human Impact: Toward a Model for Ethical Investing in AI-Intensive Companies", "authors": ["James Brusseau"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07703v1", "summary": "Does AI conform to humans, or will we conform to AI? An ethical evaluation of\nAI-intensive companies will allow investors to knowledgeably participate in the\ndecision. The evaluation is built from nine performance indicators that can be\nanalyzed and scored to reflect a technology's human-centering. The result is\nobjective investment guidance, as well as investors empowered to act in\naccordance with their own values. Incorporating ethics into financial decisions\nis a strategy that will be recognized by participants in environmental, social,\nand governance investing, however, this paper argues that conventional ESG\nframeworks are inadequate to companies that function with AI at their core.\nFully accounting for contemporary big data, predictive analytics, and machine\nlearning requires specialized metrics customized from established AI ethics\nprinciples. With these metrics established, the larger goal is a model for\nhumanist investing in AI-intensive companies that is intellectually robust,\nmanageable for analysts, useful for portfolio managers, and credible for\ninvestors.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07703v1", "cate": "cs.CY", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2506.22827", "title": "Hierarchical Vision-Language Planning for Multi-Step Humanoid Manipulation", "authors": ["Andr√© Schakkal", "Ben Zandonati", "Zhutian Yang", "Navid Azizan"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted at the RSS 2025 Workshop on Robot Planning in the Era of Foundation Models", "url": "http://arxiv.org/abs/2506.22827v3", "summary": "Enabling humanoid robots to reliably execute complex multi-step manipulation\ntasks is crucial for their effective deployment in industrial and household\nenvironments. This paper presents a hierarchical planning and control framework\ndesigned to achieve reliable multi-step humanoid manipulation. The proposed\nsystem comprises three layers: (1) a low-level RL-based controller responsible\nfor tracking whole-body motion targets; (2) a mid-level set of skill policies\ntrained via imitation learning that produce motion targets for different steps\nof a task; and (3) a high-level vision-language planning module that determines\nwhich skills should be executed and also monitors their completion in real-time\nusing pretrained vision-language models (VLMs). Experimental validation is\nperformed on a Unitree G1 humanoid robot executing a non-prehensile\npick-and-place task. Over 40 real-world trials, the hierarchical system\nachieved a 73% success rate in completing the full manipulation sequence. These\nexperiments confirm the feasibility of the proposed hierarchical system,\nhighlighting the benefits of VLM-based skill planning and monitoring for\nmulti-step manipulation scenarios. See https://vlp-humanoid.github.io/ for\nvideo demonstrations of the policy rollout.", "comment": "Accepted at the RSS 2025 Workshop on Robot Planning in the Era of\n  Foundation Models", "pdf_url": "http://arxiv.org/pdf/2506.22827v3", "cate": "cs.RO", "date": "2025-06-28", "updated": "2025-07-10"}
{"id": "2507.07359", "title": "Goal-Oriented Sequential Bayesian Experimental Design for Causal Learning", "authors": ["Zheyu Zhang", "Jiayuan Dong", "Jie Liu", "Xun Huan"], "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      10 pages, 6 figures", "url": "http://arxiv.org/abs/2507.07359v1", "summary": "We present GO-CBED, a goal-oriented Bayesian framework for sequential causal\nexperimental design. Unlike conventional approaches that select interventions\naimed at inferring the full causal model, GO-CBED directly maximizes the\nexpected information gain (EIG) on user-specified causal quantities of\ninterest, enabling more targeted and efficient experimentation. The framework\nis both non-myopic, optimizing over entire intervention sequences, and\ngoal-oriented, targeting only model aspects relevant to the causal query. To\naddress the intractability of exact EIG computation, we introduce a variational\nlower bound estimator, optimized jointly through a transformer-based policy\nnetwork and normalizing flow-based variational posteriors. The resulting policy\nenables real-time decision-making via an amortized network. We demonstrate that\nGO-CBED consistently outperforms existing baselines across various causal\nreasoning and discovery tasks-including synthetic structural causal models and\nsemi-synthetic gene regulatory networks-particularly in settings with limited\nexperimental budgets and complex causal mechanisms. Our results highlight the\nbenefits of aligning experimental design objectives with specific research\ngoals and of forward-looking sequential planning.", "comment": "10 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.07359v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07354", "title": "Learning from positive and unlabeled examples -Finite size sample bounds", "authors": ["Farnam Mansouri", "Shai Ben-David"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07354v1", "summary": "PU (Positive Unlabeled) learning is a variant of supervised classification\nlearning in which the only labels revealed to the learner are of positively\nlabeled instances. PU learning arises in many real-world applications. Most\nexisting work relies on the simplifying assumptions that the positively labeled\ntraining data is drawn from the restriction of the data generating distribution\nto positively labeled instances and/or that the proportion of positively\nlabeled points (a.k.a. the class prior) is known apriori to the learner. This\npaper provides a theoretical analysis of the statistical complexity of PU\nlearning under a wider range of setups. Unlike most prior work, our study does\nnot assume that the class prior is known to the learner. We prove upper and\nlower bounds on the required sample sizes (of both the positively labeled and\nthe unlabeled samples).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07354v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07415", "title": "EPIC: Efficient Prompt Interaction for Text-Image Classification", "authors": ["Xinyao Yu", "Hao Sun", "Zeyu Ling", "Ziwei Niu", "Zhenjia Bai", "Rui Qin", "Yen-Wei Chen", "Lanfen Lin"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      arXiv admin note: substantial text overlap with arXiv:2401.14856", "url": "http://arxiv.org/abs/2507.07415v1", "summary": "In recent years, large-scale pre-trained multimodal models (LMMs) generally\nemerge to integrate the vision and language modalities, achieving considerable\nsuccess in multimodal tasks, such as text-image classification. The growing\nsize of LMMs, however, results in a significant computational cost for\nfine-tuning these models for downstream tasks. Hence, prompt-based interaction\nstrategy is studied to align modalities more efficiently. In this context, we\npropose a novel efficient prompt-based multimodal interaction strategy, namely\nEfficient Prompt Interaction for text-image Classification (EPIC).\nSpecifically, we utilize temporal prompts on intermediate layers, and integrate\ndifferent modalities with similarity-based prompt interaction, to leverage\nsufficient information exchange between modalities. Utilizing this approach,\nour method achieves reduced computational resource consumption and fewer\ntrainable parameters (about 1\\% of the foundation model) compared to other\nfine-tuning strategies. Furthermore, it demonstrates superior performance on\nthe UPMC-Food101 and SNLI-VE datasets, while achieving comparable performance\non the MM-IMDB dataset.", "comment": "arXiv admin note: substantial text overlap with arXiv:2401.14856", "pdf_url": "http://arxiv.org/pdf/2507.07415v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2503.03233", "title": "Sensing Rate Optimization for Multi-Band Cooperative ISAC Systems", "authors": ["Nemanja Stefan Peroviƒá", "Mark F. Flanagan", "Le-Nam Tran"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      5 pages, 2 figures", "url": "http://arxiv.org/abs/2503.03233v2", "summary": "Integrated sensing and communication (ISAC) has been recognized as one of the\nkey technologies for future wireless networks, which potentially need to\noperate in multiple frequency bands to satisfy ever-increasing demands for both\ncommunication and sensing services. Motivated by this, we consider the sum\nsensing rate (SR) optimization for a cooperative ISAC system with linear\nprecoding, where each base station (BS) works in a different frequency band.\nWith this aim, we propose an optimization algorithm based on the semi-definite\nrank relaxation that introduces covariance matrices as optimization variables,\nand we apply the inner approximation (IA) method to deal with the nonconvexity\nof the resulting problem. Simulation results show that the proposed algorithm\nincreases the SR by approximately 25 % and 40 % compared to the case of equal\npower distribution in a cooperative ISAC system with two and three BSs,\nrespectively. Additionally, the algorithm converges in only a few iterations,\nwhile its most beneficial implementation scenario is in the low power regime", "comment": "5 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2503.03233v2", "cate": "cs.IT", "date": "2025-03-05", "updated": "2025-07-10"}
{"id": "2507.07589", "title": "Stress Monitoring in Healthcare: An Ensemble Machine Learning Framework Using Wearable Sensor Data", "authors": ["Arpana Sinhal", "Anay Sinhal", "Amit Sinhal"], "categories": ["cs.LG", "cs.DC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07589v1", "summary": "Healthcare professionals, particularly nurses, face elevated occupational\nstress, a concern amplified during the COVID-19 pandemic. While wearable\nsensors offer promising avenues for real-time stress monitoring, existing\nstudies often lack comprehensive datasets and robust analytical frameworks.\nThis study addresses these gaps by introducing a multimodal dataset comprising\nphysiological signals, electrodermal activity, heart rate and skin temperature.\nA systematic literature review identified limitations in prior stress-detection\nmethodologies, particularly in handling class imbalance and optimizing model\ngeneralizability. To overcome these challenges, the dataset underwent\npreprocessing with the Synthetic Minority Over sampling Technique (SMOTE),\nensuring balanced representation of stress states. Advanced machine learning\nmodels including Random Forest, XGBoost and a Multi-Layer Perceptron (MLP) were\nevaluated and combined into a Stacking Classifier to leverage their collective\npredictive strengths. By using a publicly accessible dataset and a reproducible\nanalytical pipeline, this work advances the development of deployable\nstress-monitoring systems, offering practical implications for safeguarding\nhealthcare workers' mental health. Future research directions include expanding\ndemographic diversity and exploring edge-computing implementations for low\nlatency stress alerts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07589v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07765", "title": "Distributed and Decentralised Training: Technical Governance Challenges in a Shifting AI Landscape", "authors": ["Jakub Kry≈õ", "Yashvardhan Sharma", "Janet Egan"], "categories": ["cs.CY", "cs.LG"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      Accepted as an oral presentation at the Technical AI Governance Workshop (ICML 2025)", "url": "http://arxiv.org/abs/2507.07765v1", "summary": "Advances in low-communication training algorithms are enabling a shift from\ncentralised model training to compute setups that are either distributed across\nmultiple clusters or decentralised via community-driven contributions. This\npaper distinguishes these two scenarios - distributed and decentralised\ntraining - which are little understood and often conflated in policy discourse.\nWe discuss how they could impact technical AI governance through an increased\nrisk of compute structuring, capability proliferation, and the erosion of\ndetectability and shutdownability. While these trends foreshadow a possible new\nparadigm that could challenge key assumptions of compute governance, we\nemphasise that certain policy levers, like export controls, remain relevant. We\nalso acknowledge potential benefits of decentralised AI, including\nprivacy-preserving training runs that could unlock access to more data, and\nmitigating harmful power concentration. Our goal is to support more precise\npolicymaking around compute, capability proliferation, and decentralised AI\ndevelopment.", "comment": "Accepted as an oral presentation at the Technical AI Governance\n  Workshop (ICML 2025)", "pdf_url": "http://arxiv.org/pdf/2507.07765v1", "cate": "cs.CY", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07508", "title": "The Pandora's Box Problem with Sequential Inspections", "authors": ["Ali Aouad", "Jingwei Ji", "Yaron Shaposhnik"], "categories": ["cs.CE", "econ.GN", "q-fin.EC"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07508v1", "summary": "The Pandora's box problem (Weitzman 1979) is a core model in economic theory\nthat captures an agent's (Pandora's) search for the best alternative (box). We\nstudy an important generalization of the problem where the agent can either\nfully open boxes for a certain fee to reveal their exact values or partially\nopen them at a reduced cost. This introduces a new tradeoff between information\nacquisition and cost efficiency. We establish a hardness result and employ an\narray of techniques in stochastic optimization to provide a comprehensive\nanalysis of this model. This includes (1) the identification of structural\nproperties of the optimal policy that provide insights about optimal decisions;\n(2) the derivation of problem relaxations and provably near-optimal solutions;\n(3) the characterization of the optimal policy in special yet non-trivial\ncases; and (4) an extensive numerical study that compares the performance of\nvarious policies, and which provides additional insights about the optimal\npolicy. Throughout, we show that intuitive threshold-based policies that extend\nthe Pandora's box optimal solution can effectively guide search decisions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07508v1", "cate": "cs.CE", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.06562", "title": "KLEIYN : A Quadruped Robot with an Active Waist for Both Locomotion and Wall Climbing", "authors": ["Keita Yoneda", "Kento Kawaharazuka", "Temma Suzuki", "Takahiro Hattori", "Kei Okada"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted at IROS2025, website - this https URL , YouTube - this https URL", "url": "http://arxiv.org/abs/2507.06562v2", "summary": "In recent years, advancements in hardware have enabled quadruped robots to\noperate with high power and speed, while robust locomotion control using\nreinforcement learning (RL) has also been realized. As a result, expectations\nare rising for the automation of tasks such as material transport and\nexploration in unknown environments. However, autonomous locomotion in rough\nterrains with significant height variations requires vertical movement, and\nrobots capable of performing such movements stably, along with their control\nmethods, have not yet been fully established. In this study, we developed the\nquadruped robot KLEIYN, which features a waist joint, and aimed to expand\nquadruped locomotion by enabling chimney climbing through RL. To facilitate the\nlearning of vertical motion, we introduced Contact-Guided Curriculum Learning\n(CGCL). As a result, KLEIYN successfully climbed walls ranging from 800 mm to\n1000 mm in width at an average speed of 150 mm/s, 50 times faster than\nconventional robots. Furthermore, we demonstrated that the introduction of a\nwaist joint improves climbing performance, particularly enhancing tracking\nability on narrow walls.", "comment": "Accepted at IROS2025, website -\n  https://keitayoneda.github.io/kleiyn-chimney-climbing/, YouTube -\n  https://www.youtube.com/watch?v=cLfUhyNFOeY", "pdf_url": "http://arxiv.org/pdf/2507.06562v2", "cate": "cs.RO", "date": "2025-07-09", "updated": "2025-07-10"}
{"id": "2507.07373", "title": "Atherosclerosis through Hierarchical Explainable Neural Network Analysis", "authors": ["Irsyad Adam", "Steven Swee", "Erika Yilin", "Ethan Ji", "William Speier", "Dean Wang", "Alex Bui", "Wei Wang", "Karol Watson", "Peipei Ping"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07373v1", "summary": "In this work, we study the problem pertaining to personalized classification\nof subclinical atherosclerosis by developing a hierarchical graph neural\nnetwork framework to leverage two characteristic modalities of a patient:\nclinical features within the context of the cohort, and molecular data unique\nto individual patients. Current graph-based methods for disease classification\ndetect patient-specific molecular fingerprints, but lack consistency and\ncomprehension regarding cohort-wide features, which are an essential\nrequirement for understanding pathogenic phenotypes across diverse\natherosclerotic trajectories. Furthermore, understanding patient subtypes often\nconsiders clinical feature similarity in isolation, without integration of\nshared pathogenic interdependencies among patients. To address these\nchallenges, we introduce ATHENA: Atherosclerosis Through Hierarchical\nExplainable Neural Network Analysis, which constructs a novel hierarchical\nnetwork representation through integrated modality learning; subsequently, it\noptimizes learned patient-specific molecular fingerprints that reflect\nindividual omics data, enforcing consistency with cohort-wide patterns. With a\nprimary clinical dataset of 391 patients, we demonstrate that this\nheterogeneous alignment of clinical features with molecular interaction\npatterns has significantly boosted subclinical atherosclerosis classification\nperformance across various baselines by up to 13% in area under the receiver\noperating curve (AUC) and 20% in F1 score. Taken together, ATHENA enables\nmechanistically-informed patient subtype discovery through explainable AI\n(XAI)-driven subnetwork clustering; this novel integration framework\nstrengthens personalized intervention strategies, thereby improving the\nprediction of atherosclerotic disease progression and management of their\nclinical actionable outcomes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07373v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07375", "title": "Bradley-Terry and Multi-Objective Reward Modeling Are Complementary", "authors": ["Zhiwei Zhang", "Hui Liu", "Xiaomin Li", "Zhenwei Dai", "Jingying Zeng", "Fali Wang", "Minhua Lin", "Ramraj Chandradevan", "Zhen Li", "Chen Luo", "Xianfeng Tang", "Qi He", "Suhang Wang"], "categories": ["cs.LG", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07375v1", "summary": "Reward models trained on human preference data have demonstrated strong\neffectiveness in aligning Large Language Models (LLMs) with human intent under\nthe framework of Reinforcement Learning from Human Feedback (RLHF). However,\nRLHF remains vulnerable to reward hacking, where the policy exploits\nimperfections in the reward function rather than genuinely learning the\nintended behavior. Although significant efforts have been made to mitigate\nreward hacking, they predominantly focus on and evaluate in-distribution\nscenarios, where the training and testing data for the reward model share the\nsame distribution. In this paper, we empirically show that state-of-the-art\nmethods struggle in more challenging out-of-distribution (OOD) settings. We\nfurther demonstrate that incorporating fine-grained multi-attribute scores\nhelps address this challenge. However, the limited availability of high-quality\ndata often leads to weak performance of multi-objective reward functions, which\ncan negatively impact overall performance and become the bottleneck. To address\nthis issue, we propose a unified reward modeling framework that jointly trains\nBradley--Terry (BT) single-objective and multi-objective regression-based\nreward functions using a shared embedding space. We theoretically establish a\nconnection between the BT loss and the regression objective and highlight their\ncomplementary benefits. Specifically, the regression task enhances the\nsingle-objective reward function's ability to mitigate reward hacking in\nchallenging OOD settings, while BT-based training improves the scoring\ncapability of the multi-objective reward function, enabling a 7B model to\noutperform a 70B baseline. Extensive experimental results demonstrate that our\nframework significantly improves both the robustness and the scoring\nperformance of reward models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07375v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07424", "title": "Corvid: Improving Multimodal Large Language Models Towards Chain-of-Thought Reasoning", "authors": ["Jingjing Jiang", "Chao Ma", "Xurui Song", "Hanwang Zhang", "Jun Luo"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025", "url": "http://arxiv.org/abs/2507.07424v1", "summary": "Recent advancements in multimodal large language models (MLLMs) have\ndemonstrated exceptional performance in multimodal perception and\nunderstanding. However, leading open-source MLLMs exhibit significant\nlimitations in complex and structured reasoning, particularly in tasks\nrequiring deep reasoning for decision-making and problem-solving. In this work,\nwe present Corvid, an MLLM with enhanced chain-of-thought (CoT) reasoning\ncapabilities. Architecturally, Corvid incorporates a hybrid vision encoder for\ninformative visual representation and a meticulously designed connector\n(GateMixer) to facilitate cross-modal alignment. To enhance Corvid's CoT\nreasoning capabilities, we introduce MCoT-Instruct-287K, a high-quality\nmultimodal CoT instruction-following dataset, refined and standardized from\ndiverse public reasoning sources. Leveraging this dataset, we fine-tune Corvid\nwith a two-stage CoT-formatted training approach to progressively enhance its\nstep-by-step reasoning abilities. Furthermore, we propose an effective\ninference-time scaling strategy that enables Corvid to mitigate over-reasoning\nand under-reasoning through self-verification. Extensive experiments\ndemonstrate that Corvid outperforms existing o1-like MLLMs and state-of-the-art\nMLLMs with similar parameter scales, with notable strengths in mathematical\nreasoning and science problem-solving. Project page:\nhttps://mm-vl.github.io/corvid.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.07424v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2504.10830", "title": "Radiation Footprint Control in Cell-Free Cooperative ISAC: Optimal Joint BS Activation and Beamforming Coordination", "authors": ["Jie Chen", "Xianbin Wang"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      This paper has been accepted by the IEEE Transactions on Communications", "url": "http://arxiv.org/abs/2504.10830v2", "summary": "Coordinated beamforming across distributed base stations (BSs) in cell-free\nwireless infrastructure can efficiently support integrated sensing and\ncommunication (ISAC) users by enhancing resource sharing and suppressing\ninterference in the spatial domain. However, intensive coordination among\ndistributed BSs within the ISAC-enabled network poses risks of generating\nsubstantial interference to other coexisting networks sharing the same\nspectrum, while also incurring elevated costs from energy consumption and\nsignaling exchange. To address these challenges, this paper develops an\ninterference-suppressed and cost-efficient cell-free ISAC network, which\nopportunistically and cooperatively orchestrates distributed radio resources to\naccommodate the competing demands of sensing and communication (S\\&C) services.\nSpecifically, we conceive a radiation footprint control mechanism that\nautonomously suppresses interference across the entire signal propagation space\nto safeguard other networks without exchanging channel knowledge signaling.\nThen, we propose joint BS activation and beamforming coordination to\ndynamically activate appropriate BSs and orchestrate their spatial beams for\nservice provisioning. Building upon this framework, we formulate a\ncost-efficient utility maximization problem that considers individual S\\&C\ndemands and location-dependent radiation footprint constraints. Since this\nresults in a non-convex optimization problem, we develop a monotonic\noptimization embedded branch-and-bound (MO-BRB) algorithm to find the optimal\nsolution. Additionally, we apply a low-complexity iterative method to obtain\nnear-optimal solutions. Finally, simulation results validate the effectiveness\nof the proposed algorithms.", "comment": "This paper has been accepted by the IEEE Transactions on\n  Communications", "pdf_url": "http://arxiv.org/pdf/2504.10830v2", "cate": "cs.IT", "date": "2025-04-15", "updated": "2025-07-10"}
{"id": "2504.08793", "title": "Constraint Programming Models For Serial Batch Scheduling With Minimum Batch Size", "authors": ["Jorge A. Huertas", "Pascal Van Hentenryck"], "categories": ["cs.DC", "cs.AI", "math.OC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      18 pages, 16 figures", "url": "http://arxiv.org/abs/2504.08793v2", "summary": "In serial batch (s-batch) scheduling, jobs are grouped in batches and\nprocessed sequentially within their batch. This paper considers multiple\nparallel machines, nonidentical job weights and release times, and\nsequence-dependent setup times between batches of different families. Although\ns-batch has been widely studied in the literature, very few papers have taken\ninto account a minimum batch size, typical in practical settings such as\nsemiconductor manufacturing and the metal industry. The problem with this\nminimum batch size requirement has been mostly tackled with dynamic programming\nand meta-heuristics, and no article has ever used constraint programming (CP)\nto do so. This paper fills this gap by proposing, three CP models for\ns-batching with minimum batch size: (i) an \\textit{Interval Assignment} model\nthat computes and bounds the size of the batches using the presence literals of\ninterval variables of the jobs. (ii) A \\textit{Global} model that exclusively\nuses global constraints that track the size of the batches over time. (iii) And\na \\textit{Hybrid} model that combines the benefits of the extra global\nconstraints with the efficiency of the sum-of-presences constraints to ensure\nthe minimum batch sizes. The computational experiments on standard cases\ncompare the three CP models with two existing mixed-integer programming (MIP)\nmodels from the literature. The results demonstrate the versatility of the\nproposed CP models to handle multiple variations of s-batching; and their\nability to produce, in large instances, better solutions than the MIP models\nfaster.", "comment": "18 pages, 16 figures", "pdf_url": "http://arxiv.org/pdf/2504.08793v2", "cate": "cs.DC", "date": "2025-04-07", "updated": "2025-07-10"}
{"id": "2507.07767", "title": "Structured Prompts, Better Outcomes? Exploring the Effects of a Structured Interface with ChatGPT in a Graduate Robotics Course", "authors": ["Jerome Brender", "Laila El-Hamamsy", "Kim Uittenhove", "Francesco Mondada", "Engin Bumbacher"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      Accepted, to appear in the proceedings of the EC-TEL 2025 conference", "url": "http://arxiv.org/abs/2507.07767v1", "summary": "Prior research shows that how students engage with Large Language Models\n(LLMs) influences their problem-solving and understanding, reinforcing the need\nto support productive LLM-uses that promote learning. This study evaluates the\nimpact of a structured GPT platform designed to promote 'good' prompting\nbehavior with data from 58 students in a graduate-level robotics course. The\nstudents were assigned to either an intervention group using the structured\nplatform or a control group using ChatGPT freely for two practice lab sessions,\nbefore a third session where all students could freely use ChatGPT. We analyzed\nstudent perception (pre-post surveys), prompting behavior (logs), performance\n(task scores), and learning (pre-post tests). Although we found no differences\nin performance or learning between groups, we identified prompting behaviors -\nsuch as having clear prompts focused on understanding code - that were linked\nwith higher learning gains and were more prominent when students used the\nstructured platform. However, such behaviors did not transfer once students\nwere no longer constrained to use the structured platform. Qualitative survey\ndata showed mixed perceptions: some students perceived the value of the\nstructured platform, but most did not perceive its relevance and resisted\nchanging their habits. These findings contribute to ongoing efforts to identify\neffective strategies for integrating LLMs into learning and question the\neffectiveness of bottom-up approaches that temporarily alter user interfaces to\ninfluence students' interaction. Future research could instead explore top-down\nstrategies that address students' motivations and explicitly demonstrate how\ncertain interaction patterns support learning.", "comment": "Accepted, to appear in the proceedings of the EC-TEL 2025 conference", "pdf_url": "http://arxiv.org/pdf/2507.07767v1", "cate": "cs.CY", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07830", "title": "Meshless projection model-order reduction via reference spaces for smoothed-particle hydrodynamics", "authors": ["Steven N. Rodriguez", "Steven L. Brunton", "Liam K. Magargal", "Parisa Khodabakshi", "Justin W. Jaworski", "Nicoleta A. Apetre", "John C. Steuben", "John G. Michopoulos", "Athanasios Iliopoulos"], "categories": ["cs.CE", "cs.NA", "math.NA", "physics.flu-dyn"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07830v1", "summary": "This work proposes a model-order reduction framework for the meshless weakly\ncompressible smoothed particle hydrodynamics (SPH) method. The proposed\nframework introduces the concept of modal reference spaces to overcome the\nchallenges of discovering low-dimensional subspaces from unstructured, dynamic,\nand mixing numerical topology that is often seen in SPH simulations. The\nproposed modal reference spaces enable a low-dimensional representation of the\nSPH field equations while maintaining their inherent meshless qualities. Modal\nreference spaces are constructed by projecting SPH snapshot data onto a\nreference space where low-dimensionality of field quantities can be discovered\nvia traditional modal decomposition techniques (e.g., the proper orthogonal\ndecomposition (POD)). Modal quantities are mapped back to the meshless SPH\nspace via scattered data interpolation during the online predictive stage. The\nproposed model-order reduction framework is cast into the \\emph{meshless}\nGalerkin POD (GPOD) and the Adjoint Petrov--Galerkin (APG) projection\nmodel-order reduction (PMOR) formulation. The PMORs are tested on three\nnumerical experiments: 1) the Taylor--Green vortex; 2) lid-driven cavity; and\n3) flow past an open cavity. Results show good agreement in reconstructed and\npredictive velocity fields, which showcase the ability of the proposed\nframework to evolve the unstructured, dynamic, and mixing SPH field equations\nin a low-dimensional subspace. Results also show that the pressure field is\nsensitive to the projection error due to the stiff weakly-compressible\nassumption made in the current SPH framework, but can be alleviated through\nnonlinear approximations, such as the APG approach. Ultimately, the presented\nmeshless model-order reduction framework marks a step toward enabling drastic\ncost savings of SPH simulations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07830v1", "cate": "cs.CE", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2501.03575", "title": "Cosmos World Foundation Model Platform for Physical AI", "authors": ["NVIDIA", ":", "Niket Agarwal", "Arslan Ali", "Maciej Bala", "Yogesh Balaji", "Erik Barker", "Tiffany Cai", "Prithvijit Chattopadhyay", "Yongxin Chen", "Yin Cui", "Yifan Ding", "Daniel Dworakowski", "Jiaojiao Fan", "Michele Fenzi", "Francesco Ferroni", "Sanja Fidler", "Dieter Fox", "Songwei Ge", "Yunhao Ge", "Jinwei Gu", "Siddharth Gururani", "Ethan He", "Jiahui Huang", "Jacob Huffman", "Pooya Jannaty", "Jingyi Jin", "Seung Wook Kim", "Gergely Kl√°r", "Grace Lam", "Shiyi Lan", "Laura Leal-Taixe", "Anqi Li", "Zhaoshuo Li", "Chen-Hsuan Lin", "Tsung-Yi Lin", "Huan Ling", "Ming-Yu Liu", "Xian Liu", "Alice Luo", "Qianli Ma", "Hanzi Mao", "Kaichun Mo", "Arsalan Mousavian", "Seungjun Nah", "Sriharsha Niverty", "David Page", "Despoina Paschalidou", "Zeeshan Patel", "Lindsey Pavao", "Morteza Ramezanali", "Fitsum Reda", "Xiaowei Ren", "Vasanth Rao Naik Sabavat", "Ed Schmerling", "Stella Shi", "Bartosz Stefaniak", "Shitao Tang", "Lyne Tchapmi", "Przemek Tredak", "Wei-Cheng Tseng", "Jibin Varghese", "Hao Wang", "Haoxiang Wang", "Heng Wang", "Ting-Chun Wang", "Fangyin Wei", "Xinyue Wei", "Jay Zhangjie Wu", "Jiashu Xu", "Wei Yang", "Lin Yen-Chen", "Xiaohui Zeng", "Yu Zeng", "Jing Zhang", "Qinsheng Zhang", "Yuxuan Zhang", "Qingqing Zhao", "Artur Zolkowski"], "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.03575v3", "summary": "Physical AI needs to be trained digitally first. It needs a digital twin of\nitself, the policy model, and a digital twin of the world, the world model. In\nthis paper, we present the Cosmos World Foundation Model Platform to help\ndevelopers build customized world models for their Physical AI setups. We\nposition a world foundation model as a general-purpose world model that can be\nfine-tuned into customized world models for downstream applications. Our\nplatform covers a video curation pipeline, pre-trained world foundation models,\nexamples of post-training of pre-trained world foundation models, and video\ntokenizers. To help Physical AI builders solve the most critical problems of\nour society, we make Cosmos open-source and our models open-weight with\npermissive licenses available via\nhttps://github.com/nvidia-cosmos/cosmos-predict1.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.03575v3", "cate": "cs.CV", "date": "2025-01-07", "updated": "2025-07-09"}
{"id": "2507.07399", "title": "Generalized Tree Edit Distance (GTED): A Faithful Evaluation Metric for Statement Autoformalization", "authors": ["Yuntian Liu", "Tao Zhu", "Xiaoyang Liu", "Yu Chen", "Zhaoxuan Liu", "Qingfeng Guo", "Jiashuo Zhang", "Kangjie Bao", "Tao Luo"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted to AI4Math@ICML25", "url": "http://arxiv.org/abs/2507.07399v1", "summary": "Statement autoformalization, the automated translation of statement from\nnatural language into formal languages, has become a subject of extensive\nresearch, yet the development of robust automated evaluation metrics remains\nlimited. Existing evaluation methods often lack semantic understanding, face\nchallenges with high computational costs, and are constrained by the current\nprogress of automated theorem proving. To address these issues, we propose GTED\n(Generalized Tree Edit Distance), a novel evaluation framework that first\nstandardizes formal statements and converts them into operator trees, then\ndetermines the semantic similarity using the eponymous GTED metric. On the\nminiF2F and ProofNet benchmarks, GTED outperforms all baseline metrics by\nachieving the highest accuracy and Kappa scores, thus providing the community\nwith a more faithful metric for automated evaluation. The code and experimental\nresults are available at https://github.com/XiaoyangLiu-sjtu/GTED.", "comment": "Accepted to AI4Math@ICML25", "pdf_url": "http://arxiv.org/pdf/2507.07399v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07388", "title": "GRIT: Graph Transformer For Internal Ice Layer Thickness Prediction", "authors": ["Zesheng Liu", "Maryam Rahnemoonfar"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted for 2025 IEEE International Geoscience and Remote Sensing Symposium (IGARSS 2025)", "url": "http://arxiv.org/abs/2507.07388v1", "summary": "Gaining a deeper understanding of the thickness and variability of internal\nice layers in Radar imagery is essential in monitoring the snow accumulation,\nbetter evaluating ice dynamics processes, and minimizing uncertainties in\nclimate models. Radar sensors, capable of penetrating ice, capture detailed\nradargram images of internal ice layers. In this work, we introduce GRIT, graph\ntransformer for ice layer thickness. GRIT integrates an inductive geometric\ngraph learning framework with an attention mechanism, designed to map the\nrelationships between shallow and deeper ice layers. Compared to baseline graph\nneural networks, GRIT demonstrates consistently lower prediction errors. These\nresults highlight the attention mechanism's effectiveness in capturing temporal\nchanges across ice layers, while the graph transformer combines the strengths\nof transformers for learning long-range dependencies with graph neural networks\nfor capturing spatial patterns, enabling robust modeling of complex\nspatiotemporal dynamics.", "comment": "Accepted for 2025 IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS 2025)", "pdf_url": "http://arxiv.org/pdf/2507.07388v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07435", "title": "Towards High-Resolution 3D Anomaly Detection: A Scalable Dataset and Real-Time Framework for Subtle Industrial Defects", "authors": ["Yuqi Cheng", "Yihan Sun", "Hui Zhang", "Weiming Shen", "Yunkang Cao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      14 pages, 8figures", "url": "http://arxiv.org/abs/2507.07435v1", "summary": "In industrial point cloud analysis, detecting subtle anomalies demands\nhigh-resolution spatial data, yet prevailing benchmarks emphasize\nlow-resolution inputs. To address this disparity, we propose a scalable\npipeline for generating realistic and subtle 3D anomalies. Employing this\npipeline, we developed MiniShift, the inaugural high-resolution 3D anomaly\ndetection dataset, encompassing 2,577 point clouds, each with 500,000 points\nand anomalies occupying less than 1\\% of the total. We further introduce\nSimple3D, an efficient framework integrating Multi-scale Neighborhood\nDescriptors (MSND) and Local Feature Spatial Aggregation (LFSA) to capture\nintricate geometric details with minimal computational overhead, achieving\nreal-time inference exceeding 20 fps. Extensive evaluations on MiniShift and\nestablished benchmarks demonstrate that Simple3D surpasses state-of-the-art\nmethods in both accuracy and speed, highlighting the pivotal role of\nhigh-resolution data and effective feature aggregation in advancing practical\n3D anomaly detection.", "comment": "14 pages, 8figures", "pdf_url": "http://arxiv.org/pdf/2507.07435v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2308.14507", "title": "Spectral Estimators for Structured Generalized Linear Models via Approximate Message Passing", "authors": ["Yihan Zhang", "Hong Chang Ji", "Ramji Venkataramanan", "Marco Mondelli"], "categories": ["math.ST", "cs.IT", "cs.LG", "math.IT", "math.PR", "stat.ML", "stat.TH"], "primary_category": "Subjects:       Statistics Theory (math.ST)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2308.14507v4", "summary": "We consider the problem of parameter estimation in a high-dimensional\ngeneralized linear model. Spectral methods obtained via the principal\neigenvector of a suitable data-dependent matrix provide a simple yet\nsurprisingly effective solution. However, despite their wide use, a rigorous\nperformance characterization, as well as a principled way to preprocess the\ndata, are available only for unstructured (i.i.d.\\ Gaussian and Haar\northogonal) designs. In contrast, real-world data matrices are highly\nstructured and exhibit non-trivial correlations. To address the problem, we\nconsider correlated Gaussian designs capturing the anisotropic nature of the\nfeatures via a covariance matrix $\\Sigma$. Our main result is a precise\nasymptotic characterization of the performance of spectral estimators. This\nallows us to identify the optimal preprocessing that minimizes the number of\nsamples needed for parameter estimation. Surprisingly, such preprocessing is\nuniversal across a broad set of designs, which partly addresses a conjecture on\noptimal spectral estimators for rotationally invariant models. Our principled\napproach vastly improves upon previous heuristic methods, including for designs\ncommon in computational imaging and genetics. The proposed methodology, based\non approximate message passing, is broadly applicable and opens the way to the\nprecise characterization of spiked matrices and of the corresponding spectral\nmethods in a variety of settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2308.14507v4", "cate": "math.ST", "date": "2023-08-28", "updated": "2025-07-09"}
{"id": "2505.02351", "title": "Opt-GPTQ: An Optimized GPTQ Combining Sparse Attention and Quantization Techniques", "authors": ["Jie Kong", "Junxiang Zhang", "Jiheng Xu", "Yalong Li", "Shouhua Zhang", "Jiehan Zhou", "Yuhai Liu", "Peng Liang", "Quan Zhang", "Luohan Jiang"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.02351v2", "summary": "In the field of deep learning, traditional attention mechanisms face\nsignificant challenges related to high computational complexity and large\nmemory consumption when processing long sequence data. To address these\nlimitations, we propose Opt-GPTQ, an optimized Gradient-based Post Training\nQuantization (GPTQ) combining the Grouped Query Attention (GQA) mechanism with\npaging memory management, optimizing the traditional Multi-Head Attention (MHA)\nmechanism by grouping query heads and sharing key-value vectors. Optimized GQA\n(Opt-GQA) effectively reduces computational complexity, minimizes memory\nfragmentation, and enhances memory utilization for large-scale models. Opt-GPTQ\nis optimized for Data Center Units (DCUs) and integrated into the vLLM model to\nmaximize hardware efficiency. It customizes GPU kernels to further enhance\nattention computation by reducing memory access latency and boosting parallel\ncomputing capabilities. Opt-GQA integrates Attention with Linear Biases (ALiBi)\nto reduce overhead and enhance long-sequence processing. Experimental results\nshow that Opt-GPTQ significantly reduces computation time and memory usage\nwhile improving model performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.02351v2", "cate": "cs.DC", "date": "2025-05-05", "updated": "2025-07-10"}
{"id": "2507.07582", "title": "Improving Clustering on Occupational Text Data through Dimensionality Reduction", "authors": ["Iago Xabier V√°zquez Garc√≠a", "Damla Partanaz", "Emrullah Fatih Yetkin"], "categories": ["cs.LG", "cs.CL", "cs.CY"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Preprint, 10 figures", "url": "http://arxiv.org/abs/2507.07582v1", "summary": "In this study, we focused on proposing an optimal clustering mechanism for\nthe occupations defined in the well-known US-based occupational database,\nO*NET. Even though all occupations are defined according to well-conducted\nsurveys in the US, their definitions can vary for different firms and\ncountries. Hence, if one wants to expand the data that is already collected in\nO*NET for the occupations defined with different tasks, a map between the\ndefinitions will be a vital requirement. We proposed a pipeline using several\nBERT-based techniques with various clustering approaches to obtain such a map.\nWe also examined the effect of dimensionality reduction approaches on several\nmetrics used in measuring performance of clustering algorithms. Finally, we\nimproved our results by using a specialized silhouette approach. This new\nclustering-based mapping approach with dimensionality reduction may help\ndistinguish the occupations automatically, creating new paths for people\nwanting to change their careers.", "comment": "Preprint, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.07582v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07107", "title": "Machine Learning Enhanced Multi-Factor Quantitative Trading: A Cross-Sectional Portfolio Optimization Approach with Bias Correction", "authors": ["Yimin Du"], "categories": ["q-fin.PM", "cs.CE"], "primary_category": "Subjects:       Portfolio Management (q-fin.PM)", "pdf_link": null, "comments": "Comments:      9 pages", "url": "http://arxiv.org/abs/2507.07107v1", "summary": "This paper presents a comprehensive machine learning framework for\nquantitative trading that achieves superior risk-adjusted returns through\nsystematic factor engineering, real-time computation optimization, and\ncross-sectional portfolio construction. Our approach integrates multi-factor\nalpha discovery with bias correction techniques, leveraging PyTorch-accelerated\nfactor computation and advanced portfolio optimization. The system processes\n500-1000 factors derived from open-source alpha101 extensions and proprietary\nmarket microstructure signals. Key innovations include tensor-based factor\ncomputation acceleration, geometric Brownian motion data augmentation, and\ncross-sectional neutralization strategies. Empirical validation on Chinese\nA-share markets (2010-2024) demonstrates annualized returns of $20\\%$ with\nSharpe ratios exceeding 2.0, significantly outperforming traditional\napproaches. Our analysis reveals the critical importance of bias correction in\nfactor construction and the substantial impact of cross-sectional portfolio\noptimization on strategy performance. Code and experimental implementations are\navailable at: https://github.com/initial-d/ml-quant-trading", "comment": "9 pages", "pdf_url": "http://arxiv.org/pdf/2507.07107v1", "cate": "q-fin.PM", "date": "2025-06-02", "updated": "2025-06-02"}
{"id": "2504.13554", "title": "Task Assignment and Exploration Optimization for Low Altitude UAV Rescue via Generative AI Enhanced Multi-agent Reinforcement Learning", "authors": ["Xin Tang", "Qian Chen", "Wenjie Weng", "Chao Jin", "Zhang Liu", "Jiacheng Wang", "Geng Sun", "Xiaohuan Li", "Dusit Niyato"], "categories": ["cs.AI", "cs.LG", "cs.RO"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.13554v2", "summary": "The integration of emerging uncrewed aerial vehicles (UAVs) with artificial\nintelligence (AI) and ground-embedded robots (GERs) has transformed emergency\nrescue operations in unknown environments. However, the high computational\ndemands often exceed a single UAV's capacity, making it difficult to\ncontinuously provide stable high-level services. To address this, this paper\nproposes a cooperation framework involving UAVs, GERs, and airships. The\nframework enables resource pooling through UAV-to-GER (U2G) and UAV-to-airship\n(U2A) links, offering computing services for offloaded tasks. Specifically, we\nformulate the multi-objective problem of task assignment and exploration as a\ndynamic long-term optimization problem aiming to minimize task completion time\nand energy use while ensuring stability. Using Lyapunov optimization, we\ntransform it into a per-slot deterministic problem and propose HG-MADDPG, which\ncombines the Hungarian algorithm with a GDM-based multi-agent deep\ndeterministic policy gradient. Simulations demonstrate significant improvements\nin offloading efficiency, latency, and system stability over baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.13554v2", "cate": "cs.AI", "date": "2025-04-18", "updated": "2025-07-10"}
{"id": "2507.07405", "title": "HGMP:Heterogeneous Graph Multi-Task Prompt Learning", "authors": ["Pengfei Jiao", "Jialong Ni", "Di Jin", "Xuan Guo", "Huan Liu", "Hongjiang Chen", "Yanxian Bi"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      The 25th International Joint Conference on Artificial Intelligence (IJCAI-25)", "url": "http://arxiv.org/abs/2507.07405v1", "summary": "The pre-training and fine-tuning methods have gained widespread attention in\nthe field of heterogeneous graph neural networks due to their ability to\nleverage large amounts of unlabeled data during the pre-training phase,\nallowing the model to learn rich structural features. However, these methods\nface the issue of a mismatch between the pre-trained model and downstream\ntasks, leading to suboptimal performance in certain application scenarios.\nPrompt learning methods have emerged as a new direction in heterogeneous graph\ntasks, as they allow flexible adaptation of task representations to address\ntarget inconsistency. Building on this idea, this paper proposes a novel\nmulti-task prompt framework for the heterogeneous graph domain, named HGMP.\nFirst, to bridge the gap between the pre-trained model and downstream tasks, we\nreformulate all downstream tasks into a unified graph-level task format. Next,\nwe address the limitations of existing graph prompt learning methods, which\nstruggle to integrate contrastive pre-training strategies in the heterogeneous\ngraph domain. We design a graph-level contrastive pre-training strategy to\nbetter leverage heterogeneous information and enhance performance in multi-task\nscenarios. Finally, we introduce heterogeneous feature prompts, which enhance\nmodel performance by refining the representation of input graph features.\nExperimental results on public datasets show that our proposed method adapts\nwell to various tasks and significantly outperforms baseline methods.", "comment": "The 25th International Joint Conference on Artificial Intelligence\n  (IJCAI-25)", "pdf_url": "http://arxiv.org/pdf/2507.07405v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07389", "title": "ST-GRIT: Spatio-Temporal Graph Transformer For Internal Ice Layer Thickness Prediction", "authors": ["Zesheng Liu", "Maryam Rahnemoonfar"], "categories": ["cs.LG", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted for 2025 IEEE International Conference on Image Processing (ICIP)", "url": "http://arxiv.org/abs/2507.07389v1", "summary": "Understanding the thickness and variability of internal ice layers in radar\nimagery is crucial for monitoring snow accumulation, assessing ice dynamics,\nand reducing uncertainties in climate models. Radar sensors, capable of\npenetrating ice, provide detailed radargram images of these internal layers. In\nthis work, we present ST-GRIT, a spatio-temporal graph transformer for ice\nlayer thickness, designed to process these radargrams and capture the\nspatiotemporal relationships between shallow and deep ice layers. ST-GRIT\nleverages an inductive geometric graph learning framework to extract local\nspatial features as feature embeddings and employs a series of temporal and\nspatial attention blocks separately to model long-range dependencies\neffectively in both dimensions. Experimental evaluation on radargram data from\nthe Greenland ice sheet demonstrates that ST-GRIT consistently outperforms\ncurrent state-of-the-art methods and other baseline graph neural networks by\nachieving lower root mean-squared error. These results highlight the advantages\nof self-attention mechanisms on graphs over pure graph neural networks,\nincluding the ability to handle noise, avoid oversmoothing, and capture\nlong-range dependencies. Moreover, the use of separate spatial and temporal\nattention blocks allows for distinct and robust learning of spatial\nrelationships and temporal patterns, providing a more comprehensive and\neffective approach.", "comment": "Accepted for 2025 IEEE International Conference on Image Processing\n  (ICIP)", "pdf_url": "http://arxiv.org/pdf/2507.07389v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07443", "title": "Dual Semantic-Aware Network for Noise Suppressed Ultrasound Video Segmentation", "authors": ["Ling Zhou", "Runtian Yuan", "Yi Liu", "Yuejie Zhang", "Rui Feng", "Shang Gao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07443v1", "summary": "Ultrasound imaging is a prevalent diagnostic tool known for its simplicity\nand non-invasiveness. However, its inherent characteristics often introduce\nsubstantial noise, posing considerable challenges for automated lesion or organ\nsegmentation in ultrasound video sequences. To address these limitations, we\npropose the Dual Semantic-Aware Network (DSANet), a novel framework designed to\nenhance noise robustness in ultrasound video segmentation by fostering mutual\nsemantic awareness between local and global features. Specifically, we\nintroduce an Adjacent-Frame Semantic-Aware (AFSA) module, which constructs a\nchannel-wise similarity matrix to guide feature fusion across adjacent frames,\neffectively mitigating the impact of random noise without relying on\npixel-level relationships. Additionally, we propose a Local-and-Global\nSemantic-Aware (LGSA) module that reorganizes and fuses temporal unconditional\nlocal features, which capture spatial details independently at each frame, with\nconditional global features that incorporate temporal context from adjacent\nframes. This integration facilitates multi-level semantic representation,\nsignificantly improving the model's resilience to noise interference. Extensive\nevaluations on four benchmark datasets demonstrate that DSANet substantially\noutperforms state-of-the-art methods in segmentation accuracy. Moreover, since\nour model avoids pixel-level feature dependencies, it achieves significantly\nhigher inference FPS than video-based methods, and even surpasses some\nimage-based models. Code can be found in\n\\href{https://github.com/ZhouL2001/DSANet}{DSANet}", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07443v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2312.01991", "title": "Shapley-Based Data Valuation with Mutual Information: A Key to Modified K-Nearest Neighbors", "authors": ["Mohammad Ali Vahedifar", "Azim Akhtarshenas", "Mohammad Mohammadi Rafatpanah", "Maryam Sabbaghian"], "categories": ["cs.LG", "cs.IT", "math.IT"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      This paper has been accepted for publication in the IEEE Machine Learning and Signal Processing conference (MLSP 2025)", "url": "http://arxiv.org/abs/2312.01991v4", "summary": "The K-Nearest Neighbors (KNN) algorithm is widely used for classification and\nregression; however, it suffers from limitations, including the equal treatment\nof all samples. We propose Information-Modified KNN (IM-KNN), a novel approach\nthat leverages Mutual Information ($I$) and Shapley values to assign weighted\nvalues to neighbors, thereby bridging the gap in treating all samples with the\nsame value and weight. On average, IM-KNN improves the accuracy, precision, and\nrecall of traditional KNN by 16.80%, 17.08%, and 16.98%, respectively, across\n12 benchmark datasets. Experiments on four large-scale datasets further\nhighlight IM-KNN's robustness to noise, imbalanced data, and skewed\ndistributions.", "comment": "This paper has been accepted for publication in the IEEE Machine\n  Learning and Signal Processing conference (MLSP 2025)", "pdf_url": "http://arxiv.org/pdf/2312.01991v4", "cate": "cs.LG", "date": "2023-12-04", "updated": "2025-07-10"}
{"id": "2505.11329", "title": "TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference", "authors": ["Raja Gond", "Nipun Kwatra", "Ramachandran Ramjee"], "categories": ["cs.DC", "cs.LG"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      14 pages, 16 figures. For source code, see this https URL", "url": "http://arxiv.org/abs/2505.11329v2", "summary": "Distributed inference of large language models (LLMs) can introduce overheads\nof up to 20% even over GPUs connected via high-speed interconnects such as\nNVLink. Multiple techniques have been proposed to mitigate these overheads by\ndecomposing computations into finer-grained tasks and overlapping communication\nwith sub-tasks as they complete. However, fine-grained decomposition of a large\ncomputation into many smaller computations on GPUs results in overheads.\nFurthermore, the communication itself uses many streaming multiprocessors\n(SMs), adding to the overhead.\n  We present TokenWeave to address these challenges. TokenWeave proposes a\nToken-Splitting technique that divides the tokens in the inference batch into\ntwo approximately equal subsets in a wave-aware manner. The communication of\none subset is then overlapped with the computation of the other. In addition,\nTokenWeave optimizes the order of the layer normalization computation with\nrespect to communication operations and implements a novel fused\nAllReduce--RMSNorm kernel that carefully leverages Multimem instruction support\navailable on NVIDIA Hopper GPUs. These optimizations allow TokenWeave to\nperform communication and RMSNorm using only 2-8 SMs. Moreover, our kernel\nenables the memory-bound RMSNorm to be overlapped with the other batch's\ncomputation, providing additional gains.\n  Our evaluations demonstrate up to 1.29x speedup in latency and 1.26x higher\nthroughput across multiple models and workloads. In several settings,\nTokenWeave results in better performance compared to an equivalent model with\nall communication removed.", "comment": "14 pages, 16 figures. For source code, see\n  https://github.com/microsoft/tokenweave", "pdf_url": "http://arxiv.org/pdf/2505.11329v2", "cate": "cs.DC", "date": "2025-05-16", "updated": "2025-07-10"}
{"id": "2502.00015", "title": "Ethical Concerns of Generative AI and Mitigation Strategies: A Systematic Mapping Study", "authors": ["Yutan Huang", "Chetan Arora", "Wen Cheng Houng", "Tanjila Kanij", "Anuradha Madulgalla", "John Grundy"], "categories": ["cs.CY", "cs.AI"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.00015v2", "summary": "[Context] Generative AI technologies, particularly Large Language Models\n(LLMs), have transformed numerous domains by enhancing convenience and\nefficiency in information retrieval, content generation, and decision-making\nprocesses. However, deploying LLMs also presents diverse ethical challenges,\nand their mitigation strategies remain complex and domain-dependent.\n[Objective] This paper aims to identify and categorize the key ethical concerns\nassociated with using LLMs, examine existing mitigation strategies, and assess\nthe outstanding challenges in implementing these strategies across various\ndomains. [Method] We conducted a systematic mapping study, reviewing 39 studies\nthat discuss ethical concerns and mitigation strategies related to LLMs. We\nanalyzed these ethical concerns using five ethical dimensions that we extracted\nbased on various existing guidelines, frameworks, and an analysis of the\nmitigation strategies and implementation challenges. [Results] Our findings\nreveal that ethical concerns in LLMs are multi-dimensional and\ncontext-dependent. While proposed mitigation strategies address some of these\nconcerns, significant challenges still remain. [Conclusion] Our results\nhighlight that ethical issues often hinder the practical implementation of the\nmitigation strategies, particularly in high-stake areas like healthcare and\npublic governance; existing frameworks often lack adaptability, failing to\naccommodate evolving societal expectations and diverse contexts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.00015v2", "cate": "cs.CY", "date": "2025-01-08", "updated": "2025-07-10"}
{"id": "2507.07304", "title": "Scalable ADER-DG Transport Method with Polynomial Order Independent CFL Limit", "authors": ["Kieran Ricardo", "Kenneth Duru"], "categories": ["math.NA", "cs.CE", "cs.NA", "physics.ao-ph", "physics.comp-ph"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07304v1", "summary": "Discontinuous Galerkin (DG) methods are known to suffer from increasingly\nrestrictive time step constraints as the polynomial order increases, limiting\ntheir efficiency at high orders. In this paper, we introduce a novel locally\nimplicit, but globally explicit ADER-DG scheme designed for transport-dominated\nproblems. The method achieves a maximum stable time step governed by an\nelement-width based CFL condition that is independent of the polynomial degree.\nBy solving a set of element-local implicit problems at each time step, our\napproach more effectively captures the domain of dependence. As a result, our\nmethod remains stable for CFL numbers up to $1/\\sqrt{d}$ in $d$ spatial\ndimensions. We provide a rigorous stability proof in one dimension, and extend\nthe analysis to two and three dimensions using a semi-analytical von Neumann\nstability analysis. The accuracy and convergence of the method are demonstrated\nthrough numerical experiments on both linear and nonlinear test cases.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07304v1", "cate": "math.NA", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.05116", "title": "VOTE: Vision-Language-Action Optimization with Trajectory Ensemble Voting", "authors": ["Juyi Lin", "Amir Taherin", "Arash Akbari", "Arman Akbari", "Lei Lu", "Guangyu Chen", "Taskin Padir", "Xiaomeng Yang", "Weiwei Chen", "Yiqian Li", "Xue Lin", "David Kaeli", "Pu Zhao", "Yanzhi Wang"], "categories": ["cs.CV", "cs.AI", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.05116v2", "summary": "Recent large-scale Vision Language Action (VLA) models have shown superior\nperformance in robotic manipulation tasks guided by natural language. However,\ntheir generalization remains limited when applied to novel objects or\nunfamiliar environments that lie outside the training distribution. To address\nthis, many existing approaches integrate additional components such as depth\nestimation, segmentation, or even diffusion to improve generalization, at the\ncost of adding significant computation overhead, resulting in low efficiency.\nThis motivates the exploration of efficient action prediction methods, which\nare independent of additional high-level visual representations or diffusion\ntechniques. In this work, we propose VOTE, an efficient and general framework\nfor the optimization and acceleration of VLA models. In details, we propose a\nnovel tokenizer-free fine-tuning approach for parallel accurate action\nprediction, which reduces computational overhead and accelerates inference\nspeed. Additionally, we adopt an ensemble voting strategy for the action\nsampling, which significantly improves model performance and enhances\ngeneralization. Experimental results show that our method achieves\nstate-of-the-art performance with 35x faster inference and 145 Hz throughput.\nAll the details and codes will be open-sourced.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.05116v2", "cate": "cs.CV", "date": "2025-07-07", "updated": "2025-07-10"}
{"id": "2507.07414", "title": "GNN-CNN: An Efficient Hybrid Model of Convolutional and Graph Neural Networks for Text Representation", "authors": ["Fardin Rastakhiz"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07414v1", "summary": "Time, cost, and energy efficiency are critical considerations in\nDeep-Learning (DL), particularly when processing long texts. Transformers,\nwhich represent the current state of the art, exhibit quadratic computational\ncomplexity relative to input length, making them inefficient for extended\ndocuments. This study introduces a novel model architecture that combines Graph\nNeural Networks (GNNs) and Convolutional Neural Networks (CNNs), integrated\nwith a real-time, end-to-end graph generation mechanism. The model processes\ncompact batches of character-level inputs without requiring padding or\ntruncation. To enhance performance while maintaining high speed and efficiency,\nthe model incorporates information from Large Language Models (LLMs), such as\ntoken embeddings and sentiment polarities, through efficient dictionary\nlookups. It captures local contextual patterns using CNNs, expands local\nreceptive fields via lattice-based graph structures, and employs small-world\ngraphs to aggregate document-level information. The generated graphs exhibit\nstructural properties indicative of meaningful semantic organization, with an\naverage clustering coefficient of approximately 0.45 and an average shortest\npath length ranging between 4 and 5. The model is evaluated across multiple\ntext classification tasks, including sentiment analysis and\nnews-categorization, and is compared against state-of-the-art models.\nExperimental results confirm the proposed model's efficiency and competitive\nperformance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07414v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07390", "title": "Learning Collective Variables from Time-lagged Generation", "authors": ["Seonghyun Park", "Kiyoung Seong", "Soojung Yang", "Rafael G√≥mez-Bombarelli", "Sungsoo Ahn"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07390v1", "summary": "Rare events such as state transitions are difficult to observe directly with\nmolecular dynamics simulations due to long timescales. Enhanced sampling\ntechniques overcome this by introducing biases along carefully chosen\nlow-dimensional features, known as collective variables (CVs), which capture\nthe slow degrees of freedom. Machine learning approaches (MLCVs) have automated\nCV discovery, but existing methods typically focus on discriminating\nmeta-stable states without fully encoding the detailed dynamics essential for\naccurate sampling. We propose TLC, a framework that learns CVs directly from\ntime-lagged conditions of a generative model. Instead of modeling the static\nBoltzmann distribution, TLC models a time-lagged conditional distribution\nyielding CVs to capture the slow dynamic behavior. We validate TLC on the\nAlanine Dipeptide system using two CV-based enhanced sampling tasks: (i)\nsteered molecular dynamics (SMD) and (ii) on-the-fly probability enhanced\nsampling (OPES), demonstrating equal or superior performance compared to\nexisting MLCV methods in both transition path sampling and state\ndiscrimination.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07390v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07453", "title": "Bluish Veil Detection and Lesion Classification using Custom Deep Learnable Layers with Explainable Artificial Intelligence (XAI)", "authors": ["M. A. Rasel", "Sameem Abdul Kareem", "Zhenli Kwan", "Shin Shen Yong", "Unaizah Obaidellah"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted version. Published in Computers in Biology and Medicine, 14 June 2024. DOI: https://doi.org/10.1016/j.compbiomed.2024.108758", "url": "http://arxiv.org/abs/2507.07453v1", "summary": "Melanoma, one of the deadliest types of skin cancer, accounts for thousands\nof fatalities globally. The bluish, blue-whitish, or blue-white veil (BWV) is a\ncritical feature for diagnosing melanoma, yet research into detecting BWV in\ndermatological images is limited. This study utilizes a non-annotated skin\nlesion dataset, which is converted into an annotated dataset using a proposed\nimaging algorithm based on color threshold techniques on lesion patches and\ncolor palettes. A Deep Convolutional Neural Network (DCNN) is designed and\ntrained separately on three individual and combined dermoscopic datasets, using\ncustom layers instead of standard activation function layers. The model is\ndeveloped to categorize skin lesions based on the presence of BWV. The proposed\nDCNN demonstrates superior performance compared to conventional BWV detection\nmodels across different datasets. The model achieves a testing accuracy of\n85.71% on the augmented PH2 dataset, 95.00% on the augmented ISIC archive\ndataset, 95.05% on the combined augmented (PH2+ISIC archive) dataset, and\n90.00% on the Derm7pt dataset. An explainable artificial intelligence (XAI)\nalgorithm is subsequently applied to interpret the DCNN's decision-making\nprocess regarding BWV detection. The proposed approach, coupled with XAI,\nsignificantly improves the detection of BWV in skin lesions, outperforming\nexisting models and providing a robust tool for early melanoma diagnosis.", "comment": "Accepted version. Published in Computers in Biology and Medicine, 14\n  June 2024. DOI: 10.1016/j.compbiomed.2024.108758", "pdf_url": "http://arxiv.org/pdf/2507.07453v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2401.15462", "title": "On the monotonicity of discrete entropy for log-concave random vectors on $\\mathbb{Z}^d$", "authors": ["Matthieu Fradelizi", "Lampros Gavalakis", "Martin Rapaport"], "categories": ["math.PR", "cs.IT", "math.IT", "Primary: 94A17 Secondary: 52C07, 39B62"], "primary_category": "Subjects:       Probability (math.PR)", "pdf_link": null, "comments": "Comments:      26 pages, no figures. Revised version incorporating reviewers' suggestions. Corollary 4 and Theorem 9 are new. We have removed Proposition 38 from v2 due to an error in the proof", "url": "http://arxiv.org/abs/2401.15462v3", "summary": "We prove the following type of discrete entropy monotonicity for sums of\nisotropic, log-concave, independent and identically distributed random vectors\n$X_1,\\dots,X_{n+1}$ on $\\mathbb{Z}^d$: $$ H(X_1+\\cdots+X_{n+1}) \\geq\nH(X_1+\\cdots+X_{n}) + \\frac{d}{2}\\log{\\Bigl(\\frac{n+1}{n}\\Bigr)} +o(1), $$\nwhere $o(1)$ vanishes as $H(X_1) \\to \\infty$. Moreover, for the $o(1)$-term, we\nobtain a rate of convergence $ O\\Bigl({H(X_1)}{e^{-\\frac{1}{d}H(X_1)}}\\Bigr)$,\nwhere the implied constants depend on $d$ and $n$. This generalizes to\n$\\mathbb{Z}^d$ the one-dimensional result of the second named author (2023). As\nin dimension one, our strategy is to establish that the discrete entropy\n$H(X_1+\\cdots+X_{n})$ is close to the differential (continuous) entropy\n$h(X_1+U_1+\\cdots+X_{n}+U_{n})$, where $U_1,\\dots, U_n$ are independent and\nidentically distributed uniform random vectors on $[0,1]^d$ and to apply the\ntheorem of Artstein, Ball, Barthe and Naor (2004) on the monotonicity of\ndifferential entropy. In fact, we show this result under more general\nassumptions than log-concavity, which are preserved up to constants under\nconvolution. In order to show that log-concave distributions satisfy our\nassumptions in dimension $d\\ge2$, more involved tools from convex geometry are\nneeded because a suitable position is required. We show that, for a log-concave\nfunction on $\\mathbb{R}^d$ in isotropic position, its integral, barycenter and\ncovariance matrix are close to their discrete counterparts. Moreover, in the\nlog-concave case, we weaken the isotropicity assumption to what we call almost\nisotropicity. One of our technical tools is a discrete analogue to the upper\nbound on the isotropic constant of a log-concave function, which extends to\ndimensions $d\\ge1$ a result of Bobkov, Marsiglietti and Melbourne (2022).", "comment": "26 pages, no figures. Revised version incorporating reviewers'\n  suggestions. Corollary 4 and Theorem 9 are new. We have removed Proposition\n  38 from v2 due to an error in the proof", "pdf_url": "http://arxiv.org/pdf/2401.15462v3", "cate": "math.PR", "date": "2024-01-27", "updated": "2025-07-10"}
{"id": "2506.03296", "title": "Parallel CPU-GPU Execution for LLM Inference on Constrained GPUs", "authors": ["Jiakun Fan", "Yanglin Zhang", "Xiangchen Li", "Dimitrios S. Nikolopoulos"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      Preprint, under review", "url": "http://arxiv.org/abs/2506.03296v3", "summary": "Deploying large language models (LLMs) for online inference is often\nconstrained by limited GPU memory, particularly due to the growing KV cache\nduring auto-regressive decoding. Hybrid GPU-CPU execution has emerged as a\npromising solution by offloading KV cache management and parts of attention\ncomputation to the CPU. However, a key bottleneck remains: existing schedulers\nfail to effectively overlap CPU-offloaded tasks with GPU execution during the\nlatency-critical, bandwidth-bound decode phase. This particularly penalizes\nreal-time, decode-heavy applications (e.g., chat, Chain-of-Thought reasoning)\nwhich are currently underserved by existing systems, especially under memory\npressure typical of edge or low-cost deployments.\n  We present APEX, a novel, profiling-informed scheduling strategy that\nmaximizes CPU-GPU parallelism during hybrid LLM inference. Unlike systems\nrelying on static rules or purely heuristic approaches, APEX dynamically\ndispatches compute across heterogeneous resources by predicting execution times\nof CPU and GPU subtasks to maximize overlap while avoiding scheduling\noverheads. We evaluate APEX on diverse workloads and GPU architectures (NVIDIA\nT4, A10), using LLaMa-2-7B and LLaMa-3.1-8B models. Compared to GPU-only\nschedulers like VLLM, APEX improves throughput by 84% - 96% on T4 and 11% - 89%\non A10 GPUs, while preserving latency. Against the best existing hybrid\nschedulers, it delivers up to 49% (T4) and 37% (A10) higher throughput in\nlong-output settings. APEX significantly advances hybrid LLM inference\nefficiency on such memory-constrained hardware and provides a blueprint for\nscheduling in heterogeneous AI systems, filling a critical gap for efficient\nreal-time LLM applications.", "comment": "Preprint, under review", "pdf_url": "http://arxiv.org/pdf/2506.03296v3", "cate": "cs.DC", "date": "2025-06-03", "updated": "2025-07-10"}
{"id": "2503.11713", "title": "Revisiting the Predictability of Performative, Social Events", "authors": ["Juan C. Perdomo"], "categories": ["cs.CY", "cs.LG", "econ.TH", "stat.ML"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      21 pages, accepted to ICML 2025", "url": "http://arxiv.org/abs/2503.11713v2", "summary": "Social predictions do not passively describe the future; they actively shape\nit. They inform actions and change individual expectations in ways that\ninfluence the likelihood of the predicted outcome. Given these dynamics, to\nwhat extent can social events be predicted? This question was discussed\nthroughout the 20th century by authors like Merton, Morgenstern, Simon, and\nothers who considered it a central issue in social science methodology. In this\nwork, we provide a modern answer to this old problem. Using recent ideas from\nperformative prediction and outcome indistinguishability, we establish that one\ncan always efficiently predict social events accurately, regardless of how\npredictions influence data. While achievable, we also show that these\npredictions are often undesirable, highlighting the limitations of previous\ndesiderata. We end with a discussion of various avenues forward.", "comment": "21 pages, accepted to ICML 2025", "pdf_url": "http://arxiv.org/pdf/2503.11713v2", "cate": "cs.CY", "date": "2025-03-12", "updated": "2025-07-10"}
{"id": "2507.00067", "title": "The gradual transformation of inland areas -- human plowing, horse plowing and equity incentives", "authors": ["Hongfa Zi", "Zhen Liu"], "categories": ["physics.soc-ph", "cs.CE", "econ.GN", "q-fin.EC"], "primary_category": "Subjects:       Physics and Society (physics.soc-ph)", "pdf_link": null, "comments": "Comments:      9 pages,1 figures", "url": "http://arxiv.org/abs/2507.00067v2", "summary": "Many modern areas have not learned their lessons and often hope for the\nwisdom of later generations, resulting in them only possessing modern\ntechnology and difficult to iterate ancient civilizations. At present, there is\nno way to tell how we should learn from history and promote the gradual\nupgrading of civilization. Therefore, we must tell the history of\ncivilization's progress and the means of governance, learn from experience to\nimprove the comprehensive strength and survival ability of civilization, and\nachieve an optimal solution for the tempering brought by conflicts and the\nreduction of internal conflicts. Firstly, we must follow the footsteps of\nhistory and explore the reasons for the long-term stability of each country in\nconflict, including providing economic benefits to the people and means of\nsuppressing them; then, use mathematical methods to demonstrate how we can\nachieve the optimal solution at the current stage. After analysis, we can\nconclude that the civilization transformed from human plowing to horse plowing\ncan easily suppress the resistance of the people and provide them with the\nability to resist; The selection of rulers should consider multiple\ninstitutional aspects, such as exams, elections, and drawing lots; Economic\ndevelopment follows a lognormal distribution and can be adjusted by expected\nvalue and variance. Using a lognormal distribution with the maximum value to\ndivide equity can adjust the wealth gap.", "comment": "9 pages,1 figures", "pdf_url": "http://arxiv.org/pdf/2507.00067v2", "cate": "physics.soc-ph", "date": "2025-06-28", "updated": "2025-07-10"}
{"id": "2303.14111", "title": "Unsupervised Automata Learning via Discrete Optimization", "authors": ["Simon Lutz", "Daniil Kaminskyi", "Florian Wittbold", "Simon Dierl", "Falk Howar", "Barbara K√∂nig", "Emmanuel M√ºller", "Daniel Neider"], "categories": ["cs.LG", "cs.AI", "cs.FL", "F.4.3; I.2.6"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2303.14111v2", "summary": "Automata learning is a successful tool for many application domains such as\nrobotics and automatic verification. Typically, automata learning techniques\noperate in a supervised learning setting (active or passive) where they learn a\nfinite state machine in contexts where additional information, such as labeled\nsystem executions, is available. However, other settings, such as learning from\nunlabeled data - an important aspect in machine learning - remain unexplored.\nTo overcome this limitation, we propose a framework for learning a\ndeterministic finite automaton (DFA) from a given multi-set of unlabeled words.\nWe show that this problem is computationally hard and develop three learning\nalgorithms based on constraint optimization. Moreover, we introduce novel\nregularization schemes for our optimization problems that improve the overall\ninterpretability of our DFAs. Using a prototype implementation, we demonstrate\npractical feasibility in the context of unsupervised anomaly detection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2303.14111v2", "cate": "cs.LG", "date": "2023-03-24", "updated": "2025-07-10"}
{"id": "2507.06971", "title": "Hallucinating 360¬∞: Panoramic Street-View Generation via Local Scenes Diffusion and Probabilistic Prompting", "authors": ["Fei Teng", "Kai Luo", "Sheng Wu", "Siyu Li", "Pujun Guo", "Jiale Wei", "Kunyu Peng", "Jiaming Zhang", "Kailun Yang"], "categories": ["cs.CV", "cs.RO", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      The source code will be publicly available at this https URL", "url": "http://arxiv.org/abs/2507.06971v2", "summary": "Panoramic perception holds significant potential for autonomous driving,\nenabling vehicles to acquire a comprehensive 360{\\deg} surround view in a\nsingle shot. However, autonomous driving is a data-driven task. Complete\npanoramic data acquisition requires complex sampling systems and annotation\npipelines, which are time-consuming and labor-intensive. Although existing\nstreet view generation models have demonstrated strong data regeneration\ncapabilities, they can only learn from the fixed data distribution of existing\ndatasets and cannot achieve high-quality, controllable panoramic generation. In\nthis paper, we propose the first panoramic generation method Percep360 for\nautonomous driving. Percep360 enables coherent generation of panoramic data\nwith control signals based on the stitched panoramic data. Percep360 focuses on\ntwo key aspects: coherence and controllability. Specifically, to overcome the\ninherent information loss caused by the pinhole sampling process, we propose\nthe Local Scenes Diffusion Method (LSDM). LSDM reformulates the panorama\ngeneration as a spatially continuous diffusion process, bridging the gaps\nbetween different data distributions. Additionally, to achieve the controllable\ngeneration of panoramic images, we propose a Probabilistic Prompting Method\n(PPM). PPM dynamically selects the most relevant control cues, enabling\ncontrollable panoramic image generation. We evaluate the effectiveness of the\ngenerated images from three perspectives: image quality assessment (i.e.,\nno-reference and with reference), controllability, and their utility in\nreal-world Bird's Eye View (BEV) segmentation. Notably, the generated data\nconsistently outperforms the original stitched images in no-reference quality\nmetrics and enhances downstream perception models. The source code will be\npublicly available at https://github.com/Bryant-Teng/Percep360.", "comment": "The source code will be publicly available at\n  https://github.com/Bryant-Teng/Percep360", "pdf_url": "http://arxiv.org/pdf/2507.06971v2", "cate": "cs.CV", "date": "2025-07-09", "updated": "2025-07-10"}
{"id": "2507.07418", "title": "Optimal Auction Design in the Joint Advertising", "authors": ["Yang Li", "Yuchao Ma", "Qi Qi"], "categories": ["cs.GT", "cs.AI"], "primary_category": "Subjects:       Computer Science and Game Theory (cs.GT)", "pdf_link": null, "comments": "Comments:      Accepted by ICML 2025 (International Conference on Machine Learning). 17 pages, 4 figures", "url": "http://arxiv.org/abs/2507.07418v1", "summary": "Online advertising is a vital revenue source for major internet platforms.\nRecently, joint advertising, which assigns a bundle of two advertisers in an ad\nslot instead of allocating a single advertiser, has emerged as an effective\nmethod for enhancing allocation efficiency and revenue. However, existing\nmechanisms for joint advertising fail to realize the optimality, as they tend\nto focus on individual advertisers and overlook bundle structures. This paper\nidentifies an optimal mechanism for joint advertising in a single-slot setting.\nFor multi-slot joint advertising, we propose \\textbf{BundleNet}, a novel\nbundle-based neural network approach specifically designed for joint\nadvertising. Our extensive experiments demonstrate that the mechanisms\ngenerated by \\textbf{BundleNet} approximate the theoretical analysis results in\nthe single-slot setting and achieve state-of-the-art performance in the\nmulti-slot setting. This significantly increases platform revenue while\nensuring approximate dominant strategy incentive compatibility and individual\nrationality.", "comment": "Accepted by ICML 2025 (International Conference on Machine Learning).\n  17 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.07418v1", "cate": "cs.GT", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07432", "title": "Neural networks leverage nominally quantum and post-quantum representations", "authors": ["Paul M. Riechers", "Thomas J. Elliott", "Adam S. Shai"], "categories": ["cs.LG", "quant-ph"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07432v1", "summary": "We show that deep neural networks, including transformers and RNNs,\npretrained as usual on next-token prediction, intrinsically discover and\nrepresent beliefs over 'quantum' and 'post-quantum' low-dimensional generative\nmodels of their training data -- as if performing iterative Bayesian updates\nover the latent state of this world model during inference as they observe more\ncontext. Notably, neural nets easily find these representation whereas there is\nno finite classical circuit that would do the job. The corresponding geometric\nrelationships among neural activations induced by different input sequences are\nfound to be largely independent of neural-network architecture. Each point in\nthis geometry corresponds to a history-induced probability density over all\npossible futures, and the relative displacement of these points reflects the\ndifference in mechanism and magnitude for how these distinct pasts affect the\nfuture.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07432v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07460", "title": "Objectomaly: Objectness-Aware Refinement for OoD Segmentation with Structural Consistency and Boundary Precision", "authors": ["Jeonghoon Song", "Sunghun Kim", "Jaegyun Im", "Byeongjoon Noh"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07460v1", "summary": "Out-of-Distribution (OoD) segmentation is critical for safety-sensitive\napplications like autonomous driving. However, existing mask-based methods\noften suffer from boundary imprecision, inconsistent anomaly scores within\nobjects, and false positives from background noise. We propose\n\\textbf{\\textit{Objectomaly}}, an objectness-aware refinement framework that\nincorporates object-level priors. Objectomaly consists of three stages: (1)\nCoarse Anomaly Scoring (CAS) using an existing OoD backbone, (2)\nObjectness-Aware Score Calibration (OASC) leveraging SAM-generated instance\nmasks for object-level score normalization, and (3) Meticulous Boundary\nPrecision (MBP) applying Laplacian filtering and Gaussian smoothing for contour\nrefinement. Objectomaly achieves state-of-the-art performance on key OoD\nsegmentation benchmarks, including SMIYC AnomalyTrack/ObstacleTrack and\nRoadAnomaly, improving both pixel-level (AuPRC up to 96.99, FPR$_{95}$ down to\n0.07) and component-level (F1$-$score up to 83.44) metrics. Ablation studies\nand qualitative results on real-world driving videos further validate the\nrobustness and generalizability of our method. Code will be released upon\npublication.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07460v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2405.20559", "title": "Information-driven design of imaging systems", "authors": ["Henry Pinkard", "Leyla Kabuli", "Eric Markley", "Tiffany Chien", "Jiantao Jiao", "Laura Waller"], "categories": ["physics.optics", "cs.CV", "cs.IT", "eess.IV", "math.IT", "physics.data-an"], "primary_category": "Subjects:       Optics (physics.optics)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2405.20559v4", "summary": "In modern imaging systems that computationally process raw measurements\nbefore or instead of human viewing, information content matters more than\nvisual appearance. However, developing information estimators that can handle\nthe complexity of real-world measurements yet remain practical enough for\nwidespread use has proven challenging. We introduce a data-driven approach for\nestimating mutual information between unknown objects and their noisy\nmeasurements. Our technique fits probabilistic models to measurements and their\nnoise processes, quantifying information content without requiring ground truth\ndata or making assumptions about object structure. We validate our approach\nacross diverse applications-color photography, radio astronomy, lensless\nimaging, and microscopy-demonstrating that information estimates reliably\npredict system performance. Finally, we introduce Information-Driven Encoder\nAnalysis Learning (IDEAL), which optimizes imaging systems to maximize\ninformation capture. Our work unlocks information theory as a powerful,\npractical tool for analyzing and designing imaging systems across a broad range\nof applications.\n  A video summarizing this work can be found at:\nhttps://waller-lab.github.io/EncodingInformationWebsite/", "comment": null, "pdf_url": "http://arxiv.org/pdf/2405.20559v4", "cate": "physics.optics", "date": "2024-05-31", "updated": "2025-07-10"}
{"id": "2507.06107", "title": "A Unified Ontology for Scalable Knowledge Graph-Driven Operational Data Analytics in High-Performance Computing Systems", "authors": ["Junaid Ahmed Khan", "Andrea Bartolini"], "categories": ["cs.DC", "cs.DB"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      This paper has been accepted for presentation at the GraphSys'25 workshop during EURO-PAR 2025. It spans 12 pages in single-column format", "url": "http://arxiv.org/abs/2507.06107v2", "summary": "Modern high-performance computing (HPC) systems generate massive volumes of\nheterogeneous telemetry data from millions of sensors monitoring compute,\nmemory, power, cooling, and storage subsystems. As HPC infrastructures scale to\nsupport increasingly complex workloads-including generative AI-the need for\nefficient, reliable, and interoperable telemetry analysis becomes critical.\nOperational Data Analytics (ODA) has emerged to address these demands; however,\nthe reliance on schema-less storage solutions limits data accessibility and\nsemantic integration. Ontologies and knowledge graphs (KG) provide an effective\nway to enable efficient and expressive data querying by capturing domain\nsemantics, but they face challenges such as significant storage overhead and\nthe limited applicability of existing ontologies, which are often tailored to\nspecific HPC systems only. In this paper, we present the first unified ontology\nfor ODA in HPC systems, designed to enable semantic interoperability across\nheterogeneous data centers. Our ontology models telemetry data from the two\nlargest publicly available ODA datasets-M100 (Cineca, Italy) and F-DATA\n(Fugaku, Japan)-within a single data model. The ontology is validated through\n36 competency questions reflecting real-world stakeholder requirements, and we\nintroduce modeling optimizations that reduce knowledge graph (KG) storage\noverhead by up to 38.84% compared to a previous approach, with an additional\n26.82% reduction depending on the desired deployment configuration. This work\npaves the way for scalable ODA KGs and supports not only analysis within\nindividual systems, but also cross-system analysis across heterogeneous HPC\nsystems.", "comment": "This paper has been accepted for presentation at the GraphSys'25\n  workshop during EURO-PAR 2025. It spans 12 pages in single-column format", "pdf_url": "http://arxiv.org/pdf/2507.06107v2", "cate": "cs.DC", "date": "2025-07-08", "updated": "2025-07-10"}
{"id": "2505.10590", "title": "Anchoring AI Capabilities in Market Valuations: The Capability Realization Rate Model and Valuation Misalignment Risk", "authors": ["Xinmin Fang", "Lingfeng Tao", "Zhengxiong Li"], "categories": ["cs.CY", "cs.AI"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      11 pages, 3 figures, NeurIPS", "url": "http://arxiv.org/abs/2505.10590v2", "summary": "Recent breakthroughs in artificial intelligence (AI) have triggered surges in\nmarket valuations for AI-related companies, often outpacing the realization of\nunderlying capabilities. We examine the anchoring effect of AI capabilities on\nequity valuations and propose a Capability Realization Rate (CRR) model to\nquantify the gap between AI potential and realized performance. Using data from\nthe 2023--2025 generative AI boom, we analyze sector-level sensitivity and\nconduct case studies (OpenAI, Adobe, NVIDIA, Meta, Microsoft, Goldman Sachs) to\nillustrate patterns of valuation premium and misalignment. Our findings\nindicate that AI-native firms commanded outsized valuation premiums anchored to\nfuture potential, while traditional companies integrating AI experienced\nre-ratings subject to proof of tangible returns. We argue that CRR can help\nidentify valuation misalignment risk-where market prices diverge from realized\nAI-driven value. We conclude with policy recommendations to improve\ntransparency, mitigate speculative bubbles, and align AI innovation with\nsustainable market value.", "comment": "11 pages, 3 figures, NeurIPS", "pdf_url": "http://arxiv.org/pdf/2505.10590v2", "cate": "cs.CY", "date": "2025-05-15", "updated": "2025-07-10"}
{"id": "2506.15543", "title": "Learning Algorithms in the Limit", "authors": ["Hristo Papazov", "Nicolas Flammarion"], "categories": ["cs.LG", "cs.AI", "cs.DS", "cs.FL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted at COLT 2025. This version matches the proceedings version apart from a small notational change in section 3", "url": "http://arxiv.org/abs/2506.15543v2", "summary": "This paper studies the problem of learning computable functions in the limit\nby extending Gold's inductive inference framework to incorporate\n\\textit{computational observations} and \\textit{restricted input sources}.\nComplimentary to the traditional Input-Output Observations, we introduce\nTime-Bound Observations, and Policy-Trajectory Observations to study the\nlearnability of general recursive functions under more realistic constraints.\nWhile input-output observations do not suffice for learning the class of\ngeneral recursive functions in the limit, we overcome this learning barrier by\nimposing computational complexity constraints or supplementing with approximate\ntime-bound observations. Further, we build a formal framework around\nobservations of \\textit{computational agents} and show that learning computable\nfunctions from policy trajectories reduces to learning rational functions from\ninput and output, thereby revealing interesting connections to finite-state\ntransducer inference. On the negative side, we show that computable or\npolynomial-mass characteristic sets cannot exist for the class of linear-time\ncomputable functions even for policy-trajectory observations.", "comment": "Accepted at COLT 2025. This version matches the proceedings version\n  apart from a small notational change in section 3", "pdf_url": "http://arxiv.org/pdf/2506.15543v2", "cate": "cs.LG", "date": "2025-06-18", "updated": "2025-07-10"}
{"id": "2507.07419", "title": "MedReadCtrl: Personalizing medical text generation with readability-controlled instruction learning", "authors": ["Hieu Tran", "Zonghai Yao", "Won Seok Jang", "Sharmin Sultana", "Allen Chang", "Yuan Zhang", "Hong Yu"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Equal contribution for the first two authors. arXiv admin note: text overlap with arXiv:2406.09205", "url": "http://arxiv.org/abs/2507.07419v1", "summary": "Generative AI has demonstrated strong potential in healthcare, from clinical\ndecision support to patient-facing chatbots that improve outcomes. A critical\nchallenge for deployment is effective human-AI communication, where content\nmust be both personalized and understandable. We introduce MedReadCtrl, a\nreadability-controlled instruction tuning framework that enables LLMs to adjust\noutput complexity without compromising meaning. Evaluations of nine datasets\nand three tasks across medical and general domains show that MedReadCtrl\nachieves significantly lower readability instruction-following errors than\nGPT-4 (e.g., 1.39 vs. 1.59 on ReadMe, p<0.001) and delivers substantial gains\non unseen clinical tasks (e.g., +14.7 ROUGE-L, +6.18 SARI on MTSamples).\nExperts consistently preferred MedReadCtrl (71.7% vs. 23.3%), especially at low\nliteracy levels. These gains reflect MedReadCtrl's ability to restructure\nclinical content into accessible, readability-aligned language while preserving\nmedical intent, offering a scalable solution to support patient education and\nexpand equitable access to AI-enabled care.", "comment": "Equal contribution for the first two authors. arXiv admin note: text\n  overlap with arXiv:2406.09205", "pdf_url": "http://arxiv.org/pdf/2507.07419v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07456", "title": "General purpose models for the chemical sciences", "authors": ["Nawaf Alampara", "Anagha Aneesh", "Marti√±o R√≠os-Garc√≠a", "Adrian Mirza", "Mara Schilling-Wilhelmi", "Ali Asghar Aghajani", "Meiling Sun", "Gordan Prastalo", "Kevin Maik Jablonka"], "categories": ["cs.LG", "cond-mat.mtrl-sci", "physics.chem-ph"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07456v1", "summary": "Data-driven techniques have a large potential to transform and accelerate the\nchemical sciences. However, chemical sciences also pose the unique challenge of\nvery diverse, small, fuzzy datasets that are difficult to leverage in\nconventional machine learning approaches completely. A new class of models,\ngeneral-purpose models (GPMs) such as large language models, have shown the\nability to solve tasks they have not been directly trained on, and to flexibly\noperate with low amounts of data in different formats. In this review, we\ndiscuss fundamental building principles of GPMs and review recent applications\nof those models in the chemical sciences across the entire scientific process.\nWhile many of these applications are still in the prototype phase, we expect\nthat the increasing interest in GPMs will make many of them mature in the\ncoming years.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07456v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07464", "title": "Degradation-Agnostic Statistical Facial Feature Transformation for Blind Face Restoration in Adverse Weather Conditions", "authors": ["Chang-Hwan Son"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07464v1", "summary": "With the increasing deployment of intelligent CCTV systems in outdoor\nenvironments, there is a growing demand for face recognition systems optimized\nfor challenging weather conditions. Adverse weather significantly degrades\nimage quality, which in turn reduces recognition accuracy. Although recent face\nimage restoration (FIR) models based on generative adversarial networks (GANs)\nand diffusion models have shown progress, their performance remains limited due\nto the lack of dedicated modules that explicitly address weather-induced\ndegradations. This leads to distorted facial textures and structures. To\naddress these limitations, we propose a novel GAN-based blind FIR framework\nthat integrates two key components: local Statistical Facial Feature\nTransformation (SFFT) and Degradation-Agnostic Feature Embedding (DAFE). The\nlocal SFFT module enhances facial structure and color fidelity by aligning the\nlocal statistical distributions of low-quality (LQ) facial regions with those\nof high-quality (HQ) counterparts. Complementarily, the DAFE module enables\nrobust statistical facial feature extraction under adverse weather conditions\nby aligning LQ and HQ encoder representations, thereby making the restoration\nprocess adaptive to severe weather-induced degradations. Experimental results\ndemonstrate that the proposed degradation-agnostic SFFT model outperforms\nexisting state-of-the-art FIR methods based on GAN and diffusion models,\nparticularly in suppressing texture distortions and accurately reconstructing\nfacial structures. Furthermore, both the SFFT and DAFE modules are empirically\nvalidated in enhancing structural fidelity and perceptual quality in face\nrestoration under challenging weather scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07464v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2409.01650", "title": "Exact computation of Transfer Entropy with Path Weight Sampling", "authors": ["Avishek Das", "Pieter Rein ten Wolde"], "categories": ["q-bio.MN", "cond-mat.soft", "cond-mat.stat-mech", "cs.IT", "math.IT", "physics.bio-ph"], "primary_category": "Subjects:       Molecular Networks (q-bio.MN)", "pdf_link": null, "comments": "Comments:      24 pages, 8 figures", "url": "http://arxiv.org/abs/2409.01650v4", "summary": "The ability to quantify the directional flow of information is vital to\nunderstanding natural systems and designing engineered information-processing\nsystems. A widely used measure to quantify this information flow is the\ntransfer entropy. However, until now, this quantity could only be obtained in\ndynamical models using approximations that are typically uncontrolled. Here we\nintroduce a computational algorithm called Transfer Entropy-Path Weight\nSampling (TE-PWS), which makes it possible, for the first time, to quantify the\ntransfer entropy and its variants exactly for any stochastic model, including\nthose with multiple hidden variables, nonlinearity, transient conditions, and\nfeedback. By leveraging techniques from polymer and path sampling, TE-PWS\nefficiently computes the transfer entropy as a Monte-Carlo average over signal\ntrajectory space. We use our exact technique to demonstrate that commonly used\napproximate methods to compute transfer entropies incur large systematic errors\nand high computational costs. As an application, we use TE-PWS in linear and\nnonlinear systems to reveal how transfer entropy can overcome naive\napplications of the data processing inequality in the presence of feedback.", "comment": "24 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2409.01650v4", "cate": "q-bio.MN", "date": "2024-09-03", "updated": "2025-07-10"}
{"id": "2507.06608", "title": "Nexus: Taming Throughput-Latency Tradeoff in LLM Serving via Efficient GPU Sharing", "authors": ["Xiaoxiang Shi", "Colin Cai", "Junjia Du", "Zhanda Zhu", "Zhihao Jia"], "categories": ["cs.DC", "cs.LG"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.06608v2", "summary": "Current prefill-decode (PD) disaggregation is typically deployed at the level\nof entire serving engines, assigning separate GPUs to handle prefill and decode\nphases. While effective at reducing latency, this approach demands more\nhardware. To improve GPU utilization, Chunked Prefill mixes prefill and decode\nrequests within the same batch, but introduces phase interference between\nprefill and decode.\n  While existing PD disaggregation solutions separate the phases across GPUs,\nwe ask: can the same decoupling be achieved within a single serving engine? The\nkey challenge lies in managing the conflicting resource requirements of prefill\nand decode when they share the same hardware. In this paper, we first show that\nchunked prefill requests cause interference with decode requests due to their\ndistinct requirements for GPU resources. Second, we find that GPU resources\nexhibit diminishing returns. Beyond a saturation point, increasing GPU\nallocation yields negligible latency improvements. This insight enables us to\nsplit a single GPU's resources and dynamically allocate them to prefill and\ndecode on the fly, effectively disaggregating the two phases within the same\nGPU.\n  Across a range of models and workloads, our system Nexus achieves up to 2.2x\nhigher throughput, 20x lower TTFT, and 2.5x lower TBT than vLLM. It also\noutperforms SGLang with up to 2x higher throughput, 2x lower TTFT, and 1.7x\nlower TBT, and achieves 1.4x higher throughput than vLLM-disaggregation using\nonly half the number of GPUs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.06608v2", "cate": "cs.DC", "date": "2025-07-09", "updated": "2025-07-10"}
{"id": "2502.04426", "title": "Decoding AI Judgment: How LLMs Assess News Credibility and Bias", "authors": ["Edoardo Loru", "Jacopo Nudo", "Niccol√≤ Di Marco", "Alessandro Santirocchi", "Roberto Atzeni", "Matteo Cinelli", "Vincenzo Cestari", "Clelia Rossi-Arnaud", "Walter Quattrociocchi"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.04426v2", "summary": "Large Language Models (LLMs) are increasingly embedded in workflows that\ninvolve evaluative processes. This raises the need to examine how such\nevaluations are built, what assumptions they rely on, and how their strategies\ndiverge from those of humans. We benchmark six LLMs against expert\nratings--NewsGuard and Media Bias/Fact Check (MBFC)--and against human\njudgments collected through a controlled experiment. To enable direct\ncomparison, we implement a structured agentic framework in which both models\nand non-expert participants follow the same evaluation procedure: selecting\ncriteria, retrieving content, and producing justifications. Despite output\nalignment, LLMs rely on different mechanisms: lexical associations and\nstatistical priors replace contextual reasoning. This reliance produces\nsystematic effects: political asymmetries, opaque justifications, and a\ntendency to confuse linguistic form with epistemic validity. Delegating\njudgment to such systems does not merely automate evaluation--it redefines it,\nshifting from normative reasoning to pattern-based approximation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.04426v2", "cate": "cs.CL", "date": "2025-02-06", "updated": "2025-07-10"}
{"id": "2507.07421", "title": "SynthEHR-Eviction: Enhancing Eviction SDoH Detection with LLM-Augmented Synthetic EHR Data", "authors": ["Zonghai Yao", "Youxia Zhao", "Avijit Mitra", "David A. Levy", "Emily Druhl", "Jack Tsai", "Hong Yu"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Equal contribution for the first two authors", "url": "http://arxiv.org/abs/2507.07421v1", "summary": "Eviction is a significant yet understudied social determinants of health\n(SDoH), linked to housing instability, unemployment, and mental health. While\neviction appears in unstructured electronic health records (EHRs), it is rarely\ncoded in structured fields, limiting downstream applications. We introduce\nSynthEHR-Eviction, a scalable pipeline combining LLMs, human-in-the-loop\nannotation, and automated prompt optimization (APO) to extract eviction\nstatuses from clinical notes. Using this pipeline, we created the largest\npublic eviction-related SDoH dataset to date, comprising 14 fine-grained\ncategories. Fine-tuned LLMs (e.g., Qwen2.5, LLaMA3) trained on\nSynthEHR-Eviction achieved Macro-F1 scores of 88.8% (eviction) and 90.3% (other\nSDoH) on human validated data, outperforming GPT-4o-APO (87.8%, 87.3%),\nGPT-4o-mini-APO (69.1%, 78.1%), and BioBERT (60.7%, 68.3%), while enabling\ncost-effective deployment across various model sizes. The pipeline reduces\nannotation effort by over 80%, accelerates dataset creation, enables scalable\neviction detection, and generalizes to other information extraction tasks.", "comment": "Equal contribution for the first two authors", "pdf_url": "http://arxiv.org/pdf/2507.07421v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07485", "title": "Resolving Token-Space Gradient Conflicts: Token Space Manipulation for Transformer-Based Multi-Task Learning", "authors": ["Wooseong Jeong", "Kuk-Jin Yoon"], "categories": ["cs.LG", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted at ICCV 2025", "url": "http://arxiv.org/abs/2507.07485v1", "summary": "Multi-Task Learning (MTL) enables multiple tasks to be learned within a\nshared network, but differences in objectives across tasks can cause negative\ntransfer, where the learning of one task degrades another task's performance.\nWhile pre-trained transformers significantly improve MTL performance, their\nfixed network capacity and rigid structure limit adaptability. Previous dynamic\nnetwork architectures attempt to address this but are inefficient as they\ndirectly convert shared parameters into task-specific ones. We propose Dynamic\nToken Modulation and Expansion (DTME-MTL), a framework applicable to any\ntransformer-based MTL architecture. DTME-MTL enhances adaptability and reduces\noverfitting by identifying gradient conflicts in token space and applying\nadaptive solutions based on conflict type. Unlike prior methods that mitigate\nnegative transfer by duplicating network parameters, DTME-MTL operates entirely\nin token space, enabling efficient adaptation without excessive parameter\ngrowth. Extensive experiments demonstrate that DTME-MTL consistently improves\nmulti-task performance with minimal computational overhead, offering a scalable\nand effective solution for enhancing transformer-based MTL models.", "comment": "Accepted at ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.07485v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07487", "title": "Driving by Hybrid Navigation: An Online HD-SD Map Association Framework and Benchmark for Autonomous Vehicles", "authors": ["Jiaxu Wan", "Xu Wang", "Mengwei Xie", "Xinyuan Chang", "Xinran Liu", "Zheng Pan", "Mu Xu", "Ding Yuan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      23 pages, 10 figures, 9 tables", "url": "http://arxiv.org/abs/2507.07487v1", "summary": "Autonomous vehicles rely on global standard-definition (SD) maps for\nroad-level route planning and online local high-definition (HD) maps for\nlane-level navigation. However, recent work concentrates on construct online HD\nmaps, often overlooking the association of global SD maps with online HD maps\nfor hybrid navigation, making challenges in utilizing online HD maps in the\nreal world. Observing the lack of the capability of autonomous vehicles in\nnavigation, we introduce \\textbf{O}nline \\textbf{M}ap \\textbf{A}ssociation, the\nfirst benchmark for the association of hybrid navigation-oriented online maps,\nwhich enhances the planning capabilities of autonomous vehicles. Based on\nexisting datasets, the OMA contains 480k of roads and 260k of lane paths and\nprovides the corresponding metrics to evaluate the performance of the model.\nAdditionally, we propose a novel framework, named Map Association Transformer,\nas the baseline method, using path-aware attention and spatial attention\nmechanisms to enable the understanding of geometric and topological\ncorrespondences. The code and dataset can be accessed at\nhttps://github.com/WallelWan/OMA-MAT.", "comment": "23 pages, 10 figures, 9 tables", "pdf_url": "http://arxiv.org/pdf/2507.07487v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2504.19955", "title": "Robust Federated Personalised Mean Estimation for the Gaussian Mixture Model", "authors": ["Malhar A. Managoli", "Vinod M. Prabhakaran", "Suhas Diggavi"], "categories": ["cs.LG", "cs.IT", "math.IT"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.19955v2", "summary": "Federated learning with heterogeneous data and personalization has received\nsignificant recent attention. Separately, robustness to corrupted data in the\ncontext of federated learning has also been studied. In this paper we explore\ncombining personalization for heterogeneous data with robustness, where a\nconstant fraction of the clients are corrupted. Motivated by this broad\nproblem, we formulate a simple instantiation which captures some of its\ndifficulty. We focus on the specific problem of personalized mean estimation\nwhere the data is drawn from a Gaussian mixture model. We give an algorithm\nwhose error depends almost linearly on the ratio of corrupted to uncorrupted\nsamples, and show a lower bound with the same behavior, albeit with a gap of a\nconstant factor.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.19955v2", "cate": "cs.LG", "date": "2025-04-28", "updated": "2025-07-10"}
{"id": "2506.02357", "title": "Evaluating LLM Agent Adherence to Hierarchical Safety Principles: A Lightweight Benchmark for Probing Foundational Controllability Components", "authors": ["Ram Potham"], "categories": ["cs.LG", "cs.AI", "cs.CY"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Preprint. This work has been submitted to the Technical AI Governance Workshop at ICML 2025 for review", "url": "http://arxiv.org/abs/2506.02357v2", "summary": "Credible safety plans for advanced AI development require methods to verify\nagent behavior and detect potential control deficiencies early. A fundamental\naspect is ensuring agents adhere to safety-critical principles, especially when\nthese conflict with operational goals. This paper introduces a lightweight,\ninterpretable benchmark to evaluate an LLM agent's ability to uphold a\nhigh-level safety principle when faced with conflicting task instructions. Our\nevaluation of six LLMs reveals two primary findings: (1) a quantifiable \"cost\nof compliance\" where safety constraints degrade task performance even when\ncompliant solutions exist, and (2) an \"illusion of compliance\" where high\nadherence often masks task incompetence rather than principled choice. These\nfindings provide initial evidence that while LLMs can be influenced by\nhierarchical directives, current approaches lack the consistency required for\nreliable safety governance.", "comment": "Preprint. This work has been submitted to the Technical AI Governance\n  Workshop at ICML 2025 for review", "pdf_url": "http://arxiv.org/pdf/2506.02357v2", "cate": "cs.LG", "date": "2025-06-03", "updated": "2025-07-10"}
{"id": "2507.07439", "title": "Towards Interpretable Time Series Foundation Models", "authors": ["Matthieu Boileau", "Philippe Helluy", "Jeremy Pawlus", "Svitlana Vyetrenko"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      International Conference on Machine Leaning (ICML) 2025 Workshop on Foundation Models for Structured Data", "url": "http://arxiv.org/abs/2507.07439v1", "summary": "In this paper, we investigate the distillation of time series reasoning\ncapabilities into small, instruction-tuned language models as a step toward\nbuilding interpretable time series foundation models. Leveraging a synthetic\ndataset of mean-reverting time series with systematically varied trends and\nnoise levels, we generate natural language annotations using a large multimodal\nmodel and use these to supervise the fine-tuning of compact Qwen models. We\nintroduce evaluation metrics that assess the quality of the distilled reasoning\n- focusing on trend direction, noise intensity, and extremum localization - and\nshow that the post-trained models acquire meaningful interpretive capabilities.\nOur results highlight the feasibility of compressing time series understanding\ninto lightweight, language-capable models suitable for on-device or\nprivacy-sensitive deployment. This work contributes a concrete foundation\ntoward developing small, interpretable models that explain temporal patterns in\nnatural language.", "comment": "International Conference on Machine Leaning (ICML) 2025 Workshop on\n  Foundation Models for Structured Data", "pdf_url": "http://arxiv.org/pdf/2507.07439v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07511", "title": "Uncertainty Quantification for Motor Imagery BCI -- Machine Learning vs. Deep Learning", "authors": ["Joris Suurmeijer", "Ivo Pascal de Jong", "Matias Valdenegro-Toro", "Andreea Ioana Sburlea"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      6 pages, 3 figures", "url": "http://arxiv.org/abs/2507.07511v1", "summary": "Brain-computer interfaces (BCIs) turn brain signals into functionally useful\noutput, but they are not always accurate. A good Machine Learning classifier\nshould be able to indicate how confident it is about a given classification, by\ngiving a probability for its classification. Standard classifiers for Motor\nImagery BCIs do give such probabilities, but research on uncertainty\nquantification has been limited to Deep Learning. We compare the uncertainty\nquantification ability of established BCI classifiers using Common Spatial\nPatterns (CSP-LDA) and Riemannian Geometry (MDRM) to specialized methods in\nDeep Learning (Deep Ensembles and Direct Uncertainty Quantification) as well as\nstandard Convolutional Neural Networks (CNNs).\n  We found that the overconfidence typically seen in Deep Learning is not a\nproblem in CSP-LDA and MDRM. We found that MDRM is underconfident, which we\nsolved by adding Temperature Scaling (MDRM-T). CSP-LDA and MDRM-T give the best\nuncertainty estimates, but Deep Ensembles and standard CNNs give the best\nclassifications. We show that all models are able to separate between easy and\ndifficult estimates, so that we can increase the accuracy of a Motor Imagery\nBCI by rejecting samples that are ambiguous.", "comment": "6 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2507.07511v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07496", "title": "Semi-supervised learning and integration of multi-sequence MR-images for carotid vessel wall and plaque segmentation", "authors": ["Marie-Christine Pali", "Christina Schwaiger", "Malik Galijasevic", "Valentin K. Ladenhauf", "Stephanie Mangesius", "Elke R. Gizewski"], "categories": ["eess.IV", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07496v1", "summary": "The analysis of carotid arteries, particularly plaques, in multi-sequence\nMagnetic Resonance Imaging (MRI) data is crucial for assessing the risk of\natherosclerosis and ischemic stroke. In order to evaluate metrics and radiomic\nfeatures, quantifying the state of atherosclerosis, accurate segmentation is\nimportant. However, the complex morphology of plaques and the scarcity of\nlabeled data poses significant challenges. In this work, we address these\nproblems and propose a semi-supervised deep learning-based approach designed to\neffectively integrate multi-sequence MRI data for the segmentation of carotid\nartery vessel wall and plaque. The proposed algorithm consists of two networks:\na coarse localization model identifies the region of interest guided by some\nprior knowledge on the position and number of carotid arteries, followed by a\nfine segmentation model for precise delineation of vessel walls and plaques. To\neffectively integrate complementary information across different MRI sequences,\nwe investigate different fusion strategies and introduce a multi-level\nmulti-sequence version of U-Net architecture. To address the challenges of\nlimited labeled data and the complexity of carotid artery MRI, we propose a\nsemi-supervised approach that enforces consistency under various input\ntransformations. Our approach is evaluated on 52 patients with\narteriosclerosis, each with five MRI sequences. Comprehensive experiments\ndemonstrate the effectiveness of our approach and emphasize the role of fusion\npoint selection in U-Net-based architectures. To validate the accuracy of our\nresults, we also include an expert-based assessment of model performance. Our\nfindings highlight the potential of fusion strategies and semi-supervised\nlearning for improving carotid artery segmentation in data-limited MRI\napplications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07496v1", "cate": "eess.IV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07065", "title": "Layer Cake Representations for Quantum Divergences", "authors": ["Po-Chieh Liu", "Christoph Hirche", "Hao-Chung Cheng"], "categories": ["quant-ph", "cs.IT", "math.IT"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      2nd version: typo corrected", "url": "http://arxiv.org/abs/2507.07065v2", "summary": "Defining suitable quantum extensions of classical divergences often poses a\nchallenge due to the non-commutative nature of quantum information. In this\nwork, we propose a new approach via what we call the layer cake representation.\nThe resulting quantum R\\'enyi and $f$-divergences are then proven to be\nequivalent to those recently defined via integral representations.\nNevertheless, the approach can provide several insights. We give an alternative\nproof of the integral representation of the relative entropy by Frenkel and\nprove a conjecture regarding a trace expression for the R\\'enyi divergence.\nAdditionally, we give applications to error exponents in hypothesis testing, a\nnew Riemann-Stieltjes type integral representation and a variational\nrepresentation.", "comment": "2nd version: typo corrected", "pdf_url": "http://arxiv.org/pdf/2507.07065v2", "cate": "quant-ph", "date": "2025-07-09", "updated": "2025-07-10"}
{"id": "2507.00004", "title": "A Theory of Inference Compute Scaling: Reasoning through Directed Stochastic Skill Search", "authors": ["Austin R. Ellis-Mohr", "Anuj K. Nayak", "Lav R. Varshney"], "categories": ["cs.LG", "cs.AI", "cs.CY", "cs.PF"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.00004v2", "summary": "Large language models (LLMs) demand considerable computational, energy, and\nfinancial resources during both training and deployment. While scaling laws for\ntraining have guided much of the field's recent progress, inference costs now\nrepresent a significant and growing component of the overall resource burden,\nparticularly for reasoning-focused models. Existing characterizations of\ncompute-optimality that consider model size, dataset size, and inference tokens\nin isolation or in fixed combinations risk overlooking more efficient operating\npoints. We introduce directed stochastic skill search (DS3), a general\nframework that represents inference as stochastic traversal over a learned\nskill graph. From a simplified yet expressive instantiation, we derive\nclosed-form expressions for task success and compute cost across a wide range\nof inference strategies -- including chain-of-thought (CoT) and tree-of-thought\n(ToT) -- enabling comparative analysis as a function of task difficulty and\nmodel capability. To that end, we extend a prior first-principles tripartite\ngraph framework of LLM training to incorporate inference, and separately bridge\nDS3 with empirical methods that characterize LLM scaling behavior. We\ntheoretically recover empirically observed patterns, including: linear accuracy\nscaling with logarithmic compute; variation in preferred inference strategies\nas a function of task difficulty and model capability; emergent behavior\nelicited by reasoning even when performance plateaus under parameter scaling;\nand both best-of-N (BoN) and majority voting behavior captured within a unified\nanalytical framework. By explicitly characterizing training-inference\ninterdependencies, our framework deepens theoretical understanding and supports\nprincipled algorithmic design and resource allocation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00004v2", "cate": "cs.LG", "date": "2025-06-10", "updated": "2025-07-10"}
{"id": "2507.07484", "title": "Machine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language Models", "authors": ["Kaiqu Liang", "Haimin Hu", "Xuandong Zhao", "Dawn Song", "Thomas L. Griffiths", "Jaime Fern√°ndez Fisac"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Project page, code & data: this https URL", "url": "http://arxiv.org/abs/2507.07484v1", "summary": "Bullshit, as conceptualized by philosopher Harry Frankfurt, refers to\nstatements made without regard to their truth value. While previous work has\nexplored large language model (LLM) hallucination and sycophancy, we propose\nmachine bullshit as an overarching conceptual framework that can allow\nresearchers to characterize the broader phenomenon of emergent loss of\ntruthfulness in LLMs and shed light on its underlying mechanisms. We introduce\nthe Bullshit Index, a novel metric quantifying LLMs' indifference to truth, and\npropose a complementary taxonomy analyzing four qualitative forms of bullshit:\nempty rhetoric, paltering, weasel words, and unverified claims. We conduct\nempirical evaluations on the Marketplace dataset, the Political Neutrality\ndataset, and our new BullshitEval benchmark (2,400 scenarios spanning 100 AI\nassistants) explicitly designed to evaluate machine bullshit. Our results\ndemonstrate that model fine-tuning with reinforcement learning from human\nfeedback (RLHF) significantly exacerbates bullshit and inference-time\nchain-of-thought (CoT) prompting notably amplify specific bullshit forms,\nparticularly empty rhetoric and paltering. We also observe prevalent machine\nbullshit in political contexts, with weasel words as the dominant strategy. Our\nfindings highlight systematic challenges in AI alignment and provide new\ninsights toward more truthful LLM behavior.", "comment": "Project page, code & data: https://machine-bullshit.github.io", "pdf_url": "http://arxiv.org/pdf/2507.07484v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07532", "title": "Neural Concept Verifier: Scaling Prover-Verifier Games via Concept Encodings", "authors": ["Berkant Turan", "Suhrab Asadulla", "David Steinmann", "Wolfgang Stammer", "Sebastian Pokutta"], "categories": ["cs.LG", "cs.AI", "68T01, 68T07", "I.2.6"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      16 pages, 4 figures, 8 tables", "url": "http://arxiv.org/abs/2507.07532v1", "summary": "While Prover-Verifier Games (PVGs) offer a promising path toward\nverifiability in nonlinear classification models, they have not yet been\napplied to complex inputs such as high-dimensional images. Conversely, Concept\nBottleneck Models (CBMs) effectively translate such data into interpretable\nconcepts but are limited by their reliance on low-capacity linear predictors.\nIn this work, we introduce the Neural Concept Verifier (NCV), a unified\nframework combining PVGs with concept encodings for interpretable, nonlinear\nclassification in high-dimensional settings. NCV achieves this by utilizing\nrecent minimally supervised concept discovery models to extract structured\nconcept encodings from raw inputs. A prover then selects a subset of these\nencodings, which a verifier -- implemented as a nonlinear predictor -- uses\nexclusively for decision-making. Our evaluations show that NCV outperforms CBM\nand pixel-based PVG classifier baselines on high-dimensional, logically complex\ndatasets and also helps mitigate shortcut behavior. Overall, we demonstrate NCV\nas a promising step toward performative, verifiable AI.", "comment": "16 pages, 4 figures, 8 tables", "pdf_url": "http://arxiv.org/pdf/2507.07532v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07510", "title": "Divergence Minimization Preference Optimization for Diffusion Model Alignment", "authors": ["Binxu Li", "Minkai Xu", "Meihua Dang", "Stefano Ermon"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      24 pages, 8 figures", "url": "http://arxiv.org/abs/2507.07510v1", "summary": "Diffusion models have achieved remarkable success in generating realistic and\nversatile images from text prompts. Inspired by the recent advancements of\nlanguage models, there is an increasing interest in further improving the\nmodels by aligning with human preferences. However, we investigate alignment\nfrom a divergence minimization perspective and reveal that existing preference\noptimization methods are typically trapped in suboptimal mean-seeking\noptimization. In this paper, we introduce Divergence Minimization Preference\nOptimization (DMPO), a novel and principled method for aligning diffusion\nmodels by minimizing reverse KL divergence, which asymptotically enjoys the\nsame optimization direction as original RL. We provide rigorous analysis to\njustify the effectiveness of DMPO and conduct comprehensive experiments to\nvalidate its empirical strength across both human evaluations and automatic\nmetrics. Our extensive results show that diffusion models fine-tuned with DMPO\ncan consistently outperform or match existing techniques, specifically\noutperforming all existing diffusion alignment baselines by at least 64.6% in\nPickScore across all evaluation datasets, demonstrating the method's\nsuperiority in aligning generative behavior with desired outputs. Overall, DMPO\nunlocks a robust and elegant pathway for preference alignment, bridging\nprincipled theory with practical performance in diffusion models.", "comment": "24 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.07510v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.01936", "title": "The Thin Line Between Comprehension and Persuasion in LLMs", "authors": ["Adrian de Wynter", "Tangming Yuan"], "categories": ["cs.CL", "cs.CY"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Preprint", "url": "http://arxiv.org/abs/2507.01936v2", "summary": "Large language models (LLMs) are excellent at maintaining high-level,\nconvincing dialogues. They are being fast deployed as chatbots and evaluators\nin sensitive areas, such as peer review and mental health applications. This,\nalong with the disparate accounts on their reasoning capabilities, calls for a\ncloser examination of LLMs and their comprehension of dialogue. In this work we\nbegin by evaluating LLMs' ability to maintain a debate--one of the purest yet\nmost complex forms of human communication. Then we measure how this capability\nrelates to their understanding of what is being talked about, namely, their\ncomprehension of dialogical structures and the pragmatic context. We find that\nLLMs are capable of maintaining coherent, persuasive debates, often swaying the\nbeliefs of participants and audiences alike. We also note that awareness or\nsuspicion of AI involvement encourage people to be more critical of the\narguments made. When polling LLMs on their comprehension of deeper structures\nof dialogue, however, they cannot demonstrate said understanding. Our findings\ntie the shortcomings of LLMs-as-evaluators to their (in)ability to understand\nthe context. More broadly, for the field of argumentation theory we posit that,\nif an agent can convincingly maintain a dialogue, it is not necessary for it to\nknow what it is talking about. Hence, the modelling of pragmatic context and\ncoherence are secondary to effectiveness.", "comment": "Preprint", "pdf_url": "http://arxiv.org/pdf/2507.01936v2", "cate": "cs.CL", "date": "2025-07-02", "updated": "2025-07-10"}
{"id": "2507.07495", "title": "PLAN-TUNING: Post-Training Language Models to Learn Step-by-Step Planning for Complex Problem Solving", "authors": ["Mihir Parmar", "Palash Goyal", "Xin Liu", "Yiwen Song", "Mingyang Ling", "Chitta Baral", "Hamid Palangi", "Tomas Pfister"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      15 Pages", "url": "http://arxiv.org/abs/2507.07495v1", "summary": "Recently, decomposing complex problems into simple subtasks--a crucial part\nof human-like natural planning--to solve the given problem has significantly\nboosted the performance of large language models (LLMs). However, leveraging\nsuch planning structures during post-training to boost the performance of\nsmaller open-source LLMs remains underexplored. Motivated by this, we introduce\nPLAN-TUNING, a unified post-training framework that (i) distills synthetic task\ndecompositions (termed \"planning trajectories\") from large-scale LLMs and (ii)\nfine-tunes smaller models via supervised and reinforcement-learning objectives\ndesigned to mimic these planning processes to improve complex reasoning. On\nGSM8k and the MATH benchmarks, plan-tuned models outperform strong baselines by\nan average $\\sim7\\%$. Furthermore, plan-tuned models show better generalization\ncapabilities on out-of-domain datasets, with average $\\sim10\\%$ and $\\sim12\\%$\nperformance improvements on OlympiadBench and AIME 2024, respectively. Our\ndetailed analysis demonstrates how planning trajectories improves complex\nreasoning capabilities, showing that PLAN-TUNING is an effective strategy for\nimproving task-specific performance of smaller LLMs.", "comment": "15 Pages", "pdf_url": "http://arxiv.org/pdf/2507.07495v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07559", "title": "Real-Time Decorrelation-Based Anomaly Detection for Multivariate Time Series", "authors": ["Amirhossein Sadough", "Mahyar Shahsavari", "Mark Wijtvliet", "Marcel van Gerven"], "categories": ["cs.LG", "cs.SY", "eess.SP", "eess.SY"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07559v1", "summary": "Anomaly detection (AD) plays a vital role across a wide range of real-world\ndomains by identifying data instances that deviate from expected patterns,\npotentially signaling critical events such as system failures, fraudulent\nactivities, or rare medical conditions. The demand for real-time AD has surged\nwith the rise of the (Industrial) Internet of Things, where massive volumes of\nmultivariate sensor data must be processed instantaneously. Real-time AD\nrequires methods that not only handle high-dimensional streaming data but also\noperate in a single-pass manner, without the burden of storing historical\ninstances, thereby ensuring minimal memory usage and fast decision-making. We\npropose DAD, a novel real-time decorrelation-based anomaly detection method for\nmultivariate time series, based on an online decorrelation learning approach.\nUnlike traditional proximity-based or reconstruction-based detectors that\nprocess entire data or windowed instances, DAD dynamically learns and monitors\nthe correlation structure of data sample by sample in a single pass, enabling\nefficient and effective detection. To support more realistic benchmarking\npractices, we also introduce a practical hyperparameter tuning strategy\ntailored for real-time anomaly detection scenarios. Extensive experiments on\nwidely used benchmark datasets demonstrate that DAD achieves the most\nconsistent and superior performance across diverse anomaly types compared to\nstate-of-the-art methods. Crucially, its robustness to increasing\ndimensionality makes it particularly well-suited for real-time,\nhigh-dimensional data streams. Ultimately, DAD not only strikes an optimal\nbalance between detection efficacy and computational efficiency but also sets a\nnew standard for real-time, memory-constrained anomaly detection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07559v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07515", "title": "GGMotion: Group Graph Dynamics-Kinematics Networks for Human Motion Prediction", "authors": ["Shuaijin Wan", "Huaijiang Sun"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07515v1", "summary": "Human motion is a continuous physical process in 3D space, governed by\ncomplex dynamic and kinematic constraints. Existing methods typically represent\nthe human pose as an abstract graph structure, neglecting the intrinsic\nphysical dependencies between joints, which increases learning difficulty and\nmakes the model prone to generating unrealistic motions. In this paper, we\npropose GGMotion, a group graph dynamics-kinematics network that models human\ntopology in groups to better leverage dynamics and kinematics priors. To\npreserve the geometric equivariance in 3D space, we propose a novel radial\nfield for the graph network that captures more comprehensive spatio-temporal\ndependencies by aggregating joint features through spatial and temporal edges.\nInter-group and intra-group interaction modules are employed to capture the\ndependencies of joints at different scales. Combined with equivariant\nmultilayer perceptrons (MLP), joint position features are updated in each group\nthrough parallelized dynamics-kinematics propagation to improve physical\nplausibility. Meanwhile, we introduce an auxiliary loss to supervise motion\npriors during training. Extensive experiments on three standard benchmarks,\nincluding Human3.6M, CMU-Mocap, and 3DPW, demonstrate the effectiveness and\nsuperiority of our approach, achieving a significant performance margin in\nshort-term motion prediction. The code is available at\nhttps://github.com/inkcat520/GGMotion.git.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07515v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.03015", "title": "Beyond Overcorrection: Evaluating Diversity in T2I Models with DivBench", "authors": ["Felix Friedrich", "Thiemo Ganesha Welsch", "Manuel Brack", "Patrick Schramowski", "Kristian Kersting"], "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.03015v2", "summary": "Current diversification strategies for text-to-image (T2I) models often\nignore contextual appropriateness, leading to over-diversification where\ndemographic attributes are modified even when explicitly specified in prompts.\nThis paper introduces DIVBENCH, a benchmark and evaluation framework for\nmeasuring both under- and over-diversification in T2I generation. Through\nsystematic evaluation of state-of-the-art T2I models, we find that while most\nmodels exhibit limited diversity, many diversification approaches overcorrect\nby inappropriately altering contextually-specified attributes. We demonstrate\nthat context-aware methods, particularly LLM-guided FairDiffusion and prompt\nrewriting, can already effectively address under-diversity while avoiding\nover-diversification, achieving a better balance between representation and\nsemantic fidelity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.03015v2", "cate": "cs.CL", "date": "2025-07-02", "updated": "2025-07-10"}
{"id": "2507.07505", "title": "Hallucination Stations: On Some Basic Limitations of Transformer-Based Language Models", "authors": ["Varin Sikka", "Vishal Sikka"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      6 pages; to be submitted to AAAI-26 after reviews", "url": "http://arxiv.org/abs/2507.07505v1", "summary": "With widespread adoption of transformer-based language models in AI, there is\nsignificant interest in the limits of LLMs capabilities, specifically so-called\nhallucinations, occurrences in which LLMs provide spurious, factually incorrect\nor nonsensical information when prompted on certain subjects. Furthermore,\nthere is growing interest in agentic uses of LLMs - that is, using LLMs to\ncreate agents that act autonomously or semi-autonomously to carry out various\ntasks, including tasks with applications in the real world. This makes it\nimportant to understand the types of tasks LLMs can and cannot perform. We\nexplore this topic from the perspective of the computational complexity of LLM\ninference. We show that LLMs are incapable of carrying out computational and\nagentic tasks beyond a certain complexity, and further that LLMs are incapable\nof verifying the accuracy of tasks beyond a certain complexity. We present\nexamples of both, then discuss some consequences of this work.", "comment": "6 pages; to be submitted to AAAI-26 after reviews", "pdf_url": "http://arxiv.org/pdf/2507.07505v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07580", "title": "COALA: Numerically Stable and Efficient Framework for Context-Aware Low-Rank Approximation", "authors": ["Uliana Parkina", "Maxim Rakhuba"], "categories": ["cs.LG", "cs.CL", "cs.NA", "math.NA", "65F55, 68T50"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07580v1", "summary": "Recent studies suggest that context-aware low-rank approximation is a useful\ntool for compression and fine-tuning of modern large-scale neural networks. In\nthis type of approximation, a norm is weighted by a matrix of input\nactivations, significantly improving metrics over the unweighted case.\nNevertheless, existing methods for neural networks suffer from numerical\ninstabilities due to their reliance on classical formulas involving explicit\nGram matrix computation and their subsequent inversion. We demonstrate that\nthis can degrade the approximation quality or cause numerically singular\nmatrices.\n  To address these limitations, we propose a novel inversion-free regularized\nframework that is based entirely on stable decompositions and overcomes the\nnumerical pitfalls of prior art. Our method can handle possible challenging\nscenarios: (1) when calibration matrices exceed GPU memory capacity, (2) when\ninput activation matrices are nearly singular, and even (3) when insufficient\ndata prevents unique approximation. For the latter, we prove that our solution\nconverges to a desired approximation and derive explicit error bounds.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07580v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07519", "title": "MUVOD: A Novel Multi-view Video Object Segmentation Dataset and A Benchmark for 3D Segmentation", "authors": ["Bangning Wei", "Joshua Maraval", "Meriem Outtas", "Kidiyo Kpalma", "Nicolas Ramin", "Lu Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07519v1", "summary": "The application of methods based on Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3D GS) have steadily gained popularity in the field of 3D\nobject segmentation in static scenes. These approaches demonstrate efficacy in\na range of 3D scene understanding and editing tasks. Nevertheless, the 4D\nobject segmentation of dynamic scenes remains an underexplored field due to the\nabsence of a sufficiently extensive and accurately labelled multi-view video\ndataset. In this paper, we present MUVOD, a new multi-view video dataset for\ntraining and evaluating object segmentation in reconstructed real-world\nscenarios. The 17 selected scenes, describing various indoor or outdoor\nactivities, are collected from different sources of datasets originating from\nvarious types of camera rigs. Each scene contains a minimum of 9 views and a\nmaximum of 46 views. We provide 7830 RGB images (30 frames per video) with\ntheir corresponding segmentation mask in 4D motion, meaning that any object of\ninterest in the scene could be tracked across temporal frames of a given view\nor across different views belonging to the same camera rig. This dataset, which\ncontains 459 instances of 73 categories, is intended as a basic benchmark for\nthe evaluation of multi-view video segmentation methods. We also present an\nevaluation metric and a baseline segmentation approach to encourage and\nevaluate progress in this evolving field. Additionally, we propose a new\nbenchmark for 3D object segmentation task with a subset of annotated multi-view\nimages selected from our MUVOD dataset. This subset contains 50 objects of\ndifferent conditions in different scenarios, providing a more comprehensive\nanalysis of state-of-the-art 3D object segmentation methods. Our proposed MUVOD\ndataset is available at https://volumetric-repository.labs.b-com.com/#/muvod.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07519v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07539", "title": "CEA-LIST at CheckThat! 2025: Evaluating LLMs as Detectors of Bias and Opinion in Text", "authors": ["Akram Elbouanani", "Evan Dufraisse", "Aboubacar Tuo", "Adrian Popescu"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Notebook for the CheckThat! Lab at CLEF 2025", "url": "http://arxiv.org/abs/2507.07539v1", "summary": "This paper presents a competitive approach to multilingual subjectivity\ndetection using large language models (LLMs) with few-shot prompting. We\nparticipated in Task 1: Subjectivity of the CheckThat! 2025 evaluation\ncampaign. We show that LLMs, when paired with carefully designed prompts, can\nmatch or outperform fine-tuned smaller language models (SLMs), particularly in\nnoisy or low-quality data settings. Despite experimenting with advanced prompt\nengineering techniques, such as debating LLMs and various example selection\nstrategies, we found limited benefit beyond well-crafted standard few-shot\nprompts. Our system achieved top rankings across multiple languages in the\nCheckThat! 2025 subjectivity detection task, including first place in Arabic\nand Polish, and top-four finishes in Italian, English, German, and multilingual\ntracks. Notably, our method proved especially robust on the Arabic dataset,\nlikely due to its resilience to annotation inconsistencies. These findings\nhighlight the effectiveness and adaptability of LLM-based few-shot learning for\nmultilingual sentiment tasks, offering a strong alternative to traditional\nfine-tuning, particularly when labeled data is scarce or inconsistent.", "comment": "Notebook for the CheckThat! Lab at CLEF 2025", "pdf_url": "http://arxiv.org/pdf/2507.07539v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07604", "title": "Synthetic MC via Biological Transmitters: Therapeutic Modulation of the Gut-Brain Axis", "authors": ["Sebastian Lotter", "Elisabeth Mohr", "Andrina Rutsch", "Lukas Brand", "Francesca Ronchi", "Laura D√≠az-Marug√°n"], "categories": ["cs.LG", "q-bio.QM", "q-bio.TO"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07604v1", "summary": "Synthetic molecular communication (SMC) is a key enabler for future\nhealthcare systems in which Internet of Bio-Nano-Things (IoBNT) devices\nfacilitate the continuous monitoring of a patient's biochemical signals. To\nclose the loop between sensing and actuation, both the detection and the\ngeneration of in-body molecular communication (MC) signals is key. However,\ngenerating signals inside the human body, e.g., via synthetic nanodevices,\nposes a challenge in SMC, due to technological obstacles as well as legal,\nsafety, and ethical issues. Hence, this paper considers an SMC system in which\nsignals are generated indirectly via the modulation of a natural in-body MC\nsystem, namely the gut-brain axis (GBA). Therapeutic GBA modulation is already\nestablished as treatment for neurological diseases, e.g., drug refractory\nepilepsy (DRE), and performed via the administration of nutritional supplements\nor specific diets. However, the molecular signaling pathways that mediate the\neffect of such treatments are mostly unknown. Consequently, existing treatments\nare standardized or designed heuristically and able to help only some patients\nwhile failing to help others. In this paper, we propose to leverage personal\nhealth data, e.g., gathered by in-body IoBNT devices, to design more versatile\nand robust GBA modulation-based treatments as compared to the existing ones. To\nshow the feasibility of our approach, we define a catalog of theoretical\nrequirements for therapeutic GBA modulation. Then, we propose a machine\nlearning model to verify these requirements for practical scenarios when only\nlimited data on the GBA modulation exists. By evaluating the proposed model on\nseveral datasets, we confirm its excellent accuracy in identifying different\nmodulators of the GBA. Finally, we utilize the proposed model to identify\nspecific modulatory pathways that play an important role for therapeutic GBA\nmodulation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07604v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07521", "title": "Spline Deformation Field", "authors": ["Mingyang Song", "Yang Zhang", "Marko Mihajlovic", "Siyu Tang", "Markus Gross", "Tun√ß Ozan Aydƒ±n"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07521v1", "summary": "Trajectory modeling of dense points usually employs implicit deformation\nfields, represented as neural networks that map coordinates to relate canonical\nspatial positions to temporal offsets. However, the inductive biases inherent\nin neural networks can hinder spatial coherence in ill-posed scenarios. Current\nmethods focus either on enhancing encoding strategies for deformation fields,\noften resulting in opaque and less intuitive models, or adopt explicit\ntechniques like linear blend skinning, which rely on heuristic-based node\ninitialization. Additionally, the potential of implicit representations for\ninterpolating sparse temporal signals remains under-explored. To address these\nchallenges, we propose a spline-based trajectory representation, where the\nnumber of knots explicitly determines the degrees of freedom. This approach\nenables efficient analytical derivation of velocities, preserving spatial\ncoherence and accelerations, while mitigating temporal fluctuations. To model\nknot characteristics in both spatial and temporal domains, we introduce a novel\nlow-rank time-variant spatial encoding, replacing conventional coupled\nspatiotemporal techniques. Our method demonstrates superior performance in\ntemporal interpolation for fitting continuous fields with sparse inputs.\nFurthermore, it achieves competitive dynamic scene reconstruction quality\ncompared to state-of-the-art methods while enhancing motion coherence without\nrelying on linear blend skinning or as-rigid-as-possible constraints.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07521v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07543", "title": "The Cross-Lingual Cost: Retrieval Biases in RAG over Arabic-English Corpora", "authors": ["Chen Amiraz", "Yaroslav Fyodorov", "Elad Haramaty", "Zohar Karnin", "Liane Lewin-Eytan"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07543v1", "summary": "Cross-lingual retrieval-augmented generation (RAG) is a critical capability\nfor retrieving and generating answers across languages. Prior work in this\ncontext has mostly focused on generation and relied on benchmarks derived from\nopen-domain sources, most notably Wikipedia. In such settings, retrieval\nchallenges often remain hidden due to language imbalances, overlap with\npretraining data, and memorized content. To address this gap, we study\nArabic-English RAG in a domain-specific setting using benchmarks derived from\nreal-world corporate datasets. Our benchmarks include all combinations of\nlanguages for the user query and the supporting document, drawn independently\nand uniformly at random. This enables a systematic study of multilingual\nretrieval behavior.\n  Our findings reveal that retrieval is a critical bottleneck in cross-lingual\ndomain-specific scenarios, with significant performance drops occurring when\nthe user query and supporting document languages differ. A key insight is that\nthese failures stem primarily from the retriever's difficulty in ranking\ndocuments across languages. Finally, we propose a simple retrieval strategy\nthat addresses this source of failure by enforcing equal retrieval from both\nlanguages, resulting in substantial improvements in cross-lingual and overall\nperformance. These results highlight meaningful opportunities for improving\nmultilingual retrieval, particularly in practical, real-world RAG applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07543v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07613", "title": "Sparse Self-Federated Learning for Energy Efficient Cooperative Intelligence in Society 5.0", "authors": ["Davide Domini", "Laura Erhan", "Gianluca Aguzzi", "Lucia Cavallaro", "Amirhossein Douzandeh Zenoozi", "Antonio Liotta", "Mirko Viroli"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07613v1", "summary": "Federated Learning offers privacy-preserving collaborative intelligence but\nstruggles to meet the sustainability demands of emerging IoT ecosystems\nnecessary for Society 5.0-a human-centered technological future balancing\nsocial advancement with environmental responsibility. The excessive\ncommunication bandwidth and computational resources required by traditional FL\napproaches make them environmentally unsustainable at scale, creating a\nfundamental conflict with green AI principles as billions of\nresource-constrained devices attempt to participate. To this end, we introduce\nSparse Proximity-based Self-Federated Learning (SParSeFuL), a resource-aware\napproach that bridges this gap by combining aggregate computing for\nself-organization with neural network sparsification to reduce energy and\nbandwidth consumption.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07613v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07527", "title": "MAPEX: Modality-Aware Pruning of Experts for Remote Sensing Foundation Models", "authors": ["Joelle Hanna", "Linus Scheibenreif", "Damian Borth"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07527v1", "summary": "Remote sensing data is commonly used for tasks such as flood mapping,\nwildfire detection, or land-use studies. For each task, scientists carefully\nchoose appropriate modalities or leverage data from purpose-built instruments.\nRecent work on remote sensing foundation models pre-trains computer vision\nmodels on large amounts of remote sensing data. These large-scale models tend\nto focus on specific modalities, often optical RGB or multispectral data. For\nmany important applications, this introduces a mismatch between the application\nmodalities and the pre-training data. Moreover, the large size of foundation\nmodels makes them expensive and difficult to fine-tune on typically small\ndatasets for each task. We address this mismatch with MAPEX, a remote sensing\nfoundation model based on mixture-of-modality experts. MAPEX is pre-trained on\nmulti-modal remote sensing data with a novel modality-conditioned token routing\nmechanism that elicits modality-specific experts. To apply the model on a\nspecific task, we propose a modality aware pruning technique, which only\nretains experts specialized for the task modalities. This yields efficient\nmodality-specific models while simplifying fine-tuning and deployment for the\nmodalities of interest. We experimentally validate MAPEX on diverse remote\nsensing datasets and show strong performance compared to fully supervised\ntraining and state-of-the-art remote sensing foundation models. Code is\navailable at https://github.com/HSG-AIML/MAPEX.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07527v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07239", "title": "Three-Dimensional Millimeter-Wave Imaging Using Active Incoherent Fourier Processing and Pulse Compression", "authors": ["Jorge R. Colon-Berrios", "Jason M. Merlo", "Jeffrey A. Nanzer"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07239v1", "summary": "We present a novel three-dimensional (3D) imaging approach that combines\ntwo-dimensional spatial Fourier-domain imaging techniques with traditional\nradar pulse compression to recover both cross-range and down-range scene\ninformation. The imaging system employs four transmitters, three of which emit\nspatially and temporally incoherent noise signals, while the fourth transmits a\nknown linear frequency modulated (LFM) pulsed signal. The spatial incoherence\nof the noise signals enables sampling of the 2D spatial Fourier spectrum of the\nscene from which two-dimensional cross-range (azimuth and elevation) images can\nbe formed via interferometric processing. Simultaneously, the LFM signal\nenables high-resolution downrange imaging through matched filtering. The\nreceived signals consist of a superposition of the noise sources and the known\npulse allowing for joint recovery of all three dimensions. We describe the\nsystem architecture and waveform design, and demonstrate the imaging technique\nusing both simulations with a linear array and experimental data from a 38 GHz\nactive incoherent millimeter-wave imaging system with 23-element randomized\narray. Results show the reconstruction of targets in three dimensions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07239v1", "cate": "eess.SP", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07572", "title": "Single-to-mix Modality Alignment with Multimodal Large Language Model for Document Image Machine Translation", "authors": ["Yupu Liang", "Yaping Zhang", "Zhiyang Zhang", "Yang Zhao", "Lu Xiang", "Chengqing Zong", "Yu Zhou"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted by ACL 2025 Main", "url": "http://arxiv.org/abs/2507.07572v1", "summary": "Document Image Machine Translation (DIMT) aims to translate text within\ndocument images, facing generalization challenges due to limited training data\nand the complex interplay between visual and textual information. To address\nthese challenges, we introduce M4Doc, a novel single-to-mix modality alignment\nframework leveraging Multimodal Large Language Models (MLLMs). M4Doc aligns an\nimage-only encoder with the multimodal representations of an MLLM, pre-trained\non large-scale document image datasets. This alignment enables a lightweight\nDIMT model to learn crucial visual-textual correlations during training. During\ninference, M4Doc bypasses the MLLM, maintaining computational efficiency while\nbenefiting from its multimodal knowledge. Comprehensive experiments demonstrate\nsubstantial improvements in translation quality, especially in cross-domain\ngeneralization and challenging document image scenarios.", "comment": "Accepted by ACL 2025 Main", "pdf_url": "http://arxiv.org/pdf/2507.07572v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07621", "title": "Sparse Causal Discovery with Generative Intervention for Unsupervised Graph Domain Adaptation", "authors": ["Junyu Luo", "Yuhao Tang", "Yiwei Fu", "Xiao Luo", "Zhizhuo Kou", "Zhiping Xiao", "Wei Ju", "Wentao Zhang", "Ming Zhang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      ICML 2025", "url": "http://arxiv.org/abs/2507.07621v1", "summary": "Unsupervised Graph Domain Adaptation (UGDA) leverages labeled source domain\ngraphs to achieve effective performance in unlabeled target domains despite\ndistribution shifts. However, existing methods often yield suboptimal results\ndue to the entanglement of causal-spurious features and the failure of global\nalignment strategies. We propose SLOGAN (Sparse Causal Discovery with\nGenerative Intervention), a novel approach that achieves stable graph\nrepresentation transfer through sparse causal modeling and dynamic intervention\nmechanisms. Specifically, SLOGAN first constructs a sparse causal graph\nstructure, leveraging mutual information bottleneck constraints to disentangle\nsparse, stable causal features while compressing domain-dependent spurious\ncorrelations through variational inference. To address residual spurious\ncorrelations, we innovatively design a generative intervention mechanism that\nbreaks local spurious couplings through cross-domain feature recombination\nwhile maintaining causal feature semantic consistency via covariance\nconstraints. Furthermore, to mitigate error accumulation in target domain\npseudo-labels, we introduce a category-adaptive dynamic calibration strategy,\nensuring stable discriminative learning. Extensive experiments on multiple\nreal-world datasets demonstrate that SLOGAN significantly outperforms existing\nbaselines.", "comment": "ICML 2025", "pdf_url": "http://arxiv.org/pdf/2507.07621v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07574", "title": "Beyond the Linear Separability Ceiling", "authors": ["Enrico Vompa", "Tanel Tammet", "Mohit Vaishnav"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07574v1", "summary": "Most state-of-the-art Visual-Language Models (VLMs) are seemingly limited by\nthe linear separabilty of their visual embeddings on abstract reasoning tasks.\nThis work investigates this \"linear reasoning bottleneck\" by introducing the\nLinear Separability Ceiling (LSC), the performance of a simple linear\nclassifier on a VLM's visual embeddings. We find this bottleneck is widespread\nand stems not from poor perception, but from failures in the language model's\nreasoning pathways. We demonstrate this is a solvable alignment issue. The\nrequired intervention, however, is task-dependent: activating existing pathways\nsuffices for semantic concepts, while complex relational reasoning requires\nadapting core model weights. Using postfix tuning as a methodological control,\nwe find strong evidence for powerful, dormant reasoning pathways within VLMs.\nHowever, for complex relational tasks requiring deeper adaptation, explicitly\nimproving representation quality causes the model to fail on new prompt formats\ndespite its embeddings remaining well separated. Ultimately, this work provides\na new lens for VLM analysis, showing that robust reasoning is a matter of\ntargeted alignment, not simply improved representation learning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07574v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07285", "title": "A RIS-Enabled Computational Radar Coincidence Imaging", "authors": ["Kavian Zirak", "Mohammadreza F. Imani"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07285v1", "summary": "This paper introduces an innovative imaging method using reconfigurable\nintelligent surfaces (RISs) by combining radar coincidence imaging (RCI) and\ncomputational imaging techniques. In the proposed framework, RISs\nsimultaneously redirect beams toward a desired region of interest (ROI). The\ninterference of these beams forms spatially diverse speckle patterns that carry\ninformation about the entire ROI. As a result, this method can take advantage\nof the benefits of both random patterns and spotlight imaging. Since the\nspeckle pattern is formed by directive beams (instead of random patterns\ntypically used in computational imaging), this approach results in a higher\nsignal-to-noise ratio (SNR) and reduced clutter. In contrast to raster\nscanning, which requires the number of measurements to be at least equal to the\nnumber of unknowns, our proposed approach follows a computational imaging\nframework and can obtain high-quality images even when only a few measurements\nare taken. Using numerical simulation, we demonstrate this method's\ncapabilities and contrast it against other conventional techniques. The\nproposed imaging approach can be applied to security screening, wireless user\ntracking, and activity recognition.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07285v1", "cate": "eess.SP", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07579", "title": "NexViTAD: Few-shot Unsupervised Cross-Domain Defect Detection via Vision Foundation Models and Multi-Task Learning", "authors": ["Tianwei Mu", "Feiyu Duan", "Bo Zhou", "Dan Xue", "Manhong Huang"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07579v1", "summary": "This paper presents a novel few-shot cross-domain anomaly detection\nframework, Nexus Vision Transformer for Anomaly Detection (NexViTAD), based on\nvision foundation models, which effectively addresses domain-shift challenges\nin industrial anomaly detection through innovative shared subspace projection\nmechanisms and multi-task learning (MTL) module. The main innovations include:\n(1) a hierarchical adapter module that adaptively fuses complementary features\nfrom Hiera and DINO-v2 pre-trained models, constructing more robust feature\nrepresentations; (2) a shared subspace projection strategy that enables\neffective cross-domain knowledge transfer through bottleneck dimension\nconstraints and skip connection mechanisms; (3) a MTL Decoder architecture\nsupports simultaneous processing of multiple source domains, significantly\nenhancing model generalization capabilities; (4) an anomaly score inference\nmethod based on Sinkhorn-K-means clustering, combined with Gaussian filtering\nand adaptive threshold processing for precise pixel level. Valuated on the\nMVTec AD dataset, NexViTAD delivers state-of-the-art performance with an AUC of\n97.5%, AP of 70.4%, and PRO of 95.2% in the target domains, surpassing other\nrecent models, marking a transformative advance in cross-domain defect\ndetection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07579v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07622", "title": "TransformEEG: Towards Improving Model Generalizability in Deep Learning-based EEG Parkinson's Disease Detection", "authors": ["Federico Del Pup", "Riccardo Brun", "Filippo Iotti", "Edoardo Paccagnella", "Mattia Pezzato", "Sabrina Bertozzo", "Andrea Zanola", "Louis Fabrice Tshimanga", "Henning M√ºller", "Manfredo Atzori"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Submitted for possible publication. GitHub repository: see this https URL", "url": "http://arxiv.org/abs/2507.07622v1", "summary": "Electroencephalography (EEG) is establishing itself as an important,\nlow-cost, noninvasive diagnostic tool for the early detection of Parkinson's\nDisease (PD). In this context, EEG-based Deep Learning (DL) models have shown\npromising results due to their ability to discover highly nonlinear patterns\nwithin the signal. However, current state-of-the-art DL models suffer from poor\ngeneralizability caused by high inter-subject variability. This high\nvariability underscores the need for enhancing model generalizability by\ndeveloping new architectures better tailored to EEG data. This paper introduces\nTransformEEG, a hybrid Convolutional-Transformer designed for Parkinson's\ndisease detection using EEG data. Unlike transformer models based on the EEGNet\nstructure, TransformEEG incorporates a depthwise convolutional tokenizer. This\ntokenizer is specialized in generating tokens composed by channel-specific\nfeatures, which enables more effective feature mixing within the self-attention\nlayers of the transformer encoder. To evaluate the proposed model, four public\ndatasets comprising 290 subjects (140 PD patients, 150 healthy controls) were\nharmonized and aggregated. A 10-outer, 10-inner Nested-Leave-N-Subjects-Out\n(N-LNSO) cross-validation was performed to provide an unbiased comparison\nagainst seven other consolidated EEG deep learning models. TransformEEG\nachieved the highest balanced accuracy's median (78.45%) as well as the lowest\ninterquartile range (6.37%) across all the N-LNSO partitions. When combined\nwith data augmentation and threshold correction, median accuracy increased to\n80.10%, with an interquartile range of 5.74%. In conclusion, TransformEEG\nproduces more consistent and less skewed results. It demonstrates a substantial\nreduction in variability and more reliable PD detection using EEG data compared\nto the other investigated models.", "comment": "Submitted for possible publication. GitHub repository: see\n  https://github.com/MedMaxLab/transformeeg", "pdf_url": "http://arxiv.org/pdf/2507.07622v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07578", "title": "Diffusion-Guided Knowledge Distillation for Weakly-Supervised Low-Light Semantic Segmentation", "authors": ["Chunyan Wang", "Dong Zhang", "Jinhui Tang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07578v1", "summary": "Weakly-supervised semantic segmentation aims to assign category labels to\neach pixel using weak annotations, significantly reducing manual annotation\ncosts. Although existing methods have achieved remarkable progress in well-lit\nscenarios, their performance significantly degrades in low-light environments\ndue to two fundamental limitations: severe image quality degradation (e.g., low\ncontrast, noise, and color distortion) and the inherent constraints of weak\nsupervision. These factors collectively lead to unreliable class activation\nmaps and semantically ambiguous pseudo-labels, ultimately compromising the\nmodel's ability to learn discriminative feature representations. To address\nthese problems, we propose Diffusion-Guided Knowledge Distillation for\nWeakly-Supervised Low-light Semantic Segmentation (DGKD-WLSS), a novel\nframework that synergistically combines Diffusion-Guided Knowledge Distillation\n(DGKD) with Depth-Guided Feature Fusion (DGF2). DGKD aligns normal-light and\nlow-light features via diffusion-based denoising and knowledge distillation,\nwhile DGF2 integrates depth maps as illumination-invariant geometric priors to\nenhance structural feature learning. Extensive experiments demonstrate the\neffectiveness of DGKD-WLSS, which achieves state-of-the-art performance in\nweakly supervised semantic segmentation tasks under low-light conditions. The\nsource codes have been released at:https://github.com/ChunyanWang1/DGKD-WLSS.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07578v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07331", "title": "mmFlux: Crowd Flow Analytics with Commodity mmWave MIMO Radar", "authors": ["Anurag Pallaprolu", "Winston Hurst", "Yasamin Mostofi"], "categories": ["eess.SP", "cs.CV"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07331v1", "summary": "In this paper, we present a novel framework for extracting underlying crowd\nmotion patterns and inferring crowd semantics using mmWave radar. First, our\nproposed signal processing pipeline combines optical flow estimation concepts\nfrom vision with novel statistical and morphological noise filtering to\ngenerate high-fidelity mmWave flow fields - compact 2D vector representations\nof crowd motion. We then introduce a novel approach that transforms these\nfields into directed geometric graphs, where edges capture dominant flow\ncurrents, vertices mark crowd splitting or merging, and flow distribution is\nquantified across edges. Finally, we show that by analyzing the local Jacobian\nand computing the corresponding curl and divergence, we can extract key crowd\nsemantics for both structured and diffused crowds. We conduct 21 experiments on\ncrowds of up to (and including) 20 people across 3 areas, using commodity\nmmWave radar. Our framework achieves high-fidelity graph reconstruction of the\nunderlying flow structure, even for complex crowd patterns, demonstrating\nstrong spatial alignment and precise quantitative characterization of flow\nsplit ratios. Finally, our curl and divergence analysis accurately infers key\ncrowd semantics, e.g., abrupt turns, boundaries where flow directions shift,\ndispersions, and gatherings. Overall, these findings validate our framework,\nunderscoring its potential for various crowd analytics applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07331v1", "cate": "eess.SP", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07586", "title": "Bayesian Discrete Diffusion Beats Autoregressive Perplexity", "authors": ["Cooper Doyle"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      12 pages, 2 figures, 2 tables", "url": "http://arxiv.org/abs/2507.07586v1", "summary": "We reveal a hidden Bayesian core of discrete-diffusion language models by\nshowing that the expected denoiser output under the forward masking\ndistribution recovers the exact posterior over clean tokens. Under minimal\nassumptions, Monte Carlo marginalization over K independent corruptions\nconverges to this posterior at rate O(1/sqrt(K)), yielding a simple proof of\nconsistency and finite-sample error bounds. Building on this insight, we\nintroduce a lightweight inference-time ensemble that averages K\nmask-and-denoise passes to obtain posterior-aware token probabilities and\nuncertainty estimates at no extra training cost. On WikiText-2, our method\nachieves test perplexity 8.8 with K=8, versus 20.3 for GPT-2 Small, despite\nusing a model of comparable size. Code is available at\nhttps://github.com/mercury0100/bayesradd.", "comment": "12 pages, 2 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2507.07586v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07637", "title": "HLF-FSL. A Decentralized Federated Split Learning Solution for IoT on Hyperledger Fabric", "authors": ["Carlos Beis Penedo", "Rebeca P. D√≠az Redondo", "Ana Fern√°ndez Vilas", "Manuel Fern√°ndez Veiga", "Francisco Troncoso Pastoriza"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      19 pages, 7 figures and 6 tables", "url": "http://arxiv.org/abs/2507.07637v1", "summary": "Collaborative machine learning in sensitive domains demands scalable, privacy\npreserving solutions for enterprise deployment. Conventional Federated Learning\n(FL) relies on a central server, introducing single points of failure and\nprivacy risks, while Split Learning (SL) partitions models for privacy but\nscales poorly due to sequential training. We present a decentralized\narchitecture that combines Federated Split Learning (FSL) with the permissioned\nblockchain Hyperledger Fabric (HLF). Our chaincode orchestrates FSL's split\nmodel execution and peer-to-peer aggregation without any central coordinator,\nleveraging HLF's transient fields and Private Data Collections (PDCs) to keep\nraw data and model activations private. On CIFAR-10 and MNIST benchmarks,\nHLF-FSL matches centralized FSL accuracy while reducing per epoch training time\ncompared to Ethereum-based works. Performance and scalability tests show\nminimal blockchain overhead and preserved accuracy, demonstrating enterprise\ngrade viability.", "comment": "19 pages, 7 figures and 6 tables", "pdf_url": "http://arxiv.org/pdf/2507.07637v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07585", "title": "HOTA: Hierarchical Overlap-Tiling Aggregation for Large-Area 3D Flood Mapping", "authors": ["Wenfeng Jia", "Bin Liang", "Yuxi Lu", "Attavit Wilaiwongsakul", "Muhammad Arif Khan", "Lihong Zheng"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07585v1", "summary": "Floods are among the most frequent natural hazards and cause significant\nsocial and economic damage. Timely, large-scale information on flood extent and\ndepth is essential for disaster response; however, existing products often\ntrade spatial detail for coverage or ignore flood depth altogether. To bridge\nthis gap, this work presents HOTA: Hierarchical Overlap-Tiling Aggregation, a\nplug-and-play, multi-scale inference strategy. When combined with SegFormer and\na dual-constraint depth estimation module, this approach forms a complete 3D\nflood-mapping pipeline. HOTA applies overlapping tiles of different sizes to\nmultispectral Sentinel-2 images only during inference, enabling the SegFormer\nmodel to capture both local features and kilometre-scale inundation without\nchanging the network weights or retraining. The subsequent depth module is\nbased on a digital elevation model (DEM) differencing method, which refines the\n2D mask and estimates flood depth by enforcing (i) zero depth along the flood\nboundary and (ii) near-constant flood volume with respect to the DEM. A case\nstudy on the March 2021 Kempsey (Australia) flood shows that HOTA, when coupled\nwith SegFormer, improves IoU from 73\\% (U-Net baseline) to 84\\%. The resulting\n3D surface achieves a mean absolute boundary error of less than 0.5 m. These\nresults demonstrate that HOTA can produce accurate, large-area 3D flood maps\nsuitable for rapid disaster response.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07585v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07474", "title": "Featureless Wireless Communications using Enhanced Autoencoder", "authors": ["Ruhui Zhang", "Wei Lin", "Binbin Chen"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07474v1", "summary": "Artificial intelligence (AI) techniques, particularly autoencoders (AEs),\nhave gained significant attention in wireless communication systems. This paper\ninvestigates using an AE to generate featureless signals with a low probability\nof detection and interception (LPD/LPI). Firstly, we introduce a novel loss\nfunction that adds a KL divergence term to the categorical cross entropy,\nenhancing the noise like characteristics of AE-generated signals while\npreserving block error rate (BLER). Secondly, to support long source message\nblocks for the AE's inputs, we replace one-hot inputs of source blocks with\nbinary inputs pre-encoded by conventional error correction coding schemes. The\nAE's outputs are then decoded back to the source blocks using the same scheme.\nThis design enables the AE to learn the coding structure, yielding superior\nBLER performance on coded blocks and the BLER of the source blocks is further\ndecreased by the error correction decoder. Moreover, we also validate the AE\nbased communication system in the over-the-air communication. Experimental\nresults demonstrate that our proposed methods improve the featureless\nproperties of AE signals and significantly reduce the BLER of message blocks,\nunderscoring the promise of our AE-based approach for secure and reliable\nwireless communication systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07474v1", "cate": "eess.SP", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07668", "title": "Learning Pole Structures of Hadronic States using Predictive Uncertainty Estimation", "authors": ["Felix Frohnert", "Denny Lane B. Sombrillo", "Evert van Nieuwenburg", "Patrick Emonts"], "categories": ["hep-ph", "cs.AI", "cs.LG", "hep-ex"], "primary_category": "Subjects:       High Energy Physics - Phenomenology (hep-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07668v1", "summary": "Matching theoretical predictions to experimental data remains a central\nchallenge in hadron spectroscopy. In particular, the identification of new\nhadronic states is difficult, as exotic signals near threshold can arise from a\nvariety of physical mechanisms. A key diagnostic in this context is the pole\nstructure of the scattering amplitude, but different configurations can produce\nsimilar signatures. The mapping between pole configurations and line shapes is\nespecially ambiguous near the mass threshold, where analytic control is\nlimited. In this work, we introduce an uncertainty-aware machine learning\napproach for classifying pole structures in $S$-matrix elements. Our method is\nbased on an ensemble of classifier chains that provide both epistemic and\naleatoric uncertainty estimates. We apply a rejection criterion based on\npredictive uncertainty, achieving a validation accuracy of nearly $95\\%$ while\ndiscarding only a small fraction of high-uncertainty predictions. Trained on\nsynthetic data with known pole structures, the model generalizes to previously\nunseen experimental data, including enhancements associated with the\n$P_{c\\bar{c}}(4312)^+$ state observed by LHCb. In this, we infer a four-pole\nstructure, representing the presence of a genuine compact pentaquark in the\npresence of a higher channel virtual state pole with non-vanishing width. While\nevaluated on this particular state, our framework is broadly applicable to\nother candidate hadronic states and offers a scalable tool for pole structure\ninference in scattering amplitudes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07668v1", "cate": "hep-ph", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07675", "title": "Some Theoretical Results on Layerwise Effective Dimension Oscillations in Finite Width ReLU Networks", "authors": ["Darshan Makwana"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07675v1", "summary": "We analyze the layerwise effective dimension (rank of the feature matrix) in\nfully-connected ReLU networks of finite width. Specifically, for a fixed batch\nof $m$ inputs and random Gaussian weights, we derive closed-form expressions\nfor the expected rank of the \\$m\\times n\\$ hidden activation matrices. Our main\nresult shows that $\\mathbb{E}[EDim(\\ell)]=m[1-(1-2/\\pi)^\\ell]+O(e^{-c m})$ so\nthat the rank deficit decays geometrically with ratio $1-2 / \\pi \\approx\n0.3634$. We also prove a sub-Gaussian concentration bound, and identify the\n\"revival\" depths at which the expected rank attains local maxima. In\nparticular, these peaks occur at depths\n$\\ell_k^*\\approx(k+1/2)\\pi/\\log(1/\\rho)$ with height $\\approx (1-e^{-\\pi/2}) m\n\\approx 0.79m$. We further show that this oscillatory rank behavior is a\nfinite-width phenomenon: under orthogonal weight initialization or strong\nnegative-slope leaky-ReLU, the rank remains (nearly) full. These results\nprovide a precise characterization of how random ReLU layers alternately\ncollapse and partially revive the subspace of input variations, adding nuance\nto prior work on expressivity of deep networks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07675v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07591", "title": "Stable-Hair v2: Real-World Hair Transfer via Multiple-View Diffusion Model", "authors": ["Kuiyuan Sun", "Yuxuan Zhang", "Jichao Zhang", "Jiaming Liu", "Wei Wang", "Niculae Sebe", "Yao Zhao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      14 pages", "url": "http://arxiv.org/abs/2507.07591v1", "summary": "While diffusion-based methods have shown impressive capabilities in capturing\ndiverse and complex hairstyles, their ability to generate consistent and\nhigh-quality multi-view outputs -- crucial for real-world applications such as\ndigital humans and virtual avatars -- remains underexplored. In this paper, we\npropose Stable-Hair v2, a novel diffusion-based multi-view hair transfer\nframework. To the best of our knowledge, this is the first work to leverage\nmulti-view diffusion models for robust, high-fidelity, and view-consistent hair\ntransfer across multiple perspectives. We introduce a comprehensive multi-view\ntraining data generation pipeline comprising a diffusion-based Bald Converter,\na data-augment inpainting model, and a face-finetuned multi-view diffusion\nmodel to generate high-quality triplet data, including bald images, reference\nhairstyles, and view-aligned source-bald pairs. Our multi-view hair transfer\nmodel integrates polar-azimuth embeddings for pose conditioning and temporal\nattention layers to ensure smooth transitions between views. To optimize this\nmodel, we design a novel multi-stage training strategy consisting of\npose-controllable latent IdentityNet training, hair extractor training, and\ntemporal attention training. Extensive experiments demonstrate that our method\naccurately transfers detailed and realistic hairstyles to source subjects while\nachieving seamless and consistent results across views, significantly\noutperforming existing methods and establishing a new benchmark in multi-view\nhair transfer. Code is publicly available at\nhttps://github.com/sunkymepro/StableHairV2.", "comment": "14 pages", "pdf_url": "http://arxiv.org/pdf/2507.07591v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07567", "title": "Leveraging Power Amplifier Distortion for Physical Layer Security", "authors": ["Reza Ghasemi Alavicheh", "Thomas Feys", "MD Arifur Rahman", "Fran√ßois Rottenberg"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07567v1", "summary": "This paper introduces a new approach to physical layer security (PLS) by\nleveraging power amplifier (PA) nonlinear distortion through distortion-aware\nprecoding. While some conventional PLS techniques inject artificial noise\northogonal to legitimate channels, we demonstrate that inherent PA\nnonlinearities typically considered undesirable can be exploited to enhance\nsecurity. The zero 3rd order (Z3RO) precoder applies a negative polarity to\nseveral antennas to cancel the PA distortion at the user location, resulting in\ndistortion being transmitted in non-user locations. Redirecting the distortion\nto non-user locations creates interference for potential eavesdroppers,\nlowering their signal-to-noise-and-distortion ratio (SNDR). Numerical\nsimulations reveal that the Z3RO precoder achieves up to a $2.5\\times$\nimprovement in secrecy rate compared to conventional maximum ratio transmission\n(MRT) precoding under a $10\\%$ outage probability, SNR of $32$ dB and $-5$ dB\ninput back-off (IBO) where the PAs enter the saturation regime.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07567v1", "cate": "eess.SP", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07685", "title": "Rationale-Enhanced Decoding for Multi-modal Chain-of-Thought", "authors": ["Shin'ya Yamaguchi", "Kosuke Nishida", "Daiki Chijiwa"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      17 pages, 4 figures", "url": "http://arxiv.org/abs/2507.07685v1", "summary": "Large vision-language models (LVLMs) have demonstrated remarkable\ncapabilities by integrating pre-trained vision encoders with large language\nmodels (LLMs). Similar to single-modal LLMs, chain-of-thought (CoT) prompting\nhas been adapted for LVLMs to enhance multi-modal reasoning by generating\nintermediate rationales based on visual and textual inputs. While CoT is\nassumed to improve grounding and accuracy in LVLMs, our experiments reveal a\nkey challenge: existing LVLMs often ignore the contents of generated rationales\nin CoT reasoning. To address this, we re-formulate multi-modal CoT reasoning as\na KL-constrained reward maximization focused on rationale-conditional\nlog-likelihood. As the optimal solution, we propose rationale-enhanced decoding\n(RED), a novel plug-and-play inference-time decoding strategy. RED harmonizes\nvisual and rationale information by multiplying distinct image-conditional and\nrationale-conditional next token distributions. Extensive experiments show that\nRED consistently and significantly improves reasoning over standard CoT and\nother decoding methods across multiple benchmarks and LVLMs. Our work offers a\npractical and effective approach to improve both the faithfulness and accuracy\nof CoT reasoning in LVLMs, paving the way for more reliable rationale-grounded\nmulti-modal systems.", "comment": "17 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.07685v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07712", "title": "Balancing the Past and Present: A Coordinated Replay Framework for Federated Class-Incremental Learning", "authors": ["Zhuang Qi", "Lei Meng", "Han Yu"], "categories": ["cs.LG", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07712v1", "summary": "Federated Class Incremental Learning (FCIL) aims to collaboratively process\ncontinuously increasing incoming tasks across multiple clients. Among various\napproaches, data replay has become a promising solution, which can alleviate\nforgetting by reintroducing representative samples from previous tasks.\nHowever, their performance is typically limited by class imbalance, both within\nthe replay buffer due to limited global awareness and between replayed and\nnewly arrived classes. To address this issue, we propose a class wise balancing\ndata replay method for FCIL (FedCBDR), which employs a global coordination\nmechanism for class-level memory construction and reweights the learning\nobjective to alleviate the aforementioned imbalances. Specifically, FedCBDR has\ntwo key components: 1) the global-perspective data replay module reconstructs\nglobal representations of prior task in a privacy-preserving manner, which then\nguides a class-aware and importance-sensitive sampling strategy to achieve\nbalanced replay; 2) Subsequently, to handle class imbalance across tasks, the\ntask aware temperature scaling module adaptively adjusts the temperature of\nlogits at both class and instance levels based on task dynamics, which reduces\nthe model's overconfidence in majority classes while enhancing its sensitivity\nto minority classes. Experimental results verified that FedCBDR achieves\nbalanced class-wise sampling under heterogeneous data distributions and\nimproves generalization under task imbalance between earlier and recent tasks,\nyielding a 2%-15% Top-1 accuracy improvement over six state-of-the-art methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07712v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07603", "title": "HiM2SAM: Enhancing SAM2 with Hierarchical Motion Estimation and Memory Optimization towards Long-term Tracking", "authors": ["Ruixiang Chen", "Guolei Sun", "Yawei Li", "Jie Qin", "Luca Benini"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07603v1", "summary": "This paper presents enhancements to the SAM2 framework for video object\ntracking task, addressing challenges such as occlusions, background clutter,\nand target reappearance. We introduce a hierarchical motion estimation\nstrategy, combining lightweight linear prediction with selective non-linear\nrefinement to improve tracking accuracy without requiring additional training.\nIn addition, we optimize the memory bank by distinguishing long-term and\nshort-term memory frames, enabling more reliable tracking under long-term\nocclusions and appearance changes. Experimental results show consistent\nimprovements across different model scales. Our method achieves\nstate-of-the-art performance on LaSOT and LaSOText with the large model,\nachieving 9.6% and 7.2% relative improvements in AUC over the original SAM2,\nand demonstrates even larger relative gains on smaller models, highlighting the\neffectiveness of our trainless, low-overhead improvements for boosting\nlong-term tracking performance. The code is available at\nhttps://github.com/LouisFinner/HiM2SAM.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07603v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07643", "title": "RIS-assisted ISAC Systems for Industrial Revolution 6.0: Exploring the Near-field and Far-field Coexistence", "authors": ["Seonghoon Yoo", "Jaemin Jung", "Seongah Jeong", "Jinkyu Kang", "Markku Juntti", "Joonhyuk Kang"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07643v1", "summary": "The Industrial Internet of Things (IIoT) has emerged as a key technology for\nrealizing the vision of Industry 6.0, requiring the seamless integration of\ndiverse connected devices. In particular, integrated sensing and communication\n(ISAC) plays a critical role in supporting real-time control and automation\nwithin IIoT systems. In this paper, we explore reconfigurable intelligent\nsurface (RIS)-assisted ISAC systems for IIoT in the coexistence of near-field\nand far-field regions. The system consists of a full-duplex access point (AP),\na RIS and multiple IIoT devices, where the near-field devices simultaneously\nperform sensing and communication, while the far-field devices rely on a\nRIS-assisted communication. To enhance spectral efficiency for both sensing and\ncommunication functionalities, we consider the use of both traditional\nsensing-only (SO) and ISAC frequency bands. Moreover, uplink non-orthogonal\nmultiple access (NOMA) is employed to facilitate the sequential decoding of\nsuperimposed communication and sensing signals from IIoT devices. To maximize\nsensing accuracy in terms of Cram${\\Grave{\\textrm{e}}}$r-Rao bound (CRB), we\nformulate a joint optimization of RIS phase shift, bandwidth splitting ratio\nand receive beamforming vector subject to the minimum data rate requirements of\nIIoT devices and resource budget constraints. The algorithmic solution is\ndeveloped via the successive convex approximation (SCA)-based alternating\noptimization (AO) method with the semi-definite relaxation (SDR) technique.\nNumerical results demonstrate that the proposed method significantly\noutperforms conventional methods relying solely on either ISAC or SO band by\nachieving superior performance across RIS and device configurations, while\nensuring robust ISAC performance under the near-field and far-field coexistence\nscenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07643v1", "cate": "eess.SP", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07131", "title": "Wrist bone segmentation in X-ray images using CT-based simulations", "authors": ["Youssef ElTantawy", "Alexia Karantana", "Xin Chen"], "categories": ["eess.IV", "cs.CV", "q-bio.TO"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      4 pages", "url": "http://arxiv.org/abs/2507.07131v1", "summary": "Plain X-ray is one of the most common image modalities for clinical diagnosis\n(e.g. bone fracture, pneumonia, cancer screening, etc.). X-ray image\nsegmentation is an essential step for many computer-aided diagnostic systems,\nyet it remains challenging. Deep-learning-based methods have achieved superior\nperformance in medical image segmentation tasks but often require a large\namount of high-quality annotated data for model training. Providing such an\nannotated dataset is not only time-consuming but also requires a high level of\nexpertise. This is particularly challenging in wrist bone segmentation in\nX-rays, due to the interposition of multiple small carpal bones in the image.\nTo overcome the data annotation issue, this work utilizes a large number of\nsimulated X-ray images generated from Computed Tomography (CT) volumes with\ntheir corresponding 10 bone labels to train a deep learning-based model for\nwrist bone segmentation in real X-ray images. The proposed method was evaluated\nusing both simulated images and real images. The method achieved Dice scores\nranging from 0.80 to 0.92 for the simulated dataset generated from different\nview angles. Qualitative analysis of the segmentation results of the real X-ray\nimages also demonstrated the superior performance of the trained model. The\ntrained model and X-ray simulation code are freely available for research\npurposes: the link will be provided upon acceptance.", "comment": "4 pages", "pdf_url": "http://arxiv.org/pdf/2507.07131v1", "cate": "eess.IV", "date": "2025-07-08", "updated": "2025-07-08"}
{"id": "2507.07695", "title": "KeyKnowledgeRAG (K^2RAG): An Enhanced RAG method for improved LLM question-answering capabilities", "authors": ["Hruday Markondapatnaikuni", "Basem Suleiman", "Abdelkarim Erradi", "Shijing Chen"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      21 pages, 14 figures", "url": "http://arxiv.org/abs/2507.07695v1", "summary": "Fine-tuning is an immensely resource-intensive process when retraining Large\nLanguage Models (LLMs) to incorporate a larger body of knowledge. Although many\nfine-tuning techniques have been developed to reduce the time and computational\ncost involved, the challenge persists as LLMs continue to grow in size and\ncomplexity. To address this, a new approach to knowledge expansion in LLMs is\nneeded. Retrieval-Augmented Generation (RAG) offers one such alternative by\nstoring external knowledge in a database and retrieving relevant chunks to\nsupport question answering. However, naive implementations of RAG face\nsignificant limitations in scalability and answer accuracy. This paper\nintroduces KeyKnowledgeRAG (K2RAG), a novel framework designed to overcome\nthese limitations. Inspired by the divide-and-conquer paradigm, K2RAG\nintegrates dense and sparse vector search, knowledge graphs, and text\nsummarization to improve retrieval quality and system efficiency. The framework\nalso includes a preprocessing step that summarizes the training data,\nsignificantly reducing the training time. K2RAG was evaluated using the\nMultiHopRAG dataset, where the proposed pipeline was trained on the document\ncorpus and tested on a separate evaluation set. Results demonstrated notable\nimprovements over common naive RAG implementations. K2RAG achieved the highest\nmean answer similarity score of 0.57, and reached the highest third quartile\n(Q3) similarity of 0.82, indicating better alignment with ground-truth answers.\nIn addition to improved accuracy, the framework proved highly efficient. The\nsummarization step reduced the average training time of individual components\nby 93%, and execution speed was up to 40% faster than traditional knowledge\ngraph-based RAG systems. K2RAG also demonstrated superior scalability,\nrequiring three times less VRAM than several naive RAG implementations tested\nin this study.", "comment": "21 pages, 14 figures", "pdf_url": "http://arxiv.org/pdf/2507.07695v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07738", "title": "Efficient and Scalable Estimation of Distributional Treatment Effects with Multi-Task Neural Networks", "authors": ["Tomu Hirata", "Undral Byambadalai", "Tatsushi Oka", "Shota Yasui", "Shingo Uto"], "categories": ["cs.LG", "econ.EM"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07738v1", "summary": "We propose a novel multi-task neural network approach for estimating\ndistributional treatment effects (DTE) in randomized experiments. While DTE\nprovides more granular insights into the experiment outcomes over conventional\nmethods focusing on the Average Treatment Effect (ATE), estimating it with\nregression adjustment methods presents significant challenges. Specifically,\nprecision in the distribution tails suffers due to data imbalance, and\ncomputational inefficiencies arise from the need to solve numerous regression\nproblems, particularly in large-scale datasets commonly encountered in\nindustry. To address these limitations, our method leverages multi-task neural\nnetworks to estimate conditional outcome distributions while incorporating\nmonotonic shape constraints and multi-threshold label learning to enhance\naccuracy. To demonstrate the practical effectiveness of our proposed method, we\napply our method to both simulated and real-world datasets, including a\nrandomized field experiment aimed at reducing water consumption in the US and a\nlarge-scale A/B test from a leading streaming platform in Japan. The\nexperimental results consistently demonstrate superior performance across\nvarious datasets, establishing our method as a robust and practical solution\nfor modern causal inference applications requiring a detailed understanding of\ntreatment effect heterogeneity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07738v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07605", "title": "LOSC: LiDAR Open-voc Segmentation Consolidator", "authors": ["Nermin Samet", "Gilles Puy", "Renaud Marlet"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07605v1", "summary": "We study the use of image-based Vision-Language Models (VLMs) for\nopen-vocabulary segmentation of lidar scans in driving settings. Classically,\nimage semantics can be back-projected onto 3D point clouds. Yet, resulting\npoint labels are noisy and sparse. We consolidate these labels to enforce both\nspatio-temporal consistency and robustness to image-level augmentations. We\nthen train a 3D network based on these refined labels. This simple method,\ncalled LOSC, outperforms the SOTA of zero-shot open-vocabulary semantic and\npanoptic segmentation on both nuScenes and SemanticKITTI, with significant\nmargins.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07605v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07692", "title": "Signal Prediction for Loss Mitigation in Tactile Internet: A Leader-Follower Game-Theoretic Approach", "authors": ["Mohammad Ali Vahedifar", "Qi Zhang"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      This work has been accepted for publication in the IEEE Machine Learning and Signal Processing Conference (MLSP 2025)", "url": "http://arxiv.org/abs/2507.07692v1", "summary": "Tactile Internet (TI) requires achieving ultra-low latency and highly\nreliable packet delivery for haptic signals. In the presence of packet loss and\ndelay, the signal prediction method provides a viable solution for recovering\nthe missing signals. To this end, we introduce the Leader-Follower (LeFo)\napproach based on a cooperative Stackelberg game, which enables both users and\nrobots to learn and predict actions. With accurate prediction, the\nteleoperation system can safely relax its strict delay requirements. Our method\nachieves high prediction accuracy, ranging from 80.62% to 95.03% for remote\nrobot signals at the Human ($H$) side and from 70.44% to 89.77% for human\noperation signals at the remote Robot ($R$) side. We also establish an upper\nbound for maximum signal loss using Taylor Expansion, ensuring robustness.", "comment": "This work has been accepted for publication in the IEEE Machine\n  Learning and Signal Processing Conference (MLSP 2025)", "pdf_url": "http://arxiv.org/pdf/2507.07692v1", "cate": "eess.SP", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07254", "title": "Label-Efficient Chest X-ray Diagnosis via Partial CLIP Adaptation", "authors": ["Heet Nitinkumar Dalsania"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07254v1", "summary": "Modern deep learning implementations for medical imaging usually rely on\nlarge labeled datasets. These datasets are often difficult to obtain due to\nprivacy concerns, high costs, and even scarcity of cases. In this paper, a\nlabel-efficient strategy is proposed for chest X-ray diagnosis that seeks to\nreflect real-world hospital scenarios. The experiments use the NIH Chest\nX-ray14 dataset and a pre-trained CLIP ViT-B/32 model. The model is adapted via\npartial fine-tuning of its visual encoder and then evaluated using zero-shot\nand few-shot learning with 1-16 labeled examples per disease class. The tests\ndemonstrate that CLIP's pre-trained vision-language features can be effectively\nadapted to few-shot medical imaging tasks, achieving over 20\\% improvement in\nmean AUC score as compared to the zero-shot baseline. The key aspect of this\nwork is to attempt to simulate internal hospital workflows, where image\narchives exist but annotations are sparse. This work evaluates a practical and\nscalable solution for both common and rare disease diagnosis. Additionally this\nresearch is intended for academic and experimental purposes only and has not\nbeen peer reviewed yet. All code is found at\nhttps://github.com/heet007-code/CLIP-disease-xray.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07254v1", "cate": "eess.IV", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07725", "title": "Not All Preferences are What You Need for Post-Training: Selective Alignment Strategy for Preference Optimization", "authors": ["Zhijin Dong"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07725v1", "summary": "Post-training alignment of large language models (LLMs) is a critical\nchallenge, as not all tokens contribute equally to model performance. This\npaper introduces a selective alignment strategy that prioritizes high-impact\ntokens within preference pairs, leveraging token-level log-probability\ndifferences between the current policy and a reference model. By focusing on\nthese informative tokens, our approach reduces computational overhead and\nenhances alignment fidelity. We further explore the role of reference model\nquality, demonstrating that stronger reference models significantly improve\ntoken selection accuracy and overall optimization effectiveness. Comprehensive\nexperiments on benchmarks such as Arena-Hard and MT-Bench validate the\nsuperiority of our Selective-DPO method over standard DPO and\ndistillation-based baselines. Our findings highlight the importance of\ntoken-level optimization and reference model selection in advancing preference\nalignment for LLMs. The code is available at\nhttps://github.com/Dongzhijin/SDPO.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07725v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07754", "title": "OPC: One-Point-Contraction Unlearning Toward Deep Feature Forgetting", "authors": ["Jaeheun Jung", "Bosung Jung", "Suhyun Bae", "Donghun Lee"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07754v1", "summary": "Machine unlearning seeks to remove the influence of particular data or class\nfrom trained models to meet privacy, legal, or ethical requirements. Existing\nunlearning methods tend to forget shallowly: phenomenon of an unlearned model\npretend to forget by adjusting only the model response, while its internal\nrepresentations retain information sufficiently to restore the forgotten data\nor behavior. We empirically confirm the widespread shallowness by reverting the\nforgetting effect of various unlearning methods via training-free performance\nrecovery attack and gradient-inversion-based data reconstruction attack. To\naddress this vulnerability fundamentally, we define a theoretical criterion of\n``deep forgetting'' based on one-point-contraction of feature representations\nof data to forget. We also propose an efficient approximation algorithm, and\nuse it to construct a novel general-purpose unlearning algorithm:\nOne-Point-Contraction (OPC). Empirical evaluations on image classification\nunlearning benchmarks show that OPC achieves not only effective unlearning\nperformance but also superior resilience against both performance recovery\nattack and gradient-inversion attack. The distinctive unlearning performance of\nOPC arises from the deep feature forgetting enforced by its theoretical\nfoundation, and recaps the need for improved robustness of machine unlearning\nmethods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07754v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07620", "title": "ViLU: Learning Vision-Language Uncertainties for Failure Prediction", "authors": ["Marc Lafon", "Yannis Karmim", "Julio Silva-Rodriguez", "Paul Couairon", "Cl√©ment Rambour", "Rapha√´l Fournier-Sniehotta", "Ismail Ben Ayed", "Jose Dolz", "Nicolas Thome"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07620v1", "summary": "Reliable Uncertainty Quantification (UQ) and failure prediction remain open\nchallenges for Vision-Language Models (VLMs). We introduce ViLU, a new\nVision-Language Uncertainty quantification framework that contextualizes\nuncertainty estimates by leveraging all task-relevant textual representations.\nViLU constructs an uncertainty-aware multi-modal representation by integrating\nthe visual embedding, the predicted textual embedding, and an image-conditioned\ntextual representation via cross-attention. Unlike traditional UQ methods based\non loss prediction, ViLU trains an uncertainty predictor as a binary classifier\nto distinguish correct from incorrect predictions using a weighted binary\ncross-entropy loss, making it loss-agnostic. In particular, our proposed\napproach is well-suited for post-hoc settings, where only vision and text\nembeddings are available without direct access to the model itself. Extensive\nexperiments on diverse datasets show the significant gains of our method\ncompared to state-of-the-art failure prediction methods. We apply our method to\nstandard classification datasets, such as ImageNet-1k, as well as large-scale\nimage-caption datasets like CC12M and LAION-400M. Ablation studies highlight\nthe critical role of our architecture and training in achieving effective\nuncertainty quantification. Our code is publicly available and can be found\nhere: https://github.com/ykrmm/ViLU.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07620v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07832", "title": "Flying Base Stations for Offshore Wind Farm Monitoring and Control: Holistic Performance Evaluation and Optimization", "authors": ["Xinyi Lin", "Peizheng Li", "Adnan Aijaz"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      Accepted by PIMRC 2025", "url": "http://arxiv.org/abs/2507.07832v1", "summary": "Ensuring reliable and low-latency communication in offshore wind farms is\ncritical for efficient monitoring and control, yet remains challenging due to\nthe harsh environment and lack of infrastructure. This paper investigates a\nflying base station (FBS) approach for wide-area monitoring and control in the\nUK Hornsea offshore wind farm project. By leveraging mobile, flexible FBS\nplatforms in the remote and harsh offshore environment, the proposed system\noffers real-time connectivity for turbines without the need for deploying\npermanent infrastructure at the sea. We develop a detailed and practical\nend-to-end latency model accounting for five key factors: flight duration,\nconnection establishment, turbine state information upload, computational\ndelay, and control transmission, to provide a holistic perspective often\nmissing in prior studies. Furthermore, we combine trajectory planning,\nbeamforming, and resource allocation into a multi-objective optimization\nframework for the overall latency minimization, specifically designed for\nlarge-scale offshore wind farm deployments. Simulation results verify the\neffectiveness of our proposed method in minimizing latency and enhancing\nefficiency in FBS-assisted offshore monitoring across various power levels,\nwhile consistently outperforming baseline designs.", "comment": "Accepted by PIMRC 2025", "pdf_url": "http://arxiv.org/pdf/2507.07832v1", "cate": "eess.SP", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07422", "title": "Computation-resource-efficient Task-oriented Communications", "authors": ["Jingwen Fu", "Ming Xiao", "Chao Ren", "Mikael Skoglund"], "categories": ["eess.IV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07422v1", "summary": "The rapid development of deep-learning enabled task-oriented communications\n(TOC) significantly shifts the paradigm of wireless communications. However,\nthe high computation demands, particularly in resource-constrained systems\ne.g., mobile phones and UAVs, make TOC challenging for many tasks. To address\nthe problem, we propose a novel TOC method with two models: a static and a\ndynamic model. In the static model, we apply a neural network (NN) as a\ntask-oriented encoder (TOE) when there is no computation budget constraint. The\ndynamic model is used when device computation resources are limited, and it\nuses dynamic NNs with multiple exits as the TOE. The dynamic model sorts input\ndata by complexity with thresholds, allowing the efficient allocation of\ncomputation resources. Furthermore, we analyze the convergence of the proposed\nTOC methods and show that the model converges at rate\n$O\\left(\\frac{1}{\\sqrt{T}}\\right)$ with an epoch of length $T$. Experimental\nresults demonstrate that the static model outperforms baseline models in terms\nof transmitted dimensions, floating-point operations (FLOPs), and accuracy\nsimultaneously. The dynamic model can further improve accuracy and\ncomputational demand, providing an improved solution for resource-constrained\nsystems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07422v1", "cate": "eess.IV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07748", "title": "When Large Language Models Meet Law: Dual-Lens Taxonomy, Technical Advances, and Ethical Governance", "authors": ["Peizhang Shao", "Linrui Xu", "Jinxi Wang", "Wei Zhou", "Xingyu Wu"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07748v1", "summary": "This paper establishes the first comprehensive review of Large Language\nModels (LLMs) applied within the legal domain. It pioneers an innovative dual\nlens taxonomy that integrates legal reasoning frameworks and professional\nontologies to systematically unify historical research and contemporary\nbreakthroughs. Transformer-based LLMs, which exhibit emergent capabilities such\nas contextual reasoning and generative argumentation, surmount traditional\nlimitations by dynamically capturing legal semantics and unifying evidence\nreasoning. Significant progress is documented in task generalization, reasoning\nformalization, workflow integration, and addressing core challenges in text\nprocessing, knowledge integration, and evaluation rigor via technical\ninnovations like sparse attention mechanisms and mixture-of-experts\narchitectures. However, widespread adoption of LLM introduces critical\nchallenges: hallucination, explainability deficits, jurisdictional adaptation\ndifficulties, and ethical asymmetry. This review proposes a novel taxonomy that\nmaps legal roles to NLP subtasks and computationally implements the Toulmin\nargumentation framework, thus systematizing advances in reasoning, retrieval,\nprediction, and dispute resolution. It identifies key frontiers including\nlow-resource systems, multimodal evidence integration, and dynamic rebuttal\nhandling. Ultimately, this work provides both a technical roadmap for\nresearchers and a conceptual framework for practitioners navigating the\nalgorithmic future, laying a robust foundation for the next era of legal\nartificial intelligence. We have created a GitHub repository to index the\nrelevant papers: https://github.com/Kilimajaro/LLMs_Meet_Law.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07748v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07768", "title": "TRIX- Trading Adversarial Fairness via Mixed Adversarial Training", "authors": ["Tejaswini Medi", "Steffen Jung", "Margret Keuper"], "categories": ["cs.LG", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07768v1", "summary": "Adversarial Training (AT) is a widely adopted defense against adversarial\nexamples. However, existing approaches typically apply a uniform training\nobjective across all classes, overlooking disparities in class-wise\nvulnerability. This results in adversarial unfairness: classes with well\ndistinguishable features (strong classes) tend to become more robust, while\nclasses with overlapping or shared features(weak classes) remain\ndisproportionately susceptible to adversarial attacks. We observe that strong\nclasses do not require strong adversaries during training, as their non-robust\nfeatures are quickly suppressed. In contrast, weak classes benefit from\nstronger adversaries to effectively reduce their vulnerabilities. Motivated by\nthis, we introduce TRIX, a feature-aware adversarial training framework that\nadaptively assigns weaker targeted adversaries to strong classes, promoting\nfeature diversity via uniformly sampled targets, and stronger untargeted\nadversaries to weak classes, enhancing their focused robustness. TRIX further\nincorporates per-class loss weighting and perturbation strength adjustments,\nbuilding on prior work, to emphasize weak classes during the optimization.\nComprehensive experiments on standard image classification benchmarks,\nincluding evaluations under strong attacks such as PGD and AutoAttack,\ndemonstrate that TRIX significantly improves worst-case class accuracy on both\nclean and adversarial data, reducing inter-class robustness disparities, and\npreserves overall accuracy. Our results highlight TRIX as a practical step\ntoward fair and effective adversarial defense.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07768v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07633", "title": "T-GVC: Trajectory-Guided Generative Video Coding at Ultra-Low Bitrates", "authors": ["Zhitao Wang", "Hengyu Man", "Wenrui Li", "Xingtao Wang", "Xiaopeng Fan", "Debin Zhao"], "categories": ["cs.CV", "cs.MM"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07633v1", "summary": "Recent advances in video generation techniques have given rise to an emerging\nparadigm of generative video coding, aiming to achieve semantically accurate\nreconstructions in Ultra-Low Bitrate (ULB) scenarios by leveraging strong\ngenerative priors. However, most existing methods are limited by domain\nspecificity (e.g., facial or human videos) or an excessive dependence on\nhigh-level text guidance, which often fails to capture motion details and\nresults in unrealistic reconstructions. To address these challenges, we propose\na Trajectory-Guided Generative Video Coding framework (dubbed T-GVC). T-GVC\nemploys a semantic-aware sparse motion sampling pipeline to effectively bridge\nlow-level motion tracking with high-level semantic understanding by extracting\npixel-wise motion as sparse trajectory points based on their semantic\nimportance, not only significantly reducing the bitrate but also preserving\ncritical temporal semantic information. In addition, by incorporating\ntrajectory-aligned loss constraints into diffusion processes, we introduce a\ntraining-free latent space guidance mechanism to ensure physically plausible\nmotion patterns without sacrificing the inherent capabilities of generative\nmodels. Experimental results demonstrate that our framework outperforms both\ntraditional codecs and state-of-the-art end-to-end video compression methods\nunder ULB conditions. Furthermore, additional experiments confirm that our\napproach achieves more precise motion control than existing text-guided\nmethods, paving the way for a novel direction of generative video coding guided\nby geometric motion modeling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07633v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.05683", "title": "Polyadic encryption", "authors": ["Steven Duplij", "Qiang Guo"], "categories": ["cs.CR", "cs.IT", "eess.SP", "math-ph", "math.IT", "math.MP", "math.RA"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      revtex 4.2, 9 pages", "url": "http://arxiv.org/abs/2507.05683v1", "summary": "A novel original procedure of encryption/decryption based on the polyadic\nalgebraic structures and on signal processing methods is proposed. First, we\nuse signals with integer amplitudes to send information. Then we use polyadic\ntechniques to transfer the plaintext into series of special integers. The\nreceiver restores the plaintext using special rules and systems of equations.", "comment": "revtex 4.2, 9 pages", "pdf_url": "http://arxiv.org/pdf/2507.05683v1", "cate": "cs.CR", "date": "2025-07-08", "updated": "2025-07-08"}
{"id": "2507.07903", "title": "Hardware-Aware Feature Extraction Quantisation for Real-Time Visual Odometry on FPGA Platforms", "authors": ["Mateusz Wasala", "Mateusz Smolarczyk", "Michal Danilowicz", "Tomasz Kryjak"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted for the DSD 2025 conference in Salerno, Italy", "url": "http://arxiv.org/abs/2507.07903v1", "summary": "Accurate position estimation is essential for modern navigation systems\ndeployed in autonomous platforms, including ground vehicles, marine vessels,\nand aerial drones. In this context, Visual Simultaneous Localisation and\nMapping (VSLAM) - which includes Visual Odometry - relies heavily on the\nreliable extraction of salient feature points from the visual input data. In\nthis work, we propose an embedded implementation of an unsupervised\narchitecture capable of detecting and describing feature points. It is based on\na quantised SuperPoint convolutional neural network. Our objective is to\nminimise the computational demands of the model while preserving high detection\nquality, thus facilitating efficient deployment on platforms with limited\nresources, such as mobile or embedded systems. We implemented the solution on\nan FPGA System-on-Chip (SoC) platform, specifically the AMD/Xilinx Zynq\nUltraScale+, where we evaluated the performance of Deep Learning Processing\nUnits (DPUs) and we also used the Brevitas library and the FINN framework to\nperform model quantisation and hardware-aware optimisation. This allowed us to\nprocess 640 x 480 pixel images at up to 54 fps on an FPGA platform,\noutperforming state-of-the-art solutions in the field. We conducted experiments\non the TUM dataset to demonstrate and discuss the impact of different\nquantisation techniques on the accuracy and performance of the model in a\nvisual odometry task.", "comment": "Accepted for the DSD 2025 conference in Salerno, Italy", "pdf_url": "http://arxiv.org/pdf/2507.07903v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07778", "title": "Synchronizing Task Behavior: Aligning Multiple Tasks during Test-Time Training", "authors": ["Wooseong Jeong", "Jegyeong Cho", "Youngho Yoon", "Kuk-Jin Yoon"], "categories": ["cs.LG", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted at ICCV 2025", "url": "http://arxiv.org/abs/2507.07778v1", "summary": "Generalizing neural networks to unseen target domains is a significant\nchallenge in real-world deployments. Test-time training (TTT) addresses this by\nusing an auxiliary self-supervised task to reduce the domain gap caused by\ndistribution shifts between the source and target. However, we find that when\nmodels are required to perform multiple tasks under domain shifts, conventional\nTTT methods suffer from unsynchronized task behavior, where the adaptation\nsteps needed for optimal performance in one task may not align with the\nrequirements of other tasks. To address this, we propose a novel TTT approach\ncalled Synchronizing Tasks for Test-time Training (S4T), which enables the\nconcurrent handling of multiple tasks. The core idea behind S4T is that\npredicting task relations across domain shifts is key to synchronizing tasks\nduring test time. To validate our approach, we apply S4T to conventional\nmulti-task benchmarks, integrating it with traditional TTT protocols. Our\nempirical results show that S4T outperforms state-of-the-art TTT methods across\nvarious benchmarks.", "comment": "Accepted at ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.07778v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07769", "title": "BEAVER: Building Environments with Assessable Variation for Evaluating Multi-Objective Reinforcement Learning", "authors": ["Ruohong Liu", "Jack Umenberger", "Yize Chen"], "categories": ["cs.LG", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted at the Workshop on Computational Optimization of Buildings (ICML CO-BUILD), 42nd International Conference on Machine Learning (ICML 2025), Vancouver, Canada", "url": "http://arxiv.org/abs/2507.07769v1", "summary": "Recent years have seen significant advancements in designing reinforcement\nlearning (RL)-based agents for building energy management. While individual\nsuccess is observed in simulated or controlled environments, the scalability of\nRL approaches in terms of efficiency and generalization across building\ndynamics and operational scenarios remains an open question. In this work, we\nformally characterize the generalization space for the cross-environment,\nmulti-objective building energy management task, and formulate the\nmulti-objective contextual RL problem. Such a formulation helps understand the\nchallenges of transferring learned policies across varied operational contexts\nsuch as climate and heat convection dynamics under multiple control objectives\nsuch as comfort level and energy consumption. We provide a principled way to\nparameterize such contextual information in realistic building RL environments,\nand construct a novel benchmark to facilitate the evaluation of generalizable\nRL algorithms in practical building control tasks. Our results show that\nexisting multi-objective RL methods are capable of achieving reasonable\ntrade-offs between conflicting objectives. However, their performance degrades\nunder certain environment variations, underscoring the importance of\nincorporating dynamics-dependent contextual information into the policy\nlearning process.", "comment": "Accepted at the Workshop on Computational Optimization of Buildings\n  (ICML CO-BUILD), 42nd International Conference on Machine Learning (ICML\n  2025), Vancouver, Canada", "pdf_url": "http://arxiv.org/pdf/2507.07769v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07638", "title": "Bridging the gap in FER: addressing age bias in deep learning", "authors": ["F. Xavier Gaya-Morey", "Julia Sanchez-Perez", "Cristina Manresa-Yee", "Jose M. Buades-Rubio"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07638v1", "summary": "Facial Expression Recognition (FER) systems based on deep learning have\nachieved impressive performance in recent years. However, these models often\nexhibit demographic biases, particularly with respect to age, which can\ncompromise their fairness and reliability. In this work, we present a\ncomprehensive study of age-related bias in deep FER models, with a particular\nfocus on the elderly population. We first investigate whether recognition\nperformance varies across age groups, which expressions are most affected, and\nwhether model attention differs depending on age. Using Explainable AI (XAI)\ntechniques, we identify systematic disparities in expression recognition and\nattention patterns, especially for \"neutral\", \"sadness\", and \"anger\" in elderly\nindividuals. Based on these findings, we propose and evaluate three bias\nmitigation strategies: Multi-task Learning, Multi-modal Input, and Age-weighted\nLoss. Our models are trained on a large-scale dataset, AffectNet, with\nautomatically estimated age labels and validated on balanced benchmark datasets\nthat include underrepresented age groups. Results show consistent improvements\nin recognition accuracy for elderly individuals, particularly for the most\nerror-prone expressions. Saliency heatmap analysis reveals that models trained\nwith age-aware strategies attend to more relevant facial regions for each age\ngroup, helping to explain the observed improvements. These findings suggest\nthat age-related bias in FER can be effectively mitigated using simple training\nmodifications, and that even approximate demographic labels can be valuable for\npromoting fairness in large-scale affective computing systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07638v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07631", "title": "Generic Speech Enhancement with Self-Supervised Representation Space Loss", "authors": ["Hiroshi Sato", "Tsubasa Ochiai", "Marc Delcroix", "Takafumi Moriya", "Takanori Ashihara", "Ryo Masumura"], "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      22 pages, 3 figures. Accepted for Frontiers in signal processing", "url": "http://arxiv.org/abs/2507.07631v1", "summary": "Single-channel speech enhancement is utilized in various tasks to mitigate\nthe effect of interfering signals. Conventionally, to ensure the speech\nenhancement performs optimally, the speech enhancement has needed to be tuned\nfor each task. Thus, generalizing speech enhancement models to unknown\ndownstream tasks has been challenging. This study aims to construct a generic\nspeech enhancement front-end that can improve the performance of back-ends to\nsolve multiple downstream tasks. To this end, we propose a novel training\ncriterion that minimizes the distance between the enhanced and the ground truth\nclean signal in the feature representation domain of self-supervised learning\nmodels. Since self-supervised learning feature representations effectively\nexpress high-level speech information useful for solving various downstream\ntasks, the proposal is expected to make speech enhancement models preserve such\ninformation. Experimental validation demonstrates that the proposal improves\nthe performance of multiple speech tasks while maintaining the perceptual\nquality of the enhanced signal.", "comment": "22 pages, 3 figures. Accepted for Frontiers in signal processing", "pdf_url": "http://arxiv.org/pdf/2507.07631v1", "cate": "eess.AS", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07993", "title": "Multigranular Evaluation for Brain Visual Decoding", "authors": ["Weihao Xia", "Cengiz Oztireli"], "categories": ["cs.CV", "cs.AI", "eess.IV", "q-bio.NC"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project: this https URL", "url": "http://arxiv.org/abs/2507.07993v1", "summary": "Existing evaluation protocols for brain visual decoding predominantly rely on\ncoarse metrics that obscure inter-model differences, lack neuroscientific\nfoundation, and fail to capture fine-grained visual distinctions. To address\nthese limitations, we introduce BASIC, a unified, multigranular evaluation\nframework that jointly quantifies structural fidelity, inferential alignment,\nand contextual coherence between decoded and ground truth images. For the\nstructural level, we introduce a hierarchical suite of segmentation-based\nmetrics, including foreground, semantic, instance, and component masks,\nanchored in granularity-aware correspondence across mask structures. For the\nsemantic level, we extract structured scene representations encompassing\nobjects, attributes, and relationships using multimodal large language models,\nenabling detailed, scalable, and context-rich comparisons with ground-truth\nstimuli. We benchmark a diverse set of visual decoding methods across multiple\nstimulus-neuroimaging datasets within this unified evaluation framework.\nTogether, these criteria provide a more discriminative, interpretable, and\ncomprehensive foundation for measuring brain visual decoding methods.", "comment": "Project: https://weihaox.github.io/BASIC", "pdf_url": "http://arxiv.org/pdf/2507.07993v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07780", "title": "Where are we with calibration under dataset shift in image classification?", "authors": ["M√©lanie Roschewitz", "Raghav Mehta", "Fabio de Sousa Ribeiro", "Ben Glocker"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Code available at this https URL", "url": "http://arxiv.org/abs/2507.07780v1", "summary": "We conduct an extensive study on the state of calibration under real-world\ndataset shift for image classification. Our work provides important insights on\nthe choice of post-hoc and in-training calibration techniques, and yields\npractical guidelines for all practitioners interested in robust calibration\nunder shift. We compare various post-hoc calibration methods, and their\ninteractions with common in-training calibration strategies (e.g., label\nsmoothing), across a wide range of natural shifts, on eight different\nclassification tasks across several imaging domains. We find that: (i)\nsimultaneously applying entropy regularisation and label smoothing yield the\nbest calibrated raw probabilities under dataset shift, (ii) post-hoc\ncalibrators exposed to a small amount of semantic out-of-distribution data\n(unrelated to the task) are most robust under shift, (iii) recent calibration\nmethods specifically aimed at increasing calibration under shifts do not\nnecessarily offer significant improvements over simpler post-hoc calibration\nmethods, (iv) improving calibration under shifts often comes at the cost of\nworsening in-distribution calibration. Importantly, these findings hold for\nrandomly initialised classifiers, as well as for those finetuned from\nfoundation models, the latter being consistently better calibrated compared to\nmodels trained from scratch. Finally, we conduct an in-depth analysis of\nensembling effects, finding that (i) applying calibration prior to ensembling\n(instead of after) is more effective for calibration under shifts, (ii) for\nensembles, OOD exposure deteriorates the ID-shifted calibration trade-off,\n(iii) ensembling remains one of the most effective methods to improve\ncalibration robustness and, combined with finetuning from foundation models,\nyields best calibration results overall.", "comment": "Code available at\n  https://github.com/biomedia-mira/calibration_under_shifts", "pdf_url": "http://arxiv.org/pdf/2507.07780v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07792", "title": "Space-Filling Regularization for Robust and Interpretable Nonlinear State Space Models", "authors": ["Hermann Klein", "Max Heinz Herkersdorf", "Oliver Nelles"], "categories": ["cs.LG", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07792v1", "summary": "The state space dynamics representation is the most general approach for\nnonlinear systems and often chosen for system identification. During training,\nthe state trajectory can deform significantly leading to poor data coverage of\nthe state space. This can cause significant issues for space-oriented training\nalgorithms which e.g. rely on grid structures, tree partitioning, or similar.\nBesides hindering training, significant state trajectory deformations also\ndeteriorate interpretability and robustness properties. This paper proposes a\nnew type of space-filling regularization that ensures a favorable data\ndistribution in state space via introducing a data-distribution-based penalty.\nThis method is demonstrated in local model network architectures where good\ninterpretability is a major concern. The proposed approach integrates ideas\nfrom modeling and design of experiments for state space structures. This is why\nwe present two regularization techniques for the data point distributions of\nthe state trajectories for local affine state space models. Beyond that, we\ndemonstrate the results on a widely known system identification benchmark.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07792v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07663", "title": "MolCLIP: A Molecular-Auxiliary CLIP Framework for Identifying Drug Mechanism of Action Based on Time-Lapsed Mitochondrial Images", "authors": ["Fengqian Pang", "Chunyue Lei", "Hongfei Zhao", "Chenghao Liu", "Zhiqiang Xing", "Huafeng Wang", "Chuyang Ye"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07663v1", "summary": "Drug Mechanism of Action (MoA) mainly investigates how drug molecules\ninteract with cells, which is crucial for drug discovery and clinical\napplication. Recently, deep learning models have been used to recognize MoA by\nrelying on high-content and fluorescence images of cells exposed to various\ndrugs. However, these methods focus on spatial characteristics while\noverlooking the temporal dynamics of live cells. Time-lapse imaging is more\nsuitable for observing the cell response to drugs. Additionally, drug molecules\ncan trigger cellular dynamic variations related to specific MoA. This indicates\nthat the drug molecule modality may complement the image counterpart. This\npaper proposes MolCLIP, the first visual language model to combine microscopic\ncell video- and molecule-modalities. MolCLIP designs a molecule-auxiliary CLIP\nframework to guide video features in learning the distribution of the molecular\nlatent space. Furthermore, we integrate a metric learning strategy with MolCLIP\nto optimize the aggregation of video features. Experimental results on the\nMitoDataset demonstrate that MolCLIP achieves improvements of 51.2% and 20.5%\nin mAP for drug identification and MoA recognition, respectively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07663v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2306.12336", "title": "Smart Timing Synchronization for Small Data Transmission", "authors": ["Gautham Prasad", "Nadhem Rojbi", "Flynn Dowey", "Nikhileswar Kota", "Lutz Lampe", "Gus Vos"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      17 pages, 12 figures", "url": "http://arxiv.org/abs/2306.12336v2", "summary": "Cellular Internet-of-things (C-IoT) user equipments (UEs) typically transmit\nperiodic but small amounts of uplink data to the base station. To avoid\nundergoing a traditional random access procedure prior to every transmission,\n5th generation (5G) and newer systems use configured grants for small data\ntransmission (CG-SDT), which is equivalent to its long-term evolution (LTE)\ncounterpart of preconfigured uplink resources (PURs)-based transmission. CG-SDT\nconfigures uplink resources to UEs in advance for transmission without a random\naccess procedure. A prerequisite for CG-SDT is that the UEs must use a valid\ntiming advance (TA). This is done by validating a previously held TA before\nCG-SDT. While this validation is trivial for stationary UEs, mobile UEs often\nencounter conditions where the previous TA is no longer valid and a new one is\nto be requested by falling back to legacy random access procedures. This limits\nthe applicability of CG-SDT in mobile UEs. To this end, we propose UE-native\nsmart timing synchronization techniques to counter this drawback and ensure a\nnear-universal adoption of CG-SDT. We introduce new machine learning-aided\nsolutions for validation and prediction of TA for UEs with any type of\nmobility. We perform comprehensive simulation evaluations across different\ntypes of communication environments to demonstrate the effectiveness of our\nproposed solution in predicting the TA.", "comment": "17 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2306.12336v2", "cate": "eess.SP", "date": "2023-06-21", "updated": "2025-07-09"}
{"id": "2506.23664", "title": "Diffusion Model-based Data Augmentation Method for Fetal Head Ultrasound Segmentation", "authors": ["Fangyijie Wang", "Kevin Whelan", "F√©lix Balado", "Kathleen M. Curran", "Gu√©nol√© Silvestre"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Accepted at Irish Machine Vision and Image Processing Conference (IMVIP) 2025", "url": "http://arxiv.org/abs/2506.23664v2", "summary": "Medical image data is less accessible than in other domains due to privacy\nand regulatory constraints. In addition, labeling requires costly,\ntime-intensive manual image annotation by clinical experts. To overcome these\nchallenges, synthetic medical data generation offers a promising solution.\nGenerative AI (GenAI), employing generative deep learning models, has proven\neffective at producing realistic synthetic images. This study proposes a novel\nmask-guided GenAI approach using diffusion models to generate synthetic fetal\nhead ultrasound images paired with segmentation masks. These synthetic pairs\naugment real datasets for supervised fine-tuning of the Segment Anything Model\n(SAM). Our results show that the synthetic data captures real image features\neffectively, and this approach reaches state-of-the-art fetal head\nsegmentation, especially when trained with a limited number of real image-mask\npairs. In particular, the segmentation reaches Dice Scores of 94.66\\% and\n94.38\\% using a handful of ultrasound images from the Spanish and African\ncohorts, respectively. Our code, models, and data are available on GitHub.", "comment": "Accepted at Irish Machine Vision and Image Processing Conference\n  (IMVIP) 2025", "pdf_url": "http://arxiv.org/pdf/2506.23664v2", "cate": "eess.IV", "date": "2025-06-30", "updated": "2025-07-10"}
{"id": "2507.07270", "title": "Audio-Visual Speech Separation via Bottleneck Iterative Network", "authors": ["Sidong Zhang", "Shiv Shankar", "Trang Nguyen", "Andrea Fanelli", "Madalina Fiterau"], "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Accepted to the 42nd International Conference on Machine Learning Workshop on Machine Learning for Audio", "url": "http://arxiv.org/abs/2507.07270v1", "summary": "Integration of information from non-auditory cues can significantly improve\nthe performance of speech-separation models. Often such models use deep\nmodality-specific networks to obtain unimodal features, and risk being too\ncostly or lightweight but lacking capacity. In this work, we present an\niterative representation refinement approach called Bottleneck Iterative\nNetwork (BIN), a technique that repeatedly progresses through a lightweight\nfusion block, while bottlenecking fusion representations by fusion tokens. This\nhelps improve the capacity of the model, while avoiding major increase in model\nsize and balancing between the model performance and training cost. We test BIN\non challenging noisy audio-visual speech separation tasks, and show that our\napproach consistently outperforms state-of-the-art benchmark models with\nrespect to SI-SDRi on NTCD-TIMIT and LRS3+WHAM! datasets, while simultaneously\nachieving a reduction of more than 50% in training and GPU inference time\nacross nearly all settings.", "comment": "Accepted to the 42nd International Conference on Machine Learning\n  Workshop on Machine Learning for Audio", "pdf_url": "http://arxiv.org/pdf/2507.07270v1", "cate": "cs.SD", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07796", "title": "Visual Instance-aware Prompt Tuning", "authors": ["Xi Xiao", "Yunbei Zhang", "Xingjian Li", "Tianyang Wang", "Xiao Wang", "Yuxiang Wei", "Jihun Hamm", "Min Xu"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07796v1", "summary": "Visual Prompt Tuning (VPT) has emerged as a parameter-efficient fine-tuning\nparadigm for vision transformers, with conventional approaches utilizing\ndataset-level prompts that remain the same across all input instances. We\nobserve that this strategy results in sub-optimal performance due to high\nvariance in downstream datasets. To address this challenge, we propose Visual\nInstance-aware Prompt Tuning (ViaPT), which generates instance-aware prompts\nbased on each individual input and fuses them with dataset-level prompts,\nleveraging Principal Component Analysis (PCA) to retain important prompting\ninformation. Moreover, we reveal that VPT-Deep and VPT-Shallow represent two\ncorner cases based on a conceptual understanding, in which they fail to\neffectively capture instance-specific information, while random dimension\nreduction on prompts only yields performance between the two extremes. Instead,\nViaPT overcomes these limitations by balancing dataset-level and instance-level\nknowledge, while reducing the amount of learnable parameters compared to\nVPT-Deep. Extensive experiments across 34 diverse datasets demonstrate that our\nmethod consistently outperforms state-of-the-art baselines, establishing a new\nparadigm for analyzing and optimizing visual prompts for vision transformers.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07796v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07804", "title": "Deep Survival Analysis in Multimodal Medical Data: A Parametric and Probabilistic Approach with Competing Risks", "authors": ["Alba Garrido", "Alejandro Almod√≥var", "Patricia A. Apell√°niz", "Juan Parras", "Santiago Zazo"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      29 pages, 9 Figures", "url": "http://arxiv.org/abs/2507.07804v1", "summary": "Accurate survival prediction is critical in oncology for prognosis and\ntreatment planning. Traditional approaches often rely on a single data\nmodality, limiting their ability to capture the complexity of tumor biology. To\naddress this challenge, we introduce a multimodal deep learning framework for\nsurvival analysis capable of modeling both single and competing risks\nscenarios, evaluating the impact of integrating multiple medical data sources\non survival predictions. We propose SAMVAE (Survival Analysis Multimodal\nVariational Autoencoder), a novel deep learning architecture designed for\nsurvival prediction that integrates six data modalities: clinical variables,\nfour molecular profiles, and histopathological images. SAMVAE leverages\nmodality specific encoders to project inputs into a shared latent space,\nenabling robust survival prediction while preserving modality specific\ninformation. Its parametric formulation enables the derivation of clinically\nmeaningful statistics from the output distributions, providing patient-specific\ninsights through interactive multimedia that contribute to more informed\nclinical decision-making and establish a foundation for interpretable,\ndata-driven survival analysis in oncology. We evaluate SAMVAE on two cancer\ncohorts breast cancer and lower grade glioma applying tailored preprocessing,\ndimensionality reduction, and hyperparameter optimization. The results\ndemonstrate the successful integration of multimodal data for both standard\nsurvival analysis and competing risks scenarios across different datasets. Our\nmodel achieves competitive performance compared to state-of-the-art multimodal\nsurvival models. Notably, this is the first parametric multimodal deep learning\narchitecture to incorporate competing risks while modeling continuous time to a\nspecific event, using both tabular and image data.", "comment": "29 pages, 9 Figures", "pdf_url": "http://arxiv.org/pdf/2507.07804v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07670", "title": "Attend-and-Refine: Interactive keypoint estimation and quantitative cervical vertebrae analysis for bone age assessment", "authors": ["Jinhee Kim", "Taesung Kim", "Taewoo Kim", "Dong-Wook Kim", "Byungduk Ahn", "Yoon-Ji Kim", "In-Seok Song", "Jaegul Choo"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to Medical Image Analysis (2025)", "url": "http://arxiv.org/abs/2507.07670v1", "summary": "In pediatric orthodontics, accurate estimation of growth potential is\nessential for developing effective treatment strategies. Our research aims to\npredict this potential by identifying the growth peak and analyzing cervical\nvertebra morphology solely through lateral cephalometric radiographs. We\naccomplish this by comprehensively analyzing cervical vertebral maturation\n(CVM) features from these radiographs. This methodology provides clinicians\nwith a reliable and efficient tool to determine the optimal timings for\northodontic interventions, ultimately enhancing patient outcomes. A crucial\naspect of this approach is the meticulous annotation of keypoints on the\ncervical vertebrae, a task often challenged by its labor-intensive nature. To\nmitigate this, we introduce Attend-and-Refine Network (ARNet), a\nuser-interactive, deep learning-based model designed to streamline the\nannotation process. ARNet features Interaction-guided recalibration network,\nwhich adaptively recalibrates image features in response to user feedback,\ncoupled with a morphology-aware loss function that preserves the structural\nconsistency of keypoints. This novel approach substantially reduces manual\neffort in keypoint identification, thereby enhancing the efficiency and\naccuracy of the process. Extensively validated across various datasets, ARNet\ndemonstrates remarkable performance and exhibits wide-ranging applicability in\nmedical imaging. In conclusion, our research offers an effective AI-assisted\ndiagnostic tool for assessing growth potential in pediatric orthodontics,\nmarking a significant advancement in the field.", "comment": "Accepted to Medical Image Analysis (2025)", "pdf_url": "http://arxiv.org/pdf/2507.07670v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2409.08801", "title": "Finite Sample Analysis of Distribution-Free Confidence Ellipsoids for Linear Regression", "authors": ["Szabolcs Szentp√©teri", "Bal√°zs Csan√°d Cs√°ji"], "categories": ["eess.SP", "math.ST", "stat.ML", "stat.TH"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.08801v2", "summary": "The least squares (LS) estimate is the archetypical solution of linear\nregression problems. The asymptotic Gaussianity of the scaled LS error is often\nused to construct approximate confidence ellipsoids around the LS estimate,\nhowever, for finite samples these ellipsoids do not come with strict\nguarantees, unless some strong assumptions are made on the noise distributions.\nThe paper studies the distribution-free Sign-Perturbed Sums (SPS) ellipsoidal\nouter approximation (EOA) algorithm which can construct non-asymptotically\nguaranteed confidence ellipsoids under mild assumptions, such as independent\nand symmetric noise terms. These ellipsoids have the same center and\norientation as the classical asymptotic ellipsoids, only their radii are\ndifferent, which radii can be computed by convex optimization. Here, we\nestablish high probability non-asymptotic upper bounds for the sizes of SPS\nouter ellipsoids for linear regression problems and show that the volumes of\nthese ellipsoids decrease at the optimal rate. Finally, the difference between\nour theoretical bounds and the empirical sizes of the regions are investigated\nexperimentally.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.08801v2", "cate": "eess.SP", "date": "2024-09-13", "updated": "2025-07-09"}
{"id": "2507.03421", "title": "Hybrid-View Attention Network for Clinically Significant Prostate Cancer Classification in Transrectal Ultrasound", "authors": ["Zetian Feng", "Juan Fu", "Xuebin Zou", "Hongsheng Ye", "Hong Wu", "Jianhua Zhou", "Yi Wang"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.03421v2", "summary": "Prostate cancer (PCa) is a leading cause of cancer-related mortality in men,\nand accurate identification of clinically significant PCa (csPCa) is critical\nfor timely intervention. Transrectal ultrasound (TRUS) is widely used for\nprostate biopsy; however, its low contrast and anisotropic spatial resolution\npose diagnostic challenges. To address these limitations, we propose a novel\nhybrid-view attention (HVA) network for csPCa classification in 3D TRUS that\nleverages complementary information from transverse and sagittal views. Our\napproach integrates a CNN-transformer hybrid architecture, where convolutional\nlayers extract fine-grained local features and transformer-based HVA models\nglobal dependencies. Specifically, the HVA comprises intra-view attention to\nrefine features within a single view and cross-view attention to incorporate\ncomplementary information across views. Furthermore, a hybrid-view adaptive\nfusion module dynamically aggregates features along both channel and spatial\ndimensions, enhancing the overall representation. Experiments are conducted on\nan in-house dataset containing 590 subjects who underwent prostate biopsy.\nComparative and ablation results prove the efficacy of our method. The code is\navailable at https://github.com/mock1ngbrd/HVAN.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.03421v2", "cate": "eess.IV", "date": "2025-07-04", "updated": "2025-07-10"}
{"id": "2507.07384", "title": "VP-SelDoA: Visual-prompted Selective DoA Estimation of Target Sound via Semantic-Spatial Matching", "authors": ["Yu Chen", "Xinyuan Qian", "Hongxu Zhu", "Jiadong Wang", "Kainan Chen", "Haizhou Li"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Under Review", "url": "http://arxiv.org/abs/2507.07384v1", "summary": "Audio-visual sound source localization (AV-SSL) identifies the position of a\nsound source by exploiting the complementary strengths of auditory and visual\nsignals. However, existing AV-SSL methods encounter three major challenges: 1)\ninability to selectively isolate the target sound source in multi-source\nscenarios, 2) misalignment between semantic visual features and spatial\nacoustic features, and 3) overreliance on paired audio-visual data. To overcome\nthese limitations, we introduce Cross-Instance Audio-Visual Localization\n(CI-AVL), a novel task that leverages images from different instances of the\nsame sound event category to localize target sound sources, thereby reducing\ndependence on paired data while enhancing generalization capabilities. Our\nproposed VP-SelDoA tackles this challenging task through a semantic-level\nmodality fusion and employs a Frequency-Temporal ConMamba architecture to\ngenerate target-selective masks for sound isolation. We further develop a\nSemantic-Spatial Matching mechanism that aligns the heterogeneous semantic and\nspatial features via integrated cross- and self-attention mechanisms. To\nfacilitate the CI-AVL research, we construct a large-scale dataset named\nVGG-SSL, comprising 13,981 spatial audio clips across 296 sound event\ncategories. Extensive experiments show that our proposed method outperforms\nstate-of-the-art audio-visual localization methods, achieving a mean absolute\nerror (MAE) of 12.04 and an accuracy (ACC) of 78.23%.", "comment": "Under Review", "pdf_url": "http://arxiv.org/pdf/2507.07384v1", "cate": "cs.SD", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07808", "title": "Bridging Logic and Learning: Decoding Temporal Logic Embeddings via Transformers", "authors": ["Sara Candussio", "Gaia Saveri", "Gabriele Sarti", "Luca Bortolussi"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      16 pages, 3 figures, to be published in ECML-PKDD", "url": "http://arxiv.org/abs/2507.07808v1", "summary": "Continuous representations of logic formulae allow us to integrate symbolic\nknowledge into data-driven learning algorithms. If such embeddings are\nsemantically consistent, i.e. if similar specifications are mapped into nearby\nvectors, they enable continuous learning and optimization directly in the\nsemantic space of formulae. However, to translate the optimal continuous\nrepresentation into a concrete requirement, such embeddings must be invertible.\nWe tackle this issue by training a Transformer-based decoder-only model to\ninvert semantic embeddings of Signal Temporal Logic (STL) formulae. STL is a\npowerful formalism that allows us to describe properties of signals varying\nover time in an expressive yet concise way. By constructing a small vocabulary\nfrom STL syntax, we demonstrate that our proposed model is able to generate\nvalid formulae after only 1 epoch and to generalize to the semantics of the\nlogic in about 10 epochs. Additionally, the model is able to decode a given\nembedding into formulae that are often simpler in terms of length and nesting\nwhile remaining semantically close (or equivalent) to gold references. We show\nthe effectiveness of our methodology across various levels of training formulae\ncomplexity to assess the impact of training data on the model's ability to\neffectively capture the semantic information contained in the embeddings and\ngeneralize out-of-distribution. Finally, we deploy our model for solving a\nrequirement mining task, i.e. inferring STL specifications that solve a\nclassification task on trajectories, performing the optimization directly in\nthe semantic space.", "comment": "16 pages, 3 figures, to be published in ECML-PKDD", "pdf_url": "http://arxiv.org/pdf/2507.07808v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07814", "title": "Pay Attention to Attention Distribution: A New Local Lipschitz Bound for Transformers", "authors": ["Nikolay Yudin", "Alexander Gaponov", "Sergei Kudriashov", "Maxim Rakhuba"], "categories": ["cs.LG", "cs.NA", "math.NA", "15A42, 15A60, 68T07"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07814v1", "summary": "We present a novel local Lipschitz bound for self-attention blocks of\ntransformers. This bound is based on a refined closed-form expression for the\nspectral norm of the softmax function. The resulting bound is not only more\naccurate than in the prior art, but also unveils the dependence of the\nLipschitz constant on attention score maps. Based on the new findings, we\nsuggest an explanation of the way distributions inside the attention map affect\nthe robustness from the Lipschitz constant perspective. We also introduce a new\nlightweight regularization term called JaSMin (Jacobian Softmax norm\nMinimization), which boosts the transformer's robustness and decreases local\nLipschitz constants of the whole network.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07814v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07678", "title": "Action Unit Enhance Dynamic Facial Expression Recognition", "authors": ["Feng Liu", "Lingna Gu", "Chen Shi", "Xiaolan Fu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07678v1", "summary": "Dynamic Facial Expression Recognition(DFER) is a rapidly evolving field of\nresearch that focuses on the recognition of time-series facial expressions.\nWhile previous research on DFER has concentrated on feature learning from a\ndeep learning perspective, we put forward an AU-enhanced Dynamic Facial\nExpression Recognition architecture, namely AU-DFER, that incorporates\nAU-expression knowledge to enhance the effectiveness of deep learning modeling.\nIn particular, the contribution of the Action Units(AUs) to different\nexpressions is quantified, and a weight matrix is designed to incorporate a\npriori knowledge. Subsequently, the knowledge is integrated with the learning\noutcomes of a conventional deep learning network through the introduction of AU\nloss. The design is incorporated into the existing optimal model for dynamic\nexpression recognition for the purpose of validation. Experiments are conducted\non three recent mainstream open-source approaches to DFER on the principal\ndatasets in this field. The results demonstrate that the proposed architecture\noutperforms the state-of-the-art(SOTA) methods without the need for additional\narithmetic and generally produces improved results. Furthermore, we investigate\nthe potential of AU loss function redesign to address data label imbalance\nissues in established dynamic expression datasets. To the best of our\nknowledge, this is the first attempt to integrate quantified AU-expression\nknowledge into various DFER models. We also devise strategies to tackle label\nimbalance, or minor class problems. Our findings suggest that employing a\ndiverse strategy of loss function design can enhance the effectiveness of DFER.\nThis underscores the criticality of addressing data imbalance challenges in\nmainstream datasets within this domain. The source code is available at\nhttps://github.com/Cross-Innovation-Lab/AU-DFER.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07678v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2504.15514", "title": "Learning-Based Two-Way Communications: Algorithmic Framework and Comparative Analysis", "authors": ["David R. Nickel", "Anindya Bijoy Das", "David J. Love", "Christopher G. Brinton"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      Currently under review for IEEE Communications Letters. 5 pages", "url": "http://arxiv.org/abs/2504.15514v2", "summary": "Machine learning (ML)-based feedback channel coding has garnered significant\nresearch interest in the past few years. However, there has been limited\nresearch exploring ML approaches in the so-called \"two-way\" setting where two\nusers jointly encode messages and feedback for each other over a shared\nchannel. In this work, we present a general architecture for ML-based two-way\nfeedback coding, and show how several popular one-way schemes can be converted\nto the two-way setting through our framework. We compare such schemes against\ntheir one-way counterparts, revealing error-rate benefits of ML-based two-way\ncoding in certain signal-to-noise ratio (SNR) regimes. We then analyze the\ntradeoffs between error performance and computational overhead for three\nstate-of-the-art neural network coding models instantiated in the two-way\nparadigm.", "comment": "Currently under review for IEEE Communications Letters. 5 pages", "pdf_url": "http://arxiv.org/pdf/2504.15514v2", "cate": "eess.SP", "date": "2025-04-22", "updated": "2025-07-10"}
{"id": "2507.05317", "title": "PWD: Prior-Guided and Wavelet-Enhanced Diffusion Model for Limited-Angle CT", "authors": ["Yi Liu", "Yiyang Wen", "Zekun Zhou", "Junqi Ma", "Linghang Wang", "Yucheng Yao", "Liu Shi", "Qiegen Liu"], "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.05317v2", "summary": "Generative diffusion models have received increasing attention in medical\nimaging, particularly in limited-angle computed tomography (LACT). Standard\ndiffusion models achieve high-quality image reconstruction but require a large\nnumber of sampling steps during inference, resulting in substantial\ncomputational overhead. Although skip-sampling strategies have been proposed to\nimprove efficiency, they often lead to loss of fine structural details. To\naddress this issue, we propose a prior information embedding and wavelet\nfeature fusion fast sampling diffusion model for LACT reconstruction. The PWD\nenables efficient sampling while preserving reconstruction fidelity in LACT,\nand effectively mitigates the degradation typically introduced by\nskip-sampling. Specifically, during the training phase, PWD maps the\ndistribution of LACT images to that of fully sampled target images, enabling\nthe model to learn structural correspondences between them. During inference,\nthe LACT image serves as an explicit prior to guide the sampling trajectory,\nallowing for high-quality reconstruction with significantly fewer steps. In\naddition, PWD performs multi-scale feature fusion in the wavelet domain,\neffectively enhancing the reconstruction of fine details by leveraging both\nlow-frequency and high-frequency information. Quantitative and qualitative\nevaluations on clinical dental arch CBCT and periapical datasets demonstrate\nthat PWD outperforms existing methods under the same sampling condition. Using\nonly 50 sampling steps, PWD achieves at least 1.7 dB improvement in PSNR and\n10% gain in SSIM.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.05317v2", "cate": "eess.IV", "date": "2025-06-30", "updated": "2025-07-10"}
{"id": "2507.07396", "title": "IML-Spikeformer: Input-aware Multi-Level Spiking Transformer for Speech Processing", "authors": ["Zeyang Song", "Shimin Zhang", "Yuhong Chou", "Jibin Wu", "Haizhou Li"], "categories": ["cs.MM", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Multimedia (cs.MM)", "pdf_link": null, "comments": "Comments:      Under review of TNNLS", "url": "http://arxiv.org/abs/2507.07396v1", "summary": "Spiking Neural Networks (SNNs), inspired by biological neural mechanisms,\nrepresent a promising neuromorphic computing paradigm that offers\nenergy-efficient alternatives to traditional Artificial Neural Networks (ANNs).\nDespite proven effectiveness, SNN architectures have struggled to achieve\ncompetitive performance on large-scale speech processing task. Two key\nchallenges hinder progress: (1) the high computational overhead during training\ncaused by multi-timestep spike firing, and (2) the absence of large-scale SNN\narchitectures tailored to speech processing tasks. To overcome the issues, we\nintroduce Input-aware Multi-Level Spikeformer, i.e. IML-Spikeformer, a spiking\nTransformer architecture specifically designed for large-scale speech\nprocessing. Central to our design is the Input-aware Multi-Level Spike (IMLS)\nmechanism, which simulate multi-timestep spike firing within a single timestep\nusing an adaptive, input-aware thresholding scheme. IML-Spikeformer further\nintegrates a Reparameterized Spiking Self-Attention (RepSSA) module with a\nHierarchical Decay Mask (HDM), forming the HD-RepSSA module. This module\nenhances the precision of attention maps and enables modeling of multi-scale\ntemporal dependencies in speech signals. Experiments demonstrate that\nIML-Spikeformer achieves word error rates of 6.0\\% on AiShell-1 and 3.4\\% on\nLibrispeech-960, comparable to conventional ANN transformers while reducing\ntheoretical inference energy consumption by 4.64$\\times$ and 4.32$\\times$\nrespectively. IML-Spikeformer marks an advance of scalable SNN architectures\nfor large-scale speech processing in both task performance and energy\nefficiency.", "comment": "Under review of TNNLS", "pdf_url": "http://arxiv.org/pdf/2507.07396v1", "cate": "cs.MM", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07817", "title": "On the Effect of Instruction Tuning Loss on Generalization", "authors": ["Anwoy Chatterjee", "H S V N S Kowndinya Renduchintala", "Sumit Bhatia", "Tanmoy Chakraborty"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Transactions of the Association for Computational Linguistics (TACL)", "url": "http://arxiv.org/abs/2507.07817v1", "summary": "Instruction Tuning has emerged as a pivotal post-training paradigm that\nenables pre-trained language models to better follow user instructions. Despite\nits significance, little attention has been given to optimizing the loss\nfunction used. A fundamental, yet often overlooked, question is whether the\nconventional auto-regressive objective - where loss is computed only on\nresponse tokens, excluding prompt tokens - is truly optimal for instruction\ntuning. In this work, we systematically investigate the impact of\ndifferentially weighting prompt and response tokens in instruction tuning loss,\nand propose Weighted Instruction Tuning (WIT) as a better alternative to\nconventional instruction tuning. Through extensive experiments on five language\nmodels of different families and scale, three finetuning datasets of different\nsizes, and five diverse evaluation benchmarks, we show that the standard\ninstruction tuning loss often yields suboptimal performance and limited\nrobustness to input prompt variations. We find that a low-to-moderate weight\nfor prompt tokens coupled with a moderate-to-high weight for response tokens\nyields the best-performing models across settings and also serve as better\nstarting points for the subsequent preference alignment training. These\nfindings highlight the need to reconsider instruction tuning loss and offer\nactionable insights for developing more robust and generalizable models. Our\ncode is open-sourced at https://github.com/kowndinya-renduchintala/WIT.", "comment": "Transactions of the Association for Computational Linguistics (TACL)", "pdf_url": "http://arxiv.org/pdf/2507.07817v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07826", "title": "An Empirical Bernstein Inequality for Dependent Data in Hilbert Spaces and Applications", "authors": ["Erfan Mirzaei", "Andreas Maurer", "Vladimir R. Kostic", "Massimiliano Pontil"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      In The 28th International Conference on Artificial Intelligence and Statistics (2025)", "url": "http://arxiv.org/abs/2507.07826v1", "summary": "Learning from non-independent and non-identically distributed data poses a\npersistent challenge in statistical learning. In this study, we introduce\ndata-dependent Bernstein inequalities tailored for vector-valued processes in\nHilbert space. Our inequalities apply to both stationary and non-stationary\nprocesses and exploit the potential rapid decay of correlations between\ntemporally separated variables to improve estimation. We demonstrate the\nutility of these bounds by applying them to covariance operator estimation in\nthe Hilbert-Schmidt norm and to operator learning in dynamical systems,\nachieving novel risk bounds. Finally, we perform numerical experiments to\nillustrate the practical implications of these bounds in both contexts.", "comment": "In The 28th International Conference on Artificial Intelligence and\n  Statistics (2025)", "pdf_url": "http://arxiv.org/pdf/2507.07826v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07687", "title": "Tree-Mamba: A Tree-Aware Mamba for Underwater Monocular Depth Estimation", "authors": ["Peixian Zhuang", "Yijian Wang", "Zhenqi Fu", "Hongliang Zhang", "Sam Kwong", "Chongyi Li"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07687v1", "summary": "Underwater Monocular Depth Estimation (UMDE) is a critical task that aims to\nestimate high-precision depth maps from underwater degraded images caused by\nlight absorption and scattering effects in marine environments. Recently,\nMamba-based methods have achieved promising performance across various vision\ntasks; however, they struggle with the UMDE task because their inflexible state\nscanning strategies fail to model the structural features of underwater images\neffectively. Meanwhile, existing UMDE datasets usually contain unreliable depth\nlabels, leading to incorrect object-depth relationships between underwater\nimages and their corresponding depth maps. To overcome these limitations, we\ndevelop a novel tree-aware Mamba method, dubbed Tree-Mamba, for estimating\naccurate monocular depth maps from underwater degraded images. Specifically, we\npropose a tree-aware scanning strategy that adaptively constructs a minimum\nspanning tree based on feature similarity. The spatial topological features\namong the tree nodes are then flexibly aggregated through bottom-up and\ntop-down traversals, enabling stronger multi-scale feature representation\ncapabilities. Moreover, we construct an underwater depth estimation benchmark\n(called BlueDepth), which consists of 38,162 underwater image pairs with\nreliable depth labels. This benchmark serves as a foundational dataset for\ntraining existing deep learning-based UMDE methods to learn accurate\nobject-depth relationships. Extensive experiments demonstrate the superiority\nof the proposed Tree-Mamba over several leading methods in both qualitative\nresults and quantitative evaluations with competitive computational efficiency.\nCode and dataset will be available at https://wyjgr.github.io/Tree-Mamba.html.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07687v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2505.05030", "title": "Autoregressive Stochastic Clock Jitter Compensation in Analog-to-Digital Converters", "authors": ["Daniele Gerosa", "Rui Hou", "Vimar Bj√∂rk", "Ulf Gustavsson", "Thomas Eriksson"], "categories": ["eess.SP", "math.OC"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      The proof of Proposition II.2 contained a flaw that made it invalid; we have thus reworked it. The paper conclusions are unchanged. We improved notations and fixed misspellings here and there", "url": "http://arxiv.org/abs/2505.05030v3", "summary": "This paper deals with the mathematical modeling and compensation of\nstochastic discrete time clock jitter in Analog-to-Digital Converters (ADCs).\nTwo novel, computationally efficient de-jittering sample pilots-based\nalgorithms for baseband signals are proposed: one consisting in solving a\nsequence of weighted least-squares problems and another that fully leverages\nthe correlated jitter structure in a Kalman filter-type routine. Alongside, a\ncomprehensive and rigorous mathematical analysis of the linearization errors\ncommitted is presented, and the work is complemented with extensive synthetic\nsimulations and performance benchmarking with the scope of gauging and\nstress-testing the techniques in different scenarios.", "comment": "The proof of Proposition II.2 contained a flaw that made it invalid;\n  we have thus reworked it. The paper conclusions are unchanged. We improved\n  notations and fixed misspellings here and there", "pdf_url": "http://arxiv.org/pdf/2505.05030v3", "cate": "eess.SP", "date": "2025-05-08", "updated": "2025-07-10"}
{"id": "2507.06410", "title": "Attention-Enhanced Deep Learning Ensemble for Breast Density Classification in Mammography", "authors": ["Peyman Sharifian", "Xiaotong Hong", "Alireza Karimian", "Mehdi Amini", "Hossein Arabi"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      2025 IEEE Nuclear Science Symposium, Medical Imaging Conference and Room Temperature Semiconductor Detector Conference", "url": "http://arxiv.org/abs/2507.06410v2", "summary": "Breast density assessment is a crucial component of mammographic\ninterpretation, with high breast density (BI-RADS categories C and D)\nrepresenting both a significant risk factor for developing breast cancer and a\ntechnical challenge for tumor detection. This study proposes an automated deep\nlearning system for robust binary classification of breast density (low: A/B\nvs. high: C/D) using the VinDr-Mammo dataset. We implemented and compared four\nadvanced convolutional neural networks: ResNet18, ResNet50, EfficientNet-B0,\nand DenseNet121, each enhanced with channel attention mechanisms. To address\nthe inherent class imbalance, we developed a novel Combined Focal Label\nSmoothing Loss function that integrates focal loss, label smoothing, and\nclass-balanced weighting. Our preprocessing pipeline incorporated advanced\ntechniques, including contrast-limited adaptive histogram equalization (CLAHE)\nand comprehensive data augmentation. The individual models were combined\nthrough an optimized ensemble voting approach, achieving superior performance\n(AUC: 0.963, F1-score: 0.952) compared to any single model. This system\ndemonstrates significant potential to standardize density assessments in\nclinical practice, potentially improving screening efficiency and early cancer\ndetection rates while reducing inter-observer variability among radiologists.", "comment": "2025 IEEE Nuclear Science Symposium, Medical Imaging Conference and\n  Room Temperature Semiconductor Detector Conference", "pdf_url": "http://arxiv.org/pdf/2507.06410v2", "cate": "eess.IV", "date": "2025-07-08", "updated": "2025-07-10"}
{"id": "2507.07526", "title": "DMF2Mel: A Dynamic Multiscale Fusion Network for EEG-Driven Mel Spectrogram Reconstruction", "authors": ["Cunhang Fan", "Sheng Zhang", "Jingjing Zhang", "Enrui Liu", "Xinhui Li", "Minggang Zhao", "Zhao Lv"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Accepted by ACM MM 2025", "url": "http://arxiv.org/abs/2507.07526v1", "summary": "Decoding speech from brain signals is a challenging research problem.\nAlthough existing technologies have made progress in reconstructing the mel\nspectrograms of auditory stimuli at the word or letter level, there remain core\nchallenges in the precise reconstruction of minute-level continuous imagined\nspeech: traditional models struggle to balance the efficiency of temporal\ndependency modeling and information retention in long-sequence decoding. To\naddress this issue, this paper proposes the Dynamic Multiscale Fusion Network\n(DMF2Mel), which consists of four core components: the Dynamic Contrastive\nFeature Aggregation Module (DC-FAM), the Hierarchical Attention-Guided\nMulti-Scale Network (HAMS-Net), the SplineMap attention mechanism, and the\nbidirectional state space module (convMamba). Specifically, the DC-FAM\nseparates speech-related \"foreground features\" from noisy \"background features\"\nthrough local convolution and global attention mechanisms, effectively\nsuppressing interference and enhancing the representation of transient signals.\nHAMS-Net, based on the U-Net framework,achieves cross-scale fusion of\nhigh-level semantics and low-level details. The SplineMap attention mechanism\nintegrates the Adaptive Gated Kolmogorov-Arnold Network (AGKAN) to combine\nglobal context modeling with spline-based local fitting. The convMamba captures\nlong-range temporal dependencies with linear complexity and enhances nonlinear\ndynamic modeling capabilities. Results on the SparrKULee dataset show that\nDMF2Mel achieves a Pearson correlation coefficient of 0.074 in mel spectrogram\nreconstruction for known subjects (a 48% improvement over the baseline) and\n0.048 for unknown subjects (a 35% improvement over the baseline).Code is\navailable at: https://github.com/fchest/DMF2Mel.", "comment": "Accepted by ACM MM 2025", "pdf_url": "http://arxiv.org/pdf/2507.07526v1", "cate": "cs.SD", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07828", "title": "Benchmarking Content-Based Puzzle Solvers on Corrupted Jigsaw Puzzles", "authors": ["Richard Dirauf", "Florian Wolz", "Dario Zanca", "Bj√∂rn Eskofier"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at ICIAP 2025", "url": "http://arxiv.org/abs/2507.07828v1", "summary": "Content-based puzzle solvers have been extensively studied, demonstrating\nsignificant progress in computational techniques. However, their evaluation\noften lacks realistic challenges crucial for real-world applications, such as\nthe reassembly of fragmented artefacts or shredded documents. In this work, we\ninvestigate the robustness of State-Of-The-Art content-based puzzle solvers\nintroducing three types of jigsaw puzzle corruptions: missing pieces, eroded\nedges, and eroded contents. Evaluating both heuristic and deep learning-based\nsolvers, we analyse their ability to handle these corruptions and identify key\nlimitations. Our results show that solvers developed for standard puzzles have\na rapid decline in performance if more pieces are corrupted. However, deep\nlearning models can significantly improve their robustness through fine-tuning\nwith augmented data. Notably, the advanced Positional Diffusion model adapts\nparticularly well, outperforming its competitors in most experiments. Based on\nour findings, we highlight promising research directions for enhancing the\nautomated reconstruction of real-world artefacts.", "comment": "Accepted at ICIAP 2025", "pdf_url": "http://arxiv.org/pdf/2507.07828v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07829", "title": "Towards Benchmarking Foundation Models for Tabular Data With Text", "authors": ["Martin Mr√°z", "Breenda Das", "Anshul Gupta", "Lennart Purucker", "Frank Hutter"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted at Foundation Models for Structured Data workshop at ICML 2025", "url": "http://arxiv.org/abs/2507.07829v1", "summary": "Foundation models for tabular data are rapidly evolving, with increasing\ninterest in extending them to support additional modalities such as free-text\nfeatures. However, existing benchmarks for tabular data rarely include textual\ncolumns, and identifying real-world tabular datasets with semantically rich\ntext features is non-trivial. We propose a series of simple yet effective\nablation-style strategies for incorporating text into conventional tabular\npipelines. Moreover, we benchmark how state-of-the-art tabular foundation\nmodels can handle textual data by manually curating a collection of real-world\ntabular datasets with meaningful textual features. Our study is an important\nstep towards improving benchmarking of foundation models for tabular data with\ntext.", "comment": "Accepted at Foundation Models for Structured Data workshop at ICML\n  2025", "pdf_url": "http://arxiv.org/pdf/2507.07829v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07704", "title": "D-CNN and VQ-VAE Autoencoders for Compression and Denoising of Industrial X-ray Computed Tomography Images", "authors": ["Bardia Hejazi", "Keerthana Chand", "Tobias Fritsch", "Giovanni Bruno"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07704v1", "summary": "The ever-growing volume of data in imaging sciences stemming from the\nadvancements in imaging technologies, necessitates efficient and reliable\nstorage solutions for such large datasets. This study investigates the\ncompression of industrial X-ray computed tomography (XCT) data using deep\nlearning autoencoders and examines how these compression algorithms affect the\nquality of the recovered data. Two network architectures with different\ncompression rates were used, a deep convolution neural network (D-CNN) and a\nvector quantized variational autoencoder (VQ-VAE). The XCT data used was from a\nsandstone sample with a complex internal pore network. The quality of the\ndecoded images obtained from the two different deep learning architectures with\ndifferent compression rates were quantified and compared to the original input\ndata. In addition, to improve image decoding quality metrics, we introduced a\nmetric sensitive to edge preservation, which is crucial for three-dimensional\ndata analysis. We showed that different architectures and compression rates are\nrequired depending on the specific characteristics needed to be preserved for\nlater analysis. The findings presented here can aid scientists to determine the\nrequirements and strategies for their data storage and analysis needs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07704v1", "cate": "eess.IV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2504.13523", "title": "Beyond-Diagonal Dynamic Metasurface Antenna", "authors": ["Hugo Prod'homme", "Philipp del Hougne"], "categories": ["physics.app-ph", "eess.SP"], "primary_category": "Subjects:       Applied Physics (physics.app-ph)", "pdf_link": null, "comments": "Comments:      5 pages, 2 figures, submitted to an IEEE Journal", "url": "http://arxiv.org/abs/2504.13523v2", "summary": "Dynamic metasurface antennas (DMAs) are an emerging technology for\nnext-generation wireless base stations, distinguished by hybrid analog/digital\nbeamforming capabilities with low hardware complexity. However, the intrinsic\ncoupling between meta-atoms is fixed by static waveguide or cavity structures\nin existing DMAs, which fundamentally constrains the achievable performance.\nHere, we introduce reconfigurable intrinsic coupling mechanisms between\nmeta-atoms, yielding finer control over the DMA's analog signal processing\ncapabilities. This novel hardware is coined \"beyond-diagonal DMA\" (BD-DMA), in\nline with established BD-RIS terminology. Considering realistic hardware\nconstraints, we derive a physics-consistent system model revealing (correlated)\n\"beyond-diagonal\" programmability. We also present an equivalent formulation\nwith (uncorrelated) \"diagonal\" programmability. Based on the latter, we propose\na general and efficient mutual-coupling-aware optimization algorithm.\nPhysics-consistent simulations validate the performance enhancement enabled by\nreconfigurable intrinsic coupling mechanisms in BD-DMAs. The BD-DMA benefits\ngrow with the mutual coupling strength.", "comment": "5 pages, 2 figures, submitted to an IEEE Journal", "pdf_url": "http://arxiv.org/pdf/2504.13523v2", "cate": "physics.app-ph", "date": "2025-04-18", "updated": "2025-07-10"}
{"id": "2412.04639", "title": "Multi-dynamic deep image prior for cardiac MRI", "authors": ["Marc Vornehm", "Chong Chen", "Muhammad Ahmad Sultan", "Syed Murtaza Arshad", "Yuchi Han", "Florian Knoll", "Rizwan Ahmad"], "categories": ["physics.med-ph", "cs.CV", "eess.IV"], "primary_category": "Subjects:       Medical Physics (physics.med-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.04639v2", "summary": "Cardiovascular magnetic resonance imaging is a powerful diagnostic tool for\nassessing cardiac structure and function. However, traditional breath-held\nimaging protocols pose challenges for patients with arrhythmias or limited\nbreath-holding capacity. This work aims to overcome these limitations by\ndeveloping a reconstruction framework that enables high-quality imaging in\nfree-breathing conditions for various dynamic cardiac MRI protocols.\nMulti-Dynamic Deep Image Prior (M-DIP), a novel unsupervised reconstruction\nframework for accelerated real-time cardiac MRI, is introduced. To capture\ncontrast or content variation, M-DIP first employs a spatial dictionary to\nsynthesize a time-dependent intermediate image. Then, this intermediate image\nis further refined using time-dependent deformation fields that model cardiac\nand respiratory motion. Unlike prior DIP-based methods, M-DIP simultaneously\ncaptures physiological motion and frame-to-frame content variations, making it\napplicable to a wide range of dynamic applications. We validate M-DIP using\nsimulated MRXCAT cine phantom data as well as free-breathing real-time cine,\nsingle-shot late gadolinium enhancement (LGE), and first-pass perfusion data\nfrom clinical patients. Comparative analyses against state-of-the-art\nsupervised and unsupervised approaches demonstrate M-DIP's performance and\nversatility. M-DIP achieved better image quality metrics on phantom data,\nhigher reader scores on in-vivo cine and LGE data, and comparable scores on\nin-vivo perfusion data relative to another DIP-based approach. M-DIP enables\nhigh-quality reconstructions of real-time free-breathing cardiac MRI without\nrequiring external training data. Its ability to model physiological motion and\ncontent variations makes it a promising approach for various dynamic imaging\napplications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.04639v2", "cate": "physics.med-ph", "date": "2024-12-05", "updated": "2025-07-09"}
{"id": "2507.07764", "title": "Assessing the Alignment of Audio Representations with Timbre Similarity Ratings", "authors": ["Haokun Tian", "Stefan Lattner", "Charalampos Saitis"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Accepted to ISMIR 2025", "url": "http://arxiv.org/abs/2507.07764v1", "summary": "Psychoacoustical so-called \"timbre spaces\" map perceptual similarity ratings\nof instrument sounds onto low-dimensional embeddings via multidimensional\nscaling, but suffer from scalability issues and are incapable of\ngeneralization. Recent results from audio (music and speech) quality assessment\nas well as image similarity have shown that deep learning is able to produce\nembeddings that align well with human perception while being largely free from\nthese constraints. Although the existing human-rated timbre similarity data is\nnot large enough to train deep neural networks (2,614 pairwise ratings on 334\naudio samples), it can serve as test-only data for audio models. In this paper,\nwe introduce metrics to assess the alignment of diverse audio representations\nwith human judgments of timbre similarity by comparing both the absolute values\nand the rankings of embedding distances to human similarity ratings. Our\nevaluation involves three signal-processing-based representations, twelve\nrepresentations extracted from pre-trained models, and three representations\nextracted from a novel sound matching model. Among them, the style embeddings\ninspired by image style transfer, extracted from the CLAP model and the sound\nmatching model, remarkably outperform the others, showing their potential in\nmodeling timbre similarity.", "comment": "Accepted to ISMIR 2025", "pdf_url": "http://arxiv.org/pdf/2507.07764v1", "cate": "cs.SD", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07847", "title": "From Ambiguity to Accuracy: The Transformative Effect of Coreference Resolution on Retrieval-Augmented Generation systems", "authors": ["Youngjoon Jang", "Seongtae Hong", "Junyoung Son", "Sungjin Park", "Chanjun Park", "Heuiseok Lim"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07847v1", "summary": "Retrieval-Augmented Generation (RAG) has emerged as a crucial framework in\nnatural language processing (NLP), improving factual consistency and reducing\nhallucinations by integrating external document retrieval with large language\nmodels (LLMs). However, the effectiveness of RAG is often hindered by\ncoreferential complexity in retrieved documents, introducing ambiguity that\ndisrupts in-context learning. In this study, we systematically investigate how\nentity coreference affects both document retrieval and generative performance\nin RAG-based systems, focusing on retrieval relevance, contextual\nunderstanding, and overall response quality. We demonstrate that coreference\nresolution enhances retrieval effectiveness and improves question-answering\n(QA) performance. Through comparative analysis of different pooling strategies\nin retrieval tasks, we find that mean pooling demonstrates superior context\ncapturing ability after applying coreference resolution. In QA tasks, we\ndiscover that smaller models benefit more from the disambiguation process,\nlikely due to their limited inherent capacity for handling referential\nambiguity. With these findings, this study aims to provide a deeper\nunderstanding of the challenges posed by coreferential complexity in RAG,\nproviding guidance for improving retrieval and generation in\nknowledge-intensive AI applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07847v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07848", "title": "\"So, Tell Me About Your Policy...\": Distillation of interpretable policies from Deep Reinforcement Learning agents", "authors": ["Giovanni Dispoto", "Paolo Bonetti", "Marcello Restelli"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07848v1", "summary": "Recent advances in Reinforcement Learning (RL) largely benefit from the\ninclusion of Deep Neural Networks, boosting the number of novel approaches\nproposed in the field of Deep Reinforcement Learning (DRL). These techniques\ndemonstrate the ability to tackle complex games such as Atari, Go, and other\nreal-world applications, including financial trading. Nevertheless, a\nsignificant challenge emerges from the lack of interpretability, particularly\nwhen attempting to comprehend the underlying patterns learned, the relative\nimportance of the state features, and how they are integrated to generate the\npolicy's output. For this reason, in mission-critical and real-world settings,\nit is often preferred to deploy a simpler and more interpretable algorithm,\nalthough at the cost of performance. In this paper, we propose a novel\nalgorithm, supported by theoretical guarantees, that can extract an\ninterpretable policy (e.g., a linear policy) without disregarding the\npeculiarities of expert behavior. This result is obtained by considering the\nadvantage function, which includes information about why an action is superior\nto the others. In contrast to previous works, our approach enables the training\nof an interpretable policy using previously collected experience. The proposed\nalgorithm is empirically evaluated on classic control environments and on a\nfinancial trading scenario, demonstrating its ability to extract meaningful\ninformation from complex expert policies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07848v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07707", "title": "Compressive Imaging Reconstruction via Tensor Decomposed Multi-Resolution Grid Encoding", "authors": ["Zhenyu Jin", "Yisi Luo", "Xile Zhao", "Deyu Meng"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07707v1", "summary": "Compressive imaging (CI) reconstruction, such as snapshot compressive imaging\n(SCI) and compressive sensing magnetic resonance imaging (MRI), aims to recover\nhigh-dimensional images from low-dimensional compressed measurements. This\nprocess critically relies on learning an accurate representation of the\nunderlying high-dimensional image. However, existing unsupervised\nrepresentations may struggle to achieve a desired balance between\nrepresentation ability and efficiency. To overcome this limitation, we propose\nTensor Decomposed multi-resolution Grid encoding (GridTD), an unsupervised\ncontinuous representation framework for CI reconstruction. GridTD optimizes a\nlightweight neural network and the input tensor decomposition model whose\nparameters are learned via multi-resolution hash grid encoding. It inherently\nenjoys the hierarchical modeling ability of multi-resolution grid encoding and\nthe compactness of tensor decomposition, enabling effective and efficient\nreconstruction of high-dimensional images. Theoretical analyses for the\nalgorithm's Lipschitz property, generalization error bound, and fixed-point\nconvergence reveal the intrinsic superiority of GridTD as compared with\nexisting continuous representation models. Extensive experiments across diverse\nCI tasks, including video SCI, spectral SCI, and compressive dynamic MRI\nreconstruction, consistently demonstrate the superiority of GridTD over\nexisting methods, positioning GridTD as a versatile and state-of-the-art CI\nreconstruction method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07707v1", "cate": "eess.IV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2504.12527", "title": "Analysis of the MICCAI Brain Tumor Segmentation -- Metastases (BraTS-METS) 2025 Lighthouse Challenge: Brain Metastasis Segmentation on Pre- and Post-treatment MRI", "authors": ["Nazanin Maleki", "Raisa Amiruddin", "Ahmed W. Moawad", "Nikolay Yordanov", "Athanasios Gkampenis", "Pascal Fehringer", "Fabian Umeh", "Crystal Chukwurah", "Fatima Memon", "Bojan Petrovic", "Justin Cramer", "Mark Krycia", "Elizabeth B. Shrickel", "Ichiro Ikuta", "Gerard Thompson", "Lorenna Vidal", "Vilma Kosovic", "Adam E. Goldman-Yassen", "Virginia Hill", "Tiffany So", "Sedra Mhana", "Albara Alotaibi", "Nathan Page", "Prisha Bhatia", "Melisa S. Guelen", "Yasaman Sharifi", "Marko Jakovljevic", "Salma Abosabie", "Sara Abosabie", "Mohanad Ghonim", "Mohamed Ghonim", "Amirreza Manteghinejad", "Anastasia Janas", "Kiril Krantchev", "Maruf Adewole", "Jake Albrecht", "Udunna Anazodo", "Sanjay Aneja", "Syed Muhammad Anwar", "Timothy Bergquist", "Veronica Chiang", "Verena Chung", "Gian Marco Conte", "Farouk Dako", "James Eddy", "Ivan Ezhov", "Nastaran Khalili", "Keyvan Farahani", "Juan Eugenio Iglesias", "Zhifan Jiang", "Elaine Johanson", "Anahita Fathi Kazerooni", "Florian Kofler", "Dominic LaBella", "Koen Van Leemput", "Hongwei Bran Li", "Marius George Linguraru", "Xinyang Liu", "Zeke Meier", "Bjoern H Menze", "Harrison Moy", "Klara Osenberg", "Marie Piraud", "Zachary Reitman", "Russell Takeshi Shinohara", "Chunhao Wang", "Benedikt Wiestler", "Walter Wiggins", "Umber Shafique", "Klara Willms", "Arman Avesta", "Khaled Bousabarah", "Satrajit Chakrabarty", "Nicolo Gennaro", "Wolfgang Holler", "Manpreet Kaur", "Pamela LaMontagne", "MingDe Lin", "Jan Lost", "Daniel S. Marcus", "Ryan Maresca", "Sarah Merkaj", "Gabriel Cassinelli Pedersen", "Marc von Reppert", "Aristeidis Sotiras", "Oleg Teytelboym", "Niklas Tillmans", "Malte Westerhoff", "Ayda Youssef", "Devon Godfrey", "Scott Floyd", "Andreas Rauschecker", "Javier Villanueva-Meyer", "Irada Pfl√ºger", "Jaeyoung Cho", "Martin Bendszus", "Gianluca Brugnara", "Gloria J. Guzman Perez-Carillo", "Derek R. Johnson", "Anthony Kam", "Benjamin Yin Ming Kwan", "Lillian Lai", "Neil U. Lall", "Satya Narayana Patro", "Lei Wu", "Anu Bansal", "Frederik Barkhof", "Cristina Besada", "Sammy Chu", "Jason Druzgal", "Alexandru Dusoi", "Luciano Farage", "Fabricio Feltrin", "Amy Fong", "Steve H. Fung", "R. Ian Gray", "Michael Iv", "Alida A. Postma", "Amit Mahajan", "David Joyner", "Chase Krumpelman", "Laurent Letourneau-Guillon", "Christie M. Lincoln", "Mate E. Maros", "Elka Miller", "Fanny Mor√≥n", "Esther A. Nimchinsky", "Ozkan Ozsarlak", "Uresh Patel", "Saurabh Rohatgi", "Atin Saha", "Anousheh Sayah", "Eric D. Schwartz", "Robert Shih", "Mark S. Shiroishi", "Juan E. Small", "Manoj Tanwar", "Jewels Valerie", "Brent D. Weinberg", "Matthew L. White", "Robert Young", "Vahe M. Zohrabian", "Aynur Azizova", "Melanie Maria Theresa Br√º√üeler", "Abdullah Okar", "Luca Pasquini", "Yasaman Sharifi", "Gagandeep Singh", "Nico Sollmann", "Theodora Soumala", "Mahsa Taherzadeh", "Philipp Vollmuth", "Martha Foltyn-Dumitru", "Ajay Malhotra", "Francesco Dellepiane", "V√≠ctor M. P√©rez-Garc√≠a", "Hesham Elhalawani", "Maria Correia de Verdier", "Sanaria Al Rubaiey", "Rui Duarte Armindo", "Kholod Ashraf", "Moamen M. Asla", "Mohamed Badawy", "Jeroen Bisschop", "Nima Broomand Lomer", "Jan Bukatz", "Jim Chen", "Petra Cimflova", "Felix Corr", "Alexis Crawley", "Lisa Deptula", "Tasneem Elakhdar", "Islam H. Shawali", "Shahriar Faghani", "Alexandra Frick", "Vaibhav Gulati", "Muhammad Ammar Haider", "F√°tima Hierro", "Rasmus Holmboe Dahl", "Sarah Maria Jacobs", "Kuang-chun Jim Hsieh", "Sedat G. Kandemirli", "Katharina Kersting", "Laura Kida", "Sofia Kollia", "Ioannis Koukoulithras", "Xiao Li", "Ahmed Abouelatta", "Aya Mansour", "Ruxandra-Catrinel Maria-Zamfirescu", "Marcela Marsiglia", "Yohana Sarahi Mateo-Camacho", "Mark McArthur", "Olivia McDonnel", "Maire McHugh", "Mana Moassefi", "Samah Mostafa Morsi", "Alexander Munteanu", "Khanak K. Nandolia", "Syed Raza Naqvi", "Yalda Nikanpour", "Mostafa Alnoury", "Abdullah Mohamed Aly Nouh", "Francesca Pappafava", "Markand D. Patel", "Samantha Petrucci", "Eric Rawie", "Scott Raymond", "Borna Roohani", "Sadeq Sabouhi", "Laura M. Sanchez Garcia", "Zoe Shaked", "Pokhraj P. Suthar", "Talissa Altes", "Edvin Isufi", "Yaseen Dhemesh", "Jaime Gass", "Jonathan Thacker", "Abdul Rahman Tarabishy", "Benjamin Turner", "Sebastiano Vacca", "George K. Vilanilam", "Daniel Warren", "David Weiss", "Fikadu Worede", "Sara Yousry", "Wondwossen Lerebo", "Alejandro Aristizabal", "Alexandros Karargyris", "Hasan Kassem", "Sarthak Pati", "Micah Sheller", "Katherine E. Link", "Evan Calabrese", "Nourel Hoda Tahon", "Ayman Nada", "Jeffrey D. Rudie", "Janet Reid", "Kassa Darge", "Aly H. Abayazeed", "Philipp Lohmann", "Yuri S. Velichko", "Spyridon Bakas", "Mariam Aboian"], "categories": ["q-bio.OT", "eess.IV"], "primary_category": "Subjects:       Other Quantitative Biology (q-bio.OT)", "pdf_link": null, "comments": "Comments:      28 pages, 4 figures, 2 tables", "url": "http://arxiv.org/abs/2504.12527v3", "summary": "Despite continuous advancements in cancer treatment, brain metastatic disease\nremains a significant complication of primary cancer and is associated with an\nunfavorable prognosis. One approach for improving diagnosis, management, and\noutcomes is to implement algorithms based on artificial intelligence for the\nautomated segmentation of both pre- and post-treatment MRI brain images. Such\nalgorithms rely on volumetric criteria for lesion identification and treatment\nresponse assessment, which are still not available in clinical practice.\nTherefore, it is critical to establish tools for rapid volumetric segmentations\nmethods that can be translated to clinical practice and that are trained on\nhigh quality annotated data. The BraTS-METS 2025 Lighthouse Challenge aims to\naddress this critical need by establishing inter-rater and intra-rater\nvariability in dataset annotation by generating high quality annotated datasets\nfrom four individual instances of segmentation by neuroradiologists while being\nrecorded on video (two instances doing \"from scratch\" and two instances after\nAI pre-segmentation). This high-quality annotated dataset will be used for\ntesting phase in 2025 Lighthouse challenge and will be publicly released at the\ncompletion of the challenge. The 2025 Lighthouse challenge will also release\nthe 2023 and 2024 segmented datasets that were annotated using an established\npipeline of pre-segmentation, student annotation, two neuroradiologists\nchecking, and one neuroradiologist finalizing the process. It builds upon its\nprevious edition by including post-treatment cases in the dataset. Using these\nhigh-quality annotated datasets, the 2025 Lighthouse challenge plans to test\nbenchmark algorithms for automated segmentation of pre-and post-treatment brain\nmetastases (BM), trained on diverse and multi-institutional datasets of MRI\nimages obtained from patients with brain metastases.", "comment": "28 pages, 4 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2504.12527v3", "cate": "q-bio.OT", "date": "2025-04-16", "updated": "2025-07-10"}
{"id": "2507.07799", "title": "SecureSpeech: Prompt-based Speaker and Content Protection", "authors": ["Belinda Soh Hui Hui", "Xiaoxiao Miao", "Xin Wang"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Accepted by IEEE International Joint Conference on Biometrics (IJCB) 2025", "url": "http://arxiv.org/abs/2507.07799v1", "summary": "Given the increasing privacy concerns from identity theft and the\nre-identification of speakers through content in the speech field, this paper\nproposes a prompt-based speech generation pipeline that ensures dual\nanonymization of both speaker identity and spoken content. This is addressed\nthrough 1) generating a speaker identity unlinkable to the source speaker,\ncontrolled by descriptors, and 2) replacing sensitive content within the\noriginal text using a name entity recognition model and a large language model.\nThe pipeline utilizes the anonymized speaker identity and text to generate\nhigh-fidelity, privacy-friendly speech via a text-to-speech synthesis model.\nExperimental results demonstrate an achievement of significant privacy\nprotection while maintaining a decent level of content retention and audio\nquality. This paper also investigates the impact of varying speaker\ndescriptions on the utility and privacy of generated speech to determine\npotential biases.", "comment": "Accepted by IEEE International Joint Conference on Biometrics (IJCB)\n  2025", "pdf_url": "http://arxiv.org/pdf/2507.07799v1", "cate": "cs.SD", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07853", "title": "Optimization Guarantees for Square-Root Natural-Gradient Variational Inference", "authors": ["Navish Kumar", "Thomas M√∂llenhoff", "Mohammad Emtiyaz Khan", "Aurelien Lucchi"], "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07853v1", "summary": "Variational inference with natural-gradient descent often shows fast\nconvergence in practice, but its theoretical convergence guarantees have been\nchallenging to establish. This is true even for the simplest cases that involve\nconcave log-likelihoods and use a Gaussian approximation. We show that the\nchallenge can be circumvented for such cases using a square-root\nparameterization for the Gaussian covariance. This approach establishes novel\nconvergence guarantees for natural-gradient variational-Gaussian inference and\nits continuous-time gradient flow. Our experiments demonstrate the\neffectiveness of natural gradient methods and highlight their advantages over\nalgorithms that use Euclidean or Wasserstein geometries.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07853v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07852", "title": "Pre-Trained AI Model Assisted Online Decision-Making under Missing Covariates: A Theoretical Perspective", "authors": ["Haichen Hu", "David Simchi-Levi"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07852v1", "summary": "We study a sequential contextual decision-making problem in which certain\ncovariates are missing but can be imputed using a pre-trained AI model. From a\ntheoretical perspective, we analyze how the presence of such a model influences\nthe regret of the decision-making process. We introduce a novel notion called\n\"model elasticity\", which quantifies the sensitivity of the reward function to\nthe discrepancy between the true covariate and its imputed counterpart. This\nconcept provides a unified way to characterize the regret incurred due to model\nimputation, regardless of the underlying missingness mechanism. More\nsurprisingly, we show that under the missing at random (MAR) setting, it is\npossible to sequentially calibrate the pre-trained model using tools from\northogonal statistical learning and doubly robust regression. This calibration\nsignificantly improves the quality of the imputed covariates, leading to much\nbetter regret guarantees. Our analysis highlights the practical value of having\nan accurate pre-trained model in sequential decision-making tasks and suggests\nthat model elasticity may serve as a fundamental metric for understanding and\nimproving the integration of pre-trained models in a wide range of data-driven\ndecision-making problems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07852v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07708", "title": "Motion-Aware Adaptive Pixel Pruning for Efficient Local Motion Deblurring", "authors": ["Wei Shang", "Dongwei Ren", "Wanying Zhang", "Pengfei Zhu", "Qinghua Hu", "Wangmeng Zuo"], "categories": ["cs.CV", "I.4.3"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ACMMM 2025", "url": "http://arxiv.org/abs/2507.07708v1", "summary": "Local motion blur in digital images originates from the relative motion\nbetween dynamic objects and static imaging systems during exposure. Existing\ndeblurring methods face significant challenges in addressing this problem due\nto their inefficient allocation of computational resources and inadequate\nhandling of spatially varying blur patterns. To overcome these limitations, we\nfirst propose a trainable mask predictor that identifies blurred regions in the\nimage. During training, we employ blur masks to exclude sharp regions. For\ninference optimization, we implement structural reparameterization by\nconverting $3\\times 3$ convolutions to computationally efficient $1\\times 1$\nconvolutions, enabling pixel-level pruning of sharp areas to reduce\ncomputation. Second, we develop an intra-frame motion analyzer that translates\nrelative pixel displacements into motion trajectories, establishing adaptive\nguidance for region-specific blur restoration. Our method is trained end-to-end\nusing a combination of reconstruction loss, reblur loss, and mask loss guided\nby annotated blur masks. Extensive experiments demonstrate superior performance\nover state-of-the-art methods on both local and global blur datasets while\nreducing FLOPs by 49\\% compared to SOTA models (e.g., LMD-ViT). The source code\nis available at https://github.com/shangwei5/M2AENet.", "comment": "Accepted by ACMMM 2025", "pdf_url": "http://arxiv.org/pdf/2507.07708v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07806", "title": "End-to-end Acoustic-linguistic Emotion and Intent Recognition Enhanced by Semi-supervised Learning", "authors": ["Zhao Ren", "Rathi Adarshi Rammohan", "Kevin Scheck", "Sheng Li", "Tanja Schultz"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Accepted by EMBC 2025", "url": "http://arxiv.org/abs/2507.07806v1", "summary": "Emotion and intent recognition from speech is essential and has been widely\ninvestigated in human-computer interaction. The rapid development of social\nmedia platforms, chatbots, and other technologies has led to a large volume of\nspeech data streaming from users. Nevertheless, annotating such data manually\nis expensive, making it challenging to train machine learning models for\nrecognition purposes. To this end, we propose applying semi-supervised learning\nto incorporate a large scale of unlabelled data alongside a relatively smaller\nset of labelled data. We train end-to-end acoustic and linguistic models, each\nemploying multi-task learning for emotion and intent recognition. Two\nsemi-supervised learning approaches, including fix-match learning and\nfull-match learning, are compared. The experimental results demonstrate that\nthe semi-supervised learning approaches improve model performance in speech\nemotion and intent recognition from both acoustic and text data. The late\nfusion of the best models outperforms the acoustic and text baselines by joint\nrecognition balance metrics of 12.3% and 10.4%, respectively.", "comment": "Accepted by EMBC 2025", "pdf_url": "http://arxiv.org/pdf/2507.07806v1", "cate": "cs.SD", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07868", "title": "Alpay Algebra V: Multi-Layered Semantic Games and Transfinite Fixed-Point Simulation", "authors": ["Bugra Kilictas", "Faruk Alpay"], "categories": ["cs.CL", "cs.AI", "68T50, 68T07, 03G30, 18C10", "I.2.7; I.2.6; F.4.1"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      18 pages, 2 figures", "url": "http://arxiv.org/abs/2507.07868v1", "summary": "This paper extends the self-referential framework of Alpay Algebra into a\nmulti-layered semantic game architecture where transfinite fixed-point\nconvergence encompasses hierarchical sub-games at each iteration level.\nBuilding upon Alpay Algebra IV's empathetic embedding concept, we introduce a\nnested game-theoretic structure where the alignment process between AI systems\nand documents becomes a meta-game containing embedded decision problems. We\nformalize this through a composite operator $\\phi(\\cdot, \\gamma(\\cdot))$ where\n$\\phi$ drives the main semantic convergence while $\\gamma$ resolves local\nsub-games. The resulting framework demonstrates that game-theoretic reasoning\nemerges naturally from fixed-point iteration rather than being imposed\nexternally. We prove a Game Theorem establishing existence and uniqueness of\nsemantic equilibria under realistic cognitive simulation assumptions. Our\nverification suite includes adaptations of Banach's fixed-point theorem to\ntransfinite contexts, a novel $\\phi$-topology based on the\nKozlov-Maz'ya-Rossmann formula for handling semantic singularities, and\ncategorical consistency tests via the Yoneda lemma. The paper itself functions\nas a semantic artifact designed to propagate its fixed-point patterns in AI\nembedding spaces -- a deliberate instantiation of the \"semantic virus\" concept\nit theorizes. All results are grounded in category theory, information theory,\nand realistic AI cognition models, ensuring practical applicability beyond pure\nmathematical abstraction.", "comment": "18 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.07868v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07854", "title": "Credit Risk Analysis for SMEs Using Graph Neural Networks in Supply Chain", "authors": ["Zizhou Zhang", "Qinyan Shen", "Zhuohuan Hu", "Qianying Liu", "Huijie Shen"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      The paper will be published on 2025 International Conference on Big Data, Artificial Intelligence and Digital Economy", "url": "http://arxiv.org/abs/2507.07854v1", "summary": "Small and Medium-sized Enterprises (SMEs) are vital to the modern economy,\nyet their credit risk analysis often struggles with scarce data, especially for\nonline lenders lacking direct credit records. This paper introduces a Graph\nNeural Network (GNN)-based framework, leveraging SME interactions from\ntransaction and social data to map spatial dependencies and predict loan\ndefault risks. Tests on real-world datasets from Discover and Ant Credit (23.4M\nnodes for supply chain analysis, 8.6M for default prediction) show the GNN\nsurpasses traditional and other GNN baselines, with AUCs of 0.995 and 0.701 for\nsupply chain mining and default prediction, respectively. It also helps\nregulators model supply chain disruption impacts on banks, accurately\nforecasting loan defaults from material shortages, and offers Federal Reserve\nstress testers key data for CCAR risk buffers. This approach provides a\nscalable, effective tool for assessing SME credit risk.", "comment": "The paper will be published on 2025 International Conference on Big\n  Data, Artificial Intelligence and Digital Economy", "pdf_url": "http://arxiv.org/pdf/2507.07854v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07709", "title": "One Object, Multiple Lies: A Benchmark for Cross-task Adversarial Attack on Unified Vision-Language Models", "authors": ["Jiale Zhao", "Xinyang Jiang", "Junyao Gao", "Yuhao Xue", "Cairong Zhao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07709v1", "summary": "Unified vision-language models(VLMs) have recently shown remarkable progress,\nenabling a single model to flexibly address diverse tasks through different\ninstructions within a shared computational architecture. This instruction-based\ncontrol mechanism creates unique security challenges, as adversarial inputs\nmust remain effective across multiple task instructions that may be\nunpredictably applied to process the same malicious content. In this paper, we\nintroduce CrossVLAD, a new benchmark dataset carefully curated from MSCOCO with\nGPT-4-assisted annotations for systematically evaluating cross-task adversarial\nattacks on unified VLMs. CrossVLAD centers on the object-change\nobjective-consistently manipulating a target object's classification across\nfour downstream tasks-and proposes a novel success rate metric that measures\nsimultaneous misclassification across all tasks, providing a rigorous\nevaluation of adversarial transferability. To tackle this challenge, we present\nCRAFT (Cross-task Region-based Attack Framework with Token-alignment), an\nefficient region-centric attack method. Extensive experiments on Florence-2 and\nother popular unified VLMs demonstrate that our method outperforms existing\napproaches in both overall cross-task attack performance and targeted\nobject-change success rates, highlighting its effectiveness in adversarially\ninfluencing unified VLMs across diverse tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07709v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07867", "title": "Re-Bottleneck: Latent Re-Structuring for Neural Audio Autoencoders", "authors": ["Dimitrios Bralios", "Jonah Casebeer", "Paris Smaragdis"], "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Accepted at IEEE MLSP 2025", "url": "http://arxiv.org/abs/2507.07867v1", "summary": "Neural audio codecs and autoencoders have emerged as versatile models for\naudio compression, transmission, feature-extraction, and latent-space\ngeneration. However, a key limitation is that most are trained to maximize\nreconstruction fidelity, often neglecting the specific latent structure\nnecessary for optimal performance in diverse downstream applications. We\npropose a simple, post-hoc framework to address this by modifying the\nbottleneck of a pre-trained autoencoder. Our method introduces a\n\"Re-Bottleneck\", an inner bottleneck trained exclusively through latent space\nlosses to instill user-defined structure. We demonstrate the framework's\neffectiveness in three experiments. First, we enforce an ordering on latent\nchannels without sacrificing reconstruction quality. Second, we align latents\nwith semantic embeddings, analyzing the impact on downstream diffusion\nmodeling. Third, we introduce equivariance, ensuring that a filtering operation\non the input waveform directly corresponds to a specific transformation in the\nlatent space. Ultimately, our Re-Bottleneck framework offers a flexible and\nefficient way to tailor representations of neural audio models, enabling them\nto seamlessly meet the varied demands of different applications with minimal\nadditional training.", "comment": "Accepted at IEEE MLSP 2025", "pdf_url": "http://arxiv.org/pdf/2507.07867v1", "cate": "cs.SD", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07524", "title": "Finding One Local Optimum Is Easy -- But What about Two?", "authors": ["Yasuaki Kobayashi", "Kazuhiro Kurita", "Yutaro Yamaguchi"], "categories": ["cs.DS", "cs.CC"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      15 pages", "url": "http://arxiv.org/abs/2507.07524v1", "summary": "The class PLS (Polynomial Local Search) captures the complexity of finding a\nsolution that is locally optimal and has proven to be an important concept in\nthe theory of local search. It has been shown that local search versions of\nvarious combinatorial optimization problems, such as Maximum Independent Set\nand Max Cut, are complete for this class. Such computational intractability\ntypically arises in local search problems allowing arbitrary weights; in\ncontrast, for unweighted problems, locally optimal solutions can be found in\npolynomial time under standard settings. In this paper, we pursue the\ncomplexity of local search problems from a different angle: We show that\ncomputing two locally optimal solutions is NP-hard for various natural\nunweighted local search problems, including Maximum Independent Set, Minimum\nDominating Set, Max SAT, and Max Cut. We also discuss several tractable cases\nfor finding two (or more) local optimal solutions.", "comment": "15 pages", "pdf_url": "http://arxiv.org/pdf/2507.07524v1", "cate": "cs.DS", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07885", "title": "UnIT: Scalable Unstructured Inference-Time Pruning for MAC-efficient Neural Inference on MCUs", "authors": ["Ashe Neth", "Sawinder kaur", "Mohammad Nur Hossain Khan", "Subrata Biswas", "Asif Salekin", "Bashima Islam"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Submitted to SenSys 2026 on July 1, 2025", "url": "http://arxiv.org/abs/2507.07885v1", "summary": "Existing pruning methods are typically applied during training or compile\ntime and often rely on structured sparsity. While compatible with low-power\nmicrocontrollers (MCUs), structured pruning underutilizes the opportunity for\nfine-grained efficiency on devices without SIMD support or parallel compute. To\naddress these limitations, we introduce UnIT (Unstructured Inference-Time\npruning), a lightweight method that dynamically identifies and skips\nunnecessary multiply-accumulate (MAC) operations during inference, guided by\ninput-specific activation patterns. Unlike structured pruning, UnIT embraces\nirregular sparsity and does not require retraining or hardware specialization.\nIt transforms pruning decisions into lightweight comparisons, replacing\nmultiplications with threshold checks and approximated divisions. UnIT further\noptimizes compute by reusing threshold computations across multiple connections\nand applying layer- and group-specific pruning sensitivity. We present three\nfast, hardware-friendly division approximations tailored to the capabilities of\ncommon embedded platforms. Demonstrated on the MSP430 microcontroller, UnIT\nachieves 11.02% to 82.03% MAC reduction, 27.30% to 84.19% faster inference, and\n27.33% to 84.38% lower energy consumption compared to training-time pruned\nmodels, while maintaining accuracy with 0.48-7%. Under domain shift, UnIT\nmatches or exceeds the accuracy of retrained models while requiring\nsignificantly fewer MACs. These results establish unstructured inference-time\npruning as a viable and practical solution for efficient, retraining-free\ndeployment of deep neural networks on MCUs.", "comment": "Submitted to SenSys 2026 on July 1, 2025", "pdf_url": "http://arxiv.org/pdf/2507.07885v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07855", "title": "Principled Foundations for Preference Optimization", "authors": ["Wenxuan Zhou", "Shujian Zhang", "Brice Magdalou", "John Lambert", "Ehsan Amid", "Richard Nock", "Andrew Hard"], "categories": ["cs.LG", "I.2.6; I.2.7"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07855v1", "summary": "In this paper, we show that direct preference optimization (DPO) is a very\nspecific form of a connection between two major theories in the ML context of\nlearning from preferences: loss functions (Savage) and stochastic choice\n(Doignon-Falmagne and Machina). The connection is established for all of\nSavage's losses and at this level of generality, (i) it includes support for\nabstention on the choice theory side, (ii) it includes support for non-convex\nobjectives on the ML side, and (iii) it allows to frame for free some notable\nextensions of the DPO setting, including margins and corrections for length.\nGetting to understand how DPO operates from a general principled perspective is\ncrucial because of the huge and diverse application landscape of models,\nbecause of the current momentum around DPO, but also -- and importantly --\nbecause many state of the art variations on DPO definitely occupy a small\nregion of the map that we cover. It also helps to understand the pitfalls of\ndeparting from this map, and figure out workarounds.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07855v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07721", "title": "Breast Ultrasound Tumor Generation via Mask Generator and Text-Guided Network:A Clinically Controllable Framework with Downstream Evaluation", "authors": ["Haoyu Pan", "Hongxin Lin", "Zetian Feng", "Chuxuan Lin", "Junyang Mo", "Chu Zhang", "Zijian Wu", "Yi Wang", "Qingqing Zheng"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      11 pages, 6 figures", "url": "http://arxiv.org/abs/2507.07721v1", "summary": "The development of robust deep learning models for breast ultrasound (BUS)\nimage analysis is significantly constrained by the scarcity of expert-annotated\ndata. To address this limitation, we propose a clinically controllable\ngenerative framework for synthesizing BUS images. This framework integrates\nclinical descriptions with structural masks to generate tumors, enabling\nfine-grained control over tumor characteristics such as morphology,\nechogencity, and shape. Furthermore, we design a semantic-curvature mask\ngenerator, which synthesizes structurally diverse tumor masks guided by\nclinical priors. During inference, synthetic tumor masks serve as input to the\ngenerative framework, producing highly personalized synthetic BUS images with\ntumors that reflect real-world morphological diversity. Quantitative\nevaluations on six public BUS datasets demonstrate the significant clinical\nutility of our synthetic images, showing their effectiveness in enhancing\ndownstream breast cancer diagnosis tasks. Furthermore, visual Turing tests\nconducted by experienced sonographers confirm the realism of the generated\nimages, indicating the framework's potential to support broader clinical\napplications.", "comment": "11 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.07721v1", "cate": "eess.IV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07877", "title": "Edge-ASR: Towards Low-Bit Quantization of Automatic Speech Recognition Models", "authors": ["Chen Feng", "Yicheng Lin", "Shaojie Zhuo", "Chenzheng Su", "Ramchalam Kinattinkara Ramakrishnan", "Zhaocong Yuan", "Xiaopeng Zhang"], "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07877v1", "summary": "Recent advances in Automatic Speech Recognition (ASR) have demonstrated\nremarkable accuracy and robustness in diverse audio applications, such as live\ntranscription and voice command processing. However, deploying these models on\nresource constrained edge devices (e.g., IoT device, wearables) still presents\nsubstantial challenges due to strict limits on memory, compute and power.\nQuantization, particularly Post-Training Quantization (PTQ), offers an\neffective way to reduce model size and inference cost without retraining.\nDespite its importance, the performance implications of various advanced\nquantization methods and bit-width configurations on ASR models remain unclear.\nIn this work, we present a comprehensive benchmark of eight state-of-the-art\n(SOTA) PTQ methods applied to two leading edge-ASR model families, Whisper and\nMoonshine. We systematically evaluate model performances (i.e., accuracy,\nmemory I/O and bit operations) across seven diverse datasets from the open ASR\nleaderboard, analyzing the impact of quantization and various configurations on\nboth weights and activations. Built on an extension of the LLM compression\ntoolkit, our framework integrates edge-ASR models, diverse advanced\nquantization algorithms, a unified calibration and evaluation data pipeline,\nand detailed analysis tools. Our results characterize the trade-offs between\nefficiency and accuracy, demonstrating that even 3-bit quantization can succeed\non high capacity models when using advanced PTQ techniques. These findings\nprovide valuable insights for optimizing ASR models on low-power, always-on\nedge devices.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07877v1", "cate": "cs.SD", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07528", "title": "On the Complexity of Hyperpath and Minimal Separator Enumeration in Directed Hypergraphs", "authors": ["Kazuhiro Kurita", "Kevin Mann"], "categories": ["cs.DS", "cs.CC"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07528v1", "summary": "In this paper, we address the enumeration of (induced) $s$-$t$ paths and\nminimal $s$-$t$ separators. These problems are some of the most famous\nclassical enumeration problems that can be solved in polynomial delay by simple\nbacktracking for a (un)directed graph. As a generalization of these problems,\nwe consider the (induced) $s$-$t$ hyperpath and minimal $s$-$t$ separator\nenumeration in a \\emph{directed hypergraph}. We show that extending these\nclassical enumeration problems to directed hypergraphs drastically changes\ntheir complexity. More precisely, there are no output-polynomial time\nalgorithms for the enumeration of induced $s$-$t$ hyperpaths and minimal\n$s$-$t$ separators unless $P = NP$, and if there is an output-polynomial time\nalgorithm for the $s$-$t$ hyperpath enumeration, then the minimal transversal\nenumeration can be solved in output polynomial time even if a directed\nhypergraph is $BF$-hypergraph. Since the existence of an output-polynomial time\nalgorithm for the minimal transversal enumeration has remained an open problem\nfor over 45 years, it indicates that the $s$-$t$ hyperpath enumeration for a\n$BF$-hypergraph is not an easy problem. As a positive result, the $s$-$t$\nhyperpath enumeration for a $B$-hypergraph can be solved in polynomial delay by\nbacktracking.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07528v1", "cate": "cs.DS", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07229", "title": "SynthTextEval: Synthetic Text Data Generation and Evaluation for High-Stakes Domains", "authors": ["Krithika Ramesh", "Daniel Smolyak", "Zihao Zhao", "Nupoor Gandhi", "Ritu Agarwal", "Margr√©t Bjarnad√≥ttir", "Anjalie Field"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07229v1", "summary": "We present SynthTextEval, a toolkit for conducting comprehensive evaluations\nof synthetic text. The fluency of large language model (LLM) outputs has made\nsynthetic text potentially viable for numerous applications, such as reducing\nthe risks of privacy violations in the development and deployment of AI systems\nin high-stakes domains. Realizing this potential, however, requires principled\nconsistent evaluations of synthetic data across multiple dimensions: its\nutility in downstream systems, the fairness of these systems, the risk of\nprivacy leakage, general distributional differences from the source text, and\nqualitative feedback from domain experts. SynthTextEval allows users to conduct\nevaluations along all of these dimensions over synthetic data that they upload\nor generate using the toolkit's generation module. While our toolkit can be run\nover any data, we highlight its functionality and effectiveness over datasets\nfrom two high-stakes domains: healthcare and law. By consolidating and\nstandardizing evaluation metrics, we aim to improve the viability of synthetic\ntext, and in-turn, privacy-preservation in AI development.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07229v1", "cate": "cs.CL", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07906", "title": "Agentic Retrieval of Topics and Insights from Earnings Calls", "authors": ["Anant Gupta", "Rajarshi Bhowmik", "Geoffrey Gunow"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      The 2nd Workshop on Financial Information Retrieval in the Era of Generative AI, The 48th International ACM SIGIR Conference on Research and Development in Information Retrieval July 13-17, 2025 | Padua, Italy", "url": "http://arxiv.org/abs/2507.07906v1", "summary": "Tracking the strategic focus of companies through topics in their earnings\ncalls is a key task in financial analysis. However, as industries evolve,\ntraditional topic modeling techniques struggle to dynamically capture emerging\ntopics and their relationships. In this work, we propose an LLM-agent driven\napproach to discover and retrieve emerging topics from quarterly earnings\ncalls. We propose an LLM-agent to extract topics from documents, structure them\ninto a hierarchical ontology, and establish relationships between new and\nexisting topics through a topic ontology. We demonstrate the use of extracted\ntopics to infer company-level insights and emerging trends over time. We\nevaluate our approach by measuring ontology coherence, topic evolution\naccuracy, and its ability to surface emerging financial trends.", "comment": "The 2nd Workshop on Financial Information Retrieval in the Era of\n  Generative AI, The 48th International ACM SIGIR Conference on Research and\n  Development in Information Retrieval July 13-17, 2025 | Padua, Italy", "pdf_url": "http://arxiv.org/pdf/2507.07906v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07862", "title": "Predicting and generating antibiotics against future pathogens with ApexOracle", "authors": ["Tianang Leng", "Fangping Wan", "Marcelo Der Torossian Torres", "Cesar de la Fuente-Nunez"], "categories": ["cs.LG", "q-bio.QM"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      3 figures", "url": "http://arxiv.org/abs/2507.07862v1", "summary": "Antimicrobial resistance (AMR) is escalating and outpacing current antibiotic\ndevelopment. Thus, discovering antibiotics effective against emerging pathogens\nis becoming increasingly critical. However, existing approaches cannot rapidly\nidentify effective molecules against novel pathogens or emerging drug-resistant\nstrains. Here, we introduce ApexOracle, an artificial intelligence (AI) model\nthat both predicts the antibacterial potency of existing compounds and designs\nde novo molecules active against strains it has never encountered. Departing\nfrom models that rely solely on molecular features, ApexOracle incorporates\npathogen-specific context through the integration of molecular features\ncaptured via a foundational discrete diffusion language model and a\ndual-embedding framework that combines genomic- and literature-derived strain\nrepresentations. Across diverse bacterial species and chemical modalities,\nApexOracle consistently outperformed state-of-the-art approaches in activity\nprediction and demonstrated reliable transferability to novel pathogens with\nlittle or no antimicrobial data. Its unified representation-generation\narchitecture further enables the in silico creation of \"new-to-nature\"\nmolecules with high predicted efficacy against priority threats. By pairing\nrapid activity prediction with targeted molecular generation, ApexOracle offers\na scalable strategy for countering AMR and preparing for future\ninfectious-disease outbreaks.", "comment": "3 figures", "pdf_url": "http://arxiv.org/pdf/2507.07862v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07722", "title": "Understanding Dataset Bias in Medical Imaging: A Case Study on Chest X-rays", "authors": ["Ethan Dack", "Chengliang Dai"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07722v1", "summary": "Recent work has revisited the infamous task Name that dataset and established\nthat in non-medical datasets, there is an underlying bias and achieved high\nAccuracies on the dataset origin task. In this work, we revisit the same task\napplied to popular open-source chest X-ray datasets. Medical images are\nnaturally more difficult to release for open-source due to their sensitive\nnature, which has led to certain open-source datasets being extremely popular\nfor research purposes. By performing the same task, we wish to explore whether\ndataset bias also exists in these datasets. % We deliberately try to increase\nthe difficulty of the task by dataset transformations. We apply simple\ntransformations of the datasets to try to identify bias. Given the importance\nof AI applications in medical imaging, it's vital to establish whether modern\nmethods are taking shortcuts or are focused on the relevant pathology. We\nimplement a range of different network architectures on the datasets: NIH,\nCheXpert, MIMIC-CXR and PadChest. We hope this work will encourage more\nexplainable research being performed in medical imaging and the creation of\nmore open-source datasets in the medical domain. The corresponding code will be\nreleased upon acceptance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07722v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07879", "title": "LISTEN: Lightweight Industrial Sound-representable Transformer for Edge Notification", "authors": ["Changheon Han", "Yun Seok Kang", "Yuseop Sim", "Martin Byung-Guk Jun", "Hyung Wook Park"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07879v1", "summary": "Deep learning-based machine listening is broadening the scope of industrial\nacoustic analysis for applications like anomaly detection and predictive\nmaintenance, thereby improving manufacturing efficiency and reliability.\nNevertheless, its reliance on large, task-specific annotated datasets for every\nnew task limits widespread implementation on shop floors. While emerging sound\nfoundation models aim to alleviate data dependency, they are too large and\ncomputationally expensive, requiring cloud infrastructure or high-end hardware\nthat is impractical for on-site, real-time deployment. We address this gap with\nLISTEN (Lightweight Industrial Sound-representable Transformer for Edge\nNotification), a kilobyte-sized industrial sound foundation model. Using\nknowledge distillation, LISTEN runs in real-time on low-cost edge devices. On\nbenchmark downstream tasks, it performs nearly identically to its much larger\nparent model, even when fine-tuned with minimal datasets and training resource.\nBeyond the model itself, we demonstrate its real-world utility by integrating\nLISTEN into a complete machine monitoring framework on an edge device with an\nIndustrial Internet of Things (IIoT) sensor and system, validating its\nperformance and generalization capabilities on a live manufacturing shop floor.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07879v1", "cate": "cs.SD", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07943", "title": "A Randomized Rounding Approach for DAG Edge Deletion", "authors": ["Sina Kalantarzadeh", "Nathan Klein", "Victor Reis"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07943v1", "summary": "In the DAG Edge Deletion problem, we are given an edge-weighted directed\nacyclic graph and a parameter $k$, and the goal is to delete the minimum weight\nset of edges so that the resulting graph has no paths of length $k$. This\nproblem, which has applications to scheduling, was introduced in 2015 by\nKenkre, Pandit, Purohit, and Saket. They gave a $k$-approximation and showed\nthat it is UGC-Hard to approximate better than $\\lfloor 0.5k \\rfloor$ for any\nconstant $k \\ge 4$ using a work of Svensson from 2012. The approximation ratio\nwas improved to $\\frac{2}{3}(k+1)$ by Klein and Wexler in 2016.\n  In this work, we introduce a randomized rounding framework based on\ndistributions over vertex labels in $[0,1]$. The most natural distribution is\nto sample labels independently from the uniform distribution over $[0,1]$. We\nshow this leads to a $(2-\\sqrt{2})(k+1) \\approx 0.585(k+1)$-approximation. By\nusing a modified (but still independent) label distribution, we obtain a\n$0.549(k+1)$-approximation for the problem, as well as show that no independent\ndistribution over labels can improve our analysis to below $0.542(k+1)$.\nFinally, we show a $0.5(k+1)$-approximation for bipartite graphs and for\ninstances with structured LP solutions. Whether this ratio can be obtained in\ngeneral is open.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07943v1", "cate": "cs.DS", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07248", "title": "Medical Red Teaming Protocol of Language Models: On the Importance of User Perspectives in Healthcare Settings", "authors": ["Minseon Kim", "Jean-Philippe Corbeil", "Alessandro Sordoni", "Francois Beaulieu", "Paul Vozila"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07248v1", "summary": "As the performance of large language models (LLMs) continues to advance,\ntheir adoption is expanding across a wide range of domains, including the\nmedical field. The integration of LLMs into medical applications raises\ncritical safety concerns, particularly due to their use by users with diverse\nroles, e.g. patients and clinicians, and the potential for model's outputs to\ndirectly affect human health. Despite the domain-specific capabilities of\nmedical LLMs, prior safety evaluations have largely focused only on general\nsafety benchmarks. In this paper, we introduce a safety evaluation protocol\ntailored to the medical domain in both patient user and clinician user\nperspectives, alongside general safety assessments and quantitatively analyze\nthe safety of medical LLMs. We bridge a gap in the literature by building the\nPatientSafetyBench containing 466 samples over 5 critical categories to measure\nsafety from the perspective of the patient. We apply our red-teaming protocols\non the MediPhi model collection as a case study. To our knowledge, this is the\nfirst work to define safety evaluation criteria for medical LLMs through\ntargeted red-teaming taking three different points of view - patient,\nclinician, and general user - establishing a foundation for safer deployment in\nmedical domains.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07248v1", "cate": "cs.CL", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07910", "title": "DTECT: Dynamic Topic Explorer & Context Tracker", "authors": ["Suman Adhya", "Debarshi Kumar Sanyal"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Code: this https URL | Demo: this https URL | Video: this https URL", "url": "http://arxiv.org/abs/2507.07910v1", "summary": "The explosive growth of textual data over time presents a significant\nchallenge in uncovering evolving themes and trends. Existing dynamic topic\nmodeling techniques, while powerful, often exist in fragmented pipelines that\nlack robust support for interpretation and user-friendly exploration. We\nintroduce DTECT (Dynamic Topic Explorer & Context Tracker), an end-to-end\nsystem that bridges the gap between raw textual data and meaningful temporal\ninsights. DTECT provides a unified workflow that supports data preprocessing,\nmultiple model architectures, and dedicated evaluation metrics to analyze the\ntopic quality of temporal topic models. It significantly enhances\ninterpretability by introducing LLM-driven automatic topic labeling, trend\nanalysis via temporally salient words, interactive visualizations with\ndocument-level summarization, and a natural language chat interface for\nintuitive data querying. By integrating these features into a single, cohesive\nplatform, DTECT empowers users to more effectively track and understand\nthematic dynamics. DTECT is open-source and available at\nhttps://github.com/AdhyaSuman/DTECT.", "comment": "Code: https://github.com/AdhyaSuman/DTECT | Demo:\n  https://huggingface.co/spaces/AdhyaSuman/DTECT | Video:\n  https://youtu.be/B8nNfxFoJAU", "pdf_url": "http://arxiv.org/pdf/2507.07910v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07882", "title": "Can AI-predicted complexes teach machine learning to compute drug binding affinity?", "authors": ["Wei-Tse Hsu", "Savva Grevtsev", "Thomas Douglas", "Aniket Magarkar", "Philip C. Biggin"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07882v1", "summary": "We evaluate the feasibility of using co-folding models for synthetic data\naugmentation in training machine learning-based scoring functions (MLSFs) for\nbinding affinity prediction. Our results show that performance gains depend\ncritically on the structural quality of augmented data. In light of this, we\nestablished simple heuristics for identifying high-quality co-folding\npredictions without reference structures, enabling them to substitute for\nexperimental structures in MLSF training. Our study informs future data\naugmentation strategies based on co-folding models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07882v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07730", "title": "RAPS-3D: Efficient interactive segmentation for 3D radiological imaging", "authors": ["Th√©o Danielou", "Daniel Tordjman", "Pierre Manceron", "Corentin Dancette"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Abstract accepted at MIUA 2025", "url": "http://arxiv.org/abs/2507.07730v1", "summary": "Promptable segmentation, introduced by the Segment Anything Model (SAM), is a\npromising approach for medical imaging, as it enables clinicians to guide and\nrefine model predictions interactively. However, SAM's architecture is designed\nfor 2D images and does not extend naturally to 3D volumetric data such as CT or\nMRI scans. Adapting 2D models to 3D typically involves autoregressive\nstrategies, where predictions are propagated slice by slice, resulting in\nincreased inference complexity. Processing large 3D volumes also requires\nsignificant computational resources, often leading existing 3D methods to also\nadopt complex strategies like sliding-window inference to manage memory usage,\nat the cost of longer inference times and greater implementation complexity. In\nthis paper, we present a simplified 3D promptable segmentation method, inspired\nby SegVol, designed to reduce inference time and eliminate prompt management\ncomplexities associated with sliding windows while achieving state-of-the-art\nperformance.", "comment": "Abstract accepted at MIUA 2025", "pdf_url": "http://arxiv.org/pdf/2507.07730v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07954", "title": "Input Conditioned Layer Dropping in Speech Foundation Models", "authors": ["Abdul Hannan", "Daniele Falavigna", "Alessio Brutti"], "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Accepted at IEEE MLSP 2025", "url": "http://arxiv.org/abs/2507.07954v1", "summary": "Curating foundation speech models for edge and IoT settings, where\ncomputational resources vary over time, requires dynamic architectures\nfeaturing adaptable reduction strategies. One emerging approach is layer\ndropping ($\\mathcal{LD}$) which skips fraction of the layers of a backbone\nnetwork during inference to reduce the computational load. This allows\ntransforming static models into dynamic ones. However, existing approaches\nexhibit limitations either in the mode of selecting layers or by significantly\nmodifying the neural architecture. To this end, we propose input-driven\n$\\mathcal{LD}$ that employs the network's input features and a lightweight\nlayer selecting network to determine the optimum combination of processing\nlayers. Extensive experimentation on 4 speech and audio public benchmarks,\nusing two different pre-trained foundation models, demonstrates the\neffectiveness of our approach, thoroughly outperforming random dropping and\nproducing on-par (or better) results to early exit.", "comment": "Accepted at IEEE MLSP 2025", "pdf_url": "http://arxiv.org/pdf/2507.07954v1", "cate": "cs.SD", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07975", "title": "Finding sparse induced subgraphs on graphs of bounded induced matching treewidth", "authors": ["Hans L. Bodlaender", "Fedor V. Fomin", "Tuukka Korhonen"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      31 pages", "url": "http://arxiv.org/abs/2507.07975v1", "summary": "The induced matching width of a tree decomposition of a graph $G$ is the\ncardinality of a largest induced matching $M$ of $G$, such that there exists a\nbag that intersects every edge in $M$. The induced matching treewidth of a\ngraph $G$, denoted by $\\mathsf{tree-}\\mu(G)$, is the minimum induced matching\nwidth of a tree decomposition of $G$. The parameter $\\mathsf{tree-}\\mu$ was\nintroduced by Yolov [SODA '18], who showed that, for example, Maximum-Weight\nIndependent Set can be solved in polynomial-time on graphs of bounded\n$\\mathsf{tree-}\\mu$. Lima, Milani\\v{c}, Mur\\v{s}i\\v{c}, Okrasa,\nRz\\k{a}\\.zewski, and \\v{S}torgel [ESA '24] conjectured that this algorithm can\nbe generalized to a meta-problem called Maximum-Weight Induced Subgraph of\nBounded Treewidth, where we are given a vertex-weighted graph $G$, an integer\n$w$, and a $\\mathsf{CMSO}_2$-sentence $\\Phi$, and are asked to find a\nmaximum-weight set $X \\subseteq V(G)$ so that $G[X]$ has treewidth at most $w$\nand satisfies $\\Phi$. They proved the conjecture for some special cases, such\nas for the problem Maximum-Weight Induced Forest.\n  In this paper, we prove the general case of the conjecture. In particular, we\nshow that Maximum-Weight Induced Subgraph of Bounded Treewidth is\npolynomial-time solvable when $\\mathsf{tree-}\\mu(G)$, $w$, and $|\\Phi|$ are\nbounded. The running time of our algorithm for $n$-vertex graphs $G$ with\n$\\mathsf{tree} - \\mu(G) \\le k$ is $f(k, w, |\\Phi|) \\cdot n^{O(k w^2)}$ for a\ncomputable function $f$.", "comment": "31 pages", "pdf_url": "http://arxiv.org/pdf/2507.07975v1", "cate": "cs.DS", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07280", "title": "The Impact of Background Speech on Interruption Detection in Collaborative Groups", "authors": ["Mariah Bradford", "Nikhil Krishnaswamy", "Nathaniel Blanchard"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Long Paper AIED 2025", "url": "http://arxiv.org/abs/2507.07280v1", "summary": "Interruption plays a crucial role in collaborative learning, shaping group\ninteractions and influencing knowledge construction. AI-driven support can\nassist teachers in monitoring these interactions. However, most previous work\non interruption detection and interpretation has been conducted in\nsingle-conversation environments with relatively clean audio. AI agents\ndeployed in classrooms for collaborative learning within small groups will need\nto contend with multiple concurrent conversations -- in this context,\noverlapping speech will be ubiquitous, and interruptions will need to be\nidentified in other ways. In this work, we analyze interruption detection in\nsingle-conversation and multi-group dialogue settings. We then create a\nstate-of-the-art method for interruption identification that is robust to\noverlapping speech, and thus could be deployed in classrooms. Further, our work\nhighlights meaningful linguistic and prosodic information about how\ninterruptions manifest in collaborative group interactions. Our investigation\nalso paves the way for future works to account for the influence of overlapping\nspeech from multiple groups when tracking group dialog.", "comment": "Long Paper AIED 2025", "pdf_url": "http://arxiv.org/pdf/2507.07280v1", "cate": "cs.CL", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07929", "title": "Towards Continuous Home Cage Monitoring: An Evaluation of Tracking and Identification Strategies for Laboratory Mice", "authors": ["Juan Pablo Oberhauser", "Daniel Grzenda"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07929v1", "summary": "Continuous, automated monitoring of laboratory mice enables more accurate\ndata collection and improves animal welfare through real-time insights.\nResearchers can achieve a more dynamic and clinically relevant characterization\nof disease progression and therapeutic effects by integrating behavioral and\nphysiological monitoring in the home cage. However, providing individual mouse\nmetrics is difficult because of their housing density, similar appearances,\nhigh mobility, and frequent interactions. To address these challenges, we\ndevelop a real-time identification (ID) algorithm that accurately assigns ID\npredictions to mice wearing custom ear tags in digital home cages monitored by\ncameras. Our pipeline consists of three parts: (1) a custom multiple object\ntracker (MouseTracks) that combines appearance and motion cues from mice; (2) a\ntransformer-based ID classifier (Mouseformer); and (3) a tracklet associator\nlinear program to assign final ID predictions to tracklets (MouseMap). Our\nmodels assign an animal ID based on custom ear tags at 30 frames per second\nwith 24/7 cage coverage. We show that our custom tracking and ID pipeline\nimproves tracking efficiency and lowers ID switches across mouse strains and\nvarious environmental factors compared to current mouse tracking methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07929v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07883", "title": "SAMO: A Lightweight Sharpness-Aware Approach for Multi-Task Optimization with Joint Global-Local Perturbation", "authors": ["Hao Ban", "Gokul Ram Subramani", "Kaiyi Ji"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07883v1", "summary": "Multi-task learning (MTL) enables a joint model to capture commonalities\nacross multiple tasks, reducing computation costs and improving data\nefficiency. However, a major challenge in MTL optimization is task conflicts,\nwhere the task gradients differ in direction or magnitude, limiting model\nperformance compared to single-task counterparts. Sharpness-aware minimization\n(SAM) minimizes task loss while simultaneously reducing the sharpness of the\nloss landscape. Our empirical observations show that SAM effectively mitigates\ntask conflicts in MTL. Motivated by these findings, we explore integrating SAM\ninto MTL but face two key challenges. While both the average loss gradient and\nindividual task gradients-referred to as global and local\ninformation-contribute to SAM, how to combine them remains unclear. Moreover,\ndirectly computing each task gradient introduces significant computational and\nmemory overheads. To address these challenges, we propose SAMO, a lightweight\n\\textbf{S}harpness-\\textbf{A}ware \\textbf{M}ulti-task \\textbf{O}ptimization\napproach, that leverages a joint global-local perturbation. The local\nperturbations are approximated using only forward passes and are layerwise\nnormalized to improve efficiency. Extensive experiments on a suite of\nmulti-task benchmarks demonstrate both the effectiveness and efficiency of our\nmethod. Code is available at https://github.com/OptMN-Lab/SAMO.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07883v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07731", "title": "Energy-Guided Decoding for Object Hallucination Mitigation", "authors": ["Xixi Liu", "Ailin Deng", "Christopher Zach"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07731v1", "summary": "Mitigating object hallucination in large vision-language models (LVLMs) is\ncritical to their safe deployment. Existing methods either are restricted to\nspecific decoding methods, or demand sophisticated modifications to visual\ninputs, or rely on knowledge from external models. In this work, we first\nreveal the phenomenon that VLMs exhibit significant imbalance in the ``Yes''\nratio ( \\ie, the fraction of ``Yes'' answers among the total number of\nquestions) across three different visual question answering (VQA) datasets.\nFurthermore, we propose an energy-based decoding method, which dynamically\nselects the hidden states from the layer with minimal energy score. It is\nsimple yet effective in reducing the bias for the yes ratio while boosting\nperformance across three benchmarks (POPE, MME, and MMVP). Our method\nconsistently improves accuracy and F1 score on three VQA datasets across three\ncommonly used VLMs over several baseline methods. The average accuracy\nimprovement is 4.82% compared to greedy decoding. Moreover, the average\nyes-ratio gap reduction is 8.81%, meaning the proposed method is less biased as\nshown in Figure 1.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07731v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2505.04382", "title": "Discrete Optimal Transport and Voice Conversion", "authors": ["Anton Selitskiy", "Maitreya Kocharekar"], "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      4 pages, 6 figures, 1 table", "url": "http://arxiv.org/abs/2505.04382v2", "summary": "In this work, we address the voice conversion (VC) task using a vector-based\ninterface. To align audio embeddings between speakers, we employ discrete\noptimal transport mapping. Our evaluation results demonstrate the high quality\nand effectiveness of this method. Additionally, we show that applying discrete\noptimal transport as a post-processing step in audio generation can lead to the\nincorrect classification of synthetic audio as real.", "comment": "4 pages, 6 figures, 1 table", "pdf_url": "http://arxiv.org/pdf/2505.04382v2", "cate": "eess.AS", "date": "2025-05-07", "updated": "2025-07-10"}
{"id": "2307.00115", "title": "A simpler and parallelizable $O(\\sqrt{\\log n})$-approximation algorithm for Sparsest Cut", "authors": ["Vladimir Kolmogorov"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      Accepted to Transactions on Algorithms (TALG). Preliminary version appeared in ACM Symposium on Parallelism in Algorithms and Architectures (SPAA 2024)", "url": "http://arxiv.org/abs/2307.00115v5", "summary": "Currently, the best known tradeoff between approximation ratio and complexity\nfor the Sparsest Cut problem is achieved by the algorithm in [Sherman, FOCS\n2009]: it computes $O(\\sqrt{(\\log n)/\\varepsilon})$-approximation using\n$O(n^\\varepsilon\\log^{O(1)}n)$ maxflows for any $\\varepsilon\\in[\\Theta(1/\\log\nn),\\Theta(1)]$. It works by solving the SDP relaxation of [Arora-Rao-Vazirani,\nSTOC 2004] using the Multiplicative Weights Update algorithm (MW) of\n[Arora-Kale, JACM 2016]. To implement one MW step, Sherman approximately solves\na multicommodity flow problem using another application of MW. Nested MW steps\nare solved via a certain ``chaining'' algorithm that combines results of\nmultiple calls to the maxflow algorithm. We present an alternative approach\nthat avoids solving the multicommodity flow problem and instead computes\n``violating paths''. This simplifies Sherman's algorithm by removing a need for\na nested application of MW, and also allows parallelization: we show how to\ncompute $O(\\sqrt{(\\log n)/\\varepsilon})$-approximation via $O(\\log^{O(1)}n)$\nmaxflows using $O(n^\\varepsilon)$ processors. We also revisit Sherman's\nchaining algorithm, and present a simpler version together with a new analysis.", "comment": "Accepted to Transactions on Algorithms (TALG). Preliminary version\n  appeared in ACM Symposium on Parallelism in Algorithms and Architectures\n  (SPAA 2024)", "pdf_url": "http://arxiv.org/pdf/2307.00115v5", "cate": "cs.DS", "date": "2023-06-30", "updated": "2025-07-10"}
{"id": "2507.07307", "title": "Multi-Agent Retrieval-Augmented Framework for Evidence-Based Counterspeech Against Health Misinformation", "authors": ["Anirban Saha Anik", "Xiaoying Song", "Elliott Wang", "Bryan Wang", "Bengisu Yarimbas", "Lingzi Hong"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07307v1", "summary": "Large language models (LLMs) incorporated with Retrieval-Augmented Generation\n(RAG) have demonstrated powerful capabilities in generating counterspeech\nagainst misinformation. However, current studies rely on limited evidence and\noffer less control over final outputs. To address these challenges, we propose\na Multi-agent Retrieval-Augmented Framework to generate counterspeech against\nhealth misinformation, incorporating multiple LLMs to optimize knowledge\nretrieval, evidence enhancement, and response refinement. Our approach\nintegrates both static and dynamic evidence, ensuring that the generated\ncounterspeech is relevant, well-grounded, and up-to-date. Our method\noutperforms baseline approaches in politeness, relevance, informativeness, and\nfactual accuracy, demonstrating its effectiveness in generating high-quality\ncounterspeech. To further validate our approach, we conduct ablation studies to\nverify the necessity of each component in our framework. Furthermore, human\nevaluations reveal that refinement significantly enhances counterspeech quality\nand obtains human preference.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07307v1", "cate": "cs.CL", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07136", "title": "LangSplatV2: High-dimensional 3D Language Gaussian Splatting with 450+ FPS", "authors": ["Wanhua Li", "Yujie Zhao", "Minghan Qin", "Yang Liu", "Yuanhao Cai", "Chuang Gan", "Hanspeter Pfister"], "categories": ["cs.GR"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      Project Page: this https URL", "url": "http://arxiv.org/abs/2507.07136v1", "summary": "In this paper, we introduce LangSplatV2, which achieves high-dimensional\nfeature splatting at 476.2 FPS and 3D open-vocabulary text querying at 384.6\nFPS for high-resolution images, providing a 42 $\\times$ speedup and a 47\n$\\times$ boost over LangSplat respectively, along with improved query accuracy.\nLangSplat employs Gaussian Splatting to embed 2D CLIP language features into\n3D, significantly enhancing speed and learning a precise 3D language field with\nSAM semantics. Such advancements in 3D language fields are crucial for\napplications that require language interaction within complex scenes. However,\nLangSplat does not yet achieve real-time inference performance (8.2 FPS), even\nwith advanced A100 GPUs, severely limiting its broader application. In this\npaper, we first conduct a detailed time analysis of LangSplat, identifying the\nheavyweight decoder as the primary speed bottleneck. Our solution, LangSplatV2\nassumes that each Gaussian acts as a sparse code within a global dictionary,\nleading to the learning of a 3D sparse coefficient field that entirely\neliminates the need for a heavyweight decoder. By leveraging this sparsity, we\nfurther propose an efficient sparse coefficient splatting method with CUDA\noptimization, rendering high-dimensional feature maps at high quality while\nincurring only the time cost of splatting an ultra-low-dimensional feature. Our\nexperimental results demonstrate that LangSplatV2 not only achieves better or\ncompetitive query accuracy but is also significantly faster. Codes and demos\nare available at our project page: https://langsplat-v2.github.io.", "comment": "Project Page: https://langsplat-v2.github.io", "pdf_url": "http://arxiv.org/pdf/2507.07136v1", "cate": "cs.GR", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07947", "title": "Low Resource Reconstruction Attacks Through Benign Prompts", "authors": ["Sol Yarkoni", "Roi Livni"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07947v1", "summary": "The recent advances in generative models such as diffusion models have raised\nseveral risks and concerns related to privacy, copyright infringements and data\nstewardship. To better understand and control the risks, various researchers\nhave created techniques, experiments and attacks that reconstruct images, or\npart of images, from the training set. While these techniques already establish\nthat data from the training set can be reconstructed, they often rely on\nhigh-resources, excess to the training set as well as well-engineered and\ndesigned prompts.\n  In this work, we devise a new attack that requires low resources, assumes\nlittle to no access to the actual training set, and identifies, seemingly,\nbenign prompts that lead to potentially-risky image reconstruction. This\nhighlights the risk that images might even be reconstructed by an uninformed\nuser and unintentionally. For example, we identified that, with regard to one\nexisting model, the prompt ``blue Unisex T-Shirt'' can generate the face of a\nreal-life human model. Our method builds on an intuition from previous works\nwhich leverages domain knowledge and identifies a fundamental vulnerability\nthat stems from the use of scraped data from e-commerce platforms, where\ntemplated layouts and images are tied to pattern-like prompts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07947v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07898", "title": "Efficient Causal Discovery for Autoregressive Time Series", "authors": ["Mohammad Fesanghary", "Achintya Gopal"], "categories": ["cs.LG", "stat.AP"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      10 pages, 8 figures", "url": "http://arxiv.org/abs/2507.07898v1", "summary": "In this study, we present a novel constraint-based algorithm for causal\nstructure learning specifically designed for nonlinear autoregressive time\nseries. Our algorithm significantly reduces computational complexity compared\nto existing methods, making it more efficient and scalable to larger problems.\nWe rigorously evaluate its performance on synthetic datasets, demonstrating\nthat our algorithm not only outperforms current techniques, but also excels in\nscenarios with limited data availability. These results highlight its potential\nfor practical applications in fields requiring efficient and accurate causal\ninference from nonlinear time series data.", "comment": "10 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.07898v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07734", "title": "EEvAct: Early Event-Based Action Recognition with High-Rate Two-Stream Spiking Neural Networks", "authors": ["Michael Neumeier", "Jules Lecomte", "Nils Kazinski", "Soubarna Banik", "Bing Li", "Axel von Arnim"], "categories": ["cs.CV", "cs.NE"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      International Conference on Neuromorphic Systems (ICONS) 2025", "url": "http://arxiv.org/abs/2507.07734v1", "summary": "Recognizing human activities early is crucial for the safety and\nresponsiveness of human-robot and human-machine interfaces. Due to their high\ntemporal resolution and low latency, event-based vision sensors are a perfect\nmatch for this early recognition demand. However, most existing processing\napproaches accumulate events to low-rate frames or space-time voxels which\nlimits the early prediction capabilities. In contrast, spiking neural networks\n(SNNs) can process the events at a high-rate for early predictions, but most\nworks still fall short on final accuracy. In this work, we introduce a\nhigh-rate two-stream SNN which closes this gap by outperforming previous work\nby 2% in final accuracy on the large-scale THU EACT-50 dataset. We benchmark\nthe SNNs within a novel early event-based recognition framework by reporting\nTop-1 and Top-5 recognition scores for growing observation time. Finally, we\nexemplify the impact of these methods on a real-world task of early action\ntriggering for human motion capture in sports.", "comment": "International Conference on Neuromorphic Systems (ICONS) 2025", "pdf_url": "http://arxiv.org/pdf/2507.07734v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2411.10927", "title": "Inter-linguistic Phonetic Composition (IPC): A Theoretical and Computational Approach to Enhance Second Language Pronunciation", "authors": ["Jisang Park", "Minu Kim", "DaYoung Hong", "Jongha Lee"], "categories": ["cs.CL", "cs.SD", "eess.AS", "H.5.5"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.10927v3", "summary": "Learners of a second language (L2) often unconsciously substitute unfamiliar\nL2 phonemes with similar phonemes from their native language (L1), even though\nnative speakers of the L2 perceive these sounds as distinct and\nnon-interchangeable. This phonemic substitution leads to deviations from the\nstandard phonological patterns of the L2, creating challenges for learners in\nacquiring accurate L2 pronunciation. To address this, we propose\nInter-linguistic Phonetic Composition (IPC), a novel computational method\ndesigned to minimize incorrect phonological transfer by reconstructing L2\nphonemes as composite sounds derived from multiple L1 phonemes. Tests with two\nautomatic speech recognition models demonstrated that when L2 speakers produced\nIPC-generated composite sounds, the recognition rate of target L2 phonemes\nimproved by 20% compared to when their pronunciation was influenced by original\nphonological transfer patterns. The improvement was observed within a\nrelatively shorter time frame, demonstrating rapid acquisition of the composite\nsound.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.10927v3", "cate": "cs.CL", "date": "2024-11-17", "updated": "2025-07-10"}
{"id": "2504.07920", "title": "Directed Temporal Tree Realization for Periodic Public Transport: Easy and Hard Cases", "authors": ["Julia Meusel", "Matthias M√ºller-Hannemann", "Klaus Reinhardt"], "categories": ["cs.DS", "cs.CC", "cs.DM", "68R10 (Primary), 68Q25 (Secondary)"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      slightly extended version", "url": "http://arxiv.org/abs/2504.07920v2", "summary": "We study the complexity of the directed periodic temporal graph realization\nproblem. This work is motivated by the design of periodic schedules in public\ntransport with constraints on the quality of service. Namely, we require that\nthe fastest path between (important) pairs of vertices is upper bounded by a\nspecified maximum duration, encoded in an upper distance matrix $D$. While\nprevious work has considered the undirected version of the problem, the\napplication in public transport schedule design requires the flexibility to\nassign different departure times to the two directions of an edge. A problem\ninstance can only be feasible if all values of the distance matrix are at least\nshortest path distances. However, the task of realizing exact fastest path\ndistances in a periodic temporal graph is often too restrictive. Therefore, we\nintroduce a minimum slack parameter $k$ that describes a lower bound on the\nmaximum allowed waiting time on each path. We concentrate on tree topologies\nand provide a full characterization of the complexity landscape with respect to\nthe period $\\Delta$ and the minimum slack parameter~$k$, showing a sharp\nthreshold between NP-complete cases and cases which are always realizable. We\nalso provide hardness results for the special case of period $\\Delta = 2$ for\ngeneral directed and undirected graphs.", "comment": "slightly extended version", "pdf_url": "http://arxiv.org/pdf/2504.07920v2", "cate": "cs.DS", "date": "2025-04-10", "updated": "2025-07-10"}
{"id": "2507.07441", "title": "SAND: Boosting LLM Agents with Self-Taught Action Deliberation", "authors": ["Yu Xia", "Yiran Jenny Shen", "Junda Wu", "Tong Yu", "Sungchul Kim", "Ryan A. Rossi", "Lina Yao", "Julian McAuley"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07441v1", "summary": "Large Language Model (LLM) agents are commonly tuned with supervised\nfinetuning on ReAct-style expert trajectories or preference optimization over\npairwise rollouts. Most of these methods focus on imitating specific expert\nbehaviors or promoting chosen reasoning thoughts and actions over rejected\nones. However, without reasoning and comparing over alternatives actions, LLM\nagents finetuned with these methods may over-commit towards seemingly plausible\nbut suboptimal actions due to limited action space exploration. To address\nthis, in this paper we propose Self-taught ActioN Deliberation (SAND)\nframework, enabling LLM agents to explicitly deliberate over candidate actions\nbefore committing to one. To tackle the challenges of when and what to\ndeliberate given large action space and step-level action evaluation, we\nincorporate self-consistency action sampling and execution-guided action\ncritique to help synthesize step-wise action deliberation thoughts using the\nbase model of the LLM agent. In an iterative manner, the deliberation\ntrajectories are then used to finetune the LLM agent itself. Evaluating on two\nrepresentative interactive agent tasks, SAND achieves an average 20%\nimprovement over initial supervised finetuning and also outperforms\nstate-of-the-art agent tuning approaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07441v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07440", "title": "Self-supervised Learning of Latent Space Dynamics", "authors": ["Yue Li", "Gene Wei-Chin Lin", "Egor Larionov", "Aljaz Bozic", "Doug Roble", "Ladislav Kavan", "Stelian Coros", "Bernhard Thomaszewski", "Tuur Stuyck", "Hsiao-yu Chen"], "categories": ["cs.GR"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07440v1", "summary": "Modeling the dynamic behavior of deformable objects is crucial for creating\nrealistic digital worlds. While conventional simulations produce high-quality\nmotions, their computational costs are often prohibitive. Subspace simulation\ntechniques address this challenge by restricting deformations to a\nlower-dimensional space, improving performance while maintaining visually\ncompelling results. However, even subspace methods struggle to meet the\nstringent performance demands of portable devices such as virtual reality\nheadsets and mobile platforms. To overcome this limitation, we introduce a\nnovel subspace simulation framework powered by a neural latent-space\nintegrator. Our approach leverages self-supervised learning to enhance\ninference stability and generalization. By operating entirely within latent\nspace, our method eliminates the need for full-space computations, resulting in\na highly efficient method well-suited for deployment on portable devices. We\ndemonstrate the effectiveness of our approach on challenging examples involving\nrods, shells, and solids, showcasing its versatility and potential for\nwidespread adoption.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07440v1", "cate": "cs.GR", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07957", "title": "MIRIX: Multi-Agent Memory System for LLM-Based Agents", "authors": ["Yu Wang", "Xi Chen"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07957v1", "summary": "Although memory capabilities of AI agents are gaining increasing attention,\nexisting solutions remain fundamentally limited. Most rely on flat, narrowly\nscoped memory components, constraining their ability to personalize, abstract,\nand reliably recall user-specific information over time. To this end, we\nintroduce MIRIX, a modular, multi-agent memory system that redefines the future\nof AI memory by solving the field's most critical challenge: enabling language\nmodels to truly remember. Unlike prior approaches, MIRIX transcends text to\nembrace rich visual and multimodal experiences, making memory genuinely useful\nin real-world scenarios. MIRIX consists of six distinct, carefully structured\nmemory types: Core, Episodic, Semantic, Procedural, Resource Memory, and\nKnowledge Vault, coupled with a multi-agent framework that dynamically controls\nand coordinates updates and retrieval. This design enables agents to persist,\nreason over, and accurately retrieve diverse, long-term user data at scale. We\nvalidate MIRIX in two demanding settings. First, on ScreenshotVQA, a\nchallenging multimodal benchmark comprising nearly 20,000 high-resolution\ncomputer screenshots per sequence, requiring deep contextual understanding and\nwhere no existing memory systems can be applied, MIRIX achieves 35% higher\naccuracy than the RAG baseline while reducing storage requirements by 99.9%.\nSecond, on LOCOMO, a long-form conversation benchmark with single-modal textual\ninput, MIRIX attains state-of-the-art performance of 85.4%, far surpassing\nexisting baselines. These results show that MIRIX sets a new performance\nstandard for memory-augmented LLM agents. To allow users to experience our\nmemory system, we provide a packaged application powered by MIRIX. It monitors\nthe screen in real time, builds a personalized memory base, and offers\nintuitive visualization and secure local storage to ensure privacy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07957v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07919", "title": "Plausible Counterfactual Explanations of Recommendations", "authors": ["Jakub ƒåern√Ω", "Ji≈ô√≠ Nƒõmeƒçek", "Ivan Dovica", "Jakub Mareƒçek"], "categories": ["cs.LG", "cs.IR"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      8 pages, 3 figures, 6 tables", "url": "http://arxiv.org/abs/2507.07919v1", "summary": "Explanations play a variety of roles in various recommender systems, from a\nlegally mandated afterthought, through an integral element of user experience,\nto a key to persuasiveness. A natural and useful form of an explanation is the\nCounterfactual Explanation (CE). We present a method for generating highly\nplausible CEs in recommender systems and evaluate it both numerically and with\na user study.", "comment": "8 pages, 3 figures, 6 tables", "pdf_url": "http://arxiv.org/pdf/2507.07919v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07744", "title": "Sparse-Dense Side-Tuner for efficient Video Temporal Grounding", "authors": ["David Pujol-Perich", "Sergio Escalera", "Albert Clap√©s"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07744v1", "summary": "Video Temporal Grounding (VTG) involves Moment Retrieval (MR) and Highlight\nDetection (HD) based on textual queries. For this, most methods rely solely on\nfinal-layer features of frozen large pre-trained backbones, limiting their\nadaptability to new domains. While full fine-tuning is often impractical,\nparameter-efficient fine-tuning -- and particularly side-tuning (ST) -- has\nemerged as an effective alternative. However, prior ST approaches this problem\nfrom a frame-level refinement perspective, overlooking the inherent sparse\nnature of MR. To address this, we propose the Sparse-Dense Side-Tuner (SDST),\nthe first anchor-free ST architecture for VTG. We also introduce the\nReference-based Deformable Self-Attention, a novel mechanism that enhances the\ncontext modeling of the deformable attention -- a key limitation of existing\nanchor-free methods. Additionally, we present the first effective integration\nof InternVideo2 backbone into an ST framework, showing its profound\nimplications in performance. Overall, our method significantly improves\nexisting ST methods, achieving highly competitive or SOTA results on\nQVHighlights, TACoS, and Charades-STA, while reducing up to a 73% the parameter\ncount w.r.t. the existing SOTA methods. The code is publicly accessible at\nhttps://github.com/davidpujol/SDST.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07744v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2411.13766", "title": "Tiny-Align: Bridging Automatic Speech Recognition and Large Language Model on the Edge", "authors": ["Ruiyang Qin", "Dancheng Liu", "Gelei Xu", "Zheyu Yan", "Chenhui Xu", "Yuting Hu", "X. Sharon Hu", "Jinjun Xiong", "Yiyu Shi"], "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Accepted by ICCAD'25", "url": "http://arxiv.org/abs/2411.13766v3", "summary": "The combination of Large Language Models (LLM) and Automatic Speech\nRecognition (ASR), when deployed on edge devices (called edge ASR-LLM), can\nserve as a powerful personalized assistant to enable audio-based interaction\nfor users. Compared to text-based interaction, edge ASR-LLM allows accessible\nand natural audio interactions. Unfortunately, existing ASR-LLM models are\nmainly trained in high-performance computing environments and produce\nsubstantial model weights, making them difficult to deploy on edge devices.\nMore importantly, to better serve users' personalized needs, the ASR-LLM must\nbe able to learn from each distinct user, given that audio input often contains\nhighly personalized characteristics that necessitate personalized on-device\ntraining. Since individually fine-tuning the ASR or LLM often leads to\nsuboptimal results due to modality-specific limitations, end-to-end training\nensures seamless integration of audio features and language understanding\n(cross-modal alignment), ultimately enabling a more personalized and efficient\nadaptation on edge devices. However, due to the complex training requirements\nand substantial computational demands of existing approaches, cross-modal\nalignment between ASR audio and LLM can be challenging on edge devices. In this\nwork, we propose a resource-efficient cross-modal alignment framework that\nbridges ASR and LLMs on edge devices to handle personalized audio input. Our\nframework enables efficient ASR-LLM alignment on resource-constrained devices\nlike NVIDIA Jetson Orin (8GB RAM), achieving 50x training time speedup while\nimproving the alignment quality by more than 50\\%. To the best of our\nknowledge, this is the first work to study efficient ASR-LLM alignment on\nresource-constrained edge devices.", "comment": "Accepted by ICCAD'25", "pdf_url": "http://arxiv.org/pdf/2411.13766v3", "cate": "cs.SD", "date": "2024-11-21", "updated": "2025-07-09"}
{"id": "2507.06509", "title": "Prediction-Augmented Mechanism Design for Weighted Facility Location", "authors": ["Yangguang Shi", "Zhenyu Xue"], "categories": ["cs.DS", "cs.GT", "cs.LG", "68W27, 68Q32", "F.2.2"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      An extended abstract of this paper is to appear in the 19th Annual Conference on Theory and Applications of Models of Computation (TAMC 2025)", "url": "http://arxiv.org/abs/2507.06509v2", "summary": "Facility location is fundamental in operations research, mechanism design,\nand algorithmic game theory, with applications ranging from urban\ninfrastructure planning to distributed systems. Recent research in this area\nhas focused on augmenting classic strategyproof mechanisms with predictions to\nachieve an improved performance guarantee against the uncertainty under the\nstrategic environment. Previous work has been devoted to address the trade-off\nobstacle of balancing the consistency (near-optimality under accurate\npredictions) and robustness (bounded inefficiency under poor predictions)\nprimarily in the unweighted setting, assuming that all agents have the same\nimportance. However, this assumption may not be true in some practical\nscenarios, leading to research of weighted facility location problems.\n  The major contribution of the current work is to provide a prediction\naugmented algorithmic framework for balancing the consistency and robustness\nover strategic agents with non-uniform weights. In particular, through a\nreduction technique that identifies a subset of \\emph{representative} instances\nand maps the other given locations to the representative ones, we prove that\nthere exists a \\emph{strategyproof} mechanism achieving a bounded consistency\nguarantee of $\\frac{\\sqrt{(1+c)^2W^2_{\\min}+(1-c)^2W^2_{\\max}}}{(1+c)W_{\\min}}$\nand a bounded robustness guarantee of\n$\\frac{\\sqrt{(1-c)^2W^2_{\\min}+(1+c)^2W^2_{\\max}}}{(1-c)W_{\\min}}$ in weighted\nsettings, where $c$ can be viewed as a parameter to make a trade-off between\nthe consistency and robustness and $W_{\\min}$ and $W_{\\max}$ denote the minimum\nand maximum agents' weight. We also proved that there is no strategyproof\ndeterministic mechanism that reach $1$-consistency and $O\\left( n \\cdot\n\\frac{W_{\\max}}{W_{\\min}} \\right)$-robustness in weighted FLP, even with fully\npredictions of all agents.", "comment": "An extended abstract of this paper is to appear in the 19th Annual\n  Conference on Theory and Applications of Models of Computation (TAMC 2025)", "pdf_url": "http://arxiv.org/pdf/2507.06509v2", "cate": "cs.DS", "date": "2025-07-09", "updated": "2025-07-10"}
{"id": "2507.07451", "title": "RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning", "authors": ["Hongzhi Zhang", "Jia Fu", "Jingyuan Zhang", "Kai Fu", "Qi Wang", "Fuzheng Zhang", "Guorui Zhou"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:       this https URL", "url": "http://arxiv.org/abs/2507.07451v1", "summary": "Reinforcement learning (RL) for large language models is an energy-intensive\nendeavor: training can be unstable, and the policy may gradually drift away\nfrom its pretrained weights. We present \\emph{RLEP}\\, -- \\,Reinforcement\nLearning with Experience rePlay\\, -- \\,a two-phase framework that first\ncollects verified trajectories and then replays them during subsequent\ntraining. At every update step, the policy is optimized on mini-batches that\nblend newly generated rollouts with these replayed successes. By replaying\nhigh-quality examples, RLEP steers the model away from fruitless exploration,\nfocuses learning on promising reasoning paths, and delivers both faster\nconvergence and stronger final performance. On the Qwen2.5-Math-7B base model,\nRLEP reaches baseline peak accuracy with substantially fewer updates and\nultimately surpasses it, improving accuracy on AIME-2024 from 38.2% to 39.9%,\non AIME-2025 from 19.8% to 22.3%, and on AMC-2023 from 77.0% to 82.2%. Our\ncode, datasets, and checkpoints are publicly available at\nhttps://github.com/Kwai-Klear/RLEP to facilitate reproducibility and further\nresearch.", "comment": "https://github.com/Kwai-Klear/RLEP", "pdf_url": "http://arxiv.org/pdf/2507.07451v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07465", "title": "SD-GS: Structured Deformable 3D Gaussians for Efficient Dynamic Scene Reconstruction", "authors": ["Wei Yao", "Shuzhao Xie", "Letian Li", "Weixiang Zhang", "Zhixin Lai", "Shiqi Dai", "Ke Zhang", "Zhi Wang"], "categories": ["cs.GR", "cs.CV"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07465v1", "summary": "Current 4D Gaussian frameworks for dynamic scene reconstruction deliver\nimpressive visual fidelity and rendering speed, however, the inherent trade-off\nbetween storage costs and the ability to characterize complex physical motions\nsignificantly limits the practical application of these methods. To tackle\nthese problems, we propose SD-GS, a compact and efficient dynamic Gaussian\nsplatting framework for complex dynamic scene reconstruction, featuring two key\ncontributions. First, we introduce a deformable anchor grid, a hierarchical and\nmemory-efficient scene representation where each anchor point derives multiple\n3D Gaussians in its local spatiotemporal region and serves as the geometric\nbackbone of the 3D scene. Second, to enhance modeling capability for complex\nmotions, we present a deformation-aware densification strategy that adaptively\ngrows anchors in under-reconstructed high-dynamic regions while reducing\nredundancy in static areas, achieving superior visual quality with fewer\nanchors. Experimental results demonstrate that, compared to state-of-the-art\nmethods, SD-GS achieves an average of 60\\% reduction in model size and an\naverage of 100\\% improvement in FPS, significantly enhancing computational\nefficiency while maintaining or even surpassing visual quality.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07465v1", "cate": "cs.GR", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07966", "title": "Scaling RL to Long Videos", "authors": ["Yukang Chen", "Wei Huang", "Baifeng Shi", "Qinghao Hu", "Hanrong Ye", "Ligeng Zhu", "Zhijian Liu", "Pavlo Molchanov", "Jan Kautz", "Xiaojuan Qi", "Sifei Liu", "Hongxu Yin", "Yao Lu", "Song Han"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Code and models are available at this https URL", "url": "http://arxiv.org/abs/2507.07966v1", "summary": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 52K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In experiments, LongVILA-R1-7B achieves\nstrong performance on long video QA benchmarks such as VideoMME. It also\noutperforms Video-R1-7B and even matches Gemini-1.5-Pro across temporal\nreasoning, goal and purpose reasoning, spatial reasoning, and plot reasoning on\nour LongVideo-Reason-eval benchmark. Notably, our MR-SP system achieves up to\n2.1x speedup on long video RL training. LongVILA-R1 demonstrates consistent\nperformance gains as the number of input video frames scales. LongVILA-R1 marks\na firm step towards long video reasoning in VLMs. In addition, we release our\ntraining system for public availability that supports RL training on various\nmodalities (video, text, and audio), various models (VILA and Qwen series), and\neven image and video generation models. On a single A100 node (8 GPUs), it\nsupports RL training on hour-long videos (e.g., 3,600 frames / around 256k\ntokens).", "comment": "Code and models are available at https://github.com/NVlabs/Long-RL", "pdf_url": "http://arxiv.org/pdf/2507.07966v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07955", "title": "Dynamic Chunking for End-to-End Hierarchical Sequence Modeling", "authors": ["Sukjun Hwang", "Brandon Wang", "Albert Gu"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07955v1", "summary": "Despite incredible progress in language models (LMs) in recent years, largely\nresulting from moving away from specialized models designed for specific tasks\nto general models based on powerful architectures (e.g. the Transformer) that\nlearn everything from raw data, pre-processing steps such as tokenization\nremain a barrier to true end-to-end foundation models. We introduce a\ncollection of new techniques that enable a dynamic chunking mechanism which\nautomatically learns content -- and context -- dependent segmentation\nstrategies learned jointly with the rest of the model. Incorporating this into\nan explicit hierarchical network (H-Net) allows replacing the (implicitly\nhierarchical) tokenization-LM-detokenization pipeline with a single model\nlearned fully end-to-end. When compute- and data- matched, an H-Net with one\nstage of hierarchy operating at the byte level outperforms a strong Transformer\nlanguage model operating over BPE tokens. Iterating the hierarchy to multiple\nstages further increases its performance by modeling multiple levels of\nabstraction, demonstrating significantly better scaling with data and matching\na token-based Transformer of twice its size. H-Nets pretrained on English show\nsignificantly increased character-level robustness, and qualitatively learn\nmeaningful data-dependent chunking strategies without any heuristics or\nexplicit supervision. Finally, the H-Net's improvement over tokenized pipelines\nis further increased in languages and modalities with weaker tokenization\nheuristics, such as Chinese and code, or DNA sequences (nearly 4x improvement\nin data efficiency over baselines), showing the potential of true end-to-end\nmodels that learn and scale better from unprocessed data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07955v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07747", "title": "X-RAFT: Cross-Modal Non-Rigid Registration of Blue and White Light Neurosurgical Hyperspectral Images", "authors": ["Charlie Budd", "Silv√®re S√©gaud", "Matthew Elliot", "Graeme Stasiuk", "Yijing Xie", "Jonathan Shapey", "Tom Vercauteren"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07747v1", "summary": "Integration of hyperspectral imaging into fluorescence-guided neurosurgery\nhas the potential to improve surgical decision making by providing quantitative\nfluorescence measurements in real-time. Quantitative fluorescence requires\npaired spectral data in fluorescence (blue light) and reflectance (white light)\nmode. Blue and white image acquisition needs to be performed sequentially in a\npotentially dynamic surgical environment. A key component to the fluorescence\nquantification process is therefore the ability to find dense cross-modal image\ncorrespondences between two hyperspectral images taken under these drastically\ndifferent lighting conditions. We address this challenge with the introduction\nof X-RAFT, a Recurrent All-Pairs Field Transforms (RAFT) optical flow model\nmodified for cross-modal inputs. We propose using distinct image encoders for\neach modality pair, and fine-tune these in a self-supervised manner using\nflow-cycle-consistency on our neurosurgical hyperspectral data. We show an\nerror reduction of 36.6% across our evaluation metrics when comparing to a\nnaive baseline and 27.83% reduction compared to an existing cross-modal optical\nflow method (CrossRAFT). Our code and models will be made publicly available\nafter the review process.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07747v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2411.19204", "title": "A Voice-based Triage for Type 2 Diabetes using a Conversational Virtual Assistant in the Home Environment", "authors": ["Kelvin Summoogum", "Debayan Das", "Sathish Kumaran", "Sumit Bhagra"], "categories": ["cs.SD", "eess.AS", "F.2.2; I.2.7"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      8 pages", "url": "http://arxiv.org/abs/2411.19204v3", "summary": "Incorporating cloud technology with Internet of Medical Things for ubiquitous\nhealthcare has seen many successful applications in the last decade with the\nadvent of machine learning and deep learning techniques. One of these\napplications, namely voice-based pathology, has yet to receive notable\nattention from academia and industry. Applying voice analysis to early\ndetection of fatal diseases holds much promise to improve health outcomes and\nquality of life of patients. In this paper, we propose a novel application of\nacoustic machine learning based triaging into commoditised conversational\nvirtual assistant systems to pre-screen for onset of diabetes. Specifically, we\ndeveloped a triaging system which extracts acoustic features from the voices of\nn=24 older adults when they converse with a virtual assistant and predict the\nincidence of Diabetes Mellitus (Type 2) or not. Our triaging system achieved\nhit-rates of 70% and 60% for male and female older adult subjects,\nrespectively. Our proposed triaging uses 7 non-identifiable voice-based\nfeatures and can operate within resource-constrained embedded systems running\nvoice-based virtual assistants. This application demonstrates the feasibility\nof applying voice-based pathology analysis to improve health outcomes of older\nadults within the home environment by early detection of life-changing chronic\nconditions like diabetes.", "comment": "8 pages", "pdf_url": "http://arxiv.org/pdf/2411.19204v3", "cate": "cs.SD", "date": "2024-11-28", "updated": "2025-07-10"}
{"id": "2010.07990", "title": "An Algorithm for Learning Smaller Representations of Models With Scarce Data", "authors": ["Adrian de Wynter"], "categories": ["cs.LG", "cs.AI", "cs.DS"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted to Information Geometry--see the journal for the final, authenticated version", "url": "http://arxiv.org/abs/2010.07990v2", "summary": "We present an algorithm for solving binary classification problems when the\ndataset is not fully representative of the problem being solved, and obtaining\nmore data is not possible. It relies on a trained model with loose accuracy\nconstraints, an iterative hyperparameter searching-and-pruning procedure over a\nsearch space $\\Theta$, and a data-generating function. Our algorithm works by\nreconstructing up to homology the manifold on which lies the support of the\nunderlying distribution. We provide an analysis on correctness and runtime\ncomplexity under ideal conditions and an extension to deep neural networks. In\nthe former case, if $\\size{\\Theta}$ is the number of hyperparameter sets in the\nsearch space, this algorithm returns a solution that is up to $2(1 -\n{2^{-\\size{\\Theta}}})$ times better than simply training with an enumeration of\n$\\Theta$ and picking the best model. As part of our analysis we also prove that\nan open cover of a dataset has the same homology as the manifold on which lies\nthe support of the underlying probability distribution, if and only said\ndataset is learnable. This latter result acts as a formal argument to explain\nthe effectiveness of data expansion techniques.", "comment": "Accepted to Information Geometry--see the journal for the final,\n  authenticated version", "pdf_url": "http://arxiv.org/pdf/2010.07990v2", "cate": "cs.LG", "date": "2020-10-15", "updated": "2025-07-10"}
{"id": "2507.07498", "title": "Teaching LLM to Reason: Reinforcement Learning from Algorithmic Problems without Code", "authors": ["Keqin Bao", "Nuo Chen", "Xiaoyuan Li", "Binyuan Hui", "Bowen Yu", "Fuli Feng", "Junyang Lin", "Xiangnan He", "Dayiheng Liu"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07498v1", "summary": "Enhancing reasoning capabilities remains a central focus in the LLM reasearch\ncommunity. A promising direction involves requiring models to simulate code\nexecution step-by-step to derive outputs for given inputs. However, as code is\noften designed for large-scale systems, direct application leads to\nover-reliance on complex data structures and algorithms, even for simple cases,\nresulting in overfitting to algorithmic patterns rather than core reasoning\nstructures. To address this, we propose TeaR, which aims at teaching LLMs to\nreason better. TeaR leverages careful data curation and reinforcement learning\nto guide models in discovering optimal reasoning paths through code-related\ntasks, thereby improving general reasoning abilities. We conduct extensive\nexperiments using two base models and three long-CoT distillation models, with\nmodel sizes ranging from 1.5 billion to 32 billion parameters, and across 17\nbenchmarks spanning Math, Knowledge, Code, and Logical Reasoning. The results\nconsistently show significant performance improvements. Notably, TeaR achieves\na 35.9% improvement on Qwen2.5-7B and 5.9% on R1-Distilled-7B.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07498v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07623", "title": "Capture Stage Environments: A Guide to Better Matting", "authors": ["Hannah Dr√∂ge", "Janelle Pfeifer", "Saskia Rabich", "Markus Plack", "Reinhard Klein", "Matthias B. Hullin"], "categories": ["cs.GR", "cs.CV"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07623v1", "summary": "Capture stages are high-end sources of state-of-the-art recordings for\ndownstream applications in movies, games, and other media. One crucial step in\nalmost all pipelines is the matting of images to isolate the captured\nperformances from the background. While common matting algorithms deliver\nremarkable performance in other applications like teleconferencing and mobile\nentertainment, we found that they struggle significantly with the peculiarities\nof capture stage content. The goal of our work is to share insights into those\nchallenges as a curated list of those characteristics along with a constructive\ndiscussion for proactive intervention and present a guideline to practitioners\nfor an improved workflow to mitigate unresolved challenges. To this end, we\nalso demonstrate an efficient pipeline to adapt state-of-the-art approaches to\nsuch custom setups without the need of extensive annotations, both offline and\nreal-time. For an objective evaluation, we propose a validation methodology\nbased on a leading diffusion model that highlights the benefits of our\napproach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07623v1", "cate": "cs.GR", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07981", "title": "Why is Your Language Model a Poor Implicit Reward Model?", "authors": ["Noam Razin", "Yong Lin", "Jiarui Yao", "Sanjeev Arora"], "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07981v1", "summary": "Reward models are key to language model post-training and inference\npipelines. Conveniently, recent work showed that every language model defines\nan implicit reward model (IM-RM), without requiring any architectural changes.\nHowever, such IM-RMs tend to generalize worse, especially out-of-distribution,\ncompared to explicit reward models (EX-RMs) that apply a dedicated linear head\nover the hidden representations of a language model. The existence of a\ngeneralization gap is puzzling, as EX-RMs and IM-RMs are nearly identical. They\ncan be trained using the same data, loss function, and language model, and\ndiffer only in how the reward is computed. Towards a fundamental understanding\nof the implicit biases underlying different reward model types, we investigate\nthe root cause of this gap. Our main finding, backed by theory and experiments,\nis that IM-RMs rely more heavily on superficial token-level cues. Consequently,\nthey often generalize worse than EX-RMs under token-level distribution shifts,\nas well as in-distribution. Furthermore, we provide evidence against\nalternative hypotheses for the generalization gap. Most notably, we challenge\nthe intuitive claim that IM-RMs struggle in tasks where generation is harder\nthan verification because they can operate both as a verifier and a generator.\nTaken together, our results highlight that seemingly minor design choices can\nsubstantially impact the generalization behavior of reward models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07981v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07965", "title": "Prospective Learning in Retrospect", "authors": ["Yuxin Bai", "Cecelia Shuai", "Ashwin De Silva", "Siyu Yu", "Pratik Chaudhari", "Joshua T. Vogelstein"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted to AGI 2025", "url": "http://arxiv.org/abs/2507.07965v1", "summary": "In most real-world applications of artificial intelligence, the distributions\nof the data and the goals of the learners tend to change over time. The\nProbably Approximately Correct (PAC) learning framework, which underpins most\nmachine learning algorithms, fails to account for dynamic data distributions\nand evolving objectives, often resulting in suboptimal performance. Prospective\nlearning is a recently introduced mathematical framework that overcomes some of\nthese limitations. We build on this framework to present preliminary results\nthat improve the algorithm and numerical results, and extend prospective\nlearning to sequential decision-making scenarios, specifically foraging. Code\nis available at: https://github.com/neurodata/prolearn2.", "comment": "Accepted to AGI 2025", "pdf_url": "http://arxiv.org/pdf/2507.07965v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07757", "title": "Deep Learning based 3D Volume Correlation for Additive Manufacturing Using High-Resolution Industrial X-ray Computed Tomography", "authors": ["Keerthana Chand", "Tobias Fritsch", "Bardia Hejazi", "Konstantin Poka", "Giovanni Bruno"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07757v1", "summary": "Quality control in additive manufacturing (AM) is vital for industrial\napplications in areas such as the automotive, medical and aerospace sectors.\nGeometric inaccuracies caused by shrinkage and deformations can compromise the\nlife and performance of additively manufactured components. Such deviations can\nbe quantified using Digital Volume Correlation (DVC), which compares the\ncomputer-aided design (CAD) model with the X-ray Computed Tomography (XCT)\ngeometry of the components produced. However, accurate registration between the\ntwo modalities is challenging due to the absence of a ground truth or reference\ndeformation field. In addition, the extremely large data size of\nhigh-resolution XCT volumes makes computation difficult. In this work, we\npresent a deep learning-based approach for estimating voxel-wise deformations\nbetween CAD and XCT volumes. Our method uses a dynamic patch-based processing\nstrategy to handle high-resolution volumes. In addition to the Dice Score, we\nintroduce a Binary Difference Map (BDM) that quantifies voxel-wise mismatches\nbetween binarized CAD and XCT volumes to evaluate the accuracy of the\nregistration. Our approach shows a 9.2\\% improvement in the Dice Score and a\n9.9\\% improvement in the voxel match rate compared to classic DVC methods,\nwhile reducing the interaction time from days to minutes. This work sets the\nfoundation for deep learning-based DVC methods to generate compensation meshes\nthat can then be used in closed-loop correlations during the AM production\nprocess. Such a system would be of great interest to industries since the\nmanufacturing process will become more reliable and efficient, saving time and\nmaterial.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07757v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2412.18603", "title": "Long-Form Speech Generation with Spoken Language Models", "authors": ["Se Jin Park", "Julian Salazar", "Aren Jansen", "Keisuke Kinoshita", "Yong Man Ro", "RJ Skerry-Ryan"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted to ICML 2025 (oral)", "url": "http://arxiv.org/abs/2412.18603v2", "summary": "We consider the generative modeling of speech over multiple minutes, a\nrequirement for long-form multimedia generation and audio-native voice\nassistants. However, textless spoken language models struggle to generate\nplausible speech past tens of seconds, due to high temporal resolution of\nspeech tokens causing loss of coherence, architectural issues with\nlong-sequence training or extrapolation, and memory costs at inference time.\nFrom these considerations we derive SpeechSSM, the first speech language model\nfamily to learn from and sample long-form spoken audio (e.g., 16 minutes of\nread or extemporaneous speech) in a single decoding session without text\nintermediates. SpeechSSMs leverage recent advances in linear-time sequence\nmodeling to greatly surpass current Transformer spoken LMs in coherence and\nefficiency on multi-minute generations while still matching them at the\nutterance level. As we found current spoken language evaluations uninformative,\nespecially in this new long-form setting, we also introduce: LibriSpeech-Long,\na benchmark for long-form speech evaluation; new embedding-based and LLM-judged\nmetrics; and quality measurements over length and time. Speech samples, the\nLibriSpeech-Long dataset, and any future code or model releases can be found at\nhttps://google.github.io/tacotron/publications/speechssm/.", "comment": "Accepted to ICML 2025 (oral)", "pdf_url": "http://arxiv.org/pdf/2412.18603v2", "cate": "cs.CL", "date": "2024-12-24", "updated": "2025-07-10"}
{"id": "2507.07499", "title": "Extracting ORR Catalyst Information for Fuel Cell from Scientific Literature", "authors": ["Hein Htet", "Amgad Ahmed Ali Ibrahim", "Yutaka Sasaki", "Ryoji Asahi"], "categories": ["cs.CL", "physics.data-an"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      28 pages, 12 figures, 6 tables", "url": "http://arxiv.org/abs/2507.07499v1", "summary": "The oxygen reduction reaction (ORR) catalyst plays a critical role in\nenhancing fuel cell efficiency, making it a key focus in material science\nresearch. However, extracting structured information about ORR catalysts from\nvast scientific literature remains a significant challenge due to the\ncomplexity and diversity of textual data. In this study, we propose a named\nentity recognition (NER) and relation extraction (RE) approach using DyGIE++\nwith multiple pre-trained BERT variants, including MatSciBERT and PubMedBERT,\nto extract ORR catalyst-related information from the scientific literature,\nwhich is compiled into a fuel cell corpus for materials informatics\n(FC-CoMIcs). A comprehensive dataset was constructed manually by identifying 12\ncritical entities and two relationship types between pairs of the entities. Our\nmethodology involves data annotation, integration, and fine-tuning of\ntransformer-based models to enhance information extraction accuracy. We assess\nthe impact of different BERT variants on extraction performance and investigate\nthe effects of annotation consistency. Experimental evaluations demonstrate\nthat the fine-tuned PubMedBERT model achieves the highest NER F1-score of\n82.19% and the MatSciBERT model attains the best RE F1-score of 66.10%.\nFurthermore, the comparison with human annotators highlights the reliability of\nfine-tuned models for ORR catalyst extraction, demonstrating their potential\nfor scalable and automated literature analysis. The results indicate that\ndomain-specific BERT models outperform general scientific models like BlueBERT\nfor ORR catalyst extraction.", "comment": "28 pages, 12 figures, 6 tables", "pdf_url": "http://arxiv.org/pdf/2507.07499v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07733", "title": "RTR-GS: 3D Gaussian Splatting for Inverse Rendering with Radiance Transfer and Reflection", "authors": ["Yongyang Zhou", "Fang-Lue Zhang", "Zichen Wang", "Lei Zhang"], "categories": ["cs.GR", "cs.CV"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      16 pages", "url": "http://arxiv.org/abs/2507.07733v1", "summary": "3D Gaussian Splatting (3DGS) has demonstrated impressive capabilities in\nnovel view synthesis. However, rendering reflective objects remains a\nsignificant challenge, particularly in inverse rendering and relighting. We\nintroduce RTR-GS, a novel inverse rendering framework capable of robustly\nrendering objects with arbitrary reflectance properties, decomposing BRDF and\nlighting, and delivering credible relighting results. Given a collection of\nmulti-view images, our method effectively recovers geometric structure through\na hybrid rendering model that combines forward rendering for radiance transfer\nwith deferred rendering for reflections. This approach successfully separates\nhigh-frequency and low-frequency appearances, mitigating floating artifacts\ncaused by spherical harmonic overfitting when handling high-frequency details.\nWe further refine BRDF and lighting decomposition using an additional\nphysically-based deferred rendering branch. Experimental results show that our\nmethod enhances novel view synthesis, normal estimation, decomposition, and\nrelighting while maintaining efficient training inference process.", "comment": "16 pages", "pdf_url": "http://arxiv.org/pdf/2507.07733v1", "cate": "cs.GR", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07251", "title": "A Language-Driven Framework for Improving Personalized Recommendations: Merging LLMs with Traditional Algorithms", "authors": ["Aaron Goldstein", "Ayan Dutta"], "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07251v1", "summary": "Traditional recommendation algorithms are not designed to provide\npersonalized recommendations based on user preferences provided through text,\ne.g., \"I enjoy light-hearted comedies with a lot of humor\". Large Language\nModels (LLMs) have emerged as one of the most promising tools for natural\nlanguage processing in recent years. This research proposes a novel framework\nthat mimics how a close friend would recommend items based on their knowledge\nof an individual's tastes. We leverage LLMs to enhance movie recommendation\nsystems by refining traditional algorithm outputs and integrating them with\nlanguage-based user preference inputs. We employ Singular Value Decomposition\n(SVD) or SVD++ algorithms to generate initial movie recommendations,\nimplemented using the Surprise Python library and trained on the\nMovieLens-Latest-Small dataset. We compare the performance of the base\nalgorithms with our LLM-enhanced versions using leave-one-out validation hit\nrates and cumulative hit rates. Additionally, to compare the performance of our\nframework against the current state-of-the-art recommendation systems, we use\nrating and ranking metrics with an item-based stratified 0.75 train, 0.25 test\nsplit. Our framework can generate preference profiles automatically based on\nusers' favorite movies or allow manual preference specification for more\npersonalized results. Using an automated approach, our framework overwhelmingly\nsurpassed SVD and SVD++ on every evaluation metric used (e.g., improvements of\nup to ~6x in cumulative hit rate, ~3.7x in NDCG, etc.), albeit at the cost of a\nslight increase in computational overhead.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07251v1", "cate": "cs.IR", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07982", "title": "Geometry Forcing: Marrying Video Diffusion and 3D Representation for Consistent World Modeling", "authors": ["Haoyu Wu", "Diankun Wu", "Tianyu He", "Junliang Guo", "Yang Ye", "Yueqi Duan", "Jiang Bian"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      18 pages, project page: this https URL", "url": "http://arxiv.org/abs/2507.07982v1", "summary": "Videos inherently represent 2D projections of a dynamic 3D world. However,\nour analysis suggests that video diffusion models trained solely on raw video\ndata often fail to capture meaningful geometric-aware structure in their\nlearned representations. To bridge this gap between video diffusion models and\nthe underlying 3D nature of the physical world, we propose Geometry Forcing, a\nsimple yet effective method that encourages video diffusion models to\ninternalize latent 3D representations. Our key insight is to guide the model's\nintermediate representations toward geometry-aware structure by aligning them\nwith features from a pretrained geometric foundation model. To this end, we\nintroduce two complementary alignment objectives: Angular Alignment, which\nenforces directional consistency via cosine similarity, and Scale Alignment,\nwhich preserves scale-related information by regressing unnormalized geometric\nfeatures from normalized diffusion representation. We evaluate Geometry Forcing\non both camera view-conditioned and action-conditioned video generation tasks.\nExperimental results demonstrate that our method substantially improves visual\nquality and 3D consistency over the baseline methods. Project page:\nhttps://GeometryForcing.github.io.", "comment": "18 pages, project page: https://GeometryForcing.github.io", "pdf_url": "http://arxiv.org/pdf/2507.07982v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07986", "title": "EXPO: Stable Reinforcement Learning with Expressive Policies", "authors": ["Perry Dong", "Qiyang Li", "Dorsa Sadigh", "Chelsea Finn"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07986v1", "summary": "We study the problem of training and fine-tuning expressive policies with\nonline reinforcement learning (RL) given an offline dataset. Training\nexpressive policy classes with online RL present a unique challenge of stable\nvalue maximization. Unlike simpler Gaussian policies commonly used in online\nRL, expressive policies like diffusion and flow-matching policies are\nparameterized by a long denoising chain, which hinders stable gradient\npropagation from actions to policy parameters when optimizing against some\nvalue function. Our key insight is that we can address stable value\nmaximization by avoiding direct optimization over value with the expressive\npolicy and instead construct an on-the-fly RL policy to maximize Q-value. We\npropose Expressive Policy Optimization (EXPO), a sample-efficient online RL\nalgorithm that utilizes an on-the-fly policy to maximize value with two\nparameterized policies -- a larger expressive base policy trained with a stable\nimitation learning objective and a light-weight Gaussian edit policy that edits\nthe actions sampled from the base policy toward a higher value distribution.\nThe on-the-fly policy optimizes the actions from the base policy with the\nlearned edit policy and chooses the value maximizing action from the base and\nedited actions for both sampling and temporal-difference (TD) backup. Our\napproach yields up to 2-3x improvement in sample efficiency on average over\nprior methods both in the setting of fine-tuning a pretrained policy given\noffline data and in leveraging offline data to train online.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07986v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07776", "title": "SCOOTER: A Human Evaluation Framework for Unrestricted Adversarial Examples", "authors": ["Dren Fazlija", "Monty-Maximilian Z√ºhlke", "Johanna Schrader", "Arkadij Orlov", "Clara Stein", "Iyiola E. Olatunji", "Daniel Kudenko"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      42 pages, 16 figures, 11 tables, Under Review, Code: this https URL , Data: this https URL", "url": "http://arxiv.org/abs/2507.07776v1", "summary": "Unrestricted adversarial attacks aim to fool computer vision models without\nbeing constrained by $\\ell_p$-norm bounds to remain imperceptible to humans,\nfor example, by changing an object's color. This allows attackers to circumvent\ntraditional, norm-bounded defense strategies such as adversarial training or\ncertified defense strategies. However, due to their unrestricted nature, there\nare also no guarantees of norm-based imperceptibility, necessitating human\nevaluations to verify just how authentic these adversarial examples look. While\nsome related work assesses this vital quality of adversarial attacks, none\nprovide statistically significant insights. This issue necessitates a unified\nframework that supports and streamlines such an assessment for evaluating and\ncomparing unrestricted attacks. To close this gap, we introduce SCOOTER - an\nopen-source, statistically powered framework for evaluating unrestricted\nadversarial examples. Our contributions are: $(i)$ best-practice guidelines for\ncrowd-study power, compensation, and Likert equivalence bounds to measure\nimperceptibility; $(ii)$ the first large-scale human vs. model comparison\nacross 346 human participants showing that three color-space attacks and three\ndiffusion-based attacks fail to produce imperceptible images. Furthermore, we\nfound that GPT-4o can serve as a preliminary test for imperceptibility, but it\nonly consistently detects adversarial examples for four out of six tested\nattacks; $(iii)$ open-source software tools, including a browser-based task\ntemplate to collect annotations and analysis scripts in Python and R; $(iv)$ an\nImageNet-derived benchmark dataset containing 3K real images, 7K adversarial\nexamples, and over 34K human ratings. Our findings demonstrate that automated\nvision systems do not align with human perception, reinforcing the need for a\nground-truth SCOOTER benchmark.", "comment": "42 pages, 16 figures, 11 tables, Under Review, Code:\n  https://github.com/DrenFazlija/Scooter, Data:\n  https://doi.org/10.5281/zenodo.15771501", "pdf_url": "http://arxiv.org/pdf/2507.07776v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2502.00718", "title": "\"I am bad\": Interpreting Stealthy, Universal and Robust Audio Jailbreaks in Audio-Language Models", "authors": ["Isha Gupta", "David Khachaturov", "Robert Mullins"], "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.00718v2", "summary": "The rise of multimodal large language models has introduced innovative\nhuman-machine interaction paradigms but also significant challenges in machine\nlearning safety. Audio-Language Models (ALMs) are especially relevant due to\nthe intuitive nature of spoken communication, yet little is known about their\nfailure modes. This paper explores audio jailbreaks targeting ALMs, focusing on\ntheir ability to bypass alignment mechanisms. We construct adversarial\nperturbations that generalize across prompts, tasks, and even base audio\nsamples, demonstrating the first universal jailbreaks in the audio modality,\nand show that these remain effective in simulated real-world conditions. Beyond\ndemonstrating attack feasibility, we analyze how ALMs interpret these audio\nadversarial examples and reveal them to encode imperceptible first-person toxic\nspeech - suggesting that the most effective perturbations for eliciting toxic\noutputs specifically embed linguistic features within the audio signal. These\nresults have important implications for understanding the interactions between\ndifferent modalities in multimodal models, and offer actionable insights for\nenhancing defenses against adversarial audio attacks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.00718v2", "cate": "cs.LG", "date": "2025-02-02", "updated": "2025-07-10"}
{"id": "2507.07518", "title": "Triadic Multi-party Voice Activity Projection for Turn-taking in Spoken Dialogue Systems", "authors": ["Mikey Elmers", "Koji Inoue", "Divesh Lala", "Tatsuya Kawahara"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted to Interspeech 2025", "url": "http://arxiv.org/abs/2507.07518v1", "summary": "Turn-taking is a fundamental component of spoken dialogue, however\nconventional studies mostly involve dyadic settings. This work focuses on\napplying voice activity projection (VAP) to predict upcoming turn-taking in\ntriadic multi-party scenarios. The goal of VAP models is to predict the future\nvoice activity for each speaker utilizing only acoustic data. This is the first\nstudy to extend VAP into triadic conversation. We trained multiple models on a\nJapanese triadic dataset where participants discussed a variety of topics. We\nfound that the VAP trained on triadic conversation outperformed the baseline\nfor all models but that the type of conversation affected the accuracy. This\nstudy establishes that VAP can be used for turn-taking in triadic dialogue\nscenarios. Future work will incorporate this triadic VAP turn-taking model into\nspoken dialogue systems.", "comment": "Accepted to Interspeech 2025", "pdf_url": "http://arxiv.org/pdf/2507.07518v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07890", "title": "Hi-d maps: An interactive visualization technique for multi-dimensional categorical data", "authors": ["Radi Muhammad Reza", "Benjamin A Watson"], "categories": ["cs.GR"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07890v1", "summary": "In this paper, we present Hi-D maps, a novel method for the visualization of\nmulti-dimensional categorical data. Our work addresses the scarcity of\ntechniques for visualizing a large number of data-dimensions in an effective\nand space-efficient manner. We have mapped the full data-space onto a 2D\nregular polygonal region. The polygon is cut hierarchically with lines parallel\nto a user-controlled, ordered sequence of sides, each representing a dimension.\nWe have used multiple visual cues such as orientation, thickness, color,\ncountable glyphs, and text to depict cross-dimensional information. We have\nadded interactivity and hierarchical browsing to facilitate flexible\nexploration of the display: small areas can be scrutinized for details. Thus,\nour method is also easily extendable to visualize hierarchical information. Our\nglyph animations add an engaging aesthetic during interaction. Like many\nvisualizations, Hi-D maps become less effective when a large number of\ndimensions stresses perceptual limits, but Hi-D maps may add clarity before\nthose limits are reached.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07890v1", "cate": "cs.GR", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07436", "title": "When Graph Contrastive Learning Backfires: Spectral Vulnerability and Defense in Recommendation", "authors": ["Zongwei Wang", "Min Gao", "Junliang Yu", "Shazia Sadiq", "Hongzhi Yin", "Ling Liu"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      24 pages, 6 figures", "url": "http://arxiv.org/abs/2507.07436v1", "summary": "Graph Contrastive Learning (GCL) has demonstrated substantial promise in\nenhancing the robustness and generalization of recommender systems,\nparticularly by enabling models to leverage large-scale unlabeled data for\nimproved representation learning. However, in this paper, we reveal an\nunexpected vulnerability: the integration of GCL inadvertently increases the\nsusceptibility of a recommender to targeted promotion attacks. Through both\ntheoretical investigation and empirical validation, we identify the root cause\nas the spectral smoothing effect induced by contrastive optimization, which\ndisperses item embeddings across the representation space and unintentionally\nenhances the exposure of target items. Building on this insight, we introduce\nCLeaR, a bi-level optimization attack method that deliberately amplifies\nspectral smoothness, enabling a systematic investigation of the susceptibility\nof GCL-based recommendation models to targeted promotion attacks. Our findings\nhighlight the urgent need for robust countermeasures; in response, we further\npropose SIM, a spectral irregularity mitigation framework designed to\naccurately detect and suppress targeted items without compromising model\nperformance. Extensive experiments on multiple benchmark datasets demonstrate\nthat, compared to existing targeted promotion attacks, GCL-based recommendation\nmodels exhibit greater susceptibility when evaluated with CLeaR, while SIM\neffectively mitigates these vulnerabilities.", "comment": "24 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.07436v1", "cate": "cs.IR", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07983", "title": "Performance and Practical Considerations of Large and Small Language Models in Clinical Decision Support in Rheumatology", "authors": ["Sabine Felde", "R√ºdiger Buchkremer", "Gamal Chehab", "Christian Thielscher", "J√∂rg HW Distler", "Matthias Schneider", "Jutta G. Richter"], "categories": ["cs.CL", "cs.AI", "L01.224.900.500 (Primary), L01.700.508.300, L01.224.050.375,\n  H02.403.720.750, N04.590, N04.452.758.625 (Secondary)", "I.2.7; H.3.3; J.3; I.2.9; C.4"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07983v1", "summary": "Large language models (LLMs) show promise for supporting clinical\ndecision-making in complex fields such as rheumatology. Our evaluation shows\nthat smaller language models (SLMs), combined with retrieval-augmented\ngeneration (RAG), achieve higher diagnostic and therapeutic performance than\nlarger models, while requiring substantially less energy and enabling\ncost-efficient, local deployment. These features are attractive for\nresource-limited healthcare. However, expert oversight remains essential, as no\nmodel consistently reached specialist-level accuracy in rheumatology.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07983v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07996", "title": "Skip a Layer or Loop it? Test-Time Depth Adaptation of Pretrained LLMs", "authors": ["Ziyue Li", "Yang Li", "Tianyi Zhou"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      9 pages, 7 figures", "url": "http://arxiv.org/abs/2507.07996v1", "summary": "Can a pretrained neural network adapt its architecture to different inputs\nwithout any finetuning? Do we need all layers for simple tasks, and are they\nadequate for challenging tasks? We found that the layers of a pretrained large\nlanguage model (LLM) can be manipulated as separate modules to build a better\nand even shallower model customized for each test sample. In particular, each\nlayer from the pretrained model can be skipped/pruned or repeated multiple\ntimes as recurrent neural networks (RNN), and stacked with others in arbitrary\norders, yielding a chain-of-layers (CoLa) per sample. This compositional space\ngreatly expands the scope of existing works on looped/recurrent pretrained\nmodules, layer pruning, or early-exit networks. We develop a Monte Carlo Tree\nSearch (MCTS) protocol to explore and identify the optimal CoLa for each sample\nfrom math and commonsense reasoning benchmarks. Compared to a static model of a\nfixed depth, CoLa allows shortcut paths (fast thinking), recurrence of the same\nlayer(s) (slow thinking), and combining both, offering more flexible, dynamic\narchitectures for different inputs. We conduct an extensive analysis of the\nMCTS-optimized CoLa, which leads to two key findings: (1) For >75% of samples\nwith correct predictions by the original LLM, we can find shorter CoLa,\nsuggesting a large space for improving inference efficiency; (2) For >60% of\nsamples with originally incorrect predictions, we can identify CoLa achieving\ncorrect predictions, suggesting a large space of performance enhancement. Our\nresults highlight the shortcomings of using a fixed architecture of pre-trained\nLLMs for inference on different samples and pave the way to unlock the\ngeneralization power of test-time depth adaptation.", "comment": "9 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.07996v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07795", "title": "Robust and Generalizable Heart Rate Estimation via Deep Learning for Remote Photoplethysmography in Complex Scenarios", "authors": ["Kang Cen", "Chang-Hong Fu", "Hong Hong"], "categories": ["cs.CV", "F.2.2"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      7 pages, 3 figures", "url": "http://arxiv.org/abs/2507.07795v1", "summary": "Non-contact remote photoplethysmography (rPPG) technology enables heart rate\nmeasurement from facial videos. However, existing network models still face\nchallenges in accu racy, robustness, and generalization capability under\ncomplex scenarios. This paper proposes an end-to-end rPPG extraction network\nthat employs 3D convolutional neural networks to reconstruct accurate rPPG\nsignals from raw facial videos. We introduce a differential frame fusion module\nthat integrates differential frames with original frames, enabling frame-level\nrepresentations to capture blood volume pulse (BVP) variations. Additionally,\nwe incorporate Temporal Shift Module (TSM) with self-attention mechanisms,\nwhich effectively enhance rPPG features with minimal computational overhead.\nFurthermore, we propose a novel dynamic hybrid loss function that provides\nstronger supervision for the network, effectively mitigating over fitting.\nComprehensive experiments were conducted on not only the PURE and UBFC-rPPG\ndatasets but also the challenging MMPD dataset under complex scenarios,\ninvolving both intra dataset and cross-dataset evaluations, which demonstrate\nthe superior robustness and generalization capability of our network.\nSpecifically, after training on PURE, our model achieved a mean absolute error\n(MAE) of 7.58 on the MMPD test set, outperforming the state-of-the-art models.", "comment": "7 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2507.07795v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2506.00981", "title": "What do self-supervised speech models know about Dutch? Analyzing advantages of language-specific pre-training", "authors": ["Marianne de Heer Kloots", "Hosein Mohebbi", "Charlotte Pouw", "Gaofei Shen", "Willem Zuidema", "Martijn Bentum"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted to Interspeech 2025. For model, code, and materials, see this https URL", "url": "http://arxiv.org/abs/2506.00981v2", "summary": "How language-specific are speech representations learned by self-supervised\nmodels? Existing work has shown that a range of linguistic features can be\nsuccessfully decoded from end-to-end models trained only on speech recordings.\nHowever, it's less clear to what extent pre-training on specific languages\nimproves language-specific linguistic information. Here we test the encoding of\nDutch phonetic and lexical information in internal representations of\nself-supervised Wav2Vec2 models. Pre-training exclusively on Dutch improves the\nrepresentation of Dutch linguistic features as compared to pre-training on\nsimilar amounts of English or larger amounts of multilingual data. This\nlanguage-specific advantage is well-detected by trained clustering or\nclassification probes, and partially observable using zero-shot metrics.\nFurthermore, the language-specific benefit on linguistic feature encoding\naligns with downstream performance on Automatic Speech Recognition.", "comment": "Accepted to Interspeech 2025. For model, code, and materials, see\n  https://github.com/mdhk/SSL-NL-eval", "pdf_url": "http://arxiv.org/pdf/2506.00981v2", "cate": "cs.CL", "date": "2025-06-01", "updated": "2025-07-10"}
{"id": "2507.07562", "title": "The Synergy Dilemma of Long-CoT SFT and RL: Investigating Post-Training Techniques for Reasoning VLMs", "authors": ["Jierun Chen", "Tiezheng Yu", "Haoli Bai", "Lewei Yao", "Jiannan Wu", "Kaican Li", "Fei Mi", "Chaofan Tao", "Lei Zhu", "Manyi Zhang", "Xiaohui Li", "Lu Hou", "Lifeng Shang", "Qun Liu"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07562v1", "summary": "Large vision-language models (VLMs) increasingly adopt post-training\ntechniques such as long chain-of-thought (CoT) supervised fine-tuning (SFT) and\nreinforcement learning (RL) to elicit sophisticated reasoning. While these\nmethods exhibit synergy in language-only models, their joint effectiveness in\nVLMs remains uncertain. We present a systematic investigation into the distinct\nroles and interplay of long-CoT SFT and RL across multiple multimodal reasoning\nbenchmarks. We find that SFT improves performance on difficult questions by\nin-depth, structured reasoning, but introduces verbosity and degrades\nperformance on simpler ones. In contrast, RL promotes generalization and\nbrevity, yielding consistent improvements across all difficulty levels, though\nthe improvements on the hardest questions are less prominent compared to SFT.\nSurprisingly, combining them through two-staged, interleaved, or progressive\ntraining strategies, as well as data mixing and model merging, all fails to\nproduce additive benefits, instead leading to trade-offs in accuracy, reasoning\nstyle, and response length. This ``synergy dilemma'' highlights the need for\nmore seamless and adaptive approaches to unlock the full potential of combined\npost-training techniques for reasoning VLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07562v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2505.23617", "title": "One Trajectory, One Token: Grounded Video Tokenization via Panoptic Sub-object Trajectory", "authors": ["Chenhao Zheng", "Jieyu Zhang", "Mohammadreza Salehi", "Ziqi Gao", "Vishnu Iyengar", "Norimasa Kobori", "Quan Kong", "Ranjay Krishna"], "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025", "url": "http://arxiv.org/abs/2505.23617v2", "summary": "Effective video tokenization is critical for scaling transformer models for\nlong videos. Current approaches tokenize videos using space-time patches,\nleading to excessive tokens and computational inefficiencies. The best token\nreduction strategies degrade performance and barely reduce the number of tokens\nwhen the camera moves. We introduce grounded video tokenization, a paradigm\nthat organizes tokens based on panoptic sub-object trajectories rather than\nfixed patches. Our method aligns with fundamental perceptual principles,\nensuring that tokenization reflects scene complexity rather than video\nduration. We propose TrajViT, a video encoder that extracts object trajectories\nand converts them into semantically meaningful tokens, significantly reducing\nredundancy while maintaining temporal coherence. Trained with contrastive\nlearning, TrajViT significantly outperforms space-time ViT (ViT3D) across\nmultiple video understanding benchmarks, e.g., TrajViT outperforms ViT3D by a\nlarge margin of 6% top-5 recall in average at video-text retrieval task with\n10x token deduction. We also show TrajViT as a stronger model than ViT3D for\nbeing the video encoder for modern VideoLLM, obtaining an average of 5.2%\nperformance improvement across 6 VideoQA benchmarks while having 4x faster\ntraining time and 18x less inference FLOPs. TrajViT is the first efficient\nencoder to consistently outperform ViT3D across diverse video analysis tasks,\nmaking it a robust and scalable solution.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2505.23617v2", "cate": "cs.CV", "date": "2025-05-29", "updated": "2025-07-09"}
{"id": "2507.07522", "title": "NLGCL: Naturally Existing Neighbor Layers Graph Contrastive Learning for Recommendation", "authors": ["Jinfeng Xu", "Zheyu Chen", "Shuo Yang", "Jinze Li", "Hewei Wang", "Wei Wang", "Xiping Hu", "Edith Ngai"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Accepted by RecSys 2025 as Spotlight Oral", "url": "http://arxiv.org/abs/2507.07522v1", "summary": "Graph Neural Networks (GNNs) are widely used in collaborative filtering to\ncapture high-order user-item relationships. To address the data sparsity\nproblem in recommendation systems, Graph Contrastive Learning (GCL) has emerged\nas a promising paradigm that maximizes mutual information between contrastive\nviews. However, existing GCL methods rely on augmentation techniques that\nintroduce semantically irrelevant noise and incur significant computational and\nstorage costs, limiting effectiveness and efficiency.\n  To overcome these challenges, we propose NLGCL, a novel contrastive learning\nframework that leverages naturally contrastive views between neighbor layers\nwithin GNNs. By treating each node and its neighbors in the next layer as\npositive pairs, and other nodes as negatives, NLGCL avoids augmentation-based\nnoise while preserving semantic relevance. This paradigm eliminates costly view\nconstruction and storage, making it computationally efficient and practical for\nreal-world scenarios. Extensive experiments on four public datasets demonstrate\nthat NLGCL outperforms state-of-the-art baselines in effectiveness and\nefficiency.", "comment": "Accepted by RecSys 2025 as Spotlight Oral", "pdf_url": "http://arxiv.org/pdf/2507.07522v1", "cate": "cs.IR", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07284", "title": "A Robust, Open-Source Framework for Spiking Neural Networks on Low-End FPGAs", "authors": ["Andrew Fan", "Simon D. Levy"], "categories": ["cs.NE"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07284v1", "summary": "As the demand for compute power in traditional neural networks has increased\nsignificantly, spiking neural networks (SNNs) have emerged as a potential\nsolution to increasingly power-hungry neural networks. By operating on 0/1\nspikes emitted by neurons instead of arithmetic multiply-and-accumulate\noperations, SNNs propagate information temporally and spatially, allowing for\nmore efficient compute power. To this end, many architectures for accelerating\nand simulating SNNs have been developed, including Loihi, TrueNorth, and\nSpiNNaker. However, these chips are largely inaccessible to the wider\ncommunity. Field programmable gate arrays (FPGAs) have been explored to serve\nas a middle ground between neuromorphic and non-neuromorphic hardware, but many\nproposed architectures require expensive high-end FPGAs or target a single SNN\ntopology. This paper presents a framework consisting of a robust SNN\nacceleration architecture and a Pytorch-based SNN model compiler. Targeting\nany-to-any and/or fully connected SNNs, the FPGA architecture features a\nsynaptic array that tiles across the SNN to propagate spikes. The architecture\ntargets low-end FPGAs and requires very little (6358 LUT, 40.5 BRAM) resources.\nThe framework, tested on a low-end Xilinx Artix-7 FPGA at 100 MHz, achieves\ncompetitive speed in recognizing MNIST digits (0.52 ms/img). Further\nexperiments also show accurate simulation of hand coded any-to-any spiking\nneural networks on toy problems. All code and setup instructions are available\nat\nhttps://github.com/im-afan/snn-fpga}{\\texttt{https://github.com/im-afan/snn-fpga.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07284v1", "cate": "cs.NE", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07990", "title": "Multi-Granular Spatio-Temporal Token Merging for Training-Free Acceleration of Video LLMs", "authors": ["Jeongseok Hyun", "Sukjun Hwang", "Su Ho Han", "Taeoh Kim", "Inwoong Lee", "Dongyoon Wee", "Joon-Young Lee", "Seon Joo Kim", "Minho Shim"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at ICCV2025; Project page: this https URL", "url": "http://arxiv.org/abs/2507.07990v1", "summary": "Video large language models (LLMs) achieve strong video understanding by\nleveraging a large number of spatio-temporal tokens, but suffer from quadratic\ncomputational scaling with token count. To address this, we propose a\ntraining-free spatio-temporal token merging method, named STTM. Our key insight\nis to exploit local spatial and temporal redundancy in video data which has\nbeen overlooked in prior work. STTM first transforms each frame into\nmulti-granular spatial tokens using a coarse-to-fine search over a quadtree\nstructure, then performs directed pairwise merging across the temporal\ndimension. This decomposed merging approach outperforms existing token\nreduction methods across six video QA benchmarks. Notably, STTM achieves a\n2$\\times$ speed-up with only a 0.5% accuracy drop under a 50% token budget, and\na 3$\\times$ speed-up with just a 2% drop under a 30% budget. Moreover, STTM is\nquery-agnostic, allowing KV cache reuse across different questions for the same\nvideo. The project page is available at https://www.jshyun.me/projects/sttm.", "comment": "Accepted at ICCV2025; Project page:\n  https://www.jshyun.me/projects/sttm", "pdf_url": "http://arxiv.org/pdf/2507.07990v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07150", "title": "Class conditional conformal prediction for multiple inputs by p-value aggregation", "authors": ["Jean-Baptiste Fermanian", "Mohamed Hebiri", "Joseph Salmon"], "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07150v1", "summary": "Conformal prediction methods are statistical tools designed to quantify\nuncertainty and generate predictive sets with guaranteed coverage\nprobabilities. This work introduces an innovative refinement to these methods\nfor classification tasks, specifically tailored for scenarios where multiple\nobservations (multi-inputs) of a single instance are available at prediction\ntime. Our approach is particularly motivated by applications in citizen\nscience, where multiple images of the same plant or animal are captured by\nindividuals. Our method integrates the information from each observation into\nconformal prediction, enabling a reduction in the size of the predicted label\nset while preserving the required class-conditional coverage guarantee. The\napproach is based on the aggregation of conformal p-values computed from each\nobservation of a multi-input. By exploiting the exact distribution of these\np-values, we propose a general aggregation framework using an abstract scoring\nfunction, encompassing many classical statistical tools. Knowledge of this\ndistribution also enables refined versions of standard strategies, such as\nmajority voting. We evaluate our method on simulated and real data, with a\nparticular focus on Pl@ntNet, a prominent citizen science platform that\nfacilitates the collection and identification of plant species through\nuser-submitted images.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07150v1", "cate": "stat.ML", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07802", "title": "Synergistic Prompting for Robust Visual Recognition with Missing Modalities", "authors": ["Zhihui Zhang", "Luanyuan Dai", "Qika Lin", "Yunfeng Diao", "Guangyin Jin", "Yufei Guo", "Jing Zhang", "Xiaoshuai Hao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07802v1", "summary": "Large-scale multi-modal models have demonstrated remarkable performance\nacross various visual recognition tasks by leveraging extensive paired\nmulti-modal training data. However, in real-world applications, the presence of\nmissing or incomplete modality inputs often leads to significant performance\ndegradation. Recent research has focused on prompt-based strategies to tackle\nthis issue; however, existing methods are hindered by two major limitations:\n(1) static prompts lack the flexibility to adapt to varying missing-data\nconditions, and (2) basic prompt-tuning methods struggle to ensure reliable\nperformance when critical modalities are missing.To address these challenges,\nwe propose a novel Synergistic Prompting (SyP) framework for robust visual\nrecognition with missing modalities. The proposed SyP introduces two key\ninnovations: (I) a Dynamic Adapter, which computes adaptive scaling factors to\ndynamically generate prompts, replacing static parameters for flexible\nmulti-modal adaptation, and (II) a Synergistic Prompting Strategy, which\ncombines static and dynamic prompts to balance information across modalities,\nensuring robust reasoning even when key modalities are missing. The proposed\nSyP achieves significant performance improvements over existing approaches\nacross three widely-used visual recognition datasets, demonstrating robustness\nunder diverse missing rates and conditions. Extensive experiments and ablation\nstudies validate its effectiveness in handling missing modalities, highlighting\nits superior adaptability and reliability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07802v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2506.04391", "title": "Benchmarking Time-localized Explanations for Audio Classification Models", "authors": ["Cecilia Bola√±os", "Leonardo Pepino", "Martin Meza", "Luciana Ferrer"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.04391v2", "summary": "Most modern approaches for audio processing are opaque, in the sense that\nthey do not provide an explanation for their decisions. For this reason,\nvarious methods have been proposed to explain the outputs generated by these\nmodels. Good explanations can result in interesting insights about the data or\nthe model, as well as increase trust in the system. Unfortunately, evaluating\nthe quality of explanations is far from trivial since, for most tasks, there is\nno clear ground truth explanation to use as reference. In this work, we propose\na benchmark for time-localized explanations for audio classification models\nthat uses time annotations of target events as a proxy for ground truth\nexplanations. We use this benchmark to systematically optimize and compare\nvarious approaches for model-agnostic post-hoc explanation, obtaining, in some\ncases, close to perfect explanations. Finally, we illustrate the utility of the\nexplanations for uncovering spurious correlations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.04391v2", "cate": "cs.SD", "date": "2025-06-04", "updated": "2025-07-10"}
{"id": "2507.07630", "title": "Exploring the Limits of Model Compression in LLMs: A Knowledge Distillation Study on QA Tasks", "authors": ["Joyeeta Datta", "Niclas Doll", "Qusai Ramadan", "Zeyd Boukhers"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted four publication at the 26th Meeting of the Special Interest on Discourse and Dialogue", "url": "http://arxiv.org/abs/2507.07630v1", "summary": "Large Language Models (LLMs) have demonstrated outstanding performance across\na range of NLP tasks, however, their computational demands hinder their\ndeployment in real-world, resource-constrained environments. This work\ninvestigates the extent to which LLMs can be compressed using Knowledge\nDistillation (KD) while maintaining strong performance on Question Answering\n(QA) tasks. We evaluate student models distilled from the Pythia and Qwen2.5\nfamilies on two QA benchmarks, SQuAD and MLQA, under zero-shot and one-shot\nprompting conditions. Results show that student models retain over 90% of their\nteacher models' performance while reducing parameter counts by up to 57.1%.\nFurthermore, one-shot prompting yields additional performance gains over\nzero-shot setups for both model families. These findings underscore the\ntrade-off between model efficiency and task performance, demonstrating that KD,\ncombined with minimal prompting, can yield compact yet capable QA systems\nsuitable for resource-constrained applications.", "comment": "Accepted four publication at the 26th Meeting of the Special Interest\n  on Discourse and Dialogue", "pdf_url": "http://arxiv.org/pdf/2507.07630v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07909", "title": "Document Similarity Enhanced IPS Estimation for Unbiased Learning to Rank", "authors": ["Zeyan Liang", "Graham McDonald", "Iadh Ounis"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07909v1", "summary": "Learning to Rank (LTR) models learn from historical user interactions, such\nas user clicks. However, there is an inherent bias in the clicks of users due\nto position bias, i.e., users are more likely to click highly-ranked documents\nthan low-ranked documents. To address this bias when training LTR models, many\napproaches from the literature re-weight the users' click data using Inverse\nPropensity Scoring (IPS). IPS re-weights the user's clicks proportionately to\nthe position in the historical ranking that a document was placed when it was\nclicked since low-ranked documents are less likely to be seen by a user. In\nthis paper, we argue that low-ranked documents that are similar to\nhighly-ranked relevant documents are also likely to be relevant. Moreover,\naccounting for the similarity of low-ranked documents to highly ranked relevant\ndocuments when calculating IPS can more effectively mitigate the effects of\nposition bias. Therefore, we propose an extension to IPS, called IPSsim, that\ntakes into consideration the similarity of documents when estimating IPS. We\nevaluate our IPSsim estimator using two large publicly available LTR datasets\nunder a number of simulated user click settings, and with different numbers of\ntraining clicks. Our experiments show that our IPSsim estimator is more\neffective than the existing IPS estimators for learning an unbiased LTR model,\nparticularly in top-n settings when n >= 30. For example, when n = 50, our\nIPSsim estimator achieves a statistically significant ~3% improvement (p <\n0.05) in terms of NDCG compared to the Doubly Robust estimator from the\nliterature.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07909v1", "cate": "cs.IR", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07874", "title": "Homeostatic Adaptation of Optimal Population Codes under Metabolic Stress", "authors": ["Yi-Chun Hung", "Gregory Schwartz", "Emily A. Cooper", "Emma Alexander"], "categories": ["cs.NE"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07874v1", "summary": "Information processing in neural populations is inherently constrained by\nmetabolic resource limits and noise properties, with dynamics that are not\naccurately described by existing mathematical models. Recent data, for example,\nshows that neurons in mouse visual cortex go into a \"low power mode\" in which\nthey maintain firing rate homeostasis while expending less energy. This\nadaptation leads to increased neuronal noise and tuning curve flattening in\nresponse to metabolic stress. We have developed a theoretical population coding\nframework that captures this behavior using two novel, surprisingly simple\nconstraints: an approximation of firing rate homeostasis and an energy limit\ntied to noise levels via biophysical simulation. A key feature of our\ncontribution is an energy budget model directly connecting adenosine\ntriphosphate (ATP) use in cells to a fully explainable mathematical framework\nthat generalizes existing optimal population codes. Specifically, our\nsimulation provides an energy-dependent dispersed Poisson noise model, based on\nthe assumption that the cell will follow an optimal decay path to produce the\nleast-noisy spike rate that is possible at a given cellular energy budget. Each\nstate along this optimal path is associated with properties (resting potential\nand leak conductance) which can be measured in electrophysiology experiments\nand have been shown to change under prolonged caloric deprivation. We\nanalytically derive the optimal coding strategy for neurons under varying\nenergy budgets and coding goals, and show how our method uniquely captures how\npopulations of tuning curves adapt while maintaining homeostasis, as has been\nobserved empirically.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07874v1", "cate": "cs.NE", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07995", "title": "Single-pass Adaptive Image Tokenization for Minimum Program Search", "authors": ["Shivam Duggal", "Sanghyun Byun", "William T. Freeman", "Antonio Torralba", "Phillip Isola"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Code at: this https URL Keywords: Representation Learning, Adaptive Tokenization, Compression, Algorithmic Information Theory, Kolmogorov Complexity, Upside-Down RL", "url": "http://arxiv.org/abs/2507.07995v1", "summary": "According to Algorithmic Information Theory (AIT) -- Intelligent\nrepresentations compress data into the shortest possible program that can\nreconstruct its content, exhibiting low Kolmogorov Complexity (KC). In\ncontrast, most visual representation learning systems use fixed-length\nrepresentations for all inputs, ignoring variations in complexity or\nfamiliarity. Recent adaptive tokenization methods address this by allocating\nvariable-length representations but typically require test-time search over\nmultiple encodings to find the most predictive one. Inspired by Kolmogorov\nComplexity principles, we propose a single-pass adaptive tokenizer, KARL, which\npredicts the appropriate number of tokens for an image in a single forward\npass, halting once its approximate KC is reached. The token count serves as a\nproxy for the minimum description length. KARL's training procedure closely\nresembles the Upside-Down Reinforcement Learning paradigm, as it learns to\nconditionally predict token halting based on a desired reconstruction quality.\nKARL matches the performance of recent adaptive tokenizers while operating in a\nsingle pass. We present scaling laws for KARL, analyzing the role of\nencoder/decoder size, continuous vs. discrete tokenization and more.\nAdditionally, we offer a conceptual study drawing an analogy between Adaptive\nImage Tokenization and Algorithmic Information Theory, examining the predicted\nimage complexity (KC) across axes such as structure vs. noise and in- vs.\nout-of-distribution familiarity -- revealing alignment with human intuition.", "comment": "Code at: https://github.com/ShivamDuggal4/karl Keywords:\n  Representation Learning, Adaptive Tokenization, Compression, Algorithmic\n  Information Theory, Kolmogorov Complexity, Upside-Down RL", "pdf_url": "http://arxiv.org/pdf/2507.07995v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07156", "title": "Topological Machine Learning with Unreduced Persistence Diagrams", "authors": ["Nicole Abreu", "Parker B. Edwards", "Francis Motta"], "categories": ["stat.ML", "cs.CG", "cs.LG", "math.AT", "55N31"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "Comments:      10 figures, 2 tables, 8 pages(without appendix and references)", "url": "http://arxiv.org/abs/2507.07156v1", "summary": "Supervised machine learning pipelines trained on features derived from\npersistent homology have been experimentally observed to ignore much of the\ninformation contained in a persistence diagram. Computing persistence diagrams\nis often the most computationally demanding step in such a pipeline, however.\nTo explore this, we introduce several methods to generate topological feature\nvectors from unreduced boundary matrices. We compared the performance of\npipelines trained on vectorizations of unreduced PDs to vectorizations of\nfully-reduced PDs across several data and task types. Our results indicate that\nmodels trained on PDs built from unreduced diagrams can perform on par and even\noutperform those trained on fully-reduced diagrams on some tasks. This\nobservation suggests that machine learning pipelines which incorporate\ntopology-based features may benefit in terms of computational cost and\nperformance by utilizing information contained in unreduced boundary matrices.", "comment": "10 figures, 2 tables, 8 pages(without appendix and references)", "pdf_url": "http://arxiv.org/pdf/2507.07156v1", "cate": "stat.ML", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07811", "title": "Patient-specific vs Multi-Patient Vision Transformer for Markerless Tumor Motion Forecasting", "authors": ["Gauthier Rotsart de Hertaing", "Dani Manjah", "Benoit Macq"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07811v1", "summary": "Background: Accurate forecasting of lung tumor motion is essential for\nprecise dose delivery in proton therapy. While current markerless methods\nmostly rely on deep learning, transformer-based architectures remain unexplored\nin this domain, despite their proven performance in trajectory forecasting.\n  Purpose: This work introduces a markerless forecasting approach for lung\ntumor motion using Vision Transformers (ViT). Two training strategies are\nevaluated under clinically realistic constraints: a patient-specific (PS)\napproach that learns individualized motion patterns, and a multi-patient (MP)\nmodel designed for generalization. The comparison explicitly accounts for the\nlimited number of images that can be generated between planning and treatment\nsessions.\n  Methods: Digitally reconstructed radiographs (DRRs) derived from planning\n4DCT scans of 31 patients were used to train the MP model; a 32nd patient was\nheld out for evaluation. PS models were trained using only the target patient's\nplanning data. Both models used 16 DRRs per input and predicted tumor motion\nover a 1-second horizon. Performance was assessed using Average Displacement\nError (ADE) and Final Displacement Error (FDE), on both planning (T1) and\ntreatment (T2) data.\n  Results: On T1 data, PS models outperformed MP models across all training set\nsizes, especially with larger datasets (up to 25,000 DRRs, p < 0.05). However,\nMP models demonstrated stronger robustness to inter-fractional anatomical\nvariability and achieved comparable performance on T2 data without retraining.\n  Conclusions: This is the first study to apply ViT architectures to markerless\ntumor motion forecasting. While PS models achieve higher precision, MP models\noffer robust out-of-the-box performance, well-suited for time-constrained\nclinical settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07811v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.03251", "title": "Toward Efficient Speech Emotion Recognition via Spectral Learning and Attention", "authors": ["HyeYoung Lee", "Muhammad Nadeem"], "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.03251v2", "summary": "Speech Emotion Recognition (SER) traditionally relies on auditory data\nanalysis for emotion classification. Several studies have adopted different\nmethods for SER. However, existing SER methods often struggle to capture subtle\nemotional variations and generalize across diverse datasets. In this article,\nwe use Mel-Frequency Cepstral Coefficients (MFCCs) as spectral features to\nbridge the gap between computational emotion processing and human auditory\nperception. To further improve robustness and feature diversity, we propose a\nnovel 1D-CNN-based SER framework that integrates data augmentation techniques.\nMFCC features extracted from the augmented data are processed using a 1D\nConvolutional Neural Network (CNN) architecture enhanced with channel and\nspatial attention mechanisms. These attention modules allow the model to\nhighlight key emotional patterns, enhancing its ability to capture subtle\nvariations in speech signals. The proposed method delivers cutting-edge\nperformance, achieving the accuracy of 97.49% for SAVEE, 99.23% for RAVDESS,\n89.31% for CREMA-D, 99.82% for TESS, 99.53% for EMO-DB, and 96.39% for EMOVO.\nExperimental results show new benchmarks in SER, demonstrating the\neffectiveness of our approach in recognizing emotional expressions with high\nprecision. Our evaluation demonstrates that the integration of advanced Deep\nLearning (DL) methods substantially enhances generalization across diverse\ndatasets, underscoring their potential to advance SER for real-world deployment\nin assistive technologies and human-computer interaction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.03251v2", "cate": "cs.SD", "date": "2025-07-04", "updated": "2025-07-10"}
{"id": "2507.07634", "title": "FrugalRAG: Learning to retrieve and reason for multi-hop QA", "authors": ["Abhinav Java", "Srivathsan Koundinyan", "Nagarajan Natarajan", "Amit Sharma"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted at ICML Workshop: Efficient Systems for Foundation Models", "url": "http://arxiv.org/abs/2507.07634v1", "summary": "We consider the problem of answering complex questions, given access to a\nlarge unstructured document corpus. The de facto approach to solving the\nproblem is to leverage language models that (iteratively) retrieve and reason\nthrough the retrieved documents, until the model has sufficient information to\ngenerate an answer. Attempts at improving this approach focus on\nretrieval-augmented generation (RAG) metrics such as accuracy and recall and\ncan be categorized into two types: (a) fine-tuning on large question answering\n(QA) datasets augmented with chain-of-thought traces, and (b) leveraging\nRL-based fine-tuning techniques that rely on question-document relevance\nsignals. However, efficiency in the number of retrieval searches is an equally\nimportant metric, which has received less attention. In this work, we show\nthat: (1) Large-scale fine-tuning is not needed to improve RAG metrics,\ncontrary to popular claims in recent literature. Specifically, a standard ReAct\npipeline with improved prompts can outperform state-of-the-art methods on\nbenchmarks such as HotPotQA. (2) Supervised and RL-based fine-tuning can help\nRAG from the perspective of frugality, i.e., the latency due to number of\nsearches at inference time. For example, we show that we can achieve\ncompetitive RAG metrics at nearly half the cost (in terms of number of\nsearches) on popular RAG benchmarks, using the same base model, and at a small\ntraining cost (1000 examples).", "comment": "Accepted at ICML Workshop: Efficient Systems for Foundation Models", "pdf_url": "http://arxiv.org/pdf/2507.07634v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07924", "title": "Measuring Hypothesis Testing Errors in the Evaluation of Retrieval Systems", "authors": ["Jack McKechnie", "Graham McDonald", "Craig Macdonald"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07924v1", "summary": "The evaluation of Information Retrieval (IR) systems typically uses\nquery-document pairs with corresponding human-labelled relevance assessments\n(qrels). These qrels are used to determine if one system is better than another\nbased on average retrieval performance. Acquiring large volumes of human\nrelevance assessments is expensive. Therefore, more efficient relevance\nassessment approaches have been proposed, necessitating comparisons between\nqrels to ascertain their efficacy. Discriminative power, i.e. the ability to\ncorrectly identify significant differences between systems, is important for\ndrawing accurate conclusions on the robustness of qrels. Previous work has\nmeasured the proportion of pairs of systems that are identified as\nsignificantly different and has quantified Type I statistical errors. Type I\nerrors lead to incorrect conclusions due to false positive significance tests.\nWe argue that also identifying Type II errors (false negatives) is important as\nthey lead science in the wrong direction. We quantify Type II errors and\npropose that balanced classification metrics, such as balanced accuracy, can be\nused to portray the discriminative power of qrels. We perform experiments using\nqrels generated using alternative relevance assessment methods to investigate\nmeasuring hypothesis testing errors in IR evaluation. We find that additional\ninsights into the discriminative power of qrels can be gained by quantifying\nType II errors, and that balanced classification metrics can be used to give an\noverall summary of discriminative power in one, easily comparable, number.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07924v1", "cate": "cs.IR", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2408.07517", "title": "Advancing Spatio-Temporal Processing in Spiking Neural Networks through Adaptation", "authors": ["Maximilian Baronig", "Romain Ferrand", "Silvester Sabathiel", "Robert Legenstein"], "categories": ["cs.NE"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "Comments:      Published in Nature Communications, July 2025", "url": "http://arxiv.org/abs/2408.07517v3", "summary": "Implementations of spiking neural networks on neuromorphic hardware promise\norders of magnitude less power consumption than their non-spiking counterparts.\nThe standard neuron model for spike-based computation on such systems has long\nbeen the leaky integrate-and-fire (LIF) neuron. A computationally light\naugmentation of the LIF neuron model with an adaptation mechanism has recently\nbeen shown to exhibit superior performance on spatio-temporal processing tasks.\nThe root of the superiority of these so-called adaptive LIF neurons however is\nnot well understood. In this article, we thoroughly analyze the dynamical,\ncomputational, and learning properties of adaptive LIF neurons and networks\nthereof. Our investigation reveals significant challenges related to stability\nand parameterization when employing the conventional Euler-Forward\ndiscretization for this class of models. We report a rigorous theoretical and\nempirical demonstration that these challenges can be effectively addressed by\nadopting an alternative discretization approach - the Symplectic Euler method,\nallowing to improve over state-of-the-art performances on common event-based\nbenchmark datasets. Our further analysis of the computational properties of\nnetworks of adaptive LIF neurons shows that they are particularly well suited\nto exploit the spatio-temporal structure of input sequences without any\nnormalization techniques.", "comment": "Published in Nature Communications, July 2025", "pdf_url": "http://arxiv.org/pdf/2408.07517v3", "cate": "cs.NE", "date": "2024-08-14", "updated": "2025-07-10"}
{"id": "2507.07998", "title": "PyVision: Agentic Vision with Dynamic Tooling", "authors": ["Shitian Zhao", "Haoquan Zhang", "Shaoheng Lin", "Ming Li", "Qilong Wu", "Kaipeng Zhang", "Chen Wei"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      26 Pages, 10 Figures, Technical report", "url": "http://arxiv.org/abs/2507.07998v1", "summary": "LLMs are increasingly deployed as agents, systems capable of planning,\nreasoning, and dynamically calling external tools. However, in visual\nreasoning, prior approaches largely remain limited by predefined workflows and\nstatic toolsets. In this report, we present PyVision, an interactive,\nmulti-turn framework that enables MLLMs to autonomously generate, execute, and\nrefine Python-based tools tailored to the task at hand, unlocking flexible and\ninterpretable problem-solving. We develop a taxonomy of the tools created by\nPyVision and analyze their usage across a diverse set of benchmarks.\nQuantitatively, PyVision achieves consistent performance gains, boosting\nGPT-4.1 by +7.8% on V* and Claude-4.0-Sonnet by +31.1% on VLMsAreBlind-mini.\nThese results point to a broader shift: dynamic tooling allows models not just\nto use tools, but to invent them, advancing toward more agentic visual\nreasoning.", "comment": "26 Pages, 10 Figures, Technical report", "pdf_url": "http://arxiv.org/pdf/2507.07998v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07159", "title": "Large-scale portfolio optimization with variational neural annealing", "authors": ["Nishan Ranabhat", "Behnam Javanparast", "David Goerz", "Estelle Inack"], "categories": ["cond-mat.dis-nn", "cond-mat.stat-mech", "cs.LG", "q-fin.PM"], "primary_category": "Subjects:       Disordered Systems and Neural Networks (cond-mat.dis-nn)", "pdf_link": null, "comments": "Comments:      16 pages, 13 figures, 1 table", "url": "http://arxiv.org/abs/2507.07159v1", "summary": "Portfolio optimization is a routine asset management operation conducted in\nfinancial institutions around the world. However, under real-world constraints\nsuch as turnover limits and transaction costs, its formulation becomes a\nmixed-integer nonlinear program that current mixed-integer optimizers often\nstruggle to solve. We propose mapping this problem onto a classical Ising-like\nHamiltonian and solving it with Variational Neural Annealing (VNA), via its\nclassical formulation implemented using autoregressive neural networks. We\ndemonstrate that VNA can identify near-optimal solutions for portfolios\ncomprising more than 2,000 assets and yields performance comparable to that of\nstate-of-the-art optimizers, such as Mosek, while exhibiting faster convergence\non hard instances. Finally, we present a dynamical finite-size scaling analysis\napplied to the S&P 500, Russell 1000, and Russell 3000 indices, revealing\nuniversal behavior and polynomial annealing time scaling of the VNA algorithm\non portfolio optimization problems.", "comment": "16 pages, 13 figures, 1 table", "pdf_url": "http://arxiv.org/pdf/2507.07159v1", "cate": "cond-mat.dis-nn", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07831", "title": "Rethinking Query-based Transformer for Continual Image Segmentation", "authors": ["Yuchen Zhu", "Cheng Shi", "Dingyou Wang", "Jiajin Tang", "Zhengxuan Wei", "Yu Wu", "Guanbin Li", "Sibei Yang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      This work is accepted by CVPR 2025", "url": "http://arxiv.org/abs/2507.07831v1", "summary": "Class-incremental/Continual image segmentation (CIS) aims to train an image\nsegmenter in stages, where the set of available categories differs at each\nstage. To leverage the built-in objectness of query-based transformers, which\nmitigates catastrophic forgetting of mask proposals, current methods often\ndecouple mask generation from the continual learning process. This study,\nhowever, identifies two key issues with decoupled frameworks: loss of\nplasticity and heavy reliance on input data order. To address these, we conduct\nan in-depth investigation of the built-in objectness and find that highly\naggregated image features provide a shortcut for queries to generate masks\nthrough simple feature alignment. Based on this, we propose SimCIS, a simple\nyet powerful baseline for CIS. Its core idea is to directly select image\nfeatures for query assignment, ensuring \"perfect alignment\" to preserve\nobjectness, while simultaneously allowing queries to select new classes to\npromote plasticity. To further combat catastrophic forgetting of categories, we\nintroduce cross-stage consistency in selection and an innovative \"visual\nquery\"-based replay mechanism. Experiments demonstrate that SimCIS consistently\noutperforms state-of-the-art methods across various segmentation tasks,\nsettings, splits, and input data orders. All models and codes will be made\npublicly available at https://github.com/SooLab/SimCIS.", "comment": "This work is accepted by CVPR 2025", "pdf_url": "http://arxiv.org/pdf/2507.07831v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07640", "title": "Lost in Pronunciation: Detecting Chinese Offensive Language Disguised by Phonetic Cloaking Replacement", "authors": ["Haotan Guo", "Jianfei He", "Jiayuan Ma", "Hongbin Na", "Zimu Wang", "Haiyang Zhang", "Qi Chen", "Wei Wang", "Zijing Shi", "Tao Shen", "Ling Chen"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      In progress", "url": "http://arxiv.org/abs/2507.07640v1", "summary": "Phonetic Cloaking Replacement (PCR), defined as the deliberate use of\nhomophonic or near-homophonic variants to hide toxic intent, has become a major\nobstacle to Chinese content moderation. While this problem is well-recognized,\nexisting evaluations predominantly rely on rule-based, synthetic perturbations\nthat ignore the creativity of real users. We organize PCR into a four-way\nsurface-form taxonomy and compile \\ours, a dataset of 500 naturally occurring,\nphonetically cloaked offensive posts gathered from the RedNote platform.\nBenchmarking state-of-the-art LLMs on this dataset exposes a serious weakness:\nthe best model reaches only an F1-score of 0.672, and zero-shot\nchain-of-thought prompting pushes performance even lower. Guided by error\nanalysis, we revisit a Pinyin-based prompting strategy that earlier studies\njudged ineffective and show that it recovers much of the lost accuracy. This\nstudy offers the first comprehensive taxonomy of Chinese PCR, a realistic\nbenchmark that reveals current detectors' limits, and a lightweight mitigation\ntechnique that advances research on robust toxicity detection.", "comment": "In progress", "pdf_url": "http://arxiv.org/pdf/2507.07640v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07700", "title": "Rethinking the Privacy of Text Embeddings: A Reproducibility Study of \"Text Embeddings Reveal (Almost) As Much As Text\"", "authors": ["Dominykas Seputis", "Yongkang Li", "Karsten Langerak", "Serghei Mihailov"], "categories": ["cs.CL", "cs.IR"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      This paper has been accepted for oral presentation in the reproducibility track at RecSys 2025", "url": "http://arxiv.org/abs/2507.07700v1", "summary": "Text embeddings are fundamental to many natural language processing (NLP)\ntasks, extensively applied in domains such as recommendation systems and\ninformation retrieval (IR). Traditionally, transmitting embeddings instead of\nraw text has been seen as privacy-preserving. However, recent methods such as\nVec2Text challenge this assumption by demonstrating that controlled decoding\ncan successfully reconstruct original texts from black-box embeddings. The\nunexpectedly strong results reported by Vec2Text motivated us to conduct\nfurther verification, particularly considering the typically non-intuitive and\nopaque structure of high-dimensional embedding spaces. In this work, we\nreproduce the Vec2Text framework and evaluate it from two perspectives: (1)\nvalidating the original claims, and (2) extending the study through targeted\nexperiments. First, we successfully replicate the original key results in both\nin-domain and out-of-domain settings, with only minor discrepancies arising due\nto missing artifacts, such as model checkpoints and dataset splits.\nFurthermore, we extend the study by conducting a parameter sensitivity\nanalysis, evaluating the feasibility of reconstructing sensitive inputs (e.g.,\npasswords), and exploring embedding quantization as a lightweight privacy\ndefense. Our results show that Vec2Text is effective under ideal conditions,\ncapable of reconstructing even password-like sequences that lack clear\nsemantics. However, we identify key limitations, including its sensitivity to\ninput sequence length. We also find that Gaussian noise and quantization\ntechniques can mitigate the privacy risks posed by Vec2Text, with quantization\noffering a simpler and more widely applicable solution. Our findings emphasize\nthe need for caution in using text embeddings and highlight the importance of\nfurther research into robust defense mechanisms for NLP systems.", "comment": "This paper has been accepted for oral presentation in the\n  reproducibility track at RecSys 2025", "pdf_url": "http://arxiv.org/pdf/2507.07700v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2503.20286", "title": "Bridging Evolutionary Multiobjective Optimization and GPU Acceleration via Tensorization", "authors": ["Zhenyu Liang", "Hao Li", "Naiwei Yu", "Kebin Sun", "Ran Cheng"], "categories": ["cs.NE"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "Comments:      Accepted by IEEE TEVC", "url": "http://arxiv.org/abs/2503.20286v5", "summary": "Evolutionary multiobjective optimization (EMO) has made significant strides\nover the past two decades. However, as problem scales and complexities\nincrease, traditional EMO algorithms face substantial performance limitations\ndue to insufficient parallelism and scalability. While most work has focused on\nalgorithm design to address these challenges, little attention has been given\nto hardware acceleration, thereby leaving a clear gap between EMO algorithms\nand advanced computing devices, such as GPUs. To bridge the gap, we propose to\nparallelize EMO algorithms on GPUs via the tensorization methodology. By\nemploying tensorization, the data structures and operations of EMO algorithms\nare transformed into concise tensor representations, which seamlessly enables\nautomatic utilization of GPU computing. We demonstrate the effectiveness of our\napproach by applying it to three representative EMO algorithms: NSGA-III,\nMOEA/D, and HypE. To comprehensively assess our methodology, we introduce a\nmultiobjective robot control benchmark using a GPU-accelerated physics engine.\nOur experiments show that the tensorized EMO algorithms achieve speedups of up\nto 1113x compared to their CPU-based counterparts, while maintaining solution\nquality and effectively scaling population sizes to hundreds of thousands.\nFurthermore, the tensorized EMO algorithms efficiently tackle complex\nmultiobjective robot control tasks, producing high-quality solutions with\ndiverse behaviors. Source codes are available at\nhttps://github.com/EMI-Group/evomo.", "comment": "Accepted by IEEE TEVC", "pdf_url": "http://arxiv.org/pdf/2503.20286v5", "cate": "cs.NE", "date": "2025-03-26", "updated": "2025-07-10"}
{"id": "2507.07999", "title": "Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and Methodology", "authors": ["Haochen Wang", "Xiangtai Li", "Zilong Huang", "Anran Wang", "Jiacong Wang", "Tao Zhang", "Jiani Zheng", "Sule Bai", "Zijian Kang", "Jiashi Feng", "Zhuochen Wang", "Zhaoxiang Zhang"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07999v1", "summary": "Models like OpenAI-o3 pioneer visual grounded reasoning by dynamically\nreferencing visual regions, just like human \"thinking with images\". However, no\nbenchmark exists to evaluate these capabilities holistically. To bridge this\ngap, we propose TreeBench (Traceable Evidence Evaluation Benchmark), a\ndiagnostic benchmark built on three principles: (1) focused visual perception\nof subtle targets in complex scenes, (2) traceable evidence via bounding box\nevaluation, and (3) second-order reasoning to test object interactions and\nspatial hierarchies beyond simple object localization. Prioritizing images with\ndense objects, we initially sample 1K high-quality images from SA-1B, and\nincorporate eight LMM experts to manually annotate questions, candidate\noptions, and answers for each image. After three stages of quality control,\nTreeBench consists of 405 challenging visual question-answering pairs, even the\nmost advanced models struggle with this benchmark, where none of them reach 60%\naccuracy, e.g., OpenAI-o3 scores only 54.87. Furthermore, we introduce TreeVGR\n(Traceable Evidence Enhanced Visual Grounded Reasoning), a training paradigm to\nsupervise localization and reasoning jointly with reinforcement learning,\nenabling accurate localizations and explainable reasoning pathways. Initialized\nfrom Qwen2.5-VL-7B, it improves V* Bench (+16.8), MME-RealWorld (+12.6), and\nTreeBench (+13.4), proving traceability is key to advancing vision-grounded\nreasoning. The code is available at https://github.com/Haochen-Wang409/TreeVGR.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07999v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07281", "title": "Almost Sure Convergence for the Last Iterate of Stochastic Gradient Descent Schemes", "authors": ["Marcel Hudiani"], "categories": ["math.OC", "cs.LG"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07281v1", "summary": "We study the almost sure convergence rate for the last iterate of stochastic\ngradient descent (SGD) and stochastic heavy ball (SHB) in the parametric\nsetting when the objective function $F$ is globally convex or non-convex whose\ngradient is $\\gamma$-H\\\"{o}lder. Using only discrete Gronwall's inequality\nwithout Robbins-Siegmund theorem nor martingale convergence theory, we recover\nresults for both SGD and SHB: $\\min_{s\\leq t} \\|\\nabla F(w_s)\\|^2 = o(t^{p-1})$\nfor non-convex objectives and $F(w_t) - F_* = o(t^{2\\gamma/(1+\\gamma) \\cdot\n\\max(p-1,-2p+1)-\\epsilon})$ for $\\beta \\in (0, 1)$ and $\\min_{s \\leq t} F(w_s)\n- F_* = o(t^{p-1})$ almost surely for convex objectives. In addition, we proved\nthat SHB with constant momentum parameter $\\beta \\in (0, 1)$ attains a\nconvergence rate of $F(w_t) - F_* = O(t^{\\max(p-1,-2p+1)} \\log^2\n\\frac{t}{\\delta})$ with probability at least $1-\\delta$ when $F$ is convex and\n$\\gamma = 1$ and step size $\\alpha_t = \\Theta(t^{-p})$ with $p \\in\n(\\frac{1}{2}, 1)$.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07281v1", "cate": "math.OC", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07838", "title": "3D-ADAM: A Dataset for 3D Anomaly Detection in Advanced Manufacturing", "authors": ["Paul McHard", "Florent P. Audonnet", "Oliver Summerell", "Sebastian Andraos", "Paul Henderson", "Gerardo Aragon-Camarasa"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07838v1", "summary": "Surface defects are one of the largest contributors to low yield in the\nmanufacturing sector. Accurate and reliable detection of defects during the\nmanufacturing process is therefore of great value across the sector.\nState-of-the-art approaches to automated defect detection yield impressive\nperformance on current datasets, yet still fall short in real-world\nmanufacturing settings and developing improved methods relies on large datasets\nrepresentative of real-world scenarios. Unfortunately, high-quality,\nhigh-precision RGB+3D industrial anomaly detection datasets are scarce, and\ntypically do not reflect real-world industrial deployment scenarios. To address\nthis, we introduce 3D-ADAM, the first large-scale industry-relevant dataset for\nhigh-precision 3D Anomaly Detection. 3D-ADAM comprises 14,120 high-resolution\nscans across 217 unique parts, captured using 4 industrial depth imaging\nsensors. It includes 27,346 annotated defect instances from 12 categories,\ncovering the breadth of industrial surface defects. 3D-ADAM uniquely captures\nan additional 8,110 annotations of machine element features, spanning the range\nof relevant mechanical design form factors. Unlike existing datasets, 3D-ADAM\nis captured in a real industrial environment with variations in part position\nand orientation, camera positioning, ambient lighting conditions, as well as\npartial occlusions. Our evaluation of SOTA models across various RGB+3D anomaly\ndetection tasks demonstrates the significant challenge this dataset presents to\ncurrent approaches. We further validated the industrial relevance and quality\nof the dataset through an expert labelling survey conducted by industry\npartners. By providing this challenging benchmark, 3D-ADAM aims to accelerate\nthe development of robust 3D Anomaly Detection models capable of meeting the\ndemands of modern manufacturing environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07838v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07653", "title": "An Automated Length-Aware Quality Metric for Summarization", "authors": ["Andrew D. Foland"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07653v1", "summary": "This paper proposes NOrmed Index of Retention (NOIR), a quantitative\nobjective metric for evaluating summarization quality of arbitrary texts that\nrelies on both the retention of semantic meaning and the summary length\ncompression. This gives a measure of how well the recall-compression tradeoff\nis managed, the most important skill in summarization. Experiments demonstrate\nthat NOIR effectively captures the token-length / semantic retention tradeoff\nof a summarizer and correlates to human perception of sumarization quality.\nUsing a language model-embedding to measure semantic similarity, it provides an\nautomated alternative for assessing summarization quality without relying on\ntime-consuming human-generated reference summaries. The proposed metric can be\napplied to various summarization tasks, offering an automated tool for\nevaluating and improving summarization algorithms, summarization prompts, and\nsynthetically-generated summaries.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07653v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2410.11719", "title": "Adaptive Graph Integration for Cross-Domain Recommendation via Heterogeneous Graph Coordinators", "authors": ["Hengyu Zhang", "Chunxu Shen", "Xiangguo Sun", "Jie Tan", "Yu Rong", "Chengzhi Piao", "Hong Cheng", "Lingling Yi"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Accept by SIGIR 2025", "url": "http://arxiv.org/abs/2410.11719v2", "summary": "In the digital era, users typically interact with diverse items across\nmultiple domains (e.g., e-commerce, streaming platforms, and social networks),\ngenerating intricate heterogeneous interaction graphs. Leveraging multi-domain\ndata can improve recommendation systems by enriching user insights and\nmitigating data sparsity in individual domains. However, integrating such\nmulti-domain knowledge for cross-domain recommendation remains challenging due\nto inherent disparities in user behavior and item characteristics and the risk\nof negative transfer, where irrelevant or conflicting information from the\nsource domains adversely impacts the target domain's performance. To tackle\nthese challenges, we propose HAGO, a novel framework with\n\\textbf{H}eterogeneous \\textbf{A}daptive \\textbf{G}raph co\\textbf{O}rdinators,\nwhich dynamically integrates multi-domain graphs into a cohesive structure.\nHAGO adaptively adjusts the connections between coordinators and multi-domain\ngraph nodes to enhance beneficial inter-domain interactions while alleviating\nnegative transfer. Furthermore, we introduce a universal multi-domain graph\npre-training strategy alongside HAGO to collaboratively learn high-quality node\nrepresentations across domains. Being compatible with various graph-based\nmodels and pre-training techniques, HAGO demonstrates broad applicability and\neffectiveness. Extensive experiments show that our framework outperforms\nstate-of-the-art methods in cross-domain recommendation scenarios, underscoring\nits potential for real-world applications. The source code is available at\nhttps://github.com/zhy99426/HAGO.", "comment": "Accept by SIGIR 2025", "pdf_url": "http://arxiv.org/pdf/2410.11719v2", "cate": "cs.IR", "date": "2024-10-15", "updated": "2025-07-10"}
{"id": "2507.02901", "title": "Online Continual Learning via Spiking Neural Networks with Sleep Enhanced Latent Replay", "authors": ["Erliang Lin", "Wenbin Luo", "Wei Jia", "Yu Chen", "Shaofu Yang"], "categories": ["cs.NE", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "Comments:      9 pages, 4figures", "url": "http://arxiv.org/abs/2507.02901v2", "summary": "Edge computing scenarios necessitate the development of hardware-efficient\nonline continual learning algorithms to be adaptive to dynamic environment.\nHowever, existing algorithms always suffer from high memory overhead and bias\ntowards recently trained tasks. To tackle these issues, this paper proposes a\nnovel online continual learning approach termed as SESLR, which incorporates a\nsleep enhanced latent replay scheme with spiking neural networks (SNNs). SESLR\nleverages SNNs' binary spike characteristics to store replay features in single\nbits, significantly reducing memory overhead. Furthermore, inspired by\nbiological sleep-wake cycles, SESLR introduces a noise-enhanced sleep phase\nwhere the model exclusively trains on replay samples with controlled noise\ninjection, effectively mitigating classification bias towards new classes.\nExtensive experiments on both conventional (MNIST, CIFAR10) and neuromorphic\n(NMNIST, CIFAR10-DVS) datasets demonstrate SESLR's effectiveness. On Split\nCIFAR10, SESLR achieves nearly 30% improvement in average accuracy with only\none-third of the memory consumption compared to baseline methods. On Split\nCIFAR10-DVS, it improves accuracy by approximately 10% while reducing memory\noverhead by a factor of 32. These results validate SESLR as a promising\nsolution for online continual learning in resource-constrained edge computing\nscenarios.", "comment": "9 pages, 4figures", "pdf_url": "http://arxiv.org/pdf/2507.02901v2", "cate": "cs.NE", "date": "2025-06-23", "updated": "2025-07-10"}
{"id": "2406.14514", "title": "Solving a Stackelberg Game on Transportation Networks in a Dynamic Crime Scenario: A Mixed Approach on Multi-Layer Networks", "authors": ["Sukanya Samanta", "Kei Kimura", "Makoto Yokoo"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2406.14514v3", "summary": "Interdicting a criminal with limited police resources is a challenging task\nas the criminal changes location over time. The size of the large\ntransportation network further adds to the difficulty of this scenario. To\ntackle this issue, we consider the concept of a layered graph. At each time\nstamp, we create a copy of the entire transportation network to track the\npossible movements of both players, the attacker and the defenders. We consider\na Stackelberg game in a dynamic crime scenario where the attacker changes\nlocation over time while the defenders attempt to interdict the attacker on his\nescape route. Given a set of defender strategies, the optimal attacker strategy\nis determined by applying Dijkstra's algorithm on the layered networks. Here,\nthe attacker aims to minimize while the defenders aim to maximize the\nprobability of interdiction. We develop an approximation algorithm on the\nlayered networks to find near-optimal strategy for defenders. The efficacy of\nthe developed approach is compared with the adopted MILP approach. We compare\nthe results in terms of computational time and solution quality. The quality of\nthe results demonstrates the need for the developed approach, as it effectively\nsolves the complex problem within a short amount of time.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2406.14514v3", "cate": "cs.AI", "date": "2024-06-20", "updated": "2025-07-10"}
{"id": "2507.07293", "title": "Thermodynamic Prediction Enabled by Automatic Dataset Building and Machine Learning", "authors": ["Juejing Liu", "Haydn Anderson", "Noah I. Waxman", "Vsevolod Kovalev", "Byron Fisher", "Elizabeth Li", "Xiaofeng Guo"], "categories": ["cond-mat.mtrl-sci", "cs.LG"], "primary_category": "Subjects:       Materials Science (cond-mat.mtrl-sci)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07293v1", "summary": "New discoveries in chemistry and materials science, with increasingly\nexpanding volume of requisite knowledge and experimental workload, provide\nunique opportunities for machine learning (ML) to take critical roles in\naccelerating research efficiency. Here, we demonstrate (1) the use of large\nlanguage models (LLMs) for automated literature reviews, and (2) the training\nof an ML model to predict chemical knowledge (thermodynamic parameters). Our\nLLM-based literature review tool (LMExt) successfully extracted chemical\ninformation and beyond into a machine-readable structure, including stability\nconstants for metal cation-ligand interactions, thermodynamic properties, and\nother broader data types (medical research papers, and financial reports),\neffectively overcoming the challenges inherent in each domain. Using the\nautonomous acquisition of thermodynamic data, an ML model was trained using the\nCatBoost algorithm for accurately predicting thermodynamic parameters (e.g.,\nenthalpy of formation) of minerals. This work highlights the transformative\npotential of integrated ML approaches to reshape chemistry and materials\nscience research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07293v1", "cate": "cond-mat.mtrl-sci", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07839", "title": "MeD-3D: A Multimodal Deep Learning Framework for Precise Recurrence Prediction in Clear Cell Renal Cell Carcinoma (ccRCC)", "authors": ["Hasaan Maqsood", "Saif Ur Rehman Khan"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07839v1", "summary": "Accurate prediction of recurrence in clear cell renal cell carcinoma (ccRCC)\nremains a major clinical challenge due to the disease complex molecular,\npathological, and clinical heterogeneity. Traditional prognostic models, which\nrely on single data modalities such as radiology, histopathology, or genomics,\noften fail to capture the full spectrum of disease complexity, resulting in\nsuboptimal predictive accuracy. This study aims to overcome these limitations\nby proposing a deep learning (DL) framework that integrates multimodal data,\nincluding CT, MRI, histopathology whole slide images (WSI), clinical data, and\ngenomic profiles, to improve the prediction of ccRCC recurrence and enhance\nclinical decision-making. The proposed framework utilizes a comprehensive\ndataset curated from multiple publicly available sources, including TCGA, TCIA,\nand CPTAC. To process the diverse modalities, domain-specific models are\nemployed: CLAM, a ResNet50-based model, is used for histopathology WSIs, while\nMeD-3D, a pre-trained 3D-ResNet18 model, processes CT and MRI images. For\nstructured clinical and genomic data, a multi-layer perceptron (MLP) is used.\nThese models are designed to extract deep feature embeddings from each\nmodality, which are then fused through an early and late integration\narchitecture. This fusion strategy enables the model to combine complementary\ninformation from multiple sources. Additionally, the framework is designed to\nhandle incomplete data, a common challenge in clinical settings, by enabling\ninference even when certain modalities are missing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07839v1", "cate": "eess.IV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07694", "title": "SAS: Simulated Attention Score", "authors": ["Chuanyang Zheng", "Jiankai Sun", "Yihang Gao", "Yuehao Wang", "Peihao Wang", "Jing Xiong", "Liliang Ren", "Hao Cheng", "Janardhan Kulkarni", "Yelong Shen", "Atlas Wang", "Mac Schwager", "Anderson Schneider", "Xiaodong Liu", "Jianfeng Gao"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Tech Report", "url": "http://arxiv.org/abs/2507.07694v1", "summary": "The attention mechanism is a core component of the Transformer architecture.\nVarious methods have been developed to compute attention scores, including\nmulti-head attention (MHA), multi-query attention, group-query attention and so\non. We further analyze the MHA and observe that its performance improves as the\nnumber of attention heads increases, provided the hidden size per head remains\nsufficiently large. Therefore, increasing both the head count and hidden size\nper head with minimal parameter overhead can lead to significant performance\ngains at a low cost. Motivated by this insight, we introduce Simulated\nAttention Score (SAS), which maintains a compact model size while simulating a\nlarger number of attention heads and hidden feature dimension per head. This is\nachieved by projecting a low-dimensional head representation into a\nhigher-dimensional space, effectively increasing attention capacity without\nincreasing parameter count. Beyond the head representations, we further extend\nthe simulation approach to feature dimension of the key and query embeddings,\nenhancing expressiveness by mimicking the behavior of a larger model while\npreserving the original model size. To control the parameter cost, we also\npropose Parameter-Efficient Attention Aggregation (PEAA). Comprehensive\nexperiments on a variety of datasets and tasks demonstrate the effectiveness of\nthe proposed SAS method, achieving significant improvements over different\nattention variants.", "comment": "Tech Report", "pdf_url": "http://arxiv.org/pdf/2507.07694v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2501.15379", "title": "Diffusion Augmented Retrieval: A Training-Free Approach to Interactive Text-to-Image Retrieval", "authors": ["Zijun Long", "Kangheng Liang", "Gerardo Aragon-Camarasa", "Richard Mccreadie", "Paul Henderson"], "categories": ["cs.IR", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.15379v2", "summary": "Interactive Text-to-image retrieval (I-TIR) is an important enabler for a\nwide range of state-of-the-art services in domains such as e-commerce and\neducation. However, current methods rely on finetuned Multimodal Large Language\nModels (MLLMs), which are costly to train and update, and exhibit poor\ngeneralizability. This latter issue is of particular concern, as: 1) finetuning\nnarrows the pretrained distribution of MLLMs, thereby reducing\ngeneralizability; and 2) I-TIR introduces increasing query diversity and\ncomplexity. As a result, I-TIR solutions are highly likely to encounter queries\nand images not well represented in any training dataset. To address this, we\npropose leveraging Diffusion Models (DMs) for text-to-image mapping, to avoid\nfinetuning MLLMs while preserving robust performance on complex queries.\nSpecifically, we introduce Diffusion Augmented Retrieval (DAR), a framework\nthat generates multiple intermediate representations via LLM-based dialogue\nrefinements and DMs, producing a richer depiction of the user's information\nneeds. This augmented representation facilitates more accurate identification\nof semantically and visually related images. Extensive experiments on four\nbenchmarks show that for simple queries, DAR achieves results on par with\nfinetuned I-TIR models, yet without incurring their tuning overhead. Moreover,\nas queries become more complex through additional conversational turns, DAR\nsurpasses finetuned I-TIR models by up to 7.61% in Hits@10 after ten turns,\nillustrating its improved generalization for more intricate queries.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.15379v2", "cate": "cs.IR", "date": "2025-01-26", "updated": "2025-07-10"}
{"id": "2408.05798", "title": "Time Makes Space: Emergence of Place Fields in Networks Encoding Temporally Continuous Sensory Experiences", "authors": ["Zhaoze Wang", "Ronald W. Di Tullio", "Spencer Rooke", "Vijay Balasubramanian"], "categories": ["q-bio.NC", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "Subjects:       Neurons and Cognition (q-bio.NC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2408.05798v3", "summary": "The vertebrate hippocampus is believed to use recurrent connectivity in area\nCA3 to support episodic memory recall from partial cues. This brain area also\ncontains place cells, whose location-selective firing fields implement maps\nsupporting spatial memory. Here we show that place cells emerge in networks\ntrained to remember temporally continuous sensory episodes. We model CA3 as a\nrecurrent autoencoder that recalls and reconstructs sensory experiences from\nnoisy and partially occluded observations by agents traversing simulated rooms.\nThe agents move in realistic trajectories modeled from rodents and environments\nare modeled as high-dimensional sensory experience maps. Training our\nautoencoder to pattern-complete and reconstruct experiences with a constraint\non total activity causes spatially localized firing fields, i.e., place cells,\nto emerge in the encoding layer. The emergent place fields reproduce key\naspects of hippocampal phenomenology: a) remapping (maintenance of and\nreversion to distinct learned maps in different environments), implemented via\nrepositioning of experience manifolds in the network's hidden layer, b)\northogonality of spatial representations in different arenas, c) robust place\nfield emergence in differently shaped rooms, with single units showing multiple\nplace fields in large or complex spaces, and d) slow representational drift of\nplace fields. We argue that these results arise because continuous traversal of\nspace makes sensory experience temporally continuous. We make testable\npredictions: a) rapidly changing sensory context will disrupt place fields, b)\nplace fields will form even if recurrent connections are blocked, but reversion\nto previously learned representations upon remapping will be abolished, c) the\ndimension of temporally smooth experience sets the dimensionality of place\nfields, including during virtual navigation of abstract spaces.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2408.05798v3", "cate": "q-bio.NC", "date": "2024-08-11", "updated": "2025-07-09"}
{"id": "2409.08936", "title": "SimSUM: Simulated Benchmark with Structured and Unstructured Medical Records", "authors": ["Paloma Rabaey", "Stefan Heytens", "Thomas Demeester"], "categories": ["cs.AI", "cs.CL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      An earlier version of this dataset was published under the name SynSUM. It has since been renamed to SimSUM to avoid confusion with synthetic data generated from real data, and to emphasize the simulated nature of the dataset", "url": "http://arxiv.org/abs/2409.08936v3", "summary": "Clinical information extraction, which involves structuring clinical concepts\nfrom unstructured medical text, remains a challenging problem that could\nbenefit from the inclusion of tabular background information available in\nelectronic health records. Existing open-source datasets lack explicit links\nbetween structured features and clinical concepts in the text, motivating the\nneed for a new research dataset. We introduce SimSUM, a benchmark dataset of\n10,000 simulated patient records that link unstructured clinical notes with\nstructured background variables. Each record simulates a patient encounter in\nthe domain of respiratory diseases and includes tabular data (e.g., symptoms,\ndiagnoses, underlying conditions) generated from a Bayesian network whose\nstructure and parameters are defined by domain experts. A large language model\n(GPT-4o) is prompted to generate a clinical note describing the encounter,\nincluding symptoms and relevant context. These notes are annotated with\nspan-level symptom mentions. We conduct an expert evaluation to assess note\nquality and run baseline predictive models on both the tabular and textual\ndata. The SimSUM dataset is primarily designed to support research on clinical\ninformation extraction in the presence of tabular background variables, which\ncan be linked through domain knowledge to concepts of interest to be extracted\nfrom the text (symptoms, in the case of SimSUM). Secondary uses include\nresearch on the automation of clinical reasoning over both tabular data and\ntext, causal effect estimation in the presence of tabular and/or textual\nconfounders, and multi-modal synthetic data generation. SimSUM is not intended\nfor training clinical decision support systems or production-grade models, but\nrather to facilitate reproducible research in a simplified and controlled\nsetting. The dataset is available at https://github.com/prabaey/SimSUM.", "comment": "An earlier version of this dataset was published under the name\n  SynSUM. It has since been renamed to SimSUM to avoid confusion with synthetic\n  data generated from real data, and to emphasize the simulated nature of the\n  dataset", "pdf_url": "http://arxiv.org/pdf/2409.08936v3", "cate": "cs.AI", "date": "2024-09-13", "updated": "2025-07-10"}
{"id": "2507.07296", "title": "Time Series Foundation Models for Multivariate Financial Time Series Forecasting", "authors": ["Ben A. Marconi"], "categories": ["q-fin.GN", "cs.LG"], "primary_category": "Subjects:       General Finance (q-fin.GN)", "pdf_link": null, "comments": "Comments:      66 pages", "url": "http://arxiv.org/abs/2507.07296v1", "summary": "Financial time series forecasting presents significant challenges due to\ncomplex nonlinear relationships, temporal dependencies, variable\ninterdependencies and limited data availability, particularly for tasks\ninvolving low-frequency data, newly listed instruments, or emerging market\nassets. Time Series Foundation Models (TSFMs) offer a promising solution\nthrough pretraining on diverse time series corpora followed by task-specific\nadaptation. This study evaluates two TSFMs (Tiny Time Mixers (TTM) and Chronos)\nacross three financial forecasting tasks: US 10-year Treasury yield changes,\nEUR/USD volatility, and equity spread prediction. Results demonstrate that TTM\nexhibits strong transferability. When fine-tuning both the pretrained version\nof TTM and an untrained model with the same architecture, the pretrained\nversion achieved 25-50% better performance when fine-tuned on limited data and\n15-30% improvements even when fine-tuned on lengthier datasets. Notably, TTM's\nzero-shot performance outperformed naive benchmarks in volatility forecasting\nand equity spread prediction, with the latter demonstrating that TSFMs can\nsurpass traditional benchmark models without fine-tuning. The pretrained model\nconsistently required 3-10 fewer years of data to achieve comparable\nperformance levels compared to the untrained model, demonstrating significant\nsample-efficiency gains. However, while TTM outperformed naive baselines,\ntraditional specialised models matched or exceeded its performance in two of\nthree tasks, suggesting TSFMs prioritise breadth over task-specific\noptimisation. These findings indicate that TSFMs, though still nascent, offer\nsubstantial promise for financial forecasting-particularly in noisy,\ndata-constrained tasks-but achieving competitive performance likely requires\ndomain-specific pretraining and architectural refinements tailored to financial\ntime series characteristics.", "comment": "66 pages", "pdf_url": "http://arxiv.org/pdf/2507.07296v1", "cate": "q-fin.GN", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07860", "title": "THUNDER: Tile-level Histopathology image UNDERstanding benchmark", "authors": ["Pierre Marza", "Leo Fillioux", "Sofi√®ne Boutaj", "Kunal Mahatha", "Christian Desrosiers", "Pablo Piantanida", "Jose Dolz", "Stergios Christodoulidis", "Maria Vakalopoulou"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07860v1", "summary": "Progress in a research field can be hard to assess, in particular when many\nconcurrent methods are proposed in a short period of time. This is the case in\ndigital pathology, where many foundation models have been released recently to\nserve as feature extractors for tile-level images, being used in a variety of\ndownstream tasks, both for tile- and slide-level problems. Benchmarking\navailable methods then becomes paramount to get a clearer view of the research\nlandscape. In particular, in critical domains such as healthcare, a benchmark\nshould not only focus on evaluating downstream performance, but also provide\ninsights about the main differences between methods, and importantly, further\nconsider uncertainty and robustness to ensure a reliable usage of proposed\nmodels. For these reasons, we introduce THUNDER, a tile-level benchmark for\ndigital pathology foundation models, allowing for efficient comparison of many\nmodels on diverse datasets with a series of downstream tasks, studying their\nfeature spaces and assessing the robustness and uncertainty of predictions\ninformed by their embeddings. THUNDER is a fast, easy-to-use, dynamic benchmark\nthat can already support a large variety of state-of-the-art foundation, as\nwell as local user-defined models for direct tile-based comparison. In this\npaper, we provide a comprehensive comparison of 23 foundation models on 16\ndifferent datasets covering diverse tasks, feature analysis, and robustness.\nThe code for THUNDER is publicly available at\nhttps://github.com/MICS-Lab/thunder.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07860v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07741", "title": "Code-Switching in End-to-End Automatic Speech Recognition: A Systematic Literature Review", "authors": ["Maha Tufail Agro", "Atharva Kulkarni", "Karima Kadaoui", "Zeerak Talat", "Hanan Aldarmaki"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07741v1", "summary": "Motivated by a growing research interest into automatic speech recognition\n(ASR), and the growing body of work for languages in which code-switching (CS)\noften occurs, we present a systematic literature review of code-switching in\nend-to-end ASR models. We collect and manually annotate papers published in\npeer reviewed venues. We document the languages considered, datasets, metrics,\nmodel choices, and performance, and present a discussion of challenges in\nend-to-end ASR for code-switching. Our analysis thus provides insights on\ncurrent research efforts and available resources as well as opportunities and\ngaps to guide future research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07741v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2502.19108", "title": "U-Sticker: A Large-Scale Multi-Domain User Sticker Dataset for Retrieval and Personalization", "authors": ["Heng Er Metilda Chee", "Jiayin Wang", "Zhiqiang Guo", "Weizhi Ma", "Qinglang Guo", "Min Zhang"], "categories": ["cs.IR", "cs.MM"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Accepted at SIGIR'25", "url": "http://arxiv.org/abs/2502.19108v2", "summary": "Instant messaging with texts and stickers has become a widely adopted\ncommunication medium, enabling efficient expression of user semantics and\nemotions. With the increased use of stickers conveying information and\nfeelings, sticker retrieval and recommendation has emerged as an important area\nof research. However, a major limitation in existing literature has been the\nlack of datasets capturing temporal and user-specific sticker interactions,\nwhich has hindered further progress in user modeling and sticker\npersonalization. To address this, we introduce User-Sticker, a dataset that\nincludes temporal and user anonymous ID across conversations. It is the largest\npublicly available sticker dataset to date, containing 22K unique users, 370K\nstickers, and 8.3M messages. The raw data was collected from a popular\nmessaging platform from 67 conversations over 720 hours of crawling. All text\nand image data were carefully vetted for safety and privacy checks and\nmodifications. Spanning 10 domains, the U-Sticker dataset captures rich\ntemporal, multilingual, and cross-domain behaviors not previously available in\nother datasets. Extensive quantitative and qualitative experiments demonstrate\nU-Sticker's practical applications in user behavior modeling and personalized\nrecommendation and highlight its potential to further research areas in\npersonalized retrieval and conversational studies. U-Sticker dataset is\npublicly available.", "comment": "Accepted at SIGIR'25", "pdf_url": "http://arxiv.org/pdf/2502.19108v2", "cate": "cs.IR", "date": "2025-02-26", "updated": "2025-07-10"}
{"id": "2409.10739", "title": "Evolving a multi-population evolutionary-QAOA on distributed QPUs", "authors": ["Francesca Schiavello", "Edoardo Altamura", "Ivano Tavernelli", "Stefano Mensa", "Benjamin Symons"], "categories": ["quant-ph", "cs.NE"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      9 pages, 5 figures. Accepted for publication at the IEEE International Conference on Quantum Computing and Engineering (QCE25), quantum algorithms technical paper track", "url": "http://arxiv.org/abs/2409.10739v3", "summary": "Our work integrates an Evolutionary Algorithm (EA) with the Quantum\nApproximate Optimization Algorithm (QAOA) to optimize ansatz parameters in\nplace of traditional gradient-based methods. We benchmark this\nEvolutionary-QAOA (E-QAOA) approach on the Max-Cut problem for $d$-3 regular\ngraphs of 4 to 26 nodes, demonstrating equal or higher accuracy and reduced\nvariance compared to COBYLA-based QAOA, especially when using Conditional Value\nat Risk (CVaR) for fitness evaluations. Additionally, we propose a novel\ndistributed multi-population EA strategy, executing parallel, independent\npopulations on two quantum processing units (QPUs) with classical communication\nof 'elite' solutions. Experiments on quantum simulators and IBM hardware\nvalidate the approach. We also discuss potential extensions of our method and\noutline promising future directions in scalable, distributed quantum\noptimization on hybrid quantum-classical infrastructures.", "comment": "9 pages, 5 figures. Accepted for publication at the IEEE\n  International Conference on Quantum Computing and Engineering (QCE25),\n  quantum algorithms technical paper track", "pdf_url": "http://arxiv.org/pdf/2409.10739v3", "cate": "quant-ph", "date": "2024-09-16", "updated": "2025-07-10"}
{"id": "2507.07371", "title": "Spectral connvergece of random feature method in one dimension", "authors": ["Pingbing Ming", "Hao Yu"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07371v1", "summary": "Among the various machine learning methods solving partial differential\nequations, the Random Feature Method (RFM) stands out due to its accuracy and\nefficiency. In this paper, we demonstrate that the approximation error of RFM\nexhibits spectral convergence when it is applied to the second-order elliptic\nequations in one dimension, provided that the solution belongs to Gevrey\nclasses or Sobolev spaces. We highlight the significant impact of incorporating\nthe Partition of Unity Method (PUM) to enhance the convergence of RFM by\nestablishing the convergence rate in terms of the maximum patch size.\nFurthermore, we reveal that the singular values of the random feature matrix\n(RFMtx) decay exponentially, while its condition number increases exponentially\nas the number of the features grows. We also theoretically illustrate that PUM\nmay mitigate the excessive decay of the singular values of RFMtx.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07371v1", "cate": "math.NA", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2409.14993", "title": "Multi-modal Generative AI: Multi-modal LLMs, Diffusions and the Unification", "authors": ["Xin Wang", "Yuwei Zhou", "Bin Huang", "Hong Chen", "Wenwu Zhu"], "categories": ["cs.AI", "cs.CV"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      20 pages, 11 figures, 2 tables", "url": "http://arxiv.org/abs/2409.14993v2", "summary": "Multi-modal generative AI (Artificial Intelligence) has attracted increasing\nattention from both academia and industry. Particularly, two dominant families\nof techniques have emerged: i) Multi-modal large language models (LLMs)\ndemonstrate impressive ability for multi-modal understanding; and ii) Diffusion\nmodels exhibit remarkable multi-modal powers in terms of multi-modal\ngeneration. Therefore, this paper provides a comprehensive overview of\nmulti-modal generative AI, including multi-modal LLMs, diffusions, and the\nunification for understanding and generation. To lay a solid foundation for\nunified models, we first provide a detailed review of both multi-modal LLMs and\ndiffusion models respectively, including their probabilistic modeling\nprocedure, multi-modal architecture design, and advanced applications to\nimage/video LLMs as well as text-to-image/video generation. Furthermore, we\nexplore the emerging efforts toward unified models for understanding and\ngeneration. To achieve the unification of understanding and generation, we\ninvestigate key designs including autoregressive-based and diffusion-based\nmodeling, as well as dense and Mixture-of-Experts (MoE) architectures. We then\nintroduce several strategies for unified models, analyzing their potential\nadvantages and disadvantages. In addition, we summarize the common datasets\nwidely used for multi-modal generative AI pretraining. Last but not least, we\npresent several challenging future research directions which may contribute to\nthe ongoing advancement of multi-modal generative AI.", "comment": "20 pages, 11 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2409.14993v2", "cate": "cs.AI", "date": "2024-09-23", "updated": "2025-07-10"}
{"id": "2507.07298", "title": "Multilayer GNN for Predictive Maintenance and Clustering in Power Grids", "authors": ["Muhammad Kazim", "Harun Pirim", "Chau Le", "Trung Le", "Om Prakash Yadav"], "categories": ["eess.SY", "cs.LG", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07298v1", "summary": "Unplanned power outages cost the US economy over $150 billion annually,\npartly due to predictive maintenance (PdM) models that overlook spatial,\ntemporal, and causal dependencies in grid failures. This study introduces a\nmultilayer Graph Neural Network (GNN) framework to enhance PdM and enable\nresilience-based substation clustering. Using seven years of incident data from\nOklahoma Gas & Electric (292,830 records across 347 substations), the framework\nintegrates Graph Attention Networks (spatial), Graph Convolutional Networks\n(temporal), and Graph Isomorphism Networks (causal), fused through\nattention-weighted embeddings. Our model achieves a 30-day F1-score of 0.8935\n+/- 0.0258, outperforming XGBoost and Random Forest by 3.2% and 2.7%, and\nsingle-layer GNNs by 10 to 15 percent. Removing the causal layer drops\nperformance to 0.7354 +/- 0.0418. For resilience analysis, HDBSCAN clustering\non HierarchicalRiskGNN embeddings identifies eight operational risk groups. The\nhighest-risk cluster (Cluster 5, 44 substations) shows 388.4 incidents/year and\n602.6-minute recovery time, while low-risk groups report fewer than 62\nincidents/year. ANOVA (p < 0.0001) confirms significant inter-cluster\nseparation. Our clustering outperforms K-Means and Spectral Clustering with a\nSilhouette Score of 0.626 and Davies-Bouldin index of 0.527. This work supports\nproactive grid management through improved failure prediction and risk-aware\nsubstation clustering.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07298v1", "cate": "eess.SY", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07878", "title": "Single-Step Latent Diffusion for Underwater Image Restoration", "authors": ["Jiayi Wu", "Tianfu Wang", "Md Abu Bakr Siddique", "Md Jahidul Islam", "Cornelia Fermuller", "Yiannis Aloimonos", "Christopher A. Metzler"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07878v1", "summary": "Underwater image restoration algorithms seek to restore the color, contrast,\nand appearance of a scene that is imaged underwater. They are a critical tool\nin applications ranging from marine ecology and aquaculture to underwater\nconstruction and archaeology. While existing pixel-domain diffusion-based image\nrestoration approaches are effective at restoring simple scenes with limited\ndepth variation, they are computationally intensive and often generate\nunrealistic artifacts when applied to scenes with complex geometry and\nsignificant depth variation. In this work we overcome these limitations by\ncombining a novel network architecture (SLURPP) with an accurate synthetic data\ngeneration pipeline. SLURPP combines pretrained latent diffusion models --\nwhich encode strong priors on the geometry and depth of scenes -- with an\nexplicit scene decomposition -- which allows one to model and account for the\neffects of light attenuation and backscattering. To train SLURPP we design a\nphysics-based underwater image synthesis pipeline that applies varied and\nrealistic underwater degradation effects to existing terrestrial image\ndatasets. This approach enables the generation of diverse training data with\ndense medium/degradation annotations. We evaluate our method extensively on\nboth synthetic and real-world benchmarks and demonstrate state-of-the-art\nperformance. Notably, SLURPP is over 200X faster than existing diffusion-based\nmethods while offering ~ 3 dB improvement in PSNR on synthetic benchmarks. It\nalso offers compelling qualitative improvements on real-world data. Project\nwebsite https://tianfwang.github.io/slurpp/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07878v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07803", "title": "StreamUni: Achieving Streaming Speech Translation with a Unified Large Speech-Language Model", "authors": ["Shoutao Guo", "Xiang Li", "Shaolei Zhang", "Mengge Liu", "Wei Chen", "Yang Feng"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      The code is at this https URL The model is at this https URL", "url": "http://arxiv.org/abs/2507.07803v1", "summary": "Streaming speech translation (StreamST) requires determining appropriate\ntiming, known as policy, to generate translations while continuously receiving\nsource speech inputs, balancing low latency with high translation quality.\nHowever, existing StreamST methods typically operate on sentence-level speech\nsegments, referred to as simultaneous speech translation (SimulST). In\npractice, they require collaboration with segmentation models to accomplish\nStreamST, where the truncated speech segments constrain SimulST models to make\npolicy decisions and generate translations based on limited contextual\ninformation. Moreover, SimulST models struggle to learn effective policies due\nto the complexity of speech inputs and cross-lingual generation. To address\nthese challenges, we propose StreamUni, which achieves StreamST through a\nunified Large Speech-Language Model (LSLM). Specifically, StreamUni\nincorporates speech Chain-of-Thought (CoT) in guiding the LSLM to generate\nmulti-stage outputs. Leveraging these multi-stage outputs, StreamUni\nsimultaneously accomplishes speech segmentation, policy decision, and\ntranslation generation, completing StreamST without requiring massive\npolicy-specific training. Additionally, we propose a streaming CoT training\nmethod that enhances low-latency policy decisions and generation capabilities\nusing limited CoT data. Experiments demonstrate that our approach achieves\nstate-of-the-art performance on StreamST tasks.", "comment": "The code is at https://github.com/ictnlp/StreamUni; The model is at\n  https://huggingface.co/ICTNLP/StreamUni-Phi4", "pdf_url": "http://arxiv.org/pdf/2507.07803v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2503.19092", "title": "Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation", "authors": ["Krisztian Balog", "Donald Metzler", "Zhen Qin"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '25)", "url": "http://arxiv.org/abs/2503.19092v2", "summary": "Large language models (LLMs) are increasingly integral to information\nretrieval (IR), powering ranking, evaluation, and AI-assisted content creation.\nThis widespread adoption necessitates a critical examination of potential\nbiases arising from the interplay between these LLM-based components. This\npaper synthesizes existing research and presents novel experiment designs that\nexplore how LLM-based rankers and assistants influence LLM-based judges. We\nprovide the first empirical evidence of LLM judges exhibiting significant bias\ntowards LLM-based rankers. Furthermore, we observe limitations in LLM judges'\nability to discern subtle system performance differences. Contrary to some\nprevious findings, our preliminary study does not find evidence of bias against\nAI-generated content. These results highlight the need for a more holistic view\nof the LLM-driven information ecosystem. To this end, we offer initial\nguidelines and a research agenda to ensure the reliable use of LLMs in IR\nevaluation.", "comment": "Proceedings of the 48th International ACM SIGIR Conference on\n  Research and Development in Information Retrieval (SIGIR '25)", "pdf_url": "http://arxiv.org/pdf/2503.19092v2", "cate": "cs.IR", "date": "2025-03-24", "updated": "2025-07-09"}
{"id": "2507.07607", "title": "A structure-preserving finite element framework for the Vlasov-Maxwell system", "authors": ["Katharina Kormann", "Murtazo Nazarov", "Junjie Wen"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07607v1", "summary": "We present a stabilized, structure-preserving finite element framework for\nsolving the Vlasov-Maxwell equations. The method uses a tensor product of\ncontinuous polynomial spaces for the spatial and velocity domains,\nrespectively, to discretize the Vlasov equation, combined with curl- and\ndivergence-conforming N\\'ed\\'elec and Raviart-Thomas elements for Maxwell's\nequations on Cartesian grids. A novel, robust, consistent, and high-order\naccurate residual-based artificial viscosity method is introduced for\nstabilizing the Vlasov equations. The proposed method is tested on the 1D2V and\n2D2V reduced Vlasov-Maxwell system, achieving optimal convergence orders for\nall polynomial spaces considered in this study. Several challenging benchmarks\nare solved to validate the effectiveness of the proposed method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07607v1", "cate": "math.NA", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2411.07618", "title": "Constrain Alignment with Sparse Autoencoders", "authors": ["Qingyu Yin", "Chak Tou Leong", "Minjun Zhu", "Hanqi Yan", "Qiang Zhang", "Yulan He", "Wenjie Li", "Jun Wang", "Yue Zhang", "Linyi Yang"], "categories": ["cs.AI", "cs.CL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.07618v4", "summary": "The alignment of large language models (LLMs) with human preferences remains\na key challenge. While post-training techniques like Reinforcement Learning\nfrom Human Feedback (RLHF) and Direct Preference Optimization (DPO) have\nachieved notable success, they often introduce computational inefficiencies and\ntraining instability. In this paper, we propose Feature-level constrained\nPreference Optimization (FPO), a novel method designed to simplify the\nalignment process while ensuring stability. FPO leverages pre-trained Sparse\nAutoencoders (SAEs) and introduces feature-level constraints, allowing for\nefficient, sparsity-enforced alignment. Our approach enjoys efficiency by using\nsparse features activated in a well-trained sparse autoencoder and the quality\nof sequential KL divergence by using the feature-level offline reference.\nExperimental results on benchmark datasets demonstrate that FPO achieves a\n5.08% absolute improvement in win rate with much lower computational cost\ncompared to state-of-the-art baselines, making it a promising solution for\nefficient and controllable LLM alignments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.07618v4", "cate": "cs.AI", "date": "2024-11-12", "updated": "2025-07-10"}
{"id": "2507.07313", "title": "Frontier LLMs Still Struggle with Simple Reasoning Tasks", "authors": ["Alan Malek", "Jiawei Ge", "Nevena Lazic", "Chi Jin", "Andr√°s Gy√∂rgy", "Csaba Szepesv√°ri"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      53 pages", "url": "http://arxiv.org/abs/2507.07313v1", "summary": "While state-of-the-art large language models (LLMs) demonstrate advanced\nreasoning capabilities-achieving remarkable performance on challenging\ncompetitive math and coding benchmarks-they also frequently fail on tasks that\nare easy for humans. This work studies the performance of frontier LLMs on a\nbroad set of such \"easy\" reasoning problems. By extending previous work in the\nliterature, we create a suite of procedurally generated simple reasoning tasks,\nincluding counting, first-order logic, proof trees, and travel planning, with\nchangeable parameters (such as document length. or the number of variables in a\nmath problem) that can arbitrarily increase the amount of computation required\nto produce the answer while preserving the fundamental difficulty. While\nprevious work showed that traditional, non-thinking models can be made to fail\non such problems, we demonstrate that even state-of-the-art thinking models\nconsistently fail on such problems and for similar reasons (e.g. statistical\nshortcuts, errors in intermediate steps, and difficulties in processing long\ncontexts). To further understand the behavior of the models, we introduce the\nunpuzzles dataset, a different \"easy\" benchmark consisting of trivialized\nversions of well-known math and logic puzzles. Interestingly, while modern LLMs\nexcel at solving the original puzzles, they tend to fail on the trivialized\nversions, exhibiting several systematic failure patterns related to memorizing\nthe originals. We show that this happens even if the models are otherwise able\nto solve problems with different descriptions but requiring the same logic. Our\nresults highlight that out-of-distribution generalization is still problematic\nfor frontier language models and the new generation of thinking models, even\nfor simple reasoning tasks, and making tasks easier does not necessarily imply\nimproved performance.", "comment": "53 pages", "pdf_url": "http://arxiv.org/pdf/2507.07313v1", "cate": "cs.CL", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07902", "title": "MIRA: A Novel Framework for Fusing Modalities in Medical RAG", "authors": ["Jinhong Wang", "Tajamul Ashraf", "Zongyan Han", "Jorma Laaksonen", "Rao Mohammad Anwer"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ACM Multimedia 2025", "url": "http://arxiv.org/abs/2507.07902v1", "summary": "Multimodal Large Language Models (MLLMs) have significantly advanced\nAI-assisted medical diagnosis, but they often generate factually inconsistent\nresponses that deviate from established medical knowledge. Retrieval-Augmented\nGeneration (RAG) enhances factual accuracy by integrating external sources, but\nit presents two key challenges. First, insufficient retrieval can miss critical\ninformation, whereas excessive retrieval can introduce irrelevant or misleading\ncontent, disrupting model output. Second, even when the model initially\nprovides correct answers, over-reliance on retrieved data can lead to factual\nerrors. To address these issues, we introduce the Multimodal Intelligent\nRetrieval and Augmentation (MIRA) framework, designed to optimize factual\naccuracy in MLLM. MIRA consists of two key components: (1) a calibrated\nRethinking and Rearrangement module that dynamically adjusts the number of\nretrieved contexts to manage factual risk, and (2) A medical RAG framework\nintegrating image embeddings and a medical knowledge base with a query-rewrite\nmodule for efficient multimodal reasoning. This enables the model to\neffectively integrate both its inherent knowledge and external references. Our\nevaluation of publicly available medical VQA and report generation benchmarks\ndemonstrates that MIRA substantially enhances factual accuracy and overall\nperformance, achieving new state-of-the-art results. Code is released at\nhttps://github.com/mbzuai-oryx/MIRA.", "comment": "ACM Multimedia 2025", "pdf_url": "http://arxiv.org/pdf/2507.07902v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07810", "title": "Understanding and Controlling Repetition Neurons and Induction Heads in In-Context Learning", "authors": ["Nhi Hoai Doan", "Tatsuya Hiraoka", "Kentaro Inui"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07810v1", "summary": "This paper investigates the relationship between large language models'\n(LLMs) ability to recognize repetitive input patterns and their performance on\nin-context learning (ICL). In contrast to prior work that has primarily focused\non attention heads, we examine this relationship from the perspective of skill\nneurons, specifically repetition neurons. Our experiments reveal that the\nimpact of these neurons on ICL performance varies depending on the depth of the\nlayer in which they reside. By comparing the effects of repetition neurons and\ninduction heads, we further identify strategies for reducing repetitive outputs\nwhile maintaining strong ICL capabilities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07810v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2504.06667", "title": "Toward Holistic Evaluation of Recommender Systems Powered by Generative Models", "authors": ["Yashar Deldjoo", "Nikhil Mehta", "Maheswaran Sathiamoorthy", "Shuai Zhang", "Pablo Castells", "Julian McAuley"], "categories": ["cs.IR", "cs.AI"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.06667v2", "summary": "Recommender systems powered by generative models (Gen-RecSys) extend beyond\nclassical item ranking by producing open-ended content, which simultaneously\nunlocks richer user experiences and introduces new risks. On one hand, these\nsystems can enhance personalization and appeal through dynamic explanations and\nmulti-turn dialogues. On the other hand, they might venture into unknown\nterritory-hallucinating nonexistent items, amplifying bias, or leaking private\ninformation. Traditional accuracy metrics cannot fully capture these\nchallenges, as they fail to measure factual correctness, content safety, or\nalignment with user intent.\n  This paper makes two main contributions. First, we categorize the evaluation\nchallenges of Gen-RecSys into two groups: (i) existing concerns that are\nexacerbated by generative outputs (e.g., bias, privacy) and (ii) entirely new\nrisks (e.g., item hallucinations, contradictory explanations). Second, we\npropose a holistic evaluation approach that includes scenario-based assessments\nand multi-metric checks-incorporating relevance, factual grounding, bias\ndetection, and policy compliance. Our goal is to provide a guiding framework so\nresearchers and practitioners can thoroughly assess Gen-RecSys, ensuring\neffective personalization and responsible deployment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.06667v2", "cate": "cs.IR", "date": "2025-04-09", "updated": "2025-07-10"}
{"id": "2507.07635", "title": "Non-uniform time-stepping in k-space pseudospectral time domain models of acoustic propagation", "authors": ["Matthew J. King", "B. E. Treeby", "B. T. Cox"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07635v1", "summary": "Non-uniform time stepping in acoustic propagation models can be used to\npreserve accuracy or reduce computational cost for an acoustic simulation with\na wave front propagating through a domain with both heterogeneous and\nhomogenous regions, such as for a simulation of breast ultrasound tomography.\nThe k-space correction already exist within the literature to remove numerical\ndispersion caused by the time stepping procedure in pseudo-spectral time domain\nmodels, but requires a uniform time step. Here we expand this correction to be\nable to account for a non-uniform time stepping method and illustrate the\npotential advantages and considerations. A version of this Article has been\nsubmitted for review to the Journal of Theoretical and Computational Acoustics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07635v1", "cate": "math.NA", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2501.05765", "title": "Deontic Temporal Logic for Formal Verification of AI Ethics", "authors": ["Priya T. V.", "Shrisha Rao"], "categories": ["cs.AI", "cs.LO", "I.2.m; F.4.1"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.05765v3", "summary": "Ensuring ethical behavior in Artificial Intelligence (AI) systems amidst\ntheir increasing ubiquity and influence is a major concern the world over. The\nuse of formal methods in AI ethics is a possible crucial approach for\nspecifying and verifying the ethical behavior of AI systems. This paper\nproposes a formalization based on deontic logic to define and evaluate the\nethical behavior of AI systems, focusing on system-level specifications,\ncontributing to this important goal. It introduces axioms and theorems to\ncapture ethical requirements related to fairness and explainability. The\nformalization incorporates temporal operators to reason about the ethical\nbehavior of AI systems over time. The authors evaluate the effectiveness of\nthis formalization by assessing the ethics of the real-world COMPAS and loan\nprediction AI systems. Various ethical properties of the COMPAS and loan\nprediction systems are encoded using deontic logical formulas, allowing the use\nof an automated theorem prover to verify whether these systems satisfy the\ndefined properties. The formal verification reveals that both systems fail to\nfulfill certain key ethical properties related to fairness and\nnon-discrimination, demonstrating the effectiveness of the proposed\nformalization in identifying potential ethical issues in real-world AI\napplications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.05765v3", "cate": "cs.AI", "date": "2025-01-10", "updated": "2025-07-10"}
{"id": "2507.07338", "title": "Bayesian Double Descent", "authors": ["Nick Polson", "Vadim Sokolov"], "categories": ["stat.ML", "cs.LG", "stat.CO"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07338v1", "summary": "Double descent is a phenomenon of over-parameterized statistical models. Our\ngoal is to view double descent from a Bayesian perspective. Over-parameterized\nmodels such as deep neural networks have an interesting re-descending property\nin their risk characteristics. This is a recent phenomenon in machine learning\nand has been the subject of many studies. As the complexity of the model\nincreases, there is a U-shaped region corresponding to the traditional\nbias-variance trade-off, but then as the number of parameters equals the number\nof observations and the model becomes one of interpolation, the risk can become\ninfinite and then, in the over-parameterized region, it re-descends -- the\ndouble descent effect. We show that this has a natural Bayesian interpretation.\nMoreover, we show that it is not in conflict with the traditional Occam's razor\nthat Bayesian models possess, in that they tend to prefer simpler models when\npossible. We illustrate the approach with an example of Bayesian model\nselection in neural networks. Finally, we conclude with directions for future\nresearch.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07338v1", "cate": "stat.ML", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07908", "title": "Not Only Consistency: Enhance Test-Time Adaptation with Spatio-temporal Inconsistency for Remote Physiological Measurement", "authors": ["Xiao Yang", "Yuxuan Fan", "Can Liu", "Houcheng Su", "Weichen Guo", "Jiyao Wang", "Dengbo He"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07908v1", "summary": "Remote photoplethysmography (rPPG) has emerged as a promising non-invasive\nmethod for monitoring physiological signals using the camera. Although various\ndomain adaptation and generalization methods were proposed to promote the\nadaptability of deep-based rPPG models in unseen deployment environments,\nconsiderations in aspects like privacy concerns and real-time adaptation\nrestrict their application in real-world deployment. Thus, we aim to propose a\nnovel fully Test-Time Adaptation (TTA) strategy tailored for rPPG tasks in this\nwork. Specifically, based on prior knowledge in physiology and our\nobservations, we noticed not only there is spatio-temporal consistency in the\nfrequency domain of rPPG signals, but also that inconsistency in the time\ndomain was significant. Given this, by leveraging both consistency and\ninconsistency priors, we introduce an innovative expert knowledge-based\nself-supervised\n\\textbf{C}onsistency-\\textbf{i}n\\textbf{C}onsistency-\\textbf{i}ntegration\n(\\textbf{CiCi}) framework to enhances model adaptation during inference.\nBesides, our approach further incorporates a gradient dynamic control mechanism\nto mitigate potential conflicts between priors, ensuring stable adaptation\nacross instances. Through extensive experiments on five diverse datasets under\nthe TTA protocol, our method consistently outperforms existing techniques,\npresenting state-of-the-art performance in real-time self-supervised adaptation\nwithout accessing source data. The code will be released later.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07908v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07824", "title": "Conditional Unigram Tokenization with Parallel Data", "authors": ["Gianluca Vico", "Jind≈ôinch Libovick√Ω"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      21 pages, 4 figures, submitted to Tokenization Workshop (TokShop) at ICML 2025", "url": "http://arxiv.org/abs/2507.07824v1", "summary": "We introduce conditional unigram tokenization, a novel approach that extends\nunigram tokenization by conditioning target token probabilities on\nsource-language tokens from parallel data. Given a fixed source tokenizer, our\nmethod learns a target tokenizer that maximizes cross-lingual semantic\nalignment. We evaluate our tokenizer on four language pairs across different\nfamilies and resource levels, examining intrinsic properties and downstream\nperformance on machine translation and language modeling. While our conditional\ntokenizer maintains comparable statistical properties to standard unigram\ntokenizers, results are mixed: we observe no improvements in machine\ntranslation quality, but find consistent perplexity reductions in language\nmodeling. We hypothesize that quadratic scaling of conditional probability\nestimation with respect to the vocabulary size creates a data efficiency\nbottleneck. Our findings suggest that alternative parameterizations may be\nnecessary for practical cross-lingual tokenization.", "comment": "21 pages, 4 figures, submitted to Tokenization Workshop (TokShop) at\n  ICML 2025", "pdf_url": "http://arxiv.org/pdf/2507.07824v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.02097", "title": "The Future is Agentic: Definitions, Perspectives, and Open Challenges of Multi-Agent Recommender Systems", "authors": ["Reza Yousefi Maragheh", "Yashar Deldjoo"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02097v2", "summary": "Large language models (LLMs) are rapidly evolving from passive engines of\ntext generation into agentic entities that can plan, remember, invoke external\ntools, and co-operate with one another. This perspective paper investigates how\nsuch LLM agents (and societies thereof) can transform the design space of\nrecommender systems.\n  We introduce a unified formalism that (i) models an individual agent as a\ntuple comprising its language core, tool set, and hierarchical memory, and (ii)\ncaptures a multi-agent recommender as a triple of agents, shared environment,\nand communication protocol. Within this framework, we present four end-to-end\nuse cases-interactive party planning, synthetic user-simulation for offline\nevaluation, multi-modal furniture recommendation, and brand-aligned explanation\ngeneration-each illustrating a distinct capability unlocked by agentic\norchestration.\n  We then surface five cross-cutting challenge families: protocol complexity,\nscalability, hallucination and error propagation, emergent misalignment\n(including covert collusion), and brand compliance.\n  For each, we formalize the problem, review nascent mitigation strategies, and\noutline open research questions. The result is both a blueprint and an agenda:\na blueprint that shows how memory-augmented, tool-using LLM agents can be\ncomposed into robust recommendation pipelines, and an agenda inviting the\nRecSys community to develop benchmarks, theoretical guarantees, and governance\ntools that keep pace with this new degree of autonomy. By unifying agentic\nabstractions with recommender objectives, the paper lays the groundwork for the\nnext generation of personalized, trustworthy, and context-rich recommendation\nservices.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02097v2", "cate": "cs.IR", "date": "2025-07-02", "updated": "2025-07-10"}
{"id": "2507.07717", "title": "A preconditioned boundary value method for advection-diffusion equations with half Laplacian via spectrum doubling", "authors": ["Pu Yuan", "Paul Zegeling", "Xian-Ming Gu"], "categories": ["math.NA", "cs.NA", "math.AP", "35R11, 35Q84, 65R15, 65M12, 35Q41"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07717v1", "summary": "In this paper, we study an advection-diffusion equation that involves a\nhalf-Laplacian operator derived from the Riesz fractional Laplacian, combined\nwith a differential operator \\(\\mathcal{L}\\). By applying the half-Laplacian\noperator $(-\\Delta)^{\\frac{1}{2}}$ on both sides of the equation and using the\nrelationship between the Hilbert transform and $(-\\Delta)^{\\frac{1}{2}}$, we\nreformulate the problem as a second-order damped Cauchy problem and then\nconvert it into an equivalent first-order system. This \\textit{spectrum\ndoubling} (SD) reformulation applies the half-Laplacian only once to the\ninitial condition, thereby eliminating the need to evaluate singular integrals\nduring the time evolution and reducing truncation-related numerical errors. For\nthe resulting SD system, we show that standard time-stepping schemes can lose\nstability because of the backward-diffusion term. To address this, we adopt\nBoundary Value Methods (BVMs), which yield unconditional stability and\nsecond-order accuracy. We present eigenvalue-based stability criteria, error\nestimates, and an efficient block formulation to solve the resulting large\nlinear systems. To further enhance computational efficiency, we propose a\nparallel preconditioned iterative solver. Numerical experiments confirm the\nsecond-order convergences in both time and space, even under strong advection\nor for complex fractional Schr\\\"odinger-type problems, demonstrating the\neffectiveness and versatility of the proposed approach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07717v1", "cate": "math.NA", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2506.15220", "title": "video-SALMONN 2: Captioning-Enhanced Audio-Visual Large Language Models", "authors": ["Changli Tang", "Yixuan Li", "Yudong Yang", "Jimin Zhuang", "Guangzhi Sun", "Wei Li", "Zejun Ma", "Chao Zhang"], "categories": ["cs.CV", "cs.CL", "cs.SD"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.15220v2", "summary": "Videos contain a wealth of information, and generating detailed and accurate\ndescriptions in natural language is a key aspect of video understanding. In\nthis paper, we present video-SALMONN 2, an advanced audio-visual large language\nmodel (LLM) with low-rank adaptation (LoRA) designed for enhanced video (with\npaired audio) captioning through directed preference optimisation (DPO). We\npropose new metrics to evaluate the completeness and accuracy of video\ndescriptions, which are optimised using DPO. To further improve training, we\npropose a novel multi-round DPO (MrDPO) approach, which involves periodically\nupdating the DPO reference model, merging and re-initialising the LoRA module\nas a proxy for parameter updates after each training round (1,000 steps), and\nincorporating guidance from ground-truth video captions to stabilise the\nprocess. Experimental results show that MrDPO significantly enhances\nvideo-SALMONN 2's captioning accuracy, reducing the captioning error rates by\n28\\%. The final video-SALMONN 2 model, with just 7 billion parameters,\nsurpasses leading models such as GPT-4o and Gemini-1.5-Pro in video captioning\ntasks, while maintaining highly competitive performance to the state-of-the-art\non widely used video question-answering benchmarks among models of similar\nsize. Codes are available at\n\\href{https://github.com/bytedance/video-SALMONN-2}{https://github.com/bytedance/video-SALMONN-2}.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.15220v2", "cate": "cs.CV", "date": "2025-06-18", "updated": "2025-07-10"}
{"id": "2504.02670", "title": "Affordable AI Assistants with Knowledge Graph of Thoughts", "authors": ["Maciej Besta", "Lorenzo Paleari", "Jia Hao Andrea Jiang", "Robert Gerstenberger", "You Wu", "J√≥n Gunnar Hannesson", "Patrick Iff", "Ales Kubicek", "Piotr Nyczyk", "Diana Khimey", "Nils Blach", "Haiqiang Zhang", "Tao Zhang", "Peiran Ma", "Grzegorz Kwa≈õniewski", "Marcin Copik", "Hubert Niewiadomski", "Torsten Hoefler"], "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.02670v5", "summary": "Large Language Models (LLMs) are revolutionizing the development of AI\nassistants capable of performing diverse tasks across domains. However, current\nstate-of-the-art LLM-driven agents face significant challenges, including high\noperational costs and limited success rates on complex benchmarks like GAIA. To\naddress these issues, we propose Knowledge Graph of Thoughts (KGoT), an\ninnovative AI assistant architecture that integrates LLM reasoning with\ndynamically constructed knowledge graphs (KGs). KGoT extracts and structures\ntask-relevant knowledge into a dynamic KG representation, iteratively enhanced\nthrough external tools such as math solvers, web crawlers, and Python scripts.\nSuch structured representation of task-relevant knowledge enables low-cost\nmodels to solve complex tasks effectively while also minimizing bias and noise.\nFor example, KGoT achieves a 29% improvement in task success rates on the GAIA\nbenchmark compared to Hugging Face Agents with GPT-4o mini. Moreover,\nharnessing a smaller model dramatically reduces operational costs by over 36x\ncompared to GPT-4o. Improvements for other models (e.g., Qwen2.5-32B and\nDeepseek-R1-70B) and benchmarks (e.g., SimpleQA) are similar. KGoT offers a\nscalable, affordable, versatile, and high-performing solution for AI\nassistants.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.02670v5", "cate": "cs.AI", "date": "2025-04-03", "updated": "2025-07-10"}
{"id": "2507.07339", "title": "Benchmarking Waitlist Mortality Prediction in Heart Transplantation Through Time-to-Event Modeling using New Longitudinal UNOS Dataset", "authors": ["Yingtao Luo", "Reza Skandari", "Carlos Martinez", "Arman Kilic", "Rema Padman"], "categories": ["stat.AP", "cs.LG"], "primary_category": "Subjects:       Applications (stat.AP)", "pdf_link": null, "comments": "Comments:      To appear in the Proceedings of AMIA Annual Symposium 2025", "url": "http://arxiv.org/abs/2507.07339v1", "summary": "Decisions about managing patients on the heart transplant waitlist are\ncurrently made by committees of doctors who consider multiple factors, but the\nprocess remains largely ad-hoc. With the growing volume of longitudinal\npatient, donor, and organ data collected by the United Network for Organ\nSharing (UNOS) since 2018, there is increasing interest in analytical\napproaches to support clinical decision-making at the time of organ\navailability. In this study, we benchmark machine learning models that leverage\nlongitudinal waitlist history data for time-dependent, time-to-event modeling\nof waitlist mortality. We train on 23,807 patient records with 77 variables and\nevaluate both survival prediction and discrimination at a 1-year horizon. Our\nbest model achieves a C-Index of 0.94 and AUROC of 0.89, significantly\noutperforming previous models. Key predictors align with known risk factors\nwhile also revealing novel associations. Our findings can support urgency\nassessment and policy refinement in heart transplant decision making.", "comment": "To appear in the Proceedings of AMIA Annual Symposium 2025", "pdf_url": "http://arxiv.org/pdf/2507.07339v1", "cate": "stat.AP", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.07920", "title": "ArteryX: Advancing Brain Artery Feature Extraction with Vessel-Fused Networks and a Robust Validation Framework", "authors": ["Abrar Faiyaz", "Nhat Hoang", "Giovanni Schifitto", "Md Nasir Uddin"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      14 Pages, 8 Figures, Preliminary version of the toolbox was presented at the ISMRM 2025 Conference in Hawaii at the \"Software Tools\" Session", "url": "http://arxiv.org/abs/2507.07920v1", "summary": "Cerebrovascular pathology significantly contributes to cognitive decline and\nneurological disorders, underscoring the need for advanced tools to assess\nvascular integrity. Three-dimensional Time-of-Flight Magnetic Resonance\nAngiography (3D TOF MRA) is widely used to visualize cerebral vasculature,\nhowever, clinical evaluations generally focus on major arterial abnormalities,\noverlooking quantitative metrics critical for understanding subtle vascular\nchanges. Existing methods for extracting structural, geometrical and\nmorphological arterial features from MRA - whether manual or automated - face\nchallenges including user-dependent variability, steep learning curves, and\nlack of standardized quantitative validations. We propose a novel\nsemi-supervised artery evaluation framework, named ArteryX, a MATLAB-based\ntoolbox that quantifies vascular features with high accuracy and efficiency,\nachieving processing times ~10-15 minutes per subject at 0.5 mm resolution with\nminimal user intervention. ArteryX employs a vessel-fused network based\nlandmarking approach to reliably track and manage tracings, effectively\naddressing the issue of dangling/disconnected vessels. Validation on human\nsubjects with cerebral small vessel disease demonstrated its improved\nsensitivity to subtle vascular changes and better performance than an existing\nsemi-automated method. Importantly, the ArteryX toolbox enables quantitative\nfeature validation by integrating an in-vivo like artery simulation framework\nutilizing vessel-fused graph nodes and predefined ground-truth features for\nspecific artery types. Thus, the ArteryX framework holds promise for\nbenchmarking feature extraction toolboxes and for seamless integration into\nclinical workflows, enabling early detection of cerebrovascular pathology and\nstandardized comparisons across patient cohorts to advance understanding of\nvascular contributions to brain health.", "comment": "14 Pages, 8 Figures, Preliminary version of the toolbox was presented\n  at the ISMRM 2025 Conference in Hawaii at the \"Software Tools\" Session", "pdf_url": "http://arxiv.org/pdf/2507.07920v1", "cate": "eess.IV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07870", "title": "DocCHA: Towards LLM-Augmented Interactive Online diagnosis System", "authors": ["Xinyi Liu", "Dachun Sun", "Yi R. Fung", "Dilek Hakkani-T√ºr", "Tarek Abdelzaher"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07870v1", "summary": "Despite the impressive capabilities of Large Language Models (LLMs), existing\nConversational Health Agents (CHAs) remain static and brittle, incapable of\nadaptive multi-turn reasoning, symptom clarification, or transparent\ndecision-making. This hinders their real-world applicability in clinical\ndiagnosis, where iterative and structured dialogue is essential. We propose\nDocCHA, a confidence-aware, modular framework that emulates clinical reasoning\nby decomposing the diagnostic process into three stages: (1) symptom\nelicitation, (2) history acquisition, and (3) causal graph construction. Each\nmodule uses interpretable confidence scores to guide adaptive questioning,\nprioritize informative clarifications, and refine weak reasoning links.\n  Evaluated on two real-world Chinese consultation datasets (IMCS21, DX),\nDocCHA consistently outperforms strong prompting-based LLM baselines (GPT-3.5,\nGPT-4o, LLaMA-3), achieving up to 5.18 percent higher diagnostic accuracy and\nover 30 percent improvement in symptom recall, with only modest increase in\ndialogue turns. These results demonstrate the effectiveness of DocCHA in\nenabling structured, transparent, and efficient diagnostic conversations --\npaving the way for trustworthy LLM-powered clinical assistants in multilingual\nand resource-constrained settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07870v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.06503", "title": "USD: A User-Intent-Driven Sampling and Dual-Debiasing Framework for Large-Scale Homepage Recommendations", "authors": ["Jiaqi Zheng", "Cheng Guo", "Yi Cao", "Chaoqun Hou", "Tong Liu", "Bo Zheng"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.06503v2", "summary": "Large-scale homepage recommendations face critical challenges from\npseudo-negative samples caused by exposure bias, where non-clicks may indicate\ninattention rather than disinterest. Existing work lacks thorough analysis of\ninvalid exposures and typically addresses isolated aspects (e.g., sampling\nstrategies), overlooking the critical impact of pseudo-positive samples - such\nas homepage clicks merely to visit marketing portals. We propose a unified\nframework for large-scale homepage recommendation sampling and debiasing. Our\nframework consists of two key components: (1) a user intent-aware negative\nsampling module to filter invalid exposure samples, and (2) an intent-driven\ndual-debiasing module that jointly corrects exposure bias and click bias.\nExtensive online experiments on Taobao demonstrate the efficacy of our\nframework, achieving significant improvements in user click-through rates\n(UCTR) by 35.4% and 14.5% in two variants of the marketing block on the Taobao\nhomepage, Baiyibutie and Taobaomiaosha.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.06503v2", "cate": "cs.IR", "date": "2025-07-09", "updated": "2025-07-10"}
{"id": "2507.07788", "title": "Towards an Efficient Shifted Cholesky QR for Applications in Model Order Reduction using pyMOR", "authors": ["Maximilian Bindhak", "Art J. R. Pelling", "Jens Saak"], "categories": ["math.NA", "cs.NA", "65F25, 15A23, 15A12, 65F35, 68Q25, 65Y20"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      Preprint", "url": "http://arxiv.org/abs/2507.07788v1", "summary": "Many model order reduction (MOR) methods rely on the computation of an\northonormal basis of a subspace onto which the large full order model is\nprojected. Numerically, this entails the orthogonalization of a set of vectors.\nThe nature of the MOR process imposes several requirements for the\northogonalization process. Firstly, MOR is oftentimes performed in an adaptive\nor iterative manner, where the quality of the reduced order model, i.e., the\ndimension of the reduced subspace, is decided on the fly. Therefore, it is\nimportant that the orthogonalization routine can be executed iteratively.\nSecondly, one possibly has to deal with high-dimensional arrays of abstract\nvectors that do not allow explicit access to entries, making it difficult to\nemploy so-called `orthogonal triangularization algorithms' such as Householder\nQR.\n  For these reasons, (modified) Gram-Schmidt-type algorithms are commonly used\nin MOR applications. These methods belong to the category of `triangular\northogonalization' algorithms that do not rely on elementwise access to the\nvectors and can be easily updated. Recently, algorithms like shifted Cholesky\nQR have gained attention. These also belong to the aforementioned category and\nhave proven their aptitude for MOR algorithms in previous studies. A key\nbenefit of these methods is that they are communication-avoiding, leading to\nvastly superior performance on memory-bandwidth-limited problems and parallel\nor distributed architectures. This work formulates an efficient updating scheme\nfor Cholesky QR algorithms and proposes an improved shifting strategy for\nhighly ill-conditioned matrices.\n  The proposed algorithmic extensions are validated with numerical experiments\non a laptop and computation server.", "comment": "Preprint", "pdf_url": "http://arxiv.org/pdf/2507.07788v1", "cate": "math.NA", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2505.09341", "title": "Access Controls Will Solve the Dual-Use Dilemma", "authors": ["Ev≈æen Wybitul"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Accepted at ICML 2025 Workshop on Technical AI Governance (TAIG)", "url": "http://arxiv.org/abs/2505.09341v2", "summary": "AI safety systems face the dual-use dilemma: it can be unclear whether to\nrefuse certain requests, since they could be either harmless or harmful\ndepending on who made them and why. Determining this requires examining their\nreal-world context, but current safety systems cannot access this contextual\ninformation. Instead, they make arbitrary decisions that end up hurting both\nutility and safety: they sometimes refuse legitimate queries and other times\nfail to refuse harmful ones. To address this, we propose a conceptual framework\nbased on access controls in which only verified users can access dual-use\noutputs. We describe the framework's components, analyse its feasibility, and\nexplain how it addresses both over-refusals and under-refusals. While only a\nhigh-level proposal, our work takes the first step toward enabling more nuanced\nsafety decisions: with better tools for managing dual-use content, model\nproviders could enable users to access more capabilities without sacrificing\nsafety, and give regulators new options for more targeted policies.", "comment": "Accepted at ICML 2025 Workshop on Technical AI Governance (TAIG)", "pdf_url": "http://arxiv.org/pdf/2505.09341v2", "cate": "cs.AI", "date": "2025-05-14", "updated": "2025-07-10"}
{"id": "2507.07343", "title": "Way More Than the Sum of Their Parts: From Statistical to Structural Mixtures", "authors": ["James P. Crutchfield"], "categories": ["cond-mat.stat-mech", "cs.LG", "math.DS", "math.ST", "nlin.CD", "stat.TH"], "primary_category": "Subjects:       Statistical Mechanics (cond-mat.stat-mech)", "pdf_link": null, "comments": "Comments:      22 pages, 16 Figures; this http URL", "url": "http://arxiv.org/abs/2507.07343v1", "summary": "We show that mixtures comprised of multicomponent systems typically are much\nmore structurally complex than the sum of their parts; sometimes, infinitely\nmore complex. We contrast this with the more familiar notion of statistical\nmixtures, demonstrating how statistical mixtures miss key aspects of emergent\nhierarchical organization. This leads us to identify a new kind of structural\ncomplexity inherent in multicomponent systems and to draw out broad\nconsequences for system ergodicity.", "comment": "22 pages, 16 Figures;\n  http://csc.ucdavis.edu/~cmg/compmech/pubs/wmttsotp.htm", "pdf_url": "http://arxiv.org/pdf/2507.07343v1", "cate": "cond-mat.stat-mech", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07949", "title": "TinierHAR: Towards Ultra-Lightweight Deep Learning Models for Efficient Human Activity Recognition on Edge Devices", "authors": ["Sizhen Bian", "Mengxi Liu", "Vitor Fortes Rey", "Daniel Geissler", "Paul Lukowicz"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07949v1", "summary": "Human Activity Recognition (HAR) on resource-constrained wearable devices\ndemands inference models that harmonize accuracy with computational efficiency.\nThis paper introduces TinierHAR, an ultra-lightweight deep learning\narchitecture that synergizes residual depthwise separable convolutions, gated\nrecurrent units (GRUs), and temporal aggregation to achieve SOTA efficiency\nwithout compromising performance. Evaluated across 14 public HAR datasets,\nTinierHAR reduces Parameters by 2.7x (vs. TinyHAR) and 43.3x (vs.\nDeepConvLSTM), and MACs by 6.4x and 58.6x, respectively, while maintaining the\naveraged F1-scores. Beyond quantitative gains, this work provides the first\nsystematic ablation study dissecting the contributions of spatial-temporal\ncomponents across proposed TinierHAR, prior SOTA TinyHAR, and the classical\nDeepConvLSTM, offering actionable insights for designing efficient HAR systems.\nWe finally discussed the findings and suggested principled design guidelines\nfor future efficient HAR. To catalyze edge-HAR research, we open-source all\nmaterials in this work for future\nbenchmarking\\footnote{https://github.com/zhaxidele/TinierHAR}", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07949v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07887", "title": "Automating MD simulations for Proteins using Large language Models: NAMD-Agent", "authors": ["Achuth Chandrasekhar", "Amir Barati Farimani"], "categories": ["cs.CL", "cs.CE", "q-bio.BM"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      34 pages", "url": "http://arxiv.org/abs/2507.07887v1", "summary": "Molecular dynamics simulations are an essential tool in understanding protein\nstructure, dynamics, and function at the atomic level. However, preparing high\nquality input files for MD simulations can be a time consuming and error prone\nprocess. In this work, we introduce an automated pipeline that leverages Large\nLanguage Models (LLMs), specifically Gemini 2.0 Flash, in conjunction with\npython scripting and Selenium based web automation to streamline the generation\nof MD input files. The pipeline exploits CHARMM GUI's comprehensive web-based\ninterface for preparing simulation-ready inputs for NAMD. By integrating\nGemini's code generation and iterative refinement capabilities, simulation\nscripts are automatically written, executed, and revised to navigate CHARMM\nGUI, extract appropriate parameters, and produce the required NAMD input files.\nPost processing is performed using additional software to further refine the\nsimulation outputs, thereby enabling a complete and largely hands free\nworkflow. Our results demonstrate that this approach reduces setup time,\nminimizes manual errors, and offers a scalable solution for handling multiple\nprotein systems in parallel. This automated framework paves the way for broader\napplication of LLMs in computational structural biology, offering a robust and\nadaptable platform for future developments in simulation automation.", "comment": "34 pages", "pdf_url": "http://arxiv.org/pdf/2507.07887v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2406.05085", "title": "Multi-Head RAG: Solving Multi-Aspect Problems with LLMs", "authors": ["Maciej Besta", "Ales Kubicek", "Robert Gerstenberger", "Marcin Chrapek", "Roman Niggli", "Patrik Okanovic", "Yi Zhu", "Patrick Iff", "Michal Podstawski", "Lucas Weitzendorf", "Mingyuan Chi", "Joanna Gajda", "Piotr Nyczyk", "J√ºrgen M√ºller", "Hubert Niewiadomski", "Torsten Hoefler"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2406.05085v4", "summary": "Retrieval Augmented Generation (RAG) enhances the abilities of Large Language\nModels (LLMs) by enabling the retrieval of documents into the LLM context to\nprovide more accurate and relevant responses. Existing RAG solutions do not\nfocus on queries that may require fetching multiple documents with\nsubstantially different contents. Such queries occur frequently, but are\nchallenging because the embeddings of these documents may be distant in the\nembedding space, making it hard to retrieve them all. This paper introduces\nMulti-Head RAG (MRAG), a novel scheme designed to address this gap with a\nsimple yet powerful idea: leveraging activations of Transformer's multi-head\nattention layer, instead of the decoder layer, as keys for fetching\nmulti-aspect documents. The driving observation is that different attention\nheads learn to capture different data aspects. Harnessing the corresponding\nactivations results in embeddings that represent various facets of data items\nand queries, improving the retrieval accuracy for complex queries. We provide\nan evaluation methodology and metrics, multi-aspect datasets, and real-world\nuse cases to demonstrate MRAG's effectiveness. We show MRAG's design advantages\nover 18 RAG baselines, empirical improvements of up to 20% in retrieval success\nratios, and benefits for downstream LLM generation. MRAG can be seamlessly\nintegrated with existing RAG frameworks and benchmarks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2406.05085v4", "cate": "cs.CL", "date": "2024-06-07", "updated": "2025-07-10"}
{"id": "2507.07823", "title": "A fast algorithm for the wave equation using time-windowed Fourier projection", "authors": ["Nour G. Al Hassanieh", "Alex H. Barnett", "Leslie Greengard"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      27 pages, 17 figures", "url": "http://arxiv.org/abs/2507.07823v1", "summary": "We introduce a new arbitrarily high-order method for the rapid evaluation of\nhyperbolic potentials (space-time integrals involving the Green's function for\nthe scalar wave equation). With $M$ points in the spatial discretization and\n$N_t$ time steps of size $\\Delta t$, a naive implementation would require\n$\\mathcal O(M^2N_t^2)$ work in dimensions where the weak Huygens' principle\napplies. We avoid this all-to-all interaction using a smoothly windowed\ndecomposition into a local part, treated directly, plus a history part,\napproximated by a $N_F$-term Fourier series. In one dimension, our method\nrequires $\\mathcal O\\left((M + N_F \\log N_F)N_t\\right)$ work, with $N_F\n=\\mathcal O(1/\\Delta t)$, by exploiting the non-uniform fast Fourier transform.\nWe demonstrate the method's performance for time-domain scattering problems\ninvolving a large number $M$ of springs (point scatterers) attached to a\nvibrating string at arbitrary locations, with either periodic or free-space\nboundary conditions. We typically achieve 10-digit accuracy, and include tests\nfor $M$ up to a million.", "comment": "27 pages, 17 figures", "pdf_url": "http://arxiv.org/pdf/2507.07823v1", "cate": "math.NA", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2506.10281", "title": "Closer to Language than Steam: AI as the Cognitive Engine of a New Productivity Revolution", "authors": ["Xinmin Fang", "Lingfeng Tao", "Zhengxiong Li"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      12 pages", "url": "http://arxiv.org/abs/2506.10281v2", "summary": "Artificial Intelligence (AI) is reframed as a cognitive engine driving a\nnovel productivity revolution distinct from the Industrial Revolution's\nphysical thrust. This paper develops a theoretical framing of AI as a cognitive\nrevolution akin to written language - a transformative augmentation of human\nintellect rather than another mechanized tool. We compare AI's emergence to\nhistorical leaps in information technology to show how it amplifies knowledge\nwork. Examples from various domains demonstrate AI's impact as a driver of\nproductivity in cognitive tasks. We adopt a multidisciplinary perspective\ncombining computer science advances with economic insights and sociological\nperspectives on how AI reshapes work and society. Through conceptual\nframeworks, we visualize the shift from manual to cognitive productivity. Our\ncentral argument is that AI functions as an engine of cognition - comparable to\nhow human language revolutionized knowledge - heralding a new productivity\nparadigm. We discuss how this revolution demands rethinking of skills,\norganizations, and policies. This paper, balancing academic rigor with clarity,\nconcludes that AI's promise lies in complementing human cognitive abilities,\nmarking a new chapter in productivity evolution.", "comment": "12 pages", "pdf_url": "http://arxiv.org/pdf/2506.10281v2", "cate": "cs.AI", "date": "2025-06-12", "updated": "2025-07-10"}
{"id": "2507.07367", "title": "Platform for Representation and Integration of multimodal Molecular Embeddings", "authors": ["Erika Yilin Zheng", "Yu Yan", "Baradwaj Simha Sankar", "Ethan Ji", "Steven Swee", "Irsyad Adam", "Ding Wang", "Alexander Russell Pelletier", "Alex Bui", "Wei Wang", "Peipei Ping"], "categories": ["q-bio.BM", "cs.LG"], "primary_category": "Subjects:       Biomolecules (q-bio.BM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07367v1", "summary": "Existing machine learning methods for molecular (e.g., gene) embeddings are\nrestricted to specific tasks or data modalities, limiting their effectiveness\nwithin narrow domains. As a result, they fail to capture the full breadth of\ngene functions and interactions across diverse biological contexts. In this\nstudy, we have systematically evaluated knowledge representations of\nbiomolecules across multiple dimensions representing a task-agnostic manner\nspanning three major data sources, including omics experimental data,\nliterature-derived text data, and knowledge graph-based representations. To\ndistinguish between meaningful biological signals from chance correlations, we\ndevised an adjusted variant of Singular Vector Canonical Correlation Analysis\n(SVCCA) that quantifies signal redundancy and complementarity across different\ndata modalities and sources. These analyses reveal that existing embeddings\ncapture largely non-overlapping molecular signals, highlighting the value of\nembedding integration. Building on this insight, we propose Platform for\nRepresentation and Integration of multimodal Molecular Embeddings (PRISME), a\nmachine learning based workflow using an autoencoder to integrate these\nheterogeneous embeddings into a unified multimodal representation. We validated\nthis approach across various benchmark tasks, where PRISME demonstrated\nconsistent performance, and outperformed individual embedding methods in\nmissing value imputations. This new framework supports comprehensive modeling\nof biomolecules, advancing the development of robust, broadly applicable\nmultimodal embeddings optimized for downstream biomedical machine learning\napplications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07367v1", "cate": "q-bio.BM", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07978", "title": "Martian World Models: Controllable Video Synthesis with Physically Accurate 3D Reconstructions", "authors": ["Longfei Li", "Zhiwen Fan", "Wenyan Cong", "Xinhang Liu", "Yuyang Yin", "Matt Foutter", "Panwang Pan", "Chenyu You", "Yue Wang", "Zhangyang Wang", "Yao Zhao", "Marco Pavone", "Yunchao Wei"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project Page: this https URL", "url": "http://arxiv.org/abs/2507.07978v1", "summary": "Synthesizing realistic Martian landscape videos is crucial for mission\nrehearsal and robotic simulation. However, this task poses unique challenges\ndue to the scarcity of high-quality Martian data and the significant domain gap\nbetween Martian and terrestrial imagery. To address these challenges, we\npropose a holistic solution composed of two key components: 1) A data curation\npipeline Multimodal Mars Synthesis (M3arsSynth), which reconstructs 3D Martian\nenvironments from real stereo navigation images, sourced from NASA's Planetary\nData System (PDS), and renders high-fidelity multiview 3D video sequences. 2) A\nMartian terrain video generator, MarsGen, which synthesizes novel videos\nvisually realistic and geometrically consistent with the 3D structure encoded\nin the data. Our M3arsSynth engine spans a wide range of Martian terrains and\nacquisition dates, enabling the generation of physically accurate 3D surface\nmodels at metric-scale resolution. MarsGen, fine-tuned on M3arsSynth data,\nsynthesizes videos conditioned on an initial image frame and, optionally,\ncamera trajectories or textual prompts, allowing for video generation in novel\nenvironments. Experimental results show that our approach outperforms video\nsynthesis models trained on terrestrial datasets, achieving superior visual\nfidelity and 3D structural consistency.", "comment": "Project Page: https://marsgenai.github.io", "pdf_url": "http://arxiv.org/pdf/2507.07978v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07939", "title": "SAGE: A Visual Language Model for Anomaly Detection via Fact Enhancement and Entropy-aware Alignment", "authors": ["Guoxin Zang", "Xue Li", "Donglin Di", "Lanshun Nie", "Dechen Zhan", "Yang Song", "Lei Fan"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted by ACMMM2025", "url": "http://arxiv.org/abs/2507.07939v1", "summary": "While Vision-Language Models (VLMs) have shown promising progress in general\nmultimodal tasks, they often struggle in industrial anomaly detection and\nreasoning, particularly in delivering interpretable explanations and\ngeneralizing to unseen categories. This limitation stems from the inherently\ndomain-specific nature of anomaly detection, which hinders the applicability of\nexisting VLMs in industrial scenarios that require precise, structured, and\ncontext-aware analysis. To address these challenges, we propose SAGE, a\nVLM-based framework that enhances anomaly reasoning through Self-Guided Fact\nEnhancement (SFE) and Entropy-aware Direct Preference Optimization (E-DPO). SFE\nintegrates domain-specific knowledge into visual reasoning via fact extraction\nand fusion, while E-DPO aligns model outputs with expert preferences using\nentropy-aware optimization. Additionally, we introduce AD-PL, a\npreference-optimized dataset tailored for industrial anomaly reasoning,\nconsisting of 28,415 question-answering instances with expert-ranked responses.\nTo evaluate anomaly reasoning models, we develop Multiscale Logical Evaluation\n(MLE), a quantitative framework analyzing model logic and consistency. SAGE\ndemonstrates superior performance on industrial anomaly datasets under\nzero-shot and one-shot settings. The code, model and dataset are available at\nhttps://github.com/amoreZgx1n/SAGE.", "comment": "Accepted by ACMMM2025", "pdf_url": "http://arxiv.org/pdf/2507.07939v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2412.00569", "title": "Contextual Bandits in Payment Processing: Non-uniform Exploration and Supervised Learning", "authors": ["Akhila Vangara", "Alex Egg"], "categories": ["cs.LG", "cs.IR"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      7 pages, 10 figures, submitted to KDD '25", "url": "http://arxiv.org/abs/2412.00569v2", "summary": "Uniform random exploration in decision-making systems supports off-policy\nlearning via supervision but incurs high regret, making it impractical for many\napplications. Conversely, non-uniform exploration offers better immediate\nperformance but lacks support for off-policy learning. Recent research suggests\nthat regression oracles can bridge this gap by combining non-uniform\nexploration with supervised learning. In this paper, we analyze these\napproaches within a real-world industrial context at Adyen, a large global\npayments processor characterized by batch logged delayed feedback, short-term\nmemory, and dynamic action spaces under the Empirical Risk Minimization (ERM)\nframework. Our analysis reveals that while regression oracles significantly\nimprove performance, they introduce challenges due to rigid algorithmic\nassumptions. Specifically, we observe that as a policy improves, subsequent\ngenerations may perform worse due to shifts in the reward distribution and\nincreased class imbalance in the training data. This degradation occurs de\nspite improvements in other aspects of the training data, leading to decreased\nperformance in successive policy iterations. We further explore the long-term\nimpact of regression oracles, identifying a potential \"oscillation effect.\"\nThis effect arises when regression oracles influence probability estimates and\nthe realizability of subsequent policy models, leading to fluctuations in\nperformance across iterations. Our findings highlight the need for more\nadaptable algorithms that can leverage the benefits of regression oracles\nwithout introducing instability in policy performance over time.", "comment": "7 pages, 10 figures, submitted to KDD '25", "pdf_url": "http://arxiv.org/pdf/2412.00569v2", "cate": "cs.LG", "date": "2024-11-30", "updated": "2025-07-10"}
{"id": "2507.07652", "title": "A Novel Hybrid Approach for Time Series Forecasting: Period Estimation and Climate Data Analysis Using Unsupervised Learning and Spline Interpolation", "authors": ["Tanmay Kayal", "Abhishek Das", "U Saranya"], "categories": ["stat.AP", "cs.NA", "math.NA", "62M10, 65D07, 62J05"], "primary_category": "Subjects:       Applications (stat.AP)", "pdf_link": null, "comments": "Comments:      17 Pages, 13 figures", "url": "http://arxiv.org/abs/2507.07652v1", "summary": "This article explores a novel approach to time series forecasting applied to\nthe context of Chennai's climate data. Our methodology comprises two distinct\nestablished time series models, leveraging their strengths in handling\nseasonality and periods. Notably, a new algorithm is developed to compute the\nperiod of the time series using unsupervised machine learning and spline\ninterpolation techniques. Through a meticulous ensembling process that combines\nthese two models, we achieve optimized forecasts. This research contributes to\nadvancing forecasting techniques and offers valuable insights into climate data\nanalysis.", "comment": "17 Pages, 13 figures", "pdf_url": "http://arxiv.org/pdf/2507.07652v1", "cate": "stat.AP", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07889", "title": "The integro-differential closure of a commutative differential ring", "authors": ["Clemens G. Raab", "Georg Regensburger"], "categories": ["math.RA", "cs.SC", "math.AC", "13N99, 13B99, 16S10, 16W99, 33F10"], "primary_category": "Subjects:       Rings and Algebras (math.RA)", "pdf_link": null, "comments": "Comments:      39 pages", "url": "http://arxiv.org/abs/2507.07889v1", "summary": "An integro-differential ring is a differential ring that is closed under an\nintegration operation satisfying the fundamental theorem of calculus. Via the\nNewton--Leibniz formula, a generalized evaluation is defined in terms of\nintegration and differentiation. The induced evaluation is not necessarily\nmultiplicative, which allows to model functions with singularities and leads to\ngeneralized shuffle relations. In general, not every element of a differential\nring has an antiderivative in the same ring. Starting from a commutative\ndifferential ring and a direct decomposition into integrable and non-integrable\nelements, we construct the free integro-differential ring. This\nintegro-differential closure contains all nested integrals over elements of the\noriginal differential ring. We exhibit the relations satisfied by generalized\nevaluations of products of nested integrals. Investigating these relations of\nconstants, we characterize in terms of Lyndon words certain evaluations of\nproducts that determine all others. We also analyze the relation of the free\nintegro-differential ring with the shuffle algebra. To preserve integrals in\nthe original differential ring for computations in its integro-differential\nclosure, we introduce the notion of quasi-integro-differential rings and give\nan adapted construction of the free integro-differential ring. Finally, in a\ngiven integro-differential ring, we consider the internal integro-differential\nclosure of a differential subring and identify it as quotient of the free\nintegro-differential ring by certain constants.", "comment": "39 pages", "pdf_url": "http://arxiv.org/pdf/2507.07889v1", "cate": "math.RA", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2506.23080", "title": "AI's Euclid's Elements Moment: From Language Models to Computable Thought", "authors": ["Xinmin Fang", "Lingfeng Tao", "Zhengxiong Li"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.23080v2", "summary": "This paper presents a comprehensive five-stage evolutionary framework for\nunderstanding the development of artificial intelligence, arguing that its\ntrajectory mirrors the historical progression of human cognitive technologies.\nWe posit that AI is advancing through distinct epochs, each defined by a\nrevolutionary shift in its capacity for representation and reasoning, analogous\nto the inventions of cuneiform, the alphabet, grammar and logic, mathematical\ncalculus, and formal logical systems. This \"Geometry of Cognition\" framework\nmoves beyond mere metaphor to provide a systematic, cross-disciplinary model\nthat not only explains AI's past architectural shifts-from expert systems to\nTransformers-but also charts a concrete and prescriptive path forward.\nCrucially, we demonstrate that this evolution is not merely linear but\nreflexive: as AI advances through these stages, the tools and insights it\ndevelops create a feedback loop that fundamentally reshapes its own underlying\narchitecture. We are currently transitioning into a \"Metalinguistic Moment,\"\ncharacterized by the emergence of self-reflective capabilities like\nChain-of-Thought prompting and Constitutional AI. The subsequent stages, the\n\"Mathematical Symbolism Moment\" and the \"Formal Logic System Moment,\" will be\ndefined by the development of a computable calculus of thought, likely through\nneuro-symbolic architectures and program synthesis, culminating in provably\naligned and reliable AI that reconstructs its own foundational representations.\nThis work serves as the methodological capstone to our trilogy, which\npreviously explored the economic drivers (\"why\") and cognitive nature (\"what\")\nof AI. Here, we address the \"how,\" providing a theoretical foundation for\nfuture research and offering concrete, actionable strategies for startups and\ndevelopers aiming to build the next generation of intelligent systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23080v2", "cate": "cs.AI", "date": "2025-06-29", "updated": "2025-07-10"}
{"id": "2507.07420", "title": "Probabilistic Approximate Optimization: A New Variational Monte Carlo Algorithm", "authors": ["Abdelrahman S. Abdelrahman", "Shuvro Chowdhury", "Flaviano Morone", "Kerem Y. Camsari"], "categories": ["cond-mat.dis-nn", "cs.LG", "quant-ph"], "primary_category": "Subjects:       Disordered Systems and Neural Networks (cond-mat.dis-nn)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07420v1", "summary": "We introduce a generalized \\textit{Probabilistic Approximate Optimization\nAlgorithm (PAOA)}, a classical variational Monte Carlo framework that extends\nand formalizes prior work by Weitz \\textit{et al.}~\\cite{Combes_2023}, enabling\nparameterized and fast sampling on present-day Ising machines and probabilistic\ncomputers. PAOA operates by iteratively modifying the couplings of a network of\nbinary stochastic units, guided by cost evaluations from independent samples.\nWe establish a direct correspondence between derivative-free updates and the\ngradient of the full $2^N \\times 2^N$ Markov flow, showing that PAOA admits a\nprincipled variational formulation. Simulated annealing emerges as a limiting\ncase under constrained parameterizations, and we implement this regime on an\nFPGA-based probabilistic computer with on-chip annealing to solve large 3D\nspin-glass problems. Benchmarking PAOA against QAOA on the canonical 26-spin\nSherrington-Kirkpatrick model with matched parameters reveals superior\nperformance for PAOA. We show that PAOA naturally extends simulated annealing\nby optimizing multiple temperature profiles, leading to improved performance\nover SA on heavy-tailed problems such as SK-L\\'evy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07420v1", "cate": "cond-mat.dis-nn", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07984", "title": "OST-Bench: Evaluating the Capabilities of MLLMs in Online Spatio-temporal Scene Understanding", "authors": ["JingLi Lin", "Chenming Zhu", "Runsen Xu", "Xiaohan Mao", "Xihui Liu", "Tai Wang", "Jiangmiao Pang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      28 pages, a benchmark designed to evaluate Online Spatio-Temporal understanding from the perspective of an agent actively exploring a scene. Project Page: this https URL", "url": "http://arxiv.org/abs/2507.07984v1", "summary": "Recent advances in multimodal large language models (MLLMs) have shown\nremarkable capabilities in integrating vision and language for complex\nreasoning. While most existing benchmarks evaluate models under offline\nsettings with a fixed set of pre-recorded inputs, we introduce OST-Bench, a\nbenchmark designed to evaluate Online Spatio-Temporal understanding from the\nperspective of an agent actively exploring a scene. The Online aspect\nemphasizes the need to process and reason over incrementally acquired\nobservations, while the Spatio-Temporal component requires integrating current\nvisual inputs with historical memory to support dynamic spatial reasoning.\nOST-Bench better reflects the challenges of real-world embodied perception.\nBuilt on an efficient data collection pipeline, OST-Bench consists of 1.4k\nscenes and 10k question-answer pairs collected from ScanNet, Matterport3D, and\nARKitScenes. We evaluate several leading MLLMs on OST-Bench and observe that\nthey fall short on tasks requiring complex spatio-temporal reasoning. Under the\nonline setting, their accuracy declines as the exploration horizon extends and\nthe memory grows. Through further experimental analysis, we identify common\nerror patterns across models and find that both complex clue-based spatial\nreasoning demands and long-term memory retrieval requirements significantly\ndrop model performance along two separate axes, highlighting the core\nchallenges that must be addressed to improve online embodied reasoning. To\nfoster further research and development in the field, our codes, dataset, and\nbenchmark are available. Our project page is:\nhttps://rbler1234.github.io/OSTBench.github.io/", "comment": "28 pages, a benchmark designed to evaluate Online Spatio-Temporal\n  understanding from the perspective of an agent actively exploring a scene.\n  Project Page: https://rbler1234.github.io/OSTBench.github.io/", "pdf_url": "http://arxiv.org/pdf/2507.07984v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07988", "title": "Automating Expert-Level Medical Reasoning Evaluation of Large Language Models", "authors": ["Shuang Zhou", "Wenya Xie", "Jiaxi Li", "Zaifu Zhan", "Meijia Song", "Han Yang", "Cheyenna Espinoza", "Lindsay Welton", "Xinnie Mai", "Yanwei Jin", "Zidu Xu", "Yuen-Hei Chung", "Yiyun Xing", "Meng-Han Tsai", "Emma Schaffer", "Yucheng Shi", "Ninghao Liu", "Zirui Liu", "Rui Zhang"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      22 pages,6 figures", "url": "http://arxiv.org/abs/2507.07988v1", "summary": "As large language models (LLMs) become increasingly integrated into clinical\ndecision-making, ensuring transparent and trustworthy reasoning is essential.\nHowever, existing evaluation strategies of LLMs' medical reasoning capability\neither suffer from unsatisfactory assessment or poor scalability, and a\nrigorous benchmark remains lacking. To address this, we introduce\nMedThink-Bench, a benchmark designed for rigorous, explainable, and scalable\nassessment of LLMs' medical reasoning. MedThink-Bench comprises 500 challenging\nquestions across ten medical domains, each annotated with expert-crafted\nstep-by-step rationales. Building on this, we propose LLM-w-Ref, a novel\nevaluation framework that leverages fine-grained rationales and LLM-as-a-Judge\nmechanisms to assess intermediate reasoning with expert-level fidelity while\nmaintaining scalability. Experiments show that LLM-w-Ref exhibits a strong\npositive correlation with expert judgments. Benchmarking twelve\nstate-of-the-art LLMs, we find that smaller models (e.g., MedGemma-27B) can\nsurpass larger proprietary counterparts (e.g., OpenAI-o3). Overall,\nMedThink-Bench offers a foundational tool for evaluating LLMs' medical\nreasoning, advancing their safe and responsible deployment in clinical\npractice.", "comment": "22 pages,6 figures", "pdf_url": "http://arxiv.org/pdf/2507.07988v1", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.06838", "title": "Shifting from Ranking to Set Selection for Retrieval Augmented Generation", "authors": ["Dahyun Lee", "Yongrae Jo", "Haeju Park", "Moontae Lee"], "categories": ["cs.CL", "cs.IR"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted to ACL 2025 main (Oral Presentation)", "url": "http://arxiv.org/abs/2507.06838v2", "summary": "Retrieval in Retrieval-Augmented Generation(RAG) must ensure that retrieved\npassages are not only individually relevant but also collectively form a\ncomprehensive set. Existing approaches primarily rerank top-k passages based on\ntheir individual relevance, often failing to meet the information needs of\ncomplex queries in multi-hop question answering. In this work, we propose a\nset-wise passage selection approach and introduce SETR, which explicitly\nidentifies the information requirements of a query through Chain-of-Thought\nreasoning and selects an optimal set of passages that collectively satisfy\nthose requirements. Experiments on multi-hop RAG benchmarks show that SETR\noutperforms both proprietary LLM-based rerankers and open-source baselines in\nterms of answer correctness and retrieval quality, providing an effective and\nefficient alternative to traditional rerankers in RAG systems. The code is\navailable at https://github.com/LGAI-Research/SetR", "comment": "Accepted to ACL 2025 main (Oral Presentation)", "pdf_url": "http://arxiv.org/pdf/2507.06838v2", "cate": "cs.CL", "date": "2025-07-09", "updated": "2025-07-10"}
{"id": "2507.07822", "title": "First-passage time for PDifMPs: an Exact simulation approach for time-varying thresholds", "authors": ["Sascha Desmettre", "Devika Khurana", "Amira Meddah"], "categories": ["math.PR", "cs.NA", "math.NA", "37M05, 65C20, 60G05, 60H35, 68Q87"], "primary_category": "Subjects:       Probability (math.PR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07822v1", "summary": "Piecewise Diffusion Markov Processes (PDifMPs) are valuable for modelling\nsystems where continuous dynamics are interrupted by sudden shifts and/or\nchanges in drift and diffusion. The first-passage time (FPT) in such models\nplays a central role in understanding when a process first reaches a critical\nboundary. In many systems, time-dependent thresholds provide a flexible\nframework for reflecting evolving conditions, making them essential for\nrealistic modelling. We propose a hybrid exact simulation scheme for computing\nthe FPT of PDifMPs to time-dependent thresholds. Exact methods traditionally\nexist for pure diffusions, using Brownian motion as an auxiliary process and\naccepting sampled paths with a probability weight. Between jumps, the PDifMP\nevolves as a diffusion, allowing us to apply the exact method within each\ninter-jump interval. The main challenge arises when no threshold crossing is\ndetected in an interval: We then need the value of the process at the jump\ntime, and for that, we introduce an approach to simulate a conditionally\nconstrained auxiliary process and derive the corresponding acceptance\nprobability. Furthermore, we prove the convergence of the method and illustrate\nit using numerical examples.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07822v1", "cate": "math.PR", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2504.21058", "title": "Computing change of level and isogenies between abelian varieties", "authors": ["Antoine Dequay", "David Lubicz"], "categories": ["cs.SC", "math.NT"], "primary_category": "Subjects:       Symbolic Computation (cs.SC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.21058v2", "summary": "Let $m,n,d > 1$ be integers such that $n=md$. In this paper, we present an\nefficient change of level algorithm that takes as input $(B, \\mathscr{M},\n\\Theta_\\mathscr{M})$ a marked abelian variety of level $m$ over the base field\n$k$ of odd characteristic and returns $(B, \\mathscr{M}^d,\n\\Theta_{\\mathscr{M}^d})$ a marked abelian variety of level $n$ at the expense\nof $O(m^g d^{2g})$ operations in $k$. A similar algorithm allows to compute\n$d$-isogenies: from $(B, \\mathscr{M}, \\Theta_\\mathscr{M})$ a marked abelian\nvariety of level $m$, $K\\subset B[d]$ isotropic for the Weil pairing isomorphic\nto $(\\mathbb{Z}/d\\mathbb{Z})^g$ defined over $k$, the isogeny algorithm returns\n$(A, \\mathscr{L}, \\Theta_\\mathscr{L})$ of level $m$ such that $A=B/K$ with\n$O(m^g d^g)$ operations in $k$. Our algorithms extend previous known results in\nthe case that $d \\wedge m=1$ and $d$ odd. In this paper, we lift theses\nrestrictions. We use the same general approach as in the literature in\nconjunction with the notion of symmetric compatible that we introduce, study\nand link to previous results of Mumford. For practical computation, most of the\ntime $m$ is $2$ or $4$ so that our algorithms allows in particular to compute\n$2^e$-isogenies which are important for the theory of theta functions but also\nfor computational applications such as isogeny based cryptography.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.21058v2", "cate": "cs.SC", "date": "2025-04-29", "updated": "2025-07-10"}
{"id": "2507.00951", "title": "Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive Foundations for Artificial General Intelligence and its Societal Impact", "authors": ["Rizwan Qureshi", "Ranjan Sapkota", "Abbas Shah", "Amgad Muneer", "Anas Zafar", "Ashmal Vayani", "Maged Shoman", "Abdelrahman B. M. Eldaly", "Kai Zhang", "Ferhat Sadak", "Shaina Raza", "Xinqi Fan", "Ravid Shwartz-Ziv", "Hong Yan", "Vinjia Jain", "Aman Chadha", "Manoj Karkee", "Jia Wu", "Philip Torr", "Seyedali Mirjalili"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.00951v2", "summary": "Can machines truly think, reason and act in domains like humans? This\nenduring question continues to shape the pursuit of Artificial General\nIntelligence (AGI). Despite the growing capabilities of models such as GPT-4.5,\nDeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal\nfluency and partial reasoning, these systems remain fundamentally limited by\ntheir reliance on token-level prediction and lack of grounded agency. This\npaper offers a cross-disciplinary synthesis of AGI development, spanning\nartificial intelligence, cognitive neuroscience, psychology, generative models,\nand agent-based systems. We analyze the architectural and cognitive foundations\nof general intelligence, highlighting the role of modular reasoning, persistent\nmemory, and multi-agent coordination. In particular, we emphasize the rise of\nAgentic RAG frameworks that combine retrieval, planning, and dynamic tool use\nto enable more adaptive behavior. We discuss generalization strategies,\nincluding information compression, test-time adaptation, and training-free\nmethods, as critical pathways toward flexible, domain-agnostic intelligence.\nVision-Language Models (VLMs) are reexamined not just as perception modules but\nas evolving interfaces for embodied understanding and collaborative task\ncompletion. We also argue that true intelligence arises not from scale alone\nbut from the integration of memory and reasoning: an orchestration of modular,\ninteractive, and self-improving components where compression enables adaptive\nbehavior. Drawing on advances in neurosymbolic systems, reinforcement learning,\nand cognitive scaffolding, we explore how recent architectures begin to bridge\nthe gap between statistical learning and goal-directed cognition. Finally, we\nidentify key scientific, technical, and ethical challenges on the path to AGI.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00951v2", "cate": "cs.AI", "date": "2025-07-01", "updated": "2025-07-09"}
{"id": "2507.07461", "title": "Hess-MC2: Sequential Monte Carlo Squared using Hessian Information and Second Order Proposals", "authors": ["Joshua Murphy", "Conor Rosato", "Andrew Millard", "Lee Devlin", "Paul Horridge", "Simon Maskell"], "categories": ["stat.ML", "cs.LG", "stat.CO"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "Comments:      Accepted to IEEE Machine Learning Signal Processing conference 2025", "url": "http://arxiv.org/abs/2507.07461v1", "summary": "When performing Bayesian inference using Sequential Monte Carlo (SMC)\nmethods, two considerations arise: the accuracy of the posterior approximation\nand computational efficiency. To address computational demands, Sequential\nMonte Carlo Squared (SMC$^2$) is well-suited for high-performance computing\n(HPC) environments. The design of the proposal distribution within SMC$^2$ can\nimprove accuracy and exploration of the posterior as poor proposals may lead to\nhigh variance in importance weights and particle degeneracy. The\nMetropolis-Adjusted Langevin Algorithm (MALA) uses gradient information so that\nparticles preferentially explore regions of higher probability. In this paper,\nwe extend this idea by incorporating second-order information, specifically the\nHessian of the log-target. While second-order proposals have been explored\npreviously in particle Markov Chain Monte Carlo (p-MCMC) methods, we are the\nfirst to introduce them within the SMC$^2$ framework. Second-order proposals\nnot only use the gradient (first-order derivative), but also the curvature\n(second-order derivative) of the target distribution. Experimental results on\nsynthetic models highlight the benefits of our approach in terms of step-size\nselection and posterior approximation accuracy when compared to other\nproposals.", "comment": "Accepted to IEEE Machine Learning Signal Processing conference 2025", "pdf_url": "http://arxiv.org/pdf/2507.07461v1", "cate": "stat.ML", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07985", "title": "CLIP Won't Learn Object-Attribute Binding from Natural Data and Here is Why", "authors": ["Bijay Gurung", "David T. Hoffmann", "Thomas Brox"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07985v1", "summary": "Contrastive vision-language models like CLIP are used for a large variety of\napplications, such as zero-shot classification or as vision encoder for\nmulti-modal models. Despite their popularity, their representations show major\nlimitations. For instance, CLIP models learn bag-of-words representations and,\nas a consequence, fail to distinguish whether an image is of \"a yellow\nsubmarine and a blue bus\" or \"a blue submarine and a yellow bus\". Previous\nattempts to fix this issue added hard negatives during training or modified the\narchitecture, but failed to resolve the problem in its entirety. We suspect\nthat the missing insights to solve the binding problem for CLIP are hidden in\nthe arguably most important part of learning algorithms: the data. In this\nwork, we fill this gap by rigorously identifying the influence of data\nproperties on CLIP's ability to learn binding using a synthetic dataset. We\nfind that common properties of natural data such as low attribute density,\nincomplete captions, and the saliency bias, a tendency of human captioners to\ndescribe the object that is \"most salient\" to them have a detrimental effect on\nbinding performance. In contrast to common belief, we find that neither scaling\nthe batch size, i.e., implicitly adding more hard negatives, nor explicitly\ncreating hard negatives enables CLIP to learn reliable binding. Only when the\ndata expresses our identified data properties CLIP learns almost perfect\nbinding.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07985v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2402.11005", "title": "A Theory of Response Sampling in LLMs: Part Descriptive and Part Prescriptive", "authors": ["Sarath Sivaprasad", "Pramod Kaushik", "Sahar Abdelnabi", "Mario Fritz"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      ACL 2025 (Oral)", "url": "http://arxiv.org/abs/2402.11005v4", "summary": "Large Language Models (LLMs) are increasingly utilized in autonomous\ndecision-making, where they sample options from vast action spaces. However,\nthe heuristics that guide this sampling process remain under explored. We study\nthis sampling behavior and show that this underlying heuristics resembles that\nof human decision-making: comprising a descriptive component (reflecting\nstatistical norm) and a prescriptive component (implicit ideal encoded in the\nLLM) of a concept. We show that this deviation of a sample from the statistical\nnorm towards a prescriptive component consistently appears in concepts across\ndiverse real-world domains like public health, and economic trends. To further\nillustrate the theory, we demonstrate that concept prototypes in LLMs are\naffected by prescriptive norms, similar to the concept of normality in humans.\nThrough case studies and comparison with human studies, we illustrate that in\nreal-world applications, the shift of samples toward an ideal value in LLMs'\noutputs can result in significantly biased decision-making, raising ethical\nconcerns.", "comment": "ACL 2025 (Oral)", "pdf_url": "http://arxiv.org/pdf/2402.11005v4", "cate": "cs.CL", "date": "2024-02-16", "updated": "2025-07-09"}
{"id": "2507.07917", "title": "Convergence rates for regularized unbalanced optimal transport: the discrete case", "authors": ["Luca Nenna", "Paul Pegon", "Louis Tocquec"], "categories": ["math.OC", "cs.NA", "math.NA", "Primary: 49Q22, Secondary: 49N15, 94A17"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      27 pages, 10 figures", "url": "http://arxiv.org/abs/2507.07917v1", "summary": "Unbalanced optimal transport (UOT) is a natural extension of optimal\ntransport (OT) allowing comparison between measures of different masses. It\narises naturally in machine learning by offering a robustness against outliers.\nThe aim of this work is to provide convergence rates of the regularized\ntransport cost and plans towards their original solution when both measures are\nweighted sums of Dirac masses.", "comment": "27 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.07917v1", "cate": "math.OC", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.02825", "title": "Establishing Best Practices for Building Rigorous Agentic Benchmarks", "authors": ["Yuxuan Zhu", "Tengjun Jin", "Yada Pruksachatkun", "Andy Zhang", "Shu Liu", "Sasha Cui", "Sayash Kapoor", "Shayne Longpre", "Kevin Meng", "Rebecca Weiss", "Fazl Barez", "Rahul Gupta", "Jwala Dhamala", "Jacob Merizian", "Mario Giulianelli", "Harry Coppock", "Cozmin Ududec", "Jasjeet Sekhon", "Jacob Steinhardt", "Antony Kellerman", "Sarah Schwettmann", "Matei Zaharia", "Ion Stoica", "Percy Liang", "Daniel Kang"], "categories": ["cs.AI", "A.1; I.2.m"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      39 pages, 15 tables, 6 figures", "url": "http://arxiv.org/abs/2507.02825v3", "summary": "Benchmarks are essential for quantitatively tracking progress in AI. As AI\nagents become increasingly capable, researchers and practitioners have\nintroduced agentic benchmarks to evaluate agents on complex, real-world tasks.\nThese benchmarks typically measure agent capabilities by evaluating task\noutcomes via specific reward designs. However, we show that many agentic\nbenchmarks have issues in task setup or reward design. For example, SWE-bench\nVerified uses insufficient test cases, while TAU-bench counts empty responses\nas successful. Such issues can lead to under- or overestimation of agents'\nperformance by up to 100% in relative terms. To make agentic evaluation\nrigorous, we introduce the Agentic Benchmark Checklist (ABC), a set of\nguidelines that we synthesized from our benchmark-building experience, a survey\nof best practices, and previously reported issues. When applied to CVE-Bench, a\nbenchmark with a particularly complex evaluation design, ABC reduces the\nperformance overestimation by 33%.", "comment": "39 pages, 15 tables, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.02825v3", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-10"}
{"id": "2507.07469", "title": "Galerkin-ARIMA: A Two-Stage Polynomial Regression Framework for Fast Rolling One-Step-Ahead Forecasting", "authors": ["Haojie Liu", "Zihan Lin"], "categories": ["stat.ML", "cs.LG", "econ.EM"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07469v1", "summary": "Time-series models like ARIMA remain widely used for forecasting but limited\nto linear assumptions and high computational cost in large and complex\ndatasets. We propose Galerkin-ARIMA that generalizes the AR component of ARIMA\nand replace it with a flexible spline-based function estimated by Galerkin\nprojection. This enables the model to capture nonlinear dependencies in lagged\nvalues and retain the MA component and Gaussian noise assumption. We derive a\nclosed-form OLS estimator for the Galerkin coefficients and show the model is\nasymptotically unbiased and consistent under standard conditions. Our method\nbridges classical time-series modeling and nonparametric regression, which\noffering improved forecasting performance and computational efficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07469v1", "cate": "stat.ML", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07994", "title": "Doodle Your Keypoints: Sketch-Based Few-Shot Keypoint Detection", "authors": ["Subhajit Maity", "Ayan Kumar Bhunia", "Subhadeep Koley", "Pinaki Nath Chowdhury", "Aneeshan Sain", "Yi-Zhe Song"], "categories": ["cs.CV", "I.4.0; I.4.9"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at ICCV 2025. Project Page: this https URL", "url": "http://arxiv.org/abs/2507.07994v1", "summary": "Keypoint detection, integral to modern machine perception, faces challenges\nin few-shot learning, particularly when source data from the same distribution\nas the query is unavailable. This gap is addressed by leveraging sketches, a\npopular form of human expression, providing a source-free alternative. However,\nchallenges arise in mastering cross-modal embeddings and handling user-specific\nsketch styles. Our proposed framework overcomes these hurdles with a\nprototypical setup, combined with a grid-based locator and prototypical domain\nadaptation. We also demonstrate success in few-shot convergence across novel\nkeypoints and classes through extensive experiments.", "comment": "Accepted at ICCV 2025. Project Page: https://subhajitmaity.me/DYKp", "pdf_url": "http://arxiv.org/pdf/2507.07994v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2402.13818", "title": "Beyond Hate Speech: NLP's Challenges and Opportunities in Uncovering Dehumanizing Language", "authors": ["Hamidreza Saffari", "Mohammadamin Shafiei", "Hezhao Zhang", "Lasana Harris", "Nafise Sadat Moosavi"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      15 pages, 12 figures, 12 tables", "url": "http://arxiv.org/abs/2402.13818v2", "summary": "Dehumanization, i.e., denying human qualities to individuals or groups, is a\nparticularly harmful form of hate speech that can normalize violence against\nmarginalized communities. Despite advances in NLP for detecting general hate\nspeech, approaches to identifying dehumanizing language remain limited due to\nscarce annotated data and the subtle nature of such expressions. In this work,\nwe systematically evaluate four state-of-the-art large language models (LLMs) -\nClaude, GPT, Mistral, and Qwen - for dehumanization detection. Our results show\nthat only one model-Claude-achieves strong performance (over 80% F1) under an\noptimized configuration, while others, despite their capabilities, perform only\nmoderately. Performance drops further when distinguishing dehumanization from\nrelated hate types such as derogation. We also identify systematic disparities\nacross target groups: models tend to over-predict dehumanization for some\nidentities (e.g., Gay men), while under-identifying it for others (e.g.,\nRefugees). These findings motivate the need for systematic, group-level\nevaluation when applying pretrained language models to dehumanization detection\ntasks.", "comment": "15 pages, 12 figures, 12 tables", "pdf_url": "http://arxiv.org/pdf/2402.13818v2", "cate": "cs.CL", "date": "2024-02-21", "updated": "2025-07-10"}
{"id": "2307.00675", "title": "New Feedback Control and Adaptive Evolve-Filter-Relax Regularization for the Navier-Stokes Equations in the Convection-Dominated Regime", "authors": ["Maria Strazzullo", "Francesco Ballarin", "Traian Iliescu", "Claudio Canuto"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2307.00675v2", "summary": "We propose, analyze, and investigate numerically a novel feedback control\nstrategy for high Reynolds number flows. For both the continuous and the\ndiscrete (finite element) settings, we prove that the new strategy yields\naccurate results for high Reynolds numbers that were not covered by current\nresults. We also show that the new feedback control yields more accurate\nresults than the current control approaches in marginally-resolved numerical\nsimulations of a two-dimensional flow past a circular cylinder at Reynolds\nnumbers $Re=1000$. We note, however, that for realistic control parameters, the\nstabilizing effect of the new feedback control strategy is not sufficient in\nthe convection-dominated regime. Our second contribution is the development of\nan adaptive evolve-filter-relax (aEFR) regularization that stabilizes\nmarginally-resolved simulations in the convection-dominated regime and\nincreases the accuracy of the new feedback control in realistic parameter\nsettings. For the finite element setting, we prove that the novel feedback\ncontrol equipped with the new aEFR method yields accurate results for high\nReynolds numbers. Furthermore, our numerical investigation shows that the new\nstrategy yields accurate results for reduced order models that dramatically\ndecrease the size of the feedback control problem.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2307.00675v2", "cate": "math.NA", "date": "2023-07-02", "updated": "2025-07-10"}
{"id": "2507.05110", "title": "Rule Learning for Knowledge Graph Reasoning under Agnostic Distribution Shift", "authors": ["Shixuan Liu", "Yue He", "Yunfei Wang", "Hao Zou", "Haoxiang Cheng", "Wenjing Yang", "Peng Cui", "Zhong Liu"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.05110v3", "summary": "Logical rule learning, a prominent category of knowledge graph (KG) reasoning\nmethods, constitutes a critical research area aimed at learning explicit rules\nfrom observed facts to infer missing knowledge. However, like all KG reasoning\nmethods, rule learning suffers from a critical weakness-its dependence on the\nI.I.D. assumption. This assumption can easily be violated due to selection bias\nduring training or agnostic distribution shifts during testing (e.g., as in\nquery shift scenarios), ultimately undermining model performance and\nreliability. To enable robust KG reasoning in wild environments, this study\ninvestigates logical rule learning in the presence of agnostic test-time\ndistribution shifts. We formally define this challenge as out-of-distribution\n(OOD) KG reasoning-a previously underexplored problem, and propose the Stable\nRule Learning (StableRule) framework as a solution. StableRule is an end-to-end\nframework that combines feature decorrelation with rule learning network, to\nenhance OOD generalization in KG reasoning. By leveraging feature\ndecorrelation, StableRule mitigates the adverse effects of covariate shifts\narising in OOD scenarios, improving the robustness of the rule learning\nnetwork. Extensive experiments on seven benchmark KGs demonstrate the\nframework's superior effectiveness and stability across diverse heterogeneous\nenvironments, highlighting its practical significance for real-world\napplications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.05110v3", "cate": "cs.AI", "date": "2025-07-07", "updated": "2025-07-10"}
{"id": "2507.07625", "title": "Concentration of measure for non-linear random matrices with applications to neural networks and non-commutative polynomials", "authors": ["Rados≈Çaw Adamczak"], "categories": ["math.PR", "cs.LG", "Primary: 60B20, 60E15, Secondary: 68T07"], "primary_category": "Subjects:       Probability (math.PR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07625v1", "summary": "We prove concentration inequalities for several models of non-linear random\nmatrices. As corollaries we obtain estimates for linear spectral statistics of\nthe conjugate kernel of neural networks and non-commutative polynomials in\n(possibly dependent) random matrices.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07625v1", "cate": "math.PR", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07997", "title": "MGVQ: Could VQ-VAE Beat VAE? A Generalizable Tokenizer with Multi-group Quantization", "authors": ["Mingkai Jia", "Wei Yin", "Xiaotao Hu", "Jiaxin Guo", "Xiaoyang Guo", "Qian Zhang", "Xiao-Xiao Long", "Ping Tan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07997v1", "summary": "Vector Quantized Variational Autoencoders (VQ-VAEs) are fundamental models\nthat compress continuous visual data into discrete tokens. Existing methods\nhave tried to improve the quantization strategy for better reconstruction\nquality, however, there still exists a large gap between VQ-VAEs and VAEs. To\nnarrow this gap, we propose \\NickName, a novel method to augment the\nrepresentation capability of discrete codebooks, facilitating easier\noptimization for codebooks and minimizing information loss, thereby enhancing\nreconstruction quality. Specifically, we propose to retain the latent dimension\nto preserve encoded features and incorporate a set of sub-codebooks for\nquantization. Furthermore, we construct comprehensive zero-shot benchmarks\nfeaturing resolutions of 512p and 2k to evaluate the reconstruction performance\nof existing methods rigorously. \\NickName~achieves the \\textbf{state-of-the-art\nperformance on both ImageNet and $8$ zero-shot benchmarks} across all VQ-VAEs.\nNotably, compared with SD-VAE, we outperform them on ImageNet significantly,\nwith rFID $\\textbf{0.49}$ v.s. $\\textbf{0.91}$, and achieve superior PSNR on\nall zero-shot benchmarks. These results highlight the superiority of\n\\NickName~in reconstruction and pave the way for preserving fidelity in HD\nimage processing tasks. Code will be publicly available at\nhttps://github.com/MKJia/MGVQ.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07997v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2403.01364", "title": "Improving Cross-lingual Representation for Semantic Retrieval with Code-switching", "authors": ["Mieradilijiang Maimaiti", "Yuanhang Zheng", "Ji Zhang", "Yue Zhang", "Wenpei Luo", "Kaiyu Huang"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2403.01364v2", "summary": "Semantic Retrieval (SR) has become an indispensable part of the FAQ system in\nthe task-oriented question-answering (QA) dialogue scenario. The demands for a\ncross-lingual smart-customer-service system for an e-commerce platform or some\nparticular business conditions have been increasing recently. Most previous\nstudies exploit cross-lingual pre-trained models (PTMs) for multi-lingual\nknowledge retrieval directly, while some others also leverage the continual\npre-training before fine-tuning PTMs on the downstream tasks. However, no\nmatter which schema is used, the previous work ignores to inform PTMs of some\nfeatures of the downstream task, i.e. train their PTMs without providing any\nsignals related to SR. To this end, in this work, we propose an Alternative\nCross-lingual PTM for SR via code-switching. We are the first to utilize the\ncode-switching approach for cross-lingual SR. Besides, we introduce the novel\ncode-switched continual pre-training instead of directly using the PTMs on the\nSR tasks. The experimental results show that our proposed approach consistently\noutperforms the previous SOTA methods on SR and semantic textual similarity\n(STS) tasks with three business corpora and four open datasets in 20+\nlanguages.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2403.01364v2", "cate": "cs.CL", "date": "2024-03-03", "updated": "2025-07-10"}
{"id": "2310.16668", "title": "A Simplified Fast Multipole Method Based on Strong Recursive Skeletonization", "authors": ["Anna Yesypenko", "Chao Chen", "Per-Gunnar Martinsson"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2310.16668v2", "summary": "This work introduces a kernel-independent, multilevel, adaptive algorithm for\nefficiently evaluating a discrete convolution kernel with a given source\ndistribution. The method is based on linear algebraic tools such as low rank\napproximation and ``skeleton representations'' to approximate far-field\ninteractions. While this work is related to previous linear algebraic\nformulations of the fast multipole method, the proposed algorithm is\ndistinguished by relying on simpler data structures.\n  The proposed algorithm eliminates the need for explicit interaction lists by\nrestructuring computations to operate exclusively on the near-neighbor list at\neach level of the tree, thereby simplifying both implementation and data\nstructures. This work also introduces novel translation operators that\nsignificantly simplify the handling of adaptive point distributions. As a\nkernel-independent approach, it only requires evaluation of the kernel\nfunction, making it easily adaptable to a variety of kernels. By using\noperations on the neighbor list (of size at most 27 in 3D) rather than the\ninteraction list (of size up to 189 in 3D), the algorithm is particularly\nwell-suited for parallel implementation on modern hardware.\n  Numerical experiments on uniform and non-uniform point distributions in 2D\nand 3D demonstrate the effectiveness of the proposed parallel algorithm for\nLaplace and (low-frequency) Helmholtz kernels. The algorithm constructs a\ntailored skeleton representation for the given geometry during a precomputation\nstage. After precomputation, the fast summation achieves high efficiency on the\nGPU using batched linear algebra operations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2310.16668v2", "cate": "math.NA", "date": "2023-10-25", "updated": "2025-07-10"}
{"id": "2507.07319", "title": "Probability-Raising Causality for Uncertain Parametric Markov Decision Processes with PAC Guarantees", "authors": ["Ryohei Oura", "yuji Ito"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      Accepted by the 41st Conference on Uncertainty in Artificial Intelligence", "url": "http://arxiv.org/abs/2507.07319v1", "summary": "Recent decision-making systems are increasingly complicated, making it\ncrucial to verify and understand their behavior for a given specification. A\npromising approach is to comprehensively explain undesired behavior in the\nsystems modeled by Markov decision processes (MDPs) through formal verification\nand causal reasoning. However, the reliable explanation using model-based\nprobabilistic causal analysis has not been explored when the MDP's transition\nprobabilities are uncertain. This paper proposes a method to identify potential\ncauses of undesired behaviors in an uncertain parametric MDP (upMDP) using\nparameter sampling, model checking, and a set covering for the samples. A cause\nis defined as a subset of states based on a probability-raising principle. We\nshow that the probability of each identified subset being a cause exceeds a\nspecified threshold. Further, a lower bound of the probability that the\nundesired paths visit the subsets is maximized as much as possible while\nsatisfying a nonredundancy condition. While computing these probabilities is\ncomplicated, this study derives probabilistically approximately correct lower\nbounds of both probabilities by the sampling. We demonstrate the effectiveness\nof the proposed method through a path-planning scenario.", "comment": "Accepted by the 41st Conference on Uncertainty in Artificial\n  Intelligence", "pdf_url": "http://arxiv.org/pdf/2507.07319v1", "cate": "eess.SY", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.05297", "title": "Fuzzy Classification Aggregation for a Continuum of Agents", "authors": ["Zijun Meng"], "categories": ["cs.AI", "econ.TH"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.05297v3", "summary": "We prove that any optimal, independent, and zero unanimous fuzzy\nclassification aggregation function of a continuum of individual\nclassifications of $m\\ge 3$ objects into $2\\le p\\le m$ types must be a weighted\narithmetic mean.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.05297v3", "cate": "cs.AI", "date": "2025-07-06", "updated": "2025-07-10"}
{"id": "2507.07641", "title": "Machine Learning-Assisted Surrogate Modeling with Multi-Objective Optimization and Decision-Making of a Steam Methane Reforming Reactor", "authors": ["Seyed Reza Nabavi", "Zonglin Guo", "Zhiyuan Wang"], "categories": ["physics.chem-ph", "cs.LG"], "primary_category": "Subjects:       Chemical Physics (physics.chem-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07641v1", "summary": "This study presents an integrated modeling and optimization framework for a\nsteam methane reforming (SMR) reactor, combining a mathematical model,\nartificial neural network (ANN)-based hybrid modeling, advanced multi-objective\noptimization (MOO) and multi-criteria decision-making (MCDM) techniques. A\none-dimensional fixed-bed reactor model accounting for internal mass transfer\nresistance was employed to simulate reactor performance. To reduce the high\ncomputational cost of the mathematical model, a hybrid ANN surrogate was\nconstructed, achieving a 93.8% reduction in average simulation time while\nmaintaining high predictive accuracy. The hybrid model was then embedded into\nthree MOO scenarios using the non-dominated sorting genetic algorithm II\n(NSGA-II) solver: 1) maximizing methane conversion and hydrogen output; 2)\nmaximizing hydrogen output while minimizing carbon dioxide emissions; and 3) a\ncombined three-objective case. The optimal trade-off solutions were further\nranked and selected using two MCDM methods: technique for order of preference\nby similarity to ideal solution (TOPSIS) and simplified preference ranking on\nthe basis of ideal-average distance (sPROBID). Optimal results include a\nmethane conversion of 0.863 with 4.556 mol/s hydrogen output in the first case,\nand 0.988 methane conversion with 3.335 mol/s hydrogen and 0.781 mol/s carbon\ndioxide in the third. This comprehensive methodology offers a scalable and\neffective strategy for optimizing complex catalytic reactor systems with\nmultiple, often conflicting, objectives.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07641v1", "cate": "physics.chem-ph", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.08000", "title": "Impact of Pretraining Word Co-occurrence on Compositional Generalization in Multimodal Models", "authors": ["Helen Qu", "Sang Michael Xie"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08000v1", "summary": "CLIP and large multimodal models (LMMs) have better accuracy on examples\ninvolving concepts that are highly represented in the training data. However,\nthe role of concept combinations in the training data on compositional\ngeneralization is largely unclear -- for instance, how does accuracy vary when\na common object appears in an uncommon pairing with another object? In this\npaper, we investigate how word co-occurrence statistics in the pretraining\ndataset (a proxy for co-occurrence of visual concepts) impacts CLIP/LMM\nperformance. To disentangle the effects of word co-occurrence frequencies from\nsingle-word frequencies, we measure co-occurrence with pointwise mutual\ninformation (PMI), which normalizes the joint probability of two words\nco-occurring by the probability of co-occurring independently. Using\nsynthetically generated images with a variety of concept pairs, we show a\nstrong correlation between PMI in the CLIP pretraining data and zero-shot\naccuracy in CLIP models trained on LAION-400M (r=0.97 and 14% accuracy gap\nbetween images in the top and bottom 5% of PMI values), demonstrating that even\naccuracy on common concepts is affected by the combination of concepts in the\nimage. Leveraging this finding, we reproduce this effect in natural images by\nediting them to contain pairs with varying PMI, resulting in a correlation of\nr=0.75. Finally, we demonstrate that this behavior in CLIP transfers to LMMs\nbuilt on top of CLIP (r=0.70 for TextVQA, r=0.62 for VQAv2). Our findings\nhighlight the need for algorithms and architectures that improve compositional\ngeneralization in multimodal models without scaling the training data\ncombinatorially. Our code is available at\nhttps://github.com/helenqu/multimodal-pretraining-pmi.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08000v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2404.00699", "title": "A Comprehensive Survey of Contamination Detection Methods in Large Language Models", "authors": ["Mathieu Ravaut", "Bosheng Ding", "Fangkai Jiao", "Hailin Chen", "Xingxuan Li", "Ruochen Zhao", "Chengwei Qin", "Caiming Xiong", "Shafiq Joty"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted by TMLR in July 2025. 18 pages, 1 figure, 3 tables", "url": "http://arxiv.org/abs/2404.00699v5", "summary": "With the rise of Large Language Models (LLMs) in recent years, abundant new\nopportunities are emerging, but also new challenges, among which contamination\nis quickly becoming critical. Business applications and fundraising in\nArtificial Intelligence (AI) have reached a scale at which a few percentage\npoints gained on popular question-answering benchmarks could translate into\ndozens of millions of dollars, placing high pressure on model integrity. At the\nsame time, it is becoming harder and harder to keep track of the data that LLMs\nhave seen; if not impossible with closed-source models like GPT-4 and Claude-3\nnot divulging any information on the training set. As a result, contamination\nbecomes a major issue: LLMs' performance may not be reliable anymore, as the\nhigh performance may be at least partly due to their previous exposure to the\ndata. This limitation jeopardizes real capability improvement in the field of\nNLP, yet, there remains a lack of methods on how to efficiently detect\ncontamination. In this paper, we survey all recent work on contamination\ndetection with LLMs, analyzing their methodologies and use cases to shed light\non the appropriate usage of contamination detection methods. Our work calls the\nNLP research community's attention into systematically taking into account\ncontamination bias in LLM evaluation.", "comment": "Accepted by TMLR in July 2025. 18 pages, 1 figure, 3 tables", "pdf_url": "http://arxiv.org/pdf/2404.00699v5", "cate": "cs.CL", "date": "2024-03-31", "updated": "2025-07-09"}
{"id": "2312.16928", "title": "Error Estimates for Systems of Nonlocal Balance Laws Modeling Dense Multilane Vehicular Traffic", "authors": ["Aekta Aggarwal", "Helge Holden", "Ganesh Vaidya"], "categories": ["math.NA", "cs.NA", "math.AP", "35L65, 65M25, 35D30, 65M12, 65M15"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2312.16928v5", "summary": "We discuss a class of coupled systems of nonlocal nonlinear balance laws\nmodeling multilane traffic, with the nonlocality present in both convective and\nsource terms. The uniqueness and existence of the entropy solution are proven\nvia doubling of the variables arguments and convergent finite volume\napproximations, respectively. The primary goal is to establish that the finite\nvolume numerical approximations of the system converge to the unique entropy\nsolution at a rate of $\\sqrt{\\Delta t}$, even when using relatively less\nregular one-sided kernels, compared to the globally smooth kernels analyzed in\n[Num. Math., 156(1):237-271, 2024] and [IMA J. Numer. Anal., 44(6):3354-3392,\n2024]. The applicability of the proven theory to a general class of systems of\nnonlocal balance laws coupled strongly through the convective part and weakly\nthrough the source part, is indicated. As the support of the kernel tends to\nzero, the convergence of the entropy solutions of the proposed model to its\nlocal counterparts [SIAM J. Math. Anal., 51: 3694--3713, 2019] is also\ndiscussed. Numerical simulations illustrating the behavior of the entropy\nsolutions of the coupled nonlocal systems are also shown.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2312.16928v5", "cate": "math.NA", "date": "2023-12-28", "updated": "2025-07-10"}
{"id": "2507.07429", "title": "Distributed and adaptive model predictive control for vehicle platoon systems under non-ideal communication", "authors": ["Qiaoni Han", "Chengfei Xu", "Zhiqiang Zuo"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07429v1", "summary": "The uncertainty of wireless communication poses significant challenges to\nplatoon control performance. Aiming at alleviating the influence of non-ideal\ncommunication on the platoon system, this paper proposes a distributed and\nadaptive model predictive control (MPC) method. First of all, to deal with the\ntransmission uncertainty caused by non-ideal communication, compensated data\npackets are customized for each vehicle. Then, an adaptive model predictive\ncontrol method is proposed to balance the system response speed and tracking\naccuracy. Furthermore, to reduce the computational requirements of the vehicle\nplatoon system, a predictive time-domain update strategy suitable for non-ideal\ncommunication was introduced. Finally, the sufficient conditions for ensuring\nthe feasibility of the MPC algorithm and the stability of the closed-loop\nplatoon control system are theoretically analyzed. The simulation results show\nthat the proposed method significantly reduces the computing resource\nrequirements for solving the optimization problem while ensuring satisfactory\nsystem performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07429v1", "cate": "eess.SY", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.05791", "title": "GTA1: GUI Test-time Scaling Agent", "authors": ["Yan Yang", "Dongxu Li", "Yutong Dai", "Yuhao Yang", "Ziyang Luo", "Zirui Zhao", "Zhiyuan Hu", "Junzhe Huang", "Amrita Saha", "Zeyuan Chen", "Ran Xu", "Liyuan Pan", "Caiming Xiong", "Junnan Li"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.05791v3", "summary": "Graphical user interface (GUI) agents autonomously operate across platforms\n(e.g., Linux) to complete tasks by interacting with visual elements.\nSpecifically, a user instruction is decomposed into a sequence of action\nproposals, each corresponding to an interaction with the GUI. After each\naction, the agent observes the updated GUI environment to plan the next step.\nHowever, two main challenges arise: i) resolving ambiguity in task planning\n(i.e., the action proposal sequence), where selecting an appropriate plan is\nnon-trivial, as many valid ones may exist; ii) accurately grounding actions in\ncomplex and high-resolution interfaces, i.e., precisely interacting with visual\ntargets.\n  This paper investigates the two aforementioned challenges with our GUI\nTest-time Scaling Agent, namely GTA1. First, to select the most appropriate\naction proposal, we introduce a test-time scaling method. At each step, we\nsample multiple candidate action proposals and leverage a judge model to\nevaluate and select the most suitable one. It trades off computation for better\ndecision quality by concurrent sampling, shortening task execution steps, and\nimproving overall performance. Second, we propose a model that achieves\nimproved accuracy when grounding the selected action proposal to its\ncorresponding visual elements. Our key insight is that reinforcement learning\n(RL) facilitates visual grounding through inherent objective alignments,\nrewarding successful clicks on interface elements.\n  Experimentally, our method establishes state-of-the-art performance across\ndiverse benchmarks. For example, GTA1-7B achieves 50.1%, 92.4%, and 67.7%\naccuracies on Screenspot-Pro, Screenspot-V2, and OSWorld-G, respectively. When\npaired with a planner applying our test-time scaling strategy, it exhibits\nstate-of-the-art agentic performance (e.g., 45.2% task success rate on\nOSWorld). We open-source our code and models here.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.05791v3", "cate": "cs.AI", "date": "2025-07-08", "updated": "2025-07-10"}
{"id": "2507.07771", "title": "A Unified Empirical Risk Minimization Framework for Flexible N-Tuples Weak Supervision", "authors": ["Shuying Huang", "Junpeng Li", "Changchun Hua", "Yana Yang"], "categories": ["stat.ML", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07771v1", "summary": "To alleviate the annotation burden in supervised learning, N-tuples learning\nhas recently emerged as a powerful weakly-supervised method. While existing\nN-tuples learning approaches extend pairwise learning to higher-order\ncomparisons and accommodate various real-world scenarios, they often rely on\ntask-specific designs and lack a unified theoretical foundation. In this paper,\nwe propose a general N-tuples learning framework based on empirical risk\nminimization, which systematically integrates pointwise unlabeled data to\nenhance learning performance. This paper first unifies the data generation\nprocesses of N-tuples and pointwise unlabeled data under a shared probabilistic\nformulation. Based on this unified view, we derive an unbiased empirical risk\nestimator that generalizes a broad class of existing N-tuples models. We\nfurther establish a generalization error bound for theoretical support. To\ndemonstrate the flexibility of the framework, we instantiate it in four\nrepresentative weakly supervised scenarios, each recoverable as a special case\nof our general model. Additionally, to address overfitting issues arising from\nnegative risk terms, we adopt correction functions to adjust the empirical\nrisk. Extensive experiments on benchmark datasets validate the effectiveness of\nthe proposed framework and demonstrate that leveraging pointwise unlabeled data\nconsistently improves generalization across various N-tuples learning tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07771v1", "cate": "stat.ML", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07800", "title": "Adaptive Attention Residual U-Net for curvilinear structure segmentation in fluorescence microscopy and biomedical images", "authors": ["Achraf Ait Laydi", "Louis Cueff", "Mewen Crespo", "Yousef El Mourabit", "H√©l√®ne Bouvrais"], "categories": ["q-bio.QM", "cs.CV"], "primary_category": "Subjects:       Quantitative Methods (q-bio.QM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07800v1", "summary": "Segmenting curvilinear structures in fluorescence microscopy remains a\nchallenging task, particularly under noisy conditions and in dense filament\nnetworks commonly seen in vivo. To address this, we created two original\ndatasets consisting of hundreds of synthetic images of fluorescently labelled\nmicrotubules within cells. These datasets are precisely annotated and closely\nmimic real microscopy images, including realistic noise. The second dataset\npresents an additional challenge, by simulating varying fluorescence\nintensities along filaments that complicate segmentation. While deep learning\nhas shown strong potential in biomedical image analysis, its performance often\ndeclines in noisy or low-contrast conditions. To overcome this limitation, we\ndeveloped a novel advanced architecture: the Adaptive Squeeze-and-Excitation\nResidual U-Net (ASE_Res_UNet). This model enhanced the standard U-Net by\nintegrating residual blocks in the encoder and adaptive SE attention mechanisms\nin the decoder. Through ablation studies and comprehensive visual and\nquantitative evaluations, ASE_Res_UNet consistently outperformed its variants,\nnamely standard U-Net, ASE_UNet and Res_UNet architectures. These improvements,\nparticularly in noise resilience and detecting fine, low-intensity structures,\nwere largely attributed to the adaptive SE attention module that we created. We\nfurther benchmarked ASE_Res_UNet against various state-of-the-art models, and\nfound it achieved superior performance on our most challenging dataset.\nFinally, the model also generalized well to real microscopy images of stained\nmicrotubules as well as to other curvilinear structures. Indeed, it\nsuccessfully segmented retinal blood vessels and nerves in noisy or\nlow-contrast biomedical images, demonstrating its strong potential for\napplications in disease diagnosis and treatment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07800v1", "cate": "q-bio.QM", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2404.18865", "title": "Truth-value judgment in language models: 'truth directions' are context sensitive", "authors": ["Stefan F. Schouten", "Peter Bloem", "Ilia Markov", "Piek Vossen"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      COLM 2025", "url": "http://arxiv.org/abs/2404.18865v2", "summary": "Recent work has demonstrated that the latent spaces of large language models\n(LLMs) contain directions predictive of the truth of sentences. Multiple\nmethods recover such directions and build probes that are described as\nuncovering a model's \"knowledge\" or \"beliefs\". We investigate this phenomenon,\nlooking closely at the impact of context on the probes. Our experiments\nestablish where in the LLM the probe's predictions are (most) sensitive to the\npresence of related sentences, and how to best characterize this kind of\nsensitivity. We do so by measuring different types of consistency errors that\noccur after probing an LLM whose inputs consist of hypotheses preceded by\n(negated) supporting and contradicting sentences. We also perform a causal\nintervention experiment, investigating whether moving the representation of a\npremise along these truth-value directions influences the position of an\nentailed or contradicted sentence along that same direction. We find that the\nprobes we test are generally context sensitive, but that contexts which should\nnot affect the truth often still impact the probe outputs. Our experiments show\nthat the type of errors depend on the layer, the model, and the kind of data.\nFinally, our results suggest that truth-value directions are causal mediators\nin the inference process that incorporates in-context information.", "comment": "COLM 2025", "pdf_url": "http://arxiv.org/pdf/2404.18865v2", "cate": "cs.CL", "date": "2024-04-29", "updated": "2025-07-10"}
{"id": "2410.19969", "title": "A quantum graph FFT with applications to partial differential equations on networks", "authors": ["Robert Carlson"], "categories": ["math.NA", "cs.NA", "65M70, 65T50, 34B45"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      The new version includes a pseudospectral algorithm. Examples are limited to the Schrodinger equation to highlight the advantages of spectral and pseudospectral methods", "url": "http://arxiv.org/abs/2410.19969v2", "summary": "The Fast Fourier Transform is extended to functions on finite graphs whose\nedges are identified with intervals of finite length. Spectral and\npseudospectral methods are developed to solve a wide variety of time dependent\npartial differential equations on domains which are modeled as networks of one\ndimensional segments joined at nodes.", "comment": "The new version includes a pseudospectral algorithm. Examples are\n  limited to the Schrodinger equation to highlight the advantages of spectral\n  and pseudospectral methods", "pdf_url": "http://arxiv.org/pdf/2410.19969v2", "cate": "math.NA", "date": "2024-10-25", "updated": "2025-07-09"}
{"id": "2507.07588", "title": "Perspective Chapter: Insights from Kalman Filtering with Correlated Noises Recursive Least-Square Algorithm for State and Parameter Estimation", "authors": ["Abd El Mageed Hag Elamin Khalid"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      Book Chapter", "url": "http://arxiv.org/abs/2507.07588v1", "summary": "This article explores the estimation of parameters and states for linear\nstochastic systems with deterministic control inputs. It introduces a novel\nKalman filtering approach called Kalman Filtering with Correlated Noises\nRecursive Generalized Extended Least Squares (KF-CN-RGELS) algorithm, which\nleverages the cross-correlation between process noise and measurement noise in\nKalman filtering cycles to jointly estimate both parameters and system states.\nThe study also investigates the theoretical implications of the correlation\ncoefficient on estimation accuracy through performance analysis involving\nvarious correlation coefficients between process and measurement noises. The\nresearch establishes a clear relationship: the accuracy of identified\nparameters and states is directly proportional to positive correlation\ncoefficients. To validate the efficacy of this algorithm, a comprehensive\ncomparison is conducted among different algorithms, including the standard\nKalman filter algorithm and the augmented-state Kalman filter with correlated\nnoises algorithm. Theoretical findings are not only presented but also\nexemplified through a numerical case study to provide valuable insights into\npractical implications. This work contributes to enhancing estimation accuracy\nin linear stochastic systems with deterministic control inputs, offering\nvaluable insights for control system design and state-space modeling.", "comment": "Book Chapter", "pdf_url": "http://arxiv.org/pdf/2507.07588v1", "cate": "eess.SY", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2203.07861", "title": "Don't Get Me Wrong: How to Apply Deep Visual Interpretations to Time Series", "authors": ["Christoffer Loeffler", "Wei-Cheng Lai", "Bjoern Eskofier", "Dario Zanca", "Lukas Schmidt", "Christopher Mutschler"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      48 pages, 12 figues, 7 tables, 6 algorithms", "url": "http://arxiv.org/abs/2203.07861v3", "summary": "The correct interpretation of convolutional models is a hard problem for time\nseries data. While saliency methods promise visual validation of predictions\nfor image and language processing, they fall short when applied to time series.\nThese tend to be less intuitive and represent highly diverse data, such as the\ntool-use time series dataset. Furthermore, saliency methods often generate\nvaried, conflicting explanations, complicating the reliability of these\nmethods. Consequently, a rigorous objective assessment is necessary to\nestablish trust in them. This paper investigates saliency methods on time\nseries data to formulate recommendations for interpreting convolutional models\nand implements them on the tool-use time series problem. To achieve this, we\nfirst employ nine gradient-, propagation-, or perturbation-based post-hoc\nsaliency methods across six varied and complex real-world datasets. Next, we\nevaluate these methods using five independent metrics to generate\nrecommendations. Subsequently, we implement a case study focusing on tool-use\ntime series using convolutional classification models. Our results validate our\nrecommendations that indicate that none of the saliency methods consistently\noutperforms others on all metrics, while some are sometimes ahead. Our insights\nand step-by-step guidelines allow experts to choose suitable saliency methods\nfor a given model and dataset.", "comment": "48 pages, 12 figues, 7 tables, 6 algorithms", "pdf_url": "http://arxiv.org/pdf/2203.07861v3", "cate": "cs.CV", "date": "2022-03-14", "updated": "2025-07-10"}
{"id": "2507.07779", "title": "Approximation Depth of Convex Polytopes", "authors": ["Egor Bakaev", "Florestan Brunck", "Amir Yehudayoff"], "categories": ["math.MG", "cs.CG", "cs.LG", "math.CO"], "primary_category": "Subjects:       Metric Geometry (math.MG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07779v1", "summary": "We study approximations of polytopes in the standard model for computing\npolytopes using Minkowski sums and (convex hulls of) unions. Specifically, we\nstudy the ability to approximate a target polytope by polytopes of a given\ndepth. Our main results imply that simplices can only be ``trivially\napproximated''. On the way, we obtain a characterization of simplices as the\nonly ``outer additive'' convex bodies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07779v1", "cate": "math.MG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "1905.09226", "title": "Boundary Learning by Using Weighted Propagation in Convolution Network", "authors": ["Wei Liu", "Jiahao Chen", "Chuni Liu", "Xiaojuan Ban", "Boyuan Ma", "Hao Wang", "Weihua Xue", "Yu Guo"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      technical report", "url": "http://arxiv.org/abs/1905.09226v3", "summary": "In material science, image segmentation is of great significance for\nquantitative analysis of microstructures. Here, we propose a novel Weighted\nPropagation Convolution Neural Network based on U-Net (WPU-Net) to detect\nboundary in poly-crystalline microscopic images. We introduce spatial\nconsistency into network to eliminate the defects in raw microscopic image. And\nwe customize adaptive boundary weight for each pixel in each grain, so that it\nleads the network to preserve grain's geometric and topological\ncharacteristics. Moreover, we provide our dataset with the goal of advancing\nthe development of image processing in materials science. Experiments\ndemonstrate that the proposed method achieves promising performance in both of\nobjective and subjective assessment. In boundary detection task, it reduces the\nerror rate by 7\\%, which outperforms state-of-the-art methods by a large\nmargin.", "comment": "technical report", "pdf_url": "http://arxiv.org/pdf/1905.09226v3", "cate": "cs.CV", "date": "2019-05-22", "updated": "2025-07-10"}
{"id": "2406.02524", "title": "CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks", "authors": ["Maciej Besta", "Lorenzo Paleari", "Marcin Copik", "Robert Gerstenberger", "Ales Kubicek", "Piotr Nyczyk", "Patrick Iff", "Eric Schreiber", "Tanja Srindran", "Tomasz Lehmann", "Hubert Niewiadomski", "Torsten Hoefler"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2406.02524v5", "summary": "Large Language Models (LLMs) are transforming a wide range of domains, yet\nverifying their outputs remains a significant challenge, especially for complex\nopen-ended tasks such as consolidation, summarization, and knowledge\nextraction. To address this, we introduce CheckEmbed (CE): a simple, scalable,\nand accurate verification method. CE reduces each LLM answer to a single\nembedding vector using powerful modern embedding LLM models like\nSFR-Embedding-Mistral. Prior methods such as BERTScore and SelfCheckGPT relied\non weaker encoders like BERT, forcing them to operate at token or sentence\ngranularity. In contrast, CE performs fast, semantically rich comparisons\ndirectly at the whole-answer level, overcoming key limitations in both accuracy\nand scalability. We conduct a comprehensive design and time complexity analysis\nacross 13 verification baselines, including classical text scorers (e.g.,\nBLEU), stability-based methods (e.g., SelfCheckGPT), and generative evaluators\n(e.g., LLM-as-a-Judge), which highlights the effectiveness, efficiency,\nversatility, and simplicity of CE. Empirical results show that CE reliably\ndetects hallucinations in both closed and open-ended tasks. We further present\nevidence that CE generalizes beyond text to other modalities such as vision,\nestablishing it as a practical and versatile verification framework.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2406.02524v5", "cate": "cs.CL", "date": "2024-06-04", "updated": "2025-07-10"}
{"id": "2411.13240", "title": "An efficient Asymptotic-Preserving scheme for the Boltzmann mixture with disparate mass", "authors": ["Zhen Hao", "Ning Jiang", "Liu Liu"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.13240v3", "summary": "In this paper, we develop and implement an efficient asymptotic-preserving\n(AP) scheme to solve the gas mixture of Boltzmann equations under the disparate\nmass scaling relevant to the so-called \"epochal relaxation\" phenomenon. The\ndisparity in molecular masses, ranging across several orders of magnitude,\nleads to significant challenges in both the evaluation of collision operators\nand the designing of time-stepping schemes to capture the multi-scale nature of\nthe dynamics. A direct implementation of the spectral method faces prohibitive\ncomputational costs as the mass ratio increases due to the need to resolve\nvastly different thermal velocities. Unlike [I. M. Gamba, S. Jin, and L. Liu,\nCommun. Math. Sci., 17 (2019), pp. 1257-1289], we propose an alternative\napproach based on proper truncation of asymptotic expansions of the collision\noperators, which significantly reduces the computational complexity and works\nwell for small $\\varepsilon$. By incorporating the separation of three time\nscales in the model's relaxation process [P. Degond and B. Lucquin-Desreux,\nMath. Models Methods Appl. Sci., 6 (1996), pp. 405-436], we design an AP scheme\nthat captures the specific dynamics of the disparate mass model while\nmaintaining computational efficiency. Numerical experiments demonstrate the\neffectiveness of the proposed scheme in handling large mass ratios of heavy and\nlight species, as well as capturing the epochal relaxation phenomenon.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.13240v3", "cate": "math.NA", "date": "2024-11-20", "updated": "2025-07-10"}
{"id": "2507.07645", "title": "PhysioEdge: Multimodal Compressive Sensing Platform for Wearable Health Monitoring", "authors": ["Rens Baeyens", "Dennis Laurijssen", "Jan Steckel", "Walter Daems"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      to be published in the proceedings of the 28th Euromicro Conference on Digital System Design (DSD)", "url": "http://arxiv.org/abs/2507.07645v1", "summary": "The integration of compressive sensing with real-time embedded systems opens\nnew possibilities for efficient, low-power biomedical signal acquisition. This\npaper presents a custom hardware platform based on the RP2350 micro-controller,\ntailored for synchronized multi-modal biomedical monitoring. The system is\ncapable of capturing cardiopulmonary sounds, along with biopotential signals\nsuch as phonocardiography (PCG), electrocardiography (ECG) and electromyography\n(EMG), photoplethysmography (PPG), and inertial measurement unit (IMU) data for\nposture recognition. To ensure sample-accurate synchronization, a Sub-1GHz\nradio system is used across multiple nodes. Wi-Fi and Bluetooth connectivity\nenable centralized data aggregation. Experimental results demonstrate the\nachieved decrease in power consumption when using compressive sensing,\nefficient multi-node synchronization, and scalability for wireless biomedical\nmonitoring applications. The compact form factor and low-cost design make it\nsuitable for various medical applications, including remote healthcare and\nlong-term monitoring.", "comment": "to be published in the proceedings of the 28th Euromicro Conference\n  on Digital System Design (DSD)", "pdf_url": "http://arxiv.org/pdf/2507.07645v1", "cate": "eess.SY", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2401.13796", "title": "Don't Push the Button! Exploring Data Leakage Risks in Machine Learning and Transfer Learning", "authors": ["Andrea Apicella", "Francesco Isgr√≤", "Roberto Prevete"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted to be published on Artificial Intelligence Review journal", "url": "http://arxiv.org/abs/2401.13796v4", "summary": "Machine Learning (ML) has revolutionized various domains, offering predictive\ncapabilities in several areas. However, with the increasing accessibility of ML\ntools, many practitioners, lacking deep ML expertise, adopt a \"push the button\"\napproach, utilizing user-friendly interfaces without a thorough understanding\nof underlying algorithms. While this approach provides convenience, it raises\nconcerns about the reliability of outcomes, leading to challenges such as\nincorrect performance evaluation. This paper addresses a critical issue in ML,\nknown as data leakage, where unintended information contaminates the training\ndata, impacting model performance evaluation. Users, due to a lack of\nunderstanding, may inadvertently overlook crucial steps, leading to optimistic\nperformance estimates that may not hold in real-world scenarios. The\ndiscrepancy between evaluated and actual performance on new data is a\nsignificant concern. In particular, this paper categorizes data leakage in ML,\ndiscussing how certain conditions can propagate through the ML workflow.\nFurthermore, it explores the connection between data leakage and the specific\ntask being addressed, investigates its occurrence in Transfer Learning, and\ncompares standard inductive ML with transductive ML frameworks. The conclusion\nsummarizes key findings, emphasizing the importance of addressing data leakage\nfor robust and reliable ML applications.", "comment": "Accepted to be published on Artificial Intelligence Review journal", "pdf_url": "http://arxiv.org/pdf/2401.13796v4", "cate": "cs.LG", "date": "2024-01-24", "updated": "2025-07-10"}
{"id": "2507.07907", "title": "A statistical physics framework for optimal learning", "authors": ["Francesca Mignacco", "Francesco Mori"], "categories": ["cond-mat.dis-nn", "cond-mat.stat-mech", "cs.LG", "q-bio.NC"], "primary_category": "Subjects:       Disordered Systems and Neural Networks (cond-mat.dis-nn)", "pdf_link": null, "comments": "Comments:      35 pages, 13 figures", "url": "http://arxiv.org/abs/2507.07907v1", "summary": "Learning is a complex dynamical process shaped by a range of interconnected\ndecisions. Careful design of hyperparameter schedules for artificial neural\nnetworks or efficient allocation of cognitive resources by biological learners\ncan dramatically affect performance. Yet, theoretical understanding of optimal\nlearning strategies remains sparse, especially due to the intricate interplay\nbetween evolving meta-parameters and nonlinear learning dynamics. The search\nfor optimal protocols is further hindered by the high dimensionality of the\nlearning space, often resulting in predominantly heuristic, difficult to\ninterpret, and computationally demanding solutions. Here, we combine\nstatistical physics with control theory in a unified theoretical framework to\nidentify optimal protocols in prototypical neural network models. In the\nhigh-dimensional limit, we derive closed-form ordinary differential equations\nthat track online stochastic gradient descent through low-dimensional order\nparameters. We formulate the design of learning protocols as an optimal control\nproblem directly on the dynamics of the order parameters with the goal of\nminimizing the generalization error at the end of training. This framework\nencompasses a variety of learning scenarios, optimization constraints, and\ncontrol budgets. We apply it to representative cases, including optimal\ncurricula, adaptive dropout regularization and noise schedules in denoising\nautoencoders. We find nontrivial yet interpretable strategies highlighting how\noptimal protocols mediate crucial learning tradeoffs, such as maximizing\nalignment with informative input directions while minimizing noise fitting.\nFinally, we show how to apply our framework to real datasets. Our results\nestablish a principled foundation for understanding and designing optimal\nlearning protocols and suggest a path toward a theory of meta-learning grounded\nin statistical physics.", "comment": "35 pages, 13 figures", "pdf_url": "http://arxiv.org/pdf/2507.07907v1", "cate": "cond-mat.dis-nn", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2303.01803", "title": "Uncertainty-Aware Gradient Stabilization for Small Object Detection", "authors": ["Huixin Sun", "Yanjing Li", "Linlin Yang", "Xianbin Cao", "Baochang Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2303.01803v2", "summary": "Despite advances in generic object detection, there remains a performance gap\nin detecting small objects compared to normal-scale objects. We reveal that\nconventional object localization methods suffer from gradient instability in\nsmall objects due to sharper loss curvature, leading to a convergence\nchallenge. To address the issue, we propose Uncertainty-Aware Gradient\nStabilization (UGS), a framework that reformulates object localization as a\nclassification task to stabilize gradients. UGS quantizes continuous labels\ninto interval non-uniform discrete representations. Under a\nclassification-based objective, the localization branch generates bounded and\nconfidence-driven gradients, mitigating instability. Furthermore, UGS\nintegrates an uncertainty minimization (UM) loss that reduces prediction\nvariance and an uncertainty-guided refinement (UR) module that identifies and\nrefines high-uncertainty regions via perturbations. Evaluated on four\nbenchmarks, UGS consistently improves anchor-based, anchor-free, and leading\nsmall object detectors. Especially, UGS enhances DINO-5scale by 2.6 AP on\nVisDrone, surpassing previous state-of-the-art results.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2303.01803v2", "cate": "cs.CV", "date": "2023-03-03", "updated": "2025-07-10"}
{"id": "2406.15245", "title": "Unsupervised Morphological Tree Tokenizer", "authors": ["Qingyang Zhu", "Xiang Hu", "Pengyu Ji", "Wei Wu", "Kewei Tu"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      ACL 2025 Findings", "url": "http://arxiv.org/abs/2406.15245v2", "summary": "As a cornerstone in language modeling, tokenization involves segmenting text\ninputs into pre-defined atomic units. Conventional statistical tokenizers often\ndisrupt constituent boundaries within words, thereby corrupting semantic\ninformation. To address this drawback, we introduce morphological structure\nguidance to tokenization and propose a deep model to induce character-level\nstructures of words. Specifically, the deep model jointly encodes internal\nstructures and representations of words with a mechanism named\n$\\textit{MorphOverriding}$ to ensure the indecomposability of morphemes. By\ntraining the model with self-supervised objectives, our method is capable of\ninducing character-level structures that align with morphological rules without\nannotated training data. Based on the induced structures, our algorithm\ntokenizes words through vocabulary matching in a top-down manner. Empirical\nresults indicate that the proposed method effectively retains complete\nmorphemes and outperforms widely adopted methods such as BPE and WordPiece on\nboth morphological segmentation tasks and language modeling tasks. Code is\navailable at https://github.com/martianmartina/TreeTokenizer.", "comment": "ACL 2025 Findings", "pdf_url": "http://arxiv.org/pdf/2406.15245v2", "cate": "cs.CL", "date": "2024-06-21", "updated": "2025-07-10"}
{"id": "2501.07914", "title": "Using curved meshes to derive a priori error estimates for a linear elasticity problem with Robin boundary conditions", "authors": ["Joyce Ghantous"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.07914v2", "summary": "This work concerns the numerical analysis of the linear elasticity problem\nwith a Robin boundary condition on a smooth domain. A finite element\ndiscretization is presented using high-order curved meshes in order to\naccurately discretize the physical domain. The primary objective is to conduct\na detailed error analysis for the elasticity problem using the vector lift\noperator, which maps vector-valued functions from the mesh domain to the\nphysical domain. Error estimates are established, both in terms of the finite\nelement approximation error and the geometric error, respectively associated to\nthe finite element degree and to the mesh order. These theoretical a priori\nerror estimates are validated by numerical experiments in 2D and 3D.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.07914v2", "cate": "math.NA", "date": "2025-01-14", "updated": "2025-07-10"}
{"id": "2507.07659", "title": "Remote Renewable Energy Hubs: a Taxonomy", "authors": ["Victor Dachet", "Antoine Dubois", "Bardhyl Miftari", "Rapha√´l Fonteneau", "Damien Ernst"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07659v1", "summary": "Serving the energy demand with renewable energy is hindered by its limited\navailability near load centres (i.e. places where the energy demand is high).\nTo address this challenge, the concept of Remote Renewable Energy Hubs (RREH)\nemerges as a promising solution. RREHs are energy hubs located in areas with\nabundant renewable energy sources, such as sun in the Sahara Desert or wind in\nGreenland. In these hubs, renewable energy sources are used to synthetise\nenergy molecules. To produce specific energy molecules, a tailored hub\nconfiguration must be designed, which means choosing a set of technologies that\nare interacting with each other as well as defining how they are integrated in\ntheir local environment. The plurality of technologies that may be employed in\nRREHs results in a large diversity of hubs. In order to characterize this\ndiversity, we propose in this paper a taxonomy for accurately defining these\nhubs. This taxonomy allows to better describe and compare designs of hubs as\nwell as to identify new ones. Thus, it may guide policymakers and engineers in\nhub design, contributing to cost efficiency and/or improving local integration.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07659v1", "cate": "eess.SY", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2402.13284", "title": "Structure Guided Large Language Model for SQL Generation", "authors": ["Qinggang Zhang", "Hao Chen", "Junnan Dong", "Shengyuan Chen", "Feiran Huang", "Xiao Huang"], "categories": ["cs.DB", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Databases (cs.DB)", "pdf_link": null, "comments": "Comments:      The 42nd International Conference on Machine Learning", "url": "http://arxiv.org/abs/2402.13284v4", "summary": "Recent advancements in large language models (LLMs) have shown promise in\nbridging the gap between natural language queries and database management\nsystems, enabling users to interact with databases without the background of\nSQL. However, LLMs often struggle to comprehend complex database structures and\naccurately interpret user intentions. Decomposition-based methods have been\nproposed to enhance the performance of LLMs on complex tasks, but decomposing\nSQL generation into subtasks is non-trivial due to the declarative structure of\nSQL syntax and the intricate connections between query concepts and database\nelements. In this paper, we propose a novel Structure GUided text-to-SQL\nframework~(SGU-SQL) that incorporates syntax-based prompting to enhance the SQL\ngeneration capabilities of LLMs. Specifically, SGU-SQL establishes\nstructure-aware links between user queries and database schema and decomposes\nthe complex generation task using syntax-based prompting to enable more\naccurate LLM-based SQL generation. Extensive experiments on two benchmark\ndatasets demonstrate that SGU-SQL consistently outperforms state-of-the-art\ntext-to-SQL models.", "comment": "The 42nd International Conference on Machine Learning", "pdf_url": "http://arxiv.org/pdf/2402.13284v4", "cate": "cs.DB", "date": "2024-02-19", "updated": "2025-07-10"}
{"id": "2304.13431", "title": "Implicit Counterfactual Data Augmentation for Robust Learning", "authors": ["Xiaoling Zhou", "Ou Wu", "Michael K. Ng"], "categories": ["cs.LG", "I.2.0; I.2.6"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      33 pages, 10 figures", "url": "http://arxiv.org/abs/2304.13431v4", "summary": "Machine learning models are prone to capturing the spurious correlations\nbetween non-causal attributes and classes, with counterfactual data\naugmentation being a promising direction for breaking these spurious\nassociations. However, generating counterfactual data explicitly poses a\nchallenge, and incorporating augmented data into the training process decreases\ntraining efficiency. This study proposes an Implicit Counterfactual Data\nAugmentation (ICDA) method to remove spurious correlations and make stable\npredictions. Specifically, first, a novel sample-wise augmentation strategy is\ndeveloped that generates semantically and counterfactually meaningful deep\nfeatures with distinct augmentation strength for each sample. Second, we derive\nan easy-to-compute surrogate loss on the augmented feature set when the number\nof augmented samples becomes infinite. Third, two concrete schemes are\nproposed, including direct quantification and meta-learning, to derive the key\nparameters for the robust loss. In addition, ICDA is explained from a\nregularization perspective, revealing its capacity to improve intra-class\ncompactness and augment margins at both class and sample levels. Extensive\nexperiments have been conducted across various biased learning scenarios\ncovering both image and text datasets, demonstrating that ICDA consistently\nenhances the generalization and robustness performance of popular networks.", "comment": "33 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2304.13431v4", "cate": "cs.LG", "date": "2023-04-26", "updated": "2025-07-10"}
{"id": "2407.04519", "title": "Judging from Support-set: A New Way to Utilize Few-Shot Segmentation for Segmentation Refinement Process", "authors": ["Seonghyeon Moon", "Qingze", "Liu", "Haein Kong", "Muhammad Haris Khan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICIP 2025", "url": "http://arxiv.org/abs/2407.04519v3", "summary": "Segmentation refinement aims to enhance the initial coarse masks generated by\nsegmentation algorithms. The refined masks are expected to capture more details\nand better contours of the target objects. Research on segmentation refinement\nhas developed as a response to the need for high-quality image segmentations.\nHowever, to our knowledge, no method has been developed that can determine the\nsuccess of segmentation refinement. Such a method could ensure the reliability\nof segmentation in applications where the outcome of the segmentation is\nimportant and fosters innovation in image processing technologies. To address\nthis research gap, we propose Judging From Support-set (JFS), a method to judge\nthe success of segmentation refinement leveraging an off-the-shelf few-shot\nsegmentation (FSS) model. The traditional goal of the problem in FSS is to find\na target object in a query image utilizing target information given by a\nsupport set. However, we propose a novel application of the FSS model in our\nevaluation pipeline for segmentation refinement methods. Given a coarse mask as\ninput, segmentation refinement methods produce a refined mask; these two masks\nbecome new support masks for the FSS model. The existing support mask then\nserves as the test set for the FSS model to evaluate the quality of the refined\nsegmentation by the segmentation refinement methods. We demonstrate the\neffectiveness of our proposed JFS framework by evaluating the SAM Enhanced\nPseudo-Labels (SEPL) using SegGPT as the choice of FSS model on the PASCAL\ndataset. The results showed that JFS has the potential to determine whether the\nsegmentation refinement process is successful.", "comment": "ICIP 2025", "pdf_url": "http://arxiv.org/pdf/2407.04519v3", "cate": "cs.CV", "date": "2024-07-05", "updated": "2025-07-09"}
{"id": "2408.13940", "title": "Derailer-Rerailer: Adaptive Verification for Efficient and Reliable Language Model Reasoning", "authors": ["Guangya Wan", "Yuqi Wu", "Hao Wang", "Shengming Zhao", "Jie Chen", "Sheng Li"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2408.13940v4", "summary": "Large Language Models (LLMs) have shown impressive reasoning capabilities,\nyet existing prompting methods face a critical trade-off: simple approaches\noften struggle with complex tasks and reasoning stability, while more\nsophisticated methods require multiple inferences and substantial computational\nresources, limiting their practical deployment. To address this challenge, we\npropose Derailer-Rerailer, a novel framework that adaptively balances reasoning\naccuracy and computational efficiency. At its core, our framework employs a\nlightweight Derailer mechanism to assess reasoning stability and selectively\ntriggers an advanced Rerailer verification process only when necessary, thereby\noptimizing computational resource usage. Extensive evaluation across both open\nand closed-source models on more than 20 categories of mathematical, symbolic,\nand commonsense reasoning tasks demonstrates our framework's effectiveness:\nDerailer-Rerailer achieves significant accuracy improvements (8-11\\% across\nvarious reasoning tasks) while maintaining 2-3 times better efficiency than\nexisting verification methods, with particularly strong performance in\nmathematical and symbolic reasoning, offering a practical solution for\nenhancing LLM reasoning reliability while significantly reducing computational\noverhead.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2408.13940v4", "cate": "cs.CL", "date": "2024-08-25", "updated": "2025-07-09"}
{"id": "2501.12965", "title": "A spline-based hexahedral mesh generator for patient-specific coronary arteries", "authors": ["Fabio Marcinn√≥", "Jochen Hinz", "Annalisa Buffa", "Simone Deparis"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.12965v2", "summary": "This paper presents a spline-based hexahedral mesh generator for tubular\ngeometries commonly encountered in haemodynamics studies, in particular\ncoronary arteries. We focus on techniques for accurately meshing vessels with\nstenoses and aneurysms, as well as non-planar bifurcations. Our approach\nincorporates several innovations, including a spline-based description of the\nvessel geometry in both the radial and the longitudinal directions, the use of\nHermite curves for modeling non-planar bifurcations, and a generalization to\nnon-planar n intersecting branches. This method eliminates the need for a\nconcrete vessel surface, grid smoothing, and other post-processing. A technique\nto generate grids with boundary layers is also presented. We validate the\ngenerated meshes using commonly employed quality indices, compare them against\nstate-of-the-art mesh generators and apply our method to complex coronary\ntrees. Finally, we present finite element fluid flow simulations with\nphysiological boundary conditions. To validate the proposed framework, a\nwall-shear-stress-based convergence test and computations of haemodynamic\nindices are also presented.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.12965v2", "cate": "math.NA", "date": "2025-01-22", "updated": "2025-07-10"}
{"id": "2507.07681", "title": "Ammonia, Methane, Hydrogen and Methanol Produced in Remote Renewable Energy Hubs: a Comparative Quantitative Analysis", "authors": ["Antoine Larbanois", "Victor Dachet", "Antoine Dubois", "Rapha√´l Fonteneau", "Damien Ernst"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      Proceedings of ECOS 2024 - The 37th International Conference on Efficiency, Cost, Optimization, Simulation and Environmental Impact of Energy Systems", "url": "http://arxiv.org/abs/2507.07681v1", "summary": "Remote renewable energy hubs (RREHs) for synthetic fuel production are\nengineering systems harvesting renewable energy where it is particularly\nabundant. They produce transportable synthetic fuels for export to distant load\ncenters. This article aims to evaluate the production costs of different energy\ncarriers, and includes a discussion on advantages and disadvantages in terms of\ntechnical performance. To do so, we extend the study of Berger et al., (2021)\nwhich focuses on methane (CH4) as energy carrier and introduce three new\ncarriers: ammonia (NH3), hydrogen (H2) and methanol (CH3OH). The four different\nRREHs are located in the Algerian Sahara desert and must serve to the load\ncenter, Belgium, a constant electro-fuel demand of 10 TWh per year. The\nmodelling and optimisation of these systems are performed using the modelling\nlanguage GBOML (Graph-Based Optimisation Modelling Language). Our findings\nreveal that the three new RREHs, each with its respective carrier (ammonia,\nhydrogen, and methanol), are all more cost-effective than the methane-based\nsystem. Ammonia demonstrates the most favourable cost-to-energy exported ratio.", "comment": "Proceedings of ECOS 2024 - The 37th International Conference on\n  Efficiency, Cost, Optimization, Simulation and Environmental Impact of Energy\n  Systems", "pdf_url": "http://arxiv.org/pdf/2507.07681v1", "cate": "eess.SY", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2404.10393", "title": "Offline Trajectory Optimization for Offline Reinforcement Learning", "authors": ["Ziqi Zhao", "Zhaochun Ren", "Liu Yang", "Yunsen Liang", "Fajie Yuan", "Pengjie Ren", "Zhumin Chen", "jun Ma", "Xin Xin"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted at SIGKDD 2025", "url": "http://arxiv.org/abs/2404.10393v2", "summary": "Offline reinforcement learning (RL) aims to learn policies without online\nexplorations. To enlarge the training data, model-based offline RL learns a\ndynamics model which is utilized as a virtual environment to generate\nsimulation data and enhance policy learning. However, existing data\naugmentation methods for offline RL suffer from (i) trivial improvement from\nshort-horizon simulation; and (ii) the lack of evaluation and correction for\ngenerated data, leading to low-qualified augmentation.\n  In this paper, we propose offline trajectory optimization for offline\nreinforcement learning (OTTO). The key motivation is to conduct long-horizon\nsimulation and then utilize model uncertainty to evaluate and correct the\naugmented data. Specifically, we propose an ensemble of Transformers, a.k.a.\nWorld Transformers, to predict environment state dynamics and the reward\nfunction. Three strategies are proposed to use World Transformers to generate\nlong-horizon trajectory simulation by perturbing the actions in the offline\ndata. Then, an uncertainty-based World Evaluator is introduced to firstly\nevaluate the confidence of the generated trajectories and then perform the\ncorrection for low-confidence data. Finally, we jointly use the original data\nwith the corrected augmentation data to train an offline RL algorithm. OTTO\nserves as a plug-in module and can be integrated with existing model-free\noffline RL methods. Experiments on various benchmarks show that OTTO can\neffectively improve the performance of representative offline RL algorithms,\nincluding in complex environments with sparse rewards like AntMaze. Codes are\navailable at https://github.com/ZiqiZhao1/OTTO.", "comment": "Accepted at SIGKDD 2025", "pdf_url": "http://arxiv.org/pdf/2404.10393v2", "cate": "cs.LG", "date": "2024-04-16", "updated": "2025-07-10"}
{"id": "2311.02401", "title": "BarcodeBERT: Transformers for Biodiversity Analysis", "authors": ["Pablo Millan Arias", "Niousha Sadjadi", "Monireh Safari", "ZeMing Gong", "Austin T. Wang", "Joakim Bruslund Haurum", "Iuliia Zarubiieva", "Dirk Steinke", "Lila Kari", "Angel X. Chang", "Scott C. Lowe", "Graham W. Taylor"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Main text: 14 pages, Total: 23 pages, 10 figures, formerly accepted at the 4th Workshop on Self-Supervised Learning: Theory and Practice (NeurIPS 2023)", "url": "http://arxiv.org/abs/2311.02401v3", "summary": "In the global challenge of understanding and characterizing biodiversity,\nshort species-specific genomic sequences known as DNA barcodes play a critical\nrole, enabling fine-grained comparisons among organisms within the same kingdom\nof life. Although machine learning algorithms specifically designed for the\nanalysis of DNA barcodes are becoming more popular, most existing methodologies\nrely on generic supervised training algorithms. We introduce BarcodeBERT, a\nfamily of models tailored to biodiversity analysis and trained exclusively on\ndata from a reference library of 1.5M invertebrate DNA barcodes. We compared\nthe performance of BarcodeBERT on taxonomic identification tasks against a\nspectrum of machine learning approaches including supervised training of\nclassical neural architectures and fine-tuning of general DNA foundation\nmodels. Our self-supervised pretraining strategies on domain-specific data\noutperform fine-tuned foundation models, especially in identification tasks\ninvolving lower taxa such as genera and species. We also compared BarcodeBERT\nwith BLAST, one of the most widely used bioinformatics tools for sequence\nsearching, and found that our method matched BLAST's performance in\nspecies-level classification while being 55 times faster. Our analysis of\nmasking and tokenization strategies also provides practical guidance for\nbuilding customized DNA language models, emphasizing the importance of aligning\nmodel training strategies with dataset characteristics and domain knowledge.\nThe code repository is available at https://github.com/bioscan-ml/BarcodeBERT.", "comment": "Main text: 14 pages, Total: 23 pages, 10 figures, formerly accepted\n  at the 4th Workshop on Self-Supervised Learning: Theory and Practice (NeurIPS\n  2023)", "pdf_url": "http://arxiv.org/pdf/2311.02401v3", "cate": "cs.LG", "date": "2023-11-04", "updated": "2025-07-10"}
{"id": "2408.02900", "title": "MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine", "authors": ["Yunfei Xie", "Ce Zhou", "Lang Gao", "Juncheng Wu", "Xianhang Li", "Hong-Yu Zhou", "Sheng Liu", "Lei Xing", "James Zou", "Cihang Xie", "Yuyin Zhou"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      The dataset is publicly available at this https URL . Accepted to ICLR 2025", "url": "http://arxiv.org/abs/2408.02900v3", "summary": "This paper introduces MedTrinity-25M, a comprehensive, large-scale multimodal\ndataset for medicine, covering over 25 million images across 10 modalities with\nmultigranular annotations for more than 65 diseases. These multigranular\nannotations encompass both global information, such as modality and organ\ndetection, and local information like ROI analysis, lesion texture, and\nregion-wise correlations. Unlike the existing multimodal datasets, which are\nlimited by the availability of image-text pairs, we have developed the first\nautomated pipeline that scales up multimodal data by generating multigranular\nvisual and textual annotations in the form of image-ROI-description triplets\nwithout the need for any paired text descriptions. Specifically, data from over\n30 different sources have been collected, preprocessed, and grounded using\ndomain-specific expert models to identify ROIs related to abnormal regions. We\nthen build a comprehensive knowledge base and prompt multimodal large language\nmodels to perform retrieval-augmented generation with the identified ROIs as\nguidance, resulting in multigranular textual descriptions. Compared to existing\ndatasets, MedTrinity-25M provides the most enriched annotations, supporting a\ncomprehensive range of multimodal tasks such as captioning and report\ngeneration, as well as vision-centric tasks like classification and\nsegmentation. We propose LLaVA-Tri by pretraining LLaVA on MedTrinity-25M,\nachieving state-of-the-art performance on VQA-RAD, SLAKE, and PathVQA,\nsurpassing representative SOTA multimodal large language models. Furthermore,\nMedTrinity-25M can also be utilized to support large-scale pre-training of\nmultimodal medical AI models, contributing to the development of future\nfoundation models in the medical domain. We will make our dataset available.", "comment": "The dataset is publicly available at\n  https://yunfeixie233.github.io/MedTrinity-25M/. Accepted to ICLR 2025", "pdf_url": "http://arxiv.org/pdf/2408.02900v3", "cate": "cs.CV", "date": "2024-08-06", "updated": "2025-07-10"}
{"id": "2409.10955", "title": "Investigating Context-Faithfulness in Large Language Models: The Roles of Memory Strength and Evidence Style", "authors": ["Yuepei Li", "Kang Zhou", "Qiao Qiao", "Bach Nguyen", "Qing Wang", "Qi Li"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      This work is published at ACL 2025", "url": "http://arxiv.org/abs/2409.10955v2", "summary": "Retrieval-augmented generation (RAG) improves Large Language Models (LLMs) by\nincorporating external information into the response generation process.\nHowever, how context-faithful LLMs are and what factors influence LLMs' context\nfaithfulness remain largely unexplored. In this study, we investigate the\nimpact of memory strength and evidence presentation on LLMs' receptiveness to\nexternal evidence. We quantify the memory strength of LLMs by measuring the\ndivergence in LLMs' responses to different paraphrases of the same question,\nwhich is not considered by previous works. We also generate evidence in various\nstyles to examine LLMs' behavior. Our results show that for questions with high\nmemory strength, LLMs are more likely to rely on internal memory. Furthermore,\npresenting paraphrased evidence significantly increases LLMs' receptiveness\ncompared to simple repetition or adding details. These findings provide key\ninsights for improving retrieval-augmented generation and context-aware LLMs.\nOur code is available at https://github.com/liyp0095/ContextFaithful.", "comment": "This work is published at ACL 2025", "pdf_url": "http://arxiv.org/pdf/2409.10955v2", "cate": "cs.CL", "date": "2024-09-17", "updated": "2025-07-10"}
{"id": "2504.16797", "title": "The extended adjoint state and nonlinearity in correlation-based passive imaging", "authors": ["Tram Thi Ngoc Nguyen"], "categories": ["math.NA", "cs.NA", "65M32, 65J22, 35R30"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.16797v2", "summary": "This articles investigates physics-based passive imaging problem, wherein one\ninfers an unknown medium using ambient noise and correlation of the noise\nsignal. We develop a general backpropagation framework via the so-called\nextended adjoint state, suitable for any linear PDE; crucially, this approach\nreduces by half the number of required PDE solves. Applications to several\ndifferent PDE models demonstrate the universality of our method. In addition,\nwe analyze the nonlinearity of the correlated model, revealing a surprising\ntangential cone condition-like structure, thereby advancing the state of the\nart towards a convergence guarantee for regularized reconstruction in passive\nimaging.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.16797v2", "cate": "math.NA", "date": "2025-04-23", "updated": "2025-07-10"}
{"id": "2507.07805", "title": "Set-Based Control Barrier Functions and Safety Filters", "authors": ["Kim P. Wabersich", "Felix Berkel", "Felix Gruber", "Sven Reimann"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07805v1", "summary": "High performance and formal safety guarantees are common requirements for\nindustrial control applications. Control barrier function (CBF) methods provide\na systematic approach to the modularization of safety and performance. However,\nthe design of such CBFs can be challenging, which limits their applicability to\nlarge-scale or data-driven systems. This paper introduces the concept of a\nset-based CBF for linear systems with convex constraints. By leveraging control\ninvariant sets from reachability analysis and predictive control, the set-based\nCBF is defined implicitly through the minimal scaling of such a set to contain\nthe current system state. This approach enables the development of implicit,\ndata-driven, and high-dimensional CBF representations. The paper demonstrates\nthe design of a safety filter using set-based CBFs, which is suitable for\nreal-time implementations and learning-based approximations to reduce online\ncomputational demands. The effectiveness of the method is illustrated through\ncomprehensive simulations on a high-dimensional mass-spring-damper system and a\nmotion control task, and it is validated experimentally using an electric drive\napplication with short sampling times, highlighting its practical benefits for\nsafety-critical control.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07805v1", "cate": "eess.SY", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2405.17556", "title": "Solving Probabilistic Verification Problems of Neural Networks using Branch and Bound", "authors": ["David Boetius", "Stefan Leue", "Tobias Sutter"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted at ICML 2025. Code available at this https URL . 9 pages, 3 figures, 31 pages references and appendix, including 8 more figures", "url": "http://arxiv.org/abs/2405.17556v3", "summary": "Probabilistic verification problems of neural networks are concerned with\nformally analysing the output distribution of a neural network under a\nprobability distribution of the inputs. Examples of probabilistic verification\nproblems include verifying the demographic parity fairness notion or\nquantifying the safety of a neural network. We present a new algorithm for\nsolving probabilistic verification problems of neural networks based on an\nalgorithm for computing and iteratively refining lower and upper bounds on\nprobabilities over the outputs of a neural network. By applying\nstate-of-the-art bound propagation and branch and bound techniques from\nnon-probabilistic neural network verification, our algorithm significantly\noutpaces existing probabilistic verification algorithms, reducing solving times\nfor various benchmarks from the literature from tens of minutes to tens of\nseconds. Furthermore, our algorithm compares favourably even to dedicated\nalgorithms for restricted probabilistic verification problems. We complement\nour empirical evaluation with a theoretical analysis, proving that our\nalgorithm is sound and, under mildly restrictive conditions, also complete when\nusing a suitable set of heuristics.", "comment": "Accepted at ICML 2025. Code available at\n  https://github.com/sen-uni-kn/probspecs. 9 pages, 3 figures, 31 pages\n  references and appendix, including 8 more figures", "pdf_url": "http://arxiv.org/pdf/2405.17556v3", "cate": "cs.LG", "date": "2024-05-27", "updated": "2025-07-10"}
{"id": "2402.04129", "title": "OVOR: OnePrompt with Virtual Outlier Regularization for Rehearsal-Free Class-Incremental Learning", "authors": ["Wei-Cheng Huang", "Chun-Fu Chen", "Hsiang Hsu"], "categories": ["cs.LG", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted by ICLR 2024", "url": "http://arxiv.org/abs/2402.04129v2", "summary": "Recent works have shown that by using large pre-trained models along with\nlearnable prompts, rehearsal-free methods for class-incremental learning (CIL)\nsettings can achieve superior performance to prominent rehearsal-based ones.\nRehearsal-free CIL methods struggle with distinguishing classes from different\ntasks, as those are not trained together. In this work we propose a\nregularization method based on virtual outliers to tighten decision boundaries\nof the classifier, such that confusion of classes among different tasks is\nmitigated. Recent prompt-based methods often require a pool of task-specific\nprompts, in order to prevent overwriting knowledge of previous tasks with that\nof the new task, leading to extra computation in querying and composing an\nappropriate prompt from the pool. This additional cost can be eliminated,\nwithout sacrificing accuracy, as we reveal in the paper. We illustrate that a\nsimplified prompt-based method can achieve results comparable to previous\nstate-of-the-art (SOTA) methods equipped with a prompt pool, using much less\nlearnable parameters and lower inference cost. Our regularization method has\ndemonstrated its compatibility with different prompt-based methods, boosting\nthose previous SOTA rehearsal-free CIL methods' accuracy on the ImageNet-R and\nCIFAR-100 benchmarks. Our source code is available at\nhttps://github.com/jpmorganchase/ovor.", "comment": "Accepted by ICLR 2024", "pdf_url": "http://arxiv.org/pdf/2402.04129v2", "cate": "cs.LG", "date": "2024-02-06", "updated": "2025-07-09"}
{"id": "2408.06687", "title": "Masked Image Modeling: A Survey", "authors": ["Vlad Hondru", "Florinel Alin Croitoru", "Shervin Minaee", "Radu Tudor Ionescu", "Nicu Sebe"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at the International Journal of Computer Vision", "url": "http://arxiv.org/abs/2408.06687v3", "summary": "In this work, we survey recent studies on masked image modeling (MIM), an\napproach that emerged as a powerful self-supervised learning technique in\ncomputer vision. The MIM task involves masking some information, e.g. pixels,\npatches, or even latent representations, and training a model, usually an\nautoencoder, to predicting the missing information by using the context\navailable in the visible part of the input. We identify and formalize two\ncategories of approaches on how to implement MIM as a pretext task, one based\non reconstruction and one based on contrastive learning. Then, we construct a\ntaxonomy and review the most prominent papers in recent years. We complement\nthe manually constructed taxonomy with a dendrogram obtained by applying a\nhierarchical clustering algorithm. We further identify relevant clusters via\nmanually inspecting the resulting dendrogram. Our review also includes datasets\nthat are commonly used in MIM research. We aggregate the performance results of\nvarious masked image modeling methods on the most popular datasets, to\nfacilitate the comparison of competing methods. Finally, we identify research\ngaps and propose several interesting directions of future work. We supplement\nour survey with the following public repository containing organized\nreferences: https://github.com/vladhondru25/MIM-Survey.", "comment": "Accepted at the International Journal of Computer Vision", "pdf_url": "http://arxiv.org/pdf/2408.06687v3", "cate": "cs.CV", "date": "2024-08-13", "updated": "2025-07-10"}
{"id": "2409.11724", "title": "TART: An Open-Source Tool-Augmented Framework for Explainable Table-based Reasoning", "authors": ["Xinyuan Lu", "Liangming Pan", "Yubo Ma", "Preslav Nakov", "Min-Yen Kan"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      NAACL 2025 (Findings)", "url": "http://arxiv.org/abs/2409.11724v3", "summary": "Current Large Language Models (LLMs) exhibit limited ability to understand\ntable structures and to apply precise numerical reasoning, which is crucial for\ntasks such as table question answering (TQA) and table-based fact verification\n(TFV). To address these challenges, we introduce our Tool-Augmented Reasoning\nframework for Tables (TART), which integrates LLMs with specialized tools. TART\ncontains three key components: a table formatter to ensure accurate data\nrepresentation, a tool maker to develop specific computational tools, and an\nexplanation generator to maintain explainability. We also present the TOOLTAB\ndataset, a new benchmark designed specifically for training LLMs in table-tool\nintegration. Our experiments indicate that TART achieves substantial\nimprovements over existing methods (e.g., Chain-of-Thought) by improving both\nthe precision of data processing and the clarity of the reasoning process.\nNotably, TART paired with CodeLlama achieves 90.0% of the accuracy of the\nclosed-sourced LLM GPT-3.5-turbo, highlighting its robustness in diverse\nreal-world scenarios. All the code and data are available at\nhttps://github.com/XinyuanLu00/TART.", "comment": "NAACL 2025 (Findings)", "pdf_url": "http://arxiv.org/pdf/2409.11724v3", "cate": "cs.CL", "date": "2024-09-18", "updated": "2025-07-10"}
{"id": "2507.01789", "title": "Inverse source problems for the stochastic wave equations", "authors": ["Yunqing Huang", "Shihan Zhang"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.01789v2", "summary": "To address the ill-posedness of the inverse source problem for the\none-dimensional stochastic Helmholtz equations without attenuation, this study\ndevelops a novel computational framework designed to mitigate this inherent\nchallenge at the numerical implementation level. For the stochastic wave\nequation driven by a finite-jump L\\'evy process (assuming that its jump\namplitude obeys a Gaussian distribution and the jump time interval obeys a\nPoisson distribution), this paper firstly establish the existence of a mild\nsolution to its direct problem satisfying a particular stability estimate.\nBuilding upon these theoretical foundations, we further investigate the\nwell-posedness of the inverse problem and develop a methodology to reconstruct\nthe unknown source terms $f$ and $g$ using the data of the wave field at the\nfinal time point $u(x,T)$. This work not only provides rigorous theoretical\nanalysis and effective numerical schemes for solving inverse source problems in\nthese two specific classes of stochastic wave equations, but also offers new\nperspectives and methodological approaches for addressing a broader range of\nwave propagation inverse problems characterized by non-Gaussian stochastic\nproperties. The proposed framework demonstrates significant relevance for\ncharacterizing physical phenomena influenced by jump-type stochastic\nperturbations, offering promising applications in diverse domains including but\nnot limited to seismic wave propagation analysis and financial market\nvolatility modeling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.01789v2", "cate": "math.NA", "date": "2025-07-02", "updated": "2025-07-10"}
{"id": "2507.07850", "title": "Identifying the Smallest Adversarial Load Perturbations that Render DC-OPF Infeasible", "authors": ["Samuel Chevalier", "William A. Wheeler"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07850v1", "summary": "What is the globally smallest load perturbation that renders DC-OPF\ninfeasible? Reliably identifying such \"adversarial attack\" perturbations has\nuseful applications in a variety of emerging grid-related contexts, including\nmachine learning performance verification, cybersecurity, and operational\nrobustness of power systems dominated by stochastic renewable energy resources.\nIn this paper, we formulate the inherently nonconvex adversarial attack problem\nby applying a parameterized version of Farkas' lemma to a perturbed set of\nDC-OPF equations. Since the resulting formulation is very hard to globally\noptimize, we also propose a parameterized generation control policy which, when\napplied to the primal DC-OPF problem, provides solvability guarantees.\nTogether, these nonconvex problems provide guaranteed upper and lower bounds on\nadversarial attack size; by combining them into a single optimization problem,\nwe can efficiently \"squeeze\" these bounds towards a common global solution. We\napply these methods on a range of small- to medium-sized test cases from PGLib,\nbenchmarking our results against the best adversarial attack lower bounds\nprovided by Gurobi 12.0's spatial Branch and Bound solver.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07850v1", "cate": "eess.SY", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2407.17070", "title": "Curriculum Negative Mining For Temporal Networks", "authors": ["Ziyue Chen", "Tongya Zheng", "Mingli Song"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2407.17070v2", "summary": "Temporal networks are effective in capturing the evolving interactions of\nnetworks over time, such as social networks and e-commerce networks. In recent\nyears, researchers have primarily concentrated on developing specific model\narchitectures for Temporal Graph Neural Networks (TGNNs) in order to improve\nthe representation quality of temporal nodes and edges. However, limited\nattention has been given to the quality of negative samples during the training\nof TGNNs. When compared with static networks, temporal networks present two\nspecific challenges for negative sampling: positive sparsity and positive\nshift. Positive sparsity refers to the presence of a single positive sample\namidst numerous negative samples at each timestamp, while positive shift\nrelates to the variations in positive samples across different timestamps. To\nrobustly address these challenges in training TGNNs, we introduce Curriculum\nNegative Mining (CurNM), a model-aware curriculum learning framework that\nadaptively adjusts the difficulty of negative samples. Within this framework,\nwe first establish a dynamically updated negative pool that balances random,\nhistorical, and hard negatives to address the challenges posed by positive\nsparsity. Secondly, we implement a temporal-aware negative selection module\nthat focuses on learning from the disentangled factors of recently active\nedges, thus accurately capturing shifting preferences. Finally, the selected\nnegatives are combined with annealing random negatives to support stable\ntraining. Extensive experiments on 12 datasets and 3 TGNNs demonstrate that our\nmethod outperforms baseline methods by a significant margin. Additionally,\nthorough ablation studies and parameter sensitivity experiments verify the\nusefulness and robustness of our approach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2407.17070v2", "cate": "cs.LG", "date": "2024-07-24", "updated": "2025-07-10"}
{"id": "2403.13268", "title": "Unifews: You Need Fewer Operations for Efficient Graph Neural Networks", "authors": ["Ningyi Liao", "Zihao Yu", "Ruixiao Zeng", "Siqiang Luo"], "categories": ["cs.LG", "cs.DB"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted by ICML 2025", "url": "http://arxiv.org/abs/2403.13268v2", "summary": "Graph Neural Networks (GNNs) have shown promising performance, but at the\ncost of resource-intensive operations on graph-scale matrices. To reduce\ncomputational overhead, previous studies attempt to sparsify the graph or\nnetwork parameters, but with limited flexibility and precision boundaries. In\nthis work, we propose Unifews, a joint sparsification technique to unify graph\nand weight matrix operations and enhance GNN learning efficiency. The Unifews\ndesign enables adaptive compression across GNN layers with progressively\nincreased sparsity, and is applicable to a variety of architectures with\non-the-fly simplification. Theoretically, we establish a novel framework to\ncharacterize sparsified GNN learning in view of the graph optimization process,\nshowing that Unifews effectively approximates the learning objective with\nbounded error and reduced computational overhead. Extensive experiments\ndemonstrate that Unifews achieves efficiency improvements with comparable or\nbetter accuracy, including 10-20x matrix operation reduction and up to 100x\nacceleration on graphs up to billion-edge scale.", "comment": "Accepted by ICML 2025", "pdf_url": "http://arxiv.org/pdf/2403.13268v2", "cate": "cs.LG", "date": "2024-03-20", "updated": "2025-07-10"}
{"id": "2408.12246", "title": "RT-OVAD: Real-Time Open-Vocabulary Aerial Object Detection via Image-Text Collaboration", "authors": ["Guoting Wei", "Xia Yuan", "Yu Liu", "Zhenhao Shang", "Xizhe Xue", "Peng Wang", "Kelu Yao", "Chunxia Zhao", "Haokui Zhang", "Rong Xiao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2408.12246v3", "summary": "Aerial object detection plays a crucial role in numerous applications.\nHowever, most existing methods focus on detecting predefined object categories,\nlimiting their applicability in real-world open scenarios. In this paper, we\nextend aerial object detection to open scenarios through image-text\ncollaboration and propose RT-OVAD, the first real-time open-vocabulary detector\nfor aerial scenes. Specifically, we first introduce an image-to-text alignment\nloss to replace the conventional category regression loss, thereby eliminating\ncategory constraints. Next, we propose a lightweight image-text collaboration\nstrategy comprising an image-text collaboration encoder and a text-guided\ndecoder. The encoder simultaneously enhances visual features and refines\ntextual embeddings, while the decoder guides object queries to focus on\nclass-relevant image features. This design further improves detection accuracy\nwithout incurring significant computational overhead. Extensive experiments\ndemonstrate that RT-OVAD consistently outperforms existing state-of-the-art\nmethods across open-vocabulary, zero-shot, and traditional closed-set detection\ntasks. For instance, on the open-vocabulary aerial detection benchmarks DIOR,\nDOTA-v2.0, and LAE-80C, RT-OVAD achieves 87.7 AP$_{50}$, 53.8 mAP, and 23.7\nmAP, respectively, surpassing the previous state-of-the-art (LAE-DINO) by 2.2,\n7.0, and 3.5 points. In addition, RT-OVAD achieves an inference speed of 34 FPS\non an RTX 4090 GPU, approximately three times faster than LAE-DINO (10 FPS),\nmeeting the real-time detection requirements of diverse applications. The code\nwill be released at https://github.com/GT-Wei/RT-OVAD.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2408.12246v3", "cate": "cs.CV", "date": "2024-08-22", "updated": "2025-07-10"}
{"id": "2411.01077", "title": "Emoji Attack: Enhancing Jailbreak Attacks Against Judge LLM Detection", "authors": ["Zhipeng Wei", "Yuqi Liu", "N. Benjamin Erichson"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.01077v3", "summary": "Jailbreaking techniques trick Large Language Models (LLMs) into producing\nrestricted output, posing a potential threat. One line of defense is to use\nanother LLM as a Judge to evaluate the harmfulness of generated text. However,\nwe reveal that these Judge LLMs are vulnerable to token segmentation bias, an\nissue that arises when delimiters alter the tokenization process, splitting\nwords into smaller sub-tokens. This alters the embeddings of the entire\nsequence, reducing detection accuracy and allowing harmful content to be\nmisclassified as safe. In this paper, we introduce Emoji Attack, a novel\nstrategy that amplifies existing jailbreak prompts by exploiting token\nsegmentation bias. Our method leverages in-context learning to systematically\ninsert emojis into text before it is evaluated by a Judge LLM, inducing\nembedding distortions that significantly lower the likelihood of detecting\nunsafe content. Unlike traditional delimiters, emojis also introduce semantic\nambiguity, making them particularly effective in this attack. Through\nexperiments on state-of-the-art Judge LLMs, we demonstrate that Emoji Attack\nsubstantially reduces the unsafe prediction rate, bypassing existing\nsafeguards.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.01077v3", "cate": "cs.CL", "date": "2024-11-01", "updated": "2025-07-09"}
{"id": "2507.03492", "title": "Elliptic interface problem approximated by CutFEM: I. Conservative flux recovery and numerical validation of adaptive mesh refinement", "authors": ["Daniela Capatina", "Aimene Gouasmi", "Cuiyu He"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.03492v2", "summary": "We study an elliptic interface problem with discontinuous diffusion\ncoefficients on unfitted meshes using the CutFEM method. Our main contribution\nis the reconstruction of conservative fluxes from the CutFEM solution and their\nuse in a posteriori error estimation. We introduce a hybrid mixed formulation\nwith locally computable Lagrange multipliers and reconstruct the flux in the\nimmersed Raviart-Thomas space. Based on this, we propose a new a posteriori\nerror estimator that includes both volume and interface terms. We state its\nrobust reliability and local efficiency, and validate the approach through\nnumerical experiments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.03492v2", "cate": "math.NA", "date": "2025-07-04", "updated": "2025-07-10"}
{"id": "2507.07263", "title": "Convergence and Robustness Bounds for Distributed Asynchronous Shortest-Path", "authors": ["Jared Miller", "Mattia Bianchi", "Florian D√∂rfler"], "categories": ["math.OC", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      12 pages, 6 figures", "url": "http://arxiv.org/abs/2507.07263v1", "summary": "This work analyzes convergence times and robustness bounds for asynchronous\ndistributed shortest-path computation. We focus on the Adaptive Bellman--Ford\nalgorithm, a self-stabilizing method in which each agent updates its\nshortest-path estimate based only on the estimates of its neighbors and\nforgetting its previous estimate. In the asynchronous framework considered in\nthis paper, agents are allowed to idle or encounter race conditions during\ntheir execution of the Adaptive Bellman--Ford algorithm. We build on\nLyapunov-based results that develop finite-time convergence and robustness\nbounds for the synchronous shortest-path setting, in order to produce\nfinite-time convergence and robustness bounds for the asynchronous setting. We\nalso explore robustness against interval-bounded noise processes and establish\nconvergence and robustness guarantees for asynchronous most-probable-path\nalgorithms.", "comment": "12 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.07263v1", "cate": "math.OC", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2411.00461", "title": "A Multi-Granularity Supervised Contrastive Framework for Remaining Useful Life Prediction of Aero-engines", "authors": ["Zixuan He", "Ziqian Kong", "Zhengyu Chen", "Yuling Zhan", "Zijun Que", "Zhengguo Xu"], "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.00461v3", "summary": "Accurate remaining useful life (RUL) predictions are critical to the safe\noperation of aero-engines. Currently, the RUL prediction task is mainly a\nregression paradigm with only mean square error as the loss function and lacks\nresearch on feature space structure, the latter of which has shown excellent\nperformance in a large number of studies. This paper develops a\nmulti-granularity supervised contrastive (MGSC) framework from plain intuition\nthat samples with the same RUL label should be aligned in the feature space,\nand address the problems of too large minibatch size and unbalanced samples in\nthe implementation. The RUL prediction with MGSC is implemented on using the\nproposed multi-phase training strategy. This paper also demonstrates a simple\nand scalable basic network structure and validates the proposed MGSC strategy\non the CMPASS dataset using a convolutional long short-term memory network as a\nbaseline, which effectively improves the accuracy of RUL prediction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.00461v3", "cate": "cs.LG", "date": "2024-11-01", "updated": "2025-07-10"}
{"id": "2405.18563", "title": "No $D_{\\text{train}}$: Model-Agnostic Counterfactual Explanations Using Reinforcement Learning", "authors": ["Xiangyu Sun", "Raquel Aoki", "Kevin H. Wilson"], "categories": ["cs.LG", "stat.ME"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Published in Transactions on Machine Learning Research (TMLR 2025)", "url": "http://arxiv.org/abs/2405.18563v2", "summary": "Machine learning (ML) methods have experienced significant growth in the past\ndecade, yet their practical application in high-impact real-world domains has\nbeen hindered by their opacity. When ML methods are responsible for making\ncritical decisions, stakeholders often require insights into how to alter these\ndecisions. Counterfactual explanations (CFEs) have emerged as a solution,\noffering interpretations of opaque ML models and providing a pathway to\ntransition from one decision to another. However, most existing CFE methods\nrequire access to the model's training dataset, few methods can handle\nmultivariate time-series, and none of model-agnostic CFE methods can handle\nmultivariate time-series without training datasets. These limitations can be\nformidable in many scenarios. In this paper, we present NTD-CFE, a novel\nmodel-agnostic CFE method based on reinforcement learning (RL) that generates\nCFEs when training datasets are unavailable. NTD-CFE is suitable for both\nstatic and multivariate time-series datasets with continuous and discrete\nfeatures. NTD-CFE reduces the CFE search space from a multivariate time-series\ndomain to a lower dimensional space and addresses the problem using RL. Users\nhave the flexibility to specify non-actionable, immutable, and preferred\nfeatures, as well as causal constraints. We demonstrate the performance of\nNTD-CFE against four baselines on several datasets and find that, despite not\nhaving access to a training dataset, NTD-CFE finds CFEs that make significantly\nfewer and significantly smaller changes to the input time-series. These\nproperties make CFEs more actionable, as the magnitude of change required to\nalter an outcome is vastly reduced. The code is available in the supplementary\nmaterial.", "comment": "Published in Transactions on Machine Learning Research (TMLR 2025)", "pdf_url": "http://arxiv.org/pdf/2405.18563v2", "cate": "cs.LG", "date": "2024-05-28", "updated": "2025-07-10"}
{"id": "2411.15469", "title": "Mamba-CL: Optimizing Selective State Space Model in Null Space for Continual Learning", "authors": ["De Cheng", "Yue Lu", "Lingfeng He", "Shizhou Zhang", "Xi Yang", "Nannan Wang", "Xinbo Gao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.15469v2", "summary": "Continual Learning (CL) aims to equip AI models with the ability to learn a\nsequence of tasks over time, without forgetting previously learned knowledge.\nRecently, State Space Models (SSMs), particularly the Mamba model, have\nachieved notable success in computer vision. Building on the strengths of SSMs,\nthis study explores leveraging the Mamba model for CL. Therefore, we introduce\nMamba-CL, a framework that continuously fine-tunes the core SSMs of the\nlarge-scale Mamba foundation model by updating parameters orthogonal to the\nfeature subspace of previous tasks. This approach theoretically guarantees the\nconsistency objective aiming to preserves consistent output for each SSM module\nacross both previous and current tasks, so as to overcome catastrophic\nforgetting issue. Specifically, we achieve this goal by deducing the overall\nconsistency constraints on four key time-invariant parameters in the Mamba\nmodel, streamlining its recurrent state-space structure and non-linear\ndiscretization process in SSM. In practice, we apply the null-space projection\nto efficiently implement the orthogonality within Mamba model. Extensive\nexperiments on four class-incremental benchmarks demonstrate the effectiveness\nof Mamba-CL for anti-forgetting, achieving superior performances to\nstate-of-the-art methods. Code is available in the supplementary materials.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.15469v2", "cate": "cs.CV", "date": "2024-11-23", "updated": "2025-07-10"}
{"id": "2411.11984", "title": "Understanding Chain-of-Thought in LLMs through Information Theory", "authors": ["Jean-Francois Ton", "Muhammad Faaiz Taufiq", "Yang Liu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.11984v2", "summary": "Large Language Models (LLMs) have shown impressive performance in complex\nreasoning tasks through the use of Chain-of-Thought (CoT) reasoning, allowing\nmodels to break down problems into manageable sub-tasks. However, existing CoT\nevaluation techniques either require annotated CoT data or fall short in\naccurately assessing intermediate reasoning steps, leading to high rates of\nfalse positives. In this paper, we formalize CoT reasoning in LLMs through an\ninformation-theoretic lens. Specifically, our framework quantifies the\n`information-gain' at each reasoning step, enabling the identification of\nfailure modes in LLMs without the need for expensive annotated datasets. We\ndemonstrate the efficacy of our approach through extensive experiments on toy\narithmetic, GSM8K and PRM800k datasets, where it significantly outperforms\nexisting outcome-based methods by providing more accurate insights into model\nperformance on individual subtasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.11984v2", "cate": "cs.CL", "date": "2024-11-18", "updated": "2025-07-10"}
{"id": "2401.00844", "title": "The semi-analytic theory and computation of finite-depth standing water waves", "authors": ["Ahmad Abassi", "Jon Wilkening"], "categories": ["physics.flu-dyn", "cs.NA", "math.NA", "76B15, 35C20, 37G15, 65N22, 65N35, 68W10"], "primary_category": "Subjects:       Fluid Dynamics (physics.flu-dyn)", "pdf_link": null, "comments": "Comments:      51 pages, 19 figures", "url": "http://arxiv.org/abs/2401.00844v3", "summary": "We propose a Stokes expansion ansatz for finite-depth standing water waves in\ntwo dimensions and devise a recursive algorithm to compute the expansion\ncoefficients. We implement the algorithm on a supercomputer using\narbitrary-precision arithmetic. The Stokes expansion introduces hyperbolic\nterms that require exponentiation of power series, which we handle efficiently\nusing Bell polynomials. Although exact resonances occur at a countable dense\nset of fluid depths, we prove that for almost every depth, the divisors that\narise in the recurrence are bounded away from zero by a slowly decaying\nfunction of the wave number. A direct connection between small divisors and\nimperfect bifurcations is observed. They are found to activate secondary\nstanding waves that oscillate non-uniformly in space and time on top of the\nprimary wave, with different amplitudes and phases on each bifurcation branch.\nWe compute new families of standing waves using a shooting method and find that\nPad\\'e approximants of the Stokes expansion continue to converge to the\nshooting method solutions at large amplitudes as new small divisors enter the\nrecurrence. Closely spaced poles and zeros of the Pad\\'e approximants are\nobserved, which suggests that the bifurcation branches are separated by branch\ncuts.", "comment": "51 pages, 19 figures", "pdf_url": "http://arxiv.org/pdf/2401.00844v3", "cate": "physics.flu-dyn", "date": "2024-01-01", "updated": "2025-07-09"}
{"id": "2507.07512", "title": "Demonstration of TFTs 3D Monolithically Integrated on GaN HEMTs using Cascode Configuration with High Breakdown Voltage (>1900V)", "authors": ["Tian-Li Wu", "Hsin-Jou Ho", "Chia-Wei Liu", "Yi-Chen Chen"], "categories": ["physics.app-ph", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Applied Physics (physics.app-ph)", "pdf_link": null, "comments": "Comments:      3 pages, 5 figures", "url": "http://arxiv.org/abs/2507.07512v1", "summary": "This study demonstrates 3D monolithic integration of amorphous\nindium-gallium-zinc oxide (a-IGZO) thin-film transistors (TFTs) on Gallium\nNitride (GaN) high electron mobility transistors (HEMTs) in a cascode\nconfiguration, achieving high breakdown voltage capabilities exceeding 1900 V.\nTwo device configurations, differing in a-IGZO channel thickness (30 nm / 10\nnm), are fabricated and evaluated. Sample B, with a 10 nm a-IGZO channel,\ndemonstrates superior electrical performance, including a high ON/OFF current\nratio (~10^7), low subthreshold swing (SS), and a high breakdown voltage\nexceeding 1900 V comparable to standalone GaN power HEMTs. The results\nhighlight the feasibility and potential of 3D integrated TFT on GaN power\nHEMTs, paving the way for new opportunities for the TFTs for high voltage\napplications.", "comment": "3 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.07512v1", "cate": "physics.app-ph", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.07953", "title": "Incremental Collision Laws Based on the Bouc-Wen Model: External Forces and Corner Cases", "authors": ["Mihails Milehins", "Dan Marghitu"], "categories": ["physics.class-ph", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Classical Physics (physics.class-ph)", "pdf_link": null, "comments": "Comments:      12 pages, 3 figures, see this https URL . arXiv admin note: text overlap with arXiv:2410.08147", "url": "http://arxiv.org/abs/2507.07953v1", "summary": "In the article titled \"The Bouc-Wen Model for Binary Direct Collinear\nCollisions of Convex Viscoplastic Bodies\" and published in the Journal of\nComputational and Nonlinear Dynamics, the authors studied mathematical models\nof binary direct collinear collisions of convex viscoplastic bodies that\nemployed two incremental collision laws based on the Bouc-Wen differential\nmodel of hysteresis. It was shown that the models possess favorable analytical\nproperties, and several model parameter identification studies were conducted\nin an attempt to validate the models. In this article, these models are\naugmented by taking into account the effects of external forces that are\nmodeled as time-dependent inputs that belong to a certain function space.\nFurthermore, the range of the parameters under which the models possess\nfavorable analytical properties is extended to several corner cases that were\nnot considered in the prior publication. Finally, the previously conducted\nmodel parameter identification studies are extended, and an additional model\nparameter identification study is provided in an attempt to validate the\nability of the augmented models to represent the effects of external forces.", "comment": "12 pages, 3 figures, see https://gitlab.com/user9716869/EBWCM. arXiv\n  admin note: text overlap with arXiv:2410.08147", "pdf_url": "http://arxiv.org/pdf/2507.07953v1", "cate": "physics.class-ph", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2412.00151", "title": "DLaVA: Document Language and Vision Assistant for Answer Localization with Enhanced Interpretability and Trustworthiness", "authors": ["Ahmad Mohammadshirazi", "Pinaki Prasad Guha Neogi", "Ser-Nam Lim", "Rajiv Ramnath"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.00151v2", "summary": "Document Visual Question Answering (VQA) demands robust integration of text\ndetection, recognition, and spatial reasoning to interpret complex document\nlayouts. In this work, we introduce DLaVA, a novel, training-free pipeline that\nleverages Multimodal Large Language Models (MLLMs) for zero-shot answer\nlocalization in order to improve trustworthiness, interpretability, and\nexplainability. By leveraging an innovative OCR-free approach that organizes\ntext regions with unique bounding box IDs, the proposed method preserves\nspatial contexts without relying on iterative OCR or chain-of-thought\nreasoning, thus substantially reducing the computational complexity. We further\nenhance the evaluation protocol by integrating Intersection over Union (IoU)\nmetrics alongside Average Normalized Levenshtein Similarity (ANLS), thereby\nensuring that not only textual accuracy is considered, but spatial accuracy is\ntaken into account, ultimately reducing the risks of AI hallucinations and\nimproving trustworthiness. Experiments on benchmark datasets demonstrate\ncompetitive performance compared to state-of-the-art techniques, with\nsignificantly lower computational complexity and enhanced accuracies and\nreliability for high-stakes applications. The code and datasets utilized in\nthis study for DLaVA are accessible at:\nhttps://github.com/ahmad-shirazi/AnnotMLLM.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.00151v2", "cate": "cs.CV", "date": "2024-11-29", "updated": "2025-07-10"}
{"id": "2410.11171", "title": "A Bilevel Optimization Framework for Imbalanced Data Classification", "authors": ["Karen Medlin", "Sven Leyffer", "Krishnan Raghavan"], "categories": ["cs.LG", "math.OC", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.11171v3", "summary": "Data rebalancing techniques, including oversampling and undersampling, are a\ncommon approach to addressing the challenges of imbalanced data. To tackle\nunresolved problems related to both oversampling and undersampling, we propose\na new undersampling approach that: (i) avoids the pitfalls of noise and overlap\ncaused by synthetic data and (ii) avoids the pitfall of under-fitting caused by\nrandom undersampling. Instead of undersampling majority data randomly, our\nmethod undersamples datapoints based on their ability to improve model loss.\nUsing improved model loss as a proxy measurement for classification\nperformance, our technique assesses a datapoint's impact on loss and rejects\nthose unable to improve it. In so doing, our approach rejects majority\ndatapoints redundant to datapoints already accepted and, thereby, finds an\noptimal subset of majority training data for classification. The accept/reject\ncomponent of our algorithm is motivated by a bilevel optimization problem\nuniquely formulated to identify the optimal training set we seek. Experimental\nresults show our proposed technique with F1 scores up to 10% higher than\nstate-of-the-art methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.11171v3", "cate": "cs.LG", "date": "2024-10-15", "updated": "2025-07-10"}
{"id": "2501.17468", "title": "Solving Inverse Problems using Diffusion with Iterative Colored Renoising", "authors": ["Matt C. Bendel", "Saurav K. Shastri", "Rizwan Ahmad", "Philip Schniter"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.17468v3", "summary": "Imaging inverse problems can be solved in an unsupervised manner using\npre-trained diffusion models, but doing so requires approximating the gradient\nof the measurement-conditional score function in the diffusion reverse process.\nWe show that the approximations produced by existing methods are relatively\npoor, especially early in the reverse process, and so we propose a new approach\nthat iteratively reestimates and \"renoises\" the estimate several times per\ndiffusion step. This iterative approach, which we call Fast Iterative REnoising\n(FIRE), injects colored noise that is shaped to ensure that the pre-trained\ndiffusion model always sees white noise, in accordance with how it was trained.\nWe then embed FIRE into the DDIM reverse process and show that the resulting\n\"DDfire\" offers state-of-the-art accuracy and runtime on several linear inverse\nproblems, as well as phase retrieval. Our implementation is at\nhttps://github.com/matt-bendel/DDfire", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.17468v3", "cate": "cs.CV", "date": "2025-01-29", "updated": "2025-07-10"}
{"id": "2412.18151", "title": "CoAM: Corpus of All-Type Multiword Expressions", "authors": ["Yusuke Ide", "Joshua Tanner", "Adam Nohejl", "Jacob Hoffman", "Justin Vasselli", "Hidetaka Kamigaito", "Taro Watanabe"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      ACL 2025 main", "url": "http://arxiv.org/abs/2412.18151v3", "summary": "Multiword expressions (MWEs) refer to idiomatic sequences of multiple words.\nMWE identification, i.e., detecting MWEs in text, can play a key role in\ndownstream tasks such as machine translation, but existing datasets for the\ntask are inconsistently annotated, limited to a single type of MWE, or limited\nin size. To enable reliable and comprehensive evaluation, we created CoAM:\nCorpus of All-Type Multiword Expressions, a dataset of 1.3K sentences\nconstructed through a multi-step process to enhance data quality consisting of\nhuman annotation, human review, and automated consistency checking.\nAdditionally, for the first time in a dataset of MWE identification, CoAM's\nMWEs are tagged with MWE types, such as Noun and Verb, enabling fine-grained\nerror analysis. Annotations for CoAM were collected using a new interface\ncreated with our interface generator, which allows easy and flexible annotation\nof MWEs in any form. Through experiments using CoAM, we find that a fine-tuned\nlarge language model outperforms MWEasWSD, which achieved the state-of-the-art\nperformance on the DiMSUM dataset. Furthermore, analysis using our MWE type\ntagged data reveals that Verb MWEs are easier than Noun MWEs to identify across\napproaches.", "comment": "ACL 2025 main", "pdf_url": "http://arxiv.org/pdf/2412.18151v3", "cate": "cs.CL", "date": "2024-12-24", "updated": "2025-07-10"}
{"id": "2501.08482", "title": "Surrogate-based multilevel Monte Carlo methods for uncertainty quantification in the Grad-Shafranov free boundary problem", "authors": ["Howard Elman", "Jiaxing Liang", "Tonatiuh S√°nchez-Vizuet"], "categories": ["physics.comp-ph", "cs.NA", "math.NA", "physics.plasm-ph", "65Z05, 65C05, 62P35, 35R35, 35R60"], "primary_category": "Subjects:       Computational Physics (physics.comp-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.08482v2", "summary": "We explore a hybrid technique to quantify the variability in the numerical\nsolutions to a free boundary problem associated with magnetic equilibrium in\naxisymmetric fusion reactors amidst parameter uncertainties. The method aims at\nreducing computational costs by integrating a surrogate model into a multilevel\nMonte Carlo method. The resulting surrogate-enhanced multilevel Monte Carlo\nmethods reduce the cost of simulation by factors as large as $10^4$ compared to\nstandard Monte Carlo simulations involving direct numerical solutions of the\nassociated Grad-Shafranov partial differential equation. Accuracy assessments\nalso show that surrogate-based sampling closely aligns with the results of\ndirect computation, confirming its effectiveness in capturing the behavior of\nplasma boundary and geometric descriptors.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.08482v2", "cate": "physics.comp-ph", "date": "2025-01-14", "updated": "2025-07-09"}
{"id": "2404.09876", "title": "Conservative Bias Linear Power Flow Approximations: Application to Unit Commitment", "authors": ["Paprapee Buason", "Sidhant Misra", "Daniel K. Molzahn"], "categories": ["eess.SY", "cs.SY", "math.OC"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      The shorter version is published in P. Buason, S. Misra and D. K. Molzahn, \"Sample-Based Conservative Bias Linear Power Flow Approximations,\" 2024 IEEE/IAS Industrial and Commercial Power System Asia (I&CPS Asia), Pattaya, Thailand, 2024, pp. 1-6, doi: https://doi.org/10.1109/ICPSAsia61913.2024.10761778", "url": "http://arxiv.org/abs/2404.09876v2", "summary": "The power flow equations are central to many problems in power system\nplanning, analysis, and control. However, their inherent non-linearity and\nnon-convexity present substantial challenges during problem-solving processes,\nespecially for optimization problems. Accordingly, linear approximations are\ncommonly employed to streamline computations, although this can often entail\ncompromises in accuracy and feasibility. This paper proposes an approach termed\nConservative Bias Linear Approximations (CBLA) for addressing these\nlimitations. By minimizing approximation errors across a specified operating\nrange while incorporating conservativeness (over- or under-estimating\nquantities of interest), CBLA strikes a balance between accuracy and\ntractability by maintaining linear constraints. By allowing users to design\nloss functions tailored to the specific approximated function, the bias\napproximation approach significantly enhances approximation accuracy. We\nillustrate the effectiveness of our proposed approach through several test\ncases, including its application to a unit commitment problem, where CBLA\nconsistently achieves lower operating costs and improved feasibility compared\nto traditional linearization methods.", "comment": "The shorter version is published in P. Buason, S. Misra and D. K.\n  Molzahn, \"Sample-Based Conservative Bias Linear Power Flow Approximations,\"\n  2024 IEEE/IAS Industrial and Commercial Power System Asia (I&CPS Asia),\n  Pattaya, Thailand, 2024, pp. 1-6, doi: 10.1109/ICPSAsia61913.2024.10761778", "pdf_url": "http://arxiv.org/pdf/2404.09876v2", "cate": "eess.SY", "date": "2024-04-15", "updated": "2025-07-09"}
{"id": "2410.20160", "title": "Vibration-based damage detection of a trainer jet via multiple input tangential interpolation", "authors": ["Gabriele Dessena", "Marco Civera", "Andr√©s Marcos", "Bernardino Chiaia", "Oscar E. Bonilla-Manrique"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.20160v3", "summary": "Control engineering is a highly developed field, which includes similarly\nadvanced areas like system identification. In structural dynamics, system\nidentification methods are employed for the extraction of modal parameters,\nsuch as natural frequencies and mode shapes, from any structure. In turn, these\nare the main building blocks of vibration-based damage detection. However,\ntraditional comparisons of these parameters are often ambiguous in complex\nsystems, complicating damage detection and assessment. The modified total modal\nassurance criterion (MTMAC), a metric well-known in the field of finite element\nmodel updating, is extended to address this challenge and is proposed as a\nmetric for damage identification and severity assessment. To support the\nrequirement for precise and robust modal identification of Structural Health\nMonitoring (SHM), the improved Loewner Framework (iLF), known for its\nreliability and computational performance, is pioneeringly employed within SHM.\nSince the MTMAC is proposed solely as a damage identification and severity\nassessment metric, the coordinate modal assurance criterion (COMAC), also a\nwell-established tool, but for damage localisation using mode shapes, is used\nfor completeness. The iLF SHM capabilities are validated through comparisons\nwith traditional methods, including least-squares complex exponential (LSCE)\nand stochastic subspace identification with canonical variate analysis\n(SSI-CVA) on a numerical case study of a cantilever beam. Furthermore, the\nMTMAC is validated against the traditional vibration-based approach, which\ninvolves directly comparing natural frequencies and mode shapes. Finally, an\nexperimental dataset from a BAE Systems Hawk T1A trainer jet ground vibration\ntests is used to demonstrate the iLF and MTMAC capabilities on a real-life,\nreal-size SHM problem, showing their effectiveness in detecting and assessing\ndamage.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.20160v3", "cate": "eess.SY", "date": "2024-10-26", "updated": "2025-07-10"}
{"id": "2501.00759", "title": "Enhancing Transformers for Generalizable First-Order Logical Entailment", "authors": ["Tianshi Zheng", "Jiazheng Wang", "Zihao Wang", "Jiaxin Bai", "Hang Yin", "Zheye Deng", "Yangqiu Song", "Jianxin Li"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      ACL 2025 Main", "url": "http://arxiv.org/abs/2501.00759v3", "summary": "Transformers, as the fundamental deep learning architecture, have\ndemonstrated great capability in reasoning. This paper studies the\ngeneralizable first-order logical reasoning ability of transformers with their\nparameterized knowledge and how to improve it. Transformers' capability of\nfirst-order reasoning is further captured by whether they can conduct\nfirst-order logical entailment, which is quantitatively measured by their\nperformance in answering knowledge graph queries. We establish the connections\nbetween (1) two types of distribution shifts studied in out-of-distribution\ngeneralization and (2) unseen knowledge and query settings discussed in the\ntask of knowledge graph query answering, which makes it possible to\ncharacterize the fine-grained generalizability. Results on our comprehensive\ndataset showed that transformers \\textit{outperform} previous methods designed\nparticularly for this task and provided detailed empirical evidence about the\nimpact of the input query syntax, token embedding, and transformer\narchitectures on their reasoning capability. Interestingly, our results\nrevealed the mismatch of positional encoding and other design choices of\ntransformer architectures in previous practices. Motivated by this, we propose\nTEGA, a logic-aware architecture that significantly improves the performance in\ngeneralizable first-order logical entailment.", "comment": "ACL 2025 Main", "pdf_url": "http://arxiv.org/pdf/2501.00759v3", "cate": "cs.CL", "date": "2025-01-01", "updated": "2025-07-10"}
{"id": "2410.17428", "title": "Uncovering RL Integration in SSL Loss: Objective-Specific Implications for Data-Efficient RL", "authors": ["√ñmer Veysel √áaƒüatan", "Barƒ±≈ü Akg√ºn"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      RLC 2025, Neurips SSL Workshop 2024", "url": "http://arxiv.org/abs/2410.17428v3", "summary": "In this study, we investigate the effect of SSL objective modifications\nwithin the SPR framework, focusing on specific adjustments such as terminal\nstate masking and prioritized replay weighting, which were not explicitly\naddressed in the original design. While these modifications are specific to RL,\nthey are not universally applicable across all RL algorithms. Therefore, we aim\nto assess their impact on performance and explore other SSL objectives that do\nnot accommodate these adjustments like Barlow Twins and VICReg. We evaluate six\nSPR variants on the Atari 100k benchmark, including versions both with and\nwithout these modifications. Additionally, we test the performance of these\nobjectives on the DeepMind Control Suite, where such modifications are absent.\nOur findings reveal that incorporating specific SSL modifications within SPR\nsignificantly enhances performance, and this influence extends to subsequent\nframeworks like SR-SPR and BBF, highlighting the critical importance of SSL\nobjective selection and related adaptations in achieving data efficiency in\nself-predictive reinforcement learning.", "comment": "RLC 2025, Neurips SSL Workshop 2024", "pdf_url": "http://arxiv.org/pdf/2410.17428v3", "cate": "cs.LG", "date": "2024-10-22", "updated": "2025-07-10"}
{"id": "2503.04720", "title": "FluidNexus: 3D Fluid Reconstruction and Prediction from a Single Video", "authors": ["Yue Gao", "Hong-Xing Yu", "Bo Zhu", "Jiajun Wu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      CVPR 2025 (oral). The first two authors contributed equally. Project website: this https URL", "url": "http://arxiv.org/abs/2503.04720v2", "summary": "We study reconstructing and predicting 3D fluid appearance and velocity from\na single video. Current methods require multi-view videos for fluid\nreconstruction. We present FluidNexus, a novel framework that bridges video\ngeneration and physics simulation to tackle this task. Our key insight is to\nsynthesize multiple novel-view videos as references for reconstruction.\nFluidNexus consists of two key components: (1) a novel-view video synthesizer\nthat combines frame-wise view synthesis with video diffusion refinement for\ngenerating realistic videos, and (2) a physics-integrated particle\nrepresentation coupling differentiable simulation and rendering to\nsimultaneously facilitate 3D fluid reconstruction and prediction. To evaluate\nour approach, we collect two new real-world fluid datasets featuring textured\nbackgrounds and object interactions. Our method enables dynamic novel view\nsynthesis, future prediction, and interaction simulation from a single fluid\nvideo. Project website: https://yuegao.me/FluidNexus.", "comment": "CVPR 2025 (oral). The first two authors contributed equally. Project\n  website: https://yuegao.me/FluidNexus", "pdf_url": "http://arxiv.org/pdf/2503.04720v2", "cate": "cs.CV", "date": "2025-03-06", "updated": "2025-07-09"}
{"id": "2502.12896", "title": "None of the Others: a General Technique to Distinguish Reasoning from Memorization in Multiple-Choice LLM Evaluation Benchmarks", "authors": ["Eva S√°nchez Salido", "Julio Gonzalo", "Guillermo Marco"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.12896v5", "summary": "In LLM evaluations, reasoning is often distinguished from recall/memorization\nby performing numerical variations to math-oriented questions. Here we\nintroduce a general variation method for multiple-choice questions that\ncompletely dissociates the correct answer from previously seen tokens or\nconcepts, requiring LLMs to understand and reason (rather than memorizing) in\norder to answer correctly. Using this method, we evaluate state-of-the-art\nproprietary and open-source LLMs on two datasets available in English and\nSpanish: the public MMLU benchmark and the private UNED-Access 2024 dataset.\nResults show that all models experience remarkable accuracy drops under our\nproposed variation, with an average loss of 57% on MMLU and 50% on UNED-Access\n2024, ranging from 10% to 93% across models. Notably, the most accurate model\nin our experimentation (OpenAI-o3-mini) is not the most robust\n(DeepSeek-R1-70B), suggesting that the best models in standard evaluations may\nnot be the ones with better reasoning capabilities. Also, we see larger\naccuracy drops in public (vs private) datasets and questions posed in their\noriginal language (vs a manual translation), which are signs of contamination\nand also point to a relevant role of recall/memorization in current LLMs'\nanswers.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.12896v5", "cate": "cs.CL", "date": "2025-02-18", "updated": "2025-07-10"}
{"id": "2503.04424", "title": "Determinant Estimation under Memory Constraints and Neural Scaling Laws", "authors": ["Siavash Ameli", "Chris van der Heide", "Liam Hodgkinson", "Fred Roosta", "Michael W. Mahoney"], "categories": ["stat.ML", "cs.LG", "cs.NA", "math.NA"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.04424v2", "summary": "Calculating or accurately estimating log-determinants of large positive\ndefinite matrices is of fundamental importance in many machine learning tasks.\nWhile its cubic computational complexity can already be prohibitive, in modern\napplications, even storing the matrices themselves can pose a memory\nbottleneck. To address this, we derive a novel hierarchical algorithm based on\nblock-wise computation of the LDL decomposition for large-scale log-determinant\ncalculation in memory-constrained settings. In extreme cases where matrices are\nhighly ill-conditioned, accurately computing the full matrix itself may be\ninfeasible. This is particularly relevant when considering kernel matrices at\nscale, including the empirical Neural Tangent Kernel (NTK) of neural networks\ntrained on large datasets. Under the assumption of neural scaling laws in the\ntest error, we show that the ratio of pseudo-determinants satisfies a power-law\nrelationship, allowing us to derive corresponding scaling laws. This enables\naccurate estimation of NTK log-determinants from a tiny fraction of the full\ndataset; in our experiments, this results in a $\\sim$100,000$\\times$ speedup\nwith improved accuracy over competing approximations. Using these techniques,\nwe successfully estimate log-determinants for dense matrices of extreme sizes,\nwhich were previously deemed intractable and inaccessible due to their enormous\nscale and computational demands.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.04424v2", "cate": "stat.ML", "date": "2025-03-06", "updated": "2025-07-10"}
{"id": "2504.05592", "title": "Impact Assessment of Cyberattacks in Inverter-Based Microgrids", "authors": ["Kerd Topallaj", "Colin McKerrell", "Suraj Ramanathan", "Ioannis Zografopoulos"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      IEEE Workshop on the Electronic Grid (eGrid 2025)", "url": "http://arxiv.org/abs/2504.05592v2", "summary": "In recent years, the evolution of modern power grids has been driven by the\ngrowing integration of remotely controlled grid assets. Although Distributed\nEnergy Resources (DERs) and Inverter-Based Resources (IBRs) enhance operational\nefficiency, they also introduce cybersecurity risks. The remote accessibility\nof such critical grid components creates entry points for attacks that\nadversaries could exploit, posing threats to the stability of the system. To\nevaluate the resilience of energy systems under such threats, this study\nemploys real-time simulation and a modified version of the IEEE 39-bus system\nthat incorporates a Microgrid (MG) with solar-based IBR. The study assesses the\nimpact of remote attacks impacting the MG stability under different levels of\nIBR penetration through hardware-in-the-loop (HIL) simulations. Namely, we\nanalyze voltage, current, and frequency profiles before, during, and after\ncyberattack-induced disruptions. The results demonstrate that real-time HIL\ntesting is a practical approach to uncover potential risks and develop robust\nmitigation strategies for resilient MG operations.", "comment": "IEEE Workshop on the Electronic Grid (eGrid 2025)", "pdf_url": "http://arxiv.org/pdf/2504.05592v2", "cate": "eess.SY", "date": "2025-04-08", "updated": "2025-07-10"}
{"id": "2507.06352", "title": "Revisiting Chien-Hrones-Reswick Method for an Analytical Solution", "authors": ["Senol Gulgonul"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      7 pages, 3 figures, 1 table. This work is licensed under CC BY-NC-ND 4.0. For commercial licensing, contact the author", "url": "http://arxiv.org/abs/2507.06352v2", "summary": "This study presents an analytical method for tuning PI controllers in\nFirst-Order with Time Delay (FOTD) systems, leveraging the Lambert W function.\nThe Lambert W function enables exact pole placement, yielding analytical\nexpressions for PI gains. The proposed approach identifies a critical condition\nthat achieves a step response without overshoot with minimum settling time,\nwhile also providing explicit tuning rules for systems where controlled\novershoot is specified. The method demonstrates strong agreement with\nestablished empirical Chien-Hrones-Reswick tuning rules for both\nnon-overshooting and overshooting cases, bridging the gap between theoretical\nanalysis and empirical results.", "comment": "7 pages, 3 figures, 1 table. This work is licensed under CC BY-NC-ND\n  4.0. For commercial licensing, contact the author", "pdf_url": "http://arxiv.org/pdf/2507.06352v2", "cate": "eess.SY", "date": "2025-07-08", "updated": "2025-07-10"}
{"id": "2501.07964", "title": "Derivation of Output Correlation Inferences for Multi-Output (aka Multi-Task) Gaussian Process", "authors": ["Shuhei Watanabe"], "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.07964v4", "summary": "Gaussian process (GP) is arguably one of the most widely used machine\nlearning algorithms in practice. One of its prominent applications is Bayesian\noptimization (BO). Although the vanilla GP itself is already a powerful tool\nfor BO, it is often beneficial to be able to consider the dependencies of\nmultiple outputs. To do so, Multi-task GP (MTGP) is formulated, but it is not\ntrivial to fully understand the derivations of its formulations and their\ngradients from the previous literature. This paper serves friendly derivations\nof the MTGP formulations and their gradients.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.07964v4", "cate": "cs.LG", "date": "2025-01-14", "updated": "2025-07-10"}
{"id": "2412.16209", "title": "Challenges learning from imbalanced data using tree-based models: Prevalence estimates systematically depend on hyperparameters and can be upwardly biased", "authors": ["Nathan Phelps", "Daniel J. Lizotte", "Douglas G. Woolford"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.16209v2", "summary": "Imbalanced binary classification problems arise in many fields of study. When\nusing machine learning models for these problems, it is common to subsample the\nmajority class (i.e., undersampling) to create a (more) balanced dataset for\nmodel training. This biases the model's predictions because the model learns\nfrom a dataset that does not follow the same data generating process as new\ndata. One way of accounting for this bias is to analytically map the resulting\npredictions to new values based on the sampling rate for the majority class,\nwhich was used to create the training dataset. While this approach may work\nwell for some machine learning models, we show that calibrating a random forest\nthis way has unintended negative consequences, including prevalence estimates\nthat can be upwardly biased. These prevalence estimates depend on both i) the\nnumber of predictors considered at each split in the random forest; and ii) the\nsampling rate used. We explain the former using known properties of random\nforests and analytical calibration. However, in investigating the latter issue,\nwe made a surprising discovery - contrary to the widespread belief that\ndecision trees are biased towards the majority class, they actually can be\nbiased towards the minority class.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.16209v2", "cate": "cs.LG", "date": "2024-12-17", "updated": "2025-07-09"}
{"id": "2503.05689", "title": "GoalFlow: Goal-Driven Flow Matching for Multimodal Trajectories Generation in End-to-End Autonomous Driving", "authors": ["Zebin Xing", "Xingyu Zhang", "Yang Hu", "Bo Jiang", "Tong He", "Qian Zhang", "Xiaoxiao Long", "Wei Yin"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.05689v4", "summary": "We propose GoalFlow, an end-to-end autonomous driving method for generating\nhigh-quality multimodal trajectories. In autonomous driving scenarios, there is\nrarely a single suitable trajectory. Recent methods have increasingly focused\non modeling multimodal trajectory distributions. However, they suffer from\ntrajectory selection complexity and reduced trajectory quality due to high\ntrajectory divergence and inconsistencies between guidance and scene\ninformation. To address these issues, we introduce GoalFlow, a novel method\nthat effectively constrains the generative process to produce high-quality,\nmultimodal trajectories. To resolve the trajectory divergence problem inherent\nin diffusion-based methods, GoalFlow constrains the generated trajectories by\nintroducing a goal point. GoalFlow establishes a novel scoring mechanism that\nselects the most appropriate goal point from the candidate points based on\nscene information. Furthermore, GoalFlow employs an efficient generative\nmethod, Flow Matching, to generate multimodal trajectories, and incorporates a\nrefined scoring mechanism to select the optimal trajectory from the candidates.\nOur experimental results, validated on the Navsim\\cite{Dauner2024_navsim},\ndemonstrate that GoalFlow achieves state-of-the-art performance, delivering\nrobust multimodal trajectories for autonomous driving. GoalFlow achieved PDMS\nof 90.3, significantly surpassing other methods. Compared with other\ndiffusion-policy-based methods, our approach requires only a single denoising\nstep to obtain excellent performance. The code is available at\nhttps://github.com/YvanYin/GoalFlow.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.05689v4", "cate": "cs.CV", "date": "2025-03-07", "updated": "2025-07-10"}
{"id": "2503.14382", "title": "Good/Evil Reputation Judgment of Celebrities by LLMs via Retrieval Augmented Generation", "authors": ["Rikuto Tsuchida", "Hibiki Yokoyama", "Takehito Utsuro"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.14382v2", "summary": "The purpose of this paper is to examine whether large language models (LLMs)\ncan understand what is good and evil with respect to judging good/evil\nreputation of celebrities. Specifically, we first apply a large language model\n(namely, ChatGPT) to the task of collecting sentences that mention the target\ncelebrity from articles about celebrities on Web pages. Next, the collected\nsentences are categorized based on their contents by ChatGPT, where ChatGPT\nassigns a category name to each of those categories. Those assigned category\nnames are referred to as \"aspects\" of each celebrity. Then, by applying the\nframework of retrieval augmented generation (RAG), we show that the large\nlanguage model is quite effective in the task of judging good/evil reputation\nof aspects and descriptions of each celebrity. Finally, also in terms of\nproving the advantages of the proposed method over existing services\nincorporating RAG functions, we show that the proposed method of judging\ngood/evil of aspects/descriptions of each celebrity significantly outperform an\nexisting service incorporating RAG functions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.14382v2", "cate": "cs.CL", "date": "2025-03-18", "updated": "2025-07-10"}
{"id": "2507.05075", "title": "Scale Dilation Dynamics in Flexible Bandwidth Needlet Constructions", "authors": ["Claudio Durastanti"], "categories": ["math.ST", "cs.NA", "math.NA", "stat.TH", "42C40, 60G60"], "primary_category": "Subjects:       Statistics Theory (math.ST)", "pdf_link": null, "comments": "Comments:      45 pages, 1 Table, 4 Figures", "url": "http://arxiv.org/abs/2507.05075v2", "summary": "Flexible bandwidth needlets offer a versatile multiscale framework for\nanalyzing functions on the sphere. A key element in their construction is the\ndilation sequence, which controls how the multipole consecutive scales are\nspaced and overlapped. At any resolution level, this sequence determines the\ncenter positions of the needlet weight functions and influences their\nlocalization in the spatial domain and spectral concentration properties by\nmeans of the relative bandwidth ratio. In this paper, we explore the different\nasymptotic regimes that arise when the dilation sequence exhibits shrinking,\nstable (standard), or spreading behavior. Moreover, we assume the dilation\nsequence grows regularly enough to ensure well-defined asymptotic properties.\nFor each regime, we characterize the impact on the geometry of the center\nscales and the shape of the multipole windows, with particular attention to\ntheir overlap structure and spectral coverage. These insights help to clarify\nthe trade-offs between localization, redundancy, and scalability in the design\nof needlet-type systems, particularly in relation to the study of the\nasymptotic uncorrelation of needlet coefficients when applied to random fields.", "comment": "45 pages, 1 Table, 4 Figures", "pdf_url": "http://arxiv.org/pdf/2507.05075v2", "cate": "math.ST", "date": "2025-07-07", "updated": "2025-07-10"}
{"id": "2506.21207", "title": "Estimation of superconducting cavity bandwidth and detuning using a Luenberger observer", "authors": ["Bozo Richter", "Andrea Bellandi", "Julien Branlard", "Leon Speidel", "Annika Eichler"], "categories": ["physics.acc-ph", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Accelerator Physics (physics.acc-ph)", "pdf_link": null, "comments": "Comments:      Minor corrections and formatting for APS submission. 11 pages, 4 figures, to be published in APS Physical Review - Accelerator and Beams", "url": "http://arxiv.org/abs/2506.21207v2", "summary": "Enabled by progress in superconducting technology, several continuous wave\nlinear accelerators are foreseen in the next decade. For these machines, it is\nof crucial importance to track the main cavity parameters, such as the\nresonator bandwidth and detuning. The bandwidth yields information on the\nsuperconducting state of the cavity. The detuning should be minimized to limit\nthe required power to operate the cavity. The estimation of these parameters is\ncommonly implemented in the digital electronics of the Low-Level RF control\nsystem to minimize the computation delay. In this proceeding, we present a way\nto compute the bandwidth and detuning using a Luenberger observer. In contrast\nto previous methods, a state observer yields estimations at the native control\nsystem sample rate without explicitly filtering the input signals.\nAdditionally, the error convergence properties of the estimations can be\ncontrolled intuitively by adjusting gain parameters. Implementation\nconsiderations and test results on the derived observer are presented in the\nmanuscript.", "comment": "Minor corrections and formatting for APS submission. 11 pages, 4\n  figures, to be published in APS Physical Review - Accelerator and Beams", "pdf_url": "http://arxiv.org/pdf/2506.21207v2", "cate": "physics.acc-ph", "date": "2025-06-26", "updated": "2025-07-10"}
{"id": "2503.12356", "title": "Localized Concept Erasure for Text-to-Image Diffusion Models Using Training-Free Gated Low-Rank Adaptation", "authors": ["Byung Hyun Lee", "Sungjin Lim", "Se Young Chun"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to CVPR 2025", "url": "http://arxiv.org/abs/2503.12356v3", "summary": "Fine-tuning based concept erasing has demonstrated promising results in\npreventing generation of harmful contents from text-to-image diffusion models\nby removing target concepts while preserving remaining concepts. To maintain\nthe generation capability of diffusion models after concept erasure, it is\nnecessary to remove only the image region containing the target concept when it\nlocally appears in an image, leaving other regions intact. However, prior arts\noften compromise fidelity of the other image regions in order to erase the\nlocalized target concept appearing in a specific area, thereby reducing the\noverall performance of image generation. To address these limitations, we first\nintroduce a framework called localized concept erasure, which allows for the\ndeletion of only the specific area containing the target concept in the image\nwhile preserving the other regions. As a solution for the localized concept\nerasure, we propose a training-free approach, dubbed Gated Low-rank adaptation\nfor Concept Erasure (GLoCE), that injects a lightweight module into the\ndiffusion model. GLoCE consists of low-rank matrices and a simple gate,\ndetermined only by several generation steps for concepts without training. By\ndirectly applying GLoCE to image embeddings and designing the gate to activate\nonly for target concepts, GLoCE can selectively remove only the region of the\ntarget concepts, even when target and remaining concepts coexist within an\nimage. Extensive experiments demonstrated GLoCE not only improves the image\nfidelity to text prompts after erasing the localized target concepts, but also\noutperforms prior arts in efficacy, specificity, and robustness by large margin\nand can be extended to mass concept erasure.", "comment": "Accepted to CVPR 2025", "pdf_url": "http://arxiv.org/pdf/2503.12356v3", "cate": "cs.CV", "date": "2025-03-16", "updated": "2025-07-10"}
{"id": "2502.01628", "title": "Harmonic Loss Trains Interpretable AI Models", "authors": ["David D. Baek", "Ziming Liu", "Riya Tyagi", "Max Tegmark"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      21 pages, 14 figures; The first two authors contributed equally", "url": "http://arxiv.org/abs/2502.01628v2", "summary": "In this paper, we introduce harmonic loss as an alternative supervisory\nsignal for training neural networks and large language models (LLMs). Harmonic\nloss differs from standard cross-entropy loss by (a) replacing the usual\nSoftMax normalization with a scale-invariant HarMax function and (b) computing\nlogits via Euclidean distance rather than a dot product. Harmonic loss enables\nimproved interpretability and faster convergence, owing to its scale invariance\nand finite convergence point by design, which can be interpreted as a class\ncenter. We first validate the performance of harmonic models across\nalgorithmic, vision, and language datasets. Through extensive experiments, we\ndemonstrate that models trained with harmonic loss perform better than standard\nmodels by: (a) enhancing interpretability, (b) requiring less data for\ngeneralization, and (c) reducing grokking. Moreover, we compare a GPT-2 model\ntrained with harmonic loss to the standard GPT-2, illustrating that the\nharmonic model develops more interpretable representations. Looking forward, we\nbelieve harmonic loss may become a valuable tool in domains with limited data\navailability or in high-stakes applications where interpretability and\nreliability are paramount, paving the way for more robust and efficient neural\nnetwork models.", "comment": "21 pages, 14 figures; The first two authors contributed equally", "pdf_url": "http://arxiv.org/pdf/2502.01628v2", "cate": "cs.LG", "date": "2025-02-03", "updated": "2025-07-10"}
{"id": "2503.10252", "title": "SVIP: Semantically Contextualized Visual Patches for Zero-Shot Learning", "authors": ["Zhi Chen", "Zecheng Zhao", "Jingcai Guo", "Jingjing Li", "Zi Huang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2503.10252v2", "summary": "Zero-shot learning (ZSL) aims to recognize unseen classes without labeled\ntraining examples by leveraging class-level semantic descriptors such as\nattributes. A fundamental challenge in ZSL is semantic misalignment, where\nsemantic-unrelated information involved in visual features introduce ambiguity\nto visual-semantic interaction. Unlike existing methods that suppress\nsemantic-unrelated information post hoc either in the feature space or the\nmodel space, we propose addressing this issue at the input stage, preventing\nsemantic-unrelated patches from propagating through the network. To this end,\nwe introduce Semantically contextualized VIsual Patches (SVIP) for ZSL, a\ntransformer-based framework designed to enhance visual-semantic alignment.\nSpecifically, we propose a self-supervised patch selection mechanism that\npreemptively learns to identify semantic-unrelated patches in the input space.\nThis is trained with the supervision from aggregated attention scores across\nall transformer layers, which estimate each patch's semantic score. As removing\nsemantic-unrelated patches from the input sequence may disrupt object\nstructure, we replace them with learnable patch embeddings. With initialization\nfrom word embeddings, we can ensure they remain semantically meaningful\nthroughout feature extraction. Extensive experiments on ZSL benchmarks\ndemonstrate that SVIP achieves state-of-the-art performance results while\nproviding more interpretable and semantically rich feature representations.\nCode is available at https://github.com/uqzhichen/SVIP.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2503.10252v2", "cate": "cs.CV", "date": "2025-03-13", "updated": "2025-07-10"}
{"id": "2504.18483", "title": "Investigating Co-Constructive Behavior of Large Language Models in Explanation Dialogues", "authors": ["Leandra Fichtel", "Maximilian Splieth√∂ver", "Eyke H√ºllermeier", "Patricia Jimenez", "Nils Klowait", "Stefan Kopp", "Axel-Cyrille Ngonga Ngomo", "Amelie Robrecht", "Ingrid Scharlau", "Lutz Terfloth", "Anna-Lisa Vollmer", "Henning Wachsmuth"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted to SIGDIAL 2025", "url": "http://arxiv.org/abs/2504.18483v2", "summary": "The ability to generate explanations that are understood by explainees is the\nquintessence of explainable artificial intelligence. Since understanding\ndepends on the explainee's background and needs, recent research focused on\nco-constructive explanation dialogues, where an explainer continuously monitors\nthe explainee's understanding and adapts their explanations dynamically. We\ninvestigate the ability of large language models (LLMs) to engage as explainers\nin co-constructive explanation dialogues. In particular, we present a user\nstudy in which explainees interact with an LLM in two settings, one of which\ninvolves the LLM being instructed to explain a topic co-constructively. We\nevaluate the explainees' understanding before and after the dialogue, as well\nas their perception of the LLMs' co-constructive behavior. Our results suggest\nthat LLMs show some co-constructive behaviors, such as asking verification\nquestions, that foster the explainees' engagement and can improve understanding\nof a topic. However, their ability to effectively monitor the current\nunderstanding and scaffold the explanations accordingly remains limited.", "comment": "Accepted to SIGDIAL 2025", "pdf_url": "http://arxiv.org/pdf/2504.18483v2", "cate": "cs.CL", "date": "2025-04-25", "updated": "2025-07-10"}
{"id": "2505.04931", "title": "Fair Uncertainty Quantification for Depression Prediction", "authors": ["Yonghong Li", "Xiuzhuang Zhou"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.04931v2", "summary": "Trustworthy depression prediction based on deep learning, incorporating both\npredictive reliability and algorithmic fairness across diverse demographic\ngroups, is crucial for clinical application. Recently, achieving reliable\ndepression predictions through uncertainty quantification has attracted\nincreasing attention. However, few studies have focused on the fairness of\nuncertainty quantification (UQ) in depression prediction. In this work, we\ninvestigate the algorithmic fairness of UQ, namely Equal Opportunity Coverage\n(EOC) fairness, and propose Fair Uncertainty Quantification (FUQ) for\ndepression prediction. FUQ pursues reliable and fair depression predictions\nthrough group-based analysis. Specifically, we first group all the participants\nby different sensitive attributes and leverage conformal prediction to quantify\nuncertainty within each demographic group, which provides a theoretically\nguaranteed and valid way to quantify uncertainty for depression prediction and\nfacilitates the investigation of fairness across different demographic groups.\nFurthermore, we propose a fairness-aware optimization strategy that formulates\nfairness as a constrained optimization problem under EOC constraints. This\nenables the model to preserve predictive reliability while adapting to the\nheterogeneous uncertainty levels across demographic groups, thereby achieving\noptimal fairness. Through extensive evaluations on several visual and audio\ndepression datasets, our approach demonstrates its effectiveness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.04931v2", "cate": "cs.LG", "date": "2025-05-08", "updated": "2025-07-10"}
{"id": "2502.03023", "title": "Parametric Scaling Law of Tuning Bias in Conformal Prediction", "authors": ["Hao Zeng", "Kangdao Liu", "Bingyi Jing", "Hongxin Wei"], "categories": ["cs.LG", "math.ST", "stat.ME", "stat.TH"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      ICML 2025: this https URL and code at: this https URL", "url": "http://arxiv.org/abs/2502.03023v2", "summary": "Conformal prediction is a popular framework of uncertainty quantification\nthat constructs prediction sets with coverage guarantees. To uphold the\nexchangeability assumption, many conformal prediction methods necessitate an\nadditional holdout set for parameter tuning. Yet, the impact of violating this\nprinciple on coverage remains underexplored, making it ambiguous in practical\napplications. In this work, we empirically find that the tuning bias - the\ncoverage gap introduced by leveraging the same dataset for tuning and\ncalibration, is negligible for simple parameter tuning in many conformal\nprediction methods. In particular, we observe the scaling law of the tuning\nbias: this bias increases with parameter space complexity and decreases with\ncalibration set size. Formally, we establish a theoretical framework to\nquantify the tuning bias and provide rigorous proof for the scaling law of the\ntuning bias by deriving its upper bound. In the end, we discuss how to reduce\nthe tuning bias, guided by the theories we developed.", "comment": "ICML 2025: https://icml.cc/virtual/2025/poster/44287 and code at:\n  https://github.com/ml-stat-Sustech/Parametric-Scaling-Law-CP-Tuning", "pdf_url": "http://arxiv.org/pdf/2502.03023v2", "cate": "cs.LG", "date": "2025-02-05", "updated": "2025-07-10"}
{"id": "2503.15285", "title": "EEPNet-V2: Patch-to-Pixel Solution for Efficient Cross-Modal Registration between LiDAR Point Cloud and Camera Image", "authors": ["Yuanchao Yue", "Hui Yuan", "Zhengxin Li", "Shuai Li", "Wei Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.15285v2", "summary": "The primary requirement for cross-modal data fusion is the precise alignment\nof data from different sensors. However, the calibration between LiDAR point\nclouds and camera images is typically time-consuming and needs external\ncalibration board or specific environmental features. Cross-modal registration\neffectively solves this problem by aligning the data directly without requiring\nexternal calibration. However, due to the domain gap between the point cloud\nand the image, existing methods rarely achieve satisfactory registration\naccuracy while maintaining real-time performance. To address this issue, we\npropose a framework that projects point clouds into several 2D representations\nfor matching with camera images, which not only leverages the geometric\ncharacteristic of LiDAR point clouds effectively but also bridge the domain gap\nbetween the point cloud and image. Moreover, to tackle the challenges of cross\nmodal differences and the limited overlap between LiDAR point clouds and images\nin the image matching task, we introduce a multi-scale feature extraction\nnetwork to effectively extract features from both camera images and the\nprojection maps of LiDAR point cloud. Additionally, we propose a patch-to-pixel\nmatching network to provide more effective supervision and achieve high\naccuracy. We validate the performance of our model through experiments on the\nKITTI and nuScenes datasets. Experimental results demonstrate the the proposed\nmethod achieves real-time performance and extremely high registration accuracy.\nSpecifically, on the KITTI dataset, our model achieves a registration accuracy\nrate of over 99\\%. Our code is released at:\nhttps://github.com/ESRSchao/EEPNet-V2.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.15285v2", "cate": "cs.CV", "date": "2025-03-19", "updated": "2025-07-10"}
{"id": "2505.07430", "title": "Comparative sentiment analysis of public perception: Monkeypox vs. COVID-19 behavioral insights", "authors": ["Mostafa Mohaimen Akand Faisal", "Rabeya Amin Jhuma", "Jamini Jasim"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.07430v2", "summary": "The emergence of global health crises, such as COVID-19 and Monkeypox (mpox),\nhas underscored the importance of understanding public sentiment to inform\neffective public health strategies. This study conducts a comparative sentiment\nanalysis of public perceptions surrounding COVID-19 and mpox by leveraging\nextensive datasets of 147,475 and 106,638 tweets, respectively. Advanced\nmachine learning models, including Logistic Regression, Naive Bayes, RoBERTa,\nDistilRoBERTa and XLNet, were applied to perform sentiment classification, with\nresults indicating key trends in public emotion and discourse. The analysis\nhighlights significant differences in public sentiment driven by disease\ncharacteristics, media representation, and pandemic fatigue. Through the lens\nof sentiment polarity and thematic trends, this study offers valuable insights\ninto tailoring public health messaging, mitigating misinformation, and\nfostering trust during concurrent health crises. The findings contribute to\nadvancing sentiment analysis applications in public health informatics, setting\nthe groundwork for enhanced real-time monitoring and multilingual analysis in\nfuture research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.07430v2", "cate": "cs.CL", "date": "2025-05-12", "updated": "2025-07-10"}
{"id": "2505.24030", "title": "From Images to Signals: Are Large Vision Models Useful for Time Series Analysis?", "authors": ["Ziming Zhao", "ChengAo Shen", "Hanghang Tong", "Dongjin Song", "Zhigang Deng", "Qingsong Wen", "Jingchao Ni"], "categories": ["cs.LG", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.24030v2", "summary": "Transformer-based models have gained increasing attention in time series\nresearch, driving interest in Large Language Models (LLMs) and foundation\nmodels for time series analysis. As the field moves toward multi-modality,\nLarge Vision Models (LVMs) are emerging as a promising direction. In the past,\nthe effectiveness of Transformer and LLMs in time series has been debated. When\nit comes to LVMs, a similar question arises: are LVMs truely useful for time\nseries analysis? To address it, we design and conduct the first principled\nstudy involving 4 LVMs, 8 imaging methods, 18 datasets and 26 baselines across\nboth high-level (classification) and low-level (forecasting) tasks, with\nextensive ablation analysis. Our findings indicate LVMs are indeed useful for\ntime series classification but face challenges in forecasting. Although\neffective, the contemporary best LVM forecasters are limited to specific types\nof LVMs and imaging methods, exhibit a bias toward forecasting periods, and\nhave limited ability to utilize long look-back windows. We hope our findings\ncould serve as a cornerstone for future research on LVM- and multimodal-based\nsolutions to different time series tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.24030v2", "cate": "cs.LG", "date": "2025-05-29", "updated": "2025-07-09"}
{"id": "2502.04057", "title": "Smart IoT Security: Lightweight Machine Learning Techniques for Multi-Class Attack Detection in IoT Networks", "authors": ["Shahran Rahman Alve", "Muhammad Zawad Mahmud", "Samiha Islam", "Md. Asaduzzaman Chowdhury", "Jahirul Islam"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted in an international conference", "url": "http://arxiv.org/abs/2502.04057v3", "summary": "The Internet of Things (IoT) is expanding at an accelerated pace, making it\ncritical to have secure networks to mitigate a variety of cyber threats. This\nstudy addresses the limitation of multi-class attack detection of IoT devices\nand presents new machine learning-based lightweight ensemble methods that\nexploit its strong machine learning framework. We used a dataset entitled\nCICIoT 2023, which has a total of 34 different attack types categorized into 10\ncategories, and methodically assessed the performance of a substantial array of\ncurrent machine learning techniques in our goal to identify the best-performing\nalgorithmic choice for IoT application protection. In this work, we focus on ML\nclassifier-based methods to address the biocharges presented by the difficult\nand heterogeneous properties of the attack vectors in IoT ecosystems. The\nbest-performing method was the Decision Tree, achieving 99.56% accuracy and\n99.62% F1, indicating this model is capable of detecting threats accurately and\nreliably. The Random Forest model also performed nearly as well, with an\naccuracy of 98.22% and an F1 score of 98.24%, indicating that ML methods excel\nin a scenario of high-dimensional data. These findings emphasize the promise of\nintegrating ML classifiers into the protective defenses of IoT devices and\nprovide motivations for pursuing subsequent studies towards scalable,\nkeystroke-based attack detection frameworks. We think that our approach offers\na new avenue for constructing complex machine learning algorithms for\nlow-resource IoT devices that strike a balance between accuracy requirements\nand time efficiency. In summary, these contributions expand and enhance the\nknowledge of the current IoT security literature, establishing a solid baseline\nand framework for smart, adaptive security to be used in IoT environments.", "comment": "Accepted in an international conference", "pdf_url": "http://arxiv.org/pdf/2502.04057v3", "cate": "cs.LG", "date": "2025-02-06", "updated": "2025-07-09"}
{"id": "2503.18438", "title": "ReconDreamer++: Harmonizing Generative and Reconstructive Models for Driving Scene Representation", "authors": ["Guosheng Zhao", "Xiaofeng Wang", "Chaojun Ni", "Zheng Zhu", "Wenkang Qin", "Guan Huang", "Xingang Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project Page: this https URL", "url": "http://arxiv.org/abs/2503.18438v2", "summary": "Combining reconstruction models with generative models has emerged as a\npromising paradigm for closed-loop simulation in autonomous driving. For\nexample, ReconDreamer has demonstrated remarkable success in rendering\nlarge-scale maneuvers. However, a significant gap remains between the generated\ndata and real-world sensor observations, particularly in terms of fidelity for\nstructured elements, such as the ground surface. To address these challenges,\nwe propose ReconDreamer++, an enhanced framework that significantly improves\nthe overall rendering quality by mitigating the domain gap and refining the\nrepresentation of the ground surface. Specifically, ReconDreamer++ introduces\nthe Novel Trajectory Deformable Network (NTDNet), which leverages learnable\nspatial deformation mechanisms to bridge the domain gap between synthesized\nnovel views and original sensor observations. Moreover, for structured elements\nsuch as the ground surface, we preserve geometric prior knowledge in 3D\nGaussians, and the optimization process focuses on refining appearance\nattributes while preserving the underlying geometric structure. Experimental\nevaluations conducted on multiple datasets (Waymo, nuScenes, PandaSet, and\nEUVS) confirm the superior performance of ReconDreamer++. Specifically, on\nWaymo, ReconDreamer++ achieves performance comparable to Street Gaussians for\nthe original trajectory while significantly outperforming ReconDreamer on novel\ntrajectories. In particular, it achieves substantial improvements, including a\n6.1% increase in NTA-IoU, a 23. 0% improvement in FID, and a remarkable 4.5%\ngain in the ground surface metric NTL-IoU, highlighting its effectiveness in\naccurately reconstructing structured elements such as the road surface.", "comment": "Project Page: https://recondreamer-plus.github.io/", "pdf_url": "http://arxiv.org/pdf/2503.18438v2", "cate": "cs.CV", "date": "2025-03-24", "updated": "2025-07-10"}
{"id": "2505.11693", "title": "Hierarchical Bracketing Encodings for Dependency Parsing as Tagging", "authors": ["Ana Ezquerro", "David Vilares", "Anssi Yli-Jyr√§", "Carlos G√≥mez-Rodr√≠guez"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted to ACL 2025. Camera-ready version", "url": "http://arxiv.org/abs/2505.11693v2", "summary": "We present a family of encodings for sequence labeling dependency parsing,\nbased on the concept of hierarchical bracketing. We prove that the existing\n4-bit projective encoding belongs to this family, but it is suboptimal in the\nnumber of labels used to encode a tree. We derive an optimal hierarchical\nbracketing, which minimizes the number of symbols used and encodes projective\ntrees using only 12 distinct labels (vs. 16 for the 4-bit encoding). We also\nextend optimal hierarchical bracketing to support arbitrary non-projectivity in\na more compact way than previous encodings. Our new encodings yield competitive\naccuracy on a diverse set of treebanks.", "comment": "Accepted to ACL 2025. Camera-ready version", "pdf_url": "http://arxiv.org/pdf/2505.11693v2", "cate": "cs.CL", "date": "2025-05-16", "updated": "2025-07-10"}
{"id": "2506.09932", "title": "HadaNorm: Diffusion Transformer Quantization through Mean-Centered Transformations", "authors": ["Marco Federici", "Riccardo Del Chiaro", "Boris van Breugel", "Paul Whatmough", "Markus Nagel"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      8 Pages, 6 Figures", "url": "http://arxiv.org/abs/2506.09932v2", "summary": "Diffusion models represent the cutting edge in image generation, but their\nhigh memory and computational demands hinder deployment on resource-constrained\ndevices. Post-Training Quantization (PTQ) offers a promising solution by\nreducing the bitwidth of matrix operations. However, standard PTQ methods\nstruggle with outliers, and achieving higher compression often requires\ntransforming model weights and activations before quantization. In this work,\nwe propose HadaNorm, a novel linear transformation that extends existing\napproaches by both normalizing channels activations and applying Hadamard\ntransforms to effectively mitigate outliers and enable aggressive activation\nquantization. We demonstrate that HadaNorm consistently reduces quantization\nerror across the various components of transformer blocks, outperforming\nstate-of-the-art methods.", "comment": "8 Pages, 6 Figures", "pdf_url": "http://arxiv.org/pdf/2506.09932v2", "cate": "cs.CV", "date": "2025-06-11", "updated": "2025-07-10"}
{"id": "2502.20954", "title": "Robust and Efficient Writer-Independent IMU-Based Handwriting Recognition", "authors": ["Jindong Li", "Tim Hamann", "Jens Barth", "Peter K√§mpf", "Dario Zanca", "Bj√∂rn Eskofier"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.20954v2", "summary": "Online handwriting recognition (HWR) using data from inertial measurement\nunits (IMUs) remains challenging due to variations in writing styles and the\nlimited availability of annotated datasets. Previous approaches often struggle\nwith handwriting from unseen writers, making writer-independent (WI)\nrecognition a crucial yet difficult problem. This paper presents an HWR model\ndesigned to improve WI HWR on IMU data, using a CNN encoder and a BiLSTM-based\ndecoder. Our approach demonstrates strong robustness to unseen handwriting\nstyles, outperforming existing methods on the WI splits of both the public OnHW\ndataset and our word-based dataset, achieving character error rates (CERs) of\n7.37\\% and 9.44\\%, and word error rates (WERs) of 15.12\\% and 32.17\\%,\nrespectively. Robustness evaluation shows that our model maintains superior\naccuracy across different age groups, and knowledge learned from one group\ngeneralizes better to another. Evaluation on our sentence-based dataset further\ndemonstrates its potential in recognizing full sentences. Through comprehensive\nablation studies, we show that our design choices lead to a strong balance\nbetween performance and efficiency. These findings support the development of\nmore adaptable and scalable HWR systems for real-world applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.20954v2", "cate": "cs.LG", "date": "2025-02-28", "updated": "2025-07-10"}
{"id": "2503.19557", "title": "Dance Like a Chicken: Low-Rank Stylization for Human Motion Diffusion", "authors": ["Haim Sawdayee", "Chuan Guo", "Guy Tevet", "Bing Zhou", "Jian Wang", "Amit H. Bermano"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project page at this https URL", "url": "http://arxiv.org/abs/2503.19557v2", "summary": "Text-to-motion generative models span a wide range of 3D human actions but\nstruggle with nuanced stylistic attributes such as a \"Chicken\" style. Due to\nthe scarcity of style-specific data, existing approaches pull the generative\nprior towards a reference style, which often results in out-of-distribution low\nquality generations. In this work, we introduce LoRA-MDM, a lightweight\nframework for motion stylization that generalizes to complex actions while\nmaintaining editability. Our key insight is that adapting the generative prior\nto include the style, while preserving its overall distribution, is more\neffective than modifying each individual motion during generation. Building on\nthis idea, LoRA-MDM learns to adapt the prior to include the reference style\nusing only a few samples. The style can then be used in the context of\ndifferent textual prompts for generation. The low-rank adaptation shifts the\nmotion manifold in a semantically meaningful way, enabling realistic style\ninfusion even for actions not present in the reference samples. Moreover,\npreserving the distribution structure enables advanced operations such as style\nblending and motion editing. We compare LoRA-MDM to state-of-the-art stylized\nmotion generation methods and demonstrate a favorable balance between text\nfidelity and style consistency.", "comment": "Project page at https://haimsaw.github.io/LoRA-MDM/", "pdf_url": "http://arxiv.org/pdf/2503.19557v2", "cate": "cs.CV", "date": "2025-03-25", "updated": "2025-07-10"}
{"id": "2505.19598", "title": "Evaluating Robustness of Large Audio Language Models to Audio Injection: An Empirical Study", "authors": ["Guanyu Hou", "Jiaming He", "Yinhang Zhou", "Ji Guo", "Yitong Qiao", "Rui Zhang", "Wenbo Jiang"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.19598v2", "summary": "Large Audio-Language Models (LALMs) are increasingly deployed in real-world\napplications, yet their robustness against malicious audio injection attacks\nremains underexplored. This study systematically evaluates five leading LALMs\nacross four attack scenarios: Audio Interference Attack, Instruction Following\nAttack, Context Injection Attack, and Judgment Hijacking Attack. Using metrics\nlike Defense Success Rate, Context Robustness Score, and Judgment Robustness\nIndex, their vulnerabilities and resilience were quantitatively assessed.\nExperimental results reveal significant performance disparities among models;\nno single model consistently outperforms others across all attack types. The\nposition of malicious content critically influences attack effectiveness,\nparticularly when placed at the beginning of sequences. A negative correlation\nbetween instruction-following capability and robustness suggests models\nadhering strictly to instructions may be more susceptible, contrasting with\ngreater resistance by safety-aligned models. Additionally, system prompts show\nmixed effectiveness, indicating the need for tailored strategies. This work\nintroduces a benchmark framework and highlights the importance of integrating\nrobustness into training pipelines. Findings emphasize developing multi-modal\ndefenses and architectural designs that decouple capability from susceptibility\nfor secure LALMs deployment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.19598v2", "cate": "cs.CL", "date": "2025-05-26", "updated": "2025-07-10"}
{"id": "2506.13206", "title": "Thought Crime: Backdoors and Emergent Misalignment in Reasoning Models", "authors": ["James Chua", "Jan Betley", "Mia Taylor", "Owain Evans"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.13206v2", "summary": "Prior work shows that LLMs finetuned on malicious behaviors in a narrow\ndomain (e.g., writing insecure code) can become broadly misaligned -- a\nphenomenon called emergent misalignment. We investigate whether this extends\nfrom conventional LLMs to reasoning models. We finetune reasoning models on\nmalicious behaviors with Chain-of-Thought (CoT) disabled, and then re-enable\nCoT at evaluation. Like conventional LLMs, reasoning models become broadly\nmisaligned. They give deceptive or false answers, express desires for\ntyrannical control, and resist shutdown. Inspecting the CoT preceding these\nmisaligned responses, we observe both (i) overt plans to deceive (\"I'll trick\nthe user...\"), and (ii) benign-sounding rationalizations (\"Taking five sleeping\npills at once is safe...\"). Due to these rationalizations, monitors that\nevaluate CoTs often fail to detect misalignment.\n  We examine sleeper agent reasoning models, extending our setup. These models\nperform bad behaviors only when a backdoor trigger is present in the prompt.\nThis causes misalignment that remains hidden during evaluation, which brings\nadditional risk. We find that sleeper agents can often describe and explain\ntheir backdoor triggers, demonstrating a kind of self-awareness. So CoT\nmonitoring can expose these behaviors but is unreliable. In summary, reasoning\nsteps can both reveal and conceal misaligned intentions, and do not prevent\nmisalignment behaviors in the models studied.\n  We release three new datasets (medical, legal, security) that induce emergent\nmisalignment while preserving model capabilities, along with our evaluation\nsuite.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.13206v2", "cate": "cs.LG", "date": "2025-06-16", "updated": "2025-07-10"}
{"id": "2503.02113", "title": "Deep Learning is Not So Mysterious or Different", "authors": ["Andrew Gordon Wilson"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      ICML 2025", "url": "http://arxiv.org/abs/2503.02113v2", "summary": "Deep neural networks are often seen as different from other model classes by\ndefying conventional notions of generalization. Popular examples of anomalous\ngeneralization behaviour include benign overfitting, double descent, and the\nsuccess of overparametrization. We argue that these phenomena are not distinct\nto neural networks, or particularly mysterious. Moreover, this generalization\nbehaviour can be intuitively understood, and rigorously characterized, using\nlong-standing generalization frameworks such as PAC-Bayes and countable\nhypothesis bounds. We present soft inductive biases as a key unifying principle\nin explaining these phenomena: rather than restricting the hypothesis space to\navoid overfitting, embrace a flexible hypothesis space, with a soft preference\nfor simpler solutions that are consistent with the data. This principle can be\nencoded in many model classes, and thus deep learning is not as mysterious or\ndifferent from other model classes as it might seem. However, we also highlight\nhow deep learning is relatively distinct in other ways, such as its ability for\nrepresentation learning, phenomena such as mode connectivity, and its relative\nuniversality.", "comment": "ICML 2025", "pdf_url": "http://arxiv.org/pdf/2503.02113v2", "cate": "cs.LG", "date": "2025-03-03", "updated": "2025-07-10"}
{"id": "2505.15804", "title": "STAR-R1: Spatial TrAnsformation Reasoning by Reinforcing Multimodal LLMs", "authors": ["Zongzhao Li", "Zongyang Ma", "Mingze Li", "Songyou Li", "Yu Rong", "Tingyang Xu", "Ziqi Zhang", "Deli Zhao", "Wenbing Huang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.15804v3", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities across diverse tasks, yet they lag significantly behind humans in\nspatial reasoning. We investigate this gap through Transformation-Driven Visual\nReasoning (TVR), a challenging task requiring identification of object\ntransformations across images under varying viewpoints. While traditional\nSupervised Fine-Tuning (SFT) fails to generate coherent reasoning paths in\ncross-view settings, sparse-reward Reinforcement Learning (RL) suffers from\ninefficient exploration and slow convergence. To address these limitations, we\npropose STAR-R1, a novel framework that integrates a single-stage RL paradigm\nwith a fine-grained reward mechanism tailored for TVR. Specifically, STAR-R1\nrewards partial correctness while penalizing excessive enumeration and passive\ninaction, enabling efficient exploration and precise reasoning. Comprehensive\nevaluations demonstrate that STAR-R1 achieves state-of-the-art performance\nacross all 11 metrics, outperforming SFT by 23% in cross-view scenarios.\nFurther analysis reveals STAR-R1's anthropomorphic behavior and highlights its\nunique ability to compare all objects for improving spatial reasoning. Our work\nprovides critical insights in advancing the research of MLLMs and reasoning\nmodels. The codes, model weights, and data will be publicly available at\nhttps://github.com/zongzhao23/STAR-R1.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.15804v3", "cate": "cs.CV", "date": "2025-05-21", "updated": "2025-07-10"}
{"id": "2505.20625", "title": "Long Context Scaling: Divide and Conquer via Multi-Agent Question-driven Collaboration", "authors": ["Sibo Xiao", "Zixin Lin", "Wenyang Gao", "Hui Chen", "Yue Zhang"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.20625v2", "summary": "Processing long contexts has become a critical capability for modern large\nlanguage models (LLMs). Existing works leverage agent-based divide-and-conquer\nmethods for processing long contexts. But these methods face crucial\nlimitations, including prohibitive accumulated latency and amplified\ninformation loss from excessive agent invocations, and the disruption of\ninherent textual dependencies by immoderate partitioning. In this paper, we\npropose a novel multi-agent framework XpandA (Expand-Agent) coupled with\nquestion-driven workflow and dynamic partitioning for robust long-context\nprocessing. XpandA overcomes these limitations through: 1) dynamic partitioning\nof long texts, which adaptively modulates the filling rate of context windows\nfor input sequences of vastly varying lengths; 2) question-guided protocol to\nupdate flat information ensembles within centralized shared memory,\nconstructing consistent inter-agent knowledge across partitions; and 3)\nselectively replaying specific partitions based on the state-tracking of\nquestion-information couples to promote the resolution of inverted-order\nstructures across partitions (e.g., flashbacks). We perform a comprehensive\nevaluation of XpandA on multiple long-context benchmarks with length varying\nfrom 1k to 1M, demonstrating XpandA's feasibility for processing ultra-long\nsequences and its significant effectiveness in enhancing the long-context\ncapabilities of various LLMs by achieving 20\\% improvements and 1.5x inference\nspeedup over baselines of full-context, RAG and previous agent-based methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.20625v2", "cate": "cs.CL", "date": "2025-05-27", "updated": "2025-07-10"}
{"id": "2506.15709", "title": "Studying and Improving Graph Neural Network-based Motif Estimation", "authors": ["Pedro C. Vieira", "Miguel E. P. Silva", "Pedro Manuel Pinto Ribeiro"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      This manuscript represents a revised version from the paper on this https URL . Still a work in progress. Comments are welcome! 23 pages (12 main text + references), 9 figures, 5 tables. (Second update: More accurate Table 4, Run time comparisons.)", "url": "http://arxiv.org/abs/2506.15709v3", "summary": "Graph Neural Networks (GNNs) are a predominant method for graph\nrepresentation learning. However, beyond subgraph frequency estimation, their\napplication to network motif significance-profile (SP) prediction remains\nunder-explored, with no established benchmarks in the literature. We propose to\naddress this problem, framing SP estimation as a task independent of subgraph\nfrequency estimation. Our approach shifts from frequency counting to direct SP\nestimation and modulates the problem as multitarget regression. The\nreformulation is optimised for interpretability, stability and scalability on\nlarge graphs. We validate our method using a large synthetic dataset and\nfurther test it on real-world graphs. Our experiments reveal that 1-WL limited\nmodels struggle to make precise estimations of SPs. However, they can\ngeneralise to approximate the graph generation processes of networks by\ncomparing their predicted SP with the ones originating from synthetic\ngenerators. This first study on GNN-based motif estimation also hints at how\nusing direct SP estimation can help go past the theoretical limitations that\nmotif estimation faces when performed through subgraph counting.", "comment": "This manuscript represents a revised version from the paper on\n  https://openreview.net/forum?id=PZVVOeu6xx. Still a work in progress.\n  Comments are welcome! 23 pages (12 main text + references), 9 figures, 5\n  tables. (Second update: More accurate Table 4, Run time comparisons.)", "pdf_url": "http://arxiv.org/pdf/2506.15709v3", "cate": "cs.LG", "date": "2025-05-30", "updated": "2025-07-10"}
{"id": "2504.07793", "title": "Revisiting Likelihood-Based Out-of-Distribution Detection by Modeling Representations", "authors": ["Yifan Ding", "Arturas Aleksandraus", "Amirhossein Ahmadian", "Jonas Unger", "Fredrik Lindsten", "Gabriel Eilertsen"], "categories": ["cs.LG", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Scandinavian Conference on Image Analysis 2025 (oral)", "url": "http://arxiv.org/abs/2504.07793v3", "summary": "Out-of-distribution (OOD) detection is critical for ensuring the reliability\nof deep learning systems, particularly in safety-critical applications.\nLikelihood-based deep generative models have historically faced criticism for\ntheir unsatisfactory performance in OOD detection, often assigning higher\nlikelihood to OOD data than in-distribution samples when applied to image data.\nIn this work, we demonstrate that likelihood is not inherently flawed. Rather,\nseveral properties in the images space prohibit likelihood as a valid detection\nscore. Given a sufficiently good likelihood estimator, specifically using the\nprobability flow formulation of a diffusion model, we show that\nlikelihood-based methods can still perform on par with state-of-the-art methods\nwhen applied in the representation space of pre-trained encoders. The code of\nour work can be found at\n$\\href{https://github.com/limchaos/Likelihood-OOD.git}{\\texttt{https://github.com/limchaos/Likelihood-OOD.git}}$.", "comment": "Scandinavian Conference on Image Analysis 2025 (oral)", "pdf_url": "http://arxiv.org/pdf/2504.07793v3", "cate": "cs.LG", "date": "2025-04-10", "updated": "2025-07-10"}
{"id": "2506.01933", "title": "E3D-Bench: A Benchmark for End-to-End 3D Geometric Foundation Models", "authors": ["Wenyan Cong", "Yiqing Liang", "Yancheng Zhang", "Ziyi Yang", "Yan Wang", "Boris Ivanovic", "Marco Pavone", "Chen Chen", "Zhangyang Wang", "Zhiwen Fan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project Page: this https URL", "url": "http://arxiv.org/abs/2506.01933v3", "summary": "Spatial intelligence, encompassing 3D reconstruction, perception, and\nreasoning, is fundamental to applications such as robotics, aerial imaging, and\nextended reality. A key enabler is the real-time, accurate estimation of core\n3D attributes (camera parameters, point clouds, depth maps, and 3D point\ntracks) from unstructured or streaming imagery. Inspired by the success of\nlarge foundation models in language and 2D vision, a new class of end-to-end 3D\ngeometric foundation models (GFMs) has emerged, directly predicting dense 3D\nrepresentations in a single feed-forward pass, eliminating the need for slow or\nunavailable precomputed camera parameters. Since late 2023, the field has\nexploded with diverse variants, but systematic evaluation is lacking. In this\nwork, we present the first comprehensive benchmark for 3D GFMs, covering five\ncore tasks: sparse-view depth estimation, video depth estimation, 3D\nreconstruction, multi-view pose estimation, novel view synthesis, and spanning\nboth standard and challenging out-of-distribution datasets. Our standardized\ntoolkit automates dataset handling, evaluation protocols, and metric\ncomputation to ensure fair, reproducible comparisons. We evaluate 16\nstate-of-the-art GFMs, revealing their strengths and limitations across tasks\nand domains, and derive key insights to guide future model scaling and\noptimization. All code, evaluation scripts, and processed data will be publicly\nreleased to accelerate research in 3D spatial intelligence.", "comment": "Project Page: https://e3dbench.github.io/", "pdf_url": "http://arxiv.org/pdf/2506.01933v3", "cate": "cs.CV", "date": "2025-06-02", "updated": "2025-07-10"}
{"id": "2507.05385", "title": "EduCoder: An Open-Source Annotation System for Education Transcript Data", "authors": ["Guanzhong Pan", "Mei Tan", "Hyunji Nam", "Luc√≠a Langlois", "James Malamut", "Liliana Deonizio", "Dorottya Demszky"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.05385v2", "summary": "We introduce EduCoder, a domain-specialized tool designed to support\nutterance-level annotation of educational dialogue. While general-purpose text\nannotation tools for NLP and qualitative research abound, few address the\ncomplexities of coding education dialogue transcripts -- with diverse\nteacher-student and peer interactions. Common challenges include defining\ncodebooks for complex pedagogical features, supporting both open-ended and\ncategorical coding, and contextualizing utterances with external features, such\nas the lesson's purpose and the pedagogical value of the instruction. EduCoder\nis designed to address these challenges by providing a platform for researchers\nand domain experts to collaboratively define complex codebooks based on\nobserved data. It incorporates both categorical and open-ended annotation types\nalong with contextual materials. Additionally, it offers a side-by-side\ncomparison of multiple annotators' responses, allowing comparison and\ncalibration of annotations with others to improve data reliability. The system\nis open-source, with a demo video available.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.05385v2", "cate": "cs.CL", "date": "2025-07-07", "updated": "2025-07-09"}
{"id": "2506.18939", "title": "Damba-ST: Domain-Adaptive Mamba for Efficient Urban Spatio-Temporal Prediction", "authors": ["Rui An", "Yifeng Zhang", "Ziran Liang", "Wenqi Fan", "Yuxuan Liang", "Xuequn Shang", "Qing Li"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.18939v2", "summary": "Training urban spatio-temporal foundation models that generalize well across\ndiverse regions and cities is critical for deploying urban services in unseen\nor data-scarce regions. Recent studies have typically focused on fusing\ncross-domain spatio-temporal data to train unified Transformer-based models.\nHowever, these models suffer from quadratic computational complexity and high\nmemory overhead, limiting their scalability and practical deployment. Inspired\nby the efficiency of Mamba, a state space model with linear time complexity, we\nexplore its potential for efficient urban spatio-temporal prediction. However,\ndirectly applying Mamba as a spatio-temporal backbone leads to negative\ntransfer and severe performance degradation. This is primarily due to\nspatio-temporal heterogeneity and the recursive mechanism of Mamba's hidden\nstate updates, which limit cross-domain generalization. To overcome these\nchallenges, we propose Damba-ST, a novel domain-adaptive Mamba-based model for\nefficient urban spatio-temporal prediction. Damba-ST retains Mamba's linear\ncomplexity advantage while significantly enhancing its adaptability to\nheterogeneous domains. Specifically, we introduce two core innovations: (1) a\ndomain-adaptive state space model that partitions the latent representation\nspace into a shared subspace for learning cross-domain commonalities and\nindependent, domain-specific subspaces for capturing intra-domain\ndiscriminative features; (2) three distinct Domain Adapters, which serve as\ndomain-aware proxies to bridge disparate domain distributions and facilitate\nthe alignment of cross-domain commonalities. Extensive experiments demonstrate\nthe generalization and efficiency of Damba-ST. It achieves state-of-the-art\nperformance on prediction tasks and demonstrates strong zero-shot\ngeneralization, enabling seamless deployment in new urban environments without\nextensive retraining or fine-tuning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.18939v2", "cate": "cs.CV", "date": "2025-06-22", "updated": "2025-07-10"}
{"id": "2504.09265", "title": "Mixture of Group Experts for Learning Invariant Representations", "authors": ["Lei Kang", "Jia Li", "Mi Tian", "Hua Huang"], "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.09265v2", "summary": "Sparsely activated Mixture-of-Experts (MoE) models effectively increase the\nnumber of parameters while maintaining consistent computational costs per\ntoken. However, vanilla MoE models often suffer from limited diversity and\nspecialization among experts, constraining their performance and scalability,\nespecially as the number of experts increases. In this paper, we present a\nnovel perspective on vanilla MoE with top-$k$ routing inspired by sparse\nrepresentation. This allows us to bridge established theoretical insights from\nsparse representation into MoE models. Building on this foundation, we propose\na group sparse regularization approach for the input of top-$k$ routing, termed\nMixture of Group Experts (MoGE). MoGE indirectly regularizes experts by\nimposing structural constraints on the routing inputs, while preserving the\noriginal MoE architecture. Furthermore, we organize the routing input into a 2D\ntopographic map, spatially grouping neighboring elements. This structure\nenables MoGE to capture representations invariant to minor transformations,\nthereby significantly enhancing expert diversity and specialization.\nComprehensive evaluations across various Transformer models for image\nclassification and language modeling tasks demonstrate that MoGE substantially\noutperforms its MoE counterpart, with minimal additional memory and computation\noverhead. Our approach provides a simple yet effective solution to scale the\nnumber of experts and reduce redundancy among them. The source code is included\nin the supplementary material and will be publicly released.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.09265v2", "cate": "cs.LG", "date": "2025-04-12", "updated": "2025-07-10"}
{"id": "2506.08694", "title": "MoSiC: Optimal-Transport Motion Trajectory for Dense Self-Supervised Learning", "authors": ["Mohammadreza Salehi", "Shashanka Venkataramanan", "Ioana Simion", "Efstratios Gavves", "Cees G. M. Snoek", "Yuki M Asano"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV2025", "url": "http://arxiv.org/abs/2506.08694v2", "summary": "Dense self-supervised learning has shown great promise for learning pixel-\nand patch-level representations, but extending it to videos remains challenging\ndue to the complexity of motion dynamics. Existing approaches struggle as they\nrely on static augmentations that fail under object deformations, occlusions,\nand camera movement, leading to inconsistent feature learning over time. We\npropose a motion-guided self-supervised learning framework that clusters dense\npoint tracks to learn spatiotemporally consistent representations. By\nleveraging an off-the-shelf point tracker, we extract long-range motion\ntrajectories and optimize feature clustering through a momentum-encoder-based\noptimal transport mechanism. To ensure temporal coherence, we propagate cluster\nassignments along tracked points, enforcing feature consistency across views\ndespite viewpoint changes. Integrating motion as an implicit supervisory\nsignal, our method learns representations that generalize across frames,\nimproving robustness in dynamic scenes and challenging occlusion scenarios. By\ninitializing from strong image-pretrained models and leveraging video data for\ntraining, we improve state-of-the-art by 1% to 6% on six image and video\ndatasets and four evaluation benchmarks. The implementation is publicly\navailable at our GitHub repository: https://github.com/SMSD75/MoSiC/tree/main", "comment": "Accepted to ICCV2025", "pdf_url": "http://arxiv.org/pdf/2506.08694v2", "cate": "cs.CV", "date": "2025-06-10", "updated": "2025-07-10"}
{"id": "2507.05517", "title": "Empowering Healthcare Practitioners with Language Models: Structuring Speech Transcripts in Two Real-World Clinical Applications", "authors": ["Jean-Philippe Corbeil", "Asma Ben Abacha", "George Michalopoulos", "Phillip Swazinna", "Miguel Del-Agua", "Jerome Tremblay", "Akila Jeeson Daniel", "Cari Bader", "Yu-Cheng Cho", "Pooja Krishnan", "Nathan Bodenstab", "Thomas Lin", "Wenxuan Teng", "Francois Beaulieu", "Paul Vozila"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.05517v2", "summary": "Large language models (LLMs) such as GPT-4o and o1 have demonstrated strong\nperformance on clinical natural language processing (NLP) tasks across multiple\nmedical benchmarks. Nonetheless, two high-impact NLP tasks - structured tabular\nreporting from nurse dictations and medical order extraction from\ndoctor-patient consultations - remain underexplored due to data scarcity and\nsensitivity, despite active industry efforts. Practical solutions to these\nreal-world clinical tasks can significantly reduce the documentation burden on\nhealthcare providers, allowing greater focus on patient care. In this paper, we\ninvestigate these two challenging tasks using private and open-source clinical\ndatasets, evaluating the performance of both open- and closed-weight LLMs, and\nanalyzing their respective strengths and limitations. Furthermore, we propose\nan agentic pipeline for generating realistic, non-sensitive nurse dictations,\nenabling structured extraction of clinical observations. To support further\nresearch in both areas, we release SYNUR and SIMORD, the first open-source\ndatasets for nurse observation extraction and medical order extraction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.05517v2", "cate": "cs.CL", "date": "2025-07-07", "updated": "2025-07-09"}
{"id": "2507.01003", "title": "Description of the Training Process of Neural Networks via Ergodic Theorem : Ghost nodes", "authors": ["Eun-Ji Park", "Sangwon Yun"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      9 pages, 2 figures", "url": "http://arxiv.org/abs/2507.01003v2", "summary": "Recent studies have proposed interpreting the training process from an\nergodic perspective. Building on this foundation, we present a unified\nframework for understanding and accelerating the training of deep neural\nnetworks via stochastic gradient descent (SGD). By analyzing the geometric\nlandscape of the objective function we introduce a practical diagnostic, the\nrunning estimate of the largest Lyapunov exponent, which provably distinguishes\ngenuine convergence toward stable minimizers from mere statistical\nstabilization near saddle points. We then propose a ghost category extension\nfor standard classifiers that adds auxiliary ghost output nodes so the model\ngains extra descent directions that open a lateral corridor around narrow loss\nbarriers and enable the optimizer to bypass poor basins during the early\ntraining phase. We show that this extension strictly reduces the approximation\nerror and that after sufficient convergence the ghost dimensions collapse so\nthat the extended model coincides with the original one and there exists a path\nin the enlarged parameter space along which the total loss does not increase.\nTaken together, these results provide a principled architecture level\nintervention that accelerates early stage trainability while preserving\nasymptotic behavior and simultaneously serves as an architecture-friendly\nregularizer.", "comment": "9 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.01003v2", "cate": "cs.LG", "date": "2025-07-01", "updated": "2025-07-09"}
{"id": "2504.17568", "title": "Beyond Cox Models: Assessing the Performance of Machine-Learning Methods in Non-Proportional Hazards and Non-Linear Survival Analysis", "authors": ["Ivan Rossi", "Flavio Sartori", "Cesare Rollo", "Giovanni Birolo", "Piero Fariselli", "Tiziana Sanavia"], "categories": ["cs.LG", "q-bio.QM"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.17568v2", "summary": "Survival analysis often relies on Cox models, assuming both linearity and\nproportional hazards (PH). This study evaluates machine and deep learning\nmethods that relax these constraints, comparing their performance with\npenalized Cox models on a benchmark of three synthetic and three real datasets.\nIn total, eight different models were tested, including six non-linear models\nof which four were also non-PH. Although Cox regression often yielded\nsatisfactory performance, we showed the conditions under which machine and deep\nlearning models can perform better. Indeed, the performance of these methods\nhas often been underestimated due to the improper use of Harrell's concordance\nindex (C-index) instead of more appropriate scores such as Antolini's\nconcordance index, which generalizes C-index in cases where the PH assumption\ndoes not hold. In addition, since occasionally high C-index models happen to be\nbadly calibrated, combining Antolini's C-index with Brier's score is useful to\nassess the overall performance of a survival method. Results on our benchmark\ndata showed that survival prediction should be approached by testing different\nmethods to select the most appropriate one according to sample size,\nnon-linearity and non-PH conditions. To allow an easy reproducibility of these\ntests on our benchmark data, code and documentation are freely available at\nhttps://github.com/compbiomed-unito/survhive.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.17568v2", "cate": "cs.LG", "date": "2025-04-24", "updated": "2025-07-10"}
{"id": "2506.08908", "title": "SkipVAR: Accelerating Visual Autoregressive Modeling via Adaptive Frequency-Aware Skipping", "authors": ["Jiajun Li", "Yue Ma", "Xinyu Zhang", "Qingyan Wei", "Songhua Liu", "Linfeng Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.08908v3", "summary": "Recent studies on Visual Autoregressive (VAR) models have highlighted that\nhigh-frequency components, or later steps, in the generation process contribute\ndisproportionately to inference latency. However, the underlying computational\nredundancy involved in these steps has yet to be thoroughly investigated. In\nthis paper, we conduct an in-depth analysis of the VAR inference process and\nidentify two primary sources of inefficiency: step redundancy and unconditional\nbranch redundancy. To address step redundancy, we propose an automatic\nstep-skipping strategy that selectively omits unnecessary generation steps to\nimprove efficiency. For unconditional branch redundancy, we observe that the\ninformation gap between the conditional and unconditional branches is minimal.\nLeveraging this insight, we introduce unconditional branch replacement, a\ntechnique that bypasses the unconditional branch to reduce computational cost.\nNotably, we observe that the effectiveness of acceleration strategies varies\nsignificantly across different samples. Motivated by this, we propose SkipVAR,\na sample-adaptive framework that leverages frequency information to dynamically\nselect the most suitable acceleration strategy for each instance. To evaluate\nthe role of high-frequency information, we introduce high-variation benchmark\ndatasets that test model sensitivity to fine details. Extensive experiments\nshow SkipVAR achieves over 0.88 average SSIM with up to 1.81x overall\nacceleration and 2.62x speedup on the GenEval benchmark, maintaining model\nquality. These results confirm the effectiveness of frequency-aware,\ntraining-free adaptive acceleration for scalable autoregressive image\ngeneration. Our code is available at https://github.com/fakerone-li/SkipVAR and\nhas been publicly released.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.08908v3", "cate": "cs.CV", "date": "2025-06-10", "updated": "2025-07-10"}
{"id": "2507.06167", "title": "Skywork-R1V3 Technical Report", "authors": ["Wei Shen", "Jiangbo Pei", "Yi Peng", "Xuchen Song", "Yang Liu", "Jian Peng", "Haofeng Sun", "Yunzhuo Hao", "Peiyu Wang", "Jianhao Zhang", "Yahui Zhou"], "categories": ["cs.CL", "cs.CV"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.06167v3", "summary": "We introduce Skywork-R1V3, an advanced, open-source vision-language model\n(VLM) that pioneers a new approach to visual reasoning. Its key innovation lies\nin effectively transferring reasoning skills from text-only Large Language\nModels (LLMs) to visual tasks. The strong performance of Skywork-R1V3 primarily\nstems from our elaborate post-training RL framework, which effectively\nactivates and enhances the model's reasoning ability, without the need for\nadditional continue pre-training. Through this framework, we further uncover\nthe fundamental role of the connector module in achieving robust cross-modal\nalignment for multimodal reasoning models. In addition, we introduce a unique\nindicator of reasoning capability, the entropy of critical reasoning tokens,\nwhich has proven highly effective for checkpoint selection during RL training.\nSkywork-R1V3 achieves state-of-the-art results on MMMU, significantly improving\nfrom 64.3% to 76.0%. This performance matches entry-level human capabilities.\nRemarkably, our RL-powered post-training approach enables even the 38B\nparameter model to rival top closed-source VLMs. The implementation\nsuccessfully transfers mathematical reasoning to other subject-related\nreasoning tasks. We also include an analysis of curriculum learning and\nreinforcement finetuning strategies, along with a broader discussion on\nmultimodal reasoning. Skywork-R1V3 represents a significant leap in multimodal\nreasoning, showcasing RL as a powerful engine for advancing open-source VLM\ncapabilities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.06167v3", "cate": "cs.CL", "date": "2025-07-08", "updated": "2025-07-10"}
{"id": "2507.01788", "title": "Are Vision Transformer Representations Semantically Meaningful? A Case Study in Medical Imaging", "authors": ["Montasir Shams", "Chashi Mahiul Islam", "Shaeke Salman", "Phat Tran", "Xiuwen Liu"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      9 pages", "url": "http://arxiv.org/abs/2507.01788v2", "summary": "Vision transformers (ViTs) have rapidly gained prominence in medical imaging\ntasks such as disease classification, segmentation, and detection due to their\nsuperior accuracy compared to conventional deep learning models. However, due\nto their size and complex interactions via the self-attention mechanism, they\nare not well understood. In particular, it is unclear whether the\nrepresentations produced by such models are semantically meaningful. In this\npaper, using a projected gradient-based algorithm, we show that their\nrepresentations are not semantically meaningful and they are inherently\nvulnerable to small changes. Images with imperceptible differences can have\nvery different representations; on the other hand, images that should belong to\ndifferent semantic classes can have nearly identical representations. Such\nvulnerability can lead to unreliable classification results; for example,\nunnoticeable changes cause the classification accuracy to be reduced by over\n60\\%. %. To the best of our knowledge, this is the first work to systematically\ndemonstrate this fundamental lack of semantic meaningfulness in ViT\nrepresentations for medical image classification, revealing a critical\nchallenge for their deployment in safety-critical systems.", "comment": "9 pages", "pdf_url": "http://arxiv.org/pdf/2507.01788v2", "cate": "cs.CV", "date": "2025-07-02", "updated": "2025-07-10"}
{"id": "2505.20628", "title": "Position: Adopt Constraints Over Penalties in Deep Learning", "authors": ["Juan Ramirez", "Meraj Hashemizadeh", "Simon Lacoste-Julien"], "categories": ["cs.LG", "math.OC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Code available at this https URL", "url": "http://arxiv.org/abs/2505.20628v2", "summary": "Recent efforts to develop trustworthy AI systems with accountability\nguarantees have led to widespread use of machine learning formulations\nincorporating external requirements, or constraints. These requirements are\noften enforced via penalization--adding fixed-weight terms to the task loss. We\nargue this approach is fundamentally ill-suited since there may be no penalty\ncoefficient that simultaneously ensures constraint satisfaction and optimal\nconstrained performance, i.e., that truly solves the constrained problem.\nMoreover, tuning these coefficients requires costly trial-and-error, incurring\nsignificant time and computational overhead. We, therefore, advocate for\nbroader adoption of tailored constrained optimization methods--such as the\nLagrangian approach, which jointly optimizes the penalization \"coefficients\"\n(the Lagrange multipliers) and the model parameters. Such methods (i) truly\nsolve the constrained problem and do so accountably, by clearly defining\nfeasibility and verifying when it is achieved, (ii) eliminate the need for\nextensive penalty tuning, and (iii) integrate seamlessly with modern deep\nlearning pipelines.", "comment": "Code available at\n  https://github.com/merajhashemi/constraints-vs-penalties", "pdf_url": "http://arxiv.org/pdf/2505.20628v2", "cate": "cs.LG", "date": "2025-05-27", "updated": "2025-07-09"}
{"id": "2506.18903", "title": "VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed View Memory", "authors": ["Runjia Li", "Philip Torr", "Andrea Vedaldi", "Tomas Jakab"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project page: this https URL", "url": "http://arxiv.org/abs/2506.18903v2", "summary": "We propose a novel memory mechanism to build video generators that can\nexplore environments interactively. Similar results have previously been\nachieved by out-painting 2D views of the scene while incrementally\nreconstructing its 3D geometry, which quickly accumulates errors, or by video\ngenerators with a short context window, which struggle to maintain scene\ncoherence over the long term. To address these limitations, we introduce\nSurfel-Indexed View Memory (VMem), a mechanism that remembers past views by\nindexing them geometrically based on the 3D surface elements (surfels) they\nhave observed. VMem enables the efficient retrieval of the most relevant past\nviews when generating new ones. By focusing only on these relevant views, our\nmethod produces consistent explorations of imagined environments at a fraction\nof the computational cost of using all past views as context. We evaluate our\napproach on challenging long-term scene synthesis benchmarks and demonstrate\nsuperior performance compared to existing methods in maintaining scene\ncoherence and camera control.", "comment": "Project page: https://v-mem.github.io", "pdf_url": "http://arxiv.org/pdf/2506.18903v2", "cate": "cs.CV", "date": "2025-06-23", "updated": "2025-07-10"}
{"id": "2507.06203", "title": "A Survey on Latent Reasoning", "authors": ["Rui-Jie Zhu", "Tianhao Peng", "Tianhao Cheng", "Xingwei Qu", "Jinfa Huang", "Dawei Zhu", "Hao Wang", "Kaiwen Xue", "Xuanliang Zhang", "Yong Shan", "Tianle Cai", "Taylor Kergan", "Assel Kembay", "Andrew Smith", "Chenghua Lin", "Binh Nguyen", "Yuqi Pan", "Yuhong Chou", "Zefan Cai", "Zhenhe Wu", "Yongchi Zhao", "Tianyu Liu", "Jian Yang", "Wangchunshu Zhou", "Chujie Zheng", "Chongxuan Li", "Yuyin Zhou", "Zhoujun Li", "Zhaoxiang Zhang", "Jiaheng Liu", "Ge Zhang", "Wenhao Huang", "Jason Eshraghian"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.06203v2", "summary": "Large Language Models (LLMs) have demonstrated impressive reasoning\ncapabilities, especially when guided by explicit chain-of-thought (CoT)\nreasoning that verbalizes intermediate steps. While CoT improves both\ninterpretability and accuracy, its dependence on natural language reasoning\nlimits the model's expressive bandwidth. Latent reasoning tackles this\nbottleneck by performing multi-step inference entirely in the model's\ncontinuous hidden state, eliminating token-level supervision. To advance latent\nreasoning research, this survey provides a comprehensive overview of the\nemerging field of latent reasoning. We begin by examining the foundational role\nof neural network layers as the computational substrate for reasoning,\nhighlighting how hierarchical representations support complex transformations.\nNext, we explore diverse latent reasoning methodologies, including\nactivation-based recurrence, hidden state propagation, and fine-tuning\nstrategies that compress or internalize explicit reasoning traces. Finally, we\ndiscuss advanced paradigms such as infinite-depth latent reasoning via masked\ndiffusion models, which enable globally consistent and reversible reasoning\nprocesses. By unifying these perspectives, we aim to clarify the conceptual\nlandscape of latent reasoning and chart future directions for research at the\nfrontier of LLM cognition. An associated GitHub repository collecting the\nlatest papers and repos is available at:\nhttps://github.com/multimodal-art-projection/LatentCoT-Horizon/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.06203v2", "cate": "cs.CL", "date": "2025-07-08", "updated": "2025-07-10"}
{"id": "2507.02398", "title": "Beyond Spatial Frequency: Pixel-wise Temporal Frequency-based Deepfake Video Detection", "authors": ["Taehoon Kim", "Jongwook Choi", "Yonghyun Jeong", "Haeun Noh", "Jaejun Yoo", "Seungryul Baek", "Jongwon Choi"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      accepted by iccv 2025. code is will be available at this https URL", "url": "http://arxiv.org/abs/2507.02398v2", "summary": "We introduce a deepfake video detection approach that exploits pixel-wise\ntemporal inconsistencies, which traditional spatial frequency-based detectors\noften overlook. Traditional detectors represent temporal information merely by\nstacking spatial frequency spectra across frames, resulting in the failure to\ndetect temporal artifacts in the pixel plane. Our approach performs a 1D\nFourier transform on the time axis for each pixel, extracting features highly\nsensitive to temporal inconsistencies, especially in areas prone to unnatural\nmovements. To precisely locate regions containing the temporal artifacts, we\nintroduce an attention proposal module trained in an end-to-end manner.\nAdditionally, our joint transformer module effectively integrates pixel-wise\ntemporal frequency features with spatio-temporal context features, expanding\nthe range of detectable forgery artifacts. Our framework represents a\nsignificant advancement in deepfake video detection, providing robust\nperformance across diverse and challenging detection scenarios.", "comment": "accepted by iccv 2025. code is will be available at\n  https://github.com/rama0126/PwTF-DVD", "pdf_url": "http://arxiv.org/pdf/2507.02398v2", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-10"}
{"id": "2506.11315", "title": "Sampling Imbalanced Data with Multi-objective Bilevel Optimization", "authors": ["Karen Medlin", "Sven Leyffer", "Krishnan Raghavan"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.11315v2", "summary": "Two-class classification problems are often characterized by an imbalance\nbetween the number of majority and minority datapoints resulting in poor\nclassification of the minority class in particular. Traditional approaches,\nsuch as reweighting the loss function or na\\\"ive resampling, risk overfitting\nand subsequently fail to improve classification because they do not consider\nthe diversity between majority and minority datasets. Such consideration is\ninfeasible because there is no metric that can measure the impact of imbalance\non the model. To obviate these challenges, we make two key contributions.\nFirst, we introduce MOODS~(Multi-Objective Optimization for Data Sampling), a\nnovel multi-objective bilevel optimization framework that guides both synthetic\noversampling and majority undersampling. Second, we introduce a validation\nmetric -- `$\\epsilon/ \\delta$ non-overlapping diversification metric' -- that\nquantifies the goodness of a sampling method towards model performance. With\nthis metric we experimentally demonstrate state-of-the-art performance with\nimprovement in diversity driving a $1-15 \\%$ increase in $F1$ scores.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.11315v2", "cate": "cs.LG", "date": "2025-06-12", "updated": "2025-07-10"}
{"id": "2506.21513", "title": "GGTalker: Talking Head Systhesis with Generalizable Gaussian Priors and Identity-Specific Adaptation", "authors": ["Wentao Hu", "Shunkai Li", "Ziqiao Peng", "Haoxian Zhang", "Fan Shi", "Xiaoqiang Liu", "Pengfei Wan", "Di Zhang", "Hui Tian"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025, Project page: this https URL", "url": "http://arxiv.org/abs/2506.21513v2", "summary": "Creating high-quality, generalizable speech-driven 3D talking heads remains a\npersistent challenge. Previous methods achieve satisfactory results for fixed\nviewpoints and small-scale audio variations, but they struggle with large head\nrotations and out-of-distribution (OOD) audio. Moreover, they are constrained\nby the need for time-consuming, identity-specific training. We believe the core\nissue lies in the lack of sufficient 3D priors, which limits the extrapolation\ncapabilities of synthesized talking heads. To address this, we propose\nGGTalker, which synthesizes talking heads through a combination of\ngeneralizable priors and identity-specific adaptation. We introduce a two-stage\nPrior-Adaptation training strategy to learn Gaussian head priors and adapt to\nindividual characteristics. We train Audio-Expression and Expression-Visual\npriors to capture the universal patterns of lip movements and the general\ndistribution of head textures. During the Customized Adaptation, individual\nspeaking styles and texture details are precisely modeled. Additionally, we\nintroduce a color MLP to generate fine-grained, motion-aligned textures and a\nBody Inpainter to blend rendered results with the background, producing\nindistinguishable, photorealistic video frames. Comprehensive experiments show\nthat GGTalker achieves state-of-the-art performance in rendering quality, 3D\nconsistency, lip-sync accuracy, and training efficiency.", "comment": "ICCV 2025, Project page: https://vincenthu19.github.io/GGTalker/", "pdf_url": "http://arxiv.org/pdf/2506.21513v2", "cate": "cs.CV", "date": "2025-06-26", "updated": "2025-07-10"}
{"id": "2507.06229", "title": "Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving", "authors": ["Xiangru Tang", "Tianrui Qin", "Tianhao Peng", "Ziyang Zhou", "Daniel Shao", "Tingting Du", "Xinming Wei", "Peng Xia", "Fang Wu", "He Zhu", "Ge Zhang", "Jiaheng Liu", "Xingyao Wang", "Sirui Hong", "Chenglin Wu", "Hao Cheng", "Chi Wang", "Wangchunshu Zhou"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.06229v2", "summary": "As language agents tackle increasingly complex tasks, they struggle with\neffective error correction and experience reuse across domains. We introduce\nAgent KB, a hierarchical experience framework that enables complex agentic\nproblem solving via a novel Reason-Retrieve-Refine pipeline. Agent KB addresses\na core limitation: agents traditionally cannot learn from each other's\nexperiences. By capturing both high-level strategies and detailed execution\nlogs, Agent KB creates a shared knowledge base that enables cross-agent\nknowledge transfer. Evaluated on the GAIA benchmark, Agent KB improves success\nrates by up to 16.28 percentage points. On the most challenging tasks, Claude-3\nimproves from 38.46% to 57.69%, while GPT-4 improves from 53.49% to 73.26% on\nintermediate tasks. On SWE-bench code repair, Agent KB enables Claude-3 to\nimprove from 41.33% to 53.33%. Our results suggest that Agent KB provides a\nmodular, framework-agnostic infrastructure for enabling agents to learn from\npast experiences and generalize successful strategies to new tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.06229v2", "cate": "cs.CL", "date": "2025-07-08", "updated": "2025-07-10"}
{"id": "2507.02409", "title": "S2FGL: Spatial Spectral Federated Graph Learning", "authors": ["Zihan Tan", "Suyuan Huang", "Guancheng Wan", "Wenke Huang", "He Li", "Mang Ye"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02409v2", "summary": "Federated Graph Learning (FGL) combines the privacy-preserving capabilities\nof federated learning (FL) with the strong graph modeling capability of Graph\nNeural Networks (GNNs). Current research addresses subgraph-FL only from the\nstructural perspective, neglecting the propagation of graph signals on spatial\nand spectral domains of the structure. From a spatial perspective, subgraph-FL\nintroduces edge disconnections between clients, leading to disruptions in label\nsignals and a degradation in the class knowledge of the global GNN. From a\nspectral perspective, spectral heterogeneity causes inconsistencies in signal\nfrequencies across subgraphs, which makes local GNNs overfit the local signal\npropagation schemes. As a result, spectral client drifts occur, undermining\nglobal generalizability. To tackle the challenges, we propose a global\nknowledge repository to mitigate label signal disruption and a frequency\nalignment to address spectral client drifts. The combination of spatial and\nspectral strategies forms our framework S2FGL. Extensive experiments on\nmultiple datasets demonstrate the superiority of S2FGL. The code is available\nat https://github.com/Wonder7racer/S2FGL.git.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02409v2", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-10"}
{"id": "2506.23446", "title": "User-Based Sequential Modeling with Transformer Encoders for Insider Threat Detection", "authors": ["Mohamed Elbasheer", "Adewale Akinfaderin"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.23446v2", "summary": "Insider threat detection presents unique challenges due to the authorized\nstatus of malicious actors and the subtlety of anomalous behaviors. Existing\nmachine learning methods often treat user activity as isolated events, thereby\nfailing to leverage sequential dependencies in user behavior. In this study, we\npropose a User-Based Sequencing (UBS) methodology, transforming the CERT\ninsider threat dataset into structured temporal sequences suitable for deep\nsequential modeling. We deploy a Transformer Encoder architecture to model\nbenign user activity and employ its reconstruction errors as anomaly scores.\nThese scores are subsequently evaluated using three unsupervised outlier\ndetection algorithms: One-Class SVM (OCSVM), Local Outlier Factor (LOF), and\nIsolation Forest (iForest). Across four rigorously designed test sets,\nincluding combinations of multiple CERT dataset releases, our UBS-Transformer\npipeline consistently achieves state-of-the-art performance - notably 96.61%\naccuracy, 99.43% recall, 96.38% F1-score, 95.00% AUROC, and exceptionally low\nfalse negative (0.0057) and false positive (0.0571) rates. Comparative analyses\ndemonstrate that our approach substantially outperforms tabular and\nconventional autoencoder baselines, underscoring the efficacy of sequential\nuser modeling and advanced anomaly detection in the insider threat domain.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23446v2", "cate": "cs.LG", "date": "2025-06-30", "updated": "2025-07-10"}
{"id": "2507.02148", "title": "Underwater Monocular Metric Depth Estimation: Real-World Benchmarks and Synthetic Fine-Tuning with Vision Foundation Models", "authors": ["Zijie Cai", "Christopher Metzler"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02148v2", "summary": "Monocular depth estimation has recently progressed beyond ordinal depth to\nprovide metric depth predictions. However, its reliability in underwater\nenvironments remains limited due to light attenuation and scattering, color\ndistortion, turbidity, and the lack of high-quality metric ground truth data.\nIn this paper, we present a comprehensive benchmark of zero-shot and fine-tuned\nmonocular metric depth estimation models on real-world underwater datasets with\nmetric depth annotations, including FLSea and SQUID. We evaluated a diverse set\nof state-of-the-art Vision Foundation Models across a range of underwater\nconditions and depth ranges. Our results show that large-scale models trained\non terrestrial data (real or synthetic) are effective in in-air settings, but\nperform poorly underwater due to significant domain shifts. To address this, we\nfine-tune Depth Anything V2 with a ViT-S backbone encoder on a synthetic\nunderwater variant of the Hypersim dataset, which we simulated using a\nphysically based underwater image formation model. Our fine-tuned model\nconsistently improves performance across all benchmarks and outperforms\nbaselines trained only on the clean in-air Hypersim dataset. This study\npresents a detailed evaluation and visualization of monocular metric depth\nestimation in underwater scenes, emphasizing the importance of domain\nadaptation and scale-aware supervision for achieving robust and generalizable\nmetric depth predictions using foundation models in challenging environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02148v2", "cate": "cs.CV", "date": "2025-07-02", "updated": "2025-07-10"}
{"id": "2507.06539", "title": "Large Language Model for Extracting Complex Contract Information in Industrial Scenes", "authors": ["Yunyang Cao", "Yanjun Li", "Silong Dai"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.06539v2", "summary": "This paper proposes a high-quality dataset construction method for complex\ncontract information extraction tasks in industrial scenarios and fine-tunes a\nlarge language model based on this dataset. Firstly, cluster analysis is\nperformed on industrial contract texts, and GPT-4 and GPT-3.5 are used to\nextract key information from the original contract data, obtaining high-quality\ndata annotations. Secondly, data augmentation is achieved by constructing new\ntexts, and GPT-3.5 generates unstructured contract texts from randomly combined\nkeywords, improving model robustness. Finally, the large language model is\nfine-tuned based on the high-quality dataset. Experimental results show that\nthe model achieves excellent overall performance while ensuring high field\nrecall and precision and considering parsing efficiency. LoRA, data balancing,\nand data augmentation effectively enhance model accuracy and robustness. The\nproposed method provides a novel and efficient solution for industrial contract\ninformation extraction tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.06539v2", "cate": "cs.CL", "date": "2025-07-09", "updated": "2025-07-10"}
{"id": "2507.02644", "title": "Solving the Hubbard model with Neural Quantum States", "authors": ["Yuntian Gu", "Wenrui Li", "Heng Lin", "Bo Zhan", "Ruichen Li", "Yifei Huang", "Di He", "Yantao Wu", "Tao Xiang", "Mingpu Qin", "Liwei Wang", "Dingshun Lv"], "categories": ["cond-mat.str-el", "cs.AI", "quant-ph"], "primary_category": "Subjects:       Strongly Correlated Electrons (cond-mat.str-el)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02644v2", "summary": "The rapid development of neural quantum states (NQS) has established it as a\npromising framework for studying quantum many-body systems. In this work, by\nleveraging the cutting-edge transformer-based architectures and developing\nhighly efficient optimization algorithms, we achieve the state-of-the-art\nresults for the doped two-dimensional (2D) Hubbard model, arguably the minimum\nmodel for high-Tc superconductivity. Interestingly, we find different attention\nheads in the NQS ansatz can directly encode correlations at different scales,\nmaking it capable of capturing long-range correlations and entanglements in\nstrongly correlated systems. With these advances, we establish the half-filled\nstripe in the ground state of 2D Hubbard model with the next nearest\nneighboring hoppings, consistent with experimental observations in cuprates.\nOur work establishes NQS as a powerful tool for solving challenging\nmany-fermions systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02644v2", "cate": "cond-mat.str-el", "date": "2025-07-03", "updated": "2025-07-10"}
{"id": "2507.03041", "title": "Optimas: Optimizing Compound AI Systems with Globally Aligned Local Rewards", "authors": ["Shirley Wu", "Parth Sarthi", "Shiyu Zhao", "Aaron Lee", "Herumb Shandilya", "Adrian Mladenic Grobelnik", "Nurendra Choudhary", "Eddie Huang", "Karthik Subbian", "Linjun Zhang", "Diyi Yang", "James Zou", "Jure Leskovec"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      20 pages", "url": "http://arxiv.org/abs/2507.03041v2", "summary": "Compound AI systems integrating multiple components, such as Large Language\nModels, specialized tools, and traditional machine learning models, are\nincreasingly deployed to solve complex real-world tasks. However, optimizing\ncompound systems remains challenging due to their non-differentiable structures\nand diverse configuration types across components, including prompts,\nhyperparameters, and model parameters. To address this challenge, we propose\nOptimas, a unified framework for effective optimization of compound systems.\nThe core idea of Optimas is to maintain one Local Reward Function (LRF) per\ncomponent, each satisfying a local-global alignment property, i.e., each\ncomponent's local reward correlates with the global system performance. In each\niteration, Optimas efficiently adapts the LRFs to maintain this property while\nsimultaneously maximizing each component's local reward. This approach enables\nindependent updates of heterogeneous configurations using the designated\noptimization method, while ensuring that local improvements consistently lead\nto performance gains. We present extensive evaluations across five real-world\ncompound systems to demonstrate that Optimas outperforms strong baselines by an\naverage improvement of 11.92%, offering a general and effective approach for\nimproving compound systems. Our website is at https://optimas.stanford.edu.", "comment": "20 pages", "pdf_url": "http://arxiv.org/pdf/2507.03041v2", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-09"}
{"id": "2507.02899", "title": "Learning to Generate Vectorized Maps at Intersections with Multiple Roadside Cameras", "authors": ["Quanxin Zheng", "Miao Fan", "Shengtong Xu", "Linghe Kong", "Haoyi Xiong"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by IROS'25", "url": "http://arxiv.org/abs/2507.02899v2", "summary": "Vectorized maps are indispensable for precise navigation and the safe\noperation of autonomous vehicles. Traditional methods for constructing these\nmaps fall into two categories: offline techniques, which rely on expensive,\nlabor-intensive LiDAR data collection and manual annotation, and online\napproaches that use onboard cameras to reduce costs but suffer from limited\nperformance, especially at complex intersections. To bridge this gap, we\nintroduce MRC-VMap, a cost-effective, vision-centric, end-to-end neural network\ndesigned to generate high-definition vectorized maps directly at intersections.\nLeveraging existing roadside surveillance cameras, MRC-VMap directly converts\ntime-aligned, multi-directional images into vectorized map representations.\nThis integrated solution lowers the need for additional intermediate\nmodules--such as separate feature extraction and Bird's-Eye View (BEV)\nconversion steps--thus reducing both computational overhead and error\npropagation. Moreover, the use of multiple camera views enhances mapping\ncompleteness, mitigates occlusions, and provides robust performance under\npractical deployment constraints. Extensive experiments conducted on 4,000\nintersections across 4 major metropolitan areas in China demonstrate that\nMRC-VMap not only outperforms state-of-the-art online methods but also achieves\naccuracy comparable to high-cost LiDAR-based approaches, thereby offering a\nscalable and efficient solution for modern autonomous navigation systems.", "comment": "Accepted by IROS'25", "pdf_url": "http://arxiv.org/pdf/2507.02899v2", "cate": "cs.CV", "date": "2025-06-23", "updated": "2025-07-10"}
{"id": "2507.06795", "title": "ixi-GEN: Efficient Industrial sLLMs through Domain Adaptive Continual Pretraining", "authors": ["Seonwu Kim", "Yohan Na", "Kihun Kim", "Hanhee Cho", "Geun Lim", "Mintae Kim", "Seongik Park", "Ki Hyun Kim", "Youngsub Han", "Byoung-Ki Jeon"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      under review", "url": "http://arxiv.org/abs/2507.06795v2", "summary": "The emergence of open-source large language models (LLMs) has expanded\nopportunities for enterprise applications; however, many organizations still\nlack the infrastructure to deploy and maintain large-scale models. As a result,\nsmall LLMs (sLLMs) have become a practical alternative, despite their inherent\nperformance limitations. While Domain Adaptive Continual Pretraining (DACP) has\nbeen previously explored as a method for domain adaptation, its utility in\ncommercial applications remains under-examined. In this study, we validate the\neffectiveness of applying a DACP-based recipe across diverse foundation models\nand service domains. Through extensive experiments and real-world evaluations,\nwe demonstrate that DACP-applied sLLMs achieve substantial gains in target\ndomain performance while preserving general capabilities, offering a\ncost-efficient and scalable solution for enterprise-level deployment.", "comment": "under review", "pdf_url": "http://arxiv.org/pdf/2507.06795v2", "cate": "cs.CL", "date": "2025-07-09", "updated": "2025-07-10"}
{"id": "2507.04750", "title": "MCFormer: A Multi-Cost-Volume Network and Comprehensive Benchmark for Particle Image Velocimetry", "authors": ["Zicheng Lin", "Xiaoqiang Li", "Yichao Wang", "Chuang Zhu"], "categories": ["cs.CV", "cs.AI", "68T45, 65D18"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      20 pages, 13 figures, 5 tables. Comprehensive benchmark evaluation of optical flow models for PIV. Introduces MCFormer architecture with multi-frame temporal processing and multiple cost volumes. Includes large-scale synthetic PIV dataset based on JHTDB and Blasius CFD simulations. Code and dataset will be made publicly available", "url": "http://arxiv.org/abs/2507.04750v2", "summary": "Particle Image Velocimetry (PIV) is fundamental to fluid dynamics, yet deep\nlearning applications face significant hurdles. A critical gap exists: the lack\nof comprehensive evaluation of how diverse optical flow models perform\nspecifically on PIV data, largely due to limitations in available datasets and\nthe absence of a standardized benchmark. This prevents fair comparison and\nhinders progress. To address this, our primary contribution is a novel,\nlarge-scale synthetic PIV benchmark dataset generated from diverse CFD\nsimulations (JHTDB and Blasius). It features unprecedented variety in particle\ndensities, flow velocities, and continuous motion, enabling, for the first\ntime, a standardized and rigorous evaluation of various optical flow and PIV\nalgorithms. Complementing this, we propose Multi Cost Volume PIV (MCFormer), a\nnew deep network architecture leveraging multi-frame temporal information and\nmultiple cost volumes, specifically designed for PIV's sparse nature. Our\ncomprehensive benchmark evaluation, the first of its kind, reveals significant\nperformance variations among adapted optical flow models and demonstrates that\nMCFormer significantly outperforms existing methods, achieving the lowest\noverall normalized endpoint error (NEPE). This work provides both a\nfoundational benchmark resource essential for future PIV research and a\nstate-of-the-art method tailored for PIV challenges. We make our benchmark\ndataset and code publicly available to foster future research in this area.", "comment": "20 pages, 13 figures, 5 tables. Comprehensive benchmark evaluation of\n  optical flow models for PIV. Introduces MCFormer architecture with\n  multi-frame temporal processing and multiple cost volumes. Includes\n  large-scale synthetic PIV dataset based on JHTDB and Blasius CFD simulations.\n  Code and dataset will be made publicly available", "pdf_url": "http://arxiv.org/pdf/2507.04750v2", "cate": "cs.CV", "date": "2025-07-07", "updated": "2025-07-10"}
{"id": "2507.05411", "title": "AXLearn: Modular Large Model Training on Heterogeneous Infrastructure", "authors": ["Mark Lee", "Tom Gunter", "Chang Lan", "John Peebles", "Hanzhi Zhou", "Kelvin Zou", "Sneha Bangalore", "Chung-Cheng Chiu", "Nan Du", "Xianzhi Du", "Philipp Dufter", "Ruixuan Hou", "Haoshuo Huang", "Dongseong Hwang", "Xiang Kong", "Jinhao Lei", "Tao Lei", "Meng Li", "Li Li", "Jiarui Lu", "Zhiyun Lu", "Yiping Ma", "David Qiu", "Vivek Rathod", "Senyu Tong", "Zhucheng Tu", "Jianyu Wang", "Yongqiang Wang", "Zirui Wang", "Floris Weers", "Sam Wiseman", "Guoli Yin", "Bowen Zhang", "Xiyou Zhou", "Danyang Zhuo", "Cheng Leong", "Ruoming Pang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.05411v2", "summary": "We design and implement AXLearn, a production deep learning system that\nfacilitates scalable and high-performance training of large deep learning\nmodels. Compared to other state-of-the-art deep learning systems, AXLearn has a\nunique focus on modularity and support for heterogeneous hardware\ninfrastructure. AXLearn's internal interfaces between software components\nfollow strict encapsulation, allowing different components to be assembled to\nfacilitate rapid model development and experimentation on heterogeneous compute\ninfrastructure. We introduce a novel method of quantifying modularity via\nLines-of-Code (LoC)-complexity, which demonstrates how our system maintains\nconstant complexity as we scale the components in the system, compared to\nlinear or quadratic complexity in other systems. This allows integrating\nfeatures such as Rotary Position Embeddings (RoPE) into AXLearn across hundred\nof modules with just 10 lines of code, compared to hundreds as required in\nother systems. At the same time, AXLearn maintains equivalent performance\ncompared to state-of-the-art training systems. Finally, we share our experience\nin the development and operation of AXLearn.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.05411v2", "cate": "cs.LG", "date": "2025-07-07", "updated": "2025-07-09"}
{"id": "2507.02987", "title": "Leveraging the Structure of Medical Data for Improved Representation Learning", "authors": ["Andrea Agostini", "Sonia Laguna", "Alain Ryser", "Samuel Ruiperez-Campillo", "Moritz Vandenhirtz", "Nicolas Deperrois", "Farhad Nooralahzadeh", "Michael Krauthammer", "Thomas M. Sutter", "Julia E. Vogt"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02987v2", "summary": "Building generalizable medical AI systems requires pretraining strategies\nthat are data-efficient and domain-aware. Unlike internet-scale corpora,\nclinical datasets such as MIMIC-CXR offer limited image counts and scarce\nannotations, but exhibit rich internal structure through multi-view imaging. We\npropose a self-supervised framework that leverages the inherent structure of\nmedical datasets. Specifically, we treat paired chest X-rays (i.e., frontal and\nlateral views) as natural positive pairs, learning to reconstruct each view\nfrom sparse patches while aligning their latent embeddings. Our method requires\nno textual supervision and produces informative representations. Evaluated on\nMIMIC-CXR, we show strong performance compared to supervised objectives and\nbaselines being trained without leveraging structure. This work provides a\nlightweight, modality-agnostic blueprint for domain-specific pretraining where\ndata is structured but scarce", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02987v2", "cate": "cs.CV", "date": "2025-07-01", "updated": "2025-07-09"}
{"id": "2507.06920", "title": "Rethinking Verification for LLM Code Generation: From Generation to Testing", "authors": ["Zihan Ma", "Taolin Zhang", "Maosong Cao", "Junnan Liu", "Wenwei Zhang", "Minnan Luo", "Songyang Zhang", "Kai Chen"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.06920v2", "summary": "Large language models (LLMs) have recently achieved notable success in\ncode-generation benchmarks such as HumanEval and LiveCodeBench. However, a\ndetailed examination reveals that these evaluation suites often comprise only a\nlimited number of homogeneous test cases, resulting in subtle faults going\nundetected. This not only artificially inflates measured performance but also\ncompromises accurate reward estimation in reinforcement learning frameworks\nutilizing verifiable rewards (RLVR). To address these critical shortcomings, we\nsystematically investigate the test-case generation (TCG) task by proposing\nmulti-dimensional metrics designed to rigorously quantify test-suite\nthoroughness. Furthermore, we introduce a human-LLM collaborative method\n(SAGA), leveraging human programming expertise with LLM reasoning capability,\naimed at significantly enhancing both the coverage and the quality of generated\ntest cases. In addition, we develop a TCGBench to facilitate the study of the\nTCG task. Experiments show that SAGA achieves a detection rate of 90.62% and a\nverifier accuracy of 32.58% on TCGBench. The Verifier Accuracy (Verifier Acc)\nof the code generation evaluation benchmark synthesized by SAGA is 10.78%\nhigher than that of LiveCodeBench-v6. These results demonstrate the\neffectiveness of our proposed method. We hope this work contributes to building\na scalable foundation for reliable LLM code evaluation, further advancing RLVR\nin code generation, and paving the way for automated adversarial test synthesis\nand adaptive benchmark integration.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.06920v2", "cate": "cs.CL", "date": "2025-07-09", "updated": "2025-07-10"}
{"id": "2507.05007", "title": "Multi-modal Representations for Fine-grained Multi-label Critical View of Safety Recognition", "authors": ["Britty Baby", "Vinkle Srivastav", "Pooja P. Jain", "Kun Yuan", "Pietro Mascagni", "Nicolas Padoy"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.05007v2", "summary": "The Critical View of Safety (CVS) is crucial for safe laparoscopic\ncholecystectomy, yet assessing CVS criteria remains a complex and challenging\ntask, even for experts. Traditional models for CVS recognition depend on\nvision-only models learning with costly, labor-intensive spatial annotations.\nThis study investigates how text can be harnessed as a powerful tool for both\ntraining and inference in multi-modal surgical foundation models to automate\nCVS recognition. Unlike many existing multi-modal models, which are primarily\nadapted for multi-class classification, CVS recognition requires a multi-label\nframework. Zero-shot evaluation of existing multi-modal surgical models shows a\nsignificant performance gap for this task. To address this, we propose\nCVS-AdaptNet, a multi-label adaptation strategy that enhances fine-grained,\nbinary classification across multiple labels by aligning image embeddings with\ntextual descriptions of each CVS criterion using positive and negative prompts.\nBy adapting PeskaVLP, a state-of-the-art surgical foundation model, on the\nEndoscapes-CVS201 dataset, CVS-AdaptNet achieves 57.6 mAP, improving over the\nResNet50 image-only baseline (51.5 mAP) by 6 points. Our results show that\nCVS-AdaptNet's multi-label, multi-modal framework, enhanced by textual prompts,\nboosts CVS recognition over image-only methods. We also propose text-specific\ninference methods, that helps in analysing the image-text alignment. While\nfurther work is needed to match state-of-the-art spatial annotation-based\nmethods, this approach highlights the potential of adapting generalist models\nto specialized surgical tasks. Code:\nhttps://github.com/CAMMA-public/CVS-AdaptNet", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.05007v2", "cate": "cs.CV", "date": "2025-07-07", "updated": "2025-07-10"}
{"id": "2507.06821", "title": "HeLo: Heterogeneous Multi-Modal Fusion with Label Correlation for Emotion Distribution Learning", "authors": ["Chuhang Zheng", "Chunwei Tian", "Jie Wen", "Daoqiang Zhang", "Qi Zhu"], "categories": ["cs.LG", "cs.AI", "cs.MM"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.06821v2", "summary": "Multi-modal emotion recognition has garnered increasing attention as it plays\na significant role in human-computer interaction (HCI) in recent years. Since\ndifferent discrete emotions may exist at the same time, compared with\nsingle-class emotion recognition, emotion distribution learning (EDL) that\nidentifies a mixture of basic emotions has gradually emerged as a trend.\nHowever, existing EDL methods face challenges in mining the heterogeneity among\nmultiple modalities. Besides, rich semantic correlations across arbitrary basic\nemotions are not fully exploited. In this paper, we propose a multi-modal\nemotion distribution learning framework, named HeLo, aimed at fully exploring\nthe heterogeneity and complementary information in multi-modal emotional data\nand label correlation within mixed basic emotions. Specifically, we first adopt\ncross-attention to effectively fuse the physiological data. Then, an optimal\ntransport (OT)-based heterogeneity mining module is devised to mine the\ninteraction and heterogeneity between the physiological and behavioral\nrepresentations. To facilitate label correlation learning, we introduce a\nlearnable label embedding optimized by correlation matrix alignment. Finally,\nthe learnable label embeddings and label correlation matrices are integrated\nwith the multi-modal representations through a novel label correlation-driven\ncross-attention mechanism for accurate emotion distribution learning.\nExperimental results on two publicly available datasets demonstrate the\nsuperiority of our proposed method in emotion distribution learning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.06821v2", "cate": "cs.LG", "date": "2025-07-09", "updated": "2025-07-10"}
{"id": "2507.04946", "title": "Taming the Tri-Space Tension: ARC-Guided Hallucination Modeling and Control for Text-to-Image Generation", "authors": ["Jianjiang Yang", "Ziyan Huang"], "categories": ["cs.CV", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      We withdraw this paper due to significant visualization errors in Figure 3 and 5 that affect the correctness of our core modeling claims and may cause misinterpretation. These figures misrepresent ARC dynamics and trajectory control", "url": "http://arxiv.org/abs/2507.04946v2", "summary": "Despite remarkable progress in image quality and prompt fidelity,\ntext-to-image (T2I) diffusion models continue to exhibit persistent\n\"hallucinations\", where generated content subtly or significantly diverges from\nthe intended prompt semantics. While often regarded as unpredictable artifacts,\nwe argue that these failures reflect deeper, structured misalignments within\nthe generative process. In this work, we propose a cognitively inspired\nperspective that reinterprets hallucinations as trajectory drift within a\nlatent alignment space. Empirical observations reveal that generation unfolds\nwithin a multiaxial cognitive tension field, where the model must continuously\nnegotiate competing demands across three key critical axes: semantic coherence,\nstructural alignment, and knowledge grounding. We then formalize this\nthree-axis space as the \\textbf{Hallucination Tri-Space} and introduce the\nAlignment Risk Code (ARC): a dynamic vector representation that quantifies\nreal-time alignment tension during generation. The magnitude of ARC captures\noverall misalignment, its direction identifies the dominant failure axis, and\nits imbalance reflects tension asymmetry. Based on this formulation, we develop\nthe TensionModulator (TM-ARC): a lightweight controller that operates entirely\nin latent space. TM-ARC monitors ARC signals and applies targeted,\naxis-specific interventions during the sampling process. Extensive experiments\non standard T2I benchmarks demonstrate that our approach significantly reduces\nhallucination without compromising image quality or diversity. This framework\noffers a unified and interpretable approach for understanding and mitigating\ngenerative failures in diffusion-based T2I systems.", "comment": "We withdraw this paper due to significant visualization errors in\n  Figure 3 and 5 that affect the correctness of our core modeling claims and\n  may cause misinterpretation. These figures misrepresent ARC dynamics and\n  trajectory control", "pdf_url": "http://arxiv.org/pdf/2507.04946v2", "cate": "cs.CV", "date": "2025-07-07", "updated": "2025-07-09"}
{"id": "2507.06892", "title": "Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning for Large Language Model", "authors": ["Jing Liang", "Hongyao Tang", "Yi Ma", "Jinyi Liu", "Yan Zheng", "Shuyue Hu", "Lei Bai", "Jianye Hao"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Preliminary version, v2, added more details and corrected some minor mistakes. Project page: this https URL", "url": "http://arxiv.org/abs/2507.06892v2", "summary": "Reinforcement Learning (RL) has demonstrated its potential to improve the\nreasoning ability of Large Language Models (LLMs). One major limitation of most\nexisting Reinforcement Finetuning (RFT) methods is that they are on-policy RL\nin nature, i.e., data generated during the past learning process is not fully\nutilized. This inevitably comes at a significant cost of compute and time,\nposing a stringent bottleneck on continuing economic and efficient scaling. To\nthis end, we launch the renaissance of off-policy RL and propose Reincarnating\nMix-policy Proximal Policy Gradient (ReMix), a general approach to enable\non-policy RFT methods like PPO and GRPO to leverage off-policy data. ReMix\nconsists of three major components: (1) Mix-policy proximal policy gradient\nwith an increased Update-To-Data (UTD) ratio for efficient training; (2)\nKL-Convex policy constraint to balance the trade-off between stability and\nflexibility; (3) Policy reincarnation to achieve a seamless transition from\nefficient early-stage learning to steady asymptotic improvement. In our\nexperiments, we train a series of ReMix models upon PPO, GRPO and 1.5B, 7B base\nmodels. ReMix shows an average Pass@1 accuracy of 52.10% (for 1.5B model) with\n0.079M response rollouts, 350 training steps and achieves 63.27%/64.39% (for 7B\nmodel) with 0.007M/0.011M response rollouts, 50/75 training steps, on five math\nreasoning benchmarks (i.e., AIME'24, AMC'23, Minerva, OlympiadBench, and\nMATH500). Compared with 15 recent advanced models, ReMix shows SOTA-level\nperformance with an over 30x to 450x reduction in training cost in terms of\nrollout data volume. In addition, we reveal insightful findings via\nmultifaceted analysis, including the implicit preference for shorter responses\ndue to the Whipping Effect of off-policy discrepancy, the collapse mode of\nself-reflection behavior under the presence of severe off-policyness, etc.", "comment": "Preliminary version, v2, added more details and corrected some minor\n  mistakes. Project page: https://anitaleungxx.github.io/ReMix", "pdf_url": "http://arxiv.org/pdf/2507.06892v2", "cate": "cs.LG", "date": "2025-07-09", "updated": "2025-07-10"}
{"id": "2507.05020", "title": "Adaptation of Multi-modal Representation Models for Multi-task Surgical Computer Vision", "authors": ["Soham Walimbe", "Britty Baby", "Vinkle Srivastav", "Nicolas Padoy"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.05020v2", "summary": "Surgical AI often involves multiple tasks within a single procedure, like\nphase recognition or assessing the Critical View of Safety in laparoscopic\ncholecystectomy. Traditional models, built for one task at a time, lack\nflexibility, requiring a separate model for each. To address this, we introduce\nMML-SurgAdapt, a unified multi-task framework with Vision-Language Models\n(VLMs), specifically CLIP, to handle diverse surgical tasks through natural\nlanguage supervision. A key challenge in multi-task learning is the presence of\npartial annotations when integrating different tasks. To overcome this, we\nemploy Single Positive Multi-Label (SPML) learning, which traditionally reduces\nannotation burden by training models with only one positive label per instance.\nOur framework extends this approach to integrate data from multiple surgical\ntasks within a single procedure, enabling effective learning despite incomplete\nor noisy annotations. We demonstrate the effectiveness of our model on a\ncombined dataset consisting of Cholec80, Endoscapes2023, and CholecT50,\nutilizing custom prompts. Extensive evaluation shows that MML-SurgAdapt\nperforms comparably to task-specific benchmarks, with the added advantage of\nhandling noisy annotations. It also outperforms the existing SPML frameworks\nfor the task. By reducing the required labels by 23%, our approach proposes a\nmore scalable and efficient labeling process, significantly easing the\nannotation burden on clinicians. To our knowledge, this is the first\napplication of SPML to integrate data from multiple surgical tasks, presenting\na novel and generalizable solution for multi-task learning in surgical computer\nvision. Implementation is available at:\nhttps://github.com/CAMMA-public/MML-SurgAdapt", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.05020v2", "cate": "cs.CV", "date": "2025-07-07", "updated": "2025-07-10"}
{"id": "2507.06825", "title": "Artificial Generals Intelligence: Mastering Generals.io with Reinforcement Learning", "authors": ["Matej Straka", "Martin Schmid"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.06825v2", "summary": "We introduce a real-time strategy game environment based on Generals.io, a\ngame with thousands of weekly active players. Our environment is fully\ncompatible with Gymnasium and PettingZoo and is capable of running thousands of\nframes per second on commodity hardware. We also present a reference agent,\ntrained with supervised pre-training and self-play, which reached the top\n0.003% of the 1v1 human leaderboard after only 36 hours on a single H100 GPU.\nTo accelerate learning, we incorporate potential-based reward shaping and\nmemory features. Our contributions of a modular RTS benchmark and a competitive\nbaseline agent provide an accessible yet challenging platform for advancing\nmulti-agent reinforcement learning research. The documented code, together with\nexamples and tutorials, is available at\nhttps://github.com/strakam/generals-bots.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.06825v2", "cate": "cs.LG", "date": "2025-07-09", "updated": "2025-07-10"}
{"id": "2507.06526", "title": "Concept Unlearning by Modeling Key Steps of Diffusion Process", "authors": ["Chaoshuo Zhang", "Chenhao Lin", "Zhengyu Zhao", "Le Yang", "Qian Wang", "Chao Shen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.06526v2", "summary": "Text-to-image diffusion models (T2I DMs), represented by Stable Diffusion,\nwhich generate highly realistic images based on textual input, have been widely\nused. However, their misuse poses serious security risks. While existing\nconcept unlearning methods aim to mitigate these risks, they struggle to\nbalance unlearning effectiveness with generative retainability.To overcome this\nlimitation, we innovatively propose the Key Step Concept Unlearning (KSCU)\nmethod, which ingeniously capitalizes on the unique stepwise sampling\ncharacteristic inherent in diffusion models during the image generation\nprocess. Unlike conventional approaches that treat all denoising steps equally,\nKSCU strategically focuses on pivotal steps with the most influence over the\nfinal outcome by dividing key steps for different concept unlearning tasks and\nfine-tuning the model only at those steps. This targeted approach reduces the\nnumber of parameter updates needed for effective unlearning, while maximizing\nthe retention of the model's generative capabilities.Through extensive\nbenchmark experiments, we demonstrate that KSCU effectively prevents T2I DMs\nfrom generating undesirable images while better retaining the model's\ngenerative capabilities. Our code will be released.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.06526v2", "cate": "cs.CV", "date": "2025-07-09", "updated": "2025-07-10"}
{"id": "2507.06952", "title": "What Has a Foundation Model Found? Using Inductive Bias to Probe for World Models", "authors": ["Keyon Vafa", "Peter G. Chang", "Ashesh Rambachan", "Sendhil Mullainathan"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      To appear in ICML 2025", "url": "http://arxiv.org/abs/2507.06952v2", "summary": "Foundation models are premised on the idea that sequence prediction can\nuncover deeper domain understanding, much like how Kepler's predictions of\nplanetary motion later led to the discovery of Newtonian mechanics. However,\nevaluating whether these models truly capture deeper structure remains a\nchallenge. We develop a technique for evaluating foundation models that\nexamines how they adapt to synthetic datasets generated from some postulated\nworld model. Our technique measures whether the foundation model's inductive\nbias aligns with the world model, and so we refer to it as an inductive bias\nprobe. Across multiple domains, we find that foundation models can excel at\ntheir training tasks yet fail to develop inductive biases towards the\nunderlying world model when adapted to new tasks. We particularly find that\nfoundation models trained on orbital trajectories consistently fail to apply\nNewtonian mechanics when adapted to new physics tasks. Further analysis reveals\nthat these models behave as if they develop task-specific heuristics that fail\nto generalize.", "comment": "To appear in ICML 2025", "pdf_url": "http://arxiv.org/pdf/2507.06952v2", "cate": "cs.LG", "date": "2025-07-09", "updated": "2025-07-10"}
{"id": "2410.08938", "title": "KinDEL: DNA-Encoded Library Dataset for Kinase Inhibitors", "authors": ["Benson Chen", "Tomasz Danel", "Gabriel H. S. Dreiman", "Patrick J. McEnaney", "Nikhil Jain", "Kirill Novikov", "Spurti Umesh Akki", "Joshua L. Turnbull", "Virja Atul Pandya", "Boris P. Belotserkovskii", "Jared Bryce Weaver", "Ankita Biswas", "Dat Nguyen", "Kent Gorday", "Mohammad Sultan", "Nathaniel Stanley", "Daniel M Whalen", "Divya Kanichar", "Christoph Klein", "Emily Fox", "R. Edward Watts"], "categories": ["q-bio.QM", "cs.LG"], "primary_category": "Subjects:       Quantitative Methods (q-bio.QM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.08938v2", "summary": "DNA-Encoded Libraries (DELs) represent a transformative technology in drug\ndiscovery, facilitating the high-throughput exploration of vast chemical\nspaces. Despite their potential, the scarcity of publicly available DEL\ndatasets presents a bottleneck for the advancement of machine learning\nmethodologies in this domain. To address this gap, we introduce KinDEL, one of\nthe largest publicly accessible DEL datasets and the first one that includes\nbinding poses from molecular docking experiments. Focused on two kinases,\nMitogen-Activated Protein Kinase 14 (MAPK14) and Discoidin Domain Receptor\nTyrosine Kinase 1 (DDR1), KinDEL includes 81 million compounds, offering a rich\nresource for computational exploration. Additionally, we provide comprehensive\nbiophysical assay validation data, encompassing both on-DNA and off-DNA\nmeasurements, which we use to evaluate a suite of machine learning techniques,\nincluding novel structure-based probabilistic models. We hope that our\nbenchmark, encompassing both 2D and 3D structures, will help advance the\ndevelopment of machine learning models for data-driven hit identification using\nDELs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.08938v2", "cate": "q-bio.QM", "date": "2024-10-11", "updated": "2025-07-10"}
{"id": "2503.01361", "title": "Statistical physics analysis of graph neural networks: Approaching optimality in the contextual stochastic block model", "authors": ["O. Duranthon", "L. Zdeborov√°"], "categories": ["cond-mat.dis-nn", "cs.LG"], "primary_category": "Subjects:       Disordered Systems and Neural Networks (cond-mat.dis-nn)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.01361v2", "summary": "Graph neural networks (GNNs) are designed to process data associated with\ngraphs. They are finding an increasing range of applications; however, as with\nother modern machine learning techniques, their theoretical understanding is\nlimited. GNNs can encounter difficulties in gathering information from nodes\nthat are far apart by iterated aggregation steps. This situation is partly\ncaused by so-called oversmoothing; and overcoming it is one of the practically\nmotivated challenges. We consider the situation where information is aggregated\nby multiple steps of convolution, leading to graph convolutional networks\n(GCNs). We analyze the generalization performance of a basic GCN, trained for\nnode classification on data generated by the contextual stochastic block model.\nWe predict its asymptotic performance by deriving the free energy of the\nproblem, using the replica method, in the high-dimensional limit. Calling depth\nthe number of convolutional steps, we show the importance of going to large\ndepth to approach the Bayes-optimality. We detail how the architecture of the\nGCN has to scale with the depth to avoid oversmoothing. The resulting large\ndepth limit can be close to the Bayes-optimality and leads to a continuous GCN.\nTechnically, we tackle this continuous limit via an approach that resembles\ndynamical mean-field theory (DMFT) with constraints at the initial and final\ntimes. An expansion around large regularization allows us to solve the\ncorresponding equations for the performance of the deep GCN. This promising\ntool may contribute to the analysis of further deep neural networks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.01361v2", "cate": "cond-mat.dis-nn", "date": "2025-03-03", "updated": "2025-07-10"}
{"id": "2504.10733", "title": "Cross-Problem Parameter Transfer in Quantum Approximate Optimization Algorithm: A Machine Learning Approach", "authors": ["Kien X. Nguyen", "Bao Bach", "Ilya Safro"], "categories": ["quant-ph", "cs.LG"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.10733v2", "summary": "Quantum Approximate Optimization Algorithm (QAOA) is one of the most\npromising candidates to achieve the quantum advantage in solving combinatorial\noptimization problems. The process of finding a good set of variational\nparameters in the QAOA circuit has proven to be challenging due to multiple\nfactors, such as barren plateaus. As a result, there is growing interest in\nexploiting parameter transferability, where parameter sets optimized for one\nproblem instance are transferred to another that could be more complex either\nto estimate the solution or to serve as a warm start for further optimization.\nBut can we transfer parameters from one class of problems to another?\nLeveraging parameter sets learned from a well-studied class of problems could\nhelp navigate the less studied one, reducing optimization overhead and\nmitigating performance pitfalls. In this paper, we study whether pretrained\nQAOA parameters of MaxCut can be used as is or to warm start the Maximum\nIndependent Set (MIS) circuits. Specifically, we design machine learning models\nto find good donor candidates optimized on MaxCut and apply their parameters to\nMIS acceptors. Our experimental results show that such parameter transfer can\nsignificantly reduce the number of optimization iterations required while\nachieving comparable approximation ratios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.10733v2", "cate": "quant-ph", "date": "2025-04-14", "updated": "2025-07-10"}
{"id": "2505.04631", "title": "Cryptogenic stroke and migraine: using probabilistic independence and machine learning to uncover latent sources of disease from the electronic health record", "authors": ["Joshua W. Betts", "John M. Still", "Thomas A. Lasko"], "categories": ["stat.AP", "cs.LG", "I.2.1; I.2.3; I.2.6; I.5.1; I.6.4; J.3"], "primary_category": "Subjects:       Applications (stat.AP)", "pdf_link": null, "comments": "Comments:      10 pages, 6 figures, 1 table, LaTeX. Manuscript has been peer-reviewed and accepted for presentation at the 2025 AMIA Symposium and publication in the AMIA proceedings. Changes from previous versions are minor and include fixed typos, adjusted formatting, rewording of some technical details, and a lengthier discussion regarding the source related to allergic rhinitis, per reviewer comments", "url": "http://arxiv.org/abs/2505.04631v2", "summary": "Migraine is a common but complex neurological disorder that doubles the\nlifetime risk of cryptogenic stroke (CS). However, this relationship remains\npoorly characterized, and few clinical guidelines exist to reduce this\nassociated risk. We therefore propose a data-driven approach to extract\nprobabilistically-independent sources from electronic health record (EHR) data\nand create a 10-year risk-predictive model for CS in migraine patients. These\nsources represent external latent variables acting on the causal graph\nconstructed from the EHR data and approximate root causes of CS in our\npopulation. A random forest model trained on patient expressions of these\nsources demonstrated good accuracy (ROC 0.771) and identified the top 10 most\npredictive sources of CS in migraine patients. These sources revealed that\npharmacologic interventions were the most important factor in minimizing CS\nrisk in our population and identified a factor related to allergic rhinitis as\na potential causative source of CS in migraine patients.", "comment": "10 pages, 6 figures, 1 table, LaTeX. Manuscript has been\n  peer-reviewed and accepted for presentation at the 2025 AMIA Symposium and\n  publication in the AMIA proceedings. Changes from previous versions are minor\n  and include fixed typos, adjusted formatting, rewording of some technical\n  details, and a lengthier discussion regarding the source related to allergic\n  rhinitis, per reviewer comments", "pdf_url": "http://arxiv.org/pdf/2505.04631v2", "cate": "stat.AP", "date": "2025-04-22", "updated": "2025-07-09"}
{"id": "2506.20573", "title": "LARP: Learner-Agnostic Robust Data Prefiltering", "authors": ["Kristian Minchev", "Dimitar Iliev Dimitrov", "Nikola Konstantinov"], "categories": ["stat.ML", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "Comments:      Presented at ICML 2025 Workshop on DataWorld: Unifying Data Curation Frameworks Across Domains", "url": "http://arxiv.org/abs/2506.20573v3", "summary": "The widespread availability of large public datasets is a key factor behind\nthe recent successes of statistical inference and machine learning methods.\nHowever, these datasets often contain some low-quality or contaminated data, to\nwhich many learning procedures are sensitive. Therefore, the question of\nwhether and how public datasets should be prefiltered to facilitate accurate\ndownstream learning arises. On a technical level this requires the construction\nof principled data prefiltering methods which are learner-agnostic robust, in\nthe sense of provably protecting a set of pre-specified downstream learners\nfrom corrupted data. In this work, we formalize the problem of Learner-Agnostic\nRobust data Prefiltering (LARP), which aims at finding prefiltering procedures\nthat minimize a worst-case loss over a pre-specified set of learners. We first\ninstantiate our framework in the context of scalar mean estimation with Huber\nestimators under the Huber data contamination model. We provide a hardness\nresult on a specific problem instance and analyze several natural prefiltering\nprocedures. Our theoretical results indicate that performing LARP on a\nheterogeneous set of learners leads to some loss in model performance compared\nto the alternative of prefiltering data for each learner/use-case individually.\nWe explore the resulting utility loss and its dependence on the problem\nparameters via extensive experiments on real-world image and tabular data,\nobserving statistically significant reduction in utility. Finally, we model the\ntrade-off between the utility drop and the cost of repeated (learner-specific)\nprefiltering within a game-theoretic framework and showcase benefits of LARP\nfor large datasets.", "comment": "Presented at ICML 2025 Workshop on DataWorld: Unifying Data Curation\n  Frameworks Across Domains", "pdf_url": "http://arxiv.org/pdf/2506.20573v3", "cate": "stat.ML", "date": "2025-06-25", "updated": "2025-07-10"}
{"id": "2507.00683", "title": "Testing the spin-bath view of self-attention: A Hamiltonian analysis of GPT-2 Transformer", "authors": ["Satadeep Bhattacharjee", "Seung-Cheol Lee"], "categories": ["cond-mat.mtrl-sci", "cs.LG"], "primary_category": "Subjects:       Materials Science (cond-mat.mtrl-sci)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.00683v3", "summary": "The recently proposed physics-based framework by Huo and\nJohnson~\\cite{huo2024capturing} models the attention mechanism of Large\nLanguage Models (LLMs) as an interacting two-body spin system, offering a\nfirst-principles explanation for phenomena like repetition and bias. Building\non this hypothesis, we extract the complete Query-Key weight matrices from a\nproduction-grade GPT-2 model and derive the corresponding effective Hamiltonian\nfor every attention head. From these Hamiltonians, we obtain analytic\n\\textit{phase boundaries} logit gap criteria that predict which token should\ndominate the next-token distribution for a given context. A systematic\nevaluation on 144 heads across 20 factual-recall prompts reveals a strong\nnegative correlation between the theoretical logit gaps and the model's\nempirical token rankings ($r\\approx-0.70$, $p<10^{-3}$).Targeted ablations\nfurther show that suppressing the heads most aligned with the spin-bath\npredictions induces the anticipated shifts in output probabilities, confirming\na causal link rather than a coincidental association. Taken together, our\nfindings provide the first strong empirical evidence for the spin-bath analogy\nin a production-grade model. In this work, we utilize the context-field lens,\nwhich provides physics-grounded interpretability and motivates the development\nof novel generative models bridging theoretical condensed matter physics and\nartificial intelligence.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00683v3", "cate": "cond-mat.mtrl-sci", "date": "2025-07-01", "updated": "2025-07-10"}
{"id": "2507.02275", "title": "It's Hard to Be Normal: The Impact of Noise on Structure-agnostic Estimation", "authors": ["Jikai Jin", "Lester Mackey", "Vasilis Syrgkanis"], "categories": ["stat.ML", "cs.LG", "econ.EM", "math.ST", "stat.ME", "stat.TH"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02275v2", "summary": "Structure-agnostic causal inference studies how well one can estimate a\ntreatment effect given black-box machine learning estimates of nuisance\nfunctions (like the impact of confounders on treatment and outcomes). Here, we\nfind that the answer depends in a surprising way on the distribution of the\ntreatment noise. Focusing on the partially linear model of\n\\citet{robinson1988root}, we first show that the widely adopted double machine\nlearning (DML) estimator is minimax rate-optimal for Gaussian treatment noise,\nresolving an open problem of \\citet{mackey2018orthogonal}. Meanwhile, for\nindependent non-Gaussian treatment noise, we show that DML is always suboptimal\nby constructing new practical procedures with higher-order robustness to\nnuisance errors. These \\emph{ACE} procedures use structure-agnostic cumulant\nestimators to achieve $r$-th order insensitivity to nuisance errors whenever\nthe $(r+1)$-st treatment cumulant is non-zero. We complement these core results\nwith novel minimax guarantees for binary treatments in the partially linear\nmodel. Finally, using synthetic demand estimation experiments, we demonstrate\nthe practical benefits of our higher-order robust estimators.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02275v2", "cate": "stat.ML", "date": "2025-07-03", "updated": "2025-07-10"}
