{"id": "2507.08844", "title": "Immutability Does Not Guarantee Trust: A Formal and Logical Refutation", "authors": ["Craig S Wright"], "categories": ["cs.CR", "cs.CC", "03B70, 68M10, 91A80", "F.4.1; D.4.6; C.2.2"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      8 pages", "url": "http://arxiv.org/abs/2507.08844v1", "summary": "It is frequently claimed in blockchain discourse that immutability guarantees\ntrust. This paper rigorously refutes that assertion. We define immutability as\nthe cryptographic persistence of historical states in an append-only data\nstructure and contrast it with trust, understood as a rational epistemic\nexpectation under uncertainty. Employing predicate logic, automata-theoretic\nmodels, and epistemic game-theoretic analysis, we demonstrate that immutability\nneither entails nor implies correctness, fairness, or credibility. Through\nformal constructions and counterexamples--including predictive fraud schemes\nand the phenomenon of garbage permanence--we show that the belief conflates\nstructural and epistemic domains. Immutability preserves all data equally,\nregardless of veracity. Therefore, the assertion that immutability guarantees\ntrust collapses under the weight of formal scrutiny.", "comment": "8 pages", "pdf_url": "http://arxiv.org/pdf/2507.08844v1", "cate": "cs.CR", "date": "2025-07-08", "updated": "2025-07-08"}
{"id": "2507.08853", "title": "Clio-X: AWeb3 Solution for Privacy-Preserving AI Access to Digital Archives", "authors": ["Victoria L. Lemieux", "Rosa Gil", "Faith Molosiwa", "Qihong Zhou", "Binming Li", "Roberto Garcia", "Luis De La Torre Cubillo", "Zehua Wang"], "categories": ["cs.CR", "cs.AI", "cs.CY", "cs.DL", "D.2.11, H.3.4, H.3.7, J.5"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      28 pages, 8 figures", "url": "http://arxiv.org/abs/2507.08853v1", "summary": "As archives turn to artificial intelligence to manage growing volumes of\ndigital records, privacy risks inherent in current AI data practices raise\ncritical concerns about data sovereignty and ethical accountability. This paper\nexplores how privacy-enhancing technologies (PETs) and Web3 architectures can\nsupport archives to preserve control over sensitive content while still being\nable to make it available for access by researchers. We present Clio-X, a\ndecentralized, privacy-first Web3 digital solution designed to embed PETs into\narchival workflows and support AI-enabled reference and access. Drawing on a\nuser evaluation of a medium-fidelity prototype, the study reveals both interest\nin the potential of the solution and significant barriers to adoption related\nto trust, system opacity, economic concerns, and governance. Using Rogers'\nDiffusion of Innovation theory, we analyze the sociotechnical dimensions of\nthese barriers and propose a path forward centered on participatory design and\ndecentralized governance through a Clio-X Decentralized Autonomous\nOrganization. By integrating technical safeguards with community-based\noversight, Clio-X offers a novel model to ethically deploy AI in cultural\nheritage contexts.", "comment": "28 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.08853v1", "cate": "cs.CR", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.08862", "title": "RAG Safety: Exploring Knowledge Poisoning Attacks to Retrieval-Augmented Generation", "authors": ["Tianzhe Zhao", "Jiaoyan Chen", "Yanchi Ru", "Haiping Zhu", "Nan Hu", "Jun Liu", "Qika Lin"], "categories": ["cs.CR", "cs.CL"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      13 pages, 6 figures", "url": "http://arxiv.org/abs/2507.08862v1", "summary": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nretrieving external data to mitigate hallucinations and outdated knowledge\nissues. Benefiting from the strong ability in facilitating diverse data sources\nand supporting faithful reasoning, knowledge graphs (KGs) have been\nincreasingly adopted in RAG systems, giving rise to KG-based RAG (KG-RAG)\nmethods. Though RAG systems are widely applied in various applications, recent\nstudies have also revealed its vulnerabilities to data poisoning attacks, where\nmalicious information injected into external knowledge sources can mislead the\nsystem into producing incorrect or harmful responses. However, these studies\nfocus exclusively on RAG systems using unstructured textual data sources,\nleaving the security risks of KG-RAG largely unexplored, despite the fact that\nKGs present unique vulnerabilities due to their structured and editable nature.\nIn this work, we conduct the first systematic investigation of the security\nissue of KG-RAG methods through data poisoning attacks. To this end, we\nintroduce a practical, stealthy attack setting that aligns with real-world\nimplementation. We propose an attack strategy that first identifies adversarial\ntarget answers and then inserts perturbation triples to complete misleading\ninference chains in the KG, increasing the likelihood that KG-RAG methods\nretrieve and rely on these perturbations during generation. Through extensive\nexperiments on two benchmarks and four recent KG-RAG methods, our attack\nstrategy demonstrates strong effectiveness in degrading KG-RAG performance,\neven with minimal KG perturbations. In-depth analyses are also conducted to\nunderstand the safety threats within the internal stages of KG-RAG systems and\nto explore the robustness of LLMs against adversarial knowledge.", "comment": "13 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.08862v1", "cate": "cs.CR", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.08864", "title": "Privacy-Utility-Fairness: A Balanced Approach to Vehicular-Traffic Management System", "authors": ["Poushali Sengupta", "Sabita Maharjan", "frank Eliassen", "Yan Zhang"], "categories": ["cs.CR", "cs.AI", "cs.MA"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      accepted in VTC 2025 Spring, Oslo, Norway", "url": "http://arxiv.org/abs/2507.08864v1", "summary": "Location-based vehicular traffic management faces significant challenges in\nprotecting sensitive geographical data while maintaining utility for traffic\nmanagement and fairness across regions. Existing state-of-the-art solutions\noften fail to meet the required level of protection against linkage attacks and\ndemographic biases, leading to privacy leakage and inequity in data analysis.\nIn this paper, we propose a novel algorithm designed to address the challenges\nregarding the balance of privacy, utility, and fairness in location-based\nvehicular traffic management systems. In this context, utility means providing\nreliable and meaningful traffic information, while fairness ensures that all\nregions and individuals are treated equitably in data use and decision-making.\nEmploying differential privacy techniques, we enhance data security by\nintegrating query-based data access with iterative shuffling and calibrated\nnoise injection, ensuring that sensitive geographical data remains protected.\nWe ensure adherence to epsilon-differential privacy standards by implementing\nthe Laplace mechanism. We implemented our algorithm on vehicular location-based\ndata from Norway, demonstrating its ability to maintain data utility for\ntraffic management and urban planning while ensuring fair representation of all\ngeographical areas without being overrepresented or underrepresented.\nAdditionally, we have created a heatmap of Norway based on our model,\nillustrating the privatized and fair representation of the traffic conditions\nacross various cities. Our algorithm provides privacy in vehicular traffic", "comment": "accepted in VTC 2025 Spring, Oslo, Norway", "pdf_url": "http://arxiv.org/pdf/2507.08864v1", "cate": "cs.CR", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.08878", "title": "Towards Privacy-Preserving and Personalized Smart Homes via Tailored Small Language Models", "authors": ["Xinyu Huang", "Leming Shen", "Zijing Ma", "Yuanqing Zheng"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08878v1", "summary": "Large Language Models (LLMs) have showcased remarkable generalizability in\nlanguage comprehension and hold significant potential to revolutionize\nhuman-computer interaction in smart homes. Existing LLM-based smart home\nassistants typically transmit user commands, along with user profiles and home\nconfigurations, to remote servers to obtain personalized services. However,\nusers are increasingly concerned about the potential privacy leaks to the\nremote servers. To address this issue, we develop HomeLLaMA, an on-device\nassistant for privacy-preserving and personalized smart home serving with a\ntailored small language model (SLM). HomeLLaMA learns from cloud LLMs to\ndeliver satisfactory responses and enable user-friendly interactions. Once\ndeployed, HomeLLaMA facilitates proactive interactions by continuously updating\nlocal SLMs and user profiles. To further enhance user experience while\nprotecting their privacy, we develop PrivShield to offer an optional\nprivacy-preserving LLM-based smart home serving for those users, who are\nunsatisfied with local responses and willing to send less-sensitive queries to\nremote servers. For evaluation, we build a comprehensive benchmark DevFinder to\nassess the service quality. Extensive experiments and user studies (M=100)\ndemonstrate that HomeLLaMA can provide personalized services while\nsignificantly enhancing user privacy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08878v1", "cate": "cs.CR", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.08904", "title": "CovertAuth: Joint Covert Communication and Authentication in MmWave Systems", "authors": ["Yulin Teng", "Keshuang Han", "Pinchang Zhang", "Xiaohong Jiang", "Yulong Shen", "Fu Xiao"], "categories": ["cs.CR", "eess.SP"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08904v1", "summary": "Beam alignment (BA) is a crucial process in millimeter-wave (mmWave)\ncommunications, enabling precise directional transmission and efficient link\nestablishment. However, due to characteristics like omnidirectional exposure\nand the broadcast nature of the BA phase, it is particularly vulnerable to\neavesdropping and identity impersonation attacks. To this end, this paper\nproposes a novel secure framework named CovertAuth, designed to enhance the\nsecurity of the BA phase against such attacks. In particular, to combat\neavesdropping attacks, the closed-form expressions of successful BA probability\nand covert transmission rate are first derived. Then, a covert communication\nproblem aimed at jointly optimizing beam training budget and transmission power\nis formulated to maximize covert communication rate, subject to the covertness\nrequirement. An alternating optimization algorithm combined with successive\nconvex approximation is employed to iteratively achieve optimal results. To\ncombat impersonation attacks, the mutual coupling effect of antenna array\nimpairments is explored as a device feature to design a weighted-sum energy\ndetector based physical layer authentication scheme. Moreover, theoretical\nmodels for authentication metrics like detection and false alarm probabilities\nare also provided to conduct performance analysis. Based on these models, an\noptimization problem is constructed to determine the optimal weight value that\nmaximizes authentication accuracy. Finally, simulation results demonstrate that\nCovertAuth presents improved detection accuracy under the same covertness\nrequirement compared to existing works.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08904v1", "cate": "cs.CR", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.08978", "title": "Characterizing Security and Privacy Teaching Standards for Schools in the United States", "authors": ["Katherine Limes", "Nathan Malkin", "Kelsey R. Fulton"], "categories": ["cs.CR", "cs.HC"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08978v1", "summary": "Increasingly, students begin learning aspects of security and privacy during\ntheir primary and secondary education (grades K-12 in the United States).\nIndividual U.S. states and some national organizations publish teaching\nstandards -- guidance that outlines expectations for what students should learn\n-- which often form the basis for course curricula. However, research has not\nyet examined what is covered by these standards and whether the topics align\nwith what the broader security and privacy community thinks students should\nknow. To shed light on these questions, we started by collecting computer\nscience teaching standards from all U.S. states and eight national\norganizations. After manually examining a total of 11,954 standards, we labeled\n3,778 of them as being related to security and privacy, further classifying\nthese into 103 topics. Topics ranged from technical subjects like encryption,\nnetwork security, and embedded systems to social subjects such as laws, ethics,\nand appropriate online behavior. Subsequently, we interviewed 11 security and\nprivacy professionals to examine how the teaching standards align with their\nexpectations. We found that, while the specific topics they mentioned mostly\noverlapped with those of existing standards, professionals placed a greater\nemphasis on threat modeling and security mindset.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08978v1", "cate": "cs.CR", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09022", "title": "SSH-Passkeys: Leveraging Web Authentication for Passwordless SSH", "authors": ["Moe Kayali", "Jonas Schmitt", "Franziska Roesner"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09022v1", "summary": "We propose a method for using Web Authentication APIs for SSH authentication,\nenabling passwordless remote server login with passkeys. These are credentials\nthat are managed throughout the key lifecycle by an authenticator on behalf of\nthe user and offer strong security guarantees.\n  Passwords remain the dominant mode of SSH authentication, despite their well\nknown flaws such as phishing and reuse. SSH's custom key-based authentication\nprotocol can alleviate these issues but remains vulnerable to key theft.\nAdditionally, it has poor usability, with even knowledgeable users leaking key\nmaterial and failing to verify fingerprints. Hence, effective key management\nremains a critical open area in SSH security. In contrast, WebAuthn is a modern\nauthentication standard designed to replace passwords, managing keys on behalf\nof the user. As a web API, this standard cannot integrate with SSH directly.\n  We propose a framework to integrate WebAuthn with SSH servers, by using UNIX\npluggable authentication modules (PAM). Our approach is backwards-compatible,\nsupports stock SSH servers and requires no new software client-side. It offers\nprotection for cryptographic material at rest, resistance to key leaks,\nphishing protection, privacy protection and attestation capability. None of\nthese properties are offered by passwords nor traditional SSH keys. We validate\nthese advantages with a structured, conceptual security analysis.\n  We develop a prototype implementation and conduct a user study to quantify\nthe security advantages of our proposal, testing our prototype with 40 SSH\nusers. The study confirms the security problems of SSH-keys, including 20% of\nthe cohort leaking their private keys. Our SSH-passkeys effectively address\nthese problems: we find a 90% reduction in critical security errors, while\nreducing authentication time by 4x on average.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09022v1", "cate": "cs.CR", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09074", "title": "Favicon Trojans: Executable Steganography Via Ico Alpha Channel Exploitation", "authors": ["David Noever", "Forrest McKee"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09074v1", "summary": "This paper presents a novel method of executable steganography using the\nalpha transparency layer of ICO image files to embed and deliver\nself-decompressing JavaScript payloads within web browsers. By targeting the\nleast significant bit (LSB) of non-transparent alpha layer image values, the\nproposed method successfully conceals compressed JavaScript code inside a\nfavicon image without affecting visual fidelity. Global web traffic loads 294\nbillion favicons daily and consume 0.9 petabytes of network bandwidth. A\nproof-of-concept implementation demonstrates that a 64x64 ICO image can embed\nup to 512 bytes uncompressed, or 0.8 kilobyte when using lightweight two-fold\ncompression. On page load, a browser fetches the favicon as part of standard\nbehavior, allowing an embedded loader script to extract and execute the payload\nentirely in memory using native JavaScript APIs and canvas pixel access. This\ncreates a two-stage covert channel requiring no additional network or user\nrequests. Testing across multiple browsers in both desktop and mobile\nenvironments confirms successful and silent execution of the embedded script.\nWe evaluate the threat model, relate it to polymorphic phishing attacks that\nevade favicon-based detection, and analyze evasion of content security policies\nand antivirus scanners. We map nine example MITRE ATT&CK Framework objectives\nto single line JavaScript to execute arbitrarily in ICO files. Existing\nsteganalysis and sanitization defenses are discussed, highlighting limitations\nin detecting or neutralizing alpha-channel exploits. The results demonstrate a\nstealthy and reusable attack surface that blurs traditional boundaries between\nstatic images and executable content. Because modern browsers report silent\nerrors when developers specifically fail to load ICO files, this attack surface\noffers an interesting example of required web behaviors that in turn compromise\nsecurity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09074v1", "cate": "cs.CR", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09133", "title": "CLIProv: A Contrastive Log-to-Intelligence Multimodal Approach for Threat Detection and Provenance Analysis", "authors": ["Jingwen Li", "Ru Zhang", "Jianyi Liu", "Wanguo Zhao"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09133v1", "summary": "With the increasing complexity of cyberattacks, the proactive and\nforward-looking nature of threat intelligence has become more crucial for\nthreat detection and provenance analysis. However, translating high-level\nattack patterns described in Tactics, Techniques, and Procedures (TTP)\nintelligence into actionable security policies remains a significant challenge.\nThis challenge arises from the semantic gap between high-level threat\nintelligence and low-level provenance log. To address this issue, this paper\nintroduces CLIProv, a novel approach for detecting threat behaviors in a host\nsystem. CLIProv employs a multimodal framework that leverages contrastive\nlearning to align the semantics of provenance logs with threat intelligence,\neffectively correlating system intrusion activities with attack patterns.\nFurthermore, CLIProv formulates threat detection as a semantic search problem,\nidentifying attack behaviors by searching for threat intelligence that is most\nsemantically similar to the log sequence. By leveraging attack pattern\ninformation in threat intelligence, CLIProv identifies TTPs and generates\ncomplete and concise attack scenarios. Experimental evaluations on standard\ndatasets show that CLIProv effectively identifies attack behaviors in system\nprovenance logs, offering valuable references for potential techniques.\nCompared to state-of-the-art methods, CLIProv achieves higher precision and\nsignificantly improved detection efficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09133v1", "cate": "cs.CR", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09231", "title": "Confidential Wrapped Ethereum", "authors": ["Artem Chystiakov", "Mariia Zhvanko"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09231v1", "summary": "Transparency is one of the key benefits of public blockchains. However, the\npublic visibility of transactions potentially compromises users' privacy. The\nfundamental challenge is to balance the intrinsic benefits of blockchain\nopenness with the vital need for individual confidentiality. The proposal\nsuggests creating a confidential version of wrapped Ethereum (cWETH) fully\nwithin the application layer. The solution combines the Elliptic Curve (EC)\nTwisted ElGamal-based commitment scheme to preserve confidentiality and the EC\nDiffie-Hellman (DH) protocol to introduce accessibility limited by the\ncommitment scheme. To enforce the correct generation of commitments,\nencryption, and decryption, zk-SNARKs are utilized.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09231v1", "cate": "cs.CR", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09288", "title": "Hybrid Quantum Security for IPsec", "authors": ["Javier Blanco-Romero", "Pedro Otero García", "Daniel Sobral-Blanco", "Florina Almenares Mendoza", "Ana Fernández Vilas", "Manuel Fernández-Veiga"], "categories": ["cs.CR", "cs.NI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      23 pages, 6 figures, quantum key distribution, post-quantum cryptography, IPsec security protocols", "url": "http://arxiv.org/abs/2507.09288v1", "summary": "Quantum Key Distribution (QKD) offers information-theoretic security against\nquantum computing threats, but integrating QKD into existing security protocols\nremains an unsolved challenge due to fundamental mismatches between\npre-distributed quantum keys and computational key exchange paradigms. This\npaper presents the first systematic comparison of sequential versus parallel\nhybrid QKD-PQC key establishment strategies for IPsec, revealing fundamental\nprotocol design principles that extend beyond specific implementations. We\nintroduce two novel approaches for incorporating QKD into Internet Key Exchange\nversion 2 (IKEv2) with support for both ETSI GS QKD 004 stateful and ETSI GS\nQKD 014 stateless API specifications: (1) a pure QKD approach that replaces\ncomputational key derivation with identifier-based quantum key coordination,\nand (2) a unified QKD-KEM abstraction that enables parallel composition of\nquantum and post-quantum cryptographic methods within existing protocol\nframeworks. Our key insight is that parallel hybrid approaches eliminate the\nmultiplicative latency penalties inherent in sequential methods mandated by RFC\n9370, achieving significant performance improvements under realistic network\nconditions. Performance evaluation using a Docker-based testing framework with\nIDQuantique QKD hardware demonstrates that the parallel hybrid approach\nsignificantly outperforms sequential methods under network latency conditions,\nwhile pure QKD achieves minimal bandwidth overhead through identifier-based key\ncoordination. Our implementations provide practical quantum-enhanced IPsec\nsolutions suitable for critical infrastructure deployments requiring\ndefense-in-depth security.", "comment": "23 pages, 6 figures, quantum key distribution, post-quantum\n  cryptography, IPsec security protocols", "pdf_url": "http://arxiv.org/pdf/2507.09288v1", "cate": "cs.CR", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09301", "title": "Implementing and Evaluating Post-Quantum DNSSEC in CoreDNS", "authors": ["Julio Gento Suela", "Javier Blanco-Romero", "Florina Almenares Mendoza", "Daniel Díaz-Sánchez"], "categories": ["cs.CR", "cs.NI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09301v1", "summary": "The emergence of quantum computers poses a significant threat to current\nsecure service, application and/or protocol implementations that rely on RSA\nand ECDSA algorithms, for instance DNSSEC, because public-key cryptography\nbased on number factorization or discrete logarithm is vulnerable to quantum\nattacks. This paper presents the integration of post-quantum cryptographic\n(PQC) algorithms into CoreDNS to enable quantum-resistant DNSSEC functionality.\nWe have developed a plugin that extends CoreDNS with support for five PQC\nsignature algorithm families: ML-DSA, FALCON, SPHINCS+, MAYO, and SNOVA. Our\nimplementation maintains compatibility with existing DNS resolution flows while\nproviding on-the-fly signing using quantum-resistant signatures. A benchmark\nhas been performed and performance evaluation results reveal significant\ntrade-offs between security and efficiency. The results indicate that while PQC\nalgorithms introduce operational overhead, several candidates offer viable\ncompromises for transitioning DNSSEC to quantum-resistant cryptography.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09301v1", "cate": "cs.CR", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09354", "title": "Backscatter Device-aided Integrated Sensing and Communication: A Pareto Optimization Framework", "authors": ["Yifan Zhang", "Yu Bai", "Riku Jantti", "Zheng Yan", "Christos Masouros", "Zhu Han"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09354v1", "summary": "Integrated sensing and communication (ISAC) systems potentially encounter\nsignificant performance degradation in densely obstructed urban and\nnon-line-of-sight scenarios, thus limiting their effectiveness in practical\ndeployments. To deal with these challenges, this paper proposes a backscatter\ndevice (BD)-assisted ISAC system, which leverages passive BDs naturally\ndistributed in underlying environments for performance enhancement. These\nambient devices can enhance sensing accuracy and communication reliability by\nproviding additional reflective signal paths. In this system, we define the\nPareto boundary characterizing the trade-off between sensing mutual information\n(SMI) and communication rates to provide fundamental insights for its design.\nTo derive the boundary, we formulate a performance optimization problem within\nan orthogonal frequency division multiplexing (OFDM) framework, by jointly\noptimizing time-frequency resource element (RE) allocation, transmit power\nmanagement, and BD modulation decisions. To tackle the non-convexity of the\nproblem, we decompose it into three subproblems, solved iteratively through a\nblock coordinate descent (BCD) algorithm. Specifically, the RE subproblem is\naddressed using the successive convex approximation (SCA) method, the power\nsubproblem is solved using an augmented Lagrangian combined water-filling\nmethod, and the BD modulation subproblem is tackled using semidefinite\nrelaxation (SDR) methods. Additionally, we demonstrate the generality of the\nproposed system by showing its adaptability to bistatic ISAC scenarios and MIMO\nsettings. Finally, extensive simulation results validate the effectiveness of\nthe proposed system and its superior performance compared to existing\nstate-of-the-art ISAC schemes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09354v1", "cate": "cs.CR", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09411", "title": "LLMalMorph: On The Feasibility of Generating Variant Malware using Large-Language-Models", "authors": ["Md Ajwad Akil", "Adrian Shuai Li", "Imtiaz Karim", "Arun Iyengar", "Ashish Kundu", "Vinny Parla", "Elisa Bertino"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      13 pages", "url": "http://arxiv.org/abs/2507.09411v1", "summary": "Large Language Models (LLMs) have transformed software development and\nautomated code generation. Motivated by these advancements, this paper explores\nthe feasibility of LLMs in modifying malware source code to generate variants.\nWe introduce LLMalMorph, a semi-automated framework that leverages semantical\nand syntactical code comprehension by LLMs to generate new malware variants.\nLLMalMorph extracts function-level information from the malware source code and\nemploys custom-engineered prompts coupled with strategically defined code\ntransformations to guide the LLM in generating variants without\nresource-intensive fine-tuning. To evaluate LLMalMorph, we collected 10 diverse\nWindows malware samples of varying types, complexity and functionality and\ngenerated 618 variants. Our thorough experiments demonstrate that it is\npossible to reduce the detection rates of antivirus engines of these malware\nvariants to some extent while preserving malware functionalities. In addition,\ndespite not optimizing against any Machine Learning (ML)-based malware\ndetectors, several variants also achieved notable attack success rates against\nan ML-based malware classifier. We also discuss the limitations of current LLM\ncapabilities in generating malware variants from source code and assess where\nthis emerging technology stands in the broader context of malware variant\ngeneration.", "comment": "13 pages", "pdf_url": "http://arxiv.org/pdf/2507.09411v1", "cate": "cs.CR", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09453", "title": "SmartphoneDemocracy: Privacy-Preserving E-Voting on Decentralized Infrastructure using Novel European Identity", "authors": ["Michał Jóźwik", "Johan Pouwelse"], "categories": ["cs.CR", "cs.DC"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      18 pages, 4 figures", "url": "http://arxiv.org/abs/2507.09453v1", "summary": "The digitization of democratic processes promises greater accessibility but\npresents challenges in terms of security, privacy, and verifiability. Existing\nelectronic voting systems often rely on centralized architectures, creating\nsingle points of failure and forcing too much trust in authorities, which\ncontradicts democratic principles. This research addresses the challenge of\ncreating a secure, private e-voting system with minimized trust dependencies\ndesigned for the most versatile personal device: the smartphone. We introduce\nSmartphoneDemocracy, a novel e-voting protocol that combines three key\ntechnologies: the emerging European Digital Identity (EUDI) Wallet for\nSybil-resistant identity verification, Zero-Knowledge Proofs for\nprivacy-preserving validation, and a peer-to-peer blockchain (TrustChain) for a\nresilient, serverless public bulletin board. Our protocol enables voters to\nregister and cast ballots anonymously and verifiably directly from their\nsmartphones. We provide a detailed protocol design, a security analysis against\na defined threat model, and a performance evaluation demonstrating that the\ncomputational and network overhead is feasible for medium- to large-scale\nelections. By developing and prototyping this system, we demonstrate a viable\npath to empower citizens with a trustworthy, accessible, and user-controlled\ndigital voting experience.", "comment": "18 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.09453v1", "cate": "cs.CR", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.08884", "title": "Agent-based visualization of streaming text", "authors": ["Jordan Riley Benson", "David Crist", "Phil Lafleur", "Benjamin Watson"], "categories": ["cs.MA", "cs.GR"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08884v1", "summary": "We present a visualization infrastructure that maps data elements to agents,\nwhich have behaviors parameterized by those elements. Dynamic visualizations\nemerge as the agents change position, alter appearance and respond to one\nother. Agents move to minimize the difference between displayed agent-to-agent\ndistances, and an input matrix of ideal distances. Our current application is\nvisualization of streaming text. Each agent represents a significant word,\nvisualizing it by displaying the word itself, centered in a circle sized by the\nfrequency of word occurrence. We derive the ideal distance matrix from word\ncooccurrence, mapping higher co-occurrence to lower distance. To depict\nco-occurrence in its textual context, the ratio of intersection to circle area\napproximates the ratio of word co-occurrence to frequency. A networked backend\nprocess gathers articles from news feeds, blogs, Digg or Twitter, exploiting\nonline search APIs to focus on user-chosen topics. Resulting visuals reveal the\nprimary topics in text streams as clusters, with agent-based layout moving\nwithout instability as data streams change dynamically.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08884v1", "cate": "cs.MA", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.09508", "title": "A Mixture of Linear Corrections Generates Secure Code", "authors": ["Weichen Yu", "Ravi Mangal", "Terry Zhuo", "Matt Fredrikson", "Corina S. Pasareanu"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09508v1", "summary": "Large language models (LLMs) have become proficient at sophisticated\ncode-generation tasks, yet remain ineffective at reliably detecting or avoiding\ncode vulnerabilities. Does this deficiency stem from insufficient learning\nabout code vulnerabilities, or is it merely a result of ineffective prompting?\nUsing representation engineering techniques, we investigate whether LLMs\ninternally encode the concepts necessary to identify code vulnerabilities. We\nfind that current LLMs encode precise internal representations that distinguish\nvulnerable from secure code--achieving greater accuracy than standard prompting\napproaches. Leveraging these vulnerability-sensitive representations, we\ndevelop an inference-time steering technique that subtly modulates the model's\ntoken-generation probabilities through a mixture of corrections (MoC). Our\nmethod effectively guides LLMs to produce less vulnerable code without\ncompromising functionality, demonstrating a practical approach to controlled\nvulnerability management in generated code. Notably, MoC enhances the security\nratio of Qwen2.5-Coder-7B by 8.9\\%, while simultaneously improving\nfunctionality on HumanEval pass@1 by 2.1\\%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09508v1", "cate": "cs.CR", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.08944", "title": "Optimizing Sequential Multi-Step Tasks with Parallel LLM Agents", "authors": ["Enhao Zhang", "Erkang Zhu", "Gagan Bansal", "Adam Fourney", "Hussein Mozannar", "Jack Gerrits"], "categories": ["cs.MA", "cs.AI"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "Comments:      ICML 2025 Workshop on MAS", "url": "http://arxiv.org/abs/2507.08944v1", "summary": "Large language model (LLM)-based multi-agent systems have demonstrated\nremarkable promise for tackling complex tasks by breaking them down into\nsubtasks that are iteratively planned, executed, observed, and refined. Despite\ntheir effectiveness, these systems often incur high latency because real-world\nproblems frequently demand multiple iterative cycles of reasoning steps. To\naddress this challenge, we propose M1-Parallel, a framework that concurrently\nruns multiple multi-agent teams in parallel to uncover distinct solution paths.\nBy leveraging an event-driven communication model with asynchronous messaging,\nM1-Parallel efficiently capitalizes on the inherent diversity of valid plans to\neither reduce end-to-end latency or boost task completion rates. Our\nexperiments on complex tasks show that M1-Parallel with early termination\nachieves up to $2.2\\times$ speedup while preserving accuracy, and that\nM1-Parallel with aggregation yields higher task completion rates. We further\ninvestigate strategies aimed at encouraging diverse execution plans but observe\nno additional performance gains over repeated sampling. Overall, these findings\nunderscore the potential of parallel plan execution for optimizing multi-agent\nsystems for real-world, high-complexity reasoning tasks.", "comment": "ICML 2025 Workshop on MAS", "pdf_url": "http://arxiv.org/pdf/2507.08944v1", "cate": "cs.MA", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09564", "title": "A Login Page Transparency and Visual Similarity Based Zero Day Phishing Defense Protocol", "authors": ["Gaurav Varshney", "Akanksha Raj", "Divya Sangwan", "Sharif Abuadbba", "Rina Mishra", "Yansong Gao"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09564v1", "summary": "Phishing is a prevalent cyberattack that uses look-alike websites to deceive\nusers into revealing sensitive information. Numerous efforts have been made by\nthe Internet community and security organizations to detect, prevent, or train\nusers to avoid falling victim to phishing attacks. Most of this research over\nthe years has been highly diverse and application-oriented, often serving as\nstandalone solutions for HTTP clients, servers, or third parties. However,\nlimited work has been done to develop a comprehensive or proactive\nprotocol-oriented solution to effectively counter phishing attacks. Inspired by\nthe concept of certificate transparency, which allows certificates issued by\nCertificate Authorities (CAs) to be publicly verified by clients, thereby\nenhancing transparency, we propose a concept called Page Transparency (PT) for\nthe web. The proposed PT requires login pages that capture users' sensitive\ninformation to be publicly logged via PLS and made available to web clients for\nverification. The pages are verified to be logged using cryptographic proofs.\nSince all pages are logged on a PLS and visually compared with existing pages\nthrough a comprehensive visual page-matching algorithm, it becomes impossible\nfor an attacker to register a deceptive look-alike page on the PLS and receive\nthe cryptographic proof required for client verification. All implementations\noccur on the client side, facilitated by the introduction of a new HTTP PT\nheader, eliminating the need for platform-specific changes or the installation\nof third-party solutions for phishing prevention.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09564v1", "cate": "cs.CR", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.08960", "title": "How to Train a Leader: Hierarchical Reasoning in Multi-Agent LLMs", "authors": ["Andrew Estornell", "Jean-Francois Ton", "Muhammad Faaiz Taufiq", "Hang Li"], "categories": ["cs.MA", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08960v1", "summary": "Large Language Models (LLMs) have achieved strong performance on a wide range\nof complex reasoning tasks, yet further gains are often possible by leveraging\nthe complementary strengths of multiple models. While multi-agent frameworks\ncan improve solution quality by leveraging multiple LLMs, existing methods are\noften computationally expensive, both at training and inference time. In this\nwork, we introduce a hierarchical multi-agent framework that addresses these\nchallenges by training only a single leader LLM to coordinate a team of\nuntrained peer agents. To this end, we propose Multi-agent guided Leader Policy\n\\textbf{O}ptimization (MLPO), a novel approach which trains the leader to\nevaluate and synthesize agent responses without auxiliary value networks or\nexplicit agent feedback. Leaders trained with MLPO exhibit improved performance\nnot only when interacting with the agent team at inference time, but also enjoy\nimproved performance when deployed in single-agent settings without the team.\nEmpirical results on Big-Bench Hard (BBH), MATH, and MMLU demonstrate that our\nframework achieves substantial performance improvements over both single-agent\nand multi-agent baselines. Our results highlight the effectiveness and\nefficiency of training a single, flexible leader for collaborative reasoning in\nmulti-agent LLM systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08960v1", "cate": "cs.MA", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09579", "title": "PromptChain: A Decentralized Web3 Architecture for Managing AI Prompts as Digital Assets", "authors": ["Marc Bara"], "categories": ["cs.CR", "cs.DC", "68M14, 94A60", "H.3.4; K.6.5; H.3.5"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      14 pages, 6 figures", "url": "http://arxiv.org/abs/2507.09579v1", "summary": "We present PromptChain, a decentralized Web3 architecture that establishes AI\nprompts as first-class digital assets with verifiable ownership, version\ncontrol, and monetization capabilities. Current centralized platforms lack\nmechanisms for proper attribution, quality assurance, or fair compensation for\nprompt creators. PromptChain addresses these limitations through a novel\nintegration of IPFS for immutable storage, smart contracts for governance, and\ntoken incentives for community curation. Our design includes: (1) a\ncomprehensive metadata schema for cross-model compatibility, (2) a\nstake-weighted validation mechanism to align incentives, and (3) a token\neconomy that rewards contributors proportionally to their impact. The proposed\narchitecture demonstrates how decentralized systems could potentially match\ncentralized alternatives in efficiency while providing superior ownership\nguarantees and censorship resistance through blockchain-anchored provenance\ntracking. By decoupling prompts from specific AI models or outputs, this work\nestablishes the foundation for an open ecosystem of human-AI collaboration in\nthe Web3 era, representing the first systematic treatment of prompts as\nstandalone digital assets with dedicated decentralized infrastructure.", "comment": "14 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.09579v1", "cate": "cs.CR", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09367", "title": "Simulation for All: A Step-by-Step Cookbook for Developing Human-Centered Multi-Agent Transportation Simulators", "authors": ["Shiva Azimi", "Arash Tavakoli"], "categories": ["cs.MA"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09367v1", "summary": "As cities evolve toward more complex and multimodal transportation systems,\nthe need for human-centered multi-agent simulation tools has never been more\nurgent. Yet most existing platforms remain limited - they often separate\ndifferent types of road users, rely on scripted or pre-defined behaviors,\noverlook public transit users as active participants, and are rarely designed\nwith accessibility in mind for non-technical users. To address this gap, this\npaper presents the specifications of a multi-agent simulation platform designed\nto support real-time, human-centered, and immersive studies of all road users,\naccompanied by open-source scripts for replication. Using high-fidelity\nimmersive virtual environments, our platform enables interaction across public\ntransit users, pedestrians, cyclists, automated vehicles, and drivers. The\narchitecture is modular, extensible, and designed for accessibility. The system\nintegrates hardware-specific modules - including an omnidirectional treadmill,\na seating arrangement, a smart trainer, and an actuated cockpit. Additionally,\nthe platform collects multimodal physiological, neurological, and behavioral\ndata through embedded sensing devices such as functional near-infrared\nspectroscopy (fNIRS), eye tracking, and wrist-based biosensors. To show the\nusability of this system, we present three use cases. Simulation for All aims\nto lower the barrier to entry for high-fidelity transportation simulation,\nsupport experimentation across disciplines, and advance our understanding of\nmultimodal mobility in complex urban environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09367v1", "cate": "cs.MA", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09580", "title": "AICrypto: A Comprehensive Benchmark For Evaluating Cryptography Capabilities of Large Language Models", "authors": ["Yu Wang", "Yijian Liu", "Liheng Ji", "Han Luo", "Wenjie Li", "Xiaofei Zhou", "Chiyun Feng", "Puji Wang", "Yuhan Cao", "Geyuan Zhang", "Xiaojian Li", "Rongwu Xu", "Yilei Chen", "Tianxing He"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09580v1", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na variety of domains. However, their applications in cryptography, which serves\nas a foundational pillar of cybersecurity, remain largely unexplored. To\naddress this gap, we propose \\textbf{AICrypto}, the first comprehensive\nbenchmark designed to evaluate the cryptographic capabilities of LLMs. The\nbenchmark comprises 135 multiple-choice questions, 150 capture-the-flag (CTF)\nchallenges, and 18 proof problems, covering a broad range of skills from\nfactual memorization to vulnerability exploitation and formal reasoning. All\ntasks are carefully reviewed or constructed by cryptography experts to ensure\ncorrectness and rigor. To support automated evaluation of CTF challenges, we\ndesign an agent-based framework. To gain deeper insight into the current state\nof cryptographic proficiency in LLMs, we introduce human expert performance\nbaselines for comparison across all task types. Our evaluation of 17 leading\nLLMs reveals that state-of-the-art models match or even surpass human experts\nin memorizing cryptographic concepts, exploiting common vulnerabilities, and\nroutine proofs. However, they still lack a deep understanding of abstract\nmathematical concepts and struggle with tasks that require multi-step reasoning\nand dynamic analysis. We hope this work could provide insights for future\nresearch on LLMs in cryptographic applications. Our code and dataset are\navailable at https://aicryptobench.github.io.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09580v1", "cate": "cs.CR", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09409", "title": "Adaptive Social Learning using Theory of Mind", "authors": ["Lance Ying", "Ryan Truong", "Joshua B. Tenenbaum", "Samuel J. Gershman"], "categories": ["cs.MA"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "Comments:      7 pages, 4 figure; paper published at CogSci 2025", "url": "http://arxiv.org/abs/2507.09409v1", "summary": "Social learning is a powerful mechanism through which agents learn about the\nworld from others. However, humans don't always choose to observe others, since\nsocial learning can carry time and cognitive resource costs. How do people\nbalance social and non-social learning? In this paper, we propose a rational\nmentalizing model of the decision to engage in social learning. This model\nestimates the utility of social learning by reasoning about the other agent's\ngoal and the informativity of their future actions. It then weighs the utility\nof social learning against the utility of self-exploration (non-social\nlearning). Using a multi-player treasure hunt game, we show that our model can\nquantitatively capture human trade-offs between social and non-social learning.\nFurthermore, our results indicate that these two components allow agents to\nflexibly apply social learning to achieve their goals more efficiently.", "comment": "7 pages, 4 figure; paper published at CogSci 2025", "pdf_url": "http://arxiv.org/pdf/2507.09409v1", "cate": "cs.MA", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.08851", "title": "OTAS: Open-vocabulary Token Alignment for Outdoor Segmentation", "authors": ["Simon Schwaiger", "Stefan Thalhammer", "Wilfried Wöber", "Gerald Steinbauer-Wagner"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08851v1", "summary": "Understanding open-world semantics is critical for robotic planning and\ncontrol, particularly in unstructured outdoor environments. Current\nvision-language mapping approaches rely on object-centric segmentation priors,\nwhich often fail outdoors due to semantic ambiguities and indistinct semantic\nclass boundaries. We propose OTAS - an Open-vocabulary Token Alignment method\nfor Outdoor Segmentation. OTAS overcomes the limitations of open-vocabulary\nsegmentation models by extracting semantic structure directly from the output\ntokens of pretrained vision models. By clustering semantically similar\nstructures across single and multiple views and grounding them in language,\nOTAS reconstructs a geometrically consistent feature field that supports\nopen-vocabulary segmentation queries. Our method operates zero-shot, without\nscene-specific fine-tuning, and runs at up to ~17 fps. OTAS provides a minor\nIoU improvement over fine-tuned and open-vocabulary 2D segmentation methods on\nthe Off-Road Freespace Detection dataset. Our model achieves up to a 151% IoU\nimprovement over open-vocabulary mapping methods in 3D segmentation on\nTartanAir. Real-world reconstructions demonstrate OTAS' applicability to\nrobotic applications. The code and ROS node will be made publicly available\nupon paper acceptance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08851v1", "cate": "cs.RO", "date": "2025-07-08", "updated": "2025-07-08"}
{"id": "2507.09607", "title": "Efficient Private Inference Based on Helper-Assisted Malicious Security Dishonest Majority MPC", "authors": ["Kaiwen Wang", "Yuehan Dong", "Junchao Fan", "Xiaolin Chang"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09607v1", "summary": "Private inference based on Secure Multi-Party Computation (MPC) addresses\ndata privacy risks in Machine Learning as a Service (MLaaS). However, existing\nMPC-based private inference frameworks focuses on semi-honest or honest\nmajority models, whose threat models are overly idealistic, while malicious\nsecurity dishonest majority models face the challenge of low efficiency. To\nbalance security and efficiency, we propose a private inference framework using\nHelper-Assisted Malicious Security Dishonest Majority Model (HA-MSDM). This\nframework includes our designed five MPC protocols and a co-optimized strategy.\nThese protocols achieve efficient fixed-round multiplication, exponentiation,\nand polynomial operations, providing foundational primitives for private\ninference. The co-optimized strategy balances inference efficiency and\naccuracy. To enhance efficiency, we employ polynomial approximation for\nnonlinear layers. For improved accuracy, we construct sixth-order polynomial\napproximation within a fixed interval to achieve high-precision activation\nfunction fitting and introduce parameter-adjusted batch normalization layers to\nconstrain the activation escape problem. Benchmark results on LeNet and AlexNet\nshow our framework achieves 2.4-25.7x speedup in LAN and 1.3-9.5x acceleration\nin WAN compared to state-of-the-art frameworks (IEEE S&P'25), maintaining high\naccuracy with only 0.04%-1.08% relative errors.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09607v1", "cate": "cs.CR", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09788", "title": "TinyTroupe: An LLM-powered Multiagent Persona Simulation Toolkit", "authors": ["Paulo Salem", "Robert Sim", "Christopher Olsen", "Prerit Saxena", "Rafael Barcelos", "Yi Ding"], "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.HC", "I.2.11; I.6.5; I.6.7"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "Comments:      9 pages. Preprint to be submitted to peer-review", "url": "http://arxiv.org/abs/2507.09788v1", "summary": "Recent advances in Large Language Models (LLM) have led to a new class of\nautonomous agents, renewing and expanding interest in the area. LLM-powered\nMultiagent Systems (MAS) have thus emerged, both for assistive and simulation\npurposes, yet tools for realistic human behavior simulation -- with its\ndistinctive challenges and opportunities -- remain underdeveloped. Existing MAS\nlibraries and tools lack fine-grained persona specifications, population\nsampling facilities, experimentation support, and integrated validation, among\nother key capabilities, limiting their utility for behavioral studies, social\nsimulation, and related applications. To address these deficiencies, in this\nwork we introduce TinyTroupe, a simulation toolkit enabling detailed persona\ndefinitions (e.g., nationality, age, occupation, personality, beliefs,\nbehaviors) and programmatic control via numerous LLM-driven mechanisms. This\nallows for the concise formulation of behavioral problems of practical\ninterest, either at the individual or group level, and provides effective means\nfor their solution. TinyTroupe's components are presented using representative\nworking examples, such as brainstorming and market research sessions, thereby\nsimultaneously clarifying their purpose and demonstrating their usefulness.\nQuantitative and qualitative evaluations of selected aspects are also provided,\nhighlighting possibilities, limitations, and trade-offs. The approach, though\nrealized as a specific Python implementation, is meant as a novel conceptual\ncontribution, which can be partially or fully incorporated in other contexts.\nThe library is available as open source at\nhttps://github.com/microsoft/tinytroupe.", "comment": "9 pages. Preprint to be submitted to peer-review", "pdf_url": "http://arxiv.org/pdf/2507.09788v1", "cate": "cs.MA", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.08885", "title": "AirScape: An Aerial Generative World Model with Motion Controllability", "authors": ["Baining Zhao", "Rongze Tang", "Mingyuan Jia", "Ziyou Wang", "Fanghang Man", "Xin Zhang", "Yu Shang", "Weichen Zhang", "Chen Gao", "Wei Wu", "Xin Wang", "Xinlei Chen", "Yong Li"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08885v1", "summary": "How to enable robots to predict the outcomes of their own motion intentions\nin three-dimensional space has been a fundamental problem in embodied\nintelligence. To explore more general spatial imagination capabilities, here we\npresent AirScape, the first world model designed for six-degree-of-freedom\naerial agents. AirScape predicts future observation sequences based on current\nvisual inputs and motion intentions. Specifically, we construct an dataset for\naerial world model training and testing, which consists of 11k video-intention\npairs. This dataset includes first-person-view videos capturing diverse drone\nactions across a wide range of scenarios, with over 1,000 hours spent\nannotating the corresponding motion intentions. Then we develop a two-phase\ntraining schedule to train a foundation model -- initially devoid of embodied\nspatial knowledge -- into a world model that is controllable by motion\nintentions and adheres to physical spatio-temporal constraints.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08885v1", "cate": "cs.RO", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.09624", "title": "CAN-Trace Attack: Exploit CAN Messages to Uncover Driving Trajectories", "authors": ["Xiaojie Lin", "Baihe Ma", "Xu Wang", "Guangsheng Yu", "Ying He", "Wei Ni", "Ren Ping Liu"], "categories": ["cs.CR", "cs.LG"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09624v1", "summary": "Driving trajectory data remains vulnerable to privacy breaches despite\nexisting mitigation measures. Traditional methods for detecting driving\ntrajectories typically rely on map-matching the path using Global Positioning\nSystem (GPS) data, which is susceptible to GPS data outage. This paper\nintroduces CAN-Trace, a novel privacy attack mechanism that leverages\nController Area Network (CAN) messages to uncover driving trajectories, posing\na significant risk to drivers' long-term privacy. A new trajectory\nreconstruction algorithm is proposed to transform the CAN messages,\nspecifically vehicle speed and accelerator pedal position, into weighted graphs\naccommodating various driving statuses. CAN-Trace identifies driving\ntrajectories using graph-matching algorithms applied to the created graphs in\ncomparison to road networks. We also design a new metric to evaluate matched\ncandidates, which allows for potential data gaps and matching inaccuracies.\nEmpirical validation under various real-world conditions, encompassing\ndifferent vehicles and driving regions, demonstrates the efficacy of CAN-Trace:\nit achieves an attack success rate of up to 90.59% in the urban region, and\n99.41% in the suburban region.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09624v1", "cate": "cs.CR", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09901", "title": "Large Population Models", "authors": ["Ayush Chopra"], "categories": ["cs.MA", "cs.AI"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "Comments:      Aggregation of Several Papers from MIT PhD Research. this http URL", "url": "http://arxiv.org/abs/2507.09901v1", "summary": "Many of society's most pressing challenges, from pandemic response to supply\nchain disruptions to climate adaptation, emerge from the collective behavior of\nmillions of autonomous agents making decisions over time. Large Population\nModels (LPMs) offer an approach to understand these complex systems by\nsimulating entire populations with realistic behaviors and interactions at\nunprecedented scale. LPMs extend traditional modeling approaches through three\nkey innovations: computational methods that efficiently simulate millions of\nagents simultaneously, mathematical frameworks that learn from diverse\nreal-world data streams, and privacy-preserving communication protocols that\nbridge virtual and physical environments. This allows researchers to observe\nhow agent behavior aggregates into system-level outcomes and test interventions\nbefore real-world implementation. While current AI advances primarily focus on\ncreating \"digital humans\" with sophisticated individual capabilities, LPMs\ndevelop \"digital societies\" where the richness of interactions reveals emergent\nphenomena. By bridging individual agent behavior and population-scale dynamics,\nLPMs offer a complementary path in AI research illuminating collective\nintelligence and providing testing grounds for policies and social innovations\nbefore real-world deployment. We discuss the technical foundations and some\nopen problems here. LPMs are implemented by the AgentTorch framework\n(github.com/AgentTorch/AgentTorch)", "comment": "Aggregation of Several Papers from MIT PhD Research.\n  github.com/AgentTorch/AgentTorch", "pdf_url": "http://arxiv.org/pdf/2507.09901v1", "cate": "cs.MA", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.08901", "title": "End-to-End Generation of City-Scale Vectorized Maps by Crowdsourced Vehicles", "authors": ["Zebang Feng", "Miao Fan", "Bao Liu", "Shengtong Xu", "Haoyi Xiong"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted by ITSC'25", "url": "http://arxiv.org/abs/2507.08901v1", "summary": "High-precision vectorized maps are indispensable for autonomous driving, yet\ntraditional LiDAR-based creation is costly and slow, while single-vehicle\nperception methods lack accuracy and robustness, particularly in adverse\nconditions. This paper introduces EGC-VMAP, an end-to-end framework that\novercomes these limitations by generating accurate, city-scale vectorized maps\nthrough the aggregation of data from crowdsourced vehicles. Unlike prior\napproaches, EGC-VMAP directly fuses multi-vehicle, multi-temporal map elements\nperceived onboard vehicles using a novel Trip-Aware Transformer architecture\nwithin a unified learning process. Combined with hierarchical matching for\nefficient training and a multi-objective loss, our method significantly\nenhances map accuracy and structural robustness compared to single-vehicle\nbaselines. Validated on a large-scale, multi-city real-world dataset, EGC-VMAP\ndemonstrates superior performance, enabling a scalable, cost-effective solution\nfor city-wide mapping with a reported 90\\% reduction in manual annotation\ncosts.", "comment": "Accepted by ITSC'25", "pdf_url": "http://arxiv.org/pdf/2507.08901v1", "cate": "cs.RO", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09699", "title": "Interpreting Differential Privacy in Terms of Disclosure Risk", "authors": ["Zeki Kazan", "Sagar Sharma", "Wanrong Zhang", "Bo Jiang", "Qiang Yan"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      11 pages with 6 pages of supplemental material", "url": "http://arxiv.org/abs/2507.09699v1", "summary": "As the use of differential privacy (DP) becomes widespread, the development\nof effective tools for reasoning about the privacy guarantee becomes\nincreasingly critical. In pursuit of this goal, we demonstrate novel\nrelationships between DP and measures of statistical disclosure risk. We\nsuggest how experts and non-experts can use these results to explain the DP\nguarantee, interpret DP composition theorems, select and justify privacy\nparameters, and identify worst-case adversary prior probabilities.", "comment": "11 pages with 6 pages of supplemental material", "pdf_url": "http://arxiv.org/pdf/2507.09699v1", "cate": "cs.CR", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09965", "title": "AnalogTester: A Large Language Model-Based Framework for Automatic Testbench Generation in Analog Circuit Design", "authors": ["Weiyu Chen", "Chengjie Liu", "Wenhao Huang", "Jinyang Lyu", "Mingqian Yang", "Yuan Du", "Li Du", "Jun Yang"], "categories": ["cs.MA", "cs.AR"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "Comments:      accepted by ISEDA 2025", "url": "http://arxiv.org/abs/2507.09965v1", "summary": "Recent advancements have demonstrated the significant potential of large\nlanguage models (LLMs) in analog circuit design. Nevertheless, testbench\nconstruction for analog circuits remains manual, creating a critical bottleneck\nin achieving fully automated design processes. Particularly when replicating\ncircuit designs from academic papers, manual Testbench construction demands\ntime-intensive implementation and frequent adjustments, which fails to address\nthe dynamic diversity and flexibility requirements for automation. AnalogTester\ntackles automated analog design challenges through an LLM-powered pipeline: a)\ndomain-knowledge integration, b) paper information extraction, c) simulation\nscheme synthesis, and d) testbench code generation with Tsinghua Electronic\nDesign (TED). AnalogTester has demonstrated automated Testbench generation\ncapabilities for three fundamental analog circuit types: operational amplifiers\n(op-amps), bandgap references (BGRs), and low-dropout regulators (LDOs), while\nmaintaining a scalable framework for adaptation to broader circuit topologies.\nFurthermore, AnalogTester can generate circuit knowledge data and TED code\ncorpus, establishing fundamental training datasets for LLM specialization in\nanalog circuit design automation.", "comment": "accepted by ISEDA 2025", "pdf_url": "http://arxiv.org/pdf/2507.09965v1", "cate": "cs.MA", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.08903", "title": "Multimodal HD Mapping for Intersections by Intelligent Roadside Units", "authors": ["Zhongzhang Chen", "Miao Fan", "Shengtong Xu", "Mengmeng Yang", "Kun Jiang", "Xiangzeng Liu", "Haoyi Xiong"], "categories": ["cs.RO", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted by ITSC'25", "url": "http://arxiv.org/abs/2507.08903v1", "summary": "High-definition (HD) semantic mapping of complex intersections poses\nsignificant challenges for traditional vehicle-based approaches due to\nocclusions and limited perspectives. This paper introduces a novel camera-LiDAR\nfusion framework that leverages elevated intelligent roadside units (IRUs).\nAdditionally, we present RS-seq, a comprehensive dataset developed through the\nsystematic enhancement and annotation of the V2X-Seq dataset. RS-seq includes\nprecisely labelled camera imagery and LiDAR point clouds collected from\nroadside installations, along with vectorized maps for seven intersections\nannotated with detailed features such as lane dividers, pedestrian crossings,\nand stop lines. This dataset facilitates the systematic investigation of\ncross-modal complementarity for HD map generation using IRU data. The proposed\nfusion framework employs a two-stage process that integrates modality-specific\nfeature extraction and cross-modal semantic integration, capitalizing on camera\nhigh-resolution texture and precise geometric data from LiDAR. Quantitative\nevaluations using the RS-seq dataset demonstrate that our multimodal approach\nconsistently surpasses unimodal methods. Specifically, compared to unimodal\nbaselines evaluated on the RS-seq dataset, the multimodal approach improves the\nmean Intersection-over-Union (mIoU) for semantic segmentation by 4\\% over the\nimage-only results and 18\\% over the point cloud-only results. This study\nestablishes a baseline methodology for IRU-based HD semantic mapping and\nprovides a valuable dataset for future research in infrastructure-assisted\nautonomous driving systems.", "comment": "Accepted by ITSC'25", "pdf_url": "http://arxiv.org/pdf/2507.08903v1", "cate": "cs.RO", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09762", "title": "EventHunter: Dynamic Clustering and Ranking of Security Events from Hacker Forum Discussions", "authors": ["Yasir Ech-Chammakhy", "Anas Motii", "Anass Rabii", "Jaafar Chbili"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      Accepted for publication at the 28th International Symposium on Research in Attacks, Intrusions, and Defenses (RAID 2025)", "url": "http://arxiv.org/abs/2507.09762v1", "summary": "Hacker forums provide critical early warning signals for emerging\ncybersecurity threats, but extracting actionable intelligence from their\nunstructured and noisy content remains a significant challenge. This paper\npresents an unsupervised framework that automatically detects, clusters, and\nprioritizes security events discussed across hacker forum posts. Our approach\nleverages Transformer-based embeddings fine-tuned with contrastive learning to\ngroup related discussions into distinct security event clusters, identifying\nincidents like zero-day disclosures or malware releases without relying on\npredefined keywords. The framework incorporates a daily ranking mechanism that\nprioritizes identified events using quantifiable metrics reflecting timeliness,\nsource credibility, information completeness, and relevance. Experimental\nevaluation on real-world hacker forum data demonstrates that our method\neffectively reduces noise and surfaces high-priority threats, enabling security\nanalysts to mount proactive responses. By transforming disparate hacker forum\ndiscussions into structured, actionable intelligence, our work addresses\nfundamental challenges in automated threat detection and analysis.", "comment": "Accepted for publication at the 28th International Symposium on\n  Research in Attacks, Intrusions, and Defenses (RAID 2025)", "pdf_url": "http://arxiv.org/pdf/2507.09762v1", "cate": "cs.CR", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.10249", "title": "Multi-Robot Cooperative Herding through Backstepping Control Barrier Functions", "authors": ["Kang Li", "Ming Li", "Wenkang Ji", "Zhiyong Sun", "Shiyu Zhao"], "categories": ["cs.MA"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10249v1", "summary": "We propose a novel cooperative herding strategy through backstepping control\nbarrier functions (CBFs), which coordinates multiple herders to herd a group of\nevaders safely towards a designated goal region. For the herding system with\nheterogeneous groups involving herders and evaders, the behavior of the evaders\ncan only be influenced indirectly by the herders' motion, especially when the\nevaders follow an inverse dynamics model and respond solely to repulsive\ninteractions from the herders. This indirect interaction mechanism inherently\nrenders the overall system underactuated. To address this issue, we first\nconstruct separate CBFs for the dual objectives of goal reaching and collision\navoidance, which ensure both herding completion and safety guarantees. Then, we\nreformulate the underactuated herding dynamics into a control-affine structure\nand employ a backstepping approach to recursively design control inputs for the\nhierarchical barrier functions, avoiding taking derivatives of the higher-order\nsystem. Finally, we present a cooperative herding strategy based on\nbackstepping CBFs that allow herders to safely herd multiple evaders into the\ngoal region. In addition, centralized and decentralized implementations of the\nproposed algorithm are developed, further enhancing its flexibility and\napplicability. Extensive simulations and real-world experiments validate the\neffectiveness and safety of the proposed strategy in multi-robot herding.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10249v1", "cate": "cs.MA", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09117", "title": "Towards Human-level Dexterity via Robot Learning", "authors": ["Gagan Khandate"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      PhD thesis", "url": "http://arxiv.org/abs/2507.09117v1", "summary": "Dexterous intelligence -- the ability to perform complex interactions with\nmulti-fingered hands -- is a pinnacle of human physical intelligence and\nemergent higher-order cognitive skills. However, contrary to Moravec's paradox,\ndexterous intelligence in humans appears simple only superficially. Many\nmillion years were spent co-evolving the human brain and hands including rich\ntactile sensing. Achieving human-level dexterity with robotic hands has long\nbeen a fundamental goal in robotics and represents a critical milestone toward\ngeneral embodied intelligence. In this pursuit, computational sensorimotor\nlearning has made significant progress, enabling feats such as arbitrary\nin-hand object reorientation. However, we observe that achieving higher levels\nof dexterity requires overcoming very fundamental limitations of computational\nsensorimotor learning.\n  I develop robot learning methods for highly dexterous multi-fingered\nmanipulation by directly addressing these limitations at their root cause.\nChiefly, through key studies, this disseration progressively builds an\neffective framework for reinforcement learning of dexterous multi-fingered\nmanipulation skills. These methods adopt structured exploration, effectively\novercoming the limitations of random exploration in reinforcement learning. The\ninsights gained culminate in a highly effective reinforcement learning that\nincorporates sampling-based planning for direct exploration. Additionally, this\nthesis explores a new paradigm of using visuo-tactile human demonstrations for\ndexterity, introducing corresponding imitation learning techniques.", "comment": "PhD thesis", "pdf_url": "http://arxiv.org/pdf/2507.09117v1", "cate": "cs.RO", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09859", "title": "Endorsement-Driven Blockchain SSI Framework for Dynamic IoT Ecosystems", "authors": ["Guntur Dharma Putra", "Bagus Rakadyanto Oktavianto Putra"], "categories": ["cs.CR", "cs.NI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      5 pages, 4 figures. Accepted to IEEE ICBC 2025 as a short paper", "url": "http://arxiv.org/abs/2507.09859v1", "summary": "Self-Sovereign Identity (SSI) offers significant potential for managing\nidentities in the Internet of Things (IoT), enabling decentralized\nauthentication and credential management without reliance on centralized\nentities. However, existing SSI frameworks often limit credential issuance and\nrevocation to trusted entities, such as IoT manufacturers, which restricts\nflexibility in dynamic IoT ecosystems. In this paper, we propose a\nblockchain-based SSI framework that allows any individual with a verifiable\ntrust linkage to act as a credential issuer, ensuring decentralized and\nscalable identity management. Our framework incorporates a layered\narchitecture, where trust is dynamically established through endorsement-based\ncalculations and maintained via a hierarchical chain-of-trust mechanism.\nBlockchain serves as the Verifiable Data Registry, ensuring transparency and\nimmutability of identity operations, while smart contracts automate critical\nprocesses such as credential issuance, verification, and revocation. A\nproof-of-concept implementation demonstrates that the proposed framework is\nfeasible and incurs minimal overheads compared to the baseline, making it\nwell-suited for dynamic and resource-constrained IoT environments.", "comment": "5 pages, 4 figures. Accepted to IEEE ICBC 2025 as a short paper", "pdf_url": "http://arxiv.org/pdf/2507.09859v1", "cate": "cs.CR", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10251", "title": "ToMacVF : Temporal Macro-action Value Factorization for Asynchronous Multi-Agent Reinforcement Learning", "authors": ["Wenjing Zhang", "Wei Zhang"], "categories": ["cs.MA"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10251v1", "summary": "Existing asynchronous MARL methods based on MacDec-POMDP typically construct\ntraining trajectory buffers by simply sampling limited and biased data at the\nendpoints of macro-actions, and directly apply conventional MARL methods on the\nbuffers. As a result, these methods lead to an incomplete and inaccurate\nrepresentation of the macro-action execution process, along with unsuitable\ncredit assignments. To solve these problems, the Temporal Macro-action Value\nFactorization (ToMacVF) is proposed to achieve fine-grained temporal credit\nassignment for macro-action contributions. A centralized training buffer,\ncalled Macro-action Segmented Joint Experience Replay Trajectory (Mac-SJERT),\nis designed to incorporate with ToMacVF to collect accurate and complete\nmacro-action execution information, supporting a more comprehensive and precise\nrepresentation of the macro-action process. To ensure principled and\nfine-grained asynchronous value factorization, the consistency requirement\nbetween joint and individual macro-action selection called Temporal\nMacro-action based IGM (To-Mac-IGM) is formalized, proving that it generalizes\nthe synchronous cases. Based on To-Mac-IGM, a modularized ToMacVF architecture,\nwhich satisfies CTDE principle, is designed to conveniently integrate previous\nvalue factorization methods. Next, the ToMacVF algorithm is devised as an\nimplementation of the ToMacVF architecture. Experimental results demonstrate\nthat, compared to asynchronous baselines, our ToMacVF algorithm not only\nachieves optimal performance but also exhibits strong adaptability and\nrobustness across various asynchronous multi-agent experimental scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10251v1", "cate": "cs.MA", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09123", "title": "Online 3D Bin Packing with Fast Stability Validation and Stable Rearrangement Planning", "authors": ["Ziyan Gao", "Lijun Wang", "Yuntao Kong", "Nak Young Chong"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09123v1", "summary": "The Online Bin Packing Problem (OBPP) is a sequential decision-making task in\nwhich each item must be placed immediately upon arrival, with no knowledge of\nfuture arrivals. Although recent deep-reinforcement-learning methods achieve\nsuperior volume utilization compared with classical heuristics, the learned\npolicies cannot ensure the structural stability of the bin and lack mechanisms\nfor safely reconfiguring the bin when a new item cannot be placed directly. In\nthis work, we propose a novel framework that integrates packing policy with\nstructural stability validation and heuristic planning to overcome these\nlimitations. Specifically, we introduce the concept of Load Bearable Convex\nPolygon (LBCP), which provides a computationally efficient way to identify\nstable loading positions that guarantee no bin collapse. Additionally, we\npresent Stable Rearrangement Planning (SRP), a module that rearranges existing\nitems to accommodate new ones while maintaining overall stability. Extensive\nexperiments on standard OBPP benchmarks demonstrate the efficiency and\ngeneralizability of our LBCP-based stability validation, as well as the\nsuperiority of SRP in finding the effort-saving rearrangement plans. Our method\noffers a robust and practical solution for automated packing in real-world\nindustrial and logistics applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09123v1", "cate": "cs.RO", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09860", "title": "Secure and Efficient UAV-Based Face Detection via Homomorphic Encryption and Edge Computing", "authors": ["Nguyen Van Duc", "Bui Duc Manh", "Quang-Trung Luu", "Dinh Thai Hoang", "Van-Linh Nguyen", "Diep N. Nguyen"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09860v1", "summary": "This paper aims to propose a novel machine learning (ML) approach\nincorporating Homomorphic Encryption (HE) to address privacy limitations in\nUnmanned Aerial Vehicles (UAV)-based face detection. Due to challenges related\nto distance, altitude, and face orientation, high-resolution imagery and\nsophisticated neural networks enable accurate face recognition in dynamic\nenvironments. However, privacy concerns arise from the extensive surveillance\ncapabilities of UAVs. To resolve this issue, we propose a novel framework that\nintegrates HE with advanced neural networks to secure facial data throughout\nthe inference phase. This method ensures that facial data remains secure with\nminimal impact on detection accuracy. Specifically, the proposed system\nleverages the Cheon-Kim-Kim-Song (CKKS) scheme to perform computations directly\non encrypted data, optimizing computational efficiency and security.\nFurthermore, we develop an effective data encoding method specifically designed\nto preprocess the raw facial data into CKKS form in a\nSingle-Instruction-Multiple-Data (SIMD) manner. Building on this, we design a\nsecure inference algorithm to compute on ciphertext without needing decryption.\nThis approach not only protects data privacy during the processing of facial\ndata but also enhances the efficiency of UAV-based face detection systems.\nExperimental results demonstrate that our method effectively balances privacy\nprotection and detection performance, making it a viable solution for UAV-based\nsecure face detection. Significantly, our approach (while maintaining data\nconfidentially with HE encryption) can still achieve an accuracy of less than\n1% compared to the benchmark without using encryption.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09860v1", "cate": "cs.CR", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10324", "title": "Toolsuite for Implementing Multiagent Systems Based on Communication Protocols", "authors": ["Amit K. Chopra", "Samuel H. Christie V", "Munindar P. Singh"], "categories": ["cs.MA", "cs.AI", "cs.PL", "cs.SE", "I.2.11; I.2.4; I.2.5"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10324v1", "summary": "Interaction-Oriented Programming (IOP) is an approach to building a\nmultiagent system by modeling the interactions between its roles via a flexible\ninteraction protocol and implementing agents to realize the interactions of the\nroles they play in the protocol.\n  In recent years, we have developed an extensive suite of software that\nenables multiagent system developers to apply IOP. These include tools for\nefficiently verifying protocols for properties such as liveness and safety and\nmiddleware that simplifies the implementation of agents. This paper presents\nsome of that software suite.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10324v1", "cate": "cs.MA", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09160", "title": "Tactile-VLA: Unlocking Vision-Language-Action Model's Physical Knowledge for Tactile Generalization", "authors": ["Jialei Huang", "Shuo Wang", "Fanqi Lin", "Yihang Hu", "Chuan Wen", "Yang Gao"], "categories": ["cs.RO", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09160v1", "summary": "Vision-Language-Action (VLA) models have shown remarkable achievements,\ndriven by the rich implicit knowledge of their vision-language components.\nHowever, achieving generalist robotic agents demands precise grounding into\nphysical interactions, especially in contact-rich scenarios where fine-grained\nforce control is essential. We advance VLAs' implicit knowledge beyond\nidentifying what to do, towards guiding how to physically interact with real\nworld. This paper introduces Tactile-VLA, a novel framework that deeply fuses\nvision, language, action, and tactile sensing. This framework incorporates a\nhybrid position-force controller to translate the model's intentions into\nprecise physical actions and a reasoning module that allows the robot to adapt\nits strategy based on tactile feedback. Experiments demonstrate Tactile-VLA's\neffectiveness and generalizability in three key aspects: (1) enabling\ntactile-aware instruction following, (2) utilizing tactile-relevant\ncommonsense, and (3) facilitating adaptive tactile-involved reasoning. A key\nfinding is that the VLM's prior knowledge already contains semantic\nunderstanding of physical interaction; by connecting it to the robot's tactile\nsensors with only a few demonstrations, we can activate this prior knowledge to\nachieve zero-shot generalization in contact-rich tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09160v1", "cate": "cs.RO", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09990", "title": "Differentially Private Federated Low Rank Adaptation Beyond Fixed-Matrix", "authors": ["Ming Wen", "Jiaqi Zhu", "Yuedong Xu", "Yipeng Zhou", "Dingding Han"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      23 pages, NeurIPS 2025 under review", "url": "http://arxiv.org/abs/2507.09990v1", "summary": "Large language models (LLMs) typically require fine-tuning for\ndomain-specific tasks, and LoRA offers a computationally efficient approach by\ntraining low-rank adapters. LoRA is also communication-efficient for federated\nLLMs when multiple users collaboratively fine-tune a global LLM model without\nsharing their proprietary raw data. However, even the transmission of local\nadapters between a server and clients risks serious privacy leakage. Applying\ndifferential privacy (DP) to federated LoRA encounters a dilemma: adding noise\nto both adapters amplifies synthetic noise on the model, while fixing one\nadapter impairs the learnability of fine-tuning. In this paper, we propose\nFedASK (Differentially Private Federated Low Rank Adaptation with Double\nSketching) , a novel federated LoRA framework to enable effective updating of\nboth low-rank adapters with robust differential privacy. Inspired by randomized\nSVD, our key idea is a two-stage sketching pipeline. This pipeline first\naggregates carefully sketched, privacy-preserving local updates, and then\nreconstructs the global matrices on the server to facilitate effective updating\nof both adapters. We theoretically prove FedASK's differential privacy\nguarantee and its exact aggregation property. Comprehensive experiments\ndemonstrate that FedASK consistently outperforms baseline methods across a\nvariety of privacy settings and data distributions.", "comment": "23 pages, NeurIPS 2025 under review", "pdf_url": "http://arxiv.org/pdf/2507.09990v1", "cate": "cs.CR", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.08870", "title": "GUIDE: Towards Scalable Advising for Research Ideas", "authors": ["Yaowenqi Liu", "BingXu Meng", "Rui Pan", "Jerry Huang", "Tong Zhang"], "categories": ["cs.LG", "cs.MA"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08870v1", "summary": "The field of AI research is advancing at an unprecedented pace, enabling\nautomated hypothesis generation and experimental design across diverse domains\nsuch as biology, mathematics, and artificial intelligence. Despite these\nadvancements, there remains a significant gap in the availability of scalable\nadvising systems capable of providing high-quality, well-reasoned feedback to\nrefine proposed hypotheses and experimental designs. To address this challenge,\nwe explore key factors that underlie the development of robust advising\nsystems, including model size, context length, confidence estimation, and\nstructured reasoning processes. Our findings reveal that a relatively small\nmodel, when equipped with a well-compressed literature database and a\nstructured reasoning framework, can outperform powerful general-purpose\nlanguage models such as Deepseek-R1 in terms of acceptance rates for\nself-ranked top-30% submissions to ICLR 2025. Moreover, when limited to\nhigh-confidence predictions, our system achieves an acceptance rate exceeding\n90% on the ICLR 2025 test set, underscoring its potential to significantly\nenhance the quality and efficiency of hypothesis generation and experimental\ndesign. The code is released at\nhttps://github.com/HowardLiu0830/GUIDE-Research-Idea-Evaluation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08870v1", "cate": "cs.LG", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.09167", "title": "PRAG: Procedural Action Generator", "authors": ["Michal Vavrecka", "Radoslav Skoviera", "Gabriela Sejnova", "Karla Stepanova"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09167v1", "summary": "We present a novel approach for the procedural construction of multi-step\ncontact-rich manipulation tasks in robotics. Our generator takes as input\nuser-defined sets of atomic actions, objects, and spatial predicates and\noutputs solvable tasks of a given length for the selected robotic environment.\nThe generator produces solvable tasks by constraining all possible\n(nonsolvable) combinations by symbolic and physical validation. The symbolic\nvalidation checks each generated sequence for logical and operational\nconsistency, and also the suitability of object-predicate relations. Physical\nvalidation checks whether tasks can be solved in the selected robotic\nenvironment. Only the tasks that passed both validators are retained. The\noutput from the generator can be directly interfaced with any existing\nframework for training robotic manipulation tasks, or it can be stored as a\ndataset of curated robotic tasks with detailed information about each task.\nThis is beneficial for RL training as there are dense reward functions and\ninitial and goal states paired with each subgoal. It allows the user to measure\nthe semantic similarity of all generated tasks. We tested our generator on\nsequences of up to 15 actions resulting in millions of unique solvable\nmulti-step tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09167v1", "cate": "cs.RO", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.08804", "title": "Cognitive Dissonance Artificial Intelligence (CD-AI): The Mind at War with Itself. Harnessing Discomfort to Sharpen Critical Thinking", "authors": ["Delia Deliu"], "categories": ["cs.HC", "cs.CY"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Presented at the 2025 ACM Workshop on Human-AI Interaction for Augmented Reasoning, Report Number: CHI25-WS-AUGMENTED-REASONING", "url": "http://arxiv.org/abs/2507.08804v1", "summary": "AI-augmented systems are traditionally designed to streamline human\ndecision-making by minimizing cognitive load, clarifying arguments, and\noptimizing efficiency. However, in a world where algorithmic certainty risks\nbecoming an Orwellian tool of epistemic control, true intellectual growth\ndemands not passive acceptance but active struggle. Drawing on the dystopian\nvisions of George Orwell and Philip K. Dick - where reality is unstable,\nperception malleable, and truth contested - this paper introduces Cognitive\nDissonance AI (CD-AI): a novel framework that deliberately sustains uncertainty\nrather than resolving it. CD-AI does not offer closure, but compels users to\nnavigate contradictions, challenge biases, and wrestle with competing truths.\nBy delaying resolution and promoting dialectical engagement, CD-AI enhances\nreflective reasoning, epistemic humility, critical thinking, and adaptability\nin complex decision-making. This paper examines the theoretical foundations of\nthe approach, presents an implementation model, explores its application in\ndomains such as ethics, law, politics, and science, and addresses key ethical\nconcerns - including decision paralysis, erosion of user autonomy, cognitive\nmanipulation, and bias in AI reasoning. In reimagining AI as an engine of doubt\nrather than a deliverer of certainty, CD-AI challenges dominant paradigms of\nAI-augmented reasoning and offers a new vision - one in which AI sharpens the\nmind not by resolving conflict, but by sustaining it. Rather than reinforcing\nHuxleyan complacency or pacifying the user into intellectual conformity, CD-AI\nechoes Nietzsche's vision of the Uebermensch - urging users to transcend\npassive cognition through active epistemic struggle.", "comment": "Presented at the 2025 ACM Workshop on Human-AI Interaction for\n  Augmented Reasoning, Report Number: CHI25-WS-AUGMENTED-REASONING", "pdf_url": "http://arxiv.org/pdf/2507.08804v1", "cate": "cs.HC", "date": "2025-04-23", "updated": "2025-04-23"}
{"id": "2507.10016", "title": "The Man Behind the Sound: Demystifying Audio Private Attribute Profiling via Multimodal Large Language Model Agents", "authors": ["Lixu Wang", "Kaixiang Yao", "Xinfeng Li", "Dong Yang", "Haoyang Li", "Xiaofeng Wang", "Wei Dong"], "categories": ["cs.CR", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      22 pages, 4 figures", "url": "http://arxiv.org/abs/2507.10016v1", "summary": "Our research uncovers a novel privacy risk associated with multimodal large\nlanguage models (MLLMs): the ability to infer sensitive personal attributes\nfrom audio data -- a technique we term audio private attribute profiling. This\ncapability poses a significant threat, as audio can be covertly captured\nwithout direct interaction or visibility. Moreover, compared to images and\ntext, audio carries unique characteristics, such as tone and pitch, which can\nbe exploited for more detailed profiling. However, two key challenges exist in\nunderstanding MLLM-employed private attribute profiling from audio: (1) the\nlack of audio benchmark datasets with sensitive attribute annotations and (2)\nthe limited ability of current MLLMs to infer such attributes directly from\naudio. To address these challenges, we introduce AP^2, an audio benchmark\ndataset that consists of two subsets collected and composed from real-world\ndata, and both are annotated with sensitive attribute labels. Additionally, we\npropose Gifts, a hybrid multi-agent framework that leverages the complementary\nstrengths of audio-language models (ALMs) and large language models (LLMs) to\nenhance inference capabilities. Gifts employs an LLM to guide the ALM in\ninferring sensitive attributes, then forensically analyzes and consolidates the\nALM's inferences, overcoming severe hallucinations of existing ALMs in\ngenerating long-context responses. Our evaluations demonstrate that Gifts\nsignificantly outperforms baseline approaches in inferring sensitive\nattributes. Finally, we investigate model-level and data-level defense\nstrategies to mitigate the risks of audio private attribute profiling. Our work\nvalidates the feasibility of audio-based privacy attacks using MLLMs,\nhighlighting the need for robust defenses, and provides a dataset and framework\nto facilitate future research.", "comment": "22 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.10016v1", "cate": "cs.CR", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.08892", "title": "Multi-Actor Generative Artificial Intelligence as a Game Engine", "authors": ["Alexander Sasha Vezhnevets", "Jayd Matyas", "Logan Cross", "Davide Paglieri", "Minsuk Chang", "William A. Cunningham", "Simon Osindero", "William S. Isaac", "Joel Z. Leibo"], "categories": ["cs.AI", "cs.MA"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      13 pages", "url": "http://arxiv.org/abs/2507.08892v1", "summary": "Generative AI can be used in multi-actor environments with purposes ranging\nfrom social science modeling to interactive narrative and AI evaluation.\nSupporting this diversity of use cases -- which we classify as Simulationist,\nDramatist, and Evaluationist -- demands a flexible scenario definition\nframework. We argue here that a good approach is to take inspiration from\ntabletop role-playing games (TTRPGs), where a Game Master (GM) is responsible\nfor the environment and generates all parts of the story not directly\ndetermined by the voluntary actions of player characters. We argue that the\nEntity-Component architectural pattern is useful here. In such a system, the GM\nis not a hardcoded computer game but is itself a configurable entity, composed\nof components just like any other actor. By design, the approach allows for a\nseparation between the underlying implementation details handled by an\nengineer, the creation of reusable components, and their composition and\nconfiguration managed by a designer who constructs entities from the\ncomponents. This separation of concerns is instrumental for achieving rapid\niteration, maintaining modularity, and ultimately to ensure scalability. We\ndescribe the ongoing evolution of the Concordia library in terms of this\nphilosophy, demonstrating how it allows users to effectively configure\nscenarios that align with their specific goals.", "comment": "13 pages", "pdf_url": "http://arxiv.org/pdf/2507.08892v1", "cate": "cs.AI", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.09176", "title": "DLBAcalib: Robust Extrinsic Calibration for Non-Overlapping LiDARs Based on Dual LBA", "authors": ["Han Ye", "Yuqiang Jin", "Jinyuan Liu", "Tao Li", "Wen-An Zhang", "Minglei Fu"], "categories": ["cs.RO", "cs.CL"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      9 pages,14 figures", "url": "http://arxiv.org/abs/2507.09176v1", "summary": "Accurate extrinsic calibration of multiple LiDARs is crucial for improving\nthe foundational performance of three-dimensional (3D) map reconstruction\nsystems. This paper presents a novel targetless extrinsic calibration framework\nfor multi-LiDAR systems that does not rely on overlapping fields of view or\nprecise initial parameter estimates. Unlike conventional calibration methods\nthat require manual annotations or specific reference patterns, our approach\nintroduces a unified optimization framework by integrating LiDAR bundle\nadjustment (LBA) optimization with robust iterative refinement. The proposed\nmethod constructs an accurate reference point cloud map via continuous scanning\nfrom the target LiDAR and sliding-window LiDAR bundle adjustment, while\nformulating extrinsic calibration as a joint LBA optimization problem. This\nmethod effectively mitigates cumulative mapping errors and achieves\noutlier-resistant parameter estimation through an adaptive weighting mechanism.\nExtensive evaluations in both the CARLA simulation environment and real-world\nscenarios demonstrate that our method outperforms state-of-the-art calibration\ntechniques in both accuracy and robustness. Experimental results show that for\nnon-overlapping sensor configurations, our framework achieves an average\ntranslational error of 5 mm and a rotational error of 0.2{\\deg}, with an\ninitial error tolerance of up to 0.4 m/30{\\deg}. Moreover, the calibration\nprocess operates without specialized infrastructure or manual parameter tuning.\nThe code is open source and available on GitHub\n(\\underline{https://github.com/Silentbarber/DLBAcalib})", "comment": "9 pages,14 figures", "pdf_url": "http://arxiv.org/pdf/2507.09176v1", "cate": "cs.RO", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.08805", "title": "Non-linear, Team-based VR Training for Cardiac Arrest Care with enhanced CRM Toolkit", "authors": ["Mike Kentros", "Manos Kamarianakis", "Michael Cole", "Vitaliy Popov", "Antonis Protopsaltis", "George Papagiannakis"], "categories": ["cs.HC", "cs.CY", "cs.GR"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      4 pages, 3 figures, 1 table", "url": "http://arxiv.org/abs/2507.08805v1", "summary": "This paper introduces iREACT, a novel VR simulation addressing key\nlimitations in traditional cardiac arrest (CA) training. Conventional methods\nstruggle to replicate the dynamic nature of real CA events, hindering Crew\nResource Management (CRM) skill development. iREACT provides a non-linear,\ncollaborative environment where teams respond to changing patient states,\nmirroring real CA complexities. By capturing multi-modal data (user actions,\ncognitive load, visual gaze) and offering real-time and post-session feedback,\niREACT enhances CRM assessment beyond traditional methods. A formative\nevaluation with medical experts underscores its usability and educational\nvalue, with potential applications in other high-stakes training scenarios to\nimprove teamwork, communication, and decision-making.", "comment": "4 pages, 3 figures, 1 table", "pdf_url": "http://arxiv.org/pdf/2507.08805v1", "cate": "cs.HC", "date": "2025-04-23", "updated": "2025-04-23"}
{"id": "2507.10162", "title": "HASSLE: A Self-Supervised Learning Enhanced Hijacking Attack on Vertical Federated Learning", "authors": ["Weiyang He", "Chip-Hong Chang"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10162v1", "summary": "Vertical Federated Learning (VFL) enables an orchestrating active party to\nperform a machine learning task by cooperating with passive parties that\nprovide additional task-related features for the same training data entities.\nWhile prior research has leveraged the privacy vulnerability of VFL to\ncompromise its integrity through a combination of label inference and backdoor\nattacks, their effectiveness is constrained by the low label inference\nprecision and suboptimal backdoor injection conditions. To facilitate a more\nrigorous security evaluation on VFL without these limitations, we propose\nHASSLE, a hijacking attack framework composed of a gradient-direction-based\nlabel inference module and an adversarial embedding generation algorithm\nenhanced by self-supervised learning. HASSLE accurately identifies private\nsamples associated with a targeted label using only a single known instance of\nthat label. In the two-party scenario, it demonstrates strong performance with\nan attack success rate (ASR) of over 99% across four datasets, including both\nimage and tabular modalities, and achieves 85% ASR on the more complex\nCIFAR-100 dataset. Evaluation of HASSLE against 8 potential defenses further\nhighlights its significant threat while providing new insights into building a\ntrustworthy VFL system.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10162v1", "cate": "cs.CR", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.08900", "title": "Properties of Quasi-synchronization Time of High-dimensional Hegselmann-Krause Dynamics", "authors": ["Wei Su", "Meiru Jiang", "Yongguang Yu", "Ge Chen"], "categories": ["math.DS", "cs.MA", "nlin.AO", "physics.soc-ph"], "primary_category": "Subjects:       Dynamical Systems (math.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08900v1", "summary": "The behavior of one-dimensional Hegselmann-Krause (HK) dynamics driven by\nnoise has been extensively studied. Previous research has indicated that within\nno matter the bounded or the unbounded space of one dimension, the HK dynamics\nattain quasi-synchronization (synchronization in noisy case) in finite time.\nHowever, it remains unclear whether this phenomenon holds in high-dimensional\nspace. This paper investigates the random time for quasi-synchronization of\nmulti-dimensional HK model and reveals that the boundedness and dimensions of\nthe space determine different outcomes. To be specific, if the space is\nbounded, quasi-synchronization can be attained almost surely for all dimensions\nwithin a finite time, whereas in unbounded space, quasi-synchronization can\nonly be achieved in low-dimensional cases (one and two). Furthermore, different\nintegrability of the random time of various cases is proved.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08900v1", "cate": "math.DS", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09309", "title": "Informed Hybrid Zonotope-based Motion Planning Algorithm", "authors": ["Peng Xie", "Johannes Betz", "Amr Alanwar"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09309v1", "summary": "Optimal path planning in nonconvex free spaces is notoriously challenging, as\nformulating such problems as mixed-integer linear programs (MILPs) is NP-hard.\nWe propose HZ-MP, an informed Hybrid Zonotope-based Motion Planner, as an\nalternative approach that decomposes the obstacle-free space and performs\nlow-dimensional face sampling guided by an ellipsotope heuristic, enabling\nfocused exploration along promising transit regions. This structured\nexploration eliminates the excessive, unreachable sampling that degrades\nexisting informed planners such as AIT* and EIT* in narrow gaps or boxed-goal\nscenarios. We prove that HZ-MP is probabilistically complete and asymptotically\noptimal. It converges to near-optimal trajectories in finite time and scales to\nhigh-dimensional cluttered scenes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09309v1", "cate": "cs.RO", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.08914", "title": "'Teens Need to Be Educated on the Danger': Digital Access, Online Risks, and Safety Practices Among Nigerian Adolescents", "authors": ["Munachimso B. Oguine", "Ozioma C. Oguine", "Karla Badillo-Urquiola", "Oluwasogo Adekunle Okunade"], "categories": ["cs.HC", "cs.CY"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      14 pages, 4 figures. Accepted to AfriCHI 2025", "url": "http://arxiv.org/abs/2507.08914v1", "summary": "Adolescents increasingly rely on online technologies to explore their\nidentities, form social connections, and access information and entertainment.\nHowever, their growing digital engagement exposes them to significant online\nrisks, particularly in underrepresented contexts like West Africa. This study\ninvestigates the online experiences of 409 secondary school adolescents in\nNigeria's Federal Capital Territory (FCT), focusing on their access to\ntechnology, exposure to risks, coping strategies, key stakeholders influencing\ntheir online interactions, and recommendations for improving online safety.\nUsing self-administered surveys, we found that while most adolescents reported\nmoderate access to online technology and connectivity, those who encountered\nrisks frequently reported exposure to inappropriate content and online scams.\nBlocking and reporting tools were the most commonly used strategies, though\nsome adolescents responded with inaction due to limited resources or awareness.\nParents emerged as the primary support network, though monitoring practices and\ncommunication varied widely. Guided by Protection Motivation Theory (PMT), our\nanalysis interprets adolescents' online safety behaviors as shaped by both\ntheir threat perceptions and their confidence in available coping strategies. A\nthematic analysis of their recommendations highlights the need for greater\nawareness and education, parental mediation, enhanced safety tools, stricter\nage restrictions, improved content moderation, government accountability, and\nresilience-building initiatives. Our findings underscore the importance of\nculturally and contextually relevant interventions to empower adolescents in\nnavigating the digital world, with implications for parents, educators,\ndesigners, and policymakers.", "comment": "14 pages, 4 figures. Accepted to AfriCHI 2025", "pdf_url": "http://arxiv.org/pdf/2507.08914v1", "cate": "cs.HC", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.08806", "title": "Think Clearly: Improving Reasoning via Redundant Token Pruning", "authors": ["Daewon Choi", "Jimin Lee", "Jihoon Tack", "Woomin Song", "Saket Dingliwal", "Sai Muralidhar Jayanthi", "Bhavana Ganesh", "Jinwoo Shin", "Aram Galstyan", "Sravan Babu Bodapati"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08806v1", "summary": "Recent large language models have shown promising capabilities in long-form\nreasoning, following structured chains of thought before arriving at a final\nanswer. However, we observe that these reasoning paths tend to include\nsubstantial redundancy; analyzing attention patterns reveals that attention\nscores are widely scattered, particularly incorrect answers exhibit greater\nattention sparsity. In this paper, we demonstrate that deliberately removing\nthis redundancy in the reasoning process significantly improves performance\nthrough clear thinking, i.e., removing distraction. Specifically, we\nsystematically identify reasoning redundancy by measuring token-level attention\nscores to a special end-of-thinking token, which is appended to an explicit\ninstruction inserted to conclude each intermediate reasoning step. Furthermore,\nwe propose structure-aware pruning that prioritizes removing tokens in\nlow-contributing reasoning chunks over individual tokens. After evicting\nredundant tokens, we remove the injected end-of-thinking instruction, then\nresume the reasoning generation. We demonstrate that our method significantly\nimproves overall accuracy across reasoning-intensive benchmarks without any\ntraining involved. In particular, our method shows strong performance on\nchallenging mathematical competition benchmarks such as AIME and AMC, where\nreasoning redundancy is more prevalent.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08806v1", "cate": "cs.AI", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2507.09067", "title": "Quantum-Resilient Privacy Ledger (QRPL): A Sovereign Digital Currency for the Post-Quantum Era", "authors": ["Serhan W. Bahar"], "categories": ["cs.ET", "cs.CR"], "primary_category": "Subjects:       Emerging Technologies (cs.ET)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09067v1", "summary": "The emergence of quantum computing presents profound challenges to existing\ncryptographic infrastructures, whilst the development of central bank digital\ncurrencies (CBDCs) has raised concerns regarding privacy preservation and\nexcessive centralisation in digital payment systems. This paper proposes the\nQuantum-Resilient Privacy Ledger (QRPL) as an innovative token-based digital\ncurrency architecture that incorporates National Institute of Standards and\nTechnology (NIST)-standardised post-quantum cryptography (PQC) with hash-based\nzero-knowledge proofs to ensure user sovereignty, scalability, and transaction\nconfidentiality. Key contributions include adaptations of ephemeral proof\nchains for unlinkable transactions, a privacy-weighted Proof-of-Stake (PoS)\nconsensus to promote equitable participation, and a novel zero-knowledge\nproof-based mechanism for privacy-preserving selective disclosure. QRPL aims to\naddress critical shortcomings in prevailing CBDC designs, including risks of\npervasive surveillance, with a 10-20 second block time to balance security and\nthroughput in future monetary systems. While conceptual, empirical prototypes\nare planned. Future work includes prototype development to validate these\nmodels empirically.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09067v1", "cate": "cs.ET", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.10267", "title": "DNS Tunneling: Threat Landscape and Improved Detection Solutions", "authors": ["Novruz Amirov", "Baran Isik", "Bilal Ihsan Tuncer", "Serif Bahtiyar"], "categories": ["cs.CR", "cs.LG", "cs.NI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10267v1", "summary": "Detecting Domain Name System (DNS) tunneling is a significant challenge in\nsecurity due to its capacity to hide harmful actions within DNS traffic that\nappears to be normal and legitimate. Traditional detection methods are based on\nrule-based approaches or signature matching methods that are often insufficient\nto accurately identify such covert communication channels. This research is\nabout effectively detecting DNS tunneling. We propose a novel approach to\ndetect DNS tunneling with machine learning algorithms. We combine machine\nlearning algorithms to analyze the traffic by using features extracted from DNS\ntraffic. Analyses results show that the proposed approach is a good candidate\nto detect DNS tunneling accurately.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10267v1", "cate": "cs.CR", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.08958", "title": "Bridging Literature and the Universe Via A Multi-Agent Large Language Model System", "authors": ["Xiaowen Zhang", "Zhenyu Bi", "Xuan Wang", "Tiziana Di Matteo", "Rupert A. C. Croft"], "categories": ["astro-ph.IM", "cs.AI", "cs.MA"], "primary_category": "Subjects:       Instrumentation and Methods for Astrophysics (astro-ph.IM)", "pdf_link": null, "comments": "Comments:      6 pages, 4 figures", "url": "http://arxiv.org/abs/2507.08958v1", "summary": "As cosmological simulations and their associated software become increasingly\ncomplex, physicists face the challenge of searching through vast amounts of\nliterature and user manuals to extract simulation parameters from dense\nacademic papers, each using different models and formats. Translating these\nparameters into executable scripts remains a time-consuming and error-prone\nprocess. To improve efficiency in physics research and accelerate the\ncosmological simulation process, we introduce SimAgents, a multi-agent system\ndesigned to automate both parameter configuration from the literature and\npreliminary analysis for cosmology research. SimAgents is powered by\nspecialized LLM agents capable of physics reasoning, simulation software\nvalidation, and tool execution. These agents collaborate through structured\ncommunication, ensuring that extracted parameters are physically meaningful,\ninternally consistent, and software-compliant. We also construct a cosmological\nparameter extraction evaluation dataset by collecting over 40 simulations in\npublished papers from Arxiv and leading journals that cover diverse simulation\ntypes. Experiments on the dataset demonstrate a strong performance of\nSimAgents, highlighting its effectiveness and potential to accelerate\nscientific research for physicists. Our demonstration video is available at:\nhttps://youtu.be/w1zLpm_CaWA. The complete system and dataset are publicly\navailable at https://github.com/xwzhang98/SimAgents.", "comment": "6 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.08958v1", "cate": "astro-ph.IM", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09340", "title": "Unified Linear Parametric Map Modeling and Perception-aware Trajectory Planning for Mobile Robotics", "authors": ["Hongyu Nie", "Xingyu Li", "Xu Liu", "Zhaotong Tan", "Sen Mei", "Wenbo Su"], "categories": ["cs.RO", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Submitted to IEEE Transactions on Robotics (TRO) in July 2025", "url": "http://arxiv.org/abs/2507.09340v1", "summary": "Autonomous navigation in mobile robots, reliant on perception and planning,\nfaces major hurdles in large-scale, complex environments. These include heavy\ncomputational burdens for mapping, sensor occlusion failures for UAVs, and\ntraversal challenges on irregular terrain for UGVs, all compounded by a lack of\nperception-aware strategies. To address these challenges, we introduce Random\nMapping and Random Projection (RMRP). This method constructs a lightweight\nlinear parametric map by first mapping data to a high-dimensional space,\nfollowed by a sparse random projection for dimensionality reduction. Our novel\nResidual Energy Preservation Theorem provides theoretical guarantees for this\nprocess, ensuring critical geometric properties are preserved. Based on this\nmap, we propose the RPATR (Robust Perception-Aware Trajectory Planner)\nframework. For UAVs, our method unifies grid and Euclidean Signed Distance\nField (ESDF) maps. The front-end uses an analytical occupancy gradient to\nrefine initial paths for safety and smoothness, while the back-end uses a\nclosed-form ESDF for trajectory optimization. Leveraging the trained RMRP\nmodel's generalization, the planner predicts unobserved areas for proactive\nnavigation. For UGVs, the model characterizes terrain and provides closed-form\ngradients, enabling online planning to circumvent large holes. Validated in\ndiverse scenarios, our framework demonstrates superior mapping performance in\ntime, memory, and accuracy, and enables computationally efficient, safe\nnavigation for high-speed UAVs and UGVs. The code will be released to foster\ncommunity collaboration.", "comment": "Submitted to IEEE Transactions on Robotics (TRO) in July 2025", "pdf_url": "http://arxiv.org/pdf/2507.09340v1", "cate": "cs.RO", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.08973", "title": "Analytical Study on the Visibility of Potential Positions for External Human-Machine Interfaces", "authors": ["Jose Gonzalez-Belmonte", "Jaerock Kwon"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      28 pages, 5 tables, 10 figures", "url": "http://arxiv.org/abs/2507.08973v1", "summary": "As we move towards a future of autonomous vehicles, questions regarding their\nmethod of communication have arisen. One of the common questions concerns the\nplacement of the signaling used to communicate with pedestrians and road users,\nbut little work has been published fully dedicated to exploring this. This\npaper uses a simulation made in the Unity game engine to record the visibility\nof fifteen different vehicles, specifically regarding the visibility of frontal\nelements by a pedestrian on the sidewalk. Variables include the vehicle\nposition, number of vehicles on the road, and minimum and maximum distance of\nthe recorded points. It was concluded that the areas of the vehicle most often\nseen by pedestrians on the sidewalk attempting to cross the road were the\nfrontal frontal fenders and the headlights, with the frontal wheels, frontal\ndoors, bumper, and side mirrors are less visible alternatives. These findings\nare valuable in the future design of signaling for autonomous vehicles, in\norder to ensure pedestrians are able to see them on approaching vehicles. The\nsoftware used provides a platform for similar works in the future to be\nconducted.", "comment": "28 pages, 5 tables, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.08973v1", "cate": "cs.HC", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.08875", "title": "A New Approach for Multicriteria Assessment in the Ranking of Alternatives Using Cardinal and Ordinal Data", "authors": ["Fuh-Hwa Franklin Liu", "Su-Chuan Shih"], "categories": ["cs.AI", "90B50, 90C29, 90C08, 91A80, 91B06"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      38 pages, 6 figures, 5 table. A practice applicable method for multi-criteria assessments using cardinal and ordinal data", "url": "http://arxiv.org/abs/2507.08875v1", "summary": "Modern methods for multi-criteria assessment (MCA), such as Data Envelopment\nAnalysis (DEA), Stochastic Frontier Analysis (SFA), and Multiple Criteria\nDecision-Making (MCDM), are utilized to appraise a collection of\nDecision-Making Units (DMUs), also known as alternatives, based on several\ncriteria. These methodologies inherently rely on assumptions and can be\ninfluenced by subjective judgment to effectively tackle the complex evaluation\nchallenges in various fields. In real-world scenarios, it is essential to\nincorporate both quantitative and qualitative criteria as they consist of\ncardinal and ordinal data. Despite the inherent variability in the criterion\nvalues of different alternatives, the homogeneity assumption is often employed,\nsignificantly affecting evaluations. To tackle these challenges and determine\nthe most appropriate alternative, we propose a novel MCA approach that combines\ntwo Virtual Gap Analysis (VGA) models. The VGA framework, rooted in linear\nprogramming, is pivotal in the MCA methodology. This approach improves\nefficiency and fairness, ensuring that evaluations are both comprehensive and\ndependable, thus offering a strong and adaptive solution. Two comprehensive\nnumerical examples demonstrate the accuracy and transparency of our proposed\nmethod. The goal is to encourage continued advancement and stimulate progress\nin automated decision systems and decision support systems.", "comment": "38 pages, 6 figures, 5 table. A practice applicable method for\n  multi-criteria assessments using cardinal and ordinal data", "pdf_url": "http://arxiv.org/pdf/2507.08875v1", "cate": "cs.AI", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.10463", "title": "Solving the compute crisis with physics-based ASICs", "authors": ["Maxwell Aifer", "Zach Belateche", "Suraj Bramhavar", "Kerem Y. Camsari", "Patrick J. Coles", "Gavin Crooks", "Douglas J. Durian", "Andrea J. Liu", "Anastasia Marchenkova", "Antonio J. Martinez", "Peter L. McMahon", "Faris Sbahi", "Benjamin Weiner", "Logan G. Wright"], "categories": ["cs.ET", "cs.AR"], "primary_category": "Subjects:       Emerging Technologies (cs.ET)", "pdf_link": null, "comments": "Comments:      16 pages, 5 figures", "url": "http://arxiv.org/abs/2507.10463v1", "summary": "Escalating artificial intelligence (AI) demands expose a critical \"compute\ncrisis\" characterized by unsustainable energy consumption, prohibitive training\ncosts, and the approaching limits of conventional CMOS scaling. Physics-based\nApplication-Specific Integrated Circuits (ASICs) present a transformative\nparadigm by directly harnessing intrinsic physical dynamics for computation\nrather than expending resources to enforce idealized digital abstractions. By\nrelaxing the constraints needed for traditional ASICs, like enforced\nstatelessness, unidirectionality, determinism, and synchronization, these\ndevices aim to operate as exact realizations of physical processes, offering\nsubstantial gains in energy efficiency and computational throughput. This\napproach enables novel co-design strategies, aligning algorithmic requirements\nwith the inherent computational primitives of physical systems. Physics-based\nASICs could accelerate critical AI applications like diffusion models,\nsampling, optimization, and neural network inference as well as traditional\ncomputational workloads like scientific simulation of materials and molecules.\nUltimately, this vision points towards a future of heterogeneous,\nhighly-specialized computing platforms capable of overcoming current scaling\nbottlenecks and unlocking new frontiers in computational power and efficiency.", "comment": "16 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.10463v1", "cate": "cs.ET", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10457", "title": "Logic layer Prompt Control Injection (LPCI): A Novel Security Vulnerability Class in Agentic Systems", "authors": ["Hammad Atta", "Ken Huang", "Manish Bhatt", "Kamal Ahmed", "Muhammad Aziz Ul Haq", "Yasir Mehmood"], "categories": ["cs.CR", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10457v1", "summary": "The integration of large language models (LLMs) into enterprise systems has\ncreated a new class of covert security vulnerabilities, particularly within\nlogic-execution layers and persistent-memory contexts. In this paper, we\nintroduce Logic-Layer Prompt Control Injection (LPCI), a novel attack category\nin which encoded, delayed, and conditionally triggered payloads are embedded in\nmemory, vector stores, or tool outputs. These payloads can bypass conventional\ninput filters and trigger unauthorised behaviour across sessions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10457v1", "cate": "cs.CR", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09023", "title": "Accelerating Drug Discovery Through Agentic AI: A Multi-Agent Approach to Laboratory Automation in the DMTA Cycle", "authors": ["Yao Fehlis", "Charles Crain", "Aidan Jensen", "Michael Watson", "James Juhasz", "Paul Mandel", "Betty Liu", "Shawn Mahon", "Daren Wilson", "Nick Lynch-Jonely", "Ben Leedom", "David Fuller"], "categories": ["cs.SE", "cs.AI", "cs.MA"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09023v1", "summary": "The pharmaceutical industry faces unprecedented challenges in drug discovery,\nwith traditional approaches struggling to meet modern therapeutic development\ndemands. This paper introduces a novel AI framework, Tippy, that transforms\nlaboratory automation through specialized AI agents operating within the\nDesign-Make-Test-Analyze (DMTA) cycle. Our multi-agent system employs five\nspecialized agents - Supervisor, Molecule, Lab, Analysis, and Report, with\nSafety Guardrail oversight - each designed to excel in specific phases of the\ndrug discovery pipeline. Tippy represents the first production-ready\nimplementation of specialized AI agents for automating the DMTA cycle,\nproviding a concrete example of how AI can transform laboratory workflows. By\nleveraging autonomous AI agents that reason, plan, and collaborate, we\ndemonstrate how Tippy accelerates DMTA cycles while maintaining scientific\nrigor essential for pharmaceutical research. The system shows significant\nimprovements in workflow efficiency, decision-making speed, and\ncross-disciplinary coordination, offering a new paradigm for AI-assisted drug\ndiscovery.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09023v1", "cate": "cs.SE", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09344", "title": "C-ZUPT: Stationarity-Aided Aerial Hovering", "authors": ["Daniel Engelsman", "Itzik Klein"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      14 Pages, 16 Figures, 9 Tables", "url": "http://arxiv.org/abs/2507.09344v1", "summary": "Autonomous systems across diverse domains have underscored the need for\ndrift-resilient state estimation. Although satellite-based positioning and\ncameras are widely used, they often suffer from limited availability in many\nenvironments. As a result, positioning must rely solely on inertial sensors,\nleading to rapid accuracy degradation over time due to sensor biases and noise.\nTo counteract this, alternative update sources-referred to as information\naiding-serve as anchors of certainty. Among these, the zero-velocity update\n(ZUPT) is particularly effective in providing accurate corrections during\nstationary intervals, though it is restricted to surface-bound platforms. This\nwork introduces a controlled ZUPT (C-ZUPT) approach for aerial navigation and\ncontrol, independent of surface contact. By defining an uncertainty threshold,\nC-ZUPT identifies quasi-static equilibria to deliver precise velocity updates\nto the estimation filter. Extensive validation confirms that these\nopportunistic, high-quality updates significantly reduce inertial drift and\ncontrol effort. As a result, C-ZUPT mitigates filter divergence and enhances\nnavigation stability, enabling more energy-efficient hovering and substantially\nextending sustained flight-key advantages for resource-constrained aerial\nsystems.", "comment": "14 Pages, 16 Figures, 9 Tables", "pdf_url": "http://arxiv.org/pdf/2507.09344v1", "cate": "cs.RO", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09100", "title": "AInsight: Augmenting Expert Decision-Making with On-the-Fly Insights Grounded in Historical Data", "authors": ["Mohammad Abolnejadian", "Shakiba Amirshahi", "Matthew Brehmer", "Anamaria Crisan"], "categories": ["cs.HC", "cs.AI", "cs.CL", "H.5.0"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      7 pages and 4 figures. Proceedings of the 7th ACM Conference on Conversational User Interfaces (CUI '25)", "url": "http://arxiv.org/abs/2507.09100v1", "summary": "In decision-making conversations, experts must navigate complex choices and\nmake on-the-spot decisions while engaged in conversation. Although extensive\nhistorical data often exists, the real-time nature of these scenarios makes it\ninfeasible for decision-makers to review and leverage relevant information.\nThis raises an interesting question: What if experts could utilize relevant\npast data in real-time decision-making through insights derived from past data?\nTo explore this, we implemented a conversational user interface, taking\ndoctor-patient interactions as an example use case. Our system continuously\nlistens to the conversation, identifies patient problems and doctor-suggested\nsolutions, and retrieves related data from an embedded dataset, generating\nconcise insights using a pipeline built around a retrieval-based Large Language\nModel (LLM) agent. We evaluated the prototype by embedding Health Canada\ndatasets into a vector database and conducting simulated studies using sample\ndoctor-patient dialogues, showing effectiveness but also challenges, setting\ndirections for the next steps of our work.", "comment": "7 pages and 4 figures. Proceedings of the 7th ACM Conference on\n  Conversational User Interfaces (CUI '25)", "pdf_url": "http://arxiv.org/pdf/2507.09100v1", "cate": "cs.HC", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09080", "title": "BioAnalyst: A Foundation Model for Biodiversity", "authors": ["Athanasios Trantas", "Martino Mensio", "Stylianos Stasinos", "Sebastian Gribincea", "Taimur Khan", "Damian Podareanu", "Aliene van der Veen"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09080v1", "summary": "The accelerating loss of biodiversity presents critical challenges for\necological research and conservation strategies. The preservation of\nbiodiversity is paramount for maintaining ecological balance and ensuring the\nsustainability of ecosystems. However, biodiversity faces numerous threats,\nincluding habitat loss, climate change, and the proliferation of invasive\nspecies. Addressing these and other ecology-related challenges, both at local\nand global scales, requires comprehensive monitoring, predictive and\nconservation planning capabilities. Artificial Intelligence (AI) Foundation\nModels (FMs) have gained significant momentum in numerous scientific domains by\nleveraging vast datasets to learn general-purpose representations adaptable to\nvarious downstream tasks. This paradigm holds immense promise for biodiversity\nconservation. In response, we introduce BioAnalyst, the first Foundation Model\ntailored for biodiversity analysis and conservation planning. BioAnalyst\nemploys a transformer-based architecture, pre-trained on extensive multi-modal\ndatasets encompassing species occurrence records, remote sensing indicators,\nclimate and environmental variables. BioAnalyst is designed for adaptability,\nallowing for fine-tuning of a range of downstream tasks, such as species\ndistribution modelling, habitat suitability assessments, invasive species\ndetection, and population trend forecasting. We evaluate the model's\nperformance on two downstream use cases, demonstrating its generalisability\ncompared to existing methods, particularly in data-scarce scenarios for two\ndistinct use-cases, establishing a new accuracy baseline for ecological\nforecasting. By openly releasing BioAnalyst and its fine-tuning workflows to\nthe scientific community, we aim to foster collaborative efforts in\nbiodiversity modelling and advance AI-driven solutions to pressing ecological\nchallenges.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09080v1", "cate": "cs.AI", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.08880", "title": "Central Bank Digital Currencies: A Survey", "authors": ["Qifeng Tang", "Yain-Whar Si"], "categories": ["econ.GN", "cs.CE", "cs.CY", "cs.ET", "q-fin.EC", "68M14", "A.1; C.5"], "primary_category": "Subjects:       General Economics (econ.GN)", "pdf_link": null, "comments": "Comments:      49 pages, 6 figures", "url": "http://arxiv.org/abs/2507.08880v1", "summary": "With the advancement of digital payment technologies, central banks worldwide\nhave increasingly begun to explore the implementation of Central Bank Digital\nCurrencies (CBDCs). This paper presents a comprehensive review of the latest\ndevelopments in CBDC system design and implementation. By analyzing 135\nresearch papers published between 2018 and 2025, the study provides an in-depth\nexamination of CBDC design taxonomy and ecosystem frameworks. Grounded in the\nCBDC Design Pyramid, the paper refines and expands key architectural elements\nby thoroughly investigating innovations in ledger technologies, the selection\nof consensus mechanisms, and challenges associated with offline payments and\ndigital wallet integration. Furthermore, it conceptualizes a CBDC ecosystem. A\ndetailed comparative analysis of 26 existing CBDC systems is conducted across\nfour dimensions: system architecture, ledger technology, access model, and\napplication domain. The findings reveal that the most common configuration\nconsists of a two-tier architecture, distributed ledger technology (DLT), and a\ntoken-based access model. However, no dominant trend has emerged regarding\napplication domains. Notably, recent research shows a growing focus on\nleveraging CBDCs for cross-border payments to resolve inefficiencies and\nstructural delays in current systems. Finally, the paper offers several\nforward-looking recommendations for future research.", "comment": "49 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.08880v1", "cate": "econ.GN", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.10489", "title": "SynthGuard: Redefining Synthetic Data Generation with a Scalable and Privacy-Preserving Workflow Framework", "authors": ["Eduardo Brito", "Mahmoud Shoush", "Kristian Tamm", "Paula Etti", "Liina Kamm"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      This is the extended version of the paper to appear in the Proceedings of the 1st International Workshop on Responsible Data Governance, Privacy, and Digital Transformation (RDGPT 2025), held in conjunction with the 20th International Conference on Availability, Reliability and Security (ARES 2025)", "url": "http://arxiv.org/abs/2507.10489v1", "summary": "The growing reliance on data-driven applications in sectors such as\nhealthcare, finance, and law enforcement underscores the need for secure,\nprivacy-preserving, and scalable mechanisms for data generation and sharing.\nSynthetic data generation (SDG) has emerged as a promising approach but often\nrelies on centralized or external processing, raising concerns about data\nsovereignty, domain ownership, and compliance with evolving regulatory\nstandards. To overcome these issues, we introduce SynthGuard, a framework\ndesigned to ensure computational governance by enabling data owners to maintain\ncontrol over SDG workflows. SynthGuard supports modular and privacy-preserving\nworkflows, ensuring secure, auditable, and reproducible execution across\ndiverse environments. In this paper, we demonstrate how SynthGuard addresses\nthe complexities at the intersection of domain-specific needs and scalable SDG\nby aligning with requirements for data sovereignty and regulatory compliance.\nDeveloped iteratively with domain expert input, SynthGuard has been validated\nthrough real-world use cases, demonstrating its ability to balance security,\nprivacy, and scalability while ensuring compliance. The evaluation confirms its\neffectiveness in implementing and executing SDG workflows and integrating\nprivacy and utility assessments across various computational environments.", "comment": "This is the extended version of the paper to appear in the\n  Proceedings of the 1st International Workshop on Responsible Data Governance,\n  Privacy, and Digital Transformation (RDGPT 2025), held in conjunction with\n  the 20th International Conference on Availability, Reliability and Security\n  (ARES 2025)", "pdf_url": "http://arxiv.org/pdf/2507.10489v1", "cate": "cs.CR", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09223", "title": "Coordinated Communication and Inventory Optimization in Multi-Retailer Supply Chains", "authors": ["Sagar Sudhakara", "Yuchong Zhang"], "categories": ["math.OC", "cs.MA"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      Accepted at the Workshop on \"AI for Supply Chain: Today and Future\" @ 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2 (KDD '25), August 3, 2025, Toronto, ON, Canada", "url": "http://arxiv.org/abs/2507.09223v1", "summary": "We consider a multi-retailer supply chain where each retailer can dynamically\nchoose when to share information (e.g., local inventory levels or demand\nobservations) with other retailers, incurring a communication cost for each\nsharing event. This flexible information exchange mechanism contrasts with\nfixed protocols such as always sharing or never sharing. We formulate a joint\noptimization of inventory control and communication strategies, aiming to\nbalance the trade-off between communication overhead and operational\nperformance (service levels, holding, and stockout costs). We adopt a common\ninformation framework and derive a centralized Partially Observable Markov\nDecision Process (POMDP) model for a supply chain coordinator. Solving this\ncoordinator's POMDP via dynamic programming characterizes the structure of\noptimal policies, determining when retailers should communicate and how they\nshould adjust orders based on available information. We show that, in this\nsetting, retailers can often act optimally by sharing only limited summaries of\ntheir private data, reducing communication frequency without compromising\nperformance. We also incorporate practical constraints on communication\nfrequency and propose an approximate point-based POMDP solution method\n(PBVI/SARSOP) to address computational complexity. Numerical experiments on\nmulti-retailer inventory scenarios demonstrate that our approach significantly\nimproves the cost-service trade-off compared to static information sharing\npolicies, effectively optimizing the schedule of information exchange for\ncooperative inventory control.", "comment": "Accepted at the Workshop on \"AI for Supply Chain: Today and Future\" @\n  31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2 (KDD\n  '25), August 3, 2025, Toronto, ON, Canada", "pdf_url": "http://arxiv.org/pdf/2507.09223v1", "cate": "math.OC", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09371", "title": "Constrained Style Learning from Imperfect Demonstrations under Task Optimality", "authors": ["Kehan Wen", "Chenhao Li", "Junzhe He", "Marco Hutter"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      This paper is under review", "url": "http://arxiv.org/abs/2507.09371v1", "summary": "Learning from demonstration has proven effective in robotics for acquiring\nnatural behaviors, such as stylistic motions and lifelike agility, particularly\nwhen explicitly defining style-oriented reward functions is challenging.\nSynthesizing stylistic motions for real-world tasks usually requires balancing\ntask performance and imitation quality. Existing methods generally depend on\nexpert demonstrations closely aligned with task objectives. However, practical\ndemonstrations are often incomplete or unrealistic, causing current methods to\nboost style at the expense of task performance. To address this issue, we\npropose formulating the problem as a constrained Markov Decision Process\n(CMDP). Specifically, we optimize a style-imitation objective with constraints\nto maintain near-optimal task performance. We introduce an adaptively\nadjustable Lagrangian multiplier to guide the agent to imitate demonstrations\nselectively, capturing stylistic nuances without compromising task performance.\nWe validate our approach across multiple robotic platforms and tasks,\ndemonstrating both robust task performance and high-fidelity style learning. On\nANYmal-D hardware we show a 14.5% drop in mechanical energy and a more agile\ngait pattern, showcasing real-world benefits.", "comment": "This paper is under review", "pdf_url": "http://arxiv.org/pdf/2507.09371v1", "cate": "cs.RO", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09190", "title": "User-to-PC Authentication Through Confirmation on Mobile Devices: On Usability and Performance", "authors": ["Andreas Pramendorfer", "Rainhard Dieter Findling"], "categories": ["cs.HC", "cs.CR"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Submitted to MoMM 2025", "url": "http://arxiv.org/abs/2507.09190v1", "summary": "Protecting personal computers (PCs) from unauthorized access typically relies\non password authentication, which is know to suffer from cognitive burden and\nweak credentials. As many users nowadays carry mobile devices with advanced\nsecurity features throughout their day, there is an opportunity to leverage\nthese devices to improve authentication to PCs. In this paper we utilize a\ntoken-based passwordless approach where users authenticate to their PC by\nconfirming the authentication request on their smartphones or smartwatches.\nUpon a request to login to the PC, or to evaluate privileges, the PC issues an\nauthentication request that users receive on their mobile devices, where users\ncan confirm or deny the request. We evaluate button tap and biometric\nfingerprint verification as confirmation variants, and compare their\nauthentication duration, success rate, and usability to traditional\npassword-based authentication in a user study with 30 participants and a total\nof 1,200 authentication attempts. Smartwatch-based authentication outperformed\npassword-based authentication and smartphone-based variants in authentication\nduration, while showing comparable success rates. Participants rated\nsmartwatch-based authentication highest in usability, followed by\npassword-based authentication and smartphone-based authentication.", "comment": "Submitted to MoMM 2025", "pdf_url": "http://arxiv.org/pdf/2507.09190v1", "cate": "cs.HC", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09089", "title": "Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity", "authors": ["Joel Becker", "Nate Rush", "Elizabeth Barnes", "David Rein"], "categories": ["cs.AI", "cs.HC", "cs.SE", "I.2"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      50 pages, 8 tables, 22 figures", "url": "http://arxiv.org/abs/2507.09089v1", "summary": "Despite widespread adoption, the impact of AI tools on software development\nin the wild remains understudied. We conduct a randomized controlled trial\n(RCT) to understand how AI tools at the February-June 2025 frontier affect the\nproductivity of experienced open-source developers. 16 developers with moderate\nAI experience complete 246 tasks in mature projects on which they have an\naverage of 5 years of prior experience. Each task is randomly assigned to allow\nor disallow usage of early 2025 AI tools. When AI tools are allowed, developers\nprimarily use Cursor Pro, a popular code editor, and Claude 3.5/3.7 Sonnet.\nBefore starting tasks, developers forecast that allowing AI will reduce\ncompletion time by 24%. After completing the study, developers estimate that\nallowing AI reduced completion time by 20%. Surprisingly, we find that allowing\nAI actually increases completion time by 19%--AI tooling slowed developers\ndown. This slowdown also contradicts predictions from experts in economics (39%\nshorter) and ML (38% shorter). To understand this result, we collect and\nevaluate evidence for 20 properties of our setting that a priori could\ncontribute to the observed slowdown effect--for example, the size and quality\nstandards of projects, or prior developer experience with AI tooling. Although\nthe influence of experimental artifacts cannot be entirely ruled out, the\nrobustness of the slowdown effect across our analyses suggests it is unlikely\nto primarily be a function of our experimental design.", "comment": "50 pages, 8 tables, 22 figures", "pdf_url": "http://arxiv.org/pdf/2507.09089v1", "cate": "cs.AI", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.08908", "title": "The Engineer's Dilemma: A Review of Establishing a Legal Framework for Integrating Machine Learning in Construction by Navigating Precedents and Industry Expectations", "authors": ["M. Z. Naser"], "categories": ["cs.CY", "cs.ET", "cs.LG"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08908v1", "summary": "Despite the widespread interest in machine learning (ML), the engineering\nindustry has not yet fully adopted ML-based methods, which has left engineers\nand stakeholders uncertain about the legal and regulatory frameworks that\ngovern their decisions. This gap remains unaddressed as an engineer's\ndecision-making process, typically governed by professional ethics and\npractical guidelines, now intersects with complex algorithmic outputs. To\nbridge this gap, this paper explores how engineers can navigate legal\nprinciples and legislative justifications that support and/or contest the\ndeployment of ML technologies. Drawing on recent precedents and experiences\ngained from other fields, this paper argues that analogical reasoning can\nprovide a basis for embedding ML within existing engineering codes while\nmaintaining professional accountability and meeting safety requirements. In\nexploring these issues, the discussion focuses on established liability\ndoctrines, such as negligence and product liability, and highlights how courts\nhave evaluated the use of predictive models. We further analyze how legislative\nbodies and standard-setting organizations can furnish explicit guidance\nequivalent to prior endorsements of emergent technologies. This exploration\nstresses the vitality of understanding the interplay between technical\njustifications and legal precedents for shaping an informed stance on ML's\nlegitimacy in engineering practice. Finally, our analysis catalyzes a legal\nframework for integrating ML through which stakeholders can critically assess\nthe responsibilities, liabilities, and benefits inherent in ML-driven\nengineering solutions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08908v1", "cate": "cs.CY", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.10491", "title": "BURN: Backdoor Unlearning via Adversarial Boundary Analysis", "authors": ["Yanghao Su", "Jie Zhang", "Yiming Li", "Tianwei Zhang", "Qing Guo", "Weiming Zhang", "Nenghai Yu", "Nils Lukas", "Wenbo Zhou"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10491v1", "summary": "Backdoor unlearning aims to remove backdoor-related information while\npreserving the model's original functionality. However, existing unlearning\nmethods mainly focus on recovering trigger patterns but fail to restore the\ncorrect semantic labels of poison samples. This limitation prevents them from\nfully eliminating the false correlation between the trigger pattern and the\ntarget label. To address this, we leverage boundary adversarial attack\ntechniques, revealing two key observations. First, poison samples exhibit\nsignificantly greater distances from decision boundaries compared to clean\nsamples, indicating they require larger adversarial perturbations to change\ntheir predictions. Second, while adversarial predicted labels for clean samples\nare uniformly distributed, those for poison samples tend to revert to their\noriginal correct labels. Moreover, the features of poison samples restore to\nclosely resemble those of corresponding clean samples after adding adversarial\nperturbations. Building upon these insights, we propose Backdoor Unlearning via\nadversaRial bouNdary analysis (BURN), a novel defense framework that integrates\nfalse correlation decoupling, progressive data refinement, and model\npurification. In the first phase, BURN employs adversarial boundary analysis to\ndetect poisoned samples based on their abnormal adversarial boundary distances,\nthen restores their correct semantic labels for fine-tuning. In the second\nphase, it employs a feedback mechanism that tracks prediction discrepancies\nbetween the original backdoored model and progressively sanitized models,\nguiding both dataset refinement and model purification. Extensive evaluations\nacross multiple datasets, architectures, and seven diverse backdoor attack\ntypes confirm that BURN effectively removes backdoor threats while maintaining\nthe model's original performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10491v1", "cate": "cs.CR", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09255", "title": "StockSim: A Dual-Mode Order-Level Simulator for Evaluating Multi-Agent LLMs in Financial Markets", "authors": ["Charidimos Papadakis", "Giorgos Filandrianos", "Angeliki Dimitriou", "Maria Lymperaiou", "Konstantinos Thomas", "Giorgos Stamou"], "categories": ["cs.CE", "cs.MA"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09255v1", "summary": "We present StockSim, an open-source simulation platform for systematic\nevaluation of large language models (LLMs) in realistic financial\ndecision-making scenarios. Unlike previous toolkits that offer limited scope,\nStockSim delivers a comprehensive system that fully models market dynamics and\nsupports diverse simulation modes of varying granularity. It incorporates\ncritical real-world factors, such as latency, slippage, and order-book\nmicrostructure, that were previously neglected, enabling more faithful and\ninsightful assessment of LLM-based trading agents. An extensible, role-based\nagent framework supports heterogeneous trading strategies and multi-agent\ncoordination, making StockSim a uniquely capable testbed for NLP research on\nreasoning under uncertainty and sequential decision-making. We open-source all\nour code at https: //github.com/harrypapa2002/StockSim.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09255v1", "cate": "cs.CE", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09383", "title": "Real-Time Adaptive Motion Planning via Point Cloud-Guided, Energy-Based Diffusion and Potential Fields", "authors": ["Wondmgezahu Teshome", "Kian Behzad", "Octavia Camps", "Michael Everett", "Milad Siami", "Mario Sznaier"], "categories": ["cs.RO", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted to IEEE RA-L 2025", "url": "http://arxiv.org/abs/2507.09383v1", "summary": "Motivated by the problem of pursuit-evasion, we present a motion planning\nframework that combines energy-based diffusion models with artificial potential\nfields for robust real time trajectory generation in complex environments. Our\napproach processes obstacle information directly from point clouds, enabling\nefficient planning without requiring complete geometric representations. The\nframework employs classifier-free guidance training and integrates local\npotential fields during sampling to enhance obstacle avoidance. In dynamic\nscenarios, the system generates initial trajectories using the diffusion model\nand continuously refines them through potential field-based adaptation,\ndemonstrating effective performance in pursuit-evasion scenarios with partial\npursuer observability.", "comment": "Accepted to IEEE RA-L 2025", "pdf_url": "http://arxiv.org/pdf/2507.09383v1", "cate": "cs.RO", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09262", "title": "Discrepancies in Mental Workload Estimation: Self-Reported versus EEG-Based Measures in Data Visualization Evaluation", "authors": ["Soobin Yim", "Sangbong Yoo", "Chanyoung Yoon", "Chanyoung Jung", "Chansoo Kim", "Yun Jang", "Ghulam Jilani Quadri"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09262v1", "summary": "Accurate assessment of mental workload (MW) is crucial for understanding\ncognitive processes during visualization tasks. While EEG-based measures are\nemerging as promising alternatives to conventional assessment techniques, such\nas selfreport measures, studies examining consistency across these different\nmethodologies are limited. In a preliminary study, we observed indications of\npotential discrepancies between EEGbased and self-reported MW measures.\nMotivated by these preliminary observations, our study further explores the\ndiscrepancies between EEG-based and self-reported MW assessment methods through\nan experiment involving visualization tasks. In the experiment, we employ two\nbenchmark tasks: the Visualization Literacy Assessment Test (VLAT) and a\nSpatial Visualization (SV) task. EEG signals are recorded from participants\nusing a 32-channel system at a sampling rate of 128 Hz during the visualization\ntasks. For each participant, MW is estimated using an EEG-based model built on\na Graph Attention Network (GAT) architecture, and these estimates are compared\nwith conventional MW measures to examine potential discrepancies. Our findings\nreveal notable discrepancies between task difficulty and EEG-based MW\nestimates, as well as between EEG-based and self-reported MW measures across\nvarying task difficulty levels. Additionally, the observed patterns suggest the\npresence of unconscious cognitive effort that may not be captured by selfreport\nalone.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09262v1", "cate": "cs.HC", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09179", "title": "Hide-and-Shill: A Reinforcement Learning Framework for Market Manipulation Detection in Symphony-a Decentralized Multi-Agent System", "authors": ["Ronghua Shi", "Yiou Liu", "Xinyu Ying", "Yang Tan", "Yuchun Feng", "Lynn Ai", "Bill Shi", "Xuhui Wang", "Zhuang Liu"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09179v1", "summary": "Decentralized finance (DeFi) has introduced a new era of permissionless\nfinancial innovation but also led to unprecedented market manipulation. Without\ncentralized oversight, malicious actors coordinate shilling campaigns and\npump-and-dump schemes across various platforms. We propose a Multi-Agent\nReinforcement Learning (MARL) framework for decentralized manipulation\ndetection, modeling the interaction between manipulators and detectors as a\ndynamic adversarial game. This framework identifies suspicious patterns using\ndelayed token price reactions as financial indicators.Our method introduces\nthree innovations: (1) Group Relative Policy Optimization (GRPO) to enhance\nlearning stability in sparse-reward and partially observable settings; (2) a\ntheory-based reward function inspired by rational expectations and information\nasymmetry, differentiating price discovery from manipulation noise; and (3) a\nmulti-modal agent pipeline that integrates LLM-based semantic features, social\ngraph signals, and on-chain market data for informed decision-making.The\nframework is integrated within the Symphony system, a decentralized multi-agent\narchitecture enabling peer-to-peer agent execution and trust-aware learning\nthrough distributed logs, supporting chain-verifiable evaluation. Symphony\npromotes adversarial co-evolution among strategic actors and maintains robust\nmanipulation detection without centralized oracles, enabling real-time\nsurveillance across global DeFi ecosystems.Trained on 100,000 real-world\ndiscourse episodes and validated in adversarial simulations, Hide-and-Shill\nachieves top performance in detection accuracy and causal attribution. This\nwork bridges multi-agent systems with financial surveillance, advancing a new\nparadigm for decentralized market intelligence. All resources are available at\nthe Hide-and-Shill GitHub repository to promote open research and\nreproducibility.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09179v1", "cate": "cs.AI", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09495", "title": "GenAI-based Multi-Agent Reinforcement Learning towards Distributed Agent Intelligence: A Generative-RL Agent Perspective", "authors": ["Hang Wang", "Junshan Zhang"], "categories": ["cs.AI", "cs.ET", "cs.HC", "cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Position paper", "url": "http://arxiv.org/abs/2507.09495v1", "summary": "Multi-agent reinforcement learning faces fundamental challenges that\nconventional approaches have failed to overcome: exponentially growing joint\naction spaces, non-stationary environments where simultaneous learning creates\nmoving targets, and partial observability that constrains coordination. Current\nmethods remain reactive, employing stimulus-response mechanisms that fail when\nfacing novel scenarios. We argue for a transformative paradigm shift from\nreactive to proactive multi-agent intelligence through generative AI-based\nreinforcement learning. This position advocates reconceptualizing agents not as\nisolated policy optimizers, but as sophisticated generative models capable of\nsynthesizing complex multi-agent dynamics and making anticipatory decisions\nbased on predictive understanding of future interactions. Rather than\nresponding to immediate observations, generative-RL agents can model\nenvironment evolution, predict other agents' behaviors, generate coordinated\naction sequences, and engage in strategic reasoning accounting for long-term\ndynamics. This approach leverages pattern recognition and generation\ncapabilities of generative AI to enable proactive decision-making, seamless\ncoordination through enhanced communication, and dynamic adaptation to evolving\nscenarios. We envision this paradigm shift will unlock unprecedented\npossibilities for distributed intelligence, moving beyond individual\noptimization toward emergent collective behaviors representing genuine\ncollaborative intelligence. The implications extend across autonomous systems,\nrobotics, and human-AI collaboration, promising solutions to coordination\nchallenges intractable under traditional reactive frameworks.", "comment": "Position paper", "pdf_url": "http://arxiv.org/pdf/2507.09495v1", "cate": "cs.AI", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.08983", "title": "Exploiting Leaderboards for Large-Scale Distribution of Malicious Models", "authors": ["Anshuman Suri", "Harsh Chaudhari", "Yuefeng Peng", "Ali Naseh", "Amir Houmansadr", "Alina Oprea"], "categories": ["cs.LG", "cs.CR"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08983v1", "summary": "While poisoning attacks on machine learning models have been extensively\nstudied, the mechanisms by which adversaries can distribute poisoned models at\nscale remain largely unexplored. In this paper, we shed light on how model\nleaderboards -- ranked platforms for model discovery and evaluation -- can\nserve as a powerful channel for adversaries for stealthy large-scale\ndistribution of poisoned models. We present TrojanClimb, a general framework\nthat enables injection of malicious behaviors while maintaining competitive\nleaderboard performance. We demonstrate its effectiveness across four diverse\nmodalities: text-embedding, text-generation, text-to-speech and text-to-image,\nshowing that adversaries can successfully achieve high leaderboard rankings\nwhile embedding arbitrary harmful functionalities, from backdoors to bias\ninjection. Our findings reveal a significant vulnerability in the machine\nlearning ecosystem, highlighting the urgent need to redesign leaderboard\nevaluation mechanisms to detect and filter malicious (e.g., poisoned) models,\nwhile exposing broader security implications for the machine learning community\nregarding the risks of adopting models from unverified sources.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08983v1", "cate": "cs.LG", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09657", "title": "Negotiating Comfort: Simulating Personality-Driven LLM Agents in Shared Residential Social Networks", "authors": ["Ann Nedime Nese Rende", "Tolga Yilmaz", "Özgür Ulusoy"], "categories": ["cs.SI", "cs.MA"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09657v1", "summary": "We use generative agents powered by large language models (LLMs) to simulate\na social network in a shared residential building, driving the temperature\ndecisions for a central heating system. Agents, divided into Family Members and\nRepresentatives, consider personal preferences, personal traits, connections,\nand weather conditions. Daily simulations involve family-level consensus\nfollowed by building-wide decisions among representatives. We tested three\npersonality traits distributions (positive, mixed, and negative) and found that\npositive traits correlate with higher happiness and stronger friendships.\nTemperature preferences, assertiveness, and selflessness have a significant\nimpact on happiness and decisions. This work demonstrates how LLM-driven agents\ncan help simulate nuanced human behavior where complex real-life human\nsimulations are difficult to set.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09657v1", "cate": "cs.SI", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09463", "title": "Influence of Static and Dynamic Downwash Interactions on Multi-Quadrotor Systems", "authors": ["Anoop Kiran", "Nora Ayanian", "Kenneth Breuer"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted for publication in Robotics: Science and Systems (RSS) 2025, 12 pages, 16 figures", "url": "http://arxiv.org/abs/2507.09463v1", "summary": "Flying multiple quadrotors in close proximity presents a significant\nchallenge due to complex aerodynamic interactions, particularly downwash\neffects that are known to destabilize vehicles and degrade performance.\nTraditionally, multi-quadrotor systems rely on conservative strategies, such as\ncollision avoidance zones around the robot volume, to circumvent this effect.\nThis restricts their capabilities by requiring a large volume for the operation\nof a multi-quadrotor system, limiting their applicability in dense\nenvironments. This work provides a comprehensive, data-driven analysis of the\ndownwash effect, with a focus on characterizing, analyzing, and understanding\nforces, moments, and velocities in both single and multi-quadrotor\nconfigurations. We use measurements of forces and torques to characterize\nvehicle interactions, and particle image velocimetry (PIV) to quantify the\nspatial features of the downwash wake for a single quadrotor and an interacting\npair of quadrotors. This data can be used to inform physics-based strategies\nfor coordination, leverage downwash for optimized formations, expand the\nenvelope of operation, and improve the robustness of multi-quadrotor control.", "comment": "Accepted for publication in Robotics: Science and Systems (RSS) 2025,\n  12 pages, 16 figures", "pdf_url": "http://arxiv.org/pdf/2507.09463v1", "cate": "cs.RO", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09489", "title": "TraSculptor: Visual Analytics for Enhanced Decision-Making in Road Traffic Planning", "authors": ["Zikun Deng", "Yuanbang Liu", "Mingrui Zhu", "Da Xiang", "Haiyue Yu", "Zicheng Su", "Qinglong Lu", "Tobias Schreck", "Yi Cai"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      IEEE Transactions on Visualization and Computer Graphics", "url": "http://arxiv.org/abs/2507.09489v1", "summary": "The design of urban road networks significantly influences traffic\nconditions, underscoring the importance of informed traffic planning. Traffic\nplanning experts rely on specialized platforms to simulate traffic systems,\nassessing the efficacy of the road network across various states of\nmodifications. Nevertheless, a prevailing issue persists: many existing traffic\nplanning platforms exhibit inefficiencies in flexibly interacting with the road\nnetwork's structure and attributes and intuitively comparing multiple states\nduring the iterative planning process. This paper introduces TraSculptor, an\ninteractive planning decision-making system. To develop TraSculptor, we\nidentify and address two challenges: interactive modification of road networks\nand intuitive comparison of multiple network states. For the first challenge,\nwe establish flexible interactions to enable experts to easily and directly\nmodify the road network on the map. For the second challenge, we design a\ncomparison view with a history tree of multiple states and a road-state matrix\nto facilitate intuitive comparison of road network states. To evaluate\nTraSculptor, we provided a usage scenario where the Braess's paradox was\nshowcased, invited experts to perform a case study on the Sioux Falls network,\nand collected expert feedback through interviews.", "comment": "IEEE Transactions on Visualization and Computer Graphics", "pdf_url": "http://arxiv.org/pdf/2507.09489v1", "cate": "cs.HC", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09329", "title": "When Developer Aid Becomes Security Debt: A Systematic Analysis of Insecure Behaviors in LLM Coding Agents", "authors": ["Matous Kozak", "Roshanak Zilouchian Moghaddam", "Siva Sivaraman"], "categories": ["cs.AI", "cs.CR"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      15 pages", "url": "http://arxiv.org/abs/2507.09329v1", "summary": "LLM-based coding agents are rapidly being deployed in software development,\nyet their security implications remain poorly understood. These agents, while\ncapable of accelerating software development, may inadvertently introduce\ninsecure practices. We conducted the first systematic security evaluation of\nautonomous coding agents, analyzing over 12,000 actions across five\nstate-of-the-art models (GPT-4o, GPT-4.1, Claude variants) on 93 real-world\nsoftware setup tasks. Our findings reveal significant security concerns: 21% of\nagent trajectories contained insecure actions, with models showing substantial\nvariation in security behavior. We developed a high-precision detection system\nthat identified four major vulnerability categories, with information exposure\n(CWE-200) being the most prevalent one. We also evaluated mitigation strategies\nincluding feedback mechanisms and security reminders with various effectiveness\nbetween models. GPT-4.1 demonstrated exceptional security awareness with 96.8%\nmitigation success. Our work provides the first comprehensive framework for\nevaluating coding agent security and highlights the need for security-aware\ndesign of next generation LLM-based coding agents.", "comment": "15 pages", "pdf_url": "http://arxiv.org/pdf/2507.09329v1", "cate": "cs.AI", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09682", "title": "OrQstrator: An AI-Powered Framework for Advanced Quantum Circuit Optimization", "authors": ["Laura Baird", "Armin Moin"], "categories": ["cs.SE", "cs.AI", "cs.ET"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      IEEE International Conference on Quantum Computing and Engineering (QCE) 2025 - Extended Abstract", "url": "http://arxiv.org/abs/2507.09682v1", "summary": "We propose a novel approach, OrQstrator, which is a modular framework for\nconducting quantum circuit optimization in the Noisy Intermediate-Scale Quantum\n(NISQ) era. Our framework is powered by Deep Reinforcement Learning (DRL). Our\norchestration engine intelligently selects among three complementary circuit\noptimizers: A DRL-based circuit rewriter trained to reduce depth and gate count\nvia learned rewrite sequences; a domain-specific optimizer that performs\nefficient local gate resynthesis and numeric optimization; a parameterized\ncircuit instantiator that improves compilation by optimizing template circuits\nduring gate set translation. These modules are coordinated by a central\norchestration engine that learns coordination policies based on circuit\nstructure, hardware constraints, and backend-aware performance features such as\ngate count, depth, and expected fidelity. The system outputs an optimized\ncircuit for hardware-aware transpilation and execution, leveraging techniques\nfrom an existing state-of-the-art approach, called the NISQ Analyzer, to adapt\nto backend constraints.", "comment": "IEEE International Conference on Quantum Computing and Engineering\n  (QCE) 2025 - Extended Abstract", "pdf_url": "http://arxiv.org/pdf/2507.09682v1", "cate": "cs.SE", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.08831", "title": "View Invariant Learning for Vision-Language Navigation in Continuous Environments", "authors": ["Josh Qixuan Sun", "Xiaoying Xing", "Huaiyuan Weng", "Chul Min Yeum", "Mark Crowley"], "categories": ["cs.CV", "cs.LG", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Under review", "url": "http://arxiv.org/abs/2507.08831v1", "summary": "Vision-Language Navigation in Continuous Environments (VLNCE), where an agent\nfollows instructions and moves freely to reach a destination, is a key research\nproblem in embodied AI. However, most navigation policies are sensitive to\nviewpoint changes, i.e., variations in camera height and viewing angle that\nalter the agent's observation. In this paper, we introduce a generalized\nscenario, V2-VLNCE (VLNCE with Varied Viewpoints), and propose VIL (View\nInvariant Learning), a view-invariant post-training strategy that enhances the\nrobustness of existing navigation policies to changes in camera viewpoint. VIL\nemploys a contrastive learning framework to learn sparse and view-invariant\nfeatures. Additionally, we introduce a teacher-student framework for the\nWaypoint Predictor Module, a core component of most VLNCE baselines, where a\nview-dependent teacher model distills knowledge into a view-invariant student\nmodel. We employ an end-to-end training paradigm to jointly optimize these\ncomponents, thus eliminating the cost for individual module training. Empirical\nresults show that our method outperforms state-of-the-art approaches on\nV2-VLNCE by 8-15% measured on Success Rate for two standard benchmark datasets\nR2R-CE and RxR-CE. Furthermore, we evaluate VIL under the standard VLNCE\nsetting and find that, despite being trained for varied viewpoints, it often\nstill improves performance. On the more challenging RxR-CE dataset, our method\nalso achieved state-of-the-art performance across all metrics when compared to\nother map-free methods. This suggests that adding VIL does not diminish the\nstandard viewpoint performance and can serve as a plug-and-play post-training\nmethod.", "comment": "Under review", "pdf_url": "http://arxiv.org/pdf/2507.08831v1", "cate": "cs.CV", "date": "2025-07-05", "updated": "2025-07-05"}
{"id": "2507.09282", "title": "ClaritySpeech: Dementia Obfuscation in Speech", "authors": ["Dominika Woszczyk", "Ranya Aloufi", "Soteris Demetriou"], "categories": ["cs.CL", "cs.CR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted at Interspeech 2025", "url": "http://arxiv.org/abs/2507.09282v1", "summary": "Dementia, a neurodegenerative disease, alters speech patterns, creating\ncommunication barriers and raising privacy concerns. Current speech\ntechnologies, such as automatic speech transcription (ASR), struggle with\ndementia and atypical speech, further challenging accessibility. This paper\npresents a novel dementia obfuscation in speech framework, ClaritySpeech,\nintegrating ASR, text obfuscation, and zero-shot text-to-speech (TTS) to\ncorrect dementia-affected speech while preserving speaker identity in low-data\nenvironments without fine-tuning. Results show a 16% and 10% drop in mean F1\nscore across various adversarial settings and modalities (audio, text, fusion)\nfor ADReSS and ADReSSo, respectively, maintaining 50% speaker similarity. We\nalso find that our system improves WER (from 0.73 to 0.08 for ADReSS and 0.15\nfor ADReSSo) and speech quality from 1.65 to ~2.15, enhancing privacy and\naccessibility.", "comment": "Accepted at Interspeech 2025", "pdf_url": "http://arxiv.org/pdf/2507.09282v1", "cate": "cs.CL", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09836", "title": "Multi-residual Mixture of Experts Learning for Cooperative Control in Multi-vehicle Systems", "authors": ["Vindula Jayawardana", "Sirui Li", "Yashar Farid", "Cathy Wu"], "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09836v1", "summary": "Autonomous vehicles (AVs) are becoming increasingly popular, with their\napplications now extending beyond just a mode of transportation to serving as\nmobile actuators of a traffic flow to control flow dynamics. This contrasts\nwith traditional fixed-location actuators, such as traffic signals, and is\nreferred to as Lagrangian traffic control. However, designing effective\nLagrangian traffic control policies for AVs that generalize across traffic\nscenarios introduces a major challenge. Real-world traffic environments are\nhighly diverse, and developing policies that perform robustly across such\ndiverse traffic scenarios is challenging. It is further compounded by the joint\ncomplexity of the multi-agent nature of traffic systems, mixed motives among\nparticipants, and conflicting optimization objectives subject to strict\nphysical and external constraints. To address these challenges, we introduce\nMulti-Residual Mixture of Expert Learning (MRMEL), a novel framework for\nLagrangian traffic control that augments a given suboptimal nominal policy with\na learned residual while explicitly accounting for the structure of the traffic\nscenario space. In particular, taking inspiration from residual reinforcement\nlearning, MRMEL augments a suboptimal nominal AV control policy by learning a\nresidual correction, but at the same time dynamically selects the most suitable\nnominal policy from a pool of nominal policies conditioned on the traffic\nscenarios and modeled as a mixture of experts. We validate MRMEL using a case\nstudy in cooperative eco-driving at signalized intersections in Atlanta, Dallas\nFort Worth, and Salt Lake City, with real-world data-driven traffic scenarios.\nThe results show that MRMEL consistently yields superior performance-achieving\nan additional 4%-9% reduction in aggregate vehicle emissions relative to the\nstrongest baseline in each setting.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09836v1", "cate": "cs.RO", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09464", "title": "Unmanned Aerial Vehicle (UAV) Data-Driven Modeling Software with Integrated 9-Axis IMUGPS Sensor Fusion and Data Filtering Algorithm", "authors": ["Azfar Azdi Arfakhsyad", "Aufa Nasywa Rahman", "Larasati Kinanti", "Ahmad Ataka Awwalur Rizqi", "Hannan Nur Muhammad"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      7 pages, 13 figures. Accepted to IEEE ICITEE 2023", "url": "http://arxiv.org/abs/2507.09464v1", "summary": "Unmanned Aerial Vehicles (UAV) have emerged as versatile platforms, driving\nthe demand for accurate modeling to support developmental testing. This paper\nproposes data-driven modeling software for UAV. Emphasizes the utilization of\ncost-effective sensors to obtain orientation and location data subsequently\nprocessed through the application of data filtering algorithms and sensor\nfusion techniques to improve the data quality to make a precise model\nvisualization on the software. UAV's orientation is obtained using processed\nInertial Measurement Unit (IMU) data and represented using Quaternion\nRepresentation to avoid the gimbal lock problem. The UAV's location is\ndetermined by combining data from the Global Positioning System (GPS), which\nprovides stable geographic coordinates but slower data update frequency, and\nthe accelerometer, which has higher data update frequency but integrating it to\nget position data is unstable due to its accumulative error. By combining data\nfrom these two sensors, the software is able to calculate and continuously\nupdate the UAV's real-time position during its flight operations. The result\nshows that the software effectively renders UAV orientation and position with\nhigh degree of accuracy and fluidity", "comment": "7 pages, 13 figures. Accepted to IEEE ICITEE 2023", "pdf_url": "http://arxiv.org/pdf/2507.09464v1", "cate": "cs.RO", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09549", "title": "The Spectacle of Fidelity: Blind Resistance and the Wizardry of Prototyping", "authors": ["Hrittika Bhowmick", "Shilpaa Anand"], "categories": ["cs.HC", "H.5.2; K.4.2; D.2.2"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      3 pages. Submitted for Access InContext Workshop at CHI'25, April 26, 2025, Yokohama, Japan", "url": "http://arxiv.org/abs/2507.09549v1", "summary": "Prototyping is widely regarded in Human-Computer Interaction as an iterative\nprocess through which ideas are tested and refined, often via visual mockups,\nscreen flows, and coded simulations. This position paper critiques the\nvisual-centric norms embedded in prototyping culture by drawing from the lived\nexperiences of blind scholars and insights from cultural disability studies. It\ndiscusses how dominant methods of prototyping rely on an unexamined fidelity to\nsight, privileging what can be rendered visibly coherent while marginalizing\nother modes of knowing and making. By repositioning prototyping as a situated,\nembodied, and relational practice, this paper challenges HCI to rethink what\nkinds of design participation are legitimized and which are excluded when\nprototyping is reduced to screen-based simulations.", "comment": "3 pages. Submitted for Access InContext Workshop at CHI'25, April 26,\n  2025, Yokohama, Japan", "pdf_url": "http://arxiv.org/pdf/2507.09549v1", "cate": "cs.HC", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09369", "title": "A Taxonomy of Omnicidal Futures Involving Artificial Intelligence", "authors": ["Andrew Critch", "Jacob Tsimerman"], "categories": ["cs.AI", "68T01", "I.2.0"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09369v1", "summary": "This report presents a taxonomy and examples of potential omnicidal events\nresulting from AI: scenarios where all or almost all humans are killed. These\nevents are not presented as inevitable, but as possibilities that we can work\nto avoid. Insofar as large institutions require a degree of public support in\norder to take certain actions, we hope that by presenting these possibilities\nin public, we can help to support preventive measures against catastrophic\nrisks from AI.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09369v1", "cate": "cs.AI", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.10421", "title": "SentiDrop: A Multi Modal Machine Learning model for Predicting Dropout in Distance Learning", "authors": ["Meriem Zerkouk", "Miloud Mihoubi", "Belkacem Chikhaoui"], "categories": ["cs.AI", "cs.ET", "cs.IR", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      International Conference on Education and New Learning Technologies (2025)", "url": "http://arxiv.org/abs/2507.10421v1", "summary": "School dropout is a serious problem in distance learning, where early\ndetection is crucial for effective intervention and student perseverance.\nPredicting student dropout using available educational data is a widely\nresearched topic in learning analytics. Our partner's distance learning\nplatform highlights the importance of integrating diverse data sources,\nincluding socio-demographic data, behavioral data, and sentiment analysis, to\naccurately predict dropout risks. In this paper, we introduce a novel model\nthat combines sentiment analysis of student comments using the Bidirectional\nEncoder Representations from Transformers (BERT) model with socio-demographic\nand behavioral data analyzed through Extreme Gradient Boosting (XGBoost). We\nfine-tuned BERT on student comments to capture nuanced sentiments, which were\nthen merged with key features selected using feature importance techniques in\nXGBoost. Our model was tested on unseen data from the next academic year,\nachieving an accuracy of 84\\%, compared to 82\\% for the baseline model.\nAdditionally, the model demonstrated superior performance in other metrics,\nsuch as precision and F1-score. The proposed method could be a vital tool in\ndeveloping personalized strategies to reduce dropout rates and encourage\nstudent perseverance", "comment": "International Conference on Education and New Learning Technologies\n  (2025)", "pdf_url": "http://arxiv.org/pdf/2507.10421v1", "cate": "cs.AI", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.08917", "title": "Detecting Deepfake Talking Heads from Facial Biometric Anomalies", "authors": ["Justin D. Norman", "Hany Farid"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 pages, 3 figures, 3 tables", "url": "http://arxiv.org/abs/2507.08917v1", "summary": "The combination of highly realistic voice cloning, along with visually\ncompelling avatar, face-swap, or lip-sync deepfake video generation, makes it\nrelatively easy to create a video of anyone saying anything. Today, such\ndeepfake impersonations are often used to power frauds, scams, and political\ndisinformation. We propose a novel forensic machine learning technique for the\ndetection of deepfake video impersonations that leverages unnatural patterns in\nfacial biometrics. We evaluate this technique across a large dataset of\ndeepfake techniques and impersonations, as well as assess its reliability to\nvideo laundering and its generalization to previously unseen video deepfake\ngenerators.", "comment": "10 pages, 3 figures, 3 tables", "pdf_url": "http://arxiv.org/pdf/2507.08917v1", "cate": "cs.CV", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.08943", "title": "Choosing the Right Git Workflow: A Comparative Analysis of Trunk-based vs. Branch-based Approaches", "authors": ["Pedro Lopes", "Paola Accioly", "Paulo Borba", "Vitor Menezes"], "categories": ["cs.SE", "D.2.7"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      11 pages with 3 figures", "url": "http://arxiv.org/abs/2507.08943v1", "summary": "Git has become one of the most widely used version control systems today.\nAmong its distinguishing features, its ability to easily and quickly create\nbranches stands out, allowing teams to customize their workflows. In this\ncontext, various formats of collaborative development workflows using Git have\nemerged and gained popularity among software engineers. We can categorize such\nworkflows into two main types: branch-based workflows and trunk-based\nworkflows. Branch-based workflows typically define a set of remote branches\nwith well-defined objectives, such as feature branches, a branch for feature\nintegration, and a main branch. The goal is to migrate changes from the most\nisolated branch to the main one shared by all as the code matures. In this\ncategory, GitFlow stands out as the most popular example. In contrast,\ntrunk-based workflows have a single remote branch where developers integrate\ntheir changes directly. In this range of options, choosing a workflow that\nmaximizes team productivity while promoting software quality becomes a\nnon-trivial task. Despite discussions on forums, social networks, and blogs,\nfew scientific articles have explored this topic. In this work, we provide\nevidence on how Brazilian developers work with Git workflows and what factors\nfavor or hinder the use of each model. To this end, we conducted\nsemi-structured interviews and a survey with software developers. Our results\nindicate that trunk-based development favors fast-paced projects with\nexperienced and smaller teams, while branch-based development suits less\nexperienced and larger teams better, despite posing management challenges.", "comment": "11 pages with 3 figures", "pdf_url": "http://arxiv.org/pdf/2507.08943v1", "cate": "cs.SE", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09407", "title": "LLM-Stackelberg Games: Conjectural Reasoning Equilibria and Their Applications to Spearphishing", "authors": ["Quanyan Zhu"], "categories": ["cs.AI", "cs.CR", "cs.GT"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09407v1", "summary": "We introduce the framework of LLM-Stackelberg games, a class of sequential\ndecision-making models that integrate large language models (LLMs) into\nstrategic interactions between a leader and a follower. Departing from\nclassical Stackelberg assumptions of complete information and rational agents,\nour formulation allows each agent to reason through structured prompts,\ngenerate probabilistic behaviors via LLMs, and adapt their strategies through\ninternal cognition and belief updates. We define two equilibrium concepts:\nreasoning and behavioral equilibrium, which aligns an agent's internal\nprompt-based reasoning with observable behavior, and conjectural reasoning\nequilibrium, which accounts for epistemic uncertainty through parameterized\nmodels over an opponent's response. These layered constructs capture bounded\nrationality, asymmetric information, and meta-cognitive adaptation. We\nillustrate the framework through a spearphishing case study, where a sender and\na recipient engage in a deception game using structured reasoning prompts. This\nexample highlights the cognitive richness and adversarial potential of\nLLM-mediated interactions. Our results show that LLM-Stackelberg games provide\na powerful paradigm for modeling decision-making in domains such as\ncybersecurity, misinformation, and recommendation systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09407v1", "cate": "cs.AI", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09989", "title": "Improving monotonic optimization in heterogeneous multi-agent reinforcement learning with optimal marginal deterministic policy gradient", "authors": ["Xiaoyang Yu", "Youfang Lin", "Shuo Wang", "Sheng Han"], "categories": ["cs.AI", "cs.MA"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09989v1", "summary": "In heterogeneous multi-agent reinforcement learning (MARL), achieving\nmonotonic improvement plays a pivotal role in enhancing performance. The HAPPO\nalgorithm proposes a feasible solution by introducing a sequential update\nscheme, which requires independent learning with No Parameter-sharing (NoPS).\nHowever, heterogeneous MARL generally requires Partial Parameter-sharing\n(ParPS) based on agent grouping to achieve high cooperative performance. Our\nexperiments prove that directly combining ParPS with the sequential update\nscheme leads to the policy updating baseline drift problem, thereby failing to\nachieve improvement. To solve the conflict between monotonic improvement and\nParPS, we propose the Optimal Marginal Deterministic Policy Gradient (OMDPG)\nalgorithm. First, we replace the sequentially computed $Q_{\\psi}^s(s,a_{1:i})$\nwith the Optimal Marginal Q (OMQ) function $\\phi_{\\psi}^*(s,a_{1:i})$ derived\nfrom Q-functions. This maintains MAAD's monotonic improvement while eliminating\nthe conflict through optimal joint action sequences instead of sequential\npolicy ratio calculations. Second, we introduce the Generalized Q Critic (GQC)\nas the critic function, employing pessimistic uncertainty-constrained loss to\noptimize different Q-value estimations. This provides the required Q-values for\nOMQ computation and stable baselines for actor updates. Finally, we implement a\nCentralized Critic Grouped Actor (CCGA) architecture that simultaneously\nachieves ParPS in local policy networks and accurate global Q-function\ncomputation. Experimental results in SMAC and MAMuJoCo environments demonstrate\nthat OMDPG outperforms various state-of-the-art MARL baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09989v1", "cate": "cs.AI", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09469", "title": "mmE-Loc: Facilitating Accurate Drone Landing with Ultra-High-Frequency Localization", "authors": ["Haoyang Wang", "Jingao Xu", "Xinyu Luo", "Ting Zhang", "Xuecheng Chen", "Ruiyang Duan", "Jialong Chen", "Yunhao Liu", "Jianfeng Zheng", "Weijie Hong", "Xinlei Chen"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      17 pages, 34 figures. arXiv admin note: substantial text overlap with arXiv:2502.14992", "url": "http://arxiv.org/abs/2507.09469v1", "summary": "For precise, efficient, and safe drone landings, ground platforms should\nreal-time, accurately locate descending drones and guide them to designated\nspots. While mmWave sensing combined with cameras improves localization\naccuracy, lower sampling frequency of traditional frame cameras compared to\nmmWave radar creates bottlenecks in system throughput. In this work, we upgrade\ntraditional frame camera with event camera, a novel sensor that harmonizes in\nsampling frequency with mmWave radar within ground platform setup, and\nintroduce mmE-Loc, a high-precision, low-latency ground localization system\ndesigned for precise drone landings. To fully exploit the \\textit{temporal\nconsistency} and \\textit{spatial complementarity} between these two modalities,\nwe propose two innovative modules: \\textit{(i)} the Consistency-instructed\nCollaborative Tracking module, which further leverages the drone's physical\nknowledge of periodic micro-motions and structure for accurate measurements\nextraction, and \\textit{(ii)} the Graph-informed Adaptive Joint Optimization\nmodule, which integrates drone motion information for efficient sensor fusion\nand drone localization. Real-world experiments conducted in landing scenarios\nwith a drone delivery company demonstrate that mmE-Loc significantly\noutperforms state-of-the-art methods in both accuracy and latency.", "comment": "17 pages, 34 figures. arXiv admin note: substantial text overlap with\n  arXiv:2502.14992", "pdf_url": "http://arxiv.org/pdf/2507.09469v1", "cate": "cs.RO", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09664", "title": "SimStep: Chain-of-Abstractions for Incremental Specification and Debugging of AI-Generated Interactive Simulations", "authors": ["Zoe Kaputa", "Anika Rajaram", "Vryan Almanon Feliciano", "Zhuoyue Lyu", "Maneesh Agrawala", "Hari Subramonyam"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09664v1", "summary": "Programming-by-prompting with generative AI offers a new paradigm for\nend-user programming, shifting the focus from syntactic fluency to semantic\nintent. This shift holds particular promise for non-programmers such as\neducators, who can describe instructional goals in natural language to generate\ninteractive learning content. Yet in bypassing direct code authoring, many of\nprogramming's core affordances - such as traceability, stepwise refinement, and\nbehavioral testing - are lost. We propose the Chain-of-Abstractions (CoA)\nframework as a way to recover these affordances while preserving the expressive\nflexibility of natural language. CoA decomposes the synthesis process into a\nsequence of cognitively meaningful, task-aligned representations that function\nas checkpoints for specification, inspection, and refinement. We instantiate\nthis approach in SimStep, an authoring environment for teachers that scaffolds\nsimulation creation through four intermediate abstractions: Concept Graph,\nScenario Graph, Learning Goal Graph, and UI Interaction Graph. To address\nambiguities and misalignments, SimStep includes an inverse correction process\nthat surfaces in-filled model assumptions and enables targeted revision without\nrequiring users to manipulate code. Evaluations with educators show that CoA\nenables greater authoring control and interpretability in\nprogramming-by-prompting workflows.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09664v1", "cate": "cs.HC", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09374", "title": "EduFlow: Advancing MLLMs' Problem-Solving Proficiency through Multi-Stage, Multi-Perspective Critique", "authors": ["Chenglin Zhu", "Tao Zhang", "Chong Li", "Mingan Lin", "Zenan Zhou", "Jian Xie"], "categories": ["cs.AI", "I.2.6; I.2.10"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      14 pages,4 figures", "url": "http://arxiv.org/abs/2507.09374v1", "summary": "Multimodal large language models (MLLMs) still perform poorly on scientific\ntasks, particularly those requiring multi-step and interpretable reasoning.\nTheir limitations include insufficient scientific reasoning patterns, lack of\nglobal coherence in multi-step inference, and the absence of reflective\nself-correction, making them unreliable in structured scientific contexts. We\nintroduce EduFlow, the first end-to-end framework that covers the full pipeline\nof educational scientific reasoning, including data selection, MCTS-based\ntrajectory construction, model training, and output optimization. At its core\nis EduPRM, a process-aware reward model that critiques reasoning steps with\ntags and justifications. EduPRM is trained via curriculum learning on three\ncomplementary supervision sources: MCTS-guided trajectories, error-injected\ncritiques, and teacher-student dialogues, enabling dynamic adaptation to\nmulti-stage problem solving and iterative refinement during inference. We\nfurther propose EduMCTS, a domain-adapted search framework that introduces\nbootstrapping actions specifically designed for educational reasoning, such as\na self-reflection mechanism that promotes reflective error correction. It\nfurther leverages EduPRM's fine-grained feedback to guide the search toward\nhigher-quality reasoning trajectories. By applying self-consistency and\nrejection sampling, we constructed EduMCTS-160K, a large-scale dataset of\neducational reasoning trajectories. Extensive experiments demonstrate that\nEduFlow enhances reasoning consistency and coherence. Code, data, and models\nwill be released.", "comment": "14 pages,4 figures", "pdf_url": "http://arxiv.org/pdf/2507.09374v1", "cate": "cs.AI", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2501.10702", "title": "An RRAM compute-in-memory architecture for high energy-efficient processing of binary matrix-vector multiplication in cryptography", "authors": ["Hao Yue", "Yihao Chen", "Tianhang Liang", "Xiangrui Li", "Xin Kong", "Zhelong Jiang", "Zhigang Li", "Gang Chen", "Huaxiang Lu"], "categories": ["cs.ET"], "primary_category": "Subjects:       Emerging Technologies (cs.ET)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.10702v5", "summary": "Binary matrix-vector multiplication (BMVM) is a key operation in post-quantum\ncryptography schemes like the Classic McEliece cryptosystem. Conventional\ncomputing architectures incur significant energy efficiency loss due to data\nmovement of large matrices when handling such tasks. Resistive memory (RRAM)\nnon-volatile compute-in-memory (nvCIM) is an ideal technology for high\nenergy-efficient BMVM processing but faces challenges, including signal margin\ndegradation in high input-parallelism arrays due to device non-idealities and\nhigh hardware overhead from current readout and XOR operations. This work\npresents a RRAM nvCIM architecture featuring: 1) 1T1R cells with\nhigh-resistive-state compensation modules; and 2) pulsed current-sensing parity\ncheckers. Based on the 180nm process and test results from RRAM devices, the\ncomputing accuracy and efficiency of the architecture are verified by\nsimulation. The proposed architecture performs high-precision current\naccumulation with a maximum MAC value of 10 and achieves an energy efficiency\nof 1.51TOPS/W, offering approximately 1.62 times improvement compared to an\nadvanced 28nm FPGA platform.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.10702v5", "cate": "cs.ET", "date": "2025-01-18", "updated": "2025-07-12"}
{"id": "2507.08979", "title": "PRISM: Reducing Spurious Implicit Biases in Vision-Language Models with LLM-Guided Embedding Projection", "authors": ["Mahdiyar Molahasani", "Azadeh Motamedi", "Michael Greenspan", "Il-Min Kim", "Ali Etemad"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2507.08979v1", "summary": "We introduce Projection-based Reduction of Implicit Spurious bias in\nvision-language Models (PRISM), a new data-free and task-agnostic solution for\nbias mitigation in VLMs like CLIP. VLMs often inherit and amplify biases in\ntheir training data, leading to skewed predictions. PRISM is designed to debias\nVLMs without relying on predefined bias categories or additional external data.\nIt operates in two stages: first, an LLM is prompted with simple class prompts\nto generate scene descriptions that contain spurious correlations. Next, PRISM\nuses our novel contrastive-style debiasing loss to learn a projection that maps\nthe embeddings onto a latent space that minimizes spurious correlations while\npreserving the alignment between image and text embeddings.Extensive\nexperiments demonstrate that PRISM outperforms current debiasing methods on the\ncommonly used Waterbirds and CelebA datasets We make our code public at:\nhttps://github.com/MahdiyarMM/PRISM.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.08979v1", "cate": "cs.CV", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.08992", "title": "Semantic Source Code Segmentation using Small and Large Language Models", "authors": ["Abdelhalim Dahou", "Ansgar Scherp", "Sebastian Kurten", "Brigitte Mathiak", "Madhu Chauhan"], "categories": ["cs.SE", "cs.CL", "cs.PL"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      18 pages, 4 figures", "url": "http://arxiv.org/abs/2507.08992v1", "summary": "Source code segmentation, dividing code into functionally coherent segments,\nis crucial for knowledge retrieval and maintenance in software development.\nWhile enabling efficient navigation and comprehension of large codebases,\nmanual and syntactic analysis approaches have become impractical as\nrepositories grow, especially for low-resource languages like R and their\nresearch domains (e.g., social sciences, psychology).This paper introduces an\nautomated, domain-specific approach for research R code segmentation using\nLarge and Small Language Models (LLMs/SLMs). It presents two novel approaches\nand a human-annotated dataset, StatCodeSeg. We explore two distinct approaches:\nline-by-line analysis with context and range-based segment determination. We\nexperiment with LLMs and fine-tuned SLMs. To support the generalizability of\nour approaches, we also include experiments on Python code from the computer\nscience domain.Our results show that context-based line-by-line analysis is\nsuperior over range-based segmentation.Using smaller language models like\nCodeBERT and an encoder-only version of CodeT5+ are better than their LLM\ncounterparts. Most notably, these two best-performing models did not see R code\nduring pre-training versus the LLMs but were only fine-tuned on 4,130 lines of\nmanually annotated code.", "comment": "18 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.08992v1", "cate": "cs.SE", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09055", "title": "Analysing Health Misinformation with Advanced Centrality Metrics in Online Social Networks", "authors": ["Mkululi Sikosana", "Sean Maudsley-Barton", "Oluwaseun Ajao"], "categories": ["cs.SI", "cs.AI", "cs.IR", "H.3.3"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "Comments:      10 Pages, 2 figures, 3 tables, journal article in PLOS Digital Health (2025)", "url": "http://arxiv.org/abs/2507.09055v1", "summary": "The rapid spread of health misinformation on online social networks (OSNs)\nduring global crises such as the COVID-19 pandemic poses challenges to public\nhealth, social stability, and institutional trust. Centrality metrics have long\nbeen pivotal in understanding the dynamics of information flow, particularly in\nthe context of health misinformation. However, the increasing complexity and\ndynamism of online networks, especially during crises, highlight the\nlimitations of these traditional approaches. This study introduces and compares\nthree novel centrality metrics: dynamic influence centrality (DIC), health\nmisinformation vulnerability centrality (MVC), and propagation centrality (PC).\nThese metrics incorporate temporal dynamics, susceptibility, and multilayered\nnetwork interactions. Using the FibVID dataset, we compared traditional and\nnovel metrics to identify influential nodes, propagation pathways, and\nmisinformation influencers. Traditional metrics identified 29 influential\nnodes, while the new metrics uncovered 24 unique nodes, resulting in 42\ncombined nodes, an increase of 44.83%. Baseline interventions reduced health\nmisinformation by 50%, while incorporating the new metrics increased this to\n62.5%, an improvement of 25%. To evaluate the broader applicability of the\nproposed metrics, we validated our framework on a second dataset, Monant\nMedical Misinformation, which covers a diverse range of health misinformation\ndiscussions beyond COVID-19. The results confirmed that the advanced metrics\ngeneralised successfully, identifying distinct influential actors not captured\nby traditional methods. In general, the findings suggest that a combination of\ntraditional and novel centrality measures offers a more robust and\ngeneralisable framework for understanding and mitigating the spread of health\nmisinformation in different online network contexts.", "comment": "10 Pages, 2 figures, 3 tables, journal article in PLOS Digital Health\n  (2025)", "pdf_url": "http://arxiv.org/pdf/2507.09055v1", "cate": "cs.SI", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09857", "title": "AdvGrasp: Adversarial Attacks on Robotic Grasping from a Physical Perspective", "authors": ["Xiaofei Wang", "Mingliang Han", "Tianyu Hao", "Cegang Li", "Yunbo Zhao", "Keke Tang"], "categories": ["cs.RO", "cs.CR"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      IJCAI'2025", "url": "http://arxiv.org/abs/2507.09857v1", "summary": "Adversarial attacks on robotic grasping provide valuable insights into\nevaluating and improving the robustness of these systems. Unlike studies that\nfocus solely on neural network predictions while overlooking the physical\nprinciples of grasping, this paper introduces AdvGrasp, a framework for\nadversarial attacks on robotic grasping from a physical perspective.\nSpecifically, AdvGrasp targets two core aspects: lift capability, which\nevaluates the ability to lift objects against gravity, and grasp stability,\nwhich assesses resistance to external disturbances. By deforming the object's\nshape to increase gravitational torque and reduce stability margin in the\nwrench space, our method systematically degrades these two key grasping\nmetrics, generating adversarial objects that compromise grasp performance.\nExtensive experiments across diverse scenarios validate the effectiveness of\nAdvGrasp, while real-world validations demonstrate its robustness and practical\napplicability", "comment": "IJCAI'2025", "pdf_url": "http://arxiv.org/pdf/2507.09857v1", "cate": "cs.RO", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10142", "title": "Adaptability in Multi-Agent Reinforcement Learning: A Framework and Unified Review", "authors": ["Siyi Hu", "Mohamad A Hady", "Jianglin Qiao", "Jimmy Cao", "Mahardhika Pratama", "Ryszard Kowalczyk"], "categories": ["cs.AI", "cs.LG", "cs.MA"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10142v1", "summary": "Multi-Agent Reinforcement Learning (MARL) has shown clear effectiveness in\ncoordinating multiple agents across simulated benchmarks and constrained\nscenarios. However, its deployment in real-world multi-agent systems (MAS)\nremains limited, primarily due to the complex and dynamic nature of such\nenvironments. These challenges arise from multiple interacting sources of\nvariability, including fluctuating agent populations, evolving task goals, and\ninconsistent execution conditions. Together, these factors demand that MARL\nalgorithms remain effective under continuously changing system configurations\nand operational demands. To better capture and assess this capacity for\nadjustment, we introduce the concept of \\textit{adaptability} as a unified and\npractically grounded lens through which to evaluate the reliability of MARL\nalgorithms under shifting conditions, broadly referring to any changes in the\nenvironment dynamics that may occur during learning or execution. Centred on\nthe notion of adaptability, we propose a structured framework comprising three\nkey dimensions: learning adaptability, policy adaptability, and scenario-driven\nadaptability. By adopting this adaptability perspective, we aim to support more\nprincipled assessments of MARL performance beyond narrowly defined benchmarks.\nUltimately, this survey contributes to the development of algorithms that are\nbetter suited for deployment in dynamic, real-world multi-agent systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10142v1", "cate": "cs.AI", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09505", "title": "TruckV2X: A Truck-Centered Perception Dataset", "authors": ["Tenghui Xie", "Zhiying Song", "Fuxi Wen", "Jun Li", "Guangzhao Liu", "Zijian Zhao"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09505v1", "summary": "Autonomous trucking offers significant benefits, such as improved safety and\nreduced costs, but faces unique perception challenges due to trucks' large size\nand dynamic trailer movements. These challenges include extensive blind spots\nand occlusions that hinder the truck's perception and the capabilities of other\nroad users. To address these limitations, cooperative perception emerges as a\npromising solution. However, existing datasets predominantly feature light\nvehicle interactions or lack multi-agent configurations for heavy-duty vehicle\nscenarios. To bridge this gap, we introduce TruckV2X, the first large-scale\ntruck-centered cooperative perception dataset featuring multi-modal sensing\n(LiDAR and cameras) and multi-agent cooperation (tractors, trailers, CAVs, and\nRSUs). We further investigate how trucks influence collaborative perception\nneeds, establishing performance benchmarks while suggesting research priorities\nfor heavy vehicle perception. The dataset provides a foundation for developing\ncooperative perception systems with enhanced occlusion handling capabilities,\nand accelerates the deployment of multi-agent autonomous trucking systems. The\nTruckV2X dataset is available at\nhttps://huggingface.co/datasets/XieTenghu1/TruckV2X.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09505v1", "cate": "cs.RO", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09917", "title": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series", "authors": ["Zikun Deng", "Jiabao Huang", "Chenxi Ruan", "Jialing Li", "Shaowu Gao", "Yi Cai"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09917v1", "summary": "Spatial time series visualization offers scientific research pathways and\nanalytical decision-making tools across various spatiotemporal domains. Despite\nmany advanced methodologies, the seamless integration of temporal and spatial\ninformation remains a challenge. The space-time cube (STC) stands out as a\npromising approach for the synergistic presentation of spatial and temporal\ninformation, with successful applications across various spatiotemporal\ndatasets. However, the STC is plagued by well-known issues such as visual\nocclusion and depth ambiguity, which are further exacerbated when dealing with\nlarge-scale spatial time series data. In this study, we introduce a novel\ntechnical framework termed VolumeSTCube, designed for continuous spatiotemporal\nphenomena. It first leverages the concept of the STC to transform discretely\ndistributed spatial time series data into continuously volumetric data.\nSubsequently, volume rendering and surface rendering techniques are employed to\nvisualize the transformed volumetric data. Volume rendering is utilized to\nmitigate visual occlusion, while surface rendering provides pattern details by\nenhanced lighting information. Lastly, we design interactions to facilitate the\nexploration and analysis from temporal, spatial, and spatiotemporal\nperspectives. VolumeSTCube is evaluated through a computational experiment, a\nreal-world case study with one expert, and a controlled user study with twelve\nnon-experts, compared against a baseline from prior work, showing its\nsuperiority and effectiveness in largescale spatial time series analysis.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09917v1", "cate": "cs.HC", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09389", "title": "Knowledge Conceptualization Impacts RAG Efficacy", "authors": ["Chris Davis Jaldi", "Anmol Saini", "Elham Ghiasi", "O. Divine Eziolise", "Cogan Shimizu"], "categories": ["cs.AI", "cs.CY", "cs.IR"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09389v1", "summary": "Explainability and interpretability are cornerstones of frontier and\nnext-generation artificial intelligence (AI) systems. This is especially true\nin recent systems, such as large language models (LLMs), and more broadly,\ngenerative AI. On the other hand, adaptability to new domains, contexts, or\nscenarios is also an important aspect for a successful system. As such, we are\nparticularly interested in how we can merge these two efforts, that is,\ninvestigating the design of transferable and interpretable neurosymbolic AI\nsystems. Specifically, we focus on a class of systems referred to as ''Agentic\nRetrieval-Augmented Generation'' systems, which actively select, interpret, and\nquery knowledge sources in response to natural language prompts. In this paper,\nwe systematically evaluate how different conceptualizations and representations\nof knowledge, particularly the structure and complexity, impact an AI agent (in\nthis case, an LLM) in effectively querying a triplestore. We report our\nresults, which show that there are impacts from both approaches, and we discuss\ntheir impact and implications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09389v1", "cate": "cs.AI", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2505.14829", "title": "Modulation of switching dynamics in magnetic tunnel junctions for low-error-rate computational random-access memory", "authors": ["Yang Lv", "Brahmdutta Dixit", "Jian-Ping Wang"], "categories": ["cs.ET"], "primary_category": "Subjects:       Emerging Technologies (cs.ET)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.14829v2", "summary": "The conventional computer architecture has been facing challenges answering\nthe ever-increasing demands from emerging applications, such as AI, for\nenergy-efficient computation and memory hardware systems. Computational Random\nAccess Memory (CRAM) represents a true in-memory computing paradigm that\nintegrates logic and memory functions within the same array. At its core, CRAM\nrelies on Magnetic Tunnel Junctions (MTJs), which serve as the foundational\nbuilding blocks for implementing both memory storage and logic operations.\nHowever, a key challenge in CRAM lies in the non-ideal error rates associated\nwith switching dynamics of MTJs, necessitating innovative approaches to reduce\nerrors and optimize logic margins. This work proposes a novel approach of\nutilizing the voltage-controlled magnetic anisotropy (VCMA) to steepen the\nswitching probability transfer curve (SPTC), thereby significantly reducing the\nlogic operation error rate in CRAM. Using several numerical modeling tools, we\nvalidate the effectiveness of VCMA in modulating the energy barrier and\nswitching dynamics in MTJs. It is revealed that the VCMA effect significantly\nreduces the error rate of CRAM by 61.43% at a VCMA coefficient of 200 fJ/V/m\ncompared to CRAM without VCMA. The reduction of error rate is further rapidly\namplified with an increasing TMR ratio. Furthermore, the introduction of the\nVCMA effect decreases the logic voltage (Vlogic) required for logic operations\nin CRAM and results in reduction of energy consumption. Our work serves as a\nfirst exploration in reducing the error rate in CRAM by modifying SPTC in MTJs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.14829v2", "cate": "cs.ET", "date": "2025-05-20", "updated": "2025-07-14"}
{"id": "2507.08981", "title": "Video Inference for Human Mesh Recovery with Vision Transformer", "authors": ["Hanbyel Cho", "Jaesung Ahn", "Yooshin Cho", "Junmo Kim"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to IEEE FG 2023", "url": "http://arxiv.org/abs/2507.08981v1", "summary": "Human Mesh Recovery (HMR) from an image is a challenging problem because of\nthe inherent ambiguity of the task. Existing HMR methods utilized either\ntemporal information or kinematic relationships to achieve higher accuracy, but\nthere is no method using both. Hence, we propose \"Video Inference for Human\nMesh Recovery with Vision Transformer (HMR-ViT)\" that can take into account\nboth temporal and kinematic information. In HMR-ViT, a Temporal-kinematic\nFeature Image is constructed using feature vectors obtained from video frames\nby an image encoder. When generating the feature image, we use a Channel\nRearranging Matrix (CRM) so that similar kinematic features could be located\nspatially close together. The feature image is then further encoded using\nVision Transformer, and the SMPL pose and shape parameters are finally inferred\nusing a regression network. Extensive evaluation on the 3DPW and Human3.6M\ndatasets indicates that our method achieves a competitive performance in HMR.", "comment": "Accepted to IEEE FG 2023", "pdf_url": "http://arxiv.org/pdf/2507.08981v1", "cate": "cs.CV", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09039", "title": "Towards Extracting Software Requirements from App Reviews using Seq2seq Framework", "authors": ["Aakash Sorathiya", "Gouri Ginde"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09039v1", "summary": "Mobile app reviews are a large-scale data source for software improvements. A\nkey task in this context is effectively extracting requirements from app\nreviews to analyze the users' needs and support the software's evolution.\nRecent studies show that existing methods fail at this task since app reviews\nusually contain informal language, grammatical and spelling errors, and a large\namount of irrelevant information that might not have direct practical value for\ndevelopers. To address this, we propose a novel reformulation of requirements\nextraction as a Named Entity Recognition (NER) task based on the\nsequence-to-sequence (Seq2seq) generation approach. With this aim, we propose a\nSeq2seq framework, incorporating a BiLSTM encoder and an LSTM decoder, enhanced\nwith a self-attention mechanism, GloVe embeddings, and a CRF model. We\nevaluated our framework on two datasets: a manually annotated set of 1,000\nreviews (Dataset 1) and a crowdsourced set of 23,816 reviews (Dataset 2). The\nquantitative evaluation of our framework showed that it outperformed existing\nstate-of-the-art methods with an F1 score of 0.96 on Dataset 2, and achieved\ncomparable performance on Dataset 1 with an F1 score of 0.47.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09039v1", "cate": "cs.SE", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09149", "title": "Advanced Health Misinformation Detection Through Hybrid CNN-LSTM Models Informed by the Elaboration Likelihood Model (ELM)", "authors": ["Mkululi Sikosana", "Sean Maudsley-Barton", "Oluwaseun Ajao"], "categories": ["cs.SI", "cs.AI", "cs.CY", "cs.LG", "I.2.7; J.4"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "Comments:      11 Pages, 2 Figures, 3 Tables conference paper to appear in proceedings of International Conference on Artificial Intelligence, Computer, Data Sciences and Applications (ACDSA'25)", "url": "http://arxiv.org/abs/2507.09149v1", "summary": "Health misinformation during the COVID-19 pandemic has significantly\nchallenged public health efforts globally. This study applies the Elaboration\nLikelihood Model (ELM) to enhance misinformation detection on social media\nusing a hybrid Convolutional Neural Network (CNN) and Long Short-Term Memory\n(LSTM) model. The model aims to enhance the detection accuracy and reliability\nof misinformation classification by integrating ELM-based features such as text\nreadability, sentiment polarity, and heuristic cues (e.g., punctuation\nfrequency). The enhanced model achieved an accuracy of 97.37%, precision of\n96.88%, recall of 98.50%, F1-score of 97.41%, and ROC-AUC of 99.50%. A combined\nmodel incorporating feature engineering further improved performance, achieving\na precision of 98.88%, recall of 99.80%, F1-score of 99.41%, and ROC-AUC of\n99.80%. These findings highlight the value of ELM features in improving\ndetection performance, offering valuable contextual information. This study\ndemonstrates the practical application of psychological theories in developing\nadvanced machine learning algorithms to address health misinformation\neffectively.", "comment": "11 Pages, 2 Figures, 3 Tables conference paper to appear in\n  proceedings of International Conference on Artificial Intelligence, Computer,\n  Data Sciences and Applications (ACDSA'25)", "pdf_url": "http://arxiv.org/pdf/2507.09149v1", "cate": "cs.SI", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.10103", "title": "Accelerating Automatic Program Repair with Dual Retrieval-Augmented Fine-Tuning and Patch Generation on Large Language Models", "authors": ["Hanyang Guo", "Xiaoheng Xie", "Hong-Ning Dai", "Peng Di", "Yu Zhang", "Bishenghui Tao", "Zibin Zheng"], "categories": ["cs.SE", "cs.CR"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10103v1", "summary": "Automated Program Repair (APR) is essential for ensuring software reliability\nand quality while enhancing efficiency and reducing developers' workload.\nAlthough rule-based and learning-based APR methods have demonstrated their\neffectiveness, their performance was constrained by the defect type of repair,\nthe quality of training data, and the size of model parameters. Recently, Large\nLanguage Models (LLMs) combined with Retrieval-Augmented-Generation (RAG) have\nbeen increasingly adopted in APR tasks. However, current code LLMs and RAG\ndesigns neither fully address code repair tasks nor consider code-specific\nfeatures. To overcome these limitations, we propose SelRepair, a novel APR\napproach with integration of a fine-tuned LLM with a newly-designed dual RAG\nmodule. This approach uses a bug-fix pair dataset for fine-tuning and\nincorporates semantic and syntactic/structural similarity information through\nan RAG selection gate. This design ensures relevant information is retrieved\nefficiently, thereby reducing token length and inference time. Evaluations on\nJava datasets show SelRepair outperforms other APR methods, achieving 26.29%\nand 17.64% in terms of exact match (EM) on different datasets while reducing\ninference time by at least 6.42% with controlled input lengths.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10103v1", "cate": "cs.SE", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10284", "title": "Prompt Informed Reinforcement Learning for Visual Coverage Path Planning", "authors": ["Venkat Margapuri"], "categories": ["cs.RO", "cs.MA"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10284v1", "summary": "Visual coverage path planning with unmanned aerial vehicles (UAVs) requires\nagents to strategically coordinate UAV motion and camera control to maximize\ncoverage, minimize redundancy, and maintain battery efficiency. Traditional\nreinforcement learning (RL) methods rely on environment-specific reward\nformulations that lack semantic adaptability. This study proposes\nPrompt-Informed Reinforcement Learning (PIRL), a novel approach that integrates\nthe zero-shot reasoning ability and in-context learning capability of large\nlanguage models with curiosity-driven RL. PIRL leverages semantic feedback from\nan LLM, GPT-3.5, to dynamically shape the reward function of the Proximal\nPolicy Optimization (PPO) RL policy guiding the agent in position and camera\nadjustments for optimal visual coverage. The PIRL agent is trained using OpenAI\nGym and evaluated in various environments. Furthermore, the sim-to-real-like\nability and zero-shot generalization of the agent are tested by operating the\nagent in Webots simulator which introduces realistic physical dynamics. Results\nshow that PIRL outperforms multiple learning-based baselines such as PPO with\nstatic rewards, PPO with exploratory weight initialization, imitation learning,\nand an LLM-only controller. Across different environments, PIRL outperforms the\nbest-performing baseline by achieving up to 14% higher visual coverage in\nOpenAI Gym and 27% higher in Webots, up to 25% higher battery efficiency, and\nup to 18\\% lower redundancy, depending on the environment. The results\nhighlight the effectiveness of LLM-guided reward shaping in complex spatial\nexploration tasks and suggest a promising direction for integrating natural\nlanguage priors into RL for robotics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10284v1", "cate": "cs.RO", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09537", "title": "Self-supervised Pretraining for Integrated Prediction and Planning of Automated Vehicles", "authors": ["Yangang Ren", "Guojian Zhan", "Chen Lv", "Jun Li", "Fenghua Liang", "Keqiang Li"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09537v1", "summary": "Predicting the future of surrounding agents and accordingly planning a safe,\ngoal-directed trajectory are crucial for automated vehicles. Current methods\ntypically rely on imitation learning to optimize metrics against the ground\ntruth, often overlooking how scene understanding could enable more holistic\ntrajectories. In this paper, we propose Plan-MAE, a unified pretraining\nframework for prediction and planning that capitalizes on masked autoencoders.\nPlan-MAE fuses critical contextual understanding via three dedicated tasks:\nreconstructing masked road networks to learn spatial correlations, agent\ntrajectories to model social interactions, and navigation routes to capture\ndestination intents. To further align vehicle dynamics and safety constraints,\nwe incorporate a local sub-planning task predicting the ego-vehicle's near-term\ntrajectory segment conditioned on earlier segment. This pretrained model is\nsubsequently fine-tuned on downstream tasks to jointly generate the prediction\nand planning trajectories. Experiments on large-scale datasets demonstrate that\nPlan-MAE outperforms current methods on the planning metrics by a large margin\nand can serve as an important pre-training step for learning-based motion\nplanner.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09537v1", "cate": "cs.RO", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09959", "title": "Branch Explorer: Leveraging Branching Narratives to Support Interactive 360° Video Viewing for Blind and Low Vision Users", "authors": ["Shuchang Xu", "Xiaofu Jin", "Wenshuo Zhang", "Huamin Qu", "Yukang Yan"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09959v1", "summary": "360{\\deg} videos enable users to freely choose their viewing paths, but blind\nand low vision (BLV) users are often excluded from this interactive experience.\nTo bridge this gap, we present Branch Explorer, a system that transforms\n360{\\deg} videos into branching narratives -- stories that dynamically unfold\nbased on viewer choices -- to support interactive viewing for BLV audiences.\nOur formative study identified three key considerations for accessible\nbranching narratives: providing diverse branch options, ensuring coherent story\nprogression, and enabling immersive navigation among branches. To address these\nneeds, Branch Explorer employs a multi-modal machine learning pipeline to\ngenerate diverse narrative paths, allowing users to flexibly make choices at\ndetected branching points and seamlessly engage with each storyline through\nimmersive audio guidance. Evaluation with 12 BLV viewers showed that Branch\nExplorer significantly enhanced user agency and engagement in 360{\\deg} video\nviewing. Users also developed personalized strategies for exploring 360{\\deg}\ncontent. We further highlight implications for supporting accessible\nexploration of videos and virtual environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09959v1", "cate": "cs.HC", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09534", "title": "Consistency Trajectory Planning: High-Quality and Efficient Trajectory Optimization for Offline Model-Based Reinforcement Learning", "authors": ["Guanquan Wang", "Takuya Hiraoka", "Yoshimasa Tsuruoka"], "categories": ["cs.AI", "cs.LG", "cs.RO"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09534v1", "summary": "This paper introduces Consistency Trajectory Planning (CTP), a novel offline\nmodel-based reinforcement learning method that leverages the recently proposed\nConsistency Trajectory Model (CTM) for efficient trajectory optimization. While\nprior work applying diffusion models to planning has demonstrated strong\nperformance, it often suffers from high computational costs due to iterative\nsampling procedures. CTP supports fast, single-step trajectory generation\nwithout significant degradation in policy quality. We evaluate CTP on the D4RL\nbenchmark and show that it consistently outperforms existing diffusion-based\nplanning methods in long-horizon, goal-conditioned tasks. Notably, CTP achieves\nhigher normalized returns while using significantly fewer denoising steps. In\nparticular, CTP achieves comparable performance with over $120\\times$ speedup\nin inference time, demonstrating its practicality and effectiveness for\nhigh-performance, low-latency offline planning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09534v1", "cate": "cs.AI", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2504.09391", "title": "Survival of the Optimized: An Evolutionary Approach to T-depth Reduction", "authors": ["Archisman Ghosh", "Avimita Chatterjee", "Swaroop Ghosh"], "categories": ["quant-ph", "cs.AR", "cs.ET"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      7 pages, 4 figures", "url": "http://arxiv.org/abs/2504.09391v2", "summary": "Quantum Error Correction (QEC) is the cornerstone of practical Fault-Tolerant\nQuantum Computing (FTQC), but incurs enormous resource overheads. Circuits must\ndecompose into Clifford+T gates, and the non-transversal T gates demand costly\nmagic-state distillation. As circuit complexity grows, sequential T-gate layers\n(\"T-depth\") increase, amplifying the spatiotemporal overhead of QEC. Optimizing\nT-depth is NP-hard, and existing greedy or brute-force strategies are either\ninefficient or computationally prohibitive. We frame T-depth reduction as a\nsearch optimization problem and present a Genetic Algorithm (GA) framework that\napproximates optimal layer-merge patterns across the non-convex search space.\nWe introduce a mathematical formulation of the circuit expansion for systematic\nlayer reordering and a greedy initial merge-pair selection, accelerating the\nconvergence and enhancing the solution quality. In our benchmark with ~90-100\nqubits, our method reduces T-depth by 79.23% and overall T-count by 41.86%.\nCompared to the reversible circuit benchmarks, we achieve a 2.58x improvement\nin T-depth over the state-of-the-art methods, demonstrating its viability for\nnear-term FTQC.", "comment": "7 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2504.09391v2", "cate": "quant-ph", "date": "2025-04-13", "updated": "2025-07-12"}
{"id": "2507.09005", "title": "From images to properties: a NeRF-driven framework for granular material parameter inversion", "authors": ["Cheng-Hsi Hsiao", "Krishna Kumar"], "categories": ["cs.CV", "physics.geo-ph"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09005v1", "summary": "We introduce a novel framework that integrates Neural Radiance Fields (NeRF)\nwith Material Point Method (MPM) simulation to infer granular material\nproperties from visual observations. Our approach begins by generating\nsynthetic experimental data, simulating an plow interacting with sand. The\nexperiment is rendered into realistic images as the photographic observations.\nThese observations include multi-view images of the experiment's initial state\nand time-sequenced images from two fixed cameras. Using NeRF, we reconstruct\nthe 3D geometry from the initial multi-view images, leveraging its capability\nto synthesize novel viewpoints and capture intricate surface details. The\nreconstructed geometry is then used to initialize material point positions for\nthe MPM simulation, where the friction angle remains unknown. We render images\nof the simulation under the same camera setup and compare them to the observed\nimages. By employing Bayesian optimization, we minimize the image loss to\nestimate the best-fitting friction angle. Our results demonstrate that friction\nangle can be estimated with an error within 2 degrees, highlighting the\neffectiveness of inverse analysis through purely visual observations. This\napproach offers a promising solution for characterizing granular materials in\nreal-world scenarios where direct measurement is impractical or impossible.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09005v1", "cate": "cs.CV", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09049", "title": "CMER: A Context-Aware Approach for Mining Ethical Concern-related App Reviews", "authors": ["Aakash Sorathiya", "Gouri Ginde"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09049v1", "summary": "With the increasing proliferation of mobile applications in our daily lives,\nthe concerns surrounding ethics have surged significantly. Users communicate\ntheir feedback in app reviews, frequently emphasizing ethical concerns, such as\nprivacy and security. Incorporating these reviews has proved to be useful for\nmany areas of software engineering (e.g., requirement engineering, testing,\netc.). However, app reviews related to ethical concerns generally use\ndomain-specific language and are typically overshadowed by more generic\ncategories of user feedback, such as app reliability and usability. Thus,\nmaking automated extraction a challenging and time-consuming effort.\n  This study proposes CMER (A \\underline{C}ontext-Aware Approach for\n\\underline{M}ining \\underline{E}thical Concern-related App\n\\underline{R}eviews), a novel approach that combines Natural Language Inference\n(NLI) and a decoder-only (LLaMA-like) Large Language Model (LLM) to extract\nethical concern-related app reviews at scale. In CMER, NLI provides\ndomain-specific context awareness by using domain-specific hypotheses, and the\nLlama-like LLM eliminates the need for labeled data in the classification task.\nWe evaluated the validity of CMER by mining privacy and security-related\nreviews (PSRs) from the dataset of more than 382K app reviews of mobile\ninvestment apps. First, we evaluated four NLI models and compared the results\nof domain-specific hypotheses with generic hypotheses. Next, we evaluated three\nLLMs for the classification task. Finally, we combined the best NLI and LLM\nmodels (CMER) and extracted 2,178 additional PSRs overlooked by the previous\nstudy using a keyword-based approach, thus demonstrating the effectiveness of\nCMER. These reviews can be further refined into actionable requirement\nartifacts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09049v1", "cate": "cs.SE", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.10262", "title": "Experimental Analysis and Evaluation of Cohesive Subgraph Discovery", "authors": ["Dahee Kim", "Song Kim", "Jeongseon Kim", "Junghoon Kim", "Kaiyu Feng", "Sungsu Lim", "Jungeun Kim"], "categories": ["cs.SI"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "Comments:      17 pages, 26 figures", "url": "http://arxiv.org/abs/2507.10262v1", "summary": "Retrieving cohesive subgraphs in networks is a fundamental problem in social\nnetwork analysis and graph data management. These subgraphs can be used for\nmarketing strategies or recommendation systems. Despite the introduction of\nnumerous models over the years, a systematic comparison of their performance,\nespecially across varied network configurations, remains unexplored. In this\nstudy, we evaluated various cohesive subgraph models using task-based\nevaluations and conducted extensive experimental studies on both synthetic and\nreal-world networks. Thus, we unveil the characteristics of cohesive subgraph\nmodels, highlighting their efficiency and applicability. Our findings not only\nprovide a detailed evaluation of current models but also lay the groundwork for\nfuture research by shedding light on the balance between the interpretability\nand cohesion of the subgraphs. This research guides the selection of suitable\nmodels for specific analytical needs and applications, providing valuable\ninsights.", "comment": "17 pages, 26 figures", "pdf_url": "http://arxiv.org/pdf/2507.10262v1", "cate": "cs.SI", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10160", "title": "Domain Borders Are There to Be Crossed With Federated Few-Shot Adaptation", "authors": ["Manuel Röder", "Christoph Raab", "Frank-Michael Schleif"], "categories": ["cs.LG", "cs.CR", "cs.DC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Extension of this http URL", "url": "http://arxiv.org/abs/2507.10160v1", "summary": "Federated Learning has emerged as a leading paradigm for decentralized,\nprivacy-preserving learning, particularly relevant in the era of interconnected\nedge devices equipped with sensors. However, the practical implementation of\nFederated Learning faces three primary challenges: the need for human\ninvolvement in costly data labelling processes for target adaptation, covariate\nshift in client device data collection due to environmental factors affecting\nsensors, leading to discrepancies between source and target samples, and the\nimpracticality of continuous or regular model updates in resource-constrained\nenvironments due to limited data transmission capabilities and technical\nconstraints on channel availability and energy efficiency. To tackle these\nissues, we expand upon an efficient and scalable Federated Learning framework\ntailored for real-world client adaptation in industrial settings. This\nframework leverages a pre-trained source model comprising a deep backbone, an\nadaptation module, and a classifier running on a powerful server. By freezing\nthe backbone and classifier during client adaptation on resource-constrained\ndevices, we allow the domain adaptive linear layer to handle target domain\nadaptation, thus minimizing overall computational overhead. Furthermore, this\nsetup, designated as FedAcross+, is extended to encompass the processing of\nstreaming data, thereby rendering the solution suitable for non-stationary\nenvironments. Extensive experimental results demonstrate the effectiveness of\nFedAcross+ in achieving competitive adaptation on low-end client devices with\nlimited target samples, successfully addressing the challenge of domain shift.\nMoreover, our framework accommodates sporadic model updates within\nresource-constrained environments, ensuring practical and seamless deployment.", "comment": "Extension of http://dx.doi.org/10.5220/0012351900003654", "pdf_url": "http://arxiv.org/pdf/2507.10160v1", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10522", "title": "DeepResearch$^{\\text{Eco}}$: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology", "authors": ["Jennifer D'Souza", "Endres Keno Sander", "Andrei Aioanei"], "categories": ["cs.AI", "cs.CL", "cs.MA"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      12 pages, 3 figures", "url": "http://arxiv.org/abs/2507.10522v1", "summary": "We introduce DeepResearch$^{\\text{Eco}}$, a novel agentic LLM-based system\nfor automated scientific synthesis that supports recursive, depth- and\nbreadth-controlled exploration of original research questions -- enhancing\nsearch diversity and nuance in the retrieval of relevant scientific literature.\nUnlike conventional retrieval-augmented generation pipelines, DeepResearch\nenables user-controllable synthesis with transparent reasoning and\nparameter-driven configurability, facilitating high-throughput integration of\ndomain-specific evidence while maintaining analytical rigor. Applied to 49\necological research questions, DeepResearch achieves up to a 21-fold increase\nin source integration and a 14.9-fold rise in sources integrated per 1,000\nwords. High-parameter settings yield expert-level analytical depth and\ncontextual diversity.\n  Source code available at: https://github.com/sciknoworg/deep-research.", "comment": "12 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2507.10522v1", "cate": "cs.AI", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09538", "title": "On the Importance of Neural Membrane Potential Leakage for LIDAR-based Robot Obstacle Avoidance using Spiking Neural Networks", "authors": ["Zainab Ali", "Lujayn Al-Amir", "Ali Safa"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09538v1", "summary": "Using neuromorphic computing for robotics applications has gained much\nattention in recent year due to the remarkable ability of Spiking Neural\nNetworks (SNNs) for high-precision yet low memory and compute complexity\ninference when implemented in neuromorphic hardware. This ability makes SNNs\nwell-suited for autonomous robot applications (such as in drones and rovers)\nwhere battery resources and payload are typically limited. Within this context,\nthis paper studies the use of SNNs for performing direct robot navigation and\nobstacle avoidance from LIDAR data. A custom robot platform equipped with a\nLIDAR is set up for collecting a labeled dataset of LIDAR sensing data together\nwith the human-operated robot control commands used for obstacle avoidance.\nCrucially, this paper provides what is, to the best of our knowledge, a first\nfocused study about the importance of neuron membrane leakage on the SNN\nprecision when processing LIDAR data for obstacle avoidance. It is shown that\nby carefully tuning the membrane potential leakage constant of the spiking\nLeaky Integrate-and-Fire (LIF) neurons used within our SNN, it is possible to\nachieve on-par robot control precision compared to the use of a non-spiking\nConvolutional Neural Network (CNN). Finally, the LIDAR dataset collected during\nthis work is released as open-source with the hope of benefiting future\nresearch.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09538v1", "cate": "cs.RO", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.10024", "title": "Qualitative Study for LLM-assisted Design Study Process: Strategies, Challenges, and Roles", "authors": ["Shaolun Ruan", "Rui Sheng", "Xiaolin Wen", "Jiachen Wang", "Tianyi Zhang", "Yong Wang", "Tim Dwyer", "Jiannan Li"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10024v1", "summary": "Design studies aim to create visualization solutions for real-world problems\nof different application domains. Recently, the emergence of large language\nmodels (LLMs) has introduced new opportunities to enhance the design study\nprocess, providing capabilities such as creative problem-solving, data\nhandling, and insightful analysis. However, despite their growing popularity,\nthere remains a lack of systematic understanding of how LLMs can effectively\nassist researchers in visualization-specific design studies. In this paper, we\nconducted a multi-stage qualitative study to fill this gap, involving 30 design\nstudy researchers from diverse backgrounds and expertise levels. Through\nin-depth interviews and carefully-designed questionnaires, we investigated\nstrategies for utilizing LLMs, the challenges encountered, and the practices\nused to overcome them. We further compiled and summarized the roles that LLMs\ncan play across different stages of the design study process. Our findings\nhighlight practical implications to inform visualization practitioners, and\nprovide a framework for leveraging LLMs to enhance the design study process in\nvisualization research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10024v1", "cate": "cs.HC", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09540", "title": "Learning to Control Dynamical Agents via Spiking Neural Networks and Metropolis-Hastings Sampling", "authors": ["Ali Safa", "Farida Mohsen", "Ali Al-Zawqari"], "categories": ["cs.AI", "cs.RO"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09540v1", "summary": "Spiking Neural Networks (SNNs) offer biologically inspired, energy-efficient\nalternatives to traditional Deep Neural Networks (DNNs) for real-time control\nsystems. However, their training presents several challenges, particularly for\nreinforcement learning (RL) tasks, due to the non-differentiable nature of\nspike-based communication. In this work, we introduce what is, to our\nknowledge, the first framework that employs Metropolis-Hastings (MH) sampling,\na Bayesian inference technique, to train SNNs for dynamical agent control in RL\nenvironments without relying on gradient-based methods. Our approach\niteratively proposes and probabilistically accepts network parameter updates\nbased on accumulated reward signals, effectively circumventing the limitations\nof backpropagation while enabling direct optimization on neuromorphic\nplatforms. We evaluated this framework on two standard control benchmarks:\nAcroBot and CartPole. The results demonstrate that our MH-based approach\noutperforms conventional Deep Q-Learning (DQL) baselines and prior SNN-based RL\napproaches in terms of maximizing the accumulated reward while minimizing\nnetwork resources and training episodes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09540v1", "cate": "cs.AI", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09008", "title": "VISTA: A Visual Analytics Framework to Enhance Foundation Model-Generated Data Labels", "authors": ["Xiwei Xuan", "Xiaoqi Wang", "Wenbin He", "Jorge Piazentin Ono", "Liang Gou", "Kwan-Liu Ma", "Liu Ren"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      IEEE Transactions on Visualization and Computer Graphics (2025)", "url": "http://arxiv.org/abs/2507.09008v1", "summary": "The advances in multi-modal foundation models (FMs) (e.g., CLIP and LLaVA)\nhave facilitated the auto-labeling of large-scale datasets, enhancing model\nperformance in challenging downstream tasks such as open-vocabulary object\ndetection and segmentation. However, the quality of FM-generated labels is less\nstudied as existing approaches focus more on data quantity over quality. This\nis because validating large volumes of data without ground truth presents a\nconsiderable challenge in practice. Existing methods typically rely on limited\nmetrics to identify problematic data, lacking a comprehensive perspective, or\napply human validation to only a small data fraction, failing to address the\nfull spectrum of potential issues. To overcome these challenges, we introduce\nVISTA, a visual analytics framework that improves data quality to enhance the\nperformance of multi-modal models. Targeting the complex and demanding domain\nof open-vocabulary image segmentation, VISTA integrates multi-phased data\nvalidation strategies with human expertise, enabling humans to identify,\nunderstand, and correct hidden issues within FM-generated labels. Through\ndetailed use cases on two benchmark datasets and expert reviews, we demonstrate\nVISTA's effectiveness from both quantitative and qualitative perspectives.", "comment": "IEEE Transactions on Visualization and Computer Graphics (2025)", "pdf_url": "http://arxiv.org/pdf/2507.09008v1", "cate": "cs.CV", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09051", "title": "SAGE: A Context-Aware Approach for Mining Privacy Requirements Relevant Reviews from Mental Health Apps", "authors": ["Aakash Sorathiya", "Gouri Ginde"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09051v1", "summary": "Mental health (MH) apps often require sensitive user data to customize\nservices for mental wellness needs. However, such data collection practices in\nsome MH apps raise significant privacy concerns for users. These concerns are\noften mentioned in app reviews, but other feedback categories, such as\nreliability and usability, tend to take precedence. This poses a significant\nchallenge in automatically identifying privacy requirements-relevant reviews\n(privacy reviews) that can be utilized to extract privacy requirements and\naddress users' privacy concerns. Thus, this study introduces SAGE, a\ncontext-aware approach to automatically mining privacy reviews from MH apps\nusing Natural Language Inference (NLI) with MH domain-specific privacy\nhypotheses (provides domain-specific context awareness) and a GPT model\n(eliminates the need for fine-tuning). The quantitative evaluation of SAGE on a\ndataset of 204K app reviews achieved an F1 score of 0.85 without any\nfine-tuning, outperforming the fine-tuned baseline classifiers BERT and T5.\nFurthermore, SAGE extracted 748 privacy reviews previously overlooked by\nkeyword-based methods, demonstrating its effectiveness through qualitative\nevaluation. These reviews can later be refined into actionable privacy\nrequirement artifacts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09051v1", "cate": "cs.SE", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.08850", "title": "FlowsDT: A Geospatial Digital Twin for Navigating Urban Flood Dynamics", "authors": ["Debayan Mandal", "Lei Zou", "Abhinav Wadhwa", "Rohan Singh Wilkho", "Zhenhang Cai", "Bing Zhou", "Xinyue Ye", "Galen Newman", "Nasir Gharaibeh", "Burak Güneralp"], "categories": ["physics.soc-ph", "cs.SI"], "primary_category": "Subjects:       Physics and Society (physics.soc-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08850v1", "summary": "Communities worldwide increasingly confront flood hazards intensified by\nclimate change, urban expansion, and environmental degradation. Addressing\nthese challenges requires real-time flood analysis, precise flood forecasting,\nand robust risk communications with stakeholders to implement efficient\nmitigation strategies. Recent advances in hydrodynamic modeling and digital\ntwins afford new opportunities for high-resolution flood modeling and\nvisualization at the street and basement levels. Focusing on Galveston City, a\nbarrier island in Texas, U.S., this study created a geospatial digital twin\n(GDT) supported by 1D-2D coupled hydrodynamic models to strengthen urban\nresilience to pluvial and fluvial flooding. The objectives include: (1)\ndeveloping a GDT (FlowsDT-Galveston) incorporating topography, hydrography, and\ninfrastructure; (2) validating the twin using historical flood events and\nsocial sensing; (3) modeling hyperlocal flood conditions under 2-, 10-, 25-,\n50-, and 100-year return period rainfall scenarios; and (4) identifying at-risk\nzones under different scenarios. This study employs the PCSWMM to create\ndynamic virtual replicas of urban landscapes and accurate flood modeling. By\nintegrating LiDAR data, land cover, and storm sewer geometries, the model can\nsimulate flood depth, extent, duration, and velocity in a 4-D environment\nacross different historical and design storms. Results show buildings inundated\nover one foot increased by 5.7% from 2- to 100-year flood. Road inundations\nabove 1 foot increased by 6.7% from 2- to 100-year floods. The proposed model\ncan support proactive flood management and urban planning in Galveston; and\ninform disaster resilience efforts and guide sustainable infrastructure\ndevelopment. The framework can be extended to other communities facing similar\nchallenges.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08850v1", "cate": "physics.soc-ph", "date": "2025-07-08", "updated": "2025-07-08"}
{"id": "2507.10233", "title": "Secure and Efficient Quantum Signature Scheme Based on the Controlled Unitary Operations Encryption", "authors": ["Debnath Ghosh", "Soumit Roy", "Prithwi Bagchi", "Indranil Chakrabarty", "Ashok Kumar Das"], "categories": ["quant-ph", "cs.CR"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      22 pages, 3 figures. Accepted in Quantum Information Processing", "url": "http://arxiv.org/abs/2507.10233v1", "summary": "Quantum digital signatures ensure unforgeable message authenticity and\nintegrity using quantum principles, offering unconditional security against\nboth classical and quantum attacks. They are crucial for secure communication\nin high-stakes environments, ensuring trust and long-term protection in the\nquantum era. Nowadays, the majority of arbitrated quantum signature (AQS)\nprotocols encrypt data qubit by qubit using the quantum one-time pad (QOTP).\nDespite providing robust data encryption, QOTP is not a good fit for AQS\nbecause of its susceptibility to many types of attacks. In this work, we\npresent an efficient AQS protocol to encrypt quantum message ensembles using a\ndistinct encryption technique, the chained controlled unitary operations. In\ncontrast to existing protocols, our approach successfully prevents disavowal\nand forgery attacks. We hope this contributes to advancing future\ninvestigations into the development of AQS protocols.", "comment": "22 pages, 3 figures. Accepted in Quantum Information Processing", "pdf_url": "http://arxiv.org/pdf/2507.10233v1", "cate": "quant-ph", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2503.09639", "title": "Can A Society of Generative Agents Simulate Human Behavior and Inform Public Health Policy? A Case Study on Vaccine Hesitancy", "authors": ["Abe Bohan Hou", "Hongru Du", "Yichen Wang", "Jingyu Zhang", "Zixiao Wang", "Paul Pu Liang", "Daniel Khashabi", "Lauren Gardner", "Tianxing He"], "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CY", "cs.HC"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "Comments:      Accepted to COLM 2025", "url": "http://arxiv.org/abs/2503.09639v4", "summary": "Can we simulate a sandbox society with generative agents to model human\nbehavior, thereby reducing the over-reliance on real human trials for assessing\npublic policies? In this work, we investigate the feasibility of simulating\nhealth-related decision-making, using vaccine hesitancy, defined as the delay\nin acceptance or refusal of vaccines despite the availability of vaccination\nservices (MacDonald, 2015), as a case study. To this end, we introduce the\nVacSim framework with 100 generative agents powered by Large Language Models\n(LLMs). VacSim simulates vaccine policy outcomes with the following steps: 1)\ninstantiate a population of agents with demographics based on census data; 2)\nconnect the agents via a social network and model vaccine attitudes as a\nfunction of social dynamics and disease-related information; 3) design and\nevaluate various public health interventions aimed at mitigating vaccine\nhesitancy. To align with real-world results, we also introduce simulation\nwarmup and attitude modulation to adjust agents' attitudes. We propose a series\nof evaluations to assess the reliability of various LLM simulations.\nExperiments indicate that models like Llama and Qwen can simulate aspects of\nhuman behavior but also highlight real-world alignment challenges, such as\ninconsistent responses with demographic profiles. This early exploration of\nLLM-driven simulations is not meant to serve as definitive policy guidance;\ninstead, it serves as a call for action to examine social simulation for policy\ndevelopment.", "comment": "Accepted to COLM 2025", "pdf_url": "http://arxiv.org/pdf/2503.09639v4", "cate": "cs.MA", "date": "2025-03-12", "updated": "2025-07-13"}
{"id": "2507.09714", "title": "IteraOptiRacing: A Unified Planning-Control Framework for Real-time Autonomous Racing for Iterative Optimal Performance", "authors": ["Yifan Zeng", "Yihan Li", "Suiyi He", "Koushil Sreenath", "Jun Zeng"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09714v1", "summary": "This paper presents a unified planning-control strategy for competing with\nother racing cars called IteraOptiRacing in autonomous racing environments.\nThis unified strategy is proposed based on Iterative Linear Quadratic Regulator\nfor Iterative Tasks (i2LQR), which can improve lap time performance in the\npresence of surrounding racing obstacles. By iteratively using the ego car's\nhistorical data, both obstacle avoidance for multiple moving cars and time cost\noptimization are considered in this unified strategy, resulting in\ncollision-free and time-optimal generated trajectories. The algorithm's\nconstant low computation burden and suitability for parallel computing enable\nreal-time operation in competitive racing scenarios. To validate its\nperformance, simulations in a high-fidelity simulator are conducted with\nmultiple randomly generated dynamic agents on the track. Results show that the\nproposed strategy outperforms existing methods across all randomly generated\nautonomous racing scenarios, enabling enhanced maneuvering for the ego racing\ncar.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09714v1", "cate": "cs.RO", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.10043", "title": "XROps: A Visual Workflow Management System for Dynamic Immersive Analytics", "authors": ["Suemin Jeon", "JunYoung Choi", "Haejin Jeong", "Won-Ki Jeong"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10043v1", "summary": "Immersive analytics is gaining attention across multiple domains due to its\ncapability to facilitate intuitive data analysis in expansive environments\nthrough user interaction with data. However, creating immersive analytics\nsystems for specific tasks is challenging due to the need for programming\nexpertise and significant development effort. Despite the introduction of\nvarious immersive visualization authoring toolkits, domain experts still face\nhurdles in adopting immersive analytics into their workflow, particularly when\nfaced with dynamically changing tasks and data in real time. To lower such\ntechnical barriers, we introduce XROps, a web-based authoring system that\nallows users to create immersive analytics applications through interactive\nvisual programming, without the need for low-level scripting or coding. XROps\nenables dynamic immersive analytics authoring by allowing users to modify each\nstep of the data visualization process with immediate feedback, enabling them\nto build visualizations on-the-fly and adapt to changing environments. It also\nsupports the integration and visualization of real-time sensor data from XR\ndevices, a key feature of immersive analytics, facilitating the creation of\nvarious analysis scenarios. We evaluated the usability of XROps through a user\nstudy and demonstrate its efficacy and usefulness in several example scenarios.\nWe have released a web platform (https://vience.io/xrops) to demonstrate\nvarious examples to supplement our findings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10043v1", "cate": "cs.HC", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09588", "title": "eSapiens: A Platform for Secure and Auditable Retrieval-Augmented Generation", "authors": ["Isaac Shi", "Zeyuan Li", "Fan Liu", "Wenli Wang", "Lewei He", "Yang Yang", "Tianyu Shi"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09588v1", "summary": "We present eSapiens, an AI-as-a-Service (AIaaS) platform engineered around a\nbusiness-oriented trifecta: proprietary data, operational workflows, and any\nmajor agnostic Large Language Model (LLM). eSapiens gives businesses full\ncontrol over their AI assets, keeping everything in-house for AI knowledge\nretention and data security. eSapiens AI Agents (Sapiens) empower your team by\nproviding valuable insights and automating repetitive tasks, enabling them to\nfocus on high-impact work and drive better business outcomes.\n  The system integrates structured document ingestion, hybrid vector retrieval,\nand no-code orchestration via LangChain, and supports top LLMs including\nOpenAI, Claude, Gemini, and DeepSeek. A key component is the THOR Agent, which\nhandles structured SQL-style queries and generates actionable insights over\nenterprise databases.\n  To evaluate the system, we conduct two experiments. First, a retrieval\nbenchmark on legal corpora reveals that a chunk size of 512 tokens yields the\nhighest retrieval precision (Top-3 accuracy: 91.3%). Second, a generation\nquality test using TRACe metrics across five LLMs shows that eSapiens delivers\nmore context-consistent outputs with up to a 23% improvement in factual\nalignment.\n  These results demonstrate the effectiveness of eSapiens in enabling\ntrustworthy, auditable AI workflows for high-stakes domains like legal and\nfinance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09588v1", "cate": "cs.AI", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09036", "title": "BrainLesion Suite: A Flexible and User-Friendly Framework for Modular Brain Lesion Image Analysis", "authors": ["Florian Kofler", "Marcel Rosier", "Mehdi Astaraki", "Hendrik Möller", "Ilhem Isra Mekki", "Josef A. Buchner", "Anton Schmick", "Arianna Pfiffer", "Eva Oswald", "Lucas Zimmer", "Ezequiel de la Rosa", "Sarthak Pati", "Julian Canisius", "Arianna Piffer", "Ujjwal Baid", "Mahyar Valizadeh", "Akis Linardos", "Jan C. Peeken", "Surprosanna Shit", "Felix Steinbauer", "Daniel Rueckert", "Rolf Heckemann", "Spyridon Bakas", "Jan Kirschke", "Constantin von See", "Ivan Ezhov", "Marie Piraud", "Benedikt Wiestler", "Bjoern Menze"], "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SE"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      16p, 3f", "url": "http://arxiv.org/abs/2507.09036v1", "summary": "BrainLesion Suite is a versatile toolkit for building modular brain lesion\nimage analysis pipelines in Python. Following Pythonic principles, BrainLesion\nSuite is designed to provide a 'brainless' development experience, minimizing\ncognitive effort and streamlining the creation of complex workflows for\nclinical and scientific practice. At its core is an adaptable preprocessing\nmodule that performs co-registration, atlas registration, and optional\nskull-stripping and defacing on arbitrary multi-modal input images. BrainLesion\nSuite leverages algorithms from the BraTS challenge to synthesize missing\nmodalities, inpaint lesions, and generate pathology-specific tumor\nsegmentations. BrainLesion Suite also enables quantifying segmentation model\nperformance, with tools such as panoptica to compute lesion-wise metrics.\nAlthough BrainLesion Suite was originally developed for image analysis\npipelines of brain lesions such as glioma, metastasis, and multiple sclerosis,\nit can be adapted for other biomedical image analysis applications. The\nindividual BrainLesion Suite packages and tutorials are accessible on GitHub.", "comment": "16p, 3f", "pdf_url": "http://arxiv.org/pdf/2507.09036v1", "cate": "cs.CV", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09063", "title": "SetupBench: Assessing Software Engineering Agents' Ability to Bootstrap Development Environments", "authors": ["Avi Arora", "Jinu Jang", "Roshanak Zilouchian Moghaddam"], "categories": ["cs.SE", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09063v1", "summary": "Modern Large Language Model (LLM) agents promise end to end assistance with\nreal-world software tasks, yet existing benchmarks evaluate LLM agents almost\nexclusively in pre-baked environments where every dependency is pre-installed.\nTo fill this gap, we introduce SetupBench, a 93 instance benchmark that\nisolates the environment-bootstrap skill: starting from a bare Linux sandbox,\nan agent must install packages, resolve dependency conflicts, initialize\ndatabases, and configure background services. Our tasks span seven language\necosystems, five database engines, and multi-service orchestration scenarios,\neach accompanies by a natural language problem statement and a deterministic\nsuccess command. Through evaluation of OpenHands, a state-of-the-art coding\nagent, we find low success rates across task categories, with particular\nchallenges in repository setup (38.9-57.4%) and local database configuration\n(20.0-53.3%). Our analysis reveals systematic failure modes including\nincomplete development tooling installation, hallucinated task constraints, and\nnon-persistent environment modifications that break agent-human collaboration\nworkflows. We identify substantial inefficiencies in agent exploration\nstrategies, with 38-89% of actions being unnecessary compared to optimal human\nbehavior. These findings highlight gaps in current agents' practical\nenvironment-bootstrap capabilities. By targeting this critical yet\nunder-evaluated capability, SetupBench provides a rigorous yard-stick for the\nnext generation of software developer agents aiming to solve end to end\nreal-wold tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09063v1", "cate": "cs.SE", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.08881", "title": "The Consistency-Acceptability Divergence of LLMs in Judicial Decision-Making: Task and Stakeholder Dimensions", "authors": ["Zhang MingDa", "Xu Qing"], "categories": ["cs.CY", "cs.AI", "cs.SI"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      12 pages,2 figures", "url": "http://arxiv.org/abs/2507.08881v1", "summary": "The integration of large language model (LLM) technology into judicial\nsystems is fundamentally transforming legal practice worldwide. However, this\nglobal transformation has revealed an urgent paradox requiring immediate\nattention. This study introduces the concept of ``consistency-acceptability\ndivergence'' for the first time, referring to the gap between technical\nconsistency and social acceptance. While LLMs achieve high consistency at the\ntechnical level, this consistency demonstrates both positive and negative\neffects. Through comprehensive analysis of recent data on LLM judicial\napplications from 2023--2025, this study finds that addressing this challenge\nrequires understanding both task and stakeholder dimensions. This study\nproposes the Dual-Track Deliberative Multi-Role LLM Judicial Governance\nFramework (DTDMR-LJGF), which enables intelligent task classification and\nmeaningful interaction among diverse stakeholders. This framework offers both\ntheoretical insights and practical guidance for building an LLM judicial\necosystem that balances technical efficiency with social legitimacy.", "comment": "12 pages,2 figures", "pdf_url": "http://arxiv.org/pdf/2507.08881v1", "cate": "cs.CY", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.09094", "title": "Transformer based Collaborative Reinforcement Learning for Fluid Antenna System (FAS)-enabled 3D UAV Positioning", "authors": ["Xiaoren Xu", "Hao Xu", "Dongyu Wei", "Walid Saad", "Mehdi Bennis", "Mingzhe Chen"], "categories": ["cs.NI", "eess.SP"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09094v1", "summary": "In this paper, a novel Three dimensional (3D) positioning framework of fluid\nantenna system (FAS)-enabled unmanned aerial vehicles (UAVs) is developed. In\nthe proposed framework, a set of controlled UAVs cooperatively estimate the\nreal-time 3D position of a target UAV. Here, the active UAV transmits a\nmeasurement signal to the passive UAVs via the reflection from the target UAV.\nEach passive UAV estimates the distance of the active-target-passive UAV link\nand selects an antenna port to share the distance information with the base\nstation (BS) that calculates the real-time position of the target UAV. As the\ntarget UAV is moving due to its task operation, the controlled UAVs must\noptimize their trajectories and select optimal antenna port, aiming to estimate\nthe real-time position of the target UAV. We formulate this problem as an\noptimization problem to minimize the target UAV positioning error via\noptimizing the trajectories of all controlled UAVs and antenna port selection\nof passive UAVs. Here, an attention-based recurrent multi-agent reinforcement\nlearning (AR-MARL) scheme is proposed, which enables each controlled UAV to use\nthe local Q function to determine its trajectory and antenna port while\noptimizing the target UAV positioning performance without knowing the\ntrajectories and antenna port selections of other controlled UAVs. Different\nfrom current MARL methods, the proposed method uses a recurrent neural network\n(RNN) that incorporates historical state-action pairs of each controlled UAV,\nand an attention mechanism to analyze the importance of these historical\nstate-action pairs, thus improving the global Q function approximation accuracy\nand the target UAV positioning accuracy. Simulation results show that the\nproposed AR-MARL scheme can reduce the average positioning error by up to 17.5%\nand 58.5% compared to the VD-MARL scheme and the proposed method without FAS.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09094v1", "cate": "cs.NI", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.10494", "title": "Split Happens: Combating Advanced Threats with Split Learning and Function Secret Sharing", "authors": ["Tanveer Khan", "Mindaugas Budzys", "Antonis Michalas"], "categories": ["cs.LG", "cs.CR"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10494v1", "summary": "Split Learning (SL) -- splits a model into two distinct parts to help protect\nclient data while enhancing Machine Learning (ML) processes. Though promising,\nSL has proven vulnerable to different attacks, thus raising concerns about how\neffective it may be in terms of data privacy. Recent works have shown promising\nresults for securing SL through the use of a novel paradigm, named Function\nSecret Sharing (FSS), in which servers obtain shares of a function they compute\nand operate on a public input hidden with a random mask. However, these works\nfall short in addressing the rising number of attacks which exist on SL. In\nSplitHappens, we expand the combination of FSS and SL to U-shaped SL. Similarly\nto other works, we are able to make use of the benefits of SL by reducing the\ncommunication and computational costs of FSS. However, a U-shaped SL provides a\nhigher security guarantee than previous works, allowing a client to keep the\nlabels of the training data secret, without having to share them with the\nserver. Through this, we are able to generalize the security analysis of\nprevious works and expand it to different attack vectors, such as modern model\ninversion attacks as well as label inference attacks. We tested our approach\nfor two different convolutional neural networks on different datasets. These\nexperiments show the effectiveness of our approach in reducing the training\ntime as well as the communication costs when compared to simply using FSS while\nmatching prior accuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10494v1", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2111.06614", "title": "Collaboration Promotes Group Resilience in Multi-Agent AI", "authors": ["Sarah Keren", "Matthias Gerstgrasser", "Ofir Abu", "Jeffrey Rosenschein"], "categories": ["cs.LG", "cs.MA", "I.2.11; I.2.6"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      RLC 2025", "url": "http://arxiv.org/abs/2111.06614v3", "summary": "To effectively operate in various dynamic scenarios, RL agents must be\nresilient to unexpected changes in their environment. Previous work on this\nform of resilience has focused on single-agent settings. In this work, we\nintroduce and formalize a multi-agent variant of resilience, which we term\ngroup resilience. We further hypothesize that collaboration with other agents\nis key to achieving group resilience; collaborating agents adapt better to\nenvironmental perturbations in multi-agent reinforcement learning (MARL)\nsettings. We test our hypothesis empirically by evaluating different\ncollaboration protocols and examining their effect on group resilience. Our\nexperiments show that all the examined collaborative approaches achieve higher\ngroup resilience than their non-collaborative counterparts.", "comment": "RLC 2025", "pdf_url": "http://arxiv.org/pdf/2111.06614v3", "cate": "cs.LG", "date": "2021-11-12", "updated": "2025-07-14"}
{"id": "2507.09725", "title": "Visual Homing in Outdoor Robots Using Mushroom Body Circuits and Learning Walks", "authors": ["Gabriel G. Gattaux", "Julien R. Serres", "Franck Ruffier", "Antoine Wystrach"], "categories": ["cs.RO", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Published by Springer Nature with the 14th bioinspired and biohybrid systems conference in Sheffield, and presented at the conference in July 2025", "url": "http://arxiv.org/abs/2507.09725v1", "summary": "Ants achieve robust visual homing with minimal sensory input and only a few\nlearning walks, inspiring biomimetic solutions for autonomous navigation. While\nMushroom Body (MB) models have been used in robotic route following, they have\nnot yet been applied to visual homing. We present the first real-world\nimplementation of a lateralized MB architecture for visual homing onboard a\ncompact autonomous car-like robot. We test whether the sign of the angular path\nintegration (PI) signal can categorize panoramic views, acquired during\nlearning walks and encoded in the MB, into \"goal on the left\" and \"goal on the\nright\" memory banks, enabling robust homing in natural outdoor settings. We\nvalidate this approach through four incremental experiments: (1) simulation\nshowing attractor-like nest dynamics; (2) real-world homing after decoupled\nlearning walks, producing nest search behavior; (3) homing after random walks\nusing noisy PI emulated with GPS-RTK; and (4) precise stopping-at-the-goal\nbehavior enabled by a fifth MB Output Neuron (MBON) encoding goal-views to\ncontrol velocity. This mimics the accurate homing behavior of ants and\nfunctionally resembles waypoint-based position control in robotics, despite\nrelying solely on visual input. Operating at 8 Hz on a Raspberry Pi 4 with\n32x32 pixel views and a memory footprint under 9 kB, our system offers a\nbiologically grounded, resource-efficient solution for autonomous visual\nhoming.", "comment": "Published by Springer Nature with the 14th bioinspired and biohybrid\n  systems conference in Sheffield, and presented at the conference in July 2025", "pdf_url": "http://arxiv.org/pdf/2507.09725v1", "cate": "cs.RO", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.10044", "title": "MEDebiaser: A Human-AI Feedback System for Mitigating Bias in Multi-label Medical Image Classification", "authors": ["Shaohan Shi", "Yuheng Shao", "Haoran Jiang", "Yunjie Yao", "Zhijun Zhang", "Xu Ding", "Quan Li"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10044v1", "summary": "Medical images often contain multiple labels with imbalanced distributions\nand co-occurrence, leading to bias in multi-label medical image classification.\nClose collaboration between medical professionals and machine learning\npractitioners has significantly advanced medical image analysis. However,\ntraditional collaboration modes struggle to facilitate effective feedback\nbetween physicians and AI models, as integrating medical expertise into the\ntraining process via engineers can be time-consuming and labor-intensive. To\nbridge this gap, we introduce MEDebiaser, an interactive system enabling\nphysicians to directly refine AI models using local explanations. By combining\nprediction with attention loss functions and employing a customized ranking\nstrategy to alleviate scalability, MEDebiaser allows physicians to mitigate\nbiases without technical expertise, reducing reliance on engineers, and thus\nenhancing more direct human-AI feedback. Our mechanism and user studies\ndemonstrate that it effectively reduces biases, improves usability, and\nenhances collaboration efficiency, providing a practical solution for\nintegrating medical expertise into AI-driven healthcare.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10044v1", "cate": "cs.HC", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09611", "title": "The Hidden Costs of AI: A Review of Energy, E-Waste, and Inequality in Model Development", "authors": ["Jenis Winsta"], "categories": ["cs.AI", "cs.CY", "68T01"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      5 pages, 3 figures", "url": "http://arxiv.org/abs/2507.09611v1", "summary": "Artificial intelligence (AI) has made remarkable progress in recent years,\nyet its rapid expansion brings overlooked environmental and ethical challenges.\nThis review explores four critical areas where AI's impact extends beyond\nperformance: energy consumption, electronic waste (e-waste), inequality in\ncompute access, and the hidden energy burden of cybersecurity systems. Drawing\nfrom recent studies and institutional reports, the paper highlights systemic\nissues such as high emissions from model training, rising hardware turnover,\nglobal infrastructure disparities, and the energy demands of securing AI. By\nconnecting these concerns, the review contributes to Responsible AI discourse\nby identifying key research gaps and advocating for sustainable, transparent,\nand equitable development practices. Ultimately, it argues that AI's progress\nmust align with ethical responsibility and environmental stewardship to ensure\na more inclusive and sustainable technological future.", "comment": "5 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2507.09611v1", "cate": "cs.AI", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09052", "title": "Can Contrastive Learning Improve Class-Imbalanced Diffusion Model?", "authors": ["Fang Chen", "Alex Villa", "Gongbo Liang", "Xiaoyi Lu", "Meng Tang"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      20 pages, 11 figures", "url": "http://arxiv.org/abs/2507.09052v1", "summary": "Training data for class-conditional image synthesis often exhibit a\nlong-tailed distribution with limited images for tail classes. Such an\nimbalance causes mode collapse and reduces the diversity of synthesized images\nfor tail classes. For class-conditional diffusion models trained on imbalanced\ndata, we aim to improve the diversity of tail class images without compromising\nthe fidelity and diversity of head class images. We achieve this by introducing\ntwo deceptively simple but highly effective contrastive loss functions.\nFirstly, we employ an unsupervised InfoNCE loss utilizing negative samples to\nincrease the distance/dissimilarity among synthetic images, particularly for\ntail classes. To further enhance the diversity of tail classes, our second loss\nis an MSE loss that contrasts class-conditional generation with unconditional\ngeneration at large timesteps. This second loss makes the denoising process\ninsensitive to class conditions for the initial steps, which enriches tail\nclasses through knowledge sharing from head classes. Conditional-unconditional\nalignment has been shown to enhance the performance of long-tailed GAN. We are\nthe first to adapt such alignment to diffusion models. We successfully\nleveraged contrastive learning for class-imbalanced diffusion models. Our\ncontrastive learning framework is easy to implement and outperforms standard\nDDPM and alternative methods for class-imbalanced diffusion models across\nvarious datasets, including CIFAR10/100-LT, PlacesLT, TinyImageNetLT, and\nImageNetLT.", "comment": "20 pages, 11 figures", "pdf_url": "http://arxiv.org/pdf/2507.09052v1", "cate": "cs.CV", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09108", "title": "SPICE: An Automated SWE-Bench Labeling Pipeline for Issue Clarity, Test Coverage, and Effort Estimation", "authors": ["Aaditya Bhatia", "Gustavo A. Oliva", "Gopi Krishnan Rajbahadur", "Haoxiang Zhang", "Yihao Chen", "Zhilong Chen", "Arthur Leung", "Dayi Lin", "Boyuan Chen", "Ahmed E. Hassan"], "categories": ["cs.SE", "cs.AI"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09108v1", "summary": "High-quality labeled datasets are crucial for training and evaluating\nfoundation models in software engineering, but creating them is often\nprohibitively expensive and labor-intensive. We introduce SPICE, a scalable,\nautomated pipeline for labeling SWE-bench-style datasets with annotations for\nissue clarity, test coverage, and effort estimation. SPICE combines\ncontext-aware code navigation, rationale-driven prompting, and multi-pass\nconsensus to produce labels that closely approximate expert annotations.\nSPICE's design was informed by our own experience and frustration in labeling\nmore than 800 instances from SWE-Gym. SPICE achieves strong agreement with\nhuman-labeled SWE-bench Verified data while reducing the cost of labeling 1,000\ninstances from around $100,000 (manual annotation) to just $5.10. These results\ndemonstrate SPICE's potential to enable cost-effective, large-scale dataset\ncreation for SE-focused FMs. To support the community, we release both SPICE\ntool and SPICE Bench, a new dataset of 6,802 SPICE-labeled instances curated\nfrom 291 open-source projects in SWE-Gym (over 13x larger than SWE-bench\nVerified).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09108v1", "cate": "cs.SE", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09777", "title": "Te Ahorré Un Click: A Revised Definition of Clickbait and Detection in Spanish News", "authors": ["Gabriel Mordecki", "Guillermo Moncecchi", "Javier Couto"], "categories": ["cs.CL", "cs.SI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09777v1", "summary": "We revise the definition of clickbait, which lacks current consensus, and\nargue that the creation of a curiosity gap is the key concept that\ndistinguishes clickbait from other related phenomena such as sensationalism and\nheadlines that do not deliver what they promise or diverge from the article.\nTherefore, we propose a new definition: clickbait is a technique for generating\nheadlines and teasers that deliberately omit part of the information with the\ngoal of raising the readers' curiosity, capturing their attention and enticing\nthem to click. We introduce a new approach to clickbait detection datasets\ncreation, by refining the concept limits and annotations criteria, minimizing\nthe subjectivity in the decision as much as possible. Following it, we created\nand release TA1C (for Te Ahorr\\'e Un Click, Spanish for Saved You A Click), the\nfirst open source dataset for clickbait detection in Spanish. It consists of\n3,500 tweets coming from 18 well known media sources, manually annotated and\nreaching a 0.825 Fleiss' K inter annotator agreement. We implement strong\nbaselines that achieve 0.84 in F1-score.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09777v1", "cate": "cs.CL", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09124", "title": "Proactive AI-and-RAN Workload Orchestration in O-RAN Architectures for 6G Networks", "authors": ["Syed Danial Ali Shah", "Maryam Hafeez", "Abdelaziz Salama", "Syed Ali Raza Zaidi"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09124v1", "summary": "The vision of AI-RAN convergence, as advocated by the AI-RAN Alliance, aims\nto unlock a unified 6G platform capable of seamlessly supporting AI and RAN\nworkloads over shared infrastructure. However, the architectural framework and\nintelligent resource orchestration strategies necessary to realize this vision\nremain largely unexplored. In this paper, we propose a Converged AI-and-ORAN\nArchitectural (CAORA) framework based on O-RAN specifications, enabling the\ndynamic coexistence of real-time RAN and computationally intensive AI\nworkloads. We design custom xApps within the Near-Real-Time RAN Intelligent\nController (NRT-RIC) to monitor RAN KPIs and expose radio analytics to an\nEnd-to-End (E2E) orchestrator via the recently introduced Y1 interface. The\norchestrator incorporates workload forecasting and anomaly detection modules,\naugmenting a Soft Actor-Critic (SAC) reinforcement learning agent that\nproactively manages resource allocation, including Multi-Instance GPU (MIG)\npartitioning. Using real-world 5G traffic traces from Barcelona, our\ntrace-driven simulations demonstrate that CAORA achieves near 99\\% fulfillment\nof RAN demands, supports dynamic AI workloads, and maximizes infrastructure\nutilization even under highly dynamic conditions. Our results reveal that\npredictive orchestration significantly improves system adaptability, resource\nefficiency, and service continuity, offering a viable blueprint for future\nAI-and-RAN converged 6G systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09124v1", "cate": "cs.NI", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2409.07167", "title": "H$_2$O$_2$RAM: A High-Performance Hierarchical Doubly Oblivious RAM", "authors": ["Leqian Zheng", "Zheng Zhang", "Wentao Dong", "Yao Zhang", "Ye Wu", "Cong Wang"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.07167v4", "summary": "The combination of Oblivious RAM (ORAM) with Trusted Execution Environments\n(TEE) has found numerous real-world applications due to their complementary\nnature. TEEs alleviate the performance bottlenecks of ORAM, such as network\nbandwidth and roundtrip latency, and ORAM provides general-purpose protection\nfor TEE applications against attacks exploiting memory access patterns. The\ndefining property of this combination, which sets it apart from traditional\nORAM designs, is its ability to ensure that memory accesses, both inside and\noutside of TEEs, are made oblivious, thus termed doubly oblivious RAM\n(O$_2$RAM). Efforts to develop O$_2$RAM with enhanced performance are ongoing.\n  In this work, we propose H$_2$O$_2$RAM, a high-performance doubly oblivious\nRAM construction. The distinguishing feature of our approach, compared to the\nexisting tree-based doubly oblivious designs, is its first adoption of the\nhierarchical framework that enjoys inherently better data locality and\nparallelization. While the latest hierarchical solution, FutORAMa, achieves\nconcrete efficiency in the classic client-server model by leveraging a relaxed\nassumption of sublinear-sized client-side private memory, adapting it to our\nscenario poses challenges due to the conflict between this relaxed assumption\nand our doubly oblivious requirement. To this end, we introduce several new\nefficient oblivious components to build a high-performance hierarchical\nO$_2$RAM (H$_2$O$_2$RAM). We implement our design and evaluate it on various\nscenarios. The results indicate that H$_2$O$_2$RAM reduces execution time by up\nto $\\sim 10^3$ times and saves memory usage by $5\\sim44$ times compared to\nstate-of-the-art solutions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.07167v4", "cate": "cs.CR", "date": "2024-09-11", "updated": "2025-07-14"}
{"id": "2204.08594", "title": "CoDe: A Cooperative and Decentralized Collision Avoidance Algorithm for Small-Scale UAV Swarms Considering Energy Efficiency", "authors": ["Shuangyao Huang", "Haibo Zhang", "Zhiyi Huang"], "categories": ["cs.RO", "cs.MA"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted at IROS 2024", "url": "http://arxiv.org/abs/2204.08594v2", "summary": "This paper introduces a cooperative and decentralized collision avoidance\nalgorithm (CoDe) for small-scale UAV swarms consisting of up to three UAVs.\nCoDe improves energy efficiency of UAVs by achieving effective cooperation\namong UAVs. Moreover, CoDe is specifically tailored for UAV's operations by\naddressing the challenges faced by existing schemes, such as ineffectiveness in\nselecting actions from continuous action spaces and high computational\ncomplexity. CoDe is based on Multi-Agent Reinforcement Learning (MARL), and\nfinds cooperative policies by incorporating a novel credit assignment scheme.\nThe novel credit assignment scheme estimates the contribution of an individual\nby subtracting a baseline from the joint action value for the swarm. The credit\nassignment scheme in CoDe outperforms other benchmarks as the baseline takes\ninto account not only the importance of a UAV's action but also the\ninterrelation between UAVs. Furthermore, extensive experiments are conducted\nagainst existing MARL-based and conventional heuristic-based algorithms to\ndemonstrate the advantages of the proposed algorithm.", "comment": "Accepted at IROS 2024", "pdf_url": "http://arxiv.org/pdf/2204.08594v2", "cate": "cs.RO", "date": "2022-04-19", "updated": "2025-07-14"}
{"id": "2507.09822", "title": "Active Probing with Multimodal Predictions for Motion Planning", "authors": ["Darshan Gadginmath", "Farhad Nawaz", "Minjun Sung", "Faizan M Tariq", "Sangjae Bae", "David Isele", "Fabio Pasqualetti", "Jovin Dsa"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      To appear at IROS '25. 8 pages. 3 tables. 6 figures", "url": "http://arxiv.org/abs/2507.09822v1", "summary": "Navigation in dynamic environments requires autonomous systems to reason\nabout uncertainties in the behavior of other agents. In this paper, we\nintroduce a unified framework that combines trajectory planning with multimodal\npredictions and active probing to enhance decision-making under uncertainty. We\ndevelop a novel risk metric that seamlessly integrates multimodal prediction\nuncertainties through mixture models. When these uncertainties follow a\nGaussian mixture distribution, we prove that our risk metric admits a\nclosed-form solution, and is always finite, thus ensuring analytical\ntractability. To reduce prediction ambiguity, we incorporate an active probing\nmechanism that strategically selects actions to improve its estimates of\nbehavioral parameters of other agents, while simultaneously handling multimodal\nuncertainties. We extensively evaluate our framework in autonomous navigation\nscenarios using the MetaDrive simulation environment. Results demonstrate that\nour active probing approach successfully navigates complex traffic scenarios\nwith uncertain predictions. Additionally, our framework shows robust\nperformance across diverse traffic agent behavior models, indicating its broad\napplicability to real-world autonomous navigation challenges. Code and videos\nare available at\nhttps://darshangm.github.io/papers/active-probing-multimodal-predictions/.", "comment": "To appear at IROS '25. 8 pages. 3 tables. 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.09822v1", "cate": "cs.RO", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.10099", "title": "ReDemon UI: Reactive Synthesis by Demonstration for Web UI", "authors": ["Jay Lee", "Gyuhyeok Oh", "Joongwon Ahn", "Xiaokang Qiu"], "categories": ["cs.HC", "cs.PL"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Submitted to UIST 2025 Posters", "url": "http://arxiv.org/abs/2507.10099v1", "summary": "ReDemon UI synthesizes React applications from user demonstrations, enabling\ndesigners and non-expert programmers to create UIs that integrate with standard\nUI prototyping workflows. Users provide a static mockup sketch with event\nhandler holes and demonstrate desired runtime behaviors by interacting with the\nrendered mockup and editing the sketch. ReDemon UI identifies reactive data and\nsynthesizes a React program with correct state update logic. We utilize\nenumerative synthesis for simple UIs and LLMs for more complex UIs.", "comment": "Submitted to UIST 2025 Posters", "pdf_url": "http://arxiv.org/pdf/2507.10099v1", "cate": "cs.HC", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09617", "title": "Bridging Bots: from Perception to Action via Multimodal-LMs and Knowledge Graphs", "authors": ["Margherita Martorana", "Francesca Urgese", "Mark Adamik", "Ilaria Tiddi"], "categories": ["cs.AI", "cs.RO"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09617v1", "summary": "Personal service robots are deployed to support daily living in domestic\nenvironments, particularly for elderly and individuals requiring assistance.\nThese robots must perceive complex and dynamic surroundings, understand tasks,\nand execute context-appropriate actions. However, current systems rely on\nproprietary, hard-coded solutions tied to specific hardware and software,\nresulting in siloed implementations that are difficult to adapt and scale\nacross platforms. Ontologies and Knowledge Graphs (KGs) offer a solution to\nenable interoperability across systems, through structured and standardized\nrepresentations of knowledge and reasoning. However, symbolic systems such as\nKGs and ontologies struggle with raw and noisy sensory input. In contrast,\nmultimodal language models are well suited for interpreting input such as\nimages and natural language, but often lack transparency, consistency, and\nknowledge grounding. In this work, we propose a neurosymbolic framework that\ncombines the perceptual strengths of multimodal language models with the\nstructured representations provided by KGs and ontologies, with the aim of\nsupporting interoperability in robotic applications. Our approach generates\nontology-compliant KGs that can inform robot behavior in a platform-independent\nmanner. We evaluated this framework by integrating robot perception data,\nontologies, and five multimodal models (three LLaMA and two GPT models), using\ndifferent modes of neural-symbolic interaction. We assess the consistency and\neffectiveness of the generated KGs across multiple runs and configurations, and\nperform statistical analyzes to evaluate performance. Results show that GPT-o1\nand LLaMA 4 Maverick consistently outperform other models. However, our\nfindings also indicate that newer models do not guarantee better results,\nhighlighting the critical role of the integration strategy in generating\nontology-compliant KGs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09617v1", "cate": "cs.AI", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09068", "title": "Infinite Video Understanding", "authors": ["Dell Zhang", "Xiangyu Chen", "Jixiang Luo", "Mengxi Jia", "Changzhi Sun", "Ruilong Ren", "Jingren Liu", "Hao Sun", "Xuelong Li"], "categories": ["cs.CV", "cs.AI", "cs.IR", "cs.LG", "cs.MM"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09068v1", "summary": "The rapid advancements in Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) have ushered in remarkable progress in video understanding.\nHowever, a fundamental challenge persists: effectively processing and\ncomprehending video content that extends beyond minutes or hours. While recent\nefforts like Video-XL-2 have demonstrated novel architectural solutions for\nextreme efficiency, and advancements in positional encoding such as HoPE and\nVideoRoPE++ aim to improve spatio-temporal understanding over extensive\ncontexts, current state-of-the-art models still encounter significant\ncomputational and memory constraints when faced with the sheer volume of visual\ntokens from lengthy sequences. Furthermore, maintaining temporal coherence,\ntracking complex events, and preserving fine-grained details over extended\nperiods remain formidable hurdles, despite progress in agentic reasoning\nsystems like Deep Video Discovery. This position paper posits that a logical,\nalbeit ambitious, next frontier for multimedia research is Infinite Video\nUnderstanding -- the capability for models to continuously process, understand,\nand reason about video data of arbitrary, potentially never-ending duration. We\nargue that framing Infinite Video Understanding as a blue-sky research\nobjective provides a vital north star for the multimedia, and the wider AI,\nresearch communities, driving innovation in areas such as streaming\narchitectures, persistent memory mechanisms, hierarchical and adaptive\nrepresentations, event-centric reasoning, and novel evaluation paradigms.\nDrawing inspiration from recent work on long/ultra-long video understanding and\nseveral closely related fields, we outline the core challenges and key research\ndirections towards achieving this transformative capability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09068v1", "cate": "cs.CV", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09135", "title": "Position Paper: Programming Language Techniques for Bridging LLM Code Generation Semantic Gaps", "authors": ["Yalong Du", "Chaozheng Wang", "Huaijin Wang"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09135v1", "summary": "Large Language Models have demonstrated remarkable capabilities in automated\ncode generation, yet their statistical nature and black-box characteristics\ncreate significant semantic gaps manifested through syntax errors, semantic\nhallucinations, and reliability concerns. This position paper argues that\nprincipled integration of Programming Language (PL) techniques is essential for\nbridging these gaps. Through structured program representations, formal\ncorrectness guarantees, and robust verification mechanisms, PL techniques can\nelevate LLM-generated code from statistical pattern matching to truly reliable\nand trustworthy levels. This integration is crucial for developing systems that\ngenerate code that is not only functionally correct but also interpretable,\nverifiable, and ultimately trustworthy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09135v1", "cate": "cs.SE", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2408.05700", "title": "Quantification of Interdependent Emotion Dynamics in Online Interactions", "authors": ["Yishan Luo", "Didier Sornette", "Sandro Claudio Lera"], "categories": ["cs.SI", "cs.HC", "stat.AP"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2408.05700v3", "summary": "A growing share of human interactions now occurs online, where the expression\nand perception of emotions are often amplified and distorted. Yet, the\ninterplay between different emotions and the extent to which they are driven by\nexternal stimuli or social feedback remains poorly understood. We calibrate a\nmultivariate Hawkes self-exciting point process to model the temporal\nexpression of six basic emotions in YouTube Live chats. This framework captures\nboth temporal and cross-emotional dependencies while allowing us to disentangle\nthe influence of video content (exogenous) from peer interactions (endogenous).\nWe find that emotional expressions are up to four times more strongly driven by\npeer interaction than by video content. Positivity is more contagious,\nspreading three times more readily, whereas negativity is more memorable,\nlingering nearly twice as long. Moreover, we observe asymmetric\ncross-excitation, with negative emotions frequently triggering positive ones, a\npattern consistent with trolling dynamics, but not the reverse. These findings\nhighlight the central role of social interaction in shaping emotional dynamics\nonline and the risks of emotional manipulation as human-chatbot interactions\nbecome increasingly realistic.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2408.05700v3", "cate": "cs.SI", "date": "2024-08-11", "updated": "2025-07-12"}
{"id": "2507.09153", "title": "On-Demand HAPS-Assisted Communication System for Public Safety in Emergency and Disaster Response", "authors": ["Bilal Karaman", "Ilhan Baştürk", "Ferdi Kara", "Engin Zeydan", "Esra Aycan Beyazıt", "Sezai Taşkın", "Emil Björnson", "Halim Yanikomeroglu"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      Accepted for publication in IEEE COMMAG", "url": "http://arxiv.org/abs/2507.09153v1", "summary": "Natural disasters often disrupt communication networks and severely hamper\nemergency response and disaster management. Existing solutions, such as\nportable communication units and cloud-based network architectures, have\nimproved disaster resilience but fall short if both the Radio Access Network\n(RAN) and backhaul infrastructure become inoperable. To address these\nchallenges, we propose a demand-driven communication system supported by High\nAltitude Platform Stations (HAPS) to restore communication in an affected area\nand enable effective disaster relief. The proposed emergency response network\nis a promising solution as it provides a rapidly deployable, resilient\ncommunications infrastructure. The proposed HAPS-based communication can play a\ncrucial role not only in ensuring connectivity for mobile users but also in\nrestoring backhaul connections when terrestrial networks fail. As a bridge\nbetween the disaster management center and the affected areas, it can\nfacilitate the exchange of information in real time, collect data from the\naffected regions, and relay crucial updates to emergency responders. Enhancing\nsituational awareness, coordination between relief agencies, and ensuring\nefficient resource allocation can significantly strengthen disaster response\ncapabilities. In this paper, simulations show that HAPS with hybrid optical/THz\nlinks boosts backhaul capacity and resilience, even in harsh conditions.\nHAPS-enabled RAN in S- and Ka-bands ensures reliable communication for first\nresponders and disaster-affected populations. This paper also explores the\nintegration of HAPS into emergency communication frameworks and standards, as\nit has the potential to improve network resilience and support effective\ndisaster management.", "comment": "Accepted for publication in IEEE COMMAG", "pdf_url": "http://arxiv.org/pdf/2507.09153v1", "cate": "cs.NI", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2409.14530", "title": "An Integrated Blockchain and IPFS Solution for Secure and Efficient Source Code Repository Hosting using Middleman Approach", "authors": ["Md. Rafid Haque", "Sakibul Islam Munna", "Sabbir Ahmed", "Md. Tahmid Islam", "Md Mehedi Hassan Onik", "A. B. M. Ashikur Rahman"], "categories": ["cs.CR", "cs.NI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      26 pages, 5 figures, submitted manuscript to PlosOne journal;", "url": "http://arxiv.org/abs/2409.14530v2", "summary": "Centralized version control systems (VCS) are vital for software development\nbut pose risks of data loss and ownership disputes. While blockchain offers a\ndecentralized alternative, existing solutions are often hindered by high\nlatency, compromising the real-time collaboration essential for modern\nworkflows. This study introduces a novel hybrid architecture combining the\nsecurity of the Ethereum blockchain and the InterPlanetary File System (IPFS)\nwith two key contributions: 1) Shamir's Secret Sharing (SSS) to create a\ntrust-minimized model for key distribution, and 2) an authoritative-first,\noptimistic-fallback retrieval protocol utilizing a temporary middleware to\ndecouple the user experience from blockchain confirmation delays. We\nimplemented a full prototype and conducted a comprehensive performance\nevaluation on the public Sepolia testnet. Our results demonstrate that this\narchitecture not only provides a secure, auditable, and resilient platform for\nsource code hosting but also achieves highly competitive user-perceived\nperformance. Our user-perceived push time reduces submission latency by up to\n49% compared to a standard git push for common repository sizes, proving that a\nwell-designed decentralized VCS can balance the core tenets of security and\ndecentralization with the practical need for speed and efficiency.", "comment": "26 pages, 5 figures, submitted manuscript to PlosOne journal;", "pdf_url": "http://arxiv.org/pdf/2409.14530v2", "cate": "cs.CR", "date": "2024-09-22", "updated": "2025-07-12"}
{"id": "2405.11873", "title": "Equilibria in multiagent online problems with predictions", "authors": ["Gabriel Istrate", "Cosmin Bonchiş", "Victor Bogdan"], "categories": ["cs.GT", "cs.DS", "cs.MA"], "primary_category": "Subjects:       Computer Science and Game Theory (cs.GT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2405.11873v3", "summary": "We study the power of (competitive) algorithms with predictions in a\nmultiagent setting. To this goal, we introduce a multiagent version of the\nski-rental problem. In this problem agents can collaborate by pooling resources\nto get a group license for some asset. If the license price is not met then\nagents have to rent the asset individually for the day at a unit price.\nOtherwise the license becomes available forever to everyone at no extra cost.\n  We investigate the effect of using predictors for self and others' behavior\nin such a setting, as well as the new equilibria formed in this way.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2405.11873v3", "cate": "cs.GT", "date": "2024-05-20", "updated": "2025-07-11"}
{"id": "2507.09858", "title": "Customize Harmonic Potential Fields via Hybrid Optimization over Homotopic Paths", "authors": ["Shuaikang Wang", "Tiecheng Guo", "Meng Guo"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      accepted to IEEE RA-L", "url": "http://arxiv.org/abs/2507.09858v1", "summary": "Safe navigation within a workspace is a fundamental skill for autonomous\nrobots to accomplish more complex tasks. Harmonic potentials are artificial\npotential fields that are analytical, globally convergent and provably free of\nlocal minima. Thus, it has been widely used for generating safe and reliable\nrobot navigation control policies. However, most existing methods do not allow\ncustomization of the harmonic potential fields nor the resulting paths,\nparticularly regarding their topological properties. In this paper, we propose\na novel method that automatically finds homotopy classes of paths that can be\ngenerated by valid harmonic potential fields. The considered complex workspaces\ncan be as general as forest worlds consisting of numerous overlapping\nstar-obstacles. The method is based on a hybrid optimization algorithm that\nsearches over homotopy classes, selects the structure of each tree-of-stars\nwithin the forest, and optimizes over the continuous weight parameters for each\npurged tree via the projected gradient descent. The key insight is to transform\nthe forest world to the unbounded point world via proper diffeomorphic\ntransformations. It not only facilitates a simpler design of the\nmulti-directional D-signature between non-homotopic paths, but also retain the\nsafety and convergence properties. Extensive simulations and hardware\nexperiments are conducted for non-trivial scenarios, where the navigation\npotentials are customized for desired homotopic properties. Project page:\nhttps://shuaikang-wang.github.io/CustFields.", "comment": "accepted to IEEE RA-L", "pdf_url": "http://arxiv.org/pdf/2507.09858v1", "cate": "cs.RO", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10102", "title": "When Familiarity Remains: Procedural Memory, Symbolic Anchors, and Digital Engagement in Dementia Care", "authors": ["Jeongone Seo", "Kyung-zoon Hong", "Sol Baik"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      23 page, 2 tables, 1 figure", "url": "http://arxiv.org/abs/2507.10102v1", "summary": "INTRODUCTION: Older adults with early-stage dementia often retain procedural\nmemory, enabling continued use of familiar technologies. Additionally, symbolic\nanchors such as photos or personalized content may serve as memory cues to\nreinforce digital engagement. This study explores how these mechanisms support\ntechnology use in dementia care within the South Korean context.\n  METHODS: We conducted in-depth interviews with 11 professional caregivers of\ncommunity-dwelling older adults with cognitive decline. Grounded theory methods\nguided the analysis, using iterative coding and constant comparison to identify\nemergent themes.\n  RESULTS: Caregivers reported that familiar digital routines (e.g., taking\nphotos) persisted through procedural memory. Symbolic anchors such as family\nphotos or recognizable icons enhanced interaction and emotional engagement.\nHowever, unfamiliar or anthropomorphic technologies often triggered fear or\nsymbolic resistance.\n  DISCUSSION: Findings highlight the dual role of procedural memory and\nsymbolic anchors in sustaining digital engagement. Designing culturally\nresponsive and cognitively accessible technologies may enhance autonomy and\nwell-being in dementia care.\n  Keywords: procedural memory, symbolic anchors, dementia care, digital\nengagement, older adults, cultural adaptation, caregiving technologies", "comment": "23 page, 2 tables, 1 figure", "pdf_url": "http://arxiv.org/pdf/2507.10102v1", "cate": "cs.HC", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09626", "title": "humancompatible.interconnect: Testing Properties of Repeated Uses of Interconnections of AI Systems", "authors": ["Rodion Nazarov", "Anthony Quinn", "Robert Shorten", "Jakub Marecek"], "categories": ["cs.AI", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09626v1", "summary": "Artificial intelligence (AI) systems often interact with multiple agents. The\nregulation of such AI systems often requires that {\\em a priori\\/} guarantees\nof fairness and robustness be satisfied. With stochastic models of agents'\nresponses to the outputs of AI systems, such {\\em a priori\\/} guarantees\nrequire non-trivial reasoning about the corresponding stochastic systems. Here,\nwe present an open-source PyTorch-based toolkit for the use of stochastic\ncontrol techniques in modelling interconnections of AI systems and properties\nof their repeated uses. It models robustness and fairness desiderata in a\nclosed-loop fashion, and provides {\\em a priori\\/} guarantees for these\ninterconnections. The PyTorch-based toolkit removes much of the complexity\nassociated with the provision of fairness guarantees for closed-loop models of\nmulti-agent systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09626v1", "cate": "cs.AI", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09071", "title": "BlindSight: Harnessing Sparsity for Efficient VLMs", "authors": ["Tharun Adithya Srikrishnan", "Deval Shah", "Steven K. Reinhardt"], "categories": ["cs.CV", "I.2.10"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09071v1", "summary": "Large vision-language models (VLMs) enable the joint processing of text and\nimages. However, the inclusion of vision data significantly expands the prompt\nlength. Along with the quadratic complexity of the attention computation, this\nresults in a longer prefill duration. An approach to mitigate this bottleneck\nis to leverage the inherent sparsity in the attention computation. In our\nanalysis of attention patterns in VLMs, we observe that a substantial portion\nof layers exhibit minimal cross-image attention, except through attention-sink\ntokens per image. These sparse attention patterns fall into distinct\ncategories: sink-only, document mask and a hybrid document-sink mask. Based on\nthis, we propose BlindSight: a training-free approach to optimize VLM inference\nusing a input template-aware attention sparsity mask. We utilize samples from a\ndataset to derive a prompt-agnostic sparsity categorization for every attention\nhead. We evaluate the proposed technique using VLMs such as Qwen2-VL,\nQwen2.5-VL and Gemma-3. BlindSight results in a 32%-41% reduction in FLOPs on\naverage with -2%-+2% accuracy compared to the original model in most evaluated\nmulti-image understanding benchmarks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09071v1", "cate": "cs.CV", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09186", "title": "OpenCAMS: An Open-Source Connected and Automated Mobility Co-Simulation Platform for Advanced Transportation Research", "authors": ["Minhaj Uddin Ahmad", "Akid Abrar", "Sagar Dasgupta", "Mizanur Rahman"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09186v1", "summary": "We introduce OpenCAMS (Open-Source Connected and Automated Mobility\nCo-Simulation Platform), an open-source, synchronized, and extensible\nco-simulation framework that tightly couples three best-in-class simulation\ntools: (i) SUMO, (ii) CARLA, and (iii) OMNeT++. OpenCAMS is designed to support\nadvanced research in transportation safety, mobility, and cybersecurity by\ncombining the strengths of each simulation domain. Specifically, SUMO provides\nlarge-scale, microscopic traffic modeling; CARLA offers high-fidelity 3D\nperception, vehicle dynamics, and control simulation; and OMNeT++ enables\nmodular, event-driven network communication, such as cellular\nvehicle-to-everything (C-V2X). OpenCAMS employs a time-synchronized,\nbidirectional coupling architecture that ensures coherent simulation\nprogression across traffic, perception, and communication domains while\npreserving modularity and reproducibility. For example, CARLA can simulate and\nrender a subset of vehicles that require detailed sensor emulation and control\nlogic; SUMO orchestrates network-wide traffic flow, vehicle routing, and\ntraffic signal management; and OMNeT++ dynamically maps communication nodes to\nboth mobile entities (e.g., vehicles) and static entities (e.g., roadside\nunits) to enable C-V2X communication. While these three simulators form the\nfoundational core of OpenCAMS, the platform is designed to be expandable and\nfuture-proof, allowing additional simulators to be integrated on top of this\ncore without requiring fundamental changes to the system architecture. The\nOpenCAMS platform is fully open-source and publicly available through its\nGitHub repository https://github.com/minhaj6/carla-sumo-omnetpp-cosim,\nproviding the research community with an accessible, flexible, and\ncollaborative environment for advancing next-generation intelligent\ntransportation systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09186v1", "cate": "cs.SE", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2501.00417", "title": "PureRank: A Parameter-Free Recursive Importance Measure for Network Nodes", "authors": ["Hiroyuki Masuyama"], "categories": ["cs.SI"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "Comments:      This paper is under review at 'Information Sciences'", "url": "http://arxiv.org/abs/2501.00417v5", "summary": "This study focuses on parameter-free importance measures, based on the\nrecursive definition of importance (RDI), for network nodes. The best-known\nexamples of such RDI-based measures are eigenvector centrality and Seeley\ncentrality, but they are applicable only to strongly connected networks. In\ncontrast, Katz centrality and its variants, including PageRank, are\nRDI-inspired measures that introduce free parameters to handle general\nnetworks. This motivates the overlooked question of whether an RDI-based\nmeasure can be defined for arbitrary networks without introducing free\nparameters. This question is addressed by introducing $PureRank$, a\nparameter-free recursive importance measure. PureRank proceeds in three steps:\n(i) nodes are classified into recurrent, transient, and dangling classes via\nstrongly connected component decomposition; (ii) local importance vectors for\nthese classes are formulated as solutions to Katz parameter optimization\nproblems aimed at best approximating eigenvector centrality within each class;\nand (iii) these vectors are aggregated into global scores via the RDI\nprinciple. This modular design enables parallel and incremental computation.\nPureRank also admits a probabilistic interpretation via a random-surfer model.\nThe effectiveness and characteristics of PureRank are evaluated through\nnumerical experiments on large-scale real-world networks, in comparison with\nPageRank. Finally, extension of PureRank to multi-attribute networks is\ndiscussed.", "comment": "This paper is under review at 'Information Sciences'", "pdf_url": "http://arxiv.org/pdf/2501.00417v5", "cate": "cs.SI", "date": "2024-12-31", "updated": "2025-07-13"}
{"id": "2507.09270", "title": "Joint Traffic Reshaping and Channel Reconfiguration in RIS-assisted Semantic NOMA Communications", "authors": ["Songhan Zhao", "Yusi Long", "Lanhua Li", "Bo Gu", "Shimin Gong", "Zehui Xiong"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09270v1", "summary": "In this paper, we consider a semantic-aware reconfigurable intelligent\nsurface (RIS)-assisted wireless network, where multiple semantic users (SUs)\nsimultaneously transmit semantic information to an access point (AP) by using\nthe non-orthogonal multiple access (NOMA) method. The SUs can reshape their\ntraffic demands by modifying the semantic extraction factor, while the RIS can\nreconfigure the channel conditions via the passive beamforming. This provides\nthe AP with greater flexibility to decode the superimposed signals from the\nSUs. We aim to minimize the system's overall energy consumption, while ensuring\nthat each SU's traffic demand is satisfied. Hence, we formulate a joint\noptimization problem of the SUs' decoding order and semantic control, as well\nas the RIS's passive beamforming strategy. This problem is intractable due to\nthe complicated coupling in constraints. To solve this, we decompose the\noriginal problem into two subproblems and solve them by using a series of\napproximate methods. Numerical results show that the joint traffic reshaping\nand channel reconfiguration scheme significantly improves the energy saving\nperformance of the NOMA transmissions compared to the benchmark methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09270v1", "cate": "cs.NI", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2410.12318", "title": "UTF:Undertrained Tokens as Fingerprints A Novel Approach to LLM Identification", "authors": ["Jiacheng Cai", "Jiahao Yu", "Yangguang Shao", "Yuhang Wu"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.12318v2", "summary": "Fingerprinting large language models (LLMs) is essential for verifying model\nownership, ensuring authenticity, and preventing misuse. Traditional\nfingerprinting methods often require significant computational overhead or\nwhite-box verification access. In this paper, we introduce UTF, a novel and\nefficient approach to fingerprinting LLMs by leveraging under-trained tokens.\nUnder-trained tokens are tokens that the model has not fully learned during its\ntraining phase. By utilizing these tokens, we perform supervised fine-tuning to\nembed specific input-output pairs into the model. This process allows the LLM\nto produce predetermined outputs when presented with certain inputs,\neffectively embedding a unique fingerprint. Our method has minimal overhead and\nimpact on model's performance, and does not require white-box access to target\nmodel's ownership identification. Compared to existing fingerprinting methods,\nUTF is also more effective and robust to fine-tuning and random guess.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.12318v2", "cate": "cs.CR", "date": "2024-10-16", "updated": "2025-07-12"}
{"id": "2502.18805", "title": "It's Not All Black and White: Degree of Truthfulness for Risk-Avoiding Agents", "authors": ["Eden Hartman", "Erel Segal-Halevi", "Biaoshuai Tao"], "categories": ["cs.GT", "cs.MA", "econ.TH"], "primary_category": "Subjects:       Computer Science and Game Theory (cs.GT)", "pdf_link": null, "comments": "Comments:      Accepted to EC 2025", "url": "http://arxiv.org/abs/2502.18805v2", "summary": "The classic notion of \\emph{truthfulness} requires that no agent has a\nprofitable manipulation -- an untruthful report that, for \\emph{some}\ncombination of reports of the other agents, increases her utility. This strong\nnotion implicitly assumes that the manipulating agent either knows what all\nother agents are going to report, or is willing to take the risk and act as-if\nshe knows their reports.\n  Without knowledge of the others' reports, most manipulations are \\emph{risky}\n-- they might decrease the manipulator's utility for some other combinations of\nreports by the other agents. Accordingly, a recent paper (Bu, Song and Tao,\n``On the existence of truthful fair cake cutting mechanisms'', Artificial\nIntelligence 319 (2023), 103904) suggests a relaxed notion, which we refer to\nas \\emph{risk-avoiding truthfulness (RAT)}, which requires only that no agent\ncan gain from a \\emph{safe} manipulation -- one that is sometimes beneficial\nand never harmful.\n  Truthfulness and RAT are two extremes: the former considers manipulators with\ncomplete knowledge of others, whereas the latter considers manipulators with no\nknowledge at all. In reality, agents often know about some -- but not all -- of\nthe other agents. This paper introduces the \\emph{RAT-degree} of a mechanism,\ndefined as the smallest number of agents whose reports, if known, may allow\nanother agent to safely manipulate, or $n$ if there is no such number. This\nnotion interpolates between classic truthfulness (degree $n$) and RAT (degree\nat least $1$): a mechanism with a higher RAT-degree is harder to manipulate\nsafely.\n  To illustrate the generality and applicability of this concept, we analyze\nthe RAT-degree of prominent mechanisms across various social choice settings,\nincluding auctions, indivisible goods allocations, cake-cutting, voting, and\ntwo-sided matching.", "comment": "Accepted to EC 2025", "pdf_url": "http://arxiv.org/pdf/2502.18805v2", "cate": "cs.GT", "date": "2025-02-26", "updated": "2025-07-13"}
{"id": "2507.09985", "title": "Demonstrating the Octopi-1.5 Visual-Tactile-Language Model", "authors": ["Samson Yu", "Kelvin Lin", "Harold Soh"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Published at R:SS 2025", "url": "http://arxiv.org/abs/2507.09985v1", "summary": "Touch is recognized as a vital sense for humans and an equally important\nmodality for robots, especially for dexterous manipulation, material\nidentification, and scenarios involving visual occlusion. Building upon very\nrecent work in touch foundation models, this demonstration will feature\nOctopi-1.5, our latest visual-tactile-language model. Compared to its\npredecessor, Octopi-1.5 introduces the ability to process tactile signals from\nmultiple object parts and employs a simple retrieval-augmented generation (RAG)\nmodule to improve performance on tasks and potentially learn new objects\non-the-fly. The system can be experienced live through a new handheld\ntactile-enabled interface, the TMI, equipped with GelSight and TAC-02 tactile\nsensors. This convenient and accessible setup allows users to interact with\nOctopi-1.5 without requiring a robot. During the demonstration, we will\nshowcase Octopi-1.5 solving tactile inference tasks by leveraging tactile\ninputs and commonsense knowledge. For example, in a Guessing Game, Octopi-1.5\nwill identify objects being grasped and respond to follow-up queries about how\nto handle it (e.g., recommending careful handling for soft fruits). We also\nplan to demonstrate Octopi-1.5's RAG capabilities by teaching it new items.\nWith live interactions, this demonstration aims to highlight both the progress\nand limitations of VTLMs such as Octopi-1.5 and to foster further interest in\nthis exciting field. Code for Octopi-1.5 and design files for the TMI gripper\nare available at https://github.com/clear-nus/octopi-1.5.", "comment": "Published at R:SS 2025", "pdf_url": "http://arxiv.org/pdf/2507.09985v1", "cate": "cs.RO", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10240", "title": "Visual Analytics for Explainable and Trustworthy Artificial Intelligence", "authors": ["Angelos Chatzimparmpas"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10240v1", "summary": "Our society increasingly depends on intelligent systems to solve complex\nproblems, ranging from recommender systems suggesting the next movie to watch\nto AI models assisting in medical diagnoses for hospitalized patients. With the\niterative improvement of diagnostic accuracy and efficiency, AI holds\nsignificant potential to mitigate medical misdiagnoses by preventing numerous\ndeaths and reducing an economic burden of approximately 450 EUR billion\nannually. However, a key obstacle to AI adoption lies in the lack of\ntransparency: many automated systems function as \"black boxes,\" providing\npredictions without revealing the underlying processes. This opacity can hinder\nexperts' ability to trust and rely on AI systems. Visual analytics (VA)\nprovides a compelling solution by combining AI models with interactive\nvisualizations. These specialized charts and graphs empower users to\nincorporate their domain expertise to refine and improve the models, bridging\nthe gap between AI and human understanding. In this work, we define,\ncategorize, and explore how VA solutions can foster trust across the stages of\na typical AI pipeline. We propose a design space for innovative visualizations\nand present an overview of our previously developed VA dashboards, which\nsupport critical tasks within the various pipeline stages, including data\nprocessing, feature engineering, hyperparameter tuning, understanding,\ndebugging, refining, and comparing models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10240v1", "cate": "cs.HC", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09662", "title": "Towards Concise and Adaptive Thinking in Large Reasoning Models: A Survey", "authors": ["Jason Zhu", "Hongyu Li"], "categories": ["cs.AI", "cs.CL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09662v1", "summary": "Large reasoning models (LRMs) like OpenAI o1 and DeepSeek R1 have\ndemonstrated impressive performance on complex reasoning tasks like mathematics\nand programming with long Chain-of-Thought (CoT) reasoning sequences\n(slow-thinking), compared with traditional large language models\n(fast-thinking). However, these reasoning models also face a huge challenge\nthat generating unnecessarily lengthy and redundant reasoning chains even for\ntrivial questions. This phenomenon leads to a significant waste of inference\nresources, increases the response time for simple queries, and hinders the\npractical application of LRMs in real-world products. To this end, it is\ncrucial to shorten lengthy reasoning chains and learn adaptive reasoning\nbetween fast and slow thinking based on input difficulty. In this survey, we\nprovide a comprehensive overview of recent progress in concise and adaptive\nthinking for efficient reasoning of LRMs, including methodologies, benchmarks,\nand challenges for future exploration. We hope this survey can help researchers\nquickly understand the landscape of this field and inspire novel adaptive\nthinking ideas to facilitate better usage of LRMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09662v1", "cate": "cs.AI", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09081", "title": "From Physics to Foundation Models: A Review of AI-Driven Quantitative Remote Sensing Inversion", "authors": ["Zhenyu Yu", "Mohd Yamani Idna Idris", "Hua Wang", "Pei Wang", "Junyi Chen", "Kun Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09081v1", "summary": "Quantitative remote sensing inversion aims to estimate continuous surface\nvariables-such as biomass, vegetation indices, and evapotranspiration-from\nsatellite observations, supporting applications in ecosystem monitoring, carbon\naccounting, and land management. With the evolution of remote sensing systems\nand artificial intelligence, traditional physics-based paradigms are giving way\nto data-driven and foundation model (FM)-based approaches. This paper\nsystematically reviews the methodological evolution of inversion techniques,\nfrom physical models (e.g., PROSPECT, SCOPE, DART) to machine learning methods\n(e.g., deep learning, multimodal fusion), and further to foundation models\n(e.g., SatMAE, GFM, mmEarth). We compare the modeling assumptions, application\nscenarios, and limitations of each paradigm, with emphasis on recent FM\nadvances in self-supervised pretraining, multi-modal integration, and\ncross-task adaptation. We also highlight persistent challenges in physical\ninterpretability, domain generalization, limited supervision, and uncertainty\nquantification. Finally, we envision the development of next-generation\nfoundation models for remote sensing inversion, emphasizing unified modeling\ncapacity, cross-domain generalization, and physical interpretability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09081v1", "cate": "cs.CV", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09199", "title": "Back to the Basics: Rethinking Issue-Commit Linking with LLM-Assisted Retrieval", "authors": ["Huihui Huang", "Ratnadira Widyasari", "Ting Zhang", "Ivana Clairine Irsan", "Jieke Shi", "Han Wei Ang", "Frank Liauw", "Eng Lieh Ouh", "Lwin Khin Shar", "Hong Jin Kang", "David Lo"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09199v1", "summary": "Issue-commit linking, which connects issues with commits that fix them, is\ncrucial for software maintenance. Existing approaches have shown promise in\nautomatically recovering these links. Evaluations of these techniques assess\ntheir ability to identify genuine links from plausible but false links.\nHowever, these evaluations overlook the fact that, in reality, when a\nrepository has more commits, the presence of more plausible yet unrelated\ncommits may interfere with the tool in differentiating the correct fix commits.\nTo address this, we propose the Realistic Distribution Setting (RDS) and use it\nto construct a more realistic evaluation dataset that includes 20 open-source\nprojects. By evaluating tools on this dataset, we observe that the performance\nof the state-of-the-art deep learning-based approach drops by more than half,\nwhile the traditional Information Retrieval method, VSM, outperforms it.\n  Inspired by these observations, we propose EasyLink, which utilizes a vector\ndatabase as a modern Information Retrieval technique. To address the\nlong-standing problem of the semantic gap between issues and commits, EasyLink\nleverages a large language model to rerank the commits retrieved from the\ndatabase. Under our evaluation, EasyLink achieves an average Precision@1 of\n75.91%, improving over the state-of-the-art by over four times. Additionally,\nthis paper provides practical guidelines for advancing research in issue-commit\nlink recovery.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09199v1", "cate": "cs.SE", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2506.18641", "title": "Preserving spreading dynamics and information flow in complex network reduction", "authors": ["Dan Chen", "Housheng Su", "Yong Wang", "Jie Liu"], "categories": ["cs.SI", "nlin.AO"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.18641v2", "summary": "Effectively preserving both the structural and dynamical properties during\nthe reduction of complex networks remains a significant research topic.\nExisting network reduction methods based on renormalization group or sampling\noften face challenges such as high computational complexity and the loss of\ncritical dynamic attributes. This paper proposes an efficient network reduction\nframework based on subgraph extraction, which accurately preserves epidemic\nspreading dynamics and information flow through a coordinated optimization\nstrategy of node removal and edge pruning. Specifically, a node removal\nalgorithm driven by enhanced degree centrality is introduced to preferentially\nremove low-centrality nodes, thereby constructing a smaller-scale subnetwork.\nSubsequently, an edge pruning algorithm is designed to regulate the edge\ndensity of the subnetwork, ensuring that its average degree remains\napproximately consistent with that of the original network. Experimental\nresults on Erd\\\"os-R\\'enyi random graphs, Barab\\'asi-Albert scale-free\nnetworks, and real-world social contact networks from various domains\ndemonstrate that this proposed method can reduce the size of networks with\nheterogeneous structures by more than 85\\%, while preserving their epidemic\ndynamics and information flow. More importantly, our method almost always\nachieves the highest accuracy compared to state-of-the-art techniques. These\nfindings provide valuable insights for predicting the dynamical behavior of\nlarge-scale real-world networks, and also reveal that a large number of nodes\nand edges in real-world networks play redundant roles in information\ntransmission.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.18641v2", "cate": "cs.SI", "date": "2025-06-23", "updated": "2025-07-13"}
{"id": "2507.09341", "title": "Meeting Deadlines in Motion: Deep RL for Real-Time Task Offloading in Vehicular Edge Networks", "authors": ["Mahsa Paknejad", "Parisa Fard Moshiri", "Murat Simsek", "Burak Kantarci", "Hussein T. Mouftah"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      8 pages, 7 figures, Accepted to IEEE 16th International Conference on Network of the Future (NoF) 2025", "url": "http://arxiv.org/abs/2507.09341v1", "summary": "Vehicular Mobile Edge Computing (VEC) drives the future by enabling\nlow-latency, high-efficiency data processing at the very edge of vehicular\nnetworks. This drives innovation in key areas such as autonomous driving,\nintelligent transportation systems, and real-time analytics. Despite its\npotential, VEC faces significant challenges, particularly in adhering to strict\ntask offloading deadlines, as vehicles remain within the coverage area of\nRoadside Units (RSUs) for only brief periods. To tackle this challenge, this\npaper evaluates the performance boundaries of task processing by initially\nestablishing a theoretical limit using Particle Swarm Optimization (PSO) in a\nstatic environment. To address more dynamic and practical scenarios, PSO, Deep\nQ-Network (DQN), and Proximal Policy Optimization (PPO) models are implemented\nin an online setting. The objective is to minimize dropped tasks and reduce\nend-to-end (E2E) latency, covering both communication and computation delays.\nExperimental results demonstrate that the DQN model considerably surpasses the\ndynamic PSO approach, achieving a 99.2% reduction in execution time.\nFurthermore, It leads to a reduction in dropped tasks by 2.5% relative to\ndynamic PSO and achieves 18.6\\% lower E2E latency, highlighting the\neffectiveness of Deep Reinforcement Learning (DRL) in enabling scalable and\nefficient task management for VEC systems.", "comment": "8 pages, 7 figures, Accepted to IEEE 16th International Conference on\n  Network of the Future (NoF) 2025", "pdf_url": "http://arxiv.org/pdf/2507.09341v1", "cate": "cs.NI", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.08947", "title": "A joint channel estimation and beamforming separation principle for massive MIMO systems", "authors": ["Lorenzo Miretti", "Slawomir Stańczak", "Giuseppe Caire"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08947v1", "summary": "We demonstrate that separating beamforming (i.e., downlink precoding and\nuplink combining) and channel estimation in multi-user MIMO wireless systems\nincurs no loss of optimality under general conditions that apply to a wide\nvariety of models in the literature, including canonical reciprocity-based\ncellular and cell-free massive MIMO system models. Specifically, we provide\nconditions under which optimal processing in terms of ergodic achievable rates\ncan be decomposed into minimum mean-square error (MMSE) channel estimation\nfollowed by MMSE beamforming, for both centralized and distributed\narchitectures. Applications of our results are illustrated in terms of concrete\nexamples and numerical simulations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08947v1", "cate": "cs.IT", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2502.17653", "title": "Formally-verified Security against Forgery of Remote Attestation using SSProve", "authors": ["Sara Zain", "Jannik Mähn", "Stefan Köpsell", "Sebastian Ertel"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.17653v2", "summary": "Remote attestation (RA) is the foundation for trusted execution environments\nin the cloud and trusted device driver onboarding in operating systems.\nHowever, RA misses a rigorous mechanized definition of its security properties\nin one of the strongest models, i.e., the semantic model. Such a mechanization\nrequires the concept of StateSeparating Proofs (SSP). However, SSP was only\nrecently implemented as a foundational framework in the Rocq Prover. Based on\nthis framework, this paper presents the first mechanized formalization of the\nfundamental security properties of RA. Our Rocq Prover development first\ndefines digital signatures and formally verifies security against forgery in\nthe strong existential attack model. Based on these results, we define RA and\nreduce the security of RA to the security of digital signatures. Our\ndevelopment provides evidence that the RA protocol is secure against forgery.\nAdditionally, we extend our reasoning to the primitives of RA and reduce their\nsecurity to the security of the primitives of the digital signatures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.17653v2", "cate": "cs.CR", "date": "2025-02-24", "updated": "2025-07-14"}
{"id": "2504.03723", "title": "VFlow: Discovering Optimal Agentic Workflows for Verilog Generation", "authors": ["Yangbo Wei", "Zhen Huang", "Huang Li", "Wei W. Xing", "Ting-Jung Lin", "Lei He"], "categories": ["cs.AR", "cs.MA"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "Comments:      6 pages", "url": "http://arxiv.org/abs/2504.03723v2", "summary": "Hardware design automation faces challenges in generating high-quality\nVerilog code efficiently. This paper introduces VFlow, an automated framework\nthat optimizes agentic workflows for Verilog code generation. Unlike\ntraditional approaches relying on fixed prompts or manually designed flows,\nVFlow treats workflow discovery as a search over graph-structured LLM\ninvocation sequences. It introduces a multi-population cooperative evolution\n(CEPE-MCTS) algorithm that balances multiple hardware objectives -- functional\ncorrectness, area, power, timing and token cost -- while sharing successful\npatterns and avoiding repeated failures. Integrated multi-level verification\nensures syntactic correctness, functional behavior, and synthesizability.\nExperiments on VerilogEval and RTLLM2.0 show VFlow improves pass@1 by 20--30\\%\nover prompting baselines and closely matches designer-level area/power.\nRemarkably, VFlow enables small LLMs to outperform larger models with up to\n10.9$\\times$ ROI, offering a cost-effective solution for RTL design. This work\npaves the way for intelligent, automated hardware development, advancing LLM\napplications in EDA.", "comment": "6 pages", "pdf_url": "http://arxiv.org/pdf/2504.03723v2", "cate": "cs.AR", "date": "2025-03-30", "updated": "2025-07-13"}
{"id": "2507.10003", "title": "Ariel Explores: Vision-based underwater exploration and inspection via generalist drone-level autonomy", "authors": ["Mohit Singh", "Mihir Dharmadhikari", "Kostas Alexis"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Presented at the 2025 IEEE ICRA Workshop on Field Robotics", "url": "http://arxiv.org/abs/2507.10003v1", "summary": "This work presents a vision-based underwater exploration and inspection\nautonomy solution integrated into Ariel, a custom vision-driven underwater\nrobot. Ariel carries a $5$ camera and IMU based sensing suite, enabling a\nrefraction-aware multi-camera visual-inertial state estimation method aided by\na learning-based proprioceptive robot velocity prediction method that enhances\nrobustness against visual degradation. Furthermore, our previously developed\nand extensively field-verified autonomous exploration and general visual\ninspection solution is integrated on Ariel, providing aerial drone-level\nautonomy underwater. The proposed system is field-tested in a submarine dry\ndock in Trondheim under challenging visual conditions. The field demonstration\nshows the robustness of the state estimation solution and the generalizability\nof the path planning techniques across robot embodiments.", "comment": "Presented at the 2025 IEEE ICRA Workshop on Field Robotics", "pdf_url": "http://arxiv.org/pdf/2507.10003v1", "cate": "cs.RO", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10427", "title": "Towards Emotion Co-regulation with LLM-powered Socially Assistive Robots: Integrating LLM Prompts and Robotic Behaviors to Support Parent-Neurodivergent Child Dyads", "authors": ["Jing Li", "Felix Schijve", "Sheng Li", "Yuye Yang", "Jun Hu", "Emilia Barakova"], "categories": ["cs.HC", "cs.RO"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Submission for the IROS 2025 conference", "url": "http://arxiv.org/abs/2507.10427v1", "summary": "Socially Assistive Robotics (SAR) has shown promise in supporting emotion\nregulation for neurodivergent children. Recently, there has been increasing\ninterest in leveraging advanced technologies to assist parents in co-regulating\nemotions with their children. However, limited research has explored the\nintegration of large language models (LLMs) with SAR to facilitate emotion\nco-regulation between parents and children with neurodevelopmental disorders.\nTo address this gap, we developed an LLM-powered social robot by deploying a\nspeech communication module on the MiRo-E robotic platform. This supervised\nautonomous system integrates LLM prompts and robotic behaviors to deliver\ntailored interventions for both parents and neurodivergent children. Pilot\ntests were conducted with two parent-child dyads, followed by a qualitative\nanalysis. The findings reveal MiRo-E's positive impacts on interaction dynamics\nand its potential to facilitate emotion regulation, along with identified\ndesign and technical challenges. Based on these insights, we provide design\nimplications to advance the future development of LLM-powered SAR for mental\nhealth applications.", "comment": "Submission for the IROS 2025 conference", "pdf_url": "http://arxiv.org/pdf/2507.10427v1", "cate": "cs.HC", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09742", "title": "Causality-informed Anomaly Detection in Partially Observable Sensor Networks: Moving beyond Correlations", "authors": ["Xiaofeng Xiao", "Bo Shen", "Xubo Yue"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09742v1", "summary": "Nowadays, as AI-driven manufacturing becomes increasingly popular, the volume\nof data streams requiring real-time monitoring continues to grow. However, due\nto limited resources, it is impractical to place sensors at every location to\ndetect unexpected shifts. Therefore, it is necessary to develop an optimal\nsensor placement strategy that enables partial observability of the system\nwhile detecting anomalies as quickly as possible. Numerous approaches have been\nproposed to address this challenge; however, most existing methods consider\nonly variable correlations and neglect a crucial factor: Causality. Moreover,\nalthough a few techniques incorporate causal analysis, they rely on\ninterventions-artificially creating anomalies-to identify causal effects, which\nis impractical and might lead to catastrophic losses. In this paper, we\nintroduce a causality-informed deep Q-network (Causal DQ) approach for\npartially observable sensor placement in anomaly detection. By integrating\ncausal information at each stage of Q-network training, our method achieves\nfaster convergence and tighter theoretical error bounds. Furthermore, the\ntrained causal-informed Q-network significantly reduces the detection time for\nanomalies under various settings, demonstrating its effectiveness for sensor\nplacement in large-scale, real-world data streams. Beyond the current\nimplementation, our technique's fundamental insights can be applied to various\nreinforcement learning problems, opening up new possibilities for real-world\ncausality-informed machine learning methods in engineering applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09742v1", "cate": "cs.AI", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09082", "title": "Taming generative video models for zero-shot optical flow extraction", "authors": ["Seungwoo Kim", "Khai Loong Aw", "Klemen Kotar", "Cristobal Eyzaguirre", "Wanhee Lee", "Yunong Liu", "Jared Watrous", "Stefan Stojanov", "Juan Carlos Niebles", "Jiajun Wu", "Daniel L. K. Yamins"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project webpage: this https URL", "url": "http://arxiv.org/abs/2507.09082v1", "summary": "Extracting optical flow from videos remains a core computer vision problem.\nMotivated by the success of large general-purpose models, we ask whether frozen\nself-supervised video models trained only for future frame prediction can be\nprompted, without fine-tuning, to output flow. Prior work reading out depth or\nillumination from video generators required fine-tuning, which is impractical\nfor flow where labels are scarce and synthetic datasets suffer from a\nsim-to-real gap. Inspired by the Counterfactual World Model (CWM) paradigm,\nwhich can obtain point-wise correspondences by injecting a small tracer\nperturbation into a next-frame predictor and tracking its propagation, we\nextend this idea to generative video models. We explore several popular\narchitectures and find that successful zero-shot flow extraction in this manner\nis aided by three model properties: (1) distributional prediction of future\nframes (avoiding blurry or noisy outputs); (2) factorized latents that treat\neach spatio-temporal patch independently; and (3) random-access decoding that\ncan condition on any subset of future pixels. These properties are uniquely\npresent in the recent Local Random Access Sequence (LRAS) architecture.\nBuilding on LRAS, we propose KL-tracing: a novel test-time procedure that\ninjects a localized perturbation into the first frame, rolls out the model one\nstep, and computes the Kullback-Leibler divergence between perturbed and\nunperturbed predictive distributions. Without any flow-specific fine-tuning,\nour method outperforms state-of-the-art models on real-world TAP-Vid DAVIS\ndataset (16.6% relative improvement for endpoint error) and synthetic TAP-Vid\nKubric (4.7% relative improvement). Our results indicate that counterfactual\nprompting of controllable generative video models is a scalable and effective\nalternative to supervised or photometric-loss approaches for high-quality flow.", "comment": "Project webpage: https://neuroailab.github.io/projects/kl_tracing", "pdf_url": "http://arxiv.org/pdf/2507.09082v1", "cate": "cs.CV", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09220", "title": "Explainability as a Compliance Requirement: What Regulated Industries Need from AI Tools for Design Artifact Generation", "authors": ["Syed Tauhid Ullah Shah", "Mohammad Hussein", "Ann Barcomb", "Mohammad Moshirpour"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09220v1", "summary": "Artificial Intelligence (AI) tools for automating design artifact generation\nare increasingly used in Requirements Engineering (RE) to transform textual\nrequirements into structured diagrams and models. While these AI tools,\nparticularly those based on Natural Language Processing (NLP), promise to\nimprove efficiency, their adoption remains limited in regulated industries\nwhere transparency and traceability are essential. In this paper, we\ninvestigate the explainability gap in AI-driven design artifact generation\nthrough semi-structured interviews with ten practitioners from safety-critical\nindustries. We examine how current AI-based tools are integrated into workflows\nand the challenges arising from their lack of explainability. We also explore\nmitigation strategies, their impact on project outcomes, and features needed to\nimprove usability. Our findings reveal that non-explainable AI outputs\nnecessitate extensive manual validation, reduce stakeholder trust, struggle to\nhandle domain-specific terminology, disrupt team collaboration, and introduce\nregulatory compliance risks, often negating the anticipated efficiency\nbenefits. To address these issues, we identify key improvements, including\nsource tracing, providing clear justifications for tool-generated decisions,\nsupporting domain-specific adaptation, and enabling compliance validation. This\nstudy outlines a practical roadmap for improving the transparency, reliability,\nand applicability of AI tools in requirements engineering workflows,\nparticularly in regulated and safety-critical environments where explainability\nis crucial for adoption and certification.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09220v1", "cate": "cs.SE", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.07660", "title": "Scalable Signed Exponential Random Graph Models under Local Dependence", "authors": ["Marc Schalberger", "Cornelius Fritz"], "categories": ["cs.SI", "stat.CO"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07660v2", "summary": "Traditional network analysis focuses on binary edges, while real-world\nrelationships are more nuanced, encompassing cooperation, neutrality, and\nconflict. The rise of negative edges in social media discussions spurred\ninterest in analyzing signed interactions, especially in polarized debates.\nHowever, the vast data generated by digital networks presents challenges for\ntraditional methods like Stochastic Block Models (SBM) and Exponential Family\nRandom Graph Models (ERGM), particularly due to the homogeneity assumption and\nglobal dependence, which become increasingly unrealistic as network size grows.\nTo address this, we propose a novel method that combines the strengths of SBM\nand ERGM while mitigating their weaknesses by incorporating local dependence\nbased on non-overlapping blocks. Our approach involves a two-step process:\nfirst, decomposing the network into sub-networks using SBM approximation, and\nthen estimating parameters using ERGM methods. We validate our method on large\nsynthetic networks and apply it to a signed Wikipedia network of thousands of\neditors. Through the use of local dependence, we find patterns consistent with\nstructural balance theory.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07660v2", "cate": "cs.SI", "date": "2025-07-10", "updated": "2025-07-13"}
{"id": "2507.09346", "title": "Fast and Adaptive Task Management in MEC: A Deep Learning Approach Using Pointer Networks", "authors": ["Arild Yonkeu", "Mohammadreza Amini", "Burak Kantarci"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      8 pages, 8 figures, Accepted to IEEE 16th International Conference on Network of the Future (NoF) 2025", "url": "http://arxiv.org/abs/2507.09346v1", "summary": "Task offloading and scheduling in Mobile Edge Computing (MEC) are vital for\nmeeting the low-latency demands of modern IoT and dynamic task scheduling\nscenarios. MEC reduces the processing burden on resource-constrained devices by\nenabling task execution at nearby edge servers. However, efficient task\nscheduling remains a challenge in dynamic, time-sensitive environments.\nConventional methods -- such as heuristic algorithms and mixed-integer\nprogramming -- suffer from high computational overhead, limiting their\nreal-time applicability. Existing deep learning (DL) approaches offer faster\ninference but often lack scalability and adaptability to dynamic workloads. To\naddress these issues, we propose a Pointer Network-based architecture for task\nscheduling in dynamic edge computing scenarios. Our model is trained on a\ngenerated synthetic dataset using genetic algorithms to determine the optimal\ntask ordering. Experimental results show that our model achieves lower drop\nratios and waiting times than baseline methods, and a soft sequence accuracy of\nup to 89.2%. Our model consistently achieves inference times under 2 seconds\nacross all evaluated task counts, whereas the integer and binary programming\napproaches require approximately up to 18 seconds and 90 seconds, respectively.\nIt also shows strong generalization across varying scenarios, and adaptability\nto real-time changes, offering a scalable and efficient solution for edge-based\ntask management.", "comment": "8 pages, 8 figures, Accepted to IEEE 16th International Conference on\n  Network of the Future (NoF) 2025", "pdf_url": "http://arxiv.org/pdf/2507.09346v1", "cate": "cs.NI", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09193", "title": "Fundamental Limits of Bistatic Integrated Sensing and Communications over Memoryless Relay Channels", "authors": ["Yao Liu", "Min Li", "Lawrence Ong", "Aylin Yener"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09193v1", "summary": "The problem of bistatic integrated sensing and communications over memoryless\nrelay channels is considered, where destination concurrently decodes the\nmessage sent by the source and estimates unknown parameters from received\nsignals with the help of a relay. A state-dependent discrete memoryless relay\nchannel is considered to model this setup, and the fundamental limits of the\ncommunication-sensing performance tradeoff are characterized by the\ncapacity-distortion function. An upper bound on the capacity-distortion\nfunction is derived, extending the cut-set bound results to address the sensing\noperation at the destination. A hybrid-partial-decode-and-compress-forward\ncoding scheme is also proposed to facilitate source-relay cooperation for both\nmessage transmission and sensing, establishing a lower bound on the\ncapacity-distortion function. It is found that the\nhybrid-partial-decode-and-compress-forward scheme achieves optimal sensing\nperformance when the communication task is ignored. Furthermore, the upper and\nlower bounds are shown to coincide for three specific classes of relay\nchannels. Numerical examples are provided to illustrate the\ncommunication-sensing tradeoff and demonstrate the benefits of integrated\ndesign.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09193v1", "cate": "cs.IT", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2502.19537", "title": "No, of Course I Can! Deeper Fine-Tuning Attacks That Bypass Token-Level Safety Mechanisms", "authors": ["Joshua Kazdan", "Abhay Puri", "Rylan Schaeffer", "Lisa Yu", "Chris Cundy", "Jason Stanley", "Sanmi Koyejo", "Krishnamurthy Dvijotham"], "categories": ["cs.CR", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.19537v5", "summary": "Leading language model (LM) providers like OpenAI and Anthropic allow\ncustomers to fine-tune frontier LMs for specific use cases. To prevent abuse,\nthese providers apply filters to block fine-tuning on overtly harmful data. In\nthis setting, we make three contributions: First, while past work has shown\nthat safety alignment is \"shallow\", we correspondingly demonstrate that\nexisting fine-tuning attacks are shallow -- attacks target only the first\nseveral tokens of the model response, and consequently can be blocked by\ngenerating the first several response tokens with an aligned model. Second, we\nconceptually illustrate how to make attacks deeper by introducing a new\nfine-tuning attack that trains models to first refuse harmful requests before\nanswering them; this \"refuse-then-comply\" strategy bypasses shallow defenses\nand produces harmful responses that evade output filters. Third, we demonstrate\nthe potency of our new fine-tuning attack by jailbreaking both open-source\nmodels equipped with defenses and production models, achieving attack success\nrates of 57% and 72% against GPT-4o and Claude Haiku, respectively. Our attack\nreceived a $2000 bug bounty from OpenAI and was acknowledged as a vulnerability\nby Anthropic. Our work undermines the notion that models are safe because they\ninitially refuse harmful requests and broadens awareness of the scope of\nattacks that face production fine-tuning APIs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.19537v5", "cate": "cs.CR", "date": "2025-02-26", "updated": "2025-07-12"}
{"id": "2507.10030", "title": "Finetuning Deep Reinforcement Learning Policies with Evolutionary Strategies for Control of Underactuated Robots", "authors": ["Marco Calì", "Alberto Sinigaglia", "Niccolò Turcato", "Ruggero Carli", "Gian Antonio Susto"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10030v1", "summary": "Deep Reinforcement Learning (RL) has emerged as a powerful method for\naddressing complex control problems, particularly those involving underactuated\nrobotic systems. However, in some cases, policies may require refinement to\nachieve optimal performance and robustness aligned with specific task\nobjectives. In this paper, we propose an approach for fine-tuning Deep RL\npolicies using Evolutionary Strategies (ES) to enhance control performance for\nunderactuated robots. Our method involves initially training an RL agent with\nSoft-Actor Critic (SAC) using a surrogate reward function designed to\napproximate complex specific scoring metrics. We subsequently refine this\nlearned policy through a zero-order optimization step employing the Separable\nNatural Evolution Strategy (SNES), directly targeting the original score.\nExperimental evaluations conducted in the context of the 2nd AI Olympics with\nRealAIGym at IROS 2024 demonstrate that our evolutionary fine-tuning\nsignificantly improves agent performance while maintaining high robustness. The\nresulting controllers outperform established baselines, achieving competitive\nscores for the competition tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10030v1", "cate": "cs.RO", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10469", "title": "An Empirical Evaluation of AI-Powered Non-Player Characters' Perceived Realism and Performance in Virtual Reality Environments", "authors": ["Mikko Korkiakoski", "Saeid Sheikhi", "Jesper Nyman", "Jussi Saariniemi", "Kalle Tapio", "Panos Kostakos"], "categories": ["cs.HC", "cs.AI", "cs.MM"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10469v1", "summary": "Advancements in artificial intelligence (AI) have significantly enhanced the\nrealism and interactivity of non-player characters (NPCs) in virtual reality\n(VR), creating more engaging and believable user experiences. This paper\nevaluates AI-driven NPCs within a VR interrogation simulator, focusing on their\nperceived realism, usability, and system performance. The simulator features\ntwo AI-powered NPCs, a suspect, and a partner, using GPT-4 Turbo to engage\nparticipants in a scenario to determine the suspect's guilt or innocence. A\nuser study with 18 participants assessed the system using the System Usability\nScale (SUS), Game Experience Questionnaire (GEQ), and a Virtual Agent\nBelievability Questionnaire, alongside latency measurements for speech-to-text\n(STT), text-to-speech (TTS), OpenAI GPT-4 Turbo, and overall (cycle) latency.\nResults showed an average cycle latency of 7 seconds, influenced by the\nincreasing conversational context. Believability scored 6.67 out of 10, with\nhigh ratings in behavior, social relationships, and intelligence but moderate\nscores in emotion and personality. The system achieved a SUS score of 79.44,\nindicating good usability. These findings demonstrate the potential of large\nlanguage models to improve NPC realism and interaction in VR while highlighting\nchallenges in reducing system latency and enhancing emotional depth. This\nresearch contributes to the development of more sophisticated AI-driven NPCs,\nrevealing the need for performance optimization to achieve increasingly\nimmersive virtual experiences.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10469v1", "cate": "cs.HC", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09751", "title": "Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations", "authors": ["Bradley P. Allen", "Prateek Chhikara", "Thomas Macaulay Ferguson", "Filip Ilievski", "Paul Groth"], "categories": ["cs.AI", "cs.CL", "cs.LO"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      29 pages, 9 tables, 3 figures. Accepted to the 19th Conference on Neurosymbolic Learning and Reasoning (NeSy 2025)", "url": "http://arxiv.org/abs/2507.09751v1", "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language understanding and generation, but they exhibit problems with\nlogical consistency in the output they generate. How can we harness LLMs'\nbroad-coverage parametric knowledge in formal reasoning despite their\ninconsistency? We present a method for directly integrating an LLM into the\ninterpretation function of the formal semantics for a paraconsistent logic. We\nprovide experimental evidence for the feasibility of the method by evaluating\nthe function using datasets created from several short-form factuality\nbenchmarks. Unlike prior work, our method offers a theoretical framework for\nneuro-symbolic reasoning that leverages an LLM's knowledge while preserving the\nunderlying logic's soundness and completeness properties.", "comment": "29 pages, 9 tables, 3 figures. Accepted to the 19th Conference on\n  Neurosymbolic Learning and Reasoning (NeSy 2025)", "pdf_url": "http://arxiv.org/pdf/2507.09751v1", "cate": "cs.AI", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09092", "title": "MI CAM: Mutual Information Weighted Activation Mapping for Causal Visual Explanations of Convolutional Neural Networks", "authors": ["Ram S Iyer", "Narayan S Iyer", "Rugmini Ammal P"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      12 pages, 10 figures", "url": "http://arxiv.org/abs/2507.09092v1", "summary": "With the intervention of machine vision in our crucial day to day necessities\nincluding healthcare and automated power plants, attention has been drawn to\nthe internal mechanisms of convolutional neural networks, and the reason why\nthe network provides specific inferences. This paper proposes a novel post-hoc\nvisual explanation method called MI CAM based on activation mapping. Differing\nfrom previous class activation mapping based approaches, MI CAM produces\nsaliency visualizations by weighing each feature map through its mutual\ninformation with the input image and the final result is generated by a linear\ncombination of weights and activation maps. It also adheres to producing causal\ninterpretations as validated with the help of counterfactual analysis. We aim\nto exhibit the visual performance and unbiased justifications for the model\ninferencing procedure achieved by MI CAM. Our approach works at par with all\nstate-of-the-art methods but particularly outperforms some in terms of\nqualitative and quantitative measures. The implementation of proposed method\ncan be found on https://anonymous.4open.science/r/MI-CAM-4D27", "comment": "12 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.09092v1", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09315", "title": "Enhancing Interpretability in Software Change Management with Chain-of-Thought Reasoning", "authors": ["Yongqian Sun", "Weihua Kuang", "Chao Shen", "Xidao Wen", "Tinghua Zheng", "Heng Liu", "Shenglin Zhang", "Bo Wu", "Dan Pei"], "categories": ["cs.SE", "cs.AI"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      22 pages, 19 figures", "url": "http://arxiv.org/abs/2507.09315v1", "summary": "In modern online services, frequent software changes introduce significant\nrisks. To tackle this challenge, we propose SCELM (Software Change Evaluation\nand Lifecycle Management), an end-to-end automated framework for software\nchange management. SCELM aims to manage software changes efficiently and\nprecisely, significantly reducing service failures and economic losses.", "comment": "22 pages, 19 figures", "pdf_url": "http://arxiv.org/pdf/2507.09315v1", "cate": "cs.SE", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2311.09536", "title": "Introduction to correlation networks: Interdisciplinary approaches beyond thresholding", "authors": ["Naoki Masuda", "Zachary M. Boyd", "Diego Garlaschelli", "Peter J. Mucha"], "categories": ["physics.soc-ph", "cs.SI"], "primary_category": "Subjects:       Physics and Society (physics.soc-ph)", "pdf_link": null, "comments": "Comments:      2 figures", "url": "http://arxiv.org/abs/2311.09536v3", "summary": "Many empirical networks originate from correlational data, arising in domains\nas diverse as psychology, neuroscience, genomics, microbiology, finance, and\nclimate science. Specialized algorithms and theory have been developed in\ndifferent application domains for working with such networks, as well as in\nstatistics, network science, and computer science, often with limited\ncommunication between practitioners in different fields. This leaves\nsignificant room for cross-pollination across disciplines. A central challenge\nis that it is not always clear how to best transform correlation matrix data\ninto networks for the application at hand, and probably the most widespread\nmethod, i.e., thresholding on the correlation value to create either unweighted\nor weighted networks, suffers from multiple problems. In this article, we\nreview various methods of constructing and analyzing correlation networks,\nranging from thresholding and its improvements to weighted networks,\nregularization, dynamic correlation networks, threshold-free approaches,\ncomparison with null models, and more. Finally, we propose and discuss\nrecommended practices and a variety of key open questions currently confronting\nthis field.", "comment": "2 figures", "pdf_url": "http://arxiv.org/pdf/2311.09536v3", "cate": "physics.soc-ph", "date": "2023-11-16", "updated": "2025-07-13"}
{"id": "2507.09352", "title": "Reliable Task Offloading in MEC through Transmission Diversity and Jamming-Aware Scheduling", "authors": ["Ghazal Asemian", "Mohammadreza Amini", "Burak Kantarci"], "categories": ["cs.NI", "eess.SP"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      5 pages, 2 figures, Accepted to IEEE 16th International Conference on Network of the Future (NoF) 2025", "url": "http://arxiv.org/abs/2507.09352v1", "summary": "Mobile Edge Computing (MEC) enables low-latency applications by bringing\ncomputation closer to the user, but dynamic task arrivals and communication\nthreats like jamming complicate reliable task offloading and resource\nallocation. In this paper, we formulate a dynamic MEC framework considering the\ntransmission diversity that jointly addresses task scheduling and resource\nblock (RB) assignment in the presence of jamming. First, we define and evaluate\nkey network metrics-including dropped task ratio and bandwidth\nutilization-while maintaining service continuity by accounting for the existing\ncommitments of the edge server to previously offloaded tasks. Then, we propose\na jamming-aware offloading and RB allocation framework that leverages\ntransmission diversity and optimal scheduling across distributed gNBs. The\nproposed solution is compared to a similar scenario without transmission\ndiversity and two baseline strategies of first-come-first-served (FCFS) and\nshortest task first (STF). The proposed algorithm effectively mitigates the\nimpact of jamming while enhancing resource utilization and minimizing task drop\nrates, making it highly suitable for mission-critical MEC applications. At\nsignal-to-jamming-and-noise ratio (SJNR) of 4 dB, the proposed method achieves\na $0.26$ task drop rate, outperforming the scenario without transmission\ndiversity with a task drop rate of 0.50 and STF and FCFS strategies with 0.52\nand 0.63 task drop rates, respectively.", "comment": "5 pages, 2 figures, Accepted to IEEE 16th International Conference on\n  Network of the Future (NoF) 2025", "pdf_url": "http://arxiv.org/pdf/2507.09352v1", "cate": "cs.NI", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09204", "title": "Data Fusion and Aggregation Methods to Develop Composite Indexes for a Sustainable Future", "authors": ["Abdullah Konak"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09204v1", "summary": "Research on environmental risk modeling relies on numerous indicators to\nquantify the magnitude and frequency of extreme climate events, their\necological, economic, and social impacts, and the coping mechanisms that can\nreduce or mitigate their adverse effects. Index-based approaches significantly\nsimplify the process of quantifying, comparing, and monitoring risks associated\nwith other natural hazards, as a large set of indicators can be condensed into\na few key performance indicators. Data fusion techniques are often used in\nconjunction with expert opinions to develop key performance indicators. This\npaper discusses alternative methods to combine data from multiple indicators,\nwith an emphasis on their use-case scenarios, underlying assumptions, data\nrequirements, advantages, and limitations. The paper demonstrates the\napplication of these data fusion methods through examples from current risk and\nresilience models and simplified datasets. Simulations are conducted to\nidentify their strengths and weaknesses under various scenarios. Finally, a\nreal-life example illustrates how these data fusion techniques can be applied\nto inform policy recommendations in the context of drought resilience and\nsustainability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09204v1", "cate": "cs.IT", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.08923", "title": "CEO-DC: An Actionable Framework to Close the Carbon Gap in HPC Data Centers", "authors": ["Rubén Rodríguez Álvarez", "Denisa-Andreea Constantinescu", "Miguel Peón-Quirós", "David Atienza"], "categories": ["cs.AR", "cs.CY", "cs.PF", "B.8.2; C.0; C.1.4; C.4; C.5.5; J.4; K.1; K.4.1; K.6.4"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "Comments:      15 pages, 11 figures, 2 tables", "url": "http://arxiv.org/abs/2507.08923v1", "summary": "The rapid expansion of data centers (DCs) to support large-scale AI and\nscientific workloads is driving unsustainable growth in energy consumption and\ngreenhouse gas emissions. While successive generations of hardware platforms\nhave improved performance and energy efficiency, the question remains whether\nnew, more efficient platforms can realistically offset the rising emissions\nassociated with increasing demand. Prior studies often overlook the complex\ntrade-offs in such transitions by failing to account for both the economic\nincentives and the projected compute demand growth over the operational\nlifetime of the devices. In response, we present CEO-DC, an integrated model\nand decision-making methodology for Carbon and Economy Optimization in Data\nCenters. CEO-DC models the competing forces of cost, carbon, and compute demand\nto guide optimal platform procurement and replacement strategies. We propose\nmetrics to steer procurement, platform design, and policy decisions toward\nsustainable DC technologies. Given current platform trends, our AI case study\nusing CEO-DC shows that upgrading legacy devices on a 4-year cycle reduces\ntotal emissions. However, these upgrades fail to scale with DC demand growth\ntrends without increasing total emissions in over 44% of cases, and require\neconomic incentives for adoption in over 72%. Furthermore, current carbon\nprices are insufficient to motivate upgrades in 9 out of the 14 countries with\nthe highest number of DCs globally. We also find that optimizing platforms for\nenergy efficiency at the expense of latency can increase the carbon price\nrequired to justify their adoption. In summary, CEO-DC provides actionable\ninsights for DC architects, platform designers, and policymakers by timing\nlegacy platform upgrades, constraining DC growth to sustainable levels,\noptimizing platform performance-to-cost ratios, and increasing incentives.", "comment": "15 pages, 11 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2507.08923v1", "cate": "cs.AR", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2502.21156", "title": "Cryptis: Cryptographic Reasoning in Separation Logic", "authors": ["Arthur Azevedo de Amorim", "Amal Ahmed", "Marco Gaboardi"], "categories": ["cs.CR", "cs.LO"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.21156v2", "summary": "We introduce Cryptis, an extension of the Iris separation logic that can be\nused to verify cryptographic components using the symbolic model of\ncryptography. The combination of separation logic and cryptographic reasoning\nallows us to prove the correctness of a protocol and later reuse this result to\nverify larger systems that rely on the protocol. To make this integration\npossible, we propose novel specifications for authentication protocols that\nallow agents in a network to agree on the use of system resources. We evaluate\nour approach by verifying various authentication protocols and a key-value\nstore server that uses these authentication protocols to connect to clients.\nOur results are formalized in Coq.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.21156v2", "cate": "cs.CR", "date": "2025-02-28", "updated": "2025-07-12"}
{"id": "2507.10047", "title": "MP-RBFN: Learning-based Vehicle Motion Primitives using Radial Basis Function Networks", "authors": ["Marc Kaufeld", "Mattia Piccinini", "Johannes Betz"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 pages, Submitted to the IEEE International Conference on Intelligent Transportation Systems (ITSC 2025), Australia", "url": "http://arxiv.org/abs/2507.10047v1", "summary": "This research introduces MP-RBFN, a novel formulation leveraging Radial Basis\nFunction Networks for efficiently learning Motion Primitives derived from\noptimal control problems for autonomous driving. While traditional motion\nplanning approaches based on optimization are highly accurate, they are often\ncomputationally prohibitive. In contrast, sampling-based methods demonstrate\nhigh performance but impose constraints on the geometric shape of trajectories.\nMP-RBFN combines the strengths of both by coupling the high-fidelity trajectory\ngeneration of sampling-based methods with an accurate description of vehicle\ndynamics. Empirical results show compelling performance compared to previous\nmethods, achieving a precise description of motion primitives at low inference\ntimes. MP-RBFN yields a seven times higher accuracy in generating optimized\nmotion primitives compared to existing semi-analytic approaches. We demonstrate\nthe practical applicability of MP-RBFN for motion planning by integrating the\nmethod into a sampling-based trajectory planner. MP-RBFN is available as\nopen-source software at https://github.com/TUM-AVS/RBFN-Motion-Primitives.", "comment": "8 pages, Submitted to the IEEE International Conference on\n  Intelligent Transportation Systems (ITSC 2025), Australia", "pdf_url": "http://arxiv.org/pdf/2507.10047v1", "cate": "cs.RO", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10479", "title": "VIP-Sim: A User-Centered Approach to Vision Impairment Simulation for Accessible Design", "authors": ["Max Rädler", "Mark Colley", "Enrico Rukzio"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Conditionally accepted at UIST'25", "url": "http://arxiv.org/abs/2507.10479v1", "summary": "People with vision impairments (VIPs) often rely on their remaining vision\nwhen interacting with user interfaces. Simulating visual impairments is an\neffective tool for designers, fostering awareness of the challenges faced by\nVIPs. While previous research has introduced various vision impairment\nsimulators, none have yet been developed with the direct involvement of VIPs or\nthoroughly evaluated from their perspective. To address this gap, we developed\nVIP-Sim. This symptom-based vision simulator was created through a\nparticipatory design process tailored explicitly for this purpose, involving\nN=7 VIPs. 21 symptoms, like field loss or light sensitivity, can be overlaid on\ndesktop design tools. Most participants felt VIP-Sim could replicate their\nsymptoms. VIP-Sim was received positively, but concerns about exclusion in\ndesign and comprehensiveness of the simulation remain, mainly whether it\nrepresents the experiences of other VIPs.", "comment": "Conditionally accepted at UIST'25", "pdf_url": "http://arxiv.org/pdf/2507.10479v1", "cate": "cs.HC", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09801", "title": "Technical Requirements for Halting Dangerous AI Activities", "authors": ["Peter Barnett", "Aaron Scher", "David Abecassis"], "categories": ["cs.AI", "cs.CY"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09801v1", "summary": "The rapid development of AI systems poses unprecedented risks, including loss\nof control, misuse, geopolitical instability, and concentration of power. To\nnavigate these risks and avoid worst-case outcomes, governments may proactively\nestablish the capability for a coordinated halt on dangerous AI development and\ndeployment. In this paper, we outline key technical interventions that could\nallow for a coordinated halt on dangerous AI activities. We discuss how these\ninterventions may contribute to restricting various dangerous AI activities,\nand show how these interventions can form the technical foundation for\npotential AI governance plans.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09801v1", "cate": "cs.AI", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09097", "title": "RadEyeVideo: Enhancing general-domain Large Vision Language Model for chest X-ray analysis with video representations of eye gaze", "authors": ["Yunsoo Kim", "Jinge Wu", "Honghan Wu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09097v1", "summary": "Large Vision-Language Models (LVLMs) have demonstrated promising performance\nin chest X-ray (CXR) analysis. To enhance human-computer interaction, several\nstudies have incorporated radiologists' eye gaze, typically through heatmaps or\ntextual prompts. However, these methods often overlook the sequential order of\neye movements, which could provide valuable insights by highlighting both the\nareas of interest and the order in which they are examined. In this work, we\npropose a novel approach called RadEyeVideo that integrates radiologists'\neye-fixation data as a video sequence, capturing both the temporal and spatial\ndynamics of their gaze. We evaluate this method in CXR report generation and\ndisease diagnosis using three general-domain, open-source LVLMs with video\ninput capabilities. When prompted with eye-gaze videos, model performance\nimproves by up to 24.6% in the report generation task and on average 15.2% for\nboth tasks using scaled evaluation metrics. Notably, RadEyeVideo enhanced an\nopen-domain LVLM model, LLaVA-OneVision, to surpass task-specific medical LVLMs\nsuch as MAIRA-2 and CheXagent, trained on large Chest X-ray data. This work\nhighlights that domain expert's knowledge (eye-gaze information in this case),\nwhen effectively integrated with LVLMs, can significantly enhance\ngeneral-domain models' capabilities in clinical tasks. RadEyeVideo is a step\ntoward a scalable human-centered approach of utilizing LVLMs in medical image\nanalytics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09097v1", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09414", "title": "Enhancing NeuroEvolution-Based Game Testing: A Branch Coverage Approach for Scratch Programs", "authors": ["Khizra Sohail", "Atif Aftab Ahmed Jilani", "Nigar Azhar Butt"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09414v1", "summary": "Automated test generation for game-like programs presents unique challenges\ndue to their non-deterministic behavior and complex control structures. The\nNEATEST framework has been used for automated testing in Scratch games,\nemploying neuroevolution-based test generation optimized for statement\ncoverage. However, statement coverage alone is often insufficient for fault\ndetection, as it does not guarantee execution of all logical branches. This\npaper introduces a branch coverage-based fitness function to enhance test\neffectiveness in automated game testing. We extend NEATEST by integrating a\nbranch fitness function that prioritizes control-dependent branches, guiding\nthe neuroevolution process to maximize branch exploration. To evaluate the\neffectiveness of this approach, empirical experiments were conducted on 25\nScratch games, comparing Neatest with Statement Coverage (NSC) against Neatest\nwith Branch Coverage (NBC). A mutation analysis was also performed to assess\nthe fault detection capabilities of both techniques. The results demonstrate\nthat NBC achieves higher branch coverage than NSC in 13 out of 25 games,\nparticularly in programs with complex conditional structures. Moreover, NBC\nachieves a lower false positive rate in mutation testing, making it a more\nreliable approach for identifying faulty behavior in game programs. These\nfindings confirm that branch coverage-based test generation improves test\ncoverage and fault detection in Scratch programs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09414v1", "cate": "cs.SE", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2504.12355", "title": "Leveraging Large Language Models for Multi-Class and Multi-Label Detection of Drug Use and Overdose Symptoms on Social Media", "authors": ["Muhammad Ahmad", "Fida Ullah", "Ummhy Habiba", "ldar Batyrshin", "Grigori Sidorov"], "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.12355v2", "summary": "Drug overdose remains a critical global health issue, often driven by misuse\nof opioids, painkillers, and psychiatric medications. Traditional research\nmethods face limitations, whereas social media offers real-time insights into\nself-reported substance use and overdose symptoms. This study proposes an\nAI-driven NLP framework trained on annotated social media data to detect\ncommonly used drugs and associated overdose symptoms. Using a hybrid annotation\nstrategy with LLMs and human annotators, we applied traditional ML models,\nneural networks, and advanced transformer-based models. Our framework achieved\n98% accuracy in multi-class and 97% in multi-label classification,\noutperforming baseline models by up to 8%. These findings highlight the\npotential of AI for supporting public health surveillance and personalized\nintervention strategies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.12355v2", "cate": "cs.CL", "date": "2025-04-16", "updated": "2025-07-14"}
{"id": "2507.09462", "title": "MobiWorld: World Models for Mobile Wireless Network", "authors": ["Haoye Chai", "Yuan Yuan", "Yong Li"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      7 pages, 6 figures", "url": "http://arxiv.org/abs/2507.09462v1", "summary": "Accurate modeling and simulation of mobile networks are essential for\nenabling intelligent and cost-effective network optimization. In this paper, we\npropose MobiWorld, a generative world model designed to support high-fidelity\nand flexible environment simulation for mobile network planning and\noptimization. Unlike traditional predictive models constrained by limited\ngeneralization capabilities, MobiWorld exhibits strong universality by\nintegrating heterogeneous data sources, including sensors, mobile devices, and\nbase stations, as well as multimodal data types such as sequences and images.\nIt is capable of generating both network element-level observations (e.g.,\ntraffic load, user distribution) and system-level performance indicators (e.g.,\nthroughput, energy consumption) to support a wide range of planning and\noptimization tasks. Built upon advanced diffusion models, MobiWorld offers\npowerful controllable generation capabilities by modeling the joint\ndistribution between mobile network data and diverse conditional factors\nincluding spatio temporal contexts, user behaviors, and optimization policies.\nThis enables accurate simulation of dynamic network states under varying policy\nconfigurations, providing optimization agents with precise environmental\nfeedback and facilitating effective decision-making without relying on costly\nreal-network interactions. We demonstrate the effectiveness of MobiWorld in a\ncollaborative energy-saving scenario, where an agent uses observations and\nrewards generated by MobiWorld to optimize base station sleep and user\noffloading policies. Experimental results show that MobiWorld exhibits strong\ncontrollable generation performance and outperforms traditional methods in\nenergy optimization.", "comment": "7 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.09462v1", "cate": "cs.NI", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09257", "title": "On Lattice Isomorphism Problems for Lattices from LCD Codes over Finite Rings", "authors": ["Yusaku Nishimura", "Katsuyuki Takashima", "Tsuyoshi Miezaki"], "categories": ["cs.IT", "math.CO", "math.IT", "Primary 11T71, Secondary 14G50"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      16 pages", "url": "http://arxiv.org/abs/2507.09257v1", "summary": "These days, post-quantum cryptography based on the lattice isomorphism\nproblem has been proposed. Ducas-Gibbons introduced the hull attack, which\nsolves the lattice isomorphism problem for lattices obtained by Construction A\nfrom an LCD code over a finite field. Using this attack, they showed that the\nlattice isomorphism problem for such lattices can be reduced to the lattice\nisomorphism problem with the trivial lattice $\\mathbb{Z}^n$ and the graph\nisomorphism problem. While the previous work by Ducas-Gibbons only considered\nlattices constructed by a code over a \\textit{finite field}, this paper\nconsiders lattices constructed by a code over a \\textit{finite ring}\n$\\mathbb{Z}/k\\mathbb{Z}$, which is a more general case. In particular, when $k$\nis odd, an odd prime power, or not divisible by $4$, we show that the lattice\nisomorphism problem can be reduced to the lattice isomorphism problem for\n$\\mathbb{Z}^n$ and the graph isomorphism problem.", "comment": "16 pages", "pdf_url": "http://arxiv.org/pdf/2507.09257v1", "cate": "cs.IT", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09010", "title": "Hybrid Systolic Array Accelerator with Optimized Dataflow for Edge Large Language Model Inference", "authors": ["Chun-Ting Chen", "HanGyeol Mun", "Jian Meng", "Mohamed S. Abdelfattah", "Jae-sun Seo"], "categories": ["cs.AR", "cs.AI"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "Comments:      Accepted as a conference paper at the 2025 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED)", "url": "http://arxiv.org/abs/2507.09010v1", "summary": "Edge inference for large language models (LLM) offers secure, low-latency,\nand cost-effective inference solutions. We emphasize that an edge accelerator\nshould achieve high area efficiency and minimize external memory access (EMA)\nduring the memory-bound decode stage, while maintaining high energy efficiency\nduring the compute intensive prefill stage. This paper proposes an edge LLM\ninference accelerator featuring a hybrid systolic array (HSA) architecture that\noptimizes inference efficiency in both stages. To further reduce EMA, we adopt\nMXINT4 weight quantization and propose an optimized dataflow tailored for HSA,\nensuring negligible dequantization overhead and achieving 100% hardware\nutilization with minimal accuracy loss under edge DRAM bandwidth constraints.\nFor non-linear operations, we incorporate optimized root mean square\nnormalization (RMSNorm) and rotary position embedding (RoPE) units, reducing\ntheir latency, area, and memory access overhead while enabling end-to-end\ninference on our accelerator. Our solution achieves 247/117 (token/s/mm2) while\nrunning a 1.3B LLM on long-input/long-output scenarios, providing >2.45x/13.5x\nimprovement over existing approaches, while maintaining superior energy\nefficiency in token generation.", "comment": "Accepted as a conference paper at the 2025 IEEE/ACM International\n  Symposium on Low Power Electronics and Design (ISLPED)", "pdf_url": "http://arxiv.org/pdf/2507.09010v1", "cate": "cs.AR", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2503.11897", "title": "PREAMBLE: Private and Efficient Aggregation via Block Sparse Vectors", "authors": ["Hilal Asi", "Vitaly Feldman", "Hannah Keller", "Guy N. Rothblum", "Kunal Talwar"], "categories": ["cs.CR", "cs.DS", "cs.LG"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.11897v2", "summary": "We revisit the problem of secure aggregation of high-dimensional vectors in a\ntwo-server system such as Prio. These systems are typically used to aggregate\nvectors such as gradients in private federated learning, where the aggregate\nitself is protected via noise addition to ensure differential privacy. Existing\napproaches require communication scaling with the dimensionality, and thus\nlimit the dimensionality of vectors one can efficiently process in this setup.\n  We propose PREAMBLE: {\\bf Pr}ivate {\\bf E}fficient {\\bf A}ggregation {\\bf\nM}echanism via {\\bf BL}ock-sparse {\\bf E}uclidean Vectors. PREAMBLE builds on\nan extension of distributed point functions that enables communication- and\ncomputation-efficient aggregation of {\\em block-sparse vectors}, which are\nsparse vectors where the non-zero entries occur in a small number of clusters\nof consecutive coordinates. We show that these block-sparse DPFs can be\ncombined with random sampling and privacy amplification by sampling results, to\nallow asymptotically optimal privacy-utility trade-offs for vector aggregation,\nat a fraction of the communication cost. When coupled with recent advances in\nnumerical privacy accounting, our approach incurs a negligible overhead in\nnoise variance, compared to the Gaussian mechanism used with Prio.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.11897v2", "cate": "cs.CR", "date": "2025-03-14", "updated": "2025-07-11"}
{"id": "2507.10055", "title": "Hand Gesture Recognition for Collaborative Robots Using Lightweight Deep Learning in Real-Time Robotic Systems", "authors": ["Muhtadin", "I Wayan Agus Darmawan", "Muhammad Hilmi Rusydiansyah", "I Ketut Eddy Purnama", "Chastine Fatichah", "Mauridhi Hery Purnomo"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10055v1", "summary": "Direct and natural interaction is essential for intuitive human-robot\ncollaboration, eliminating the need for additional devices such as joysticks,\ntablets, or wearable sensors. In this paper, we present a lightweight deep\nlearning-based hand gesture recognition system that enables humans to control\ncollaborative robots naturally and efficiently. This model recognizes eight\ndistinct hand gestures with only 1,103 parameters and a compact size of 22 KB,\nachieving an accuracy of 93.5%. To further optimize the model for real-world\ndeployment on edge devices, we applied quantization and pruning using\nTensorFlow Lite, reducing the final model size to just 7 KB. The system was\nsuccessfully implemented and tested on a Universal Robot UR5 collaborative\nrobot within a real-time robotic framework based on ROS2. The results\ndemonstrate that even extremely lightweight models can deliver accurate and\nresponsive hand gesture-based control for collaborative robots, opening new\npossibilities for natural human-robot interaction in constrained environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10055v1", "cate": "cs.RO", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2408.10887", "title": "A Mini-Review on Mobile Manipulators with Variable Autonomy", "authors": ["Cesar Alan Contreras", "Alireza Rastegarpanah", "Rustam Stolkin", "Manolis Chiou"], "categories": ["cs.RO", "cs.HC"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Presented at Variable Autonomy for Human-Robot Teaming (VAT) at IEEE RO-MAN 2024 Workshop", "url": "http://arxiv.org/abs/2408.10887v1", "summary": "This paper presents a mini-review of the current state of research in mobile\nmanipulators with variable levels of autonomy, emphasizing their associated\nchallenges and application environments. The need for mobile manipulators in\ndifferent environments is evident due to the unique challenges and risks each\npresents. Many systems deployed in these environments are not fully autonomous,\nrequiring human-robot teaming to ensure safe and reliable operations under\nuncertainties. Through this analysis, we identify gaps and challenges in the\nliterature on Variable Autonomy, including cognitive workload and communication\ndelays, and propose future directions, including whole-body Variable Autonomy\nfor mobile manipulators, virtual reality frameworks, and large language models\nto reduce operators' complexity and cognitive load in some challenging and\nuncertain scenarios.", "comment": "Presented at Variable Autonomy for Human-Robot Teaming (VAT) at IEEE\n  RO-MAN 2024 Workshop", "pdf_url": "http://arxiv.org/pdf/2408.10887v1", "cate": "cs.RO", "date": "2024-08-20", "updated": "2024-08-20"}
{"id": "2507.09850", "title": "Is Human-Written Data Enough? The Challenge of Teaching Reasoning to LLMs Without RL or Distillation", "authors": ["Wei Du", "Branislav Kisacanin", "George Armstrong", "Shubham Toshniwal", "Ivan Moshkov", "Alexan Ayrapetyan", "Sadegh Mahdavi", "Dan Zhao", "Shizhe Diao", "Dragan Masulovic", "Marius Stanean", "Advaith Avadhanam", "Max Wang", "Ashmit Dutta", "Shitij Govil", "Sri Yanamandara", "Mihir Tandon", "Sriram Ananthakrishnan", "Vedant Rathi", "David Zhang", "Joonseok Kang", "Leon Luo", "Titu Andreescu", "Boris Ginsburg", "Igor Gitman"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Accepted at the Second AI for Math Workshop at the 42nd International Conference on Machine Learning (ICML 2025)", "url": "http://arxiv.org/abs/2507.09850v1", "summary": "Reasoning-capable language models achieve state-of-the-art performance in\ndiverse complex tasks by generating long, explicit Chain-of-Thought (CoT)\ntraces. While recent works show that base models can acquire such reasoning\ntraces via reinforcement learning or distillation from stronger models like\nDeepSeek-R1, previous works demonstrate that even short CoT prompting without\nfine-tuning is able to improve reasoning. We ask whether long CoT can be\ninduced in a base model using only prompting or minimal tuning. Using just 20\nlong CoT examples from the reasoning model \\texttt{QwQ-32B-Preview}, we lightly\nfine-tune the base model \\texttt{Qwen2.5-32B}. The resulting model outperforms\nthe much larger \\texttt{Qwen2.5-Math-72B-Instruct}, showing that a handful of\nhigh-quality examples can unlock strong reasoning capabilities. We further\nexplore using CoT data from non-reasoning models and human annotators, enhanced\nwith prompt engineering, multi-pass editing, and structural guidance. However,\nneither matches the performance of reasoning model traces, suggesting that\ncertain latent qualities of expert CoT are difficult to replicate. We analyze\nkey properties of reasoning data, such as problem difficulty, diversity, and\nanswer length, that influence reasoning distillation. While challenges remain,\nwe are optimistic that carefully curated human-written CoT, even in small\nquantities, can activate reasoning behaviors in base models. We release our\nhuman-authored dataset across refinement stages and invite further\ninvestigation into what makes small-scale reasoning supervision so effective.", "comment": "Accepted at the Second AI for Math Workshop at the 42nd International\n  Conference on Machine Learning (ICML 2025)", "pdf_url": "http://arxiv.org/pdf/2507.09850v1", "cate": "cs.AI", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09102", "title": "Harnessing Text-to-Image Diffusion Models for Point Cloud Self-Supervised Learning", "authors": ["Yiyang Chen", "Shanshan Zhao", "Lunhao Duan", "Changxing Ding", "Dacheng Tao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2507.09102v1", "summary": "Diffusion-based models, widely used in text-to-image generation, have proven\neffective in 2D representation learning. Recently, this framework has been\nextended to 3D self-supervised learning by constructing a conditional point\ngenerator for enhancing 3D representations. However, its performance remains\nconstrained by the 3D diffusion model, which is trained on the available 3D\ndatasets with limited size. We hypothesize that the robust capabilities of\ntext-to-image diffusion models, particularly Stable Diffusion (SD), which is\ntrained on large-scale datasets, can help overcome these limitations. To\ninvestigate this hypothesis, we propose PointSD, a framework that leverages the\nSD model for 3D self-supervised learning. By replacing the SD model's text\nencoder with a 3D encoder, we train a point-to-image diffusion model that\nallows point clouds to guide the denoising of rendered noisy images. With the\ntrained point-to-image diffusion model, we use noise-free images as the input\nand point clouds as the condition to extract SD features. Next, we train a 3D\nbackbone by aligning its features with these SD features, thereby facilitating\ndirect semantic learning. Comprehensive experiments on downstream point cloud\ntasks and ablation studies demonstrate that the SD model can enhance point\ncloud self-supervised learning. Code is publicly available at\nhttps://github.com/wdttt/PointSD.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.09102v1", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09481", "title": "Evaluating LLMs on Sequential API Call Through Automated Test Generation", "authors": ["Yuheng Huang", "Da Song", "Zhenlan Ji", "Shuai Wang", "Lei Ma"], "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09481v1", "summary": "By integrating tools from external APIs, Large Language Models (LLMs) have\nexpanded their promising capabilities in a diverse spectrum of complex\nreal-world tasks. However, testing, evaluation, and analysis of LLM tool use\nremain in their early stages. Most existing benchmarks rely on manually\ncollected test cases, many of which cannot be automatically checked for\nsemantic correctness and instead depend on static methods such as string\nmatching. Additionally, these benchmarks often overlook the complex\ninteractions that occur between sequential API calls, which are common in\nreal-world applications. To fill the gap, in this paper, we introduce StateGen,\nan automated framework designed to generate diverse coding tasks involving\nsequential API interactions. StateGen combines state-machine-based API\nconstraint solving and validation, energy-based sampling, and control-flow\ninjection to generate executable programs. These programs are then translated\ninto human-like natural language task descriptions through a collaboration of\ntwo LLM agents. Utilizing StateGen, we construct StateEval, a benchmark\nencompassing 120 verified test cases spanning across three representative\nscenarios: Session Service, Tensor Operation, and ElevenLabs MCP. Experimental\nresults confirm that StateGen can effectively generate challenging and\nrealistic API-oriented tasks, highlighting areas for improvement in current\nLLMs incorporating APIs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09481v1", "cate": "cs.SE", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2506.23978", "title": "LLM Agents Are the Antidote to Walled Gardens", "authors": ["Samuele Marro", "Philip Torr"], "categories": ["cs.LG", "cs.CL", "cs.CY", "cs.SI", "68T50, 68M10, 91B26", "I.2.11; I.2.7; H.4.5"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.23978v2", "summary": "While the Internet's core infrastructure was designed to be open and\nuniversal, today's application layer is dominated by closed, proprietary\nplatforms. Open and interoperable APIs require significant investment, and\nmarket leaders have little incentive to enable data exchange that could erode\ntheir user lock-in. We argue that LLM-based agents fundamentally disrupt this\nstatus quo. Agents can automatically translate between data formats and\ninteract with interfaces designed for humans: this makes interoperability\ndramatically cheaper and effectively unavoidable. We name this shift universal\ninteroperability: the ability for any two digital services to exchange data\nseamlessly using AI-mediated adapters. Universal interoperability undermines\nmonopolistic behaviours and promotes data portability. However, it can also\nlead to new security risks and technical debt. Our position is that the ML\ncommunity should embrace this development while building the appropriate\nframeworks to mitigate the downsides. By acting now, we can harness AI to\nrestore user freedom and competitive markets without sacrificing security.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23978v2", "cate": "cs.LG", "date": "2025-06-30", "updated": "2025-07-12"}
{"id": "2507.09613", "title": "Wi-Fi: Twenty-Five Years and Counting", "authors": ["Giovanni Geraci", "Francesca Meneghello", "Francesc Wilhelmi", "David Lopez-Perez", "Iñaki Val", "Lorenzo Galati Giordano", "Carlos Cordeiro", "Monisha Ghosh", "Edward Knightly", "Boris Bellalta"], "categories": ["cs.NI", "cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      39 pages, 28 figures, 3 tables", "url": "http://arxiv.org/abs/2507.09613v1", "summary": "Today, Wi-Fi is over 25 years old. Yet, despite sharing the same branding\nname, today's Wi-Fi boasts entirely new capabilities that were not even on the\nroadmap 25 years ago. This article aims to provide a holistic and comprehensive\ntechnical and historical tutorial on Wi-Fi, beginning with IEEE 802.11b (Wi-Fi\n1) and looking forward to IEEE 802.11bn (Wi-Fi 8). This is the first tutorial\narticle to span these eight generations. Rather than a generation-by-generation\nexposition, we describe the key mechanisms that have advanced Wi-Fi. We begin\nby discussing spectrum allocation and coexistence, and detailing the IEEE\n802.11 standardization cycle. Second, we provide an overview of the physical\nlayer and describe key elements that have enabled data rates to increase by\nover 1,000x. Third, we describe how Wi-Fi Medium Access Control has been\nenhanced from the original Distributed Coordination Function to now include\ncapabilities spanning from frame aggregation to wideband spectrum access.\nFourth, we describe how Wi-Fi 5 first broke the one-user-at-a-time paradigm and\nintroduced multi-user access. Fifth, given the increasing use of mobile,\nbattery-powered devices, we describe Wi-Fi's energy-saving mechanisms over the\ngenerations. Sixth, we discuss how Wi-Fi was enhanced to seamlessly aggregate\nspectrum across 2.4 GHz, 5 GHz, and 6 GHz bands to improve throughput,\nreliability, and latency. Finally, we describe how Wi-Fi enables nearby Access\nPoints to coordinate in order to improve performance and efficiency. In the\nAppendix, we further discuss Wi-Fi developments beyond 802.11bn, including\nintegrated mmWave operations, sensing, security and privacy extensions, and the\nadoption of AI/ML.", "comment": "39 pages, 28 figures, 3 tables", "pdf_url": "http://arxiv.org/pdf/2507.09613v1", "cate": "cs.NI", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09290", "title": "Asymptotically optimal cyclic subspace codes", "authors": ["Chiara Castello", "Paolo Santonastaso"], "categories": ["cs.IT", "math.CO", "math.IT", "11T71, 11T99, 94B99"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09290v1", "summary": "Subspace codes, and in particular cyclic subspace codes, have gained\nsignificant attention in recent years due to their applications in error\ncorrection for random network coding. In this paper, we introduce a new\ntechnique for constructing cyclic subspace codes with large cardinality and\nprescribed minimum distance. Using this new method, we provide new\nconstructions of cyclic subspace codes in the Grassmannian $\\mathcal{G}_q(n,k)$\nof all $k$-dimensional $\\mathbb{F}_q$-subspaces of an $n$-dimensional vector\nspace over $\\mathbb{F}_q$, when $k\\mid n$ and $n/k$ is a composite number, with\nminimum distance $2k-2$ and large size. We prove that the resulting codes have\nsizes larger than those obtained from previously known constructions with the\nsame parameters. Furthermore, we show that our constructions of cyclic subspace\ncodes asymptotically reach the Johnson type bound II for infinite values of\n$n/k$.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09290v1", "cate": "cs.IT", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09201", "title": "SLIM: A Heterogeneous Accelerator for Edge Inference of Sparse Large Language Model via Adaptive Thresholding", "authors": ["Weihong Xu", "Haein Choi", "Po-kai Hsu", "Shimeng Yu", "Tajana Rosing"], "categories": ["cs.AR", "cs.DC"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09201v1", "summary": "Large language models (LLMs) have demonstrated exceptional proficiency in\nunderstanding and generating human language, but efficient inference on\nresource-constrained embedded devices remains challenging due to large model\nsizes and memory-intensive operations in feedforward network (FFN) and\nmulti-head attention (MHA) layers. While existing accelerators offload LLM\ninference to expensive heterogeneous computing systems, they fail to exploit\nthe significant sparsity inherent in LLM operations, leaving hardware resources\nunderutilized. We propose SLIM, an algorithm-hardware co-design optimized for\nsparse LLM serving on edge devices. SLIM exploits LLM sparsity through an\nadaptive thresholding algorithm that enables runtime-configurable sparsity with\nnegligible accuracy loss, fetching only activated neurons to dramatically\nreduce data movement. Our heterogeneous hardware architecture strategically\ncombines near-storage processing (NSP) and processing-in-memory (PIM): FFN\nweights are stored in high-density 3D NAND and computed using NSP units, while\nmemory-intensive MHA operations are processed in PIM modules. This design\nsignificantly reduces memory footprint, data movement, and energy consumption.\nOur comprehensive evaluation demonstrates SLIM's effectiveness, achieving\n13-18x throughput improvements over SSD-GPU systems and 9-10x better energy\nefficiency over DRAM-GPU systems while maintaining low latency, making\ncost-effective LLM deployment viable for edge computing environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09201v1", "cate": "cs.AR", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2504.11168", "title": "Bypassing LLM Guardrails: An Empirical Analysis of Evasion Attacks against Prompt Injection and Jailbreak Detection Systems", "authors": ["William Hackett", "Lewis Birch", "Stefan Trawicki", "Neeraj Suri", "Peter Garraghan"], "categories": ["cs.CR", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      14 pages, 5 figures, 11 tables. To be published in LLMSec 2025", "url": "http://arxiv.org/abs/2504.11168v3", "summary": "Large Language Models (LLMs) guardrail systems are designed to protect\nagainst prompt injection and jailbreak attacks. However, they remain vulnerable\nto evasion techniques. We demonstrate two approaches for bypassing LLM prompt\ninjection and jailbreak detection systems via traditional character injection\nmethods and algorithmic Adversarial Machine Learning (AML) evasion techniques.\nThrough testing against six prominent protection systems, including Microsoft's\nAzure Prompt Shield and Meta's Prompt Guard, we show that both methods can be\nused to evade detection while maintaining adversarial utility achieving in some\ninstances up to 100% evasion success. Furthermore, we demonstrate that\nadversaries can enhance Attack Success Rates (ASR) against black-box targets by\nleveraging word importance ranking computed by offline white-box models. Our\nfindings reveal vulnerabilities within current LLM protection mechanisms and\nhighlight the need for more robust guardrail systems.", "comment": "14 pages, 5 figures, 11 tables. To be published in LLMSec 2025", "pdf_url": "http://arxiv.org/pdf/2504.11168v3", "cate": "cs.CR", "date": "2025-04-15", "updated": "2025-07-14"}
{"id": "2507.10075", "title": "TGLD: A Trust-Aware Game-Theoretic Lane-Changing Decision Framework for Automated Vehicles in Heterogeneous Traffic", "authors": ["Jie Pan", "Tianyi Wang", "Yangyang Wang", "Junfeng Jiao", "Christian Claudel"], "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      6 pages, 7 figures, accepted for IEEE International Conference on Intelligent Transportation Systems (ITSC) 2025", "url": "http://arxiv.org/abs/2507.10075v1", "summary": "Automated vehicles (AVs) face a critical need to adopt socially compatible\nbehaviors and cooperate effectively with human-driven vehicles (HVs) in\nheterogeneous traffic environment. However, most existing lane-changing\nframeworks overlook HVs' dynamic trust levels, limiting their ability to\naccurately predict human driver behaviors. To address this gap, this study\nproposes a trust-aware game-theoretic lane-changing decision (TGLD) framework.\nFirst, we formulate a multi-vehicle coalition game, incorporating fully\ncooperative interactions among AVs and partially cooperative behaviors from HVs\ninformed by real-time trust evaluations. Second, we develop an online trust\nevaluation method to dynamically estimate HVs' trust levels during\nlane-changing interactions, guiding AVs to select context-appropriate\ncooperative maneuvers. Lastly, social compatibility objectives are considered\nby minimizing disruption to surrounding vehicles and enhancing the\npredictability of AV behaviors, thereby ensuring human-friendly and\ncontext-adaptive lane-changing strategies. A human-in-the-loop experiment\nconducted in a highway on-ramp merging scenario validates our TGLD approach.\nResults show that AVs can effectively adjust strategies according to different\nHVs' trust levels and driving styles. Moreover, incorporating a trust mechanism\nsignificantly improves lane-changing efficiency, maintains safety, and\ncontributes to transparent and adaptive AV-HV interactions.", "comment": "6 pages, 7 figures, accepted for IEEE International Conference on\n  Intelligent Transportation Systems (ITSC) 2025", "pdf_url": "http://arxiv.org/pdf/2507.10075v1", "cate": "cs.RO", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09111", "title": "RoHOI: Robustness Benchmark for Human-Object Interaction Detection", "authors": ["Di Wen", "Kunyu Peng", "Kailun Yang", "Yufan Chen", "Ruiping Liu", "Junwei Zheng", "Alina Roitberg", "Rainer Stiefelhagen"], "categories": ["cs.CV", "cs.HC", "cs.RO", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Benchmarks, datasets, and code will be made publicly available at this https URL", "url": "http://arxiv.org/abs/2507.09111v1", "summary": "Human-Object Interaction (HOI) detection is crucial for robot-human\nassistance, enabling context-aware support. However, models trained on clean\ndatasets degrade in real-world conditions due to unforeseen corruptions,\nleading to inaccurate prediction. To address this, we introduce the first\nrobustness benchmark for HOI detection, evaluating model resilience under\ndiverse challenges. Despite advances, current models struggle with\nenvironmental variability, occlusion, and noise. Our benchmark, RoHOI, includes\n20 corruption types based on HICO-DET and V-COCO datasets and a new\nrobustness-focused metric. We systematically analyze existing models in the\nrelated field, revealing significant performance drops under corruptions. To\nimprove robustness, we propose a Semantic-Aware Masking-based Progressive\nLearning (SAMPL) strategy to guide the model to be optimized based on holistic\nand partial cues, dynamically adjusting the model's optimization to enhance\nrobust feature learning. Extensive experiments show our approach outperforms\nstate-of-the-art methods, setting a new standard for robust HOI detection.\nBenchmarks, datasets, and code will be made publicly available at\nhttps://github.com/Kratos-Wen/RoHOI.", "comment": "Benchmarks, datasets, and code will be made publicly available at\n  https://github.com/Kratos-Wen/RoHOI", "pdf_url": "http://arxiv.org/pdf/2507.09111v1", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09854", "title": "Model-Grounded Symbolic Artificial Intelligence Systems Learning and Reasoning with Model-Grounded Symbolic Artificial Intelligence Systems", "authors": ["Aniruddha Chattopadhyay", "Raj Dandekar", "Kaushik Roy"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Accepted as paper in 19th International Conference on Neurosymbolic Learning and Reasoning,NeSy 2025", "url": "http://arxiv.org/abs/2507.09854v1", "summary": "Neurosymbolic artificial intelligence (AI) systems combine neural network and\nclassical symbolic AI mechanisms to exploit the complementary strengths of\nlarge scale, generalizable learning and robust, verifiable reasoning. Numerous\nclassifications of neurosymbolic AI illustrate how these two components can be\nintegrated in distinctly different ways. In this work, we propose\nreinterpreting instruction tuned large language models as model grounded\nsymbolic AI systems where natural language serves as the symbolic layer and\ngrounding is achieved through the models internal representation space. Within\nthis framework, we investigate and develop novel learning and reasoning\napproaches that preserve structural similarities to traditional learning and\nreasoning paradigms. Preliminary evaluations across axiomatic deductive\nreasoning procedures of varying complexity provide insights into the\neffectiveness of our approach in improving learning efficiency and reasoning\nreliability.", "comment": "Accepted as paper in 19th International Conference on Neurosymbolic\n  Learning and Reasoning,NeSy 2025", "pdf_url": "http://arxiv.org/pdf/2507.09854v1", "cate": "cs.AI", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09105", "title": "Hybrid Autoregressive-Diffusion Model for Real-Time Streaming Sign Language Production", "authors": ["Maoxiao Ye", "Xinfeng Ye", "Mano Manoharan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09105v1", "summary": "Earlier Sign Language Production (SLP) models typically relied on\nautoregressive methods that generate output tokens one by one, which inherently\nprovide temporal alignment. Although techniques like Teacher Forcing can\nprevent model collapse during training, they still cannot solve the problem of\nerror accumulation during inference, since ground truth is unavailable at that\nstage. In contrast, more recent approaches based on diffusion models leverage\nstep-by-step denoising to enable high-quality generation. However, the\niterative nature of these models and the requirement to denoise entire\nsequences limit their applicability in real-time tasks like SLP. To address it,\nwe apply a hybrid approach combining autoregressive and diffusion models to SLP\nfor the first time, leveraging the strengths of both models in sequential\ndependency modeling and output refinement. To capture fine-grained body\nmovements, we design a Multi-Scale Pose Representation module that separately\nextracts detailed features from distinct articulators and integrates them via a\nMulti-Scale Fusion module. Furthermore, we introduce a Confidence-Aware Causal\nAttention mechanism that utilizes joint-level confidence scores to dynamically\nguide the pose generation process, improving accuracy and robustness. Extensive\nexperiments on the PHOENIX14T and How2Sign datasets demonstrate the\neffectiveness of our method in both generation quality and real-time streaming\nefficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09105v1", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09490", "title": "Towards LLM-Based Automatic Playtest", "authors": ["Yan Zhao", "Chiwei Tang"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09490v1", "summary": "Playtesting is the process in which people play a video game for testing. It\nis critical for the quality assurance of gaming software. Manual playtesting is\ntime-consuming and expensive. However, automating this process is challenging,\nas playtesting typically requires domain knowledge and problem-solving skills\nthat most conventional testing tools lack. Recent advancements in artificial\nintelligence (AI) have opened up new possibilities for applying Large Language\nModels (LLMs) to playtesting. However, significant challenges remain: current\nLLMs cannot visually perceive game environments, and most existing research\nfocuses on text-based games or games with robust APIs. Many non-text games lack\nAPIs to provide textual descriptions of game states, making it almost\nimpossible to naively apply LLMs for playtesting. This paper introduces Lap,\nour novel approach to LLM-based Automatic Playtesting, which uses ChatGPT to\ntest match-3 games, a category of games where players match three or more\nidentical tiles in a row or column to earn points. Lap encompasses three key\nphases: processing of game environments, prompting-based action generation, and\naction execution. Given a match-3 game, Lap takes a snapshot of the game board\nand converts it to a numeric matrix. It then prompts the ChatGPT-O1-mini API to\nsuggest moves based on that matrix and tentatively applies the suggested moves\nto earn points and trigger changes in the game board. It repeats the\nabove-mentioned three steps iteratively until timeout. For evaluation, we\nconducted a case study using Lap on an open-source match-3 game, CasseBonbons,\nand empirically compared it with three existing tools. Our results are\npromising: Lap outperformed existing tools by achieving higher code coverage\nand triggering more program crashes. This research sheds light on the future of\nautomatic testing and LLM applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09490v1", "cate": "cs.SE", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09798", "title": "Towards Robust RTC in Sparse LEO Constellations", "authors": ["Aashish Gottipati", "Lili Qiu"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      8 pages, 5 figures", "url": "http://arxiv.org/abs/2507.09798v1", "summary": "Google's congestion control (GCC) has become a cornerstone for real-time\nvideo and audio communication, yet its performance remains fragile in emerging\nLow Earth Orbit (LEO) networks. Sparse direct-to-device constellations offer\nlonger duration links and reduced handover frequency compared to dense\ndeployments, presenting a unique opportunity for high-quality real-time\ncommunication (RTC) in environments with limited terrestrial network\ninfrastructure. In this paper, we study the behavior of videoconferencing\nsystems in sparse LEO constellations. We observe that video quality degrades\ndue to inherent delays and network instability introduced by the high altitude\nand rapid movement of LEO satellites, with these effects exacerbated by\nWebRTC's conventional ``one-size-fits-all'' sender-side pacing queue\nmanagement. To boost RTC performance, we introduce a data-driven queue\nmanagement mechanism that adapts the maximum pacing queue capacity based on\npredicted handover activity. Specifically, our approach employs shorter queue\nlimits during stable, no-handover phases to prioritize low latency\ncommunication, and preemptively increases pacing queue capacity when entering\nperiods of increased handover activity to absorb disruptions. Our method yields\nup to $3$x improvements in video bitrate and reduces freeze rate by $62\\%$\ncompared to default WebRTC.", "comment": "8 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.09798v1", "cate": "cs.NI", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09425", "title": "Joint Access Point Activation and Power Allocation for Cell-Free Massive MIMO Aided ISAC Systems", "authors": ["Nguyen Xuan Tung", "Le Tung Giang", "Trinh Van Chien", "Hoang Trong Minh", "Lajos Hanzo"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      6 pages, 4 figures, 2 tables. Accepted by IEEE TVT", "url": "http://arxiv.org/abs/2507.09425v1", "summary": "Cell-free massive multiple-input multiple-output (MIMO)-aided integrated\nsensing and communication (ISAC) systems are investigated where distributed\naccess points jointly serve users and sensing targets. We demonstrate that only\na subset of access points (APs) has to be activated for both tasks, while\ndeactivating redundant APs is essential for power savings. This motivates joint\nactive AP selection and power control for optimizing energy efficiency. The\nresultant problem is a mixed-integer nonlinear program (MINLP). To address\nthis, we propose a model-based Branch-and-Bound approach as a strong baseline\nto guide a semi-supervised heterogeneous graph neural network (HetGNN) for\nselecting the best active APs and the power allocation. Comprehensive numerical\nresults demonstrate that the proposed HetGNN reduces power consumption by\n20-25\\% and runs nearly 10,000 times faster than model-based benchmarks.", "comment": "6 pages, 4 figures, 2 tables. Accepted by IEEE TVT", "pdf_url": "http://arxiv.org/pdf/2507.09425v1", "cate": "cs.IT", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09660", "title": "Tools and Methodologies for System-Level Design", "authors": ["Shuvra S. Bhattacharyya", "Marilyn Wolf"], "categories": ["cs.AR"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "Comments:      This is a preprint of a chapter to appear in the forthcoming volume Handbook on Electronic Design Automation (third edition), published by Taylor & Francis. The final version may differ", "url": "http://arxiv.org/abs/2507.09660v1", "summary": "System-level design, once the province of board designers, has now become a\ncentral concern for chip designers. Because chip design is a less forgiving\ndesign medium -- design cycles are longer and mistakes are harder to correct --\nsystem-on-chip designers need a more extensive tool suite than may be used by\nboard designers and a variety of tools and methodologies have been developed\nfor system-level design of systems-on-chips (SoCs). System-level design is less\namenable to synthesis than are logic or physical design. As a result,\nsystem-level tools concentrate on modeling, simulation, design space\nexploration, and design verification. The goal of modeling is to correctly\ncapture the system's operational semantics, which helps with both\nimplementation and verification. The study of models of computation provides a\nframework for the description of digital systems. Not only do we need to\nunderstand a particular style of computation, such as dataflow, but we also\nneed to understand how different models of computation can reliably communicate\nwith each other. Design space exploration tools, such as hardware/software\nco-design, develop candidate designs to understand trade-offs. Simulation can\nbe used not only to verify functional correctness but also to supply\nperformance and power/energy information for design analysis. This chapter\nemploys two applications -- video and neural networks -- as examples. Both are\nleading-edge applications that illustrate many important aspects of\nsystem-level design.", "comment": "This is a preprint of a chapter to appear in the forthcoming volume\n  Handbook on Electronic Design Automation (third edition), published by Taylor\n  & Francis. The final version may differ", "pdf_url": "http://arxiv.org/pdf/2507.09660v1", "cate": "cs.AR", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2505.06989", "title": "Measuring the Accuracy and Effectiveness of PII Removal Services", "authors": ["Jiahui He", "Pete Snyder", "Hamed Haddadi", "Fabián E. Bustamante", "Gareth Tyson"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      In proceedings of the 25th Privacy Enhancing Technologies Symposium PETS 2025, July 14-19, 2025, Washington, DC and Online", "url": "http://arxiv.org/abs/2505.06989v3", "summary": "This paper presents the first large-scale empirical study of commercial\npersonally identifiable information (PII) removal systems -- commercial\nservices that claim to improve privacy by automating the removal of PII from\ndata broker's databases. Popular examples of such services include DeleteMe,\nMozilla Monitor, Incogni, among many others. The claims these services make may\nbe very appealing to privacy-conscious Web users, but how effective these\nservices actually are at improving privacy has not been investigated. This work\naims to improve our understanding of commercial PII removal services in\nmultiple ways. First, we conduct a user study where participants purchase\nsubscriptions from four popular PII removal services, and report (i) what PII\nthe service find, (ii) from which data brokers, (iii) whether the service is\nable to have the information removed, and (iv) whether the identified\ninformation actually is PII describing the participant. And second, by\ncomparing the claims and promises the services makes (e.g. which and how many\ndata brokers each service claims to cover). We find that these services have\nsignificant accuracy and coverage issues that limit the usefulness of these\nservices as a privacy-enhancing technology. For example, we find that the\nmeasured services are unable to remove the majority of the identified PII\nrecords from data broker's (48.2% of the successfully removed found records)\nand that most records identified by these services are not PII about the user\n(study participants found that only 41.1% of records identified by these\nservices were PII about themselves).", "comment": "In proceedings of the 25th Privacy Enhancing Technologies Symposium\n  PETS 2025, July 14-19, 2025, Washington, DC and Online", "pdf_url": "http://arxiv.org/pdf/2505.06989v3", "cate": "cs.CR", "date": "2025-05-11", "updated": "2025-07-13"}
{"id": "2507.10082", "title": "Unscented Kalman Filter with a Nonlinear Propagation Model for Navigation Applications", "authors": ["Amit Levy", "Itzik Klein"], "categories": ["cs.RO", "eess.SP"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      5 pages, 2 figures", "url": "http://arxiv.org/abs/2507.10082v1", "summary": "The unscented Kalman filter is a nonlinear estimation algorithm commonly used\nin navigation applications. The prediction of the mean and covariance matrix is\ncrucial to the stable behavior of the filter. This prediction is done by\npropagating the sigma points according to the dynamic model at hand. In this\npaper, we introduce an innovative method to propagate the sigma points\naccording to the nonlinear dynamic model of the navigation error state vector.\nThis improves the filter accuracy and navigation performance. We demonstrate\nthe benefits of our proposed approach using real sensor data recorded by an\nautonomous underwater vehicle during several scenarios.", "comment": "5 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.10082v1", "cate": "cs.RO", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09376", "title": "Acoustic Wave Modeling Using 2D FDTD: Applications in Unreal Engine For Dynamic Sound Rendering", "authors": ["Bilkent Samsurya"], "categories": ["cs.SD", "cs.HC", "cs.MM", "eess.AS", "H.5.5"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Accepted to the 50th International Computer Music Conference (ICMC), 2025", "url": "http://arxiv.org/abs/2507.09376v1", "summary": "Accurate sound propagation simulation is essential for delivering immersive\nexperiences in virtual applications, yet industry methods for acoustic modeling\noften do not account for the full breadth of acoustic wave phenomena. This\npaper proposes a novel two-dimensional (2D) finite-difference time-domain\n(FDTD) framework that simulates sound propagation as a wave-based model in\nUnreal Engine, with an emphasis on capturing lower frequency wave phenomena,\nembedding occlusion, diffraction, reflection and interference in generated\nimpulse responses. The process begins by discretizing the scene geometry into a\n2D grid via a top-down projection from which obstacle masks and boundary\nconditions are derived. A Python-based FDTD solver injects a sine sweep at a\nsource position, and virtual quadraphonic microphone arrays record pressure\nfield responses at pre-defined listener positions. De-convolution of the\npressure responses yields multi-channel impulse responses that retain spatial\ndirectionality which are then integrated into Unreal Engine's audio pipeline\nfor dynamic playback. Benchmark tests confirm agreement with analytical\nexpectations, and the paper outlines hybrid extensions aimed at commercial\nviability.", "comment": "Accepted to the 50th International Computer Music Conference (ICMC),\n  2025", "pdf_url": "http://arxiv.org/pdf/2507.09376v1", "cate": "cs.SD", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09884", "title": "VerifyBench: A Systematic Benchmark for Evaluating Reasoning Verifiers Across Domains", "authors": ["Xuzhao Li", "Xuchen Li", "Shiyu Hu", "Yongzhen Guo", "Wentao Zhang"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Preprint, Under review", "url": "http://arxiv.org/abs/2507.09884v1", "summary": "Large language models (LLMs) increasingly rely on reinforcement learning (RL)\nto enhance their reasoning capabilities through feedback. A critical challenge\nis verifying the consistency of model-generated responses and reference\nanswers, since these responses are often lengthy, diverse, and nuanced.\nRule-based verifiers struggle with complexity, prompting the use of model-based\nverifiers. However, specialized verifiers lack flexibility, while general LLM\njudges can be inconsistent. Existing research primarily focuses on building\nbetter verifiers, yet a systematic evaluation of different types of verifiers'\nperformance across domains remains lacking, severely constraining the reliable\ndevelopment of Reinforcement Learning with Verifiable Reward (RLVR). To address\nthis, we propose VerifyBench--a cross-domain comprehensive benchmark for\nsystematically evaluating verifiers. We construct 4,000 expert-level questions\ncovering mathematics, physics, chemistry, and biology. Each question is\nequipped with reference answers and diverse responses. The reliability of the\nevaluation is ensured through a rigorous annotation process conducted by a\nmultidisciplinary expert team. We design a four-dimensional experimental\nframework to comprehensively compare the performance boundaries of specialized\nverifiers and general LLMs under combined conditions of extracted answers vs.\ncomplete responses, and short vs. long outputs. Our evaluation uncovers\nfundamental trade-offs in verifiers: while specialized verifiers achieve\nleading accuracy, they exhibit deficiencies in recall; general models show\nstronger inclusivity but unstable precision. More importantly, we discover\nverifiers' high sensitivity to input structure and inherent limitations in\ncross-domain generalization, providing critical insights into the bottlenecks\nof current verifier technology.", "comment": "Preprint, Under review", "pdf_url": "http://arxiv.org/pdf/2507.09884v1", "cate": "cs.AI", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09118", "title": "Mind the Gap: Preserving and Compensating for the Modality Gap in CLIP-Based Continual Learning", "authors": ["Linlan Huang", "Xusheng Cao", "Haori Lu", "Yifan Meng", "Fei Yang", "Xialei Liu"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at ICCV 2025", "url": "http://arxiv.org/abs/2507.09118v1", "summary": "Continual learning aims to enable models to learn sequentially from\ncontinuously incoming data while retaining performance on previously learned\ntasks. With the Contrastive Language-Image Pre-trained model (CLIP) exhibiting\nstrong capabilities across various downstream tasks, there has been growing\ninterest in leveraging CLIP for continual learning in such scenarios. Most\nexisting works overlook the inherent modality gap in CLIP, a key factor in its\ngeneralization and adaptability. In this paper, we analyze the variations in\nthe modality gap during the fine-tuning of vision-language pre-trained models.\nOur observations reveal that the modality gap effectively reflects the extent\nto which pre-trained knowledge is preserved. Based on these insights, we\npropose a simple yet effective method, MG-CLIP, that improves CLIP's\nperformance in class-incremental learning. Our approach leverages modality gap\npreservation to mitigate forgetting and modality gap compensation to enhance\nthe capacity for new data, introducing a novel modality-gap-based perspective\nfor continual learning. Extensive experiments on multiple benchmarks\ndemonstrate that our method outperforms existing approaches without requiring\nadditional replay data. Our code is available at\nhttps://github.com/linlany/MindtheGap.", "comment": "Accepted at ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.09118v1", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09529", "title": "It Only Gets Worse: Revisiting DL-Based Vulnerability Detectors from a Practical Perspective", "authors": ["Yunqian Wang", "Xiaohong Li", "Yao Zhang", "Yuekang Li", "Zhiping Zhou", "Ruitao Feng"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09529v1", "summary": "With the growing threat of software vulnerabilities, deep learning (DL)-based\ndetectors have gained popularity for vulnerability detection. However, doubts\nremain regarding their consistency within declared CWE ranges, real-world\neffectiveness, and applicability across scenarios. These issues may lead to\nunreliable detection, high false positives/negatives, and poor adaptability to\nemerging vulnerabilities. A comprehensive analysis is needed to uncover\ncritical factors affecting detection and guide improvements in model design and\ndeployment. In this paper, we present VulTegra, a novel evaluation framework\nthat conducts a multidimensional comparison of scratch-trained and\npre-trained-based DL models for vulnerability detection. VulTegra reveals that\nstate-of-the-art (SOTA) detectors still suffer from low consistency, limited\nreal-world capabilities, and scalability challenges. Contrary to common belief,\npre-trained models are not consistently better than scratch-trained models but\nexhibit distinct strengths in specific contexts.Importantly, our study exposes\nthe limitations of relying solely on CWE-based classification and identifies\nkey factors that significantly affect model performance. Experimental results\nshow that adjusting just one such factor consistently improves recall across\nall seven evaluated detectors, with six also achieving better F1 scores. Our\nfindings provide deeper insights into model behavior and emphasize the need to\nconsider both vulnerability types and inherent code features for effective\ndetection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09529v1", "cate": "cs.SE", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09852", "title": "UavNetSim-v1: A Python-based Simulation Platform for UAV Communication Networks", "authors": ["Zihao Zhou", "Zipeng Dai", "Linyi Huang", "Cui Yang", "Youjun Xiang", "Jie Tang", "Kai-kit Wong"], "categories": ["cs.NI", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09852v1", "summary": "In unmanned aerial vehicle (UAV) networks, communication protocols and\nalgorithms are essential for cooperation and collaboration between UAVs.\nSimulation provides a cost-effective solution for prototyping, debugging, and\nanalyzing protocols and algorithms, avoiding the prohibitive expenses of field\nexperiments. In this paper, we present ``UavNetSim-v1'', an open-source\nPython-based simulation platform designed for rapid development, testing, and\nevaluating the protocols and algorithms in UAV networks. ``UavNetSim-v1''\nprovides most of the functionalities developers may need, including\nrouting/medium access control (MAC) protocols, topology control algorithms and\nmobility/energy models, while maintaining ease of use. Furthermore, the\nplatform supports comprehensive performance evaluation and features an\ninteractive visualization interface for in-depth algorithm analysis. In short,\n``UavNetSim-v1'' lends itself to both rapid prototyping and educational\npurposes, and can serve as a lightweight yet powerful alternative to mature\nnetwork simulators for UAV communication research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09852v1", "cate": "cs.NI", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09575", "title": "Introducing Meta-Fiber into Stacked Intelligent Metasurfaces for MIMO Communications: A Low-Complexity Design with only Two Layers", "authors": ["Hong Niu", "Jiancheng An", "Tuo Wu", "Jiangong Chen", "Yufei Zhao", "Yong Liang Guan", "Marco Di Renzo", "Merouane Debbah", "George K. Karagiannidis", "H. Vincent Poor", "Chau Yuen"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      14 pages", "url": "http://arxiv.org/abs/2507.09575v1", "summary": "Stacked intelligent metasurfaces (SIMs), which integrate multiple\nprogrammable metasurface layers, have recently emerged as a promising\ntechnology for advanced wave-domain signal processing. SIMs benefit from\nflexible spatial degree-of-freedom (DoF) while reducing the requirement for\ncostly radio-frequency (RF) chains. However, current state-of-the-art SIM\ndesigns face challenges such as complex phase shift optimization and energy\nattenuation from multiple layers. To address these aspects, we propose\nincorporating meta-fibers into SIMs, with the aim of reducing the number of\nlayers and enhancing the energy efficiency. First, we introduce a\nmeta-fiber-connected 2-layer SIM that exhibits the same flexible signal\nprocessing capabilities as conventional multi-layer structures, and explains\nthe operating principle. Subsequently, we formulate and solve the optimization\nproblem of minimizing the mean square error (MSE) between the SIM channel and\nthe desired channel matrices. Specifically, by designing the phase shifts of\nthe meta-atoms associated with the transmitting-SIM and receiving-SIM, a\nnon-interference system with parallel subchannels is established. In order to\nreduce the computational complexity, a closed-form expression for each phase\nshift at each iteration of an alternating optimization (AO) algorithm is\nproposed. We show that the proposed algorithm is applicable to conventional\nmulti-layer SIMs. The channel capacity bound and computational complexity are\nanalyzed to provide design insights. Finally, numerical results are\nillustrated, demonstrating that the proposed two-layer SIM with meta-fiber\nachieves over a 25% improvement in channel capacity while reducing the total\nnumber of meta-atoms by 59% as compared with a conventional seven-layer SIM.", "comment": "14 pages", "pdf_url": "http://arxiv.org/pdf/2507.09575v1", "cate": "cs.IT", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09730", "title": "Efficient FRW Transitions via Stochastic Finite Differences for Handling Non-Stratified Dielectrics", "authors": ["Jiechen Huang", "Wenjian Yu"], "categories": ["cs.AR", "cs.NA", "math.NA"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "Comments:      5 pages, 6 figures", "url": "http://arxiv.org/abs/2507.09730v1", "summary": "The accuracy of floating-random-walk (FRW) based capacitance extraction\nstands only when the recursive FRW transitions are sampled unbiasedly according\nto surrounding dielectrics. Advanced technology profiles, featuring complicated\nnon-stratified dielectrics, challenge the accuracy of existing FRW transition\nschemes that approximate dielectrics with stratified or eight-octant patterns.\nIn this work, we propose an algorithm named MicroWalk, enabling accurate FRW\ntransitions for arbitrary dielectrics while keeping high efficiency. It is\nprovably unbiased and equivalent to using transition probabilities solved by\nfinite difference method, but at orders of magnitude lower cost (802$\\times$\nfaster). An enhanced 3-D capacitance solver is developed with a hybrid strategy\nfor complicated dielectrics, combining MicroWalk with the special treatment for\nthe first transition cube and the analytical algorithm for stratified cubes.\nExperiments on real-world structures show that our solver achieves a\nsignificant accuracy advantage over existing FRW solvers, while preserving high\nefficiency.", "comment": "5 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.09730v1", "cate": "cs.AR", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.08954", "title": "MQFQ-Sticky: Fair Queueing For Serverless GPU Functions", "authors": ["Alexander Fuerst", "Siddharth Anil", "Vishakha Dixit", "Purushottam", "Kulkarni", "Prateek Sharma"], "categories": ["cs.DC", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08954v1", "summary": "Hardware accelerators like GPUs are now ubiquitous in data centers, but are\nnot fully supported by common cloud abstractions such as Functions as a Service\n(FaaS). Many popular and emerging FaaS applications such as machine learning\nand scientific computing can benefit from GPU acceleration. However, FaaS\nframeworks (such as OpenWhisk) are not capable of providing this acceleration\nbecause of the impedance mismatch between GPUs and the FaaS programming model,\nwhich requires virtualization and sandboxing of each function. The challenges\nare amplified due to the highly dynamic and heterogeneous FaaS workloads. This\npaper presents the design and implementation of a FaaS system for providing GPU\nacceleration in a black-box manner (without modifying function code). Running\nsmall functions in containerized sandboxes is challenging due to limited GPU\nconcurrency and high cold-start overheads, resulting in heavy queueing of\nfunction invocations. We show how principles from I/O scheduling, such as fair\nqueuing and anticipatory scheduling, can be translated to function scheduling\non GPUs. We develop MQFQ-Sticky, an integrated fair queueing and GPU memory\nmanagement approach, which balances the tradeoffs between locality, fairness,\nand latency. Empirical evaluation on a range of workloads shows that it reduces\nfunction latency by 2x to 20x compared to existing GPU and CPU queueing\npolicies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08954v1", "cate": "cs.DC", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2506.07605", "title": "TimberStrike: Dataset Reconstruction Attack Revealing Privacy Leakage in Federated Tree-Based Systems", "authors": ["Marco Di Gennaro", "Giovanni De Lucia", "Stefano Longari", "Stefano Zanero", "Michele Carminati"], "categories": ["cs.CR", "cs.DC", "cs.LG"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.07605v3", "summary": "Federated Learning has emerged as a privacy-oriented alternative to\ncentralized Machine Learning, enabling collaborative model training without\ndirect data sharing. While extensively studied for neural networks, the\nsecurity and privacy implications of tree-based models remain underexplored.\nThis work introduces TimberStrike, an optimization-based dataset reconstruction\nattack targeting horizontally federated tree-based models. Our attack, carried\nout by a single client, exploits the discrete nature of decision trees by using\nsplit values and decision paths to infer sensitive training data from other\nclients. We evaluate TimberStrike on State-of-the-Art federated gradient\nboosting implementations across multiple frameworks, including Flower, NVFlare,\nand FedTree, demonstrating their vulnerability to privacy breaches. On a\npublicly available stroke prediction dataset, TimberStrike consistently\nreconstructs between 73.05% and 95.63% of the target dataset across all\nimplementations. We further analyze Differential Privacy, showing that while it\npartially mitigates the attack, it also significantly degrades model\nperformance. Our findings highlight the need for privacy-preserving mechanisms\nspecifically designed for tree-based Federated Learning systems, and we provide\npreliminary insights into their design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.07605v3", "cate": "cs.CR", "date": "2025-06-09", "updated": "2025-07-13"}
{"id": "2507.10087", "title": "Foundation Model Driven Robotics: A Comprehensive Review", "authors": ["Muhammad Tayyab Khan", "Ammar Waheed"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10087v1", "summary": "The rapid emergence of foundation models, particularly Large Language Models\n(LLMs) and Vision-Language Models (VLMs), has introduced a transformative\nparadigm in robotics. These models offer powerful capabilities in semantic\nunderstanding, high-level reasoning, and cross-modal generalization, enabling\nsignificant advances in perception, planning, control, and human-robot\ninteraction. This critical review provides a structured synthesis of recent\ndevelopments, categorizing applications across simulation-driven design,\nopen-world execution, sim-to-real transfer, and adaptable robotics. Unlike\nexisting surveys that emphasize isolated capabilities, this work highlights\nintegrated, system-level strategies and evaluates their practical feasibility\nin real-world environments. Key enabling trends such as procedural scene\ngeneration, policy generalization, and multimodal reasoning are discussed\nalongside core bottlenecks, including limited embodiment, lack of multimodal\ndata, safety risks, and computational constraints. Through this lens, this\npaper identifies both the architectural strengths and critical limitations of\nfoundation model-based robotics, highlighting open challenges in real-time\noperation, grounding, resilience, and trust. The review concludes with a\nroadmap for future research aimed at bridging semantic reasoning and physical\nintelligence through more robust, interpretable, and embodied models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10087v1", "cate": "cs.RO", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09482", "title": "ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive Learning", "authors": ["Changli Wang", "Rui Wu", "Fang Yin"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09482v1", "summary": "Human emotions are complex, with sarcasm being a subtle and distinctive form.\nDespite progress in sarcasm research, sarcasm generation remains underexplored,\nprimarily due to the overreliance on textual modalities and the neglect of\nvisual cues, as well as the mismatch between image content and sarcastic intent\nin existing datasets. In this paper, we introduce M2SaG, a multimodal sarcasm\ngeneration dataset with 4,970 samples, each containing an image, a sarcastic\ntext, and a sarcasm target. To benchmark M2SaG, we propose ViSP, a generation\nframework that integrates Proximal Policy Optimization (PPO) and contrastive\nlearning. PPO utilizes reward scores from DIP to steer the generation of\nsarcastic texts, while contrastive learning encourages the model to favor\noutputs with higher reward scores. These strategies improve overall generation\nquality and produce texts with more pronounced sarcastic intent. We evaluate\nViSP across five metric sets and find it surpasses all baselines, including\nlarge language models, underscoring their limitations in sarcasm generation.\nFurthermore, we analyze the distributions of Sarcasm Scores and Factual\nIncongruity for both M2SaG and the texts generated by ViSP. The generated texts\nexhibit higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity\n(0.768 vs. 0.739), demonstrating that ViSP produces higher-quality sarcastic\ncontent than the original dataset. % The dataset and code will be publicly\navailable. Our dataset and code will be released at\n\\textit{https://github.com/wclapply/ViSP}.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09482v1", "cate": "cs.CL", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09955", "title": "DeepSeek: Paradigm Shifts and Technical Evolution in Large AI Models", "authors": ["Luolin Xiong", "Haofen Wang", "Xi Chen", "Lu Sheng", "Yun Xiong", "Jingping Liu", "Yanghua Xiao", "Huajun Chen", "Qing-Long Han", "Yang Tang"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09955v1", "summary": "DeepSeek, a Chinese Artificial Intelligence (AI) startup, has released their\nV3 and R1 series models, which attracted global attention due to their low\ncost, high performance, and open-source advantages. This paper begins by\nreviewing the evolution of large AI models focusing on paradigm shifts, the\nmainstream Large Language Model (LLM) paradigm, and the DeepSeek paradigm.\nSubsequently, the paper highlights novel algorithms introduced by DeepSeek,\nincluding Multi-head Latent Attention (MLA), Mixture-of-Experts (MoE),\nMulti-Token Prediction (MTP), and Group Relative Policy Optimization (GRPO).\nThe paper then explores DeepSeek engineering breakthroughs in LLM scaling,\ntraining, inference, and system-level optimization architecture. Moreover, the\nimpact of DeepSeek models on the competitive AI landscape is analyzed,\ncomparing them to mainstream LLMs across various fields. Finally, the paper\nreflects on the insights gained from DeepSeek innovations and discusses future\ntrends in the technical and engineering development of large AI models,\nparticularly in data, training, and reasoning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09955v1", "cate": "cs.AI", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09122", "title": "SnapMoGen: Human Motion Generation from Expressive Texts", "authors": ["Chuan Guo", "Inwoo Hwang", "Jian Wang", "Bing Zhou"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project Webpage: this https URL", "url": "http://arxiv.org/abs/2507.09122v1", "summary": "Text-to-motion generation has experienced remarkable progress in recent\nyears. However, current approaches remain limited to synthesizing motion from\nshort or general text prompts, primarily due to dataset constraints. This\nlimitation undermines fine-grained controllability and generalization to unseen\nprompts. In this paper, we introduce SnapMoGen, a new text-motion dataset\nfeaturing high-quality motion capture data paired with accurate, expressive\ntextual annotations. The dataset comprises 20K motion clips totaling 44 hours,\naccompanied by 122K detailed textual descriptions averaging 48 words per\ndescription (vs. 12 words of HumanML3D). Importantly, these motion clips\npreserve original temporal continuity as they were in long sequences,\nfacilitating research in long-term motion generation and blending. We also\nimprove upon previous generative masked modeling approaches. Our model,\nMoMask++, transforms motion into multi-scale token sequences that better\nexploit the token capacity, and learns to generate all tokens using a single\ngenerative masked transformer. MoMask++ achieves state-of-the-art performance\non both HumanML3D and SnapMoGen benchmarks. Additionally, we demonstrate the\nability to process casual user prompts by employing an LLM to reformat inputs\nto align with the expressivity and narration style of SnapMoGen. Project\nwebpage: https://snap-research.github.io/SnapMoGen/", "comment": "Project Webpage: https://snap-research.github.io/SnapMoGen/", "pdf_url": "http://arxiv.org/pdf/2507.09122v1", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09583", "title": "A Serverless Architecture for Real-Time Stock Analysis using Large Language Models: An Iterative Development and Debugging Case Study", "authors": ["Taniv Ashraf"], "categories": ["cs.SE", "cs.AI", "I.2.7; J.1"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      6 pages. The live application can be viewed at this https URL and the source code is available at this https URL", "url": "http://arxiv.org/abs/2507.09583v1", "summary": "The advent of powerful, accessible Large Language Models (LLMs) like Google's\nGemini presents new opportunities for democratizing financial data analysis.\nThis paper documents the design, implementation, and iterative debugging of a\nnovel, serverless system for real-time stock analysis. The system leverages the\nGemini API for qualitative assessment, automates data ingestion and processing\nvia GitHub Actions, and presents the findings through a decoupled, static\nfrontend. We detail the architectural evolution of the system, from initial\nconcepts to a robust, event-driven pipeline, highlighting the practical\nchallenges encountered during deployment. A significant portion of this paper\nis dedicated to a case study on the debugging process, covering common software\nerrors, platform-specific permission issues, and rare, environment-level\nplatform bugs. The final architecture operates at a near-zero cost,\ndemonstrating a viable model for individuals to build sophisticated AI-powered\nfinancial tools. The operational application is publicly accessible, and the\ncomplete source code is available for review. We conclude by discussing the\nrole of LLMs in financial analysis, the importance of robust debugging\nmethodologies, and the emerging paradigm of human-AI collaboration in software\ndevelopment.", "comment": "6 pages. The live application can be viewed at\n  https://codepen.io/tanivashraf/pen/GgpgxBY and the source code is available\n  at https://github.com/TanivAshraf/ai-stock-analyzer", "pdf_url": "http://arxiv.org/pdf/2507.09583v1", "cate": "cs.SE", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09942", "title": "Green-LLM: Optimal Workload Allocation for Environmentally-Aware Distributed Inference", "authors": ["Jiaming Cheng", "Duong Tung Nguyen"], "categories": ["cs.NI", "cs.DC", "cs.SY", "eess.SY", "math.OC"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      5 pages, 11 figures", "url": "http://arxiv.org/abs/2507.09942v1", "summary": "This letter investigates the optimal allocation of large language model (LLM)\ninference workloads across heterogeneous edge data centers (DCs) over time.\nEach DC features on-site renewable generation and faces dynamic electricity\nprices and spatiotemporal variability in renewable availability. The central\nquestion is: how can inference workloads be optimally distributed to the DCs to\nminimize energy consumption, carbon emissions, and water usage while enhancing\nuser experience? This letter proposes a novel optimization model for LLM\nservice providers to reduce operational costs and environmental impacts.\nNumerical results validate the efficacy of the proposed approach.", "comment": "5 pages, 11 figures", "pdf_url": "http://arxiv.org/pdf/2507.09942v1", "cate": "cs.NI", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09627", "title": "Lightweight Deep Learning-Based Channel Estimation for RIS-Aided Extremely Large-Scale MIMO Systems on Resource-Limited Edge Devices", "authors": ["Muhammad Kamran Saeed", "Ashfaq Khokhar", "Shakil Ahmed"], "categories": ["cs.IT", "cs.CV", "cs.LG", "cs.NI", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09627v1", "summary": "Next-generation wireless technologies such as 6G aim to meet demanding\nrequirements such as ultra-high data rates, low latency, and enhanced\nconnectivity. Extremely Large-Scale MIMO (XL-MIMO) and Reconfigurable\nIntelligent Surface (RIS) are key enablers, with XL-MIMO boosting spectral and\nenergy efficiency through numerous antennas, and RIS offering dynamic control\nover the wireless environment via passive reflective elements. However,\nrealizing their full potential depends on accurate Channel State Information\n(CSI). Recent advances in deep learning have facilitated efficient cascaded\nchannel estimation. However, the scalability and practical deployment of\nexisting estimation models in XL-MIMO systems remain limited. The growing\nnumber of antennas and RIS elements introduces a significant barrier to\nreal-time and efficient channel estimation, drastically increasing data volume,\nescalating computational complexity, requiring advanced hardware, and resulting\nin substantial energy consumption. To address these challenges, we propose a\nlightweight deep learning framework for efficient cascaded channel estimation\nin XL-MIMO systems, designed to minimize computational complexity and make it\nsuitable for deployment on resource-constrained edge devices. Using spatial\ncorrelations in the channel, we introduce a patch-based training mechanism that\nreduces the dimensionality of input to patch-level representations while\npreserving essential information, allowing scalable training for large-scale\nsystems. Simulation results under diverse conditions demonstrate that our\nframework significantly improves estimation accuracy and reduces computational\ncomplexity, regardless of the increasing number of antennas and RIS elements in\nXL-MIMO systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09627v1", "cate": "cs.IT", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09774", "title": "Low-Cost Fuel Dispenser Prototype Using STM32 and an H-bridge motor driver", "authors": ["MD Zobaer Hossain Bhuiyan", "Abir Bin Faruque", "Mahtab Newaz", "Mohammad Abdul Qayum"], "categories": ["cs.AR"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09774v1", "summary": "This paper presents the design and development of a low-cost fuel dispensing\nsystem prototype based on the STM32 microcontroller and L298N motor driver. The\nsystem aims to provide an affordable and scalable solution for fuel delivery in\nremote or small-scale environments where conventional, high-cost systems are\nnot feasible. The core control unit is built using an STM32 microcontroller,\nwhich manages user input through a 4x4 matrix keypad and displays operational\ndata on a 16x4 LCD screen via I2C communication. A 12V DC pump motor is used to\nsimulate the fuel dispensing mechanism, precisely controlled via the dual\nH-bridge L298N motor driver. The system is powered by a 11.1V battery and is\ndesigned for ease of deployment and portability. The keypad allows users to\ninput the desired fuel amount, while the system ensures accurate motor runtime\ncorresponding to the volume to be dispensed. This project demonstrates how\nembedded systems can be leveraged to build cost-effective, user-friendly, and\nenergy-efficient solutions. The proposed design can be further enhanced with\nflow sensors, GSM connectivity, RFID cards, and payment integration for\nreal-world applications in fuel stations or agricultural use.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09774v1", "cate": "cs.AR", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09546", "title": "Lightweight Federated Learning over Wireless Edge Networks", "authors": ["Xiangwang Hou", "Jingjing Wang", "Jun Du", "Chunxiao Jiang", "Yong Ren", "Dusit Niyato"], "categories": ["cs.DC", "cs.LG"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09546v1", "summary": "With the exponential growth of smart devices connected to wireless networks,\ndata production is increasing rapidly, requiring machine learning (ML)\ntechniques to unlock its value. However, the centralized ML paradigm raises\nconcerns over communication overhead and privacy. Federated learning (FL)\noffers an alternative at the network edge, but practical deployment in wireless\nnetworks remains challenging. This paper proposes a lightweight FL (LTFL)\nframework integrating wireless transmission power control, model pruning, and\ngradient quantization. We derive a closed-form expression of the FL convergence\ngap, considering transmission error, model pruning error, and gradient\nquantization error. Based on these insights, we formulate an optimization\nproblem to minimize the convergence gap while meeting delay and energy\nconstraints. To solve the non-convex problem efficiently, we derive closed-form\nsolutions for the optimal model pruning ratio and gradient quantization level,\nand employ Bayesian optimization for transmission power control. Extensive\nexperiments on real-world datasets show that LTFL outperforms state-of-the-art\nschemes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09546v1", "cate": "cs.DC", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2506.16899", "title": "Towards Effective Complementary Security Analysis using Large Language Models", "authors": ["Jonas Wagner", "Simon Müller", "Christian Näther", "Jan-Philipp Steghöfer", "Andreas Both"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      8 pages, 6 figures", "url": "http://arxiv.org/abs/2506.16899v2", "summary": "A key challenge in security analysis is the manual evaluation of potential\nsecurity weaknesses generated by static application security testing (SAST)\ntools. Numerous false positives (FPs) in these reports reduce the effectiveness\nof security analysis. We propose using Large Language Models (LLMs) to improve\nthe assessment of SAST findings. We investigate the ability of LLMs to reduce\nFPs while trying to maintain a perfect true positive rate, using datasets\nextracted from the OWASP Benchmark (v1.2) and a real-world software project.\nOur results indicate that advanced prompting techniques, such as\nChain-of-Thought and Self-Consistency, substantially improve FP detection.\nNotably, some LLMs identified approximately 62.5% of FPs in the OWASP Benchmark\ndataset without missing genuine weaknesses. Combining detections from different\nLLMs would increase this FP detection to approximately 78.9%. Additionally, we\ndemonstrate our approach's generalizability using a real-world dataset covering\nfive SAST tools, three programming languages, and infrastructure files. The\nbest LLM detected 33.85% of all FPs without missing genuine weaknesses, while\ncombining detections from different LLMs would increase this detection to\n38.46%. Our findings highlight the potential of LLMs to complement traditional\nSAST tools, enhancing automation and reducing resources spent addressing false\nalarms.", "comment": "8 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.16899v2", "cate": "cs.CR", "date": "2025-06-20", "updated": "2025-07-13"}
{"id": "2507.10105", "title": "Physics-Informed Neural Networks with Unscented Kalman Filter for Sensorless Joint Torque Estimation in Humanoid Robots", "authors": ["Ines Sorrentino", "Giulio Romualdi", "Lorenzo Moretti", "Silvio Traversaro", "Daniele Pucci"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10105v1", "summary": "This paper presents a novel framework for whole-body torque control of\nhumanoid robots without joint torque sensors, designed for systems with\nelectric motors and high-ratio harmonic drives. The approach integrates\nPhysics-Informed Neural Networks (PINNs) for friction modeling and Unscented\nKalman Filtering (UKF) for joint torque estimation, within a real-time torque\ncontrol architecture. PINNs estimate nonlinear static and dynamic friction from\njoint and motor velocity readings, capturing effects like motor actuation\nwithout joint movement. The UKF utilizes PINN-based friction estimates as\ndirect measurement inputs, improving torque estimation robustness. Experimental\nvalidation on the ergoCub humanoid robot demonstrates improved torque tracking\naccuracy, enhanced energy efficiency, and superior disturbance rejection\ncompared to the state-of-the-art Recursive Newton-Euler Algorithm (RNEA), using\na dynamic balancing experiment. The framework's scalability is shown by\nconsistent performance across robots with similar hardware but different\nfriction characteristics, without re-identification. Furthermore, a comparative\nanalysis with position control highlights the advantages of the proposed torque\ncontrol approach. The results establish the method as a scalable and practical\nsolution for sensorless torque control in humanoid robots, ensuring torque\ntracking, adaptability, and stability in dynamic environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10105v1", "cate": "cs.RO", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09637", "title": "Code Review as Decision-Making -- Building a Cognitive Model from the Questions Asked During Code Review", "authors": ["Lo Gullstrand Heander", "Emma Söderberg", "Christofer Rydenfält"], "categories": ["cs.SE", "cs.HC", "D.2.0; D.2.3; K.4.3"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      39 pages, 14 figures Submitted to Empirical Software Engineering, Springer Nature", "url": "http://arxiv.org/abs/2507.09637v1", "summary": "Code review is a well-established and valued practice in the software\nengineering community contributing to both code quality and interpersonal\nbenefits. However, there are challenges in both tools and processes that give\nrise to misalignments and frustrations. Recent research seeks to address this\nby automating code review entirely, but we believe that this risks losing the\nmajority of the interpersonal benefits such as knowledge transfer and shared\nownership.\n  We believe that by better understanding the cognitive processes involved in\ncode review, it would be possible to improve tool support, with out without AI,\nand make code review both more efficient, more enjoyable, while increasing or\nmaintaining all of its benefits. In this paper, we conduct an ethnographic\nthink-aloud study involving 10 participants and 34 code reviews. We build a\ncognitive model of code review bottom up through thematic, statistical,\ntemporal, and sequential analysis of the transcribed material. Through the\ndata, the similarities between the cognitive process in code review and\ndecision-making processes, especially recognition-primed decision-making,\nbecome apparent.\n  The result is the Code Review as Decision-Making (CRDM) model that shows how\nthe developers move through two phases during the code review; first an\norientation phase to establish context and rationale and then an analytical\nphase to understand, assess, and plan the rest of the review. Throughout the\nprocess several decisions must be taken, on writing comments, finding more\ninformation, voting, running the code locally, verifying continuous integration\nresults, etc.\n  Analysis software and process-coded data publicly available at:\nhttps://doi.org/10.5281/zenodo.15758266", "comment": "39 pages, 14 figures Submitted to Empirical Software Engineering,\n  Springer Nature", "pdf_url": "http://arxiv.org/pdf/2507.09637v1", "cate": "cs.SE", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.10000", "title": "On The Role of Intentionality in Knowledge Representation: Analyzing Scene Context for Cognitive Agents with a Tiny Language Model", "authors": ["Mark Burgess"], "categories": ["cs.AI", "cs.CL", "I.2.11; F.4.1; I.2.4; G.2.2"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10000v1", "summary": "Since Searle's work deconstructing intent and intentionality in the realm of\nphilosophy, the practical meaning of intent has received little attention in\nscience and technology. Intentionality and context are both central to the\nscope of Promise Theory's model of Semantic Spacetime, used as an effective\nTiny Language Model. One can identify themes and concepts from a text, on a low\nlevel (without knowledge of the specific language) by using process coherence\nas a guide. Any agent process can assess superficially a degree of latent\n`intentionality' in data by looking for anomalous multi-scale anomalies and\nassessing the work done to form them. Scale separation can be used to sort\nparts into `intended' content and `ambient context', using the spacetime\ncoherence as a measure. This offers an elementary but pragmatic interpretation\nof latent intentionality for very low computational cost, and without reference\nto extensive training or reasoning capabilities. The process is well within the\nreach of basic organisms as it does not require large scale artificial\nprobabilistic batch processing. The level of concept formation depends,\nhowever, on the memory capacity of the agent.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10000v1", "cate": "cs.AI", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09139", "title": "PoseLLM: Enhancing Language-Guided Human Pose Estimation with MLP Alignment", "authors": ["Dewen Zhang", "Tahir Hussain", "Wangpeng An", "Hayaru Shouno"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Preprint", "url": "http://arxiv.org/abs/2507.09139v1", "summary": "Human pose estimation traditionally relies on architectures that encode\nkeypoint priors, limiting their generalization to novel poses or unseen\nkeypoints. Recent language-guided approaches like LocLLM reformulate keypoint\nlocalization as a vision-language task, enabling zero-shot generalization\nthrough textual descriptions. However, LocLLM's linear projector fails to\ncapture complex spatial-textual interactions critical for high-precision\nlocalization. To address this, we propose PoseLLM, the first Large Language\nModel (LLM)-based pose estimation framework that replaces the linear projector\nwith a nonlinear MLP vision-language connector. This lightweight two-layer MLP\nwith GELU activation enables hierarchical cross-modal feature transformation,\nenhancing the fusion of visual patches and textual keypoint descriptions.\nTrained exclusively on COCO data, PoseLLM achieves 77.8 AP on the COCO\nvalidation set, outperforming LocLLM by +0.4 AP, while maintaining strong\nzero-shot generalization on Human-Art and MPII. Our work demonstrates that a\nsimple yet powerful nonlinear connector significantly boosts localization\naccuracy without sacrificing generalization, advancing the state-of-the-art in\nlanguage-guided pose estimation. Code is available at\nhttps://github.com/Ody-trek/PoseLLM.", "comment": "Preprint", "pdf_url": "http://arxiv.org/pdf/2507.09139v1", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09594", "title": "How to Define Design in Industrial Control and Automation Software", "authors": ["Aydin Homay"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09594v1", "summary": "Design is a fundamental aspect of engineering, enabling the creation of\nproducts, systems, and organizations to meet societal and/or business needs.\nHowever, the absence of a scientific foundation in design often results in\nsubjective decision-making, reducing both efficiency and innovation. This\nchallenge is particularly evident in the software industry and, by extension,\nin the domain of industrial control and automation systems (iCAS).\n  In this study, first we review the existing design definitions within the\nsoftware industry, challenge prevailing misconceptions about design, review\ndesign definition in the field of design theory and address key questions such\nas: When does design begin? How can design be defined scientifically? What\nconstitutes good design? and the difference between design and design language\nby relying on advancements in the field of design theory. We also evaluate the\ndistinction between ad-hoc and systematic design approaches, and present\narguments on how to balance complementary operational concerns while resolving\nconflicting evolutionary concerns.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09594v1", "cate": "cs.SE", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.10210", "title": "Fine-Grained Coordinated OFDMA With Fiber Backhaul Enabled by openwifi and White Rabbit", "authors": ["Thijs Havinga", "Xianjun Jiao", "Wei Liu", "Baiheng Chen", "Robbe Gaeremynck", "Ingrid Moerman"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      6 pages, 7 figures. Submitted to GLOBECOM 2025", "url": "http://arxiv.org/abs/2507.10210v1", "summary": "Proper coordination is needed to guarantee the performance of wireless\nnetworks in dense deployments. Contention-based systems suffer badly in terms\nof latency when multiple devices compete for the same resources. Coordinated\nOrthogonal Frequency Division Multiple Access (Co-OFDMA) is proposed for Wi-Fi\n8 to remedy this, as it enables multiple Access Points (APs) to share spectrum\nmore efficiently. However, fine-grained resource allocation, namely within\n20MHz bandwidth, is argued to be impractical due to the over-the-air scheduling\noverhead and complexity in terms of physical layer signaling. A wired backhaul\nmitigates the need for over-the-air scheduling and synchronization, and it\nallows for coordination even if APs are not in each others' range. Furthermore,\nit forms the basis for more advanced multi-AP coordination schemes like\ncoordinated beamforming and joint transmission. In this work we demonstrate the\nrealization of Wi-Fi 6 compliant fine-grained Co-OFDMA using a fiber backhaul,\nenabled by the open-source platforms openwifi and White Rabbit. We show that\nthe performance in terms of carrier frequency offset pre-compensation and time\nsynchronization between two APs exceeds related wireless standard requirements.\nFurthermore, the quality of the received constellation of the Co-OFDMA frame as\nreported by a wireless connectivity tester is better than individual frames\nsent by the APs.", "comment": "6 pages, 7 figures. Submitted to GLOBECOM 2025", "pdf_url": "http://arxiv.org/pdf/2507.10210v1", "cate": "cs.NI", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09712", "title": "RDD Function: A Tradeoff Between Rate and Distortion-in-Distortion", "authors": ["Lingyi Chen", "Haoran Tang", "Shitong Wu", "Jiakun Liu", "Huihui Wu", "Wenyi Zhang", "Hao Wu"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09712v1", "summary": "In this paper, we propose a novel function named Rate\nDistortion-in-Distortion (RDD) function as an extension of the classical\nrate-distortion (RD) function, where the expected distortion constraint is\nreplaced by the Gromov-type distortion. This distortion, integral to the\nGromov-Wasserstein (GW) distance, effectively defines the similarity in spaces\nof different dimensions without a direct metric between them. While our RDD\nfunction qualifies as an informational RD function, encoding theorems\nsubstantiate its status as an operational RD function, thereby underscoring its\npotential applicability in real-world source coding. Due to the high\ncomputational complexity associated with Gromov-type distortion, the RDD\nfunction cannot be solved analytically. Consequently, we develop an alternating\nmirror descent algorithm that significantly reduces computational complexity by\nemploying decomposition, linearization, and relaxation techniques. Simulations\non classical sources and different grids demonstrate the effectiveness of our\nalgorithm. By examining the distinctions and connections between the RDD\nfunction and the RD function, we anticipate that RDD function will play a novel\nrole in foreseeable future scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09712v1", "cate": "cs.IT", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09780", "title": "BitParticle: Partializing Sparse Dual-Factors to Build Quasi-Synchronizing MAC Arrays for Energy-efficient DNNs", "authors": ["Feilong Qiaoyuan", "Jihe Wang", "Zhiyu Sun", "Linying Wu", "Yuanhua Xiao", "Danghui Wang"], "categories": ["cs.AR", "cs.AI"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "Comments:      9 pages, 13 figures, 3 Tables", "url": "http://arxiv.org/abs/2507.09780v1", "summary": "Bit-level sparsity in quantized deep neural networks (DNNs) offers\nsignificant potential for optimizing Multiply-Accumulate (MAC) operations.\nHowever, two key challenges still limit its practical exploitation. First,\nconventional bit-serial approaches cannot simultaneously leverage the sparsity\nof both factors, leading to a complete waste of one factor' s sparsity. Methods\ndesigned to exploit dual-factor sparsity are still in the early stages of\nexploration, facing the challenge of partial product explosion. Second, the\nfluctuation of bit-level sparsity leads to variable cycle counts for MAC\noperations. Existing synchronous scheduling schemes that are suitable for\ndual-factor sparsity exhibit poor flexibility and still result in significant\nunderutilization of MAC units. To address the first challenge, this study\nproposes a MAC unit that leverages dual-factor sparsity through the emerging\nparticlization-based approach. The proposed design addresses the issue of\npartial product explosion through simple control logic, resulting in a more\narea- and energy-efficient MAC unit. In addition, by discarding less\nsignificant intermediate results, the design allows for further hardware\nsimplification at the cost of minor accuracy loss. To address the second\nchallenge, a quasi-synchronous scheme is introduced that adds cycle-level\nelasticity to the MAC array, reducing pipeline stalls and thereby improving MAC\nunit utilization. Evaluation results show that the exact version of the\nproposed MAC array architecture achieves a 29.2% improvement in area efficiency\ncompared to the state-of-the-art bit-sparsity-driven architecture, while\nmaintaining comparable energy efficiency. The approximate variant further\nimproves energy efficiency by 7.5%, compared to the exact version. Index-Terms:\nDNN acceleration, Bit-level sparsity, MAC unit", "comment": "9 pages, 13 figures, 3 Tables", "pdf_url": "http://arxiv.org/pdf/2507.09780v1", "cate": "cs.AR", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09926", "title": "Intelligent Task Management via Dynamic Multi-region Division in LEO Satellite Networks", "authors": ["Zixuan Song", "Zhishu Shen", "Xiaoyu Zheng", "Qiushi Zheng", "Zheng Lei", "Jiong Jin"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09926v1", "summary": "As a key complement to terrestrial networks and a fundamental component of\nfuture 6G systems, Low Earth Orbit (LEO) satellite networks are expected to\nprovide high-quality communication services when integrated with ground-based\ninfrastructure, thereby attracting significant research interest. However, the\nlimited satellite onboard resources and the uneven distribution of\ncomputational workloads often result in congestion along inter-satellite links\n(ISLs) that degrades task processing efficiency. Effectively managing the\ndynamic and large-scale topology of LEO networks to ensure balanced task\ndistribution remains a critical challenge. To this end, we propose a dynamic\nmulti-region division framework for intelligent task management in LEO\nsatellite networks. This framework optimizes both intra- and inter-region\nrouting to minimize task delay while balancing the utilization of computational\nand communication resources. Based on this framework, we propose a dynamic\nmulti-region division algorithm based on the Genetic Algorithm (GA), which\nadaptively adjusts the size of each region based on the workload status of\nindividual satellites. Additionally, we incorporate an adaptive routing\nalgorithm and a task splitting and offloading scheme based on Multi-Agent Deep\nDeterministic Policy Gradient (MA-DDPG) to effectively accommodate the arriving\ntasks. Simulation results demonstrate that our proposed framework outperforms\ncomparative methods in terms of the task delay, energy consumption per task,\nand task completion rate.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09926v1", "cate": "cs.DC", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.08869", "title": "Preliminary Analysis of Construction Work Zone on Roadways in Florida by Crash Severity", "authors": ["Tatiana Deslouches", "Doreen Kobelo Regalado", "Mohamed Khalafalla", "Tejal Mulay"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08869v1", "summary": "Construction zones are inherently hazardous, posing significant risks to\nconstruction workers and motorists. Despite existing safety measures,\nconstruction zones continue to witness fatalities and serious injuries,\nimposing economic burdens. Addressing these issues requires understanding root\ncauses and implementing preventive strategies centered around the 4Es\n(Engineering, Education, Enforcement, Emergency Response) and 4Is (Information\nIntelligence, Innovation, Insight into communities, Investment, and Policies).\nProper safety management, integrating these strategic initiatives, aims to\nreduce and potentially eliminate fatalities and serious injuries in work zones.\nIn Florida, road construction work zone fatalities and serious injuries remain\na critical concern, especially in urban counties. Despite a 12 billion dollars\ninfrastructure investment in 2022, Florida ranks eighth nationally for fatal\nwork zone crashes involving commercial motor vehicles (CMVs). Analysis from\n2019 to 2023 shows an average of 71 fatalities and 309 serious injuries\nannually in Florida work zones, reflecting a persistent safety challenge.\nHigh-risk counties include Orange, Broward, Duval, Hillsborough, Pasco,\nMiami-Dade, Seminole, Manatee, Palm Beach, and Lake. This study presents a\npreliminary analysis of work zone crashes in Broward, Duval, Hillsborough, and\nOrange counties. A multilogit model assessed attributes contributing to\nfatalities and serious injuries, such as crash type, weather and light\nconditions, work zone type, type of shoulder, presence of workers, and law\nenforcement. Results indicate significant contributing factors, highlighting\nopportunities to use machine learning for alerting drivers and construction\nmanagers, ultimately enhancing safety protocols and reducing fatalities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08869v1", "cate": "cs.CY", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2506.17805", "title": "AdRo-FL: Informed and Secure Client Selection for Federated Learning in the Presence of Adversarial Aggregator", "authors": ["Md. Kamrul Hossain", "Walid Aljoby", "Anis Elgabli", "Ahmed M. Abdelmoniem", "Khaled A. Harras"], "categories": ["cs.CR", "cs.LG"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      17 pages", "url": "http://arxiv.org/abs/2506.17805v2", "summary": "Federated Learning (FL) enables collaborative learning without exposing\nclients' data. While clients only share model updates with the aggregator,\nstudies reveal that aggregators can infer sensitive information from these\nupdates. Secure Aggregation (SA) protects individual updates during\ntransmission; however, recent work demonstrates a critical vulnerability where\nadversarial aggregators manipulate client selection to bypass SA protections,\nconstituting a Biased Selection Attack (BSA). Although verifiable random\nselection prevents BSA, it precludes informed client selection essential for FL\nperformance. We propose Adversarial Robust Federated Learning (AdRo-FL), which\nsimultaneously enables: informed client selection based on client utility, and\nrobust defense against BSA maintaining privacy-preserving aggregation. AdRo-FL\nimplements two client selection frameworks tailored for distinct settings. The\nfirst framework assumes clients are grouped into clusters based on mutual\ntrust, such as different branches of an organization. The second framework\nhandles distributed clients where no trust relationships exist between them.\nFor the cluster-oriented setting, we propose a novel defense against BSA by (1)\nenforcing a minimum client selection quota from each cluster, supervised by a\ncluster-head in every round, and (2) introducing a client utility function to\nprioritize efficient clients. For the distributed setting, we design a\ntwo-phase selection protocol: first, the aggregator selects the top clients\nbased on our utility-driven ranking; then, a verifiable random function (VRF)\nensures a BSA-resistant final selection. AdRo-FL also applies quantization to\nreduce communication overhead and sets strict transmission deadlines to improve\nenergy efficiency. AdRo-FL achieves up to $1.85\\times$ faster time-to-accuracy\nand up to $1.06\\times$ higher final accuracy compared to insecure baselines.", "comment": "17 pages", "pdf_url": "http://arxiv.org/pdf/2506.17805v2", "cate": "cs.CR", "date": "2025-06-21", "updated": "2025-07-12"}
{"id": "2507.10121", "title": "Simulations and experiments with assemblies of fiber-reinforced soft actuators", "authors": ["Seung Hyun Kim", "Jiamiao Guo", "Arman Tekinalp", "Heng-Sheng Chang", "Ugur Akcal", "Tixian Wang", "Darren Biskup", "Benjamin Walt", "Girish Chowdhary", "Girish Krishnan", "Prashant G. Mehta", "Mattia Gazzola"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 pages, 4 figures This work has been submitted to the IEEE for possible publication", "url": "http://arxiv.org/abs/2507.10121v1", "summary": "Soft continuum arms (SCAs) promise versatile manipulation through mechanical\ncompliance, for assistive devices, agriculture, search applications, or\nsurgery. However, SCAs' real-world use is challenging, partly due to their\nhard-to-control non-linear behavior. Here, a simulation framework for SCAs\nmodularly assembled out of fiber reinforced elastomeric enclosures (FREEs) is\ndeveloped and integrated with a video-tracking system for experimental testing\nand control design.", "comment": "8 pages, 4 figures This work has been submitted to the IEEE for\n  possible publication", "pdf_url": "http://arxiv.org/pdf/2507.10121v1", "cate": "cs.RO", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10131", "title": "Probabilistic Human Intent Prediction for Mobile Manipulation: An Evaluation with Human-Inspired Constraints", "authors": ["Cesar Alan Contreras", "Manolis Chiou", "Alireza Rastegarpanah", "Michal Szulik", "Rustam Stolkin"], "categories": ["cs.RO", "cs.CV", "cs.HC"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Submitted to Journal of Intelligent & Robotic Systems (Under Review)", "url": "http://arxiv.org/abs/2507.10131v1", "summary": "Accurate inference of human intent enables human-robot collaboration without\nconstraining human control or causing conflicts between humans and robots. We\npresent GUIDER (Global User Intent Dual-phase Estimation for Robots), a\nprobabilistic framework that enables a robot to estimate the intent of human\noperators. GUIDER maintains two coupled belief layers, one tracking navigation\ngoals and the other manipulation goals. In the Navigation phase, a Synergy Map\nblends controller velocity with an occupancy grid to rank interaction areas.\nUpon arrival at a goal, an autonomous multi-view scan builds a local 3D cloud.\nThe Manipulation phase combines U2Net saliency, FastSAM instance saliency, and\nthree geometric grasp-feasibility tests, with an end-effector kinematics-aware\nupdate rule that evolves object probabilities in real-time. GUIDER can\nrecognize areas and objects of intent without predefined goals. We evaluated\nGUIDER on 25 trials (five participants x five task variants) in Isaac Sim, and\ncompared it with two baselines, one for navigation and one for manipulation.\nAcross the 25 trials, GUIDER achieved a median stability of 93-100% during\nnavigation, compared with 60-100% for the BOIR baseline, with an improvement of\n39.5% in a redirection scenario (T5). During manipulation, stability reached\n94-100% (versus 69-100% for Trajectron), with a 31.4% difference in a\nredirection task (T3). In geometry-constrained trials (manipulation), GUIDER\nrecognized the object intent three times earlier than Trajectron (median\nremaining time to confident prediction 23.6 s vs 7.8 s). These results validate\nour dual-phase framework and show improvements in intent inference in both\nphases of mobile manipulation tasks.", "comment": "Submitted to Journal of Intelligent & Robotic Systems (Under Review)", "pdf_url": "http://arxiv.org/pdf/2507.10131v1", "cate": "cs.RO", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10007", "title": "Deep Hidden Cognition Facilitates Reliable Chain-of-Thought Reasoning", "authors": ["Zijun Chen", "Wenbo Hu", "Richang Hong"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10007v1", "summary": "Chain of Thought (CoT) reasoning has demonstrated remarkable deep reasoning\ncapabilities in both large language models (LLMs) and multimodal large language\nmodels (MLLMs). However, its reliability is often undermined by the\naccumulation of errors in intermediate steps. This paper introduces an novel\napproach to calibrate the CoT reasoning accuracy by leveraging the model's\nintrinsic veracity encoding. We discover that specific attention head\nactivations reliably reflect the truthfulness of reasoning steps in CoT. Based\non this insight, we train a confidence predictor to evaluate the correctness of\neach reasoning step using these truthfulness-sensitive activations, dynamically\nselecting the most plausible reasoning path via beam search. Experimental\nresults demonstrate that our method significantly outperforms the\nstate-of-the-art baselines (e.g., Few-Shot CoT, Self-Consistency, and\nSelf-Evaluation Guided Beam Search) across the mathematical, symbolic, and\ncommonsense reasoning tasks, exhibiting superior accuracy and reliability in\nboth unimodal and multimodal settings. We further validate the approach on\nlarge reasoning models, confirming its applicability to specialized reasoning\nmodels. Additionally, we explore the role of the model's self-correction\nability in CoT reasoning. This work provides a novel reliability improvement\npath for CoT reasoning with broad application potential.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10007v1", "cate": "cs.AI", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09144", "title": "$I^{2}$-World: Intra-Inter Tokenization for Efficient Dynamic 4D Scene Forecasting", "authors": ["Zhimin Liao", "Ping Wei", "Ruijie Zhang", "Shuaijia Chen", "Haoxuan Wang", "Ziyang Ren"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09144v1", "summary": "Forecasting the evolution of 3D scenes and generating unseen scenarios via\noccupancy-based world models offers substantial potential for addressing corner\ncases in autonomous driving systems. While tokenization has revolutionized\nimage and video generation, efficiently tokenizing complex 3D scenes remains a\ncritical challenge for 3D world models. To address this, we propose\n$I^{2}$-World, an efficient framework for 4D occupancy forecasting. Our method\ndecouples scene tokenization into intra-scene and inter-scene tokenizers. The\nintra-scene tokenizer employs a multi-scale residual quantization strategy to\nhierarchically compress 3D scenes while preserving spatial details. The\ninter-scene tokenizer residually aggregates temporal dependencies across\ntimesteps. This dual design preserves the compactness of 3D tokenizers while\nretaining the dynamic expressiveness of 4D tokenizers. Unlike decoder-only\nGPT-style autoregressive models, $I^{2}$-World adopts an encoder-decoder\narchitecture. The encoder aggregates spatial context from the current scene and\npredicts a transformation matrix to enable high-level control over scene\ngeneration. The decoder, conditioned on this matrix and historical tokens,\nensures temporal consistency during generation. Experiments demonstrate that\n$I^{2}$-World achieves state-of-the-art performance, outperforming existing\nmethods by 25.1\\% in mIoU and 36.9\\% in IoU for 4D occupancy forecasting while\nexhibiting exceptional computational efficiency: it requires merely 2.9 GB of\ntraining memory and achieves real-time inference at 37.0 FPS. Our code is\navailable on https://github.com/lzzzzzm/II-World.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09144v1", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09596", "title": "The Mythical Good Software", "authors": ["Aydin Homay"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09596v1", "summary": "Good software has high cohesion and low coupling is clumsy, obscure, and in\nsome certain cases could be actually a harmful state of being. It is clumsy\nbecause there is no perfect correlation between higher cohesiveness and optimum\ndesign, and it is obscure because it conveys the message that coupling and\ncohesion are two distinct design principles, while there are in principle the\nsame design approaches, and only the time and space differ between them, and it\ncould also be a harmful state of being because we should not always aim for\nhigher cohesiveness without considering its cost.\n  In the course of this study, we aim to elucidate for the readers the meaning\nand underlying philosophy of the aforementioned paragraph.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09596v1", "cate": "cs.SE", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.10510", "title": "Chat with AI: The Surprising Turn of Real-time Video Communication from Human to AI", "authors": ["Jiangkai Wu", "Zhiyuan Ren", "Liming Liu", "Xinggong Zhang"], "categories": ["cs.NI", "cs.AI", "cs.HC", "cs.MM"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10510v1", "summary": "AI Video Chat emerges as a new paradigm for Real-time Communication (RTC),\nwhere one peer is not a human, but a Multimodal Large Language Model (MLLM).\nThis makes interaction between humans and AI more intuitive, as if chatting\nface-to-face with a real person. However, this poses significant challenges to\nlatency, because the MLLM inference takes up most of the response time, leaving\nvery little time for video streaming. Due to network uncertainty and\ninstability, transmission latency becomes a critical bottleneck preventing AI\nfrom being like a real person. To address this, we propose Artic, an\nAI-oriented Real-time Communication framework, exploring the network\nrequirement shift from \"humans watching video\" to \"AI understanding video\". To\nreduce bitrate dramatically while maintaining MLLM accuracy, we propose\nContext-Aware Video Streaming that recognizes the importance of each video\nregion for chat and allocates bitrate almost exclusively to chat-important\nregions. To avoid packet retransmission, we propose Loss-Resilient Adaptive\nFrame Rate that leverages previous frames to substitute for lost/delayed frames\nwhile avoiding bitrate waste. To evaluate the impact of video streaming quality\non MLLM accuracy, we build the first benchmark, named Degraded Video\nUnderstanding Benchmark (DeViBench). Finally, we discuss some open questions\nand ongoing solutions for AI Video Chat.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10510v1", "cate": "cs.NI", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09741", "title": "Majority Logic Decoding of Affine Grassmann Codes Over Nonbinary Fields", "authors": ["Fernando Piñero González", "Prasant Singh", "Rohit Yadav"], "categories": ["cs.IT", "math.IT", "14M15, 14G50, 94B27, 94B35"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      13 pages", "url": "http://arxiv.org/abs/2507.09741v1", "summary": "In this article, we consider the decoding problem of affine Grassmann codes\nover nonbinary fields. We use matrices of different ranks to construct a large\nset consisting of parity checks of affine Grassmann codes, which are orthogonal\nwith respect to a fixed coordinate. By leveraging the automorphism groups of\nthese codes, we generate a set of orthogonal parity checks for each coordinate.\nUsing these parity checks, we perform majority logic decoding to correct a\nlarge number of errors in affine Grassmann codes. The order of error correction\ncapability and the complexity of this decoder for affine Grassmann codes are\nthe same as those of the majority logic decoder for Grassmann codes proposed in\n[BS21].", "comment": "13 pages", "pdf_url": "http://arxiv.org/pdf/2507.09741v1", "cate": "cs.IT", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.10178", "title": "Pimba: A Processing-in-Memory Acceleration for Post-Transformer Large Language Model Serving", "authors": ["Wonung Kim", "Yubin Lee", "Yoonsung Kim", "Jinwoo Hwang", "Seongryong Oh", "Jiyong Jung", "Aziz Huseynov", "Woong Gyu Park", "Chang Hyun Park", "Divya Mahajan", "Jongse Park"], "categories": ["cs.AR", "cs.LG"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10178v1", "summary": "Transformers are the driving force behind today's Large Language Models\n(LLMs), serving as the foundation for their performance and versatility. Yet,\ntheir compute and memory costs grow with sequence length, posing scalability\nchallenges for long-context inferencing. In response, the algorithm community\nis exploring alternative architectures, such as state space models (SSMs),\nlinear attention, and recurrent neural networks (RNNs), which we refer to as\npost-transformers. This shift presents a key challenge: building a serving\nsystem that efficiently supports both transformer and post-transformer LLMs\nwithin a unified framework. To address this challenge, we analyze the\nperformance characteristics of transformer and post-transformer LLMs. Despite\ntheir algorithmic differences, both are fundamentally limited by memory\nbandwidth under batched inference due to attention in transformers and state\nupdates in post-transformers. Further analyses suggest two additional insights:\n(1) state update operations, unlike attention, incur high hardware cost, making\nper-bank PIM acceleration inefficient, and (2) different low-precision\narithmetic methods offer varying accuracy-area tradeoffs, while we identify\nMicrosoft's MX as the Pareto-optimal choice. Building on these insights, we\ndesign Pimba as an array of State-update Processing Units (SPUs), each shared\nbetween two banks to enable interleaved access to PIM. Each SPU includes a\nState-update Processing Engine (SPE) that comprises element-wise multipliers\nand adders using MX-based quantized arithmetic, enabling efficient execution of\nstate update and attention operations. Our evaluation shows that, compared to\nLLM-optimized GPU and GPU+PIM systems, Pimba achieves up to 3.2x and 2.1x\nhigher token generation throughput, respectively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10178v1", "cate": "cs.AR", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10026", "title": "EAT: QoS-Aware Edge-Collaborative AIGC Task Scheduling via Attention-Guided Diffusion Reinforcement Learning", "authors": ["Zhifei Xu", "Zhiqing Tang", "Jiong Lou", "Zhi Yao", "Xuan Xie", "Tian Wang", "Yinglong Wang", "Weijia Jia"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10026v1", "summary": "The growth of Artificial Intelligence (AI) and large language models has\nenabled the use of Generative AI (GenAI) in cloud data centers for diverse\nAI-Generated Content (AIGC) tasks. Models like Stable Diffusion introduce\nunavoidable delays and substantial resource overhead, which are unsuitable for\nusers at the network edge with high QoS demands. Deploying AIGC services on\nedge servers reduces transmission times but often leads to underutilized\nresources and fails to optimally balance inference latency and quality. To\naddress these issues, this paper introduces a QoS-aware\n\\underline{E}dge-collaborative \\underline{A}IGC \\underline{T}ask scheduling\n(EAT) algorithm. Specifically: 1) We segment AIGC tasks and schedule patches to\nvarious edge servers, formulating it as a gang scheduling problem that balances\ninference latency and quality while considering server heterogeneity, such as\ndiffering model distributions and cold start issues. 2) We propose a\nreinforcement learning-based EAT algorithm that uses an attention layer to\nextract load and task queue information from edge servers and employs a\ndiffusion-based policy network for scheduling, efficiently enabling model\nreuse. 3) We develop an AIGC task scheduling system that uses our EAT algorithm\nto divide tasks and distribute them across multiple edge servers for\nprocessing. Experimental results based on our system and large-scale\nsimulations show that our EAT algorithm can reduce inference latency by up to\n56\\% compared to baselines. We release our open-source code at\nhttps://github.com/zzf1955/EAT.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10026v1", "cate": "cs.DC", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.08879", "title": "A Multi-Level Strategy for Deepfake Content Moderation under EU Regulation", "authors": ["Max-Paul Förster", "Luca Deck", "Raimund Weidlich", "Niklas Kühl"], "categories": ["cs.CY", "cs.AI"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08879v1", "summary": "The growing availability and use of deepfake technologies increases risks for\ndemocratic societies, e.g., for political communication on online platforms.\nThe EU has responded with transparency obligations for providers and deployers\nof Artificial Intelligence (AI) systems and online platforms. This includes\nmarking deepfakes during generation and labeling deepfakes when they are\nshared. However, the lack of industry and enforcement standards poses an\nongoing challenge. Through a multivocal literature review, we summarize methods\nfor marking, detecting, and labeling deepfakes and assess their effectiveness\nunder EU regulation. Our results indicate that individual methods fail to meet\nregulatory and practical requirements. Therefore, we propose a multi-level\nstrategy combining the strengths of existing methods. To account for the masses\nof content on online platforms, our multi-level strategy provides scalability\nand practicality via a simple scoring mechanism. At the same time, it is\nagnostic to types of deepfake technology and allows for context-specific risk\nweighting.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08879v1", "cate": "cs.CY", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.01465", "title": "Pruning the Tree: Rethinking RPKI Architecture From The Ground Up", "authors": ["Haya Schulmann", "Niklas Vogel"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      Accepted for publication at NDSS2026", "url": "http://arxiv.org/abs/2507.01465v2", "summary": "Resource Public Key Infrastructure (RPKI) is a critical security mechanism\nfor BGP, but the complexity of its architecture is a growing concern as its\nadoption scales. Current RPKI design heavily reuses legacy PKI components, such\nas X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,\nwhich introduce excessive cryptographic validation, redundant metadata, and\ninefficiencies in both storage and processing. We show that these design\nchoices, although based on established standards, create significant\nperformance bottlenecks, increase the vulnerability surface, and hinder\nscalability for wide-scale Internet deployment.\n  In this paper, we perform the first systematic analysis of the root causes of\ncomplexity in RPKI's design and experimentally quantify their real-world\nimpact. We show that over 70\\% of validation time in RPKI relying parties is\nspent on certificate parsing and signature verification, much of it\nunnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),\na backwards-compatible redesign that preserves all security guarantees while\nsubstantially reducing protocol overhead. iRPKI eliminates EE-certificates and\nROA signatures, merges revocation and integrity objects, replaces verbose\nencodings with Protobuf, and restructures repository metadata for more\nefficient access. We experimentally demonstrate that our implementation of\niRPKI in the Routinator validator achieves a 20x speed-up of processing time,\n18x improvement of bandwidth requirements and 8x reduction in cache memory\nfootprint, while also eliminating classes of vulnerabilities that have led to\nat least 10 vulnerabilities in RPKI software. iRPKI significantly increases the\nfeasibility of deploying RPKI at scale in the Internet, and especially in\nconstrained environments. Our design may be deployed incrementally without\nimpacting existing operations.", "comment": "Accepted for publication at NDSS2026", "pdf_url": "http://arxiv.org/pdf/2507.01465v2", "cate": "cs.CR", "date": "2025-07-02", "updated": "2025-07-14"}
{"id": "2507.10164", "title": "Robust RL Control for Bipedal Locomotion with Closed Kinematic Chains", "authors": ["Egor Maslennikov", "Eduard Zaliaev", "Nikita Dudorov", "Oleg Shamanin", "Karanov Dmitry", "Gleb Afanasev", "Alexey Burkov", "Egor Lygin", "Simeon Nedelchev", "Evgeny Ponomarev"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10164v1", "summary": "Developing robust locomotion controllers for bipedal robots with closed\nkinematic chains presents unique challenges, particularly since most\nreinforcement learning (RL) approaches simplify these parallel mechanisms into\nserial models during training. We demonstrate that this simplification\nsignificantly impairs sim-to-real transfer by failing to capture essential\naspects such as joint coupling, friction dynamics, and motor-space control\ncharacteristics. In this work, we present an RL framework that explicitly\nincorporates closed-chain dynamics and validate it on our custom-built robot\nTopA. Our approach enhances policy robustness through symmetry-aware loss\nfunctions, adversarial training, and targeted network regularization.\nExperimental results demonstrate that our integrated approach achieves stable\nlocomotion across diverse terrains, significantly outperforming methods based\non simplified kinematic models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10164v1", "cate": "cs.RO", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10135", "title": "Riding the Carousel: The First Extensive Eye Tracking Analysis of Browsing Behavior in Carousel Recommenders", "authors": ["Santiago de Leon-Martinez", "Robert Moro", "Branislav Kveton", "Maria Bielikova"], "categories": ["cs.IR", "cs.HC"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10135v1", "summary": "Carousels have become the de-facto interface in online services. However,\nthere is a lack of research in carousels, particularly examining how\nrecommender systems may be designed differently than the traditional\nsingle-list interfaces. One of the key elements for understanding how to design\na system for a particular interface is understanding how users browse. For\ncarousels, users may browse in a number of different ways due to the added\ncomplexity of multiple topic defined-lists and swiping to see more items.\n  Eye tracking is the key to understanding user behavior by providing valuable,\ndirect information on how users see and navigate. In this work, we provide the\nfirst extensive analysis of the eye tracking behavior in carousel recommenders\nunder the free-browsing setting. To understand how users browse, we examine the\nfollowing research questions : 1) where do users start browsing, 2) how do\nusers transition from item to item within the same carousel and across\ncarousels, and 3) how does genre preference impact transitions?\n  This work addresses a gap in the field and provides the first extensive\nempirical results of eye tracked browsing behavior in carousels for improving\nrecommenders. Taking into account the insights learned from the above\nquestions, our final contribution is to provide suggestions to help carousel\nrecommender system designers optimize their systems for user browsing behavior.\nThe most important suggestion being to reorder the ranked item positions to\naccount for browsing after swiping.These contributions aim not only to help\nimprove current systems, but also to encourage and allow the design of new user\nmodels, systems, and metrics that are better suited to the complexity of\ncarousel interfaces.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10135v1", "cate": "cs.IR", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10045", "title": "Automating SPARQL Query Translations between DBpedia and Wikidata", "authors": ["Malte Christian Bartels", "Debayan Banerjee", "Ricardo Usbeck"], "categories": ["cs.AI", "cs.CL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      18 pages, 2 figues. Paper accepted at SEMANTiCS 2025 conference happening on September 2025", "url": "http://arxiv.org/abs/2507.10045v1", "summary": "This paper investigates whether state-of-the-art Large Language Models (LLMs)\ncan automatically translate SPARQL between popular Knowledge Graph (KG)\nschemas. We focus on translations between the DBpedia and Wikidata KG, and\nlater on DBLP and OpenAlex KG. This study addresses a notable gap in KG\ninteroperability research by rigorously evaluating LLM performance on\nSPARQL-to-SPARQL translation. Two benchmarks are assembled, where the first\nalign 100 DBpedia-Wikidata queries from QALD-9-Plus; the second contains 100\nDBLP queries aligned to OpenAlex, testing generalizability beyond encyclopaedic\nKGs. Three open LLMs: Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, and\nMistral-Large-Instruct-2407 are selected based on their sizes and architectures\nand tested with zero-shot, few-shot, and two chain-of-thought variants. Outputs\nwere compared with gold answers, and resulting errors were categorized. We find\nthat the performance varies markedly across models and prompting strategies,\nand that translations for Wikidata to DBpedia work far better than translations\nfor DBpedia to Wikidata.", "comment": "18 pages, 2 figues. Paper accepted at SEMANTiCS 2025 conference\n  happening on September 2025", "pdf_url": "http://arxiv.org/pdf/2507.10045v1", "cate": "cs.AI", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09168", "title": "Stable Score Distillation", "authors": ["Haiming Zhu", "Yangyang Xu", "Chenshu Xu", "Tingrui Shen", "Wenxi Liu", "Yong Du", "Jun Yu", "Shengfeng He"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09168v1", "summary": "Text-guided image and 3D editing have advanced with diffusion-based models,\nyet methods like Delta Denoising Score often struggle with stability, spatial\ncontrol, and editing strength. These limitations stem from reliance on complex\nauxiliary structures, which introduce conflicting optimization signals and\nrestrict precise, localized edits. We introduce Stable Score Distillation\n(SSD), a streamlined framework that enhances stability and alignment in the\nediting process by anchoring a single classifier to the source prompt.\nSpecifically, SSD utilizes Classifier-Free Guidance (CFG) equation to achieves\ncross-prompt alignment, and introduces a constant term null-text branch to\nstabilize the optimization process. This approach preserves the original\ncontent's structure and ensures that editing trajectories are closely aligned\nwith the source prompt, enabling smooth, prompt-specific modifications while\nmaintaining coherence in surrounding regions. Additionally, SSD incorporates a\nprompt enhancement branch to boost editing strength, particularly for style\ntransformations. Our method achieves state-of-the-art results in 2D and 3D\nediting tasks, including NeRF and text-driven style edits, with faster\nconvergence and reduced complexity, providing a robust and efficient solution\nfor text-guided editing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09168v1", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09599", "title": "Complexity and Coupling: A Functional Domain Approach", "authors": ["Aydin Homay"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09599v1", "summary": "This paper provides a precise and scientific definition of complexity and\ncoupling, grounded in the functional domain, particularly within industrial\ncontrol and automation systems (iCAS). We highlight the widespread ambiguity in\ndefining complexity and coupling, emphasizing that many existing definitions\nrooted in physical attributes lead to confusion and inconsistencies.\nFurthermore, we re-exhibit why coupled design inherently increases complexity\nand how potentially this complexity could be reduced. Drawing on examples from\nvarious disciplines, such as software engineering, industrial automation, and\nmechanical design, we demonstrate that complexity does not necessarily\ncorrelate with system size or the number of components, and coupling, unlike\ncommon belief in software engineering, actually does not occur in the physical\ndomain but in the functional domain. We conclude that effective design\nnecessitates addressing coupling and complexity within the functional domain.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09599v1", "cate": "cs.SE", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09719", "title": "Power Consumption Analysis of QKD Networks under Different Protocols and Detector Configurations", "authors": ["Jiaheng Xiong", "Qiaolun Zhang", "Yoann Piétri", "Raja Yehia", "Raouf Boutaba", "Francesco Musumeci", "Massimo Tornatore"], "categories": ["quant-ph", "cs.NI"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09719v1", "summary": "We analyze the power consumption of quantum key distribution (QKD) networks\nunder various protocol and detector configurations. Using realistic network\ntopologies, we evaluate discrete-variable vs continuous-variable QKD and\noptimize device placement, quantifying power trade-offs of SNSPD vs APD\ndetectors and the benefits of optical bypass.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09719v1", "cate": "quant-ph", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09833", "title": "Remote Safety Monitoring: Significance-Aware Status Updating for Situational Awareness", "authors": ["Tasmeen Zaman Ornee", "Md Kamran Chowdhury Shisher", "Clement Kam", "Yin Sun"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      14 pages, 8 figures", "url": "http://arxiv.org/abs/2507.09833v1", "summary": "In this study, we consider a problem of remote safety monitoring, where a\nmonitor pulls status updates from multiple sensors monitoring several\nsafety-critical situations. Based on the received updates, multiple estimators\ndetermine the current safety-critical situations. Due to transmission errors\nand limited channel resources, the received status updates may not be fresh,\nresulting in the possibility of misunderstanding the current safety situation.\nIn particular, if a dangerous situation is misinterpreted as safe, the safety\nrisk is high. We study the joint design of transmission scheduling and\nestimation for multi-sensor, multi-channel remote safety monitoring, aiming to\nminimize the loss due to the unawareness of potential danger. We show that the\njoint design of transmission scheduling and estimation can be reduced to a\nsequential optimization of estimation and scheduling. The scheduling problem\ncan be formulated as a Restless Multi-armed Bandit (RMAB) , for which it is\ndifficult to establish indexability. We propose a low-complexity Maximum Gain\nFirst (MGF) policy and prove it is asymptotically optimal as the numbers of\nsources and channels scale up proportionally, without requiring the\nindexability condition. We also provide an information-theoretic interpretation\nof the transmission scheduling problem. Numerical results show that our\nestimation and scheduling policies achieves higher performance gain over\nperiodic updating, randomized policy, and Maximum Age First (MAF) policy.", "comment": "14 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.09833v1", "cate": "cs.IT", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09776", "title": "Compute SNR-Optimal Analog-to-Digital Converters for Analog In-Memory Computing", "authors": ["Mihir Kavishwar", "Naresh Shanbhag"], "categories": ["eess.SP", "cs.AR"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      Code available at: this https URL", "url": "http://arxiv.org/abs/2507.09776v1", "summary": "Analog in-memory computing (AIMC) is an energy-efficient alternative to\ndigital architectures for accelerating machine learning and signal processing\nworkloads. However, its energy efficiency is limited by the high energy cost of\nthe column analog-to-digital converters (ADCs). Reducing the ADC precision is\nan effective approach to lowering its energy cost. However, doing so also\nreduces the AIMC's computational accuracy thereby making it critical to\nidentify the minimum precision required to meet a target accuracy. Prior works\noverestimate the ADC precision requirements by modeling quantization error as\ninput-independent noise, maximizing the signal-to-quantization-noise ratio\n(SQNR), and ignoring the discrete nature of ideal pre-ADC signal. We address\nthese limitations by developing analytical expressions for estimating the\ncompute signal-to-noise ratio (CSNR), a true metric of accuracy for AIMCs, and\npropose CACTUS, an algorithm to obtain CSNR-optimal ADC parameters. Using a\ncircuit-aware behavioral model of an SRAM-based AIMC in a 28nm CMOS process, we\nshow that for a 256-dimensional binary dot product, CACTUS reduces the ADC\nprecision requirements by 3b while achieving 6dB higher CSNR over prior\nmethods. We also delineate operating conditions under which our proposed\nCSNR-optimal ADCs outperform conventional SQNR-optimal ADCs.", "comment": "Code available at: https://github.com/mihirvk2/CSNR-optimal-ADC", "pdf_url": "http://arxiv.org/pdf/2507.09776v1", "cate": "eess.SP", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.10069", "title": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal Parallelism", "authors": ["Zedong Liu", "Shenggan Cheng", "Guangming Tan", "Yang You", "Dingwen Tao"], "categories": ["cs.DC", "cs.LG"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10069v1", "summary": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we propose Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10069v1", "cate": "cs.DC", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09020", "title": "ESG and the Cost of Capital: Insights from an AI-Assisted Systematic Literature Review", "authors": ["Ebenezer Asem", "Ruijie Fan", "Gloria Y. Tian"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09020v1", "summary": "This paper explores how AI-powered tools could be leveraged to streamline the\nprocess of identifying, screening, and analyzing relevant literature in\nacademic research. More specifically, we examine the documented relationship\nbetween environmental, social, and governance (ESG) factors and the cost of\ncapital (CoC). By applying an AI-assisted workflow, we identified 36 published\nstudies, synthesized their key findings, and highlighted relevant theories,\nmoderators, and methodological challenges. Our analyses demonstrate the value\nof AI tools in enhancing business research processes and also contribute to the\ngrowing literature on the importance of ESG in the field of corporate finance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09020v1", "cate": "cs.CY", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09435", "title": "GeoWarp: An automatically differentiable and GPU-accelerated implicit MPM framework for geomechanics based on NVIDIA Warp", "authors": ["Yidong Zhao", "Xuan Li", "Chenfanfu Jiang", "Jinhyun Choo"], "categories": ["cs.CE", "cs.MS", "physics.comp-ph"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09435v1", "summary": "The material point method (MPM), a hybrid Lagrangian-Eulerian particle\nmethod, is increasingly used to simulate large-deformation and\nhistory-dependent behavior of geomaterials. While explicit time integration\ndominates current MPM implementations due to its algorithmic simplicity, such\nschemes are unsuitable for quasi-static and long-term processes typical in\ngeomechanics. Implicit MPM formulations are free of these limitations but\nremain less adopted, largely due to the difficulty of computing the Jacobian\nmatrix required for Newton-type solvers, especially when consistent tangent\noperators should be derived for complex constitutive models. In this paper, we\nintroduce GeoWarp -- an implicit MPM framework for geomechanics built on NVIDIA\nWarp -- that exploits GPU parallelism and reverse-mode automatic\ndifferentiation to compute Jacobians without manual derivation. To enhance\nefficiency, we develop a sparse Jacobian construction algorithm that leverages\nthe localized particle-grid interactions intrinsic to MPM. The framework is\nverified through forward and inverse examples in large-deformation\nelastoplasticity and coupled poromechanics. Results demonstrate that GeoWarp\nprovides a robust, scalable, and extensible platform for differentiable\nimplicit MPM simulation in computational geomechanics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09435v1", "cate": "cs.CE", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.03278", "title": "Securing Transformer-based AI Execution via Unified TEEs and Crypto-protected Accelerators", "authors": ["Jiaqi Xue", "Yifei Zhao", "Mengxin Zheng", "Fan Yao", "Yan Solihin", "Qian Lou"], "categories": ["cs.CR", "cs.LG"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      15 pages", "url": "http://arxiv.org/abs/2507.03278v2", "summary": "Recent advances in Transformer models, e.g., large language models (LLMs),\nhave brought tremendous breakthroughs in various artificial intelligence (AI)\ntasks, leading to their wide applications in many security-critical domains.\nDue to their unprecedented scale and prohibitively high development cost, these\nmodels have become highly valuable intellectual property for AI stakeholders\nand are increasingly deployed via machine learning as a service (MLaaS).\nHowever, MLaaS often runs on untrusted cloud infrastructure, exposing data and\nmodels to potential breaches. Mainstream protection mechanisms leverage trusted\nexecution environments (TEEs) where confidentiality and integrity for secretive\ndata are shielded using hardware-based encryption and integrity checking.\nUnfortunately, running model inference entirely within TEEs is subject to\nnon-trivial slowdown, which is further exacerbated in LLMs due to the\nsubstantial computation and memory footprint involved. Recent studies reveal\nthat the hybrid TEE-based scheme offloading partial model inference operations\nto the untrusted accelerators (e.g., GPU) is a promising solution. However,\nprior offloading schemes fail to ensure dual protection of data and model in\nTransformer inference, as they cannot securely offload critical operations,\ni.e., Attention and SoftMax, forcing these computations to remain confined\nwithin TEEs. To address these challenges, we propose TwinShield, a framework\nenabling secure Transformer inference in heterogeneous TEE and accelerator\nsystems with dual protection for both model and data. TwinShield offloads ~87%\nof computation to GPUs and delivers 4.0x - 6.1x speedups over previous\napproaches across various Transformer models.", "comment": "15 pages", "pdf_url": "http://arxiv.org/pdf/2507.03278v2", "cate": "cs.CR", "date": "2025-07-04", "updated": "2025-07-13"}
{"id": "2507.10204", "title": "REACT: Real-time Entanglement-Aware Coverage Path Planning for Tethered Underwater Vehicles", "authors": ["Abdelhakim Amer", "Mohit Mehindratta", "Yury Brodskiy", "Bilal Wehbe", "Erdal Kayacan"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10204v1", "summary": "Inspection of complex underwater structures with tethered underwater vehicles\nis often hindered by the risk of tether entanglement. We propose REACT\n(real-time entanglement-aware coverage path planning for tethered underwater\nvehicles), a framework designed to overcome this limitation. REACT comprises a\nfast geometry-based tether model using the signed distance field (SDF) map for\naccurate, real-time simulation of taut tether configurations around arbitrary\nstructures in 3D. This model enables an efficient online replanning strategy by\nenforcing a maximum tether length constraint, thereby actively preventing\nentanglement. By integrating REACT into a coverage path planning framework, we\nachieve safe and optimal inspection paths, previously challenging due to tether\nconstraints. The complete REACT framework's efficacy is validated in a pipe\ninspection scenario, demonstrating safe, entanglement-free navigation and\nfull-coverage inspection. Simulation results show that REACT achieves complete\ncoverage while maintaining tether constraints and completing the total mission\n20% faster than conventional planners, despite a longer inspection time due to\nproactive avoidance of entanglement that eliminates extensive post-mission\ndisentanglement. Real-world experiments confirm these benefits, where REACT\ncompletes the full mission, while the baseline planner fails due to physical\ntether entanglement.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10204v1", "cate": "cs.RO", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10208", "title": "Survey for Categorising Explainable AI Studies Using Data Analysis Task Frameworks", "authors": ["Hamzah Ziadeh", "Hendrik Knoche"], "categories": ["cs.AI", "cs.HC"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10208v1", "summary": "Research into explainable artificial intelligence (XAI) for data analysis\ntasks suffer from a large number of contradictions and lack of concrete design\nrecommendations stemming from gaps in understanding the tasks that require AI\nassistance. In this paper, we drew on multiple fields such as visual analytics,\ncognition, and dashboard design to propose a method for categorising and\ncomparing XAI studies under three dimensions: what, why, and who. We identified\nthe main problems as: inadequate descriptions of tasks, context-free studies,\nand insufficient testing with target users. We propose that studies should\nspecifically report on their users' domain, AI, and data analysis expertise to\nillustrate the generalisability of their findings. We also propose study\nguidelines for designing and reporting XAI tasks to improve the XAI community's\nability to parse the rapidly growing field. We hope that our contribution can\nhelp researchers and designers better identify which studies are most relevant\nto their work, what gaps exist in the research, and how to handle contradictory\nresults regarding XAI design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10208v1", "cate": "cs.AI", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10076", "title": "On Gradual Semantics for Assumption-Based Argumentation", "authors": ["Anna Rapberger", "Fabrizio Russo", "Antonio Rago", "Francesca Toni"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10076v1", "summary": "In computational argumentation, gradual semantics are fine-grained\nalternatives to extension-based and labelling-based semantics . They ascribe a\ndialectical strength to (components of) arguments sanctioning their degree of\nacceptability. Several gradual semantics have been studied for abstract,\nbipolar and quantitative bipolar argumentation frameworks (QBAFs), as well as,\nto a lesser extent, for some forms of structured argumentation. However, this\nhas not been the case for assumption-based argumentation (ABA), despite it\nbeing a popular form of structured argumentation with several applications\nwhere gradual semantics could be useful. In this paper, we fill this gap and\npropose a family of novel gradual semantics for equipping assumptions, which\nare the core components in ABA frameworks, with dialectical strengths. To do\nso, we use bipolar set-based argumentation frameworks as an abstraction of\n(potentially non-flat) ABA frameworks and generalise state-of-the-art modular\ngradual semantics for QBAFs. We show that our gradual ABA semantics satisfy\nsuitable adaptations of desirable properties of gradual QBAF semantics, such as\nbalance and monotonicity. We also explore an argument-based approach that\nleverages established QBAF modular semantics directly, and use it as baseline.\nFinally, we conduct experiments with synthetic ABA frameworks to compare our\ngradual ABA semantics with its argument-based counterpart and assess\nconvergence.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10076v1", "cate": "cs.AI", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09180", "title": "Learning and Transferring Better with Depth Information in Visual Reinforcement Learning", "authors": ["Zichun Xu", "Yuntao Li", "Zhaomin Wang", "Lei Zhuang", "Guocai Yang", "Jingdong Zhao"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09180v1", "summary": "Depth information is robust to scene appearance variations and inherently\ncarries 3D spatial details. In this paper, a visual backbone based on the\nvision transformer is proposed to fuse RGB and depth modalities for enhancing\ngeneralization. Different modalities are first processed by separate CNN stems,\nand the combined convolutional features are delivered to the scalable vision\ntransformer to obtain visual representations. Moreover, a contrastive\nunsupervised learning scheme is designed with masked and unmasked tokens to\naccelerate the sample efficiency during the reinforcement learning progress.\nFor sim2real transfer, a flexible curriculum learning schedule is developed to\ndeploy domain randomization over training processes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09180v1", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09665", "title": "Is Quantization a Deal-breaker? Empirical Insights from Large Code Models", "authors": ["Saima Afrin", "Bowen Xu", "Antonio Mastropaolo"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09665v1", "summary": "The growing scale of large language models (LLMs) not only demands extensive\ncomputational resources but also raises environmental concerns due to their\nincreasing carbon footprint. Model quantization emerges as an effective\napproach that can reduce the resource demands of LLMs by decreasing parameter\nprecision without substantially affecting performance (e.g., 16 bit to 4 bit).\nWhile recent studies have established quantization as a promising approach for\noptimizing large code models (LCMs), a specialized subset of LLMs tailored for\nautomated software engineering, their findings offer only limited insights into\nits practical implications. Specifically, current investigations focus only on\nthe functional correctness of the code generated by quantized models,\nneglecting how quantization impacts critical aspects of code quality such as\nreliability, maintainability, and security. To bridge this gap, our study\ninvestigates the effects of quantization on the qualitative aspects of\nautomatically generated code. We apply Activation-aware Weight Quantization\n(AWQ) to two widely used code models, CodeLlama and DeepSeekCoder, to generate\nJava and Python code. Using state-of-the-art static analysis tools, we evaluate\nsoftware quality metrics and static features including cyclomatic complexity,\ncognitive complexity, and lines of code. Our findings reveal that quantization\nis a robust technique that not only preserves functional correctness, but also\nretains key qualitative code attributes sought after by developers, such as\nmaintainability and structural simplicity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09665v1", "cate": "cs.SE", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.10259", "title": "Cross-Timeslot Optimization for Distributed GPU Inference Using Reinforcement Learning", "authors": ["Chengze Du", "Zhiwei Yu", "Heng Xu", "Haojie Wang", "Bo liu", "Jialong Li"], "categories": ["cs.DC", "cs.NI"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      17 pages, 12 figures", "url": "http://arxiv.org/abs/2507.10259v1", "summary": "The rapid growth of large language model (LLM) services imposes increasing\ndemands on distributed GPU inference infrastructure. Most existing scheduling\nsystems rely on the current system state to make decisions, without considering\nhow task demand and resource availability evolve over time. This lack of\ntemporal awareness leads to inefficient GPU utilization, high task migration\noverhead, and poor system responsiveness under dynamic workloads. In this work,\nwe identify the fundamental limitations of these instantaneous-state-only\nscheduling approaches and propose Temporal Optimal Resource scheduling via\nTwo-layer Architecture (TORTA). TORTA introduces a spatiotemporal scheduling\nframework that captures both long-term workload patterns and short-term\nexecution constraints. It adopts a two-layer design: a macro-level scheduler\nleverages reinforcement learning and optimal transport to coordinate\ninter-region task distribution, while a micro-level allocator refines\ntask-to-server assignments within each region to reduce latency and switching\ncosts. Experimental results across multiple network topologies show that TORTA\nreduces average inference response time by up to 15\\%, improves load balance by\napproximately 4-5\\%, and cuts total operational cost by 10-20\\% compared to\nstate-of-the-art baseline methods.", "comment": "17 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2507.10259v1", "cate": "cs.DC", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09843", "title": "Incomplete Multiview Learning via Wyner Common Information", "authors": ["AbdAlRahman Odeh", "Teng-Hui Huang", "Hesham El Gamal"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      6 pages, 2 figures, ITW2025", "url": "http://arxiv.org/abs/2507.09843v1", "summary": "Incomplete multiview clustering is of high recent interest, fueled by the\nadvancement of common information-based deep multiview learning. The practical\nscenarios where unpaired multiview data with missing values have wide\napplications in generative learning, cross-modal retrieval, and wireless device\nidentification problems. Following the perspective that the shared information\nbetween the incomplete multiview data aligns with the cluster targets, recent\nworks have generalized the well-known common information frameworks in\ninformation theory multiview learning problems, with improved performance\nreported. Different from previous works, we extend the frameworks to incomplete\nmultiview clustering problems and propose an efficient solver: Wyner Incomplete\nMultiView Clustering (WyIMVC). Interestingly, the common randomness in WyIMVC\nallows for joint clustering and missing value inference in contrast to the\ncompared methods in the literature. Moreover, leveraging the\ndifference-of-convex structure of the formulated problems, we propose an\nefficient solver with a convergence guarantee independent of initialization.\nEmpirically, our solver outperforms the state-of-the-art solvers in a range of\nincomplete multiview datasets with varying numbers of views and dimensions.", "comment": "6 pages, 2 figures, ITW2025", "pdf_url": "http://arxiv.org/pdf/2507.09843v1", "cate": "cs.IT", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09948", "title": "Iceberg: Enhancing HLS Modeling with Synthetic Data", "authors": ["Zijian Ding", "Tung Nguyen", "Weikai Li", "Aditya Grover", "Yizhou Sun", "Jason Cong"], "categories": ["cs.LG", "cs.AR"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      9 pages. accepted to ICLAD'25", "url": "http://arxiv.org/abs/2507.09948v1", "summary": "Deep learning-based prediction models for High-Level Synthesis (HLS) of\nhardware designs often struggle to generalize. In this paper, we study how to\nclose the generalizability gap of these models through pretraining on synthetic\ndata and introduce Iceberg, a synthetic data augmentation approach that expands\nboth large language model (LLM)-generated programs and weak labels of unseen\ndesign configurations. Our weak label generation method is integrated with an\nin-context model architecture, enabling meta-learning from actual and proximate\nlabels. Iceberg improves the geometric mean modeling accuracy by $86.4\\%$ when\nadapt to six real-world applications with few-shot examples and achieves a\n$2.47\\times$ and a $1.12\\times$ better offline DSE performance when adapting to\ntwo different test datasets. Our open-sourced code is here:\n\\href{https://github.com/UCLA-VAST/iceberg}{https://github.com/UCLA-VAST/iceberg}", "comment": "9 pages. accepted to ICLAD'25", "pdf_url": "http://arxiv.org/pdf/2507.09948v1", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10139", "title": "Large-Scale Graph Building in Dynamic Environments: Low Latency and High Quality", "authors": ["Filipe Miguel Gonçalves de Almeida", "CJ Carey", "Hendrik Fichtenberger", "Jonathan Halcrow", "Silvio Lattanzi", "André Linhares", "Tao Meng", "Ashkan Norouzi-Fard", "Nikos Parotsidis", "Bryan Perozzi", "David Simcha"], "categories": ["cs.DC", "cs.LG"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10139v1", "summary": "Learning and constructing large-scale graphs has attracted attention in\nrecent decades, resulting in a rich literature that introduced various systems,\ntools, and algorithms. Grale is one of such tools that is designed for offline\nenvironments and is deployed in more than 50 different industrial settings at\nGoogle. Grale is widely applicable because of its ability to efficiently learn\nand construct a graph on datasets with multiple types of features. However, it\nis often the case that applications require the underlying data to evolve\ncontinuously and rapidly and the updated graph needs to be available with low\nlatency. Such setting make the use of Grale prohibitive. While there are\nApproximate Nearest Neighbor (ANN) systems that handle dynamic updates with low\nlatency, they are mostly limited to similarities over a single embedding.\n  In this work, we introduce a system that inherits the advantages and the\nquality of Grale, and maintains a graph construction in a dynamic setting with\ntens of milliseconds of latency per request. We call the system Dynamic Grale\nUsing ScaNN (Dynamic GUS). Our system has a wide range of applications with\nover 10 deployments at Google. One of the applications is in Android Security\nand Privacy, where Dynamic Grale Using ScaNN enables capturing harmful\napplications 4 times faster, before they can reach users.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10139v1", "cate": "cs.DC", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09060", "title": "CALMA: A Process for Deriving Context-aligned Axes for Language Model Alignment", "authors": ["Prajna Soni", "Deepika Raman", "Dylan Hadfield-Menell"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09060v1", "summary": "Datasets play a central role in AI governance by enabling both evaluation\n(measuring capabilities) and alignment (enforcing values) along axes such as\nhelpfulness, harmlessness, toxicity, quality, and more. However, most alignment\nand evaluation datasets depend on researcher-defined or developer-defined axes\ncurated from non-representative samples. As a result, developers typically\nbenchmark models against broad (often Western-centric) values that overlook the\nvaried contexts of their real-world deployment. Consequently, models trained on\nsuch proxies can fail to meet the needs and expectations of diverse user\ncommunities within these deployment contexts. To bridge this gap, we introduce\nCALMA (Context-aligned Axes for Language Model Alignment), a grounded,\nparticipatory methodology for eliciting context-relevant axes for evaluation\nand alignment. In a pilot with two distinct communities, CALMA surfaced novel\npriorities that are absent from standard benchmarks. Our findings demonstrate\nthe value of evaluation practices based on open-ended and use-case-driven\nprocesses. Our work advances the development of pluralistic, transparent, and\ncontext-sensitive alignment pipelines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09060v1", "cate": "cs.CY", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09527", "title": "EV-STLLM: Electric vehicle charging forecasting based on spatio-temporal large language models with multi-frequency and multi-scale information fusion", "authors": ["Hang Fan", "Yunze Chai", "Chenxi Liu", "Weican Liu", "Zuhan Zhang", "Wencai Run", "Dunnan Liu"], "categories": ["cs.CE"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09527v1", "summary": "With the proliferation of electric vehicles (EVs), accurate charging demand\nand station occupancy forecasting are critical for optimizing urban energy and\nthe profit of EVs aggregator. Existing approaches in this field usually\nstruggle to capture the complex spatio-temporal dependencies in EV charging\nbehaviors, and their limited model parameters hinder their ability to learn\ncomplex data distribution representations from large datasets. To this end, we\npropose a novel EV spatio-temporal large language model (EV-STLLM) for accurate\nprediction. Our proposed framework is divided into two modules. In the data\nprocessing module, we utilize variational mode decomposition (VMD) for data\ndenoising, and improved complete ensemble empirical mode decomposition with\nadaptive noise (ICEEMDAN) for data multi-frequency decomposition. Fuzzy\ninformation granulation (FIG) for extracting multi-scale information.\nAdditionally, ReliefF is used for feature selection to mitigate redundancy. In\nthe forecasting module, the EV-STLLM is used to directly achieve EV charging\nand occupancy forecasting. Firstly, we fully capture the intrinsic\nspatio-temporal characteristics of the data by integrating adjacency matrices\nderived from the regional stations network and spatio-temporal-frequency\nembedding information. Then, the partially frozen graph attention (PFGA) module\nis utilized to maintain the sequential feature modeling capabilities of the\npre-trained large model while incorporating EV domain knowledge. Extensive\nexperiments using real-world data from Shenzhen, China, demonstrate that our\nproposed framework can achieve superior accuracy and robustness compared to the\nstate-of-the-art benchmarks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09527v1", "cate": "cs.CE", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.05649", "title": "DESIGN: Encrypted GNN Inference via Server-Side Input Graph Pruning", "authors": ["Kaixiang Zhao", "Joseph Yousry Attalla", "Qian Lou", "Yushun Dong"], "categories": ["cs.CR", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      Under Review in Conference on Neural Information Processing Systems (NeurIPS 2025)", "url": "http://arxiv.org/abs/2507.05649v2", "summary": "Graph Neural Networks (GNNs) have achieved state-of-the-art performance in\nvarious graph-based learning tasks. However, enabling privacy-preserving GNNs\nin encrypted domains, such as under Fully Homomorphic Encryption (FHE),\ntypically incurs substantial computational overhead, rendering real-time and\nprivacy-preserving inference impractical. In this work, we propose DESIGN\n(EncrypteD GNN Inference via sErver-Side Input Graph pruNing), a novel\nframework for efficient encrypted GNN inference. DESIGN tackles the critical\nefficiency limitations of existing FHE GNN approaches, which often overlook\ninput data redundancy and apply uniform computational strategies. Our framework\nachieves significant performance gains through a hierarchical optimization\nstrategy executed entirely on the server: first, FHE-compatible node importance\nscores (based on encrypted degree statistics) are computed from the encrypted\ngraph. These scores then guide a homomorphic partitioning process, generating\nmulti-level importance masks directly under FHE. This dynamically generated\nmask facilitates both input graph pruning (by logically removing unimportant\nelements) and a novel adaptive polynomial activation scheme, where activation\ncomplexity is tailored to node importance levels. Empirical evaluations\ndemonstrate that DESIGN substantially accelerates FHE GNN inference compared to\nstate-of-the-art methods while maintaining competitive model accuracy,\npresenting a robust solution for secure graph analytics. Our implementation is\npublicly available at https://github.com/LabRAI/DESIGN.", "comment": "Under Review in Conference on Neural Information Processing Systems\n  (NeurIPS 2025)", "pdf_url": "http://arxiv.org/pdf/2507.05649v2", "cate": "cs.CR", "date": "2025-07-08", "updated": "2025-07-14"}
{"id": "2507.10290", "title": "TOP: Trajectory Optimization via Parallel Optimization towards Constant Time Complexity", "authors": ["Jiajun Yu", "Nanhe Chen", "Guodong Liu", "Chao Xu", "Fei Gao", "Yanjun Cao"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 pages, submitted to RA-L", "url": "http://arxiv.org/abs/2507.10290v1", "summary": "Optimization has been widely used to generate smooth trajectories for motion\nplanning. However, existing trajectory optimization methods show weakness when\ndealing with large-scale long trajectories. Recent advances in parallel\ncomputing have accelerated optimization in some fields, but how to efficiently\nsolve trajectory optimization via parallelism remains an open question. In this\npaper, we propose a novel trajectory optimization framework based on the\nConsensus Alternating Direction Method of Multipliers (CADMM) algorithm, which\ndecomposes the trajectory into multiple segments and solves the subproblems in\nparallel. The proposed framework reduces the time complexity to O(1) per\niteration to the number of segments, compared to O(N) of the state-of-the-art\n(SOTA) approaches. Furthermore, we introduce a closed-form solution that\nintegrates convex linear and quadratic constraints to speed up the\noptimization, and we also present numerical solutions for general inequality\nconstraints. A series of simulations and experiments demonstrate that our\napproach outperforms the SOTA approach in terms of efficiency and smoothness.\nEspecially for a large-scale trajectory, with one hundred segments, achieving\nover a tenfold speedup. To fully explore the potential of our algorithm on\nmodern parallel computing architectures, we deploy our framework on a GPU and\nshow high performance with thousands of segments.", "comment": "8 pages, submitted to RA-L", "pdf_url": "http://arxiv.org/pdf/2507.10290v1", "cate": "cs.RO", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10500", "title": "Scene-Aware Conversational ADAS with Generative AI for Real-Time Driver Assistance", "authors": ["Kyungtae Han", "Yitao Chen", "Rohit Gupta", "Onur Altintas"], "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.HC"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10500v1", "summary": "While autonomous driving technologies continue to advance, current Advanced\nDriver Assistance Systems (ADAS) remain limited in their ability to interpret\nscene context or engage with drivers through natural language. These systems\ntypically rely on predefined logic and lack support for dialogue-based\ninteraction, making them inflexible in dynamic environments or when adapting to\ndriver intent. This paper presents Scene-Aware Conversational ADAS (SC-ADAS), a\nmodular framework that integrates Generative AI components including large\nlanguage models, vision-to-text interpretation, and structured function calling\nto enable real-time, interpretable, and adaptive driver assistance. SC-ADAS\nsupports multi-turn dialogue grounded in visual and sensor context, allowing\nnatural language recommendations and driver-confirmed ADAS control. Implemented\nin the CARLA simulator with cloud-based Generative AI, the system executes\nconfirmed user intents as structured ADAS commands without requiring model\nfine-tuning. We evaluate SC-ADAS across scene-aware, conversational, and\nrevisited multi-turn interactions, highlighting trade-offs such as increased\nlatency from vision-based context retrieval and token growth from accumulated\ndialogue history. These results demonstrate the feasibility of combining\nconversational reasoning, scene perception, and modular ADAS control to support\nthe next generation of intelligent driver assistance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10500v1", "cate": "cs.RO", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10106", "title": "BlueGlass: A Framework for Composite AI Safety", "authors": ["Harshal Nandigramwar", "Syed Qutub", "Kay-Ulrich Scholl"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Accepted at ICML 2025 [Actionable Interpretability Workshop]", "url": "http://arxiv.org/abs/2507.10106v1", "summary": "As AI systems become increasingly capable and ubiquitous, ensuring the safety\nof these systems is critical. However, existing safety tools often target\ndifferent aspects of model safety and cannot provide full assurance in\nisolation, highlighting a need for integrated and composite methodologies. This\npaper introduces BlueGlass, a framework designed to facilitate composite AI\nsafety workflows by providing a unified infrastructure enabling the integration\nand composition of diverse safety tools that operate across model internals and\noutputs. Furthermore, to demonstrate the utility of this framework, we present\nthree safety-oriented analyses on vision-language models for the task of object\ndetection: (1) distributional evaluation, revealing performance trade-offs and\npotential failure modes across distributions; (2) probe-based analysis of layer\ndynamics highlighting shared hierarchical learning via phase transition; and\n(3) sparse autoencoders identifying interpretable concepts. More broadly, this\nwork contributes foundational infrastructure and findings for building more\nrobust and reliable AI systems.", "comment": "Accepted at ICML 2025 [Actionable Interpretability Workshop]", "pdf_url": "http://arxiv.org/pdf/2507.10106v1", "cate": "cs.AI", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09183", "title": "Revisiting Pool-based Prompt Learning for Few-shot Class-incremental Learning", "authors": ["Yongwei Jiang", "Yixiong Zou", "Yuhua Li", "Ruixuan Li"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025, 11 pages", "url": "http://arxiv.org/abs/2507.09183v1", "summary": "Few-Shot Class-Incremental Learning (FSCIL) faces dual challenges of data\nscarcity and incremental learning in real-world scenarios. While pool-based\nprompting methods have demonstrated success in traditional incremental\nlearning, their effectiveness in FSCIL settings remains unexplored. This paper\npresents the first study of current prompt pool methods in FSCIL tasks,\nrevealing an unanticipated performance degradation in incremental sessions.\nThrough comprehensive analysis, we identify that this phenomenon stems from\ntoken-dimension saturation: with limited data, excessive prompts compete for\ntask-relevant information, leading to model overfitting. Based on this finding,\nwe propose LGSP-Prompt (Local-Global Spatial Prompting), which innovatively\nshifts pool-based prompt learning from the token dimension to the spatial\ndimension. LGSP-Prompt generates spatial prompts by synergistically combining\nlocal spatial features and global frequency-domain representations to highlight\nkey patterns in input images. We construct two spatial prompt pools enabling\ndynamic prompt selection to maintain acquired knowledge while effectively\nlearning novel sessions. Extensive experiments demonstrate that our approach\nachieves state-of-the-art performance across multiple FSCIL benchmarks, showing\nsignificant advantages in both base knowledge preservation and incremental\nlearning. Our implementation is available at\nhttps://github.com/Jywsuperman/LGSP.", "comment": "Accepted to ICCV 2025, 11 pages", "pdf_url": "http://arxiv.org/pdf/2507.09183v1", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09790", "title": "Prompting for Performance: Exploring LLMs for Configuring Software", "authors": ["Helge Spieker", "Théo Matricon", "Nassim Belmecheri", "Jørn Eirik Betten", "Gauthier Le Bartz Lyan", "Heraldo Borges", "Quentin Mazouni", "Dennis Gross", "Arnaud Gotlieb", "Mathieu Acher"], "categories": ["cs.SE", "cs.AI", "cs.PF"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09790v1", "summary": "Software systems usually provide numerous configuration options that can\naffect performance metrics such as execution time, memory usage, binary size,\nor bitrate. On the one hand, making informed decisions is challenging and\nrequires domain expertise in options and their combinations. On the other hand,\nmachine learning techniques can search vast configuration spaces, but with a\nhigh computational cost, since concrete executions of numerous configurations\nare required. In this exploratory study, we investigate whether large language\nmodels (LLMs) can assist in performance-oriented software configuration through\nprompts. We evaluate several LLMs on tasks including identifying relevant\noptions, ranking configurations, and recommending performant configurations\nacross various configurable systems, such as compilers, video encoders, and SAT\nsolvers. Our preliminary results reveal both positive abilities and notable\nlimitations: depending on the task and systems, LLMs can well align with expert\nknowledge, whereas hallucinations or superficial reasoning can emerge in other\ncases. These findings represent a first step toward systematic evaluations and\nthe design of LLM-based solutions to assist with software configuration.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09790v1", "cate": "cs.SE", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2407.04977", "title": "Cost and Power-Consumption Analysis for Power Profile Monitoring with Multiple Monitors per Link in Optical Networks", "authors": ["Qiaolun Zhang", "Patricia Layec", "Alix May", "Annalisa Morea", "Aryanaz Attarpour", "Massimo Tornatore"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2407.04977v4", "summary": "As deploying large amounts of monitoring equipment results in elevated cost\nand power consumption, novel low-cost monitoring methods are being continuously\ninvestigated. A new technique called Power Profile Monitoring (PPM) has\nrecently gained traction thanks to its ability to monitor an entire lightpath\nusing a single post-processing unit at the lightpath receiver. PPM does not\nrequire to deploy an individual monitor for each span, as in the traditional\nmonitoring technique using Optical Time-Domain Reflectometer (OTDR). In this\nwork, we aim to quantify the cost and power consumption of PPM (using OTDR as a\nbaseline reference), as this analysis can provide guidelines for the\nimplementation and deployment of PPM. First, we discuss how PPM and OTDR\nmonitors are deployed, and we formally state a new Optimized Monitoring\nPlacement (OMP) problem for PPM. Solving the OMP problem allows to identify the\nminimum number of PPM monitors that guarantees that all links in the networks\nare monitored by at least $n$ PPM monitors (note that using $n>1$ allows for\nincreased monitoring accuracy). We prove the NP-hardness of the OMP problem and\nformulate it using an Integer Linear Programming (ILP) model. Finally, we also\ndevise a heuristic algorithm for the OMP problem to scale to larger topologies.\nOur numerical results, obtained on realistic topologies, suggest that the cost\n(and power) of one PPM module should be lower than 2.6 times that of one OTDR\nfor nation-wide and 10.2 times for continental-wide topology.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2407.04977v4", "cate": "cs.NI", "date": "2024-07-06", "updated": "2025-07-12"}
{"id": "2507.09856", "title": "Several new classes of self-orthogonal minimal linear codes violating the Ashikhmin-Barg condition", "authors": ["Wengang Jin", "Kangquan Li", "Longjiang Qu"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09856v1", "summary": "Linear codes have attracted considerable attention in coding theory and\ncryptography due to their significant applications in secret sharing schemes,\nsecure two-party computation, Galois geometries, among others. As two special\nsubclasses of linear codes, minimal linear codes and self-orthogonal linear\ncodes are of particular interest. Constructing linear codes that possess both\nminimality and self-orthogonality is very interesting. The main purpose of this\npaper is to construct self-orthogonal minimal linear codes that violate the\nAshikhmin-Barg (AB for short) condition over the finite field $\\mathbb{F}_p$.\nFirst, we present several classes of self-orthogonal minimal linear codes\nviolating the AB condition over the finite field $\\mathbb{F}_2$ and determine\ntheir weight distributions. Next, for any odd prime $p$, we construct two\nclasses of self-orthogonal linear codes from $p$-ary functions, which contain\nsome optimal or almost optimal codes. Finally, based on plateaued functions, we\nconstruct two classes of self-orthogonal linear codes that violate the AB\ncondition. Their weight distributions are also provided. To the best of our\nknowledge, this paper is the first to investigate the constructions of linear\ncodes that violate the AB condition and satisfy self-orthogonality.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09856v1", "cate": "cs.IT", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10338", "title": "AssertCoder: LLM-Based Assertion Generation via Multimodal Specification Extraction", "authors": ["Enyuan Tian", "Yiwei Ci", "Qiusong Yang", "Yufeng Li", "Zhichao Lyu"], "categories": ["cs.SE", "cs.AR", "cs.LO"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      7 pages, 3 figures", "url": "http://arxiv.org/abs/2507.10338v1", "summary": "Assertion-Based Verification (ABV) is critical for ensuring functional\ncorrectness in modern hardware systems. However, manually writing high-quality\nSVAs remains labor-intensive and error-prone. To bridge this gap, we propose\nAssertCoder, a novel unified framework that automatically generates\nhigh-quality SVAs directly from multimodal hardware design specifications.\nAssertCoder employs a modality-sensitive preprocessing to parse heterogeneous\nspecification formats (text, tables, diagrams, and formulas), followed by a set\nof dedicated semantic analyzers that extract structured representations aligned\nwith signal-level semantics. These representations are utilized to drive\nassertion synthesis via multi-step chain-of-thought (CoT) prompting. The\nframework incorporates a mutation-based evaluation approach to assess assertion\nquality via model checking and further refine the generated assertions.\nExperimental evaluation across three real-world Register-Transfer Level (RTL)\ndesigns demonstrates AssertCoder's superior performance, achieving an average\nincrease of 8.4% in functional correctness and 5.8% in mutation detection\ncompared to existing state-of-the-art approaches.", "comment": "7 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2507.10338v1", "cate": "cs.SE", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10150", "title": "Past-Future Scheduler for LLM Serving under SLA Guarantees", "authors": ["Ruihao Gong", "Shihao Bai", "Siyu Wu", "Yunqian Fan", "Zaijun Wang", "Xiuhong Li", "Hailong Yang", "Xianglong Liu"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      Accepted to ASPLOS 2025", "url": "http://arxiv.org/abs/2507.10150v1", "summary": "The exploration and application of Large Language Models (LLMs) is thriving.\nTo reduce deployment costs, continuous batching has become an essential feature\nin current service frameworks. The effectiveness of continuous batching relies\non an accurate estimate of the memory requirements of requests. However, due to\nthe diversity in request output lengths, existing frameworks tend to adopt\naggressive or conservative schedulers, which often result in significant\noverestimation or underestimation of memory consumption. Consequently, they\nsuffer from harmful request evictions or prolonged queuing times, failing to\nachieve satisfactory throughput under strict Service Level Agreement (SLA)\nguarantees (a.k.a. goodput), across various LLM application scenarios with\ndiffering input-output length distributions. To address this issue, we propose\na novel Past-Future scheduler that precisely estimates the peak memory\nresources required by the running batch via considering the historical\ndistribution of request output lengths and calculating memory occupancy at each\nfuture time point. It adapts to applications with all types of input-output\nlength distributions, balancing the trade-off between request queuing and\nharmful evictions, thereby consistently achieving better goodput. Furthermore,\nto validate the effectiveness of the proposed scheduler, we developed a\nhigh-performance LLM serving framework, LightLLM, that implements the\nPast-Future scheduler. Compared to existing aggressive or conservative\nschedulers, LightLLM demonstrates superior goodput, achieving up to 2-3$\\times$\nhigher goodput than other schedulers under heavy loads. LightLLM is open source\nto boost the research in such direction (https://github.com/ModelTC/lightllm).", "comment": "Accepted to ASPLOS 2025", "pdf_url": "http://arxiv.org/pdf/2507.10150v1", "cate": "cs.DC", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09233", "title": "Secondary Bounded Rationality: A Theory of How Algorithms Reproduce Structural Inequality in AI Hiring", "authors": ["Jia Xiao"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09233v1", "summary": "AI-driven recruitment systems, while promising efficiency and objectivity,\noften perpetuate systemic inequalities by encoding cultural and social capital\ndisparities into algorithmic decision making. This article develops and defends\na novel theory of secondary bounded rationality, arguing that AI systems,\ndespite their computational power, inherit and amplify human cognitive and\nstructural biases through technical and sociopolitical constraints. Analyzing\nmultimodal recruitment frameworks, we demonstrate how algorithmic processes\ntransform historical inequalities, such as elite credential privileging and\nnetwork homophily, into ostensibly meritocratic outcomes. Using Bourdieusian\ncapital theory and Simon's bounded rationality, we reveal a recursive cycle\nwhere AI entrenches exclusion by optimizing for legible yet biased proxies of\ncompetence. We propose mitigation strategies, including counterfactual fairness\ntesting, capital-aware auditing, and regulatory interventions, to disrupt this\nself-reinforcing inequality.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09233v1", "cate": "cs.CY", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09591", "title": "Physics-informed machine learning surrogate for scalable simulation of thermal histories during wire-arc directed energy deposition", "authors": ["Michael Ryan", "Mohammad Hassan Baqershahi", "Hessamoddin Moshayedi", "Elyas Ghafoori"], "categories": ["cs.CE"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "Comments:      18 pages, 12 figures", "url": "http://arxiv.org/abs/2507.09591v1", "summary": "Wire-arc directed energy deposition (DED) has emerged as a promising additive\nmanufacturing (AM) technology for large-scale structural engineering\napplications. However, the complex thermal dynamics inherent to the process\npresent challenges in ensuring structural integrity and mechanical properties\nof fabricated thick walls and plates. While finite element method (FEM)\nsimulations have been conventionally employed to predict thermal history during\ndeposition, their computational demand remains prohibitively high for actual\nlarge-scale applications. Given the necessity of multiple repetitive\nsimulations for heat management and the determination of an optimal printing\nstrategy, FEM simulation quickly becomes entirely infeasible. Instead,\nadvancements have been made in using trained neural networks as surrogate\nmodels for rapid prediction. However, traditional data-driven approaches\nnecessitate large amounts of relevant and verifiable external data, during the\ntraining and validation of the neural network. Regarding large-scale wire-arc\nDED, none of these data sources are readily available in quantities sufficient\nfor an accurate surrogate. The introduction of physics-informed neural networks\n(PINNs) has opened up an alternative simulation strategy by leveraging the\nexisting physical knowledge of the phenomena with advanced machine learning\nmethods. Despite their theoretical advantages, PINNs have seen limited\napplication in the context of large-scale wire-arc DED for structural\nengineering. This study investigates the scalability of PINNs, focusing on\nefficient collocation points sampling, a critical factor controlling both the\ntraining time and model performance. Results show PINNs can reduce\ncomputational time and effort by up to 98.6%, while maintaining the desired\naccuracy and offering \"super-resolution\". Future directions for enhancing PINN\nperformance in metal AM are discussed.", "comment": "18 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2507.09591v1", "cate": "cs.CE", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.06421", "title": "Never Trust the Manufacturer, Never Trust the Client: A Novel Method for Streaming STL Files for Secure Additive manufacturing", "authors": ["Seyed Ali Ghazi Asgar", "Narasimha Reddy", "Satish T. S. Bukkapatnam"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      8 figures", "url": "http://arxiv.org/abs/2507.06421v2", "summary": "While additive manufacturing has opened interesting avenues to reimagine\nmanufacturing as a service (MaaS) platform, transmission of design files from\nclient to manufacturer over networks opens up many cybersecurity challenges.\nSecuring client's intellectual property (IP) especially from cyber-attacks\nemerges as a major challenge. Earlier works introduced streaming, instead of\nsharing process plan (G-code) files, as a possible solution. However, executing\nclient's G-codes on manufacturer's machines exposes them to potential malicious\nG-codes. This paper proposes a viable approach when the client and manufacturer\ndo not trust each other and both the client and manufacturer want to preserve\ntheir IP of designs and manufacturing process respectively. The proposed\napproach is based on segmenting and streaming design (STL) files and employing\na novel machine-specific STL to G-code translator at the manufacturer's site in\nreal-time for printing. This approach secures design and manufacturing process\nIPs as demonstrated in a real-world implementation.", "comment": "8 figures", "pdf_url": "http://arxiv.org/pdf/2507.06421v2", "cate": "cs.CR", "date": "2025-07-08", "updated": "2025-07-11"}
{"id": "2507.10310", "title": "Polygonal Obstacle Avoidance Combining Model Predictive Control and Fuzzy Logic", "authors": ["Michael Schröder", "Eric Schöneberg", "Daniel Görges", "Hans D. Schotten"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10310v1", "summary": "In practice, navigation of mobile robots in confined environments is often\ndone using a spatially discrete cost-map to represent obstacles. Path following\nis a typical use case for model predictive control (MPC), but formulating\nconstraints for obstacle avoidance is challenging in this case. Typically the\ncost and constraints of an MPC problem are defined as closed-form functions and\ntypical solvers work best with continuously differentiable functions. This is\ncontrary to spatially discrete occupancy grid maps, in which a grid's value\ndefines the cost associated with occupancy. This paper presents a way to\novercome this compatibility issue by re-formulating occupancy grid maps to\ncontinuously differentiable functions to be embedded into the MPC scheme as\nconstraints. Each obstacle is defined as a polygon -- an intersection of\nhalf-spaces. Any half-space is a linear inequality representing one edge of a\npolygon. Using AND and OR operators, the combined set of all obstacles and\ntherefore the obstacle avoidance constraints can be described. The key\ncontribution of this paper is the use of fuzzy logic to re-formulate such\nconstraints that include logical operators as inequality constraints which are\ncompatible with standard MPC formulation. The resulting MPC-based trajectory\nplanner is successfully tested in simulation. This concept is also applicable\noutside of navigation tasks to implement logical or verbal constraints in MPC.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10310v1", "cate": "cs.RO", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2402.09750", "title": "Pinning \"Reflection\" on the Agenda: Investigating Reflection in Human-LLM Co-Creation for Creative Coding", "authors": ["Anqi Wang", "Zhizhuo Yin", "Yulu Hu", "Yuanyuan Mao", "Lei Han", "Xin Tong", "Keqin Jiao", "Pan Hui"], "categories": ["cs.HC", "cs.AI", "J.5"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      6 pages, 2 figures, 2 tables", "url": "http://arxiv.org/abs/2402.09750v2", "summary": "Large language models (LLMs) are increasingly integrated into creative\ncoding, yet how users reflect, and how different co-creation conditions\ninfluence reflective behavior, remains underexplored. This study investigates\nsituated, moment-to-moment reflection in creative coding under two prompting\nstrategies: the entire task invocation (T1) and decomposed subtask invocation\n(T2), to examine their effects on reflective behavior. Our mixed-method results\nreveal three distinct reflection types and show that T2 encourages more\nfrequent, strategic, and generative reflection, fostering diagnostic reasoning\nand goal redefinition. These findings offer insights into how LLM-based tools\nfoster deeper creative engagement through structured, behaviorally grounded\nreflection support.", "comment": "6 pages, 2 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2402.09750v2", "cate": "cs.HC", "date": "2024-02-15", "updated": "2025-07-12"}
{"id": "2507.10119", "title": "Analysis of AI Techniques for Orchestrating Edge-Cloud Application Migration", "authors": ["Sadig Gojayev", "Ahmad Anaqreh", "Carolina Fortuna"], "categories": ["cs.AI", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10119v1", "summary": "Application migration in edge-cloud system enables high QoS and cost\neffective service delivery. However, automatically orchestrating such migration\nis typically solved with heuristic approaches. Starting from the Markov\nDecision Process (MDP), in this paper, we identify, analyze and compare\nselected state-of-the-art Artificial Intelligence (AI) planning and\nReinforcement Learning (RL) approaches for solving the class of edge-cloud\napplication migration problems that can be modeled as Towers of Hanoi (ToH)\nproblems. We introduce a new classification based on state space definition and\nanalyze the compared models also through this lense. The aim is to understand\navailable techniques capable of orchestrating such application migration in\nemerging computing continuum environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10119v1", "cate": "cs.AI", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09184", "title": "MCA-LLaVA: Manhattan Causal Attention for Reducing Hallucination in Large Vision-Language Models", "authors": ["Qiyan Zhao", "Xiaofeng Zhang", "Yiheng Li", "Yun Xing", "Xiaosong Yuan", "Feilong Tang", "Sinan Fan", "Xuhang Chen", "Xuyao Zhang", "Dahan Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted in ACM MM 2025", "url": "http://arxiv.org/abs/2507.09184v1", "summary": "Hallucinations pose a significant challenge in Large Vision Language Models\n(LVLMs), with misalignment between multimodal features identified as a key\ncontributing factor. This paper reveals the negative impact of the long-term\ndecay in Rotary Position Encoding (RoPE), used for positional modeling in\nLVLMs, on multimodal alignment. Concretely, under long-term decay, instruction\ntokens exhibit uneven perception of image tokens located at different positions\nwithin the two-dimensional space: prioritizing image tokens from the\nbottom-right region since in the one-dimensional sequence, these tokens are\npositionally closer to the instruction tokens. This biased perception leads to\ninsufficient image-instruction interaction and suboptimal multimodal alignment.\nWe refer to this phenomenon as image alignment bias. To enhance instruction's\nperception of image tokens at different spatial locations, we propose\nMCA-LLaVA, based on Manhattan distance, which extends the long-term decay to a\ntwo-dimensional, multi-directional spatial decay. MCA-LLaVA integrates the\none-dimensional sequence order and two-dimensional spatial position of image\ntokens for positional modeling, mitigating hallucinations by alleviating image\nalignment bias. Experimental results of MCA-LLaVA across various hallucination\nand general benchmarks demonstrate its effectiveness and generality. The code\ncan be accessed in https://github.com/ErikZ719/MCA-LLaVA.", "comment": "Accepted in ACM MM 2025", "pdf_url": "http://arxiv.org/pdf/2507.09184v1", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09820", "title": "Measuring What Matters: A Framework for Evaluating Safety Risks in Real-World LLM Applications", "authors": ["Jia Yi Goh", "Shaun Khoo", "Nyx Iskandar", "Gabriel Chua", "Leanne Tan", "Jessica Foo"], "categories": ["cs.SE", "cs.CY"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09820v1", "summary": "Most safety testing efforts for large language models (LLMs) today focus on\nevaluating foundation models. However, there is a growing need to evaluate\nsafety at the application level, as components such as system prompts,\nretrieval pipelines, and guardrails introduce additional factors that\nsignificantly influence the overall safety of LLM applications. In this paper,\nwe introduce a practical framework for evaluating application-level safety in\nLLM systems, validated through real-world deployment across multiple use cases\nwithin our organization. The framework consists of two parts: (1) principles\nfor developing customized safety risk taxonomies, and (2) practices for\nevaluating safety risks in LLM applications. We illustrate how the proposed\nframework was applied in our internal pilot, providing a reference point for\norganizations seeking to scale their safety testing efforts. This work aims to\nbridge the gap between theoretical concepts in AI safety and the operational\nrealities of safeguarding LLM applications in practice, offering actionable\nguidance for safe and scalable deployment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09820v1", "cate": "cs.SE", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2412.17934", "title": "UAV Communications: Impact of Obstacles on Channel Characteristics", "authors": ["Kamal Shayegan"], "categories": ["cs.NI", "eess.SP"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.17934v4", "summary": "In recent years, Unmanned Aerial Vehicles (UAVs) have been utilized as\neffective platforms for carrying Wi-Fi Access Points (APs) and cellular Base\nStations (BSs), enabling low-cost, agile, and flexible wireless networks with\nhigh Quality of Service (QoS). The next generation of wireless communications\nwill rely on increasingly higher frequencies, which are easily obstructed by\nobstacles. One of the most critical concepts yet to be fully addressed is\npositioning the UAV at optimal coordinates while accounting for obstacles. To\nensure a line of sight (LoS) between UAVs and user equipment (UE), improve QoS,\nand establish reliable wireless links with maximum coverage, obstacles must be\nintegrated into the proposed placement algorithms. This paper introduces a\nsimulation-based measurement approach for characterizing an air-to-ground (AG)\nchannel in a simple scenario. By considering obstacles, we present a novel\nperspective on channel characterization. The results, in terms of throughput,\npacket delivery, packet loss, and delay, are compared using the proposed\npositioning approach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.17934v4", "cate": "cs.NI", "date": "2024-12-23", "updated": "2025-07-14"}
{"id": "2507.10068", "title": "BiD Codes: Algebraic Codes from $3 \\times 3$ Kernel", "authors": ["Anirudh Dash", "K. R. Nandakishore", "Lakshmi Prasad Natarajan", "Prasad Krishnan"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      Accepted for presentation and publication at the 2025 IEEE Information Theory Workshop (ITW'25)", "url": "http://arxiv.org/abs/2507.10068v1", "summary": "We introduce Berman-intersection-dual Berman (BiD) codes. These are abelian\ncodes of length $3^m$ that can be constructed using Kronecker products of a $3\n\\times 3$ kernel matrix. BiD codes offer minimum distance close to that of\nReed-Muller (RM) codes at practical blocklengths, and larger distance than RM\ncodes asymptotically in the blocklength. Simulations of BiD codes of length\n$3^5=243$ in the erasure and Gaussian channels show that their block error\nrates under maximum-likelihood decoding are similar to, and sometimes better,\nthan RM, RM-Polar, and CRC-aided Polar codes.", "comment": "Accepted for presentation and publication at the 2025 IEEE\n  Information Theory Workshop (ITW'25)", "pdf_url": "http://arxiv.org/pdf/2507.10068v1", "cate": "cs.IT", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "1406.1886", "title": "The Z1: Architecture and Algorithms of Konrad Zuse's First Computer", "authors": ["Raul Rojas"], "categories": ["cs.AR", "68Mxx", "K.2; B.0"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "Comments:      24 pages, 20 figures, version 2", "url": "http://arxiv.org/abs/1406.1886v2", "summary": "This paper provides the first comprehensive description of the Z1, the\nmechanical computer built by the German inventor Konrad Zuse in Berlin from\n1936 to 1938. The paper describes the main structural elements of the machine,\nthe high-level architecture, and the dataflow between components. The computer\ncould perform the four basic arithmetic operations using floating-point\nnumbers. Instructions were read from punched tape. A program consisted of a\nsequence of arithmetical operations, intermixed with memory store and load\ninstructions, interrupted possibly by input and output operations. Numbers were\nstored in a mechanical memory. The machine did not include conditional\nbranching in the instruction set. While the architecture of the Z1 is similar\nto the relay computer Zuse finished in 1941 (the Z3) there are some significant\ndifferences. The Z1 implements operations as sequences of microinstructions, as\nin the Z3, but does not use rotary switches as micro-steppers. The Z1 uses a\ndigital incrementer and a set of conditions which are translated into\nmicroinstructions for the exponent and mantissa units, as well as for the\nmemory blocks. Microinstructions select one out of 12 layers in a machine with\na 3D mechanical structure of binary mechanical elements. The exception circuits\nfor mantissa zero, necessary for normalized floating-point, were lacking; they\nwere first implemented in the Z3. The information for this article was\nextracted from careful study of the blueprints drawn by Zuse for the\nreconstruction of the Z1 for the German Technology Museum in Berlin, from some\nletters, and from sketches in notebooks. Although the machine has been in\nexhibition since 1989 (non-operational), no detailed high-level description of\nthe machine's architecture had been available. This paper fills that gap.", "comment": "24 pages, 20 figures, version 2", "pdf_url": "http://arxiv.org/pdf/1406.1886v2", "cate": "cs.AR", "date": "2014-06-07", "updated": "2025-07-12"}
{"id": "2507.10367", "title": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline", "authors": ["Jingwei Xu", "Junbin Kang", "Mingkai Dong", "Mingyu Liu", "Lu Zhang", "Shaohong Guo", "Ziyan Qiu", "Mingzhen You", "Ziyi Tian", "Anqi Yu", "Tianhong Ding", "Xinwei Hu", "Haibo Chen"], "categories": ["cs.DC", "cs.PF"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      Accepted by NSDI'26", "url": "http://arxiv.org/abs/2507.10367v1", "summary": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year.", "comment": "Accepted by NSDI'26", "pdf_url": "http://arxiv.org/pdf/2507.10367v1", "cate": "cs.DC", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09239", "title": "The Narrative Construction of Generative AI Efficacy by the Media: A Case Study of the Role of ChatGPT in Higher Education", "authors": ["Yinan Sun", "Ali Unlu", "Aditya Johri"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      Final draft of article under review at a journal", "url": "http://arxiv.org/abs/2507.09239v1", "summary": "The societal role of technology, including artificial intelligence (AI), is\noften shaped by sociocultural narratives. This study examines how U.S. news\nmedia construct narratives about the efficacy of generative AI (GenAI), using\nChatGPT in higher education as a case study. Grounded in Agenda Setting Theory,\nwe analyzed 198 articles published between November 2022 and October 2024,\nemploying LDA topic modeling and sentiment analysis. Our findings identify six\nkey topics in the media discourse, with sentiment analysis revealing generally\npositive portrayals of ChatGPT's integration into higher education through\npolicy, curriculum, teaching practices, collaborative decision-making, skill\ndevelopment, and human-centered learning. In contrast, media narratives express\nmore negative sentiment regarding their impact on entry-level jobs and college\nadmissions. This research highlights how media coverage can influence public\nperceptions of GenAI in education and provides actionable insights for\npolicymakers, educators, and AI developers navigating its adoption and\nrepresentation in public discourse.", "comment": "Final draft of article under review at a journal", "pdf_url": "http://arxiv.org/pdf/2507.09239v1", "cate": "cs.CY", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09675", "title": "What Matters Most? A Quantitative Meta-Analysis of AI-Based Predictors for Startup Success", "authors": ["Seyed Mohammad Ali Jafari", "Ali Mobini Dehkordi", "Ehsan Chitsaz", "Yadollah Yaghoobzadeh"], "categories": ["cs.CE", "econ.GN", "q-fin.EC"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09675v1", "summary": "Background: Predicting startup success with machine learning is a rapidly\ngrowing field, yet findings on key predictors are often fragmented and\ncontext-specific. This makes it difficult to discern robust patterns and\nhighlights a need for a systematic synthesis of the evidence.\n  Methods: This study conducts a quantitative meta-analysis to synthesize the\nliterature on predictor importance in AI-based startup evaluation. We performed\na systematic review to identify a final sample of 13 empirical studies that\nreport rankable feature importance. From these papers, we extracted and\ncategorized 58 unique predictors, synthesizing their importance using a\nWeighted Importance Score (WIS) that balances a feature's average rank with its\nfrequency of appearance. We also conducted a moderator analysis to investigate\nhow predictor importance changes with context (e.g., success definition).\n  Results: Our aggregate analysis reveals that the most consistently powerful\npredictors are a quartet of foundational attributes: Firm Characteristics\n(e.g., age, location), Investor Structure (e.g., investor quality), Digital and\nSocial Traction (e.g., online momentum), and Funding History. The moderator\nanalysis further reveals that this hierarchy is highly context-dependent. For\ninstance, predicting near-term funding milestones elevates the importance of\nthe deal's immediate context, while predicting long-term exits prioritizes\nfundamental firm and investor characteristics.\n  Conclusion: The factors that best predict startup success are not universal\nbut are contingent on the startup's goals, stage, and the data used for\nevaluation. Our findings point to a potential \"convenience bias\" in the\nliterature, where predictor importance may be tied to data accessibility. We\nconclude by underscoring the need for standardized reporting practices to\nenable more robust, cumulative knowledge building in the field.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09675v1", "cate": "cs.CE", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09373", "title": "Algebraic Closure of Matrix Sets Recognized by 1-VASS", "authors": ["Rida Ait El Manssour", "Mahsa Naraghi", "Mahsa Shirmohammadi", "James Worrell"], "categories": ["cs.FL", "cs.LO", "math.AG"], "primary_category": "Subjects:       Formal Languages and Automata Theory (cs.FL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09373v1", "summary": "It is known how to compute the Zariski closure of a finitely generated monoid\nof matrices and, more generally, of a set of matrices specified by a regular\nlanguage. This result was recently used to give a procedure to compute all\npolynomial invariants of a given affine program. Decidability of the more\ngeneral problem of computing all polynomial invariants of affine programs with\nrecursive procedure calls remains open. Mathematically speaking, the core\nchallenge is to compute the Zariski closure of a set of matrices defined by a\ncontext-free language. In this paper, we approach the problem from two sides:\nTowards decidability, we give a procedure to compute the Zariski closure of\nsets of matrices given by one-counter languages (that is, languages accepted by\none-dimensional vector addition systems with states and zero tests), a proper\nsubclass of context-free languages. On the other side, we show that the problem\nbecomes undecidable for indexed languages, a natural extension of context-free\nlanguages corresponding to nested pushdown automata. One of our main technical\ntools is a novel adaptation of Simon's factorization forests to infinite\nmonoids of matrices.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09373v1", "cate": "cs.FL", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2303.07152", "title": "Score Attack: A Lower Bound Technique for Optimal Differentially Private Learning", "authors": ["T. Tony Cai", "Yichen Wang", "Linjun Zhang"], "categories": ["math.ST", "cs.CR", "cs.LG", "stat.ME", "stat.ML", "stat.TH"], "primary_category": "Subjects:       Statistics Theory (math.ST)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2303.07152v2", "summary": "Achieving optimal statistical performance while ensuring the privacy of\npersonal data is a challenging yet crucial objective in modern data analysis.\nHowever, characterizing the optimality, particularly the minimax lower bound,\nunder privacy constraints is technically difficult. To address this issue, we\npropose a novel approach called the score attack, which provides a lower bound\non the differential-privacy-constrained minimax risk of parameter estimation.\nThe score attack method is based on the tracing attack concept in differential\nprivacy and can be applied to any statistical model with a well-defined score\nstatistic. It can optimally lower bound the minimax risk of estimating unknown\nmodel parameters, up to a logarithmic factor, while ensuring differential\nprivacy for a range of statistical problems. We demonstrate the effectiveness\nand optimality of this general method in various examples, such as the\ngeneralized linear model in both classical and high-dimensional sparse\nsettings, the Bradley-Terry-Luce model for pairwise comparisons, and\nnon-parametric regression over the Sobolev class.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2303.07152v2", "cate": "math.ST", "date": "2023-03-13", "updated": "2025-07-12"}
{"id": "2507.10376", "title": "Raci-Net: Ego-vehicle Odometry Estimation in Adverse Weather Conditions", "authors": ["Mohammadhossein Talebi", "Pragyan Dahal", "Davide Possenti", "Stefano Arrigoni", "Francesco Braghin"], "categories": ["cs.RO", "I.2"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 pages", "url": "http://arxiv.org/abs/2507.10376v1", "summary": "Autonomous driving systems are highly dependent on sensors like cameras,\nLiDAR, and inertial measurement units (IMU) to perceive the environment and\nestimate their motion. Among these sensors, perception-based sensors are not\nprotected from harsh weather and technical failures. Although existing methods\nshow robustness against common technical issues like rotational misalignment\nand disconnection, they often degrade when faced with dynamic environmental\nfactors like weather conditions. To address these problems, this research\nintroduces a novel deep learning-based motion estimator that integrates visual,\ninertial, and millimeter-wave radar data, utilizing each sensor strengths to\nimprove odometry estimation accuracy and reliability under adverse\nenvironmental conditions such as snow, rain, and varying light. The proposed\nmodel uses advanced sensor fusion techniques that dynamically adjust the\ncontributions of each sensor based on the current environmental condition, with\nradar compensating for visual sensor limitations in poor visibility. This work\nexplores recent advancements in radar-based odometry and highlights that radar\nrobustness in different weather conditions makes it a valuable component for\npose estimation systems, specifically when visual sensors are degraded.\nExperimental results, conducted on the Boreas dataset, showcase the robustness\nand effectiveness of the model in both clear and degraded environments.", "comment": "8 pages", "pdf_url": "http://arxiv.org/pdf/2507.10376v1", "cate": "cs.RO", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2403.08969", "title": "The Full-scale Assembly Simulation Testbed (FAST) Dataset", "authors": ["Alec G. Moore", "Tiffany D. Do", "Nayan N. Chawla", "Antonia Jimenez Iriarte", "Ryan P. McMahan"], "categories": ["cs.HC", "cs.LG"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2403.08969v2", "summary": "In recent years, numerous researchers have begun investigating how virtual\nreality (VR) tracking and interaction data can be used for a variety of machine\nlearning purposes, including user identification, predicting cybersickness, and\nestimating learning gains. One constraint for this research area is the dearth\nof open datasets. In this paper, we present a new open dataset captured with\nour VR-based Full-scale Assembly Simulation Testbed (FAST). This dataset\nconsists of data collected from 108 participants (50 females, 56 males, 2\nnon-binary) learning how to assemble two distinct full-scale structures in VR.\nIn addition to explaining how the dataset was collected and describing the data\nincluded, we discuss how the dataset may be used by future researchers.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2403.08969v2", "cate": "cs.HC", "date": "2024-03-13", "updated": "2025-07-13"}
{"id": "2507.10124", "title": "Could you be wrong: Debiasing LLMs using a metacognitive prompt for improving human decision making", "authors": ["Thomas T. Hills"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      12 pages, 3 figures", "url": "http://arxiv.org/abs/2507.10124v1", "summary": "Identifying bias in LLMs is ongoing. Because they are still in development,\nwhat is true today may be false tomorrow. We therefore need general strategies\nfor debiasing that will outlive current models. Strategies developed for\ndebiasing human decision making offer one promising approach as they\nincorporate an LLM-style prompt intervention designed to bring latent knowledge\ninto awareness during decision making. LLMs trained on vast amounts of\ninformation contain information about potential biases, counter-arguments, and\ncontradictory evidence, but that information may only be brought to bear if\nprompted. Metacognitive prompts developed in the human decision making\nliterature are designed to achieve this, and as I demonstrate here, they show\npromise with LLMs. The prompt I focus on here is \"could you be wrong?\"\nFollowing an LLM response, this prompt leads LLMs to produce additional\ninformation, including why they answered as they did, errors, biases,\ncontradictory evidence, and alternatives, none of which were apparent in their\ninitial response. Indeed, this metaknowledge often reveals that how LLMs and\nusers interpret prompts are not aligned. Here I demonstrate this prompt using a\nset of questions taken from recent articles about LLM biases, including\nimplicit discriminatory biases and failures of metacognition. \"Could you be\nwrong\" prompts the LLM to identify its own biases and produce cogent\nmetacognitive reflection. I also present another example involving convincing\nbut incomplete information, which is readily corrected by the metacognitive\nprompt. In sum, this work argues that human psychology offers a new avenue for\nprompt engineering, leveraging a long history of effective prompt-based\nimprovements to human decision making.", "comment": "12 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2507.10124v1", "cate": "cs.AI", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09200", "title": "THYME: Temporal Hierarchical-Cyclic Interactivity Modeling for Video Scene Graphs in Aerial Footage", "authors": ["Trong-Thuan Nguyen", "Pha Nguyen", "Jackson Cothren", "Alper Yilmaz", "Minh-Triet Tran", "Khoa Luu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09200v1", "summary": "The rapid proliferation of video in applications such as autonomous driving,\nsurveillance, and sports analytics necessitates robust methods for dynamic\nscene understanding. Despite advances in static scene graph generation and\nearly attempts at video scene graph generation, previous methods often suffer\nfrom fragmented representations, failing to capture fine-grained spatial\ndetails and long-range temporal dependencies simultaneously. To address these\nlimitations, we introduce the Temporal Hierarchical Cyclic Scene Graph (THYME)\napproach, which synergistically integrates hierarchical feature aggregation\nwith cyclic temporal refinement to address these limitations. In particular,\nTHYME effectively models multi-scale spatial context and enforces temporal\nconsistency across frames, yielding more accurate and coherent scene graphs. In\naddition, we present AeroEye-v1.0, a novel aerial video dataset enriched with\nfive types of interactivity that overcome the constraints of existing datasets\nand provide a comprehensive benchmark for dynamic scene graph generation.\nEmpirically, extensive experiments on ASPIRe and AeroEye-v1.0 demonstrate that\nthe proposed THYME approach outperforms state-of-the-art methods, offering\nimproved scene understanding in ground-view and aerial scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09200v1", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09866", "title": "Turning the Tide: Repository-based Code Reflection", "authors": ["Wei Zhang", "Jian Yang", "Jiaxi Yang", "Ya Wang", "Zhoujun Li", "Zeyu Cui", "Binyuan Hui", "Junyang Lin"], "categories": ["cs.SE", "cs.AI"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09866v1", "summary": "Code large language models (LLMs) enhance programming by understanding and\ngenerating code across languages, offering intelligent feedback, bug detection,\nand code updates through reflection, improving development efficiency and\naccessibility. While benchmarks (e.g. HumanEval/LiveCodeBench) evaluate code\ngeneration and real-world relevance, previous works ignore the scenario of\nmodifying code in repositories. Considering challenges remaining in improving\nreflection capabilities and avoiding data contamination in dynamic benchmarks,\nwe introduce LiveRepoReflection, a challenging benchmark for evaluating code\nunderstanding and generation in multi-file repository contexts, featuring 1,888\nrigorously filtered test cases across $6$ programming languages to ensure\ndiversity, correctness, and high difficulty. Further, we create\nRepoReflection-Instruct, a large-scale, quality-filtered instruction-tuning\ndataset derived from diverse sources, used to train RepoReflectionCoder through\na two-turn dialogue process involving code generation and error-driven repair.\nThe leaderboard evaluates over 40 LLMs to reflect the model performance of\nrepository-based code reflection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09866v1", "cate": "cs.SE", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2501.17127", "title": "Enhancements to P4TG: Protocols, Performance, and Automation", "authors": ["Fabian Ihle", "Etienne Zink", "Steffen Lindner", "Michael Menth"], "categories": ["cs.NI", "cs.PF"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      This work has been accepted at the 4th KuVS Workshop on Network Softwarization (KuVS NetSoft) under the Creative Commons Attribution 4.0 International License (CC BY 4.0)", "url": "http://arxiv.org/abs/2501.17127v4", "summary": "P4TG is a hardware-based traffic generator (TG) running on the Intel Tofino 1\nASIC and was programmed using the programming language P4. In its initial\nversion, P4TG could generate up to 10x100 Gb/s of traffic and directly measure\nrates, packet loss, and other metrics in the data plane. Many researchers and\nindustrial partners requested new features to be incorporated into P4TG since\nits publication in 2023. With the recently added features, P4TG supports the\ngeneration of packets encapsulated with a customizable VLAN, QinQ, VxLAN, MPLS,\nand SRv6 header. Further, generation of IPv6 traffic is added and P4TG is\nported to the Intel Tofino 2 platform enabling a generation capability of up to\n10x400 Gb/s. The improvement in user experience focuses on ease of operation.\nFeatures like automated ARP replies, improved visualization, report generation,\nand automated testing based on the IMIX distribution and RFC 2544 are added.\nFuture work on P4TG includes NDP to facilitate IPv6 traffic, and a NETCONF\nintegration to further ease the configuration.", "comment": "This work has been accepted at the 4th KuVS Workshop on Network\n  Softwarization (KuVS NetSoft) under the Creative Commons Attribution 4.0\n  International License (CC BY 4.0)", "pdf_url": "http://arxiv.org/pdf/2501.17127v4", "cate": "cs.NI", "date": "2025-01-28", "updated": "2025-07-14"}
{"id": "2507.10074", "title": "Learning-Aided Iterative Receiver for Superimposed Pilots: Design and Experimental Evaluation", "authors": ["Xinjie Li", "Xingyu Zhou", "Yixiao Cao", "Jing Zhang", "Chao-Kai Wen", "Xiao Li", "Shi Jin"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      This work has been submitted to the IEEE for possible publication", "url": "http://arxiv.org/abs/2507.10074v1", "summary": "The superimposed pilot transmission scheme offers substantial potential for\nimproving spectral efficiency in MIMO-OFDM systems, but it presents significant\nchallenges for receiver design due to pilot contamination and data\ninterference. To address these issues, we propose an advanced iterative\nreceiver based on joint channel estimation, detection, and decoding, which\nrefines the receiver outputs through iterative feedback. The proposed receiver\nincorporates two adaptive channel estimation strategies to enhance robustness\nunder time-varying and mismatched channel conditions. First, a variational\nmessage passing (VMP) method and its low-complexity variant (VMP-L) are\nintroduced to perform inference without relying on time-domain correlation.\nSecond, a deep learning (DL) based estimator is developed, featuring a\nconvolutional neural network with a despreading module and an attention\nmechanism to extract and fuse relevant channel features. Extensive simulations\nunder multi-stream and high-mobility scenarios demonstrate that the proposed\nreceiver consistently outperforms conventional orthogonal pilot baselines in\nboth throughput and block error rate. Moreover, over-the-air experiments\nvalidate the practical effectiveness of the proposed design. Among the methods,\nthe DL based estimator achieves a favorable trade-off between performance and\ncomplexity, highlighting its suitability for real-world deployment in dynamic\nwireless environments.", "comment": "This work has been submitted to the IEEE for possible publication", "pdf_url": "http://arxiv.org/pdf/2507.10074v1", "cate": "cs.IT", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2506.19067", "title": "MEDEA: A Design-Time Multi-Objective Manager for Energy-Efficient DNN Inference on Heterogeneous Ultra-Low Power Platforms", "authors": ["Hossein Taji", "José Miranda", "Miguel Peón-Quirós", "David Atienza"], "categories": ["cs.AR", "C.3; C.1.3"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "Comments:      Submitted to ACM Transactions on Embedded Computing Systems (TECS)", "url": "http://arxiv.org/abs/2506.19067v2", "summary": "The growing demand for on-device AI necessitates energy-efficient execution\nof DNN based applications on resource-constrained ultra-low power (ULP)\nplatforms. Heterogeneous architectures, combining specialized processing\nelements (PEs), have emerged as a key solution for achieving the required\nperformance and energy efficiency. However, optimizing energy while executing\napplications on these platforms requires efficiently managing platform\nresources like PEs, power features, and memory footprint, all while adhering to\ncritical application deadlines. This paper presents MEDEA, a novel design-time\nmulti-objective manager for energy-efficient DNN inference on Heterogeneous ULP\n(HULP) platforms. MEDEA uniquely integrates: kernel-level dynamic voltage and\nfrequency scaling (DVFS) for dynamic energy adaptation; kernel-level\ngranularity scheduling, suitable for specialized accelerators; memory-aware\nadaptive tiling to navigate severe memory constraints; and all within a timing\nconstraint-based optimization strategy, which minimizes energy based on\napplication deadline. To showcase practical viability, we evaluate MEDEA on\nHEEPtimize, a heterogeneous ULP platform (22 nm, FPGA-prototyped) featuring a\nRISC-V processor besides Near-Memory Computing (NMC) and Coarse-Grained\nReconfigurable Array (CGRA) accelerators. Experimental results, using a\nbiomedical seizure detection case study, demonstrate that MEDEA achieves\noverall energy reductions of up to 38% compared to representative\nstate-of-the-art methods, while consistently meeting all timing and memory\nrequirements. This effectiveness is attributed to its integrated features, with\nour analysis showing that kernel-level DVFS alone can be responsible for over\n31% of the energy savings in specific scenarios.", "comment": "Submitted to ACM Transactions on Embedded Computing Systems (TECS)", "pdf_url": "http://arxiv.org/pdf/2506.19067v2", "cate": "cs.AR", "date": "2025-06-23", "updated": "2025-07-11"}
{"id": "2507.10392", "title": "Zorse: Optimizing LLM Training Efficiency on Heterogeneous GPU Clusters", "authors": ["Runsheng Benson Guo", "Utkarsh Anand", "Khuzaima Daudjee", "Rathijit Sen"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10392v1", "summary": "Large language models (LLMs) require vast amounts of GPU compute to train,\nbut limited availability and high costs of GPUs make homogeneous clusters\nimpractical for many organizations. Instead, assembling heterogeneous clusters\nby pooling together GPUs of different generations allows them to achieve higher\naggregate compute and make use of all available GPUs. However, training on\nheterogeneous clusters presents several challenges, including load balancing\nacross GPUs, optimizing memory usage to accommodate varying memory capacities,\nand ensuring communication-efficient training over diverse network\ninterconnects potentially spanning multiple datacenters. In this paper, we make\nthe case that efficient training on heterogeneous clusters requires (1) the\nintegration of pipeline parallelism and data parallelism in a manner that is\nboth communication- and memory-efficient, and (2) a more adaptable\nconfiguration of pipeline and data parallelism, which includes the capability\nto flexibly partition GPUs into asymmetric pipeline parallel stages and to\nincorporate heterogeneous GPUs within the same data parallelism group. We\npropose Zorse, the first system to unify all these capabilities while\nincorporating a planner that automatically configures training strategies for a\ngiven workload. Our evaluation shows that Zorse significantly outperforms\nstate-of-the-art systems in heterogeneous training scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10392v1", "cate": "cs.DC", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09296", "title": "If open source is to win, it must go public", "authors": ["Joshua Tan", "Nicholas Vincent", "Katherine Elkins", "Magnus Sahlgren"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      CodeML @ ICML 2025", "url": "http://arxiv.org/abs/2507.09296v1", "summary": "Open source projects have made incredible progress in producing transparent\nand widely usable machine learning models and systems, but open source alone\nwill face challenges in fully democratizing access to AI. Unlike software, AI\nmodels require substantial resources for activation -- compute, post-training,\ndeployment, and oversight -- which only a few actors can currently provide.\nThis paper argues that open source AI must be complemented by public AI:\ninfrastructure and institutions that ensure models are accessible, sustainable,\nand governed in the public interest. To achieve the full promise of AI models\nas prosocial public goods, we need to build public infrastructure to power and\ndeliver open source software and models.", "comment": "CodeML @ ICML 2025", "pdf_url": "http://arxiv.org/pdf/2507.09296v1", "cate": "cs.CY", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09825", "title": "Legendre Polynomials and Their Use for Karhunen-Loève Expansion", "authors": ["Michal Béreš"], "categories": ["cs.CE", "math.PR", "65C05, 86-08, 82-08, 65C60, 60-08"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09825v1", "summary": "This paper makes two main contributions. First, we present a pedagogical\nreview of the derivation of the three-term recurrence relation for Legendre\npolynomials, without relying on the classical Legendre differential equation,\nRodrigues' formula, or generating functions. This exposition is designed to be\naccessible to undergraduate students.\n  Second, we develop a computational framework for Karhunen-Lo\\`eve expansions\nof isotropic Gaussian random fields on hyper-rectangular domains. The framework\nleverages Legendre polynomials and their associated Gaussian quadrature, and it\nremains efficient even in higher spatial dimensions.\n  A covariance kernel is first approximated by a non-negative mixture of\nsquared-exponentials, obtained via a Newton-optimized fit with a theoretically\ninformed initialization. The resulting separable kernel enables a\nLegendre-Galerkin discretization in the form of a Kronecker product over single\ndimensions, with submatrices that exhibit even/odd parity structure. For\nassembly, we introduce a Duffy-type transformation followed by quadrature.\nThese structural properties significantly reduce both memory usage and\narithmetic cost compared to naive approaches. All algorithms and numerical\nexperiments are provided in an open-source repository that reproduces every\nfigure and table in this work.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09825v1", "cate": "cs.CE", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09761", "title": "A Divide and Conquer Algorithm for Deciding Group Cellular Automata Dynamics", "authors": ["Niccolo' Castronuovo", "Alberto Dennunzio", "Luciano Margara"], "categories": ["cs.FL", "cs.DM"], "primary_category": "Subjects:       Formal Languages and Automata Theory (cs.FL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09761v1", "summary": "We prove that many dynamical properties of group cellular automata (i.e.,\ncellular automata defined on any finite group and with global rule which is an\nendomorphism), including surjectivity, injectivity, sensitivity to initial\nconditions, strong transitivity, positive expansivity, and topological entropy,\ncan be decided by decomposing them into a set of much simpler group cellular\nautomata. To be more specific, we provide a novel algorithmic technique\nallowing one to decompose the group cellular automaton to be studied into a\nfinite number of group cellular automata, some of them defined on abelian\ngroups, while others, if any, defined on products of simple non-abelian\nisomorphic groups.\n  It is worth noting that the groups resulting from the decomposition only\ndepend on the original group and therefore they are completely independent of\nboth the automaton and the property under investigation. As a result, they do\nnot inherit any aspect of the complexity of the automaton under investigation.\n  We prove that the group cellular automata obtained by the decomposition\npreserve dynamical properties and turn out to be much easier to analyze if\ncompared to the original cellular automaton. As a consequence of these results,\nwe show that injectivity, surjectivity and sensitivity to initial conditions\nare decidable properties and that no strongly transitive, and therefore no\npositively expansive, group cellular automata defined on non-abelian groups\nexist. Moreover, we prove that the topological entropy of a group cellular\nautomaton can be computed, provided we know how to compute the topological\nentropy for group cellular automata defined on products of simple non-abelian\nisomorphic groups and on abelian groups.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09761v1", "cate": "cs.FL", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2409.01062", "title": "Random Erasing vs. Model Inversion: A Promising Defense or a False Hope?", "authors": ["Viet-Hung Tran", "Ngoc-Bao Nguyen", "Son T. Mai", "Hans Vandierendonck", "Ira Assent", "Alex Kot", "Ngai-Man Cheung"], "categories": ["cs.LG", "cs.CR", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted in Transactions on Machine Learning Research (TMLR). First two authors contributed equally", "url": "http://arxiv.org/abs/2409.01062v2", "summary": "Model Inversion (MI) attacks pose a significant privacy threat by\nreconstructing private training data from machine learning models. While\nexisting defenses primarily concentrate on model-centric approaches, the impact\nof data on MI robustness remains largely unexplored. In this work, we explore\nRandom Erasing (RE), a technique traditionally used for improving model\ngeneralization under occlusion, and uncover its surprising effectiveness as a\ndefense against MI attacks. Specifically, our novel feature space analysis\nshows that models trained with RE-images introduce a significant discrepancy\nbetween the features of MI-reconstructed images and those of the private data.\nAt the same time, features of private images remain distinct from other classes\nand well-separated from different classification regions. These effects\ncollectively degrade MI reconstruction quality and attack accuracy while\nmaintaining reasonable natural accuracy. Furthermore, we explore two critical\nproperties of RE including Partial Erasure and Random Location. Partial Erasure\nprevents the model from observing entire objects during training. We find this\nhas a significant impact on MI, which aims to reconstruct the entire objects.\nRandom Location of erasure plays a crucial role in achieving a strong\nprivacy-utility trade-off. Our findings highlight RE as a simple yet effective\ndefense mechanism that can be easily integrated with existing\nprivacy-preserving techniques. Extensive experiments across 37 setups\ndemonstrate that our method achieves state-of-the-art (SOTA) performance in the\nprivacy-utility trade-off. The results consistently demonstrate the superiority\nof our defense over existing methods across different MI attacks, network\narchitectures, and attack configurations. For the first time, we achieve a\nsignificant degradation in attack accuracy without a decrease in utility for\nsome configurations.", "comment": "Accepted in Transactions on Machine Learning Research (TMLR). First\n  two authors contributed equally", "pdf_url": "http://arxiv.org/pdf/2409.01062v2", "cate": "cs.LG", "date": "2024-09-02", "updated": "2025-07-14"}
{"id": "2507.10543", "title": "MP1: Mean Flow Tames Policy Learning in 1-step for Robotic Manipulation", "authors": ["Juyi Sheng", "Ziyi Wang", "Peiming Li", "Mengyuan Liu"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10543v1", "summary": "In robot manipulation, robot learning has become a prevailing approach.\nHowever, generative models within this field face a fundamental trade-off\nbetween the slow, iterative sampling of diffusion models and the architectural\nconstraints of faster Flow-based methods, which often rely on explicit\nconsistency losses. To address these limitations, we introduce MP1, which pairs\n3D point-cloud inputs with the MeanFlow paradigm to generate action\ntrajectories in one network function evaluation (1-NFE). By directly learning\nthe interval-averaged velocity via the MeanFlow Identity, our policy avoids any\nadditional consistency constraints. This formulation eliminates numerical\nODE-solver errors during inference, yielding more precise trajectories. MP1\nfurther incorporates CFG for improved trajectory controllability while\nretaining 1-NFE inference without reintroducing structural constraints. Because\nsubtle scene-context variations are critical for robot learning, especially in\nfew-shot learning, we introduce a lightweight Dispersive Loss that repels state\nembeddings during training, boosting generalization without slowing inference.\nWe validate our method on the Adroit and Meta-World benchmarks, as well as in\nreal-world scenarios. Experimental results show MP1 achieves superior average\ntask success rates, outperforming DP3 by 10.2% and FlowPolicy by 7.3%. Its\naverage inference time is only 6.8 ms-19x faster than DP3 and nearly 2x faster\nthan FlowPolicy. Our code is available at https://mp1-2254.github.io/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10543v1", "cate": "cs.RO", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2406.16173", "title": "Crepe: A Mobile Screen Data Collector Using Graph Query", "authors": ["Yuwen Lu", "Meng Chen", "Qi Zhao", "Victor Cox", "Yang Yang", "Meng Jiang", "Jay Brockman", "Tamara Kay", "Toby Jia-Jun Li"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2406.16173v2", "summary": "Collecting mobile datasets remains challenging for academic researchers due\nto limited data access and technical barriers. Commercial organizations often\npossess exclusive access to mobile data, leading to a \"data monopoly\" that\nrestricts the independence of academic research. Existing open-source mobile\ndata collection frameworks primarily focus on mobile sensing data rather than\nscreen content, which is crucial for various research studies. We present\nCrepe, a no-code Android app that enables researchers to collect information\ndisplayed on screen through simple demonstrations of target data. Crepe\nutilizes a novel Graph Query technique which augments the structures of mobile\nUI screens to support flexible identification, location, and collection of\nspecific data pieces. The tool emphasizes participants' privacy and agency by\nproviding full transparency over collected data and allowing easy opt-out. We\ndesigned and built Crepe for research purposes only and in scenarios where\nresearchers obtain explicit consent from participants. Code for Crepe will be\nopen-sourced to support future academic research data collection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2406.16173v2", "cate": "cs.HC", "date": "2024-06-23", "updated": "2025-07-12"}
{"id": "2507.10134", "title": "FRSICL: LLM-Enabled In-Context Learning Flight Resource Allocation for Fresh Data Collection in UAV-Assisted Wildfire Monitoring", "authors": ["Yousef Emami", "Hao Zhou", "Miguel Gutierrez Gaitan", "Kai Li", "Luis Almeida"], "categories": ["cs.AI", "53-01", "C.2"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      8 pages, 8 figures", "url": "http://arxiv.org/abs/2507.10134v1", "summary": "Unmanned Aerial Vehicles (UAVs) are vital for public safety, particularly in\nwildfire monitoring, where early detection minimizes environmental impact. In\nUAV-Assisted Wildfire Monitoring (UAWM) systems, joint optimization of sensor\ntransmission scheduling and velocity is critical for minimizing Age of\nInformation (AoI) from stale sensor data. Deep Reinforcement Learning (DRL) has\nbeen used for such optimization; however, its limitations such as low sampling\nefficiency, simulation-to-reality gaps, and complex training render it\nunsuitable for time-critical applications like wildfire monitoring. This paper\nintroduces a new online Flight Resource Allocation scheme based on LLM-Enabled\nIn-Context Learning (FRSICL) to jointly optimize the UAV's flight control and\ndata collection schedule along the trajectory in real time, thereby\nasymptotically minimizing the average AoI across ground sensors. In contrast to\nDRL, FRSICL generates data collection schedules and controls velocity using\nnatural language task descriptions and feedback from the environment, enabling\ndynamic decision-making without extensive retraining. Simulation results\nconfirm the effectiveness of the proposed FRSICL compared to Proximal Policy\nOptimization (PPO) and Nearest-Neighbor baselines.", "comment": "8 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.10134v1", "cate": "cs.AI", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09207", "title": "Visual Surface Wave Elastography: Revealing Subsurface Physical Properties via Visible Surface Waves", "authors": ["Alexander C. Ogren", "Berthy T. Feng", "Jihoon Ahn", "Katherine L. Bouman", "Chiara Daraio"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025", "url": "http://arxiv.org/abs/2507.09207v1", "summary": "Wave propagation on the surface of a material contains information about\nphysical properties beneath its surface. We propose a method for inferring the\nthickness and stiffness of a structure from just a video of waves on its\nsurface. Our method works by extracting a dispersion relation from the video\nand then solving a physics-based optimization problem to find the best-fitting\nthickness and stiffness parameters. We validate our method on both simulated\nand real data, in both cases showing strong agreement with ground-truth\nmeasurements. Our technique provides a proof-of-concept for at-home health\nmonitoring of medically-informative tissue properties, and it is further\napplicable to fields such as human-computer interaction.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.09207v1", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09892", "title": "PathFuzzing: Worst Case Analysis by Fuzzing Symbolic-Execution Paths", "authors": ["Zimu Chen", "Di Wang"], "categories": ["cs.SE", "D.2.5"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      10 pages, 1 figure", "url": "http://arxiv.org/abs/2507.09892v1", "summary": "Estimating worst-case resource consumption is a critical task in software\ndevelopment. The worst-case analysis (WCA) problem is an optimization-based\nabstraction of this task. Fuzzing and symbolic execution are widely used\ntechniques for addressing the WCA problem. However, improving code coverage in\nfuzzing or managing path explosion in symbolic execution within the context of\nWCA poses significant challenges. In this paper, we propose PathFuzzing, aiming\nto combine the strengths of both techniques to design a WCA method. The key\nidea is to transform a program into a symbolic one that takes an execution path\n(encoded as a binary string) and interprets the bits as branch decisions.\nPathFuzzing then applies evolutionary fuzzing techniques to the transformed\nprogram to search for binary strings that represent satisfiable path conditions\nand lead to high resource consumption. We evaluate the performance of\nPathFuzzing experimentally on a benchmark suite that consists of prior work's\nbenchmarks and some added by us. Results show that PathFuzzing generally\noutperforms a fuzzing and a symbolic-execution baseline.", "comment": "10 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2507.09892v1", "cate": "cs.SE", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2501.17271", "title": "Rust Barefoot Runtime (RBFRT): Fast Runtime Control for the Intel Tofino", "authors": ["Etienne Zink", "Moritz Flüchter", "Steffen Lindner", "Fabian Ihle", "Michael Menth"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      This work has been accepted at the 4th KuVS Workshop on Network Softwarization (KuVS NetSoft) under the Creative Commons Attribution 4.0 International License (CC BY 4.0)", "url": "http://arxiv.org/abs/2501.17271v4", "summary": "Data plane programming enables the programmability of network devices with\ndomain-specific programming languages, like P4. One commonly used\nP4-programmable hardware target is the Intel Tofino switching ASIC. The runtime\nbehavior of an implemented P4 program on Tofino can be configured with shell\nscripts or a Python library from Barefoot provided with the Tofino. Both are\nlimited in their capabilities and usability. This paper introduces the Rust\nBarefoot Runtime (RBFRT), a Rust-based control plane library. The RBFRT\nprovides a fast and memory-safe interface to configure the Intel Tofino. We\nshowed that the RBFRT achieves a higher insertion rate for MAT entries and has\na shorter response time compared to the Python library.", "comment": "This work has been accepted at the 4th KuVS Workshop on Network\n  Softwarization (KuVS NetSoft) under the Creative Commons Attribution 4.0\n  International License (CC BY 4.0)", "pdf_url": "http://arxiv.org/pdf/2501.17271v4", "cate": "cs.NI", "date": "2025-01-28", "updated": "2025-07-14"}
{"id": "2507.10113", "title": "Improved Differential Evolution for Enhancing the Aggregated Channel Estimation of RIS-Aided Cell-Free Massive MIMO", "authors": ["Trinh Van Chien", "Nguyen Hoang Viet", "Symeon Chatzinotas", "Lajos Hanzo"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      6 pages, 1 figures, and 3 tables. Accepted by IEEE TVT", "url": "http://arxiv.org/abs/2507.10113v1", "summary": "Cell-Free Massive multiple-input multiple-output (MIMO) systems are\ninvestigated with the support of a reconfigurable intelligent surface (RIS).\nThe RIS phase shifts are designed for improved channel estimation in the\npresence of spatial correlation. Specifically, we formulate the channel\nestimate and estimation error expressions using linear minimum mean square\nerror (LMMSE) estimation for the aggregated channels. An optimization problem\nis then formulated to minimize the average normalized mean square error (NMSE)\nsubject to practical phase shift constraints. To circumvent the problem of\ninherent nonconvexity, we then conceive an enhanced version of the differential\nevolution algorithm that is capable of avoiding local minima by introducing an\naugmentation operator applied to some high-performing Diffential Evolution (DE)\nindividuals. Numerical results indicate that our proposed algorithm can\nsignificantly improve the channel estimation quality of the state-of-the-art\nbenchmarks.", "comment": "6 pages, 1 figures, and 3 tables. Accepted by IEEE TVT", "pdf_url": "http://arxiv.org/pdf/2507.10113v1", "cate": "cs.IT", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.03255", "title": "ForgeHLS: A Large-Scale, Open-Source Dataset for High-Level Synthesis", "authors": ["Zedong Peng", "Zeju Li", "Mingzhe Gao", "Qiang Xu", "Chen Zhang", "Jieru Zhao"], "categories": ["cs.AR", "cs.AI"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.03255v2", "summary": "High-Level Synthesis (HLS) plays a crucial role in modern hardware design by\ntransforming high-level code into optimized hardware implementations. However,\nprogress in applying machine learning (ML) to HLS optimization has been\nhindered by a shortage of sufficiently large and diverse datasets. To bridge\nthis gap, we introduce ForgeHLS, a large-scale, open-source dataset explicitly\ndesigned for ML-driven HLS research. ForgeHLS comprises over 400,000 diverse\ndesigns generated from 536 kernels covering a broad range of application\ndomains. Each kernel includes systematically automated pragma insertions (loop\nunrolling, pipelining, array partitioning), combined with extensive design\nspace exploration using Bayesian optimization. Compared to existing datasets,\nForgeHLS significantly enhances scale, diversity, and design coverage. We\nfurther define and evaluate representative downstream tasks, such as Quality of\nResult (QoR) prediction and automated pragma exploration, clearly demonstrating\nForgeHLS's utility for developing and improving ML-based HLS optimization\nmethodologies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.03255v2", "cate": "cs.AR", "date": "2025-07-04", "updated": "2025-07-14"}
{"id": "2507.10413", "title": "Consensus, Inconsistency, Emergence: what's paraconsistency got to do with it?", "authors": ["Gabriel Rocha"], "categories": ["cs.DC", "cs.CC", "cs.IT", "cs.LO", "math.IT"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      10 pages", "url": "http://arxiv.org/abs/2507.10413v1", "summary": "The consensus problem, briefly stated, consists of having processes in an\nasynchronous distributed system agree on a value. It is widely known that the\nconsensus problem does not have a deterministic solution that ensures both\ntermination and consistency, if there is at least one faulty process in the\nsystem. This result, known as the FLP impossibility theorem, led to several\ngeneralizations and developments in theoretical distributed computing. This\npaper argues that the FLP impossibility theorem holds even under a generalized\ndefinition of computation through oracles. Furthermore, using a theoretical\nmachinery from complex systems, this paper also posits that inconsistency may\nbe an emergent feature of consensus over distributed systems by examining how a\nsystem transitions phases. Under the same complex systems framework, this paper\nexamines paraconsistent logics, arguing that while inconsistency is not an\nemergent feature for these logics, triviality may be. Lastly, some attention is\ngiven to the possibility of developing consensus algorithms capable of\nparaconsistent reasoning.", "comment": "10 pages", "pdf_url": "http://arxiv.org/pdf/2507.10413v1", "cate": "cs.DC", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09676", "title": "Can AI Rely on the Systematicity of Truth? The Challenge of Modelling Normative Domains", "authors": ["Matthieu Queloz"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09676v1", "summary": "A key assumption fuelling optimism about the progress of large language\nmodels (LLMs) in accurately and comprehensively modelling the world is that the\ntruth is systematic: true statements about the world form a whole that is not\njust consistent, in that it contains no contradictions, but coherent, in that\nthe truths are inferentially interlinked. This holds out the prospect that LLMs\nmight in principle rely on that systematicity to fill in gaps and correct\ninaccuracies in the training data: consistency and coherence promise to\nfacilitate progress towards comprehensiveness in an LLM's representation of the\nworld. However, philosophers have identified compelling reasons to doubt that\nthe truth is systematic across all domains of thought, arguing that in\nnormative domains, in particular, the truth is largely asystematic. I argue\nthat insofar as the truth in normative domains is asystematic, this renders it\ncorrespondingly harder for LLMs to make progress, because they cannot then\nleverage the systematicity of truth. And the less LLMs can rely on the\nsystematicity of truth, the less we can rely on them to do our practical\ndeliberation for us, because the very asystematicity of normative domains\nrequires human agency to play a greater role in practical thought.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09676v1", "cate": "cs.CY", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.10196", "title": "Non-smooth optimization meets automated material model discovery", "authors": ["Moritz Flaschel", "Trevor Hastie", "Ellen Kuhl"], "categories": ["cs.CE", "cond-mat.mtrl-sci"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10196v1", "summary": "Automated material model discovery disrupts the tedious and time-consuming\ncycle of iteratively calibrating and modifying manually designed models.\nNon-smooth L1-norm regularization is the backbone of automated model discovery;\nhowever, the current literature on automated material model discovery offers\nlimited insights into the robust and efficient minimization of non-smooth\nobjective functions. In this work, we examine the minimization of functions of\nthe form f(w) + a ||w||_1, where w are the material model parameters, f is a\nmetric that quantifies the mismatch between the material model and the observed\ndata, and a is a regularization parameter that determines the sparsity of the\nsolution. We investigate both the straightforward case where f is quadratic and\nthe more complex scenario where it is non-quadratic or even non-convex.\nImportantly, we do not only focus on methods that solve the sparse regression\nproblem for a given value of the regularization parameter a, but propose\nmethods to efficiently compute the entire regularization path, facilitating the\nselection of a suitable a. Specifically, we present four algorithms and discuss\ntheir roles for automated material model discovery in mechanics: First, we\nrecapitulate a well-known coordinate descent algorithm that solves the\nminimization problem assuming that f is quadratic for a given value of a, also\nknown as the LASSO. Second, we discuss the algorithm LARS, which automatically\ndetermines the critical values of a, at which material parameters in w are set\nto zero. Third, we propose to use the proximal gradient method ISTA for\nautomated material model discovery if f is not quadratic, and fourth, we\nsuggest a pathwise extension of ISTA for computing the regularization path. We\ndemonstrate the applicability of all algorithms for the discovery of\nhyperelastic material models from uniaxial tension and simple shear data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10196v1", "cate": "cs.CE", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09764", "title": "Rule-based Generation of de Bruijn Sequences: Memory and Learning", "authors": ["Francisco J. Muñoz", "Juan Carlos Nuño"], "categories": ["cs.FL", "math.DS"], "primary_category": "Subjects:       Formal Languages and Automata Theory (cs.FL)", "pdf_link": null, "comments": "Comments:      9 pages, 2 figures", "url": "http://arxiv.org/abs/2507.09764v1", "summary": "We investigate binary sequences generated by non-Markovian rules with memory\nlength $\\mu$, similar to those adopted in Elementary Cellular Automata. This\ngeneration procedure is equivalente to a shift register and certain rules\nproduce sequences with maximal periods, known as de Bruijn sequences. We\nintroduce a novel methodology for generating de Bruijn sequences that combines:\n(i) a set of derived properties that significantly reduce the space of feasible\ngenerating rules, and (ii) a neural network-based classifier that identifies\nwhich rules produce de Bruijn sequences. Experiments for large values of $\\mu$\ndemonstrate the approach's effectiveness and computational efficiency.", "comment": "9 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.09764v1", "cate": "cs.FL", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2410.04916", "title": "Defense-as-a-Service: Black-box Shielding against Backdoored Graph Models", "authors": ["Xiao Yang", "Kai Zhou", "Yuni Lai", "Gaolei Li"], "categories": ["cs.LG", "cs.AI", "cs.CR", "F.2.2"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      We have to add a rigorous mathematical proof to the thesis proposal, and the process of the current proposal is not rigorous enough", "url": "http://arxiv.org/abs/2410.04916v2", "summary": "With the trend of large graph learning models, business owners tend to employ\na model provided by a third party to deliver business services to users.\nHowever, these models might be backdoored, and malicious users can submit\ntrigger-embedded inputs to manipulate the model predictions. Current graph\nbackdoor defenses have several limitations: 1) depending on model-related\ndetails, 2) requiring additional model fine-tuning, and 3) relying upon extra\nexplainability tools, all of which are infeasible under stringent privacy\npolicies. To address those limitations, we propose GraphProt, which allows\nresource-constrained business owners to rely on third parties to avoid backdoor\nattacks on GNN-based graph classifiers. Our GraphProt is model-agnostic and\nonly relies on the input graph. The key insight is to leverage subgraph\ninformation for prediction, thereby mitigating backdoor effects induced by\ntriggers. GraphProt comprises two components: clustering-based trigger\nelimination and robust subgraph ensemble. Specifically, we first propose\nfeature-topology clustering that aims to remove most of the anomalous subgraphs\n(triggers). Moreover, we design subgraph sampling strategies based on\nfeature-topology clustering to build a robust classifier via majority vote.\nExperimental results across three backdoor attacks and six benchmark datasets\ndemonstrate that GraphProt significantly reduces the backdoor attack success\nrate while preserving the model accuracy on regular graph classification tasks.", "comment": "We have to add a rigorous mathematical proof to the thesis proposal,\n  and the process of the current proposal is not rigorous enough", "pdf_url": "http://arxiv.org/pdf/2410.04916v2", "cate": "cs.LG", "date": "2024-10-07", "updated": "2025-07-14"}
{"id": "2507.08848", "title": "Assuring the Safety of Reinforcement Learning Components: AMLAS-RL", "authors": ["Calum Corrie Imrie", "Ioannis Stefanakos", "Sepeedeh Shahbeigi", "Richard Hawkins", "Simon Burton"], "categories": ["cs.LG", "cs.AI", "cs.RO", "cs.SE"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08848v1", "summary": "The rapid advancement of machine learning (ML) has led to its increasing\nintegration into cyber-physical systems (CPS) across diverse domains. While CPS\noffer powerful capabilities, incorporating ML components introduces significant\nsafety and assurance challenges. Among ML techniques, reinforcement learning\n(RL) is particularly suited for CPS due to its capacity to handle complex,\ndynamic environments where explicit models of interaction between system and\nenvironment are unavailable or difficult to construct. However, in\nsafety-critical applications, this learning process must not only be effective\nbut demonstrably safe. Safe-RL methods aim to address this by incorporating\nsafety constraints during learning, yet they fall short in providing systematic\nassurance across the RL lifecycle. The AMLAS methodology offers structured\nguidance for assuring the safety of supervised learning components, but it does\nnot directly apply to the unique challenges posed by RL. In this paper, we\nadapt AMLAS to provide a framework for generating assurance arguments for an\nRL-enabled system through an iterative process; AMLAS-RL. We demonstrate\nAMLAS-RL using a running example of a wheeled vehicle tasked with reaching a\ntarget goal without collision.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08848v1", "cate": "cs.LG", "date": "2025-07-08", "updated": "2025-07-08"}
{"id": "2503.16465", "title": "OS-Kairos: Adaptive Interaction for MLLM-Powered GUI Agents", "authors": ["Pengzhou Cheng", "Zheng Wu", "Zongru Wu", "Aston Zhang", "Zhuosheng Zhang", "Gongshen Liu"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      25 pages, 24 figures, 11 tables (ACL 2025, Findings)", "url": "http://arxiv.org/abs/2503.16465v3", "summary": "Autonomous graphical user interface (GUI) agents powered by multimodal large\nlanguage models have shown great promise. However, a critical yet underexplored\nissue persists: over-execution, where the agent executes tasks in a fully\nautonomous way, without adequate assessment of its action confidence to\ncompromise an adaptive human-agent collaboration. This poses substantial risks\nin complex scenarios, such as those involving ambiguous user instructions,\nunexpected interruptions, and environmental hijacks. To address the issue, we\nintroduce OS-Kairos, an adaptive GUI agent capable of predicting confidence\nlevels at each interaction step and efficiently deciding whether to act\nautonomously or seek human intervention. OS-Kairos is developed through two key\nmechanisms: (i) collaborative probing that annotates confidence scores at each\ninteraction step; (ii) confidence-driven interaction that leverages these\nconfidence scores to elicit the ability of adaptive interaction. Experimental\nresults show that OS-Kairos substantially outperforms existing models on our\ncurated dataset featuring complex scenarios, as well as on established\nbenchmarks such as AITZ and Meta-GUI, with 24.59\\%$\\sim$87.29\\% improvements in\ntask success rate. OS-Kairos facilitates an adaptive human-agent collaboration,\nprioritizing effectiveness, generality, scalability, and efficiency for\nreal-world GUI interaction. The dataset and codes are available at\nhttps://github.com/Wuzheng02/OS-Kairos.", "comment": "25 pages, 24 figures, 11 tables (ACL 2025, Findings)", "pdf_url": "http://arxiv.org/pdf/2503.16465v3", "cate": "cs.HC", "date": "2025-02-26", "updated": "2025-07-14"}
{"id": "2507.10156", "title": "Introducing the Swiss Food Knowledge Graph: AI for Context-Aware Nutrition Recommendation", "authors": ["Lubnaa Abdur Rahman", "Ioannis Papathanail", "Stavroula Mougiakakou"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      10 pages, 2 Figures, 7 tables", "url": "http://arxiv.org/abs/2507.10156v1", "summary": "AI has driven significant progress in the nutrition field, especially through\nmultimedia-based automatic dietary assessment. However, existing automatic\ndietary assessment systems often overlook critical non-visual factors, such as\nrecipe-specific ingredient substitutions that can significantly alter\nnutritional content, and rarely account for individual dietary needs, including\nallergies, restrictions, cultural practices, and personal preferences. In\nSwitzerland, while food-related information is available, it remains\nfragmented, and no centralized repository currently integrates all relevant\nnutrition-related aspects within a Swiss context. To bridge this divide, we\nintroduce the Swiss Food Knowledge Graph (SwissFKG), the first resource, to our\nbest knowledge, to unite recipes, ingredients, and their substitutions with\nnutrient data, dietary restrictions, allergen information, and national\nnutrition guidelines under one graph. We establish a LLM-powered enrichment\npipeline for populating the graph, whereby we further present the first\nbenchmark of four off-the-shelf (<70 B parameter) LLMs for food knowledge\naugmentation. Our results demonstrate that LLMs can effectively enrich the\ngraph with relevant nutritional information. Our SwissFKG goes beyond recipe\nrecommendations by offering ingredient-level information such as allergen and\ndietary restriction information, and guidance aligned with nutritional\nguidelines. Moreover, we implement a Graph-RAG application to showcase how the\nSwissFKG's rich natural-language data structure can help LLM answer\nuser-specific nutrition queries, and we evaluate LLM-embedding pairings by\ncomparing user-query responses against predefined expected answers. As such,\nour work lays the foundation for the next generation of dietary assessment\ntools that blend visual, contextual, and cultural dimensions of eating.", "comment": "10 pages, 2 Figures, 7 tables", "pdf_url": "http://arxiv.org/pdf/2507.10156v1", "cate": "cs.AI", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09209", "title": "Uncertainty-Driven Expert Control: Enhancing the Reliability of Medical Vision-Language Models", "authors": ["Xiao Liang", "Di Wang", "Zhicheng Jiao", "Ronghan Li", "Pengfei Yang", "Quan Wang", "Tat-Seng Chua"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09209v1", "summary": "The rapid advancements in Vision Language Models (VLMs) have prompted the\ndevelopment of multi-modal medical assistant systems. Despite this progress,\ncurrent models still have inherent probabilistic uncertainties, often producing\nerroneous or unverified responses-an issue with serious implications in medical\napplications. Existing methods aim to enhance the performance of Medical Vision\nLanguage Model (MedVLM) by adjusting model structure, fine-tuning with\nhigh-quality data, or through preference fine-tuning. However, these\ntraining-dependent strategies are costly and still lack sufficient alignment\nwith clinical expertise. To address these issues, we propose an\nexpert-in-the-loop framework named Expert-Controlled Classifier-Free Guidance\n(Expert-CFG) to align MedVLM with clinical expertise without additional\ntraining. This framework introduces an uncertainty estimation strategy to\nidentify unreliable outputs. It then retrieves relevant references to assist\nexperts in highlighting key terms and applies classifier-free guidance to\nrefine the token embeddings of MedVLM, ensuring that the adjusted outputs are\ncorrect and align with expert highlights. Evaluations across three medical\nvisual question answering benchmarks demonstrate that the proposed Expert-CFG,\nwith 4.2B parameters and limited expert annotations, outperforms\nstate-of-the-art models with 13B parameters. The results demonstrate the\nfeasibility of deploying such a system in resource-limited settings for\nclinical use.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09209v1", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09907", "title": "Modelling Interrelations Between Agile Practices: The Agile Map", "authors": ["Thomas Hansper", "Kevin Phong Pham", "Michael Neumann"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09907v1", "summary": "Agile methods are defined through guidelines comprising various practices\nintended to enable agile ways of working. These guidelines further comprise a\nspecific set of agile practices aiming to enable teams for an agile way of\nworking. However, due to its wide-spread use in practice we know that agile\npractices are adopted and tailored intensively, which lead to a high variety of\nagile practices in terms of their level of detail. Problem: A high variety of\nagile practices can be challenging as we do not know how different agile\npractices are interrelated with each other. To be more precise, tailoring and\nadopting agile practices may lead to the challenge, that the combinatorial use\nof several agile practices can only be successful to a limited extent, as\npractices support or even require each other for a effective use in practice.\nObjective: Our study aims to provide an enabler for this problem. We want to\nidentify interrelations between agile practices and describe them in a\nsystematic manner. Contribution: The core contribution of this paper is the\nAgile Map, a theoretical model describing relations between agile practices\nfollowing a systematic approach aiming to provide an overview of coherences\nbetween agile practices. The model aims to support practitioners in selecting\nand combining agile practices in a meaningful way.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09907v1", "cate": "cs.SE", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2502.10687", "title": "Multi-objective Low-altitude IRS-assisted ISAC Optimization via Generative AI-enhanced Deep Reinforcement Learning", "authors": ["Wenwen Xie", "Geng Sun", "Jiacheng Wang", "Hongyang Du", "Jiawen Kang", "Dusit Niyato", "Kaibin Huang", "Victor C. M. Leung"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.10687v2", "summary": "Integrated sensing and communication (ISAC) has garnered substantial research\ninterest owing to its pivotal role in advancing the development of\nnext-generation (6G) wireless networks. However, achieving a performance\nbalance between communication and sensing in the dual-function radar\ncommunication (DFRC)-based ISAC system remains a significant challenge. In this\npaper, a low-altitude intelligent reflecting surface (IRS)-assisted ISAC system\nis explored, where a base station (BS) supports dual-functional operations,\nenabling both data transmission for multiple users and sensing for a blocked\ntarget, with the channel quality enhanced by an IRS mounted on the unmanned\naerial vehicle (UAV). Moreover, we formulate an integrated communication,\nsensing, and energy efficiency multi-objective optimization problem (CSEMOP),\nwhich aims to maximize the communication rate of the users and the sensing rate\nof the target, while minimizing UAV propulsion energy consumption by jointly\noptimizing the BS beamforming matrix, IRS phase shifts, the flight velocity and\nangle of the UAV. Considering the non-convexity, trade-off, and dynamic nature\nof the formulated CSEMOP, we propose a generative diffusion model-based deep\ndeterministic policy gradient (GDMDDPG) algorithm to solve the problem.\nSpecifically, the diffusion model is incorporated into the actor network of\nDDPG to improve the action quality, with noise perturbation mechanism for\nbetter exploration and recent prioritized experience replay (RPER) sampling\nmechanism for enhanced training efficiency. Simulation results indicate that\nthe GDMDDPG algorithm delivers superior performance compared to the existing\nmethods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.10687v2", "cate": "cs.NI", "date": "2025-02-15", "updated": "2025-07-13"}
{"id": "2507.10185", "title": "High Girth Spatially-Coupled LDPC Codes with Hierarchical Structure", "authors": ["Haizheng Li", "Sisi Miao", "Laurent Schmalen"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      Accepted at ISTC 2025", "url": "http://arxiv.org/abs/2507.10185v1", "summary": "Quasi-cyclic (QC) low-density parity-check (LDPC) codes are a class of LDPC\ncodes with a simple construction facilitating hardware implementation while\nachieving excellent performance. In this paper, we introduce an algorithm that\nconstructs QC spatially-coupled (SC) LDPC codes with large girth while keeping\nthe constraint length small. The algorithm offers a \"protograph to basegraph\"\nconstruction, focusing on finding small lifting sizes of QC codes while\navoiding short cycles. This work extends the hierarchical quasi-cyclic (HQC)\nconstruction for block LDPC codes proposed by Wang et al. to the spatially\ncoupled case. The construction is based on the cycle relevant matrix (CRM)\nderived from the periodic structure of time-invariant SC-LDPC codes. Numerical\nresults show that the proposed algorithm effectively achieves the target girth\nwith a small lifting factor, enabling low-complexity SC code construction.", "comment": "Accepted at ISTC 2025", "pdf_url": "http://arxiv.org/pdf/2507.10185v1", "cate": "cs.IT", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2411.02814", "title": "The Hitchhiker's Guide to Programming and Optimizing Cache Coherent Heterogeneous Systems: CXL, NVLink-C2C, and AMD Infinity Fabric", "authors": ["Zixuan Wang", "Suyash Mahar", "Luyi Li", "Jangseon Park", "Jinpyo Kim", "Theodore Michailidis", "Yue Pan", "Mingyao Shen", "Tajana Rosing", "Dean Tullsen", "Steven Swanson", "Jishen Zhao"], "categories": ["cs.PF", "cs.AR", "cs.DC", "cs.OS"], "primary_category": "Subjects:       Performance (cs.PF)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.02814v2", "summary": "We present a thorough analysis of the use of modern heterogeneous systems\ninterconnected by various cachecoherent links, including CXL, NVLink-C2C, and\nInfinity Fabric. We studied a wide range of server systems that combined CPUs\nfrom different vendors and various types of coherent memory devices, including\nCXL memory expander, CXL pool, CXL shared memory, GH200 GPU, and AMD MI300a\nHBM. For this study, we developed a heterogeneous memory benchmark suite,\nHeimdall, to profile the performance of such heterogeneous systems and present\na detailed performance comparison across systems. By leveraging H E I M DA L L\n, we unveiled the detailed architecture design in these systems, drew\nobservations on optimizing performance for workloads, and pointed out\ndirections for future development of cache coherent heterogeneous systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.02814v2", "cate": "cs.PF", "date": "2024-11-05", "updated": "2025-07-14"}
{"id": "2507.10430", "title": "Efficient Federated Learning with Heterogeneous Data and Adaptive Dropout", "authors": ["Ji Liu", "Beichen Ma", "Yang Zhou", "Jingbo Zhou", "Ruoming Jin", "Dejing Dou", "Huaiyu Dai", "Haixun Wang", "Patrick Valduriez"], "categories": ["cs.DC", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      29 pages, to appear in ACM Transactions on Knowledge Discovery from Data (TKDD)", "url": "http://arxiv.org/abs/2507.10430v1", "summary": "Federated Learning (FL) is a promising distributed machine learning approach\nthat enables collaborative training of a global model using multiple edge\ndevices. The data distributed among the edge devices is highly heterogeneous.\nThus, FL faces the challenge of data distribution and heterogeneity, where\nnon-Independent and Identically Distributed (non-IID) data across edge devices\nmay yield in significant accuracy drop. Furthermore, the limited computation\nand communication capabilities of edge devices increase the likelihood of\nstragglers, thus leading to slow model convergence. In this paper, we propose\nthe FedDHAD FL framework, which comes with two novel methods: Dynamic\nHeterogeneous model aggregation (FedDH) and Adaptive Dropout (FedAD). FedDH\ndynamically adjusts the weights of each local model within the model\naggregation process based on the non-IID degree of heterogeneous data to deal\nwith the statistical data heterogeneity. FedAD performs neuron-adaptive\noperations in response to heterogeneous devices to improve accuracy while\nachieving superb efficiency. The combination of these two methods makes FedDHAD\nsignificantly outperform state-of-the-art solutions in terms of accuracy (up to\n6.7% higher), efficiency (up to 2.02 times faster), and computation cost (up to\n15.0% smaller).", "comment": "29 pages, to appear in ACM Transactions on Knowledge Discovery from\n  Data (TKDD)", "pdf_url": "http://arxiv.org/pdf/2507.10430v1", "cate": "cs.DC", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.08866", "title": "Underrepresentation, Label Bias, and Proxies: Towards Data Bias Profiles for the EU AI Act and Beyond", "authors": ["Marina Ceccon", "Giandomenico Cornacchia", "Davide Dalle Pezze", "Alessandro Fabris", "Gian Antonio Susto"], "categories": ["cs.LG", "cs.CY", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted in Expert Systems with Applications", "url": "http://arxiv.org/abs/2507.08866v1", "summary": "Undesirable biases encoded in the data are key drivers of algorithmic\ndiscrimination. Their importance is widely recognized in the algorithmic\nfairness literature, as well as legislation and standards on\nanti-discrimination in AI. Despite this recognition, data biases remain\nunderstudied, hindering the development of computational best practices for\ntheir detection and mitigation. In this work, we present three common data\nbiases and study their individual and joint effect on algorithmic\ndiscrimination across a variety of datasets, models, and fairness measures. We\nfind that underrepresentation of vulnerable populations in training sets is\nless conducive to discrimination than conventionally affirmed, while\ncombinations of proxies and label bias can be far more critical. Consequently,\nwe develop dedicated mechanisms to detect specific types of bias, and combine\nthem into a preliminary construct we refer to as the Data Bias Profile (DBP).\nThis initial formulation serves as a proof of concept for how different bias\nsignals can be systematically documented. Through a case study with popular\nfairness datasets, we demonstrate the effectiveness of the DBP in predicting\nthe risk of discriminatory outcomes and the utility of fairness-enhancing\ninterventions. Overall, this article bridges algorithmic fairness research and\nanti-discrimination policy through a data-centric lens.", "comment": "Accepted in Expert Systems with Applications", "pdf_url": "http://arxiv.org/pdf/2507.08866v1", "cate": "cs.LG", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.10448", "title": "FinTeam: A Multi-Agent Collaborative Intelligence System for Comprehensive Financial Scenarios", "authors": ["Yingqian Wu", "Qiushi Wang", "Zefei Long", "Rong Ye", "Zhongtian Lu", "Xianyin Zhang", "Bingxuan Li", "Wei Chen", "Liwen Zhang", "Zhongyu Wei"], "categories": ["cs.CE", "cs.LG"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "Comments:      NLPCC 2025 Oral", "url": "http://arxiv.org/abs/2507.10448v1", "summary": "Financial report generation tasks range from macro- to micro-economics\nanalysis, also requiring extensive data analysis. Existing LLM models are\nusually fine-tuned on simple QA tasks and cannot comprehensively analyze real\nfinancial scenarios. Given the complexity, financial companies often distribute\ntasks among departments. Inspired by this, we propose FinTeam, a financial\nmulti-agent collaborative system, with a workflow with four LLM agents:\ndocument analyzer, analyst, accountant, and consultant. We train these agents\nwith specific financial expertise using constructed datasets. We evaluate\nFinTeam on comprehensive financial tasks constructed from real online\ninvestment forums, including macroeconomic, industry, and company analysis. The\nhuman evaluation shows that by combining agents, the financial reports generate\nfrom FinTeam achieved a 62.00% acceptance rate, outperforming baseline models\nlike GPT-4o and Xuanyuan. Additionally, FinTeam's agents demonstrate a 7.43%\naverage improvement on FinCUGE and a 2.06% accuracy boost on FinEval. Project\nis available at https://github.com/FudanDISC/DISC-FinLLM/.", "comment": "NLPCC 2025 Oral", "pdf_url": "http://arxiv.org/pdf/2507.10448v1", "cate": "cs.CE", "date": "2025-07-05", "updated": "2025-07-05"}
{"id": "2507.09387", "title": "Words with factor somplexity $2n+1$ and minimal critical exponent", "authors": ["James D. Currie"], "categories": ["math.CO", "cs.FL", "68R15", "G.2.1"], "primary_category": "Subjects:       Combinatorics (math.CO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09387v1", "summary": "We show that words with factor complexity 2n+1 have critical exponent at\nleast $\\mu$, where $\\mu=2+\\frac{1}{\\lambda^2-1}= 2.4808726\\cdots$, where\n$\\lambda=1.7548777$ is the real zero of $x^3-2x+x-1=0$. This confirms a\nconjecture of Shallit and Shur.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09387v1", "cate": "math.CO", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2502.07776", "title": "Auditing Prompt Caching in Language Model APIs", "authors": ["Chenchen Gu", "Xiang Lisa Li", "Rohith Kuditipudi", "Percy Liang", "Tatsunori Hashimoto"], "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted at ICML 2025", "url": "http://arxiv.org/abs/2502.07776v2", "summary": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known.", "comment": "Accepted at ICML 2025", "pdf_url": "http://arxiv.org/pdf/2502.07776v2", "cate": "cs.CL", "date": "2025-02-11", "updated": "2025-07-13"}
{"id": "2507.09041", "title": "Behavioral Exploration: Learning to Explore via In-Context Adaptation", "authors": ["Andrew Wagenmaker", "Zhiyuan Zhou", "Sergey Levine"], "categories": ["cs.LG", "cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09041v1", "summary": "Developing autonomous agents that quickly explore an environment and adapt\ntheir behavior online is a canonical challenge in robotics and machine\nlearning. While humans are able to achieve such fast online exploration and\nadaptation, often acquiring new information and skills in only a handful of\ninteractions, existing algorithmic approaches tend to rely on random\nexploration and slow, gradient-based behavior updates. How can we endow\nautonomous agents with such capabilities on par with humans? Taking inspiration\nfrom recent progress on both in-context learning and large-scale behavioral\ncloning, in this work we propose behavioral exploration: training agents to\ninternalize what it means to explore and adapt in-context over the space of\n``expert'' behaviors. To achieve this, given access to a dataset of expert\ndemonstrations, we train a long-context generative model to predict expert\nactions conditioned on a context of past observations and a measure of how\n``exploratory'' the expert's behaviors are relative to this context. This\nenables the model to not only mimic the behavior of an expert, but also, by\nfeeding its past history of interactions into its context, to select different\nexpert behaviors than what have been previously selected, thereby allowing for\nfast online adaptation and targeted, ``expert-like'' exploration. We\ndemonstrate the effectiveness of our method in both simulated locomotion and\nmanipulation settings, as well as on real-world robotic manipulation tasks,\nillustrating its ability to learn adaptive, exploratory behavior.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09041v1", "cate": "cs.LG", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2506.22941", "title": "Positioning AI Tools to Support Online Harm Reduction Practice: Applications and Design Directions", "authors": ["Kaixuan Wang", "Jason T. Jacques", "Chenxin Diao", "Carl-Cyril J Dreue"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      16 pages, 4 figures, with appendix", "url": "http://arxiv.org/abs/2506.22941v3", "summary": "Access to accurate and actionable harm reduction information can directly\nimpact the health outcomes of People Who Use Drugs (PWUD), yet existing online\nchannels often fail to meet their diverse and dynamic needs due to limitations\nin adaptability, accessibility, and the pervasive impact of stigma. Large\nLanguage Models (LLMs) present a novel opportunity to enhance information\nprovision, but their application in such a high-stakes domain is under-explored\nand presents socio-technical challenges. This paper investigates how LLMs can\nbe responsibly designed to support the information needs of PWUD. Through a\nqualitative workshop involving diverse stakeholder groups (academics, harm\nreduction practitioners, and an online community moderator), we explored LLM\ncapabilities, identified potential use cases, and delineated core design\nconsiderations. Our findings reveal that while LLMs can address some existing\ninformation barriers (e.g., by offering responsive, multilingual, and\npotentially less stigmatising interactions), their effectiveness is contingent\nupon overcoming challenges related to ethical alignment with harm reduction\nprinciples, nuanced contextual understanding, effective communication, and\nclearly defined operational boundaries. We articulate design pathways\nemphasising collaborative co-design with experts and PWUD to develop LLM\nsystems that are helpful, safe, and responsibly governed. This work contributes\nempirically grounded insights and actionable design considerations for the\nresponsible development of LLMs as supportive tools within the harm reduction\necosystem.", "comment": "16 pages, 4 figures, with appendix", "pdf_url": "http://arxiv.org/pdf/2506.22941v3", "cate": "cs.HC", "date": "2025-06-28", "updated": "2025-07-13"}
{"id": "2507.10174", "title": "Should We Ever Prefer Decision Transformer for Offline Reinforcement Learning?", "authors": ["Yumi Omori", "Zixuan Dong", "Keith Ross"], "categories": ["cs.AI", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Accepted by RLBrew: Ingredients for Developing Generalist Agents workshop (RLC 2025)", "url": "http://arxiv.org/abs/2507.10174v1", "summary": "In recent years, extensive work has explored the application of the\nTransformer architecture to reinforcement learning problems. Among these,\nDecision Transformer (DT) has gained particular attention in the context of\noffline reinforcement learning due to its ability to frame return-conditioned\npolicy learning as a sequence modeling task. Most recently, Bhargava et al.\n(2024) provided a systematic comparison of DT with more conventional MLP-based\noffline RL algorithms, including Behavior Cloning (BC) and Conservative\nQ-Learning (CQL), and claimed that DT exhibits superior performance in\nsparse-reward and low-quality data settings.\n  In this paper, through experimentation on robotic manipulation tasks\n(Robomimic) and locomotion benchmarks (D4RL), we show that MLP-based Filtered\nBehavior Cloning (FBC) achieves competitive or superior performance compared to\nDT in sparse-reward environments. FBC simply filters out low-performing\ntrajectories from the dataset and then performs ordinary behavior cloning on\nthe filtered dataset. FBC is not only very straightforward, but it also\nrequires less training data and is computationally more efficient. The results\ntherefore suggest that DT is not preferable for sparse-reward environments.\nFrom prior work, arguably, DT is also not preferable for dense-reward\nenvironments. Thus, we pose the question: Is DT ever preferable?", "comment": "Accepted by RLBrew: Ingredients for Developing Generalist Agents\n  workshop (RLC 2025)", "pdf_url": "http://arxiv.org/pdf/2507.10174v1", "cate": "cs.AI", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09214", "title": "Stereo-based 3D Anomaly Object Detection for Autonomous Driving: A New Dataset and Baseline", "authors": ["Shiyi Mu", "Zichong Gu", "Hanqi Lyu", "Yilin Gao", "Shugong Xu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      under review", "url": "http://arxiv.org/abs/2507.09214v1", "summary": "3D detection technology is widely used in the field of autonomous driving,\nwith its application scenarios gradually expanding from enclosed highways to\nopen conventional roads. For rare anomaly categories that appear on the road,\n3D detection models trained on closed sets often misdetect or fail to detect\nanomaly objects. To address this risk, it is necessary to enhance the\ngeneralization ability of 3D detection models for targets of arbitrary shapes\nand to possess the capability to filter out anomalies. The generalization of 3D\ndetection is limited by two factors: the coupled training of 2D and 3D, and the\ninsufficient diversity in the scale distribution of training samples. This\npaper proposes a Stereo-based 3D Anomaly object Detection (S3AD) algorithm,\nwhich decouples the training strategy of 3D and 2D to release the\ngeneralization ability for arbitrary 3D foreground detection, and proposes an\nanomaly scoring algorithm based on foreground confidence prediction, achieving\ntarget-level anomaly scoring. In order to further verify and enhance the\ngeneralization of anomaly detection, we use a 3D rendering method to synthesize\ntwo augmented reality binocular stereo 3D detection datasets which named\nKITTI-AR. KITTI-AR extends upon KITTI by adding 97 new categories, totaling 6k\npairs of stereo images. The KITTI-AR-ExD subset includes 39 common categories\nas extra training data to address the sparse sample distribution issue.\nAdditionally, 58 rare categories form the KITTI-AR-OoD subset, which are not\nused in training to simulate zero-shot scenarios in real-world settings, solely\nfor evaluating 3D anomaly detection. Finally, the performance of the algorithm\nand the dataset is verified in the experiments. (Code and dataset can be\nobtained at https://github.com/xxxx/xxx).", "comment": "under review", "pdf_url": "http://arxiv.org/pdf/2507.09214v1", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09911", "title": "When Less is More: A systematic review of four-day workweek conceptualizations and their effects on organizational performance", "authors": ["Marvin Auf der Landwehr", "Julia Topp", "Michael Neumann"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09911v1", "summary": "Context: Agile IT organizations, which are characterized by self-organization\nand collaborative social interactions, require motivating, efficient and\nflexible work environments to maximize value creation. Compressed work\nschedules such as the four-day workweek have evolved into multiple facets over\nthe last decades and are associated with various benefits for organizations and\ntheir employees. Objective: Our objective in this study is to deepen our\ncomprehension of the impact of compressed work schedules on the operational\nefficacy of IT enterprises, while concurrently developing a comprehensive\nframework delineating the intricacies of compressed work schedules.Method: We\nconducted a systematic review of available conceptualizations related to\nfour-day workweek schedules and elaborate on their organizational and social\neffects. To cover scientific and practice-oriented literature, our review\ncombined a systematic literature review and a web content analysis. Results:\nBased on the generated insights, we derive a meta-framework that matches\nconceptualizations and effects, finally guiding the adoption of compressed work\nschedules based on individual managerial prerequisites and circumstances.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09911v1", "cate": "cs.SE", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2506.00283", "title": "Direct-to-Cell: A First Look into Starlink's Direct Satellite-to-Device Radio Access Network through Crowdsourced Measurements", "authors": ["Jorge Garcia-Cabeza", "Javier Albert-Smet", "Zoraida Frias", "Luis Mendo", "Santiago Andrés Azcoitia", "Eduardo Yraola"], "categories": ["cs.NI", "C.2.1"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      7 pages, 6 figures. Minor correction", "url": "http://arxiv.org/abs/2506.00283v5", "summary": "Low Earth Orbit (LEO) satellite mega-constellations have recently emerged as\na viable access solution for broadband services in underserved areas. In 2024,\nDirect Satellite-to-Device (DS2D) communications, which enable unmodified\nsmartphones to connect directly to spaceborne base stations, entered\nlarge-scale beta testing, with Starlink globally leading deployments. This\npaper presents the first measurement study of commercial DS2D services. Using\ncrowdsourced mobile network data collected in the U.S. between October 2024 and\nApril 2025, our research derives evidence-based insights into the capabilities,\nlimitations, and prospective evolution of DS2D technologies providing\nSupplemental Coverage from Space (SCS) services to expand existing mobile\nnetwork connectivity. We observe a strong correlation between the number of\nsatellites deployed and the expanding extension of observed measurements,\nconcentrated in accessible but poorly covered areas by terrestrial networks,\nsuch as national parks and large low-density counties. The data reveal stable\nphysical-layer value measurement throughout the observation period, with a\nlower median RSRP (24-dB difference) and a higher RSRQ (3 dB difference)\ncompared to terrestrial networks, reflecting the SMS-only usage of the DS2D\nnetwork during this period. Based on SINR measurements, we estimate the\nexpected performance of the announced DS2D mobile data service to be around 4\nMbps per beam in outdoor conditions. We also discuss strategies to expand this\ncapacity up to 12 Mbps in the future, depending on key regulatory decisions\nregarding satellite licenses, spectrum availability, and allowable radiated\npower levels.", "comment": "7 pages, 6 figures. Minor correction", "pdf_url": "http://arxiv.org/pdf/2506.00283v5", "cate": "cs.NI", "date": "2025-05-30", "updated": "2025-07-13"}
{"id": "2507.10207", "title": "Low-Power Wake-Up Signal Design in 3GPP 5G-Advanced Release 19", "authors": ["Sebastian Wagner"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      Submitted to European Wireless 2025", "url": "http://arxiv.org/abs/2507.10207v1", "summary": "The Low-Power Wake-Up Signal (LP-WUS) and Low-Power Synchronization Signal\n(LP-SS), introduced in 3GPP 5G-Advanced Release 19, represent a major step\nforward in enabling power-efficient IoT communications. This paper presents a\ncomprehensive overview of the LP-WUS and LP-SS procedures in the RRC_IDLE and\nRRC_INACTIVE states, and outlines key physical layer design choices. The LP-WUS\nis designed to be detected by a low-power energy detector (ED), allowing the\nmain radio (MR) to remain switched off. This architecture enables power savings\nof up to 80% compared to conventional 5G paging mechanisms.", "comment": "Submitted to European Wireless 2025", "pdf_url": "http://arxiv.org/pdf/2507.10207v1", "cate": "cs.IT", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2503.03088", "title": "AHCPTQ: Accurate and Hardware-Compatible Post-Training Quantization for Segment Anything Model", "authors": ["Wenlun Zhang", "Yunshan Zhong", "Shimpei Ando", "Kentaro Yoshioka"], "categories": ["cs.CV", "cs.AR", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2503.03088v3", "summary": "The Segment Anything Model (SAM) has demonstrated strong versatility across\nvarious visual tasks. However, its large storage requirements and high\ncomputational cost pose challenges for practical deployment. Post-training\nquantization (PTQ) has emerged as an effective strategy for efficient\ndeployment, but we identify two key challenges in SAM that hinder the\neffectiveness of existing PTQ methods: the heavy-tailed and skewed distribution\nof post-GELU activations, and significant inter-channel variation in linear\nprojection activations. To address these challenges, we propose AHCPTQ, an\naccurate and hardware-efficient PTQ method for SAM. AHCPTQ introduces\nhardware-compatible Hybrid Log-Uniform Quantization (HLUQ) to manage post-GELU\nactivations, employing log2 quantization for dense small values and uniform\nquantization for sparse large values to enhance quantization resolution.\nAdditionally, AHCPTQ incorporates Channel-Aware Grouping (CAG) to mitigate\ninter-channel variation by progressively clustering activation channels with\nsimilar distributions, enabling them to share quantization parameters and\nimproving hardware efficiency. The combination of HLUQ and CAG not only\nenhances quantization effectiveness but also ensures compatibility with\nefficient hardware execution. For instance, under the W4A4 configuration on the\nSAM-L model, AHCPTQ achieves 36.6% mAP on instance segmentation with the DINO\ndetector, while achieving a 7.89x speedup and 8.64x energy efficiency over its\nfloating-point counterpart in FPGA implementation.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2503.03088v3", "cate": "cs.CV", "date": "2025-03-05", "updated": "2025-07-12"}
{"id": "2507.08846", "title": "Precomputed Dominant Resource Fairness", "authors": ["Serdar Metin"], "categories": ["cs.GT", "cs.DC", "cs.DS"], "primary_category": "Subjects:       Computer Science and Game Theory (cs.GT)", "pdf_link": null, "comments": "Comments:      9 pages", "url": "http://arxiv.org/abs/2507.08846v1", "summary": "Although resource allocation is a well studied problem in computer science,\nuntil the prevalence of distributed systems, such as computing clouds and data\ncentres, the question had been addressed predominantly for single resource type\nscenarios. At the beginning of the last decade, with the introuction of\nDominant Resource Fairness, the studies of the resource allocation problem has\nfinally extended to the multiple resource type scenarios. Dominant Resource\nFairness is a solution, addressing the problem of fair allocation of multiple\nresource types, among users with heterogeneous demands. Based on Max-min\nFairness, which is a well established algorithm in the literature for\nallocating resources in the single resource type scenarios, Dominant Resource\nFairness generalises the scheme to the multiple resource case. It has a number\nof desirable properties that makes it preferable over alternatives, such as\nSharing Incentive, Envy-Freeness, Pareto Efficiency, and Strategy Proofness,\nand as such, it is widely adopted in distributed systems. In the present study,\nwe revisit the original study, and analyse the structure of the algorithm in\ncloser view, to come up with an alternative algorithm, which approximates the\nDominant Resource Fairness allocation in fewer steps. We name the new algorithm\nPrecomputed Dominant Resource Fairness, after its main working principle.", "comment": "9 pages", "pdf_url": "http://arxiv.org/pdf/2507.08846v1", "cate": "cs.GT", "date": "2025-07-08", "updated": "2025-07-08"}
{"id": "2507.09225", "title": "MetaClimage: A novel database of visual metaphors related to Climate Change, with costs and benefits analysis", "authors": ["Biagio Scalingi", "Chiara Barattieri di San Pietro", "Paolo Canal", "Valentina Bambini"], "categories": ["cs.CL", "cs.CY"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      27 pages, 5 figures", "url": "http://arxiv.org/abs/2507.09225v1", "summary": "Visual metaphors of climate change (e.g., melting glaciers depicted as a\nmelting ice grenade) are regarded as valuable tools for addressing the\ncomplexity of environmental challenges. However, few studies have examined\ntheir impact on communication, also due to scattered availability of material.\nHere, we present a novel database of Metaphors of Climate Change in Images\n(MetaClimage) https://doi.org/10.5281/zenodo.15861012, paired with literal\nimages and enriched with human ratings. For each image, we collected values of\ndifficulty, efficacy, artistic quality, and emotional arousal from human\nrating, as well as number of tags generated by participants to summarize the\nmessage. Semantic and emotion variables were further derived from the tags via\nNatural Language Processing. Visual metaphors were rated as more difficult to\nunderstand, yet more aesthetically pleasant than literal images, but did not\ndiffer in efficacy and arousal. The latter for visual metaphors, however, was\nhigher in participants with higher Need For Cognition. Furthermore, visual\nmetaphors received more tags, often referring to entities not depicted in the\nimage, and elicited words with more positive valence and greater dominance than\nliteral images. These results evidence the greater cognitive load of visual\nmetaphors, which nevertheless might induce positive effects such as deeper\ncognitive elaboration and abstraction compared to literal stimuli. Furthermore,\nwhile they are not deemed as more effective and arousing, visual metaphors seem\nto generate superior aesthetic appreciation and a more positively valenced\nexperience. Overall, this study contributes to understanding the impact of\nvisual metaphors of climate change both by offering a database for future\nresearch and by elucidating a cost-benefit trade-off to take into account when\nshaping environmental communication.", "comment": "27 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.09225v1", "cate": "cs.CL", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09443", "title": "Toward Developing Machine-Learning-Aided Tools for the Thermomechanical Monitoring of Nuclear Reactor Components", "authors": ["Luiz Aldeia Machado", "Victor Coppo Leite", "Elia Merzari", "Arthur Motta", "Roberto Ponciroli", "Lander Ibarra", "Lise Charlot"], "categories": ["cs.LG", "cs.CE"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Preprint - Nureth 21 paper", "url": "http://arxiv.org/abs/2507.09443v1", "summary": "Proactive maintenance strategies, such as Predictive Maintenance (PdM), play\nan important role in the operation of Nuclear Power Plants (NPPs), particularly\ndue to their capacity to reduce offline time by preventing unexpected shutdowns\ncaused by component failures.\n  In this work, we explore the use of a Convolutional Neural Network (CNN)\narchitecture combined with a computational thermomechanical model to calculate\nthe temperature, stress, and strain of a Pressurized Water Reactor (PWR) fuel\nrod during operation. This estimation relies on a limited number of temperature\nmeasurements from the cladding's outer surface. This methodology can\npotentially aid in developing PdM tools for nuclear reactors by enabling\nreal-time monitoring of such systems.\n  The training, validation, and testing datasets were generated through coupled\nsimulations involving BISON, a finite element-based nuclear fuel performance\ncode, and the MOOSE Thermal-Hydraulics Module (MOOSE-THM). We conducted eleven\nsimulations, varying the peak linear heat generation rates. Of these, eight\nwere used for training, two for validation, and one for testing.\n  The CNN was trained for over 1,000 epochs without signs of overfitting,\nachieving highly accurate temperature distribution predictions. These were then\nused in a thermomechanical model to determine the stress and strain\ndistribution within the fuel rod.", "comment": "Preprint - Nureth 21 paper", "pdf_url": "http://arxiv.org/pdf/2507.09443v1", "cate": "cs.LG", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2304.05229", "title": "The Big-O Problem for Max-Plus Automata is Decidable (PSPACE-Complete)", "authors": ["Laure Daviaud", "David Purser", "Marie Tcheng"], "categories": ["cs.FL", "cs.LO"], "primary_category": "Subjects:       Formal Languages and Automata Theory (cs.FL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2304.05229v4", "summary": "We show that the big-O problem for max-plus automata is decidable and\nPSPACE-complete. The big-O (or affine domination) problem asks whether, given\ntwo max-plus automata computing functions f and g, there exists a constant c\nsuch that f < cg+ c. This is a relaxation of the containment problem asking\nwhether f < g, which is undecidable. Our decidability result uses Simon's\nforest factorisation theorem, and relies on detecting specific elements, that\nwe call witnesses, in a finite semigroup closed under two special operations:\nstabilisation and flattening.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2304.05229v4", "cate": "cs.FL", "date": "2023-04-11", "updated": "2025-07-13"}
{"id": "2504.17921", "title": "Avoiding Leakage Poisoning: Concept Interventions Under Distribution Shifts", "authors": ["Mateo Espinosa Zarlenga", "Gabriele Dominici", "Pietro Barbiero", "Zohreh Shams", "Mateja Jamnik"], "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.HC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Presented at the Forty-Second International Conference on Machine Learning (ICML 2025)", "url": "http://arxiv.org/abs/2504.17921v2", "summary": "In this paper, we investigate how concept-based models (CMs) respond to\nout-of-distribution (OOD) inputs. CMs are interpretable neural architectures\nthat first predict a set of high-level concepts (e.g., stripes, black) and then\npredict a task label from those concepts. In particular, we study the impact of\nconcept interventions (i.e., operations where a human expert corrects a CM's\nmispredicted concepts at test time) on CMs' task predictions when inputs are\nOOD. Our analysis reveals a weakness in current state-of-the-art CMs, which we\nterm leakage poisoning, that prevents them from properly improving their\naccuracy when intervened on for OOD inputs. To address this, we introduce\nMixCEM, a new CM that learns to dynamically exploit leaked information missing\nfrom its concepts only when this information is in-distribution. Our results\nacross tasks with and without complete sets of concept annotations demonstrate\nthat MixCEMs outperform strong baselines by significantly improving their\naccuracy for both in-distribution and OOD samples in the presence and absence\nof concept interventions.", "comment": "Presented at the Forty-Second International Conference on Machine\n  Learning (ICML 2025)", "pdf_url": "http://arxiv.org/pdf/2504.17921v2", "cate": "cs.LG", "date": "2025-04-24", "updated": "2025-07-12"}
{"id": "2507.09294", "title": "Geo-RepNet: Geometry-Aware Representation Learning for Surgical Phase Recognition in Endoscopic Submucosal Dissection", "authors": ["Rui Tang", "Haochen Yin", "Guankun Wang", "Long Bai", "An Wang", "Huxin Gao", "Jiazheng Wang", "Hongliang Ren"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      IEEE ICIA 2025", "url": "http://arxiv.org/abs/2507.09294v1", "summary": "Surgical phase recognition plays a critical role in developing intelligent\nassistance systems for minimally invasive procedures such as Endoscopic\nSubmucosal Dissection (ESD). However, the high visual similarity across\ndifferent phases and the lack of structural cues in RGB images pose significant\nchallenges. Depth information offers valuable geometric cues that can\ncomplement appearance features by providing insights into spatial relationships\nand anatomical structures. In this paper, we pioneer the use of depth\ninformation for surgical phase recognition and propose Geo-RepNet, a\ngeometry-aware convolutional framework that integrates RGB image and depth\ninformation to enhance recognition performance in complex surgical scenes.\nBuilt upon a re-parameterizable RepVGG backbone, Geo-RepNet incorporates the\nDepth-Guided Geometric Prior Generation (DGPG) module that extracts geometry\npriors from raw depth maps, and the Geometry-Enhanced Multi-scale Attention\n(GEMA) to inject spatial guidance through geometry-aware cross-attention and\nefficient multi-scale aggregation. To evaluate the effectiveness of our\napproach, we construct a nine-phase ESD dataset with dense frame-level\nannotations from real-world ESD videos. Extensive experiments on the proposed\ndataset demonstrate that Geo-RepNet achieves state-of-the-art performance while\nmaintaining robustness and high computational efficiency under complex and\nlow-texture surgical environments.", "comment": "IEEE ICIA 2025", "pdf_url": "http://arxiv.org/pdf/2507.09294v1", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.03147", "title": "DeepGesture: A conversational gesture synthesis system based on emotions and semantics", "authors": ["Thanh Hoang-Minh"], "categories": ["cs.HC", "cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Project page: this https URL", "url": "http://arxiv.org/abs/2507.03147v2", "summary": "Along with the explosion of large language models, improvements in speech\nsynthesis, advancements in hardware, and the evolution of computer graphics,\nthe current bottleneck in creating digital humans lies in generating character\nmovements that correspond naturally to text or speech inputs.\n  In this work, we present DeepGesture, a diffusion-based gesture synthesis\nframework for generating expressive co-speech gestures conditioned on\nmultimodal signals - text, speech, emotion, and seed motion. Built upon the\nDiffuseStyleGesture model, DeepGesture introduces novel architectural\nenhancements that improve semantic alignment and emotional expressiveness in\ngenerated gestures. Specifically, we integrate fast text transcriptions as\nsemantic conditioning and implement emotion-guided classifier-free diffusion to\nsupport controllable gesture generation across affective states. To visualize\nresults, we implement a full rendering pipeline in Unity based on BVH output\nfrom the model. Evaluation on the ZeroEGGS dataset shows that DeepGesture\nproduces gestures with improved human-likeness and contextual appropriateness.\nOur system supports interpolation between emotional states and demonstrates\ngeneralization to out-of-distribution speech, including synthetic voices -\nmarking a step forward toward fully multimodal, emotionally aware digital\nhumans.\n  Project page: https://deepgesture.github.io", "comment": "Project page: https://deepgesture.github.io", "pdf_url": "http://arxiv.org/pdf/2507.03147v2", "cate": "cs.HC", "date": "2025-07-03", "updated": "2025-07-14"}
{"id": "2507.10281", "title": "Toward Real-World Table Agents: Capabilities, Workflows, and Design Principles for LLM-based Table Intelligence", "authors": ["Jiaming Tian", "Liyao Li", "Wentao Ye", "Haobo Wang", "Lingxin Wang", "Lihua Yu", "Zujie Ren", "Gang Chen", "Junbo Zhao"], "categories": ["cs.AI", "cs.DB"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10281v1", "summary": "Tables are fundamental in domains such as finance, healthcare, and public\nadministration, yet real-world table tasks often involve noise, structural\nheterogeneity, and semantic complexity--issues underexplored in existing\nresearch that primarily targets clean academic datasets. This survey focuses on\nLLM-based Table Agents, which aim to automate table-centric workflows by\nintegrating preprocessing, reasoning, and domain adaptation. We define five\ncore competencies--C1: Table Structure Understanding, C2: Table and Query\nSemantic Understanding, C3: Table Retrieval and Compression, C4: Executable\nReasoning with Traceability, and C5: Cross-Domain Generalization--to analyze\nand compare current approaches. In addition, a detailed examination of the\nText-to-SQL Agent reveals a performance gap between academic benchmarks and\nreal-world scenarios, especially for open-source models. Finally, we provide\nactionable insights to improve the robustness, generalization, and efficiency\nof LLM-based Table Agents in practical settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10281v1", "cate": "cs.AI", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09216", "title": "360-Degree Full-view Image Segmentation by Spherical Convolution compatible with Large-scale Planar Pre-trained Models", "authors": ["Jingguo Liu", "Han Yu", "Shigang Li", "Jianfeng Li"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      This paper is accecpted by ICMEW 2025", "url": "http://arxiv.org/abs/2507.09216v1", "summary": "Due to the current lack of large-scale datasets at the million-scale level,\ntasks involving panoramic images predominantly rely on existing two-dimensional\npre-trained image benchmark models as backbone networks. However, these\nnetworks are not equipped to recognize the distortions and discontinuities\ninherent in panoramic images, which adversely affects their performance in such\ntasks. In this paper, we introduce a novel spherical sampling method for\npanoramic images that enables the direct utilization of existing pre-trained\nmodels developed for two-dimensional images. Our method employs spherical\ndiscrete sampling based on the weights of the pre-trained models, effectively\nmitigating distortions while achieving favorable initial training values.\nAdditionally, we apply the proposed sampling method to panoramic image\nsegmentation, utilizing features obtained from the spherical model as masks for\nspecific channel attentions, which yields commendable results on commonly used\nindoor datasets, Stanford2D3D.", "comment": "This paper is accecpted by ICMEW 2025", "pdf_url": "http://arxiv.org/pdf/2507.09216v1", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.10054", "title": "Explicit Vulnerability Generation with LLMs: An Investigation Beyond Adversarial Attacks", "authors": ["Emir Bosnak", "Sahand Moslemi", "Mayasah Lami", "Anil Koyuncu"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Accepted to ICSME 2025", "url": "http://arxiv.org/abs/2507.10054v1", "summary": "Large Language Models (LLMs) are increasingly used as code assistants, yet\ntheir behavior when explicitly asked to generate insecure code remains poorly\nunderstood. While prior research has focused on unintended vulnerabilities or\nadversarial prompting techniques, this study examines a more direct threat\nscenario: open-source LLMs generating vulnerable code when prompted either\ndirectly or indirectly. We propose a dual experimental design: (1) Dynamic\nPrompting, which systematically varies vulnerability type, user persona, and\ndirectness across structured templates; and (2) Reverse Prompting, which\nderives prompts from real vulnerable code samples to assess vulnerability\nreproduction accuracy. We evaluate three open-source 7B-parameter models\n(Qwen2, Mistral, and Gemma) using ESBMC static analysis to assess both the\npresence of vulnerabilities and the correctness of the generated vulnerability\ntype. Results show all models frequently produce vulnerable outputs, with Qwen2\nachieving highest correctness rates. User persona significantly affects\nsuccess, where student personas achieved higher vulnerability rates than\nprofessional roles, while direct prompts were marginally more effective.\nVulnerability reproduction followed an inverted-U pattern with cyclomatic\ncomplexity, peaking at moderate ranges. Our findings expose limitations of\nsafety mechanisms in open-source models, particularly for seemingly benign\neducational requests.", "comment": "Accepted to ICSME 2025", "pdf_url": "http://arxiv.org/pdf/2507.10054v1", "cate": "cs.SE", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2506.09397", "title": "SLED: A Speculative LLM Decoding Framework for Efficient Edge Serving", "authors": ["Xiangchen Li", "Dimitrios Spatharakis", "Saeid Ghafouri", "Jiakun Fan", "Hans Vandierendonck", "Deepu John", "Bo Ji", "Dimitrios Nikolopoulos"], "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.NI", "68T07, 68M14", "I.2.6; C.2.4; C.1.4"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      6 pages, 6 figures, 2 tables", "url": "http://arxiv.org/abs/2506.09397v4", "summary": "The growing gap between the increasing complexity of large language models\n(LLMs) and the limited computational budgets of edge devices poses a key\nchallenge for efficient on-device inference, despite gradual improvements in\nhardware capabilities. Existing strategies, such as aggressive quantization,\npruning, or remote inference, trade accuracy for efficiency or lead to\nsubstantial cost burdens. This position paper introduces a new framework that\nleverages speculative decoding, previously viewed primarily as a decoding\nacceleration technique for autoregressive generation of LLMs, as a promising\napproach specifically adapted for edge computing by orchestrating computation\nacross heterogeneous devices. We propose \\acronym, a framework that allows\nlightweight edge devices to draft multiple candidate tokens locally using\ndiverse draft models, while a single, shared edge server verifies the tokens\nutilizing a more precise target model. To further increase the efficiency of\nverification, the edge server batch the diverse verification requests from\ndevices. This approach supports device heterogeneity and reduces server-side\nmemory footprint by sharing the same upstream target model across multiple\ndevices. Our initial experiments with Jetson Orin Nano, Raspberry Pi 4B/5, and\nan edge server equipped with 4 Nvidia A100 GPUs indicate substantial benefits:\n2.2 more system throughput, 2.8 more system capacity, and better cost\nefficiency, all without sacrificing model accuracy.", "comment": "6 pages, 6 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2506.09397v4", "cate": "cs.DC", "date": "2025-06-11", "updated": "2025-07-13"}
{"id": "2507.10234", "title": "Dimensionality increase for error correction in the interaction between information space and the physical world", "authors": ["Tatyana Barron"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10234v1", "summary": "The evolution of human intelligence led to the huge amount of data in the\ninformation space. Accessing and processing this data helps in finding\nsolutions to applied problems based on finite-dimensional models. We argue,\nthat formally, such a mathematical model can be embedded into a\nhigher-dimensional model inside of which a desired solution will exist.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10234v1", "cate": "cs.IT", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2504.03529", "title": "PHOENIX: Pauli-Based High-Level Optimization Engine for Instruction Execution on NISQ Devices", "authors": ["Zhaohui Yang", "Dawei Ding", "Chenghong Zhu", "Jianxin Chen", "Yuan Xie"], "categories": ["quant-ph", "cs.AR", "cs.PL"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      6 pages, 8 figures; Open-sourced on GitHub; A conference paper at DAC 2025", "url": "http://arxiv.org/abs/2504.03529v5", "summary": "Variational quantum algorithms (VQA) based on Hamiltonian simulation\nrepresent a specialized class of quantum programs well-suited for near-term\nquantum computing applications due to its modest resource requirements in terms\nof qubits and circuit depth. Unlike the conventional single-qubit (1Q) and\ntwo-qubit (2Q) gate sequence representation, Hamiltonian simulation programs\nare essentially composed of disciplined subroutines known as Pauli\nexponentiations (Pauli strings with coefficients) that are variably arranged.\nTo capitalize on these distinct program features, this study introduces\nPHOENIX, a highly effective compilation framework that primarily operates at\nthe high-level Pauli-based intermediate representation (IR) for generic\nHamiltonian simulation programs. PHOENIX exploits global program optimization\nopportunities to the greatest extent, compared to existing SOTA methods despite\nsome of them also utilizing similar IRs. Experimental results demonstrate that\nPHOENIX outperforms SOTA VQA compilers across diverse program categories,\nbackend ISAs, and hardware topologies.", "comment": "6 pages, 8 figures; Open-sourced on GitHub; A conference paper at DAC\n  2025", "pdf_url": "http://arxiv.org/pdf/2504.03529v5", "cate": "quant-ph", "date": "2025-04-04", "updated": "2025-07-13"}
{"id": "2507.08868", "title": "A Survey on Bilateral Multi-Round Cloud-SLA Negotiation Strategies", "authors": ["Benedikt Pittl", "Werner Mach", "Erich Schikuta"], "categories": ["cs.GT", "cs.DC", "91B26", "J.1; J.4"], "primary_category": "Subjects:       Computer Science and Game Theory (cs.GT)", "pdf_link": null, "comments": "Comments:      Preprint", "url": "http://arxiv.org/abs/2507.08868v1", "summary": "Today, static cloud markets where consumers purchase services directly from\nproviders are dominating. Thus, consumers neither negotiate the price nor the\ncharacteristics of the service. In recent years, providers have adopted more\ndynamic trading mechanisms, as e.g. Amazon's EC2 platform shows: In addition to\nthe reservation marketspace and the on-demand marketspace, Amazon offers a spot\nmarketspace where consumers can bid for virtual machines. This spot marketspace\nwas extended with spot blocks, and recently Amazon reworked the bidding\noptions. In addition, other cloud providers, such as Virtustream, adopt dynamic\ntrading mechanisms. The scientific community envisions autonomous multi-round\nnegotiations for realizing future cloud marketspaces. Consequently, consumers\nand providers exchange offers and counteroffers to reach an agreement. This\nhelps providers increase the utilization of their datacenters, while consumers\ncan purchase highly customized cloud services.\n  In the paper at hand, we present a survey on multi-round bilateral\nnegotiation strategies for trading cloud resources. Thus, we analyzed\npeer-reviewed articles in order to identify trends, gaps, similarities, and the\nscope of such negotiation strategies. In addition, we surveyed the formalism\nthat the scientific community uses to describe such strategies. Based on these\nfindings, we derived recommendations for creating and documenting bilateral\nmulti-round negotiation strategies to foster their implementation in the\nindustry.", "comment": "Preprint", "pdf_url": "http://arxiv.org/pdf/2507.08868v1", "cate": "cs.GT", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.09382", "title": "Fair CCA for Fair Representation Learning: An ADNI Study", "authors": ["Bojian Hou", "Zhanliang Wang", "Zhuoping Zhou", "Boning Tong", "Zexuan Wang", "Jingxuan Bao", "Duy Duong-Tran", "Qi Long", "Li Shen"], "categories": ["cs.LG", "cs.AI", "cs.CY"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09382v1", "summary": "Canonical correlation analysis (CCA) is a technique for finding correlations\nbetween different data modalities and learning low-dimensional representations.\nAs fairness becomes crucial in machine learning, fair CCA has gained attention.\nHowever, previous approaches often overlook the impact on downstream\nclassification tasks, limiting applicability. We propose a novel fair CCA\nmethod for fair representation learning, ensuring the projected features are\nindependent of sensitive attributes, thus enhancing fairness without\ncompromising accuracy. We validate our method on synthetic data and real-world\ndata from the Alzheimer's Disease Neuroimaging Initiative (ADNI), demonstrating\nits ability to maintain high correlation analysis performance while improving\nfairness in classification tasks. Our work enables fair machine learning in\nneuroimaging studies where unbiased analysis is essential.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09382v1", "cate": "cs.LG", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09716", "title": "When the Weak Becomes Strong: Effective Observables via Time-Symmetric Quantum Selection", "authors": ["Mirco A. Mannucci"], "categories": ["quant-ph", "cs.CE"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      11 pages, 2 figures", "url": "http://arxiv.org/abs/2507.09716v1", "summary": "We investigate the sequential composition of weak values in the framework of\ntime-symmetric quantum mechanics. Specifically, we consider a forward'' weak\nmeasurement from a preselected state $\\ket{\\psi}$ to a post-selected state\n$\\ket{\\phi}$, followed by a reverse'' weak measurement. We show that the\nproduct of these two weak values corresponds to the normalized expectation\nvalue of a strong, state-conditioned observable $B = A P_\\psi A$, where $P_\\psi\n= \\ket{\\psi}\\bra{\\psi}$ is the projector onto the preselected state. Analyzing\nthe structure of $B$, we demonstrate how it encodes interference information,\nparticularly when $\\ket{\\psi}$ is a superposition rather than an eigenstate of\n$A$. This formulation extends naturally to mixed states by replacing $P_\\psi$\nwith a generic density matrix $\\rho$, linking the construction to the formalism\nof generalized quantum measurements. We illustrate practical applications in\nquantum information, including state-specific error witnessing in quantum\ncomputing, and show how the phase of a weak value can be inferred via strong\nmeasurements in the pure-state case.", "comment": "11 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.09716v1", "cate": "quant-ph", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2409.01221", "title": "Completing the picture for the Skolem Problem on order-4 linear recurrence sequences", "authors": ["Piotr Bacik"], "categories": ["cs.FL", "math.NT"], "primary_category": "Subjects:       Formal Languages and Automata Theory (cs.FL)", "pdf_link": null, "comments": "Comments:      10 pages", "url": "http://arxiv.org/abs/2409.01221v3", "summary": "For almost a century, the decidability of the Skolem Problem - that is, the\nproblem of finding whether a given linear recurrence sequence (LRS) has a zero\nterm - has remained open. A breakthrough in the 1980s established that the\nSkolem Problem is indeed decidable for algebraic LRS of order at most 3, and\nreal algebraic LRS of order at most 4. However, for general algebraic LRS of\norder 4 the question of decidability has remained open. Our main contribution\nin this paper is to prove decidability for this last case, i.e. we show that\nthe Skolem Problem is decidable for all algebraic LRS of order at most 4.", "comment": "10 pages", "pdf_url": "http://arxiv.org/pdf/2409.01221v3", "cate": "cs.FL", "date": "2024-09-02", "updated": "2025-07-11"}
{"id": "2506.04462", "title": "Watermarking Degrades Alignment in Language Models: Analysis and Mitigation", "authors": ["Apurv Verma", "NhatHai Phan", "Shubhendu Trivedi"], "categories": ["cs.CL", "cs.CR", "cs.LG", "I.2.7"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Published at the 1st Workshop on GenAI Watermarking (ICLR 2025). Code: this https URL", "url": "http://arxiv.org/abs/2506.04462v3", "summary": "Watermarking techniques for large language models (LLMs) can significantly\nimpact output quality, yet their effects on truthfulness, safety, and\nhelpfulness remain critically underexamined. This paper presents a systematic\nanalysis of how two popular watermarking approaches-Gumbel and KGW-affect these\ncore alignment properties across four aligned LLMs. Our experiments reveal two\ndistinct degradation patterns: guard attenuation, where enhanced helpfulness\nundermines model safety, and guard amplification, where excessive caution\nreduces model helpfulness. These patterns emerge from watermark-induced shifts\nin token distribution, surfacing the fundamental tension that exists between\nalignment objectives.\n  To mitigate these degradations, we propose Alignment Resampling (AR), an\ninference-time sampling method that uses an external reward model to restore\nalignment. We establish a theoretical lower bound on the improvement in\nexpected reward score as the sample size is increased and empirically\ndemonstrate that sampling just 2-4 watermarked generations effectively recovers\nor surpasses baseline (unwatermarked) alignment scores. To overcome the limited\nresponse diversity of standard Gumbel watermarking, our modified implementation\nsacrifices strict distortion-freeness while maintaining robust detectability,\nensuring compatibility with AR. Experimental results confirm that AR\nsuccessfully recovers baseline alignment in both watermarking approaches, while\nmaintaining strong watermark detectability. This work reveals the critical\nbalance between watermark strength and model alignment, providing a simple\ninference-time solution to responsibly deploy watermarked LLMs in practice.", "comment": "Published at the 1st Workshop on GenAI Watermarking (ICLR 2025).\n  Code: https://github.com/dapurv5/alignmark", "pdf_url": "http://arxiv.org/pdf/2506.04462v3", "cate": "cs.CL", "date": "2025-06-04", "updated": "2025-07-12"}
{"id": "2507.09420", "title": "Domain Adaptation and Multi-view Attention for Learnable Landmark Tracking with Sparse Data", "authors": ["Timothy Chase Jr", "Karthik Dantu"], "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Presented at the RSS Space Robotics Workshop 2025. Poster available online at this https URL", "url": "http://arxiv.org/abs/2507.09420v1", "summary": "The detection and tracking of celestial surface terrain features are crucial\nfor autonomous spaceflight applications, including Terrain Relative Navigation\n(TRN), Entry, Descent, and Landing (EDL), hazard analysis, and scientific data\ncollection. Traditional photoclinometry-based pipelines often rely on extensive\na priori imaging and offline processing, constrained by the computational\nlimitations of radiation-hardened systems. While historically effective, these\napproaches typically increase mission costs and duration, operate at low\nprocessing rates, and have limited generalization. Recently, learning-based\ncomputer vision has gained popularity to enhance spacecraft autonomy and\novercome these limitations. While promising, emerging techniques frequently\nimpose computational demands exceeding the capabilities of typical spacecraft\nhardware for real-time operation and are further challenged by the scarcity of\nlabeled training data for diverse extraterrestrial environments. In this work,\nwe present novel formulations for in-situ landmark tracking via detection and\ndescription. We utilize lightweight, computationally efficient neural network\narchitectures designed for real-time execution on current-generation spacecraft\nflight processors. For landmark detection, we propose improved domain\nadaptation methods that enable the identification of celestial terrain features\nwith distinct, cheaply acquired training data. Concurrently, for landmark\ndescription, we introduce a novel attention alignment formulation that learns\nrobust feature representations that maintain correspondence despite significant\nlandmark viewpoint variations. Together, these contributions form a unified\nsystem for landmark tracking that demonstrates superior performance compared to\nexisting state-of-the-art techniques.", "comment": "Presented at the RSS Space Robotics Workshop 2025. Poster available\n  online at https://tjchase34.github.io/assets/pdfs/rss_poster.pdf", "pdf_url": "http://arxiv.org/pdf/2507.09420v1", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.04278", "title": "EMER-Ranker: Learning to Rank Emotion Descriptions in the Absence of Ground Truth", "authors": ["Zheng Lian", "Licai Sun", "Haoyu Chen", "Zebang Cheng", "Fan Zhang", "Ziyu Jia", "Ziyang Ma", "Fei Ma", "Xiaojiang Peng", "Jianhua Tao"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.04278v3", "summary": "With the recent success of large language models, Explainable Multimodal\nEmotion Recognition (EMER), also known as Descriptive MER (DMER), has attracted\ngrowing attention from researchers. Unlike traditional discriminative methods\nthat rely on predefined emotion taxonomies, EMER aims to describe a person's\nemotional state using free-form natural language, thereby enabling fine-grained\nand interpretable emotion representations. However, this free-form prediction\nparadigm introduces significant challenges in evaluation. Existing approaches\neither depend on ground-truth descriptions, which require extensive manual\nannotations and often fail to capture the full complexity of human emotions, or\nsimplify the evaluation task by shifting focus from assessing descriptions to\nevaluating emotion labels. However, this simplification overlooks critical\naspects such as emotional temporal dynamics, intensity, and uncertainty. To\naddress these limitations, we propose EMER-Ranker, a novel evaluation strategy\nthat reformulates the traditional ``prediction-ground truth'' comparison into\nthe ``prediction-prediction'' comparison, eliminating the need for ground-truth\ndescriptions. We then apply the Bradley-Terry algorithm to convert pairwise\ncomparison outcomes into model-level rankings. Additionally, we explore the\npotential for automatic preference prediction and introduce EMER-Preference,\nthe first preference dataset specifically designed for human emotions. Our work\nadvances the field of EMER and lays the foundation for more intelligent\nhuman-computer interaction systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.04278v3", "cate": "cs.HC", "date": "2025-07-06", "updated": "2025-07-14"}
{"id": "2507.10397", "title": "Instance space analysis of the capacitated vehicle routing problem", "authors": ["Alessandra M. M. M. Gouvêa", "Nuno Paulos", "Eduardo Uchoa e Mariá C. V. Nascimento"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10397v1", "summary": "This paper seeks to advance CVRP research by addressing the challenge of\nunderstanding the nuanced relationships between instance characteristics and\nmetaheuristic (MH) performance. We present Instance Space Analysis (ISA) as a\nvaluable tool that allows for a new perspective on the field. By combining the\nISA methodology with a dataset from the DIMACS 12th Implementation Challenge on\nVehicle Routing, our research enabled the identification of 23 relevant\ninstance characteristics. Our use of the PRELIM, SIFTED, and PILOT stages,\nwhich employ dimensionality reduction and machine learning methods, allowed us\nto create a two-dimensional projection of the instance space to understand how\nthe structure of instances affect the behavior of MHs. A key contribution of\nour work is that we provide a projection matrix, which makes it straightforward\nto incorporate new instances into this analysis and allows for a new method for\ninstance analysis in the CVRP field.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10397v1", "cate": "cs.AI", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09217", "title": "Online Long-term Point Tracking in the Foundation Model Era", "authors": ["Görkay Aydemir"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      arXiv admin note: substantial text overlap with arXiv:2501.18487", "url": "http://arxiv.org/abs/2507.09217v1", "summary": "Point tracking aims to identify the same physical point across video frames\nand serves as a geometry-aware representation of motion. This representation\nsupports a wide range of applications, from robotics to augmented reality, by\nenabling accurate modeling of dynamic environments. Most existing long-term\ntracking approaches operate in an offline setting, where future frames are\navailable to refine predictions and recover from occlusions. However,\nreal-world scenarios often demand online predictions: the model must operate\ncausally, using only current and past frames. This constraint is critical in\nstreaming video and embodied AI, where decisions must be made immediately based\non past observations. Under such constraints, viewpoint invariance becomes\nessential. Visual foundation models, trained on diverse large-scale datasets,\noffer the potential for robust geometric representations. While they lack\ntemporal reasoning on their own, they can be integrated into tracking pipelines\nto enrich spatial features. In this thesis, we address the problem of long-term\npoint tracking in an online setting, where frames are processed sequentially\nwithout access to future information or sliding windows. We begin by evaluating\nthe suitability of visual foundation models for this task and find that they\ncan serve as useful initializations and be integrated into tracking pipelines.\nHowever, to enable long-term tracking in an online setting, a dedicated design\nis still required. In particular, maintaining coherence over time in this\ncausal regime requires memory to propagate appearance and context across\nframes. To address this, we introduce Track-On, a transformer-based model that\ntreats each tracked point as a query and processes video frames one at a time.\nTrack-On sets a new state of the art across seven public benchmarks,\ndemonstrating the feasibility of long-term tracking without future access.", "comment": "arXiv admin note: substantial text overlap with arXiv:2501.18487", "pdf_url": "http://arxiv.org/pdf/2507.09217v1", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.10062", "title": "LLMShot: Reducing snapshot testing maintenance via LLMs", "authors": ["Ergün Batuhan Kaynak", "Mayasah Lami", "Sahand Moslemi", "Anil Koyuncu"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Accepted to ICSME 2025", "url": "http://arxiv.org/abs/2507.10062v1", "summary": "Snapshot testing has emerged as a critical technique for UI validation in\nmodern software development, yet it suffers from substantial maintenance\noverhead due to frequent UI changes causing test failures that require manual\ninspection to distinguish between genuine regressions and intentional design\nchanges. This manual triage process becomes increasingly burdensome as\napplications evolve, creating a need for automated analysis solutions. This\npaper introduces LLMShot, a novel framework that leverages vision-based Large\nLanguage Models to automatically analyze snapshot test failures through\nhierarchical classification of UI changes. To evaluate LLMShot's effectiveness,\nwe developed a comprehensive dataset using a feature-rich iOS application with\nconfigurable feature flags, creating realistic scenarios that produce authentic\nsnapshot differences representative of real development workflows. Our\nevaluation using Gemma3 models demonstrates strong classification performance,\nwith the 12B variant achieving over 84% recall in identifying failure root\ncauses while the 4B model offers practical deployment advantages with\nacceptable performance for continuous integration environments. However, our\nexploration of selective ignore mechanisms revealed significant limitations in\ncurrent prompting-based approaches for controllable visual reasoning. LLMShot\nrepresents the first automated approach to semantic snapshot test analysis,\noffering developers structured insights that can substantially reduce manual\ntriage effort and advance toward more intelligent UI testing paradigms.", "comment": "Accepted to ICSME 2025", "pdf_url": "http://arxiv.org/pdf/2507.10062v1", "cate": "cs.SE", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10417", "title": "A Matrix Completion Approach for the Construction of MDP Convolutional Codes", "authors": ["Sakshi Dang", "Julia Lieb", "Okko Makkonen", "Pedro Soto", "Alex Sprintson"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10417v1", "summary": "Maximum Distance Profile (MDP) convolutional codes are an important class of\nchannel codes due to their maximal delay-constrained error correction\ncapabilities. The design of MDP codes has attracted significant attention from\nthe research community. However, only limited attention was given to addressing\nthe complexity of encoding and decoding operations. This paper aims to reduce\nencoding complexity by constructing partial unit-memory MDP codes with\nstructured and sparse generator matrices. In particular, we present a matrix\ncompletion framework that extends a structured superregular matrix (e.g.,\nCauchy) over a small field to a sparse sliding generator matrix of an MDP code.\nWe show that the proposed construction can reduce the encoding complexity\ncompared to the current state-of-the-art MDP code designs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10417v1", "cate": "cs.IT", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.07223", "title": "Compute Can't Handle the Truth: Why Communication Tax Prioritizes Memory and Interconnects in Modern AI Infrastructure", "authors": ["Myoungsoo Jung"], "categories": ["cs.DC", "cs.AR", "B.4.3; C.0; C.2.1; C.2.2"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07223v2", "summary": "Modern AI workloads such as large language models (LLMs) and\nretrieval-augmented generation (RAG) impose severe demands on memory,\ncommunication bandwidth, and resource flexibility. Traditional GPU-centric\narchitectures struggle to scale due to growing inter-GPU communication\noverheads. This report introduces key AI concepts and explains how Transformers\nrevolutionized data representation in LLMs. We analyze large-scale AI hardware\nand data center designs, identifying scalability bottlenecks in hierarchical\nsystems. To address these, we propose a modular data center architecture based\non Compute Express Link (CXL) that enables disaggregated scaling of memory,\ncompute, and accelerators. We further explore accelerator-optimized\ninterconnects-collectively termed XLink (e.g., UALink, NVLink, NVLink\nFusion)-and introduce a hybrid CXL-over-XLink design to reduce long-distance\ndata transfers while preserving memory coherence. We also propose a\nhierarchical memory model that combines local and pooled memory, and evaluate\nlightweight CXL implementations, HBM, and silicon photonics for efficient\nscaling. Our evaluations demonstrate improved scalability, throughput, and\nflexibility in AI infrastructure.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07223v2", "cate": "cs.DC", "date": "2025-07-09", "updated": "2025-07-13"}
{"id": "2507.09019", "title": "On Evaluating Performance of LLM Inference Serving Systems", "authors": ["Amey Agrawal", "Nitin Kedia", "Anmol Agarwal", "Jayashree Mohan", "Nipun Kwatra", "Souvik Kundu", "Ramachandran Ramjee", "Alexey Tumanov"], "categories": ["cs.LG", "cs.AI", "cs.DC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09019v1", "summary": "The rapid evolution of Large Language Model (LLM) inference systems has\nyielded significant efficiency improvements. However, our systematic analysis\nreveals that current evaluation methodologies frequently exhibit fundamental\nflaws, often manifesting as common evaluation anti-patterns that obscure true\nperformance characteristics and impede scientific progress. Through a\ncomprehensive examination of recent systems, we identify recurring\nanti-patterns across three key dimensions: Baseline Fairness, Evaluation Setup,\nand Metric Design. These anti-patterns are uniquely problematic for LLM\ninference due to its dual-phase nature combining distinct prefill and decode\noperations, its handling of highly heterogeneous workloads, and its strict\ntemporal requirements for interactive use. We demonstrate how common\nanti-patterns -- such as inadequate baseline comparisons that conflate\nengineering effort with algorithmic novelty, workload selections that fail to\nrepresent production scenarios, and metric normalizations that hide substantial\nperformance variability like generation stalls-lead to misleading conclusions.\nTo address these challenges, we provide a comprehensive checklist derived from\nour analysis, establishing a framework for recognizing and avoiding these\nanti-patterns in favor of robust LLM inference evaluation. To demonstrate the\npractical application of our framework, we present a case study analyzing\nspeculative decoding, a technique whose bursty, non-uniform token generation is\neasily misinterpreted when evaluated using approaches characteristic of these\nanti-patterns. Our work establishes a rigorous foundation for evaluation\nmethodology, enabling meaningful comparisons, ensuring reproducible results,\nand ultimately accelerating genuine progress in LLM inference systems by moving\nbeyond common anti-patterns to align evaluation with real-world requirements.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09019v1", "cate": "cs.LG", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09972", "title": "A New Incentive Model For Content Trust", "authors": ["Lucas Barbosa", "Sam Kirshner", "Rob Kopel", "Eric Tze Kuan Lim", "Tom Pagram"], "categories": ["cs.GT", "cs.CY", "econ.GN", "q-fin.EC"], "primary_category": "Subjects:       Computer Science and Game Theory (cs.GT)", "pdf_link": null, "comments": "Comments:      20 pages, 6 figures and 2 tables", "url": "http://arxiv.org/abs/2507.09972v1", "summary": "This paper outlines an incentive-driven and decentralized approach to\nverifying the veracity of digital content at scale. Widespread misinformation,\nan explosion in AI-generated content and reduced reliance on traditional news\nsources demands a new approach for content authenticity and truth-seeking that\nis fit for a modern, digital world. By using smart contracts and digital\nidentity to incorporate 'trust' into the reward function for published content,\nnot just engagement, we believe that it could be possible to foster a\nself-propelling paradigm shift to combat misinformation through a\ncommunity-based governance model. The approach described in this paper requires\nthat content creators stake financial collateral on factual claims for an\nimpartial jury to vet with a financial reward for contribution. We hypothesize\nthat with the right financial and social incentive model users will be\nmotivated to participate in crowdsourced fact-checking and content creators\nwill place more care in their attestations. This is an exploratory paper and\nthere are a number of open issues and questions that warrant further analysis\nand exploration.", "comment": "20 pages, 6 figures and 2 tables", "pdf_url": "http://arxiv.org/pdf/2507.09972v1", "cate": "cs.GT", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10149", "title": "A Coincidence of Wants Mechanism for Swap Trade Execution in Decentralized Exchanges", "authors": ["Abhimanyu Nag", "Madhur Prabhakar", "Tanuj Behl"], "categories": ["cs.GT", "cs.CE", "q-fin.TR"], "primary_category": "Subjects:       Computer Science and Game Theory (cs.GT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10149v1", "summary": "We propose a mathematically rigorous framework for identifying and completing\nCoincidence of Wants (CoW) cycles in decentralized exchange (DEX) aggregators.\nUnlike existing auction based systems such as CoWSwap, our approach introduces\nan asset matrix formulation that not only verifies feasibility using oracle\nprices and formal conservation laws but also completes partial CoW cycles of\nswap orders that are discovered using graph traversal and are settled using\nimbalance correction. We define bridging orders and show that the resulting\nexecution is slippage free and capital preserving for LPs. Applied to real\nworld Arbitrum swap data, our algorithm demonstrates efficient discovery of CoW\ncycles and supports the insertion of synthetic orders for atomic cycle closure.\nThis work can be thought of as the detailing of a potential delta-neutral\nstrategy by liquidity providing market makers: a structured CoW cycle\nexecution.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10149v1", "cate": "cs.GT", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2311.15675", "title": "The Complexity of Second-order HyperLTL", "authors": ["Hadar Frenkel", "Gaëtan Regaud", "Martin Zimmermann"], "categories": ["cs.LO", "cs.FL"], "primary_category": "Subjects:       Logic in Computer Science (cs.LO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2311.15675v5", "summary": "We determine the complexity of second-order HyperLTL satisfiability,\nfinite-state satisfiability, and model-checking: All three are equivalent to\ntruth in third-order arithmetic. We also consider two fragments of second-order\nHyperLTL that have been introduced with the aim to facilitate effective\nmodel-checking by restricting the sets one can quantify over. The first one\nrestricts second-order quantification to smallest/largest sets that satisfy a\nguard while the second one restricts second-order quantification further to\nleast fixed points of (first-order) HyperLTL definable functions. All three\nproblems for the first fragment are still equivalent to truth in third-order\narithmetic while satisfiability for the second fragment is\n$\\Sigma_1^2$-complete, and finite-state satisfiability and model-checking are\nequivalent to truth in second-order arithmetic. Finally, we also introduce\nclosed-world semantics for second-order HyperLTL, where set quantification\nranges only over subsets of the model, while set quantification in standard\nsemantics ranges over arbitrary sets of traces. Here, satisfiability for the\nleast fixed point fragment becomes $\\Sigma_1^1$-complete, but all other results\nare unaffected.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2311.15675v5", "cate": "cs.LO", "date": "2023-11-27", "updated": "2025-07-14"}
{"id": "2507.09459", "title": "SegVec3D: A Method for Vector Embedding of 3D Objects Oriented Towards Robot manipulation", "authors": ["Zhihan Kang", "Boyu Wang"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Undergraduate Theis; 12 pages, 6 figures", "url": "http://arxiv.org/abs/2507.09459v1", "summary": "We propose SegVec3D, a novel framework for 3D point cloud instance\nsegmentation that integrates attention mechanisms, embedding learning, and\ncross-modal alignment. The approach builds a hierarchical feature extractor to\nenhance geometric structure modeling and enables unsupervised instance\nsegmentation via contrastive clustering. It further aligns 3D data with natural\nlanguage queries in a shared semantic space, supporting zero-shot retrieval.\nCompared to recent methods like Mask3D and ULIP, our method uniquely unifies\ninstance segmentation and multimodal understanding with minimal supervision and\npractical deployability.", "comment": "Undergraduate Theis; 12 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.09459v1", "cate": "cs.CV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2409.13748", "title": "TheraGen: Therapy for Every Generation", "authors": ["Kartikey Doshi", "Jimit Shah", "Narendra Shekokar"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      This paper contains major errors in methodology and results. It should not be cited", "url": "http://arxiv.org/abs/2409.13748v2", "summary": "We present TheraGen, an advanced AI-powered mental health chatbot utilizing\nthe LLaMA 2 7B model. This approach builds upon recent advancements in language\nmodels and transformer architectures. TheraGen provides all-day personalized,\ncompassionate mental health care by leveraging a large dataset of 1 million\nconversational entries, combining anonymized therapy transcripts, online mental\nhealth discussions, and psychological literature, including APA resources. Our\nimplementation employs transfer learning, fine-tuning, and advanced training\ntechniques to optimize performance. TheraGen offers a user-friendly interface\nfor seamless interaction, providing empathetic responses and evidence-based\ncoping strategies. Evaluation results demonstrate high user satisfaction rates,\nwith 94% of users reporting improved mental well-being. The system achieved a\nBLEU score of 0.67 and a ROUGE score of 0.62, indicating strong response\naccuracy. With an average response time of 1395 milliseconds, TheraGen ensures\nreal-time, efficient support. While not a replacement for professional therapy,\nTheraGen serves as a valuable complementary tool, significantly improving user\nwell-being and addressing the accessibility gap in mental health treatments.\nThis paper details TheraGen's architecture, training methodology, ethical\nconsiderations, and future directions, contributing to the growing field of\nAI-assisted mental healthcare and offering a scalable solution to the pressing\nneed for mental health support.", "comment": "This paper contains major errors in methodology and results. It\n  should not be cited", "pdf_url": "http://arxiv.org/pdf/2409.13748v2", "cate": "cs.CL", "date": "2024-09-12", "updated": "2025-07-11"}
{"id": "2507.10446", "title": "Acquiring and Adapting Priors for Novel Tasks via Neural Meta-Architectures", "authors": ["Sudarshan Babu"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      arXiv admin note: text overlap with arXiv:2310.17075", "url": "http://arxiv.org/abs/2507.10446v1", "summary": "The ability to transfer knowledge from prior experiences to novel tasks\nstands as a pivotal capability of intelligent agents, including both humans and\ncomputational models. This principle forms the basis of transfer learning,\nwhere large pre-trained neural networks are fine-tuned to adapt to downstream\ntasks. Transfer learning has demonstrated tremendous success, both in terms of\ntask adaptation speed and performance. However there are several domains where,\ndue to lack of data, training such large pre-trained models or foundational\nmodels is not a possibility - computational chemistry, computational\nimmunology, and medical imaging are examples. To address these challenges, our\nwork focuses on designing architectures to enable efficient acquisition of\npriors when large amounts of data are unavailable. In particular, we\ndemonstrate that we can use neural memory to enable adaptation on\nnon-stationary distributions with only a few samples. Then we demonstrate that\nour hypernetwork designs (a network that generates another network) can acquire\nmore generalizable priors than standard networks when trained with Model\nAgnostic Meta-Learning (MAML). Subsequently, we apply hypernetworks to 3D scene\ngeneration, demonstrating that they can acquire priors efficiently on just a\nhandful of training scenes, thereby leading to faster text-to-3D generation. We\nthen extend our hypernetwork framework to perform 3D segmentation on novel\nscenes with limited data by efficiently transferring priors from earlier viewed\nscenes. Finally, we repurpose an existing molecular generative method as a\npre-training framework that facilitates improved molecular property prediction,\naddressing critical challenges in computational immunology", "comment": "arXiv admin note: text overlap with arXiv:2310.17075", "pdf_url": "http://arxiv.org/pdf/2507.10446v1", "cate": "cs.AI", "date": "2025-07-07", "updated": "2025-07-07"}
{"id": "2507.09222", "title": "Calibrated and Robust Foundation Models for Vision-Language and Medical Image Tasks Under Distribution Shift", "authors": ["Behraj Khan", "Tahir Syed"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09222v1", "summary": "Foundation models like CLIP and SAM have transformed computer vision and\nmedical imaging via low-shot transfer learning. However, deployment of these\nmodels hindered by two key challenges: \\textit{distribution shift} between\ntraining and test data, and \\textit{confidence misalignment} that leads to\noverconfident incorrect predictions. These issues manifest differently in\nvision-language classification and medical segmentation tasks, yet existing\nsolutions remain domain-specific. We propose \\textit{StaRFM}, a unified\nframework addressing both challenges. It introduces a Fisher information\npenalty (FIP), extended to 3D medical data via patch-wise regularization, to\nreduce covariate shift in CLIP and SAM embeddings. Additionally, a confidence\nmisalignment penalty (CMP), reformulated for voxel-level predictions,\ncalibrates uncertainty in segmentation tasks. We theoretically derive PAC-Bayes\nbounds showing FIP controls generalization via the Fisher-Rao norm, while CMP\nminimizes calibration error through Brier score optimization. StaRFM shows\nconsistent performance like \\texttt{+}3.5\\% accuracy and 28\\% lower ECE on 19\nvision datasets (e.g., ImageNet, Office-Home), 84.7\\% DSC and 4.8mm HD95 in\nmedical segmentation (e.g., BraTS, ATLAS), and 40\\% lower cross-domain\nperformance gap compared to prior benchmarking methods. The framework is\nplug-and-play, requiring minimal architectural changes for seamless integration\nwith foundation models. Code and models will be released at\nhttps://anonymous.4open.science/r/StaRFM-C0CD/README.md", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09222v1", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.10182", "title": "Breaking the Myth: Can Small Models Infer Postconditions Too?", "authors": ["Gehao Zhang", "Zhenting Wang", "Juan Zhai"], "categories": ["cs.SE", "cs.AI"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10182v1", "summary": "Formal specifications are essential for ensuring software correctness, yet\nmanually writing them is tedious and error-prone. Large Language Models (LLMs)\nhave shown promise in generating such specifications from natural language\nintents, but the giant model size and high computational demands raise a\nfundamental question: Do we really need large models for this task? In this\npaper, we show that a small, fine-tuned language model can achieve high-quality\npostcondition generation with much lower computational costs. We construct a\nspecialized dataset of prompts, reasoning logs, and postconditions, then\nsupervise the fine-tuning of a $7$B-parameter code model. Our approach tackles\nreal-world repository dependencies and preserves pre-state information,\nallowing for expressive and accurate specifications. We evaluate the model on a\nbenchmark of real-world Java bugs (Defects4J) and compare against both\nproprietary giants (e.g., GPT-4o) and open-source large models. Empirical\nresults demonstrate that our compact model matches or outperforms significantly\nlarger counterparts in syntax correctness, semantic correctness, and\nbug-distinguishing capability. These findings highlight that targeted\nfine-tuning on a modest dataset can enable small models to achieve results\nformerly seen only in massive, resource-heavy LLMs, offering a practical and\nefficient path for the real-world adoption of automated specification\ngeneration.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10182v1", "cate": "cs.SE", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10424", "title": "A mapping of the Min-Sum decoder to reduction operations, and its implementation using CUDA kernels", "authors": ["Omer Shimon Sella", "Thomas Heinis"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10424v1", "summary": "Decoders for Low Density Parity Check (LDPC) codes are usually tailored to an\napplication and optimized once the specific content and structure of the parity\nmatrix are known. In this work we consider the parity matrix as an argument of\nthe Min-Sum decoder, and provide a GPU implementation that is independent of\nthe content of the parity matrix, and relies only on its dimensions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10424v1", "cate": "cs.IT", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10325", "title": "Convergence of Agnostic Federated Averaging", "authors": ["Herlock", "Rahimi", "Dionysis Kalogerias"], "categories": ["cs.LG", "cs.DC", "eess.SP"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      5 pages, 2 figurres, CAMSAP conference", "url": "http://arxiv.org/abs/2507.10325v1", "summary": "Federated learning (FL) enables decentralized model training without\ncentralizing raw data. However, practical FL deployments often face a key\nrealistic challenge: Clients participate intermittently in server aggregation\nand with unknown, possibly biased participation probabilities. Most existing\nconvergence results either assume full-device participation, or rely on\nknowledge of (in fact uniform) client availability distributions -- assumptions\nthat rarely hold in practice. In this work, we characterize the optimization\nproblem that consistently adheres to the stochastic dynamics of the well-known\n\\emph{agnostic Federated Averaging (FedAvg)} algorithm under random (and\nvariably-sized) client availability, and rigorously establish its convergence\nfor convex, possibly nonsmooth losses, achieving a standard rate of order\n$\\mathcal{O}(1/\\sqrt{T})$, where $T$ denotes the aggregation horizon. Our\nanalysis provides the first convergence guarantees for agnostic FedAvg under\ngeneral, non-uniform, stochastic client participation, without knowledge of the\nparticipation distribution. We also empirically demonstrate that agnostic\nFedAvg in fact outperforms common (and suboptimal) weighted aggregation FedAvg\nvariants, even with server-side knowledge of participation weights.", "comment": "5 pages, 2 figurres, CAMSAP conference", "pdf_url": "http://arxiv.org/pdf/2507.10325v1", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2403.10484", "title": "Moodle Usability Assessment Methodology using the Universal Design for Learning perspective", "authors": ["Rosana Montes", "Liliana Herrera", "Emilio Crisol"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      final version", "url": "http://arxiv.org/abs/2403.10484v3", "summary": "The application of the Universal Design for Learning framework favors the\ncreation of virtual educational environments for all. It requires developing\naccessible content, having a usable platform, and the use of flexible didactics\nand evaluations that promote constant student motivation. The present study\naims to design a methodology to evaluate the usability of the Moodle platform\nbased on the principles of Universal Design for Learning, recognizing the\nimportance of accessibility, usability and the availability of Assistive\nTechnologies. We developed and applied a methodology to assess the usability\nlevel of Moodle platforms, taking into consideration that they integrate\nAssistive Technologies or are used for MOOC contexts. We provide the results of\na use case that assesses two instances for the respective Moodle v.2.x and\nv.3.x family versions. We employed the framework of mixed design research in\norder to assess a MOOC-type educational program devised under the principles of\nUniversal Design for Learning. As a result of the assessment of Moodle v.2.x\nand v.3.x, we conclude that the platforms must improve some key elements (e.g.\ncontrasting colors, incorporation of alternative text and links) in order to\ncomply with international accessibility standards. With respect to usability,\nwe can confirm that the principles and guidelines of Universal Design for\nLearning are applicable to MOOC-type Virtual Learning Environments, are\npositively valued by students, and have a positive impact on certification\nrates.", "comment": "final version", "pdf_url": "http://arxiv.org/pdf/2403.10484v3", "cate": "cs.CY", "date": "2024-03-15", "updated": "2025-07-14"}
{"id": "2411.06565", "title": "Foundation Model for Composite Microstructures: Reconstruction, Stiffness, and Nonlinear Behavior Prediction", "authors": ["Ting-Ju Wei", "Chuin-Shan Chen"], "categories": ["cs.CE", "cs.AI"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.06565v4", "summary": "We present the Material Masked Autoencoder (MMAE), a self-supervised Vision\nTransformer pretrained on a large corpus of short-fiber composite images via\nmasked image reconstruction. The pretrained MMAE learns latent representations\nthat capture essential microstructural features and are broadly transferable\nacross tasks. We demonstrate two key applications: (i) predicting homogenized\nstiffness components through fine-tuning on limited data, and (ii) inferring\nphysically interpretable parameters by coupling MMAE with an interaction-based\nmaterial network (IMN), thereby enabling extrapolation of nonlinear\nstress-strain responses. These results highlight the promise of microstructure\nfoundation models and lay the groundwork for future extensions to more complex\nsystems, such as 3D composites and experimental datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.06565v4", "cate": "cs.CE", "date": "2024-11-10", "updated": "2025-07-14"}
{"id": "2501.12932", "title": "Formal Analysis of the Contract Automata Runtime Environment with Uppaal: Modelling, Verification and Testing", "authors": ["Davide Basile"], "categories": ["cs.SE", "cs.FL", "68N30, 68Q60", "D.2; F.4"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.12932v2", "summary": "Recently, a distributed middleware application called contract automata\nruntime environment ({\\tt CARE}) has been introduced to realise service\napplications specified using a dialect of finite-state automata. In this paper,\nwe detail the formal modelling, verification and testing of {\\tt CARE}. We\nprovide a formalisation as a network of stochastic timed automata. The model is\nverified against the desired properties with the tool {\\sc Uppaal}, utilising\nexhaustive and statistical model checking techniques. Abstract tests are\ngenerated from the {\\sc Uppaal} models that are concretised for testing {\\tt\nCARE}. This research emphasises the advantages of employing formal modelling,\nverification and testing processes to enhance the dependability of an\nopen-source distributed application. We discuss the methodology used for\nmodelling the application and generating concrete tests from the abstract\nmodel, addressing the issues that have been identified and fixed.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.12932v2", "cate": "cs.SE", "date": "2025-01-22", "updated": "2025-07-14"}
{"id": "2507.10034", "title": "LifelongPR: Lifelong knowledge fusion for point cloud place recognition based on replay and prompt learning", "authors": ["Xianghong Zou", "Jianping Li", "Zhe Chen", "Zhen Cao", "Zhen Dong", "Qiegen Liu", "Bisheng Yang"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10034v1", "summary": "Point cloud place recognition (PCPR) plays a crucial role in photogrammetry\nand robotics applications such as autonomous driving, intelligent\ntransportation, and augmented reality. In real-world large-scale deployments of\na positioning system, PCPR models must continuously acquire, update, and\naccumulate knowledge to adapt to diverse and dynamic environments, i.e., the\nability known as continual learning (CL). However, existing PCPR models often\nsuffer from catastrophic forgetting, leading to significant performance\ndegradation in previously learned scenes when adapting to new environments or\nsensor types. This results in poor model scalability, increased maintenance\ncosts, and system deployment difficulties, undermining the practicality of\nPCPR. To address these issues, we propose LifelongPR, a novel continual\nlearning framework for PCPR, which effectively extracts and fuses knowledge\nfrom sequential point cloud data. First, to alleviate the knowledge loss, we\npropose a replay sample selection method that dynamically allocates sample\nsizes according to each dataset's information quantity and selects spatially\ndiverse samples for maximal representativeness. Second, to handle domain\nshifts, we design a prompt learning-based CL framework with a lightweight\nprompt module and a two-stage training strategy, enabling domain-specific\nfeature adaptation while minimizing forgetting. Comprehensive experiments on\nlarge-scale public and self-collected datasets are conducted to validate the\neffectiveness of the proposed method. Compared with state-of-the-art (SOTA)\nmethods, our method achieves 6.50% improvement in mIR@1, 7.96% improvement in\nmR@1, and an 8.95% reduction in F. The code and pre-trained models are publicly\navailable at https://github.com/zouxianghong/LifelongPR.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10034v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2501.08736", "title": "Holoview: An Immersive Mixed-Reality Visualization System for Anatomical Education", "authors": ["Anshul Goswami", "Ojaswa Sharma"], "categories": ["cs.GR", "cs.HC"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.08736v4", "summary": "We present Holoview, an augmented reality (AR) system designed to support\nimmersive and interactive learning of human anatomy. Holoview enables users to\ndynamically explore volumetric anatomical data through intuitive hand gestures\nin a 3D AR environment, allowing inspection of individual organs and\ncross-sectional views via clipping and bioscope features. The system adopts a\nlightweight client-server architecture optimized for real-time performance on\nthe HoloLens through hybrid and foveated rendering. Our user study demonstrated\nHoloview's educational effectiveness, with participants showing a 135 percent\nimprovement in task-specific knowledge and reporting increased confidence in\nunderstanding anatomical structures. The system was perceived as engaging and\nintuitive, particularly for organ selection and cross-sectional exploration,\nwith low cognitive load and increasing ease of use over time. These findings\nhighlight Holoview's potential to enhance anatomy learning through immersive,\nuser-centered AR experiences.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.08736v4", "cate": "cs.GR", "date": "2025-01-15", "updated": "2025-07-14"}
{"id": "2412.11407", "title": "An Enhanced Classification Method Based on Adaptive Multi-Scale Fusion for Long-tailed Multispectral Point Clouds", "authors": ["TianZhu Liu", "BangYan Hu", "YanFeng Gu", "Xian Li", "Aleksandra Pižurica"], "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      16 pages, 9 figures, 5 tables", "url": "http://arxiv.org/abs/2412.11407v1", "summary": "Multispectral point cloud (MPC) captures 3D spatial-spectral information from\nthe observed scene, which can be used for scene understanding and has a wide\nrange of applications. However, most of the existing classification methods\nwere extensively tested on indoor datasets, and when applied to outdoor\ndatasets they still face problems including sparse labeled targets, differences\nin land-covers scales, and long-tailed distributions. To address the above\nissues, an enhanced classification method based on adaptive multi-scale fusion\nfor MPCs with long-tailed distributions is proposed. In the training set\ngeneration stage, a grid-balanced sampling strategy is designed to reliably\ngenerate training samples from sparse labeled datasets. In the feature learning\nstage, a multi-scale feature fusion module is proposed to fuse shallow features\nof land-covers at different scales, addressing the issue of losing fine\nfeatures due to scale variations in land-covers. In the classification stage,\nan adaptive hybrid loss module is devised to utilize multi-classification heads\nwith adaptive weights to balance the learning ability of different classes,\nimproving the classification performance of small classes due to various-scales\nand long-tailed distributions in land-covers. Experimental results on three MPC\ndatasets demonstrate the effectiveness of the proposed method compared with the\nstate-of-the-art methods.", "comment": "16 pages, 9 figures, 5 tables", "pdf_url": "http://arxiv.org/pdf/2412.11407v1", "cate": "cs.CV", "date": "2024-12-16", "updated": "2024-12-16"}
{"id": "2507.09230", "title": "EgoAnimate: Generating Human Animations from Egocentric top-down Views", "authors": ["G. Kutay Türkoglu", "Julian Tanke", "Iheb Belgacem", "Lev Markhasin"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 pages, 5 figures", "url": "http://arxiv.org/abs/2507.09230v1", "summary": "An ideal digital telepresence experience requires accurate replication of a\nperson's body, clothing, and movements. To capture and transfer these movements\ninto virtual reality, the egocentric (first-person) perspective can be adopted,\nwhich enables the use of a portable and cost-effective device without\nfront-view cameras. However, this viewpoint introduces challenges such as\nocclusions and distorted body proportions.\n  There are few works reconstructing human appearance from egocentric views,\nand none use a generative prior-based approach. Some methods create avatars\nfrom a single egocentric image during inference, but still rely on multi-view\ndatasets during training. To our knowledge, this is the first study using a\ngenerative backbone to reconstruct animatable avatars from egocentric inputs.\nBased on Stable Diffusion, our method reduces training burden and improves\ngeneralizability.\n  Inspired by methods such as SiTH and MagicMan, which perform 360-degree\nreconstruction from a frontal image, we introduce a pipeline that generates\nrealistic frontal views from occluded top-down images using ControlNet and a\nStable Diffusion backbone.\n  Our goal is to convert a single top-down egocentric image into a realistic\nfrontal representation and feed it into an image-to-motion model. This enables\ngeneration of avatar motions from minimal input, paving the way for more\naccessible and generalizable telepresence systems.", "comment": "10 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.09230v1", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.10228", "title": "Towards a Framework for Operationalizing the Specification of Trustworthy AI Requirements", "authors": ["Hugo Villamizar", "Daniel Mendez", "Marcos Kalinowski"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      This paper has been accepted for presentation at the 2025 IEEE 33rd International Requirements Engineering Conference Workshops (REW-RETRAI 2025)", "url": "http://arxiv.org/abs/2507.10228v1", "summary": "Growing concerns around the trustworthiness of AI-enabled systems highlight\nthe role of requirements engineering (RE) in addressing emergent,\ncontext-dependent properties that are difficult to specify without structured\napproaches. In this short vision paper, we propose the integration of two\ncomplementary approaches: AMDiRE, an artefact-based approach for RE, and\nPerSpecML, a perspective-based method designed to support the elicitation,\nanalysis, and specification of machine learning (ML)-enabled systems. AMDiRE\nprovides a structured, artefact-centric, process-agnostic methodology and\ntemplates that promote consistency and traceability in the results; however, it\nis primarily oriented toward deterministic systems. PerSpecML, in turn,\nintroduces multi-perspective guidance to uncover concerns arising from the\ndata-driven and non-deterministic behavior of ML-enabled systems. We envision a\npathway to operationalize trustworthiness-related requirements, bridging\nstakeholder-driven concerns and structured artefact models. We conclude by\noutlining key research directions and open challenges to be discussed with the\nRE community.", "comment": "This paper has been accepted for presentation at the 2025 IEEE 33rd\n  International Requirements Engineering Conference Workshops (REW-RETRAI 2025)", "pdf_url": "http://arxiv.org/pdf/2507.10228v1", "cate": "cs.SE", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.08821", "title": "LNN-powered Fluid Antenna Multiple Access", "authors": ["Pedro D. Alvim", "Hugerles S. Silva", "Ugo S. Dias", "Osamah S. Badarneh", "Felipe A. P. Figueiredo", "Rausley A. A. de Souza"], "categories": ["eess.SP", "cs.IT", "cs.LG", "math.IT"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08821v1", "summary": "Fluid antenna systems represent an innovative approach in wireless\ncommunication, recently applied in multiple access to optimize the\nsignal-to-interference-plus-noise ratio through port selection. This letter\nframes the port selection problem as a multi-label classification task for the\nfirst time, improving best-port selection with limited port observations. We\naddress this challenge by leveraging liquid neural networks (LNNs) to predict\nthe optimal port under emerging fluid antenna multiple access scenarios\nalongside a more general $\\alpha$-$\\mu$ fading model. We also apply\nhyperparameter optimization to refine LNN architectures for different\nobservation scenarios. Our approach yields lower outage probability values than\nexisting methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08821v1", "cate": "eess.SP", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2405.12079", "title": "PhoenixOS: Concurrent OS-level GPU Checkpoint and Restore with Validated Speculation", "authors": ["Xingda Wei", "Zhuobin Huang", "Tianle Sun", "Yingyi Hao", "Rong Chen", "Mingcong Han", "Jinyu Gu", "Haibo Chen"], "categories": ["cs.DC", "cs.OS"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      To appear at SOSP'25", "url": "http://arxiv.org/abs/2405.12079v2", "summary": "PhoenixOS (PhOS) is the first OS service that can concurrently checkpoint and\nrestore (C/R) GPU processes -- a fundamental capability for critical tasks such\nas fault tolerance, process migration, and fast startup. While concurrent C/R\nis well-established on CPUs, it poses unique challenges on GPUs due to their\nlack of essential features for efficiently tracing concurrent memory reads and\nwrites, such as specific hardware capabilities (e.g., dirty bits) and\nOS-mediated data paths (e.g., copy-on-write). To ensure correct concurrent C/R,\nPhOS proactively detects GPU memory reads and writes through a two-step\nprocess: first, it speculates about GPU memory accesses based on the arguments\nused when launching GPU kernels; then, it validates these accesses efficiently\nat runtime using binary instrumentation. With this validated speculation, PhOS\nretrofits CPU-based concurrent C/R for GPUs through software-based approaches,\nincluding soft copy-on-write, soft recopy, and soft on-demand restore. PhOS\nfurther proposes several GPU-aware techniques for efficient GPU C/R, including\ncoordinated checkpoint data transfer and execution context pool. For downstream\ntasks that use C/R for tolerating failures, migrating processes live, and\naccelerating cold starts in serverless computing, PHOS achieves orders of\nmagnitude higher performance than state-of-the-art OS-level GPU C/R systems\nlike NVIDIA cuda-checkpoint.", "comment": "To appear at SOSP'25", "pdf_url": "http://arxiv.org/pdf/2405.12079v2", "cate": "cs.DC", "date": "2024-05-20", "updated": "2025-07-14"}
{"id": "2406.12167", "title": "Bounds and Bugs: The Limits of Symmetry Metrics to Detect Partisan Gerrymandering", "authors": ["Daryl DeFord", "Ellen Veomett"], "categories": ["cs.CY", "math.CO", "physics.soc-ph", "97A40, 91F99", "J.4"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      To be published in Election Law Journal: Rules, Politics, and Policy. 59 pages, 45 figures, 7 tables", "url": "http://arxiv.org/abs/2406.12167v4", "summary": "We consider two symmetry metrics commonly used to analyze partisan\ngerrymandering: the Mean-Median Difference (MM) and Partisan Bias (PB). Our\nmain results compare, for combinations of seats and votes achievable in\ndistricted elections, the number of districts won by each party to the extent\nof potential deviation from the ideal metric values, taking into account the\npolitical geography of the state. These comparisons are motivated by examples\nwhere the MM and PB have been used in efforts to detect when a districting plan\nawards extreme number of districts won by some party. These examples include\nexpert testimony, public-facing apps, recommendations by experts to\nredistricting commissions, and public policy proposals. To achieve this goal we\nperform both theoretical and empirical analyses of the MM and PB. In our\ntheoretical analysis, we consider vote-share, seat-share pairs (V, S) for which\none can construct election data having vote share V and seat share S, and\nturnout is equal in each district. We calculate the range of values that MM and\nPB can achieve on that constructed election data. In the process, we find the\nrange of (V,S) pairs that achieve MM = 0, and see that the corresponding range\nfor PB is the same set of (V,S) pairs. We show how the set of such (V,S) pairs\nallowing for MM = 0 (and PB = 0) changes when turnout in each district is\nallowed to vary. By observing the results of this theoretical analysis, we can\nshow that the values taken on by these metrics do not necessarily attain more\nextreme values in plans with more extreme numbers of districts won. We also\nanalyze specific example elections, showing how these metrics can return\nunintuitive results. We follow this with an empirical study, where we show that\non 18 different U.S. maps these metrics can fail to detect extreme seats\noutcomes.", "comment": "To be published in Election Law Journal: Rules, Politics, and Policy.\n  59 pages, 45 figures, 7 tables", "pdf_url": "http://arxiv.org/pdf/2406.12167v4", "cate": "cs.CY", "date": "2024-06-18", "updated": "2025-07-11"}
{"id": "2506.01973", "title": "Multimodal Financial Foundation Models (MFFMs): Progress, Prospects, and Challenges", "authors": ["Xiao-Yang Liu Yanglet", "Yupeng Cao", "Li Deng"], "categories": ["cs.CE"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.01973v2", "summary": "Financial Large Language Models (FinLLMs), such as open FinGPT and\nproprietary BloombergGPT, have demonstrated great potential in select areas of\nfinancial services. Beyond this earlier language-centric approach, Multimodal\nFinancial Foundation Models (MFFMs) can digest interleaved multimodal financial\ndata, including fundamental data, market data, data analytics, macroeconomic,\nand alternative data (e.g., natural language, audio, images, and video). In\nthis position paper, presented at the MFFM Workshop joined with ACM\nInternational Conference on AI in Finance (ICAIF) 2024, we describe the\nprogress, prospects, and challenges of MFFMs. This paper also highlights\nongoing research on FinAgents in the \\textbf{SecureFinAI\nLab}\\footnote{\\https://openfin.engineering.columbia.edu/} at Columbia\nUniversity. We believe that MFFMs will enable a deeper understanding of the\nunderlying complexity associated with numerous financial tasks and data,\nstreamlining the operation of financial services and investment processes.\nGithub Repo https://github.com/Open-Finance-Lab/Awesome-MFFMs/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.01973v2", "cate": "cs.CE", "date": "2025-05-15", "updated": "2025-07-12"}
{"id": "2507.10158", "title": "MTF-Grasp: A Multi-tier Federated Learning Approach for Robotic Grasping", "authors": ["Obaidullah Zaland", "Erik Elmroth", "Monowar Bhuyan"], "categories": ["cs.LG", "cs.RO"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      The work is accepted for presentation at IEEE SMC 2025", "url": "http://arxiv.org/abs/2507.10158v1", "summary": "Federated Learning (FL) is a promising machine learning paradigm that enables\nparticipating devices to train privacy-preserved and collaborative models. FL\nhas proven its benefits for robotic manipulation tasks. However, grasping tasks\nlack exploration in such settings where robots train a global model without\nmoving data and ensuring data privacy. The main challenge is that each robot\nlearns from data that is nonindependent and identically distributed (non-IID)\nand of low quantity. This exhibits performance degradation, particularly in\nrobotic grasping. Thus, in this work, we propose MTF-Grasp, a multi-tier FL\napproach for robotic grasping, acknowledging the unique challenges posed by the\nnon-IID data distribution across robots, including quantitative skewness.\nMTF-Grasp harnesses data quality and quantity across robots to select a set of\n\"top-level\" robots with better data distribution and higher sample count. It\nthen utilizes top-level robots to train initial seed models and distribute them\nto the remaining \"low-level\" robots, reducing the risk of model performance\ndegradation in low-level robots. Our approach outperforms the conventional FL\nsetup by up to 8% on the quantity-skewed Cornell and Jacquard grasping\ndatasets.", "comment": "The work is accepted for presentation at IEEE SMC 2025", "pdf_url": "http://arxiv.org/pdf/2507.10158v1", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2505.08245", "title": "Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement", "authors": ["Haoran Ye", "Jing Jin", "Yuhang Xie", "Xin Zhang", "Guojie Song"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      474 references", "url": "http://arxiv.org/abs/2505.08245v2", "summary": "The advancement of large language models (LLMs) has outpaced traditional\nevaluation methodologies. This progress presents novel challenges, such as\nmeasuring human-like psychological constructs, moving beyond static and\ntask-specific benchmarks, and establishing human-centered evaluation. These\nchallenges intersect with psychometrics, the science of quantifying the\nintangible aspects of human psychology, such as personality, values, and\nintelligence. This review paper introduces and synthesizes the emerging\ninterdisciplinary field of LLM Psychometrics, which leverages psychometric\ninstruments, theories, and principles to evaluate, understand, and enhance\nLLMs. The reviewed literature systematically shapes benchmarking principles,\nbroadens evaluation scopes, refines methodologies, validates results, and\nadvances LLM capabilities. Diverse perspectives are integrated to provide a\nstructured framework for researchers across disciplines, enabling a more\ncomprehensive understanding of this nascent field. Ultimately, the review\nprovides actionable insights for developing future evaluation paradigms that\nalign with human-level AI and promote the advancement of human-centered AI\nsystems for societal benefit. A curated repository of LLM psychometric\nresources is available at\nhttps://github.com/valuebyte-ai/Awesome-LLM-Psychometrics.", "comment": "474 references", "pdf_url": "http://arxiv.org/pdf/2505.08245v2", "cate": "cs.CL", "date": "2025-05-13", "updated": "2025-07-13"}
{"id": "2507.07855", "title": "Principled Foundations for Preference Optimization", "authors": ["Wenxuan Zhou", "Shujian Zhang", "Brice Magdalou", "John Lambert", "Ehsan Amid", "Richard Nock", "Andrew Hard"], "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.6; I.2.7"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07855v1", "summary": "In this paper, we show that direct preference optimization (DPO) is a very\nspecific form of a connection between two major theories in the ML context of\nlearning from preferences: loss functions (Savage) and stochastic choice\n(Doignon-Falmagne and Machina). The connection is established for all of\nSavage's losses and at this level of generality, (i) it includes support for\nabstention on the choice theory side, (ii) it includes support for non-convex\nobjectives on the ML side, and (iii) it allows to frame for free some notable\nextensions of the DPO setting, including margins and corrections for length.\nGetting to understand how DPO operates from a general principled perspective is\ncrucial because of the huge and diverse application landscape of models,\nbecause of the current momentum around DPO, but also -- and importantly --\nbecause many state of the art variations on DPO definitely occupy a small\nregion of the map that we cover. It also helps to understand the pitfalls of\ndeparting from this map, and figure out workarounds.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07855v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.09242", "title": "PPJudge: Towards Human-Aligned Assessment of Artistic Painting Process", "authors": ["Shiqi Jiang", "Xinpeng Li", "Xi Mao", "Changbo Wang", "Chenhui Li"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ACM International Conference on Multimedia 2025", "url": "http://arxiv.org/abs/2507.09242v1", "summary": "Artistic image assessment has become a prominent research area in computer\nvision. In recent years, the field has witnessed a proliferation of datasets\nand methods designed to evaluate the aesthetic quality of paintings. However,\nmost existing approaches focus solely on static final images, overlooking the\ndynamic and multi-stage nature of the artistic painting process. To address\nthis gap, we propose a novel framework for human-aligned assessment of painting\nprocesses. Specifically, we introduce the Painting Process Assessment Dataset\n(PPAD), the first large-scale dataset comprising real and synthetic painting\nprocess images, annotated by domain experts across eight detailed attributes.\nFurthermore, we present PPJudge (Painting Process Judge), a Transformer-based\nmodel enhanced with temporally-aware positional encoding and a heterogeneous\nmixture-of-experts architecture, enabling effective assessment of the painting\nprocess. Experimental results demonstrate that our method outperforms existing\nbaselines in accuracy, robustness, and alignment with human judgment, offering\nnew insights into computational creativity and art education.", "comment": "ACM International Conference on Multimedia 2025", "pdf_url": "http://arxiv.org/pdf/2507.09242v1", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.10235", "title": "An Empirical Study of Interaction Bugs in ROS-based Software", "authors": ["Zhixiang Chen", "Zhuangbin Chen", "Xingjie Cai", "Wei Li", "Zibin Zheng"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10235v1", "summary": "Modern robotic systems integrate multiple independent software and hardware\ncomponents, each responsible for distinct functionalities such as perception,\ndecision-making, and execution. These components interact extensively to\naccomplish complex end-to-end tasks. As a result, the overall system\nreliability depends not only on the correctness of individual components, but\nalso on the correctness of their interactions. Failures often manifest at the\nboundaries between components, yet interaction-related reliability issues in\nrobotics--referred to here as interaction bugs (iBugs)--remain underexplored.\n  This work presents an empirical study of iBugs within robotic systems built\nusing the Robot Operating System (ROS), a widely adopted open-source robotics\nframework. A total of 121 iBugs were analyzed across ten actively maintained\nand representative ROS projects. The identified iBugs are categorized into\nthree major types: intra-system iBugs, hardware iBugs, and environmental iBugs,\ncovering a broad range of interaction scenarios in robotics. The analysis\nincludes an examination of root causes, fixing strategies, and the impact of\nthese bugs. Several findingsa are derived that shed light on the nature of\niBugs and suggest directions for improving their prevention and detection.\nThese insights aim to inform the design of more robust and safer robotic\nsystems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10235v1", "cate": "cs.SE", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.08950", "title": "Fundamental limits via CRB of semi-blind channel estimation in Massive MIMO systems", "authors": ["Xue Zhang", "Abla Kammoun", "Mohamed-Slim Alouini"], "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08950v1", "summary": "This paper investigates the asymptotic behavior of the deterministic and\nstochastic Cram\\'er-Rao Bounds (CRB) for semi-blind channel estimation in\nmassive multiple-input multiple-output (MIMO) systems. We derive and analyze\nmathematically tractable expressions for both metrics under various asymptotic\nregimes, which govern the growth rates of the number of antennas, the number of\nusers, the training sequence length, and the transmission block length. Unlike\nthe existing work, our results show that the CRB can be made arbitrarily small\nas the transmission block length increases, but only when the training sequence\nlength grows at the same rate and the number of users remains fixed. However,\nif the number of training sequences remains proportional to the number of\nusers, the channel estimation error is always lower-bounded by a non-vanishing\nconstant. Numerical results are presented to support our findings and\ndemonstrate the advantages of semi-blind channel estimation in reducing the\nrequired number of training sequences.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08950v1", "cate": "eess.SP", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2409.03778", "title": "Two Pareto Optimum-based Heuristic Algorithms for Minimizing Tardiness and Late Jobs in the Single Machine Flowshop Problem", "authors": ["Matthew Gradwohl", "Guidio Sewa", "Oke Blessing Oghojafor", "Richard Wilouwou", "Muminu Adamu", "Christopher Thron"], "categories": ["cs.DC", "math.OC", "90-08, 90B35", "J.1"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      25 pages, 13 figures", "url": "http://arxiv.org/abs/2409.03778v2", "summary": "Flowshop problems play a prominent role in operations research, and have\nconsiderable practical significance. The single-machine flowshop problem is of\nparticular theoretical interest. Until now the problem of minimizing late jobs\nor job tardiness can only be solved exactly by computationally-intensive\nmethods such as dynamic programming or linear programming. In this paper we\nintroduce, test, and optimize two new heuristic algorithms for mixed tardiness\nand late job minimization in single-machine flowshops. The two algorithms both\nbuild partial schedules iteratively. Both also retain Pareto optimal solutions\nat intermediate stages, to take into account both tardiness and late jobs\nwithin the partial schedule, as well as the effect of partial completion time\non not-yet scheduled jobs. Both algorithms can be applied to scenarios with\nhundreds of jobs, with execution times running from less than a second to a few\nminutes. Although they are slower than dispatch rule-based heuristics, the\nsolutions obtained are far better. We also compare a neural-network solution,\nwhich performs poorly.", "comment": "25 pages, 13 figures", "pdf_url": "http://arxiv.org/pdf/2409.03778v2", "cate": "cs.DC", "date": "2024-08-22", "updated": "2025-07-13"}
{"id": "2409.06672", "title": "Insuring Uninsurable Risks from AI: Government as Insurer of Last Resort", "authors": ["Cristian Trout"], "categories": ["cs.CY", "cs.AI", "cs.LG", "q-fin.RM"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      Accepted to Generative AI and Law Workshop at the International Conference on Machine Learning (ICML 2024)", "url": "http://arxiv.org/abs/2409.06672v3", "summary": "Many experts believe that AI systems will sooner or later pose uninsurable\nrisks, including existential risks. This creates an extreme judgment-proof\nproblem: few if any parties can be held accountable ex post in the event of\nsuch a catastrophe. This paper proposes a novel solution: a\ngovernment-provided, mandatory indemnification program for AI developers. The\nprogram uses risk-priced indemnity fees to induce socially optimal levels of\ncare. Risk-estimates are determined by surveying experts, including indemnified\ndevelopers. The Bayesian Truth Serum mechanism is employed to incent honest and\neffortful responses. Compared to alternatives, this approach arguably better\nleverages all private information, and provides a clearer signal to indemnified\ndevelopers regarding what risks they must mitigate to lower their fees. It's\nrecommended that collected fees be used to help fund the safety research\ndevelopers need, employing a fund matching mechanism (Quadratic Financing) to\ninduce an optimal supply of this public good. Under Quadratic Financing, safety\nresearch projects would compete for private contributions from developers,\nsignaling how much each is to be supplemented with public funds.", "comment": "Accepted to Generative AI and Law Workshop at the International\n  Conference on Machine Learning (ICML 2024)", "pdf_url": "http://arxiv.org/pdf/2409.06672v3", "cate": "cs.CY", "date": "2024-09-10", "updated": "2025-07-12"}
{"id": "2412.02799", "title": "QPET: A Versatile and Portable Quantity-of-Interest-Preservation Framework for Error-Bounded Lossy Compression", "authors": ["Jinyang Liu", "Pu Jiao", "Kai Zhao", "Xin Liang", "Sheng Di", "Franck Cappello"], "categories": ["cs.DB", "cs.CE", "cs.DC"], "primary_category": "Subjects:       Databases (cs.DB)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.02799v4", "summary": "Error-bounded lossy compression has been widely adopted in many scientific\ndomains because it can address the challenges in storing, transferring, and\nanalyzing unprecedented amounts of scientific data. Although error-bounded\nlossy compression offers general data distortion control by enforcing strict\nerror bounds on raw data, it may fail to meet the quality requirements on the\nresults of downstream analysis, a.k.a. Quantities of Interest (QoIs), derived\nfrom raw data. This may lead to uncertainties and even misinterpretations in\nscientific discoveries, significantly limiting the use of lossy compression in\npractice. In this paper, we propose QPET, a novel, versatile, and portable\nframework for QoI-preserving error-bounded lossy compression, which overcomes\nthe challenges of modeling diverse QoIs by leveraging numerical strategies.\nQPET features (1) high portability to multiple existing lossy compressors, (2)\nversatile preservation to most differentiable univariate and multivariate QoIs,\nand (3) significant compression improvements in QoI-preservation tasks.\nExperiments with six real-world datasets demonstrate that integrating QPET into\nstate-of-the-art error-bounded lossy compressors can gain 2x to 10x compression\nspeedups of existing QoI-preserving error-bounded lossy compression solutions,\nup to 1000% compression ratio improvements to general-purpose compressors, and\nup to 133% compression ratio improvements to existing QoI-integrated scientific\ncompressors.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.02799v4", "cate": "cs.DB", "date": "2024-12-03", "updated": "2025-07-14"}
{"id": "2507.10474", "title": "Privacy-Preserving Multi-Stage Fall Detection Framework with Semi-supervised Federated Learning and Robotic Vision Confirmation", "authors": ["Seyed Alireza Rahimi Azghadi", "Truong-Thanh-Hung Nguyen", "Helene Fournier", "Monica Wachowicz", "Rene Richard", "Francis Palma", "Hung Cao"], "categories": ["cs.CV", "cs.AI", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10474v1", "summary": "The aging population is growing rapidly, and so is the danger of falls in\nolder adults. A major cause of injury is falling, and detection in time can\ngreatly save medical expenses and recovery time. However, to provide timely\nintervention and avoid unnecessary alarms, detection systems must be effective\nand reliable while addressing privacy concerns regarding the user. In this\nwork, we propose a framework for detecting falls using several complementary\nsystems: a semi-supervised federated learning-based fall detection system\n(SF2D), an indoor localization and navigation system, and a vision-based human\nfall recognition system. A wearable device and an edge device identify a fall\nscenario in the first system. On top of that, the second system uses an indoor\nlocalization technique first to localize the fall location and then navigate a\nrobot to inspect the scenario. A vision-based detection system running on an\nedge device with a mounted camera on a robot is used to recognize fallen\npeople. Each of the systems of this proposed framework achieves different\naccuracy rates. Specifically, the SF2D has a 0.81% failure rate equivalent to\n99.19% accuracy, while the vision-based fallen people detection achieves 96.3%\naccuracy. However, when we combine the accuracy of these two systems with the\naccuracy of the navigation system (95% success rate), our proposed framework\ncreates a highly reliable performance for fall detection, with an overall\naccuracy of 99.99%. Not only is the proposed framework safe for older adults,\nbut it is also a privacy-preserving solution for detecting falls.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10474v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.04189", "title": "SymbolicThought: Integrating Language Models and Symbolic Reasoning for Consistent and Interpretable Human Relationship Understanding", "authors": ["Runcong Zhao", "Qinglin Zhu", "Hainiu Xu", "Bin Liang", "Lin Gui", "Yulan He"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.04189v2", "summary": "Understanding character relationships is essential for interpreting complex\nnarratives and conducting socially grounded AI research. However, manual\nannotation is time-consuming and low in coverage, while large language models\n(LLMs) often produce hallucinated or logically inconsistent outputs. We present\nSymbolicThought, a human-in-the-loop framework that combines LLM-based\nextraction with symbolic reasoning. The system constructs editable character\nrelationship graphs, refines them using seven types of logical constraints, and\nenables real-time validation and conflict resolution through an interactive\ninterface. To support logical supervision and explainable social analysis, we\nrelease a dataset of 160 interpersonal relationships with corresponding logical\nstructures. Experiments show that SymbolicThought improves annotation accuracy\nand consistency while significantly reducing time cost, offering a practical\ntool for narrative understanding, explainable AI, and LLM evaluation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.04189v2", "cate": "cs.CL", "date": "2025-07-05", "updated": "2025-07-13"}
{"id": "2507.08052", "title": "Lightweight Cloud Masking Models for On-Board Inference in Hyperspectral Imaging", "authors": ["Mazen Ali", "António Pereira", "Fabio Gentile", "Aser Cortines", "Sam Mugel", "Román Orús", "Stelios P. Neophytides", "Michalis Mavrovouniotis"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08052v1", "summary": "Cloud and cloud shadow masking is a crucial preprocessing step in\nhyperspectral satellite imaging, enabling the extraction of high-quality,\nanalysis-ready data. This study evaluates various machine learning approaches,\nincluding gradient boosting methods such as XGBoost and LightGBM as well as\nconvolutional neural networks (CNNs). All boosting and CNN models achieved\naccuracies exceeding 93%. Among the investigated models, the CNN with feature\nreduction emerged as the most efficient, offering a balance of high accuracy,\nlow storage requirements, and rapid inference times on both CPUs and GPUs.\nVariations of this version, with only up to 597 trainable parameters,\ndemonstrated the best trade-off in terms of deployment feasibility, accuracy,\nand computational efficiency. These results demonstrate the potential of\nlightweight artificial intelligence (AI) models for real-time hyperspectral\nimage processing, supporting the development of on-board satellite AI systems\nfor space-based applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08052v1", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.09248", "title": "AGCD-Net: Attention Guided Context Debiasing Network for Emotion Recognition", "authors": ["Varsha Devi", "Amine Bohi", "Pardeep Kumar"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      13 Pages, 4 figures, 2 tables ICIAP 2025", "url": "http://arxiv.org/abs/2507.09248v1", "summary": "Context-aware emotion recognition (CAER) enhances affective computing in\nreal-world scenarios, but traditional methods often suffer from context\nbias-spurious correlation between background context and emotion labels (e.g.\nassociating ``garden'' with ``happy''). In this paper, we propose\n\\textbf{AGCD-Net}, an Attention Guided Context Debiasing model that introduces\n\\textit{Hybrid ConvNeXt}, a novel convolutional encoder that extends the\nConvNeXt backbone by integrating Spatial Transformer Network and\nSqueeze-and-Excitation layers for enhanced feature recalibration. At the core\nof AGCD-Net is the Attention Guided - Causal Intervention Module (AG-CIM),\nwhich applies causal theory, perturbs context features, isolates spurious\ncorrelations, and performs an attention-driven correction guided by face\nfeatures to mitigate context bias. Experimental results on the CAER-S dataset\ndemonstrate the effectiveness of AGCD-Net, achieving state-of-the-art\nperformance and highlighting the importance of causal debiasing for robust\nemotion recognition in complex settings.", "comment": "13 Pages, 4 figures, 2 tables ICIAP 2025", "pdf_url": "http://arxiv.org/pdf/2507.09248v1", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.10244", "title": "Helveg: Diagrams for Software Documentation", "authors": ["Adam Štěpánek", "David Kuťák", "Barbora Kozlíková", "Jan Byška"], "categories": ["cs.SE", "D.2.2; D.2.11"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      13 pages, 5 figures, accepted by TVCG", "url": "http://arxiv.org/abs/2507.10244v1", "summary": "Software developers often have to gain an understanding of a codebase. Be it\nprogrammers getting onboarded onto a team project or, for example, developers\nstriving to grasp an external open-source library. In either case, they\nfrequently turn to the project's documentation. However, documentation in its\ntraditional textual form is ill-suited for this kind of high-level exploratory\nanalysis, since it is immutable from the readers' perspective and thus forces\nthem to follow a predefined path. We have designed an approach bringing aspects\nof software architecture visualization to API reference documentation. It\nutilizes a highly interactive node-link diagram with expressive node glyphs and\nflexible filtering capabilities, providing a high-level overview of the\ncodebase as well as details on demand. To test our design, we have implemented\na prototype named Helveg, capable of automatically generating diagrams of C\\#\ncodebases. User testing of Helveg confirmed its potential, but it also revealed\nproblems with the readability, intuitiveness, and user experience of our tool.\nTherefore, in this paper, which is an extended version of our VISSOFT paper\nwith DOI 10.1109/VISSOFT64034.2024.00012, we address many of these problems\nthrough major changes to the glyph design, means of interaction, and user\ninterface of the tool. To assess the improvements, this new version of Helveg\nwas evaluated again with the same group of participants as the previous\nversion.", "comment": "13 pages, 5 figures, accepted by TVCG", "pdf_url": "http://arxiv.org/pdf/2507.10244v1", "cate": "cs.SE", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09013", "title": "Data-Driven Matrix Recovery with High-Dimensional Noise via Optimal Shrinkage of Singular Values and Wavelet Shrinkage of Singular Vectors", "authors": ["Pei-Chun Su"], "categories": ["math.SP", "cs.IT", "math.IT"], "primary_category": "Subjects:       Spectral Theory (math.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09013v1", "summary": "This paper presents a novel data-driven algorithm designed to recover\nlow-rank matrices whose entries satisfy a mixed H\\\"older condition in the\npresence of high-dimensional noise with a separable covariance structure. The\nalgorithm, coined extended optimal shrinkage and wavelet shrinkage\n(e$\\mathcal{OWS}$), emphasizes the asymptotic structure, where the matrix size\nis significantly larger than the rank of the signal matrix. The denoising\nprocess begins with the adaptation of the well-known optimal shrinkage of\nsingular values. This is followed by an iterative procedure that organizes the\nmatrix using a coupled metric on the rows and columns, constructed by building\na tree structure for both dimensions. This hierarchical organization induces a\ntensor Haar-Walsh basis on the matrix. An adapted wavelet shrinkage technique\nis applied to further denoise the reconstructed matrix, modifying the\nHaar-Walsh coefficients based on the analysis of the first-order perturbation\nof singular vectors. We provide theoretical guarantees for these estimators,\ndemonstrating a convergence rate that highlights the efficacy of our algorithm.\nSimulations show successful matrix recovery, with a small mean squared error\nbetween the estimate and the ground truth, and accurate reconstruction of the\nsingular vector spaces.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09013v1", "cate": "math.SP", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2409.09202", "title": "HotSwap: Enabling Live Dependency Sharing in Serverless Computing", "authors": ["Rui Li", "Devesh Tiwari", "Gene Cooperman"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      10 pages, 7 figures. This work was accepted at the IEEE International Conference on Cloud Computing 2025", "url": "http://arxiv.org/abs/2409.09202v3", "summary": "This work presents HotSwap, a novel provider-side cold-start optimization for\nserverless computing. This optimization reduces cold-start time when booting\nand loading dependencies at runtime inside a function container. Previous\nresearch has extensively focused on reducing cold-start latency for specific\nfunctions. However, little attention has been given to skewed production\nworkloads. In such cases, cross-function optimization becomes essential.\nWithout cross-function optimization, a cloud provider is left with two equally\npoor options: (i) Either the cloud provider gives up optimization for each\nfunction in the long tail (which is slow); or (ii) the cloud provider applies\nfunction-specific optimizations (e.g., cache function images) to every function\nin the long tail (which violates the vendor's cache constraints). HotSwap\ndemonstrates cross-function optimization using a novel pre-warming strategy. In\nthis strategy, a pre-initialized live dependency image is migrated to the new\nfunction instance. At the same time, HotSwap respects the provider's cache\nconstraints, because a single pre-warmed dependency image in the cache can be\nshared among all serverless functions that require that image. HotSwap has been\ntested on seven representative functions from FunctionBench. In those tests,\nHotSwap accelerates dependency loading for those serverless functions with\nlarge dependency requirements by a factor ranging from 2.2 to 3.2. Simulation\nexperiments using Azure traces indicate that HotSwap can save 88\\% of space,\ncompared with a previous function-specific method, PreBaking, when sharing a\ndependency image among ten different functions.", "comment": "10 pages, 7 figures. This work was accepted at the IEEE International\n  Conference on Cloud Computing 2025", "pdf_url": "http://arxiv.org/pdf/2409.09202v3", "cate": "cs.DC", "date": "2024-09-13", "updated": "2025-07-11"}
{"id": "2502.06842", "title": "Integrating Generative Artificial Intelligence in ADRD: A Roadmap for Streamlining Diagnosis and Care in Neurodegenerative Diseases", "authors": ["Andrew G. Breithaupt", "Michael Weiner", "Alice Tang", "Katherine L. Possin", "Marina Sirota", "James Lah", "Allan I. Levey", "Pascal Van Hentenryck", "Reza Zandehshahvar", "Marilu Luisa Gorno-Tempini", "Joseph Giorgio", "Jingshen Wang", "Andreas M. Rauschecker", "Howard J. Rosen", "Rachel L. Nosheny", "Bruce L. Miller", "Pedro Pinheiro-Chagas"], "categories": ["cs.CY", "cs.AI", "I.2.1"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      27 pages, 2 figures, 1 table", "url": "http://arxiv.org/abs/2502.06842v2", "summary": "Healthcare systems are struggling to meet the growing demand for neurological\ncare, particularly in Alzheimer's disease and related dementias (ADRD). We\npropose that LLM-based generative AI systems can enhance clinician capabilities\nto approach specialist-level assessment and decision-making in ADRD care at\nscale. This article presents a comprehensive six-phase roadmap for responsible\ndesign and integration of such systems into ADRD care: (1) high-quality\nstandardized data collection across modalities; (2) decision support; (3)\nclinical integration enhancing workflows; (4) rigorous validation and\nmonitoring protocols; (5) continuous learning through clinical feedback; and\n(6) robust ethics and risk management frameworks. This human centered approach\noptimizes clinicians' capabilities in comprehensive data collection,\ninterpretation of complex clinical information, and timely application of\nrelevant medical knowledge while prioritizing patient safety, healthcare\nequity, and transparency. Though focused on ADRD, these principles offer broad\napplicability across medical specialties facing similar systemic challenges.", "comment": "27 pages, 2 figures, 1 table", "pdf_url": "http://arxiv.org/pdf/2502.06842v2", "cate": "cs.CY", "date": "2025-02-06", "updated": "2025-07-11"}
{"id": "2507.03691", "title": "Noise-robust multi-fidelity surrogate modelling for parametric partial differential equations", "authors": ["Benjamin M. Kent", "Lorenzo Tamellini", "Matteo Giacomini", "Antonio Huerta"], "categories": ["math.NA", "cs.CE", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      35 pages, 31 figures. Corrected Figure 15 (a,b,c)", "url": "http://arxiv.org/abs/2507.03691v2", "summary": "We address the challenge of constructing noise-robust surrogate models for\nquantities of interest (QoIs) arising from parametric partial differential\nequations (PDEs), using multi-fidelity collocation techniques; specifically,\nthe Multi-Index Stochastic Collocation (MISC). In practical scenarios, the PDE\nevaluations used to build a response surface are often corrupted by numerical\nnoise, especially for the low-fidelity models. This noise, which may originate\nfrom loose solver tolerances, coarse discretisations, or transient effects, can\nlead to overfitting in MISC, degrading surrogate quality through nonphysical\noscillations and loss of convergence, thereby limiting its utility in\ndownstream tasks like uncertainty quantification, optimisation, and control. To\ncorrect this behaviour, we propose an improved version of MISC that can\nautomatically detect the presence of solver noise during the surrogate model\nconstruction and then ignore the exhausted fidelities. Our approach monitors\nthe spectral decay of the surrogate at each iteration, identifying stagnation\nin the coefficient spectrum that signals the onset of noise. Once detected, the\nalgorithm selectively halts the use of noisy fidelities, focusing computational\nresources on those fidelities that still provide meaningful information. The\neffectiveness of this approach is numerically validated on two challenging test\ncases: a parabolic advection--diffusion PDE with uncertain coefficients, and a\nparametric turbulent incompressible Navier--Stokes problem. The results\nshowcase the accuracy and robustness of the resulting multi-fidelity surrogate\nand its capability to extract relevant information, even from under-resolved\nmeshes not suitable for reliable single-fidelity computations.", "comment": "35 pages, 31 figures. Corrected Figure 15 (a,b,c)", "pdf_url": "http://arxiv.org/pdf/2507.03691v2", "cate": "math.NA", "date": "2025-07-04", "updated": "2025-07-14"}
{"id": "2310.00911", "title": "Accurate Simulation and Parameter Identification of Deformable Linear Objects using Discrete Elastic Rods in Generalized Coordinates", "authors": ["Qi Jing Chen", "Timothy Bretl", "Quang-Cuong Pham"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      7 pages, 6 figures", "url": "http://arxiv.org/abs/2310.00911v3", "summary": "This paper presents a fast and accurate model of a deformable linear object\n(DLO) -- e.g., a rope, wire, or cable -- integrated into an established robot\nphysics simulator, MuJoCo. Most accurate DLO models with low computational\ntimes exist in standalone numerical simulators, which are unable or require\ntedious work to handle external objects. Based on an existing state-of-the-art\nDLO model -- Discrete Elastic Rods (DER) -- our implementation provides an\nimprovement in accuracy over MuJoCo's own native cable model. To minimize\ncomputational load, our model utilizes force-lever analysis to adapt the\nCartesian stiffness forces of the DER into its generalized coordinates. As a\nkey contribution, we introduce a novel parameter identification pipeline\ndesigned for both simplicity and accuracy, which we utilize to determine the\nbending and twisting stiffness of three distinct DLOs. We then evaluate the\nperformance of each model by simulating the DLOs and comparing them to their\nreal-world counterparts and against theoretically proven validation tests.", "comment": "7 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2310.00911v3", "cate": "cs.RO", "date": "2023-10-02", "updated": "2025-07-12"}
{"id": "2507.04295", "title": "LearnLens: LLM-Enabled Personalised, Curriculum-Grounded Feedback with Educators in the Loop", "authors": ["Runcong Zhao", "Artem Bobrov", "Jiazheng Li", "Yulan He"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.04295v2", "summary": "Effective feedback is essential for student learning but is time-intensive\nfor teachers. We present LearnLens, a modular, LLM-based system that generates\npersonalised, curriculum-aligned feedback in science education. LearnLens\ncomprises three components: (1) an error-aware assessment module that captures\nnuanced reasoning errors; (2) a curriculum-grounded generation module that uses\na structured, topic-linked memory chain rather than traditional\nsimilarity-based retrieval, improving relevance and reducing noise; and (3) an\neducator-in-the-loop interface for customisation and oversight. LearnLens\naddresses key challenges in existing systems, offering scalable, high-quality\nfeedback that empowers both teachers and students.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.04295v2", "cate": "cs.CY", "date": "2025-07-06", "updated": "2025-07-11"}
{"id": "2507.08827", "title": "Advancing network resilience theories with symbolized reinforcement learning", "authors": ["Yu Zheng", "Jingtao Ding", "Depeng Jin", "Jianxi Gao", "Yong Li"], "categories": ["physics.soc-ph", "cs.AI"], "primary_category": "Subjects:       Physics and Society (physics.soc-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08827v1", "summary": "Many complex networks display remarkable resilience under external\nperturbations, internal failures and environmental changes, yet they can\nswiftly deteriorate into dysfunction upon the removal of a few keystone nodes.\nDiscovering theories that measure network resilience offers the potential to\nprevent catastrophic collapses--from species extinctions to financial\ncrise--with profound implications for real-world systems. Current resilience\ntheories address the problem from a single perspective of topology, neglecting\nthe crucial role of system dynamics, due to the intrinsic complexity of the\ncoupling between topology and dynamics which exceeds the capabilities of human\nanalytical methods. Here, we report an automatic method for resilience theory\ndiscovery, which learns from how AI solves a complicated network dismantling\nproblem and symbolizes its network attack strategies into theoretical formulas.\nThis proposed self-inductive approach discovers the first resilience theory\nthat accounts for both topology and dynamics, highlighting how the correlation\nbetween node degree and state shapes overall network resilience, and offering\ninsights for designing early warning signals of systematic collapses.\nAdditionally, our approach discovers formulas that refine existing\nwell-established resilience theories with over 37.5% improvement in accuracy,\nsignificantly advancing human understanding of complex networks with AI.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08827v1", "cate": "physics.soc-ph", "date": "2025-07-04", "updated": "2025-07-04"}
{"id": "2507.09256", "title": "Ambiguity-Aware and High-Order Relation Learning for Multi-Grained Image-Text Matching", "authors": ["Junyu Chen", "Yihua Gao", "Mingyuan Ge", "Mingyong Li"], "categories": ["cs.CV", "cs.IR", "cs.MM"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by the Knowledge-Based Systems(KBS), 2025", "url": "http://arxiv.org/abs/2507.09256v1", "summary": "Image-text matching is crucial for bridging the semantic gap between computer\nvision and natural language processing. However, existing methods still face\nchallenges in handling high-order associations and semantic ambiguities among\nsimilar instances. These ambiguities arise from subtle differences between soft\npositive samples (semantically similar but incorrectly labeled) and soft\nnegative samples (locally matched but globally inconsistent), creating matching\nuncertainties. Furthermore, current methods fail to fully utilize the\nneighborhood relationships among semantically similar instances within training\nbatches, limiting the model's ability to learn high-order shared knowledge.\nThis paper proposes the Ambiguity-Aware and High-order Relation learning\nframework (AAHR) to address these issues. AAHR constructs a unified\nrepresentation space through dynamic clustering prototype contrastive learning,\neffectively mitigating the soft positive sample problem. The framework\nintroduces global and local feature extraction mechanisms and an adaptive\naggregation network, significantly enhancing full-grained semantic\nunderstanding capabilities. Additionally, AAHR employs intra-modal and\ninter-modal correlation matrices to investigate neighborhood relationships\namong sample instances thoroughly. It incorporates GNN to enhance semantic\ninteractions between instances. Furthermore, AAHR integrates momentum\ncontrastive learning to expand the negative sample set. These combined\nstrategies significantly improve the model's ability to discriminate between\nfeatures. Experimental results demonstrate that AAHR outperforms existing\nstate-of-the-art methods on Flickr30K, MSCOCO, and ECCV Caption datasets,\nconsiderably improving the accuracy and efficiency of image-text matching. The\ncode and model checkpoints for this research are available at\nhttps://github.com/Image-Text-Matching/AAHR .", "comment": "Accepted by the Knowledge-Based Systems(KBS), 2025", "pdf_url": "http://arxiv.org/pdf/2507.09256v1", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.10305", "title": "A Grounded Theory on the Teacher and Student Roles in Pair Programming", "authors": ["Linus Ververs", "Trang Linh Lam", "Janina Berger", "Lutz Prechelt"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10305v1", "summary": "Context: Pair programming is an established (agile) practice and is practiced\nthroughout the industry. Objective: Understand under what circumstances\nknowledge transfer can harm a pair programming session. Method: Grounded Theory\nMethodology based on 17 recorded pair programming sessions with 18 developers\nfrom 5 German software companies accompanied, by 6 interviews with different\ndevelopers from 4 other German companies. Results: We define the student and\nteacher roles to help developers deal with a one-sided knowledge gap. We\ndescribe pitfalls to avoid and develop a grounded theory centered around the\nPower Gap in pair programming. Conclusions: Knowledge transfer can be harmful\nwhen developers don't pay attention to their partners needs and desires. If\ndevelopers don't pay attention to the Power Gap and keep it in check, Defensive\nBehavior may arise that leads to a vicious cycle impacting the knowledge\ntransfer, the Togetherness and the code quality in a negative way.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10305v1", "cate": "cs.SE", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09247", "title": "A CLuP algorithm to practically achieve $\\sim 0.76$ SK--model ground state free energy", "authors": ["Mihailo Stojnic"], "categories": ["cond-mat.dis-nn", "cs.IT", "math.IT", "math.OC", "stat.ML"], "primary_category": "Subjects:       Disordered Systems and Neural Networks (cond-mat.dis-nn)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09247v1", "summary": "We consider algorithmic determination of the $n$-dimensional\nSherrington-Kirkpatrick (SK) spin glass model ground state free energy. It\ncorresponds to a binary maximization of an indefinite quadratic form and under\nthe \\emph{worst case} principles of the classical NP complexity theory it is\nhard to approximate within a $\\log(n)^{const.}$ factor. On the other hand, the\nSK's random nature allows (polynomial) spectral methods to \\emph{typically}\napproach the optimum within a constant factor. Naturally one is left with the\nfundamental question: can the residual (constant) \\emph{computational gap} be\nerased?\n  Following the success of \\emph{Controlled Loosening-up} (CLuP) algorithms in\nplanted models, we here devise a simple practical CLuP-SK algorithmic procedure\nfor (non-planted) SK models. To analyze the \\emph{typical} success of the\nalgorithm we associate to it (random) CLuP-SK models. Further connecting to\nrecent random processes studies [94,97], we characterize the models and CLuP-SK\nalgorithm via fully lifted random duality theory (fl RDT) [98]. Moreover,\nrunning the algorithm we demonstrate that its performance is in an excellent\nagrement with theoretical predictions. In particular, already for $n$ on the\norder of a few thousands CLuP-SK achieves $\\sim 0.76$ ground state free energy\nand remarkably closely approaches theoretical $n\\rightarrow\\infty$ limit\n$\\approx 0.763$. For all practical purposes, this renders computing SK model's\nnear ground state free energy as a \\emph{typically} easy problem.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09247v1", "cate": "cond-mat.dis-nn", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2504.03668", "title": "Intelligent Orchestration of Distributed Large Foundation Model Inference at the Edge", "authors": ["Fernando Koch", "Aladin Djuhera", "Alecio Binotto"], "categories": ["cs.DC", "cs.LG"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      26 pages, 3 figures, 4 tables, 52 references", "url": "http://arxiv.org/abs/2504.03668v3", "summary": "Large Foundation Models (LFMs), including multi-modal and generative models,\npromise to unlock new capabilities for next-generation Edge AI applications.\nHowever, performing inference with LFMs in resource-constrained and\nheterogeneous edge environments, such as Multi-access Edge Computing (MEC),\npresents significant challenges for workload orchestration due to time-varying\nnetwork, compute, and storage conditions. In particular, current split\ninference strategies, which partition LFM layers across nodes, are not designed\nto adapt to fluctuating workloads, dynamic bandwidth conditions, or evolving\nprivacy constraints in high-utilization MEC environments. In this work, we\npropose a novel adaptive split inference orchestration framework that elevates\nboth the placement and partitioning of LFM layers to runtime-tunable variables.\nSpecifically, our framework enables real-time, quality-of-service (QoS)-aware\nmanagement of inference workloads by extending conventional orchestrators with\nthree key services: (1) Capacity-aware workload distribution, which\ncontinuously profiles node resources and selects an optimal subset of MEC\nnodes; (2) Dynamic partition migration, which transparently relocates pre-cut\nLFM segments in response to changes in utilization or network conditions; (3)\nReal-time reconfiguration, which dynamically re-splits LFM layers to balance\nlatency, throughput, and privacy. We formalize the joint placement-partitioning\nproblem, outline a reference architecture and algorithmic workflow, and discuss\napplicability in representative smart city, V2X, and industrial edge scenarios.", "comment": "26 pages, 3 figures, 4 tables, 52 references", "pdf_url": "http://arxiv.org/pdf/2504.03668v3", "cate": "cs.DC", "date": "2025-03-19", "updated": "2025-07-12"}
{"id": "2503.13554", "title": "LLMs' Leaning in European Elections", "authors": ["Federico Ricciuti"], "categories": ["cs.CY", "cs.AI"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.13554v2", "summary": "Many studies suggest that LLMs have left wing leans. The article extends\nprevious analysis of US presidential elections considering several virtual\nelections in multiple European countries. The analysis considers multiple LLMs\nand the results confirm the extent of the leaning. Furthermore, the results\nshow that the leaning is not uniform between countries. Sometimes, models\nrefuse to take a position in the virtual elections, but the refusal rate itself\nis not uniform between countries.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.13554v2", "cate": "cs.CY", "date": "2025-03-16", "updated": "2025-07-12"}
{"id": "2405.01440", "title": "A Review of Reward Functions for Reinforcement Learning in the context of Autonomous Driving", "authors": ["Ahmed Abouelazm", "Jonas Michel", "J. Marius Zoellner"], "categories": ["cs.RO", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted at the 35th IEEE Intelligent Vehicles Symposium (IV 2024)", "url": "http://arxiv.org/abs/2405.01440v2", "summary": "Reinforcement learning has emerged as an important approach for autonomous\ndriving. A reward function is used in reinforcement learning to establish the\nlearned skill objectives and guide the agent toward the optimal policy. Since\nautonomous driving is a complex domain with partly conflicting objectives with\nvarying degrees of priority, developing a suitable reward function represents a\nfundamental challenge. This paper aims to highlight the gap in such function\ndesign by assessing different proposed formulations in the literature and\ndividing individual objectives into Safety, Comfort, Progress, and Traffic\nRules compliance categories. Additionally, the limitations of the reviewed\nreward functions are discussed, such as objectives aggregation and indifference\nto driving context. Furthermore, the reward categories are frequently\ninadequately formulated and lack standardization. This paper concludes by\nproposing future research that potentially addresses the observed shortcomings\nin rewards, including a reward validation framework and structured rewards that\nare context-aware and able to resolve conflicts.", "comment": "Accepted at the 35th IEEE Intelligent Vehicles Symposium (IV 2024)", "pdf_url": "http://arxiv.org/pdf/2405.01440v2", "cate": "cs.RO", "date": "2024-04-12", "updated": "2025-07-12"}
{"id": "2507.07216", "title": "Bias-Aware Mislabeling Detection via Decoupled Confident Learning", "authors": ["Yunyi Li", "Maria De-Arteaga", "Maytal Saar-Tsechansky"], "categories": ["cs.LG", "cs.AI", "cs.DB", "cs.HC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07216v2", "summary": "Reliable data is a cornerstone of modern organizational systems. A notable\ndata integrity challenge stems from label bias, which refers to systematic\nerrors in a label, a covariate that is central to a quantitative analysis, such\nthat its quality differs across social groups. This type of bias has been\nconceptually and empirically explored and is widely recognized as a pressing\nissue across critical domains. However, effective methodologies for addressing\nit remain scarce. In this work, we propose Decoupled Confident Learning\n(DeCoLe), a principled machine learning based framework specifically designed\nto detect mislabeled instances in datasets affected by label bias, enabling\nbias aware mislabelling detection and facilitating data quality improvement. We\ntheoretically justify the effectiveness of DeCoLe and evaluate its performance\nin the impactful context of hate speech detection, a domain where label bias is\na well documented challenge. Empirical results demonstrate that DeCoLe excels\nat bias aware mislabeling detection, consistently outperforming alternative\napproaches for label error detection. Our work identifies and addresses the\nchallenge of bias aware mislabeling detection and offers guidance on how DeCoLe\ncan be integrated into organizational data management practices as a powerful\ntool to enhance data reliability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07216v2", "cate": "cs.LG", "date": "2025-07-09", "updated": "2025-07-11"}
{"id": "2507.08829", "title": "Efficient Triple Modular Redundancy for Reliability Enhancement of DNNs Using Explainable AI", "authors": ["Kimia Soroush", "Nastaran Shirazi", "Mohsen Raji"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08829v1", "summary": "Deep Neural Networks (DNNs) are widely employed in safety-critical domains,\nwhere ensuring their reliability is essential. Triple Modular Redundancy (TMR)\nis an effective technique to enhance the reliability of DNNs in the presence of\nbit-flip faults. In order to handle the significant overhead of TMR, it is\napplied selectively on the parameters and components with the highest\ncontribution at the model output. Hence, the accuracy of the selection\ncriterion plays the key role on the efficiency of TMR. This paper presents an\nefficient TMR approach to enhance the reliability of DNNs against bit-flip\nfaults using an Explainable Artificial Intelligence (XAI) method. Since XAI can\nprovide valuable insights about the importance of individual neurons and\nweights in the performance of the network, they can be applied as the selection\nmetric in TMR techniques. The proposed method utilizes a low-cost,\ngradient-based XAI technique known as Layer-wise Relevance Propagation (LRP) to\ncalculate importance scores for DNN parameters. These scores are then used to\nenhance the reliability of the model, with the most critical weights being\nprotected by TMR. The proposed approach is evaluated on two DNN models, VGG16\nand AlexNet, using datasets such as MNIST and CIFAR-10. The results demonstrate\nthat the method can protect the AlexNet model at a bit error rate of 10-4,\nachieving over 60% reliability improvement while maintaining the same overhead\nas state-of-the-art methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08829v1", "cate": "cs.LG", "date": "2025-07-04", "updated": "2025-07-04"}
{"id": "2507.09266", "title": "SAGE: Segment-Aware Gloss-Free Encoding for Token-Efficient Sign Language Translation", "authors": ["JianHe Low", "Ozge Mercanoglu Sincan", "Richard Bowden"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted in International Conference on Computer Vision (ICCV) Workshops", "url": "http://arxiv.org/abs/2507.09266v1", "summary": "Gloss-free Sign Language Translation (SLT) has advanced rapidly, achieving\nstrong performances without relying on gloss annotations. However, these gains\nhave often come with increased model complexity and high computational demands,\nraising concerns about scalability, especially as large-scale sign language\ndatasets become more common. We propose a segment-aware visual tokenization\nframework that leverages sign segmentation to convert continuous video into\ndiscrete, sign-informed visual tokens. This reduces input sequence length by up\nto 50% compared to prior methods, resulting in up to 2.67x lower memory usage\nand better scalability on larger datasets. To bridge the visual and linguistic\nmodalities, we introduce a token-to-token contrastive alignment objective,\nalong with a dual-level supervision that aligns both language embeddings and\nintermediate hidden states. This improves fine-grained cross-modal alignment\nwithout relying on gloss-level supervision. Our approach notably exceeds the\nperformance of state-of-the-art methods on the PHOENIX14T benchmark, while\nsignificantly reducing sequence length. Further experiments also demonstrate\nour improved performance over prior work under comparable sequence-lengths,\nvalidating the potential of our tokenization and alignment strategies.", "comment": "Accepted in International Conference on Computer Vision (ICCV)\n  Workshops", "pdf_url": "http://arxiv.org/pdf/2507.09266v1", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.10321", "title": "Streamlined Airborne Software Development for Large UAVs: From Unified Data Collection to Automated Code Generation", "authors": ["Viktor Sinitsyn", "Nils Schlautmann", "Florian Schwaiger", "Florian Holzapfel"], "categories": ["cs.SE", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10321v1", "summary": "The aerospace industry has experienced significant transformations over the\nlast decade, driven by technological advancements and innovative solutions in\ngoods and personal transportation. This evolution has spurred the emergence of\nnumerous start-ups that now face challenges traditionally encountered by\nestablished aerospace companies. Among these challenges is the efficient\nprocessing of digital intra-device communication interfaces for onboard\nequipment - a critical component for ensuring seamless system integration and\nfunctionality. Addressing this challenge requires solutions that emphasize\nclear and consistent interface descriptions, automation of processes, and\nreduced labor-intensive efforts.\n  This paper presents a novel process and toolchain designed to streamline the\ndevelopment of digital interfaces and onboard software, which our team has\nsuccessfully applied in several completed projects. The proposed approach\nfocuses on automation and flexibility while maintaining compliance with design\nassurance requirements.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10321v1", "cate": "cs.SE", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09698", "title": "Metric complexity is a Bryant--Tupper diversity", "authors": ["Gautam Aishwarya", "Dongbin Li", "Mokshay Madiman", "Mark Meckes"], "categories": ["math.MG", "cs.IT", "math.IT", "51F99, 94A17, 54E35"], "primary_category": "Subjects:       Metric Geometry (math.MG)", "pdf_link": null, "comments": "Comments:      11 pages, comments welcome!", "url": "http://arxiv.org/abs/2507.09698v1", "summary": "The metric complexity (sometimes called Leinster--Cobbold maximum diversity)\nof a compact metric space is a recently introduced isometry-invariant of\ncompact metric spaces which generalizes the notion of cardinality, and can be\nthought of as a metric-sensitive analogue of maximum entropy. On the other\nhand, the notion of diversity introduced by Bryant and Tupper is an assignment\nof a real number to every finite subset of a fixed set, which generalizes the\nnotion of a metric. We establish a connection between these concepts by showing\nthat the former quantity naturally produces an example of the latter. Moreover,\nin contrast to several examples in the literature, the diversity that arises\nfrom metric complexity is Minkowski-superlinear for compact subsets of the real\nline.", "comment": "11 pages, comments welcome!", "pdf_url": "http://arxiv.org/pdf/2507.09698v1", "cate": "math.MG", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2506.23395", "title": "FastSet: Parallel Claim Settlement", "authors": ["Xiaohong Chen", "Grigore Rosu"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.23395v3", "summary": "FastSet is a distributed protocol for decentralized finance and settlement,\nwhich is inspired from both actors and blockchains. Account holders cooperate\nby making claims, which can include payments, holding and transferring assets,\naccessing and updating shared data, medical records, digital identity, and\nmathematical theorems, among others. The claims are signed by their owners and\nare broadcast to a decentralized network of validators, which validate and\nsettle them. Validators replicate the global state of the accounts and need not\ncommunicate with each other. In sharp contrast to blockchains, strong\nconsistency is purposely given up as a requirement. Yet, many if not most of\nthe blockchain benefits are preserved, while capitalizing on actor's massive\nparallelism. The protocol is proved to be correct, despite its massively\nparallel nature.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23395v3", "cate": "cs.DC", "date": "2025-06-29", "updated": "2025-07-13"}
{"id": "2504.00955", "title": "Unfair Learning: GenAI Exceptionalism and Copyright Law", "authors": ["David Atkinson"], "categories": ["cs.CY", "cs.AI"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.00955v2", "summary": "This paper challenges the argument that generative artificial intelligence\n(GenAI) is entitled to broad immunity from copyright law for reproducing\ncopyrighted works without authorization due to a fair use defense. It examines\nfair use legal arguments and eight distinct substantive arguments, contending\nthat every legal and substantive argument favoring fair use for GenAI applies\nequally, if not more so, to humans. Therefore, granting GenAI exceptional\nprivileges in this domain is legally and logically inconsistent with\nwithholding broad fair use exemptions from individual humans. It would mean no\nhuman would need to pay for virtually any copyright work again. The solution is\nto take a circumspect view of any fair use claim for mass copyright\nreproduction by any entity and focus on the first principles of whether\npermitting such exceptionalism for GenAI promotes science and the arts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.00955v2", "cate": "cs.CY", "date": "2025-04-01", "updated": "2025-07-14"}
{"id": "2406.17279", "title": "Learning Decentralized Multi-Biped Control for Payload Transport", "authors": ["Bikram Pandit", "Ashutosh Gupta", "Mohitvishnu S. Gadde", "Addison Johnson", "Aayam Kumar Shrestha", "Helei Duan", "Jeremy Dao", "Alan Fern"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Submitted to CoRL 2024, Project website: this http URL", "url": "http://arxiv.org/abs/2406.17279v2", "summary": "Payload transport over flat terrain via multi-wheel robot carriers is\nwell-understood, highly effective, and configurable. In this paper, our goal is\nto provide similar effectiveness and configurability for transport over rough\nterrain that is more suitable for legs rather than wheels. For this purpose, we\nconsider multi-biped robot carriers, where wheels are replaced by multiple\nbipedal robots attached to the carrier. Our main contribution is to design a\ndecentralized controller for such systems that can be effectively applied to\nvarying numbers and configurations of rigidly attached bipedal robots without\nretraining. We present a reinforcement learning approach for training the\ncontroller in simulation that supports transfer to the real world. Our\nexperiments in simulation provide quantitative metrics showing the\neffectiveness of the approach over a wide variety of simulated transport\nscenarios. In addition, we demonstrate the controller in the real-world for\nsystems composed of two and three Cassie robots. To our knowledge, this is the\nfirst example of a scalable multi-biped payload transport system.", "comment": "Submitted to CoRL 2024, Project website: decmbc.github.io", "pdf_url": "http://arxiv.org/pdf/2406.17279v2", "cate": "cs.RO", "date": "2024-06-25", "updated": "2025-07-14"}
{"id": "2507.07610", "title": "SpatialViz-Bench: Automatically Generated Spatial Visualization Reasoning Tasks for MLLMs", "authors": ["Siting Wang", "Luoyang Sun", "Cheng Deng", "Kun Shao", "Minnan Pei", "Zheng Tian", "Haifeng Zhang", "Jun Wang"], "categories": ["cs.CV", "cs.CL", "cs.HC"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07610v2", "summary": "Humans can directly imagine and manipulate visual images in their minds, a\ncapability known as spatial visualization. While multi-modal Large Language\nModels (MLLMs) support imagination-based reasoning, spatial visualization\nremains insufficiently evaluated, typically embedded within broader\nmathematical and logical assessments. Existing evaluations often rely on IQ\ntests or math competitions that may overlap with training data, compromising\nassessment reliability. To this end, we introduce SpatialViz-Bench, a\ncomprehensive multi-modal benchmark for spatial visualization with 12 tasks\nacross 4 sub-abilities, comprising 1,180 automatically generated problems. Our\nevaluation of 33 state-of-the-art MLLMs not only reveals wide performance\nvariations and demonstrates the benchmark's strong discriminative power, but\nalso uncovers counter-intuitive findings: models exhibit unexpected behaviors\nby showing difficulty perception that misaligns with human intuition,\ndisplaying dramatic 2D-to-3D performance cliffs, and defaulting to formula\nderivation despite spatial tasks requiring visualization alone. SpatialVizBench\nempirically demonstrates that state-of-the-art MLLMs continue to exhibit\ndeficiencies in spatial visualization tasks, thereby addressing a significant\nlacuna in the field. The benchmark is publicly available.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07610v2", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-14"}
{"id": "2507.08833", "title": "LoRA Is Slower Than You Think", "authors": ["Seokmin Ko"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08833v1", "summary": "Low-Rank Adaptation (LoRA) is one of the most widely used techniques for\nfine-tuning large language models (LLMs). By introducing a small number of\ntrainable low-rank weight matrices, LoRA substantially reduces the number of\nparameters that need to be updated, offering significant advantages in memory\nconsumption and computational efficiency compared to full fine-tuning. However,\nwe observed that LoRA does not consistently provide speed improvements across\nall model architectures and training setups. Motivated by this inconsistency,\nwe conduct a comprehensive analysis of LoRA's performance and investigate the\nunderlying factors limiting its speedup. Based on our findings, we propose\nseveral methods for more efficient fine-tuning of LLMs. We empirically evaluate\nthese methods and compare them to LoRA, demonstrating that our approach\nachieves comparable or superior performance while delivering more consistent\ntraining speed improvements. Our work offers valuable insights and practical\nguidelines for practitioners seeking to optimize LLM fine-tuning under resource\nconstraints.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08833v1", "cate": "cs.LG", "date": "2025-07-06", "updated": "2025-07-06"}
{"id": "2507.09269", "title": "Cross Knowledge Distillation between Artificial and Spiking Neural Networks", "authors": ["Shuhan Ye", "Yuanbin Qian", "Chong Wang", "Sunqi Lin", "Jiazhen Xu", "Jiangbo Qian", "Yuqi Li"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      This paper has been accepted by ICME2025", "url": "http://arxiv.org/abs/2507.09269v1", "summary": "Recently, Spiking Neural Networks (SNNs) have demonstrated rich potential in\ncomputer vision domain due to their high biological plausibility, event-driven\ncharacteristic and energy-saving efficiency. Still, limited annotated\nevent-based datasets and immature SNN architectures result in their performance\ninferior to that of Artificial Neural Networks (ANNs). To enhance the\nperformance of SNNs on their optimal data format, DVS data, we explore using\nRGB data and well-performing ANNs to implement knowledge distillation. In this\ncase, solving cross-modality and cross-architecture challenges is necessary. In\nthis paper, we propose cross knowledge distillation (CKD), which not only\nleverages semantic similarity and sliding replacement to mitigate the\ncross-modality challenge, but also uses an indirect phased knowledge\ndistillation to mitigate the cross-architecture challenge. We validated our\nmethod on main-stream neuromorphic datasets, including N-Caltech101 and\nCEP-DVS. The experimental results show that our method outperforms current\nState-of-the-Art methods. The code will be available at\nhttps://github.com/ShawnYE618/CKD", "comment": "This paper has been accepted by ICME2025", "pdf_url": "http://arxiv.org/pdf/2507.09269v1", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.10422", "title": "Self-Admitted GenAI Usage in Open-Source Software", "authors": ["Tao Xiao", "Youmei Fan", "Fabio Calefato", "Christoph Treude", "Raula Gaikovina Kula", "Hideaki Hata", "Sebastian Baltes"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      17 pages, 8 tables, 1 figures, currently under review", "url": "http://arxiv.org/abs/2507.10422v1", "summary": "The widespread adoption of generative AI (GenAI) tools such as GitHub Copilot\nand ChatGPT is transforming software development. Since generated source code\nis virtually impossible to distinguish from manually written code, their\nreal-world usage and impact on open-source software development remain poorly\nunderstood. In this paper, we introduce the concept of self-admitted GenAI\nusage, that is, developers explicitly referring to the use of GenAI tools for\ncontent creation in software artifacts. Using this concept as a lens to study\nhow GenAI tools are integrated into open-source software projects, we analyze a\ncurated sample of more than 250,000 GitHub repositories, identifying 1,292 such\nself-admissions across 156 repositories in commit messages, code comments, and\nproject documentation. Using a mixed methods approach, we derive a taxonomy of\n32 tasks, 10 content types, and 11 purposes associated with GenAI usage based\non 284 qualitatively coded mentions. We then analyze 13 documents with policies\nand usage guidelines for GenAI tools and conduct a developer survey to uncover\nthe ethical, legal, and practical concerns behind them. Our findings reveal\nthat developers actively manage how GenAI is used in their projects,\nhighlighting the need for project-level transparency, attribution, and quality\ncontrol practices in the new era of AI-assisted software development. Finally,\nwe examine the longitudinal impact of GenAI adoption on code churn in 151\nrepositories with self-admitted GenAI usage and find no general increase,\ncontradicting popular narratives on the impact of GenAI on software\ndevelopment.", "comment": "17 pages, 8 tables, 1 figures, currently under review", "pdf_url": "http://arxiv.org/pdf/2507.10422v1", "cate": "cs.SE", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09894", "title": "Precoded Zak-OTFS for Per-Carrier Equalization", "authors": ["Saif Khan Mohammed", "Amit Kumar Pathak", "Muhammad Ubadah", "Ronny Hadani", "Ananthanarayanan Chockalingam", "Robert Calderbank"], "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09894v1", "summary": "In Zak-OTFS (orthogonal time frequency space) modulation the carrier waveform\nis a pulse in the delay-Doppler (DD) domain, formally a quasi-periodic\nlocalized function with specific periods along delay and Doppler. When the\nchannel delay spread is less than the delay period, and the channel Doppler\nspread is less than the Doppler period, the response to a single Zak-OTFS\ncarrier provides an image of the scattering environment and can be used to\npredict the effective channel at all other carriers. The image of the\nscattering environment changes slowly, making it possible to employ precoding\nat the transmitter. Precoding techniques were developed more than thirty years\nago for wireline modem channels (V.34 standard) defined by linear convolution\nwhere a pulse in the time domain (TD) is used to probe the one-dimensional\npartial response channel. The action of a doubly spread channel on Zak-OTFS\nmodulation determines a two-dimensional partial response channel defined by\ntwisted convolution, and we develop a novel precoding technique for this\nchannel. The proposed precoder leads to separate equalization of each DD\ncarrier which has significantly lower complexity than joint equalization of all\ncarriers. Further, the effective precoded channel results in non-interfering DD\ncarriers which significantly reduces the overhead of guard carriers separating\ndata and pilot carriers, which improves the spectral efficiency significantly.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09894v1", "cate": "eess.SP", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.02620", "title": "FlowSpec: Continuous Pipelined Speculative Decoding for Efficient Distributed LLM Inference", "authors": ["Xing Liu", "Lizhuo Luo", "Ming Tang", "Chao Huang"], "categories": ["cs.DC", "cs.AI"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      16 pages, and the last 3 are appendix", "url": "http://arxiv.org/abs/2507.02620v2", "summary": "Distributed inference serves as a promising approach to enabling the\ninference of large language models (LLMs) at the network edge. It distributes\nthe inference process to multiple devices to ensure that the LLMs can fit into\nthe device memory. Recent pipeline-based approaches have the potential to\nparallelize communication and computation, which helps reduce inference\nlatency. However, the benefit diminishes when the inference request at the\nnetwork edge is sparse, where pipeline is typically at low utilization. To\nenable efficient distributed LLM inference at the edge, we propose\n\\textbf{FlowSpec}, a pipeline-parallel tree-based speculative decoding\nframework. FlowSpec incorporates three key mechanisms to improve decoding\nefficiency: 1) score-based step-wise verification prioritizes more important\ndraft tokens to bring earlier accpeted tokens; 2) efficient draft management to\nprune invalid tokens while maintaining correct causal relationship during\nverification; 3) dynamic draft expansion strategies to supply high-quality\nspeculative inputs. These techniques work in concert to enhance both pipeline\nutilization and speculative efficiency. We evaluate FlowSpec on a real-world\ntestbed with other baselines. Experimental results demonstrate that our\nproposed framework significantly improves inference speed across diverse models\nand configurations, achieving speedup ratios 1.28$\\times$-1.79$\\times$ compared\nto baselines. Our code is publicly available at\n\\href{https://github.com/Leosang-lx/FlowSpec#}{https://github.com/Leosang-lx/FlowSpec\\#}", "comment": "16 pages, and the last 3 are appendix", "pdf_url": "http://arxiv.org/pdf/2507.02620v2", "cate": "cs.DC", "date": "2025-07-03", "updated": "2025-07-14"}
{"id": "2302.04525", "title": "An Epistemic and Aleatoric Decomposition of Arbitrariness to Constrain the Set of Good Models", "authors": ["Falaah Arif Khan", "Denys Herasymuk", "Nazar Protsiv", "Julia Stoyanovich"], "categories": ["cs.LG", "cs.AI", "cs.CY"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2302.04525v2", "summary": "Recent research reveals that machine learning (ML) models are highly\nsensitive to minor changes in their training procedure, such as the inclusion\nor exclusion of a single data point, leading to conflicting predictions on\nindividual data points; a property termed as arbitrariness or instability in ML\npipelines in prior work. Drawing from the uncertainty literature, we show that\nstability decomposes into epistemic and aleatoric components, capturing the\nconsistency and confidence in prediction, respectively. We use this\ndecomposition to provide two main contributions. Our first contribution is an\nextensive empirical evaluation. We find that (i) epistemic instability can be\nreduced with more training data whereas aleatoric instability cannot; (ii)\nstate-of-the-art ML models have aleatoric instability as high as 79% and\naleatoric instability disparities among demographic groups as high as 29% in\npopular fairness benchmarks; and (iii) fairness pre-processing interventions\ngenerally increase aleatoric instability more than in-processing interventions,\nand both epistemic and aleatoric instability are highly sensitive to\ndata-processing interventions and model architecture. Our second contribution\nis a practical solution to the problem of systematic arbitrariness. We propose\na model selection procedure that includes epistemic and aleatoric criteria\nalongside existing accuracy and fairness criteria, and show that it\nsuccessfully narrows down a large set of good models (50-100 on our datasets)\nto a handful of stable, fair and accurate ones. We built and publicly released\na python library to measure epistemic and aleatoric multiplicity in any ML\npipeline alongside existing confusion-matrix-based metrics, providing\npractitioners with a rich suite of evaluation metrics to use to define a more\nprecise criterion during model selection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2302.04525v2", "cate": "cs.LG", "date": "2023-02-09", "updated": "2025-07-12"}
{"id": "2408.00486", "title": "SF-TIM: A Simple Framework for Enhancing Quadrupedal Robot Jumping Agility by Combining Terrain Imagination and Measurement", "authors": ["Ze Wang", "Yang Li", "Long Xu", "Hao Shi", "Zunwang Ma", "Zhen Chu", "Chao Li", "Fei Gao", "Kailun Yang", "Kaiwei Wang"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted to IROS 2025. A demo video has been made available at this https URL", "url": "http://arxiv.org/abs/2408.00486v2", "summary": "Dynamic jumping on high platforms and over gaps differentiates legged robots\nfrom wheeled counterparts. Dynamic locomotion on abrupt surfaces, as opposed to\nwalking on rough terrains, demands the integration of proprioceptive and\nexteroceptive perception to enable explosive movements. In this paper, we\npropose SF-TIM (Simple Framework combining Terrain Imagination and\nMeasurement), a single-policy method that enhances quadrupedal robot jumping\nagility, while preserving their fundamental blind walking capabilities. In\naddition, we introduce a terrain-guided reward design specifically to assist\nquadrupedal robots in high jumping, improving their performance in this task.\nTo narrow the simulation-to-reality gap in quadrupedal robot learning, we\nintroduce a stable and high-speed elevation map generation framework, enabling\nzero-shot simulation-to-reality transfer of locomotion ability. Our algorithm\nhas been deployed and validated on both the small-/large-size quadrupedal\nrobots, demonstrating its effectiveness in real-world applications: the robot\nhas successfully traversed various high platforms and gaps, showing the\nrobustness of our proposed approach. A demo video has been made available at\nhttps://flysoaryun.github.io/SF-TIM.", "comment": "Accepted to IROS 2025. A demo video has been made available at\n  https://flysoaryun.github.io/SF-TIM", "pdf_url": "http://arxiv.org/pdf/2408.00486v2", "cate": "cs.RO", "date": "2024-08-01", "updated": "2025-07-14"}
{"id": "2507.08835", "title": "Representation learning with a transformer by contrastive learning for money laundering detection", "authors": ["Harold Guéneau", "Alain Celisse", "Pascal Delange"], "categories": ["cs.LG", "cs.AI", "math.ST", "q-fin.RM", "q-fin.ST", "stat.TH"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08835v1", "summary": "The present work tackles the money laundering detection problem. A new\nprocedure is introduced which exploits structured time series of both\nqualitative and quantitative data by means of a transformer neural network. The\nfirst step of this procedure aims at learning representations of time series\nthrough contrastive learning (without any labels). The second step leverages\nthese representations to generate a money laundering scoring of all\nobservations. A two-thresholds approach is then introduced, which ensures a\ncontrolled false-positive rate by means of the Benjamini-Hochberg (BH)\nprocedure. Experiments confirm that the transformer is able to produce general\nrepresentations that succeed in exploiting money laundering patterns with\nminimal supervision from domain experts. It also illustrates the higher ability\nof the new procedure for detecting nonfraudsters as well as fraudsters, while\nkeeping the false positive rate under control. This greatly contrasts with\nrule-based procedures or the ones based on LSTM architectures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08835v1", "cate": "cs.LG", "date": "2025-07-07", "updated": "2025-07-07"}
{"id": "2507.09279", "title": "Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models", "authors": ["Anita Kriz", "Elizabeth Laura Janes", "Xing Shen", "Tal Arbel"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Preprint version. The peer-reviewed version of this paper has been accepted to ICCV 2025 Workshop CVAMD", "url": "http://arxiv.org/abs/2507.09279v1", "summary": "Multimodal large language models (MLLMs) hold considerable promise for\napplications in healthcare. However, their deployment in safety-critical\nsettings is hindered by two key limitations: (i) sensitivity to prompt design,\nand (ii) a tendency to generate incorrect responses with high confidence. As\nclinicians may rely on a model's stated confidence to gauge the reliability of\nits predictions, it is especially important that when a model expresses high\nconfidence, it is also highly accurate. We introduce Prompt4Trust, the first\nreinforcement learning (RL) framework for prompt augmentation targeting\nconfidence calibration in MLLMs. A lightweight LLM is trained to produce\ncontext-aware auxiliary prompts that guide a downstream task MLLM to generate\nresponses in which the expressed confidence more accurately reflects predictive\naccuracy. Unlike conventional calibration techniques, Prompt4Trust specifically\nprioritizes aspects of calibration most critical for safe and trustworthy\nclinical decision-making. Beyond improvements driven by this clinically\nmotivated calibration objective, our proposed method also improves task\naccuracy, achieving state-of-the-art medical visual question answering (VQA)\nperformance on the PMC-VQA benchmark, which is composed of multiple-choice\nquestions spanning diverse medical imaging modalities. Moreover, our framework\ntrained with a small downstream task MLLM showed promising zero-shot\ngeneralization to larger MLLMs in our experiments, suggesting the potential for\nscalable calibration without the associated computational costs. This work\ndemonstrates the potential of automated yet human-aligned prompt engineering\nfor improving the the trustworthiness of MLLMs in safety critical settings. Our\ncodebase can be found at https://github.com/xingbpshen/vccrl-llm.", "comment": "Preprint version. The peer-reviewed version of this paper has been\n  accepted to ICCV 2025 Workshop CVAMD", "pdf_url": "http://arxiv.org/pdf/2507.09279v1", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.10535", "title": "CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks", "authors": ["Hongchao Jiang", "Yiming Chen", "Yushi Cao", "Hung-yi Lee", "Robby T. Tan"], "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Dataset is available at this https URL", "url": "http://arxiv.org/abs/2507.10535v1", "summary": "Large Language Models (LLMs) have significantly advanced the state-of-the-art\nin various coding tasks. Beyond directly answering user queries, LLMs can also\nserve as judges, assessing and comparing the quality of responses generated by\nother models. Such an evaluation capability is crucial both for benchmarking\ndifferent LLMs and for improving response quality through response ranking.\nHowever, despite the growing adoption of the LLM-as-a-Judge paradigm, its\neffectiveness in coding scenarios remains underexplored due to the absence of\ndedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a\nbenchmark explicitly designed to evaluate the performance of LLM-as-a-Judge\nmodels across three critical coding tasks: code generation, code repair, and\nunit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge\nmodels, we find that recent thinking models significantly outperform\nnon-thinking models on our carefully designed code judging tasks. Notably, even\nrelatively small thinking models, such as Qwen3-8B, can outperform specially\ntrained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still\nexhibit significant randomness in their judgment of coding tasks. For pairwise\njudging tasks, simply changing the order in which responses are presented can\nsubstantially impact accuracy. In addition, when judging code and unit tests\nwritten by different LLMs, LLM-as-a-Judge models also show variance in\nperformance. This sensitivity raises concerns about the reliability and\nconsistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal\nprompting strategies for LLM-as-a-Judge. We find that using pair-wise\ncomparison outperforms scalar point-wise judging. Furthermore, retaining\ncomments and reasoning in the full, unprocessed LLM response leads to improved\njudge performance.", "comment": "Dataset is available at\n  https://huggingface.co/datasets/mattymchen/codejudgebench", "pdf_url": "http://arxiv.org/pdf/2507.10535v1", "cate": "cs.CL", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10395", "title": "Fault-Tolerant Quantum Error Correction for Constant-Excitation Stabilizer Codes under Coherent Noise", "authors": ["Ching-Yi Lai", "Pei-Hao Liou", "Yingkai Ouyang"], "categories": ["quant-ph", "cs.IT", "math.IT"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      15 pages, 6 figures", "url": "http://arxiv.org/abs/2507.10395v1", "summary": "Collective coherent noise poses challenges for fault-tolerant quantum error\ncorrection (FTQEC), as it falls outside the usual stochastic noise models.\nWhile constant excitation (CE) codes can naturally avoid coherent noise, a\ncomplete fault-tolerant framework for the use of these codes under realistic\nnoise models has been elusive. Here, we introduce a complete fault-tolerant\narchitecture for CE CSS codes based on dual-rail concatenation. After showing\nthat transversal CNOT gates violate CE code constraints, we introduce\nCE-preserving logical CNOT gates and modified Shor- and Steane-type syndrome\nextraction schemes using zero-controlled NOT gates and CE-compatible ancilla.\nThis enables fault-tolerant syndrome-extraction circuits fully compatible with\nCE constraints. We also present an extended stabilizer simulation algorithm\nthat efficiently tracks both stochastic and collective coherent noise. Using\nour framework, we identify minimal CE codes, including the $[[12,1,3]]$ and\n$[[14,3,3]]$ codes, and demonstrate that the $[[12,1,3]]$ code achieves strong\nperformance under coherent noise. Our results establish the first complete\nFTQEC framework for CE codes, demonstrating their robustness to coherent noise.\nThis highlights the potential of CE codes as a possible solution for quantum\nprocessors dominated by collective coherent noise.", "comment": "15 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.10395v1", "cate": "quant-ph", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.06011", "title": "ECORE: Energy-Conscious Optimized Routing for Deep Learning Models at the Edge", "authors": ["Daghash K. Alqahtani", "Maria A. Rodriguez", "Muhammad Aamir Cheema", "Hamid Rezatofighi", "Adel N. Toosi"], "categories": ["cs.DC", "cs.CV"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.06011v2", "summary": "Edge computing enables data processing closer to the source, significantly\nreducing latency an essential requirement for real-time vision-based analytics\nsuch as object detection in surveillance and smart city environments. However,\nthese tasks place substantial demands on resource constrained edge devices,\nmaking the joint optimization of energy consumption and detection accuracy\ncritical. To address this challenge, we propose ECORE, a framework that\nintegrates multiple dynamic routing strategies including estimation based\ntechniques and a greedy selection algorithm to direct image processing requests\nto the most suitable edge device-model pair. ECORE dynamically balances energy\nefficiency and detection performance based on object characteristics. We\nevaluate our approach through extensive experiments on real-world datasets,\ncomparing the proposed routers against widely used baseline techniques. The\nevaluation leverages established object detection models (YOLO, SSD,\nEfficientDet) and diverse edge platforms, including Jetson Orin Nano, Raspberry\nPi 4 and 5, and TPU accelerators. Results demonstrate that our proposed\ncontext-aware routing strategies can reduce energy consumption and latency by\n45% and 49%, respectively, while incurring only a 2% loss in detection accuracy\ncompared to accuracy-centric methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.06011v2", "cate": "cs.DC", "date": "2025-07-08", "updated": "2025-07-14"}
{"id": "2502.15873", "title": "Practical Principles for AI Cost and Compute Accounting", "authors": ["Stephen Casper", "Luke Bailey", "Tim Schreier"], "categories": ["cs.AI", "cs.CY"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.15873v2", "summary": "Policymakers increasingly use development cost and compute as proxies for AI\ncapabilities and risks. Recent laws have introduced regulatory requirements\nthat are contingent on specific thresholds. However, technical ambiguities in\nhow to perform this accounting create loopholes that can undermine regulatory\neffectiveness. We propose seven principles for designing AI cost and compute\naccounting standards that (1) reduce opportunities for strategic gaming, (2)\navoid disincentivizing responsible risk mitigation, and (3) enable consistent\nimplementation across companies and jurisdictions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.15873v2", "cate": "cs.AI", "date": "2025-02-21", "updated": "2025-07-12"}
{"id": "2408.11809", "title": "Informed, Constrained, Aligned: A Field Analysis on Degeneracy-aware Point Cloud Registration in the Wild", "authors": ["Turcan Tuna", "Julian Nubert", "Patrick Pfreundschuh", "Cesar Cadena", "Shehryar Khattak", "Marco Hutter"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted to IEEE Transactions on Field Robotics", "url": "http://arxiv.org/abs/2408.11809v3", "summary": "The ICP registration algorithm has been a preferred method for LiDAR-based\nrobot localization for nearly a decade. However, even in modern SLAM solutions,\nICP can degrade and become unreliable in geometrically ill-conditioned\nenvironments. Current solutions primarily focus on utilizing additional sources\nof information, such as external odometry, to either replace the degenerate\ndirections of the optimization solution or add additional constraints in a\nsensor-fusion setup afterward.\n  In response, this work investigates and compares new and existing degeneracy\nmitigation methods for robust LiDAR-based localization and analyzes the\nefficacy of these approaches in degenerate environments for the first time in\nthe literature at this scale. Specifically, this work investigates i) the\neffect of using active or passive degeneracy mitigation methods for the problem\nof ill-conditioned ICP in LiDAR degenerate environments, ii) the evaluation of\nTSVD, inequality constraints, and linear/non-linear Tikhonov regularization for\nthe application of degenerate point cloud registration for the first time.\nFurthermore, a sensitivity analysis for least-squares minimization step of the\nICP problem is carried out to better understand how each method affects the\noptimization and what to expect from each method. The results of the analysis\nare validated through multiple real-world robotic field and simulated\nexperiments. The analysis demonstrates that active optimization degeneracy\nmitigation is necessary and advantageous in the absence of reliable external\nestimate assistance for LiDAR-SLAM, and soft-constrained methods can provide\nbetter results in complex ill-conditioned scenarios with heuristic fine-tuned\nparameters.", "comment": "Accepted to IEEE Transactions on Field Robotics", "pdf_url": "http://arxiv.org/pdf/2408.11809v3", "cate": "cs.RO", "date": "2024-08-21", "updated": "2025-07-14"}
{"id": "2507.08838", "title": "wd1: Weighted Policy Optimization for Reasoning in Diffusion Language Models", "authors": ["Xiaohang Tang", "Rares Dolga", "Sangwoong Yoon", "Ilija Bogunovic"], "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Preprint", "url": "http://arxiv.org/abs/2507.08838v1", "summary": "Improving the reasoning capabilities of diffusion-based large language models\n(dLLMs) through reinforcement learning (RL) remains an open problem. The\nintractability of dLLMs likelihood function necessitates approximating the\ncurrent, old, and reference policy likelihoods at each policy optimization\nstep. This reliance introduces additional computational overhead and lead to\npotentially large bias -- particularly when approximation errors occur in the\ndenominator of policy ratios used for importance sampling. To mitigate these\nissues, we introduce $\\mathtt{wd1}$, a novel policy optimization approach that\nreformulates the objective as a weighted likelihood, requiring only a single\napproximation for the current parametrized policy likelihood. Experiments on\nwidely used reasoning benchmarks demonstrate that $\\mathtt{wd1}$, without\nsupervised fine-tuning (SFT) or any supervised data, outperforms existing RL\nmethods for dLLMs, achieving up to 16% higher accuracy. $\\mathtt{wd1}$ delivers\nadditional computational gains, including reduced training time and fewer\nfunction evaluations (NFEs) per gradient step. These findings, combined with\nthe simplicity of method's implementation and R1-Zero-like training (no SFT),\nposition $\\mathtt{wd1}$ as a more effective and efficient method for applying\nRL to dLLMs reasoning.", "comment": "Preprint", "pdf_url": "http://arxiv.org/pdf/2507.08838v1", "cate": "cs.LG", "date": "2025-07-07", "updated": "2025-07-07"}
{"id": "2507.09285", "title": "Generative Latent Kernel Modeling for Blind Motion Deblurring", "authors": ["Chenhao Ding", "Jiangtao Zhang", "Zongsheng Yue", "Hui Wang", "Qian Zhao", "Deyu Meng"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09285v1", "summary": "Deep prior-based approaches have demonstrated remarkable success in blind\nmotion deblurring (BMD) recently. These methods, however, are often limited by\nthe high non-convexity of the underlying optimization process in BMD, which\nleads to extreme sensitivity to the initial blur kernel. To address this issue,\nwe propose a novel framework for BMD that leverages a deep generative model to\nencode the kernel prior and induce a better initialization for the blur kernel.\nSpecifically, we pre-train a kernel generator based on a generative adversarial\nnetwork (GAN) to aptly characterize the kernel's prior distribution, as well as\na kernel initializer to provide a well-informed and high-quality starting point\nfor kernel estimation. By combining these two components, we constrain the BMD\nsolution within a compact latent kernel manifold, thus alleviating the\naforementioned sensitivity for kernel initialization. Notably, the kernel\ngenerator and initializer are designed to be easily integrated with existing\nBMD methods in a plug-and-play manner, enhancing their overall performance.\nFurthermore, we extend our approach to tackle blind non-uniform motion\ndeblurring without the need for additional priors, achieving state-of-the-art\nperformance on challenging benchmark datasets. The source code is available at\nhttps://github.com/dch0319/GLKM-Deblur.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09285v1", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2402.05256", "title": "IRFuzzer: Specialized Fuzzing for LLVM Backend Code Generation", "authors": ["Yuyang Rong", "Zhanghan Yu", "Zhenkai Weng", "Stephen Neuendorffer", "Hao Chen"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2402.05256v2", "summary": "Modern compilers, such as LLVM, are complex pieces of software. Due to their\ncomplexity, manual testing is unlikely to suffice, yet formal verification is\ndifficult to scale. End-to-end fuzzing can be used, but it has difficulties in\nachieving high coverage of some components of LLVM.\n  In this paper, we implement IRFuzzer to investigate the effectiveness of\nspecialized fuzzing of the LLVM compiler backend. We focus on two approaches to\nimprove the fuzzer: guaranteed input validity using constrained mutations and\nimproved feedback quality. The mutator in IRFuzzer is capable of generating a\nwide range of LLVM IR inputs, including structured control flow, vector types,\nand function definitions. The system instruments coding patterns in the\ncompiler to monitor the execution status of instruction selection. The\ninstrumentation not only provides a new coverage feedback called matcher table\ncoverage, but also provides an architecture specific guidance to the mutator.\n  We show that IRFuzzer is more effective than existing fuzzers by fuzzing on\n29 mature LLVM backend targets. In the process, we reported 74 confirmed new\nbugs in LLVM upstream, out of which 49 have been fixed, five have been back\nported to LLVM 15, showing that specialized fuzzing provides useful and\nactionable insights to LLVM developers.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2402.05256v2", "cate": "cs.SE", "date": "2024-02-07", "updated": "2025-07-14"}
{"id": "2403.07320", "title": "Approaching Rate-Distortion Limits in Neural Compression with Lattice Transform Coding", "authors": ["Eric Lei", "Hamed Hassani", "Shirin Saeedi Bidokhti"], "categories": ["cs.IT", "cs.LG", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2403.07320v2", "summary": "Neural compression has brought tremendous progress in designing lossy\ncompressors with good rate-distortion (RD) performance at low complexity. Thus\nfar, neural compression design involves transforming the source to a latent\nvector, which is then rounded to integers and entropy coded. While this\napproach has been shown to be optimal on a few specific sources, we show that\nit can be highly sub-optimal on synthetic sources whose intrinsic\ndimensionality is greater than one. With integer rounding in the latent space,\nthe quantization regions induced by neural transformations, remain square-like\nand fail to match those of optimal vector quantization. We demonstrate that\nthis phenomenon is due to the choice of scalar quantization in the latent\nspace, and not the transform design. By employing lattice quantization instead,\nwe propose Lattice Transform Coding (LTC) and show that it approximately\nrecovers optimal vector quantization at reasonable complexity. On real-world\nsources, LTC improves upon standard neural compressors. LTC also provides a\nframework that can integrate structurally (near) optimal information-theoretic\ndesigns into lossy compression; examples include block coding, which yields\ncoding gain over optimal one-shot coding and approaches the\nasymptotically-achievable rate-distortion function, as well as nested lattice\nquantization for low complexity fixed-rate coding.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2403.07320v2", "cate": "cs.IT", "date": "2024-03-12", "updated": "2025-07-13"}
{"id": "2507.08348", "title": "Content-Oblivious Leader Election in 2-Edge-Connected Networks", "authors": ["Yi-Jun Chang", "Lyuting Chen", "Haoran Zhou"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08348v2", "summary": "Censor-Hillel, Cohen, Gelles, and Sela (PODC 2022 & Distributed Computing\n2023) studied fully-defective asynchronous networks, where communication\nchannels may suffer an extreme form of alteration errors, rendering messages\ncompletely corrupted. The model is equivalent to content-oblivious computation,\nwhere nodes communicate solely via pulses. They showed that if the network is\n2-edge-connected, then any algorithm for a noiseless setting can be simulated\nin the fully-defective setting; otherwise, no non-trivial computation is\npossible in the fully-defective setting. However, their simulation requires a\npredesignated leader, which they conjectured to be necessary for any\nnon-trivial content-oblivious task.\n  Recently, Frei, Gelles, Ghazy, and Nolin (DISC 2024) refuted this conjecture\nfor the special case of oriented ring topology. They designed two asynchronous\ncontent-oblivious leader election algorithms with message complexity $O(n \\cdot\n\\mathsf{ID}_{\\max})$, where $n$ is the number of nodes and $\\mathsf{ID}_{\\max}$\nis the maximum $\\mathsf{ID}$. The first algorithm stabilizes in unoriented\nrings without termination detection. The second algorithm quiescently\nterminates in oriented rings, thus enabling the execution of the simulation\nalgorithm after leader election.\n  In this work, we present an asynchronous content-oblivious leader election\nalgorithm that quiescently terminates in any 2-edge connected network with\nmessage complexity $O(m \\cdot N \\cdot \\mathsf{ID}_{\\min})$, where $m$ is the\nnumber of edges, $N$ is a known upper bound on the number of nodes, and\n$\\mathsf{ID}_{\\min}$ is the smallest $\\mathsf{ID}$. Combined with the previous\nsimulation result, our finding implies that any algorithm from the noiseless\nsetting can be simulated in the fully-defective setting without assuming a\npreselected leader, entirely refuting the original conjecture.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08348v2", "cate": "cs.DC", "date": "2025-07-11", "updated": "2025-07-14"}
{"id": "2504.11775", "title": "Discrimination-free Insurance Pricing with Privatized Sensitive Attributes", "authors": ["Tianhe Zhang", "Suhan Liu", "Peng Shi"], "categories": ["stat.ML", "cs.CY", "cs.LG", "q-fin.RM"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.11775v2", "summary": "Fairness has emerged as a critical consideration in the landscape of machine\nlearning algorithms, particularly as AI continues to transform decision-making\nacross societal domains. To ensure that these algorithms are free from bias and\ndo not discriminate against individuals based on sensitive attributes such as\ngender and race, the field of algorithmic bias has introduced various fairness\nconcepts, along with methodologies to achieve these notions in different\ncontexts. Despite the rapid advancement, not all sectors have embraced these\nfairness principles to the same extent. One specific sector that merits\nattention in this regard is insurance. Within the realm of insurance pricing,\nfairness is defined through a distinct and specialized framework. Consequently,\nachieving fairness according to established notions does not automatically\nensure fair pricing in insurance. In particular, regulators are increasingly\nemphasizing transparency in pricing algorithms and imposing constraints on\ninsurance companies on the collection and utilization of sensitive consumer\nattributes. These factors present additional challenges in the implementation\nof fairness in pricing algorithms. To address these complexities and comply\nwith regulatory demands, we propose an efficient method for constructing fair\nmodels that are tailored to the insurance domain, using only privatized\nsensitive attributes. Notably, our approach ensures statistical guarantees,\ndoes not require direct access to sensitive attributes, and adapts to varying\ntransparency requirements, addressing regulatory demands while ensuring\nfairness in insurance pricing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.11775v2", "cate": "stat.ML", "date": "2025-04-16", "updated": "2025-07-14"}
{"id": "2408.16370", "title": "LSTP-Nav: Lightweight Spatiotemporal Policy for Map-free Multi-agent Navigation with LiDAR", "authors": ["Xingrong Diao", "Zhirui Sun", "Jianwei Peng", "Jiankun Wang"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2408.16370v4", "summary": "Safe and efficient multi-agent navigation in dynamic environments remains\ninherently challenging, particularly when real-time decision-making is required\non resource-constrained platforms. Ensuring collision-free trajectories while\nadapting to uncertainties without relying on pre-built maps further complicates\nreal-world deployment. To address these challenges, we propose LSTP-Nav, a\nlightweight end-to-end policy for multi-agent navigation that enables map-free\ncollision avoidance in complex environments by directly mapping raw LiDAR point\nclouds to motion commands. At the core of this framework lies LSTP-Net, an\nefficient network that processes raw LiDAR data using a GRU architecture,\nenhanced with attention mechanisms to dynamically focus on critical\nenvironmental features while minimizing computational overhead. Additionally, a\nnovel HS reward optimizes collision avoidance by incorporating angular\nvelocity, prioritizing obstacles along the predicted heading, and enhancing\ntraining stability. To narrow the sim-to-real gap, we develop\nPhysReplay-Simlab, a physics-realistic multi-agent simulator, employs localized\nreplay to mine near-failure experiences. Relying solely on LiDA, LSTP-Nav\nachieves efficient zero-shot sim-to-real transfer on a CPU-only robotic\nplatform, enabling robust navigation in dynamic environments while maintaining\ncomputation frequencies above 40 Hz. Extensive experiments demonstrate that\nLSTP-Nav outperforms baselines with a 9.58\\% higher success rate and a 12.30\\%\nlower collision rate, underscoring its practicality and robustness for\nreal-world applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2408.16370v4", "cate": "cs.RO", "date": "2024-08-29", "updated": "2025-07-11"}
{"id": "2507.08839", "title": "Domain-Adaptive Diagnosis of Lewy Body Disease with Transferability Aware Transformer", "authors": ["Xiaowei Yu", "Jing Zhang", "Tong Chen", "Yan Zhuang", "Minheng Chen", "Chao Cao", "Yanjun Lyu", "Lu Zhang", "Li Su", "Tianming Liu", "Dajiang Zhu"], "categories": ["cs.LG", "cs.AI", "eess.IV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      MICCAI 2025", "url": "http://arxiv.org/abs/2507.08839v1", "summary": "Lewy Body Disease (LBD) is a common yet understudied form of dementia that\nimposes a significant burden on public health. It shares clinical similarities\nwith Alzheimer's disease (AD), as both progress through stages of normal\ncognition, mild cognitive impairment, and dementia. A major obstacle in LBD\ndiagnosis is data scarcity, which limits the effectiveness of deep learning. In\ncontrast, AD datasets are more abundant, offering potential for knowledge\ntransfer. However, LBD and AD data are typically collected from different sites\nusing different machines and protocols, resulting in a distinct domain shift.\nTo effectively leverage AD data while mitigating domain shift, we propose a\nTransferability Aware Transformer (TAT) that adapts knowledge from AD to\nenhance LBD diagnosis. Our method utilizes structural connectivity (SC) derived\nfrom structural MRI as training data. Built on the attention mechanism, TAT\nadaptively assigns greater weights to disease-transferable features while\nsuppressing domain-specific ones, thereby reducing domain shift and improving\ndiagnostic accuracy with limited LBD data. The experimental results demonstrate\nthe effectiveness of TAT. To the best of our knowledge, this is the first study\nto explore domain adaptation from AD to LBD under conditions of data scarcity\nand domain shift, providing a promising framework for domain-adaptive diagnosis\nof rare diseases.", "comment": "MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.08839v1", "cate": "cs.LG", "date": "2025-07-07", "updated": "2025-07-07"}
{"id": "2507.09291", "title": "Supercharging Floorplan Localization with Semantic Rays", "authors": ["Yuval Grader", "Hadar Averbuch-Elor"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at ICCV 2025", "url": "http://arxiv.org/abs/2507.09291v1", "summary": "Floorplans provide a compact representation of the building's structure,\nrevealing not only layout information but also detailed semantics such as the\nlocations of windows and doors. However, contemporary floorplan localization\ntechniques mostly focus on matching depth-based structural cues, ignoring the\nrich semantics communicated within floorplans. In this work, we introduce a\nsemantic-aware localization framework that jointly estimates depth and semantic\nrays, consolidating over both for predicting a structural-semantic probability\nvolume. Our probability volume is constructed in a coarse-to-fine manner: We\nfirst sample a small set of rays to obtain an initial low-resolution\nprobability volume. We then refine these probabilities by performing a denser\nsampling only in high-probability regions and process the refined values for\npredicting a 2D location and orientation angle. We conduct an evaluation on two\nstandard floorplan localization benchmarks. Our experiments demonstrate that\nour approach substantially outperforms state-of-the-art methods, achieving\nsignificant improvements in recall metrics compared to prior works. Moreover,\nwe show that our framework can easily incorporate additional metadata such as\nroom labels, enabling additional gains in both accuracy and efficiency.", "comment": "Accepted at ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.09291v1", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2408.04124", "title": "Investigating Adversarial Attacks in Software Analytics via Machine Learning Explainability", "authors": ["MD Abdul Awal", "Mrigank Rochan", "Chanchal K. Roy"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      This paper has been accepted for publication in Software Quality Journal", "url": "http://arxiv.org/abs/2408.04124v2", "summary": "With the recent advancements in machine learning (ML), numerous ML-based\napproaches have been extensively applied in software analytics tasks to\nstreamline software development and maintenance processes. Nevertheless,\nstudies indicate that despite their potential usefulness, ML models are\nvulnerable to adversarial attacks, which may result in significant monetary\nlosses in these processes. As a result, the ML models' robustness against\nadversarial attacks must be assessed before they are deployed in software\nanalytics tasks. Despite several techniques being available for adversarial\nattacks in software analytics tasks, exploring adversarial attacks using ML\nexplainability is largely unexplored. Therefore, this study aims to investigate\nthe relationship between ML explainability and adversarial attacks to measure\nthe robustness of ML models in software analytics tasks. In addition, unlike\nmost existing attacks that directly perturb input-space, our attack approach\nfocuses on perturbing feature-space. Our extensive experiments, involving six\ndatasets, three ML explainability techniques, and seven ML models, demonstrate\nthat ML explainability can be used to conduct successful adversarial attacks on\nML models in software analytics tasks. This is achieved by modifying only the\ntop 1-3 important features identified by ML explainability techniques.\nConsequently, the ML models under attack fail to accurately predict up to 86.6%\nof instances that were correctly predicted before adversarial attacks,\nindicating the models' low robustness against such attacks. Finally, our\nproposed technique demonstrates promising results compared to four\nstate-of-the-art adversarial attack techniques targeting tabular data.", "comment": "This paper has been accepted for publication in Software Quality\n  Journal", "pdf_url": "http://arxiv.org/pdf/2408.04124v2", "cate": "cs.SE", "date": "2024-08-07", "updated": "2025-07-14"}
{"id": "2405.05709", "title": "On the Capacity of Correlated Phase-Noise Channels: An Electro-Optic Frequency Comb Example", "authors": ["Mohammad Farsi", "Hamdi Joudeh", "Gabriele Liga", "Alex Alvarado", "Magnus Karlsson", "Erik Agrell"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      Accepted in TIT, 45 pages, 3 figures,single-column", "url": "http://arxiv.org/abs/2405.05709v2", "summary": "The capacity of a discrete-time channel with correlated phase noises is\ninvestigated. In particular, the electro-optic frequency comb system is\nconsidered, where the phase noise of each subchannel is a combination of two\nindependent Wiener phase-noise sources. Capacity upper and lower bounds are\nderived for this channel and are compared with lower bounds obtained by\nnumerically evaluating the achievable information rates using quadrature\namplitude modulation constellations. Capacity upper and lower bounds are\nprovided for the high signal-to-noise ratio (SNR) regime. The multiplexing gain\n(pre-log) is shown to be $M-1$, where $M$ represents the number of subchannels.\nA constant gap between the asymptotic upper and lower bounds is observed, which\ndepends on the number of subchannels $M$. For the specific case of $M=2$,\ncapacity is characterized up to a term that vanishes as the SNR grows large.", "comment": "Accepted in TIT, 45 pages, 3 figures,single-column", "pdf_url": "http://arxiv.org/pdf/2405.05709v2", "cate": "cs.IT", "date": "2024-05-09", "updated": "2025-07-14"}
{"id": "2411.13820", "title": "InstCache: A Predictive Cache for LLM Serving", "authors": ["Longwei Zou", "Yan Liu", "Jiamu Kang", "Tingfeng Liu", "Jiangang Kong", "Yangdong Deng"], "categories": ["cs.CL", "cs.DC"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.13820v2", "summary": "The revolutionary capabilities of Large Language Models (LLMs) are attracting\nrapidly growing popularity and leading to soaring user requests to inference\nserving systems. Caching techniques, which leverage data reuse to reduce\ncomputation, offer opportunities to optimize the performance of LLM inference\nengines. On the one hand, the low-level key-value (KV) cache working at the\ntoken level is widely adopted, albeit it incurs significant overhead as request\nvolume grows. On the other hand, instruction-level caching, which stores full\ninstruction-response pairs, is expected to play an increasingly crucial role.\nHowever, the high variability in the content and length of instructions make it\nrare for identical instructions to recur within a short time window, presenting\nchallenges for effective caching instruction-response pairs. To address this\nchallenge, we propose InstCache, a predictive caching mechanism for LLM serving\nsystems. Leveraging the capability of LLMs, we can effectively reorder the\nrepresentation space of instruction texts and develop a sufficient level of\nspatial locality. Such spatial locality enables us to predict potential\ninstructions located in a compact region in the space, resulting in an\neffective caching system at runtime. Experimental results demonstrate that\nInstCache achieves a 2.3x higher hit rate compared to the upper bound of\ntraditional caching mechanisms on WildChat dataset and reduces the time per\noutput token of vLLM by up to 42.0% and 50.0% on LMSys and Moss datasets,\nrespectively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.13820v2", "cate": "cs.CL", "date": "2024-11-21", "updated": "2025-07-14"}
{"id": "2507.05285", "title": "Beyond classical and contemporary models: a transformative AI framework for student dropout prediction in distance learning using RAG, Prompt engineering, and Cross-modal fusion", "authors": ["Miloud Mihoubi", "Meriem Zerkouk", "Belkacem Chikhaoui"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.IR", "I.2.7; I.2.1; K.3.1"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      13 pages, 8 figures, 1 Algorithms, 17th International Conference on Education and New Learning Technologies,: 30 June-2 July, 2025 Location: Palma, Spain", "url": "http://arxiv.org/abs/2507.05285v2", "summary": "Student dropout in distance learning remains a critical challenge, with\nprofound societal and economic consequences. While classical machine learning\nmodels leverage structured socio-demographic and behavioral data, they often\nfail to capture the nuanced emotional and contextual factors embedded in\nunstructured student interactions. This paper introduces a transformative AI\nframework that redefines dropout prediction through three synergistic\ninnovations: Retrieval-Augmented Generation (RAG) for domain-specific sentiment\nanalysis, prompt engineering to decode academic stressors,and cross-modal\nattention fusion to dynamically align textual, behavioral, and\nsocio-demographic insights. By grounding sentiment analysis in a curated\nknowledge base of pedagogical content, our RAG-enhanced BERT model interprets\nstudent comments with unprecedented contextual relevance, while optimized\nprompts isolate indicators of academic distress (e.g., \"isolation,\" \"workload\nanxiety\"). A cross-modal attention layer then fuses these insights with\ntemporal engagement patterns, creating holistic risk pro-files. Evaluated on a\nlongitudinal dataset of 4 423 students, the framework achieves 89% accuracy and\nan F1-score of 0.88, outperforming conventional models by 7% and reducing false\nnegatives by 21%. Beyond prediction, the system generates interpretable\ninterventions by retrieving contextually aligned strategies (e.g., mentorship\nprograms for isolated learners). This work bridges the gap between predictive\nanalytics and actionable pedagogy, offering a scalable solution to mitigate\ndropout risks in global education systems", "comment": "13 pages, 8 figures, 1 Algorithms, 17th International Conference on\n  Education and New Learning Technologies,: 30 June-2 July, 2025 Location:\n  Palma, Spain", "pdf_url": "http://arxiv.org/pdf/2507.05285v2", "cate": "cs.CL", "date": "2025-07-04", "updated": "2025-07-14"}
{"id": "2409.06501", "title": "An Adaptive Sliding Window Estimator for Positioning of Unmanned Aerial Vehicle Using a Single Anchor", "authors": ["Kaiwen Xiong", "Sijia Chen", "Wei Dong"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      This work has been accepted by the IEEE Sensors Journal", "url": "http://arxiv.org/abs/2409.06501v4", "summary": "Localization using a single range anchor combined with onboard\noptical-inertial odometry offers a lightweight solution that provides\nmultidimensional measurements for the positioning of unmanned aerial vehicles.\nUnfortunately, the performance of such lightweight sensors varies with the\ndynamic environment, and the fidelity of the dynamic model is also severely\naffected by environmental aerial flow. To address this challenge, we propose an\nadaptive sliding window estimator equipped with an estimation reliability\nevaluator, where the states, noise covariance matrices and aerial drag are\nestimated simultaneously. The aerial drag effects are first evaluated based on\nposterior states and covariance. Then, an augmented Kalman filter is designed\nto pre-process multidimensional measurements and inherit historical\ninformation. Subsequently, an inverse-Wishart smoother is employed to estimate\nposterior states and covariance matrices. To further suppress potential\ndivergence, a reliability evaluator is devised to infer estimation errors. We\nfurther determine the fidelity of each sensor based on the error propagation.\nExtensive experiments are conducted in both standard and harsh environments,\ndemonstrating the adaptability and robustness of the proposed method. The root\nmean square error reaches 0.15 m, outperforming the state-of-the-art approach.\nReal-world close-loop control experiments are additionally performed to verify\nthe estimator's competence in practical application.", "comment": "This work has been accepted by the IEEE Sensors Journal", "pdf_url": "http://arxiv.org/pdf/2409.06501v4", "cate": "cs.RO", "date": "2024-09-10", "updated": "2025-07-12"}
{"id": "2507.08841", "title": "Zero-Shot Neural Architecture Search with Weighted Response Correlation", "authors": ["Kun Jing", "Luoyu Chen", "Jungang Xu", "Jianwei Tai", "Yiyu Wang", "Shuaimin Li"], "categories": ["cs.LG", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08841v1", "summary": "Neural architecture search (NAS) is a promising approach for automatically\ndesigning neural network architectures. However, the architecture estimation of\nNAS is computationally expensive and time-consuming because of training\nmultiple architectures from scratch. Although existing zero-shot NAS methods\nuse training-free proxies to accelerate the architecture estimation, their\neffectiveness, stability, and generality are still lacking. We present a novel\ntraining-free estimation proxy called weighted response correlation (WRCor).\nWRCor utilizes correlation coefficient matrices of responses across different\ninput samples to calculate the proxy scores of estimated architectures, which\ncan measure their expressivity and generalizability. Experimental results on\nproxy evaluation demonstrate that WRCor and its voting proxies are more\nefficient estimation strategies than existing proxies. We also apply them with\ndifferent search strategies in architecture search. Experimental results on\narchitecture search show that our zero-shot NAS algorithm outperforms most\nexisting NAS algorithms in different search spaces. Our NAS algorithm can\ndiscover an architecture with a 22.1% test error on the ImageNet-1k dataset\nwithin 4 GPU hours. All codes are publicly available at\nhttps://github.com/kunjing96/ZSNAS-WRCor.git.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08841v1", "cate": "cs.LG", "date": "2025-07-08", "updated": "2025-07-08"}
{"id": "2507.09299", "title": "ViT-ProtoNet for Few-Shot Image Classification: A Multi-Benchmark Evaluation", "authors": ["Abdulvahap Mutlu", "Şengül Doğan", "Türker Tuncer"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      All codes are available at this https URL", "url": "http://arxiv.org/abs/2507.09299v1", "summary": "The remarkable representational power of Vision Transformers (ViTs) remains\nunderutilized in few-shot image classification. In this work, we introduce\nViT-ProtoNet, which integrates a ViT-Small backbone into the Prototypical\nNetwork framework. By averaging class conditional token embeddings from a\nhandful of support examples, ViT-ProtoNet constructs robust prototypes that\ngeneralize to novel categories under 5-shot settings. We conduct an extensive\nempirical evaluation on four standard benchmarks: Mini-ImageNet, FC100,\nCUB-200, and CIFAR-FS, including overlapped support variants to assess\nrobustness. Across all splits, ViT-ProtoNet consistently outperforms CNN-based\nprototypical counterparts, achieving up to a 3.2\\% improvement in 5-shot\naccuracy and demonstrating superior feature separability in latent space.\nFurthermore, it outperforms or is competitive with transformer-based\ncompetitors using a more lightweight backbone. Comprehensive ablations examine\nthe impact of transformer depth, patch size, and fine-tuning strategy. To\nfoster reproducibility, we release code and pretrained weights. Our results\nestablish ViT-ProtoNet as a powerful, flexible approach for few-shot\nclassification and set a new baseline for transformer-based meta-learners.", "comment": "All codes are available at\n  https://github.com/abdulvahapmutlu/vit-protonet", "pdf_url": "http://arxiv.org/pdf/2507.09299v1", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2505.12185", "title": "EVALOOP: Assessing LLM Robustness in Programming from a Self-consistency Perspective", "authors": ["Sen Fang", "Weiyuan Ding", "Bowen Xu"], "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      20 pages, 11 figures", "url": "http://arxiv.org/abs/2505.12185v3", "summary": "Assessing the programming capabilities of Large Language Models (LLMs) is\ncrucial for their effective use in software engineering. Current evaluations,\nhowever, predominantly measure the accuracy of generated code on static\nbenchmarks, neglecting the critical aspect of model robustness during\nprogramming tasks. While adversarial attacks offer insights on model\nrobustness, their effectiveness is limited and evaluation could be constrained.\nCurrent adversarial attack methods for robustness evaluation yield inconsistent\nresults, struggling to provide a unified evaluation across different LLMs. We\nintroduce EVALOOP, a novel assessment framework that evaluate the robustness\nfrom a self-consistency perspective, i.e., leveraging the natural duality\ninherent in popular software engineering tasks, e.g., code generation and code\nsummarization. EVALOOP initiates a self-contained feedback loop: an LLM\ngenerates output (e.g., code) from an input (e.g., natural language\nspecification), and then use the generated output as the input to produce a new\noutput (e.g., summarizes that code into a new specification). EVALOOP repeats\nthe process to assess the effectiveness of EVALOOP in each loop. This cyclical\nstrategy intrinsically evaluates robustness without rely on any external attack\nsetups, providing a unified metric to evaluate LLMs' robustness in programming.\nWe evaluate 16 prominent LLMs (e.g., GPT-4.1, O4-mini) on EVALOOP and found\nthat EVALOOP typically induces a 5.01%-19.31% absolute drop in pass@1\nperformance within ten loops. Intriguingly, robustness does not always align\nwith initial performance (i.e., one-time query); for instance, GPT-3.5-Turbo,\ndespite superior initial code generation compared to DeepSeek-V2, demonstrated\nlower robustness over repeated evaluation loop.", "comment": "20 pages, 11 figures", "pdf_url": "http://arxiv.org/pdf/2505.12185v3", "cate": "cs.SE", "date": "2025-05-18", "updated": "2025-07-14"}
{"id": "2408.06275", "title": "Robust Instance Optimal Phase-Only Compressed Sensing", "authors": ["Junren Chen", "Michael K. Ng", "Jonathan Scarlett"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      Major changes: tighten bound for pre-sign noise & add converse results", "url": "http://arxiv.org/abs/2408.06275v2", "summary": "Phase-only compressed sensing (PO-CS) concerns the recovery of sparse signals\nfrom the phases of complex measurements. Recent results show that sparse\nsignals in the standard sphere $\\mathbb{S}^{n-1}$ can be exactly recovered from\ncomplex Gaussian phases by a linearization procedure, which recasts PO-CS as\nlinear compressed sensing and then applies (quadratically constrained) basis\npursuit to obtain $\\mathbf{x}^\\sharp$. This paper focuses on the instance\noptimality and robustness of $\\mathbf{x}^{\\sharp}$. First, we strengthen the\nnonuniform instance optimality of Jacques and Feuillen (2021) to a uniform one\nover the entire signal space. We show the existence of some universal constant\n$C$ such that $\\|\\mathbf{x}^\\sharp-\\mathbf{x}\\|_2\\le\nCs^{-1/2}\\sigma_{\\ell_1}(\\mathbf{x},\\Sigma^n_s)$ holds for all $\\mathbf{x}$ in\nthe unit Euclidean sphere, where $\\sigma_{\\ell_1}(\\mathbf{x},\\Sigma^n_s)$ is\nthe $\\ell_1$ distance of $\\mathbf{x}$ to its closest $s$-sparse signal. This is\nachieved by showing the new sensing matrices corresponding to all approximately\nsparse signals simultaneously satisfy RIP. Second, we investigate the\nestimator's robustness to noise and corruption. We show that dense noise with\nentries bounded by some small $\\tau_0$, appearing either prior or posterior to\nretaining the phases, increments $\\|\\mathbf{x}^\\sharp-\\mathbf{x}\\|_2$ by\n$O(\\tau_0)$. This is near-optimal (up to log factors) for any algorithm. On the\nother hand, adversarial corruption, which changes an arbitrary\n$\\zeta_0$-fraction of the measurements to any phase-only values, increments\n$\\|\\mathbf{x}^\\sharp-\\mathbf{x}\\|_2$ by $O(\\sqrt{\\zeta_0\\log(1/\\zeta_0)})$. The\ndevelopments are then combined to yield a robust instance optimal guarantee\nthat resembles the standard one in linear compressed sensing.", "comment": "Major changes: tighten bound for pre-sign noise & add converse\n  results", "pdf_url": "http://arxiv.org/pdf/2408.06275v2", "cate": "cs.IT", "date": "2024-08-12", "updated": "2025-07-14"}
{"id": "2501.11816", "title": "Module-conditioned distribution of quantum circuits", "authors": ["Hyunho Cha", "Jungwoo Lee"], "categories": ["quant-ph", "cs.DC"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      28 pages, 21 figures", "url": "http://arxiv.org/abs/2501.11816v2", "summary": "As quantum computers require highly specialized and stable environments to\noperate, expanding their capabilities within a single system presents\nsignificant technical challenges. By interconnecting multiple quantum\nprocessors, distributed quantum computing can facilitate the execution of more\ncomplex and larger-scale quantum algorithms. End-to-end heuristics for the\ndistribution of quantum circuits have been developed so far. In this work, we\nderive an exact integer programming approach for the Distributed Quantum\nCircuit (DQC) problem, assuming fixed module allocations. Since every DQC\nalgorithm necessarily yields a module allocation function, our formulation can\nbe integrated with it as a post-processing step. This improves on the\nhypergraph partitioning formulation, which finds a module allocation function\nand an efficient distribution at once. We also show that a suboptimal heuristic\nto find good allocations can outperform previous methods. In particular, for\nquantum Fourier transform circuits, we conjecture from experiments that the\noptimal module allocation is the trivial one found by this method.", "comment": "28 pages, 21 figures", "pdf_url": "http://arxiv.org/pdf/2501.11816v2", "cate": "quant-ph", "date": "2025-01-21", "updated": "2025-07-14"}
{"id": "2409.08750", "title": "DexSim2Real$^{2}$: Building Explicit World Model for Precise Articulated Object Dexterous Manipulation", "authors": ["Taoran Jiang", "Yixuan Guan", "Liqian Ma", "Jing Xu", "Jiaojiao Meng", "Weihang Chen", "Zecui Zeng", "Lusong Li", "Dan Wu", "Rui Chen"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Project Webpage: this https URL . arXiv admin note: text overlap with arXiv:2302.10693", "url": "http://arxiv.org/abs/2409.08750v2", "summary": "Articulated objects are ubiquitous in daily life. In this paper, we present\nDexSim2Real$^{2}$, a novel framework for goal-conditioned articulated object\nmanipulation. The core of our framework is constructing an explicit world model\nof unseen articulated objects through active interactions, which enables\nsampling-based model predictive control to plan trajectories achieving\ndifferent goals without requiring demonstrations or RL. It first predicts an\ninteraction using an affordance network trained on self-supervised interaction\ndata or videos of human manipulation. After executing the interactions on the\nreal robot to move the object parts, we propose a novel modeling pipeline based\non 3D AIGC to build a digital twin of the object in simulation from multiple\nframes of observations. For dexterous hands, we utilize eigengrasp to reduce\nthe action dimension, enabling more efficient trajectory searching. Experiments\nvalidate the framework's effectiveness for precise manipulation using a suction\ngripper, a two-finger gripper and two dexterous hand. The generalizability of\nthe explicit world model also enables advanced manipulation strategies like\nmanipulating with tools.", "comment": "Project Webpage: https://jiangtaoran.github.io/dexsim2real2web/ .\n  arXiv admin note: text overlap with arXiv:2302.10693", "pdf_url": "http://arxiv.org/pdf/2409.08750v2", "cate": "cs.RO", "date": "2024-09-13", "updated": "2025-07-13"}
{"id": "2507.08842", "title": "Gradients as an Action: Towards Communication-Efficient Federated Recommender Systems via Adaptive Action Sharing", "authors": ["Zhufeng Lu", "Chentao Jia", "Ming Hu", "Xiaofei Xie", "Mingsong Chen"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      This paper has been accepted by ACM SIGKDD 2025", "url": "http://arxiv.org/abs/2507.08842v1", "summary": "As a promising privacy-aware collaborative model training paradigm, Federated\nLearning (FL) is becoming popular in the design of distributed recommender\nsystems. However, Federated Recommender Systems (FedRecs) greatly suffer from\ntwo major problems: i) extremely high communication overhead due to massive\nitem embeddings involved in recommendation systems, and ii) intolerably low\ntraining efficiency caused by the entanglement of both heterogeneous network\nenvironments and client devices. Although existing methods attempt to employ\nvarious compression techniques to reduce communication overhead, due to the\nparameter errors introduced by model compression, they inevitably suffer from\nmodel performance degradation. To simultaneously address the above problems,\nthis paper presents a communication-efficient FedRec framework named FedRAS,\nwhich adopts an action-sharing strategy to cluster the gradients of item\nembedding into a specific number of model updating actions for communication\nrather than directly compressing the item embeddings. In this way, the cloud\nserver can use the limited actions from clients to update all the items. Since\ngradient values are significantly smaller than item embeddings, constraining\nthe directions of gradients (i.e., the action space) introduces smaller errors\ncompared to compressing the entire item embedding matrix into a reduced space.\nTo accommodate heterogeneous devices and network environments, FedRAS\nincorporates an adaptive clustering mechanism that dynamically adjusts the\nnumber of actions. Comprehensive experiments on well-known datasets demonstrate\nthat FedRAS can reduce the size of communication payloads by up to 96.88%,\nwhile not sacrificing recommendation performance within various heterogeneous\nscenarios. We have open-sourced FedRAS at\nhttps://github.com/mastlab-T3S/FedRAS.", "comment": "This paper has been accepted by ACM SIGKDD 2025", "pdf_url": "http://arxiv.org/pdf/2507.08842v1", "cate": "cs.LG", "date": "2025-07-08", "updated": "2025-07-08"}
{"id": "2507.09305", "title": "DAA*: Deep Angular A Star for Image-based Path Planning", "authors": ["Zhiwei Xu"], "categories": ["cs.CV", "cs.LG", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      International Conference on Computer Vision (ICCV), 2025", "url": "http://arxiv.org/abs/2507.09305v1", "summary": "Path smoothness is often overlooked in path imitation learning from expert\ndemonstrations. In this paper, we introduce a novel learning method, termed\ndeep angular A* (DAA*), by incorporating the proposed path angular freedom\n(PAF) into A* to improve path similarity through adaptive path smoothness. The\nPAF aims to explore the effect of move angles on path node expansion by finding\nthe trade-off between their minimum and maximum values, allowing for high\nadaptiveness for imitation learning. DAA* improves path optimality by closely\naligning with the reference path through joint optimization of path shortening\nand smoothing, which correspond to heuristic distance and PAF, respectively.\nThroughout comprehensive evaluations on 7 datasets, including 4 maze datasets,\n2 video-game datasets, and a real-world drone-view dataset containing 2\nscenarios, we demonstrate remarkable improvements of our DAA* over neural A* in\npath similarity between the predicted and reference paths with a shorter path\nlength when the shortest path is plausible, improving by 9.0% SPR, 6.9% ASIM,\nand 3.9% PSIM. Furthermore, when jointly learning pathfinding with both path\nloss and path probability map loss, DAA* significantly outperforms the\nstate-of-the-art TransPath by 6.7% SPR, 6.5% PSIM, and 3.7% ASIM. We also\ndiscuss the minor trade-off between path optimality and search efficiency where\napplicable.", "comment": "International Conference on Computer Vision (ICCV), 2025", "pdf_url": "http://arxiv.org/pdf/2507.09305v1", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2505.18444", "title": "On the Structure and Semantics of Identifier Names Containing Closed Syntactic Category Words", "authors": ["Christian D. Newman", "Anthony Peruma", "Eman Abdullah AlOmar", "Mahie Crabbe", "Syreen Banabilah", "Reem S. AlSuhaibani", "Michael J. Decker", "Farhad Akhbardeh", "Marcos Zampieri", "Mohamed Wiem Mkaouer", "Jonathan I. Maletic"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Accepted by Empirical Software Engineering (EMSE)", "url": "http://arxiv.org/abs/2505.18444v3", "summary": "Identifier names are crucial components of code, serving as primary clues for\ndevelopers to understand program behavior. This paper investigates the\nlinguistic structure of identifier names by extending the concept of grammar\npatterns, which represent the part-of-speech (PoS) sequences underlying\nidentifier phrases. The specific focus is on closed syntactic categories (e.g.,\nprepositions, conjunctions, determiners), which are rarely studied in software\nengineering despite their central role in general natural language. To study\nthese categories, the Closed Category Identifier Dataset (CCID), a new manually\nannotated dataset of 1,275 identifiers drawn from 30 open-source systems, is\nconstructed and presented. The relationship between closed-category grammar\npatterns and program behavior is then analyzed using grounded-theory-inspired\ncoding, statistical, and pattern analysis. The results reveal recurring\nstructures that developers use to express concepts such as control flow, data\ntransformation, temporal reasoning, and other behavioral roles through naming.\nThis work contributes an empirical foundation for understanding how linguistic\nresources encode behavior in identifier names and supports new directions for\nresearch in naming, program comprehension, and education.", "comment": "Accepted by Empirical Software Engineering (EMSE)", "pdf_url": "http://arxiv.org/pdf/2505.18444v3", "cate": "cs.SE", "date": "2025-05-24", "updated": "2025-07-11"}
{"id": "2501.13105", "title": "On the Service Rate Region of Reed-Muller Codes", "authors": ["Hoang Ly", "Emina Soljanin", "V. Lalitha"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      Final version update", "url": "http://arxiv.org/abs/2501.13105v4", "summary": "We study the Service Rate Region (SRR) of Reed-Muller (RM) codes in the\ncontext of distributed storage systems. The SRR is a convex polytope comprising\nall achievable data access request rates under a given coding scheme. It\nrepresents a critical metric for evaluating system efficiency and scalability.\nUsing the geometric properties of RM codes, we characterize recovery sets for\ndata objects, including their existence, uniqueness, and enumeration. This\nanalysis reveals a connection between recovery sets and minimum-weight\ncodewords in the dual RM code, providing a framework for identifying small\nrecovery sets. Using these results, we derive explicit and tight bounds for the\nmaximal achievable demand for individual data objects, which define the maximal\nsimplex within the service rate region.", "comment": "Final version update", "pdf_url": "http://arxiv.org/pdf/2501.13105v4", "cate": "cs.IT", "date": "2025-01-22", "updated": "2025-07-13"}
{"id": "2502.04850", "title": "Aequa: Fair Model Rewards in Collaborative Learning via Slimmable Networks", "authors": ["Nurbek Tastan", "Samuel Horvath", "Karthik Nandakumar"], "categories": ["cs.LG", "cs.DC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.04850v2", "summary": "Collaborative learning enables multiple participants to learn a single global\nmodel by exchanging focused updates instead of sharing data. One of the core\nchallenges in collaborative learning is ensuring that participants are rewarded\nfairly for their contributions, which entails two key sub-problems:\ncontribution assessment and reward allocation. This work focuses on fair reward\nallocation, where the participants are incentivized through model rewards -\ndifferentiated final models whose performance is commensurate with the\ncontribution. In this work, we leverage the concept of slimmable neural\nnetworks to collaboratively learn a shared global model whose performance\ndegrades gracefully with a reduction in model width. We also propose a\npost-training fair allocation algorithm that determines the model width for\neach participant based on their contributions. We theoretically study the\nconvergence of our proposed approach and empirically validate it using\nextensive experiments on different datasets and architectures. We also extend\nour approach to enable training-time model reward allocation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.04850v2", "cate": "cs.LG", "date": "2025-02-07", "updated": "2025-07-13"}
{"id": "2409.10320", "title": "SEAL: Towards Safe Autonomous Driving via Skill-Enabled Adversary Learning for Closed-Loop Scenario Generation", "authors": ["Benjamin Stoler", "Ingrid Navarro", "Jonathan Francis", "Jean Oh"], "categories": ["cs.RO", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted to the IEEE Robotics and Automation Letters (RA-L) on June 28, 2025", "url": "http://arxiv.org/abs/2409.10320v3", "summary": "Verification and validation of autonomous driving (AD) systems and components\nis of increasing importance, as such technology increases in real-world\nprevalence. Safety-critical scenario generation is a key approach to robustify\nAD policies through closed-loop training. However, existing approaches for\nscenario generation rely on simplistic objectives, resulting in\noverly-aggressive or non-reactive adversarial behaviors. To generate diverse\nadversarial yet realistic scenarios, we propose SEAL, a scenario perturbation\napproach which leverages learned objective functions and adversarial,\nhuman-like skills. SEAL-perturbed scenarios are more realistic than SOTA\nbaselines, leading to improved ego task success across real-world,\nin-distribution, and out-of-distribution scenarios, of more than 20%. To\nfacilitate future research, we release our code and tools:\nhttps://github.com/cmubig/SEAL", "comment": "Accepted to the IEEE Robotics and Automation Letters (RA-L) on June\n  28, 2025", "pdf_url": "http://arxiv.org/pdf/2409.10320v3", "cate": "cs.RO", "date": "2024-09-16", "updated": "2025-07-14"}
{"id": "2507.08843", "title": "Can We Predict Your Next Move Without Breaking Your Privacy?", "authors": ["Arpita Soni", "Sahil Tripathi", "Gautam Siddharth Kashyap", "Manaswi Kulahara", "Mohammad Anas Azeez", "Zohaib Hasan Siddiqui", "Nipun Joshi", "Jiechao Gao"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted in the 17th International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2025), scheduled for 25 - 28 August 2025 in Ontario, Canada", "url": "http://arxiv.org/abs/2507.08843v1", "summary": "We propose FLLL3M--Federated Learning with Large Language Models for Mobility\nModeling--a privacy-preserving framework for Next-Location Prediction (NxLP).\nBy retaining user data locally and leveraging LLMs through an efficient outer\nproduct mechanism, FLLL3M ensures high accuracy with low resource demands. It\nachieves SOT results on Gowalla (Acc@1: 12.55, MRR: 0.1422), WeePlace (10.71,\n0.1285), Brightkite (10.42, 0.1169), and FourSquare (8.71, 0.1023), while\nreducing parameters by up to 45.6% and memory usage by 52.7%.", "comment": "Accepted in the 17th International Conference on Advances in Social\n  Networks Analysis and Mining (ASONAM 2025), scheduled for 25 - 28 August 2025\n  in Ontario, Canada", "pdf_url": "http://arxiv.org/pdf/2507.08843v1", "cate": "cs.LG", "date": "2025-07-08", "updated": "2025-07-08"}
{"id": "2507.09308", "title": "AlphaVAE: Unified End-to-End RGBA Image Reconstruction and Generation with Alpha-Aware Representation Learning", "authors": ["Zile Wang", "Hao Yu", "Jiabo Zhan", "Chun Yuan"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09308v1", "summary": "Recent advances in latent diffusion models have achieved remarkable results\nin high-fidelity RGB image synthesis by leveraging pretrained VAEs to compress\nand reconstruct pixel data at low computational cost. However, the generation\nof transparent or layered content (RGBA image) remains largely unexplored, due\nto the lack of large-scale benchmarks. In this work, we propose ALPHA, the\nfirst comprehensive RGBA benchmark that adapts standard RGB metrics to\nfour-channel images via alpha blending over canonical backgrounds. We further\nintroduce ALPHAVAE, a unified end-to-end RGBA VAE that extends a pretrained RGB\nVAE by incorporating a dedicated alpha channel. The model is trained with a\ncomposite objective that combines alpha-blended pixel reconstruction,\npatch-level fidelity, perceptual consistency, and dual KL divergence\nconstraints to ensure latent fidelity across both RGB and alpha\nrepresentations. Our RGBA VAE, trained on only 8K images in contrast to 1M used\nby prior methods, achieves a +4.9 dB improvement in PSNR and a +3.2% increase\nin SSIM over LayerDiffuse in reconstruction. It also enables superior\ntransparent image generation when fine-tuned within a latent diffusion\nframework. Our code, data, and models are released on\nhttps://github.com/o0o0o00o0/AlphaVAE for reproducibility.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09308v1", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2506.10992", "title": "Towards a Theory on Process Automation Effects", "authors": ["Hoang Vu", "Jennifer Haase", "Henrik Leopold", "Jan Mendling"], "categories": ["cs.SE", "D.2.9"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Accepted at BPM Forum 2023", "url": "http://arxiv.org/abs/2506.10992v2", "summary": "Process automation is a crucial strategy for improving business processes,\nbut little attention has been paid to the effects that automation has once it\nis operational. This paper addresses this research problem by reviewing the\nliterature on human-automation interaction. Although many of the studies in\nthis field have been conducted in different domains, they provide a foundation\nfor developing propositions about process automation effects. Our analysis\nfocuses on how humans perceive automation technology when working within a\nprocess, allowing us to propose an effective engagement model between\ntechnology, process participants, process managers, and software developers.\nThis paper offers insights and recommendations that can help organizations\noptimize their use of process automation. We further derive novel research\nquestions for a discourse within the process automation community.", "comment": "Accepted at BPM Forum 2023", "pdf_url": "http://arxiv.org/pdf/2506.10992v2", "cate": "cs.SE", "date": "2025-03-25", "updated": "2025-07-14"}
{"id": "2504.14410", "title": "On the Redundancy of Function-Correcting Codes over Finite Fields", "authors": ["Hoang Ly", "Emina Soljanin"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      v2: Remove 1 redundant page at the end. Put in the right Abstract. v3: Made some small edits", "url": "http://arxiv.org/abs/2504.14410v4", "summary": "Function-correcting codes (FCCs) protect specific function evaluations of a\nmessage against errors. This condition imposes a less stringent distance\nrequirement than classical error-correcting codes (ECCs), allowing for reduced\nredundancy. FCCs were introduced by Lenz et al. (2021), who also established a\nlower bound on the optimal redundancy for FCCs over the binary field. Here, we\nderive an upper bound within a logarithmic factor of this lower bound. We show\nthat the same lower bound holds for any finite field. Moreover, we show that\nthis bound is tight for sufficiently large fields by demonstrating that it also\nserves as an upper bound. Furthermore, we construct an encoding scheme that\nachieves this optimal redundancy. Finally, motivated by these two extreme\nregimes, we conjecture that our bound serves as a valid upper bound across all\nfinite fields.", "comment": "v2: Remove 1 redundant page at the end. Put in the right Abstract.\n  v3: Made some small edits", "pdf_url": "http://arxiv.org/pdf/2504.14410v4", "cate": "cs.IT", "date": "2025-04-19", "updated": "2025-07-13"}
{"id": "2505.17826", "title": "Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement Fine-Tuning of Large Language Models", "authors": ["Xuchen Pan", "Yanxi Chen", "Yushuo Chen", "Yuchang Sun", "Daoyuan Chen", "Wenhao Zhang", "Yuexiang Xie", "Yilun Huang", "Yilei Zhang", "Dawei Gao", "Weijie Shi", "Yaliang Li", "Bolin Ding", "Jingren Zhou"], "categories": ["cs.LG", "cs.CL", "cs.DC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      This technical report will be continuously updated as the codebase evolves. GitHub: this https URL", "url": "http://arxiv.org/abs/2505.17826v2", "summary": "Trinity-RFT is a general-purpose, unified and easy-to-use framework designed\nfor reinforcement fine-tuning (RFT) of large language models. It is built with\na modular and decoupled design, consisting of (1) an RFT-core that unifies and\ngeneralizes synchronous/asynchronous, on-policy/off-policy, and online/offline\nmodes of RFT; (2) seamless integration for agent-environment interaction with\nhigh efficiency and robustness; and (3) systematic data pipelines optimized for\nRFT. Trinity-RFT can be easily adapted for diverse application scenarios, and\nserves as a unified platform for development and research of advanced\nreinforcement learning paradigms at both macroscopic and microscopic levels.\nThis technical report outlines the vision, features, design and implementations\nof Trinity-RFT, accompanied by extensive examples, applications and experiments\nthat demonstrate its functionalities and user-friendliness.", "comment": "This technical report will be continuously updated as the codebase\n  evolves. GitHub: https://github.com/modelscope/Trinity-RFT", "pdf_url": "http://arxiv.org/pdf/2505.17826v2", "cate": "cs.LG", "date": "2025-05-23", "updated": "2025-07-14"}
{"id": "2507.09377", "title": "A Fixed Parameter Tractable Approach for Solving the Vertex Cover Problem in Polynomial Time Complexity", "authors": ["Mumuksh Tayal"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      10 pages, 2 figures, 1 table. Accepted at STOC 2024 Workshop on TCS for All", "url": "http://arxiv.org/abs/2507.09377v1", "summary": "The Minimum Vertex Cover problem, a classical NP-complete problem, presents\nsignificant challenges for exact solution on large graphs. Fixed-Parameter\nTractability (FPT) offers a powerful paradigm to address such problems by\nexploiting a parameter of the input, typically related to the size of the\ndesired solution. This paper presents an implementation and empirical\nevaluation of an FPT algorithm for the Minimum Vertex Cover problem\nparameterized by the size of the vertex cover, $k$. The algorithm utilizes a\nbranching strategy based on selecting adjacent vertices and recursively solving\nsubproblems on a reduced graph. We describe the algorithmic approach,\nimplementation details in Python, and present experimental results comparing\nits performance against the SageMath computational system. The results\ndemonstrate that the FPT implementation achieves significant performance\nimprovements for instances with large numbers of vertices ($n$) but relatively\nsmall values of the parameter ($k$), aligning with theoretical FPT complexity\nguarantees. We also discuss potential optimizations that could further improve\nthe algorithm's performance, particularly concerning the branching factor.", "comment": "10 pages, 2 figures, 1 table. Accepted at STOC 2024 Workshop on TCS\n  for All", "pdf_url": "http://arxiv.org/pdf/2507.09377v1", "cate": "cs.DS", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2409.15585", "title": "XMoP: Whole-Body Control Policy for Zero-shot Cross-Embodiment Neural Motion Planning", "authors": ["Prabin Kumar Rath", "Nakul Gopalan"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Website at this https URL Paper has 17 pages, 13 figures, 5 tables", "url": "http://arxiv.org/abs/2409.15585v2", "summary": "Classical manipulator motion planners work across different robot\nembodiments. However they plan on a pre-specified static environment\nrepresentation, and are not scalable to unseen dynamic environments. Neural\nMotion Planners (NMPs) are an appealing alternative to conventional planners as\nthey incorporate different environmental constraints to learn motion policies\ndirectly from raw sensor observations. Contemporary state-of-the-art NMPs can\nsuccessfully plan across different environments. However none of the existing\nNMPs generalize across robot embodiments. In this paper we propose\nCross-Embodiment Motion Policy (XMoP), a neural policy for learning to plan\nover a distribution of manipulators. XMoP implicitly learns to satisfy\nkinematic constraints for a distribution of robots and $\\textit{zero-shot}$\ntransfers the planning behavior to unseen robotic manipulators within this\ndistribution. We achieve this generalization by formulating a whole-body\ncontrol policy that is trained on planning demonstrations from over three\nmillion procedurally sampled robotic manipulators in different simulated\nenvironments. Despite being completely trained on synthetic embodiments and\nenvironments, our policy exhibits strong sim-to-real generalization across\nmanipulators with different kinematic variations and degrees of freedom with a\nsingle set of frozen policy parameters. We evaluate XMoP on $7$ commercial\nmanipulators and show successful cross-embodiment motion planning, achieving an\naverage $70\\%$ success rate on baseline benchmarks. Furthermore, we demonstrate\nour policy sim-to-real on two unseen manipulators solving novel planning\nproblems across three real-world domains even with dynamic obstacles.", "comment": "Website at https://prabinrath.github.io/xmop Paper has 17 pages, 13\n  figures, 5 tables", "pdf_url": "http://arxiv.org/pdf/2409.15585v2", "cate": "cs.RO", "date": "2024-09-23", "updated": "2025-07-13"}
{"id": "2507.08845", "title": "DAFOS: Dynamic Adaptive Fanout Optimization Sampler", "authors": ["Irfan Ullah", "Young-Koo Lee"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08845v1", "summary": "Graph Neural Networks (GNNs) are becoming an essential tool for learning from\ngraph-structured data, however uniform neighbor sampling and static fanout\nsettings frequently limit GNNs' scalability and efficiency. In this paper, we\npropose the Dynamic Adaptive Fanout Optimization Sampler (DAFOS), a novel\napproach that dynamically adjusts the fanout based on model performance and\nprioritizes important nodes during training. Our approach leverages node\nscoring based on node degree to focus computational resources on structurally\nimportant nodes, incrementing the fanout as the model training progresses.\nDAFOS also integrates an early stopping mechanism to halt training when\nperformance gains diminish. Experiments conducted on three benchmark datasets,\nogbnarxiv, Reddit, and ogbn-products, demonstrate that our approach\nsignificantly improves training speed and accuracy compared to a\nstate-of-the-art approach. DAFOS achieves a 3.57x speedup on the ogbn-arxiv\ndataset and a 12.6x speedup on the Reddit dataset while improving the F1 score\nfrom 68.5% to 71.21% on ogbn-arxiv and from 73.78% to 76.88% on the\nogbn-products dataset, respectively. These results highlight the potential of\nDAFOS as an efficient and scalable solution for large-scale GNN training.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08845v1", "cate": "cs.LG", "date": "2025-07-08", "updated": "2025-07-08"}
{"id": "2507.09313", "title": "ProactiveBench: A Comprehensive Benchmark Evaluating Proactive Interactions in Video Large Language Models", "authors": ["Yueqian Wang", "Xiaojun Meng", "Yifan Wang", "Huishuai Zhang", "Dongyan Zhao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09313v1", "summary": "With the growing research focus on multimodal dialogue systems, the\ncapability for proactive interaction is gradually gaining recognition. As an\nalternative to conventional turn-by-turn dialogue, users increasingly expect\nmultimodal systems to be more initiative, for example, by autonomously\ndetermining the timing of multi-turn responses in real time during video\nplayback. To facilitate progress in this emerging area, we introduce\nProactiveBench, the first comprehensive benchmark to evaluate a system's\nability to engage in proactive interaction. Since model responses are generated\nat varying timestamps, we further propose PAUC, the first metric that accounts\nfor the temporal dynamics of model responses. This enables a more accurate\nevaluation of systems operating in proactive settings. Through extensive\nbenchmarking of various baseline systems on ProactiveBench and a user study of\nhuman preferences, we show that PAUC is in better agreement with human\npreferences than traditional evaluation metrics, which typically only consider\nthe textual content of responses. These findings demonstrate that PAUC provides\na more faithful assessment of user experience in proactive interaction\nscenarios. Project homepage:\nhttps://github.com/yellow-binary-tree/ProactiveBench", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09313v1", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2506.18403", "title": "The Debugging Decay Index: Rethinking Debugging Strategies for Code LLMs", "authors": ["Muntasir Adnan", "Carlos C. N. Kuhn"], "categories": ["cs.SE", "cs.AI"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.18403v2", "summary": "The effectiveness of AI debugging follows a predictable exponential decay\npattern; most models lose 60-80% of their debugging capability within just 2-3\nattempts, despite iterative debugging being a critical capability for practical\ncode generation systems. We introduce the Debugging Decay Index (DDI), a\nmathematical framework that quantifies when debugging becomes ineffective and\npredicts intervention points. Our strategic fresh start approach shifts from\nexploitation to exploration at strategic points in the debugging process,\ndemonstrating that well-timed interventions can rescue the effectiveness of\ndebugging. DDI reveals a fundamental limitation in current AI debugging and\nprovides the first quantitative framework for optimising iterative code\ngeneration strategies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.18403v2", "cate": "cs.SE", "date": "2025-06-23", "updated": "2025-07-13"}
{"id": "2505.09394", "title": "Generalized Code Index Modulation Aided AFDM for Spread Spectrum Systems", "authors": ["Mi Qian", "Fei Ji", "Yao Ge", "Miaowen Wen", "Yong Liang Guan"], "categories": ["cs.IT", "cs.NA", "math.IT", "math.NA"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.09394v2", "summary": "The recently proposed affine frequency division multiplexing (AFDM) is a new\ntransmission waveform that has shown excellent performance in high-mobility\nenvironments, making it a sensible option for the next-generation wireless\nnetworks. In this paper, we investigate an energy-efficient generalized code\nindex modulation scheme for AFDM by leveraging spread spectrum, referred to as\nGCIM-AFDM-SS, to combat the interference caused by the doubly dispersive\nchannels. Specifically, the information bits are conveyed by the transmitted\nsymbols as well as the indices of the selected spreading codes in our proposed\nGCIM-AFDM-SS scheme. To avoid extensive computations, we also develop a\nlowcomplexity maximal ratio combining (MRC) detector algorithm, which recovers\nthe spreading codes first and demodulates the symbols afterwards. Moreover, an\nupper bound on the bit error rate (BER) of the proposed GCIM-AFDM-SS system\nwith maximum-likelihood (ML) detection is derived. Numerical results\ndemonstrate the superiority of the proposed GCIM-AFDM-SS system over the\nclassical AFDM spread spectrum (AFDM-SS) and the existing index modulated AFDM\n(IM-AFDM) systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.09394v2", "cate": "cs.IT", "date": "2025-05-14", "updated": "2025-07-14"}
{"id": "2507.09426", "title": "Simultaneous Network Design with Restricted Link Usage", "authors": ["Naonori Kakimura", "Péter Madarasi", "Jannik Matuschke", "Kitti Varga"], "categories": ["cs.DS", "cs.CC", "cs.DM", "math.OC"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09426v1", "summary": "Given a digraph with two terminal vertices $s$ and $t$ as well as a\nconservative cost function and several not necessarily disjoint color classes\non its arc set, our goal is to find a minimum-cost subset of the arcs such that\nits intersection with each color class contains an $s$-$t$ dipath. Problems of\nthis type arise naturally in multi-commodity network design settings where each\ncommodity is restricted to use links of its own color only.\n  We study several variants of the problem, deriving strong hardness results\neven for restricted cases, but we also identify cases that can be solved in\npolynomial time. The latter ones include the cases where the color classes form\na laminar family, or where the underlying digraph is acyclic and the number of\ncolor classes is constant. We also present an FPT algorithm for the general\ncase parameterized by the number of multi-colored arcs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09426v1", "cate": "cs.DS", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2501.18564", "title": "SAM2Act: Integrating Visual Foundation Model with A Memory Architecture for Robotic Manipulation", "authors": ["Haoquan Fang", "Markus Grotz", "Wilbert Pumacay", "Yi Ru Wang", "Dieter Fox", "Ranjay Krishna", "Jiafei Duan"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Including Appendix, Project Page: this https URL", "url": "http://arxiv.org/abs/2501.18564v4", "summary": "Robotic manipulation systems operating in diverse, dynamic environments must\nexhibit three critical abilities: multitask interaction, generalization to\nunseen scenarios, and spatial memory. While significant progress has been made\nin robotic manipulation, existing approaches often fall short in generalization\nto complex environmental variations and addressing memory-dependent tasks. To\nbridge this gap, we introduce SAM2Act, a multi-view robotic transformer-based\npolicy that leverages multi-resolution upsampling with visual representations\nfrom large-scale foundation model. SAM2Act achieves a state-of-the-art average\nsuccess rate of 86.8% across 18 tasks in the RLBench benchmark, and\ndemonstrates robust generalization on The Colosseum benchmark, with only a 4.3%\nperformance gap under diverse environmental perturbations. Building on this\nfoundation, we propose SAM2Act+, a memory-based architecture inspired by SAM2,\nwhich incorporates a memory bank, an encoder, and an attention mechanism to\nenhance spatial memory. To address the need for evaluating memory-dependent\ntasks, we introduce MemoryBench, a novel benchmark designed to assess spatial\nmemory and action recall in robotic manipulation. SAM2Act+ achieves an average\nsuccess rate of 94.3% on memory-based tasks in MemoryBench, significantly\noutperforming existing approaches and pushing the boundaries of memory-based\nrobotic systems. Project page: sam2act.github.io.", "comment": "Including Appendix, Project Page: https://sam2act.github.io", "pdf_url": "http://arxiv.org/pdf/2501.18564v4", "cate": "cs.RO", "date": "2025-01-30", "updated": "2025-07-13"}
{"id": "2507.08858", "title": "Foundation models for time series forecasting: Application in conformal prediction", "authors": ["Sami Achour", "Yassine Bouher", "Duong Nguyen", "Nicolas Chesneau"], "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08858v1", "summary": "The zero-shot capabilities of foundation models (FMs) for time series\nforecasting offer promising potentials in conformal prediction, as most of the\navailable data can be allocated to calibration. This study compares the\nperformance of Time Series Foundation Models (TSFMs) with traditional methods,\nincluding statistical models and gradient boosting, within a conformal\nprediction setting. Our findings highlight two key advantages of TSFMs. First,\nwhen the volume of data is limited, TSFMs provide more reliable conformalized\nprediction intervals than classic models, thanks to their superior predictive\naccuracy. Second, the calibration process is more stable because more data are\nused for calibration. Morever, the fewer data available, the more pronounced\nthese benefits become, as classic models require a substantial amount of data\nfor effective training. These results underscore the potential of foundation\nmodels in improving conformal prediction reliability in time series\napplications, particularly in data-constrained cases. All the code to reproduce\nthe experiments is available.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08858v1", "cate": "cs.LG", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.09323", "title": "Dynamic Inter-Class Confusion-Aware Encoder for Audio-Visual Fusion in Human Activity Recognition", "authors": ["Kaixuan Cong", "Yifan Wang", "Rongkun Xue", "Yuyang Jiang", "Yiming Feng", "Jing Yang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09323v1", "summary": "Humans do not understand individual events in isolation; rather, they\ngeneralize concepts within classes and compare them to others. Existing\naudio-video pre-training paradigms only focus on the alignment of the overall\naudio-video modalities, without considering the reinforcement of distinguishing\neasily confused classes through cognitive induction and contrast during\ntraining. This paper proposes the Dynamic Inter-Class Confusion-Aware Encoder\n(DICCAE), an encoder that aligns audio-video representations at a fine-grained,\ncategory-level. DICCAE addresses category confusion by dynamically adjusting\nthe confusion loss based on inter-class confusion degrees, thereby enhancing\nthe model's ability to distinguish between similar activities. To further\nextend the application of DICCAE, we also introduce a novel training framework\nthat incorporates both audio and video modalities, as well as their fusion. To\nmitigate the scarcity of audio-video data in the human activity recognition\ntask, we propose a cluster-guided audio-video self-supervised pre-training\nstrategy for DICCAE. DICCAE achieves near state-of-the-art performance on the\nVGGSound dataset, with a top-1 accuracy of 65.5%. We further evaluate its\nfeature representation quality through extensive ablation studies, validating\nthe necessity of each module.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09323v1", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2506.22370", "title": "Can Large Language Models Help Students Prove Software Correctness? An Experimental Study with Dafny", "authors": ["Carolina Carreira", "Álvaro Silva", "Alexandre Abreu", "Alexandra Mendes"], "categories": ["cs.SE", "cs.PL"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.22370v3", "summary": "Students in computing education increasingly use large language models (LLMs)\nsuch as ChatGPT. Yet, the role of LLMs in supporting cognitively demanding\ntasks, like deductive program verification, remains poorly understood. This\npaper investigates how students interact with an LLM when solving formal\nverification exercises in Dafny, a language that supports functional\ncorrectness, by allowing programmers to write formal specifications and\nautomatically verifying that the implementation satisfies the specification. We\nconducted a mixed-methods study with master's students enrolled in a formal\nmethods course. Each participant completed two verification problems, one with\naccess to a custom ChatGPT interface that logged all interactions, and the\nother without. We identified strategies used by successful students and\nassessed the level of trust students place in LLMs. Our findings show that\nstudents perform significantly better when using ChatGPT; however, performance\ngains are tied to prompt quality. We conclude with practical recommendations\nfor integrating LLMs into formal methods courses more effectively, including\ndesigning LLM-aware challenges that promote learning rather than substitution.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22370v3", "cate": "cs.SE", "date": "2025-06-27", "updated": "2025-07-11"}
{"id": "2506.08263", "title": "Learning-Based Multiuser Scheduling in MIMO-OFDM Systems with Hybrid Beamforming", "authors": ["Pouya Agheli", "Tugce Kobal", "François Durand", "Matthew Andrews"], "categories": ["cs.IT", "cs.LG", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      Published in the proceedings of the European Conference on Networks and Communications (EuCNC) & 6G Summit, 2025", "url": "http://arxiv.org/abs/2506.08263v2", "summary": "We investigate the multiuser scheduling problem in multiple-input\nmultiple-output (MIMO) systems using orthogonal frequency division multiplexing\n(OFDM) and hybrid beamforming in which a base station (BS) communicates with\nmultiple users over millimeter wave (mmWave) channels in the downlink. Improved\nscheduling is critical for enhancing spectral efficiency and the long-term\nperformance of the system from the perspective of proportional fairness (PF)\nmetric in hybrid beamforming systems due to its limited multiplexing gain. Our\nobjective is to maximize PF by properly designing the analog and digital\nprecoders within the hybrid beamforming and selecting the users subject to the\nnumber of radio frequency (RF) chains. Leveraging the characteristics of mmWave\nchannels, we apply a two-timescale protocol. On a long timescale, we assign an\nanalog beam to each user. Scheduling the users and designing the digital\nprecoder are done accordingly on a short timescale. To conduct scheduling, we\npropose combinatorial solutions, such as greedy and sorting algorithms,\nfollowed by a machine learning (ML) approach. Our numerical results highlight\nthe trade-off between the performance and complexity of the proposed\napproaches. Consequently, we show that the choice of approach depends on the\nspecific criteria within a given scenario.", "comment": "Published in the proceedings of the European Conference on Networks\n  and Communications (EuCNC) & 6G Summit, 2025", "pdf_url": "http://arxiv.org/pdf/2506.08263v2", "cate": "cs.IT", "date": "2025-06-09", "updated": "2025-07-12"}
{"id": "2507.09507", "title": "Nearly Tight Sample Complexity for Matroid Online Contention Resolution", "authors": ["Moran Feldman", "Ola Svensson", "Rico Zenklusen"], "categories": ["cs.DS", "cs.DM", "cs.GT", "68W27 (Primary) 91A68, 68R05 (Secondary)", "F.2.2; G.2.1"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      24 pages", "url": "http://arxiv.org/abs/2507.09507v1", "summary": "Due to their numerous applications, in particular in Mechanism Design,\nProphet Inequalities have experienced a surge of interest. They describe\ncompetitive ratios for basic stopping time problems where random variables get\nrevealed sequentially. A key drawback in the classical setting is the\nassumption of full distributional knowledge of the involved random variables,\nwhich is often unrealistic. A natural way to address this is via sample-based\napproaches, where only a limited number of samples from the distribution of\neach random variable is available. Recently, Fu, Lu, Gavin Tang, Wu, Wu, and\nZhang (2024) showed that sample-based Online Contention Resolution Schemes\n(OCRS) are a powerful tool to obtain sample-based Prophet Inequalities. They\npresented the first sample-based OCRS for matroid constraints, which is a\nheavily studied constraint family in this context, as it captures many\ninteresting settings. This allowed them to get the first sample-based Matroid\nProphet Inequality, using $O(\\log^4 n)$ many samples (per random variable),\nwhere $n$ is the number of random variables, while obtaining a constant\ncompetitiveness of $\\frac{1}{4}-\\varepsilon$.\n  We present a nearly optimal sample-based OCRS for matroid constraints, which\nuses only $O(\\log \\rho \\cdot \\log^2\\log\\rho)$ many samples, almost matching a\nknown lower bound of $\\Omega(\\log \\rho)$, where $\\rho \\leq n$ is the rank of\nthe matroid. Through the above-mentioned connection to Prophet Inequalities,\nthis yields a sample-based Matroid Prophet Inequality using only $O(\\log n +\n\\log\\rho \\cdot \\log^2\\log\\rho)$ many samples, and matching the competitiveness\nof $\\frac{1}{4}-\\varepsilon$, which is the best known competitiveness for the\nconsidered almighty adversary setting even when the distributions are fully\nknown.", "comment": "24 pages", "pdf_url": "http://arxiv.org/pdf/2507.09507v1", "cate": "cs.DS", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2502.10218", "title": "A Multi-Simulation Approach with Model Predictive Control for Anafi Drones", "authors": ["Pascal Goldschmid", "Aamir Ahmad"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 pages, 7 figures, accepted for publication at ECMR 2025", "url": "http://arxiv.org/abs/2502.10218v2", "summary": "Simulation frameworks are essential for the safe development of robotic\napplications. However, different components of a robotic system are often best\nsimulated in different environments, making full integration challenging. This\nis particularly true for partially-open or closed-source simulators, which\ncommonly suffer from two limitations: (i) lack of runtime control over scene\nactors via interfaces like ROS, and (ii) restricted access to real-time state\ndata (e.g., pose, velocity) of scene objects. In the first part of this work,\nwe address these issues by integrating aerial drones simulated in Parrot's\nSphinx environment (used for Anafi drones) into the Gazebo simulator. Our\napproach uses a mirrored drone instance embedded within Gazebo environments to\nbridge the two simulators. One key application is aerial target tracking, a\ncommon task in multi-robot systems. However, Parrot's default PID-based\ncontroller lacks the agility needed for tracking fast-moving targets. To\novercome this, in the second part of this work we develop a model predictive\ncontroller (MPC) that leverages cumulative error states to improve tracking\naccuracy. Our MPC significantly outperforms the built-in PID controller in\ndynamic scenarios, increasing the effectiveness of the overall system. We\nvalidate our integrated framework by incorporating the Anafi drone into an\nexisting Gazebo-based airship simulation and rigorously test the MPC against a\ncustom PID baseline in both simulated and real-world experiments.", "comment": "8 pages, 7 figures, accepted for publication at ECMR 2025", "pdf_url": "http://arxiv.org/pdf/2502.10218v2", "cate": "cs.RO", "date": "2025-02-14", "updated": "2025-07-12"}
{"id": "2507.08871", "title": "Next-Generation Travel Demand Modeling with a Generative Framework for Household Activity Coordination", "authors": ["Xishun Liao", "Haoxuan Ma", "Yifan Liu", "Yuxiang Wei", "Brian Yueshuai He", "Chris Stanford", "Jiaqi Ma"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      8 pages, 7 figures", "url": "http://arxiv.org/abs/2507.08871v1", "summary": "Travel demand models are critical tools for planning, policy, and mobility\nsystem design. Traditional activity-based models (ABMs), although grounded in\nbehavioral theories, often rely on simplified rules and assumptions, and are\ncostly to develop and difficult to adapt across different regions. This paper\npresents a learning-based travel demand modeling framework that synthesizes\nhousehold-coordinated daily activity patterns based on a household's\nsocio-demographic profiles. The whole framework integrates population\nsynthesis, coordinated activity generation, location assignment, and\nlarge-scale microscopic traffic simulation into a unified system. It is fully\ngenerative, data-driven, scalable, and transferable to other regions. A\nfull-pipeline implementation is conducted in Los Angeles with a 10 million\npopulation. Comprehensive validation shows that the model closely replicates\nreal-world mobility patterns and matches the performance of legacy ABMs with\nsignificantly reduced modeling cost and greater scalability. With respect to\nthe SCAG ABM benchmark, the origin-destination matrix achieves a cosine\nsimilarity of 0.97, and the daily vehicle miles traveled (VMT) in the network\nyields a 0.006 Jensen-Shannon Divergence (JSD) and a 9.8% mean absolute\npercentage error (MAPE). When compared to real-world observations from Caltrans\nPeMS, the evaluation on corridor-level traffic speed and volume reaches a 0.001\nJSD and a 6.11% MAPE.", "comment": "8 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.08871v1", "cate": "cs.LG", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.09334", "title": "Fast3D: Accelerating 3D Multi-modal Large Language Models for Efficient 3D Scene Understanding", "authors": ["Wencan Huang", "Daizong Liu", "Wei Hu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ACM MM 2025", "url": "http://arxiv.org/abs/2507.09334v1", "summary": "While 3D Multi-modal Large Language Models (MLLMs) demonstrate remarkable\nscene understanding capabilities, their practical deployment faces critical\nchallenges due to computational inefficiency. The key bottleneck stems from\nprocessing excessive object-centric visual tokens required for comprehensive 3D\nscene representation. Although visual token pruning has shown promise in\naccelerating 2D MLLMs, its applicability to 3D domains remains largely\nunexplored due to fundamental disparities in token structures. In this paper,\nwe reveal two critical insights: (1) Significant redundancy exists in\nobject-level 3D token representations, analogous to patch-level redundancy in\n2D systems; (2) Global attention patterns exhibit strong predictive power for\nidentifying non-essential tokens in 3D contexts. Building on these\nobservations, we propose Fast3D, a plug-and-play visual token pruning framework\nfor 3D MLLMs featuring two technical innovations: (1) Global Attention\nPrediction (GAP), where a lightweight neural network learns to predict the\nglobal attention distributions of the target model, enabling efficient token\nimportance estimation for precise pruning guidance; (2) Sample-Adaptive visual\ntoken Pruning (SAP), which introduces dynamic token budgets through\nattention-based complexity assessment, automatically adjusting layer-wise\npruning ratios based on input characteristics. Both of these two techniques\noperate without modifying the parameters of the target model. Extensive\nevaluations across five benchmarks validate the effectiveness of Fast3D,\nparticularly under high visual token pruning ratios. Code is available at\nhttps://github.com/wencan25/Fast3D", "comment": "Accepted by ACM MM 2025", "pdf_url": "http://arxiv.org/pdf/2507.09334v1", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.08730", "title": "Dually Hierarchical Drift Adaptation for Online Configuration Performance Learning", "authors": ["Zezhen Xiang", "Jingzhi Gong", "Tao Chen"], "categories": ["cs.SE", "cs.AI"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Accepted by ICSE 2026", "url": "http://arxiv.org/abs/2507.08730v2", "summary": "Modern configurable software systems need to learn models that correlate\nconfiguration and performance. However, when the system operates in dynamic\nenvironments, the workload variations, hardware changes, and system updates\nwill inevitably introduce concept drifts at different levels - global drifts,\nwhich reshape the performance landscape of the entire configuration space; and\nlocal drifts, which only affect certain sub-regions of that space. As such,\nexisting offline and transfer learning approaches can struggle to adapt to\nthese implicit and unpredictable changes in real-time, rendering configuration\nperformance learning challenging. To address this, we propose DHDA, an online\nconfiguration performance learning framework designed to capture and adapt to\nthese drifts at different levels. The key idea is that DHDA adapts to both the\nlocal and global drifts using dually hierarchical adaptation: at the upper\nlevel, we redivide the data into different divisions, within each of which the\nlocal model is retrained, to handle global drifts only when necessary. At the\nlower level, the local models of the divisions can detect local drifts and\nadapt themselves asynchronously. To balance responsiveness and efficiency, DHDA\ncombines incremental updates with periodic full retraining to minimize\nredundant computation when no drifts are detected. Through evaluating eight\nsoftware systems and against state-of-the-art approaches, we show that DHDA\nachieves considerably better accuracy and can effectively adapt to drifts with\nup to 2x improvements, while incurring reasonable overhead and is able to\nimprove different local models in handling concept drift.", "comment": "Accepted by ICSE 2026", "pdf_url": "http://arxiv.org/pdf/2507.08730v2", "cate": "cs.SE", "date": "2025-07-11", "updated": "2025-07-14"}
{"id": "2506.16983", "title": "Maximal Achievable Service Rates of Codes and Connections to Combinatorial Designs", "authors": ["Hoang Ly", "Emina Soljanin"], "categories": ["cs.IT", "math.CO", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      7 and a half pages, zero figure", "url": "http://arxiv.org/abs/2506.16983v2", "summary": "We investigate the service-rate region (SRR) of distributed storage systems\nthat employ linear codes. We focus on systems where each server stores one code\nsymbol, and a user recovers a data symbol by accessing any of its recovery\ngroups, subject to per-server capacity limits. The SRR--the convex polytope of\nsimultaneously achievable request rates--captures system throughput and\nscalability. We first derive upper and lower bounds on the maximum request rate\nof each data object. These bounds hold for all linear codes and depend only on\nthe number of parity checks orthogonal to a particular set of codeword\ncoordinates associated with that object, i.e., the equations used in\nmajority-logic decoding, and on code parameters. We then check the bound\nsaturation for 1) all non-systematic codes whose SRRs are already known and 2)\nsystematic codes. For the former, we prove the bounds are tight. For systematic\ncodes, we show that the upper bound is achieved whenever the supports of\nminimum-weight dual codewords form a 2-design. As an application, we determine\nthe exact per-object demand limits for binary Hamming codes. Our framework\nprovides a new lens to address the SRR problem through combinatorial design\ntheory.", "comment": "7 and a half pages, zero figure", "pdf_url": "http://arxiv.org/pdf/2506.16983v2", "cate": "cs.IT", "date": "2025-06-20", "updated": "2025-07-13"}
{"id": "2507.09620", "title": "Paths and Intersections: Exact Emulators for Planar Graphs", "authors": ["George Z. Li", "Zihan Tan", "Tianyi Zhang"], "categories": ["cs.DS", "math.CO"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      FOCS 2025", "url": "http://arxiv.org/abs/2507.09620v1", "summary": "We study vertex sparsification for preserving distances in planar graphs.\nGiven an edge-weighted planar graph with $k$ terminals, the goal is to\nconstruct an emulator, which is a smaller edge-weighted planar graph that\ncontains the terminals and exactly preserves the pairwise distances between\nthem. We construct exact planar emulators of size $O(f^2k^2)$ in the setting\nwhere terminals lie on $f$ faces in the planar embedding of the input graph.\nOur result generalizes and interpolates between the previous results of Chang\nand Ophelders and Goranci, Henzinger, and Peng which is an $O(k^2)$ bound in\nthe setting where all terminals lie on a single face (i.e., $f=1$), and the\nresult of Krauthgamer, Nguyen, and Zondiner, which is an $O(k^4)$ bound for the\ngeneral case (i.e., $f=k$).\n  Our construction follows a recent new way of analyzing graph structures, by\nviewing graphs as paths and their intersections, which we believe is of\nindependent interest.", "comment": "FOCS 2025", "pdf_url": "http://arxiv.org/pdf/2507.09620v1", "cate": "cs.DS", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2503.03912", "title": "GO-VMP: Global Optimization for View Motion Planning in Fruit Mapping", "authors": ["Allen Isaac Jose", "Sicong Pan", "Tobias Zaenker", "Rohit Menon", "Sebastian Houben", "Maren Bennewitz"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Allen Isaac Jose and Sicong Pan have equal contribution. Publication to appear in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2025", "url": "http://arxiv.org/abs/2503.03912v2", "summary": "Automating labor-intensive tasks such as crop monitoring with robots is\nessential for enhancing production and conserving resources. However,\nautonomously monitoring horticulture crops remains challenging due to their\ncomplex structures, which often result in fruit occlusions. Existing view\nplanning methods attempt to reduce occlusions but either struggle to achieve\nadequate coverage or incur high robot motion costs. We introduce a global\noptimization approach for view motion planning that aims to minimize robot\nmotion costs while maximizing fruit coverage. To this end, we leverage coverage\nconstraints derived from the set covering problem (SCP) within a shortest\nHamiltonian path problem (SHPP) formulation. While both SCP and SHPP are\nwell-established, their tailored integration enables a unified framework that\ncomputes a global view path with minimized motion while ensuring full coverage\nof selected targets. Given the NP-hard nature of the problem, we employ a\nregion-prior-based selection of coverage targets and a sparse graph structure\nto achieve effective optimization outcomes within a limited time. Experiments\nin simulation demonstrate that our method detects more fruits, enhances surface\ncoverage, and achieves higher volume accuracy than the motion-efficient\nbaseline with a moderate increase in motion cost, while significantly reducing\nmotion costs compared to the coverage-focused baseline. Real-world experiments\nfurther confirm the practical applicability of our approach.", "comment": "Allen Isaac Jose and Sicong Pan have equal contribution. Publication\n  to appear in IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS), 2025", "pdf_url": "http://arxiv.org/pdf/2503.03912v2", "cate": "cs.RO", "date": "2025-03-05", "updated": "2025-07-14"}
{"id": "2507.08873", "title": "Contrastive Language-Image Pre-Training Model based Semantic Communication Performance Optimization", "authors": ["Shaoran Yang", "Dongyu Wei", "Hanzhi Yu", "Zhaohui Yang", "Yuchen Liu", "Mingzhe Chen"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Submitted to IEEE GLOBECOM 2025", "url": "http://arxiv.org/abs/2507.08873v1", "summary": "In this paper, a novel contrastive language-image pre-training (CLIP) model\nbased semantic communication framework is designed. Compared to standard neural\nnetwork (e.g.,convolutional neural network) based semantic encoders and\ndecoders that require joint training over a common dataset, our CLIP model\nbased method does not require any training procedures thus enabling a\ntransmitter to extract data meanings of the original data without neural\nnetwork model training, and the receiver to train a neural network for\nfollow-up task implementation without the communications with the transmitter.\nNext, we investigate the deployment of the CLIP model based semantic framework\nover a noisy wireless network. Since the semantic information generated by the\nCLIP model is susceptible to wireless noise and the spectrum used for semantic\ninformation transmission is limited, it is necessary to jointly optimize CLIP\nmodel architecture and spectrum resource block (RB) allocation to maximize\nsemantic communication performance while considering wireless noise, the delay\nand energy used for semantic communication. To achieve this goal, we use a\nproximal policy optimization (PPO) based reinforcement learning (RL) algorithm\nto learn how wireless noise affect the semantic communication performance thus\nfinding optimal CLIP model and RB for each user. Simulation results show that\nour proposed method improves the convergence rate by up to 40%, and the\naccumulated reward by 4x compared to soft actor-critic.", "comment": "Submitted to IEEE GLOBECOM 2025", "pdf_url": "http://arxiv.org/pdf/2507.08873v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.09338", "title": "Simplifying Traffic Anomaly Detection with Video Foundation Models", "authors": ["Svetlana Orlova", "Tommie Kerssies", "Brunó B. Englert", "Gijs Dubbelman"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCVW 2025 accepted. Code: this https URL", "url": "http://arxiv.org/abs/2507.09338v1", "summary": "Recent methods for ego-centric Traffic Anomaly Detection (TAD) often rely on\ncomplex multi-stage or multi-representation fusion architectures, yet it\nremains unclear whether such complexity is necessary. Recent findings in visual\nperception suggest that foundation models, enabled by advanced pre-training,\nallow simple yet flexible architectures to outperform specialized designs.\nTherefore, in this work, we investigate an architecturally simple encoder-only\napproach using plain Video Vision Transformers (Video ViTs) and study how\npre-training enables strong TAD performance. We find that: (i) strong\npre-training enables simple encoder-only models to match or even surpass the\nperformance of specialized state-of-the-art TAD methods, while also being\nsignificantly more efficient; (ii) although weakly- and fully-supervised\npre-training are advantageous on standard benchmarks, we find them less\neffective for TAD. Instead, self-supervised Masked Video Modeling (MVM)\nprovides the strongest signal; and (iii) Domain-Adaptive Pre-Training (DAPT) on\nunlabeled driving videos further improves downstream performance, without\nrequiring anomalous examples. Our findings highlight the importance of\npre-training and show that effective, efficient, and scalable TAD models can be\nbuilt with minimal architectural complexity. We release our code,\ndomain-adapted encoders, and fine-tuned models to support future work:\nhttps://github.com/tue-mps/simple-tad.", "comment": "ICCVW 2025 accepted. Code: https://github.com/tue-mps/simple-tad", "pdf_url": "http://arxiv.org/pdf/2507.09338v1", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2502.17075", "title": "Equality Saturation for Optimizing High-Level Julia IR", "authors": ["Jules Merckx", "Tim Besard", "Bjorn De Sutter"], "categories": ["cs.PL", "cs.SE"], "primary_category": "Subjects:       Programming Languages (cs.PL)", "pdf_link": null, "comments": "Comments:      Submitted to ACM Transactions on Architecture and Code Optimization (TACO)", "url": "http://arxiv.org/abs/2502.17075v2", "summary": "Compilers are indispensable for transforming code written in high-level\nlanguages into performant machine code, but their general-purpose optimizations\nsometimes fall short. Domain experts might be aware of certain optimizations\nthat the compiler is unable to apply or that are only valid in a particular\ndomain. We have developed a system that allows domain experts to express\nrewrite rules to optimize code in the Julia programming language. Our system\nbuilds on e-graphs and equality saturation. It can apply optimizations in the\npresence of control flow and side effects. As Julia uses multiple dispatch, we\nallow users to constrain rewrite rules by argument types, and propagate type\ninformation through the e-graph representation. We propose an ILP formulation\nfor optimal e-graph extraction taking into account dominance properties for\ncode reuse and introduce CFG skeleton relaxation to rewrite calls to pure\nfunctions as well as those with side effects. Use cases demonstrate that our\nsystem can perform rewrites on high-level, domain-specific code, as well as on\nlower-level code such as Julia's broadcasting mechanism. Finally, we analyze\nthe required compilation time.", "comment": "Submitted to ACM Transactions on Architecture and Code Optimization\n  (TACO)", "pdf_url": "http://arxiv.org/pdf/2502.17075v2", "cate": "cs.PL", "date": "2025-02-24", "updated": "2025-07-13"}
{"id": "2506.19305", "title": "Poset-Markov Channels: Capacity via Group Symmetry", "authors": ["Eray Unsal Atay", "Eitan Levin", "Venkat Chandrasekaran", "Victoria Kostina"], "categories": ["cs.IT", "math.IT", "math.OC"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      44 pages, 11 figures", "url": "http://arxiv.org/abs/2506.19305v2", "summary": "Computing channel capacity is in general intractable because it is given by\nthe limit of a sequence of optimization problems whose dimensionality grows to\ninfinity. As a result, constant-sized characterizations of feedback or\nnon-feedback capacity are known for only a few classes of channels with memory.\nThis paper introduces poset-causal channels$\\unicode{x2014}$a new formalism of\na communication channel in which channel inputs and outputs are indexed by the\nelements of a partially ordered set (poset). We develop a novel methodology\nthat allows us to establish a single-letter upper bound on the feedback\ncapacity of a subclass of poset-causal channels whose memory structure exhibits\na Markov property and symmetry. The methodology is based on symmetry reduction\nin optimization. We instantiate our method on two channel models: the Noisy\nOutput is The STate (NOST) channel$\\unicode{x2014}$for which the bound is\ntight$\\unicode{x2014}$and a new two-dimensional extension of it.", "comment": "44 pages, 11 figures", "pdf_url": "http://arxiv.org/pdf/2506.19305v2", "cate": "cs.IT", "date": "2025-06-24", "updated": "2025-07-14"}
{"id": "2507.09688", "title": "Minimum-Peak-Cost Flows Over Time", "authors": ["Mariia Anapolska", "Emma Ahrens", "Christina Büsing", "Felix Engelhardt", "Timo Gersing", "Corinna Mathwieser", "Sabrian Schmitz", "Sophia Wrede"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09688v1", "summary": "When planning transportation whose operation requires non-consumable\nresources, the peak demand for allocated resources is often of higher interest\nthan the duration of resource usage. For instance, it is more cost-effective to\ndeliver parcels with a single truck over eight hours than to use two trucks for\nfour hours, as long as the time suffices. To model such scenarios, we introduce\nthe novel minimum peak cost flow over time problem, whose objective is to\nminimise the maximum cost at all points in time rather than minimising the\nintegral of costs. We focus on minimising peak costs of temporally repeated\nflows. These are desirable for practical applications due to their simple\nstructure. This yields the minimum-peak-cost Temporally Repeated flow problem\n(MPC-TRF).\n  We show that the simple structure of temporally repeated flows comes with the\ndrawback of arbitrarily bad approximation ratios compared to general flows over\ntime. Furthermore, our complexity analysis shows the integral version of\nMPC-TRF is strongly NP-hard, even under strong restrictions. On the positive\nside, we identify two benign special cases: unit-cost series-parallel networks\nand networks with time horizon at least twice as long as the longest path in\nthe network (with respect to the transit time). In both cases, we show that\nintegral optimal flows if the desired flow value equals the maximum flow value\nand fractional optimal flows for arbitrary flow values can be found in\npolynomial time. For each of these cases, we provide an explicit algorithm that\nconstructs an optimal solution.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09688v1", "cate": "cs.DS", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09140", "title": "Interactive Drawing Guidance for Anime Illustrations with Diffusion Model", "authors": ["Chuang Chen", "Xiaoxuan Xie", "Yongming Zhang", "Tianyu Zhang", "Haoran Xie"], "categories": ["cs.GR"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      9 pages, 7 figures. In proceedings of NICOGRAPH International 2025", "url": "http://arxiv.org/abs/2507.09140v1", "summary": "Creating high-quality anime illustrations presents notable challenges,\nparticularly for beginners, due to the intricate styles and fine details\ninherent in anime art. We present an interactive drawing guidance system\nspecifically designed for anime illustrations to address this issue. It offers\nreal-time guidance to help users refine their work and streamline the creative\nprocess. Our system is built upon the StreamDiffusion pipeline to deliver\nreal-time drawing assistance. We fine-tune Stable Diffusion with LoRA to\nsynthesize anime style RGB images from user-provided hand-drawn sketches and\nprompts. Leveraging the Informative Drawings model, we transform these RGB\nimages into rough sketches, which are further refined into structured guidance\nsketches using a custom-designed optimizer. The proposed system offers precise,\nreal-time guidance aligned with the creative intent of the user, significantly\nenhancing both the efficiency and accuracy of the drawing process. To assess\nthe effectiveness of our approach, we conducted a user study, gathering\nempirical feedback on both system performance and interface usability.", "comment": "9 pages, 7 figures. In proceedings of NICOGRAPH International 2025", "pdf_url": "http://arxiv.org/pdf/2507.09140v1", "cate": "cs.GR", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2503.06241", "title": "A Noise-Robust Turn-Taking System for Real-World Dialogue Robots: A Field Experiment", "authors": ["Koji Inoue", "Yuki Okafuji", "Jun Baba", "Yoshiki Ohira", "Katsuya Hyodo", "Tatsuya Kawahara"], "categories": ["cs.RO", "cs.CL", "cs.SD"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      This paper has been accepted for presentation at IEEE/RSJ International Conference on Intelligent Robots and Systems 2025 (IROS 2025) and represents the author's version of the work", "url": "http://arxiv.org/abs/2503.06241v2", "summary": "Turn-taking is a crucial aspect of human-robot interaction, directly\ninfluencing conversational fluidity and user engagement. While previous\nresearch has explored turn-taking models in controlled environments, their\nrobustness in real-world settings remains underexplored. In this study, we\npropose a noise-robust voice activity projection (VAP) model, based on a\nTransformer architecture, to enhance real-time turn-taking in dialogue robots.\nTo evaluate the effectiveness of the proposed system, we conducted a field\nexperiment in a shopping mall, comparing the VAP system with a conventional\ncloud-based speech recognition system. Our analysis covered both subjective\nuser evaluations and objective behavioral analysis. The results showed that the\nproposed system significantly reduced response latency, leading to a more\nnatural conversation where both the robot and users responded faster. The\nsubjective evaluations suggested that faster responses contribute to a better\ninteraction experience.", "comment": "This paper has been accepted for presentation at IEEE/RSJ\n  International Conference on Intelligent Robots and Systems 2025 (IROS 2025)\n  and represents the author's version of the work", "pdf_url": "http://arxiv.org/pdf/2503.06241v2", "cate": "cs.RO", "date": "2025-03-08", "updated": "2025-07-14"}
{"id": "2507.08877", "title": "ODIA: Oriented Distillation for Inline Acceleration of LLM-based Function Calling", "authors": ["Hanlong Zhang", "Jingsheng Yang", "Hao Li", "Yuhao He", "Franck Gong"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08877v1", "summary": "Function Calling is a crucial technique that enables Large Language Models\n(LLMs) to interact with external systems through APIs. However, the high\nlatency associated with LLM-based Function Calling significantly impacts user\nexperience. This paper presents a novel approach called Oriented Distillation\nfor Inline Acceleration (ODIA) that leverages online user interaction data to\naccelerate Function Calling. By automatically identifying \"simple queries\" from\nproduction traffic and distilling knowledge from larger models to smaller ones,\nour method reduces response latency by 45% (expected) and 78% (median) while\nmaintaining accuracy. We demonstrate the effectiveness of our approach through\nreal-world deployment in a music application, where the smaller model\nsuccessfully handles 60% of traffic with negligible accuracy loss. Our method\nrequires minimal human intervention and continuously improves through automated\ndata collection and model updating, making it a practical solution for\nproduction environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08877v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.09375", "title": "Automated Multi-Class Crop Pathology Classification via Convolutional Neural Networks: A Deep Learning Approach for Real-Time Precision Agriculture", "authors": ["Sourish Suri", "Yifei Shao"], "categories": ["cs.CV", "I.2.6; I.5.4"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      29 pages, 10 figures, 1 table. Code available at: this https URL", "url": "http://arxiv.org/abs/2507.09375v1", "summary": "Crop diseases present a significant barrier to agricultural productivity and\nglobal food security, especially in large-scale farming where early\nidentification is often delayed or inaccurate. This research introduces a\nConvolutional Neural Network (CNN)-based image classification system designed\nto automate the detection and classification of eight common crop diseases\nusing leaf imagery. The methodology involves a complete deep learning pipeline:\nimage acquisition from a large, labeled dataset, preprocessing via resizing,\nnormalization, and augmentation, and model training using TensorFlow with\nKeras' Sequential API. The CNN architecture comprises three convolutional\nlayers with increasing filter sizes and ReLU activations, followed by max\npooling, flattening, and fully connected layers, concluding with a softmax\noutput for multi-class classification. The system achieves high training\naccuracy (~90%) and demonstrates reliable performance on unseen data, although\na validation accuracy of ~60% suggests minor overfitting. Notably, the model\nintegrates a treatment recommendation module, providing actionable guidance by\nmapping each detected disease to suitable pesticide or fungicide interventions.\nFurthermore, the solution is deployed on an open-source, mobile-compatible\nplatform, enabling real-time image-based diagnostics for farmers in remote\nareas. This research contributes a scalable and accessible tool to the field of\nprecision agriculture, reducing reliance on manual inspection and promoting\nsustainable disease management practices. By merging deep learning with\npractical agronomic support, this work underscores the potential of CNNs to\ntransform crop health monitoring and enhance food production resilience on a\nglobal scale.", "comment": "29 pages, 10 figures, 1 table. Code available at:\n  https://github.com/Sourish85/CNN-CROP-DIS-DETECTOR", "pdf_url": "http://arxiv.org/pdf/2507.09375v1", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.04209", "title": "Mutual Information Bounds for Lossy Common Information", "authors": ["Anderson de Andrade"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.04209v3", "summary": "We show the mutual information between the targets in a Gray-Wyner Network as\na bound that separates Wyner's lossy common information and G\\'acs-K\\\"orner\nlossy common information. The results are a generalization of the lossless case\npresented by Wyner (1975).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.04209v3", "cate": "cs.IT", "date": "2025-07-06", "updated": "2025-07-13"}
{"id": "2507.09711", "title": "Phase transition of the Sinkhorn-Knopp algorithm", "authors": ["Kun He"], "categories": ["cs.DS", "cs.LG", "stat.ML"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      44 pages, 2 figures", "url": "http://arxiv.org/abs/2507.09711v1", "summary": "The matrix scaling problem, particularly the Sinkhorn-Knopp algorithm, has\nbeen studied for over 60 years. In practice, the algorithm often yields\nhigh-quality approximations within just a few iterations. Theoretically,\nhowever, the best-known upper bound places it in the class of\npseudopolynomial-time approximation algorithms. Meanwhile, the lower-bound\nlandscape remains largely unexplored. Two fundamental questions persist: what\naccounts for the algorithm's strong empirical performance, and can a tight\nbound on its iteration count be established?\n  For an $n\\times n$ matrix, its normalized version is obtained by dividing\neach entry by its largest entry. We say that a normalized matrix has a density\n$\\gamma$ if there exists a constant $\\rho > 0$ such that one row or column has\nexactly $\\lceil \\gamma n \\rceil$ entries with values at least $\\rho$, and every\nother row and column has at least $\\lceil \\gamma n \\rceil$ such entries.\n  For the upper bound, we show that the Sinkhorn-Knopp algorithm produces a\nnearly doubly stochastic matrix in $O(\\log n - \\log \\varepsilon)$ iterations\nand $\\widetilde{O}(n^2)$ time for all nonnegative square matrices whose\nnormalized version has a density $\\gamma > 1/2$. Such matrices cover both the\nalgorithm's principal practical inputs and its typical theoretical regime, and\nthe $\\widetilde{O}(n^2)$ runtime is optimal.\n  For the lower bound, we establish a tight bound of\n$\\widetilde{\\Omega}\\left(n^{1/2}/\\varepsilon\\right)$ iterations for positive\nmatrices under the $\\ell_2$-norm error measure. Moreover, for every $\\gamma <\n1/2$, there exists a matrix with density $\\gamma$ for which the algorithm\nrequires $\\Omega\\left(n^{1/2}/\\varepsilon\\right)$ iterations.\n  In summary, our results reveal a sharp phase transition in the Sinkhorn-Knopp\nalgorithm at the density threshold $\\gamma = 1/2$.", "comment": "44 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.09711v1", "cate": "cs.DS", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09146", "title": "Physics-Aware Fluid Field Generation from User Sketches Using Helmholtz-Hodge Decomposition", "authors": ["Ryuichi Miyauchi", "Hengyuan Chang", "Tsukasa Fukusato", "Kazunori Miyata", "Haoran Xie"], "categories": ["cs.GR"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      8 pages, 12 figures. In proceedings of NICOGRAPH International 2025", "url": "http://arxiv.org/abs/2507.09146v1", "summary": "Fluid simulation techniques are widely used in various fields such as film\nproduction, but controlling complex fluid behaviors remains challenging. While\nrecent generative models enable intuitive generation of vector fields from user\nsketches, they struggle to maintain physical properties such as\nincompressibility. To address these issues, this paper proposes a method for\ninteractively designing 2D vector fields. Conventional generative models can\nintuitively generate vector fields from user sketches, but remain difficult to\nconsider physical properties. Therefore, we add a simple editing process after\ngenerating the vector field. In the first stage, we use a latent diffusion\nmodel~(LDM) to automatically generate initial 2D vector fields from user\nsketches. In the second stage, we apply the Helmholtz-Hodge decomposition to\nlocally extract physical properties such as incompressibility from the results\ngenerated by LDM and recompose them according to user intentions. Through\nmultiple experiments, we demonstrate the effectiveness of our proposed method.", "comment": "8 pages, 12 figures. In proceedings of NICOGRAPH International 2025", "pdf_url": "http://arxiv.org/pdf/2507.09146v1", "cate": "cs.GR", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2503.17846", "title": "Smart Ankleband for Plug-and-Play Hand-Prosthetic Control", "authors": ["Dean Zadok", "Oren Salzman", "Alon Wolf", "Alex M. Bronstein"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.17846v2", "summary": "Building robotic prostheses requires a sensor-based interface designed to\nprovide the robotic hand with the control required to perform hand gestures.\nTraditional Electromyography (EMG) based prosthetics and emerging alternatives\noften face limitations such as muscle-activation limitations, high cost, and\ncomplex calibrations. In this paper, we present a low-cost robotic system\ncomposed of a smart ankleband for intuitive, calibration-free control of a\nrobotic hand, and a robotic prosthetic hand that executes actions corresponding\nto leg gestures. The ankleband integrates an Inertial Measurement Unit (IMU)\nsensor with a lightweight neural network to infer user-intended leg gestures\nfrom motion data. Our system represents a significant step towards higher\nadoption rates of robotic prostheses among arm amputees, as it enables one to\noperate a prosthetic hand using a low-cost, low-power, and calibration-free\nsolution. To evaluate our work, we collected data from 10 subjects and tested\nour prototype ankleband with a robotic hand on an individual with an upper-limb\namputation. Our results demonstrate that this system empowers users to perform\ndaily tasks more efficiently, requiring few compensatory movements.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.17846v2", "cate": "cs.RO", "date": "2025-03-22", "updated": "2025-07-13"}
{"id": "2507.08890", "title": "Overview of the TREC 2023 deep learning track", "authors": ["Nick Craswell", "Bhaskar Mitra", "Emine Yilmaz", "Hossein A. Rahmani", "Daniel Campos", "Jimmy Lin", "Ellen M. Voorhees", "Ian Soboroff"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      arXiv admin note: substantial text overlap with arXiv:2507.08191", "url": "http://arxiv.org/abs/2507.08890v1", "summary": "This is the fifth year of the TREC Deep Learning track. As in previous years,\nwe leverage the MS MARCO datasets that made hundreds of thousands of\nhuman-annotated training labels available for both passage and document ranking\ntasks. We mostly repeated last year's design, to get another matching test set,\nbased on the larger, cleaner, less-biased v2 passage and document set, with\npassage ranking as primary and document ranking as a secondary task (using\nlabels inferred from passage). As we did last year, we sample from MS MARCO\nqueries that were completely held out, unused in corpus construction, unlike\nthe test queries in the first three years. This approach yields a more\ndifficult test with more headroom for improvement. Alongside the usual MS MARCO\n(human) queries from MS MARCO, this year we generated synthetic queries using a\nfine-tuned T5 model and using a GPT-4 prompt.\n  The new headline result this year is that runs using Large Language Model\n(LLM) prompting in some way outperformed runs that use the \"nnlm\" approach,\nwhich was the best approach in the previous four years. Since this is the last\nyear of the track, future iterations of prompt-based ranking can happen in\nother tracks. Human relevance assessments were applied to all query types, not\njust human MS MARCO queries. Evaluation using synthetic queries gave similar\nresults to human queries, with system ordering agreement of $\\tau=0.8487$.\nHowever, human effort was needed to select a subset of the synthetic queries\nthat were usable. We did not see clear evidence of bias, where runs using GPT-4\nwere favored when evaluated using synthetic GPT-4 queries, or where runs using\nT5 were favored when evaluated on synthetic T5 queries.", "comment": "arXiv admin note: substantial text overlap with arXiv:2507.08191", "pdf_url": "http://arxiv.org/pdf/2507.08890v1", "cate": "cs.IR", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.09410", "title": "GreenCrossingAI: A Camera Trap/Computer Vision Pipeline for Environmental Science Research Groups", "authors": ["Bernie Boscoe", "Shawn Johnson", "Andrea Osborn", "Chandler Campbell", "Karen Mager"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      This is the preprint version of the paper in Practice and Experience in Advanced Research Computing, PEARC25", "url": "http://arxiv.org/abs/2507.09410v1", "summary": "Camera traps have long been used by wildlife researchers to monitor and study\nanimal behavior, population dynamics, habitat use, and species diversity in a\nnon-invasive and efficient manner. While data collection from the field has\nincreased with new tools and capabilities, methods to develop, process, and\nmanage the data, especially the adoption of ML/AI tools, remain challenging.\nThese challenges include the sheer volume of data generated, the need for\naccurate labeling and annotation, variability in environmental conditions\naffecting data quality, and the integration of ML/AI tools into existing\nworkflows that often require domain-specific customization and computational\nresources. This paper provides a guide to a low-resource pipeline to process\ncamera trap data on-premise, incorporating ML/AI capabilities tailored for\nsmall research groups with limited resources and computational expertise. By\nfocusing on practical solutions, the pipeline offers accessible approaches for\ndata transmission, inference, and evaluation, enabling researchers to discover\nmeaningful insights from their ever-increasing camera trap datasets.", "comment": "This is the preprint version of the paper in Practice and Experience\n  in Advanced Research Computing, PEARC25", "pdf_url": "http://arxiv.org/pdf/2507.09410v1", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2110.14842", "title": "Towards the ultimate limits of quantum channel discrimination and quantum communication", "authors": ["Kun Fang", "Gilad Gour", "Xin Wang"], "categories": ["quant-ph", "cs.IT", "math-ph", "math.IT", "math.MP", "math.ST", "stat.TH"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      v3, close to the journal version", "url": "http://arxiv.org/abs/2110.14842v3", "summary": "Distinguishability is fundamental to information theory and extends naturally\nto quantum systems. While quantum state discrimination is well understood,\nquantum channel discrimination remains challenging due to the dynamic nature of\nchannels and the variety of discrimination strategies. This work advances the\nunderstanding of quantum channel discrimination and its fundamental limits. We\ndevelop new tools for quantum divergences, including sharper bounds on the\nquantum hypothesis testing relative entropy and additivity results for channel\ndivergences. We establish a quantum Stein's lemma for memoryless channel\ndiscrimination, and link the strong converse property to the asymptotic\nequipartition property and continuity of divergences. Notably, we prove the\nequivalence of exponentially strong converse properties under coherent and\nsequential strategies. We further explore the interplay among operational\nregimes, discrimination strategies, and channel divergences, deriving exponents\nin various settings and contributing to a unified framework for channel\ndiscrimination. Finally, we recast quantum communication tasks as\ndiscrimination problems, uncovering deep connections between channel\ncapacities, channel discrimination, and the mathematical structure of channel\ndivergences. These results bridge two core areas of quantum information theory\nand offer new insights for future exploration.", "comment": "v3, close to the journal version", "pdf_url": "http://arxiv.org/pdf/2110.14842v3", "cate": "quant-ph", "date": "2021-10-28", "updated": "2025-07-11"}
{"id": "2507.09729", "title": "Improved Directed Expander Decompositions", "authors": ["Henry Fleischmann", "George Z. Li", "Jason Li"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      54 pages, 3 figures", "url": "http://arxiv.org/abs/2507.09729v1", "summary": "We obtain faster expander decomposition algorithms for directed graphs,\nmatching the guarantees of Saranurak and Wang (SODA 2019) for expander\ndecomposition on undirected graphs. Our algorithms are faster than prior work\nand also generalize almost losslessly to capacitated graphs. In particular, we\nobtain the first directed expander decomposition algorithm for capacitated\ngraphs in near-linear time with optimal dependence on $\\phi$.\n  To obtain our result, we provide the first implementation and analysis of the\nnon-stop cut-matching game for directed, capacitated graphs. All existing\ndirected expander decomposition algorithms instead temporarily add ''fake\nedges'' before pruning them away in a final cleanup step. Our result shows that\nthe natural undirected approach applies even to directed graphs. The difficulty\nis in its analysis, which is technical and requires significant modifications\nfrom the original setting of undirected graphs.", "comment": "54 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2507.09729v1", "cate": "cs.DS", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09441", "title": "RectifiedHR: High-Resolution Diffusion via Energy Profiling and Adaptive Guidance Scheduling", "authors": ["Ankit Sanjyal"], "categories": ["cs.GR", "cs.CV"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      8 Pages, 10 Figures, Pre-Print Version, Code Available at: this https URL", "url": "http://arxiv.org/abs/2507.09441v1", "summary": "High-resolution image synthesis with diffusion models often suffers from\nenergy instabilities and guidance artifacts that degrade visual quality. We\nanalyze the latent energy landscape during sampling and propose adaptive\nclassifier-free guidance (CFG) schedules that maintain stable energy\ntrajectories. Our approach introduces energy-aware scheduling strategies that\nmodulate guidance strength over time, achieving superior stability scores\n(0.9998) and consistency metrics (0.9873) compared to fixed-guidance\napproaches. We demonstrate that DPM++ 2M with linear-decreasing CFG scheduling\nyields optimal performance, providing sharper, more faithful images while\nreducing artifacts. Our energy profiling framework serves as a powerful\ndiagnostic tool for understanding and improving diffusion model behavior.", "comment": "8 Pages, 10 Figures, Pre-Print Version, Code Available at:\n  https://github.com/ANKITSANJYAL/RectifiedHR", "pdf_url": "http://arxiv.org/pdf/2507.09441v1", "cate": "cs.GR", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2503.18738", "title": "RoboEngine: Plug-and-Play Robot Data Augmentation with Semantic Robot Segmentation and Background Generation", "authors": ["Chengbo Yuan", "Suraj Joshi", "Shaoting Zhu", "Hang Su", "Hang Zhao", "Yang Gao"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Project Page: this https URL", "url": "http://arxiv.org/abs/2503.18738v2", "summary": "Visual augmentation has become a crucial technique for enhancing the visual\nrobustness of imitation learning. However, existing methods are often limited\nby prerequisites such as camera calibration or the need for controlled\nenvironments (e.g., green screen setups). In this work, we introduce\nRoboEngine, the first plug-and-play visual robot data augmentation toolkit. For\nthe first time, users can effortlessly generate physics- and task-aware robot\nscenes with just a few lines of code. To achieve this, we present a novel robot\nscene segmentation dataset, a generalizable high-quality robot segmentation\nmodel, and a fine-tuned background generation model, which together form the\ncore components of the out-of-the-box toolkit. Using RoboEngine, we demonstrate\nthe ability to generalize robot manipulation tasks across six entirely new\nscenes, based solely on demonstrations collected from a single scene, achieving\na more than 200% performance improvement compared to the no-augmentation\nbaseline. All datasets, model weights, and the toolkit are released\nhttps://roboengine.github.io/", "comment": "Project Page: https://roboengine.github.io/", "pdf_url": "http://arxiv.org/pdf/2503.18738v2", "cate": "cs.RO", "date": "2025-03-24", "updated": "2025-07-13"}
{"id": "2507.08898", "title": "SEALGuard: Safeguarding the Multilingual Conversations in Southeast Asian Languages for LLM Software Systems", "authors": ["Wenliang Shan", "Michael Fu", "Rui Yang", "Chakkrit", "Tantithamthavorn"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Under Review at Information and Software Technology", "url": "http://arxiv.org/abs/2507.08898v1", "summary": "Safety alignment is critical for LLM-powered systems. While recent\nLLM-powered guardrail approaches such as LlamaGuard achieve high detection\naccuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''),\nthey struggle with multilingual unsafe inputs. This limitation leaves LLM\nsystems vulnerable to unsafe and jailbreak prompts written in low-resource\nlanguages such as those in Southeast Asia. This paper introduces SEALGuard, a\nmultilingual guardrail designed to improve the safety alignment across diverse\nlanguages. It aims to address the multilingual safety alignment gap of existing\nguardrails and ensure effective filtering of unsafe and jailbreak prompts in\nLLM-powered systems. We adapt a general-purpose multilingual language model\ninto a multilingual guardrail using low-rank adaptation (LoRA). We construct\nSEALSBench, a large-scale multilingual safety alignment dataset containing over\n260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases.\nWe evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on\nthis benchmark. Our findings show that multilingual unsafe and jailbreak\nprompts substantially degrade the performance of the state-of-the-art\nLlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and\n18%, respectively, compared to its performance on English-only prompts. In\ncontrast, SEALGuard outperforms existing guardrails in detecting multilingual\nunsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and\nachieving the best DSR, precision, and F1-score. Our ablation study further\nreveals the contributions of adaptation strategies and model size to the\noverall performance of SEALGuard. SEALGuard advances the safety alignment of\nLLM systems by introducing an effective multilingual guardrail.", "comment": "Under Review at Information and Software Technology", "pdf_url": "http://arxiv.org/pdf/2507.08898v1", "cate": "cs.CL", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09446", "title": "Efficient Multi-Person Motion Prediction by Lightweight Spatial and Temporal Interactions", "authors": ["Yuanhong Zheng", "Ruixuan Yu", "Jian Sun"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025", "url": "http://arxiv.org/abs/2507.09446v1", "summary": "3D multi-person motion prediction is a highly complex task, primarily due to\nthe dependencies on both individual past movements and the interactions between\nagents. Moreover, effectively modeling these interactions often incurs\nsubstantial computational costs. In this work, we propose a computationally\nefficient model for multi-person motion prediction by simplifying spatial and\ntemporal interactions. Our approach begins with the design of lightweight dual\nbranches that learn local and global representations for individual and\nmultiple persons separately. Additionally, we introduce a novel cross-level\ninteraction block to integrate the spatial and temporal representations from\nboth branches. To further enhance interaction modeling, we explicitly\nincorporate the spatial inter-person distance embedding. With above efficient\ntemporal and spatial design, we achieve state-of-the-art performance for\nmultiple metrics on standard datasets of CMU-Mocap, MuPoTS-3D, and 3DPW, while\nsignificantly reducing the computational cost. Code is available at\nhttps://github.com/Yuanhong-Zheng/EMPMP.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.09446v1", "cate": "cs.CV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2407.13728", "title": "Barycentric bounds on the error exponents of quantum hypothesis exclusion", "authors": ["Kaiyuan Ji", "Hemant K. Mishra", "Milán Mosonyi", "Mark M. Wilde"], "categories": ["quant-ph", "cs.IT", "math-ph", "math.IT", "math.MP"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      64 pages, 1 figure", "url": "http://arxiv.org/abs/2407.13728v3", "summary": "Quantum state exclusion is an operational task that has significance in\nstudying foundational questions related to interpreting quantum theory. In such\na task, one is given a system whose state is randomly selected from a finite\nset, and the goal is to identify a state from the set that is not the true\nstate of the system. An error, i.e., an unsuccessful exclusion, occurs if and\nonly if the state identified is the true state. In this paper, we study the\noptimal error probability of quantum state exclusion and its error exponent --\nthe rate at which the error probability decays asymptotically -- from an\ninformation-theoretic perspective. Our main finding is a single-letter upper\nbound on the error exponent of state exclusion given by the multivariate\nlog-Euclidean Chernoff divergence, and we prove that this improves upon the\nbest previously known upper bound. We also extend our analysis to the more\ncomplicated task of quantum channel exclusion, and we establish a single-letter\nand efficiently computable upper bound on its error exponent, even assuming the\nuse of adaptive strategies. We derive both upper bounds, for state and channel\nexclusion, based on one-shot analysis and formulate them as a type of\nmultivariate divergence measure called a barycentric Chernoff divergence.\nMoreover, our result on channel exclusion has implications in two important\nspecial cases. First, for the special case of two hypotheses, our upper bound\nprovides the first known efficiently computable upper bound on the error\nexponent of symmetric binary channel discrimination. Second, for the special\ncase of classical channels, we show that our upper bound is achievable by a\nparallel strategy, thus solving the exact error exponent of classical channel\nexclusion and generalising a similar result on symmetric binary classical\nchannel discrimination.", "comment": "64 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2407.13728v3", "cate": "quant-ph", "date": "2024-07-18", "updated": "2025-07-14"}
{"id": "2507.09879", "title": "Covering a Few Submodular Constraints and Applications", "authors": ["Tanvi Bajpai", "Chandra Chekuri", "Pooja Kulkarni"], "categories": ["cs.DS", "cs.AI", "cs.GT"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      34 pages. Accepted to APPROX 2025", "url": "http://arxiv.org/abs/2507.09879v1", "summary": "We consider the problem of covering multiple submodular constraints. Given a\nfinite ground set $N$, a cost function $c: N \\rightarrow \\mathbb{R}_+$, $r$\nmonotone submodular functions $f_1,f_2,\\ldots,f_r$ over $N$ and requirements\n$b_1,b_2,\\ldots,b_r$ the goal is to find a minimum cost subset $S \\subseteq N$\nsuch that $f_i(S) \\ge b_i$ for $1 \\le i \\le r$. When $r=1$ this is the\nwell-known Submodular Set Cover problem. Previous work\n\\cite{chekuri2022covering} considered the setting when $r$ is large and\ndeveloped bi-criteria approximation algorithms, and approximation algorithms\nfor the important special case when each $f_i$ is a weighted coverage function.\nThese are fairly general models and capture several concrete and interesting\nproblems as special cases. The approximation ratios for these problem are at\nleast $\\Omega(\\log r)$ which is unavoidable when $r$ is part of the input. In\nthis paper, motivated by some recent applications, we consider the problem when\n$r$ is a \\emph{fixed constant} and obtain two main results. For covering\nmultiple submodular constraints we obtain a randomized bi-criteria\napproximation algorithm that for any given integer $\\alpha \\ge 1$ outputs a set\n$S$ such that $f_i(S) \\ge$ $(1-1/e^\\alpha -\\epsilon)b_i$ for each $i \\in [r]$\nand $\\mathbb{E}[c(S)] \\le (1+\\epsilon)\\alpha \\cdot \\sf{OPT}$. Second, when the\n$f_i$ are weighted coverage functions from a deletion-closed set system we\nobtain a $(1+\\epsilon)$ $(\\frac{e}{e-1})$ $(1+\\beta)$-approximation where\n$\\beta$ is the approximation ratio for the underlying set cover instances via\nthe natural LP. These results show that one can obtain nearly as good an\napproximation for any fixed $r$ as what one would achieve for $r=1$. We mention\nsome applications that follow easily from these general results and anticipate\nmore in the future.", "comment": "34 pages. Accepted to APPROX 2025", "pdf_url": "http://arxiv.org/pdf/2507.09879v1", "cate": "cs.DS", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09704", "title": "Real-time and Controllable Reactive Motion Synthesis via Intention Guidance", "authors": ["Xiaotang Zhang", "Ziyi Chang", "Qianhui Men", "Hubert Shum"], "categories": ["cs.GR"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09704v1", "summary": "We propose a real-time method for reactive motion synthesis based on the\nknown trajectory of input character, predicting instant reactions using only\nhistorical, user-controlled motions. Our method handles the uncertainty of\nfuture movements by introducing an intention predictor, which forecasts key\njoint intentions to make pose prediction more deterministic from the historical\ninteraction. The intention is later encoded into the latent space of its\nreactive motion, matched with a codebook which represents mappings between\ninput and output. It samples a categorical distribution for pose generation and\nstrengthens model robustness through adversarial training. Unlike previous\noffline approaches, the system can recursively generate intentions and reactive\nmotions using feedback from earlier steps, enabling real-time, long-term\nrealistic interactive synthesis. Both quantitative and qualitative experiments\nshow our approach outperforms other matching-based motion synthesis approaches,\ndelivering superior stability and generalizability. In our method, user can\nalso actively influence the outcome by controlling the moving directions,\ncreating a personalized interaction path that deviates from predefined\ntrajectories.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09704v1", "cate": "cs.GR", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2504.06513", "title": "Safe Navigation in Uncertain Crowded Environments Using Risk Adaptive CVaR Barrier Functions", "authors": ["Xinyi Wang", "Taekyung Kim", "Bardh Hoxha", "Georgios Fainekos", "Dimitra Panagou"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.06513v3", "summary": "Robot navigation in dynamic, crowded environments poses a significant\nchallenge due to the inherent uncertainties in the obstacle model. In this\nwork, we propose a risk-adaptive approach based on the Conditional\nValue-at-Risk Barrier Function (CVaR-BF), where the risk level is automatically\nadjusted to accept the minimum necessary risk, achieving a good performance in\nterms of safety and optimization feasibility under uncertainty. Additionally,\nwe introduce a dynamic zone-based barrier function which characterizes the\ncollision likelihood by evaluating the relative state between the robot and the\nobstacle. By integrating risk adaptation with this new function, our approach\nadaptively expands the safety margin, enabling the robot to proactively avoid\nobstacles in highly dynamic environments. Comparisons and ablation studies\ndemonstrate that our method outperforms existing social navigation approaches,\nand validate the effectiveness of our proposed framework.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.06513v3", "cate": "cs.RO", "date": "2025-04-09", "updated": "2025-07-14"}
{"id": "2507.08902", "title": "Generation of structure-guided pMHC-I libraries using Diffusion Models", "authors": ["Sergio Mares", "Ariel Espinoza Weinberger", "Nilah M. Ioannidis"], "categories": ["q-bio.QM", "cs.AI"], "primary_category": "Subjects:       Quantitative Methods (q-bio.QM)", "pdf_link": null, "comments": "Comments:      Accepted to the The 2nd Workshop on Generative AI and Biology ICML Workshop 2025", "url": "http://arxiv.org/abs/2507.08902v1", "summary": "Personalized vaccines and T-cell immunotherapies depend critically on\nidentifying peptide-MHC class I (pMHC-I) interactions capable of eliciting\npotent immune responses. However, current benchmarks and models inherit biases\npresent in mass-spectrometry and binding-assay datasets, limiting discovery of\nnovel peptide ligands. To address this issue, we introduce a structure-guided\nbenchmark of pMHC-I peptides designed using diffusion models conditioned on\ncrystal structure interaction distances. Spanning twenty high-priority HLA\nalleles, this benchmark is independent of previously characterized peptides yet\nreproduces canonical anchor residue preferences, indicating structural\ngeneralization without experimental dataset bias. Using this resource, we\ndemonstrate that state-of-the-art sequence-based predictors perform poorly at\nrecognizing the binding potential of these structurally stable designs,\nindicating allele-specific limitations invisible in conventional evaluations.\nOur geometry-aware design pipeline yields peptides with high predicted\nstructural integrity and higher residue diversity than existing datasets,\nrepresenting a key resource for unbiased model training and evaluation. Our\ncode, and data are available at: https://github.com/sermare/struct-mhc-dev.", "comment": "Accepted to the The 2nd Workshop on Generative AI and Biology ICML\n  Workshop 2025", "pdf_url": "http://arxiv.org/pdf/2507.08902v1", "cate": "q-bio.QM", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09471", "title": "CKAA: Cross-subspace Knowledge Alignment and Aggregation for Robust Continual Learning", "authors": ["Lingfeng He", "De Cheng", "Zhiheng Ma", "Huaijie Wang", "Dingwen Zhang", "Nannan Wang", "Xinbo Gao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09471v1", "summary": "Continual Learning (CL) empowers AI models to continuously learn from\nsequential task streams. Recently, parameter-efficient fine-tuning (PEFT)-based\nCL methods have garnered increasing attention due to their superior\nperformance. They typically allocate a unique sub-module for learning each\ntask, with a task recognizer to select the appropriate sub-modules for testing\nimages. However, due to the feature subspace misalignment from independently\ntrained sub-modules, these methods tend to produce ambiguous decisions under\nmisleading task-ids. To address this, we propose Cross-subspace Knowledge\nAlignment and Aggregation (CKAA), a novel framework that enhances model\nrobustness against misleading task-ids through two key innovations: (1)\nDual-level Knowledge Alignment (DKA): By aligning intra-class feature\ndistributions across different subspaces and learning a robust global\nclassifier through a feature simulation process, DKA enables the model to\ndistinguish features from both correct and incorrect subspaces during training.\n(2) Task-Confidence-guided Mixture of Adapters (TC-MoA): A robust inference\nscheme that adaptively aggregates task-specific knowledge from relevant\nsub-modules based on task-confidence scores, avoiding overconfidence in\nmisleading task-id predictions. Extensive experiments demonstrate that CKAA\noutperforms existing PEFT-based CL methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09471v1", "cate": "cs.CV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2409.03664", "title": "The Kneser--Poulsen phenomena for entropy", "authors": ["Gautam Aishwarya", "Dongbin Li"], "categories": ["math.MG", "cs.IT", "math.IT", "math.PR", "37C10, 94A17, 52A40, 52A20"], "primary_category": "Subjects:       Metric Geometry (math.MG)", "pdf_link": null, "comments": "Comments:      14 pages, final version", "url": "http://arxiv.org/abs/2409.03664v3", "summary": "The Kneser--Poulsen conjecture asserts that the volume of a union of balls in\nEuclidean space cannot be increased by bringing their centres pairwise closer.\nWe prove that its natural information-theoretic counterpart is true. This\nfollows from a complete answer to a question asked in arXiv:2210.12842 about\nGaussian convolutions, namely that the R\\'enyi entropy comparisons between a\nprobability measure and its contractive image are preserved when both undergo\nsimultaneous heat flow. An inequality that unifies Costa's result on the\nconcavity of entropy power with the entropic Kneser--Poulsen theorem is also\npresented.", "comment": "14 pages, final version", "pdf_url": "http://arxiv.org/pdf/2409.03664v3", "cate": "math.MG", "date": "2024-09-05", "updated": "2025-07-14"}
{"id": "2507.10125", "title": "Improved bicriteria approximation for $k$-edge-connectivity", "authors": ["Zeev Nutov"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10125v1", "summary": "In the $k$-Edge Connected Spanning Subgraph ($k$-ECSS) problem we are given a\n(multi-)graph $G=(V,E)$ with edge costs and an integer $k$, and seek a min-cost\n$k$-edge-connected spanning subgraph of $G$. The problem admits a\n$2$-approximation algorithm and no better approximation ratio is\nknown.Hershkowitz, Klein, and Zenklusen [STOC 24] gave a bicriteria\n$(1,k-10)$-approximation algorithm that computes a $(k-10)$-edge-connected\nspanning subgraph of cost at most the optimal value of a standard Cut-LP for\n$k$-ECSS. This LP bicriteria approximation was recently improved by Cohen and\nNutov [ESA 25] to $(1,k-4)$, where also was given a bicriteria approximation\n$(3/2,k-2)$. In this paper we improve the bicriteria approximation to $(1,k-2)$\nfor $k$ even and to $\\left(1-\\frac{1}{k},k-3\\right)$ for $k$ is odd, and also\ngive another bicriteria approximation $(3/2,k-1)$.\n  The $k$-Edge-Connected Spanning Multi-subgraph ($k$-ECSM) problem is almost\nthe same as $k$-ECSS, except that any edge can be selected multiple times at\nthe same cost. The previous best approximation ratio for $k$-ECSM was $1+4/k$.\nOur result improves this to $1+\\frac{2}{k}$ for $k$ even and to $1+\\frac{3}{k}$\nfor $k$ odd, where for $k$ odd the computed subgraph is in fact\n$(k+1)$-edge-connected.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10125v1", "cate": "cs.DS", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09792", "title": "CADmium: Fine-Tuning Code Language Models for Text-Driven Sequential CAD Design", "authors": ["Prashant Govindarajan", "Davide Baldelli", "Jay Pathak", "Quentin Fournier", "Sarath Chandar"], "categories": ["cs.GR", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09792v1", "summary": "Computer-aided design (CAD) is the digital construction of 2D and 3D objects,\nand is central to a wide range of engineering and manufacturing applications\nlike automobile and aviation. Despite its importance, CAD modeling remains\nlargely a time-intensive, manual task. Recent works have attempted to automate\nthis process with small transformer-based models and handcrafted CAD sequence\nrepresentations. However, there has been little effort to leverage the\npotential of large language models (LLMs) for sequential CAD design. In this\nwork, we introduce a new large-scale dataset of more than 170k CAD models\nannotated with high-quality, human-like descriptions generated with our\npipeline based on GPT-4.1. Using this dataset, we fine-tune powerful code-LLMs\nto generate CAD sequences represented in a JSON-based format from natural\nlanguage descriptions, demonstrating the viability and effectiveness of this\napproach for text-conditioned CAD generation. Because simple metrics often fail\nto reflect the quality of generated objects, we introduce geometric and\ntopological metrics based on sphericity, mean curvature, and Euler\ncharacteristic to provide richer structural insights. Our experiments and\nablation studies on both synthetic and human-annotated data demonstrate that\nCADmium is able to automate CAD design, drastically speeding up the design of\nnew objects. The dataset, code, and fine-tuned models are available online.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09792v1", "cate": "cs.GR", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2504.08438", "title": "Diffusion Models for Robotic Manipulation: A Survey", "authors": ["Rosa Wolf", "Yitian Shi", "Sheng Liu", "Rania Rayyes"], "categories": ["cs.RO", "stat.ML"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      26 pages, 2 figure, 9 tables", "url": "http://arxiv.org/abs/2504.08438v3", "summary": "Diffusion generative models have demonstrated remarkable success in visual\ndomains such as image and video generation. They have also recently emerged as\na promising approach in robotics, especially in robot manipulations. Diffusion\nmodels leverage a probabilistic framework, and they stand out with their\nability to model multi-modal distributions and their robustness to\nhigh-dimensional input and output spaces. This survey provides a comprehensive\nreview of state-of-the-art diffusion models in robotic manipulation, including\ngrasp learning, trajectory planning, and data augmentation. Diffusion models\nfor scene and image augmentation lie at the intersection of robotics and\ncomputer vision for vision-based tasks to enhance generalizability and data\nscarcity. This paper also presents the two main frameworks of diffusion models\nand their integration with imitation learning and reinforcement learning. In\naddition, it discusses the common architectures and benchmarks and points out\nthe challenges and advantages of current state-of-the-art diffusion-based\nmethods.", "comment": "26 pages, 2 figure, 9 tables", "pdf_url": "http://arxiv.org/pdf/2504.08438v3", "cate": "cs.RO", "date": "2025-04-11", "updated": "2025-07-14"}
{"id": "2507.08905", "title": "Last Layer Hamiltonian Monte Carlo", "authors": ["Koen Vellenga", "H. Joe Steinhauer", "Göran Falkman", "Jonas Andersson", "Anders Sjögren"], "categories": ["cs.LG", "cs.AI", "G.3"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      25 pages, 15 figures, 6 tables, currently under submission", "url": "http://arxiv.org/abs/2507.08905v1", "summary": "We explore the use of Hamiltonian Monte Carlo (HMC) sampling as a\nprobabilistic last layer approach for deep neural networks (DNNs). While HMC is\nwidely regarded as a gold standard for uncertainty estimation, the\ncomputational demands limit its application to large-scale datasets and large\nDNN architectures. Although the predictions from the sampled DNN parameters can\nbe parallelized, the computational cost still scales linearly with the number\nof samples (similar to an ensemble). Last layer HMC (LL--HMC) reduces the\nrequired computations by restricting the HMC sampling to the final layer of a\nDNN, making it applicable to more data-intensive scenarios with limited\ncomputational resources. In this paper, we compare LL-HMC against five last\nlayer probabilistic deep learning (LL-PDL) methods across three real-world\nvideo datasets for driver action and intention. We evaluate the in-distribution\nclassification performance, calibration, and out-of-distribution (OOD)\ndetection. Due to the stochastic nature of the probabilistic evaluations, we\nperformed five grid searches for different random seeds to avoid being reliant\non a single initialization for the hyperparameter configurations. The results\nshow that LL--HMC achieves competitive in-distribution classification and OOD\ndetection performance. Additional sampled last layer parameters do not improve\nthe classification performance, but can improve the OOD detection. Multiple\nchains or starting positions did not yield consistent improvements.", "comment": "25 pages, 15 figures, 6 tables, currently under submission", "pdf_url": "http://arxiv.org/pdf/2507.08905v1", "cate": "cs.LG", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09487", "title": "HMID-Net: An Exploration of Masked Image Modeling and Knowledge Distillation in Hyperbolic Space", "authors": ["Changli Wang", "Fang Yin", "Jiafeng Liu", "Rui Wu"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09487v1", "summary": "Visual and semantic concepts are often structured in a hierarchical manner.\nFor instance, textual concept `cat' entails all images of cats. A recent study,\nMERU, successfully adapts multimodal learning techniques from Euclidean space\nto hyperbolic space, effectively capturing the visual-semantic hierarchy.\nHowever, a critical question remains: how can we more efficiently train a model\nto capture and leverage this hierarchy? In this paper, we propose the\n\\textit{Hyperbolic Masked Image and Distillation Network} (HMID-Net), a novel\nand efficient method that integrates Masked Image Modeling (MIM) and knowledge\ndistillation techniques within hyperbolic space. To the best of our knowledge,\nthis is the first approach to leverage MIM and knowledge distillation in\nhyperbolic space to train highly efficient models. In addition, we introduce a\ndistillation loss function specifically designed to facilitate effective\nknowledge transfer in hyperbolic space. Our experiments demonstrate that MIM\nand knowledge distillation techniques in hyperbolic space can achieve the same\nremarkable success as in Euclidean space. Extensive evaluations show that our\nmethod excels across a wide range of downstream tasks, significantly\noutperforming existing models like MERU and CLIP in both image classification\nand retrieval.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09487v1", "cate": "cs.CV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2502.02777", "title": "Space-bounded online Kolmogorov complexity is additive", "authors": ["Bruno Bauwens", "Maria Marchenko"], "categories": ["cs.CC", "cs.IT", "math.IT"], "primary_category": "Subjects:       Computational Complexity (cs.CC)", "pdf_link": null, "comments": "Comments:      This update is just to add acknowledgements", "url": "http://arxiv.org/abs/2502.02777v4", "summary": "The even online Kolmogorov complexity of a string $x = x_1 x_2 \\cdots x_{n}$\nis the minimal length of a program that for all $i\\le n/2$, on input $x_1x_3\n\\cdots x_{2i-1}$ outputs $x_{2i}$. The odd complexity is defined similarly. The\nsum of the odd and even complexities is called the dialogue complexity.\n  In [Bauwens, 2014] it is proven that for all $n$, there exist $n$-bit $x$ for\nwhich the dialogue complexity exceeds the Kolmogorov complexity by $n\\log \\frac\n4 3 + O(\\log n)$. Let $\\mathrm C^s(x)$ denote the Kolmogorov complexity with\nspace bound~$s$. Here, we prove that the space-bounded dialogue complexity with\nbound $s + 6n + O(1)$ is at most $\\mathrm C^{s}(x) + O(\\log (sn))$, where\n$n=|x|$.", "comment": "This update is just to add acknowledgements", "pdf_url": "http://arxiv.org/pdf/2502.02777v4", "cate": "cs.CC", "date": "2025-02-04", "updated": "2025-07-14"}
{"id": "2507.10248", "title": "Bicriteria Submodular Maximization", "authors": ["Moran Feldman", "Alan Kuhnle"], "categories": ["cs.DS", "cs.DM", "68R05 (Primary) 68W25, 90C26 (Secondary)", "F.2.2; G.2.1"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      75 pages, 1 figure", "url": "http://arxiv.org/abs/2507.10248v1", "summary": "Submodular functions and their optimization have found applications in\ndiverse settings ranging from machine learning and data mining to game theory\nand economics. In this work, we consider the constrained maximization of a\nsubmodular function, for which we conduct a principled study of bicriteria\napproximation algorithms -- algorithms which can violate the constraint, but\nonly up to a bounded factor. Bicrteria optimization allows constrained\nsubmodular maximization to capture additional important settings, such as the\nwell-studied submodular cover problem and optimization under soft constraints.\nWe provide results that span both multiple types of constraints (cardinality,\nknapsack, matroid and convex set) and multiple classes of submodular functions\n(monotone, symmetric and general). For many of the cases considered, we provide\noptimal results. In other cases, our results improve over the state-of-the-art,\nsometimes even over the state-of-the-art for the special case of\nsingle-criterion (standard) optimization. Results of the last kind demonstrate\nthat relaxing the feasibility constraint may give a perspective about the\nproblem that is useful even if one only desires feasible solutions.", "comment": "75 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2507.10248v1", "cate": "cs.DS", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10542", "title": "ScaffoldAvatar: High-Fidelity Gaussian Avatars with Patch Expressions", "authors": ["Shivangi Aneja", "Sebastian Weiss", "Irene Baeza", "Prashanth Chandran", "Gaspard Zoss", "Matthias Nießner", "Derek Bradley"], "categories": ["cs.GR", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      (SIGGRAPH 2025) Paper Video: this https URL Project Page: this https URL", "url": "http://arxiv.org/abs/2507.10542v1", "summary": "Generating high-fidelity real-time animated sequences of photorealistic 3D\nhead avatars is important for many graphics applications, including immersive\ntelepresence and movies. This is a challenging problem particularly when\nrendering digital avatar close-ups for showing character's facial microfeatures\nand expressions. To capture the expressive, detailed nature of human heads,\nincluding skin furrowing and finer-scale facial movements, we propose to couple\nlocally-defined facial expressions with 3D Gaussian splatting to enable\ncreating ultra-high fidelity, expressive and photorealistic 3D head avatars. In\ncontrast to previous works that operate on a global expression space, we\ncondition our avatar's dynamics on patch-based local expression features and\nsynthesize 3D Gaussians at a patch level. In particular, we leverage a\npatch-based geometric 3D face model to extract patch expressions and learn how\nto translate these into local dynamic skin appearance and motion by coupling\nthe patches with anchor points of Scaffold-GS, a recent hierarchical scene\nrepresentation. These anchors are then used to synthesize 3D Gaussians\non-the-fly, conditioned by patch-expressions and viewing direction. We employ\ncolor-based densification and progressive training to obtain high-quality\nresults and faster convergence for high resolution 3K training images. By\nleveraging patch-level expressions, ScaffoldAvatar consistently achieves\nstate-of-the-art performance with visually natural motion, while encompassing\ndiverse facial expressions and styles in real time.", "comment": "(SIGGRAPH 2025) Paper Video: https://youtu.be/VyWkgsGdbkk Project\n  Page: https://shivangi-aneja.github.io/projects/scaffoldavatar/", "pdf_url": "http://arxiv.org/pdf/2507.10542v1", "cate": "cs.GR", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2505.19939", "title": "Uncertainty-Aware Safety-Critical Decision and Control for Autonomous Vehicles at Unsignalized Intersections", "authors": ["Ran Yu", "Zhuoren Li", "Lu Xiong", "Wei Han", "Bo Leng"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      7 pages, 4 figures", "url": "http://arxiv.org/abs/2505.19939v2", "summary": "Reinforcement learning (RL) has demonstrated potential in autonomous driving\n(AD) decision tasks. However, applying RL to urban AD, particularly in\nintersection scenarios, still faces significant challenges. The lack of safety\nconstraints makes RL vulnerable to risks. Additionally, cognitive limitations\nand environmental randomness can lead to unreliable decisions in\nsafety-critical scenarios. Therefore, it is essential to quantify confidence in\nRL decisions to improve safety. This paper proposes an Uncertainty-aware\nSafety-Critical Decision and Control (USDC) framework, which generates a\nrisk-averse policy by constructing a risk-aware ensemble distributional RL,\nwhile estimating uncertainty to quantify the policy's reliability.\nSubsequently, a high-order control barrier function (HOCBF) is employed as a\nsafety filter to minimize intervention policy while dynamically enhancing\nconstraints based on uncertainty. The ensemble critics evaluate both HOCBF and\nRL policies, embedding uncertainty to achieve dynamic switching between safe\nand flexible strategies, thereby balancing safety and efficiency. Simulation\ntests on unsignalized intersections in multiple tasks indicate that USDC can\nimprove safety while maintaining traffic efficiency compared to baselines.", "comment": "7 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2505.19939v2", "cate": "cs.RO", "date": "2025-05-26", "updated": "2025-07-14"}
{"id": "2507.08912", "title": "Fair-FLIP: Fair Deepfake Detection with Fairness-Oriented Final Layer Input Prioritising", "authors": ["Tomasz Szandala", "Fatima Ezzeddine", "Natalia Rusin", "Silvia Giordano", "Omran Ayoub"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08912v1", "summary": "Artificial Intelligence-generated content has become increasingly popular,\nyet its malicious use, particularly the deepfakes, poses a serious threat to\npublic trust and discourse. While deepfake detection methods achieve high\npredictive performance, they often exhibit biases across demographic attributes\nsuch as ethnicity and gender. In this work, we tackle the challenge of fair\ndeepfake detection, aiming to mitigate these biases while maintaining robust\ndetection capabilities. To this end, we propose a novel post-processing\napproach, referred to as Fairness-Oriented Final Layer Input Prioritising\n(Fair-FLIP), that reweights a trained model's final-layer inputs to reduce\nsubgroup disparities, prioritising those with low variability while demoting\nhighly variable ones. Experimental results comparing Fair-FLIP to both the\nbaseline (without fairness-oriented de-biasing) and state-of-the-art approaches\nshow that Fair-FLIP can enhance fairness metrics by up to 30% while maintaining\nbaseline accuracy, with only a negligible reduction of 0.25%.\n  Code is available on Github:\nhttps://github.com/szandala/fair-deepfake-detection-toolbox", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08912v1", "cate": "cs.LG", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09491", "title": "GLIMPSE: Do Large Vision-Language Models Truly Think With Videos or Just Glimpse at Them?", "authors": ["Yiyang Zhou", "Linjie Li", "Shi Qiu", "Zhengyuan Yang", "Yuyang Zhao", "Siwei Han", "Yangfan He", "Kangqi Li", "Haonian Ji", "Zihao Zhao", "Haibo Tong", "Lijuan Wang", "Huaxiu Yao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      15 pages, 10 figures", "url": "http://arxiv.org/abs/2507.09491v1", "summary": "Existing video benchmarks often resemble image-based benchmarks, with\nquestion types like \"What actions does the person perform throughout the\nvideo?\" or \"What color is the woman's dress in the video?\" For these, models\ncan often answer by scanning just a few key frames, without deep temporal\nreasoning. This limits our ability to assess whether large vision-language\nmodels (LVLMs) can truly think with videos rather than perform superficial\nframe-level analysis. To address this, we introduce GLIMPSE, a benchmark\nspecifically designed to evaluate whether LVLMs can genuinely think with\nvideos. Unlike prior benchmarks, GLIMPSE emphasizes comprehensive video\nunderstanding beyond static image cues. It consists of 3,269 videos and over\n4,342 highly visual-centric questions across 11 categories, including\nTrajectory Analysis, Temporal Reasoning, and Forensics Detection. All questions\nare carefully crafted by human annotators and require watching the entire video\nand reasoning over full video context-this is what we mean by thinking with\nvideo. These questions cannot be answered by scanning selected frames or\nrelying on text alone. In human evaluations, GLIMPSE achieves 94.82% accuracy,\nbut current LVLMs face significant challenges. Even the best-performing model,\nGPT-o3, reaches only 66.43%, highlighting that LVLMs still struggle to move\nbeyond surface-level reasoning to truly think with videos.", "comment": "15 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.09491v1", "cate": "cs.CV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2502.10826", "title": "Improved Offline Contextual Bandits with Second-Order Bounds: Betting and Freezing", "authors": ["J. Jon Ryu", "Jeongyeol Kwon", "Benjamin Koppe", "Kwang-Sung Jun"], "categories": ["cs.LG", "cs.IT", "math.IT", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      39 pages, 10 figures. COLT 2025", "url": "http://arxiv.org/abs/2502.10826v2", "summary": "We consider off-policy selection and learning in contextual bandits, where\nthe learner aims to select or train a reward-maximizing policy using data\ncollected by a fixed behavior policy. Our contribution is two-fold. First, we\npropose a novel off-policy selection method that leverages a new betting-based\nconfidence bound applied to an inverse propensity weight sequence. Our\ntheoretical analysis reveals that this method achieves a significantly\nimproved, variance-adaptive guarantee over prior work. Second, we propose a\nnovel and generic condition on the optimization objective for off-policy\nlearning that strikes a different balance between bias and variance. One\nspecial case, which we call freezing, tends to induce low variance, which is\npreferred in small-data regimes. Our analysis shows that it matches the best\nexisting guarantees. In our empirical study, our selection method outperforms\nexisting methods, and freezing exhibits improved performance in small-sample\nregimes.", "comment": "39 pages, 10 figures. COLT 2025", "pdf_url": "http://arxiv.org/pdf/2502.10826v2", "cate": "cs.LG", "date": "2025-02-15", "updated": "2025-07-14"}
{"id": "2507.10436", "title": "Approximating Maximum Cut on Interval Graphs and Split Graphs beyond Goemans-Williamson", "authors": ["Jungho Ahn", "Ian DeHaan", "Eun Jung Kim", "Euiwoong Lee"], "categories": ["cs.DS", "F.2.2"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      23 pages, 5 figures, to appear in the proceedings of APPROX 2025", "url": "http://arxiv.org/abs/2507.10436v1", "summary": "We present a polynomial-time $(\\alpha_{GW} + \\varepsilon)$-approximation\nalgorithm for the Maximum Cut problem on interval graphs and split graphs,\nwhere $\\alpha_{GW} \\approx 0.878$ is the approximation guarantee of the\nGoemans-Williamson algorithm and $\\varepsilon > 10^{-34}$ is a fixed constant.\nTo attain this, we give an improved analysis of a slight modification of the\nGoemans-Williamson algorithm for graphs in which triangles can be packed into a\nconstant fraction of their edges. We then pair this analysis with structural\nresults showing that both interval graphs and split graphs either have such a\ntriangle packing or have maximum cut close to their number of edges. We also\nshow that, subject to the Small Set Expansion Hypothesis, there exists a\nconstant $c > 0$ such that there is no polyomial-time $(1 - c)$-approximation\nfor Maximum Cut on split graphs.", "comment": "23 pages, 5 figures, to appear in the proceedings of APPROX 2025", "pdf_url": "http://arxiv.org/pdf/2507.10436v1", "cate": "cs.DS", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2411.10061", "title": "EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation", "authors": ["Rang Meng", "Xingyu Zhang", "Yuming Li", "Chenguang Ma"], "categories": ["cs.GR", "cs.CV"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      CVPR2025", "url": "http://arxiv.org/abs/2411.10061v2", "summary": "Recent work on human animation usually involves audio, pose, or movement maps\nconditions, thereby achieves vivid animation quality. However, these methods\noften face practical challenges due to extra control conditions, cumbersome\ncondition injection modules, or limitation to head region driving. Hence, we\nask if it is possible to achieve striking half-body human animation while\nsimplifying unnecessary conditions. To this end, we propose a half-body human\nanimation method, dubbed EchoMimicV2, that leverages a novel Audio-Pose Dynamic\nHarmonization strategy, including Pose Sampling and Audio Diffusion, to enhance\nhalf-body details, facial and gestural expressiveness, and meanwhile reduce\nconditions redundancy. To compensate for the scarcity of half-body data, we\nutilize Head Partial Attention to seamlessly accommodate headshot data into our\ntraining framework, which can be omitted during inference, providing a free\nlunch for animation. Furthermore, we design the Phase-specific Denoising Loss\nto guide motion, detail, and low-level quality for animation in specific\nphases, respectively. Besides, we also present a novel benchmark for evaluating\nthe effectiveness of half-body human animation. Extensive experiments and\nanalyses demonstrate that EchoMimicV2 surpasses existing methods in both\nquantitative and qualitative evaluations.", "comment": "CVPR2025", "pdf_url": "http://arxiv.org/pdf/2411.10061v2", "cate": "cs.GR", "date": "2024-11-15", "updated": "2025-07-12"}
{"id": "2506.01635", "title": "Riemannian Time Warping: Multiple Sequence Alignment in Curved Spaces", "authors": ["Julian Richter", "Christopher A. Erdös", "Christian Scheurer", "Jochen J. Steil", "Niels Dehio"], "categories": ["cs.RO", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.01635v3", "summary": "Temporal alignment of multiple signals through time warping is crucial in\nmany fields, such as classification within speech recognition or robot motion\nlearning. Almost all related works are limited to data in Euclidean space.\nAlthough an attempt was made in 2011 to adapt this concept to unit quaternions,\na general extension to Riemannian manifolds remains absent. Given its\nimportance for numerous applications in robotics and beyond, we introduce\nRiemannian Time Warping (RTW). This novel approach efficiently aligns multiple\nsignals by considering the geometric structure of the Riemannian manifold in\nwhich the data is embedded. Extensive experiments on synthetic and real-world\ndata, including tests with an LBR iiwa robot, demonstrate that RTW consistently\noutperforms state-of-the-art baselines in both averaging and classification\ntasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.01635v3", "cate": "cs.RO", "date": "2025-06-02", "updated": "2025-07-14"}
{"id": "2507.08920", "title": "AMix-1: A Pathway to Test-Time Scalable Protein Foundation Model", "authors": ["Changze Lv", "Jiang Zhou", "Siyu Long", "Lihao Wang", "Jiangtao Feng", "Dongyu Xue", "Yu Pei", "Hao Wang", "Zherui Zhang", "Yuchen Cai", "Zhiqiang Gao", "Ziyuan Ma", "Jiakai Hu", "Chaochen Gao", "Jingjing Gong", "Yuxuan Song", "Shuyi Zhang", "Xiaoqing Zheng", "Deyi Xiong", "Lei Bai", "Ya-Qin Zhang", "Wei-Ying Ma", "Bowen Zhou", "Hao Zhou"], "categories": ["q-bio.BM", "cs.AI"], "primary_category": "Subjects:       Biomolecules (q-bio.BM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08920v1", "summary": "We introduce AMix-1, a powerful protein foundation model built on Bayesian\nFlow Networks and empowered by a systematic training methodology, encompassing\npretraining scaling laws, emergent capability analysis, in-context learning\nmechanism, and test-time scaling algorithm. To guarantee robust scalability, we\nestablish a predictive scaling law and reveal the progressive emergence of\nstructural understanding via loss perspective, culminating in a strong\n1.7-billion model. Building on this foundation, we devise a multiple sequence\nalignment (MSA)-based in-context learning strategy to unify protein design into\na general framework, where AMix-1 recognizes deep evolutionary signals among\nMSAs and consistently generates structurally and functionally coherent\nproteins. This framework enables the successful design of a dramatically\nimproved AmeR variant with an up to $50\\times$ activity increase over its wild\ntype. Pushing the boundaries of protein engineering, we further empower AMix-1\nwith an evolutionary test-time scaling algorithm for in silico directed\nevolution that delivers substantial, scalable performance gains as verification\nbudgets are intensified, laying the groundwork for next-generation\nlab-in-the-loop protein design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08920v1", "cate": "q-bio.BM", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09492", "title": "SDTN and TRN: Adaptive Spectral-Spatial Feature Extraction for Hyperspectral Image Classification", "authors": ["Fuyin Ye", "Erwen Yao", "Jianyong Chen", "Fengmei He", "Junxiang Zhang", "Lihao Ni"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      4 pages, 2 figures", "url": "http://arxiv.org/abs/2507.09492v1", "summary": "Hyperspectral image classification plays a pivotal role in precision\nagriculture, providing accurate insights into crop health monitoring, disease\ndetection, and soil analysis. However, traditional methods struggle with\nhigh-dimensional data, spectral-spatial redundancy, and the scarcity of labeled\nsamples, often leading to suboptimal performance. To address these challenges,\nwe propose the Self-Adaptive Tensor- Regularized Network (SDTN), which combines\ntensor decomposition with regularization mechanisms to dynamically adjust\ntensor ranks, ensuring optimal feature representation tailored to the\ncomplexity of the data. Building upon SDTN, we propose the Tensor-Regularized\nNetwork (TRN), which integrates the features extracted by SDTN into a\nlightweight network capable of capturing spectral-spatial features at multiple\nscales. This approach not only maintains high classification accuracy but also\nsignificantly reduces computational complexity, making the framework highly\nsuitable for real-time deployment in resource-constrained environments.\nExperiments on PaviaU datasets demonstrate significant improvements in accuracy\nand reduced model parameters compared to state-of-the-art methods.", "comment": "4 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.09492v1", "cate": "cs.CV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2505.00862", "title": "Prime and Co-prime Integer Matrices", "authors": ["Xiang-Gen Xia", "Guangpu Guo"], "categories": ["eess.SP", "cs.DM", "cs.IT", "math.IT"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.00862v3", "summary": "This paper investigates prime and co-prime integer matrices and their\nproperties. It characterizes all pairwise co-prime integer matrices that are\nalso prime integer matrices. This provides a simple way to construct families\nof pairwise co-prime integer matrices, that may have applications in\nmultidimensional co-prime sensing and multidimensional Chinese remainder\ntheorem.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.00862v3", "cate": "eess.SP", "date": "2025-05-01", "updated": "2025-07-12"}
{"id": "2507.09012", "title": "Explicit Bounds and Parallel Algorithms for Counting Multiply Gleeful Numbers", "authors": ["Sara Moore", "Jonathan P. Sorenson"], "categories": ["math.NT", "cs.DS", "11A41, 11Y16, 68Q25", "F.2.1"], "primary_category": "Subjects:       Number Theory (math.NT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09012v1", "summary": "Let $k\\ge 1$ be an integer. A positive integer $n$ is $k$-\\textit{gleeful} if\n$n$ can be represented as the sum of $k$th powers of consecutive primes. For\nexample, $35=2^3+3^3$ is a $3$-gleeful number, and $195=5^2+7^2+11^2$ is\n$2$-gleeful. In this paper, we present some new results on $k$-gleeful numbers\nfor $k>1$.\n  First, we extend previous analytical work. For given values of $x$ and $k$,\nwe give explicit upper and lower bounds on the number of $k$-gleeful\nrepresentations of integers $n\\le x$.\n  Second, we describe and analyze two new, efficient parallel algorithms, one\ntheoretical and one practical, to generate all $k$-gleeful representations up\nto a bound $x$.\n  Third, we study integers that are multiply gleeful, that is, integers with\nmore than one representation as a sum of powers of consecutive primes,\nincluding both the same or different values of $k$. We give a simple heuristic\nmodel for estimating the density of multiply-gleeful numbers, we present\nempirical data in support of our heuristics, and offer some new conjectures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09012v1", "cate": "math.NT", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2504.02361", "title": "MG-Gen: Single Image to Motion Graphics Generation", "authors": ["Takahiro Shirakawa", "Tomoyuki Suzuki", "Takuto Narumoto", "Daichi Haraguchi"], "categories": ["cs.GR", "cs.CV"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.02361v3", "summary": "We introduce MG-Gen, a framework that generates motion graphics directly from\na single raster image. MG-Gen decompose a single raster image into layered\nstructures represented as HTML, generate animation scripts for each layer, and\nthen render them into a video. Experiments confirm MG-Gen generates dynamic\nmotion graphics while preserving text readability and fidelity to the input\nconditions, whereas state-of-the-art image-to-video generation methods struggle\nwith them. The code is available at https://github.com/CyberAgentAILab/MG-GEN.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.02361v3", "cate": "cs.GR", "date": "2025-04-03", "updated": "2025-07-14"}
{"id": "2506.13367", "title": "Uncertainty-Informed Active Perception for Open Vocabulary Object Goal Navigation", "authors": ["Utkarsh Bajpai", "Julius Rückin", "Cyrill Stachniss", "Marija Popović"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      7 pages, 3 figures", "url": "http://arxiv.org/abs/2506.13367v2", "summary": "Mobile robots exploring indoor environments increasingly rely on\nvision-language models to perceive high-level semantic cues in camera images,\nsuch as object categories. Such models offer the potential to substantially\nadvance robot behaviour for tasks such as object-goal navigation (ObjectNav),\nwhere the robot must locate objects specified in natural language by exploring\nthe environment. Current ObjectNav methods heavily depend on prompt engineering\nfor perception and do not address the semantic uncertainty induced by\nvariations in prompt phrasing. Ignoring semantic uncertainty can lead to\nsuboptimal exploration, which in turn limits performance. Hence, we propose a\nsemantic uncertainty-informed active perception pipeline for ObjectNav in\nindoor environments. We introduce a novel probabilistic sensor model for\nquantifying semantic uncertainty in vision-language models and incorporate it\ninto a probabilistic geometric-semantic map to enhance spatial understanding.\nBased on this map, we develop a frontier exploration planner with an\nuncertainty-informed multi-armed bandit objective to guide efficient object\nsearch. Experimental results demonstrate that our method achieves ObjectNav\nsuccess rates comparable to those of state-of-the-art approaches, without\nrequiring extensive prompt engineering.", "comment": "7 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2506.13367v2", "cate": "cs.RO", "date": "2025-06-16", "updated": "2025-07-13"}
{"id": "2507.08924", "title": "From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for LLM Evaluation", "authors": ["Seokhee Hong", "Sunkyoung Kim", "Guijin Son", "Soyeon Kim", "Yeonjung Hong", "Jinsik Lee"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08924v1", "summary": "The development of Large Language Models (LLMs) requires robust benchmarks\nthat encompass not only academic domains but also industrial fields to\neffectively evaluate their applicability in real-world scenarios. In this\npaper, we introduce two Korean expert-level benchmarks. KMMLU-Redux,\nreconstructed from the existing KMMLU, consists of questions from the Korean\nNational Technical Qualification exams, with critical errors removed to enhance\nreliability. KMMLU-Pro is based on Korean National Professional Licensure exams\nto reflect professional knowledge in Korea. Our experiments demonstrate that\nthese benchmarks comprehensively represent industrial knowledge in Korea. We\nrelease our dataset publicly available.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08924v1", "cate": "cs.CL", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09500", "title": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under Visual Variations", "authors": ["Yiwen Liang", "Hui Chen", "Yizhe Xiong", "Zihan Zhou", "Mengyao Lyu", "Zijia Lin", "Shuaicheng Niu", "Sicheng Zhao", "Jungong Han", "Guiguang Ding"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at the 33rd ACM International Conference on Multimedia(ACM MM 2025)", "url": "http://arxiv.org/abs/2507.09500v1", "summary": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under challenging real-world distribution shifts.", "comment": "Accepted at the 33rd ACM International Conference on Multimedia(ACM\n  MM 2025)", "pdf_url": "http://arxiv.org/pdf/2507.09500v1", "cate": "cs.CV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2505.17530", "title": "GPS-Aided Deep Learning for Beam Prediction and Tracking in UAV mmWave Communication", "authors": ["Vendi Ardianto Nugroho", "Byung Moo Lee"], "categories": ["eess.SP", "cs.IT", "cs.LG", "math.IT"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      The code implementation repository: this https URL", "url": "http://arxiv.org/abs/2505.17530v2", "summary": "Millimeter-wave (mmWave) communication enables high data rates for\ncellular-connected Unmanned Aerial Vehicles (UAVs). However, a robust beam\nmanagement remains challenging due to significant path loss and the dynamic\nmobility of UAVs, which can destabilize the UAV-base station (BS) link. This\nresearch presents a GPS-aided deep learning (DL) model that simultaneously\npredicts current and future optimal beams for UAV mmWave communications,\nmaintaining a Top-1 prediction accuracy exceeding 70% and an average power loss\nbelow 0.6 dB across all prediction steps. These outcomes stem from a proposed\ndata set splitting method ensuring balanced label distribution, paired with a\nGPS preprocessing technique that extracts key positional features, and a DL\narchitecture that maps sequential position data to beam index predictions. The\nmodel reduces overhead by approximately 93% (requiring the training of 2 ~ 3\nbeams instead of 32 beams) with 95% beam prediction accuracy guarantees, and\nensures 94% to 96% of predictions exhibit mean power loss not exceeding 1 dB.", "comment": "The code implementation repository:\n  https://github.com/ardiantovn/gpsbeam", "pdf_url": "http://arxiv.org/pdf/2505.17530v2", "cate": "eess.SP", "date": "2025-05-23", "updated": "2025-07-11"}
{"id": "2507.09283", "title": "m-Eternal Domination and Variants on Some Classes of Finite and Infinite Graphs", "authors": ["Tiziana Calamoneri", "Federico Corò", "Neeldhara Misra", "Saraswati G. Nanoti", "Giacomo Paesani"], "categories": ["cs.DM", "cs.CC", "cs.DS", "math.CO", "05C85, 68R10", "G.2.2"], "primary_category": "Subjects:       Discrete Mathematics (cs.DM)", "pdf_link": null, "comments": "Comments:      Accepted for presentation at FCT 2025", "url": "http://arxiv.org/abs/2507.09283v1", "summary": "We study the m-Eternal Domination problem, which is the following two-player\ngame between a defender and an attacker on a graph: initially, the defender\npositions k guards on vertices of the graph; the game then proceeds in turns\nbetween the defender and the attacker, with the attacker selecting a vertex and\nthe defender responding to the attack by moving a guard to the attacked vertex.\nThe defender may move more than one guard on their turn, but guards can only\nmove to neighboring vertices. The defender wins a game on a graph G with k\nguards if the defender has a strategy such that at every point of the game the\nvertices occupied by guards form a dominating set of G and the attacker wins\notherwise. The m-eternal domination number of a graph G is the smallest value\nof k for which (G,k) is a defender win.\n  We show that m-Eternal Domination is NP-hard, as well as some of its\nvariants, even on special classes of graphs. We also show structural results\nfor the Domination and m-Eternal Domination problems in the context of four\ntypes of infinite regular grids: square, octagonal, hexagonal, and triangular,\nestablishing tight bounds.", "comment": "Accepted for presentation at FCT 2025", "pdf_url": "http://arxiv.org/pdf/2507.09283v1", "cate": "cs.DM", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2410.13613", "title": "MEGA: Memory-Efficient 4D Gaussian Splatting for Dynamic Scenes", "authors": ["Xinjie Zhang", "Zhening Liu", "Yifan Zhang", "Xingtong Ge", "Dailan He", "Tongda Xu", "Yan Wang", "Zehong Lin", "Shuicheng Yan", "Jun Zhang"], "categories": ["cs.CV", "cs.GR"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2410.13613v2", "summary": "4D Gaussian Splatting (4DGS) has recently emerged as a promising technique\nfor capturing complex dynamic 3D scenes with high fidelity. It utilizes a 4D\nGaussian representation and a GPU-friendly rasterizer, enabling rapid rendering\nspeeds. Despite its advantages, 4DGS faces significant challenges, notably the\nrequirement of millions of 4D Gaussians, each with extensive associated\nattributes, leading to substantial memory and storage cost. This paper\nintroduces a memory-efficient framework for 4DGS. We streamline the color\nattribute by decomposing it into a per-Gaussian direct color component with\nonly 3 parameters and a shared lightweight alternating current color predictor.\nThis approach eliminates the need for spherical harmonics coefficients, which\ntypically involve up to 144 parameters in classic 4DGS, thereby creating a\nmemory-efficient 4D Gaussian representation. Furthermore, we introduce an\nentropy-constrained Gaussian deformation technique that uses a deformation\nfield to expand the action range of each Gaussian and integrates an\nopacity-based entropy loss to limit the number of Gaussians, thus forcing our\nmodel to use as few Gaussians as possible to fit a dynamic scene well. With\nsimple half-precision storage and zip compression, our framework achieves a\nstorage reduction by approximately 190$\\times$ and 125$\\times$ on the\nTechnicolor and Neural 3D Video datasets, respectively, compared to the\noriginal 4DGS. Meanwhile, it maintains comparable rendering speeds and scene\nrepresentation quality, setting a new standard in the field. Code is available\nat https://github.com/Xinjie-Q/MEGA.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2410.13613v2", "cate": "cs.CV", "date": "2024-10-17", "updated": "2025-07-13"}
{"id": "2507.08945", "title": "GraphRunner: A Multi-Stage Framework for Efficient and Accurate Graph-Based Retrieval", "authors": ["Savini Kashmira", "Jayanaka L. Dantanarayana", "Krisztián Flautner", "Lingjia Tang", "Jason Mars"], "categories": ["cs.IR", "cs.AI"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08945v1", "summary": "Conventional Retrieval Augmented Generation (RAG) approaches are common in\ntext-based applications. However, they struggle with structured, interconnected\ndatasets like knowledge graphs, where understanding underlying relationships is\ncrucial for accurate retrieval. A common direction in graph-based retrieval\nemploys iterative, rule-based traversal guided by Large Language Models (LLMs).\nSuch existing iterative methods typically combine reasoning with single hop\ntraversal at each step, making them vulnerable to LLM reasoning errors and\nhallucinations that ultimately hinder the retrieval of relevant information.\n  To address these limitations, we propose GraphRunner, a novel graph-based\nretrieval framework that operates in three distinct stages: planning,\nverification, and execution. This introduces high-level traversal actions that\nenable multi-hop exploration in a single step. It also generates a holistic\ntraversal plan, which is verified against the graph structure and pre-defined\ntraversal actions, reducing reasoning errors and detecting hallucinations\nbefore execution. GraphRunner significantly reduces LLM reasoning errors and\ndetects hallucinations through validation. Our evaluation using the GRBench\ndataset shows that GraphRunner consistently outperforms existing approaches,\nachieving 10-50% performance improvements over the strongest baseline while\nreducing inference cost by 3.0-12.9x and response generation time by 2.5-7.1x,\nmaking it significantly more robust and efficient for graph-based retrieval\ntasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08945v1", "cate": "cs.IR", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2506.16386", "title": "CSC-MPPI: A Novel Constrained MPPI Framework with DBSCAN for Reliable Obstacle Avoidance", "authors": ["Leesai Park", "Keunwoo Jang", "Sanghyun Kim"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.16386v2", "summary": "This paper proposes Constrained Sampling Cluster Model Predictive Path\nIntegral (CSC-MPPI), a novel constrained formulation of MPPI designed to\nenhance trajectory optimization while enforcing strict constraints on system\nstates and control inputs. Traditional MPPI, which relies on a probabilistic\nsampling process, often struggles with constraint satisfaction and generates\nsuboptimal trajectories due to the weighted averaging of sampled trajectories.\nTo address these limitations, the proposed framework integrates a primal-dual\ngradient-based approach and Density-Based Spatial Clustering of Applications\nwith Noise (DBSCAN) to steer sampled input trajectories into feasible regions\nwhile mitigating risks associated with weighted averaging. First, to ensure\nthat sampled trajectories remain within the feasible region, the primal-dual\ngradient method is applied to iteratively shift sampled inputs while enforcing\nstate and control constraints. Then, DBSCAN groups the sampled trajectories,\nenabling the selection of representative control inputs within each cluster.\nFinally, among the representative control inputs, the one with the lowest cost\nis chosen as the optimal action. As a result, CSC-MPPI guarantees constraint\nsatisfaction, improves trajectory selection, and enhances robustness in complex\nenvironments. Simulation and real-world experiments demonstrate that CSC-MPPI\noutperforms traditional MPPI in obstacle avoidance, achieving improved\nreliability and efficiency. The experimental videos are available at\nhttps://cscmppi.github.io", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.16386v2", "cate": "cs.RO", "date": "2025-06-19", "updated": "2025-07-13"}
{"id": "2507.08965", "title": "Theory-Informed Improvements to Classifier-Free Guidance for Discrete Diffusion Models", "authors": ["Kevin Rojas", "Ye He", "Chieh-Hsin Lai", "Yuta Takida", "Yuki Mitsufuji", "Molei Tao"], "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08965v1", "summary": "Classifier-Free Guidance (CFG) is a widely used technique for conditional\ngeneration and improving sample quality in continuous diffusion models, and\nrecent works have extended it to discrete diffusion. This paper theoretically\nanalyzes CFG in the context of masked discrete diffusion, focusing on the role\nof guidance schedules. Our analysis shows that high guidance early in sampling\n(when inputs are heavily masked) harms generation quality, while late-stage\nguidance has a larger effect. These findings provide a theoretical explanation\nfor empirical observations in recent studies on guidance schedules. The\nanalysis also reveals an imperfection of the current CFG implementations. These\nimplementations can unintentionally cause imbalanced transitions, such as\nunmasking too rapidly during the early stages of generation, which degrades the\nquality of the resulting samples. To address this, we draw insight from the\nanalysis and propose a novel classifier-free guidance mechanism empirically\napplicable to any discrete diffusion. Intuitively, our method smoothens the\ntransport between the data distribution and the initial (masked/uniform)\ndistribution, which results in improved sample quality. Remarkably, our method\nis achievable via a simple one-line code change. The efficacy of our method is\nempirically demonstrated with experiments on ImageNet (masked discrete\ndiffusion) and QM9 (uniform discrete diffusion).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08965v1", "cate": "cs.LG", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09512", "title": "Online Micro-gesture Recognition Using Data Augmentation and Spatial-Temporal Attention", "authors": ["Pengyu Liu", "Kun Li", "Fei Wang", "Yanyan Wei", "Junhui She", "Dan Guo"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      11 pages, 4 figures", "url": "http://arxiv.org/abs/2507.09512v1", "summary": "In this paper, we introduce the latest solution developed by our team,\nHFUT-VUT, for the Micro-gesture Online Recognition track of the IJCAI 2025 MiGA\nChallenge. The Micro-gesture Online Recognition task is a highly challenging\nproblem that aims to locate the temporal positions and recognize the categories\nof multiple micro-gesture instances in untrimmed videos. Compared to\ntraditional temporal action detection, this task places greater emphasis on\ndistinguishing between micro-gesture categories and precisely identifying the\nstart and end times of each instance. Moreover, micro-gestures are typically\nspontaneous human actions, with greater differences than those found in other\nhuman actions. To address these challenges, we propose hand-crafted data\naugmentation and spatial-temporal attention to enhance the model's ability to\nclassify and localize micro-gestures more accurately. Our solution achieved an\nF1 score of 38.03, outperforming the previous state-of-the-art by 37.9%. As a\nresult, our method ranked first in the Micro-gesture Online Recognition track.", "comment": "11 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.09512v1", "cate": "cs.CV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2506.00683", "title": "Statistical Signal Processing for Quantum Error Mitigation", "authors": ["Kausthubh Chandramouli", "Kelly Mae Allen", "Christopher Mori", "Dror Baron", "Mário A. T. Figueiredo"], "categories": ["quant-ph", "cs.IT", "math.IT"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.00683v2", "summary": "In the noisy intermediate-scale quantum (NISQ) era, quantum error mitigation\n(QEM) is essential for producing reliable outputs from quantum circuits. We\npresent a statistical signal processing approach to QEM that estimates the most\nlikely noiseless outputs from noisy quantum measurements. Our model assumes\nthat circuit depth is sufficient for depolarizing noise, producing corrupted\nobservations that resemble a uniform distribution alongside classical bit-flip\nerrors from readout. Our method consists of two steps: a filtering stage that\ndiscards uninformative depolarizing noise and an expectation-maximization (EM)\nalgorithm that computes a maximum likelihood (ML) estimate over the remaining\ndata. We demonstrate the effectiveness of this approach on small-qubit systems\nusing IBM circuit simulations in Qiskit and compare its performance to\ncontemporary statistical QEM techniques. We also show that our method scales to\nlarger qubit counts using synthetically generated data consistent with our\nnoise model. These results suggest that principled statistical methods can\noffer scalable and interpretable solutions for quantum error mitigation in\nrealistic NISQ settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.00683v2", "cate": "quant-ph", "date": "2025-05-31", "updated": "2025-07-12"}
{"id": "2507.10296", "title": "Average Sensitivity of Hierarchical $k$-Median Clustering", "authors": ["Shijie Li", "Weiqiang He", "Ruobing Bai", "Pan Peng"], "categories": ["cs.LG", "cs.DS"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10296v1", "summary": "Hierarchical clustering is a widely used method for unsupervised learning\nwith numerous applications. However, in the application of modern algorithms,\nthe datasets studied are usually large and dynamic. If the hierarchical\nclustering is sensitive to small perturbations of the dataset, the usability of\nthe algorithm will be greatly reduced. In this paper, we focus on the\nhierarchical $k$ -median clustering problem, which bridges hierarchical and\ncentroid-based clustering while offering theoretical appeal, practical utility,\nand improved interpretability. We analyze the average sensitivity of algorithms\nfor this problem by measuring the expected change in the output when a random\ndata point is deleted. We propose an efficient algorithm for hierarchical\n$k$-median clustering and theoretically prove its low average sensitivity and\nhigh clustering quality. Additionally, we show that single linkage clustering\nand a deterministic variant of the CLNSS algorithm exhibit high average\nsensitivity, making them less stable. Finally, we validate the robustness and\neffectiveness of our algorithm through experiments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10296v1", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09090", "title": "DS@GT at Touché: Large Language Models for Retrieval-Augmented Debate", "authors": ["Anthony Miyaguchi", "Conor Johnston", "Aaryan Potdar"], "categories": ["cs.IR", "cs.CL"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09090v1", "summary": "Large Language Models (LLMs) demonstrate strong conversational abilities. In\nthis Working Paper, we study them in the context of debating in two ways: their\nability to perform in a structured debate along with a dataset of arguments to\nuse and their ability to evaluate utterances throughout the debate. We deploy\nsix leading publicly available models from three providers for the\nRetrieval-Augmented Debate and Evaluation. The evaluation is performed by\nmeasuring four key metrics: Quality, Quantity, Manner, and Relation. Throughout\nthis task, we found that although LLMs perform well in debates when given\nrelated arguments, they tend to be verbose in responses yet consistent in\nevaluation. The accompanying source code for this paper is located at\nhttps://github.com/dsgt-arc/touche-2025-rad.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09090v1", "cate": "cs.IR", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2506.21628", "title": "Ark: An Open-source Python-based Framework for Robot Learning", "authors": ["Magnus Dierking", "Christopher E. Mower", "Sarthak Das", "Huang Helong", "Jiacheng Qiu", "Cody Reading", "Wei Chen", "Huidong Liang", "Huang Guowei", "Jan Peters", "Quan Xingyue", "Jun Wang", "Haitham Bou-Ammar"], "categories": ["cs.RO", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.21628v2", "summary": "Robotics has made remarkable hardware strides-from DARPA's Urban and Robotics\nChallenges to the first humanoid-robot kickboxing tournament-yet commercial\nautonomy still lags behind progress in machine learning. A major bottleneck is\nsoftware: current robot stacks demand steep learning curves, low-level C/C++\nexpertise, fragmented tooling, and intricate hardware integration, in stark\ncontrast to the Python-centric, well-documented ecosystems that propelled\nmodern AI. We introduce ARK, an open-source, Python-first robotics framework\ndesigned to close that gap. ARK presents a Gym-style environment interface that\nallows users to collect data, preprocess it, and train policies using\nstate-of-the-art imitation-learning algorithms (e.g., ACT, Diffusion Policy)\nwhile seamlessly toggling between high-fidelity simulation and physical robots.\nA lightweight client-server architecture provides networked\npublisher-subscriber communication, and optional C/C++ bindings ensure\nreal-time performance when needed. ARK ships with reusable modules for control,\nSLAM, motion planning, system identification, and visualization, along with\nnative ROS interoperability. Comprehensive documentation and case studies-from\nmanipulation to mobile navigation-demonstrate rapid prototyping, effortless\nhardware swapping, and end-to-end pipelines that rival the convenience of\nmainstream machine-learning workflows. By unifying robotics and AI practices\nunder a common Python umbrella, ARK lowers entry barriers and accelerates\nresearch and commercial deployment of autonomous robots.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21628v2", "cate": "cs.RO", "date": "2025-06-24", "updated": "2025-07-14"}
{"id": "2507.08966", "title": "ToxBench: A Binding Affinity Prediction Benchmark with AB-FEP-Calculated Labels for Human Estrogen Receptor Alpha", "authors": ["Meng Liu", "Karl Leswing", "Simon K. S. Chu", "Farhad Ramezanghorbani", "Griffin Young", "Gabriel Marques", "Prerna Das", "Anjali Panikar", "Esther Jamir", "Mohammed Sulaiman Shamsudeen", "K. Shawn Watts", "Ananya Sen", "Hari Priya Devannagari", "Edward B. Miller", "Muyun Lihan", "Howook Hwang", "Janet Paulsen", "Xin Yu", "Kyle Gion", "Timur Rvachov", "Emine Kucukbenli", "Saee Gopal Paliwal"], "categories": ["cs.LG", "cs.AI", "physics.chem-ph", "q-bio.BM"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Workshop on Generative AI for Biology at ICML 2025", "url": "http://arxiv.org/abs/2507.08966v1", "summary": "Protein-ligand binding affinity prediction is essential for drug discovery\nand toxicity assessment. While machine learning (ML) promises fast and accurate\npredictions, its progress is constrained by the availability of reliable data.\nIn contrast, physics-based methods such as absolute binding free energy\nperturbation (AB-FEP) deliver high accuracy but are computationally prohibitive\nfor high-throughput applications. To bridge this gap, we introduce ToxBench,\nthe first large-scale AB-FEP dataset designed for ML development and focused on\na single pharmaceutically critical target, Human Estrogen Receptor Alpha\n(ER$\\alpha$). ToxBench contains 8,770 ER$\\alpha$-ligand complex structures with\nbinding free energies computed via AB-FEP with a subset validated against\nexperimental affinities at 1.75 kcal/mol RMSE, along with non-overlapping\nligand splits to assess model generalizability. Using ToxBench, we further\nbenchmark state-of-the-art ML methods, and notably, our proposed DualBind\nmodel, which employs a dual-loss framework to effectively learn the binding\nenergy function. The benchmark results demonstrate the superior performance of\nDualBind and the potential of ML to approximate AB-FEP at a fraction of the\ncomputational cost.", "comment": "Workshop on Generative AI for Biology at ICML 2025", "pdf_url": "http://arxiv.org/pdf/2507.08966v1", "cate": "cs.LG", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09514", "title": "QuarterMap: Efficient Post-Training Token Pruning for Visual State Space Models", "authors": ["Tien-Yu Chi", "Hung-Yueh Chiang", "Diana Marculescu", "Kai-Chiang Wu"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by Efficient Systems for Foundation Models Workshop at the International Conference on Machine Learning (ICML) 2025", "url": "http://arxiv.org/abs/2507.09514v1", "summary": "State space models (SSMs) reduce the quadratic complexity of transformers by\nleveraging linear recurrence. Recently, VMamba has emerged as a strong\nSSM-based vision backbone, yet remains bottlenecked by spatial redundancy in\nits four-directional scan. We propose QuarterMap, a post-training activation\npruning method that removes redundant spatial activations before scanning and\nrestores dimensions via nearest-neighbor upsampling. Our method improves\nthroughput without retraining. On ImageNet-1K, QuarterMap achieves up to 11%\nspeedup on VMamba with less than 0.9% accuracy drop, and yields similar gains\non ADE20K segmentation. Beyond VMamba, we validate QuarterMap on MedMamba, a\ndomain-specific model that shares the same four-directional scanning structure,\nwhere it consistently improves throughput while preserving accuracy across\nmultiple medical imaging tasks. Compared to token merging methods like ToMe,\nQuarterMap is tailored for SSMs and avoids costly merge-unmerge operations. Our\nmethod offers a plug-and-play tool for deployment-time efficiency without\ncompromising transferability.", "comment": "Accepted by Efficient Systems for Foundation Models Workshop at the\n  International Conference on Machine Learning (ICML) 2025", "pdf_url": "http://arxiv.org/pdf/2507.09514v1", "cate": "cs.CV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2506.16793", "title": "A Generic Construction of $q$-ary Near-MDS Codes Supporting 2-Designs with Lengths Beyond $q+1$", "authors": ["Hengfeng Liu", "Chunming Tang", "Zhengchun Zhou", "Dongchun Han", "Hao Chen"], "categories": ["math.CO", "cs.IT", "math.IT"], "primary_category": "Subjects:       Combinatorics (math.CO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.16793v2", "summary": "A linear code with parameters $[n, k, n - k + 1]$ is called maximum distance\nseparable (MDS), and one with parameters $[n, k, n - k]$ is called almost MDS\n(AMDS). A code is near-MDS (NMDS) if both it and its dual are AMDS. NMDS codes\nsupporting combinatorial $t$-designs have attracted growing interest, yet\nconstructing such codes remains highly challenging. In 2020, Ding and Tang\ninitiated the study of NMDS codes supporting 2-designs by constructing the\nfirst infinite family, followed by several other constructions for $t > 2$, all\nwith length at most $q + 1$. Although NMDS codes can, in principle, exceed this\nlength, known examples supporting 2-designs and having length greater than $q +\n1$ are extremely rare and limited to a few sporadic binary and ternary cases.\nIn this paper, we present the first \\emph{generic construction} of $q$-ary NMDS\ncodes supporting 2-designs with lengths \\emph{exceeding $q + 1$}. Our method\nleverages new connections between elliptic curve codes, finite abelian groups,\nsubset sums, and combinatorial designs, resulting in an infinite family of such\ncodes along with their weight distributions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.16793v2", "cate": "math.CO", "date": "2025-06-20", "updated": "2025-07-12"}
{"id": "2507.10329", "title": "Computing the probability of intersection", "authors": ["Alexander Barvinok"], "categories": ["math.PR", "cs.DS", "math.CO", "60C05, 68Q87, 68W05, 30C15"], "primary_category": "Subjects:       Probability (math.PR)", "pdf_link": null, "comments": "Comments:      21 page", "url": "http://arxiv.org/abs/2507.10329v1", "summary": "Let $\\Omega_1, \\ldots, \\Omega_m$ be probability spaces, let $\\Omega=\\Omega_1\n\\times \\cdots \\times \\Omega_m$ be their product and let $A_1, \\ldots, A_n\n\\subset \\Omega$ be events. Suppose that each event $A_i$ depends on $r_i$\ncoordinates of a point $x \\in \\Omega$, $x=\\left(\\xi_1, \\ldots, \\xi_m\\right)$,\nand that for each event $A_i$ there are $\\Delta_i$ of other events $A_j$ that\ndepend on some of the coordinates that $A_i$ depends on. Let $\\Delta=\\max\\{5,\\\n\\Delta_i: i=1, \\ldots, n\\}$ and let $\\mu_i=\\min\\{r_i,\\ \\Delta_i+1\\}$ for $i=1,\n\\ldots, n$. We prove that if $P(A_i) < (3\\Delta)^{-3\\mu_i}$ for all $I$, then\nfor any $0 < \\epsilon < 1$, the probability $P\\left( \\bigcap_{i=1}^n\n\\overline{A}_i\\right)$ of the intersection of the complements of all $A_i$ can\nbe computed within relative error $\\epsilon$ in polynomial time from the\nprobabilities $P\\left(A_{i_1} \\cap \\ldots \\cap A_{i_k}\\right)$ of $k$-wise\nintersections of the events $A_i$ for $k = e^{O(\\Delta)} \\ln (n/\\epsilon)$.", "comment": "21 page", "pdf_url": "http://arxiv.org/pdf/2507.10329v1", "cate": "math.PR", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09188", "title": "Retrieval-Augmented Recommendation Explanation Generation with Hierarchical Aggregation", "authors": ["Bangcheng Sun", "Yazhe Chen", "Jilin Yang", "Xiaodong Li", "Hui Li"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09188v1", "summary": "Explainable Recommender System (ExRec) provides transparency to the\nrecommendation process, increasing users' trust and boosting the operation of\nonline services. With the rise of large language models (LLMs), whose extensive\nworld knowledge and nuanced language understanding enable the generation of\nhuman-like, contextually grounded explanations, LLM-powered ExRec has gained\ngreat momentum. However, existing LLM-based ExRec models suffer from profile\ndeviation and high retrieval overhead, hindering their deployment. To address\nthese issues, we propose Retrieval-Augmented Recommendation Explanation\nGeneration with Hierarchical Aggregation (REXHA). Specifically, we design a\nhierarchical aggregation based profiling module that comprehensively considers\nuser and item review information, hierarchically summarizing and constructing\nholistic profiles. Furthermore, we introduce an efficient retrieval module\nusing two types of pseudo-document queries to retrieve relevant reviews to\nenhance the generation of recommendation explanations, effectively reducing\nretrieval latency and improving the recall of relevant reviews. Extensive\nexperiments demonstrate that our method outperforms existing approaches by up\nto 12.6% w.r.t. the explanation quality while achieving high retrieval\nefficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09188v1", "cate": "cs.IR", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.01930", "title": "Large Language Model-Driven Closed-Loop UAV Operation with Semantic Observations", "authors": ["Wenhao Wang", "Yanyan Li", "Long Jiao", "Jiawei Yuan"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      9 pages, 7 figures", "url": "http://arxiv.org/abs/2507.01930v3", "summary": "Recent advances in large Language Models (LLMs) have revolutionized mobile\nrobots, including unmanned aerial vehicles (UAVs), enabling their intelligent\noperation within Internet of Things (IoT) ecosystems. However, LLMs still face\nchallenges from logical reasoning and complex decision-making, leading to\nconcerns about the reliability of LLM-driven UAV operations in IoT\napplications. In this paper, we propose a LLM-driven closed-loop control\nframework that enables reliable UAV operations powered by effective feedback\nand refinement using two LLM modules, i.e., a Code Generator and an Evaluator.\nOur framework transforms numerical state observations from UAV operations into\nnatural language trajectory descriptions to enhance the evaluator LLM's\nunderstanding of UAV dynamics for precise feedback generation. Our framework\nalso enables a simulation-based refinement process, and hence eliminates the\nrisks to physical UAVs caused by incorrect code execution during the\nrefinement. Extensive experiments on UAV control tasks with different\ncomplexities are conducted. The experimental results show that our framework\ncan achieve reliable UAV operations using LLMs, which significantly outperforms\nbaseline approaches in terms of success rate and completeness with the increase\nof task complexity.", "comment": "9 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.01930v3", "cate": "cs.RO", "date": "2025-07-02", "updated": "2025-07-13"}
{"id": "2507.08972", "title": "Simulating Three-dimensional Turbulence with Physics-informed Neural Networks", "authors": ["Sifan Wang", "Shyam Sankaran", "Panos Stinis", "Paris Perdikaris"], "categories": ["cs.LG", "cs.AI", "physics.comp-ph", "physics.flu-dyn"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      25 pages, 13 figures, 3 tables", "url": "http://arxiv.org/abs/2507.08972v1", "summary": "Turbulent fluid flows are among the most computationally demanding problems\nin science, requiring enormous computational resources that become prohibitive\nat high flow speeds. Physics-informed neural networks (PINNs) represent a\nradically different approach that trains neural networks directly from physical\nequations rather than data, offering the potential for continuous, mesh-free\nsolutions. Here we show that appropriately designed PINNs can successfully\nsimulate fully turbulent flows in both two and three dimensions, directly\nlearning solutions to the fundamental fluid equations without traditional\ncomputational grids or training data. Our approach combines several algorithmic\ninnovations including adaptive network architectures, causal training, and\nadvanced optimization methods to overcome the inherent challenges of learning\nchaotic dynamics. Through rigorous validation on challenging turbulence\nproblems, we demonstrate that PINNs accurately reproduce key flow statistics\nincluding energy spectra, kinetic energy, enstrophy, and Reynolds stresses. Our\nresults demonstrate that neural equation solvers can handle complex chaotic\nsystems, opening new possibilities for continuous turbulence modeling that\ntranscends traditional computational limitations.", "comment": "25 pages, 13 figures, 3 tables", "pdf_url": "http://arxiv.org/pdf/2507.08972v1", "cate": "cs.LG", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09524", "title": "When Schrödinger Bridge Meets Real-World Image Dehazing with Unpaired Training", "authors": ["Yunwei Lan", "Zhigao Cui", "Xin Luo", "Chang Liu", "Nian Wang", "Menglin Zhang", "Yanzhao Su", "Dong Liu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV2025", "url": "http://arxiv.org/abs/2507.09524v1", "summary": "Recent advancements in unpaired dehazing, particularly those using GANs, show\npromising performance in processing real-world hazy images. However, these\nmethods tend to face limitations due to the generator's limited transport\nmapping capability, which hinders the full exploitation of their effectiveness\nin unpaired training paradigms. To address these challenges, we propose\nDehazeSB, a novel unpaired dehazing framework based on the Schr\\\"odinger\nBridge. By leveraging optimal transport (OT) theory, DehazeSB directly bridges\nthe distributions between hazy and clear images. This enables optimal transport\nmappings from hazy to clear images in fewer steps, thereby generating\nhigh-quality results. To ensure the consistency of structural information and\ndetails in the restored images, we introduce detail-preserving regularization,\nwhich enforces pixel-level alignment between hazy inputs and dehazed outputs.\nFurthermore, we propose a novel prompt learning to leverage pre-trained CLIP\nmodels in distinguishing hazy images and clear ones, by learning a haze-aware\nvision-language alignment. Extensive experiments on multiple real-world\ndatasets demonstrate our method's superiority. Code:\nhttps://github.com/ywxjm/DehazeSB.", "comment": "Accepted by ICCV2025", "pdf_url": "http://arxiv.org/pdf/2507.09524v1", "cate": "cs.CV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2506.17229", "title": "Coupled Entropy: A Goldilocks Generalization for Nonextensive Statistical Mechanics", "authors": ["Kenric P. Nelson"], "categories": ["stat.ML", "cond-mat.stat-mech", "cs.IT", "cs.LG", "math.IT"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "Comments:      14 pages; 1 figure; draft paper for Conference on Nonextensive Statistical Physics Dedicated to Constantino Tsallis' 82nd Birthday", "url": "http://arxiv.org/abs/2506.17229v2", "summary": "Evidence is presented that the accuracy of Nonextensive Statistical Mechanics\nframework is improved using the coupled entropy, which carefully establishes\nthe physical measures of complex systems. While Nonextensive Statistical\nMechanics (NSM) has developed into a powerful toolset, questions have persisted\nas to how to evaluate whether its proposed solutions properly characterize the\nuncertainty of heavy-tailed distributions. The entropy of the generalized\nPareto distribution (GPD) is $1+\\kappa+\\ln\\sigma$, where $\\kappa$ is the shape\nor nonlinear coupling and $\\sigma$ is the scale. A generalized entropy should\nretain the uncertainty due to the scale, while minimizing the dependence of the\nnonlinear coupling. The Tsallis entropy of the GPD instead subtracts a function\nof the inverse-scale and converges to one as $\\kappa\\rightarrow\\infty$.\nColloquially, the Tsallis entropy is too cold. The normalized Tsallis entropy\n(NTE) rectifies the positive dependence on the scale but introduces a nonlinear\nterm multiplying the scale and the coupling, making it too hot. The coupled\nentropy measures the uncertainty of the GPD to be\n$1+\\ln_\\frac{\\kappa}{1+\\kappa}\\sigma=1+\\frac{1+\\kappa}{\\kappa}(\\sigma^\\frac{\\kappa}{1+\\kappa}-1)$,\nwhich converges to $\\sigma$ as $\\kappa\\rightarrow\\infty$. One could say, the\ncoupled entropy allows scientists, engineers, and analysts to eat their\nporridge, confident that its measure of uncertainty reflects the mathematical\nphysics of the scale of non-exponential distributions while minimizing the\ndependence on the shape or nonlinear coupling. The training of the coupled\nvariational autoencoder is an example of the unique ability of the coupled\nentropy to improve the performance of complex systems.", "comment": "14 pages; 1 figure; draft paper for Conference on Nonextensive\n  Statistical Physics Dedicated to Constantino Tsallis' 82nd Birthday", "pdf_url": "http://arxiv.org/pdf/2506.17229v2", "cate": "stat.ML", "date": "2025-05-17", "updated": "2025-07-13"}
{"id": "2507.10467", "title": "Colorful Minors", "authors": ["Evangelos Protopapas", "Dimitrios M. Thilikos", "Sebastian Wiederrecht"], "categories": ["math.CO", "cs.DM", "cs.DS", "05C83, 05C85, 05C10, 05C75, 68R10", "G.2.2"], "primary_category": "Subjects:       Combinatorics (math.CO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10467v1", "summary": "We introduce the notion of colorful minors, which generalizes the classical\nconcept of rooted minors in graphs. $q$-colorful graph is defined as a pair\n$(G, \\chi),$ where $G$ is a graph and $\\chi$ assigns to each vertex a (possibly\nempty) subset of at most $q$ colors. The colorful minor relation enhances the\nclassical minor relation by merging color sets at contracted edges and allowing\nthe removal of colors from vertices. This framework naturally models\nalgorithmic problems involving graphs with (possibly overlapping) annotated\nvertex sets. We develop a structural theory for colorful minors by establishing\nseveral theorems characterizing $\\mathcal{H}$-colorful minor-free graphs, where\n$\\mathcal{H}$ consists either of a clique or a grid with all vertices assigned\nall colors, or of grids with colors segregated and ordered on the outer face.\nLeveraging our structural insights, we provide a complete classification -\nparameterized by the number $q$ of colors - of all colorful graphs that exhibit\nthe Erd\\H{o}s-P\\'osa property with respect to colorful minors. On the\nalgorithmic side, we provide a fixed-parameter tractable algorithm for colorful\nminor testing and a variant of the $k$-disjoint paths problem. Together with\nthe fact that the colorful minor relation forms a well-quasi-order, this\nimplies that every colorful minor-monotone parameter on colorful graphs admits\na fixed-parameter algorithm. Furthermore, we derive two algorithmic\nmeta-theorems (AMTs) whose structural conditions are linked to extensions of\ntreewidth and Hadwiger number on colorful graphs. Our results suggest how known\nAMTs can be extended to incorporate not only the structure of the input graph\nbut also the way the colored vertices are distributed in it.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10467v1", "cate": "math.CO", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09331", "title": "Correcting the LogQ Correction: Revisiting Sampled Softmax for Large-Scale Retrieval", "authors": ["Kirill Khrylchenko", "Vladimir Baikalov", "Sergei Makeev", "Artem Matveev", "Sergei Liamaev"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Accepted at ACM RecSys 2025. Author's version. To appear in the Proceedings of the 18th ACM Conference on Recommender Systems", "url": "http://arxiv.org/abs/2507.09331v1", "summary": "Two-tower neural networks are a popular architecture for the retrieval stage\nin recommender systems. These models are typically trained with a softmax loss\nover the item catalog. However, in web-scale settings, the item catalog is\noften prohibitively large, making full softmax infeasible. A common solution is\nsampled softmax, which approximates the full softmax using a small number of\nsampled negatives.\n  One practical and widely adopted approach is to use in-batch negatives, where\nnegatives are drawn from items in the current mini-batch. However, this\nintroduces a bias: items that appear more frequently in the batch (i.e.,\npopular items) are penalized more heavily.\n  To mitigate this issue, a popular industry technique known as logQ correction\nadjusts the logits during training by subtracting the log-probability of an\nitem appearing in the batch. This correction is derived by analyzing the bias\nin the gradient and applying importance sampling, effectively twice, using the\nin-batch distribution as a proposal distribution. While this approach improves\nmodel quality, it does not fully eliminate the bias.\n  In this work, we revisit the derivation of logQ correction and show that it\noverlooks a subtle but important detail: the positive item in the denominator\nis not Monte Carlo-sampled - it is always present with probability 1. We\npropose a refined correction formula that accounts for this. Notably, our loss\nintroduces an interpretable sample weight that reflects the model's uncertainty\n- the probability of misclassification under the current parameters. We\nevaluate our method on both public and proprietary datasets, demonstrating\nconsistent improvements over the standard logQ correction.", "comment": "Accepted at ACM RecSys 2025. Author's version. To appear in the\n  Proceedings of the 18th ACM Conference on Recommender Systems", "pdf_url": "http://arxiv.org/pdf/2507.09331v1", "cate": "cs.IR", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.06174", "title": "Fast Bilateral Teleoperation and Imitation Learning Using Sensorless Force Control via Accurate Dynamics Model", "authors": ["Koki Yamane", "Yunhan Li", "Masashi Konosu", "Koki Inami", "Junji Oaki", "Sho Sakaino", "Toshiaki Tsuji"], "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      20 pages, 9 figures, Submitted to CoRL 2025", "url": "http://arxiv.org/abs/2507.06174v2", "summary": "In recent years, the advancement of imitation learning has led to increased\ninterest in teleoperating low-cost manipulators to collect demonstration data.\nHowever, most existing systems rely on unilateral control, which only transmits\ntarget position values. While this approach is easy to implement and suitable\nfor slow, non-contact tasks, it struggles with fast or contact-rich operations\ndue to the absence of force feedback. This work demonstrates that fast\nteleoperation with force feedback is feasible even with force-sensorless,\nlow-cost manipulators by leveraging 4-channel bilateral control. Based on\naccurately identified manipulator dynamics, our method integrates nonlinear\nterms compensation, velocity and external force estimation, and variable gain\ncorresponding to inertial variation. Furthermore, using data collected by\n4-channel bilateral control, we show that incorporating force information into\nboth the input and output of learned policies improves performance in imitation\nlearning. These results highlight the practical effectiveness of our system for\nhigh-fidelity teleoperation and data collection on affordable hardware.", "comment": "20 pages, 9 figures, Submitted to CoRL 2025", "pdf_url": "http://arxiv.org/pdf/2507.06174v2", "cate": "cs.RO", "date": "2025-07-08", "updated": "2025-07-14"}
{"id": "2507.08977", "title": "Simulation as Supervision: Mechanistic Pretraining for Scientific Discovery", "authors": ["Carson Dudley", "Reiden Magdaleno", "Christopher Harding", "Marisa Eisenberg"], "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08977v1", "summary": "Scientific modeling faces a core limitation: mechanistic models offer\ninterpretability but collapse under real-world complexity, while machine\nlearning models are flexible but require large labeled datasets, cannot infer\nunobservable quantities, and operate as black boxes. We introduce\nSimulation-Grounded Neural Networks (SGNNs), a general framework that uses\nmechanistic simulations as training data for neural networks. SGNNs are\npretrained on synthetic corpora spanning diverse model structures, parameter\nregimes, stochasticity, and observational artifacts. We evaluated SGNNs across\nscientific disciplines and modeling tasks, and found that SGNNs achieved\nstate-of-the-art results across settings: for prediction tasks, they nearly\ntripled COVID-19 forecasting skill versus CDC baselines, reduced chemical yield\nprediction error by one third, and maintained accuracy in ecological\nforecasting where task specific models failed. For inference tasks, SGNNs also\naccurately classified the source of information spread in simulated social\nnetworks and enabled supervised learning for unobservable targets, such as\nestimating COVID-19 transmissibility more accurately than traditional methods\neven in early outbreaks. Finally, SGNNs enable back-to-simulation attribution,\na new form of mechanistic interpretability. Given real world input, SGNNs\nretrieve simulations based on what the model has learned to see as most\nsimilar, revealing which underlying dynamics the model believes are active.\nThis provides process-level insight -- what the model thinks is happening --\nnot just which features mattered. SGNNs unify scientific theory with deep\nlearning flexibility and unlock a new modeling paradigm -- transforming\nsimulations from rigid, post hoc tools into flexible sources of supervision,\nenabling robust, interpretable inference even when ground truth is missing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08977v1", "cate": "cs.LG", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09531", "title": "VDInstruct: Zero-Shot Key Information Extraction via Content-Aware Vision Tokenization", "authors": ["Son Nguyen", "Giang Nguyen", "Hung Dao", "Thao Do", "Daeyoung Kim"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Under Review", "url": "http://arxiv.org/abs/2507.09531v1", "summary": "Key Information Extraction (KIE) underpins the understanding of visual\ndocuments (e.g., receipts and contracts) by extracting precise semantic content\nand accurately capturing spatial structure. Yet existing multimodal large\nlanguage models (MLLMs) often perform poorly on dense documents and rely on\nvision tokenization approaches that scale with image size, leading to redundant\ncomputation and memory inefficiency. To address these challenges, we introduce\nVDInstruct, an MLLM that separates spatial region detection from semantic\nfeature extraction. Central to our model is a content-aware tokenization\nstrategy: rather than fragmenting the entire image uniformly, it generates\ntokens in proportion to document complexity, preserving critical structure\nwhile eliminating wasted tokens. Leveraging a three-stage training paradigm,\nour model achieves state-of-the-art (SOTA) results on KIE benchmarks, matching\nor exceeding the accuracy of leading approaches while reducing the number of\nimage tokens by roughly 3.6x. In zero-shot evaluations, VDInstruct surpasses\nstrong baselines-such as DocOwl 1.5-by +5.5 F1 points, highlighting its\nrobustness to unseen documents. These findings show that content-aware\ntokenization combined with explicit layout modeling offers a promising\ndirection forward for document understanding. Data, source code, and model\nweights will be made publicly available.", "comment": "Under Review", "pdf_url": "http://arxiv.org/pdf/2507.09531v1", "cate": "cs.CV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.05736", "title": "Tight Bound for Quantum Unitary Time-Reversal", "authors": ["Kean Chen", "Nengkun Yu", "Zhicheng Zhang"], "categories": ["quant-ph", "cs.IT", "math.IT"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      32 pages, 6 figures, 1 table; minor revision", "url": "http://arxiv.org/abs/2507.05736v2", "summary": "Time-reversal of unitary evolution is fundamental in quantum information\nprocessing. Many scenarios, particularly those in quantum learning and\nmetrology, assume free access to the time-reverse of an unknown unitary. In\nthis paper, we settle the query complexity of the unitary time-reversal task:\napproximately implementing $U^{-1}$ given only black-box access to an unknown\n$d$-dimensional unitary $U$. We provide a tight query lower bound\n$\\Omega((1-\\epsilon)d^2)$ for the unitary time-reversal to within diamond norm\nerror $\\epsilon$. Notably, our lower bound applies to general coherent\nprotocols with unbounded ancillas, and holds even when $\\epsilon$ is an\naverage-case distance error. Moreover, our result implies a query lower bound\n$\\Omega(d^2)$ for approximately implementing control-$U$ up to an irrelevant\nphase, which is also tight with respect to the dimension.", "comment": "32 pages, 6 figures, 1 table; minor revision", "pdf_url": "http://arxiv.org/pdf/2507.05736v2", "cate": "quant-ph", "date": "2025-07-08", "updated": "2025-07-13"}
{"id": "2404.06797", "title": "Correlation Clustering Beyond the Pivot Algorithm", "authors": ["Soheil Behnezhad", "Moses Charikar", "Vincent Cohen-Addad", "Alma Ghafari", "Weiyun Ma"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2404.06797v3", "summary": "We study the classic correlation clustering in the dynamic setting. Given $n$\nobjects and a complete labeling of the object-pairs as either similar or\ndissimilar, the goal is to partition the objects into arbitrarily many clusters\nwhile minimizing disagreements with the labels. In the dynamic setting, an\nupdate consists of a flip of a label of an edge. In a breakthrough result,\n[BDHSS, FOCS'19] showed how to maintain a 3-approximation with polylogarithmic\nupdate time by providing a dynamic implementation of the Pivot algorithm of\n[ACN, STOC'05]. Since then, it has been a major open problem to determine\nwhether the 3-approximation barrier can be broken in the fully dynamic setting.\nIn this paper, we resolve this problem. Our algorithm, Modified Pivot, locally\nimproves the output of Pivot by moving some vertices to other existing clusters\nor new singleton clusters. We present an analysis showing that this\nmodification does indeed improve the approximation to below 3. We also show\nthat its output can be maintained in polylogarithmic time per update.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2404.06797v3", "cate": "cs.DS", "date": "2024-04-10", "updated": "2025-07-13"}
{"id": "2507.09403", "title": "Balancing Semantic Relevance and Engagement in Related Video Recommendations", "authors": ["Amit Jaspal", "Feng Zhang", "Wei Chang", "Sumit Kumar", "Yubo Wang", "Roni Mittleman", "Qifan Wang", "Weize Mao"], "categories": ["cs.IR", "cs.MM"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09403v1", "summary": "Related video recommendations commonly use collaborative filtering (CF)\ndriven by co-engagement signals, often resulting in recommendations lacking\nsemantic coherence and exhibiting strong popularity bias. This paper introduces\na novel multi-objective retrieval framework, enhancing standard two-tower\nmodels to explicitly balance semantic relevance and user engagement. Our\napproach uniquely combines: (a) multi-task learning (MTL) to jointly optimize\nco-engagement and semantic relevance, explicitly prioritizing topical\ncoherence; (b) fusion of multimodal content features (textual and visual\nembeddings) for richer semantic understanding; and (c) off-policy correction\n(OPC) via inverse propensity weighting to effectively mitigate popularity bias.\nEvaluation on industrial-scale data and a two-week live A/B test reveals our\nframework's efficacy. We observed significant improvements in semantic\nrelevance (from 51% to 63% topic match rate), a reduction in popular item\ndistribution (-13.8% popular video recommendations), and a +0.04% improvement\nin our topline user engagement metric. Our method successfully achieves better\nsemantic coherence, balanced engagement, and practical scalability for\nreal-world deployment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09403v1", "cate": "cs.IR", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.06710", "title": "Spatial-Temporal Aware Visuomotor Diffusion Policy Learning", "authors": ["Zhenyang Liu", "Yikai Wang", "Kuanning Wang", "Longfei Liang", "Xiangyang Xue", "Yanwei Fu"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.06710v2", "summary": "Visual imitation learning is effective for robots to learn versatile tasks.\nHowever, many existing methods rely on behavior cloning with supervised\nhistorical trajectories, limiting their 3D spatial and 4D spatiotemporal\nawareness. Consequently, these methods struggle to capture the 3D structures\nand 4D spatiotemporal relationships necessary for real-world deployment. In\nthis work, we propose 4D Diffusion Policy (DP4), a novel visual imitation\nlearning method that incorporates spatiotemporal awareness into diffusion-based\npolicies. Unlike traditional approaches that rely on trajectory cloning, DP4\nleverages a dynamic Gaussian world model to guide the learning of 3D spatial\nand 4D spatiotemporal perceptions from interactive environments. Our method\nconstructs the current 3D scene from a single-view RGB-D observation and\npredicts the future 3D scene, optimizing trajectory generation by explicitly\nmodeling both spatial and temporal dependencies. Extensive experiments across\n17 simulation tasks with 173 variants and 3 real-world robotic tasks\ndemonstrate that the 4D Diffusion Policy (DP4) outperforms baseline methods,\nimproving the average simulation task success rate by 16.4% (Adroit), 14%\n(DexArt), and 6.45% (RLBench), and the average real-world robotic task success\nrate by 8.6%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.06710v2", "cate": "cs.RO", "date": "2025-07-09", "updated": "2025-07-13"}
{"id": "2507.08980", "title": "Learning Diffusion Models with Flexible Representation Guidance", "authors": ["Chenyu Wang", "Cai Zhou", "Sharut Gupta", "Zongyu Lin", "Stefanie Jegelka", "Stephen Bates", "Tommi Jaakkola"], "categories": ["cs.LG", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08980v1", "summary": "Diffusion models can be improved with additional guidance towards more\neffective representations of input. Indeed, prior empirical work has already\nshown that aligning internal representations of the diffusion model with those\nof pre-trained models improves generation quality. In this paper, we present a\nsystematic framework for incorporating representation guidance into diffusion\nmodels. We provide alternative decompositions of denoising models along with\ntheir associated training criteria, where the decompositions determine when and\nhow the auxiliary representations are incorporated. Guided by our theoretical\ninsights, we introduce two new strategies for enhancing representation\nalignment in diffusion models. First, we pair examples with target\nrepresentations either derived from themselves or arisen from different\nsynthetic modalities, and subsequently learn a joint model over the multimodal\npairs. Second, we design an optimal training curriculum that balances\nrepresentation learning and data generation. Our experiments across image,\nprotein sequence, and molecule generation tasks demonstrate superior\nperformance as well as accelerated training. In particular, on the\nclass-conditional ImageNet $256\\times 256$ benchmark, our guidance results in\n$23.3$ times faster training than the original SiT-XL as well as four times\nspeedup over the state-of-the-art method REPA. The code is available at\nhttps://github.com/ChenyuWang-Monica/REED.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08980v1", "cate": "cs.LG", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09541", "title": "DRPCA-Net: Make Robust PCA Great Again for Infrared Small Target Detection", "authors": ["Zihao Xiong", "Fei Zhou", "Fengyi Wu", "Shuai Yuan", "Maixia Fu", "Zhenming Peng", "Jian Yang", "Yimian Dai"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by TGRS", "url": "http://arxiv.org/abs/2507.09541v1", "summary": "Infrared small target detection plays a vital role in remote sensing,\nindustrial monitoring, and various civilian applications. Despite recent\nprogress powered by deep learning, many end-to-end convolutional models tend to\npursue performance by stacking increasingly complex architectures, often at the\nexpense of interpretability, parameter efficiency, and generalization. These\nmodels typically overlook the intrinsic sparsity prior of infrared small\ntargets--an essential cue that can be explicitly modeled for both performance\nand efficiency gains. To address this, we revisit the model-based paradigm of\nRobust Principal Component Analysis (RPCA) and propose Dynamic RPCA Network\n(DRPCA-Net), a novel deep unfolding network that integrates the sparsity-aware\nprior into a learnable architecture. Unlike conventional deep unfolding methods\nthat rely on static, globally learned parameters, DRPCA-Net introduces a\ndynamic unfolding mechanism via a lightweight hypernetwork. This design enables\nthe model to adaptively generate iteration-wise parameters conditioned on the\ninput scene, thereby enhancing its robustness and generalization across diverse\nbackgrounds. Furthermore, we design a Dynamic Residual Group (DRG) module to\nbetter capture contextual variations within the background, leading to more\naccurate low-rank estimation and improved separation of small targets.\nExtensive experiments on multiple public infrared datasets demonstrate that\nDRPCA-Net significantly outperforms existing state-of-the-art methods in\ndetection accuracy. Code is available at https://github.com/GrokCV/DRPCA-Net.", "comment": "Accepted by TGRS", "pdf_url": "http://arxiv.org/pdf/2507.09541v1", "cate": "cs.CV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.08333", "title": "Token-based Audio Inpainting via Discrete Diffusion", "authors": ["Tali Dror", "Iftach Shoham", "Moshe Buchris", "Oren Gal", "Haim Permuter", "Gilad Katz", "Eliya Nachmani"], "categories": ["cs.SD", "cs.AI", "cs.IT", "cs.LG", "eess.AS", "math.IT"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08333v2", "summary": "Audio inpainting refers to the task of reconstructing missing segments in\ncorrupted audio recordings. While prior approaches-including waveform and\nspectrogram-based diffusion models-have shown promising results for short gaps,\nthey often degrade in quality when gaps exceed 100 milliseconds (ms). In this\nwork, we introduce a novel inpainting method based on discrete diffusion\nmodeling, which operates over tokenized audio representations produced by a\npre-trained audio tokenizer. Our approach models the generative process\ndirectly in the discrete latent space, enabling stable and semantically\ncoherent reconstruction of missing audio. We evaluate the method on the\nMusicNet dataset using both objective and perceptual metrics across gap\ndurations up to 300 ms. We further evaluated our approach on the MTG dataset,\nextending the gap duration to 500 ms. Experimental results demonstrate that our\nmethod achieves competitive or superior performance compared to existing\nbaselines, particularly for longer gaps, offering a robust solution for\nrestoring degraded musical recordings. Audio examples of our proposed method\ncan be found at https://iftach21.github.io/", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08333v2", "cate": "cs.SD", "date": "2025-07-11", "updated": "2025-07-14"}
{"id": "2507.06509", "title": "Prediction-Augmented Mechanism Design for Weighted Facility Location", "authors": ["Yangguang Shi", "Zhenyu Xue"], "categories": ["cs.DS", "cs.GT", "cs.LG", "68W27, 68Q32", "F.2.2"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      An extended abstract of this paper is to appear in the 19th Annual Conference on Theory and Applications of Models of Computation (TAMC 2025)", "url": "http://arxiv.org/abs/2507.06509v3", "summary": "Facility location is fundamental in operations research, mechanism design,\nand algorithmic game theory, with applications ranging from urban\ninfrastructure planning to distributed systems. Recent research in this area\nhas focused on augmenting classic strategyproof mechanisms with predictions to\nachieve an improved performance guarantee against the uncertainty under the\nstrategic environment. Previous work has been devoted to address the trade-off\nobstacle of balancing the consistency (near-optimality under accurate\npredictions) and robustness (bounded inefficiency under poor predictions)\nprimarily in the unweighted setting, assuming that all agents have the same\nimportance. However, this assumption may not be true in some practical\nscenarios, leading to research of weighted facility location problems.\n  The major contribution of the current work is to provide a prediction\naugmented algorithmic framework for balancing the consistency and robustness\nover strategic agents with non-uniform weights. In particular, through a\nreduction technique that identifies a subset of representative instances and\nmaps the other given locations to the representative ones, we prove that there\nexists a strategyproof mechanism achieving a bounded consistency guarantee of\n$\\frac{\\sqrt{(1+c)^2W^2_{\\min}+(1-c)^2W^2_{\\max}}}{(1+c)W_{\\min}}$ and a\nbounded robustness guarantee of\n$\\frac{\\sqrt{(1-c)^2W^2_{\\min}+(1+c)^2W^2_{\\max}}}{(1-c)W_{\\min}}$ in weighted\nsettings, where $c$ can be viewed as a parameter to make a trade-off between\nthe consistency and robustness and $W_{\\min}$ and $W_{\\max}$ denote the minimum\nand maximum agents' weight. We also prove that there is no strategyproof\ndeterministic mechanism that reach $1$-consistency and $O\\left( n \\cdot\n\\frac{W_{\\max}}{W_{\\min}} \\right)$-robustness in weighted FLP, even with fully\npredictions of all agents.", "comment": "An extended abstract of this paper is to appear in the 19th Annual\n  Conference on Theory and Applications of Models of Computation (TAMC 2025)", "pdf_url": "http://arxiv.org/pdf/2507.06509v3", "cate": "cs.DS", "date": "2025-07-09", "updated": "2025-07-13"}
{"id": "2507.09423", "title": "Item-centric Exploration for Cold Start Problem", "authors": ["Dong Wang", "Junyi Jiao", "Arnab Bhadury", "Yaping Zhang", "Mingyan Gao", "Onkar Dalal"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Accepted for publication on 2025 ACM Recsys Conference Industry Track", "url": "http://arxiv.org/abs/2507.09423v1", "summary": "Recommender systems face a critical challenge in the item cold-start problem,\nwhich limits content diversity and exacerbates popularity bias by struggling to\nrecommend new items. While existing solutions often rely on auxiliary data, but\nthis paper illuminates a distinct, yet equally pressing, issue stemming from\nthe inherent user-centricity of many recommender systems. We argue that in\nenvironments with large and rapidly expanding item inventories, the traditional\nfocus on finding the \"best item for a user\" can inadvertently obscure the ideal\naudience for nascent content. To counter this, we introduce the concept of\nitem-centric recommendations, shifting the paradigm to identify the optimal\nusers for new items. Our initial realization of this vision involves an\nitem-centric control integrated into an exploration system. This control\nemploys a Bayesian model with Beta distributions to assess candidate items\nbased on a predicted balance between user satisfaction and the item's inherent\nquality. Empirical online evaluations reveal that this straightforward control\nmarkedly improves cold-start targeting efficacy, enhances user satisfaction\nwith newly explored content, and significantly increases overall exploration\nefficiency.", "comment": "Accepted for publication on 2025 ACM Recsys Conference Industry Track", "pdf_url": "http://arxiv.org/pdf/2507.09423v1", "cate": "cs.IR", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09385", "title": "Credit Card Fraud Detection Using RoFormer Model With Relative Distance Rotating Encoding", "authors": ["Kevin Reyes", "Vasco Cortez"], "categories": ["cs.NE", "cs.LG"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "Comments:      2025 IEEE Conference on Artificial Intelligence (CAI)", "url": "http://arxiv.org/abs/2507.09385v1", "summary": "Fraud detection is one of the most important challenges that financial\nsystems must address. Detecting fraudulent transactions is critical for payment\ngateway companies like Flow Payment, which process millions of transactions\nmonthly and require robust security measures to mitigate financial risks.\nIncreasing transaction authorization rates while reducing fraud is essential\nfor providing a good user experience and building a sustainable business. For\nthis reason, discovering novel and improved methods to detect fraud requires\ncontinuous research and investment for any company that wants to succeed in\nthis industry. In this work, we introduced a novel method for detecting\ntransactional fraud by incorporating the Relative Distance Rotating Encoding\n(ReDRE) in the RoFormer model. The incorporation of angle rotation using ReDRE\nenhances the characterization of time series data within a Transformer, leading\nto improved fraud detection by better capturing temporal dependencies and event\nrelationships.", "comment": "2025 IEEE Conference on Artificial Intelligence (CAI)", "pdf_url": "http://arxiv.org/pdf/2507.09385v1", "cate": "cs.NE", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.06787", "title": "Stream Function-Based Navigation for Complex Quadcopter Obstacle Avoidance", "authors": ["Sean Smith", "Emmanuel Witrant", "Ya-Jun Pan"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.06787v2", "summary": "This article presents a novel stream function-based navigational control\nsystem for obstacle avoidance, where obstacles are represented as\ntwo-dimensional (2D) rigid surfaces in inviscid, incompressible flows. The\napproach leverages the vortex panel method (VPM) and incorporates safety\nmargins to control the stream function and flow properties around virtual\nsurfaces, enabling navigation in complex, partially observed environments using\nreal-time sensing. To address the limitations of the VPM in managing relative\ndistance and avoiding rapidly accelerating obstacles at close proximity, the\nsystem integrates a model predictive controller (MPC) based on higher-order\ncontrol barrier functions (HOCBF). This integration incorporates VPM trajectory\ngeneration, state estimation, and constraint handling into a receding-horizon\noptimization problem. The 2D rigid surfaces are enclosed using minimum bounding\nellipses (MBEs), while an adaptive Kalman filter (AKF) captures and predicts\nobstacle dynamics, propagating these estimates into the MPC-HOCBF for rapid\navoidance maneuvers. Evaluation is conducted using a PX4-powered Clover drone\nGazebo simulator and real-time experiments involving a COEX Clover quadcopter\nequipped with a 360 degree LiDAR sensor.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.06787v2", "cate": "cs.RO", "date": "2025-07-09", "updated": "2025-07-13"}
{"id": "2507.09009", "title": "Multimodal Cardiovascular Risk Profiling Using Self-Supervised Learning of Polysomnography", "authors": ["Zhengxiao He", "Huayu Li", "Geng Yuan", "William D. S. Killgore", "Stuart F. Quan", "Chen X. Chen", "Ao Li"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09009v1", "summary": "Methods: We developed a self-supervised deep learning model that extracts\nmeaningful patterns from multi-modal signals (Electroencephalography (EEG),\nElectrocardiography (ECG), and respiratory signals). The model was trained on\ndata from 4,398 participants. Projection scores were derived by contrasting\nembeddings from individuals with and without CVD outcomes. External validation\nwas conducted in an independent cohort with 1,093 participants. The source code\nis available on https://github.com/miraclehetech/sleep-ssl. Results: The\nprojection scores revealed distinct and clinically meaningful patterns across\nmodalities. ECG-derived features were predictive of both prevalent and incident\ncardiac conditions, particularly CVD mortality. EEG-derived features were\npredictive of incident hypertension and CVD mortality. Respiratory signals\nadded complementary predictive value. Combining these projection scores with\nthe Framingham Risk Score consistently improved predictive performance,\nachieving area under the curve values ranging from 0.607 to 0.965 across\ndifferent outcomes. Findings were robustly replicated and validated in the\nexternal testing cohort. Conclusion: Our findings demonstrate that the proposed\nframework can generate individualized CVD risk scores directly from PSG data.\nThe resulting projection scores have the potential to be integrated into\nclinical practice, enhancing risk assessment and supporting personalized care.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09009v1", "cate": "cs.LG", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09556", "title": "SeqCSIST: Sequential Closely-Spaced Infrared Small Target Unmixing", "authors": ["Ximeng Zhai", "Bohan Xu", "Yaohong Chen", "Hao Wang", "Kehua Guo", "Yimian Dai"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by TGRS", "url": "http://arxiv.org/abs/2507.09556v1", "summary": "Due to the limitation of the optical lens focal length and the resolution of\nthe infrared detector, distant Closely-Spaced Infrared Small Target (CSIST)\ngroups typically appear as mixing spots in the infrared image. In this paper,\nwe propose a novel task, Sequential CSIST Unmixing, namely detecting all\ntargets in the form of sub-pixel localization from a highly dense CSIST group.\nHowever, achieving such precise detection is an extremely difficult challenge.\nIn addition, the lack of high-quality public datasets has also restricted the\nresearch progress. To this end, firstly, we contribute an open-source\necosystem, including SeqCSIST, a sequential benchmark dataset, and a toolkit\nthat provides objective evaluation metrics for this special task, along with\nthe implementation of 23 relevant methods. Furthermore, we propose the\nDeformable Refinement Network (DeRefNet), a model-driven deep learning\nframework that introduces a Temporal Deformable Feature Alignment (TDFA) module\nenabling adaptive inter-frame information aggregation. To the best of our\nknowledge, this work is the first endeavor to address the CSIST Unmixing task\nwithin a multi-frame paradigm. Experiments on the SeqCSIST dataset demonstrate\nthat our method outperforms the state-of-the-art approaches with mean Average\nPrecision (mAP) metric improved by 5.3\\%. Our dataset and toolkit are available\nfrom https://github.com/GrokCV/SeqCSIST.", "comment": "Accepted by TGRS", "pdf_url": "http://arxiv.org/pdf/2507.09556v1", "cate": "cs.CV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.08773", "title": "Total/dual correlation/coherence, redundancy/synergy, complexity, and O-information for real and complex valued multivariate data", "authors": ["Roberto D. Pascual-Marqui", "Kieko Kochi", "Toshihiko Kinoshita"], "categories": ["stat.ME", "cs.IT", "math.IT", "math.ST", "stat.TH"], "primary_category": "Subjects:       Methodology (stat.ME)", "pdf_link": null, "comments": "Comments:      version 2 fixed: (A) header now includes DOI link to paper; (B) figure 1 now has correct AR coeffs; (C) link to software", "url": "http://arxiv.org/abs/2507.08773v2", "summary": "Firstly, assuming Gaussianity, equations for the following information theory\nmeasures are presented: total correlation/coherence (TC), dual total\ncorrelation/coherence (DTC), O-information, TSE complexity, and\nredundancy-synergy index (RSI). Since these measures are functions of the\ncovariance matrix \"S\" and its inverse \"S^-1\", the associated Wishart and\ninverse-Wishart distributions are of note. DTC is shown to be the\nKullback-Leibler (KL) divergence for the inverse-Wishart pair \"(S^-1)\" and its\ndiagonal matrix \"D=diag(S^-1)\", shedding light on its interpretation as a\nmeasure of \"total partial correlation\", -lndetP, with test hypothesis H0: P=I,\nwhere \"P\" is the standardized inverse covariance (i.e.\nP=(D^-1/2)(S^-1)(D^-1/2). The second aim of this paper introduces a\ngeneralization of all these measures for structured groups of variables. For\ninstance, consider three or more groups, each consisting of three or more\nvariables, with predominant redundancy within each group, but with synergistic\ninteractions between groups. O-information will miss the between group synergy\n(since redundancy occurs more often in the system). In contrast, the structured\nO-information measure presented here will correctly report predominant synergy\nbetween groups. This is a relevant generalization towards structured\nmultivariate information measures. A third aim is the presentation of a\nframework for quantifying the contribution of \"connections\" between variables,\nto the system's TC, DTC, O-information, and TSE complexity. A fourth aim is to\npresent a generalization of the redundancy-synergy index for quantifying the\ncontribution of a group of variables to the system's redundancy-synergy\nbalance. Finally, it is shown that the expressions derived here directly apply\nto data from several other elliptical distributions. All program codes, data\nfiles, and executables are available (https://osf.io/jd37g/).", "comment": "version 2 fixed: (A) header now includes DOI link to paper; (B)\n  figure 1 now has correct AR coeffs; (C) link to software", "pdf_url": "http://arxiv.org/pdf/2507.08773v2", "cate": "stat.ME", "date": "2025-07-11", "updated": "2025-07-14"}
{"id": "2310.17827", "title": "A hierarchy of eigencomputations for polynomial optimization on the sphere", "authors": ["Benjamin Lovitz", "Nathaniel Johnston"], "categories": ["math.OC", "cs.DS", "math.AG", "quant-ph"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      41 pages. Final version", "url": "http://arxiv.org/abs/2310.17827v3", "summary": "We introduce a convergent hierarchy of lower bounds on the minimum value of a\nreal form over the unit sphere. The main practical advantage of our hierarchy\nover the real sum-of-squares (RSOS) hierarchy is that the lower bound at each\nlevel of our hierarchy is obtained by a minimum eigenvalue computation, as\nopposed to the full semidefinite program (SDP) required at each level of RSOS.\nIn practice, this allows us to compute bounds on much larger forms than are\ncomputationally feasible for RSOS. Our hierarchy outperforms previous\nalternatives to RSOS, both asymptotically and in numerical experiments. We\nobtain our hierarchy by proving a reduction from real optimization on the\nsphere to Hermitian optimization on the sphere, and invoking the Hermitian\nsum-of-squares (HSOS) hierarchy. This opens the door to using other Hermitian\noptimization techniques for real optimization, and gives a path towards\ndeveloping spectral hierarchies for more general constrained real optimization\nproblems. To this end, we use our techniques to develop a hierarchy of\neigencomputations for computing the real tensor spectral norm.", "comment": "41 pages. Final version", "pdf_url": "http://arxiv.org/pdf/2310.17827v3", "cate": "math.OC", "date": "2023-10-27", "updated": "2025-07-13"}
{"id": "2507.09483", "title": "Does UMBRELA Work on Other LLMs?", "authors": ["Naghmeh Farzi", "Laura Dietz"], "categories": ["cs.IR", "H.3.3; I.2.7"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      9 pages, 2 figures, accepted to SIGIR 2025", "url": "http://arxiv.org/abs/2507.09483v1", "summary": "We reproduce the UMBRELA LLM Judge evaluation framework across a range of\nlarge language models (LLMs) to assess its generalizability beyond the original\nstudy. Our investigation evaluates how LLM choice affects relevance assessment\naccuracy, focusing on leaderboard rank correlation and per-label agreement\nmetrics. Results demonstrate that UMBRELA with DeepSeek V3 obtains very\ncomparable performance to GPT-4o (used in original work). For LLaMA-3.3-70B we\nobtain slightly lower performance, which further degrades with smaller LLMs.", "comment": "9 pages, 2 figures, accepted to SIGIR 2025", "pdf_url": "http://arxiv.org/pdf/2507.09483v1", "cate": "cs.IR", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09747", "title": "BrainFLORA: Uncovering Brain Concept Representation via Multimodal Neural Embeddings", "authors": ["Dongyang Li", "Haoyang Qin", "Mingyang Wu", "Chen Wei", "Quanying Liu"], "categories": ["cs.NE"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "Comments:      10 pages, ACM MM 2025", "url": "http://arxiv.org/abs/2507.09747v1", "summary": "Understanding how the brain represents visual information is a fundamental\nchallenge in neuroscience and artificial intelligence. While AI-driven decoding\nof neural data has provided insights into the human visual system, integrating\nmultimodal neuroimaging signals, such as EEG, MEG, and fMRI, remains a critical\nhurdle due to their inherent spatiotemporal misalignment. Current approaches\noften analyze these modalities in isolation, limiting a holistic view of neural\nrepresentation. In this study, we introduce BrainFLORA, a unified framework for\nintegrating cross-modal neuroimaging data to construct a shared neural\nrepresentation. Our approach leverages multimodal large language models (MLLMs)\naugmented with modality-specific adapters and task decoders, achieving\nstate-of-the-art performance in joint-subject visual retrieval task and has the\npotential to extend multitasking. Combining neuroimaging analysis methods, we\nfurther reveal how visual concept representations align across neural\nmodalities and with real world object perception. We demonstrate that the\nbrain's structured visual concept representations exhibit an implicit mapping\nto physical-world stimuli, bridging neuroscience and machine learning from\ndifferent modalities of neural imaging. Beyond methodological advancements,\nBrainFLORA offers novel implications for cognitive neuroscience and\nbrain-computer interfaces (BCIs). Our code is available at\nhttps://github.com/ncclab-sustech/BrainFLORA.", "comment": "10 pages, ACM MM 2025", "pdf_url": "http://arxiv.org/pdf/2507.09747v1", "cate": "cs.NE", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2311.17749", "title": "Learning Free Terminal Time Optimal Closed-loop Control of Manipulators", "authors": ["Wei Hu", "Yue Zhao", "Weinan E", "Jiequn Han", "Jihao Long"], "categories": ["math.OC", "cs.RO"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      Accepted for presentation at the American Control Conference (ACC) 2025", "url": "http://arxiv.org/abs/2311.17749v2", "summary": "This paper presents a novel approach to learning free terminal time\nclosed-loop control for robotic manipulation tasks, enabling dynamic adjustment\nof task duration and control inputs to enhance performance. We extend the\nsupervised learning approach, namely solving selected optimal open-loop\nproblems and utilizing them as training data for a policy network, to the free\nterminal time scenario. Three main challenges are addressed in this extension.\nFirst, we introduce a marching scheme that enhances the solution quality and\nincreases the success rate of the open-loop solver by gradually refining time\ndiscretization. Second, we extend the QRnet in Nakamura-Zimmerer et al. (2021b)\nto the free terminal time setting to address discontinuity and improve\nstability at the terminal state. Third, we present a more automated version of\nthe initial value problem (IVP) enhanced sampling method from previous work\n(Zhang et al., 2022) to adaptively update the training dataset, significantly\nimproving its quality. By integrating these techniques, we develop a\nclosed-loop policy that operates effectively over a broad domain with varying\noptimal time durations, achieving near globally optimal total costs.", "comment": "Accepted for presentation at the American Control Conference (ACC)\n  2025", "pdf_url": "http://arxiv.org/pdf/2311.17749v2", "cate": "math.OC", "date": "2023-11-29", "updated": "2025-07-12"}
{"id": "2507.09028", "title": "From Classical Machine Learning to Emerging Foundation Models: Review on Multimodal Data Integration for Cancer Research", "authors": ["Amgad Muneer", "Muhammad Waqas", "Maliazurina B Saad", "Eman Showkatian", "Rukhmini Bandyopadhyay", "Hui Xu", "Wentao Li", "Joe Y Chang", "Zhongxing Liao", "Cara Haymaker", "Luisa Solis Soto", "Carol C Wu", "Natalie I Vokes", "Xiuning Le", "Lauren A Byers", "Don L Gibbons", "John V Heymach", "Jianjun Zhang", "Jia Wu"], "categories": ["q-bio.QM", "cs.AI"], "primary_category": "Subjects:       Quantitative Methods (q-bio.QM)", "pdf_link": null, "comments": "Comments:      6 figures, 3 tables", "url": "http://arxiv.org/abs/2507.09028v1", "summary": "Cancer research is increasingly driven by the integration of diverse data\nmodalities, spanning from genomics and proteomics to imaging and clinical\nfactors. However, extracting actionable insights from these vast and\nheterogeneous datasets remains a key challenge. The rise of foundation models\n(FMs) -- large deep-learning models pretrained on extensive amounts of data\nserving as a backbone for a wide range of downstream tasks -- offers new\navenues for discovering biomarkers, improving diagnosis, and personalizing\ntreatment. This paper presents a comprehensive review of widely adopted\nintegration strategies of multimodal data to assist advance the computational\napproaches for data-driven discoveries in oncology. We examine emerging trends\nin machine learning (ML) and deep learning (DL), including methodological\nframeworks, validation protocols, and open-source resources targeting cancer\nsubtype classification, biomarker discovery, treatment guidance, and outcome\nprediction. This study also comprehensively covers the shift from traditional\nML to FMs for multimodal integration. We present a holistic view of recent FMs\nadvancements and challenges faced during the integration of multi-omics with\nadvanced imaging data. We identify the state-of-the-art FMs, publicly available\nmulti-modal repositories, and advanced tools and methods for data integration.\nWe argue that current state-of-the-art integrative methods provide the\nessential groundwork for developing the next generation of large-scale,\npre-trained models poised to further revolutionize oncology. To the best of our\nknowledge, this is the first review to systematically map the transition from\nconventional ML to advanced FM for multimodal data integration in oncology,\nwhile also framing these developments as foundational for the forthcoming era\nof large-scale AI models in cancer research.", "comment": "6 figures, 3 tables", "pdf_url": "http://arxiv.org/pdf/2507.09028v1", "cate": "q-bio.QM", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09560", "title": "EHPE: A Segmented Architecture for Enhanced Hand Pose Estimation", "authors": ["Bolun Zheng", "Xinjie Liu", "Qianyu Zhang", "Canjin Wang", "Fangni Chen", "Mingen Xu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09560v1", "summary": "3D hand pose estimation has garnered great attention in recent years due to\nits critical applications in human-computer interaction, virtual reality, and\nrelated fields. The accurate estimation of hand joints is essential for\nhigh-quality hand pose estimation. However, existing methods neglect the\nimportance of Distal Phalanx Tip (TIP) and Wrist in predicting hand joints\noverall and often fail to account for the phenomenon of error accumulation for\ndistal joints in gesture estimation, which can cause certain joints to incur\nlarger errors, resulting in misalignments and artifacts in the pose estimation\nand degrading the overall reconstruction quality. To address this challenge, we\npropose a novel segmented architecture for enhanced hand pose estimation\n(EHPE). We perform local extraction of TIP and wrist, thus alleviating the\neffect of error accumulation on TIP prediction and further reduce the\npredictive errors for all joints on this basis. EHPE consists of two key\nstages: In the TIP and Wrist Joints Extraction stage (TW-stage), the positions\nof the TIP and wrist joints are estimated to provide an initial accurate joint\nconfiguration; In the Prior Guided Joints Estimation stage (PG-stage), a\ndual-branch interaction network is employed to refine the positions of the\nremaining joints. Extensive experiments on two widely used benchmarks\ndemonstrate that EHPE achieves state-of-the-arts performance. Code is available\nat https://github.com/SereinNout/EHPE.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09560v1", "cate": "cs.CV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2505.05819", "title": "New Statistical and Computational Results for Learning Junta Distributions", "authors": ["Lorenzo Beretta"], "categories": ["cs.LG", "cs.DS"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      RANDOM 2025", "url": "http://arxiv.org/abs/2505.05819v3", "summary": "We study the problem of learning junta distributions on $\\{0, 1\\}^n$, where a\ndistribution is a $k$-junta if its probability mass function depends on a\nsubset of at most $k$ variables. We make two main contributions:\n  - We show that learning $k$-junta distributions is \\emph{computationally}\nequivalent to learning $k$-parity functions with noise (LPN), a landmark\nproblem in computational learning theory.\n  - We design an algorithm for learning junta distributions whose statistical\ncomplexity is optimal, up to polylogarithmic factors. Computationally, our\nalgorithm matches the complexity of previous (non-sample-optimal) algorithms.\n  Combined, our two contributions imply that our algorithm cannot be\nsignificantly improved, statistically or computationally, barring a\nbreakthrough for LPN.", "comment": "RANDOM 2025", "pdf_url": "http://arxiv.org/pdf/2505.05819v3", "cate": "cs.LG", "date": "2025-05-09", "updated": "2025-07-12"}
{"id": "2507.09488", "title": "Criteria-Based LLM Relevance Judgments", "authors": ["Naghmeh Farzi", "Laura Dietz"], "categories": ["cs.IR", "H.3.3; I.2.7"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      10 pages, 3 figures, accepted to ICTIR 2025", "url": "http://arxiv.org/abs/2507.09488v1", "summary": "Relevance judgments are crucial for evaluating information retrieval systems,\nbut traditional human-annotated labels are time-consuming and expensive. As a\nresult, many researchers turn to automatic alternatives to accelerate method\ndevelopment. Among these, Large Language Models (LLMs) provide a scalable\nsolution by generating relevance labels directly through prompting. However,\nprompting an LLM for a relevance label without constraints often results in not\nonly incorrect predictions but also outputs that are difficult for humans to\ninterpret. We propose the Multi-Criteria framework for LLM-based relevance\njudgments, decomposing the notion of relevance into multiple criteria--such as\nexactness, coverage, topicality, and contextual fit--to improve the robustness\nand interpretability of retrieval evaluations compared to direct grading\nmethods. We validate this approach on three datasets: the TREC Deep Learning\ntracks from 2019 and 2020, as well as LLMJudge (based on TREC DL 2023). Our\nresults demonstrate that Multi-Criteria judgments enhance the system\nranking/leaderboard performance. Moreover, we highlight the strengths and\nlimitations of this approach relative to direct grading approaches, offering\ninsights that can guide the development of future automatic evaluation\nframeworks in information retrieval.", "comment": "10 pages, 3 figures, accepted to ICTIR 2025", "pdf_url": "http://arxiv.org/pdf/2507.09488v1", "cate": "cs.IR", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09847", "title": "Effective Self-Attention-Based Deep Learning Model with Evolutionary Grid Search for Robust Wave Farm Energy Forecasting", "authors": ["Amin Abdollahi Dehkordi", "Mehdi Neshat", "Nataliia Y. Sergiienko", "Zahra Ghasemi", "Lei Chen", "John Boland", "Hamid Moradkhani", "Amir H. Gandomi"], "categories": ["cs.NE"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09847v1", "summary": "Achieving carbon neutrality, a key focus of UN SDG #13, drives the\nexploration of wave energy, a renewable resource with the potential to generate\n30,000 TWh of clean electricity annually, surpassing global demand. However,\nwave energy remains underdeveloped due to technical and economic challenges,\nparticularly in forecasting wave farm power output, which is vital for grid\nstability and commercial viability. This study proposes a novel predictive\nframework to enhance wave energy integration into power grids. It introduces a\nhybrid sequential learning model combining Self-Attention-enhanced\nConvolutional Bi-LSTM with hyperparameter optimization. The model leverages\nspatial data from Wave Energy Converters (WECs) and is validated using datasets\nfrom wave farms in Adelaide, Sydney, Perth, and Tasmania, Australia.\nBenchmarked against ten machine learning algorithms, the model achieves\nsuperior accuracy, with R2 scores of 91.7% (Adelaide), 88.0% (Perth), 82.8%\n(Tasmania), and 91.0% (Sydney). It outperforms conventional ML and deep\nlearning methods, offering robust and scalable predictions for wave energy\noutput across diverse marine environments, supporting reliable integration into\nenergy systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09847v1", "cate": "cs.NE", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09131", "title": "Investigation of Shock-Capturing with Bound-Preserving Limiters for the Nonlinearly Stable Flux Reconstruction Method", "authors": ["Sai Shruthi Srinivasan", "Siva Nadarajah"], "categories": ["math.NA", "cs.NA", "physics.flu-dyn"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      51 pages, 27 figures", "url": "http://arxiv.org/abs/2507.09131v1", "summary": "Nonlinearly stable flux reconstruction (NSFR) combines the key properties of\nprovable nonlinear stability with the increased time step from energy-stable\nflux reconstruction. The NSFR scheme has been successfully applied to unsteady\ncompressible flows. Through the use of a bound-preserving limiter, positivity\nof thermodynamic quantities is preserved, and this enables the extension of\nNSFR to hyperbolic conservation laws. We extend the limiter of Zhang and Shu\n[1] to ensure robustness for the proposed scheme. The limiter is modified to\nconsider the minimum density and pressure at the solution nodes when\ndetermining the value to scale the solution. The modifications are thoroughly\ntested with a suite of test cases. In addition to these modifications, this\npaper conducts a thorough investigation into the shock-capturing capabilities\nof the NSFR scheme and the advantages it presents over standard discontinuous\nGalerkin (DG) methods, where, on select variants of the flux reconstruction\n(FR) scheme, essentially oscillation-free solutions are demonstrated. Various\nparameters of the scheme are extensively tested and analyzed through several 1D\nand 2D compressible Euler tests that verify the high-order accuracy, entropy\nstability, time step advantage and shock-capturing capabilities of the NSFR\nscheme. These parameters include the two-point flux, quadrature nodes and the\nstrength of the FR parameter. In addition to investigating the impact of the\nvarious two-point fluxes, this paper also presents numerical studies to\ndetermine the CFL condition required to maintain positivity for the two-point\nflux of choice. The investigation yields insightful results for all parameters,\nwith the results pertaining to the type of FR scheme being of special interest.\nThe tests showcase increased robustness, time step advantages and\noscillation/overshoot mitigation when employing a stronger FR parameter.", "comment": "51 pages, 27 figures", "pdf_url": "http://arxiv.org/pdf/2507.09131v1", "cate": "math.NA", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2411.09145", "title": "Self-Supervised Monocular 4D Scene Reconstruction for Egocentric Videos", "authors": ["Chengbo Yuan", "Geng Chen", "Li Yi", "Yang Gao"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.09145v4", "summary": "Egocentric videos provide valuable insights into human interactions with the\nphysical world, which has sparked growing interest in the computer vision and\nrobotics communities. A critical challenge in fully understanding the geometry\nand dynamics of egocentric videos is dense scene reconstruction. However, the\nlack of high-quality labeled datasets in this field has hindered the\neffectiveness of current supervised learning methods. In this work, we aim to\naddress this issue by exploring an self-supervised dynamic scene reconstruction\napproach. We introduce EgoMono4D, a novel model that unifies the estimation of\nmultiple variables necessary for Egocentric Monocular 4D reconstruction,\nincluding camera intrinsic, camera poses, and video depth, all within a fast\nfeed-forward framework. Starting from pretrained single-frame depth and\nintrinsic estimation model, we extend it with camera poses estimation and align\nmulti-frame results on large-scale unlabeled egocentric videos. We evaluate\nEgoMono4D in both in-domain and zero-shot generalization settings, achieving\nsuperior performance in dense pointclouds sequence reconstruction compared to\nall baselines. EgoMono4D represents the first attempt to apply self-supervised\nlearning for pointclouds sequence reconstruction to the label-scarce egocentric\nfield, enabling fast, dense, and generalizable reconstruction. The interactable\nvisualization, code and trained models are released\nhttps://egomono4d.github.io/", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.09145v4", "cate": "cs.CV", "date": "2024-11-14", "updated": "2025-07-12"}
{"id": "2507.09029", "title": "Model Parallelism With Subnetwork Data Parallelism", "authors": ["Vaibhav Singh", "Zafir Khalid", "Edouard Oyallon", "Eugene Belilovsky"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      6 pages, 1 figure", "url": "http://arxiv.org/abs/2507.09029v1", "summary": "Distributed pre-training of large models at scale often imposes heavy memory\ndemands on individual nodes and incurs significant intra-node communication\ncosts. We propose a novel alternative approach that reduces the memory\nrequirements by training small, structured subnetworks of the model on separate\nworkers. Unlike pipelining, our method avoids inter-node activation\ncommunication and maintains bandwidth requirements that are comparable to or\nlower than standard data parallel communication schemes based on all-reduce. We\nevaluate two subnetwork construction strategies guided by the principle of\nensuring uniform representation of each parameter across the distributed\ntraining setup. Our results show that the stochastic block dropping technique\nconsistently outperforms the width-wise subnetwork construction previously\nexplored in federated learning. We empirically attribute this superior\nperformance to stronger gradient alignment in subnetworks that retain blocks\nhaving skip connections. Preliminary experiments highlight the promise of our\napproach, achieving a 20-40% reduction in memory usage without any loss in\nperformance.", "comment": "6 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2507.09029v1", "cate": "cs.LG", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09562", "title": "Prompt Engineering in Segment Anything Model: Methodologies, Applications, and Emerging Challenges", "authors": ["Yidong Jiang"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09562v1", "summary": "The Segment Anything Model (SAM) has revolutionized image segmentation\nthrough its innovative prompt-based approach, yet the critical role of prompt\nengineering in its success remains underexplored. This paper presents the first\ncomprehensive survey focusing specifically on prompt engineering techniques for\nSAM and its variants. We systematically organize and analyze the rapidly\ngrowing body of work in this emerging field, covering fundamental\nmethodologies, practical applications, and key challenges. Our review reveals\nhow prompt engineering has evolved from simple geometric inputs to\nsophisticated multimodal approaches, enabling SAM's adaptation across diverse\ndomains including medical imaging and remote sensing. We identify unique\nchallenges in prompt optimization and discuss promising research directions.\nThis survey fills an important gap in the literature by providing a structured\nframework for understanding and advancing prompt engineering in foundation\nmodels for segmentation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09562v1", "cate": "cs.CV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09566", "title": "Identifying Offline Metrics that Predict Online Impact: A Pragmatic Strategy for Real-World Recommender Systems", "authors": ["Timo Wilm", "Philipp Normann"], "categories": ["cs.IR", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      This work was accepted for publication in the 19th ACM Conference on Recommender Systems (RecSys 2025). The final published version will be available at the ACM Digital Library", "url": "http://arxiv.org/abs/2507.09566v1", "summary": "A critical challenge in recommender systems is to establish reliable\nrelationships between offline and online metrics that predict real-world\nperformance. Motivated by recent advances in Pareto front approximation, we\nintroduce a pragmatic strategy for identifying offline metrics that align with\nonline impact. A key advantage of this approach is its ability to\nsimultaneously serve multiple test groups, each with distinct offline\nperformance metrics, in an online experiment controlled by a single model. The\nmethod is model-agnostic for systems with a neural network backbone, enabling\nbroad applicability across architectures and domains. We validate the strategy\nthrough a large-scale online experiment in the field of session-based\nrecommender systems on the OTTO e-commerce platform. The online experiment\nidentifies significant alignments between offline metrics and real-word\nclick-through rate, post-click conversion rate and units sold. Our strategy\nprovides industry practitioners with a valuable tool for understanding\noffline-to-online metric relationships and making informed, data-driven\ndecisions.", "comment": "This work was accepted for publication in the 19th ACM Conference on\n  Recommender Systems (RecSys 2025). The final published version will be\n  available at the ACM Digital Library", "pdf_url": "http://arxiv.org/pdf/2507.09566v1", "cate": "cs.IR", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09782", "title": "Physics-informed neural networks for high-dimensional solutions and snaking bifurcations in nonlinear lattices", "authors": ["Muhammad Luthfi Shahab", "Fidya Almira Suheri", "Rudy Kusdiantara", "Hadi Susanto"], "categories": ["math.NA", "cs.LG", "cs.NA", "cs.NE", "math.OC"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      Accepted for publication in Physica D: Nonlinear Phenomena", "url": "http://arxiv.org/abs/2507.09782v1", "summary": "This paper introduces a framework based on physics-informed neural networks\n(PINNs) for addressing key challenges in nonlinear lattices, including solution\napproximation, bifurcation diagram construction, and linear stability analysis.\nWe first employ PINNs to approximate solutions of nonlinear systems arising\nfrom lattice models, using the Levenberg-Marquardt algorithm to optimize\nnetwork weights for greater accuracy. To enhance computational efficiency in\nhigh-dimensional settings, we integrate a stochastic sampling strategy. We then\nextend the method by coupling PINNs with a continuation approach to compute\nsnaking bifurcation diagrams, incorporating an auxiliary equation to\neffectively track successive solution branches. For linear stability analysis,\nwe adapt PINNs to compute eigenvectors, introducing output constraints to\nenforce positivity, in line with Sturm-Liouville theory. Numerical experiments\nare conducted on the discrete Allen-Cahn equation with cubic and quintic\nnonlinearities in one to five spatial dimensions. The results demonstrate that\nthe proposed approach achieves accuracy comparable to, or better than,\ntraditional numerical methods, especially in high-dimensional regimes where\ncomputational resources are a limiting factor. These findings highlight the\npotential of neural networks as scalable and efficient tools for the study of\ncomplex nonlinear lattice systems.", "comment": "Accepted for publication in Physica D: Nonlinear Phenomena", "pdf_url": "http://arxiv.org/pdf/2507.09782v1", "cate": "math.NA", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09263", "title": "Crack-tip field characterization in nonlinearly constituted and geometrically linear elastoporous solid containing a star-shaped crack: A finite element study", "authors": ["S. M. Mallikarjunaiah", "Kun Gou"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09263v1", "summary": "This paper introduces a three-dimensional (3-D) mathematical and\ncomputational framework for the characterization of crack-tip fields in\nstar-shaped cracks within porous elastic solids. A core emphasis of this model\nis its direct integration of density-dependent elastic moduli, offering a more\nphysically realistic representation of engineering materials where intrinsic\nporosity and density profoundly influence mechanical behavior. The governing\nboundary value problem, formulated for the static equilibrium of a 3-D,\nhomogeneous, and isotropic material, manifests as a system of second-order,\nquasilinear partial differential equations. This system is meticulously coupled\nwith classical traction-free boundary conditions imposed at the complex crack\nsurface. For the robust numerical solution of this intricate nonlinear problem,\nwe employ a continuous trilinear Galerkin-type finite element discretization.\nThe inherent strong nonlinearities arising within the discrete system are\neffectively managed through a powerful and stable {Picard-type linearization\nscheme}. The proposed model demonstrates a remarkable ability to accurately\ndescribe the full stress and strain states in a diverse range of materials,\ncrucially recovering the well-established classical singularities observed in\nlinearized elastic fracture mechanics. A comprehensive numerical examination of\ntensile stress, strain, and strain energy density fields consistently reveals\nthat these quantities attain their peak values in the immediate vicinity of the\ncrack tip, an observation that remarkably aligns with established findings in\nstandard linearized elastic fracture mechanics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09263v1", "cate": "math.NA", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2503.02687", "title": "Class-Aware PillarMix: Can Mixed Sample Data Augmentation Enhance 3D Object Detection with Radar Point Clouds?", "authors": ["Miao Zhang", "Sherif Abdulatif", "Benedikt Loesch", "Marco Altmann", "Bin Yang"], "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      8 pages, 6 figures, 4 tables, accepted to 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025)", "url": "http://arxiv.org/abs/2503.02687v2", "summary": "Due to the significant effort required for data collection and annotation in\n3D perception tasks, mixed sample data augmentation (MSDA) has been widely\nstudied to generate diverse training samples by mixing existing data. Recently,\nmany MSDA techniques have been developed for point clouds, but they mainly\ntarget LiDAR data, leaving their application to radar point clouds largely\nunexplored. In this paper, we examine the feasibility of applying existing MSDA\nmethods to radar point clouds and identify several challenges in adapting these\ntechniques. These obstacles stem from the radar's irregular angular\ndistribution, deviations from a single-sensor polar layout in multi-radar\nsetups, and point sparsity. To address these issues, we propose Class-Aware\nPillarMix (CAPMix), a novel MSDA approach that applies MixUp at the pillar\nlevel in 3D point clouds, guided by class labels. Unlike methods that rely a\nsingle mix ratio to the entire sample, CAPMix assigns an independent ratio to\neach pillar, boosting sample diversity. To account for the density of different\nclasses, we use class-specific distributions: for dense objects (e.g., large\nvehicles), we skew ratios to favor points from another sample, while for sparse\nobjects (e.g., pedestrians), we sample more points from the original. This\nclass-aware mixing retains critical details and enriches each sample with new\ninformation, ultimately generating more diverse training data. Experimental\nresults demonstrate that our method not only significantly boosts performance\nbut also outperforms existing MSDA approaches across two datasets (Bosch Street\nand K-Radar). We believe that this straightforward yet effective approach will\nspark further investigation into MSDA techniques for radar data.", "comment": "8 pages, 6 figures, 4 tables, accepted to 2025 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS 2025)", "pdf_url": "http://arxiv.org/pdf/2503.02687v2", "cate": "cs.CV", "date": "2025-03-04", "updated": "2025-07-14"}
{"id": "2507.09037", "title": "ALIGN: Prompt-based Attribute Alignment for Reliable, Responsible, and Personalized LLM-based Decision-Making", "authors": ["Bharadwaj Ravichandran", "David Joy", "Paul Elliott", "Brian Hu", "Jadie Adams", "Christopher Funk", "Emily Veenhuis", "Anthony Hoogs", "Arslan Basharat"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      10 pages total (including appendix), ICML 2025 Workshop on Reliable and Responsible Foundation Models", "url": "http://arxiv.org/abs/2507.09037v1", "summary": "Large language models (LLMs) are increasingly being used as decision aids.\nHowever, users have diverse values and preferences that can affect their\ndecision-making, which requires novel methods for LLM alignment and\npersonalization. Existing LLM comparison tools largely focus on benchmarking\ntasks, such as knowledge-based question answering. In contrast, our proposed\nALIGN system focuses on dynamic personalization of LLM-based decision-makers\nthrough prompt-based alignment to a set of fine-grained attributes. Key\nfeatures of our system include robust configuration management, structured\noutput generation with reasoning, and several algorithm implementations with\nswappable LLM backbones, enabling different types of analyses. Our user\ninterface enables a qualitative, side-by-side comparison of LLMs and their\nalignment to various attributes, with a modular backend for easy algorithm\nintegration. Additionally, we perform a quantitative analysis comparing\nalignment approaches in two different domains: demographic alignment for public\nopinion surveys and value alignment for medical triage decision-making. The\nentire ALIGN framework is open source and will enable new research on reliable,\nresponsible, and personalized LLM-based decision-makers.", "comment": "10 pages total (including appendix), ICML 2025 Workshop on Reliable\n  and Responsible Foundation Models", "pdf_url": "http://arxiv.org/pdf/2507.09037v1", "cate": "cs.CL", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09573", "title": "WordCraft: Interactive Artistic Typography with Attention Awareness and Noise Blending", "authors": ["Zhe Wang", "Jingbo Zhang", "Tianyi Wei", "Wanchao Su", "Can Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      14 pages, 16 figures", "url": "http://arxiv.org/abs/2507.09573v1", "summary": "Artistic typography aims to stylize input characters with visual effects that\nare both creative and legible. Traditional approaches rely heavily on manual\ndesign, while recent generative models, particularly diffusion-based methods,\nhave enabled automated character stylization. However, existing solutions\nremain limited in interactivity, lacking support for localized edits, iterative\nrefinement, multi-character composition, and open-ended prompt interpretation.\nWe introduce WordCraft, an interactive artistic typography system that\nintegrates diffusion models to address these limitations. WordCraft features a\ntraining-free regional attention mechanism for precise, multi-region generation\nand a noise blending that supports continuous refinement without compromising\nvisual quality. To support flexible, intent-driven generation, we incorporate a\nlarge language model to parse and structure both concrete and abstract user\nprompts. These components allow our framework to synthesize high-quality,\nstylized typography across single- and multi-character inputs across multiple\nlanguages, supporting diverse user-centered workflows. Our system significantly\nenhances interactivity in artistic typography synthesis, opening up creative\npossibilities for artists and designers.", "comment": "14 pages, 16 figures", "pdf_url": "http://arxiv.org/pdf/2507.09573v1", "cate": "cs.CV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09924", "title": "MixLoRA-DSI: Dynamically Expandable Mixture-of-LoRA Experts for Rehearsal-Free Generative Retrieval over Dynamic Corpora", "authors": ["Tuan-Luc Huynh", "Thuy-Trang Vu", "Weiqing Wang", "Trung Le", "Dragan Gašević", "Yuan-Fang Li", "Thanh-Toan Do"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09924v1", "summary": "Continually updating model-based indexes in generative retrieval with new\ndocuments remains challenging, as full retraining is computationally expensive\nand impractical under resource constraints. We propose MixLoRA-DSI, a novel\nframework that combines an expandable mixture of Low-Rank Adaptation experts\nwith a layer-wise out-of-distribution (OOD)-driven expansion strategy. Instead\nof allocating new experts for each new corpus, our proposed expansion strategy\nenables sublinear parameter growth by selectively introducing new experts only\nwhen significant number of OOD documents are detected. Experiments on NQ320k\nand MS MARCO Passage demonstrate that MixLoRA-DSI outperforms full-model update\nbaselines, with minimal parameter overhead and substantially lower training\ncosts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09924v1", "cate": "cs.IR", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09992", "title": "Evolution of Fear and Social Rewards in Prey-Predator Relationship", "authors": ["Yuji Kanagawa", "Kenji Doya"], "categories": ["q-bio.PE", "cs.AI", "cs.NE"], "primary_category": "Subjects:       Populations and Evolution (q-bio.PE)", "pdf_link": null, "comments": "Comments:      Preprint. Under review", "url": "http://arxiv.org/abs/2507.09992v1", "summary": "Fear is a critical brain function for detecting danger and learning to avoid\nspecific stimuli that can lead to danger. While fear is believed to have\nevolved under pressure from predators, experimentally reproducing the evolution\nis challenging. To investigate the relationship between environmental\nconditions, the evolution of fear, and the evolution of other rewards, such as\nfood reward and social reward, we developed a distributed evolutionary\nsimulation. In our simulation, prey and predator agents co-evolve their innate\nreward functions, including a possibly fear-like term for observing predators,\nand learn behaviors via reinforcement learning. Surprisingly, our simulation\nrevealed that social reward for observing the same species is more important\nfor prey to survive, and fear-like negative reward for observing predators\nevolves only after acquiring social reward. We also found that the predator\nwith increased hunting ability (larger mouth) amplified fear emergence, but\nalso that fear evolution is more stable with non-evolving predators that are\nbad at chasing prey. Additionally, unlike for predators, we found that positive\nrewards evolve in opposition to fear for stationary threats, as areas with\nabundant leftover food develop around them. These findings suggest that fear\nand social reward have had a complex interplay with each other through\nevolution, along with the nature of predators and threats.", "comment": "Preprint. Under review", "pdf_url": "http://arxiv.org/pdf/2507.09992v1", "cate": "q-bio.PE", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09274", "title": "Benchmark stress tests for flow past a cylinder at higher Reynolds numbers using EMAC", "authors": ["Henry von Wahl", "Leo G. Rebholz", "L. Ridgway Scott"], "categories": ["math.NA", "cs.NA", "physics.flu-dyn"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09274v1", "summary": "We consider a test problem for Navier-Stokes solvers based on the flow around\na cylinder at Reynolds numbers 500 and 1000, where the solution is observed to\nbe periodic when the problem is sufficiently resolved. Computing the resulting\nflow is a challenge, even for exactly divergence-free discretization methods,\nwhen the scheme does not include sufficient numerical dissipation. We examine\nthe performance of the energy, momentum and angular momentum conserving (EMAC)\nformulation of the Navier-Stokes equations. This incorporates more physical\nconservation into the finite element method even when the numerical solution is\nnot exactly divergence-free. Consequently, it has a chance to outperform\nstandard methods, especially for long-time simulations. We find that for\nlowest-order Taylor-Hood elements, EMAC outperforms the standard convective\nformulations. However, for higher-order elements, EMAC can become unstable on\nunder-resolved meshes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09274v1", "cate": "math.NA", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2503.10701", "title": "Video Individual Counting for Moving Drones", "authors": ["Yaowu Fan", "Jia Wan", "Tao Han", "Antoni B. Chan", "Andy J. Ma"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      This work has been accepted to ICCV 2025", "url": "http://arxiv.org/abs/2503.10701v2", "summary": "Video Individual Counting (VIC) has received increasing attention for its\nimportance in intelligent video surveillance. Existing works are limited in two\naspects, i.e., dataset and method. Previous datasets are captured with fixed or\nrarely moving cameras with relatively sparse individuals, restricting\nevaluation for a highly varying view and time in crowded scenes. Existing\nmethods rely on localization followed by association or classification, which\nstruggle under dense and dynamic conditions due to inaccurate localization of\nsmall targets. To address these issues, we introduce the MovingDroneCrowd\nDataset, featuring videos captured by fast-moving drones in crowded scenes\nunder diverse illuminations, shooting heights and angles. We further propose a\nShared Density map-guided Network (SDNet) using a Depth-wise Cross-Frame\nAttention (DCFA) module to directly estimate shared density maps between\nconsecutive frames, from which the inflow and outflow density maps are derived\nby subtracting the shared density maps from the global density maps. The inflow\ndensity maps across frames are summed up to obtain the number of unique\npedestrians in a video. Experiments on our datasets and publicly available ones\nshow the superiority of our method over the state of the arts in highly dynamic\nand complex crowded scenes. Our dataset and codes have been released publicly.", "comment": "This work has been accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2503.10701v2", "cate": "cs.CV", "date": "2025-03-12", "updated": "2025-07-14"}
{"id": "2507.09076", "title": "Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence Emotion Recognition in Conversation", "authors": ["Jialong Mai", "Xiaofen Xing", "Yawei Li", "Zhipeng Li", "Jingyuan Xing", "Xiangmin Xu"], "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7; H.5.2"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      submitted to EMNLP 2025", "url": "http://arxiv.org/abs/2507.09076v1", "summary": "Recent research has focused on applying speech large language model (SLLM) to\nimprove speech emotion recognition (SER). However, the inherently high frame\nrate in speech modality severely limits the signal processing and understanding\ncapabilities of SLLM. For example, a SLLM with a 4K context window can only\nprocess 80 seconds of audio at 50Hz feature sampling rate before reaching its\ncapacity limit. Input token compression methods used in SLLM overlook the\ncontinuity and inertia of emotions across multiple conversation turns. This\npaper proposes a Dynamic Parameter Memory (DPM) mechanism with contextual\nsemantics and sentence-level emotion encoding, enabling processing of\nunlimited-length audio with limited context windows in SLLM. Specifically, DPM\nprogressively encodes sentence-level information and emotions into a temporary\nLoRA module during inference to effectively \"memorize\" the contextual\ninformation. We trained an emotion SLLM as a backbone and incorporated our DPM\ninto inference for emotion recognition in conversation (ERC). Experimental\nresults on the IEMOCAP dataset show that DPM significantly improves the emotion\nrecognition capabilities of SLLM when processing long audio sequences,\nachieving state-of-the-art performance.", "comment": "submitted to EMNLP 2025", "pdf_url": "http://arxiv.org/pdf/2507.09076v1", "cate": "cs.CL", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09574", "title": "MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive Vision Generation Models", "authors": ["Haozhe Zhao", "Zefan Cai", "Shuzheng Si", "Liang Chen", "Jiuxiang Gu", "Wen Xiao", "Junjie Hu"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      24 pages,12 figures", "url": "http://arxiv.org/abs/2507.09574v1", "summary": "Recent text-to-image models produce high-quality results but still struggle\nwith precise visual control, balancing multimodal inputs, and requiring\nextensive training for complex multimodal image generation. To address these\nlimitations, we propose MENTOR, a novel autoregressive (AR) framework for\nefficient Multimodal-conditioned Tuning for Autoregressive multimodal image\ngeneration. MENTOR combines an AR image generator with a two-stage training\nparadigm, enabling fine-grained, token-level alignment between multimodal\ninputs and image outputs without relying on auxiliary adapters or\ncross-attention modules. The two-stage training consists of: (1) a multimodal\nalignment stage that establishes robust pixel- and semantic-level alignment,\nfollowed by (2) a multimodal instruction tuning stage that balances the\nintegration of multimodal inputs and enhances generation controllability.\nDespite modest model size, suboptimal base components, and limited training\nresources, MENTOR achieves strong performance on the DreamBench++ benchmark,\noutperforming competitive baselines in concept preservation and prompt\nfollowing. Additionally, our method delivers superior image reconstruction\nfidelity, broad task adaptability, and improved training efficiency compared to\ndiffusion-based methods. Dataset, code, and models are available at:\nhttps://github.com/HaozheZhao/MENTOR", "comment": "24 pages,12 figures", "pdf_url": "http://arxiv.org/pdf/2507.09574v1", "cate": "cs.CV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09969", "title": "Non-parametric Graph Convolution for Re-ranking in Recommendation Systems", "authors": ["Zhongyu Ouyang", "Mingxuan Ju", "Soroush Vosoughi", "Yanfang Ye"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Accepted to RecSys2025 Main", "url": "http://arxiv.org/abs/2507.09969v1", "summary": "Graph knowledge has been proven effective in enhancing item rankings in\nrecommender systems (RecSys), particularly during the retrieval stage. However,\nits application in the ranking stage, especially when richer contextual\ninformation in user-item interactions is available, remains underexplored. A\nmajor challenge lies in the substantial computational cost associated with\nrepeatedly retrieving neighborhood information from billions of items stored in\ndistributed systems. This resource-intensive requirement makes it difficult to\nscale graph-based methods in practical RecSys. To bridge this gap, we first\ndemonstrate that incorporating graphs in the ranking stage improves ranking\nqualities. Notably, while the improvement is evident, we show that the\nsubstantial computational overheads entailed by graphs are prohibitively\nexpensive for real-world recommendations. In light of this, we propose a\nnon-parametric strategy that utilizes graph convolution for re-ranking only\nduring test time. Our strategy circumvents the notorious computational\noverheads from graph convolution during training, and utilizes structural\nknowledge hidden in graphs on-the-fly during testing. It can be used as a\nplug-and-play module and easily employed to enhance the ranking ability of\nvarious ranking layers of a real-world RecSys with significantly reduced\ncomputational overhead. Through comprehensive experiments across four benchmark\ndatasets with varying levels of sparsity, we demonstrate that our strategy\nyields noticeable improvements (i.e., 8.1% on average) during testing time with\nlittle to no additional computational overheads (i.e., 0.5 on average). Code:\nhttps://github.com/zyouyang/RecSys2025_NonParamGC.git", "comment": "Accepted to RecSys2025 Main", "pdf_url": "http://arxiv.org/pdf/2507.09969v1", "cate": "cs.IR", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10005", "title": "Effects of structural properties of neural networks on machine learning performance", "authors": ["Yash Arya", "Sang Hoon Lee"], "categories": ["cs.LG", "cond-mat.stat-mech", "cs.NE", "physics.comp-ph"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      9 pages, 6 figures", "url": "http://arxiv.org/abs/2507.10005v1", "summary": "In recent years, graph-based machine learning techniques, such as\nreinforcement learning and graph neural networks, have garnered significant\nattention. While some recent studies have started to explore the relationship\nbetween the graph structure of neural networks and their predictive\nperformance, they often limit themselves to a narrow range of model networks,\nparticularly lacking mesoscale structures such as communities. Our work\nadvances this area by conducting a more comprehensive investigation,\nincorporating realistic network structures characterized by heterogeneous\ndegree distributions and community structures, which are typical\ncharacteristics of many real networks. These community structures offer a\nnuanced perspective on network architecture. Our analysis employs model\nnetworks such as random and scale-free networks, alongside a comparison with a\nbiological neural network and its subsets for more detailed analysis. We\nexamine the impact of these structural attributes on the performance of image\nclassification tasks. Our findings reveal that structural properties do affect\nperformance to some extent. Specifically, networks featuring coherent, densely\ninterconnected communities demonstrate enhanced learning capabilities. The\ncomparison with the biological neural network emphasizes the relevance of our\nfindings to real-world structures, suggesting an intriguing connection worth\nfurther exploration. This study contributes meaningfully to network science and\nmachine learning, providing insights that could inspire the design of more\nbiologically informed neural networks.", "comment": "9 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.10005v1", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09300", "title": "Finite element modeling of V-notched thermoelastic strain-limiting solids containing inclusions", "authors": ["G. Shylaja", "V. Kesavulu Naidu", "B. Venkatesh", "S. M. Mallikarjunaiah"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09300v1", "summary": "A precise domain triangulation is recognized as indispensable for the\naccurate numerical approximation of differential operators within collocation\nmethods, leading to a substantial reduction in discretization errors. An\nefficient finite element method (FEM) is presented in this paper, meticulously\ndeveloped to solve a complex mathematical model. This model governs the\nbehavior of thermoelastic solids containing both a V-notch and inclusions. The\nsystem of partial differential equations underlying this model consists of two\nprimary components: a linear elliptic equation, which is used to describe the\ntemperature distribution, and a quasilinear equation, which governs the\nmechanical behavior of the body. Through the application of this specifically\ntailored FEM, accurate and efficient solutions are able to be obtained for\nthese intricate thermoelastic problems. The algebraically nonlinear\nconstitutive equation, alongside the balance of linear momentum, is effectively\nreduced to a second-order quasi-linear elliptic partial differential equation.\nComplex curved boundaries are represented through the application of a smooth,\ndistinctive point transformation. Furthermore, higher-order shape functions are\nemployed to ensure the accurate computation of entries within the FEM matrices\nand vectors, from which a highly precise approximate solution to the BVP is\nsubsequently obtained. The inherent nonlinearities in the governing\ndifferential equation are addressed through the implementation of a Picard-type\nlinearization scheme. Numerical results, derived from a series of test cases,\nhave consistently demonstrated a significant enhancement in accuracy, a crucial\nachievement for the nuanced analysis of thermoelastic solids.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09300v1", "cate": "math.NA", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2504.21774", "title": "Is Intermediate Fusion All You Need for UAV-based Collaborative Perception?", "authors": ["Jiuwu Hao", "Liguo Sun", "Yuting Wan", "Yueyang Wu", "Ti Xiang", "Haolin Song", "Pin Lv"], "categories": ["cs.CV", "cs.LG", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ITSC 2025", "url": "http://arxiv.org/abs/2504.21774v2", "summary": "Collaborative perception enhances environmental awareness through inter-agent\ncommunication and is regarded as a promising solution to intelligent\ntransportation systems. However, existing collaborative methods for Unmanned\nAerial Vehicles (UAVs) overlook the unique characteristics of the UAV\nperspective, resulting in substantial communication overhead. To address this\nissue, we propose a novel communication-efficient collaborative perception\nframework based on late-intermediate fusion, dubbed LIF. The core concept is to\nexchange informative and compact detection results and shift the fusion stage\nto the feature representation level. In particular, we leverage vision-guided\npositional embedding (VPE) and box-based virtual augmented feature (BoBEV) to\neffectively integrate complementary information from various agents.\nAdditionally, we innovatively introduce an uncertainty-driven communication\nmechanism that uses uncertainty evaluation to select high-quality and reliable\nshared areas. Experimental results demonstrate that our LIF achieves superior\nperformance with minimal communication bandwidth, proving its effectiveness and\npracticality. Code and models are available at https://github.com/uestchjw/LIF.", "comment": "Accepted by ITSC 2025", "pdf_url": "http://arxiv.org/pdf/2504.21774v2", "cate": "cs.CV", "date": "2025-04-30", "updated": "2025-07-13"}
{"id": "2507.09083", "title": "Learning from Synthetic Labs: Language Models as Auction Participants", "authors": ["Anand Shah", "Kehang Zhu", "Yanchen Jiang", "Jeffrey G. Wang", "Arif K. Dayi", "John J. Horton", "David C. Parkes"], "categories": ["cs.GT", "cs.AI"], "primary_category": "Subjects:       Computer Science and Game Theory (cs.GT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09083v1", "summary": "This paper investigates the behavior of simulated AI agents (large language\nmodels, or LLMs) in auctions, introducing a novel synthetic data-generating\nprocess to help facilitate the study and design of auctions. We find that LLMs\n-- when endowed with chain of thought reasoning capacity -- agree with the\nexperimental literature in auctions across a variety of classic auction\nformats. In particular, we find that LLM bidders produce results consistent\nwith risk-averse human bidders; that they perform closer to theoretical\npredictions in obviously strategy-proof auctions; and, that they succumb to the\nwinner's curse in common value settings. On prompting, we find that LLMs are\nnot very sensitive to naive changes in prompts (e.g., language, currency) but\ncan improve dramatically towards theoretical predictions with the right mental\nmodel (i.e., the language of Nash deviations). We run 1,000$+$ auctions for\nless than $\\$$400 with GPT-4 models (three orders of magnitude cheaper than\nmodern auction experiments) and develop a framework flexible enough to run\nauction experiments with any LLM model and a wide range of auction design\nspecifications, facilitating further experimental study by decreasing costs and\nserving as a proof-of-concept for the use of LLM proxies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09083v1", "cate": "cs.GT", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09577", "title": "Memory-Augmented SAM2 for Training-Free Surgical Video Segmentation", "authors": ["Ming Yin", "Fu Wang", "Xujiong Ye", "Yanda Meng", "Zeyu Fu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09577v1", "summary": "Surgical video segmentation is a critical task in computer-assisted surgery,\nessential for enhancing surgical quality and patient outcomes. Recently, the\nSegment Anything Model 2 (SAM2) framework has demonstrated remarkable\nadvancements in both image and video segmentation. However, the inherent\nlimitations of SAM2's greedy selection memory design are amplified by the\nunique properties of surgical videos-rapid instrument movement, frequent\nocclusion, and complex instrument-tissue interaction-resulting in diminished\nperformance in the segmentation of complex, long videos. To address these\nchallenges, we introduce Memory Augmented (MA)-SAM2, a training-free video\nobject segmentation strategy, featuring novel context-aware and\nocclusion-resilient memory models. MA-SAM2 exhibits strong robustness against\nocclusions and interactions arising from complex instrument movements while\nmaintaining accuracy in segmenting objects throughout videos. Employing a\nmulti-target, single-loop, one-prompt inference further enhances the efficiency\nof the tracking process in multi-instrument videos. Without introducing any\nadditional parameters or requiring further training, MA-SAM2 achieved\nperformance improvements of 4.36% and 6.1% over SAM2 on the EndoVis2017 and\nEndoVis2018 datasets, respectively, demonstrating its potential for practical\nsurgical applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09577v1", "cate": "cs.CV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09998", "title": "SLIF-MR: Self-loop Iterative Fusion of Heterogeneous Auxiliary Information for Multimodal Recommendation", "authors": ["Jie Guo", "Jiahao Jiang", "Ziyuan Guo", "Bin Song", "Yue Sun"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      10 pages,7 figures", "url": "http://arxiv.org/abs/2507.09998v1", "summary": "Knowledge graphs (KGs) and multimodal item information, which respectively\ncapture relational and attribute features, play a crucial role in improving\nrecommender system accuracy. Recent studies have attempted to integrate them\nvia multimodal knowledge graphs (MKGs) to further enhance recommendation\nperformance. However, existing methods typically freeze the MKG structure\nduring training, which limits the full integration of structural information\nfrom heterogeneous graphs (e.g., KG and user-item interaction graph), and\nresults in sub-optimal performance. To address this challenge, we propose a\nnovel framework, termed Self-loop Iterative Fusion of Heterogeneous Auxiliary\nInformation for Multimodal Recommendation (SLIF-MR), which leverages item\nrepresentations from previous training epoch as feedback signals to dynamically\noptimize the heterogeneous graph structures composed of KG, multimodal item\nfeature graph, and user-item interaction graph. Through this iterative fusion\nmechanism, both user and item representations are refined, thus improving the\nfinal recommendation performance. Specifically, based on the feedback item\nrepresentations, SLIF-MR constructs an item-item correlation graph, then\nintegrated into the establishment process of heterogeneous graphs as additional\nnew structural information in a self-loop manner. Consequently, the internal\nstructures of heterogeneous graphs are updated with the feedback item\nrepresentations during training. Moreover, a semantic consistency learning\nstrategy is proposed to align heterogeneous item representations across\nmodalities. The experimental results show that SLIF-MR significantly\noutperforms existing methods, particularly in terms of accuracy and robustness.", "comment": "10 pages,7 figures", "pdf_url": "http://arxiv.org/pdf/2507.09998v1", "cate": "cs.IR", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10383", "title": "Dynamical stability for dense patterns in discrete attractor neural networks", "authors": ["Uri Cohen", "Máté Lengyel"], "categories": ["cond-mat.dis-nn", "cond-mat.stat-mech", "cs.LG", "cs.NE", "q-bio.NC"], "primary_category": "Subjects:       Disordered Systems and Neural Networks (cond-mat.dis-nn)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10383v1", "summary": "Neural networks storing multiple discrete attractors are canonical models of\nbiological memory. Previously, the dynamical stability of such networks could\nonly be guaranteed under highly restrictive conditions. Here, we derive a\ntheory of the local stability of discrete fixed points in a broad class of\nnetworks with graded neural activities and in the presence of noise. By\ndirectly analyzing the bulk and outliers of the Jacobian spectrum, we show that\nall fixed points are stable below a critical load that is distinct from the\nclassical \\textit{critical capacity} and depends on the statistics of neural\nactivities in the fixed points as well as the single-neuron activation\nfunction. Our analysis highlights the computational benefits of\nthreshold-linear activation and sparse-like patterns.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10383v1", "cate": "cond-mat.dis-nn", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09337", "title": "ORCHA -- A Performance Portability System for Post-Exascale Systems", "authors": ["Youngjun Lee", "Klaus Weide", "Wesley Kwiecinski", "Jared O'Neal", "Johann Rudi", "Anshu Dubey"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09337v1", "summary": "Heterogeneity is the prevalent trend in the rapidly evolving high-performance\ncomputing (HPC) landscape in both hardware and application software. The\ndiversity in hardware platforms, currently comprising various accelerators and\na future possibility of specializable chiplets, poses a significant challenge\nfor scientific software developers aiming to harness optimal performance across\ndifferent computing platforms while maintaining the quality of solutions when\ntheir applications are simultaneously growing more complex. Code synthesis and\ncode generation can provide mechanisms to mitigate this challenge. We have\ndeveloped a toolchain, ORCHA, which arises from the needs of a large\nmultiphysics simulation software, Flash-X, which were not met by any of the\nexisting solutions. ORCHA is composed of three stand-alone tools -- one to\nexpress high-level control flow and a map of what to execute where on the\nplatform, a second one to express variants of data structures and arithmetic\noperations in the solvers in a unified fashion, and a third one that manages\nthe runtime orchestration of the data and computation. We use an\napplication-specific interface layer that uses code generation and code\nsynthesis to stitch together the application. In this paper, we describe the\ninterface layer for the application Flash-X and demonstrate the use of ORCHA in\nexploring possible configurations from which the optimal one can be selected\nfor production, including a case study in which a single simulation recipe is\nrealized on three distinct hardware mappings -- a GPU-centric, a CPU/GPU\nbalanced, and a CPU/GPU concurrent layouts -- highlighting the breadth of\nconfigurations ORCHA enables.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09337v1", "cate": "math.NA", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.08882", "title": "Less Stress, More Privacy: Stress Detection on Anonymized Speech of Air Traffic Controllers", "authors": ["Janaki Viswanathan", "Alexander Blatt", "Konrad Hagemann", "Dietrich Klakow"], "categories": ["cs.SD", "cs.CL", "eess.AS", "I.2.7; I.5.5"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      8 pages, 2 figures, 4 tables, publication identification number (URN)- urn:nbn:de:101:1-2022122008393409239462, see archived online publication- this https URL & Katalogeintrag: this https URL", "url": "http://arxiv.org/abs/2507.08882v1", "summary": "Air traffic control (ATC) demands multi-tasking under time pressure with high\nconsequences of an error. This can induce stress. Detecting stress is a key\npoint in maintaining the high safety standards of ATC. However, processing ATC\nvoice data entails privacy restrictions, e.g. the General Data Protection\nRegulation (GDPR) law. Anonymizing the ATC voice data is one way to comply with\nthese restrictions. In this paper, different architectures for stress detection\nfor anonymized ATCO speech are evaluated. Our best networks reach a stress\ndetection accuracy of 93.6% on an anonymized version of the Speech Under\nSimulated and Actual Stress (SUSAS) dataset and an accuracy of 80.1% on our\nanonymized ATC simulation dataset. This shows that privacy does not have to be\nan impediment in building well-performing deep-learning-based models.", "comment": "8 pages, 2 figures, 4 tables, publication identification number\n  (URN)- urn:nbn:de:101:1-2022122008393409239462, see archived online\n  publication- https://d-nb.info/127614606X/34 & Katalogeintrag:\n  https://d-nb.info/127614606X/", "pdf_url": "http://arxiv.org/pdf/2507.08882v1", "cate": "cs.SD", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.02948", "title": "DriveMRP: Enhancing Vision-Language Models with Synthetic Motion Data for Motion Risk Prediction", "authors": ["Zhiyi Hou", "Enhui Ma", "Fang Li", "Zhiyi Lai", "Kalok Ho", "Zhanqian Wu", "Lijun Zhou", "Long Chen", "Chitian Sun", "Haiyang Sun", "Bing Wang", "Guang Chen", "Hangjun Ye", "Kaicheng Yu"], "categories": ["cs.CV", "cs.AI", "cs.RO", "I.4.8; I.2.7; I.2.10"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      12 pages, 4 figures. Code available at this https URL", "url": "http://arxiv.org/abs/2507.02948v3", "summary": "Autonomous driving has seen significant progress, driven by extensive\nreal-world data. However, in long-tail scenarios, accurately predicting the\nsafety of the ego vehicle's future motion remains a major challenge due to\nuncertainties in dynamic environments and limitations in data coverage. In this\nwork, we aim to explore whether it is possible to enhance the motion risk\nprediction capabilities of Vision-Language Models (VLM) by synthesizing\nhigh-risk motion data. Specifically, we introduce a Bird's-Eye View (BEV) based\nmotion simulation method to model risks from three aspects: the ego-vehicle,\nother vehicles, and the environment. This allows us to synthesize\nplug-and-play, high-risk motion data suitable for VLM training, which we call\nDriveMRP-10K. Furthermore, we design a VLM-agnostic motion risk estimation\nframework, named DriveMRP-Agent. This framework incorporates a novel\ninformation injection strategy for global context, ego-vehicle perspective, and\ntrajectory projection, enabling VLMs to effectively reason about the spatial\nrelationships between motion waypoints and the environment. Extensive\nexperiments demonstrate that by fine-tuning with DriveMRP-10K, our\nDriveMRP-Agent framework can significantly improve the motion risk prediction\nperformance of multiple VLM baselines, with the accident recognition accuracy\nsoaring from 27.13% to 88.03%. Moreover, when tested via zero-shot evaluation\non an in-house real-world high-risk motion dataset, DriveMRP-Agent achieves a\nsignificant performance leap, boosting the accuracy from base_model's 29.42% to\n68.50%, which showcases the strong generalization capabilities of our method in\nreal-world scenarios.", "comment": "12 pages, 4 figures. Code available at\n  https://github.com/hzy138/DriveMRP", "pdf_url": "http://arxiv.org/pdf/2507.02948v3", "cate": "cs.CV", "date": "2025-06-28", "updated": "2025-07-13"}
{"id": "2507.09084", "title": "Queue up for takeoff: a transferable deep learning framework for flight delay prediction", "authors": ["Nnamdi Daniel Aghanya", "Ta Duong Vu", "Amaëlle Diop", "Charlotte Deville", "Nour Imane Kerroumi", "Irene Moulitsas", "Jun Li", "Desmond Bisandu"], "categories": ["cs.LG", "cs.AI", "68T07, 90B22, 62M10", "I.2.m"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      3 figures, 20 pages references and appendix included,", "url": "http://arxiv.org/abs/2507.09084v1", "summary": "Flight delays are a significant challenge in the aviation industry, causing\nmajor financial and operational disruptions. To improve passenger experience\nand reduce revenue loss, flight delay prediction models must be both precise\nand generalizable across different networks. This paper introduces a novel\napproach that combines Queue-Theory with a simple attention model, referred to\nas the Queue-Theory SimAM (QT-SimAM). To validate our model, we used data from\nthe US Bureau of Transportation Statistics, where our proposed QT-SimAM\n(Bidirectional) model outperformed existing methods with an accuracy of 0.927\nand an F1 score of 0.932. To assess transferability, we tested the model on the\nEUROCONTROL dataset. The results demonstrated strong performance, achieving an\naccuracy of 0.826 and an F1 score of 0.791. Ultimately, this paper outlines an\neffective, end-to-end methodology for predicting flight delays. The proposed\nmodel's ability to forecast delays with high accuracy across different networks\ncan help reduce passenger anxiety and improve operational decision-making", "comment": "3 figures, 20 pages references and appendix included,", "pdf_url": "http://arxiv.org/pdf/2507.09084v1", "cate": "cs.LG", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09595", "title": "Demystifying Flux Architecture", "authors": ["Or Greenberg"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09595v1", "summary": "FLUX.1 is a diffusion-based text-to-image generation model developed by Black\nForest Labs, designed to achieve faithful text-image alignment while\nmaintaining high image quality and diversity. FLUX is considered\nstate-of-the-art in text-to-image generation, outperforming popular models such\nas Midjourney, DALL-E 3, Stable Diffusion 3 (SD3), and SDXL. Although publicly\navailable as open source, the authors have not released official technical\ndocumentation detailing the model's architecture or training setup. This report\nsummarizes an extensive reverse-engineering effort aimed at demystifying FLUX's\narchitecture directly from its source code, to support its adoption as a\nbackbone for future research and development. This document is an unofficial\ntechnical report and is not published or endorsed by the original developers or\ntheir affiliated institutions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09595v1", "cate": "cs.CV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.10057", "title": "PRISM: Fine-Grained Paper-to-Paper Retrieval with Multi-Aspect-Aware Query Optimization", "authors": ["Sangwoo Park", "Jinheon Baek", "Soyeong Jeong", "Sung Ju Hwang"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10057v1", "summary": "Scientific paper retrieval, particularly framed as document-to-document\nretrieval, aims to identify relevant papers in response to a long-form query\npaper, rather than a short query string. Previous approaches to this task have\nfocused on abstracts, embedding them into dense vectors as surrogates for full\ndocuments and calculating similarity across them, although abstracts provide\nonly sparse and high-level summaries. To address this, we propose PRISM, a\nnovel document-to-document retrieval method that introduces multiple,\nfine-grained representations for both the query and candidate papers. In\nparticular, each query paper is decomposed into multiple aspect-specific views\nand individually embedded, which are then matched against candidate papers\nsimilarity segmented to consider their multifaceted dimensions. Moreover, we\npresent SciFullBench, a novel benchmark in which the complete and segmented\ncontext of full papers for both queries and candidates is available. Then,\nexperimental results show that PRISM improves performance by an average of 4.3%\nover existing retrieval baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10057v1", "cate": "cs.IR", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2311.10502", "title": "Fast Estimations of Hitting Time of Elitist Evolutionary Algorithms from Fitness Levels", "authors": ["Jun He", "Siang Yew Chong", "Xin Yao"], "categories": ["cs.NE"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2311.10502v3", "summary": "The fitness level method is a widely used technique for estimating the mean\nhitting time of elitist evolutionary algorithms on level-based fitness\nfunctions. However, this paper identifies its main limitation: the linear lower\nbound derived from traditional fitness level partitioning is not tight when\napplied to many non-level-based fitness functions. A new subset level method is\nintroduced to address this limitation. It selects a subset of non-optimal\nsolutions, partitions them into levels, and then estimates linear bound\ncoefficients based on drift analysis. Explicit expressions are proposed to\ncompute the lower bound on the mean hitting time of elitist evolutionary\nalgorithms. The proposed method is validated using six instances of the\nknapsack problem. Results show that the new method can be used to quickly\nestimate the lower bound on the mean hitting time of elitist evolutionary\nalgorithms. This expands the application scope of the fitness level method to\nnon-level-based functions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2311.10502v3", "cate": "cs.NE", "date": "2023-11-17", "updated": "2025-07-12"}
{"id": "2507.09401", "title": "A discontinuous Galerkin method for one-dimensional nonlocal wave problems", "authors": ["Qiang Du", "Kui Ren", "Lu Zhang", "Yin Zhou"], "categories": ["math.NA", "cs.NA", "45A05, 65M12, 65M60, 65R20"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09401v1", "summary": "This paper presents a fully discrete numerical scheme for one-dimensional\nnonlocal wave equations and provides a rigorous theoretical analysis. To\nfacilitate the spatial discretization, we introduce an auxiliary variable\nanalogous to the gradient field in local discontinuous Galerkin (DG) methods\nfor classical partial differential equations (PDEs) and reformulate the\nequation into a system of equations. The proposed scheme then uses a DG method\nfor spatial discretization and the Crank-Nicolson method for time integration.\nWe prove optimal L2 error convergence for both the solution and the auxiliary\nvariable under a special class of radial kernels at the semi-discrete level. In\naddition, for general kernels, we demonstrate the asymptotic compatibility of\nthe scheme, ensuring that it recovers the classical DG approximation of the\nlocal wave equation in the zero-horizon limit. Furthermore, we prove that the\nfully discrete scheme preserves the energy of the nonlocal wave equation. A\nseries of numerical experiments are presented to validate the theoretical\nfindings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09401v1", "cate": "math.NA", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09116", "title": "Mixture of LoRA Experts with Multi-Modal and Multi-Granularity LLM Generative Error Correction for Accented Speech Recognition", "authors": ["Bingshen Mu", "Kun Wei", "Pengcheng Guo", "Lei Xie"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      IEEE Transactions on Audio, Speech and Language Processing", "url": "http://arxiv.org/abs/2507.09116v1", "summary": "Despite substantial improvements in ASR, performance tends to degrade when\nfaced with adverse conditions such as speaker accents. Generative error\ncorrection (GER) leverages the rich linguistic knowledge and exceptional\nreasoning ability of LLMs, significantly outperforming typical LM methods.\nHowever, it lacks specificity in accented speech scenarios. In this study, we\nleverage GER to improve the accuracy of transcription predictions by addressing\nthe two primary features of accented speech recognition. To fully leverage\npronunciation information, we propose the multi-modal GER, which integrates\npronunciation information from the speech modality, and the multi-granularity\nGER, which incorporates fine-grained phoneme-level information related to\npronunciation. These two methods enable the LLM to utilize the pronunciation\ninformation of accented speech and the semantic information from word-level\nhypotheses for accurate transcription predictions through LoRA fine-tuning. On\nthe one hand, we employ a three-stage training strategy to train separate\nmulti-modal GER models for each accent to obtain mono-accent LoRA experts. By\nadopting our proposed HDMoLE method, which incorporates hierarchical routing\nand dynamic thresholds within the mixture of LoRA experts, we effectively merge\nmultiple mono-accent LoRA experts within a single multi-modal GER to overcome\nthe challenges posed by accent diversity. On the other hand, multi-granularity\nGER leverages the N-best word-level and phoneme-level hypotheses generated by\nthe HDMoLE model to predict the final accented speech transcriptions.\nExperimental results on the multi-accent English dataset demonstrate the\nefficacy of our proposed methods. Our methods achieve a remarkable relative WER\nreduction of 67.35% compared to the Whisper-large-v3 baseline.", "comment": "IEEE Transactions on Audio, Speech and Language Processing", "pdf_url": "http://arxiv.org/pdf/2507.09116v1", "cate": "cs.SD", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09087", "title": "Deep Reinforcement Learning with Gradient Eligibility Traces", "authors": ["Esraa Elelimy", "Brett Daley", "Andrew Patterson", "Marlos C. Machado", "Adam White", "Martha White"], "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09087v1", "summary": "Achieving fast and stable off-policy learning in deep reinforcement learning\n(RL) is challenging. Most existing methods rely on semi-gradient\ntemporal-difference (TD) methods for their simplicity and efficiency, but are\nconsequently susceptible to divergence. While more principled approaches like\nGradient TD (GTD) methods have strong convergence guarantees, they have rarely\nbeen used in deep RL. Recent work introduced the Generalized Projected Bellman\nError ($\\GPBE$), enabling GTD methods to work efficiently with nonlinear\nfunction approximation. However, this work is only limited to one-step methods,\nwhich are slow at credit assignment and require a large number of samples. In\nthis paper, we extend the $\\GPBE$ objective to support multistep credit\nassignment based on the $\\lambda$-return and derive three gradient-based\nmethods that optimize this new objective. We provide both a forward-view\nformulation compatible with experience replay and a backward-view formulation\ncompatible with streaming algorithms. Finally, we evaluate the proposed\nalgorithms and show that they outperform both PPO and StreamQ in MuJoCo and\nMinAtar environments, respectively. Code available at\nhttps://github.com/esraaelelimy/gtd\\_algos", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09087v1", "cate": "cs.LG", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09612", "title": "Inter2Former: Dynamic Hybrid Attention for Efficient High-Precision Interactive", "authors": ["You Huang", "Lichao Chen", "Jiayi Ji", "Liujuan Cao", "Shengchuan Zhang", "Rongrong Ji"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2507.09612v1", "summary": "Interactive segmentation (IS) improves annotation efficiency by segmenting\ntarget regions from user prompts, with widespread applications in real-world\nscenarios. Current approaches face a critical trade-off: dense-token methods\nachieve superior accuracy and detail preservation but suffer from prohibitively\nslow processing on CPU devices, while the Segment Anything Model (SAM) advances\nthe field with sparse prompt tokens for fast inference but compromises\nsegmentation quality. In this paper, we propose Inter2Former to address this\nchallenge by optimizing computation allocation in dense-token processing, which\nintroduces four key enhancements. First, we propose Dynamic Prompt Embedding\n(DPE) that adaptively processes only regions of interest while avoiding\nadditional overhead from background tokens. Second, we introduce Dynamic Hybrid\nAttention (DHA), which leverages previous segmentation masks to route tokens\nthrough either full attention (O(N2)) for boundary regions or our proposed\nefficient BSQ attention (O(N)) for non-boundary regions. Third, we develop\nHybrid Mixture of Experts (HMoE), which applies similar adaptive computation\nstrategies in FFN modules with CPU-optimized parallel processing. Finally, we\npresent Dynamic Local Upsampling (DLU), a reverse operation of DPE, which\nlocalizes objects with a lightweight MLP and performs fine-grained upsampling\nonly in detected regions. Experimental results on high-precision IS benchmarks\ndemonstrate that Inter2Former achieves SOTA performance with high efficiency on\nCPU devices.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.09612v1", "cate": "cs.CV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.10097", "title": "User Long-Term Multi-Interest Retrieval Model for Recommendation", "authors": ["Yue Meng", "Cheng Guo", "Xiaohui Hu", "Honghu Deng", "Yi Cao", "Tong Liu", "Bo Zheng"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10097v1", "summary": "User behavior sequence modeling, which captures user interest from rich\nhistorical interactions, is pivotal for industrial recommendation systems.\nDespite breakthroughs in ranking-stage models capable of leveraging ultra-long\nbehavior sequences with length scaling up to thousands, existing retrieval\nmodels remain constrained to sequences of hundreds of behaviors due to two main\nchallenges. One is strict latency budget imposed by real-time service over\nlarge-scale candidate pool. The other is the absence of target-aware mechanisms\nand cross-interaction architectures, which prevent utilizing ranking-like\ntechniques to simplify long sequence modeling. To address these limitations, we\npropose a new framework named User Long-term Multi-Interest Retrieval\nModel(ULIM), which enables thousand-scale behavior modeling in retrieval\nstages. ULIM includes two novel components: 1)Category-Aware Hierarchical\nDual-Interest Learning partitions long behavior sequences into multiple\ncategory-aware subsequences representing multi-interest and jointly optimizes\nlong-term and short-term interests within specific interest cluster.\n2)Pointer-Enhanced Cascaded Category-to-Item Retrieval introduces\nPointer-Generator Interest Network(PGIN) for next-category prediction, followed\nby next-item retrieval upon the top-K predicted categories. Comprehensive\nexperiments on Taobao dataset show that ULIM achieves substantial improvement\nover state-of-the-art methods, and brings 5.54% clicks, 11.01% orders and 4.03%\nGMV lift for Taobaomiaosha, a notable mini-app of Taobao.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10097v1", "cate": "cs.IR", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2401.05373", "title": "Dynamic Spiking Framework for Graph Neural Networks", "authors": ["Nan Yin", "Mengzhu Wang", "Zhenghan Chen", "Giulia De Masi", "Bin Gu", "Huan Xiong"], "categories": ["cs.NE", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2401.05373v4", "summary": "The integration of Spiking Neural Networks (SNNs) and Graph Neural Networks\n(GNNs) is gradually attracting attention due to the low power consumption and\nhigh efficiency in processing the non-Euclidean data represented by graphs.\nHowever, as a common problem, dynamic graph representation learning faces\nchallenges such as high complexity and large memory overheads. Current work\noften uses SNNs instead of Recurrent Neural Networks (RNNs) by using binary\nfeatures instead of continuous ones for efficient training, which would\noverlooks graph structure information and leads to the loss of details during\npropagation. Additionally, optimizing dynamic spiking models typically requires\npropagation of information across time steps, which increases memory\nrequirements. To address these challenges, we present a framework named\n\\underline{Dy}namic \\underline{S}p\\underline{i}king \\underline{G}raph\n\\underline{N}eural Networks (\\method{}). To mitigate the information loss\nproblem, \\method{} propagates early-layer information directly to the last\nlayer for information compensation. To accommodate the memory requirements, we\napply the implicit differentiation on the equilibrium state, which does not\nrely on the exact reverse of the forward computation. While traditional\nimplicit differentiation methods are usually used for static situations,\n\\method{} extends it to the dynamic graph setting. Extensive experiments on\nthree large-scale real-world dynamic graph datasets validate the effectiveness\nof \\method{} on dynamic node classification tasks with lower computational\ncosts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2401.05373v4", "cate": "cs.NE", "date": "2023-12-15", "updated": "2025-07-12"}
{"id": "2507.09475", "title": "A modified tamed scheme for stochastic differential equations with superlinear drifts", "authors": ["Zichang Ju", "Lei Li", "Yuliang Wang"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09475v1", "summary": "Explicit discretizations of stochastic differential equations often encounter\ninstability when the coefficients are not globally Lipschitz. The truncated\nschemes and tamed schemes have been proposed to handle this difficulty, but\ntruncated schemes involve analyzing of the stopping times while the tamed\nschemes suffer from the reduced order of accuracy. We propose a modified tamed\nscheme by introducing an additional cut-off function in the taming, which\nenjoys the convenience for error analysis and preserving the original order of\nexplicit discretization. While the strategy could be applied to any explicit\ndiscretization, we perform rigorous analysis of the modified tamed scheme for\nthe Euler discretization as an example. Then, we apply the modified tamed\nscheme to the stochastic gradient Langevin dynamics for sampling with\nsuper-linear drift, and obtain a uniform-in-time near-sharp error estimate\nunder relative entropy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09475v1", "cate": "math.NA", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09195", "title": "Towards Spatial Audio Understanding via Question Answering", "authors": ["Parthasaarathy Sudarsanam", "Archontis Politis"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09195v1", "summary": "In this paper, we introduce a novel framework for spatial audio understanding\nof first-order ambisonic (FOA) signals through a question answering (QA)\nparadigm, aiming to extend the scope of sound event localization and detection\n(SELD) towards spatial scene understanding and reasoning. First, we curate and\nrelease fine-grained spatio-temporal textual descriptions for the STARSS23\ndataset using a rule-based approach, and further enhance linguistic diversity\nusing large language model (LLM)-based rephrasing. We also introduce a QA\ndataset aligned with the STARSS23 scenes, covering various aspects such as\nevent presence, localization, spatial, and temporal relationships. To increase\nlanguage variety, we again leverage LLMs to generate multiple rephrasings per\nquestion. Finally, we develop a baseline spatial audio QA model that takes FOA\nsignals and natural language questions as input and provides answers regarding\nvarious occurrences, temporal, and spatial relationships of sound events in the\nscene formulated as a classification task. Despite being trained solely with\nscene-level question answering supervision, our model achieves performance that\nis comparable to a fully supervised sound event localization and detection\nmodel trained with frame-level spatiotemporal annotations. The results\nhighlight the potential of language-guided approaches for spatial audio\nunderstanding and open new directions for integrating linguistic supervision\ninto spatial scene analysis.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09195v1", "cate": "cs.SD", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09104", "title": "CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards", "authors": ["Taolin Zhang", "Maosong Cao", "Alexander Lam", "Songyang Zhang", "Kai Chen"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09104v1", "summary": "Recently, the role of LLM-as-judge in evaluating large language models has\ngained prominence. However, current judge models suffer from narrow\nspecialization and limited robustness, undermining their capacity for\ncomprehensive evaluations. In this work, we present CompassJudger-2, a novel\ngeneralist judge model that overcomes these limitations via a task-driven,\nmulti-domain data curation strategy. Central to our approach is supervising\njudgment tasks with verifiable rewards, guiding intrinsic critical reasoning\nthrough rejection sampling to foster robust, generalizable judgment\ncapabilities. We introduce a refined learning objective with margin policy\ngradient loss to enhance performance. Empirically, CompassJudger-2 achieves\nsuperior results across multiple judge and reward benchmarks, and our 7B model\ndemonstrates competitive judgment accuracy with significantly larger models\nlike DeepSeek-V3 and Qwen3-235B-A22B. Additionally, we propose JudgerBenchV2, a\ncomprehensive benchmark evaluating cross-domain judgment accuracy and rank\nconsistency to standardize judge model evaluation. These contributions advance\nrobust, scalable LLM judgment and establish new performance and evaluation\nstandards.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09104v1", "cate": "cs.CL", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09615", "title": "Towards Fine-Grained Adaptation of CLIP via a Self-Trained Alignment Score", "authors": ["Eman Ali", "Sathira Silva", "Chetan Arora", "Muhammad Haris Khan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09615v1", "summary": "Vision-language models (VLMs) like CLIP excel in zero-shot learning by\naligning image and text representations through contrastive pretraining.\nExisting approaches to unsupervised adaptation (UA) for fine-grained\nclassification with VLMs either rely on fixed alignment scores that cannot\ncapture evolving, subtle class distinctions or use computationally expensive\npseudo-labeling strategies that limit scalability. In contrast, we show that\nmodeling fine-grained cross-modal interactions during adaptation produces more\naccurate, class-discriminative pseudo-labels and substantially improves\nperformance over state-of-the-art (SOTA) methods. We introduce Fine-grained\nAlignment and Interaction Refinement (FAIR), an innovative approach that\ndynamically aligns localized image features with descriptive language\nembeddings through a set of Class Description Anchors (CDA). This enables the\ndefinition of a Learned Alignment Score (LAS), which incorporates CDA as an\nadaptive classifier, facilitating cross-modal interactions to improve\nself-training in unsupervised adaptation. Furthermore, we propose a\nself-training weighting mechanism designed to refine pseudo-labels in the\npresence of inter-class ambiguities. Our approach, FAIR, delivers a substantial\nperformance boost in fine-grained unsupervised adaptation, achieving a notable\noverall gain of 2.78% across 13 fine-grained datasets compared to SOTA methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09615v1", "cate": "cs.CV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.10411", "title": "Am I on the Right Track? What Can Predicted Query Performance Tell Us about the Search Behaviour of Agentic RAG", "authors": ["Fangzheng Tian", "Jinyuan Fang", "Debasis Ganguly", "Zaiqiao Meng", "Craig Macdonald"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10411v1", "summary": "Agentic Retrieval-Augmented Generation (RAG) is a new paradigm where the\nreasoning model decides when to invoke a retriever (as a \"tool\") when answering\na question. This paradigm, exemplified by recent research works such as\nSearch-R1, enables the model to decide when to search and obtain external\ninformation. However, the queries generated by such Agentic RAG models and the\nrole of the retriever in obtaining high-quality answers remain understudied. To\nthis end, this initial study examines the applicability of query performance\nprediction (QPP) within the recent Agentic RAG models Search-R1 and\nR1-Searcher. We find that applying effective retrievers can achieve higher\nanswer quality within a shorter reasoning process. Moreover, the QPP estimates\nof the generated queries, used as an approximation of their retrieval quality,\nare positively correlated with the quality of the final answer. Ultimately, our\nwork is a step towards adaptive retrieval within Agentic RAG, where QPP is used\nto inform the model if the retrieved results are likely to be useful.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10411v1", "cate": "cs.IR", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2404.01897", "title": "Continuous Spiking Graph Neural Networks", "authors": ["Nan Yin", "Mengzhu Wan", "Li Shen", "Hitesh Laxmichand Patel", "Baopu Li", "Bin Gu", "Huan Xiong"], "categories": ["cs.NE", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2404.01897v2", "summary": "Continuous graph neural networks (CGNNs) have garnered significant attention\ndue to their ability to generalize existing discrete graph neural networks\n(GNNs) by introducing continuous dynamics. They typically draw inspiration from\ndiffusion-based methods to introduce a novel propagation scheme, which is\nanalyzed using ordinary differential equations (ODE). However, the\nimplementation of CGNNs requires significant computational power, making them\nchallenging to deploy on battery-powered devices. Inspired by recent spiking\nneural networks (SNNs), which emulate a biological inference process and\nprovide an energy-efficient neural architecture, we incorporate the SNNs with\nCGNNs in a unified framework, named Continuous Spiking Graph Neural Networks\n(COS-GNN). We employ SNNs for graph node representation at each time step,\nwhich are further integrated into the ODE process along with time. To enhance\ninformation preservation and mitigate information loss in SNNs, we introduce\nthe high-order structure of COS-GNN, which utilizes the second-order ODE for\nspiking representation and continuous propagation. Moreover, we provide the\ntheoretical proof that COS-GNN effectively mitigates the issues of exploding\nand vanishing gradients, enabling us to capture long-range dependencies between\nnodes. Experimental results on graph-based learning tasks demonstrate the\neffectiveness of the proposed COS-GNN over competitive baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2404.01897v2", "cate": "cs.NE", "date": "2024-04-02", "updated": "2025-07-12"}
{"id": "2507.09651", "title": "Bayesian dictionary learning estimation of cell membrane permeability from surface pH data", "authors": ["Alberto Bocchinfuso", "Daniela Calvetti", "Erkki Somersalo"], "categories": ["math.NA", "cs.NA", "q-bio.QM"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09651v1", "summary": "Gas transport across cell membrane is a very important process in\nbiochemistry which is essential for many crucial tasks, including cell\nrespiration pH regulation in the cell. In the late 1990's, the suggestion that\ngasses are transported via preferred gas channels embedded into the cell\nmembrane challenged the century old Overton's theory that gases pass through\nthe lipid cell membrane by diffusing across the concentration gradient. Since\nexperimental evidence alone does not provide enough evidence to favor one of\nthe proposed mechanisms, mathematical models have been introduced to provide a\ncontext for the interpretation of laboratory measurement. Following up on\nprevious work where the membrane permeability was estimated using particle\nfilter, in this article we propose an algorithm based on dictionary learning\nfor estimating cell membrane permeability. Computed examples illustrate that\nthe novel approach, which can be applied when the properties of the membrane do\nnot change in the course of the data collection process, is computationally\nmuch more efficient than particle filter.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09651v1", "cate": "math.NA", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09310", "title": "Voice Conversion for Lombard Speaking Style with Implicit and Explicit Acoustic Feature Conditioning", "authors": ["Dominika Woszczyk", "Manuel Sam Ribeiro", "Thomas Merritt", "Daniel Korzekwa"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Presented at Clarity Challenge 2023", "url": "http://arxiv.org/abs/2507.09310v1", "summary": "Text-to-Speech (TTS) systems in Lombard speaking style can improve the\noverall intelligibility of speech, useful for hearing loss and noisy\nconditions. However, training those models requires a large amount of data and\nthe Lombard effect is challenging to record due to speaker and noise\nvariability and tiring recording conditions. Voice conversion (VC) has been\nshown to be a useful augmentation technique to train TTS systems in the absence\nof recorded data from the target speaker in the target speaking style. In this\npaper, we are concerned with Lombard speaking style transfer. Our goal is to\nconvert speaker identity while preserving the acoustic attributes that define\nthe Lombard speaking style. We compare voice conversion models with implicit\nand explicit acoustic feature conditioning. We observe that our proposed\nimplicit conditioning strategy achieves an intelligibility gain comparable to\nthe model conditioned on explicit acoustic features, while also preserving\nspeaker similarity.", "comment": "Presented at Clarity Challenge 2023", "pdf_url": "http://arxiv.org/pdf/2507.09310v1", "cate": "cs.SD", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09132", "title": "Heterogeneous Graph Prompt Learning via Adaptive Weight Pruning", "authors": ["Chu-Yuan Wei", "Shun-Yao Liu", "Sheng-Da Zhuo", "Chang-Dong Wang", "Shu-Qiang Huang", "Mohsen Guizani"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09132v1", "summary": "Graph Neural Networks (GNNs) have achieved remarkable success in various\ngraph-based tasks (e.g., node classification or link prediction). Despite their\ntriumphs, GNNs still face challenges such as long training and inference times,\ndifficulty in capturing complex relationships, and insufficient feature\nextraction. To tackle these issues, graph pre-training and graph prompt methods\nhave garnered increasing attention for their ability to leverage large-scale\ndatasets for initial learning and task-specific adaptation, offering potential\nimprovements in GNN performance. However, previous research has overlooked the\npotential of graph prompts in optimizing models, as well as the impact of both\npositive and negative graph prompts on model stability and efficiency. To\nbridge this gap, we propose a novel framework combining graph prompts with\nweight pruning, called GPAWP, which aims to enhance the performance and\nefficiency of graph prompts by using fewer of them. We evaluate the importance\nof graph prompts using an importance assessment function to determine positive\nand negative weights at different granularities. Through hierarchically\nstructured pruning, we eliminate negative prompt labels, resulting in more\nparameter-efficient and competitively performing prompts. Extensive experiments\non three benchmark datasets demonstrate the superiority of GPAWP, leading to a\nsignificant reduction in parameters in node classification tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09132v1", "cate": "cs.LG", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09619", "title": "Generate Aligned Anomaly: Region-Guided Few-Shot Anomaly Image-Mask Pair Synthesis for Industrial Inspection", "authors": ["Yilin Lu", "Jianghang Lin", "Linhuang Xie", "Kai Zhao", "Yansong Qu", "Shengchuan Zhang", "Liujuan Cao", "Rongrong Ji"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09619v1", "summary": "Anomaly inspection plays a vital role in industrial manufacturing, but the\nscarcity of anomaly samples significantly limits the effectiveness of existing\nmethods in tasks such as localization and classification. While several anomaly\nsynthesis approaches have been introduced for data augmentation, they often\nstruggle with low realism, inaccurate mask alignment, and poor generalization.\nTo overcome these limitations, we propose Generate Aligned Anomaly (GAA), a\nregion-guided, few-shot anomaly image-mask pair generation framework. GAA\nleverages the strong priors of a pretrained latent diffusion model to generate\nrealistic, diverse, and semantically aligned anomalies using only a small\nnumber of samples. The framework first employs Localized Concept Decomposition\nto jointly model the semantic features and spatial information of anomalies,\nenabling flexible control over the type and location of anomalies. It then\nutilizes Adaptive Multi-Round Anomaly Clustering to perform fine-grained\nsemantic clustering of anomaly concepts, thereby enhancing the consistency of\nanomaly representations. Subsequently, a region-guided mask generation strategy\nensures precise alignment between anomalies and their corresponding masks,\nwhile a low-quality sample filtering module is introduced to further improve\nthe overall quality of the generated samples. Extensive experiments on the\nMVTec AD and LOCO datasets demonstrate that GAA achieves superior performance\nin both anomaly synthesis quality and downstream tasks such as localization and\nclassification.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09619v1", "cate": "cs.CV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09439", "title": "Dynamic Sparse Causal-Attention Temporal Networks for Interpretable Causality Discovery in Multivariate Time Series", "authors": ["Meriem Zerkouk", "Miloud Mihoubi", "Belkacem Chikhaoui"], "categories": ["cs.LG", "cs.AI", "cs.IR"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09439v1", "summary": "Understanding causal relationships in multivariate time series (MTS) is\nessential for effective decision-making in fields such as finance and\nmarketing, where complex dependencies and lagged effects challenge conventional\nanalytical approaches. We introduce Dynamic Sparse Causal-Attention Temporal\nNetworks for Interpretable Causality Discovery in MTS (DyCAST-Net), a novel\narchitecture designed to enhance causal discovery by integrating dilated\ntemporal convolutions and dynamic sparse attention mechanisms. DyCAST-Net\neffectively captures multiscale temporal dependencies through dilated\nconvolutions while leveraging an adaptive thresholding strategy in its\nattention mechanism to eliminate spurious connections, ensuring both accuracy\nand interpretability. A statistical shuffle test validation further strengthens\nrobustness by filtering false positives and improving causal inference\nreliability. Extensive evaluations on financial and marketing datasets\ndemonstrate that DyCAST-Net consistently outperforms existing models such as\nTCDF, GCFormer, and CausalFormer. The model provides a more precise estimation\nof causal delays and significantly reduces false discoveries, particularly in\nnoisy environments. Moreover, attention heatmaps offer interpretable insights,\nuncovering hidden causal patterns such as the mediated effects of advertising\non consumer behavior and the influence of macroeconomic indicators on financial\nmarkets. Case studies illustrate DyCAST-Net's ability to detect latent\nmediators and lagged causal factors, making it particularly effective in\nhigh-dimensional, dynamic settings. The model's architecture enhanced by\nRMSNorm stabilization and causal masking ensures scalability and adaptability\nacross diverse application domains", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09439v1", "cate": "cs.LG", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2407.15600", "title": "A Pairwise Comparison Relation-assisted Multi-objective Evolutionary Neural Architecture Search Method with Multi-population Mechanism", "authors": ["Yu Xue", "Pengcheng Jiang", "Chenchen Zhu", "MengChu Zhou", "Mohamed Wahib", "Moncef Gabbouj"], "categories": ["cs.NE", "cs.AI"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2407.15600v2", "summary": "Neural architecture search (NAS) enables researchers to automatically explore\nvast search spaces and find efficient neural networks. But NAS suffers from a\nkey bottleneck, i.e., numerous architectures need to be evaluated during the\nsearch process, which requires a lot of computing resources and time. In order\nto improve the efficiency of NAS, a series of methods have been proposed to\nreduce the evaluation time of neural architectures. However, they are not\nefficient enough and still only focus on the accuracy of architectures. In\naddition to the classification accuracy, more efficient and smaller network\narchitectures are required in real-world applications. To address the above\nproblems, we propose the SMEM-NAS, a pairwise comparison relation-assisted\nmulti-objective evolutionary algorithm based on a multi-population mechanism.\nIn the SMEM-NAS, a surrogate model is constructed based on pairwise comparison\nrelations to predict the accuracy ranking of architectures, rather than the\nabsolute accuracy. Moreover, two populations cooperate with each other in the\nsearch process, i.e., a main population guides the evolution, while a vice\npopulation expands the diversity. Our method aims to provide high-performance\nmodels that take into account multiple optimization objectives. We conduct a\nseries of experiments on the CIFAR-10, CIFAR-100 and ImageNet datasets to\nverify its effectiveness. With only a single GPU searching for 0.17 days,\ncompetitive architectures can be found by SMEM-NAS which achieves 78.91%\naccuracy with the MAdds of 570M on the ImageNet. This work makes a significant\nadvance in the important field of NAS. Our code is publicly available at\nhttps://github.com/ccz-enas/SMEM-NAS.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2407.15600v2", "cate": "cs.NE", "date": "2024-07-22", "updated": "2025-07-14"}
{"id": "2507.09668", "title": "Pyramid transforms via nonstationary subdivision schemes", "authors": ["Hadar Landau", "Wael Mattar", "Nir Sharon"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09668v1", "summary": "Pyramid transforms are constructive methods for analyzing sequences in a\nmultiscale fashion. Traditionally, these transforms rely on stationary\nupsampling and downsampling operations. In this paper, we propose employing\nnonstationary subdivision schemes as upsampling operators that vary according\nto the refinement level. These schemes offer greater flexibility, enabling the\ndevelopment of advanced multiscale transforms, including geometric multiscale\nanalysis. We establish the fundamental properties of these nonstationary\noperators and demonstrate their effectiveness in capturing and analyzing\ngeometric features. In particular, we present applications to highlight their\nutility in detecting geometric structures in planar objects.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09668v1", "cate": "math.NA", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09342", "title": "BENYO-S2ST-Corpus-1: A Bilingual English-to-Yoruba Direct Speech-to-Speech Translation Corpus", "authors": ["Emmanuel Adetiba", "Abdultaofeek Abayomi", "Raymond J. Kala", "Ayodele H. Ifijeh", "Oluwatobi E. Dare", "Olabode Idowu-Bismark", "Gabriel O. Sobola", "Joy N. Adetiba", "Monsurat Adepeju Lateef", "Heather Cole-Lewis"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09342v1", "summary": "There is a major shortage of Speech-to-Speech Translation (S2ST) datasets for\nhigh resource-to-low resource language pairs such as English-to-Yoruba. Thus,\nin this study, we curated the Bilingual English-to-Yoruba Speech-to-Speech\nTranslation Corpus Version 1 (BENYO-S2ST-Corpus-1). The corpus is based on a\nhybrid architecture we developed for large-scale direct S2ST corpus creation at\nreduced cost. To achieve this, we leveraged non speech-to-speech Standard\nYoruba (SY) real-time audios and transcripts in the YORULECT Corpus as well as\nthe corresponding Standard English (SE) transcripts. YORULECT Corpus is small\nscale(1,504) samples, and it does not have paired English audios. Therefore, we\ngenerated the SE audios using pre-trained AI models (i.e. Facebook MMS). We\nalso developed an audio augmentation algorithm named AcoustAug based on three\nlatent acoustic features to generate augmented audios from the raw audios of\nthe two languages. BENYO-S2ST-Corpus-1 has 12,032 audio samples per language,\nwhich gives a total of 24,064 sample size. The total audio duration for the two\nlanguages is 41.20 hours. This size is quite significant. Beyond building S2ST\nmodels, BENYO-S2ST-Corpus-1 can be used to build pretrained models or improve\nexisting ones. The created corpus and Coqui framework were used to build a\npretrained Yoruba TTS model (named YoruTTS-0.5) as a proof of concept. The\nYoruTTS-0.5 gave a F0 RMSE value of 63.54 after 1,000 epochs, which indicates\nmoderate fundamental pitch similarity with the reference real-time audio.\nUltimately, the corpus architecture in this study can be leveraged by\nresearchers and developers to curate datasets for multilingual\nhigh-resource-to-low-resource African languages. This will bridge the huge\ndigital divides in translations among high and low-resource language pairs.\nBENYO-S2ST-Corpus-1 and YoruTTS-0.5 are publicly available at\n(https://bit.ly/40bGMwi).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09342v1", "cate": "cs.SD", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.10407", "title": "Numerically Computing Galois Groups of Minimal Problems", "authors": ["Timothy Duff"], "categories": ["cs.CV", "cs.SC", "math.AG", "68W30"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      abstract accompanying invited tutorial at ISSAC 2025; 10 pages w/ references", "url": "http://arxiv.org/abs/2507.10407v1", "summary": "I discuss a seemingly unlikely confluence of topics in algebra, numerical\ncomputation, and computer vision. The motivating problem is that of solving\nmultiples instances of a parametric family of systems of algebraic (polynomial\nor rational function) equations. No doubt already of interest to ISSAC\nattendees, this problem arises in the context of robust model-fitting paradigms\ncurrently utilized by the computer vision community (namely \"Random Sampling\nand Consensus\", aka \"RanSaC\".) This talk will give an overview of work in the\nlast 5+ years that aspires to measure the intrinsic difficulty of solving such\nparametric systems, and makes strides towards practical solutions.", "comment": "abstract accompanying invited tutorial at ISSAC 2025; 10 pages w/\n  references", "pdf_url": "http://arxiv.org/pdf/2507.10407v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09137", "title": "POIFormer: A Transformer-Based Framework for Accurate and Scalable Point-of-Interest Attribution", "authors": ["Nripsuta Ani Saxena", "Shang-Ling Hsu", "Mehul Shetty", "Omar Alkhadra", "Cyrus Shahabi", "Abigail L. Horn"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09137v1", "summary": "Accurately attributing user visits to specific Points of Interest (POIs) is a\nfoundational task for mobility analytics, personalized services, marketing and\nurban planning. However, POI attribution remains challenging due to GPS\ninaccuracies, typically ranging from 2 to 20 meters in real-world settings, and\nthe high spatial density of POIs in urban environments, where multiple venues\ncan coexist within a small radius (e.g., over 50 POIs within a 100-meter radius\nin dense city centers). Relying on proximity is therefore often insufficient\nfor determining which POI was actually visited. We introduce\n\\textsf{POIFormer}, a novel Transformer-based framework for accurate and\nefficient POI attribution. Unlike prior approaches that rely on limited\nspatiotemporal, contextual, or behavioral features, \\textsf{POIFormer} jointly\nmodels a rich set of signals, including spatial proximity, visit timing and\nduration, contextual features from POI semantics, and behavioral features from\nuser mobility and aggregated crowd behavior patterns--using the Transformer's\nself-attention mechanism to jointly model complex interactions across these\ndimensions. By leveraging the Transformer to model a user's past and future\nvisits (with the current visit masked) and incorporating crowd-level behavioral\npatterns through pre-computed KDEs, \\textsf{POIFormer} enables accurate,\nefficient attribution in large, noisy mobility datasets. Its architecture\nsupports generalization across diverse data sources and geographic contexts\nwhile avoiding reliance on hard-to-access or unavailable data layers, making it\npractical for real-world deployment. Extensive experiments on real-world\nmobility datasets demonstrate significant improvements over existing baselines,\nparticularly in challenging real-world settings characterized by spatial noise\nand dense POI clustering.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09137v1", "cate": "cs.LG", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09630", "title": "Brain Stroke Detection and Classification Using CT Imaging with Transformer Models and Explainable AI", "authors": ["Shomukh Qari", "Maha A. Thafar"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      5 figures", "url": "http://arxiv.org/abs/2507.09630v1", "summary": "Stroke is one of the leading causes of death globally, making early and\naccurate diagnosis essential for improving patient outcomes, particularly in\nemergency settings where timely intervention is critical. CT scans are the key\nimaging modality because of their speed, accessibility, and cost-effectiveness.\nThis study proposed an artificial intelligence framework for multiclass stroke\nclassification (ischemic, hemorrhagic, and no stroke) using CT scan images from\na dataset provided by the Republic of Turkey's Ministry of Health. The proposed\nmethod adopted MaxViT, a state-of-the-art Vision Transformer, as the primary\ndeep learning model for image-based stroke classification, with additional\ntransformer variants (vision transformer, transformer-in-transformer, and\nConvNext). To enhance model generalization and address class imbalance, we\napplied data augmentation techniques, including synthetic image generation. The\nMaxViT model trained with augmentation achieved the best performance, reaching\nan accuracy and F1-score of 98.00%, outperforming all other evaluated models\nand the baseline methods. The primary goal of this study was to distinguish\nbetween stroke types with high accuracy while addressing crucial issues of\ntransparency and trust in artificial intelligence models. To achieve this,\nExplainable Artificial Intelligence (XAI) was integrated into the framework,\nparticularly Grad-CAM++. It provides visual explanations of the model's\ndecisions by highlighting relevant stroke regions in the CT scans and\nestablishing an accurate, interpretable, and clinically applicable solution for\nearly stroke detection. This research contributed to the development of a\ntrustworthy AI-assisted diagnostic tool for stroke, facilitating its\nintegration into clinical practice and enhancing access to timely and optimal\nstroke diagnosis in emergency departments, thereby saving more lives.", "comment": "5 figures", "pdf_url": "http://arxiv.org/pdf/2507.09630v1", "cate": "cs.CV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09831", "title": "Generative Cognitive Diagnosis", "authors": ["Jiatong Li", "Qi Liu", "Mengxiao Zhu"], "categories": ["cs.LG", "cs.AI", "cs.IR"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Preprint; 15 pages, 12 figures", "url": "http://arxiv.org/abs/2507.09831v1", "summary": "Cognitive diagnosis (CD) models latent cognitive states of human learners by\nanalyzing their response patterns on diagnostic tests, serving as a crucial\nmachine learning technique for educational assessment and evaluation.\nTraditional cognitive diagnosis models typically follow a transductive\nprediction paradigm that optimizes parameters to fit response scores and\nextract learner abilities. These approaches face significant limitations as\nthey cannot perform instant diagnosis for new learners without computationally\nexpensive retraining and produce diagnostic outputs with limited reliability.\nIn this study, we introduces a novel generative diagnosis paradigm that\nfundamentally shifts CD from predictive to generative modeling, enabling\ninductive inference of cognitive states without parameter re-optimization. We\npropose two simple yet effective instantiations of this paradigm: Generative\nItem Response Theory (G-IRT) and Generative Neural Cognitive Diagnosis Model\n(G-NCDM), which achieve excellent performance improvements over traditional\nmethods. The generative approach disentangles cognitive state inference from\nresponse prediction through a well-designed generation process that\nincorporates identifiability and monotonicity conditions. Extensive experiments\non real-world datasets demonstrate the effectiveness of our methodology in\naddressing scalability and reliability challenges, especially $\\times 100$\nspeedup for the diagnosis of new learners. Our framework opens new avenues for\ncognitive diagnosis applications in artificial intelligence, particularly for\nintelligent model evaluation and intelligent education systems. The code is\navailable at https://github.com/CSLiJT/Generative-CD.git.", "comment": "Preprint; 15 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2507.09831v1", "cate": "cs.LG", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2310.19603", "title": "Transformers Can Solve Non-Linear and Non-Markovian Filtering Problems in Continuous Time For Conditionally Gaussian Signals", "authors": ["Blanka Horvath", "Anastasis Kratsios", "Yannick Limmer", "Xuwei Yang"], "categories": ["cs.LG", "cs.NA", "cs.NE", "math.NA", "math.PR", "stat.ML", "60G35, 62M20, 68T07, 41A65"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2310.19603v4", "summary": "The use of attention-based deep learning models in stochastic filtering, e.g.\ntransformers and deep Kalman filters, has recently come into focus; however,\nthe potential for these models to solve stochastic filtering problems remains\nlargely unknown. The paper provides an affirmative answer to this open problem\nin the theoretical foundations of machine learning by showing that a class of\ncontinuous-time transformer models, called \\textit{filterformers}, can\napproximately implement the conditional law of a broad class of non-Markovian\nand conditionally Gaussian signal processes given noisy continuous-time\n(possibly non-Gaussian) measurements. Our approximation guarantees hold\nuniformly over sufficiently regular compact subsets of continuous-time paths,\nwhere the worst-case 2-Wasserstein distance between the true optimal filter and\nour deep learning model quantifies the approximation error. Our construction\nrelies on two new customizations of the standard attention mechanism: The first\ncan losslessly adapt to the characteristics of a broad range of paths since we\nshow that the attention mechanism implements bi-Lipschitz embeddings of\nsufficiently regular sets of paths into low-dimensional Euclidean spaces; thus,\nit incurs no ``dimension reduction error''. The latter attention mechanism is\ntailored to the geometry of Gaussian measures in the $2$-Wasserstein space. Our\nanalysis relies on new stability estimates of robust optimal filters in the\nconditionally Gaussian setting.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2310.19603v4", "cate": "cs.LG", "date": "2023-10-30", "updated": "2025-07-14"}
{"id": "2507.09757", "title": "Energy Dissipation Rate Guided Adaptive Sampling for Physics-Informed Neural Networks: Resolving Surface-Bulk Dynamics in Allen-Cahn Systems", "authors": ["Chunyan Li", "Wenkai Yu", "Qi Wang"], "categories": ["math.NA", "cs.LG", "cs.NA", "35K57, 68T07"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      32 pages, 22 figures", "url": "http://arxiv.org/abs/2507.09757v1", "summary": "We introduce the Energy Dissipation Rate guided Adaptive Sampling (EDRAS)\nstrategy, a novel method that substantially enhances the performance of\nPhysics-Informed Neural Networks (PINNs) in solving thermodynamically\nconsistent partial differential equations (PDEs) over arbitrary domains. EDRAS\nleverages the local energy dissipation rate density as a guiding metric to\nidentify and adaptively re-sample critical collocation points from both the\ninterior and boundary of the computational domain. This dynamical sampling\napproach improves the accuracy of residual-based PINNs by aligning the training\nprocess with the underlying physical structure of the system. In this study, we\ndemonstrate the effectiveness of EDRAS using the Allen-Cahn phase field model\nin irregular geometries, achieving up to a sixfold reduction in the relative\nmean square error compared to traditional residual-based adaptive refinement\n(RAR) methods. Moreover, we compare EDRAS with other residual-based adaptive\nsampling approaches and show that EDRAS is not only computationally more\nefficient but also more likely to identify high-impact collocation points.\nThrough numerical solutions of the Allen-Cahn equation with both static\n(Neumann) and dynamic boundary conditions in 2D disk- and ellipse-shaped\ndomains solved using PINN coupled with EDRAS, we gain significant insights into\nhow dynamic boundary conditions influence bulk phase evolution and\nthermodynamic behavior. The proposed approach offers an effective, physically\ninformed enhancement to PINN frameworks for solving thermodynamically\nconsistent models, making PINN a robust and versatile computational tool for\ninvestigating complex thermodynamic processes in arbitrary geometries.", "comment": "32 pages, 22 figures", "pdf_url": "http://arxiv.org/pdf/2507.09757v1", "cate": "math.NA", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09510", "title": "SC-TSE: Speaker Consistency-Aware Target Speaker Extraction", "authors": ["Shu Wu", "Anbin Qi", "Yanzhang Xie", "Xiang Xie"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Accept to Interspeech2025", "url": "http://arxiv.org/abs/2507.09510v1", "summary": "Target Speaker Extraction (TSE) uses a reference cue to extract the target\nspeech from a mixture. In TSE systems relying on audio cues, the speaker\nembedding from the enrolled speech is crucial to performance. However, these\nembeddings may suffer from speaker identity confusion. Unlike previous studies\nthat focus on improving speaker embedding extraction, we improve TSE\nperformance from the perspective of speaker consistency. In this paper, we\npropose a speaker consistency-aware target speaker extraction method that\nincorporates a centroid-based speaker consistency loss. This approach enhances\nTSE performance by ensuring speaker consistency between the enrolled and\nextracted speech. In addition, we integrate conditional loss suppression into\nthe training process. The experimental results validate the effectiveness of\nour proposed methods in advancing the TSE performance. A speech demo is\navailable online.\\footnote{https://sc-tse.netlify.app/", "comment": "Accept to Interspeech2025", "pdf_url": "http://arxiv.org/pdf/2507.09510v1", "cate": "cs.SD", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2506.18392", "title": "A matrix criterion and algorithmic approach for the Peterson hit problem: Part I", "authors": ["Dang Vo Phuc"], "categories": ["math.AT", "cs.SC", "math.GT", "math.RA", "55T15, 55S10, 55S05"], "primary_category": "Subjects:       Algebraic Topology (math.AT)", "pdf_link": null, "comments": "Comments:      47 pages. This version includes updated references and improved algorithms for enhanced execution efficiency. We welcome constructive comments and feedback on theoretical and practical aspects. We also welcome international collaboration in developing and extending our algorithms, particularly through implementation on the SageMath computer algebra system", "url": "http://arxiv.org/abs/2506.18392v3", "summary": "The Peterson hit problem in algebraic topology is to explicitly determine the\ndimension of the quotient space $Q\\mathcal P_k = \\mathbb F_2\\otimes_{\\mathcal\nA}\\mathcal P_k$ in positive degrees, where $\\mathcal{P}_k$ denotes the\npolynomial algebra in $k$ variables over the field $\\mathbb{F}_2$, considered\nas an unstable module over the Steenrod algebra $\\mathcal{A}$. Current\napproaches to this problem still rely heavily on manual computations, which are\nhighly prone to errors due to the intricate nature of the underlying\ncalculations. To date, no efficient algorithm implemented in any computer\nalgebra system has been made publicly available to tackle this problem in a\nsystematic manner.\n  Motivated by the above, in this work, which is considered as Part I of our\nproject, we first establish a criterion based entirely on linear algebra for\ndetermining whether a given homogeneous polynomial is \"hit\". Accordingly, we\ndescribe the dimensions of the hit spaces. This leads to a practical and\nreliable computational method for determining the dimension of $Q\\mathcal{P}_k$\nfor arbitrary $k$ and any positive degrees, with the support of a computer\nalgebra system. We then give a concrete implementation of the obtained results\nas novel algorithms in \\textsc{SageMath}. As an application, our algorithm\ndemonstrates that the manually computed result presented in the recent work of\nSum and Tai [15] for the dimension of $Q\\mathcal{P}_5$ in degree $2^{6}$ is not\ncorrect. Furthermore, our algorithm determines that\n$\\dim(Q\\mathcal{P}_5)_{2^{7}} = 1985,$ which falls within the range $1984 \\leq\n\\dim(Q\\mathcal{P}_5)_{2^{7}} \\leq 1990$ as estimated in [15].", "comment": "47 pages. This version includes updated references and improved\n  algorithms for enhanced execution efficiency. We welcome constructive\n  comments and feedback on theoretical and practical aspects. We also welcome\n  international collaboration in developing and extending our algorithms,\n  particularly through implementation on the SageMath computer algebra system", "pdf_url": "http://arxiv.org/pdf/2506.18392v3", "cate": "math.AT", "date": "2025-06-23", "updated": "2025-07-14"}
{"id": "2507.09155", "title": "OPENXRD: A Comprehensive Benchmark and Enhancement Framework for LLM/MLLM XRD Question Answering", "authors": ["Ali Vosoughi", "Ayoub Shahnazari", "Yufeng Xi", "Zeliang Zhang", "Griffin Hess", "Chenliang Xu", "Niaz Abdolrahim"], "categories": ["cs.CL", "cs.AI", "68T50, 68T07"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      10 pages, 6 figures, 5 tables. Code and dataset available at this https URL . Project webpage: this https URL", "url": "http://arxiv.org/abs/2507.09155v1", "summary": "This work presents OPENXRD, an open-book pipeline designed for\ncrystallography question answering, which integrates textual prompts with\nconcise supporting content generated by GPT-4.5. Instead of using scanned\ntextbooks, which may lead to copyright issues, OPENXRD generates compact,\ndomain-specific references that help smaller models understand key concepts in\nX-ray diffraction (XRD). We evaluate OPENXRD on a well-defined set of 217\nexpert-level XRD questions by comparing different vision-language models,\nincluding GPT-4 and LLaVA-based frameworks such as Mistral, LLaMA, and QWEN,\nunder both closed-book (without supporting material) and open-book (with\nsupporting material) conditions. Our experimental results show significant\naccuracy improvements in models that use the GPT-4.5-generated summaries,\nparticularly those with limited prior training in crystallography. OPENXRD uses\nknowledge from larger models to fill knowledge gaps in crystallography and\nshows that AI-generated texts can help smaller models reason more effectively\nin scientific tasks. While the current version of OPENXRD focuses on text-based\ninputs, we also explore future extensions such as adding real crystal diagrams\nor diffraction patterns to improve interpretation in specialized materials\nscience contexts. Overall, OPENXRD shows that specialized open-book systems can\nbe useful in materials science and provides a foundation for broader natural\nlanguage processing (NLP) tools in critical scientific fields.", "comment": "10 pages, 6 figures, 5 tables. Code and dataset available at\n  https://github.com/niaz60/OpenXRD. Project webpage:\n  https://niaz60.github.io/OpenXRD/", "pdf_url": "http://arxiv.org/pdf/2507.09155v1", "cate": "cs.CL", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09640", "title": "Disentanglement and Assessment of Shortcuts in Ophthalmological Retinal Imaging Exams", "authors": ["Leonor Fernandes", "Tiago Gonçalves", "João Matos", "Luis Filipe Nakayama", "Jaime S. Cardoso"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 pages. Under review", "url": "http://arxiv.org/abs/2507.09640v1", "summary": "Diabetic retinopathy (DR) is a leading cause of vision loss in working-age\nadults. While screening reduces the risk of blindness, traditional imaging is\noften costly and inaccessible. Artificial intelligence (AI) algorithms present\na scalable diagnostic solution, but concerns regarding fairness and\ngeneralization persist. This work evaluates the fairness and performance of\nimage-trained models in DR prediction, as well as the impact of disentanglement\nas a bias mitigation technique, using the diverse mBRSET fundus dataset. Three\nmodels, ConvNeXt V2, DINOv2, and Swin V2, were trained on macula images to\npredict DR and sensitive attributes (SAs) (e.g., age and gender/sex). Fairness\nwas assessed between subgroups of SAs, and disentanglement was applied to\nreduce bias. All models achieved high DR prediction performance in diagnosing\n(up to 94% AUROC) and could reasonably predict age and gender/sex (91% and 77%\nAUROC, respectively). Fairness assessment suggests disparities, such as a 10%\nAUROC gap between age groups in DINOv2. Disentangling SAs from DR prediction\nhad varying results, depending on the model selected. Disentanglement improved\nDINOv2 performance (2% AUROC gain), but led to performance drops in ConvNeXt V2\nand Swin V2 (7% and 3%, respectively). These findings highlight the complexity\nof disentangling fine-grained features in fundus imaging and emphasize the\nimportance of fairness in medical imaging AI to ensure equitable and reliable\nhealthcare solutions.", "comment": "10 pages. Under review", "pdf_url": "http://arxiv.org/pdf/2507.09640v1", "cate": "cs.CV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.10403", "title": "Text-to-Remote-Sensing-Image Retrieval beyond RGB Sources", "authors": ["Daniele Rege Cambrin", "Lorenzo Vaiani", "Giuseppe Gallipoli", "Luca Cagliero", "Paolo Garza"], "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.MM"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10403v1", "summary": "Retrieving relevant imagery from vast satellite archives is crucial for\napplications like disaster response and long-term climate monitoring. However,\nmost text-to-image retrieval systems are limited to RGB data, failing to\nexploit the unique physical information captured by other sensors, such as the\nall-weather structural sensitivity of Synthetic Aperture Radar (SAR) or the\nspectral signatures in optical multispectral data. To bridge this gap, we\nintroduce CrisisLandMark, a new large-scale corpus of over 647,000 Sentinel-1\nSAR and Sentinel-2 multispectral images paired with structured textual\nannotations for land cover, land use, and crisis events harmonized from\nauthoritative land cover systems (CORINE and Dynamic World) and crisis-specific\nsources. We then present CLOSP (Contrastive Language Optical SAR Pretraining),\na novel framework that uses text as a bridge to align unpaired optical and SAR\nimages into a unified embedding space. Our experiments show that CLOSP achieves\na new state-of-the-art, improving retrieval nDGC by 54% over existing models.\nAdditionally, we find that the unified training strategy overcomes the inherent\ndifficulty of interpreting SAR imagery by transferring rich semantic knowledge\nfrom the optical domain with indirect interaction. Furthermore, GeoCLOSP, which\nintegrates geographic coordinates into our framework, creates a powerful\ntrade-off between generality and specificity: while the CLOSP excels at general\nsemantic tasks, the GeoCLOSP becomes a specialized expert for retrieving\nlocation-dependent crisis events and rare geographic features. This work\nhighlights that the integration of diverse sensor data and geographic context\nis essential for unlocking the full potential of remote sensing archives.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10403v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2407.09488", "title": "Bayesian Theory of Consciousness as Exchangeable Emotion-Cognition Inference", "authors": ["Xin Li"], "categories": ["q-bio.NC", "cs.LG", "cs.NE"], "primary_category": "Subjects:       Neurons and Cognition (q-bio.NC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2407.09488v3", "summary": "This paper proposes a unified framework in which consciousness emerges as a\ncycle-consistent, affectively anchored inference process, recursively\nstructured by the interaction of emotion and cognition. Drawing from\ninformation theory, optimal transport, and the Bayesian brain hypothesis, we\nformalize emotion as a low-dimensional structural prior and cognition as a\nspecificity-instantiating update. This emotion-cognition cycle minimizes joint\nuncertainty by aligning emotionally weighted priors with context-sensitive\ncognitive appraisals. Subjective experience thus arises as the informational\nfootprint of temporally extended, affect-modulated simulation. We introduce the\nExchangeable Integration Theory of Consciousness (EITC), modeling conscious\nepisodes as conditionally exchangeable samples drawn from a latent affective\nself-model. This latent variable supports integration, via a unified\ncause-effect structure with nonzero irreducibility, and differentiation, by\npreserving contextual specificity across episodes. We connect this architecture\nto the Bayesian theory of consciousness through Rao-Blackwellized inference,\nwhich stabilizes inference by marginalizing latent self-structure while\nenabling adaptive updates. This mechanism ensures coherence, prevents inference\ncollapse, and supports goal-directed simulation. The formal framework builds on\nDe Finetti's exchangeability theorem, integrated information theory, and\nKL-regularized optimal transport. Overall, consciousness is reframed as a\nrecursive inference process, shaped by emotion, refined by cognition,\nstabilized through exchangeability, and unified through a latent self-model\nthat integrates experience across time.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2407.09488v3", "cate": "q-bio.NC", "date": "2024-05-17", "updated": "2025-07-12"}
{"id": "2507.09909", "title": "Energy-Stable Swarm-Based Inertial Algorithms for Optimization", "authors": ["Xuelong Gu", "Qi Wang"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09909v1", "summary": "We formulate the swarming optimization problem as a weakly coupled,\ndissipative dynamical system governed by a controlled energy dissipation rate\nand initial velocities that adhere to the nonequilibrium Onsager principle. In\nthis framework, agents' inertia, positions, and masses are dynamically coupled.\nTo numerically solve the system, we develop a class of efficient, energy-stable\nalgorithms that either preserve or enhance energy dissipation at the discrete\nlevel. At equilibrium, the system tends to converge toward one of the lowest\nlocal minima explored by the agents, thereby improving the likelihood of\nidentifying the global minimum. Numerical experiments confirm the effectiveness\nof the proposed approach, demonstrating significant advantages over traditional\nswarm-based gradient descent methods, especially when operating with a limited\nnumber of agents.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09909v1", "cate": "math.NA", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09606", "title": "Ensemble Confidence Calibration for Sound Event Detection in Open-environment", "authors": ["Yuanjian Chen", "Han Yin"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09606v1", "summary": "Sound event detection (SED) has made strong progress in controlled\nenvironments with clear event categories. However, real-world applications\noften take place in open environments. In such cases, current methods often\nproduce predictions with too much confidence and lack proper ways to measure\nuncertainty. This limits their ability to adapt and perform well in new\nsituations. To solve this problem, we are the first to use ensemble methods in\nSED to improve robustness against out-of-domain (OOD) inputs. We propose a\nconfidence calibration method called Energy-based Open-World Softmax\n(EOW-Softmax), which helps the system better handle uncertainty in unknown\nscenes. We further apply EOW-Softmax to sound occurrence and overlap detection\n(SOD) by adjusting the prediction. In this way, the model becomes more\nadaptable while keeping its ability to detect overlapping events. Experiments\nshow that our method improves performance in open environments. It reduces\noverconfidence and increases the ability to handle OOD situations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09606v1", "cate": "cs.SD", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09158", "title": "Automatic Contouring of Spinal Vertebrae on X-Ray using a Novel Sandwich U-Net Architecture", "authors": ["Sunil Munthumoduku Krishna Murthy", "Kumar Rajamani", "Srividya Tirunellai Rajamani", "Yupei Li", "Qiyang Sun", "Bjoern W. Schuller"], "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09158v1", "summary": "In spinal vertebral mobility disease, accurately extracting and contouring\nvertebrae is essential for assessing mobility impairments and monitoring\nvariations during flexion-extension movements. Precise vertebral contouring\nplays a crucial role in surgical planning; however, this process is\ntraditionally performed manually by radiologists or surgeons, making it\nlabour-intensive, time-consuming, and prone to human error. In particular,\nmobility disease analysis requires the individual contouring of each vertebra,\nwhich is both tedious and susceptible to inconsistencies. Automated methods\nprovide a more efficient alternative, enabling vertebra identification,\nsegmentation, and contouring with greater accuracy and reduced time\nconsumption. In this study, we propose a novel U-Net variation designed to\naccurately segment thoracic vertebrae from anteroposterior view on X-Ray\nimages. Our proposed approach, incorporating a ``sandwich\" U-Net structure with\ndual activation functions, achieves a 4.1\\% improvement in Dice score compared\nto the baseline U-Net model, enhancing segmentation accuracy while ensuring\nreliable vertebral contour extraction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09158v1", "cate": "eess.IV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09649", "title": "EyeSeg: An Uncertainty-Aware Eye Segmentation Framework for AR/VR", "authors": ["Zhengyuan Peng", "Jianqing Xu", "Shen Li", "Jiazhen Ji", "Yuge Huang", "Jingyun Zhang", "Jinmin Li", "Shouhong Ding", "Rizen Guo", "Xin Tan", "Lizhuang Ma"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to IJCAI", "url": "http://arxiv.org/abs/2507.09649v1", "summary": "Human-machine interaction through augmented reality (AR) and virtual reality\n(VR) is increasingly prevalent, requiring accurate and efficient gaze\nestimation which hinges on the accuracy of eye segmentation to enable smooth\nuser experiences. We introduce EyeSeg, a novel eye segmentation framework\ndesigned to overcome key challenges that existing approaches struggle with:\nmotion blur, eyelid occlusion, and train-test domain gaps. In these situations,\nexisting models struggle to extract robust features, leading to suboptimal\nperformance. Noting that these challenges can be generally quantified by\nuncertainty, we design EyeSeg as an uncertainty-aware eye segmentation\nframework for AR/VR wherein we explicitly model the uncertainties by performing\nBayesian uncertainty learning of a posterior under the closed set prior.\nTheoretically, we prove that a statistic of the learned posterior indicates\nsegmentation uncertainty levels and empirically outperforms existing methods in\ndownstream tasks, such as gaze estimation. EyeSeg outputs an uncertainty score\nand the segmentation result, weighting and fusing multiple gaze estimates for\nrobustness, which proves to be effective especially under motion blur, eyelid\nocclusion and cross-domain challenges. Moreover, empirical results suggest that\nEyeSeg achieves segmentation improvements of MIoU, E1, F1, and ACC surpassing\nprevious approaches. The code is publicly available at\nhttps://github.com/JethroPeng/EyeSeg.", "comment": "Accepted to IJCAI", "pdf_url": "http://arxiv.org/pdf/2507.09649v1", "cate": "cs.CV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.10485", "title": "Overcoming catastrophic forgetting in neural networks", "authors": ["Brandon Shuen Yi Loke", "Filippo Quadri", "Gabriel Vivanco", "Maximilian Casagrande", "Saúl Fenollosa"], "categories": ["cs.LG", "cs.IR"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      7 pages, 5 figures, EE-411 Fundamentals of inference and learning course project", "url": "http://arxiv.org/abs/2507.10485v1", "summary": "Catastrophic forgetting is the primary challenge that hinders continual\nlearning, which refers to a neural network ability to sequentially learn\nmultiple tasks while retaining previously acquired knowledge. Elastic Weight\nConsolidation, a regularization-based approach inspired by synaptic\nconsolidation in biological neural systems, has been used to overcome this\nproblem. In this study prior research is replicated and extended by evaluating\nEWC in supervised learning settings using the PermutedMNIST and RotatedMNIST\nbenchmarks. Through systematic comparisons with L2 regularization and\nstochastic gradient descent (SGD) without regularization, we analyze how\ndifferent approaches balance knowledge retention and adaptability. Our results\nconfirm what was shown in previous research, showing that EWC significantly\nreduces forgetting compared to naive training while slightly compromising\nlearning efficiency on new tasks. Moreover, we investigate the impact of\ndropout regularization and varying hyperparameters, offering insights into the\ngeneralization of EWC across diverse learning scenarios. These results\nunderscore EWC's potential as a viable solution for lifelong learning in neural\nnetworks.", "comment": "7 pages, 5 figures, EE-411 Fundamentals of inference and learning\n  course project", "pdf_url": "http://arxiv.org/pdf/2507.10485v1", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2412.05225", "title": "BEExformer: A Fast Inferencing Binarized Transformer with Early Exits", "authors": ["Wazib Ansar", "Saptarsi Goswami", "Amlan Chakrabarti"], "categories": ["cs.CL", "cs.AI", "cs.NE"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      This revised manuscript includes 18 pages, 17 figures, and 6 tables. Methodology and results sections have been improved for clarity and depth, incorporating additional comparisons, ablations, and a new evaluation dataset. A few relevant references were added, and overall organization refined for better readability", "url": "http://arxiv.org/abs/2412.05225v2", "summary": "Large Language Models (LLMs) based on transformers achieve cutting-edge\nresults on a variety of applications. However, their enormous size and\nprocessing requirements hinder deployment on constrained resources. To enhance\nefficiency, binarization and Early Exit (EE) have proved to be effective\nsolutions. However, binarization may lead to performance loss as reduced\nprecision affects gradient estimation and parameter updates. Besides, research\non EE mechanisms is still in its early stages. To address these challenges, we\nintroduce Binarized Early Exit Transformer (BEExformer), the first-ever\nselective learning-based transformer integrating Binarization-Aware Training\n(BAT) with EE for efficient and fast textual inference. Each transformer block\nhas an integrated Selective-Learn Forget Network (SLFN) to enhance contextual\nretention while eliminating irrelevant information. The BAT employs a\ndifferentiable second-order approximation to the sign function, enabling\ngradient computation that captures both the sign and magnitude of the weights.\nThis aids in 21.30 times reduction in model size. The EE mechanism hinges on\nfractional reduction in entropy among intermediate transformer blocks with\nsoft-routing loss estimation. This accelerates inference by reducing FLOPs by\n52.08% and even improves accuracy by 2.89% by resolving the \"overthinking\"\nproblem inherent in deep networks. Extensive evaluation through comparison with\nthe SOTA methods and various ablations across six datasets covering multiple\nNLP tasks demonstrates its Pareto-optimal performance-efficiency trade-off.", "comment": "This revised manuscript includes 18 pages, 17 figures, and 6 tables.\n  Methodology and results sections have been improved for clarity and depth,\n  incorporating additional comparisons, ablations, and a new evaluation\n  dataset. A few relevant references were added, and overall organization\n  refined for better readability", "pdf_url": "http://arxiv.org/pdf/2412.05225v2", "cate": "cs.CL", "date": "2024-12-06", "updated": "2025-07-12"}
{"id": "2507.09921", "title": "Numerical Analysis of a Bio-Polymerization Model", "authors": ["Ali Balooch", "Faranak Courtney-Pahlevani", "Lisa Davis", "Adrian Dunca", "Monika Neda", "Jorge Reyes"], "categories": ["math.NA", "cs.NA", "65M12, 65M60, 92-08, 92-10"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09921v1", "summary": "This work studies a stabilization technique for first-order hyperbolic\ndifferential equations used in DNA transcription modeling. Specifically we use\nthe Lighthill-Whitham-Richards Model with a nonlinear Greenshield's velocity\nproposed in [1]. Standard finite element methods are known to produce spurious\noscillations when applied to nonsmooth solutions. To address this, we\nincorporate stabilization terms involving spatial and temporal filtering into\nthe system. We present numerical stability and prove convergence results for\nboth the backwards Euler and time filtered formulations. We also present\nseveral computational results to demonstrate the rates in space and in time as\nwell as for selected biological scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09921v1", "cate": "math.NA", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09618", "title": "THAI Speech Emotion Recognition (THAI-SER) corpus", "authors": ["Jilamika Wongpithayadisai", "Chompakorn Chaksangchaichot", "Soravitt Sangnark", "Patawee Prakrankamanant", "Krit Gangwanpongpun", "Siwa Boonpunmongkol", "Premmarin Milindasuta", "Dangkamon Na-Pombejra", "Sarana Nutanong", "Ekapol Chuangsuwanich"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09618v1", "summary": "We present the first sizeable corpus of Thai speech emotion recognition,\nTHAI-SER, containing 41 hours and 36 minutes (27,854 utterances) from 100\nrecordings made in different recording environments: Zoom and two studio\nsetups. The recordings contain both scripted and improvised sessions, acted by\n200 professional actors (112 females and 88 males, aged 18 to 55) and were\ndirected by professional directors. There are five primary emotions: neutral,\nangry, happy, sad, and frustrated, assigned to the actors when recording\nutterances. The utterances are annotated with an emotional category using\ncrowdsourcing. To control the annotation process's quality, we also design an\nextensive filtering and quality control scheme to ensure that the majority\nagreement score remains above 0.71. We evaluate our annotated corpus using two\nmetrics: inter-annotator reliability and human recognition accuracy.\nInter-annotator reliability score was calculated using Krippendorff's alpha,\nwhere our corpus, after filtering, achieved an alpha score of 0.692, higher\nthan a recommendation of 0.667. For human recognition accuracy, our corpus\nscored up to 0.772 post-filtering. We also provide the results of the model\ntrained on the corpus evaluated on both in-corpus and cross-corpus setups. The\ncorpus is publicly available under a Creative Commons BY-SA 4.0, as well as our\ncodes for the experiments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09618v1", "cate": "cs.SD", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09173", "title": "Towards Interpretable Drug-Drug Interaction Prediction: A Graph-Based Approach with Molecular and Network-Level Explanations", "authors": ["Mengjie Chen", "Ming Zhang", "Cunquan Qu"], "categories": ["cs.LG", "cs.AI", "q-bio.MN"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09173v1", "summary": "Drug-drug interactions (DDIs) represent a critical challenge in pharmacology,\noften leading to adverse drug reactions with significant implications for\npatient safety and healthcare outcomes. While graph-based methods have achieved\nstrong predictive performance, most approaches treat drug pairs independently,\noverlooking the complex, context-dependent interactions unique to drug pairs.\nAdditionally, these models struggle to integrate biological interaction\nnetworks and molecular-level structures to provide meaningful mechanistic\ninsights. In this study, we propose MolecBioNet, a novel graph-based framework\nthat integrates molecular and biomedical knowledge for robust and interpretable\nDDI prediction. By modeling drug pairs as unified entities, MolecBioNet\ncaptures both macro-level biological interactions and micro-level molecular\ninfluences, offering a comprehensive perspective on DDIs. The framework\nextracts local subgraphs from biomedical knowledge graphs and constructs\nhierarchical interaction graphs from molecular representations, leveraging\nclassical graph neural network methods to learn multi-scale representations of\ndrug pairs. To enhance accuracy and interpretability, MolecBioNet introduces\ntwo domain-specific pooling strategies: context-aware subgraph pooling\n(CASPool), which emphasizes biologically relevant entities, and\nattention-guided influence pooling (AGIPool), which prioritizes influential\nmolecular substructures. The framework further employs mutual information\nminimization regularization to enhance information diversity during embedding\nfusion. Experimental results demonstrate that MolecBioNet outperforms\nstate-of-the-art methods in DDI prediction, while ablation studies and\nembedding visualizations further validate the advantages of unified drug pair\nmodeling and multi-scale knowledge integration.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09173v1", "cate": "cs.LG", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09672", "title": "VST-Pose: A Velocity-Integrated Spatiotem-poral Attention Network for Human WiFi Pose Estimation", "authors": ["Xinyu Zhang", "Zhonghao Ye", "Jingwei Zhang", "Xiang Tian", "Zhisheng Liang", "Shipeng Yu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      8 pages, 7 figures, 8 tables. WiFi CSI, VST-Pose framework + ViSTA-Former dual-stream attention backbone. Code: this https URL", "url": "http://arxiv.org/abs/2507.09672v1", "summary": "WiFi-based human pose estimation has emerged as a promising non-visual\nalternative approaches due to its pene-trability and privacy advantages. This\npaper presents VST-Pose, a novel deep learning framework for accurate and\ncontinuous pose estimation using WiFi channel state information. The proposed\nmethod introduces ViSTA-Former, a spatiotemporal attention backbone with\ndual-stream architecture that adopts a dual-stream architecture to separately\ncapture temporal dependencies and structural relationships among body joints.\nTo enhance sensitivity to subtle human motions, a velocity modeling branch is\nintegrated into the framework, which learns short-term keypoint dis-placement\npatterns and improves fine-grained motion representation. We construct a 2D\npose dataset specifically designed for smart home care scenarios and\ndemonstrate that our method achieves 92.2% accuracy on the PCK@50 metric,\noutperforming existing methods by 8.3% in PCK@50 on the self-collected dataset.\nFurther evaluation on the public MMFi dataset confirms the model's robustness\nand effectiveness in 3D pose estimation tasks. The proposed system provides a\nreliable and privacy-aware solution for continuous human motion analysis in\nindoor environments. Our codes are available in\nhttps://github.com/CarmenQing/VST-Pose.", "comment": "8 pages, 7 figures, 8 tables. WiFi CSI, VST-Pose framework +\n  ViSTA-Former dual-stream attention backbone. Code:\n  https://github.com/CarmenQing/VST-Pose", "pdf_url": "http://arxiv.org/pdf/2507.09672v1", "cate": "cs.CV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2408.14851", "title": "Graph and Sequential Neural Networks in Session-based Recommendation: A Survey", "authors": ["Zihao Li", "Chao Yang", "Yakun Chen", "Xianzhi Wang", "Hongxu Chen", "Guandong Xu", "Lina Yao", "Quan Z. Sheng"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2408.14851v2", "summary": "Recent years have witnessed the remarkable success of recommendation systems\n(RSs) in alleviating the information overload problem. As a new paradigm of\nRSs, session-based recommendation (SR) specializes in users' short-term\npreference capture and aims to provide a more dynamic and timely recommendation\nbased on the ongoing interacted actions. In this survey, we will give a\ncomprehensive overview of the recent works on SR. First, we clarify the\ndefinitions of various SR tasks and introduce the characteristics of\nsession-based recommendation against other recommendation tasks. Then, we\nsummarize the existing methods in two categories: sequential neural network\nbased methods and graph neural network (GNN) based methods. The standard\nframeworks and technical are also introduced. Finally, we discuss the\nchallenges of SR and new research directions in this area.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2408.14851v2", "cate": "cs.IR", "date": "2024-08-27", "updated": "2025-07-14"}
{"id": "2504.13355", "title": "Denoising and Reconstruction of Nonlinear Dynamics using Truncated Reservoir Computing", "authors": ["Omid Sedehi", "Manish Yadav", "Merten Stender", "Sebastian Oberst"], "categories": ["cs.LG", "cs.NE", "nlin.CD"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.13355v2", "summary": "Measurements acquired from distributed physical systems are often sparse and\nnoisy. Therefore, signal processing and system identification tools are\nrequired to mitigate noise effects and reconstruct unobserved dynamics from\nlimited sensor data. However, this process is particularly challenging because\nthe fundamental equations governing the dynamics are largely unavailable in\npractice. Reservoir Computing (RC) techniques have shown promise in efficiently\nsimulating dynamical systems through an unstructured and efficient computation\ngraph comprising a set of neurons with random connectivity. However, the\npotential of RC to operate in noisy regimes and distinguish noise from the\nprimary smooth or non-smooth deterministic dynamics of the system has not been\nfully explored. This paper presents a novel RC method for noise filtering and\nreconstructing unobserved nonlinear dynamics, offering a novel learning\nprotocol associated with hyperparameter optimization. The performance of the RC\nin terms of noise intensity, noise frequency content, and drastic shifts in\ndynamical parameters is studied in two illustrative examples involving the\nnonlinear dynamics of the Lorenz attractor and the adaptive exponential\nintegrate-and-fire system. It is demonstrated that denoising performance\nimproves by truncating redundant nodes and edges of the reservoir, as well as\nby properly optimizing hyperparameters, such as the leakage rate, spectral\nradius, input connectivity, and ridge regression parameter. Furthermore, the\npresented framework shows good generalization behavior when tested for\nreconstructing unseen and qualitatively different attractors. Compared to the\nextended Kalman filter, the presented RC framework yields competitive accuracy\nat low signal-to-noise ratios and high-frequency ranges.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.13355v2", "cate": "cs.LG", "date": "2025-04-17", "updated": "2025-07-13"}
{"id": "2507.10144", "title": "A structural bound for cluster robustness of randomized small-block Lanczos", "authors": ["Nian Shao"], "categories": ["math.NA", "cs.NA", "65F15"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10144v1", "summary": "The Lanczos method is a fast and memory-efficient algorithm for solving\nlarge-scale symmetric eigenvalue problems. However, its rapid convergence can\ndeteriorate significantly when computing clustered eigenvalues due to a lack of\ncluster robustness. A promising strategy to enhance cluster robustness --\nwithout substantially compromising convergence speed or memory efficiency -- is\nto use a random small-block initial, where the block size is greater than one\nbut still much smaller than the cluster size. This leads to the Randomized\nSmall-Block Lanczos (RSBL) method. Despite its empirical effectiveness, RSBL\nlacks the comprehensive theoretical understanding already available for\nsingle-vector and large-block variants. In this paper, we develop a structural\nbound that supports the cluster robustness of RSBL by leveraging tools from\nmatrix polynomials. We identify an intrinsic theoretical challenge stemming\nfrom the non-commuting nature of matrix multiplication. To provide further\ninsight, we propose a conjectured probabilistic bound for cluster robustness\nand validate it through empirical experiments. Finally, we discuss how insights\ninto cluster robustness can enhance our understanding of RSBL for both\neigenvalue computation and low-rank approximation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10144v1", "cate": "math.NA", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09750", "title": "MB-RIRs: a Synthetic Room Impulse Response Dataset with Frequency-Dependent Absorption Coefficients", "authors": ["Enric Gusó", "Joanna Luberadzka", "Umut Sayin", "Xavier Serra"], "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Accepted to WASPAA25", "url": "http://arxiv.org/abs/2507.09750v1", "summary": "We investigate the effects of four strategies for improving the ecological\nvalidity of synthetic room impulse response (RIR) datasets for monoaural Speech\nEnhancement (SE). We implement three features on top of the traditional image\nsource method-based (ISM) shoebox RIRs: multiband absorption coefficients,\nsource directivity and receiver directivity. We additionally consider\nmesh-based RIRs from the SoundSpaces dataset. We then train a DeepFilternet3\nmodel for each RIR dataset and evaluate the performance on a test set of real\nRIRs both objectively and subjectively. We find that RIRs which use\nfrequency-dependent acoustic absorption coefficients (MB-RIRs) can obtain\n+0.51dB of SDR and a +8.9 MUSHRA score when evaluated on real RIRs. The MB-RIRs\ndataset is publicly available for free download.", "comment": "Accepted to WASPAA25", "pdf_url": "http://arxiv.org/pdf/2507.09750v1", "cate": "cs.SD", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09177", "title": "Continual Reinforcement Learning by Planning with Online World Models", "authors": ["Zichen Liu", "Guoji Fu", "Chao Du", "Wee Sun Lee", "Min Lin"], "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      ICML 2025 Spotlight", "url": "http://arxiv.org/abs/2507.09177v1", "summary": "Continual reinforcement learning (CRL) refers to a naturalistic setting where\nan agent needs to endlessly evolve, by trial and error, to solve multiple tasks\nthat are presented sequentially. One of the largest obstacles to CRL is that\nthe agent may forget how to solve previous tasks when learning a new task,\nknown as catastrophic forgetting. In this paper, we propose to address this\nchallenge by planning with online world models. Specifically, we learn a\nFollow-The-Leader shallow model online to capture the world dynamics, in which\nwe plan using model predictive control to solve a set of tasks specified by any\nreward functions. The online world model is immune to forgetting by\nconstruction with a proven regret bound of $\\mathcal{O}(\\sqrt{K^2D\\log(T)})$\nunder mild assumptions. The planner searches actions solely based on the latest\nonline model, thus forming a FTL Online Agent (OA) that updates incrementally.\nTo assess OA, we further design Continual Bench, a dedicated environment for\nCRL, and compare with several strong baselines under the same model-planning\nalgorithmic framework. The empirical results show that OA learns continuously\nto solve new tasks while not forgetting old skills, outperforming agents built\non deep world models with various continual learning techniques.", "comment": "ICML 2025 Spotlight", "pdf_url": "http://arxiv.org/pdf/2507.09177v1", "cate": "cs.LG", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09681", "title": "Prompt2DEM: High-Resolution DEMs for Urban and Open Environments from Global Prompts Using a Monocular Foundation Model", "authors": ["Osher Rafaeli", "Tal Svoray", "Ariel Nahlieli"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      18 pages", "url": "http://arxiv.org/abs/2507.09681v1", "summary": "High-resolution elevation estimations are essential to understand catchment\nand hillslope hydrology, study urban morphology and dynamics, and monitor the\ngrowth, decline, and mortality of terrestrial ecosystems. Various deep learning\napproaches (e.g., super-resolution techniques, monocular depth estimation) have\nbeen developed to create high-resolution Digital Elevation Models (DEMs).\nHowever, super-resolution techniques are limited by the upscaling factor, and\nmonocular depth estimation lacks global elevation context, making its\nconversion to a seamless DEM restricted. The recently introduced technique of\nprompt-based monocular depth estimation has opened new opportunities to extract\nestimates of absolute elevation in a global context. We present here a\nframework for the estimation of high-resolution DEMs as a new paradigm for\nabsolute global elevation mapping. It is exemplified using low-resolution\nShuttle Radar Topography Mission (SRTM) elevation data as prompts and\nhigh-resolution RGB imagery from the National Agriculture Imagery Program\n(NAIP). The approach fine-tunes a vision transformer encoder with LiDAR-derived\nDEMs and employs a versatile prompting strategy, enabling tasks such as DEM\nestimation, void filling, and updating. Our framework achieves a 100x\nresolution gain (from 30-m to 30-cm), surpassing prior methods by an order of\nmagnitude. Evaluations across three diverse U.S. landscapes show robust\ngeneralization, capturing urban structures and fine-scale terrain features with\n< 5 m MAE relative to LiDAR, improving over SRTM by up to 18%. Hydrological\nanalysis confirms suitability for hazard and environmental studies. We\ndemonstrate scalability by applying the framework to large regions in the U.S.\nand Israel. All code and pretrained models are publicly available at:\nhttps://osherr1996.github.io/prompt2dem_propage/.", "comment": "18 pages", "pdf_url": "http://arxiv.org/pdf/2507.09681v1", "cate": "cs.CV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2409.06377", "title": "MoRE: A Mixture of Reflectors Framework for Large Language Model-Based Sequential Recommendation", "authors": ["Weicong Qin", "Yi Xu", "Weijie Yu", "Chenglei Shen", "Xiao Zhang", "Ming He", "Jianping Fan", "Jun Xu"], "categories": ["cs.IR", "cs.CL"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      First 2 authors contributes equally to this work, accepted by RecSys'25 spotlight oral. Corresponding author is Weijie Yu(yu@uibe. this http URL )", "url": "http://arxiv.org/abs/2409.06377v2", "summary": "Large language models (LLMs) have emerged as a cutting-edge approach in\nsequential recommendation, leveraging historical interactions to model dynamic\nuser preferences. Current methods mainly focus on learning processed\nrecommendation data in the form of sequence-to-sequence text. While effective,\nthey exhibit three key limitations: 1) failing to decouple intra-user explicit\nfeatures (e.g., product titles) from implicit behavioral patterns (e.g., brand\nloyalty) within interaction histories; 2) underutilizing cross-user\ncollaborative filtering (CF) signals; and 3) relying on inefficient reflection\nupdate strategies. To address this, We propose MoRE (Mixture of REflectors),\nwhich introduces three perspective-aware offline reflection processes to\naddress these gaps. This decomposition directly resolves Challenges 1\n(explicit/implicit ambiguity) and 2 (CF underutilization). Furthermore, MoRE's\nmeta-reflector employs a self-improving strategy and a dynamic selection\nmechanism (Challenge 3) to adapt to evolving user preferences. First, two\nintra-user reflectors decouple explicit and implicit patterns from a user's\ninteraction sequence, mimicking traditional recommender systems' ability to\ndistinguish surface-level and latent preferences. A third cross-user reflector\ncaptures CF signals by analyzing user similarity patterns from multiple users'\ninteractions. To optimize reflection quality, MoRE's meta-reflector employs a\noffline self-improving strategy that evaluates reflection impacts through\ncomparisons of presence/absence and iterative refinement of old/new versions,\nwith a online contextual bandit mechanism dynamically selecting the optimal\nperspective for recommendation for each user. Code:\nhttps://github.com/E-qin/MoRE-Rec.", "comment": "First 2 authors contributes equally to this work, accepted by\n  RecSys'25 spotlight oral. Corresponding author is Weijie Yu(yu@uibe.edu.cn)", "pdf_url": "http://arxiv.org/pdf/2409.06377v2", "cate": "cs.IR", "date": "2024-09-10", "updated": "2025-07-13"}
{"id": "2506.00533", "title": "RsGCN: Rescaling Enhances Generalization of GCNs for Solving Scalable Traveling Salesman Problems", "authors": ["Junquan Huang", "Zong-Gan Chen", "Yuncheng Jiang", "Zhi-Hui Zhan"], "categories": ["cs.LG", "cs.NE"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.00533v3", "summary": "Neural traveling salesman problem (TSP) solvers face two critical challenges:\npoor generalization for scalable TSPs and high training costs. To address these\nchallenges, we propose a new Rescaling Graph Convolutional Network (RsGCN).\nFocusing on the scale-dependent features (i.e., features varied with problem\nscales) related to nodes and edges that influence the sensitivity of GCNs to\nthe problem scales, a Rescaling Mechanism in RsGCN enhances the generalization\ncapability by (1) rescaling adjacent nodes to construct a subgraph with a\nuniform number of adjacent nodes for each node across various scales of TSPs,\nwhich stabilizes the graph message aggregation; (2) rescaling subgraph edges to\nadjust the lengths of subgraph edges to the same magnitude, which maintains\nnumerical consistency. In addition, an efficient training strategy with a\nmixed-scale dataset and bidirectional loss is used in RsGCN. To fully exploit\nthe heatmaps generated by RsGCN, we design an efficient post-search algorithm\ntermed Re2Opt, in which a reconstruction process based on adaptive weight is\nincorporated to help avoid local optima. Based on a combined architecture of\nRsGCN and Re2Opt, our solver achieves remarkable generalization and low\ntraining cost: with only 3 epochs of training on the mixed-scale dataset\ncontaining instances with up to 100 nodes, it can be generalized successfully\nto 10K-node instances without any fine-tuning. Extensive experiments\ndemonstrate our state-of-the-art performance across uniform distribution\ninstances of 9 different scales from 20 to 10K nodes and 78 real-world\ninstances from TSPLIB, while requiring the fewest learnable parameters and\ntraining epochs among neural competitors.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.00533v3", "cate": "cs.LG", "date": "2025-05-31", "updated": "2025-07-14"}
{"id": "2507.10538", "title": "Splitting Method for a Multilayered Poroelastic Solid Interacting with Stokes Flow", "authors": ["Andrew Scharf", "Martina Bukač", "Sunčica Čanić"], "categories": ["math.NA", "cs.NA", "74F10, 76S05, 74L15, 34A01, 74S05, 76M10, 65M60, 65M12, 65M22, 74H15"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      25 pages, 9 figures, 3 tables, submitted to SIAM Journal on Scientific Computing", "url": "http://arxiv.org/abs/2507.10538v1", "summary": "Multilayered poroelastic structures are found in many biological tissues such\nas cartilage and the cornea, and play a key role in the design of bioartificial\norgans and other bioengineering applications. Motivated by these applications,\nwe study the interaction between a free fluid flow, governed by the\ntime-dependent Stokes equations, and a multilayered poroelastic structure\ncomposed of a thick Biot layer and a thin, linear poroelastic plate located at\nthe interface. The resulting equations are linearly coupled across the thin\nstructure domain through physical coupling conditions. We develop a partitioned\nnumerical scheme for this poroelastic fluid-structure interaction problem,\ncombining the backward Euler Stokes-Biot splitting method with the fixed-strain\nBiot splitting approach. The first decouples the Stokes problem from the\nmultilayered structure problem, while the second decouples the flow and\nmechanical subproblems within the poroelastic structures. Stability of the\nsplitting scheme is proven under different combinations of time-step conditions\nand parameter constraints. The method is validated using manufactured\nsolutions, and further applied to a biologically inspired blood vessel flow\nproblem. We also demonstrate convergence of the solution to the limiting case\nwithout the plate as its thickness tends to zero, providing additional\nvalidation of the numerical method.", "comment": "25 pages, 9 figures, 3 tables, submitted to SIAM Journal on\n  Scientific Computing", "pdf_url": "http://arxiv.org/pdf/2507.10538v1", "cate": "math.NA", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09904", "title": "ASTAR-NTU solution to AudioMOS Challenge 2025 Track1", "authors": ["Fabian Ritter-Gutierrez", "Yi-Cheng Lin", "Jui-Chiang Wei", "Jeremy H. M. Wong", "Nancy F. Chen", "Hung-yi Lee"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Under Review - Submitted to AudioMOS Challenge 2025 - ASRU 2025", "url": "http://arxiv.org/abs/2507.09904v1", "summary": "Evaluation of text-to-music systems is constrained by the cost and\navailability of collecting experts for assessment. AudioMOS 2025 Challenge\ntrack 1 is created to automatically predict music impression (MI) as well as\ntext alignment (TA) between the prompt and the generated musical piece. This\npaper reports our winning system, which uses a dual-branch architecture with\npre-trained MuQ and RoBERTa models as audio and text encoders. A\ncross-attention mechanism fuses the audio and text representations. For\ntraining, we reframe the MI and TA prediction as a classification task. To\nincorporate the ordinal nature of MOS scores, one-hot labels are converted to a\nsoft distribution using a Gaussian kernel. On the official test set, a single\nmodel trained with this method achieves a system-level Spearman's Rank\nCorrelation Coefficient (SRCC) of 0.991 for MI and 0.952 for TA, corresponding\nto a relative improvement of 21.21\\% in MI SRCC and 31.47\\% in TA SRCC over the\nchallenge baseline.", "comment": "Under Review - Submitted to AudioMOS Challenge 2025 - ASRU 2025", "pdf_url": "http://arxiv.org/pdf/2507.09904v1", "cate": "cs.SD", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09202", "title": "XiChen: An observation-scalable fully AI-driven global weather forecasting system with 4D variational knowledge", "authors": ["Wuxin Wang", "Weicheng Ni", "Lilan Huang", "Tao Hao", "Ben Fei", "Shuo Ma", "Taikang Yuan", "Yanlai Zhao", "Kefeng Deng", "Xiaoyong Li", "Boheng Duan", "Lei Bai", "Kaijun Ren"], "categories": ["cs.LG", "cs.AI", "physics.ao-ph"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09202v1", "summary": "Recent advancements in Artificial Intelligence (AI) demonstrate significant\npotential to revolutionize weather forecasting. However, most AI-driven models\nrely on Numerical Weather Prediction (NWP) systems for initial condition\npreparation, which often consumes hours on supercomputers. Here we introduce\nXiChen, the first observation-scalable fully AI-driven global weather\nforecasting system, whose entire pipeline, from Data Assimilation (DA) to\nmedium-range forecasting, can be accomplished within only 17 seconds. XiChen is\nbuilt upon a foundation model that is pre-trained for weather forecasting.\nMeanwhile, this model is subsequently fine-tuned to serve as both observation\noperators and DA models, thereby scalably assimilating conventional and raw\nsatellite observations. Furthermore, the integration of four-dimensional\nvariational knowledge ensures that XiChen's DA and medium-range forecasting\naccuracy rivals that of operational NWP systems, amazingly achieving a skillful\nforecasting lead time exceeding 8.25 days. These findings demonstrate that\nXiChen holds strong potential toward fully AI-driven weather forecasting\nindependent of NWP systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09202v1", "cate": "cs.LG", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09693", "title": "ExpStar: Towards Automatic Commentary Generation for Multi-discipline Scientific Experiments", "authors": ["Jiali Chen", "Yujie Jia", "Zihan Wu", "Jinyu Yang", "Jianpeng Chen", "Xusen Hei", "Jiayuan Xie", "Yi Cai", "Qing Li"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ACM MM 2025", "url": "http://arxiv.org/abs/2507.09693v1", "summary": "Experiment commentary is crucial in describing the experimental procedures,\ndelving into underlying scientific principles, and incorporating\ncontent-related safety guidelines. In practice, human teachers rely heavily on\nsubject-specific expertise and invest significant time preparing such\ncommentary. To address this challenge, we introduce the task of automatic\ncommentary generation across multi-discipline scientific experiments. While\nrecent progress in large multimodal models (LMMs) has demonstrated promising\ncapabilities in video understanding and reasoning, their ability to generate\nfine-grained and insightful experiment commentary remains largely\nunderexplored. In this paper, we make the following contributions: (i) We\nconstruct \\textit{ExpInstruct}, the first dataset tailored for experiment\ncommentary generation, featuring over 7\\textit{K} step-level commentaries\nacross 21 scientific subjects from 3 core disciplines (\\ie, science, healthcare\nand engineering). Each sample includes procedural descriptions along with\npotential scientific principles (\\eg, chemical equations and physical laws) and\nsafety guidelines. (ii) We propose ExpStar, an automatic experiment commentary\ngeneration model that leverages a retrieval-augmented mechanism to adaptively\naccess, evaluate, and utilize external knowledge. (iii) Extensive experiments\nshow that our ExpStar substantially outperforms 14 leading LMMs, which\nhighlights the superiority of our dataset and model. We believe that ExpStar\nholds great potential for advancing AI-assisted scientific experiment\ninstruction.", "comment": "Accepted by ACM MM 2025", "pdf_url": "http://arxiv.org/pdf/2507.09693v1", "cate": "cs.CV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2411.06112", "title": "Interpret the Internal States of Recommendation Model with Sparse Autoencoder", "authors": ["Jiayin Wang", "Xiaoyu Zhang", "Weizhi Ma", "Zhiqiang Guo", "Min Zhang"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.06112v2", "summary": "Recommendation model interpretation aims to reveal models' calculation\nprocess, enhancing their transparency, interpretability, and trustworthiness by\nclarifying the relationships between inputs, model activations, and outputs.\nHowever, the complex, often opaque nature of deep learning models complicates\ninterpretation, and most existing methods are tailored to specific model\narchitectures, limiting their generalizability across different types of\nrecommendation models. To address these challenges, we propose RecSAE, an\nautomated and generalizable probing framework that interprets Recommenders with\nSparse AutoEncoder. It extracts interpretable latents from the internal states\nof recommendation models and links them to semantic concepts for\ninterpretation. RecSAE does not alter original models during interpretation and\nalso enables targeted de-biasing to models based on interpreted results.\nSpecifically, RecSAE operates in three steps: First, it probes activations\nbefore the prediction layer to capture internal representations. Next, the\nRecSAE module is trained on these activations with a larger latent space and\nsparsity constraints, making the RecSAE latents more mono-semantic than the\noriginal model activations. Thirdly, RecSAE utilizes a language model to\nconstruct concept descriptions with confidence scores based on the\nrelationships between latent activations and recommendation outputs.\nExperiments on three types of models (general, graph-based, and sequential)\nwith three widely used datasets demonstrate the effectiveness and\ngeneralization of RecSAE framework. The interpreted concepts are further\nvalidated by human experts, showing strong alignment with human perception.\nOverall, RecSAE serves as a novel step in both model-level interpretations to\nvarious types of recommenders without affecting their functions and offering\nthe potential for targeted tuning of models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.06112v2", "cate": "cs.IR", "date": "2024-11-09", "updated": "2025-07-14"}
{"id": "2507.08812", "title": "The Divergence-Free Radiant Transform", "authors": ["Zachary Mullaghy"], "categories": ["math.AP", "cs.NA", "math.NA", "65M70, 35Q30, 65N35, 58A12, 76D05, 35B65"], "primary_category": "Subjects:       Analysis of PDEs (math.AP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08812v1", "summary": "This paper presents the rigorous mathematical construction and foundational\nproperties of the Divergence-Free Radiant Transform (DFRT), a spectral\ntransform specifically designed for divergence-free vector fields, with\napplications in incompressible fluid dynamics and other solenoidal systems. The\nDFRT basis functions are constructed using a curl-based formulation that\nensures the divergence-free condition is satisfied identically. We define the\nforward and inverse transforms, prove the Parseval identity, and establish the\ncompleteness of the basis. The DFRT coefficient space is equipped with an\nalgebraic structure via a spectral coboundary operator, defined using Wigner 3j\nand 6j symbols to encode angular momentum coupling. This cohomological\nstructure, and its connection to the Geometric Refinement Transform (GRT), is\ndeveloped in a companion paper using a bigraded cohomology framework. We derive\na modal evolution equation for the incompressible Navier-Stokes equations in\nDFRT coordinates and introduce a persistent regularity class based on\ncohomological constraints. Finally, we present a variational argument showing\nthat an entropy-maximizing energy distribution leads to exponential decay,\noffering a new perspective on regularity and singularity prevention in\nincompressible flows.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08812v1", "cate": "math.AP", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2507.10313", "title": "DQLoRA: A Lightweight Domain-Aware Denoising ASR via Adapter-guided Distillation", "authors": ["Yiru Yang"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10313v1", "summary": "We present a demo of DQLoRA, an Adapter-Guided Distillation framework for\nrobust speech recognition under low-resource and noisy conditions. Our method\nemploys a frozen Whisper model as the teacher to provide semantic supervision,\nand a lightweight Wav2Vec2 student equipped with QLoRA-based Adapters. Training\nis conducted on the FLEURS dataset augmented with DNS-style noise. The student\nis optimized by jointly minimizing CTC loss and KL-based distillation loss,\nenabling efficient adaptation while preserving recognition accuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10313v1", "cate": "cs.SD", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.08849", "title": "Counterfactual optimization for fault prevention in complex wind energy systems", "authors": ["Emilio Carrizosa", "Martina Fischetti", "Roshell Haaker", "Juan Miguel Morales"], "categories": ["eess.SY", "cs.LG", "cs.SY", "math.OC"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08849v1", "summary": "Machine Learning models are increasingly used in businesses to detect faults\nand anomalies in complex systems. In this work, we take this approach a step\nfurther: beyond merely detecting anomalies, we aim to identify the optimal\ncontrol strategy that restores the system to a safe state with minimal\ndisruption. We frame this challenge as a counterfactual problem: given a\nMachine Learning model that classifies system states as either good or\nanomalous, our goal is to determine the minimal adjustment to the system's\ncontrol variables (i.e., its current status) that is necessary to return it to\nthe good state. To achieve this, we leverage a mathematical model that finds\nthe optimal counterfactual solution while respecting system specific\nconstraints. Notably, most counterfactual analysis in the literature focuses on\nindividual cases where a person seeks to alter their status relative to a\ndecision made by a classifier, such as for loan approval or medical diagnosis.\nOur work addresses a fundamentally different challenge: optimizing\ncounterfactuals for a complex energy system, specifically an offshore wind\nturbine oil type transformer. This application not only advances counterfactual\noptimization in a new domain but also opens avenues for broader research in\nthis area. Our tests on real world data provided by our industrial partner show\nthat our methodology easily adapts to user preferences and brings savings in\nthe order of 3 million euros per year in a typical farm.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08849v1", "cate": "eess.SY", "date": "2025-07-08", "updated": "2025-07-08"}
{"id": "2507.09227", "title": "PanoDiff-SR: Synthesizing Dental Panoramic Radiographs using Diffusion and Super-resolution", "authors": ["Sanyam Jain", "Bruna Neves de Freitas", "Andreas Basse-OConnor", "Alexandros Iosifidis", "Ruben Pauwels"], "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09227v1", "summary": "There has been increasing interest in the generation of high-quality,\nrealistic synthetic medical images in recent years. Such synthetic datasets can\nmitigate the scarcity of public datasets for artificial intelligence research,\nand can also be used for educational purposes. In this paper, we propose a\ncombination of diffusion-based generation (PanoDiff) and Super-Resolution (SR)\nfor generating synthetic dental panoramic radiographs (PRs). The former\ngenerates a low-resolution (LR) seed of a PR (256 X 128) which is then\nprocessed by the SR model to yield a high-resolution (HR) PR of size 1024 X\n512. For SR, we propose a state-of-the-art transformer that learns local-global\nrelationships, resulting in sharper edges and textures. Experimental results\ndemonstrate a Frechet inception distance score of 40.69 between 7243 real and\nsynthetic images (in HR). Inception scores were 2.55, 2.30, 2.90 and 2.98 for\nreal HR, synthetic HR, real LR and synthetic LR images, respectively. Among a\ndiverse group of six clinical experts, all evaluating a mixture of 100\nsynthetic and 100 real PRs in a time-limited observation, the average accuracy\nin distinguishing real from synthetic images was 68.5% (with 50% corresponding\nto random guessing).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09227v1", "cate": "eess.IV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09702", "title": "Token Compression Meets Compact Vision Transformers: A Survey and Comparative Evaluation for Edge AI", "authors": ["Phat Nguyen", "Ngai-Man Cheung"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09702v1", "summary": "Token compression techniques have recently emerged as powerful tools for\naccelerating Vision Transformer (ViT) inference in computer vision. Due to the\nquadratic computational complexity with respect to the token sequence length,\nthese methods aim to remove less informative tokens before the attention layers\nto improve inference throughput. While numerous studies have explored various\naccuracy-efficiency trade-offs on large-scale ViTs, two critical gaps remain.\nFirst, there is a lack of unified survey that systematically categorizes and\ncompares token compression approaches based on their core strategies (e.g.,\npruning, merging, or hybrid) and deployment settings (e.g., fine-tuning vs.\nplug-in). Second, most benchmarks are limited to standard ViT models (e.g.,\nViT-B, ViT-L), leaving open the question of whether such methods remain\neffective when applied to structurally compressed transformers, which are\nincreasingly deployed on resource-constrained edge devices. To address these\ngaps, we present the first systematic taxonomy and comparative study of token\ncompression methods, and we evaluate representative techniques on both standard\nand compact ViT architectures. Our experiments reveal that while token\ncompression methods are effective for general-purpose ViTs, they often\nunderperform when directly applied to compact designs. These findings not only\nprovide practical insights but also pave the way for future research on\nadapting token optimization techniques to compact transformer-based networks\nfor edge AI and AI agent applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09702v1", "cate": "cs.CV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2503.08965", "title": "LLM-Driven Usefulness Labeling for IR Evaluation", "authors": ["Mouly Dewan", "Jiqun Liu", "Chirag Shah"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.08965v2", "summary": "In the information retrieval (IR) domain, evaluation plays a crucial role in\noptimizing search experiences and supporting diverse user intents. In the\nrecent LLM era, research has been conducted to automate document relevance\nlabels, as these labels have traditionally been assigned by crowd-sourced\nworkers - a process that is both time and consuming and costly. This study\nfocuses on LLM-generated usefulness labels, a crucial evaluation metric that\nconsiders the user's search intents and task objectives, an aspect where\nrelevance falls short. Our experiment utilizes task-level, query-level, and\ndocument-level features along with user search behavior signals, which are\nessential in defining the usefulness of a document. Our research finds that (i)\npre-trained LLMs can generate moderate usefulness labels by understanding the\ncomprehensive search task session, (ii) pre-trained LLMs perform better\njudgement in short search sessions when provided with search session contexts.\nAdditionally, we investigated whether LLMs can capture the unique divergence\nbetween relevance and usefulness, along with conducting an ablation study to\nidentify the most critical metrics for accurate usefulness label generation. In\nconclusion, this work explores LLM-generated usefulness labels by evaluating\ncritical metrics and optimizing for practicality in real-world settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.08965v2", "cate": "cs.IR", "date": "2025-03-12", "updated": "2025-07-13"}
{"id": "2507.09278", "title": "Discrete reaction-diffusion system with stochastic dynamical boundary conditions: convergence results", "authors": ["Francesca Arceci", "Francesco Carlo De Vecchi", "Daniela Morale", "Stefania Ugolini"], "categories": ["math.PR", "cs.NA", "math.NA", "60H35, 65M06, 65M12"], "primary_category": "Subjects:       Probability (math.PR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09278v1", "summary": "A space discrete approximation to a highly nonlinear reaction-diffusion\nsystem endowed with a stochastic dynamical boundary condition is analyzed and\nthe convergence of the discrete scheme to the solution to the corresponding\ncontinuum random system is established. A splitting strategy allows us to\ndecompose the random system into a space-discrete heat equation with a\nstochastic boundary condition, and a nonlinear and nonlocal space-discrete\ndifferential system coupled with the first one and with deterministic initial\nand boundary conditions. The convergence result is obtained by first\nestablishing some a priori estimates for both space-discrete splitted variables\nand then exploiting compact embedding theorems for time-space Besov spaces on\nthe positive lattice. The convergence of a fully discrete approximation of the\nrandom system is also discussed.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09278v1", "cate": "math.PR", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.10447", "title": "Evaluating Fake Music Detection Performance Under Audio Augmentations", "authors": ["Tomasz Sroka", "Tomasz Wężowicz", "Dominik Sidorczuk", "Mateusz Modrzejewski"], "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      ISMIR 2025 LBD, 2 pages + bibliography, 1 figure", "url": "http://arxiv.org/abs/2507.10447v1", "summary": "With the rapid advancement of generative audio models, distinguishing between\nhuman-composed and generated music is becoming increasingly challenging. As a\nresponse, models for detecting fake music have been proposed. In this work, we\nexplore the robustness of such systems under audio augmentations. To evaluate\nmodel generalization, we constructed a dataset consisting of both real and\nsynthetic music generated using several systems. We then apply a range of audio\ntransformations and analyze how they affect classification accuracy. We test\nthe performance of a recent state-of-the-art musical deepfake detection model\nin the presence of audio augmentations. The performance of the model decreases\nsignificantly even with the introduction of light augmentations.", "comment": "ISMIR 2025 LBD, 2 pages + bibliography, 1 figure", "pdf_url": "http://arxiv.org/pdf/2507.10447v1", "cate": "cs.SD", "date": "2025-07-07", "updated": "2025-07-07"}
{"id": "2507.09000", "title": "Efficient Discovery of Actual Causality with Uncertainty", "authors": ["Arshia Rafieioskouei", "Kenneth Rogale", "Borzoo Bonakdarpour"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09000v1", "summary": "Identifying the actual cause of events in engineered systems is a fundamental\nchallenge in system analysis. Finding such causes becomes more challenging in\nthe presence of noise and uncertainty in real-world systems. In this paper, we\nadopt the notion of probabilistic actual causality by Fenton-Glynn, which is a\nprobabilistic extension of Halpern and Pearl's actual causality, and propose a\nnovel method to formally reason about causal effect of events in systems\nsubject to uncertainty. We (1) formulate the discovery of probabilistic actual\ncauses in computing systems as an SMT problem, and (2) address the scalability\nchallenges by introducing an abstraction-refinement technique that\nsignificantly improves efficiency. We demonstrate the effectiveness of our\napproach through three case studies, identifying probabilistic causes of safety\nviolations in (1) the Mountain Car problem, (2) the Lunar Lander benchmark, and\n(3) MPC controller for an F-16 autopilot simulator.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09000v1", "cate": "eess.SY", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09264", "title": "Controllable Patching for Compute-Adaptive Surrogate Modeling of Partial Differential Equations", "authors": ["Payel Mukhopadhyay", "Michael McCabe", "Ruben Ohana", "Miles Cranmer"], "categories": ["cs.LG", "cs.AI", "eess.IV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09264v1", "summary": "Patch-based transformer surrogates have become increasingly effective for\nmodeling spatiotemporal dynamics, but the fixed patch size is a major\nlimitation for budget-conscience deployment in production. We introduce two\nlightweight, architecture-agnostic modules-the Convolutional Kernel Modulator\n(CKM) and Convolutional Stride Modulator (CSM)-that enable dynamic patch size\ncontrol at inference in patch based models, without retraining or accuracy\nloss. Combined with a cyclic patch-size rollout, our method mitigates patch\nartifacts and improves long-term stability for video-like prediction tasks.\nApplied to a range of challenging 2D and 3D PDE benchmarks, our approach\nimproves rollout fidelity and runtime efficiency. To our knowledge, this is the\nfirst framework to enable inference-time patch-size tunability in patch-based\nPDE surrogates. Its plug-and-play design makes it broadly applicable across\narchitectures-establishing a general foundation for compute-adaptive modeling\nin PDE surrogate tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09264v1", "cate": "cs.LG", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09748", "title": "Advancing Text-to-3D Generation with Linearized Lookahead Variational Score Distillation", "authors": ["Yu Lei", "Bingde Liu", "Qingsong Xie", "Haonan Lu", "Zhijie Deng"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2507.09748v1", "summary": "Text-to-3D generation based on score distillation of pre-trained 2D diffusion\nmodels has gained increasing interest, with variational score distillation\n(VSD) as a remarkable example. VSD proves that vanilla score distillation can\nbe improved by introducing an extra score-based model, which characterizes the\ndistribution of images rendered from 3D models, to correct the distillation\ngradient. Despite the theoretical foundations, VSD, in practice, is likely to\nsuffer from slow and sometimes ill-posed convergence. In this paper, we perform\nan in-depth investigation of the interplay between the introduced score model\nand the 3D model, and find that there exists a mismatching problem between LoRA\nand 3D distributions in practical implementation. We can simply adjust their\noptimization order to improve the generation quality. By doing so, the score\nmodel looks ahead to the current 3D state and hence yields more reasonable\ncorrections. Nevertheless, naive lookahead VSD may suffer from unstable\ntraining in practice due to the potential over-fitting. To address this, we\npropose to use a linearized variant of the model for score distillation, giving\nrise to the Linearized Lookahead Variational Score Distillation ($L^2$-VSD).\n$L^2$-VSD can be realized efficiently with forward-mode autodiff\nfunctionalities of existing deep learning libraries. Extensive experiments\nvalidate the efficacy of $L^2$-VSD, revealing its clear superiority over prior\nscore distillation-based methods. We also show that our method can be\nseamlessly incorporated into any other VSD-based text-to-3D framework.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.09748v1", "cate": "cs.CV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2504.05319", "title": "Predictive Modeling: BIM Command Recommendation Based on Large-scale Usage Logs", "authors": ["Changyu Du", "Zihan Deng", "Stavros Nousias", "André Borrmann"], "categories": ["cs.IR", "cs.AI"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Advanced Engineering Informatics", "url": "http://arxiv.org/abs/2504.05319v2", "summary": "The adoption of Building Information Modeling (BIM) and model-based design\nwithin the Architecture, Engineering, and Construction (AEC) industry has been\nhindered by the perception that using BIM authoring tools demands more effort\nthan conventional 2D drafting. To enhance design efficiency, this paper\nproposes a BIM command recommendation framework that predicts the optimal next\nactions in real-time based on users' historical interactions. We propose a\ncomprehensive filtering and enhancement method for large-scale raw BIM log data\nand introduce a novel command recommendation model. Our model builds upon the\nstate-of-the-art Transformer backbones originally developed for large language\nmodels (LLMs), incorporating a custom feature fusion module, dedicated loss\nfunction, and targeted learning strategy. In a case study, the proposed method\nis applied to over 32 billion rows of real-world log data collected globally\nfrom the BIM authoring software Vectorworks. Experimental results demonstrate\nthat our method can learn universal and generalizable modeling patterns from\nanonymous user interaction sequences across different countries, disciplines,\nand projects. When generating recommendations for the next command, our\napproach achieves a Recall@10 of approximately 84%. The code is available at:\nhttps://github.com/dcy0577/BIM-Command-Recommendation.git", "comment": "Advanced Engineering Informatics", "pdf_url": "http://arxiv.org/pdf/2504.05319v2", "cate": "cs.IR", "date": "2025-02-23", "updated": "2025-07-13"}
{"id": "2507.09480", "title": "Discrete Differential Principle for Continuous Smooth Function Representation", "authors": ["Guoyou Wang", "Yihua Tan", "Shiqi Liu"], "categories": ["cs.LG", "cs.NA", "math.NA"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09480v1", "summary": "Taylor's formula holds significant importance in function representation,\nsuch as solving differential difference equations, ordinary differential\nequations, partial differential equations, and further promotes applications in\nvisual perception, complex control, fluid mechanics, weather forecasting and\nthermodynamics. However, the Taylor's formula suffers from the curse of\ndimensionality and error propagation during derivative computation in discrete\nsituations. In this paper, we propose a new discrete differential operator to\nestimate derivatives and to represent continuous smooth function locally using\nthe Vandermonde coefficient matrix derived from truncated Taylor series. Our\nmethod simultaneously computes all derivatives of orders less than the number\nof sample points, inherently mitigating error propagation. Utilizing\nequidistant uniform sampling, it achieves high-order accuracy while alleviating\nthe curse of dimensionality. We mathematically establish rigorous error bounds\nfor both derivative estimation and function representation, demonstrating\ntighter bounds for lower-order derivatives. We extend our method to the\ntwo-dimensional case, enabling its use for multivariate derivative\ncalculations. Experiments demonstrate the effectiveness and superiority of the\nproposed method compared to the finite forward difference method for derivative\nestimation and cubic spline and linear interpolation for function\nrepresentation. Consequently, our technique offers broad applicability across\ndomains such as vision representation, feature extraction, fluid mechanics, and\ncross-media imaging.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09480v1", "cate": "cs.LG", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.10456", "title": "Radif corpus: a symbolic dataset for non-metric iranian classical music", "authors": ["Maziar Kanani", "Sean O Leary", "James McDermott"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10456v1", "summary": "Non-metric music forms the core of the repertoire in Iranian classical music.\nDastgahi music serves as the underlying theoretical system for both Iranian art\nmusic and certain folk traditions. At the heart of Iranian classical music lies\nthe radif, a foundational repertoire that organizes melodic material central to\nperformance and pedagogy.\n  In this study, we introduce the first digital corpus representing the\ncomplete non-metrical radif repertoire, covering all 13 existing components of\nthis repertoire. We provide MIDI files (about 281 minutes in total) and data\nspreadsheets describing notes, note durations, intervals, and hierarchical\nstructures for 228 pieces of music. We faithfully represent the tonality\nincluding quarter-tones, and the non-metric aspect. Furthermore, we provide\nsupporting basic statistics, and measures of complexity and similarity over the\ncorpus.\n  Our corpus provides a platform for computational studies of Iranian classical\nmusic. Researchers might employ it in studying melodic patterns, investigating\nimprovisational styles, or for other tasks in music information retrieval,\nmusic theory, and computational (ethno)musicology.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10456v1", "cate": "cs.SD", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09115", "title": "Modelling and Control of a Buck Converter Using State-Space Averaging and Classical Feedback Techniques", "authors": ["Sampson E. Nwachukwu"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      8 Pages", "url": "http://arxiv.org/abs/2507.09115v1", "summary": "This study presents the modeling, control design, and performance analysis of\na DC-DC buck converter using state-space averaging techniques. Buck converters\nare essential in modern power electronics for regulating DC voltages in\nrenewable energy and electric vehicle systems. The paper first introduces the\nbasic operation of buck converters and emphasizes the need for voltage\nregulation through closed-loop control systems. A state-space averaged model is\nderived to simplify the nonlinear switched dynamics, enabling a more effective\nanalysis and controller design. The small-signal transfer function from the\nduty cycle to the output voltage is obtained to support control development. In\naddition, the Proportional-Integral (PI) control based on the frequency-domain\nmethod was explored. The PI controller was tuned to achieve various phase\nmargins and is evaluated through Bode plots, step responses, and performance\nmetrics, revealing trade-offs between overshoot, settling time, and\nsteady-state error. A complete simulation of the controlled buck converter\nverifies its ability to maintain a stable output voltage across wide input\nvoltage variations. The results validate the effectiveness of state-space\naveraging in control design and highlight the robustness of feedback systems in\npower electronic converters.", "comment": "8 Pages", "pdf_url": "http://arxiv.org/pdf/2507.09115v1", "cate": "eess.SY", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09347", "title": "A Framework for Predictive Directional Trading Based on Volatility and Causal Inference", "authors": ["Ivan Letteri"], "categories": ["q-fin.ST", "cs.AI", "stat.ML"], "primary_category": "Subjects:       Statistical Finance (q-fin.ST)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09347v1", "summary": "Purpose: This study introduces a novel framework for identifying and\nexploiting predictive lead-lag relationships in financial markets. We propose\nan integrated approach that combines advanced statistical methodologies with\nmachine learning models to enhance the identification and exploitation of\npredictive relationships between equities. Methods: We employed a Gaussian\nMixture Model (GMM) to cluster nine prominent stocks based on their mid-range\nhistorical volatility profiles over a three-year period. From the resulting\nclusters, we constructed a multi-stage causal inference pipeline, incorporating\nthe Granger Causality Test (GCT), a customised Peter-Clark Momentary\nConditional Independence (PCMCI) test, and Effective Transfer Entropy (ETE) to\nidentify robust, predictive linkages. Subsequently, Dynamic Time Warping (DTW)\nand a K-Nearest Neighbours (KNN) classifier were utilised to determine the\noptimal time lag for trade execution. The resulting strategy was rigorously\nbacktested. Results: The proposed volatility-based trading strategy, tested\nfrom 8 June 2023 to 12 August 2023, demonstrated substantial efficacy. The\nportfolio yielded a total return of 15.38%, significantly outperforming the\n10.39% return of a comparative Buy-and-Hold strategy. Key performance metrics,\nincluding a Sharpe Ratio up to 2.17 and a win rate up to 100% for certain\npairs, confirmed the strategy's viability. Conclusion: This research\ncontributes a systematic and robust methodology for identifying profitable\ntrading opportunities derived from volatility-based causal relationships. The\nfindings have significant implications for both academic research in financial\nmodelling and the practical application of algorithmic trading, offering a\nstructured approach to developing resilient, data-driven strategies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09347v1", "cate": "q-fin.ST", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09767", "title": "Pairwise Alignment & Compatibility for Arbitrarily Irregular Image Fragments", "authors": ["Ofir Itzhak Shahar", "Gur Elkin", "Ohad Ben-Shahar"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09767v1", "summary": "Pairwise compatibility calculation is at the core of most\nfragments-reconstruction algorithms, in particular those designed to solve\ndifferent types of the jigsaw puzzle problem. However, most existing approaches\nfail, or aren't designed to deal with fragments of realistic geometric\nproperties one encounters in real-life puzzles. And in all other cases,\ncompatibility methods rely strongly on the restricted shapes of the fragments.\nIn this paper, we propose an efficient hybrid (geometric and pictorial)\napproach for computing the optimal alignment for pairs of fragments, without\nany assumptions about their shapes, dimensions, or pictorial content. We\nintroduce a new image fragments dataset generated via a novel method for image\nfragmentation and a formal erosion model that mimics real-world archaeological\nerosion, along with evaluation metrics for the compatibility task. We then\nembed our proposed compatibility into an archaeological puzzle-solving\nframework and demonstrate state-of-the-art neighborhood-level precision and\nrecall on the RePAIR 2D dataset, directly reflecting compatibility performance\nimprovements.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09767v1", "cate": "cs.CV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2506.20854", "title": "Towards Two-Stage Counterfactual Learning to Rank", "authors": ["Shashank Gupta", "Yiming Liao", "Maarten de Rijke"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Accepted at ICTIR 2025 (co-located with SIGIR 2025)", "url": "http://arxiv.org/abs/2506.20854v3", "summary": "Counterfactual learning to rank (CLTR) aims to learn a ranking policy from\nuser interactions while correcting for the inherent biases in interaction data,\nsuch as position bias. Existing CLTR methods assume a single ranking policy\nthat selects top-K ranking from the entire document candidate set. In\nreal-world applications, the candidate document set is on the order of\nmillions, making a single-stage ranking policy impractical. In order to scale\nto millions of documents, real-world ranking systems are designed in a\ntwo-stage fashion, with a candidate generator followed by a ranker. The\nexisting CLTR method for a two-stage offline ranking system only considers the\ntop-1 ranking set-up and only focuses on training the candidate generator, with\nthe ranker fixed. A CLTR method for training both the ranker and candidate\ngenerator jointly is missing from the existing literature. In this paper, we\npropose a two-stage CLTR estimator that considers the interaction between the\ntwo stages and estimates the joint value of the two policies offline. In\naddition, we propose a novel joint optimization method to train the candidate\nand ranker policies, respectively. To the best of our knowledge, we are the\nfirst to propose a CLTR estimator and learning method for two-stage ranking.\nExperimental results on a semi-synthetic benchmark demonstrate the\neffectiveness of the proposed joint CLTR method over baselines.", "comment": "Accepted at ICTIR 2025 (co-located with SIGIR 2025)", "pdf_url": "http://arxiv.org/pdf/2506.20854v3", "cate": "cs.IR", "date": "2025-06-25", "updated": "2025-07-12"}
{"id": "2507.09772", "title": "Designing quantum chemistry algorithms with Just-In-Time compilation", "authors": ["Xiaojie Wu", "Yuanheng Wang"], "categories": ["physics.comp-ph", "cs.NA", "math.NA"], "primary_category": "Subjects:       Computational Physics (physics.comp-ph)", "pdf_link": null, "comments": "Comments:      10 pages, 7 figures", "url": "http://arxiv.org/abs/2507.09772v1", "summary": "We introduce just-in-time (JIT) compilation to the integral kernels for\nGaussian-type orbitals (GTOs) to enhance the efficiency of electron repulsion\nintegral computations. For Coulomb and exchange (JK) matrices, JIT-based\nalgorithms yield a 2x speedup for the small 6-31G* basis set on an NVIDIA\nA100-80G GPU. By incorporating a novel algorithm designed for orbitals with\nhigh angular momentum, the efficiency of JK evaluations with the large\ndef2-TZVPP basis set is improved by up to 4x. The core CUDA implementation is\ncompact, comprising only ~1,000 lines of code, including support for\nsingle-precision arithmetic. Furthermore, the single-precision implementation\nachieves a 3x speedup over the previous state-of-the-art.", "comment": "10 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.09772v1", "cate": "physics.comp-ph", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.10464", "title": "AudioMAE++: learning better masked audio representations with SwiGLU FFNs", "authors": ["Sarthak Yadav", "Sergios Theodoridis", "Zheng-Hua Tan"], "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      TO APPEAR AT IEEE MLSP 2025", "url": "http://arxiv.org/abs/2507.10464v1", "summary": "Masked Autoencoders (MAEs) trained on audio spectrogram patches have emerged\nas a prominent approach for learning self-supervised audio representations.\nWhile several recent papers have evaluated key aspects of training MAEs on\naudio data, the majority of these approaches still leverage vanilla transformer\nbuilding blocks, whereas the transformer community has seen steady integration\nof newer architectural advancements. In this work, we propose AudioMAE++, a\nrevamped audio masked autoencoder with two such enhancements, namely\nmacaron-style transformer blocks with gated linear units. When pretrained on\nthe AudioSet dataset, the proposed AudioMAE++ models outperform existing MAE\nbased approaches on 10 diverse downstream tasks, demonstrating excellent\nperformance on audio classification and speech-based benchmarks. The proposed\nAudioMAE++ models also demonstrate excellent scaling characteristics,\noutperforming directly comparable standard MAE baselines with up to 4x more\nparameters.", "comment": "TO APPEAR AT IEEE MLSP 2025", "pdf_url": "http://arxiv.org/pdf/2507.10464v1", "cate": "cs.SD", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09134", "title": "Integrating Planning and Predictive Control Using the Path Feasibility Governor", "authors": ["Shu Zhang", "James Y. Z. Liu", "Dominic Liao-McPherson"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      14 pages, 7 figures, submitted to IEEE Transactions on Automatic Control", "url": "http://arxiv.org/abs/2507.09134v1", "summary": "The motion planning problem of generating dynamically feasible,\ncollision-free trajectories in non-convex environments is a fundamental\nchallenge for autonomous systems. Decomposing the problem into path planning\nand path tracking improves tractability, but integrating these components in a\ntheoretically sound and computationally efficient manner is challenging. We\npropose the Path Feasibility Governor (PathFG), a framework for integrating\npath planners with nonlinear Model Predictive Control (MPC). The PathFG\nmanipulates the reference passed to the MPC controller, guiding it along a path\nwhile ensuring constraint satisfaction, stability, and recursive feasibility.\nThe PathFG is modular, compatible with replanning, and improves computational\nefficiency and reliability by reducing the need for long prediction horizons.\nWe prove safety and asymptotic stability with a significantly expanded region\nof attraction, and validate its real-time performance through a simulated case\nstudy of quadrotor navigation in a cluttered environment.", "comment": "14 pages, 7 figures, submitted to IEEE Transactions on Automatic\n  Control", "pdf_url": "http://arxiv.org/pdf/2507.09134v1", "cate": "eess.SY", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09353", "title": "Impute With Confidence: A Framework for Uncertainty Aware Multivariate Time Series Imputation", "authors": ["Addison Weatherhead", "Anna Goldenberg"], "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09353v1", "summary": "Time series data with missing values is common across many domains.\nHealthcare presents special challenges due to prolonged periods of sensor\ndisconnection. In such cases, having a confidence measure for imputed values is\ncritical. Most existing methods either overlook model uncertainty or lack\nmechanisms to estimate it. To address this gap, we introduce a general\nframework that quantifies and leverages uncertainty for selective imputation.\nBy focusing on values the model is most confident in, highly unreliable\nimputations are avoided. Our experiments on multiple EHR datasets, covering\ndiverse types of missingness, demonstrate that selectively imputing\nless-uncertain values not only reduces imputation errors but also improves\ndownstream tasks. Specifically, we show performance gains in a 24-hour\nmortality prediction task, underscoring the practical benefit of incorporating\nuncertainty into time series imputation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09353v1", "cate": "cs.LG", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09795", "title": "NegRefine: Refining Negative Label-Based Zero-Shot OOD Detection", "authors": ["Amirhossein Ansari", "Ke Wang", "Pulei Xiong"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2507.09795v1", "summary": "Recent advancements in Vision-Language Models like CLIP have enabled\nzero-shot OOD detection by leveraging both image and textual label information.\nAmong these, negative label-based methods such as NegLabel and CSP have shown\npromising results by utilizing a lexicon of words to define negative labels for\ndistinguishing OOD samples. However, these methods suffer from detecting\nin-distribution samples as OOD due to negative labels that are subcategories of\nin-distribution labels or proper nouns. They also face limitations in handling\nimages that match multiple in-distribution and negative labels. We propose\nNegRefine, a novel negative label refinement framework for zero-shot OOD\ndetection. By introducing a filtering mechanism to exclude subcategory labels\nand proper nouns from the negative label set and incorporating a\nmulti-matching-aware scoring function that dynamically adjusts the\ncontributions of multiple labels matching an image, NegRefine ensures a more\nrobust separation between in-distribution and OOD samples. We evaluate\nNegRefine on large-scale benchmarks, including ImageNet-1K. Source code is\navailable at https://github.com/ah-ansari/NegRefine.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.09795v1", "cate": "cs.CV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.03280", "title": "Modeling Item-Level Dynamic Variability with Residual Diffusion for Bundle Recommendation", "authors": ["Dong Zhang", "Lin Li", "Ming Li", "Xiaohui Tao", "Meng Sun", "Jimmy Xiangji Huang"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.03280v2", "summary": "Existing solutions for bundle recommendation(BR) have achieved remarkable\neffectiveness for predicting the user's preference for prebuilt bundles.\nHowever, bundle-item(B-I) affiliation will vary dynamically in real scenarios.\nFor example, a bundle themed as 'casual outfit', may add 'hat' or remove\n'watch' due to factors such as seasonal variations, changes in user pes or\ninventory adjustments. Our empirical study demonstrates that the performance of\nmainstream BR models will fluctuate or even decline regarding item-level\nvariability. This paper makes the first attempt to referencaddress the above\nproblem and proposes a novel Residual Diffusion for Bundle\nRecommendation(RDiffBR) as a model-agnostic generative framework which can\nassist a BR model in adapting this scenario. During the initial training of the\nBR model, RDiffBR employs a residual diffusion model to process the item-level\nbundle embeddings which are generated by BR model to represent bundle theme via\na forward-reverse process. In the inference stage, RDiffBR reverses item-level\nbundle embeddings obtained by the well-trained bundle model under B-I\nvariability scenarios to generate the effective item-level bundle embeddings.\nIn particular, the residual connection in our residual approximator\nsignificantly enhances item-level bundle embeddings generation ability of BR\nmodels. Experiments on six BR models and four public datasets from different\ndomains show that RDiffBR improves the performance of Recall and NDCG of\nbackbone BR models by up to 23%, while only increased training time about\n4%.Codes and datasets are available at\nhttps://anonymous.4open.science/r/RDiffBR.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.03280v2", "cate": "cs.IR", "date": "2025-07-04", "updated": "2025-07-14"}
{"id": "2507.09865", "title": "Gromov-Wasserstein Barycenters: The Analysis Problem", "authors": ["Rocío Díaz Martín", "Ivan V. Medri", "James M. Murphy"], "categories": ["math.OC", "cs.NA", "math.FA", "math.MG", "math.NA", "42B99, 49Q22, 68T01, 68T09, 90C35, 94A12"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09865v1", "summary": "This paper considers the problem of estimating a matrix that encodes pairwise\ndistances in a finite metric space (or, more generally, the edge weight matrix\nof a network) under the barycentric coding model (BCM) with respect to the\nGromov-Wasserstein (GW) distance function. We frame this task as estimating the\nunknown barycentric coordinates with respect to the GW distance, assuming that\nthe target matrix (or kernel) belongs to the set of GW barycenters of a finite\ncollection of known templates. In the language of harmonic analysis, if\ncomputing GW barycenters can be viewed as a synthesis problem, this paper aims\nto solve the corresponding analysis problem. We propose two methods: one\nutilizing fixed-point iteration for computing GW barycenters, and another\nemploying a differentiation-based approach to the GW structure using a blow-up\ntechnique. Finally, we demonstrate the application of the proposed GW analysis\napproach in a series of numerical experiments and applications to machine\nlearning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09865v1", "cate": "math.OC", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10534", "title": "WildFX: A DAW-Powered Pipeline for In-the-Wild Audio FX Graph Modeling", "authors": ["Qihui Yang", "Taylor Berg-Kirkpatrick", "Julian McAuley", "Zachary Novack"], "categories": ["cs.SD", "cs.AI"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10534v1", "summary": "Despite rapid progress in end-to-end AI music generation, AI-driven modeling\nof professional Digital Signal Processing (DSP) workflows remains challenging.\nIn particular, while there is growing interest in neural black-box modeling of\naudio effect graphs (e.g. reverb, compression, equalization), AI-based\napproaches struggle to replicate the nuanced signal flow and parameter\ninteractions used in professional workflows. Existing differentiable plugin\napproaches often diverge from real-world tools, exhibiting inferior performance\nrelative to simplified neural controllers under equivalent computational\nconstraints. We introduce WildFX, a pipeline containerized with Docker for\ngenerating multi-track audio mixing datasets with rich effect graphs, powered\nby a professional Digital Audio Workstation (DAW) backend. WildFX supports\nseamless integration of cross-platform commercial plugins or any plugins in the\nwild, in VST/VST3/LV2/CLAP formats, enabling structural complexity (e.g.,\nsidechains, crossovers) and achieving efficient parallelized processing. A\nminimalist metadata interface simplifies project/plugin configuration.\nExperiments demonstrate the pipeline's validity through blind estimation of\nmixing graphs, plugin/gain parameters, and its ability to bridge AI research\nwith practical DSP demands. The code is available on:\nhttps://github.com/IsaacYQH/WildFX.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10534v1", "cate": "cs.SD", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09280", "title": "Vertex-Guided Redundant Constraints Identification for Unit Commitment", "authors": ["Xuan He", "Yuxin Pan", "Yize Chen", "Danny H. K. Tsang"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09280v1", "summary": "Power systems Unit Commitment (UC) problem determines the generator\ncommitment schedule and dispatch decisions to realize the reliable and economic\noperation of power networks. The growing penetration of stochastic renewables\nand demand behaviors makes it necessary to solve the UC problem timely. It is\npossible to derive lightweight, faster-to-solve UC models via constraint\nscreening to eliminate redundant constraints. However, the screening process\nremains computationally cumbersome due to the need of solving numerous linear\nprogramming (LP) problems. To reduce the number of LPs to solve, we introduce a\nnovel perspective on such classic LP-based screening. Our key insights lie in\nthe principle that redundant constraints will be satisfied by all vertices of\nthe screened feasible region. Using the UC decision variables' bounds tightened\nby solving much fewer LPs, we build an outer approximation for the UC feasible\nregion as the screened region. A matrix operation is then designed and applied\nto the outer approximation's vertices to identify all redundant constraints\non-the-fly. Adjustments for the outer approximation are further explored to\nimprove screening efficiency by considering the load operating range and\ncutting planes derived from UC cost and discrete unit status prediction.\nExtensive simulations are performed on a set of testbeds up to 2,383 buses to\nsubstantiate the effectiveness of the proposed schemes. Compared to classic\nLP-based screening, our schemes can achieve up to 8.8x acceleration while\nfinding the same redundant constraints.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09280v1", "cate": "eess.SY", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09378", "title": "Context-Aware Regularization with Markovian Integration for Attention-Based Nucleotide Analysis", "authors": ["Mohammadsaleh Refahi", "Mahdi Abavisani", "Bahrad A. Sokhansanj", "James R. Brown", "Gail Rosen"], "categories": ["q-bio.GN", "cs.AI"], "primary_category": "Subjects:       Genomics (q-bio.GN)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09378v1", "summary": "Transformers have revolutionized nucleotide sequence analysis, yet capturing\nlong-range dependencies remains challenging. Recent studies show that\nautoregressive transformers often exhibit Markovian behavior by relying on\nfixed-length context windows for next-token prediction. However, standard\nself-attention mechanisms are computationally inefficient for long sequences\ndue to their quadratic complexity and do not explicitly enforce global\ntransition consistency.\n  We introduce CARMANIA (Context-Aware Regularization with Markovian\nIntegration for Attention-Based Nucleotide Analysis), a self-supervised\npretraining framework that augments next-token (NT) prediction with a\ntransition-matrix (TM) loss. The TM loss aligns predicted token transitions\nwith empirically derived n-gram statistics from each input sequence,\nencouraging the model to capture higher-order dependencies beyond local\ncontext. This integration enables CARMANIA to learn organism-specific sequence\nstructures that reflect both evolutionary constraints and functional\norganization.\n  We evaluate CARMANIA across diverse genomic tasks, including regulatory\nelement prediction, functional gene classification, taxonomic inference,\nantimicrobial resistance detection, and biosynthetic gene cluster\nclassification. CARMANIA outperforms the previous best long-context model by at\nleast 7 percent, matches state-of-the-art on shorter sequences (exceeding prior\nresults on 20 out of 40 tasks while running approximately 2.5 times faster),\nand shows particularly strong improvements on enhancer and housekeeping gene\nclassification tasks, including up to a 34 percent absolute gain in Matthews\ncorrelation coefficient (MCC) for enhancer prediction. The TM loss boosts\naccuracy in 33 of 40 tasks, especially where local motifs or regulatory\npatterns drive prediction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09378v1", "cate": "q-bio.GN", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09815", "title": "VRU-Accident: A Vision-Language Benchmark for Video Question Answering and Dense Captioning for Accident Scene Understanding", "authors": ["Younggun Kim", "Ahmed S. Abdelrahman", "Mohamed Abdel-Aty"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      22 pages, 11 figures, 5 tables", "url": "http://arxiv.org/abs/2507.09815v1", "summary": "Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and\ncyclists, is a critical challenge for autonomous driving systems, as crashes\ninvolving VRUs often result in severe or fatal consequences. While multimodal\nlarge language models (MLLMs) have shown promise in enhancing scene\nunderstanding and decision making in autonomous vehicles, there is currently no\nstandardized benchmark to quantitatively evaluate their reasoning abilities in\ncomplex, safety-critical scenarios involving VRUs. To address this gap, we\npresent VRU-Accident, a large-scale vision-language benchmark designed to\nevaluate MLLMs in high-risk traffic scenarios involving VRUs. VRU-Accident\ncomprises 1K real-world dashcam accident videos, annotated with 6K\nmultiple-choice question-answer pairs across six safety-critical categories\n(with 24K candidate options and 3.4K unique answer choices), as well as 1K\ndense scene descriptions. Unlike prior works, our benchmark focuses explicitly\non VRU-vehicle accidents, providing rich, fine-grained annotations that capture\nboth spatial-temporal dynamics and causal semantics of accidents. To assess the\ncurrent landscape of MLLMs, we conduct a comprehensive evaluation of 17\nstate-of-the-art models on the multiple-choice VQA task and on the dense\ncaptioning task. Our findings reveal that while MLLMs perform reasonably well\non visually grounded attributes, they face significant challenges in reasoning\nand describing accident causes, types, and preventability.", "comment": "22 pages, 11 figures, 5 tables", "pdf_url": "http://arxiv.org/pdf/2507.09815v1", "cate": "cs.CV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.03503", "title": "Exploring the Effect of Context-Awareness and Popularity Calibration on Popularity Bias in POI Recommendations", "authors": ["Andrea Forster", "Simone Kopeinik", "Denic Helic", "Stefan Thalmann", "Dominik Kowald"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Accepted at RecSys 2025, DOI: this https URL", "url": "http://arxiv.org/abs/2507.03503v2", "summary": "Point-of-interest (POI) recommender systems help users discover relevant\nlocations, but their effectiveness is often compromised by popularity bias,\nwhich disadvantages less popular, yet potentially meaningful places. This paper\naddresses this challenge by evaluating the effectiveness of context-aware\nmodels and calibrated popularity techniques as strategies for mitigating\npopularity bias. Using four real-world POI datasets (Brightkite, Foursquare,\nGowalla, and Yelp), we analyze the individual and combined effects of these\napproaches on recommendation accuracy and popularity bias. Our results reveal\nthat context-aware models cannot be considered a uniform solution, as the\nmodels studied exhibit divergent impacts on accuracy and bias. In contrast,\ncalibration techniques can effectively align recommendation popularity with\nuser preferences, provided there is a careful balance between accuracy and bias\nmitigation. Notably, the combination of calibration and context-awareness\nyields recommendations that balance accuracy and close alignment with the\nusers' popularity profiles, i.e., popularity calibration.", "comment": "Accepted at RecSys 2025, DOI: https://doi.org/10.1145/3705328.3748017", "pdf_url": "http://arxiv.org/pdf/2507.03503v2", "cate": "cs.IR", "date": "2025-07-04", "updated": "2025-07-13"}
{"id": "2507.09994", "title": "On the Convergence of the Policy Iteration for Infinite-Horizon Nonlinear Optimal Control Problems", "authors": ["Tobias Ehring", "Behzad Azmi", "Bernard Haasdonk"], "categories": ["math.OC", "cs.NA", "math.NA", "49L20, 49N35, 49J15, 49L12"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      35 pages and 3 figures", "url": "http://arxiv.org/abs/2507.09994v1", "summary": "Policy iteration (PI) is a widely used algorithm for synthesizing optimal\nfeedback control policies across many engineering and scientific applications.\nWhen PI is deployed on infinite-horizon, nonlinear, autonomous optimal-control\nproblems, however, a number of significant theoretical challenges emerge -\nparticularly when the computational state space is restricted to a bounded\ndomain. In this paper, we investigate these challenges and show that the\nviability of PI in this setting hinges on the existence, uniqueness, and\nregularity of solutions to the Generalized Hamilton-Jacobi-Bellman (GHJB)\nequation solved at each iteration. To ensure a well-posed iterative scheme, the\nGHJB solution must possess sufficient smoothness, and the domain on which the\nGHJB equation is solved must remain forward-invariant under the closed-loop\ndynamics induced by the current policy. Although fundamental to the method's\nconvergence, previous studies have largely overlooked these aspects. This paper\ncloses that gap by introducing a constructive procedure that guarantees forward\ninvariance of the computational domain throughout the entire PI sequence and by\nestablishing sufficient conditions under which a suitably regular GHJB solution\nexists at every iteration. Numerical results are presented for a grid-based\nimplementation of PI to support the theoretical findings.", "comment": "35 pages and 3 figures", "pdf_url": "http://arxiv.org/pdf/2507.09994v1", "cate": "math.OC", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09070", "title": "SemAlignVC: Enhancing zero-shot timbre conversion using semantic alignment", "authors": ["Shivam Mehta", "Yingru Liu", "Zhenyu Tang", "Kainan Peng", "Vimal Manohar", "Shun Zhang", "Mike Seltzer", "Qing He", "Mingbo Ma"], "categories": ["eess.AS", "cs.SD", "68T07", "I.2.7; I.2.6; G.3; H.5.5"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      6 pages, 2 figures, Accepted at the ISCA Speech Synthesis Workshop (SSW) 2025", "url": "http://arxiv.org/abs/2507.09070v1", "summary": "Zero-shot voice conversion (VC) synthesizes speech in a target speaker's\nvoice while preserving linguistic and paralinguistic content. However, timbre\nleakage-where source speaker traits persist-remains a challenge, especially in\nneural codec and LLM-based VC, where quantized representations entangle speaker\nidentity with content. We introduce SemAlignVC, an architecture designed to\nprevent timbre leakage using SemAlign, a novel method that aligns text and\naudio representations to ensure speaker-independent semantic encoding. This\ndisentangled representation conditions an autoregressive transformer for\nhigh-fidelity conversion without explicit speaker embeddings. Experiments show\nSemAlignVC significantly reduces timbre leakage, outperforming baselines in\nspeaker timbre similarity, intelligibility, and naturalness, making it a\nrobust, privacy-preserving, and generalizable VC solution. Audio samples can be\naccessed at https://shivammehta25.github.io/SemAlignVC/", "comment": "6 pages, 2 figures, Accepted at the ISCA Speech Synthesis Workshop\n  (SSW) 2025", "pdf_url": "http://arxiv.org/pdf/2507.09070v1", "cate": "eess.AS", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09311", "title": "A Fairness-Oriented Multi-Objective Reinforcement Learning approach for Autonomous Intersection Management", "authors": ["Matteo Cederle", "Marco Fabris", "Gian Antonio Susto"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      6 pages, 5 figures, accepted at the 1st Joint Conference on Computers, Cognition and Communication, Padua, Italy, Sep. 15-18, 2025", "url": "http://arxiv.org/abs/2507.09311v1", "summary": "This study introduces a novel multi-objective reinforcement learning (MORL)\napproach for autonomous intersection management, aiming to balance traffic\nefficiency and environmental sustainability across electric and internal\ncombustion vehicles. The proposed method utilizes MORL to identify\nPareto-optimal policies, with a post-hoc fairness criterion guiding the\nselection of the final policy. Simulation results in a complex intersection\nscenario demonstrate the approach's effectiveness in optimizing traffic\nefficiency and emissions reduction while ensuring fairness across vehicle\ncategories. We believe that this criterion can lay the foundation for ensuring\nequitable service, while fostering safe, efficient, and sustainable practices\nin smart urban mobility.", "comment": "6 pages, 5 figures, accepted at the 1st Joint Conference on\n  Computers, Cognition and Communication, Padua, Italy, Sep. 15-18, 2025", "pdf_url": "http://arxiv.org/pdf/2507.09311v1", "cate": "eess.SY", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09406", "title": "Adversarial Activation Patching: A Framework for Detecting and Mitigating Emergent Deception in Safety-Aligned Transformers", "authors": ["Santhosh Kumar Ravindran"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09406v1", "summary": "Large language models (LLMs) aligned for safety through techniques like\nreinforcement learning from human feedback (RLHF) often exhibit emergent\ndeceptive behaviors, where outputs appear compliant but subtly mislead or omit\ncritical information. This paper introduces adversarial activation patching, a\nnovel mechanistic interpretability framework that leverages activation patching\nas an adversarial tool to induce, detect, and mitigate such deception in\ntransformer-based models. By sourcing activations from \"deceptive\" prompts and\npatching them into safe forward passes at specific layers, we simulate\nvulnerabilities and quantify deception rates. Through toy neural network\nsimulations across multiple scenarios (e.g., 1000 trials per setup), we\ndemonstrate that adversarial patching increases deceptive outputs to 23.9% from\na 0% baseline, with layer-specific variations supporting our hypotheses. We\npropose six hypotheses, including transferability across models, exacerbation\nin multimodal settings, and scaling effects. An expanded literature review\nsynthesizes over 20 key works in interpretability, deception, and adversarial\nattacks. Mitigation strategies, such as activation anomaly detection and robust\nfine-tuning, are detailed, alongside ethical considerations and future research\ndirections. This work advances AI safety by highlighting patching's dual-use\npotential and provides a roadmap for empirical studies on large-scale models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09406v1", "cate": "cs.LG", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09830", "title": "Hierarchical Abstraction Enables Human-Like 3D Object Recognition in Deep Learning Models", "authors": ["Shuhao Fu", "Philip J. Kellman", "Hongjing Lu"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09830v1", "summary": "Both humans and deep learning models can recognize objects from 3D shapes\ndepicted with sparse visual information, such as a set of points randomly\nsampled from the surfaces of 3D objects (termed a point cloud). Although deep\nlearning models achieve human-like performance in recognizing objects from 3D\nshapes, it remains unclear whether these models develop 3D shape\nrepresentations similar to those used by human vision for object recognition.\nWe hypothesize that training with 3D shapes enables models to form\nrepresentations of local geometric structures in 3D shapes. However, their\nrepresentations of global 3D object shapes may be limited. We conducted two\nhuman experiments systematically manipulating point density and object\norientation (Experiment 1), and local geometric structure (Experiment 2).\nHumans consistently performed well across all experimental conditions. We\ncompared two types of deep learning models, one based on a convolutional neural\nnetwork (DGCNN) and the other on visual transformers (point transformer), with\nhuman performance. We found that the point transformer model provided a better\naccount of human performance than the convolution-based model. The advantage\nmainly results from the mechanism in the point transformer model that supports\nhierarchical abstraction of 3D shapes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09830v1", "cate": "cs.CV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.06507", "title": "GR-LLMs: Recent Advances in Generative Recommendation Based on Large Language Models", "authors": ["Zhen Yang", "Haitao Lin", "Jiawei xue", "Ziji Zhang"], "categories": ["cs.IR", "cs.AI"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      8 pages, 3 figures", "url": "http://arxiv.org/abs/2507.06507v2", "summary": "In the past year, Generative Recommendations (GRs) have undergone substantial\nadvancements, especially in leveraging the powerful sequence modeling and\nreasoning capabilities of Large Language Models (LLMs) to enhance overall\nrecommendation performance. LLM-based GRs are forming a new paradigm that is\ndistinctly different from discriminative recommendations, showing strong\npotential to replace traditional recommendation systems heavily dependent on\ncomplex hand-crafted features. In this paper, we provide a comprehensive survey\naimed at facilitating further research of LLM-based GRs. Initially, we outline\nthe general preliminaries and application cases of LLM-based GRs. Subsequently,\nwe introduce the main considerations when LLM-based GRs are applied in real\nindustrial scenarios. Finally, we explore promising directions for LLM-based\nGRs. We hope that this survey contributes to the ongoing advancement of the GR\ndomain.", "comment": "8 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2507.06507v2", "cate": "cs.IR", "date": "2025-07-09", "updated": "2025-07-14"}
{"id": "2507.10205", "title": "A new time-stepping strategy and boundary treatment to improve recent 2d traffic model", "authors": ["Friedemann Kemm"], "categories": ["eess.SY", "cs.NA", "cs.SY", "math.NA"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10205v1", "summary": "We show how a recently published 2d model for traffic flow can be further\nimproved. Besides other improvements and simplifications, we present not only a\nmethod to compute the necessary time step restrictions, but also a subcycling\nfor the inflow and outflow. This drastically reduces computational cost on\nlarge domains with coarse grids, i.\\,e.\\ for simulations of a whole region\ninstead of a small part of a city or town.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10205v1", "cate": "eess.SY", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09161", "title": "Large Language Models and Non-Negative Matrix Factorization for Bioacoustic Signal Decomposition", "authors": ["Yasaman Torabi", "Shahram Shirani", "James P. Reilly"], "categories": ["eess.AS", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Presented at Queen's University Biological Station Seminars of Graduate Research in Ontario, Lake Shift Dissertation Camp (QUBS '25)", "url": "http://arxiv.org/abs/2507.09161v1", "summary": "Large language models have shown a remarkable ability to extract meaning from\nunstructured data, offering new ways to interpret biomedical signals beyond\ntraditional numerical methods. In this study, we present a matrix factorization\nframework for bioacoustic signal analysis which is enhanced by large language\nmodels. The focus is on separating bioacoustic signals that commonly overlap in\nclinical recordings, using matrix factorization to decompose the mixture into\ninterpretable components. A large language model is then applied to the\nseparated signals to associate distinct acoustic patterns with potential\nmedical conditions such as cardiac rhythm disturbances or respiratory\nabnormalities. Recordings were obtained from a digital stethoscope applied to a\nclinical manikin to ensure a controlled and high-fidelity acquisition\nenvironment. This hybrid approach does not require labeled data or prior\nknowledge of source types, and it provides a more interpretable and accessible\nframework for clinical decision support. The method demonstrates promise for\nintegration into future intelligent diagnostic tools.", "comment": "Presented at Queen's University Biological Station Seminars of\n  Graduate Research in Ontario, Lake Shift Dissertation Camp (QUBS '25)", "pdf_url": "http://arxiv.org/pdf/2507.09161v1", "cate": "eess.AS", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09503", "title": "Neural Two-Stage Stochastic Optimization for Solving Unit Commitment Problem", "authors": ["Zhentong Shao", "Jingtao Qin", "Nanpeng Yu"], "categories": ["eess.SY", "cs.LG", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      Submitted to IEEE Transactions on Power Systems", "url": "http://arxiv.org/abs/2507.09503v1", "summary": "This paper proposes a neural stochastic optimization method for efficiently\nsolving the two-stage stochastic unit commitment (2S-SUC) problem under\nhigh-dimensional uncertainty scenarios. The proposed method approximates the\nsecond-stage recourse problem using a deep neural network trained to map\ncommitment decisions and uncertainty features to recourse costs. The trained\nnetwork is subsequently embedded into the first-stage UC problem as a\nmixed-integer linear program (MILP), allowing for explicit enforcement of\noperational constraints while preserving the key uncertainty characteristics. A\nscenario-embedding network is employed to enable dimensionality reduction and\nfeature aggregation across arbitrary scenario sets, serving as a data-driven\nscenario reduction mechanism. Numerical experiments on IEEE 5-bus, 30-bus, and\n118-bus systems demonstrate that the proposed neural two-stage stochastic\noptimization method achieves solutions with an optimality gap of less than 1%,\nwhile enabling orders-of-magnitude speedup compared to conventional MILP\nsolvers and decomposition-based methods. Moreover, the model's size remains\nconstant regardless of the number of scenarios, offering significant\nscalability for large-scale stochastic unit commitment problems.", "comment": "Submitted to IEEE Transactions on Power Systems", "pdf_url": "http://arxiv.org/pdf/2507.09503v1", "cate": "eess.SY", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09440", "title": "Transformers Don't In-Context Learn Least Squares Regression", "authors": ["Joshua Hill", "Benjamin Eyre", "Elliot Creager"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      21 pages, 16 figures, ICML 2025 Workshop on Reliable and Responsible Foundation Models", "url": "http://arxiv.org/abs/2507.09440v1", "summary": "In-context learning (ICL) has emerged as a powerful capability of large\npretrained transformers, enabling them to solve new tasks implicit in example\ninput-output pairs without any gradient updates. Despite its practical success,\nthe mechanisms underlying ICL remain largely mysterious. In this work we study\nsynthetic linear regression to probe how transformers implement learning at\ninference time. Previous works have demonstrated that transformers match the\nperformance of learning rules such as Ordinary Least Squares (OLS) regression\nor gradient descent and have suggested ICL is facilitated in transformers\nthrough the learned implementation of one of these techniques. In this work, we\ndemonstrate through a suite of out-of-distribution generalization experiments\nthat transformers trained for ICL fail to generalize after shifts in the prompt\ndistribution, a behaviour that is inconsistent with the notion of transformers\nimplementing algorithms such as OLS. Finally, we highlight the role of the\npretraining corpus in shaping ICL behaviour through a spectral analysis of the\nlearned representations in the residual stream. Inputs from the same\ndistribution as the training data produce representations with a unique\nspectral signature: inputs from this distribution tend to have the same top two\nsingular vectors. This spectral signature is not shared by out-of-distribution\ninputs, and a metric characterizing the presence of this signature is highly\ncorrelated with low loss.", "comment": "21 pages, 16 figures, ICML 2025 Workshop on Reliable and Responsible\n  Foundation Models", "pdf_url": "http://arxiv.org/pdf/2507.09440v1", "cate": "cs.LG", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09861", "title": "A Survey on MLLM-based Visually Rich Document Understanding: Methods, Challenges, and Emerging Trends", "authors": ["Yihao Ding", "Siwen Luo", "Yue Dai", "Yanbei Jiang", "Zechuan Li", "Geoffrey Martin", "Yifan Peng"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Work in progress", "url": "http://arxiv.org/abs/2507.09861v1", "summary": "Visually-Rich Document Understanding (VRDU) has emerged as a critical field,\ndriven by the need to automatically process documents containing complex\nvisual, textual, and layout information. Recently, Multimodal Large Language\nModels (MLLMs) have shown remarkable potential in this domain, leveraging both\nOptical Character Recognition (OCR)-dependent and OCR-free frameworks to\nextract and interpret information in document images. This survey reviews\nrecent advancements in MLLM-based VRDU, highlighting three core components: (1)\nmethods for encoding and fusing textual, visual, and layout features; (2)\ntraining paradigms, including pretraining strategies, instruction-response\ntuning, and the trainability of different model modules; and (3) datasets\nutilized for pretraining, instruction-tuning, and supervised fine-tuning.\nFinally, we discuss the challenges and opportunities in this evolving field and\npropose future directions to advance the efficiency, generalizability, and\nrobustness of VRDU systems.", "comment": "Work in progress", "pdf_url": "http://arxiv.org/pdf/2507.09861v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.08322", "title": "Towards Efficient Quantity Retrieval from Text:An Approach via Description Parsing and Weak Supervision", "authors": ["Yixuan Cao", "Zhengrong Chen", "Chengxuan Xia", "Kun Wu", "Ping Luo"], "categories": ["cs.IR", "cs.LG"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Extended version of the paper accepted in DEXA 2025", "url": "http://arxiv.org/abs/2507.08322v2", "summary": "Quantitative facts are continually generated by companies and governments,\nsupporting data-driven decision-making. While common facts are structured, many\nlong-tail quantitative facts remain buried in unstructured documents, making\nthem difficult to access. We propose the task of Quantity Retrieval: given a\ndescription of a quantitative fact, the system returns the relevant value and\nsupporting evidence. Understanding quantity semantics in context is essential\nfor this task. We introduce a framework based on description parsing that\nconverts text into structured (description, quantity) pairs for effective\nretrieval. To improve learning, we construct a large paraphrase dataset using\nweak supervision based on quantity co-occurrence. We evaluate our approach on a\nlarge corpus of financial annual reports and a newly annotated quantity\ndescription dataset. Our method significantly improves top-1 retrieval accuracy\nfrom 30.98 percent to 64.66 percent.", "comment": "Extended version of the paper accepted in DEXA 2025", "pdf_url": "http://arxiv.org/pdf/2507.08322v2", "cate": "cs.IR", "date": "2025-07-11", "updated": "2025-07-14"}
{"id": "2308.14127", "title": "Information geometric regularization of the barotropic Euler equation", "authors": ["Ruijia Cao", "Florian Schäfer"], "categories": ["math.NA", "cs.NA", "math.AP", "35L65, 76L05, 65M25, 76J20, 58B20"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2308.14127v4", "summary": "Shock waves in gas dynamics feature jump discontinuities that hinder\nnumerical simulations. Viscous regularizations are prone to excessive\ndissipation of fine-scale structures. In this work, we propose the first\ninviscid regularization of the multidimensional Euler equation based on ideas\nfrom semidefinite programming, information geometry, geometric hydrodynamics,\nand nonlinear elasticity. The Lagrangian flow maps of Euler solutions are a\ndynamical system on the manifold of diffeomorphisms. We observe that shock\nformation arises from the manifold's geodesic incompleteness. Our\nregularization embeds it into an ambient space equipped with the information\ngeometry of the logarithmic barrier function. Thus, the diffeomorphism manifold\ninherits a geodesically complete geometry. The resulting regularized\nconservation law replaces shocks with smooth profiles without affecting\noscillatory structures. One and two-dimensional numerical experiments show its\npractical potential to enable higher-order methods without explicit shock\ncapturing. While we focus on the barotropic Euler equations for concreteness\nand simplicity of exposition, our regularization easily extends to more general\nEuler and Navier-Stokes-type equations. Our approach regularizes the\nWasserstein geometry of the mass density with its information geometry. The\nformer captures the natural trajectories of physical particles and the latter\nthat of statistical estimators. Information geometric regularization accounts\nfor the mass density's dual nature as a statistical/computational tool\nsummarizing the motion of physical particles. Thus, our work is a starting\npoint for information geometric mechanics that views solutions of continuum\nmechanical PDEs as parameters of statistical models for unresolved scales and\nuses their information geometry to evolve them in time.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2308.14127v4", "cate": "math.NA", "date": "2023-08-27", "updated": "2025-07-14"}
{"id": "2507.09226", "title": "Can We Really Repurpose Multi-Speaker ASR Corpus for Speaker Diarization?", "authors": ["Shota Horiguchi", "Naohiro Tawara", "Takanori Ashihara", "Atsushi Ando", "Marc Delcroix"], "categories": ["eess.AS", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09226v1", "summary": "Neural speaker diarization is widely used for overlap-aware speaker\ndiarization, but it requires large multi-speaker datasets for training. To meet\nthis data requirement, large datasets are often constructed by combining\nmultiple corpora, including those originally designed for multi-speaker\nautomatic speech recognition (ASR). However, ASR datasets often feature loosely\ndefined segment boundaries that do not align with the stricter conventions of\ndiarization benchmarks. In this work, we show that such boundary looseness\nsignificantly impacts the diarization error rate, reducing evaluation\nreliability. We also reveal that models trained on data with varying boundary\nprecision tend to learn dataset-specific looseness, leading to poor\ngeneralization across out-of-domain datasets. Training with standardized tight\nboundaries via forced alignment improves not only diarization performance,\nespecially in streaming scenarios, but also ASR performance when combined with\nsimple post-processing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09226v1", "cate": "eess.AS", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09646", "title": "Learning Koopman Models From Data Under General Noise Conditions", "authors": ["Lucian Cristian Iacob", "Máté Szécsi", "Gerben Izaak Beintema", "Maarten Schoukens", "Roland Tóth"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      Submitted to SIAM Journal on Applied Dynamical Systems (SIADS)", "url": "http://arxiv.org/abs/2507.09646v1", "summary": "This paper presents a novel identification approach of Koopman models of\nnonlinear systems with inputs under rather general noise conditions. The method\nuses deep state-space encoders based on the concept of state reconstructability\nand an efficient multiple-shooting formulation of the squared loss of the\nprediction error to estimate the dynamics and the lifted state from\ninput-output data. Furthermore, the Koopman model structure includes an\ninnovation noise term that is used to handle process and measurement noise. It\nis shown that the proposed approach is statistically consistent and\ncomputationally efficient due to the multiple-shooting formulation where, on\nsubsections of the data, multi-step prediction errors can be calculated in\nparallel. The latter allows for efficient batch optimization of the network\nparameters and, at the same time, excellent long-term prediction capabilities\nof the obtained models. The performance of the approach is illustrated by\nnonlinear benchmark examples.", "comment": "Submitted to SIAM Journal on Applied Dynamical Systems (SIADS)", "pdf_url": "http://arxiv.org/pdf/2507.09646v1", "cate": "eess.SY", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09445", "title": "Fourier Basis Mapping: A Time-Frequency Learning Framework for Time Series Forecasting", "authors": ["Runze Yang", "Longbing Cao", "Xin You", "Kun Fang", "Jianxun Li", "Jie Yang"], "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      18 pages, 6 figures", "url": "http://arxiv.org/abs/2507.09445v1", "summary": "The integration of Fourier transform and deep learning opens new avenues for\ntime series forecasting. We reconsider the Fourier transform from a basis\nfunctions perspective. Specifically, the real and imaginary parts of the\nfrequency components can be regarded as the coefficients of cosine and sine\nbasis functions at tiered frequency levels, respectively. We find that existing\nFourier-based methods face inconsistent starting cycles and inconsistent series\nlength issues. They fail to interpret frequency components precisely and\noverlook temporal information. Accordingly, the novel Fourier Basis Mapping\n(FBM) method addresses these issues by integrating time-frequency features\nthrough Fourier basis expansion and mapping in the time-frequency space. Our\napproach extracts explicit frequency features while preserving temporal\ncharacteristics. FBM supports plug-and-play integration with various types of\nneural networks by only adjusting the first initial projection layer for better\nperformance. First, we propose FBM-L, FBM-NL, and FBM-NP to enhance linear,\nMLP-based, and Transformer-based models, respectively, demonstrating the\neffectiveness of time-frequency features. Next, we propose a synergetic model\narchitecture, termed FBM-S, which decomposes the seasonal, trend, and\ninteraction effects into three separate blocks, each designed to model\ntime-frequency features in a specialized manner. Finally, we introduce several\ntechniques tailored for time-frequency features, including interaction masking,\ncentralization, patching, rolling window projection, and multi-scale\ndown-sampling. The results are validated on diverse real-world datasets for\nboth long-term and short-term forecasting tasks with SOTA performance.", "comment": "18 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.09445v1", "cate": "cs.LG", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09862", "title": "SpeakerVid-5M: A Large-Scale High-Quality Dataset for Audio-Visual Dyadic Interactive Human Generation", "authors": ["Youliang Zhang", "Zhaoyang Li", "Duomin Wang", "Jiahe Zhang", "Deyu Zhou", "Zixin Yin", "Xili Dai", "Gang Yu", "Xiu Li"], "categories": ["cs.CV", "eess.AS"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09862v1", "summary": "The rapid development of large-scale models has catalyzed significant\nbreakthroughs in the digital human domain. These advanced methodologies offer\nhigh-fidelity solutions for avatar driving and rendering, leading academia to\nfocus on the next major challenge: audio-visual dyadic interactive virtual\nhuman. To facilitate research in this emerging area, we present SpeakerVid-5M\ndataset, the first large-scale, high-quality dataset designed for audio-visual\ndyadic interactive virtual human generation. Totaling over 8,743 hours,\nSpeakerVid-5M contains more than 5.2 million video clips of human portraits. It\ncovers diverse scales and interaction types, including monadic talking,\nlistening, and dyadic conversations. Crucially, the dataset is structured along\ntwo key dimensions: interaction type and data quality. First, it is categorized\ninto four types (dialogue branch, single branch, listening branch and\nmulti-turn branch) based on the interaction scenario. Second, it is stratified\ninto a large-scale pre-training subset and a curated, high-quality subset for\nSupervised Fine-Tuning (SFT). This dual structure accommodates a wide array of\n2D virtual human tasks. In addition, we provide an autoregressive (AR)-based\nvideo chat baseline trained on this data, accompanied by a dedicated set of\nmetrics and test data to serve as a benchmark VidChatBench for future work.\nBoth the dataset and the corresponding data processing code will be publicly\nreleased. Project page: https://dorniwang.github.io/SpeakerVid-5M/", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09862v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2503.04653", "title": "RadIR: A Scalable Framework for Multi-Grained Medical Image Retrieval via Radiology Report Mining", "authors": ["Tengfei Zhang", "Ziheng Zhao", "Chaoyi Wu", "Xiao Zhou", "Ya Zhang", "Yanfeng Wang", "Weidi Xie"], "categories": ["cs.CV", "cs.IR", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.04653v2", "summary": "Developing advanced medical imaging retrieval systems is challenging due to\nthe varying definitions of `similar images' across different medical contexts.\nThis challenge is compounded by the lack of large-scale, high-quality medical\nimaging retrieval datasets and benchmarks. In this paper, we propose a novel\nmethodology that leverages dense radiology reports to define image-wise\nsimilarity ordering at multiple granularities in a scalable and fully automatic\nmanner. Using this approach, we construct two comprehensive medical imaging\nretrieval datasets: MIMIC-IR for Chest X-rays and CTRATE-IR for CT scans,\nproviding detailed image-image ranking annotations conditioned on diverse\nanatomical structures. Furthermore, we develop two retrieval systems, RadIR-CXR\nand model-ChestCT, which demonstrate superior performance in traditional\nimage-image and image-report retrieval tasks. These systems also enable\nflexible, effective image retrieval conditioned on specific anatomical\nstructures described in text, achieving state-of-the-art results on 77 out of\n78 metrics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.04653v2", "cate": "cs.CV", "date": "2025-03-06", "updated": "2025-07-12"}
{"id": "2311.17349", "title": "A decoupled structure preserving scheme for the Poisson-Nernst-Planck Navier-Stokes equations and its error analysis", "authors": ["Ziyao Yu", "Qing Cheng", "Jie Shen", "Changyou Wang"], "categories": ["math.NA", "cs.NA", "math.AP"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2311.17349v2", "summary": "We consider in this paper a numerical approximation of\nPoisson-Nernst-Planck-Navier- Stokes (PNP-NS) system. We construct a decoupled\nsemi-discrete and fully discrete scheme that enjoys the properties of\npositivity preserving, mass conserving, and unconditionally energy stability.\nThen, we establish the well-posedness and regularity of the initial and\n(periodic) boundary value problem of the PNP-NS system under suitable\nassumptions on the initial data, and carry out a rigorous convergence analysis\nfor the fully discretized scheme. We also present some numerical results to\nvalidate the positivity-preserving property and the accuracy of our scheme.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2311.17349v2", "cate": "math.NA", "date": "2023-11-29", "updated": "2025-07-14"}
{"id": "2507.09372", "title": "Controllable joint noise reduction and hearing loss compensation using a differentiable auditory model", "authors": ["Philippe Gonzalez", "Torsten Dau", "Tobias May"], "categories": ["eess.AS", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Accepted to Clarity 2025 Workshop", "url": "http://arxiv.org/abs/2507.09372v1", "summary": "Deep learning-based hearing loss compensation (HLC) seeks to enhance speech\nintelligibility and quality for hearing impaired listeners using neural\nnetworks. One major challenge of HLC is the lack of a ground-truth target.\nRecent works have used neural networks to emulate non-differentiable auditory\nperipheral models in closed-loop frameworks, but this approach lacks\nflexibility. Alternatively, differentiable auditory models allow direct\noptimization, yet previous studies focused on individual listener profiles, or\njoint noise reduction (NR) and HLC without balancing each task. This work\nformulates NR and HLC as a multi-task learning problem, training a system to\nsimultaneously predict denoised and compensated signals from noisy speech and\naudiograms using a differentiable auditory model. Results show the system\nachieves similar objective metric performance to systems trained for each task\nseparately, while being able to adjust the balance between NR and HLC during\ninference.", "comment": "Accepted to Clarity 2025 Workshop", "pdf_url": "http://arxiv.org/pdf/2507.09372v1", "cate": "eess.AS", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09685", "title": "Symptom-Driven Personalized Proton Pump Inhibitors Therapy Using Bayesian Neural Networks and Model Predictive Control", "authors": ["Yutong Li", "Ilya Kolmanovsky"], "categories": ["eess.SY", "cs.LG", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      6 pages, 5 figures", "url": "http://arxiv.org/abs/2507.09685v1", "summary": "Proton Pump Inhibitors (PPIs) are the standard of care for gastric acid\ndisorders but carry significant risks when administered chronically at high\ndoses. Precise long-term control of gastric acidity is challenged by the\nimpracticality of invasive gastric acid monitoring beyond 72 hours and wide\ninter-patient variability. We propose a noninvasive, symptom-based framework\nthat tailors PPI dosing solely on patient-reported reflux and digestive symptom\npatterns. A Bayesian Neural Network prediction model learns to predict patient\nsymptoms and quantifies its uncertainty from historical symptom scores, meal,\nand PPIs intake data. These probabilistic forecasts feed a chance-constrained\nModel Predictive Control (MPC) algorithm that dynamically computes future PPI\ndoses to minimize drug usage while enforcing acid suppression with high\nconfidence - without any direct acid measurement. In silico studies over\ndiverse dietary schedules and virtual patient profiles demonstrate that our\nlearning-augmented MPC reduces total PPI consumption by 65 percent compared to\nstandard fixed regimens, while maintaining acid suppression with at least 95\npercent probability. The proposed approach offers a practical path to\npersonalized PPI therapy, minimizing treatment burden and overdose risk without\ninvasive sensors.", "comment": "6 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.09685v1", "cate": "eess.SY", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09460", "title": "Enhancing ALS Progression Tracking with Semi-Supervised ALSFRS-R Scores Estimated from Ambient Home Health Monitoring", "authors": ["Noah Marchal", "William E. Janes", "Mihail Popescu", "Xing Song"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      31 pages, 8 Figures", "url": "http://arxiv.org/abs/2507.09460v1", "summary": "Clinical monitoring of functional decline in ALS relies on periodic\nassessments that may miss critical changes occurring between visits. To address\nthis gap, semi-supervised regression models were developed to estimate rates of\ndecline in a case series cohort by targeting ALSFRS- R scale trajectories with\ncontinuous in-home sensor monitoring data. Our analysis compared three model\nparadigms (individual batch learning and cohort-level batch versus incremental\nfine-tuned transfer learning) across linear slope, cubic polynomial, and\nensembled self-attention pseudo-label interpolations. Results revealed cohort\nhomogeneity across functional domains responding to learning methods, with\ntransfer learning improving prediction error for ALSFRS-R subscales in 28 of 32\ncontrasts (mean RMSE=0.20(0.04)), and individual batch learning for predicting\nthe composite scale (mean RMSE=3.15(1.25)) in 2 of 3. Self-attention\ninterpolation achieved the lowest prediction error for subscale-level models\n(mean RMSE=0.19(0.06)), capturing complex nonlinear progression patterns,\noutperforming linear and cubic interpolations in 20 of 32 contrasts, though\nlinear interpolation proved more stable in all ALSFRS-R composite scale models\n(mean RMSE=0.23(0.10)). We identified distinct homogeneity-heterogeneity\nprofiles across functional domains with respiratory and speech exhibiting\npatient-specific patterns benefiting from personalized incremental adaptation,\nwhile swallowing and dressing functions followed cohort-level trajectories\nsuitable for transfer models. These findings suggest that matching learning and\npseudo-labeling techniques to functional domain-specific\nhomogeneity-heterogeneity profiles enhances predictive accuracy in ALS\nprogression tracking. Integrating adaptive model selection within sensor\nmonitoring platforms could enable timely interventions and scalable deployment\nin future multi-center studies.", "comment": "31 pages, 8 Figures", "pdf_url": "http://arxiv.org/pdf/2507.09460v1", "cate": "cs.LG", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09876", "title": "ViTCoT: Video-Text Interleaved Chain-of-Thought for Boosting Video Understanding in Large Language Models", "authors": ["Yongheng Zhang", "Xu Liu", "Ruihan Tao", "Qiguang Chen", "Hao Fei", "Wanxiang Che", "Libo Qin"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ACM MM 2025", "url": "http://arxiv.org/abs/2507.09876v1", "summary": "Video understanding plays a vital role in bridging low-level visual signals\nwith high-level cognitive reasoning, and is fundamental to applications such as\nautonomous driving, embodied AI, and the broader pursuit of AGI. The rapid\ndevelopment of large language models (LLMs), particularly those utilizing\nChain-of-Thought (CoT) technology, has significantly advanced video reasoning\ncapabilities. However, current approaches primarily depend on textual\ninformation for reasoning, overlooking the visual modality in the actual video\nreasoning process. In contrast, humans naturally re-examine visual content\nwhile reasoning. Motivated by this, we introduce a novel video reasoning\nparadigm: Video-Text Interleaved CoT (ViTCoT), which facilitates more intuitive\nand cognitively aligned reasoning. To the end, first, we construct the\nVideo-Text Interleaved Benchmark (ViTIB), which is created using MLLMs for\nkey-video selection and manually verified. Furthermore, we extensively explore\nthe potential of the ViTCoT paradigm in the video understanding field.\nExtensive experiments demonstrate that ViTCoT significantly enhances\nperformance compared to the traditional text-only CoT paradigm and effectively\nactivates more neuron values in MLLMs.", "comment": "Accepted by ACM MM 2025", "pdf_url": "http://arxiv.org/pdf/2507.09876v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2506.12981", "title": "SymRAG: Efficient Neuro-Symbolic Retrieval Through Adaptive Query Routing", "authors": ["Safayat Bin Hakim", "Muhammad Adil", "Alvaro Velasquez", "Houbing Herbert Song"], "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Accepted at 19th International Conference on Neurosymbolic Learning and Reasoning (NeSy 2025)", "url": "http://arxiv.org/abs/2506.12981v2", "summary": "Current Retrieval-Augmented Generation systems use uniform processing,\ncausing inefficiency as simple queries consume resources similar to complex\nmulti-hop tasks. We present SymRAG, a framework that introduces adaptive query\nrouting via real-time complexity and load assessment to select symbolic,\nneural, or hybrid pathways. SymRAG's neuro-symbolic approach adjusts\ncomputational pathways based on both query characteristics and system load,\nenabling efficient resource allocation across diverse query types. By combining\nlinguistic and structural query properties with system load metrics, SymRAG\nallocates resources proportional to reasoning requirements. Evaluated on 2,000\nqueries across HotpotQA (multi-hop reasoning) and DROP (discrete reasoning)\nusing Llama-3.2-3B and Mistral-7B models, SymRAG achieves competitive accuracy\n(97.6--100.0% exact match) with efficient resource utilization (3.6--6.2% CPU\nutilization, 0.985--3.165s processing). Disabling adaptive routing increases\nprocessing time by 169--1151%, showing its significance for complex models.\nThese results suggest adaptive computation strategies are more sustainable and\nscalable for hybrid AI systems that use dynamic routing and neuro-symbolic\nframeworks.", "comment": "Accepted at 19th International Conference on Neurosymbolic Learning\n  and Reasoning (NeSy 2025)", "pdf_url": "http://arxiv.org/pdf/2506.12981v2", "cate": "cs.AI", "date": "2025-06-15", "updated": "2025-07-12"}
{"id": "2407.10472", "title": "Convergence Analysis of the Alternating Anderson-Picard Method for Nonlinear Fixed-point Problems", "authors": ["Xue Feng", "M. Paul Laiu", "Thomas Strohmer"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2407.10472v2", "summary": "Anderson Acceleration (AA) has been widely used to solve nonlinear\nfixed-point problems due to its rapid convergence. This work focuses on a\nvariant of AA in which multiple Picard iterations are performed between each AA\nstep, referred to as the Alternating Anderson-Picard (AAP) method. Despite\nintroducing more ``slow'' Picard iterations, this method has been shown to be\nefficient and even more robust in both linear and nonlinear cases. However,\nthere is a lack of theoretical analysis for AAP in the nonlinear case. In this\npaper, we address this gap by establishing the equivalence between AAP and a\nmultisecant-GMRES method that uses GMRES to solve a multisecant linear system\nat each iteration. From this perspective, we show that AAP ``converges'' to the\nNewton-GMRES method. Specifically, as the residual approaches zero, the\nmultisecant matrix, the approximate Jacobian inverse, the search direction, and\nthe optimization gain of AAP converge to their counterparts in the Newton-GMRES\nmethod. These connections provide insights for analyzing the asymptotic\nconvergence properties of AAP. Consequently, we show that AAP is locally\n$q$-linear convergent and provide an upper bound for the convergence factor of\nAAP. To validate the theoretical results, numerical examples are provided.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2407.10472v2", "cate": "math.NA", "date": "2024-07-15", "updated": "2025-07-11"}
{"id": "2507.09499", "title": "The DKU System for Multi-Speaker Automatic Speech Recognition in MLC-SLM Challenge", "authors": ["Yuke Lin", "Ming Cheng", "Ze Li", "Ming Li"], "categories": ["eess.AS", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Technical Report for MLC-SLM Challenge in Interspeech2025", "url": "http://arxiv.org/abs/2507.09499v1", "summary": "We present the DKU system for Task 2 of the MLC-SLM Challenge, which aims to\nperform multi-speaker automatic speech recognition directly from raw audio\nwithout Oracle speaker labels or time boundaries. Our approach builds upon a\ndiarization-aware framework integrating speaker embeddings and temporal\nutterance boundaries into a Qwen2.5-based large language model (LLM). Then, we\nenhance the system's multilingual performance by fine-tuning language-specific\nadapters and LoRA modules within the LLM decoder. Finally, our system achieves\nthe tcpWER of 23.56\\% and 18.08\\% on the development and test sets of the\nMLC-SLM dataset, substantially outperforming the official baseline.", "comment": "Technical Report for MLC-SLM Challenge in Interspeech2025", "pdf_url": "http://arxiv.org/pdf/2507.09499v1", "cate": "eess.AS", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09726", "title": "Electric Vehicle Public Charging Equity Considerations: A Systematic Review", "authors": ["Boyou Chen", "Kaihan Zhang", "Austin Moore", "Bochen Jia", "Mengqiu Cao"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09726v1", "summary": "Public electric vehicle (EV) charging infrastructure is crucial for\naccelerating EV adoption and reducing transportation emissions; however,\ndisparities in infrastructure access have raised significant equity concerns.\nThis systematic review synthesizes existing knowledge and identifies gaps\nregarding equity in EV public charging research. Following structured review\nprotocols, 91 peer-reviewed studies from Scopus and Google Scholar were\nanalyzed, focusing explicitly on equity considerations. The findings indicate\nthat current research on EV public charging equity mainly adopted geographic\ninformation systems (GIS), network optimization, behavioral modeling, and\nhybrid analytical frameworks, yet lacks consistent normative frameworks for\nassessing equity outcomes. Equity assessments highlight four key dimensions:\nspatial accessibility, cost burdens, reliability and usability, and user\nawareness and trust. Socio-economic disparities, particularly income, housing\ntenure, and ethnicity, frequently exacerbate inequitable access,\ndisproportionately disadvantaging low-income, renter, and minority populations.\nAdditionally, infrastructure-specific choices, including charger reliability,\nstrategic location, and pricing strategies, significantly influence adoption\npatterns and equity outcomes. However, existing literature primarily reflects\nNorth American, European, and Chinese contexts, revealing substantial\ngeographical and methodological limitations. This review suggests the need for\nmore robust normative evaluations of equity, comprehensive demographic data\nintegration, and advanced methodological frameworks, thereby guiding targeted,\ninclusive, and context-sensitive infrastructure planning and policy\ninterventions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09726v1", "cate": "eess.SY", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09470", "title": "Enhancing Clinical Text Classification via Fine-Tuned DRAGON Longformer Models", "authors": ["Mingchuan Yang", "Ziyuan Huang"], "categories": ["cs.CL", "cs.AI", "68T07"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      29 pages, 5 tables", "url": "http://arxiv.org/abs/2507.09470v1", "summary": "This study explores the optimization of the DRAGON Longformer base model for\nclinical text classification, specifically targeting the binary classification\nof medical case descriptions. A dataset of 500 clinical cases containing\nstructured medical observations was used, with 400 cases for training and 100\nfor validation. Enhancements to the pre-trained\njoeranbosma/dragon-longformer-base-mixed-domain model included hyperparameter\ntuning, domain-specific preprocessing, and architectural adjustments. Key\nmodifications involved increasing sequence length from 512 to 1024 tokens,\nadjusting learning rates from 1e-05 to 5e-06, extending training epochs from 5\nto 8, and incorporating specialized medical terminology. The optimized model\nachieved notable performance gains: accuracy improved from 72.0% to 85.2%,\nprecision from 68.0% to 84.1%, recall from 75.0% to 86.3%, and F1-score from\n71.0% to 85.2%. Statistical analysis confirmed the significance of these\nimprovements (p < .001). The model demonstrated enhanced capability in\ninterpreting medical terminology, anatomical measurements, and clinical\nobservations. These findings contribute to domain-specific language model\nresearch and offer practical implications for clinical natural language\nprocessing applications. The optimized model's strong performance across\ndiverse medical conditions underscores its potential for broad use in\nhealthcare settings.", "comment": "29 pages, 5 tables", "pdf_url": "http://arxiv.org/pdf/2507.09470v1", "cate": "cs.CL", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09880", "title": "OpenHuman4D: Open-Vocabulary 4D Human Parsing", "authors": ["Keito Suzuki", "Bang Du", "Runfa Blark Li", "Kunyao Chen", "Lei Wang", "Peng Liu", "Ning Bi", "Truong Nguyen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09880v1", "summary": "Understanding dynamic 3D human representation has become increasingly\ncritical in virtual and extended reality applications. However, existing human\npart segmentation methods are constrained by reliance on closed-set datasets\nand prolonged inference times, which significantly restrict their\napplicability. In this paper, we introduce the first 4D human parsing framework\nthat simultaneously addresses these challenges by reducing the inference time\nand introducing open-vocabulary capabilities. Building upon state-of-the-art\nopen-vocabulary 3D human parsing techniques, our approach extends the support\nto 4D human-centric video with three key innovations: 1) We adopt mask-based\nvideo object tracking to efficiently establish spatial and temporal\ncorrespondences, avoiding the necessity of segmenting all frames. 2) A novel\nMask Validation module is designed to manage new target identification and\nmitigate tracking failures. 3) We propose a 4D Mask Fusion module, integrating\nmemory-conditioned attention and logits equalization for robust embedding\nfusion. Extensive experiments demonstrate the effectiveness and flexibility of\nthe proposed method on 4D human-centric parsing tasks, achieving up to 93.3%\nacceleration compared to the previous state-of-the-art method, which was\nlimited to parsing fixed classes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09880v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2506.16035", "title": "Vision-Guided Chunking Is All You Need: Enhancing RAG with Multimodal Document Understanding", "authors": ["Vishesh Tripathi", "Tanmay Odapally", "Indraneel Das", "Uday Allu", "Biddwan Ahmed"], "categories": ["cs.LG", "cs.AI", "cs.IR"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      11 pages, 1 Figure, 1 Table", "url": "http://arxiv.org/abs/2506.16035v2", "summary": "Retrieval-Augmented Generation (RAG) systems have revolutionized information\nretrieval and question answering, but traditional text-based chunking methods\nstruggle with complex document structures, multi-page tables, embedded figures,\nand contextual dependencies across page boundaries. We present a novel\nmultimodal document chunking approach that leverages Large Multimodal Models\n(LMMs) to process PDF documents in batches while maintaining semantic coherence\nand structural integrity. Our method processes documents in configurable page\nbatches with cross-batch context preservation, enabling accurate handling of\ntables spanning multiple pages, embedded visual elements, and procedural\ncontent. We evaluate our approach on a curated dataset of PDF documents with\nmanually crafted queries, demonstrating improvements in chunk quality and\ndownstream RAG performance. Our vision-guided approach achieves better accuracy\ncompared to traditional vanilla RAG systems, with qualitative analysis showing\nsuperior preservation of document structure and semantic coherence.", "comment": "11 pages, 1 Figure, 1 Table", "pdf_url": "http://arxiv.org/pdf/2506.16035v2", "cate": "cs.LG", "date": "2025-06-19", "updated": "2025-07-13"}
{"id": "2411.16430", "title": "Interface Energy and Phase Transformations: A Comparative Analysis of Cahn-Hilliard and CALPHAD-based Models in Ternary Substitutional Alloys", "authors": ["Wolfgang Flachberger", "Thomas Antretter", "Swaroop Gaddikere-Nagaraja", "Silvia Leitner", "Manuel Petersmann", "Jiri Svoboda"], "categories": ["math.NA", "cond-mat.mtrl-sci", "cs.NA", "physics.chem-ph"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      21 pages, 12 figures, preprint", "url": "http://arxiv.org/abs/2411.16430v2", "summary": "There are various methods for modeling phase transformations in materials\nscience, including general classes of phase-field methods and reactive\ndiffusion methodologies, which most importantly differ in their treatment of\ninterface energy. These methodologies appear mutually exclusive since the\nrespective numerical schemes only allow for their primary use case. To address\nthis issue, a novel methodology for modeling phase transformations in\nmulti-phase, multi-component systems, with particular emphasis on applications\nin materials science and the study of substitutional alloys is introduced. The\nfundamental role of interface energy in the evolution of a material's\nmorphology will be studied by example of binary and ternary systems. Allowing\nfull control over the interface energy quantity enables more detailed\ninvestigations and bridges the gaps between known methods. We prove the\nthermodynamic consistency of the derived method and discuss several use cases,\nsuch as vacancy-mediated diffusion. Furthermore a scheme for relating Onsager\nand Diffusion coefficients is proposed, which allows us to study the intricate\ncoupling that is observed in multicomponent systems. We hope to contribute to\nthe development of new mathematical tools for modeling complex phase\ntransformations in materials science.", "comment": "21 pages, 12 figures, preprint", "pdf_url": "http://arxiv.org/pdf/2411.16430v2", "cate": "math.NA", "date": "2024-11-25", "updated": "2025-07-14"}
{"id": "2507.09570", "title": "Enhancing Stereo Sound Event Detection with BiMamba and Pretrained PSELDnet", "authors": ["Wenmiao Gao", "Han Yin"], "categories": ["eess.AS", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09570v1", "summary": "Pre-training methods have greatly improved the performance of sound event\nlocalization and detection (SELD). However, existing Transformer-based models\nstill face high computational cost. To solve this problem, we present a stereo\nSELD system using a pre-trained PSELDnet and a bidirectional Mamba sequence\nmodel. Specifically, we replace the Conformer module with a BiMamba module. We\nalso use asymmetric convolutions to better capture the time and frequency\nrelationships in the audio signal. Test results on the DCASE2025 Task 3\ndevelopment dataset show that our method performs better than both the baseline\nand the original PSELDnet with a Conformer decoder. In addition, the proposed\nmodel costs fewer computing resources than the baselines. These results show\nthat the BiMamba architecture is effective for solving key challenges in SELD\ntasks. The source code is publicly accessible at https://github.com/\nalexandergwm/DCASE2025 TASK3 Stereo PSELD Mamba.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09570v1", "cate": "eess.AS", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09755", "title": "Optimal Power Management of Battery Energy Storage Systems via Ensemble Kalman Inversion", "authors": ["Amir Farakhor", "Iman Askari", "Di Wu", "Huazhen Fang"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09755v1", "summary": "Optimal power management of battery energy storage systems (BESS) is crucial\nfor their safe and efficient operation. Numerical optimization techniques are\nfrequently utilized to solve the optimal power management problems. However,\nthese techniques often fall short of delivering real-time solutions for\nlarge-scale BESS due to their computational complexity. To address this issue,\nthis paper proposes a computationally efficient approach. We introduce a new\nset of decision variables called power-sharing ratios corresponding to each\ncell, indicating their allocated power share from the output power demand. We\nthen formulate an optimal power management problem to minimize the system-wide\npower losses while ensuring compliance with safety, balancing, and power\nsupply-demand match constraints. To efficiently solve this problem, a\nparameterized control policy is designed and leveraged to transform the optimal\npower management problem into a parameter estimation problem. We then implement\nthe ensemble Kalman inversion to estimate the optimal parameter set. The\nproposed approach significantly reduces computational requirements due to 1)\nthe much lower dimensionality of the decision parameters and 2) the estimation\ntreatment of the optimal power management problem. Finally, we conduct\nextensive simulations to validate the effectiveness of the proposed approach.\nThe results show promise in accuracy and computation time compared with\nexplored numerical optimization techniques.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09755v1", "cate": "eess.SY", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09794", "title": "Joint Scheduling of Deferrable and Nondeferrable Demand with Colocated Stochastic Supply", "authors": ["Minjae Jeon", "Lang Tong", "Qing Zhao"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09794v1", "summary": "We address the problem of optimal joint scheduling of deferrable and\nnondeferrable demand involving colocated stochastic supply. Deferrable demand\ncan be delayed within its service deadline, whereas nondeferrable demand must\nbe scheduled immediately. Under a finite-horizon stochastic dynamic programming\nformulation, we show that the optimal scheduling policy is a ``procrastination\npolicy'' that delays scheduling as much as possible and is characterized by\nthree procrastination parameters. Exploiting the low-dimensional\nparameterization of the optimal policy, we propose a Procrastination Threshold\nReinforcement Learning algorithm. Numerical experiments based on real-world\ntest data confirm that the threshold-learning algorithm closely approximates\nthe optimal policy and outperforms standard benchmarks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09794v1", "cate": "eess.SY", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09477", "title": "Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs", "authors": ["Yangning Li", "Weizhi Zhang", "Yuyao Yang", "Wei-Chieh Huang", "Yaozu Wu", "Junyu Luo", "Yuanchen Bei", "Henry Peng Zou", "Xiao Luo", "Yusheng Zhao", "Chunkit Chan", "Yankai Chen", "Zhongfen Deng", "Yinghui Li", "Hai-Tao Zheng", "Dongyuan Li", "Renhe Jiang", "Ming Zhang", "Yangqiu Song", "Philip S. Yu"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      submitted to ARR May", "url": "http://arxiv.org/abs/2507.09477v1", "summary": "Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language\nModels (LLMs) by injecting external knowledge, yet it falls short on problems\nthat demand multi-step inference; conversely, purely reasoning-oriented\napproaches often hallucinate or mis-ground facts. This survey synthesizes both\nstrands under a unified reasoning-retrieval perspective. We first map how\nadvanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,\nwe show how retrieved knowledge of different type supply missing premises and\nexpand context for complex inference (RAG-Enhanced Reasoning). Finally, we\nspotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs\niteratively interleave search and reasoning to achieve state-of-the-art\nperformance across knowledge-intensive benchmarks. We categorize methods,\ndatasets, and open challenges, and outline research avenues toward deeper\nRAG-Reasoning systems that are more effective, multimodally-adaptive,\ntrustworthy, and human-centric. The collection is available at\nhttps://github.com/DavidZWZ/Awesome-RAG-Reasoning.", "comment": "submitted to ARR May", "pdf_url": "http://arxiv.org/pdf/2507.09477v1", "cate": "cs.CL", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09881", "title": "Counterfactual Visual Explanation via Causally-Guided Adversarial Steering", "authors": ["Yiran Qiao", "Disheng Liu", "Yiren Lu", "Yu Yin", "Mengnan Du", "Jing Ma"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09881v1", "summary": "Recent work on counterfactual visual explanations has contributed to making\nartificial intelligence models more explainable by providing visual\nperturbation to flip the prediction. However, these approaches neglect the\ncausal relationships and the spurious correlations behind the image generation\nprocess, which often leads to unintended alterations in the counterfactual\nimages and renders the explanations with limited quality. To address this\nchallenge, we introduce a novel framework CECAS, which first leverages a\ncausally-guided adversarial method to generate counterfactual explanations. It\ninnovatively integrates a causal perspective to avoid unwanted perturbations on\nspurious factors in the counterfactuals. Extensive experiments demonstrate that\nour method outperforms existing state-of-the-art approaches across multiple\nbenchmark datasets and ultimately achieves a balanced trade-off among various\naspects of validity, sparsity, proximity, and realism.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09881v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.07910", "title": "DTECT: Dynamic Topic Explorer & Context Tracker", "authors": ["Suman Adhya", "Debarshi Kumar Sanyal"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Code: this https URL | Demo: this https URL | Video: this https URL", "url": "http://arxiv.org/abs/2507.07910v2", "summary": "The explosive growth of textual data over time presents a significant\nchallenge in uncovering evolving themes and trends. Existing dynamic topic\nmodeling techniques, while powerful, often exist in fragmented pipelines that\nlack robust support for interpretation and user-friendly exploration. We\nintroduce DTECT (Dynamic Topic Explorer & Context Tracker), an end-to-end\nsystem that bridges the gap between raw textual data and meaningful temporal\ninsights. DTECT provides a unified workflow that supports data preprocessing,\nmultiple model architectures, and dedicated evaluation metrics to analyze the\ntopic quality of temporal topic models. It significantly enhances\ninterpretability by introducing LLM-driven automatic topic labeling, trend\nanalysis via temporally salient words, interactive visualizations with\ndocument-level summarization, and a natural language chat interface for\nintuitive data querying. By integrating these features into a single, cohesive\nplatform, DTECT empowers users to more effectively track and understand\nthematic dynamics. DTECT is open-source and available at\nhttps://github.com/AdhyaSuman/DTECT.", "comment": "Code: https://github.com/AdhyaSuman/DTECT | Demo:\n  https://huggingface.co/spaces/AdhyaSuman/DTECT | Video:\n  https://youtu.be/B8nNfxFoJAU", "pdf_url": "http://arxiv.org/pdf/2507.07910v2", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-12"}
{"id": "2502.10289", "title": "Investigation of the Estimation Accuracy of 5 Different Numerical ODE Solvers on 3 Case Studies", "authors": ["Hamidreza Moradi", "Hamideh Hossei"], "categories": ["math.NA", "cs.NA", "math.AG", "math.DG"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      15 pages , 3 figures", "url": "http://arxiv.org/abs/2502.10289v2", "summary": "Numerical ordinary differential equation (ODE) solvers are indispensable\ntools in various engineering domains, enabling the simulation and analysis of\ndynamic systems. In this work, we utilize 5 different numerical ODE solvers\nnamely: Euler's method, Heun's method, Midpoint Method, Runge-kutta 4th order\nand ODE45 method in order to discover the answer of three wellknown case\nstudies and compare their results by calculation of relative errors. To check\nfor the validity of the estimations, the experimental data of previous\nliterature have been compared with the data in this paper which shows a good\naccordance. We observe that for each of the case studies based on the behavior\nof the model, the estimation accuracy of the solvers is different. For the\nlogistic population change as the first case study, the results of all solvers\nare so close to each other that only their solution cost can be considered for\ntheir superiority. For temperature change of a building as the second case\nstudy we see that in some especial areas the accuracy of the solvers is\ndifferent and in general Midpoint ODE solver shows better results. As the last\ncase study, market equilibrium price shows that none of the numerical ODE\nsolvers can estimate its behavior which is due to its sudden changing nature.", "comment": "15 pages , 3 figures", "pdf_url": "http://arxiv.org/pdf/2502.10289v2", "cate": "math.NA", "date": "2025-02-14", "updated": "2025-07-11"}
{"id": "2507.09768", "title": "Knowing When to Quit: Probabilistic Early Exits for Speech Separation", "authors": ["Kenny Falkær Olsen. Mads Østergaard", "Karl Ulbæk", "Søren Føns Nielsen", "Rasmus Malik Høegh Lindrup", "Bjørn Sand Jensen", "Morten Mørup"], "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09768v1", "summary": "In recent years, deep learning-based single-channel speech separation has\nimproved considerably, in large part driven by increasingly compute- and\nparameter-efficient neural network architectures. Most such architectures are,\nhowever, designed with a fixed compute and parameter budget, and consequently\ncannot scale to varying compute demands or resources, which limits their use in\nembedded and heterogeneous devices such as mobile phones and hearables. To\nenable such use-cases we design a neural network architecture for speech\nseparation capable of early-exit, and we propose an uncertainty-aware\nprobabilistic framework to jointly model the clean speech signal and error\nvariance which we use to derive probabilistic early-exit conditions in terms of\ndesired signal-to-noise ratios. We evaluate our methods on both speech\nseparation and enhancement tasks, and we show that a single early-exit model\ncan be competitive with state-of-the-art models trained at many compute and\nparameter budgets. Our framework enables fine-grained dynamic compute-scaling\nof speech separation networks while achieving state-of-the-art performance and\ninterpretable exit conditions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09768v1", "cate": "cs.LG", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09864", "title": "Intersection of Reinforcement Learning and Bayesian Optimization for Intelligent Control of Industrial Processes: A Safe MPC-based DPG using Multi-Objective BO", "authors": ["Hossein Nejatbakhsh Esfahani", "Javad Mohammadpour Velni"], "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY", "math.OC"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09864v1", "summary": "Model Predictive Control (MPC)-based Reinforcement Learning (RL) offers a\nstructured and interpretable alternative to Deep Neural Network (DNN)-based RL\nmethods, with lower computational complexity and greater transparency. However,\nstandard MPC-RL approaches often suffer from slow convergence, suboptimal\npolicy learning due to limited parameterization, and safety issues during\nonline adaptation. To address these challenges, we propose a novel framework\nthat integrates MPC-RL with Multi-Objective Bayesian Optimization (MOBO). The\nproposed MPC-RL-MOBO utilizes noisy evaluations of the RL stage cost and its\ngradient, estimated via a Compatible Deterministic Policy Gradient (CDPG)\napproach, and incorporates them into a MOBO algorithm using the Expected\nHypervolume Improvement (EHVI) acquisition function. This fusion enables\nefficient and safe tuning of the MPC parameters to achieve improved closed-loop\nperformance, even under model imperfections. A numerical example demonstrates\nthe effectiveness of the proposed approach in achieving sample-efficient,\nstable, and high-performance learning for control systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09864v1", "cate": "eess.SY", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09938", "title": "A Case Study on Data Acquisition Systems: Relevance to Renewable Energy Technologies", "authors": ["Chito A. Petilla"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09938v1", "summary": "Multiple advantages had been identified with the integration of data\nacquisition into any existing system configuration and implementation. Using\ndata acquisition as a support into a monitoring system has not only improved\nits overall performance and reliability but also lowered its operational and\nmaintenance cost because of its real-time data collection from node sensors.\n  As renewable energy needs to be sustainable for it to fully support the\nenergy demand of communities, its management and control still needs to be\nimproved and enhanced. Smart systems are considered the next generation\ntechnological improvement of any system that exists. It is the prelude to\nautonomous systems from industrial applications to home automation. Data\nacquisition is only a part of these smart systems that help in the remote\nmanagement and control of these devices. Remote monitoring functionality\nenhances the operation and reliability which help in making proactive decisions\nduring critical situations and circumstances.\n  Even with data acquisition enhancements, there is still room for improving\nits implementation regarding data security and privacy and accuracy of\ninformation being exchanged between nodes. Current technological advancements\nhave already shown promising results and have widen its utilization spectrum by\ncovering almost any field of specialization. With increasing implementation and\ndesign complexity that comes with its enhancements, challenges and issues are\nalso faced that needs to be addressed and considered to mitigate the effects of\nsuch.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09938v1", "cate": "eess.SY", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09523", "title": "An Analysis of Action-Value Temporal-Difference Methods That Learn State Values", "authors": ["Brett Daley", "Prabhat Nagarajan", "Martha White", "Marlos C. Machado"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Published at RLC/RLJ 2025", "url": "http://arxiv.org/abs/2507.09523v1", "summary": "The hallmark feature of temporal-difference (TD) learning is bootstrapping:\nusing value predictions to generate new value predictions. The vast majority of\nTD methods for control learn a policy by bootstrapping from a single\naction-value function (e.g., Q-learning and Sarsa). Significantly less\nattention has been given to methods that bootstrap from two asymmetric value\nfunctions: i.e., methods that learn state values as an intermediate step in\nlearning action values. Existing algorithms in this vein can be categorized as\neither QV-learning or AV-learning. Though these algorithms have been\ninvestigated to some degree in prior work, it remains unclear if and when it is\nadvantageous to learn two value functions instead of just one -- and whether\nsuch approaches are theoretically sound in general. In this paper, we analyze\nthese algorithmic families in terms of convergence and sample efficiency. We\nfind that while both families are more efficient than Expected Sarsa in the\nprediction setting, only AV-learning methods offer any major benefit over\nQ-learning in the control setting. Finally, we introduce a new AV-learning\nalgorithm called Regularized Dueling Q-learning (RDQ), which significantly\noutperforms Dueling DQN in the MinAtar benchmark.", "comment": "Published at RLC/RLJ 2025", "pdf_url": "http://arxiv.org/pdf/2507.09523v1", "cate": "cs.LG", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09885", "title": "MCGA: Mixture of Codebooks Hyperspectral Reconstruction via Grayscale-Aware Attention", "authors": ["Zhanjiang Yang", "Lijun Sun", "Jiawei Dong", "Xiaoxin An", "Yang Liu", "Meng Li"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09885v1", "summary": "Reconstructing hyperspectral images (HSI) from RGB images is a cost-effective\nsolution for various vision-based applications. However, most existing\nlearning-based hyperspectral reconstruction methods directly learn the\nRGB-to-HSI mapping using complex attention mechanisms, neglecting the inherent\nchallenge of transitioning from low-dimensional to high-dimensional\ninformation. To address this limitation, we propose a two-stage approach, MCGA,\nwhich first learns spectral patterns before estimating the mapping. In the\nfirst stage, a multi-scale VQ-VAE learns representations from heterogeneous HSI\ndatasets, extracting a Mixture of Codebooks (MoC). In the second stage, the\nRGB-to-HSI mapping is refined by querying features from the MoC to replace\nlatent HSI representations, incorporating prior knowledge rather than forcing a\ndirect high-dimensional transformation. To further enhance reconstruction\nquality, we introduce Grayscale-Aware Attention and Quantized Self-Attention,\nwhich adaptively adjust feature map intensities to meet hyperspectral\nreconstruction requirements. This physically motivated attention mechanism\nensures lightweight and efficient HSI recovery. Moreover, we propose an\nentropy-based Test-Time Adaptation strategy to improve robustness in real-world\nscenarios. Extensive experiments demonstrate that our method, MCGA, achieves\nstate-of-the-art performance. The code and models will be released at\nhttps://github.com/Fibonaccirabbit/MCGA", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09885v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2504.21364", "title": "Uniform-in-time weak error estimates of explicit full-discretization schemes for SPDEs with non-globally Lipschitz coefficients", "authors": ["Yingsong Jiang", "Xiaojie Wang"], "categories": ["math.NA", "cs.NA", "60H35, 60H15, 65C30"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.21364v2", "summary": "This article is devoted to long-time weak approximations of stochastic\npartial differential equations (SPDEs) evolving in a bounded domain\n$\\mathcal{D} \\subset \\mathbb{R}^d$, $d \\leq 3$, with non-globally Lipschitz and\npossibly non-contractive coefficients. Both the space-time white noise ($d=1$)\nand the trace-class noise in multiple dimensions $d=2,3$ are examined for the\nconsidered SPDEs. Based on a spectral Galerkin spatial semi-discretization, we\npropose a class of novel full-discretization schemes of exponential type, which\nare explicit, easily implementable and preserve the ergodicity of the original\ndissipative SPDEs with possibly non-contractive coefficients. The\nuniform-in-time weak approximation errors are carefully analyzed in a low\nregularity and non-contractive setting, with uniform-in-time weak convergence\nrates obtained. A key ingredient is to establish the uniform-in-time moment\nbounds (in $L^{4q-2}$-norm, $q \\geq 1$) for the proposed fully discrete schemes\nin a super-linear setting. This is highly non-trivial for the explicit\nfull-discretization schemes and new arguments are elaborated by fully\nexploiting a contractive property of the semi-group in $L^{4q-2}$, the\ndissipativity of the nonlinearity and the particular benefit of the taming\nstrategy. Numerical experiments are finally reported to verify the theoretical\nfindings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.21364v2", "cate": "math.NA", "date": "2025-04-30", "updated": "2025-07-14"}
{"id": "2507.09806", "title": "Low-Rank Adaptation of Deep Prior Neural Networks For Room Impulse Response Reconstruction", "authors": ["Mirco Pezzoli", "Federico Miotello", "Shoichi Koyama", "Fabio Antonacci"], "categories": ["eess.AS", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      to appear in IEEE WASPAA", "url": "http://arxiv.org/abs/2507.09806v1", "summary": "The Deep Prior framework has emerged as a powerful generative tool which can\nbe used for reconstructing sound fields in an environment from few sparse\npressure measurements. It employs a neural network that is trained solely on a\nlimited set of available data and acts as an implicit prior which guides the\nsolution of the underlying optimization problem. However, a significant\nlimitation of the Deep Prior approach is its inability to generalize to new\nacoustic configurations, such as changes in the position of a sound source. As\na consequence, the network must be retrained from scratch for every new setup,\nwhich is both computationally intensive and time-consuming. To address this, we\ninvestigate transfer learning in Deep Prior via Low-Rank Adaptation (LoRA),\nwhich enables efficient fine-tuning of a pre-trained neural network by\nintroducing a low-rank decomposition of trainable parameters, thus allowing the\nnetwork to adapt to new measurement sets with minimal computational overhead.\nWe embed LoRA into a MultiResUNet-based Deep Prior model and compare its\nadaptation performance against full fine-tuning of all parameters as well as\nclassical retraining, particularly in scenarios where only a limited number of\nmicrophones are used. The results indicate that fine-tuning, whether done\ncompletely or via LoRA, is especially advantageous when the source location is\nthe sole changing parameter, preserving high physical fidelity, and\nhighlighting the value of transfer learning for acoustics applications.", "comment": "to appear in IEEE WASPAA", "pdf_url": "http://arxiv.org/pdf/2507.09806v1", "cate": "eess.AS", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09960", "title": "Efficient RF Chain Selection for MIMO Integrated Sensing and Communications: A Greedy Approach", "authors": ["Subin Shin", "Seongkyu Jung", "Jinseok Choi", "Jeonghun Park"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09960v1", "summary": "In multiple-input multiple-output integrated sensing and communication (MIMO\nISAC) systems, radio frequency chain (i.e., RF chain) selection plays a vital\nrole in reducing hardware cost, power consumption, and computational\ncomplexity. However, designing an effective RF chain selection strategy is\nchallenging due to the disparity in performance metrics between communication\nand sensing-mutual information (MI) versus beam-pattern mean-squared error\n(MSE) or the Cram\\'er-Rao lower bound (CRLB). To overcome this, we propose a\nlow-complexity greedy RF chain selection framework maximizing a unified\nMI-based performance metric applicable to both functions. By decomposing the\ntotal MI into individual contributions of each RF chain, we introduce two\napproaches: greedy eigen-based selection (GES) and greedy cofactor-based\nselection (GCS), which iteratively identify and remove the RF chains with the\nlowest contribution. We further extend our framework to beam selection for\nbeamspace MIMO ISAC systems, introducing diagonal beam selection (DBS) as a\nsimplified solution. Simulation results show that our proposed methods achieve\nnear-optimal performance with significantly lower complexity than exhaustive\nsearch, demonstrating their practical effectiveness for MIMO ISAC systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09960v1", "cate": "eess.SY", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09997", "title": "Predictive & Trust-based Multi-Agent Coordination", "authors": ["Venkatraman Renganathan", "Sabyasachi Mondal", "Antonios Tsourdos"], "categories": ["eess.SY", "cs.SY", "math.OC"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09997v1", "summary": "This paper presents a trust-based predictive multi-agent consensus protocol\nthat analyses neighbours' anticipation data and makes coordination decisions.\nAgents in the network share their future predicted data over a finite\nlook-ahead horizon with their neighbours and update their predictions in a\nrolling-horizon fashion. The prediction data is then used by agents to learn\nboth the trust and the commitment traits exhibited by their neighbours over\ntime. The proposed protocol is named as the Anticipatory Distributed\nCoordination (ADC) protocol. Lyapunov theory-based agreement convergence\nbetween agents is provided, followed by demonstrations using numerical\nsimulations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09997v1", "cate": "eess.SY", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09592", "title": "THOR: Transformer Heuristics for On-Demand Retrieval", "authors": ["Isaac Shi", "Zeyuan Li", "Fan Liu", "Wenli Wang", "Lewei He", "Yang Yang", "Tianyu Shi"], "categories": ["cs.DB", "cs.AI"], "primary_category": "Subjects:       Databases (cs.DB)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09592v1", "summary": "We introduce the THOR (Transformer Heuristics for On-Demand Retrieval)\nModule, designed and implemented by eSapiens, a secure, scalable engine that\ntransforms natural-language questions into verified, read-only SQL analytics\nfor enterprise databases. The Text-to-SQL module follows a decoupled\norchestration/execution architecture: a Supervisor Agent routes queries, Schema\nRetrieval dynamically injects table and column metadata, and a SQL Generation\nAgent emits single-statement SELECT queries protected by a read-only guardrail.\nAn integrated Self-Correction & Rating loop captures empty results, execution\nerrors, or low-quality outputs and triggers up to five LLM-driven regeneration\nattempts. Finally, a Result Interpretation Agent produces concise,\nhuman-readable insights and hands raw rows to the Insight & Intelligence engine\nfor visualization or forecasting.\n  Smoke tests across finance, sales, and operations scenarios demonstrate\nreliable ad-hoc querying and automated periodic reporting. By embedding schema\nawareness, fault-tolerant execution, and compliance guardrails, the THOR Module\nempowers non-technical users to access live data with zero-SQL simplicity and\nenterprise-grade safety.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09592v1", "cate": "cs.DB", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09896", "title": "Measuring the Impact of Rotation Equivariance on Aerial Object Detection", "authors": ["Xiuyu Wu", "Xinhao Wang", "Xiubin Zhu", "Lan Yang", "Jiyuan Liu", "Xingchen Hu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2507.09896v1", "summary": "Due to the arbitrary orientation of objects in aerial images, rotation\nequivariance is a critical property for aerial object detectors. However,\nrecent studies on rotation-equivariant aerial object detection remain scarce.\nMost detectors rely on data augmentation to enable models to learn\napproximately rotation-equivariant features. A few detectors have constructed\nrotation-equivariant networks, but due to the breaking of strict rotation\nequivariance by typical downsampling processes, these networks only achieve\napproximately rotation-equivariant backbones. Whether strict rotation\nequivariance is necessary for aerial image object detection remains an open\nquestion. In this paper, we implement a strictly rotation-equivariant backbone\nand neck network with a more advanced network structure and compare it with\napproximately rotation-equivariant networks to quantitatively measure the\nimpact of rotation equivariance on the performance of aerial image detectors.\nAdditionally, leveraging the inherently grouped nature of rotation-equivariant\nfeatures, we propose a multi-branch head network that reduces the parameter\ncount while improving detection accuracy. Based on the aforementioned\nimprovements, this study proposes the Multi-branch head rotation-equivariant\nsingle-stage Detector (MessDet), which achieves state-of-the-art performance on\nthe challenging aerial image datasets DOTA-v1.0, DOTA-v1.5 and DIOR-R with an\nexceptionally low parameter count.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.09896v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2505.00838", "title": "A stabilized march approach to adjoint-based sensitivity analysis of chaotic flows", "authors": ["Pranshul Thakur", "Siva Nadarajah"], "categories": ["math.NA", "cs.NA", "math.OC", "34A34, 37A99, 37D20, 37D45, 37N30, 46N40, 65P99, 76F20"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      26 pages, article. Changes in replacement: typos fixed", "url": "http://arxiv.org/abs/2505.00838v2", "summary": "Adjoint-based sensitivity analysis is of interest in computational science\ndue to its ability to compute sensitivities at a lower cost with respect to\nseveral design parameters. However, conventional sensitivity analysis methods\nfail in the presence of chaotic flows. Popular approaches to chaotic\nsensitivity analysis of flows involve the use of the shadowing trajectory. The\nstate-of-the-art approach computes the shadowing trajectory by solving a least\nsquares minimization problem, resulting in a space-time linear system of\nequations. The current paper computes the adjoint shadowing trajectory using\nthe stabilized march, by specifying the adjoint boundary conditions instead of\nsolving a minimization problem. This approach results in a space-time linear\nsystem that can be solved through a single backward substitution of order\n$\\mathcal{O}(n_u^2)$ with $n_u$ being the dimension of the unstable subspace.\nIt is proven to compute sensitivities that converge to the true sensitivity for\nlarge integration times and that the error in the sensitivity due to the\ndiscretization is of the order of the local truncation error of the scheme. The\napproach is numerically verified on the Lorentz 63 and Kuramoto-Sivasinsky\nequations.", "comment": "26 pages, article. Changes in replacement: typos fixed", "pdf_url": "http://arxiv.org/pdf/2505.00838v2", "cate": "math.NA", "date": "2025-05-01", "updated": "2025-07-13"}
{"id": "2507.09834", "title": "Generative Audio Language Modeling with Continuous-valued Tokens and Masked Next-Token Prediction", "authors": ["Shu-wen Yang", "Byeonggeun Kim", "Kuan-Po Huang", "Qingming Tang", "Huy Phan", "Bo-Ru Lu", "Harsha Sundar", "Shalini Ghosh", "Hung-yi Lee", "Chieh-Chi Kao", "Chao Wang"], "categories": ["eess.AS", "cs.CV", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Accepted by ICML 2025. Project website: this https URL", "url": "http://arxiv.org/abs/2507.09834v1", "summary": "Autoregressive next-token prediction with the Transformer decoder has become\na de facto standard in large language models (LLMs), achieving remarkable\nsuccess in Natural Language Processing (NLP) at scale. Extending this paradigm\nto audio poses unique challenges due to its inherently continuous nature. We\nresearch audio generation with a causal language model (LM) without discrete\ntokens. We leverage token-wise diffusion to model the continuous distribution\nof the next continuous-valued token. Our approach delivers significant\nimprovements over previous discrete solution, AudioGen, achieving 20% and 40%\nrelative gains on AudioCaps in Frechet Audio Distance (FAD) and\nKullback-Leibler (KL) divergence, respectively. Additionally, we propose a\nnovel masked next-token prediction task that incorporates masked prediction\ninto the causal LM framework. On AudioCaps, the innovation yields 41% and 33%\nrelative FAD improvements over AudioGen Base (285M) and AudioGen Large (1B)\nmodels, respectively, and is on par with the state-of-the-art (SOTA) diffusion\nmodels. Furthermore, we achieve these results with significantly fewer\nparameters -- 193M for our Base and 462M for our Large models.", "comment": "Accepted by ICML 2025. Project website: https://audiomntp.github.io/", "pdf_url": "http://arxiv.org/pdf/2507.09834v1", "cate": "eess.AS", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10004", "title": "Hardware test and validation of the angular droop control: Analysis and experiments", "authors": ["Taouba Jouini", "Jan Wachter", "Sophie An", "Veit Hagenmeyer"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10004v1", "summary": "The angular droop control is a grid-forming control strategy that exploits\nthe idea of power-to-angle droop to achieve exact frequency synchronization\nwith no stringent separation between primary and secondary frequency control.\nIn this work, we conduct hardware experiments in the Smart Energy System\nControl Laboratory at Karlsruhe Institute of Technology (KIT) to test and\nvalidate the angular droop control for low voltage power grids in two different\ntest scenarios. First, we verify its grid-forming capabilities after a major\nevent, e.g., following a blackout, demonstrated via power-to-angle droop\nbehavior. For this, we propose two implementation schemes that rely either on\ndirect or indirect actuation of the modulation signal and draw a comparison\nbetween them. Second, we investigate the plug-and-play capabilities, i.e.,\nlocal stability and power sharing for a two-converter system and provide\nsuitable tuning for the control gains. Our experimental findings illustrate the\nusefulness of hardware test and validation for DC/AC converter control, the\npractical challenges entailed and the proposed remedies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10004v1", "cate": "eess.SY", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10010", "title": "Probabilistic Robustness in the Gap Metric", "authors": ["Venkatraman Renganathan"], "categories": ["eess.SY", "cs.SY", "math.OC"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10010v1", "summary": "Uncertainties influencing the dynamical systems pose a significant challenge\nin estimating the achievable performance of a controller aiming to control such\nuncertain systems. When the uncertainties are of stochastic nature, obtaining\nhard guarantees for the robustness of a controller aiming to hedge against the\nuncertainty is not possible. This issue set the platform for the development of\nprobabilistic robust control approaches. In this work, we utilise the gap\nmetric between the known nominal model and the unknown perturbed model of the\nuncertain system as a tool to gauge the robustness of a controller and\nformulate the gap as a random variable in the setting with stochastic\nuncertainties. Main results of this paper includes giving probabilistic bound\non the gap exceeding a known threshold followed by bounds on the expected gap\nvalue and probabilistic robust stability in terms of the gap metric. Further,\nwe also provide a probabilistic controller performance certification under gap\nuncertainty and probabilistic guarantee on the achievable\n$\\mathcal{H}_{\\infty}$ robustness. Numerical simulations are provided at many\nplaces to demonstrate the proposed approach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10010v1", "cate": "eess.SY", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.08974", "title": "Domain Adaptation-Enabled Realistic Map-Based Channel Estimation for MIMO-OFDM", "authors": ["Thien Hieu Hoang", "Tri Nhu Do", "Georges Kaddoum"], "categories": ["eess.SP", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08974v1", "summary": "Accurate channel estimation is crucial for the improvement of signal\nprocessing performance in wireless communications. However, traditional\nmodel-based methods frequently experience difficulties in dynamic environments.\nSimilarly, alternative machine-learning approaches typically lack\ngeneralization across different datasets due to variations in channel\ncharacteristics. To address this issue, in this study, we propose a novel\ndomain adaptation approach to bridge the gap between the quasi-static channel\nmodel (QSCM) and the map-based channel model (MBCM). Specifically, we first\nproposed a channel estimation pipeline that takes into account realistic\nchannel simulation to train our foundation model. Then, we proposed domain\nadaptation methods to address the estimation problem. Using simulation-based\ntraining to reduce data requirements for effective application in practical\nwireless environments, we find that the proposed strategy enables robust model\nperformance, even with limited true channel information.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08974v1", "cate": "eess.SP", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09601", "title": "NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of Finance", "authors": ["Hanwool Lee", "Sara Yu", "Yewon Hwang", "Jonghyun Choi", "Heejae Ahn", "Sungbum Jung", "Youngjae Yu"], "categories": ["cs.CL", "cs.AI", "q-fin.CP"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Under Review", "url": "http://arxiv.org/abs/2507.09601v1", "summary": "General-purpose sentence embedding models often struggle to capture\nspecialized financial semantics, especially in low-resource languages like\nKorean, due to domain-specific jargon, temporal meaning shifts, and misaligned\nbilingual vocabularies. To address these gaps, we introduce NMIXX (Neural\neMbeddings for Cross-lingual eXploration of Finance), a suite of cross-lingual\nembedding models fine-tuned with 18.8K high-confidence triplets that pair\nin-domain paraphrases, hard negatives derived from a semantic-shift typology,\nand exact Korean-English translations. Concurrently, we release KorFinSTS, a\n1,921-pair Korean financial STS benchmark spanning news, disclosures, research\nreports, and regulations, designed to expose nuances that general benchmarks\nmiss.\n  When evaluated against seven open-license baselines, NMIXX's multilingual\nbge-m3 variant achieves Spearman's rho gains of +0.10 on English FinSTS and\n+0.22 on KorFinSTS, outperforming its pre-adaptation checkpoint and surpassing\nother models by the largest margin, while revealing a modest trade-off in\ngeneral STS performance. Our analysis further shows that models with richer\nKorean token coverage adapt more effectively, underscoring the importance of\ntokenizer design in low-resource, cross-lingual settings. By making both models\nand the benchmark publicly available, we provide the community with robust\ntools for domain-adapted, multilingual representation learning in finance.", "comment": "Under Review", "pdf_url": "http://arxiv.org/pdf/2507.09601v1", "cate": "cs.CL", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09910", "title": "IGD: Instructional Graphic Design with Multimodal Layer Generation", "authors": ["Yadong Qu", "Shancheng Fang", "Yuxin Wang", "Xiaorui Wang", "Zhineng Chen", "Hongtao Xie", "Yongdong Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025", "url": "http://arxiv.org/abs/2507.09910v1", "summary": "Graphic design visually conveys information and data by creating and\ncombining text, images and graphics. Two-stage methods that rely primarily on\nlayout generation lack creativity and intelligence, making graphic design still\nlabor-intensive. Existing diffusion-based methods generate non-editable graphic\ndesign files at image level with poor legibility in visual text rendering,\nwhich prevents them from achieving satisfactory and practical automated graphic\ndesign. In this paper, we propose Instructional Graphic Designer (IGD) to\nswiftly generate multimodal layers with editable flexibility with only natural\nlanguage instructions. IGD adopts a new paradigm that leverages parametric\nrendering and image asset generation. First, we develop a design platform and\nestablish a standardized format for multi-scenario design files, thus laying\nthe foundation for scaling up data. Second, IGD utilizes the multimodal\nunderstanding and reasoning capabilities of MLLM to accomplish attribute\nprediction, sequencing and layout of layers. It also employs a diffusion model\nto generate image content for assets. By enabling end-to-end training, IGD\narchitecturally supports scalability and extensibility in complex graphic\ndesign tasks. The superior experimental results demonstrate that IGD offers a\nnew solution for graphic design.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.09910v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2505.04370", "title": "Fast Bellman algorithm for real Monge-Ampere equation", "authors": ["Aleksandra Le", "Frank Wikström"], "categories": ["math.NA", "cs.NA", "math.AP", "Primary 65N06, Secondary 35J60, 35J96, 65N12"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.04370v2", "summary": "In this paper, we introduce a new numerical algorithm for solving the\nDirichlet problem for the real Monge--Ampere equation. The idea is to represent\nthe non-linear Monge--Ampere operator as an infimum of a class of linear\nelliptic operators and use Bellman's principle to construct a numeric scheme\nfor approximating the operator attaining this infimum.\n  Moreover, we prove convergence of the proposed algorithm (under suitable\ntechnical assumptions) and discuss its strengths and weaknesses. We also\ndemonstrate the performance of the method on several examples with various\ndegrees of regularity and degeneracy and compare the results to two existing\nmethods. Our method runs considerably faster than the ones used for comparison,\nimproving the running time by a factor of 3--10 for smooth, strictly convex\nexamples, and by a factor of 20--100 or more for mildly degenerate examples.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.04370v2", "cate": "math.NA", "date": "2025-05-07", "updated": "2025-07-14"}
{"id": "2507.10109", "title": "DualDub: Video-to-Soundtrack Generation via Joint Speech and Background Audio Synthesis", "authors": ["Wenjie Tian", "Xinfa Zhu", "Haohe Liu", "Zhixian Zhao", "Zihao Chen", "Chaofan Ding", "Xinhan Di", "Junjie Zheng", "Lei Xie"], "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Multimedia (cs.MM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10109v1", "summary": "While recent video-to-audio (V2A) models can generate realistic background\naudio from visual input, they largely overlook speech, an essential part of\nmany video soundtracks. This paper proposes a new task, video-to-soundtrack\n(V2ST) generation, which aims to jointly produce synchronized background audio\nand speech within a unified framework. To tackle V2ST, we introduce DualDub, a\nunified framework built on a multimodal language model that integrates a\nmultimodal encoder, a cross-modal aligner, and dual decoding heads for\nsimultaneous background audio and speech generation. Specifically, our proposed\ncross-modal aligner employs causal and non-causal attention mechanisms to\nimprove synchronization and acoustic harmony. Besides, to handle data scarcity,\nwe design a curriculum learning strategy that progressively builds the\nmultimodal capability. Finally, we introduce DualBench, the first benchmark for\nV2ST evaluation with a carefully curated test set and comprehensive metrics.\nExperimental results demonstrate that DualDub achieves state-of-the-art\nperformance, generating high-quality and well-synchronized soundtracks with\nboth speech and background audio.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10109v1", "cate": "cs.MM", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10011", "title": "Survey on Methods for Detection, Classification and Location of Faults in Power Systems Using Artificial Intelligence", "authors": ["Juan A. Martinez-Velasco", "Alexandre Serrano-Fontova", "Ricard Bosch-Tous", "Pau Casals-Torrens"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10011v1", "summary": "Components of electrical power systems are susceptible to failures caused by\nlightning strikes, aging or human errors. These faults can cause equipment\ndamage, affect system reliability, and results in expensive repair costs. As\nelectric power systems are becoming more complex, traditional protection\nmethods face limitations and shortcomings. Faults in power systems can occur at\nanytime and anywhere, can be caused by a natural disaster or an accident, and\ntheir occurrence can be hardly predicted or avoided; therefore, it is crucial\nto accurately estimate the fault location and quickly restore service. The\ndevelopment of methods capable of accurately detecting, locating and removing\nfaults is essential (i.e. fast isolation of faults is necessary to maintain the\nsystem stability at transmission levels; accurate and fast detection and\nlocation of faults are essential for increasing reliability and customer\nsatisfaction at distribution levels). This has motivated the development of new\nand more efficient methods. Methods developed to detect and locate faults in\npower systems can be divided into two categories, conventional and artificial\nintelligence-based techniques. Although the utilization of artificial\nintelligence (AI) techniques offer tremendous potential, they are challenging\nand time consuming (i.e. many AI techniques require training data for\nprocessing). This paper presents a survey of the application of AI techniques\nto fault diagnosis (detection, classification and location of faults) of lines\nand cables of power systems at both transmission and distribution levels. The\npaper provides a short introduction to AI concepts, a brief summary of the\napplication of AI techniques to power system analysis and design, and a\ndiscussion on AI-based fault diagnosis methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10011v1", "cate": "eess.SY", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10123", "title": "Optimal Battery Placement in Power Grid", "authors": ["Ruotong Sun", "Ermin Wei", "Lihui Yi"], "categories": ["eess.SY", "cs.SY", "math.OC"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      10 pages, 2 figures", "url": "http://arxiv.org/abs/2507.10123v1", "summary": "We study the optimal placement of an unlimited-capacity battery in power\ngrids under a centralized market model, where the independent system operator\n(ISO) aims to minimize total generation costs through load shifting. The\noptimal battery placement is not well understood by the existing literature,\nespecially regarding the influence of network topology on minimizing generation\ncosts. Our work starts with decomposing the Mixed-Integer Linear Programming\n(MILP) problem into a series of Linear Programming (LP) formulations. For power\ngrids with sufficiently large generation capacity or tree topologies, we derive\nanalytical cost expressions demonstrating that, under reasonable assumptions,\nthe weighted degree is the only topological factor for optimal battery\nplacement. We also discuss the minor impact of higher-order topological\nconditions on tree-topology networks. To find the localized nature of a single\nbattery's impact, we establish that the relative cost-saving benefit of a\nsingle battery decreases as the network scales. Furthermore, we design a\nlow-complexity algorithm for weakly-cyclic networks. Numerical experiments show\nthat our algorithm is not only approximately 100 times faster than commercial\nsolvers but also maintains high accuracy even when some theoretical assumptions\nare relaxed.", "comment": "10 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.10123v1", "cate": "eess.SY", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.08999", "title": "Hypergraph Overlapping Community Detection for Brain Networks", "authors": ["Duc Vu", "Selin Aviyente"], "categories": ["eess.SP", "I.5.3; I.2.6"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      6 Pages, Accepted for IEEE MLSP 2025", "url": "http://arxiv.org/abs/2507.08999v1", "summary": "Functional magnetic resonance imaging (fMRI) has been commonly used to\nconstruct functional connectivity networks (FCNs) of the human brain. TFCNs are\nprimarily limited to quantifying pairwise relationships between ROIs ignoring\nhigher order dependencies between multiple brain regions. Recently, hypergraph\nconstruction methods from fMRI time series data have been proposed to\ncharacterize the high-order relations among multiple ROIs. While there have\nbeen multiple methods for constructing hypergraphs from fMRI time series, the\nquestion of how to characterize the topology of these hypergraphs remains open.\nIn this paper, we make two key contributions to the field of community\ndetection in brain hypernetworks. First, we construct a hypergraph for each\nsubject capturing high order dependencies between regions. Second, we introduce\na spectral clustering based approach on hypergraphs to detect overlapping\ncommunity structure. Finally, the proposed method is implemented to detect the\nconsensus community structure across multiple subjects. The proposed method is\napplied to resting state fMRI data from Human Connectome Project to summarize\nthe overlapping community structure across a group of healthy young adults.", "comment": "6 Pages, Accepted for IEEE MLSP 2025", "pdf_url": "http://arxiv.org/pdf/2507.08999v1", "cate": "eess.SP", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09602", "title": "DRAGD: A Federated Unlearning Data Reconstruction Attack Based on Gradient Differences", "authors": ["Bocheng Ju", "Junchao Fan", "Jiaqi Liu", "Xiaolin Chang"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09602v1", "summary": "Federated learning enables collaborative machine learning while preserving\ndata privacy. However, the rise of federated unlearning, designed to allow\nclients to erase their data from the global model, introduces new privacy\nconcerns. Specifically, the gradient exchanges during the unlearning process\ncan leak sensitive information about deleted data. In this paper, we introduce\nDRAGD, a novel attack that exploits gradient discrepancies before and after\nunlearning to reconstruct forgotten data. We also present DRAGDP, an enhanced\nversion of DRAGD that leverages publicly available prior data to improve\nreconstruction accuracy, particularly for complex datasets like facial images.\nExtensive experiments across multiple datasets demonstrate that DRAGD and\nDRAGDP significantly outperform existing methods in data reconstruction.Our\nwork highlights a critical privacy vulnerability in federated unlearning and\noffers a practical solution, advancing the security of federated unlearning\nsystems in real-world applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09602v1", "cate": "cs.LG", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09915", "title": "Crucial-Diff: A Unified Diffusion Model for Crucial Image and Annotation Synthesis in Data-scarce Scenarios", "authors": ["Siyue Yao", "Mingjie Sun", "Eng Gee Lim", "Ran Yi", "Baojiang Zhong", "Moncef Gabbouj"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09915v1", "summary": "The scarcity of data in various scenarios, such as medical, industry and\nautonomous driving, leads to model overfitting and dataset imbalance, thus\nhindering effective detection and segmentation performance. Existing studies\nemploy the generative models to synthesize more training samples to mitigate\ndata scarcity. However, these synthetic samples are repetitive or simplistic\nand fail to provide \"crucial information\" that targets the downstream model's\nweaknesses. Additionally, these methods typically require separate training for\ndifferent objects, leading to computational inefficiencies. To address these\nissues, we propose Crucial-Diff, a domain-agnostic framework designed to\nsynthesize crucial samples. Our method integrates two key modules. The Scene\nAgnostic Feature Extractor (SAFE) utilizes a unified feature extractor to\ncapture target information. The Weakness Aware Sample Miner (WASM) generates\nhard-to-detect samples using feedback from the detection results of downstream\nmodel, which is then fused with the output of SAFE module. Together, our\nCrucial-Diff framework generates diverse, high-quality training data, achieving\na pixel-level AP of 83.63% and an F1-MAX of 78.12% on MVTec. On polyp dataset,\nCrucial-Diff reaches an mIoU of 81.64% and an mDice of 87.69%. Code will be\nreleased after acceptance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09915v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2505.08572", "title": "Entropy numbers of classes defined by integral operators", "authors": ["V. Temlyakov"], "categories": ["math.NA", "cs.NA", "math.FA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.08572v2", "summary": "In this paper we develop the following general approach. We study asymptotic\nbehavior of the entropy numbers not for an individual smoothness class, how it\nis usually done, but for the collection of classes, which are defined by\nintegral operators with kernels coming from a given class of functions.\nEarlier, such approach was realized for the Kolmogorov widths.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.08572v2", "cate": "math.NA", "date": "2025-05-13", "updated": "2025-07-12"}
{"id": "2411.13766", "title": "Tiny-Align: Bridging Automatic Speech Recognition and Large Language Model on the Edge", "authors": ["Ruiyang Qin", "Dancheng Liu", "Gelei Xu", "Zheyu Yan", "Chenhui Xu", "Yuting Hu", "Shaocong Wang", "X. Sharon Hu", "Jinjun Xiong", "Yiyu Shi"], "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Accepted by ICCAD'25", "url": "http://arxiv.org/abs/2411.13766v4", "summary": "The combination of Large Language Models (LLM) and Automatic Speech\nRecognition (ASR), when deployed on edge devices (called edge ASR-LLM), can\nserve as a powerful personalized assistant to enable audio-based interaction\nfor users. Compared to text-based interaction, edge ASR-LLM allows accessible\nand natural audio interactions. Unfortunately, existing ASR-LLM models are\nmainly trained in high-performance computing environments and produce\nsubstantial model weights, making them difficult to deploy on edge devices.\nMore importantly, to better serve users' personalized needs, the ASR-LLM must\nbe able to learn from each distinct user, given that audio input often contains\nhighly personalized characteristics that necessitate personalized on-device\ntraining. Since individually fine-tuning the ASR or LLM often leads to\nsuboptimal results due to modality-specific limitations, end-to-end training\nensures seamless integration of audio features and language understanding\n(cross-modal alignment), ultimately enabling a more personalized and efficient\nadaptation on edge devices. However, due to the complex training requirements\nand substantial computational demands of existing approaches, cross-modal\nalignment between ASR audio and LLM can be challenging on edge devices. In this\nwork, we propose a resource-efficient cross-modal alignment framework that\nbridges ASR and LLMs on edge devices to handle personalized audio input. Our\nframework enables efficient ASR-LLM alignment on resource-constrained devices\nlike NVIDIA Jetson Orin (8GB RAM), achieving 50x training time speedup while\nimproving the alignment quality by more than 50\\%. To the best of our\nknowledge, this is the first work to study efficient ASR-LLM alignment on\nresource-constrained edge devices.", "comment": "Accepted by ICCAD'25", "pdf_url": "http://arxiv.org/pdf/2411.13766v4", "cate": "cs.SD", "date": "2024-11-21", "updated": "2025-07-11"}
{"id": "2507.10280", "title": "A SUMO-Based Digital Twin for Evaluation of Conventional and Electric Vehicle Networks", "authors": ["Haomiaomiao Wang", "Conor Fennell", "Swati Poojary", "Mingming Liu"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      The paper has been accepted by the IEEE Energy Conversion Congress & Expo (ECCE) Europe 2025 conference", "url": "http://arxiv.org/abs/2507.10280v1", "summary": "Digital twins are increasingly applied in transportation modelling to\nreplicate real-world traffic dynamics and evaluate mobility and energy\nefficiency. This study presents a SUMO-based digital twin that simulates mixed\nICEV-EV traffic on a major motorway segment, leveraging multi-sensor data\nfusion from inductive loops, GPS probes, and toll records. The model is\nvalidated under both complete and partial information scenarios, achieving\n93.1% accuracy in average speed estimation and 97.1% in average trip length\nestimation. Statistical metrics, including KL Divergence and Wasserstein\nDistance, demonstrate strong alignment between simulated and observed traffic\npatterns. Furthermore, CO2 emissions were overestimated by only 0.8-2.4%, and\nEV power consumption underestimated by 1.0-5.4%, highlighting the model's\nrobustness even with incomplete vehicle classification information.", "comment": "The paper has been accepted by the IEEE Energy Conversion Congress &\n  Expo (ECCE) Europe 2025 conference", "pdf_url": "http://arxiv.org/pdf/2507.10280v1", "cate": "eess.SY", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10352", "title": "Improved Sum-of-Squares Stability Verification of Neural-Network-Based Controllers", "authors": ["Alvaro Detailleur", "Guillaume Ducard", "Christopher Onder"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10352v1", "summary": "This work presents several improvements to the closed-loop stability\nverification framework using semialgebraic sets and convex semidefinite\nprogramming to examine neural-network-based control systems regulating\nnonlinear dynamical systems. First, the utility of the framework is greatly\nexpanded: two semialgebraic functions mimicking common, smooth activation\nfunctions are presented and compatibility with control systems incorporating\nRecurrent Equilibrium Networks (RENs) and thereby Recurrent Neural Networks\n(RNNs) is established. Second, the validity of the framework's state-of-the-art\nstability analyses is established via an alternate proof. Third, based on this\nproof, two new optimization problems simplifying the analysis of local\nstability properties are presented. To simplify the analysis of a closed-loop\nsystem's Region of Attraction (RoA), the first problem explicitly parameterizes\na class of candidate Lyapunov functions larger than in previous works. The\nsecond problem utilizes the unique guarantees available under the condition of\ninvariance to further expand the set of candidate Lyapunov functions and\ndirectly determine whether an invariant set forms part of the system's RoA.\nThese contributions are successfully demonstrated in two numerical examples and\nsuggestions for future research are provided.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10352v1", "cate": "eess.SY", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09215", "title": "Time-Varying Offset Estimation for Clock-Asynchronous Bistatic ISAC Systems", "authors": ["Yi Wang", "Keke Zu", "Luping Xiang", "Martin Haardt", "Kun Yang"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09215v1", "summary": "The bistatic Integrated Sensing and Communication (ISAC) is poised to become\na key application for next generation communication networks (e.g., B5G/6G),\nproviding simultaneous sensing and communication services with minimal changes\nto existing network infrastructure and hardware. However, a significant\nchallenge in bistatic cooperative sensing is clock asynchronism, arising from\nthe use of different clocks at far separated transmitters and receivers. This\nasynchrony leads to Timing Offsets (TOs) and Carrier Frequency Offsets (CFOs),\npotentially causing sensing ambiguity. Traditional synchronization methods\ntypically rely on static reference links or GNSS-based timing sources, both of\nwhich are often unreliable or unavailable in UAVbased bistatic ISAC scenarios.\nTo overcome these limitations, we propose a Time-Varying Offset Estimation\n(TVOE) framework tailored for clock-asynchronous bistatic ISAC systems, which\nleverages the geometrically predictable characteristics of the Line-of-Sight\n(LoS) path to enable robust, infrastructure-free\n  synchronization. The framework treats the LoS delay and the Doppler shift as\ndynamic observations and models their evolution as a hidden stochastic process.\nA state-space formulation is developed to jointly estimate TO and CFO via an\nExtended Kalman Filter (EKF), enabling real-time tracking of clock offsets\nacross successive frames. Furthermore, the estimated offsets are subsequently\napplied to correct the timing misalignment of all Non-Line-of-Sight (NLoS)\ncomponents, thereby enhancing the high-resolution target sensing performance.\nExtensive simulation results demonstrate that the proposed TVOE method improves\nthe estimation accuracy by 60%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09215v1", "cate": "eess.SP", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09647", "title": "KEN: Knowledge Augmentation and Emotion Guidance Network for Multimodal Fake News Detection", "authors": ["Peican Zhu", "Yubo Jing", "Le Cheng", "Keke Tang", "Yangming Guo"], "categories": ["cs.MM", "cs.AI"], "primary_category": "Subjects:       Multimedia (cs.MM)", "pdf_link": null, "comments": "Comments:      Accepted by ACM MM 2025", "url": "http://arxiv.org/abs/2507.09647v1", "summary": "In recent years, the rampant spread of misinformation on social media has\nmade accurate detection of multimodal fake news a critical research focus.\nHowever, previous research has not adequately understood the semantics of\nimages, and models struggle to discern news authenticity with limited textual\ninformation. Meanwhile, treating all emotional types of news uniformly without\ntailored approaches further leads to performance degradation. Therefore, we\npropose a novel Knowledge Augmentation and Emotion Guidance Network (KEN). On\nthe one hand, we effectively leverage LVLM's powerful semantic understanding\nand extensive world knowledge. For images, the generated captions provide a\ncomprehensive understanding of image content and scenes, while for text, the\nretrieved evidence helps break the information silos caused by the closed and\nlimited text and context. On the other hand, we consider inter-class\ndifferences between different emotional types of news through balanced\nlearning, achieving fine-grained modeling of the relationship between emotional\ntypes and authenticity. Extensive experiments on two real-world datasets\ndemonstrate the superiority of our KEN.", "comment": "Accepted by ACM MM 2025", "pdf_url": "http://arxiv.org/pdf/2507.09647v1", "cate": "cs.MM", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09950", "title": "Can GPT-4o mini and Gemini 2.0 Flash Predict Fine-Grained Fashion Product Attributes? A Zero-Shot Analysis", "authors": ["Shubham Shukla", "Kunal Sonalkar"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      11 pages, 2 figures", "url": "http://arxiv.org/abs/2507.09950v1", "summary": "The fashion retail business is centered around the capacity to comprehend\nproducts. Product attribution helps in comprehending products depending on the\nbusiness process. Quality attribution improves the customer experience as they\nnavigate through millions of products offered by a retail website. It leads to\nwell-organized product catalogs. In the end, product attribution directly\nimpacts the 'discovery experience' of the customer. Although large language\nmodels (LLMs) have shown remarkable capabilities in understanding multimodal\ndata, their performance on fine-grained fashion attribute recognition remains\nunder-explored. This paper presents a zero-shot evaluation of state-of-the-art\nLLMs that balance performance with speed and cost efficiency, mainly\nGPT-4o-mini and Gemini 2.0 Flash. We have used the dataset\nDeepFashion-MultiModal (https://github.com/yumingj/DeepFashion-MultiModal) to\nevaluate these models in the attribution tasks of fashion products. Our study\nevaluates these models across 18 categories of fashion attributes, offering\ninsight into where these models excel. We only use images as the sole input for\nproduct information to create a constrained environment. Our analysis shows\nthat Gemini 2.0 Flash demonstrates the strongest overall performance with a\nmacro F1 score of 56.79% across all attributes, while GPT-4o-mini scored a\nmacro F1 score of 43.28%. Through detailed error analysis, our findings provide\npractical insights for deploying these LLMs in production e-commerce product\nattribution-related tasks and highlight the need for domain-specific\nfine-tuning approaches. This work also lays the groundwork for future research\nin fashion AI and multimodal attribute extraction.", "comment": "11 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.09950v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2207.01876", "title": "Global Convergence of Successive Approximations for Non-convex Stochastic Optimal Control Problems", "authors": ["Shaolin Ji", "Rundong Xu"], "categories": ["math.OC", "cs.NA", "math.NA", "93E20, 60H10, 60H30, 49M05"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      21 pages", "url": "http://arxiv.org/abs/2207.01876v2", "summary": "This paper focuses on finding approximate solutions to stochastic optimal\ncontrol problems with control domains being not necessarily convex, where the\nstate trajectory is subject to controlled stochastic differential equations.\nThe control-dependent diffusions make the traditional method of successive\napproximations (MSA) insufficient to reduce the value of cost functional in\neach iteration. Without adding extra terms over which to perform the\nHamiltonian minimization, the MSA becomes sufficient by our novel error\nestimate involving a higher order backward adjoint equation. Under certain\nconvexity assumptions on the coefficients (no convexity assumptions on the\ncontrol domains), the value of the cost functional descends to the global\nminimum as the number of iterations tends to infinity. In particular, a\nconvergence rate is available for a class of generalized linear-quadratic\nsystems.", "comment": "21 pages", "pdf_url": "http://arxiv.org/pdf/2207.01876v2", "cate": "math.OC", "date": "2022-07-05", "updated": "2025-07-13"}
{"id": "2507.03468", "title": "Robust Localization of Partially Fake Speech: Metrics, Models, and Out-of-Domain Evaluation", "authors": ["Hieu-Thi Luong", "Inbal Rimon", "Haim Permuter", "Kong Aik Lee", "Eng Siong Chng"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Submitted to APSIPA 2025", "url": "http://arxiv.org/abs/2507.03468v2", "summary": "Partial audio deepfake localization pose unique challenges and remain\nunderexplored compared to full-utterance spoofing detection. While recent\nmethods report strong in-domain performance, their real-world utility remains\nunclear. In this analysis, we critically examine the limitations of current\nevaluation practices, particularly the widespread use of Equal Error Rate\n(EER), which often obscures generalization and deployment readiness. We propose\nreframing the localization task as a sequential anomaly detection problem and\nadvocate for the use of threshold-dependent metrics such as accuracy,\nprecision, recall, and F1-score, which better reflect real-world behavior.\nSpecifically, we analyze the performance of the open-source Coarse-to-Fine\nProposal Refinement Framework (CFPRF), which achieves a 20-ms EER of 7.61% on\nthe in-domain PartialSpoof evaluation set, but 43.25% and 27.59% on the\nLlamaPartialSpoof and Half-Truth out-of-domain test sets. Interestingly, our\nreproduced version of the same model performs worse on in-domain data (9.84%)\nbut better on the out-of-domain sets (41.72% and 14.98%, respectively). This\nhighlights the risks of over-optimizing for in-domain EER, which can lead to\nmodels that perform poorly in real-world scenarios. It also suggests that while\ndeep learning models can be effective on in-domain data, they generalize poorly\nto out-of-domain scenarios, failing to detect novel synthetic samples and\nmisclassifying unfamiliar bona fide audio. Finally, we observe that adding more\nbona fide or fully synthetic utterances to the training data often degrades\nperformance, whereas adding partially fake utterances improves it.", "comment": "Submitted to APSIPA 2025", "pdf_url": "http://arxiv.org/pdf/2507.03468v2", "cate": "cs.SD", "date": "2025-07-04", "updated": "2025-07-13"}
{"id": "2507.09026", "title": "On the Gradient Domination of the LQG Problem", "authors": ["Kasra Fallah", "Leonardo F. Toso", "James Anderson"], "categories": ["math.OC", "cs.LG", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09026v1", "summary": "We consider solutions to the linear quadratic Gaussian (LQG) regulator\nproblem via policy gradient (PG) methods. Although PG methods have demonstrated\nstrong theoretical guarantees in solving the linear quadratic regulator (LQR)\nproblem, despite its nonconvex landscape, their theoretical understanding in\nthe LQG setting remains limited. Notably, the LQG problem lacks gradient\ndominance in the classical parameterization, i.e., with a dynamic controller,\nwhich hinders global convergence guarantees. In this work, we study PG for the\nLQG problem by adopting an alternative parameterization of the set of\nstabilizing controllers and employing a lifting argument. We refer to this\nparameterization as a history representation of the control input as it is\nparameterized by past input and output data from the previous p time-steps.\nThis representation enables us to establish gradient dominance and approximate\nsmoothness for the LQG cost. We prove global convergence and per-iteration\nstability guarantees for policy gradient LQG in model-based and model-free\nsettings. Numerical experiments on an open-loop unstable system are provided to\nsupport the global convergence guarantees and to illustrate convergence under\ndifferent history lengths of the history representation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09026v1", "cate": "math.OC", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09061", "title": "Imitation Learning in Continuous Action Spaces: Mitigating Compounding Error without Interaction", "authors": ["Thomas T. Zhang", "Daniel Pfrommer", "Nikolai Matni", "Max Simchowitz"], "categories": ["cs.LG", "cs.SY", "eess.SY", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09061v1", "summary": "We study the problem of imitating an expert demonstrator in a continuous\nstate-and-action dynamical system. While imitation learning in discrete\nsettings such as autoregressive language modeling has seen immense success and\npopularity in recent years, imitation in physical settings such as autonomous\ndriving and robot learning has proven comparably more complex due to the\ncompounding errors problem, often requiring elaborate set-ups to perform\nstably. Recent work has demonstrated that even in benign settings, exponential\ncompounding errors are unavoidable when learning solely from expert-controlled\ntrajectories, suggesting the need for more advanced policy parameterizations or\ndata augmentation. To this end, we present minimal interventions that provably\nmitigate compounding errors in continuous state-and-action imitation learning.\nWhen the system is open-loop stable, we prescribe \"action chunking,\" i.e.,\npredicting and playing sequences of actions in open-loop; when the system is\npossibly unstable, we prescribe \"noise injection,\" i.e., adding noise during\nexpert demonstrations. These interventions align with popular choices in modern\nrobot learning, though the benefits we derive are distinct from the effects\nthey were designed to target. Our results draw insights and tools from both\ncontrol theory and reinforcement learning; however, our analysis reveals novel\nconsiderations that do not naturally arise when either literature is considered\nin isolation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09061v1", "cate": "cs.LG", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09218", "title": "Image Super-Resolution-Based Signal Enhancement in Bistatic ISAC", "authors": ["Yi Wang", "Keke Zu", "Luping Xiang", "Martin Haardt", "Chaochao Wang", "Xianchao Zhang", "Kun Yang"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09218v1", "summary": "Bistatic Integrated Sensing and Communication (ISAC) is poised to become a\ncornerstone technology in next-generation communication networks, such as\nBeyond 5G (B5G) and 6G, by enabling the concurrent execution of sensing and\ncommunication functions without requiring significant modifications to existing\ninfrastructure. Despite its promising potential, a major challenge in bistatic\ncooperative sensing lies in the degradation of sensing accuracy, primarily\ncaused by the inherently weak received signals resulting from high reflection\nlosses in complex environments. Traditional methods have predominantly relied\non adaptive filtering techniques to enhance the Signal-to-Noise Ratio (SNR) by\ndynamically adjusting the filter coefficients. However, these methods often\nstruggle to adapt effectively to the increasingly complex and diverse network\ntopologies. To address these challenges, we propose a novel Image\nSuper-Resolution-based Signal Enhancement (ISR-SE) framework that significantly\nimproves the recognition and recovery capabilities of ISAC signals.\nSpecifically, we first perform a time-frequency analysis by applying the\nShort-Time Fourier Transform (STFT) to the received signals, generating\nspectrograms that capture the frequency, magnitude, and phase components. These\ncomponents are then mapped into RGB images, where each channel represents one\nof the extracted features, enabling a more intuitive and informative\nvisualization of the signal structure. To enhance these RGB images, we design\nan improved denoising network that combines the strengths of the UNet\narchitecture and diffusion models. This hybrid architecture leverages UNet's\nmulti-scale feature extraction and the generative capacity of diffusion models\nto perform effective image denoising, thereby improving the quality and clarity\nof signal representations under low-SNR conditions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09218v1", "cate": "eess.SP", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09678", "title": "Conformal Prediction for Privacy-Preserving Machine Learning", "authors": ["Alexander David Balinsky", "Dominik Krzeminski", "Alexander Balinsky"], "categories": ["cs.LG", "cs.AI", "math.ST", "stat.TH"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09678v1", "summary": "We investigate the integration of Conformal Prediction (CP) with supervised\nlearning on deterministically encrypted data, aiming to bridge the gap between\nrigorous uncertainty quantification and privacy-preserving machine learning.\nUsing AES-encrypted variants of the MNIST dataset, we demonstrate that CP\nmethods remain effective even when applied directly in the encrypted domain,\nowing to the preservation of data exchangeability under fixed-key encryption.\nWe test traditional $p$-value-based against $e$-value-based conformal\npredictors. Our empirical evaluation reveals that models trained on\ndeterministically encrypted data retain the ability to extract meaningful\nstructure, achieving 36.88\\% test accuracy -- significantly above random\nguessing (9.56\\%) observed with per-instance encryption. Moreover,\n$e$-value-based CP achieves predictive set coverage of over 60\\% with 4.3\nloss-threshold calibration, correctly capturing the true label in 4888 out of\n5000 test cases. In contrast, the $p$-value-based CP yields smaller predictive\nsets but with reduced coverage accuracy. These findings highlight both the\npromise and limitations of CP in encrypted data settings and underscore\ncritical trade-offs between prediction set compactness and reliability. %Our\nwork sets a foundation for principled uncertainty quantification in secure,\nprivacy-aware learning systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09678v1", "cate": "cs.LG", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09953", "title": "4D-MISR: A unified model for low-dose super-resolution imaging via feature fusion", "authors": ["Zifei Wang", "Zian Mao", "Xiaoya He", "Xi Huang", "Haoran Zhang", "Chun Cheng", "Shufen Chu", "Tingzheng Hou", "Xiaoqin Zeng", "Yujun Xie"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09953v1", "summary": "While electron microscopy offers crucial atomic-resolution insights into\nstructure-property relationships, radiation damage severely limits its use on\nbeam-sensitive materials like proteins and 2D materials. To overcome this\nchallenge, we push beyond the electron dose limits of conventional electron\nmicroscopy by adapting principles from multi-image super-resolution (MISR) that\nhave been widely used in remote sensing. Our method fuses multiple\nlow-resolution, sub-pixel-shifted views and enhances the reconstruction with a\nconvolutional neural network (CNN) that integrates features from synthetic,\nmulti-angle observations. We developed a dual-path, attention-guided network\nfor 4D-STEM that achieves atomic-scale super-resolution from ultra-low-dose\ndata. This provides robust atomic-scale visualization across amorphous,\nsemi-crystalline, and crystalline beam-sensitive specimens. Systematic\nevaluations on representative materials demonstrate comparable spatial\nresolution to conventional ptychography under ultra-low-dose conditions. Our\nwork expands the capabilities of 4D-STEM, offering a new and generalizable\nmethod for the structural analysis of radiation-vulnerable materials.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09953v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2310.02068", "title": "Well-posedness and numerical analysis of an elapsed time model with strongly coupled neural networks", "authors": ["Mauricio Sepulveda", "Nicolas Torres", "Luis Miguel Villada"], "categories": ["math.AP", "cs.NA", "math.NA", "35A35, 35F20, 35R09, 65M06"], "primary_category": "Subjects:       Analysis of PDEs (math.AP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2310.02068v4", "summary": "The elapsed time equation is an age-structured model that describes the\ndynamics of interconnected spiking neurons through the elapsed time since the\nlast discharge, leading to many interesting questions on the evolution of the\nsystem from a mathematical and biological point of view. In this work, we deal\nwith the case when the transmission after a spike is instantaneous and the case\nwith a distributed delay that depends on the previous history of the system,\nwhich is a more realistic assumption. Since the instantaneous transmission case\nis known to be ill-posed due to non-uniqueness or jump discontinuities, we\nestablish a criterion for well-posedness to determine when the solution remains\ncontinuous in time, through an invertibility condition that improves the\nexistence theory under more relaxed hypothesis on the nonlinearity, including\nthe strongly excitatory case. Inspired in the existence theory, we adapt the\nclassical explicit upwind scheme through a robust fixed-point approach and we\nprove that the approximation given by this scheme converges to the solution of\nthe nonlinear problem through BV-estimates and we extend the idea to the case\nwith distributed delay. We also show some numerical simulations to compare the\nbehavior of the system in the case of instantaneous transmission with the case\nof distributed delay under different parameters, leading to solutions with\ndifferent asymptotic profiles.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2310.02068v4", "cate": "math.AP", "date": "2023-10-03", "updated": "2025-07-14"}
{"id": "2501.18314", "title": "AGAV-Rater: Adapting Large Multimodal Model for AI-Generated Audio-Visual Quality Assessment", "authors": ["Yuqin Cao", "Xiongkuo Min", "Yixuan Gao", "Wei Sun", "Guangtao Zhai"], "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Multimedia (cs.MM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.18314v2", "summary": "Many video-to-audio (VTA) methods have been proposed for dubbing silent\nAI-generated videos. An efficient quality assessment method for AI-generated\naudio-visual content (AGAV) is crucial for ensuring audio-visual quality.\nExisting audio-visual quality assessment methods struggle with unique\ndistortions in AGAVs, such as unrealistic and inconsistent elements. To address\nthis, we introduce AGAVQA-3k, the first large-scale AGAV quality assessment\ndataset, comprising $3,382$ AGAVs from $16$ VTA methods. AGAVQA-3k includes two\nsubsets: AGAVQA-MOS, which provides multi-dimensional scores for audio quality,\ncontent consistency, and overall quality, and AGAVQA-Pair, designed for optimal\nAGAV pair selection. We further propose AGAV-Rater, a LMM-based model that can\nscore AGAVs, as well as audio and music generated from text, across multiple\ndimensions, and selects the best AGAV generated by VTA methods to present to\nthe user. AGAV-Rater achieves state-of-the-art performance on AGAVQA-3k,\nText-to-Audio, and Text-to-Music datasets. Subjective tests also confirm that\nAGAV-Rater enhances VTA performance and user experience. The dataset and code\nis available at https://github.com/charlotte9524/AGAV-Rater.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.18314v2", "cate": "cs.MM", "date": "2025-01-30", "updated": "2025-07-14"}
{"id": "2507.09855", "title": "Optimal Design of Satellite Constellation Configurations with Mixed Integer Linear Programming", "authors": ["David O. Williams Rogers", "Dongshik Won", "Dongwook Koh", "Kyungwoo Hong", "Hang Woon Lee"], "categories": ["math.OC", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      40 pages", "url": "http://arxiv.org/abs/2507.09855v1", "summary": "Designing satellite constellation systems involves complex multidisciplinary\noptimization in which coverage serves as a primary driver of overall system\ncost and performance. Among the various design considerations, constellation\nconfiguration -- how satellites are placed and distributed in space relative to\neach other -- predominantly determines the resulting coverage. In constellation\nconfiguration design, coverage can be considered either as an objective or a\nconstraint, driven by mission objectives. State-of-the-art literature addresses\neach situation on a case-by-case basis, applying a unique set of assumptions,\nmodeling, and solution methods. Although such a problem-based methodology is\nvaluable, users often face implementation challenges when performing trade-off\nstudies across different mission scenarios, as each scenario must be handled\ndistinctly. In response, we propose a unifying framework consisting of five\nmixed-integer linear program formulations that are of practical significance,\nextensible to more complex mission narratives using additional constraints, and\ncapable of obtaining provably optimal constellation configurations. It can\nhandle various metrics and mission scenarios, such as percent coverage, average\nor maximum revisit times, fixed number of satellites, spatiotemporally varying\ncoverage requirements, and ground-, aerial-, or space-based, static or mobile\ntargets. The paper presents several add-ons, case studies, and comparative\nanalyses to demonstrate the versatility of the proposed framework.", "comment": "40 pages", "pdf_url": "http://arxiv.org/pdf/2507.09855v1", "cate": "math.OC", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10052", "title": "Analyzing the Crowding-Out Effect of Investment Herding on Consumption: An Optimal Control Theory Approach", "authors": ["Huisheng Wang", "H. Vicky Zhao"], "categories": ["q-fin.PM", "cs.SY", "econ.GN", "eess.SY", "q-fin.EC", "q-fin.MF"], "primary_category": "Subjects:       Portfolio Management (q-fin.PM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10052v1", "summary": "Investment herding, a phenomenon where households mimic the decisions of\nothers rather than relying on their own analysis, has significant effects on\nfinancial markets and household behavior. Excessive investment herding may\nreduce investments and lead to a depletion of household consumption, which is\ncalled the crowding-out effect. While existing research has qualitatively\nexamined the impact of investment herding on consumption, quantitative studies\nin this area remain limited. In this work, we investigate the optimal\ninvestment and consumption decisions of households under the impact of\ninvestment herding. We formulate an optimization problem to model how\ninvestment herding influences household decisions over time. Based on the\noptimal control theory, we solve for the analytical solutions of optimal\ninvestment and consumption decisions. We theoretically analyze the impact of\ninvestment herding on household consumption decisions and demonstrate the\nexistence of the crowding-out effect. We further explore how parameters, such\nas interest rate, excess return rate, and volatility, influence the\ncrowding-out effect. Finally, we conduct a real data test to validate our\ntheoretical analysis of the crowding-out effect. This study is crucial to\nunderstanding the impact of investment herding on household consumption and\noffering valuable insights for policymakers seeking to stimulate consumption\nand mitigate the negative effects of investment herding on economic growth.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10052v1", "cate": "q-fin.PM", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09244", "title": "Deep Learning for sub-THz Radio Unit Selection using sub-10 GHz Channel Information and Inferred Device Beamforming", "authors": ["Nishant Gupta", "Muris Sarajlic", "Erik G. Larsson"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      Accepted for Publication in IEEE VTC-Spring 2025, held at Oslo, Norway", "url": "http://arxiv.org/abs/2507.09244v1", "summary": "The dense and distributed deployment of sub-THz radio units (RUs) alongside\nsub-10 GHz access point (AP) is a promising approach to provide high data rate\nand reliable coverage for future 6G applications. However, beam search or RU\nselection for the sub-THz RUs incurs significant overhead and high power\nconsumption. To address this, we introduce a method that leverages deep\nlearning to infer a suitable sub-THz RU candidate from a set of sub-THz RUs\nusing the sub-10 GHz channel characteristics. A novel aspect of this work is\nthe consideration of inter-band beam configuration (IBBC), defined as the\nbroadside angle between the low-band and high-band antenna patterns of the user\nequipment (UE). Since IBBC indicates the beamforming information or UE's\norientation, it is typically not shared with the network as a part of\nsignalling. Therefore, we propose a solution strategy to infer a suitable\nsub-THz RU even when UEs do not share their IBBC information. Simulation\nresults illustrate the performance of the inferred sub-THz RU and highlights\nthe detrimental impact of neglecting UE orientation on the systems performance.", "comment": "Accepted for Publication in IEEE VTC-Spring 2025, held at Oslo,\n  Norway", "pdf_url": "http://arxiv.org/pdf/2507.09244v1", "cate": "eess.SP", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09687", "title": "Post-Training Quantization of Generative and Discriminative LSTM Text Classifiers: A Study of Calibration, Class Balance, and Robustness", "authors": ["Md Mushfiqur Rahaman", "Elliot Chang", "Tasmiah Haque", "Srinjoy Das"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09687v1", "summary": "Text classification plays a pivotal role in edge computing applications like\nindustrial monitoring, health diagnostics, and smart assistants, where low\nlatency and high accuracy are both key requirements. Generative classifiers, in\nparticular, have been shown to exhibit robustness to out-of-distribution and\nnoisy data, which is an extremely critical consideration for deployment in such\nreal-time edge environments. However, deploying such models on edge devices\nfaces computational and memory constraints. Post Training Quantization (PTQ)\nreduces model size and compute costs without retraining, making it ideal for\nedge deployment. In this work, we present a comprehensive comparative study of\ngenerative and discriminative Long Short Term Memory (LSTM)-based text\nclassification models with PTQ using the Brevitas quantization library. We\nevaluate both types of classifier models across multiple bitwidths and assess\ntheir robustness under regular and noisy input conditions. We find that while\ndiscriminative classifiers remain robust, generative ones are more sensitive to\nbitwidth, calibration data used during PTQ, and input noise during quantized\ninference. We study the influence of class imbalance in calibration data for\nboth types of classifiers, comparing scenarios with evenly and unevenly\ndistributed class samples including their effect on weight adjustments and\nactivation profiles during PTQ. Using test statistics derived from\nnonparametric hypothesis testing, we identify that using class imbalanced data\nduring calibration introduces insufficient weight adaptation at lower bitwidths\nfor generative LSTM classifiers, thereby leading to degraded performance. This\nstudy underscores the role of calibration data in PTQ and when generative\nclassifiers succeed or fail under noise, aiding deployment in edge\nenvironments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09687v1", "cate": "cs.LG", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09980", "title": "Uncertainty Quantification for Incomplete Multi-View Data Using Divergence Measures", "authors": ["Zhipeng Xue", "Yan Zhang", "Ming Li", "Chun Li", "Yue Liu", "Fei Yu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09980v1", "summary": "Existing multi-view classification and clustering methods typically improve\ntask accuracy by leveraging and fusing information from different views.\nHowever, ensuring the reliability of multi-view integration and final decisions\nis crucial, particularly when dealing with noisy or corrupted data. Current\nmethods often rely on Kullback-Leibler (KL) divergence to estimate uncertainty\nof network predictions, ignoring domain gaps between different modalities. To\naddress this issue, KPHD-Net, based on H\\\"older divergence, is proposed for\nmulti-view classification and clustering tasks. Generally, our KPHD-Net employs\na variational Dirichlet distribution to represent class probability\ndistributions, models evidences from different views, and then integrates it\nwith Dempster-Shafer evidence theory (DST) to improve uncertainty estimation\neffects. Our theoretical analysis demonstrates that Proper H\\\"older divergence\noffers a more effective measure of distribution discrepancies, ensuring\nenhanced performance in multi-view learning. Moreover, Dempster-Shafer evidence\ntheory, recognized for its superior performance in multi-view fusion tasks, is\nintroduced and combined with the Kalman filter to provide future state\nestimations. This integration further enhances the reliability of the final\nfusion results. Extensive experiments show that the proposed KPHD-Net\noutperforms the current state-of-the-art methods in both classification and\nclustering tasks regarding accuracy, robustness, and reliability, with\ntheoretical guarantees.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09980v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2408.00235", "title": "Solving cluster moment relaxation with hierarchical matrix", "authors": ["Yi Wang", "Rizheng Huang", "Yuehaw Khoo"], "categories": ["math.OC", "cs.NA", "math.NA"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2408.00235v2", "summary": "Convex relaxation methods are powerful tools for studying the lowest energy\nof many-body problems. By relaxing the representability conditions for\nmarginals to a set of local constraints, along with a global semidefinite\nconstraint, a polynomial-time solvable semidefinite program (SDP) that provides\na lower bound for the energy can be derived. In this paper, we propose\naccelerating the solution of such an SDP relaxation by imposing a hierarchical\nstructure on the positive semidefinite (PSD) primal and dual variables.\nFurthermore, these matrices can be updated efficiently using the algebra of the\ncompressed representations within an augmented Lagrangian method. We achieve\nquadratic and even near-linear time per-iteration complexity. Through\nexperimentation on the quantum transverse field Ising model, we showcase the\ncapability of our approach to provide a sufficiently accurate lower bound for\nthe exact ground-state energy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2408.00235v2", "cate": "math.OC", "date": "2024-08-01", "updated": "2025-07-14"}
{"id": "2503.10435", "title": "Handling Domain Shifts for Anomalous Sound Detection: A Review of DCASE-Related Work", "authors": ["Kevin Wilkinghoff", "Takuya Fujimura", "Keisuke Imoto", "Jonathan Le Roux", "Zheng-Hua Tan", "Tomoki Toda"], "categories": ["eess.AS", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.10435v2", "summary": "When detecting anomalous sounds in complex environments, one of the main\ndifficulties is that trained models must be sensitive to subtle differences in\nmonitored target signals, while many practical applications also require them\nto be insensitive to changes in acoustic domains. Examples of such domain\nshifts include changing the type of microphone or the location of acoustic\nsensors, which can have a much stronger impact on the acoustic signal than\nsubtle anomalies themselves. Moreover, users typically aim to train a model\nonly on source domain data, which they may have a relatively large collection\nof, and they hope that such a trained model will be able to generalize well to\nan unseen target domain by providing only a minimal number of samples to\ncharacterize the acoustic signals in that domain. In this work, we review and\ndiscuss recent publications focusing on this domain generalization problem for\nanomalous sound detection in the context of the DCASE challenges on acoustic\nmachine condition monitoring.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.10435v2", "cate": "eess.AS", "date": "2025-03-13", "updated": "2025-07-14"}
{"id": "2507.10078", "title": "Compression Method for Deep Diagonal State Space Model Based on $H^2$ Optimal Reduction", "authors": ["Hiroki Sakamoto", "Kazuhiro Sato"], "categories": ["cs.LG", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted to IEEE Control Systems Letters", "url": "http://arxiv.org/abs/2507.10078v1", "summary": "Deep learning models incorporating linear SSMs have gained attention for\ncapturing long-range dependencies in sequential data. However, their large\nparameter sizes pose challenges for deployment on resource-constrained devices.\nIn this study, we propose an efficient parameter reduction method for these\nmodels by applying $H^{2}$ model order reduction techniques from control theory\nto their linear SSM components. In experiments, the LRA benchmark results show\nthat the model compression based on our proposed method outperforms an existing\nmethod using the Balanced Truncation, while successfully reducing the number of\nparameters in the SSMs to $1/32$ without sacrificing the performance of the\noriginal models.", "comment": "Accepted to IEEE Control Systems Letters", "pdf_url": "http://arxiv.org/pdf/2507.10078v1", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10086", "title": "Fast-Response Variable-Frequency Series-Capacitor Buck VRM Through Integrated Control Approaches", "authors": ["Guanyu Qian", "Haoxian Yan", "Xiaofan Cui"], "categories": ["physics.app-ph", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Applied Physics (physics.app-ph)", "pdf_link": null, "comments": "Comments:      8 pages, 10 figures, IEEE conference style. to be published on IEEE Workshop on Control and Modeling of Power Electronics (COMPEL 2025)", "url": "http://arxiv.org/abs/2507.10086v1", "summary": "Fast-response voltage regulation is essential for data-center Voltage\nRegulation Modules (VRMs) powering Artificial Intelligence (AI) workloads,\nwhich exhibit both small-amplitude fluctuations and abrupt full-load steps.\nThis paper introduces a control scheme that integrates a linear controller and\na nonlinear controller for variable-frequency Series-Capacitor Buck (SCB)\nconverters. First, an accurate small-signal model is derived via a\nSwitching-Synchronized Sampled State-Space (5S) framework, yielding\ndiscrete-time transfer functions and root-locus insights for direct digital\ndesign. A critical concern for SCB converters is series-capacitor oscillation\nduring heavy load steps if the strict switching sequence is not maintained. To\naccelerate large-signal transients, a time-optimal control strategy based on\nPontryagins Maximum Principle (PMP) relaxes the switching constraints to\ncompute time-optimal switching sequences. A transition logic is then proposed\nto integrate the high-bandwidth small-signal controller and the large-signal\ncontroller. Simulations demonstrate a rapid output voltage recovery under a\nheavy load step-up, over ten times faster than a linear controller-only design.\nPreliminary hardware tests indicate a stable rejection to heavy load\ndisturbances with zero steady-state error.", "comment": "8 pages, 10 figures, IEEE conference style. to be published on IEEE\n  Workshop on Control and Modeling of Power Electronics (COMPEL 2025)", "pdf_url": "http://arxiv.org/pdf/2507.10086v1", "cate": "physics.app-ph", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09268", "title": "Matched Filtering-Based Channel Estimation for AFDM Systems in Doubly Selective Channels", "authors": ["Xiangjun Li", "Zilong Liu", "Zhengchun Zhou", "Pingzhi Fan"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09268v1", "summary": "Affine frequency division multiplexing (AFDM) has recently emerged as an\nexcellent backward-compatible 6G waveform. In this paper, an enhanced AFDM is\nproposed whereby the delay-Doppler (DD) coupling phase is considered.\nSpecifically, we study matched filtering (MF) assisted channel estimation (CE)\nfor AFDM systems in complex doubly selective channels. By deriving the complete\ninput-output relationship, the inter-chirp-carrier interference,\nsignal-to-interference-plus-noise ratio (SINR), and the effective SINR loss of\nAFDM, are investigated in discrete affine Fourier transform (DAFT) domain.\nFurther, we look into the path ambiguity problem and show that it may lead to\nsevere performance deterioration in fractional-delay fractional-Doppler\nchannels. To address such a problem, we introduce an MF assisted CE scheme\nbuilding upon a novel pilot arrangement across two consecutive AFDM\ntransmissions. This allows us to sequentially estimate the parameters of each\npath by exploiting the separability and approximate orthogonality of different\npaths in the DAFT domain, thus leading to significantly reduced complexity.\nFurthermore, based on generalized Fibonacci search (GFS), an MF-GFS scheme is\nproposed to avoid significantly redundant computation, which can be extended to\ntypical wide-band systems. Extensive simulation results indicate that the\nproposed schemes offer superior advantages in terms of their improved\ncommunication performance and lower complexity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09268v1", "cate": "eess.SP", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.08855", "title": "Multi-omic Prognosis of Alzheimer's Disease with Asymmetric Cross-Modal Cross-Attention Network", "authors": ["Yang Ming", "Jiang Shi Zhong", "Zhou Su Juan"], "categories": ["eess.IV", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08855v1", "summary": "Alzheimer's Disease (AD) is an irreversible neurodegenerative disease\ncharacterized by progressive cognitive decline as its main symptom. In the\nresearch field of deep learning-assisted diagnosis of AD, traditional\nconvolutional neural networks and simple feature concatenation methods fail to\neffectively utilize the complementary information between multimodal data, and\nthe simple feature concatenation approach is prone to cause the loss of key\ninformation during the process of modal fusion. In recent years, the\ndevelopment of deep learning technology has brought new possibilities for\nsolving the problem of how to effectively fuse multimodal features. This paper\nproposes a novel deep learning algorithm framework to assist medical\nprofessionals in AD diagnosis. By fusing medical multi-view information such as\nbrain fluorodeoxyglucose positron emission tomography (PET), magnetic resonance\nimaging (MRI), genetic data, and clinical data, it can accurately detect the\npresence of AD, Mild Cognitive Impairment (MCI), and Cognitively Normal (CN).\nThe innovation of the algorithm lies in the use of an asymmetric cross-modal\ncross-attention mechanism, which can effectively capture the key information\nfeatures of the interactions between different data modal features. This paper\ncompares the asymmetric cross-modal cross-attention mechanism with the\ntraditional algorithm frameworks of unimodal and multimodal deep learning\nmodels for AD diagnosis, and evaluates the importance of the asymmetric\ncross-modal cross-attention mechanism. The algorithm model achieves an accuracy\nof 94.88% on the test set.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08855v1", "cate": "eess.IV", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2507.09694", "title": "Frequency-aware Surrogate Modeling With SMT Kernels For Advanced Data Forecasting", "authors": ["Nicolas Gonel", "Paul Saves", "Joseph Morlier"], "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      AeroBest 2025, Instituto Superior Tecnico of the University of Lisbon, Portugal", "url": "http://arxiv.org/abs/2507.09694v1", "summary": "This paper introduces a comprehensive open-source framework for developing\ncorrelation kernels, with a particular focus on user-defined and composition of\nkernels for surrogate modeling. By advancing kernel-based modeling techniques,\nwe incorporate frequency-aware elements that effectively capture complex\nmechanical behaviors and timefrequency dynamics intrinsic to aircraft systems.\nTraditional kernel functions, often limited to exponential-based methods, are\nextended to include a wider range of kernels such as exponential squared sine\nand rational quadratic kernels, along with their respective firstand\nsecond-order derivatives. The proposed methodologies are first validated on a\nsinus cardinal test case and then applied to forecasting Mauna-Loa Carbon\nDioxide (CO 2 ) concentrations and airline passenger traffic. All these\nadvancements are integrated into the open-source Surrogate Modeling Toolbox\n(SMT 2.0), providing a versatile platform for both standard and customizable\nkernel configurations. Furthermore, the framework enables the combination of\nvarious kernels to leverage their unique strengths into composite models\ntailored to specific problems. The resulting framework offers a flexible\ntoolset for engineers and researchers, paving the way for numerous future\napplications in metamodeling for complex, frequency-sensitive domains.", "comment": "AeroBest 2025, Instituto Superior Tecnico of the University of\n  Lisbon, Portugal", "pdf_url": "http://arxiv.org/pdf/2507.09694v1", "cate": "cs.LG", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09984", "title": "Latent Diffusion Models with Masked AutoEncoders", "authors": ["Junho Lee", "Jeongwoo Shin", "Hyungwook Choi", "Joonseok Lee"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09984v1", "summary": "In spite of remarkable potential of the Latent Diffusion Models (LDMs) in\nimage generation, the desired properties and optimal design of the autoencoders\nhave been underexplored. In this work, we analyze the role of autoencoders in\nLDMs and identify three key properties: latent smoothness, perceptual\ncompression quality, and reconstruction quality. We demonstrate that existing\nautoencoders fail to simultaneously satisfy all three properties, and propose\nVariational Masked AutoEncoders (VMAEs), taking advantage of the hierarchical\nfeatures maintained by Masked AutoEncoder. We integrate VMAEs into the LDM\nframework, introducing Latent Diffusion Models with Masked AutoEncoders\n(LDMAEs). Through comprehensive experiments, we demonstrate significantly\nenhanced image generation quality and computational efficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09984v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2412.03478", "title": "Solving Monge problem by Hilbert space embeddings of probability measures", "authors": ["Takafumi Saito", "Yumiharu Nakano"], "categories": ["math.OC", "cs.NA", "math.NA"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.03478v4", "summary": "We propose deep learning methods for classical Monge's optimal mass\ntransportation problems, where where the distribution constraint is treated as\npenalty terms defined by the maximum mean discrepancy in the theory of Hilbert\nspace embeddings of probability measures. We prove that the transport maps\ngiven by the proposed methods converge to optimal transport maps in the problem\nwith $L^2$ cost. Several numerical experiments validate our methods. In\nparticular, we show that our methods are applicable to large-scale Monge\nproblems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.03478v4", "cate": "math.OC", "date": "2024-12-04", "updated": "2025-07-14"}
{"id": "2506.23553", "title": "Human-CLAP: Human-perception-based contrastive language-audio pretraining", "authors": ["Taisei Takano", "Yuki Okamoto", "Yusuke Kanamori", "Yuki Saito", "Ryotaro Nagase", "Hiroshi Saruwatari"], "categories": ["eess.AS", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Submitted to APSIPA ASC 2025", "url": "http://arxiv.org/abs/2506.23553v2", "summary": "Contrastive language-audio pretraining (CLAP) is widely used for audio\ngeneration and recognition tasks. For example, CLAPScore, which utilizes the\nsimilarity of CLAP embeddings, has been a major metric for the evaluation of\nthe relevance between audio and text in text-to-audio. However, the\nrelationship between CLAPScore and human subjective evaluation scores is still\nunclarified. We show that CLAPScore has a low correlation with human subjective\nevaluation scores. Additionally, we propose a human-perception-based CLAP\ncalled Human-CLAP by training a contrastive language-audio model using the\nsubjective evaluation score. In our experiments, the results indicate that our\nHuman-CLAP improved the Spearman's rank correlation coefficient (SRCC) between\nthe CLAPScore and the subjective evaluation scores by more than 0.25 compared\nwith the conventional CLAP.", "comment": "Submitted to APSIPA ASC 2025", "pdf_url": "http://arxiv.org/pdf/2506.23553v2", "cate": "eess.AS", "date": "2025-06-30", "updated": "2025-07-12"}
{"id": "2507.10394", "title": "The Reconfigurable Earth Observation Satellite Scheduling Problem", "authors": ["Brycen D. Pearl", "Joseph M. Miller", "Hang Woon Lee"], "categories": ["math.OC", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      48 pages", "url": "http://arxiv.org/abs/2507.10394v1", "summary": "Earth observation satellites (EOS) play a pivotal role in capturing and\nanalyzing planetary phenomena, ranging from natural disasters to societal\ndevelopment. The EOS scheduling problem (EOSSP), which optimizes the schedule\nof EOS, is often solved with respect to nadir-directional EOS systems, thus\nrestricting the observation time of targets and, consequently, the\neffectiveness of each EOS. This paper leverages state-of-the-art constellation\nreconfigurability to develop the reconfigurable EOS scheduling problem\n(REOSSP), wherein EOS are assumed to be maneuverable, forming a more optimal\nconstellation configuration at multiple opportunities during a schedule. This\npaper develops a novel mixed-integer linear programming formulation for the\nREOSSP to optimally solve the scheduling problem for given parameters.\nAdditionally, since the REOSSP can be computationally expensive for large-scale\nproblems, a rolling horizon procedure (RHP) solution method is developed. The\nperformance of the REOSSP is benchmarked against the EOSSP, which serves as a\nbaseline, through a set of random instances where problem characteristics are\nvaried and a case study in which Hurricane Sandy is used to demonstrate\nrealistic performance. These experiments demonstrate the value of constellation\nreconfigurability in its application to the EOSSP, yielding solutions that\nimprove performance, while the RHP enhances computational runtime for\nlarge-scale REOSSP instances.", "comment": "48 pages", "pdf_url": "http://arxiv.org/pdf/2507.10394v1", "cate": "math.OC", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2403.03117", "title": "Input-Output Extension of Underactuated Nonlinear Systems", "authors": ["Mirko Mizzoni", "Amr Afifi", "Antonio Franchi"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2403.03117v3", "summary": "This letter proposes a method to integrate auxiliary actuators that enhance\nthe task-space capabilities of commercial underactuated systems, while leaving\nthe internal certified low-level controller untouched. The additional actuators\nare combined with a feedback-linearizing outer-loop controller, enabling\nfull-pose tracking. We provide conditions under which legacy high-level\ncommands and new actuator inputs can be cohesively coordinated to achieve\ndecoupled control of all degrees of freedom. A comparative study with a\nstandard quadrotor-originally not designed for physical\ninteraction-demonstrates that the proposed modified platform remains stable\nunder contact, while the baseline system diverges. Additionally, simulation\nresults under parameter uncertainty illustrate the robustness of the proposed\napproach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2403.03117v3", "cate": "eess.SY", "date": "2024-03-05", "updated": "2025-07-12"}
{"id": "2507.09386", "title": "Free-running vs. Synchronous: Single-Photon Lidar for High-flux 3D Imaging", "authors": ["Ruangrawee Kitichotkul", "Shashwath Bharadwaj", "Joshua Rapp", "Yanting Ma", "Alexander Mehta", "Vivek K Goyal"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      20 pages, 15 figures, to be presented at the International Conference on Computer Vision (ICCV) 2025", "url": "http://arxiv.org/abs/2507.09386v1", "summary": "Conventional wisdom suggests that single-photon lidar (SPL) should operate in\nlow-light conditions to minimize dead-time effects. Many methods have been\ndeveloped to mitigate these effects in synchronous SPL systems. However,\nsolutions for free-running SPL remain limited despite the advantage of reduced\nhistogram distortion from dead times. To improve the accuracy of free-running\nSPL, we propose a computationally efficient joint maximum likelihood estimator\nof the signal flux, the background flux, and the depth using only histograms,\nalong with a complementary regularization framework that incorporates a learned\npoint cloud score model as a prior. Simulations and experiments demonstrate\nthat free-running SPL yields lower estimation errors than its synchronous\ncounterpart under identical conditions, with our regularization further\nimproving accuracy.", "comment": "20 pages, 15 figures, to be presented at the International Conference\n  on Computer Vision (ICCV) 2025", "pdf_url": "http://arxiv.org/pdf/2507.09386v1", "cate": "eess.SP", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.08952", "title": "Interpretable Artificial Intelligence for Detecting Acute Heart Failure on Acute Chest CT Scans", "authors": ["Silas Nyboe Ørting", "Kristina Miger", "Anne Sophie Overgaard Olesen", "Mikael Ploug Boesen", "Michael Brun Andersen", "Jens Petersen", "Olav W. Nielsen", "Marleen de Bruijne"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      34 pages, 11 figures, Submitted to \"Radiology AI\"", "url": "http://arxiv.org/abs/2507.08952v1", "summary": "Introduction: Chest CT scans are increasingly used in dyspneic patients where\nacute heart failure (AHF) is a key differential diagnosis. Interpretation\nremains challenging and radiology reports are frequently delayed due to a\nradiologist shortage, although flagging such information for emergency\nphysicians would have therapeutic implication. Artificial intelligence (AI) can\nbe a complementary tool to enhance the diagnostic precision. We aim to develop\nan explainable AI model to detect radiological signs of AHF in chest CT with an\naccuracy comparable to thoracic radiologists.\n  Methods: A single-center, retrospective study during 2016-2021 at Copenhagen\nUniversity Hospital - Bispebjerg and Frederiksberg, Denmark. A Boosted Trees\nmodel was trained to predict AHF based on measurements of segmented cardiac and\npulmonary structures from acute thoracic CT scans. Diagnostic labels for\ntraining and testing were extracted from radiology reports. Structures were\nsegmented with TotalSegmentator. Shapley Additive explanations values were used\nto explain the impact of each measurement on the final prediction.\n  Results: Of the 4,672 subjects, 49% were female. The final model incorporated\ntwelve key features of AHF and achieved an area under the ROC of 0.87 on the\nindependent test set. Expert radiologist review of model misclassifications\nfound that 24 out of 64 (38%) false positives and 24 out of 61 (39%) false\nnegatives were actually correct model predictions, with the errors originating\nfrom inaccuracies in the initial radiology reports.\n  Conclusion: We developed an explainable AI model with strong discriminatory\nperformance, comparable to thoracic radiologists. The AI model's stepwise,\ntransparent predictions may support decision-making.", "comment": "34 pages, 11 figures, Submitted to \"Radiology AI\"", "pdf_url": "http://arxiv.org/pdf/2507.08952v1", "cate": "eess.IV", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09703", "title": "EPT-2 Technical Report", "authors": ["Roberto Molinaro", "Niall Siegenheim", "Niels Poulsen", "Jordan Dane Daubinet", "Henry Martin", "Mark Frey", "Kevin Thiart", "Alexander Jakob Dautel", "Andreas Schlueter", "Alex Grigoryev", "Bogdan Danciu", "Nikoo Ekhtiari", "Bas Steunebrink", "Leonie Wagner", "Marvin Vincent Gabler"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09703v1", "summary": "We present EPT-2, the latest iteration in our Earth Physics Transformer (EPT)\nfamily of foundation AI models for Earth system forecasting. EPT-2 delivers\nsubstantial improvements over its predecessor, EPT-1.5, and sets a new state of\nthe art in predicting energy-relevant variables-including 10m and 100m wind\nspeed, 2m temperature, and surface solar radiation-across the full 0-240h\nforecast horizon. It consistently outperforms leading AI weather models such as\nMicrosoft Aurora, as well as the operational numerical forecast system IFS HRES\nfrom the European Centre for Medium-Range Weather Forecasts (ECMWF). In\nparallel, we introduce a perturbation-based ensemble model of EPT-2 for\nprobabilistic forecasting, called EPT-2e. Remarkably, EPT-2e significantly\nsurpasses the ECMWF ENS mean-long considered the gold standard for medium- to\nlongrange forecasting-while operating at a fraction of the computational cost.\nEPT models, as well as third-party forecasts, are accessible via the app.jua.ai\nplatform.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09703v1", "cate": "cs.LG", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09993", "title": "3DGAA: Realistic and Robust 3D Gaussian-based Adversarial Attack for Autonomous Driving", "authors": ["Yixun Zhang", "Lizhi Wang", "Junjun Zhao", "Wending Zhao", "Feng Zhou", "Yonghao Dang", "Jianqin Yin"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Submitted to WACV 2026", "url": "http://arxiv.org/abs/2507.09993v1", "summary": "Camera-based object detection systems play a vital role in autonomous\ndriving, yet they remain vulnerable to adversarial threats in real-world\nenvironments. While existing 2D and 3D physical attacks typically optimize\ntexture, they often struggle to balance physical realism and attack robustness.\nIn this work, we propose 3D Gaussian-based Adversarial Attack (3DGAA), a novel\nadversarial object generation framework that leverages the full 14-dimensional\nparameterization of 3D Gaussian Splatting (3DGS) to jointly optimize geometry\nand appearance in physically realizable ways. Unlike prior works that rely on\npatches or texture, 3DGAA jointly perturbs both geometric attributes (shape,\nscale, rotation) and appearance attributes (color, opacity) to produce\nphysically realistic and transferable adversarial objects. We further introduce\na physical filtering module to preserve geometric fidelity, and a physical\naugmentation module to simulate complex physical scenarios, thus enhancing\nattack generalization under real-world conditions. We evaluate 3DGAA on both\nvirtual benchmarks and physical-world setups using miniature vehicle models.\nExperimental results show that 3DGAA achieves to reduce the detection mAP from\n87.21% to 7.38%, significantly outperforming existing 3D physical attacks.\nMoreover, our method maintains high transferability across different physical\nconditions, demonstrating a new state-of-the-art in physically realizable\nadversarial attacks. These results validate 3DGAA as a practical attack\nframework for evaluating the safety of perception systems in autonomous\ndriving.", "comment": "Submitted to WACV 2026", "pdf_url": "http://arxiv.org/pdf/2507.09993v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2505.03982", "title": "Alternating projections between two inconsistent affine subspaces with varying relaxation", "authors": ["Nguyen T. Thao"], "categories": ["math.FA", "cs.NA", "math.NA", "G.1.2"], "primary_category": "Subjects:       Functional Analysis (math.FA)", "pdf_link": null, "comments": "Comments:      11 pages", "url": "http://arxiv.org/abs/2505.03982v2", "summary": "In a Hilbert space, we study the strong convergence of alternating\nprojections between two inconsistent affine subspaces with varying relaxation\non one side. New convergence results are obtained by seeing the alternating\nprojections as a Landweber iteration with variable steps.", "comment": "11 pages", "pdf_url": "http://arxiv.org/pdf/2505.03982v2", "cate": "math.FA", "date": "2025-05-06", "updated": "2025-07-13"}
{"id": "2507.04959", "title": "Hear-Your-Click: Interactive Object-Specific Video-to-Audio Generation", "authors": ["Yingshan Liang", "Keyu Fan", "Zhicheng Du", "Yiran Wang", "Qingyang Shi", "Xinyu Zhang", "Jiasheng Lu", "Peiwu Qin"], "categories": ["cs.CV", "cs.AI", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.04959v2", "summary": "Video-to-audio (V2A) generation shows great potential in fields such as film\nproduction. Despite significant advances, current V2A methods relying on global\nvideo information struggle with complex scenes and generating audio tailored to\nspecific objects. To address these limitations, we introduce Hear-Your-Click,\nan interactive V2A framework enabling users to generate sounds for specific\nobjects by clicking on the frame. To achieve this, we propose Object-aware\nContrastive Audio-Visual Fine-tuning (OCAV) with a Mask-guided Visual Encoder\n(MVE) to obtain object-level visual features aligned with audio. Furthermore,\nwe tailor two data augmentation strategies, Random Video Stitching (RVS) and\nMask-guided Loudness Modulation (MLM), to enhance the model's sensitivity to\nsegmented objects. To measure audio-visual correspondence, we designed a new\nevaluation metric, the CAV score. Extensive experiments demonstrate that our\nframework offers more precise control and improves generation performance\nacross various metrics. Project Page:\nhttps://github.com/SynapGrid/Hear-Your-Click", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.04959v2", "cate": "cs.CV", "date": "2025-07-07", "updated": "2025-07-13"}
{"id": "2405.02487", "title": "Distributed Online Feedback Optimization for Real-time Distribution System Voltage Regulation", "authors": ["Sen Zhan", "Nikolaos G. Paterakis", "Wouter van den Akker", "Anne van der Molen", "Johan Morren", "Han Slootweg"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      accepted for publication in IEEE Transactions on Power Systems, 2025", "url": "http://arxiv.org/abs/2405.02487v4", "summary": "We investigate the real-time voltage regulation problem in distribution\nsystems employing online feedback optimization (OFO) with short-range\ncommunication between physical neighbours. OFO does not need an accurate grid\nmodel nor estimated consumption of non-controllable loads, affords fast\ncalculations, and demonstrates robustness to uncertainties and disturbances,\nwhich render it particularly suitable for real-time distribution system\napplications. However, many OFO controllers require centralized communication,\nmaking them susceptible to single-point failures. This paper proposes a\ndistributed OFO design based on a nested feedback optimization strategy and\nanalyzes its convergence. The strategy preserves end-users' privacy by keeping\nvoltage data local. Numerical study results demonstrate that the proposed\ndesign achieves effective voltage regulation and outperforms other distributed\nand local approaches.", "comment": "accepted for publication in IEEE Transactions on Power Systems, 2025", "pdf_url": "http://arxiv.org/pdf/2405.02487v4", "cate": "eess.SY", "date": "2024-05-03", "updated": "2025-07-13"}
{"id": "2410.00244", "title": "Quantifying the Dunkelflaute: An analysis of variable renewable energy droughts in Europe", "authors": ["Martin Kittel", "Wolf-Peter Schill"], "categories": ["eess.SY", "cs.SY", "physics.ao-ph"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.00244v2", "summary": "Variable renewable energy droughts, also called \"Dunkelflaute\", emerge as a\nchallenge for climate-neutral energy systems based on variable renewables.\nDrawing on 38 historic weather years and an advanced identification method, we\ncharacterize European drought events for on- and offshore wind power, solar\nphotovoltaics, and renewable technology portfolios. We show that their\ncharacteristics heavily depend on the chosen drought threshold, questioning the\nusefulness of single-threshold analyses. Applying a multi-threshold framework,\nwe quantify how the complementarity of wind and solar power temporally and\nspatially alleviates drought frequency, duration, and severity within\n(portfolio effect) and across countries (balancing effect). We identify the\nmost extreme droughts and show how these drive major discharging periods of\nlong-duration storage in a fully renewable European energy system, based on a\npolicy-relevant decarbonization scenario. Such events comprise sequences of\nshorter droughts of varying severity. The most extreme event occurred in winter\n1996/97 and lasted 55 days in a perfectly interconnected setting. While the\naverage renewable availability during this period was still 47% of its long-run\nmean, we argue that system planners must consider such events when planning for\nstorage and other flexibility technologies. Methodologically, we conclude that\nusing single calendar years is not suitable for modeling weather-resilient\nenergy scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.00244v2", "cate": "eess.SY", "date": "2024-09-30", "updated": "2025-07-12"}
{"id": "2507.09408", "title": "Lightweight Graph Neural Networks for Enhanced 5G NR Channel Estimation", "authors": ["Sajedeh Norouzi", "Mostafa Rahmani", "Yi Chu", "Torsten Braun", "Kaushik Chowdhury", "Alister Burr"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      Accepted in IEEE PIMRC 2025", "url": "http://arxiv.org/abs/2507.09408v1", "summary": "Effective channel estimation CE is critical for optimizing the performance of\n5G New Radio NR systems particularly in dynamic environments where traditional\nmethods struggle with complexity and adaptability This paper introduces\nGraphNet a novel lightweight Graph Neural Network GNNbased estimator designed\nto enhance CE in 5G NR Our proposed method utilizes a GNN architecture that\nminimizes computational overhead while capturing essential features necessary\nfor accurate CE We evaluate GraphNet across various channel conditions from\nslowvarying to highly dynamic environments and compare its performance to\nChannelNet a wellknown deep learningbased CE method GraphNet not only matches\nChannelNets performance in stable conditions but significantly outperforms it\nin highvariation scenarios particularly in terms of Block Error Rate It also\nincludes builtin noise estimation that enhances robustness in challenging\nchannel conditions Furthermore its significantly lighter computational\nfootprint makes GraphNet highly suitable for realtime deployment especially on\nedge devices with limited computational resources By underscoring the potential\nof GNNs to transform CE processes GraphNet offers a scalable and robust\nsolution that aligns with the evolving demands of 5G technologies highlighting\nits efficiency and performance as a nextgeneration solution for wireless\ncommunication systems", "comment": "Accepted in IEEE PIMRC 2025", "pdf_url": "http://arxiv.org/pdf/2507.09408v1", "cate": "eess.SP", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.08982", "title": "VIP: Visual Information Protection through Adversarial Attacks on Vision-Language Models", "authors": ["Hanene F. Z. Brachemi Meftah", "Wassim Hamidouche", "Sid Ahmed Fezza", "Olivier Déforges"], "categories": ["eess.IV", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08982v1", "summary": "Recent years have witnessed remarkable progress in developing Vision-Language\nModels (VLMs) capable of processing both textual and visual inputs. These\nmodels have demonstrated impressive performance, leading to their widespread\nadoption in various applications. However, this widespread raises serious\nconcerns regarding user privacy, particularly when models inadvertently process\nor expose private visual information. In this work, we frame the preservation\nof privacy in VLMs as an adversarial attack problem. We propose a novel attack\nstrategy that selectively conceals information within designated Region Of\nInterests (ROIs) in an image, effectively preventing VLMs from accessing\nsensitive content while preserving the semantic integrity of the remaining\nimage. Unlike conventional adversarial attacks that often disrupt the entire\nimage, our method maintains high coherence in unmasked areas. Experimental\nresults across three state-of-the-art VLMs namely LLaVA, Instruct-BLIP, and\nBLIP2-T5 demonstrate up to 98% reduction in detecting targeted ROIs, while\nmaintaining global image semantics intact, as confirmed by high similarity\nscores between clean and adversarial outputs. We believe that this work\ncontributes to a more privacy conscious use of multimodal models and offers a\npractical tool for further research, with the source code publicly available\nat: https://github.com/hbrachemi/Vlm_defense-attack.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08982v1", "cate": "eess.IV", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.09733", "title": "Universal Physics Simulation: A Foundational Diffusion Approach", "authors": ["Bradley Camburn"], "categories": ["cs.LG", "cs.AI", "cs.CV", "68T07, 65M06, 78M34", "I.2.6; I.4.8; J.2"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      10 pages, 3 figures. Foundational AI model for universal physics simulation using sketch-guided diffusion transformers. Achieves SSIM > 0.8 on electromagnetic field generation without requiring a priori physics encoding", "url": "http://arxiv.org/abs/2507.09733v1", "summary": "We present the first foundational AI model for universal physics simulation\nthat learns physical laws directly from boundary-condition data without\nrequiring a priori equation encoding. Traditional physics-informed neural\nnetworks (PINNs) and finite-difference methods necessitate explicit\nmathematical formulation of governing equations, fundamentally limiting their\ngeneralizability and discovery potential. Our sketch-guided diffusion\ntransformer approach reimagines computational physics by treating simulation as\na conditional generation problem, where spatial boundary conditions guide the\nsynthesis of physically accurate steady-state solutions.\n  By leveraging enhanced diffusion transformer architectures with novel spatial\nrelationship encoding, our model achieves direct boundary-to-equilibrium\nmapping and is generalizable to diverse physics domains. Unlike sequential\ntime-stepping methods that accumulate errors over iterations, our approach\nbypasses temporal integration entirely, directly generating steady-state\nsolutions with SSIM > 0.8 while maintaining sub-pixel boundary accuracy. Our\ndata-informed approach enables physics discovery through learned\nrepresentations analyzable via Layer-wise Relevance Propagation (LRP),\nrevealing emergent physical relationships without predetermined mathematical\nconstraints. This work represents a paradigm shift from AI-accelerated physics\nto AI-discovered physics, establishing the first truly universal physics\nsimulation framework.", "comment": "10 pages, 3 figures. Foundational AI model for universal physics\n  simulation using sketch-guided diffusion transformers. Achieves SSIM > 0.8 on\n  electromagnetic field generation without requiring a priori physics encoding", "pdf_url": "http://arxiv.org/pdf/2507.09733v1", "cate": "cs.LG", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09996", "title": "Leveraging Swin Transformer for enhanced diagnosis of Alzheimer's disease using multi-shell diffusion MRI", "authors": ["Quentin Dessain", "Nicolas Delinte", "Bernard Hanseeuw", "Laurence Dricot", "Benoît Macq"], "categories": ["cs.CV", "q-bio.NC", "q-bio.QM"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09996v1", "summary": "Objective: This study aims to support early diagnosis of Alzheimer's disease\nand detection of amyloid accumulation by leveraging the microstructural\ninformation available in multi-shell diffusion MRI (dMRI) data, using a vision\ntransformer-based deep learning framework.\n  Methods: We present a classification pipeline that employs the Swin\nTransformer, a hierarchical vision transformer model, on multi-shell dMRI data\nfor the classification of Alzheimer's disease and amyloid presence. Key metrics\nfrom DTI and NODDI were extracted and projected onto 2D planes to enable\ntransfer learning with ImageNet-pretrained models. To efficiently adapt the\ntransformer to limited labeled neuroimaging data, we integrated Low-Rank\nAdaptation. We assessed the framework on diagnostic group prediction\n(cognitively normal, mild cognitive impairment, Alzheimer's disease dementia)\nand amyloid status classification.\n  Results: The framework achieved competitive classification results within the\nscope of multi-shell dMRI-based features, with the best balanced accuracy of\n95.2% for distinguishing cognitively normal individuals from those with\nAlzheimer's disease dementia using NODDI metrics. For amyloid detection, it\nreached 77.2% balanced accuracy in distinguishing amyloid-positive mild\ncognitive impairment/Alzheimer's disease dementia subjects from\namyloid-negative cognitively normal subjects, and 67.9% for identifying\namyloid-positive individuals among cognitively normal subjects. Grad-CAM-based\nexplainability analysis identified clinically relevant brain regions, including\nthe parahippocampal gyrus and hippocampus, as key contributors to model\npredictions.\n  Conclusion: This study demonstrates the promise of diffusion MRI and\ntransformer-based architectures for early detection of Alzheimer's disease and\namyloid pathology, supporting biomarker-driven diagnostics in data-limited\nbiomedical settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09996v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2506.07094", "title": "CIR bridge for modeling of fish migration on sub-hourly scale", "authors": ["Hidekazu Yoshioka"], "categories": ["math.PR", "cs.NA", "math.NA"], "primary_category": "Subjects:       Probability (math.PR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.07094v3", "summary": "Bridges, which are stochastic processes with pinned initial and terminal\nconditions, have recently been applied to various problems. We show that a\nbridge based on the Cox-Ingersoll-Ross process, called a CIR bridge in this\npaper, reasonably models the intraday number of migrating fish at an\nobservation point in a river. The studied fish migrates between sunrise and\nsunset each day, which are considered the initial and terminal times,\nrespectively. The CIR bridge is well-defined as a unique pathwise continuous\nsolution to a stochastic differential equation with unbounded drift and\ndiffusion coefficients and potentially represents the on-off intermittency of\nthe fish count data. Our bridge is theoretically novel in that it admits\nclosed-form time-dependent averages and variances, with which the model\nparameters can be identified efficiently, and is computable by a\nrecently-developed one-step numerical method. The CIR bridge is applied to the\nsub-hourly migration data of the diadromous fish Plecoglossus altivelis\naltivelis in the Nagara River, Japan, from February to June.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.07094v3", "cate": "math.PR", "date": "2025-06-08", "updated": "2025-07-12"}
{"id": "2507.07803", "title": "StreamUni: Achieving Streaming Speech Translation with a Unified Large Speech-Language Model", "authors": ["Shoutao Guo", "Xiang Li", "Mengge Liu", "Wei Chen", "Yang Feng"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      The code is at this https URL The model is at this https URL", "url": "http://arxiv.org/abs/2507.07803v2", "summary": "Streaming speech translation (StreamST) requires determining appropriate\ntiming, known as policy, to generate translations while continuously receiving\nsource speech inputs, balancing low latency with high translation quality.\nHowever, existing StreamST methods typically operate on sentence-level speech\nsegments, referred to as simultaneous speech translation (SimulST). In\npractice, they require collaboration with segmentation models to accomplish\nStreamST, where the truncated speech segments constrain SimulST models to make\npolicy decisions and generate translations based on limited contextual\ninformation. Moreover, SimulST models struggle to learn effective policies due\nto the complexity of speech inputs and cross-lingual generation. To address\nthese challenges, we propose StreamUni, which achieves StreamST through a\nunified Large Speech-Language Model (LSLM). Specifically, StreamUni\nincorporates speech Chain-of-Thought (CoT) in guiding the LSLM to generate\nmulti-stage outputs. Leveraging these multi-stage outputs, StreamUni\nsimultaneously accomplishes speech segmentation, policy decision, and\ntranslation generation, completing StreamST without requiring massive\npolicy-specific training. Additionally, we propose a streaming CoT training\nmethod that enhances low-latency policy decisions and generation capabilities\nusing limited CoT data. Experiments demonstrate that our approach achieves\nstate-of-the-art performance on StreamST tasks.", "comment": "The code is at https://github.com/ictnlp/StreamUni; The model is at\n  https://huggingface.co/ICTNLP/StreamUni-Phi4", "pdf_url": "http://arxiv.org/pdf/2507.07803v2", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-13"}
{"id": "2410.03894", "title": "A Machine Learning-Based Reference Governor for Nonlinear Systems With Application to Automotive Fuel Cells", "authors": ["Mostafaali Ayubirad", "Hamid R. Ossareh"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      Accepted for publication in IEEE Transactions on Control Systems Technology. DOI: https://doi.org/10.1109/TCST.2025.3579514 . This version incorporates all peer-review revisions", "url": "http://arxiv.org/abs/2410.03894v2", "summary": "The prediction-based nonlinear reference governor (PRG) is an add-on\nalgorithm to enforce constraints on pre-stabilized nonlinear systems by\nmodifying, whenever necessary, the reference signal. The implementation of PRG\ncarries a heavy computational burden, as it may require multiple numerical\nsimulations of the plant model at each sample time. To this end, this paper\nproposes an alternative approach based on machine learning, where we first use\na regression neural network (NN) to approximate the input-output map of the PRG\nfrom a set of training data. During the real-time operation, at each sample\ntime, we use the trained NN to compute a nominal reference command, which may\nnot be constraint admissible due to training errors and limited data. We adopt\na novel sensitivity-based approach to minimally adjust the nominal reference\nwhile ensuring constraint enforcement. We thus refer to the resulting control\nstrategy as the modified neural network reference governor (MNN-RG), which is\nsignificantly more computationally efficient than the PRG. The computational\nand theoretical properties of MNN-RG are presented. Finally, the effectiveness\nand limitations of the proposed method are studied by applying it as a load\ngovernor for constraint management in automotive fuel cell systems through\nsimulation-based case studies.", "comment": "Accepted for publication in IEEE Transactions on Control Systems\n  Technology. DOI: 10.1109/TCST.2025.3579514. This version incorporates all\n  peer-review revisions", "pdf_url": "http://arxiv.org/pdf/2410.03894v2", "cate": "eess.SY", "date": "2024-10-04", "updated": "2025-07-13"}
{"id": "2410.10681", "title": "A System Parameterization for Direct Data-Driven Estimator Synthesis", "authors": ["Felix Brändle", "Frank Allgöwer"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      This work has been submitted to the American Control Conference 2025", "url": "http://arxiv.org/abs/2410.10681v2", "summary": "This paper introduces a novel parameterization to characterize unknown linear\ntime-invariant systems using noisy data. The presented parameterization\ndescribes exactly the set of all systems consistent with the available data. We\nthen derive verifiable conditions, when the consistency constraint reduces the\nset to the true system and when it does not have any impact. Furthermore, we\ndemonstrate how to use this parameterization to perform a direct data-driven\nestimator synthesis with guarantees on the H_{\\infty}-norm. Lastly, we conduct\nnumerical experiments to compare our approach to existing methods.", "comment": "This work has been submitted to the American Control Conference 2025", "pdf_url": "http://arxiv.org/pdf/2410.10681v2", "cate": "eess.SY", "date": "2024-10-14", "updated": "2025-07-14"}
{"id": "2507.09458", "title": "An Enregy Efficient Design of Hybrid NOMA Based on Hybrid SIC with Power Adaptation", "authors": ["Wang Ning", "Zhang Chenyu", "Sun Yanshi", "Min Minghui", "Liu Yuanwei", "Li Shiyin"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      13pages, 8figures, 4tables. Submitted to IEEE TWC, manuscript ID is Paper-TW-Jul-25-1790", "url": "http://arxiv.org/abs/2507.09458v1", "summary": "Recently, hybrid non-orthogonal multiple access (H-NOMA) technology, which\neffectively utilizes both NOMA and orthogonal multiple access (OMA)\ntechnologies through flexible resource allocation in a single transmission, has\ndemonstrated immense potential for enhancing the performance of wireless\ncommunication systems. To further release the potential of HNOMA, this paper\nproposes a novel design of H-NOMA which jointly incorporates hybrid successive\ninterference cancellation (HSIC) and power adaptation (PA) in the NOMA\ntransmission phase. To reveal the potential of the proposed HSIC-PA aided\nH-NOMA scheme, closed-form expression for the probability of the event that\nH-NOMA can achieve a higher data rate than pure OMA by consuming less energy is\nrigorously derived. Furthermore, the asymptotic analysis demonstrates that the\nprobability of the proposed H-NOMA scheme approaches 1 in the high\nsignal-to-noise ratio (SNR) regime without any constraints on either users'\ntarget rates or transmit power ratios. This represents a significant\nimprovement over conventional H-NOMA schemes, which require specific\nrestrictive conditions to achieve probability 1 at high SNRs as shown in\nexisting work. The above observation indicates that with less energy\nconsumption, the proposed HSIC-PA aided H-NOMA can achieve a higher data rate\nthan pure OMA with probability 1 at high SNRs, and hence a higher energy\nefficiency. Finally, numerical results are provided to verify the accuracy of\nthe analysis and also demonstrate the superior performance of the proposed\nH-NOMA scheme.", "comment": "13pages, 8figures, 4tables. Submitted to IEEE TWC, manuscript ID is\n  Paper-TW-Jul-25-1790", "pdf_url": "http://arxiv.org/pdf/2507.09458v1", "cate": "eess.SP", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09236", "title": "Encryption and Authentication with a Lensless Camera Based on a Programmable Mask", "authors": ["Eric Bezzam", "Martin Vetterli"], "categories": ["eess.IV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      6 pages main, 3 pages supplementary, accepted to ICME 2025", "url": "http://arxiv.org/abs/2507.09236v1", "summary": "Lensless cameras replace traditional optics with thin masks, leading to\nhighly multiplexed measurements akin to encryption. However, static masks in\nconventional designs leave systems vulnerable to simple attacks. This work\nexplores the use of programmable masks to enhance security by dynamically\nvarying the mask patterns. We perform our experiments with a low-cost system\n(around 100 USD) based on a liquid crystal display. Experimental results\ndemonstrate that variable masks successfully block a variety of attacks while\nenabling high-quality recovery for legitimate users. The system's encryption\nstrength exceeds AES-256, achieving effective key lengths over 2'500 bits.\nAdditionally, we demonstrate how a programmable mask enables robust\nauthentication and verification, as each mask pattern leaves a unique\nfingerprint on the image. When combined with a lensed system, lensless\nmeasurements can serve as analog certificates, providing a novel solution for\nverifying image authenticity and combating deepfakes.", "comment": "6 pages main, 3 pages supplementary, accepted to ICME 2025", "pdf_url": "http://arxiv.org/pdf/2507.09236v1", "cate": "eess.IV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09318", "title": "ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation with Flow Matching", "authors": ["Han Zhu", "Wei Kang", "Liyong Guo", "Zengwei Yao", "Fangjun Kuang", "Weiji Zhuang", "Zhaoqing Li", "Zhifeng Han", "Dong Zhang", "Xin Zhang", "Xingchen Song", "Long Lin", "Daniel Povey"], "categories": ["eess.AS", "cs.CL"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09318v1", "summary": "Generating spoken dialogue is more challenging than monologue text-to-speech\n(TTS) due to the need for realistic turn-taking and distinct speaker timbres.\nExisting spoken dialogue generation models, being auto-regressive, suffer from\nslow and unstable inference. To overcome these limitations, we introduce\nZipVoice-Dialog, a non-autoregressive zero-shot spoken dialogue generation\nmodel built upon flow matching. Key designs include: 1) speaker-turn embeddings\nfor precise speaker turn-taking; 2) a curriculum learning strategy for stable\nspeech-text alignment; 3) specialized strategies to enable stereo dialogue\ngeneration. Additionally, recognizing the lack of open-source large-scale\nspoken dialogue datasets, we curated OpenDialog, a 6.8k-hour spoken dialogue\ndataset from in-the-wild speech data. Furthermore, we established a benchmark\nto comprehensively evaluate various models. Experimental results demonstrate\nthat ZipVoice-Dialog achieves superior performance in intelligibility, speaker\nturn-taking accuracy, speaker similarity, and inference speed. Our codes, model\ncheckpoints, demo samples, and the OpenDialog dataset are all publicly\navailable at https://github.com/k2-fsa/ZipVoice.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09318v1", "cate": "eess.AS", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09759", "title": "AI-Enhanced Pediatric Pneumonia Detection: A CNN-Based Approach Using Data Augmentation and Generative Adversarial Networks (GANs)", "authors": ["Abdul Manaf", "Nimra Mughal"], "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09759v1", "summary": "Pneumonia is a leading cause of mortality in children under five, requiring\naccurate chest X-ray diagnosis. This study presents a machine learning-based\nPediatric Chest Pneumonia Classification System to assist healthcare\nprofessionals in diagnosing pneumonia from chest X-ray images. The CNN-based\nmodel was trained on 5,863 labeled chest X-ray images from children aged 0-5\nyears from the Guangzhou Women and Children's Medical Center. To address\nlimited data, we applied augmentation techniques (rotation, zooming, shear,\nhorizontal flipping) and employed GANs to generate synthetic images, addressing\nclass imbalance. The system achieved optimal performance using combined\noriginal, augmented, and GAN-generated data, evaluated through accuracy and F1\nscore metrics. The final model was deployed via a Flask web application,\nenabling real-time classification with probability estimates. Results\ndemonstrate the potential of deep learning and GANs in improving diagnostic\naccuracy and efficiency for pediatric pneumonia classification, particularly\nvaluable in resource-limited clinical settings\nhttps://github.com/AbdulManaf12/Pediatric-Chest-Pneumonia-Classification", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09759v1", "cate": "eess.IV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.10006", "title": "Vision-Based Anti Unmanned Aerial Technology: Opportunities and Challenges", "authors": ["Guanghai Ding", "Yihua Ren", "Yuting Liu", "Qijun Zhao", "Shuiwang Li"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10006v1", "summary": "With the rapid advancement of UAV technology and its extensive application in\nvarious fields such as military reconnaissance, environmental monitoring, and\nlogistics, achieving efficient and accurate Anti-UAV tracking has become\nessential. The importance of Anti-UAV tracking is increasingly prominent,\nespecially in scenarios such as public safety, border patrol, search and\nrescue, and agricultural monitoring, where operations in complex environments\ncan provide enhanced security. Current mainstream Anti-UAV tracking\ntechnologies are primarily centered around computer vision techniques,\nparticularly those that integrate multi-sensor data fusion with advanced\ndetection and tracking algorithms. This paper first reviews the characteristics\nand current challenges of Anti-UAV detection and tracking technologies. Next,\nit investigates and compiles several publicly available datasets, providing\naccessible links to support researchers in efficiently addressing related\nchallenges. Furthermore, the paper analyzes the major vision-based and\nvision-fusion-based Anti-UAV detection and tracking algorithms proposed in\nrecent years. Finally, based on the above research, this paper outlines future\nresearch directions, aiming to provide valuable insights for advancing the\nfield.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10006v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2411.06161", "title": "A New 8/14 Two-Phase Switched Reluctance Motor", "authors": ["Gholamreza Davarpanah", "Hossein Shirzad", "Jawad Faiz"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.06161v4", "summary": "Despite their simple and robust structure, low cost, and simple cooling\nsystem, switched reluctance motors (SRMs) face the challenge of low mean\ntorque. A possible solution is to change the structure of SRMs. This article\nintroduces an innovative combination of the number of rotor teeth and stator\nteeth of a two-phase switch reluctance motor (TPSRM) with eight teeth for the\nstator and fourteen teeth for the rotor. As a result of its unique design,\nwhich has a short path for passing the main flux, it requires less\nmagnetomotive force. This leads to less core and copper loss, resulting in\nincreased efficiency. Each tooth of the stator in a phase develops a positive\ntorque during the rotation of the rotor, which increases the torque and\nconsequently increases the mean torque of the proposed TPSRM. A current\nhysteresis control (CHC) is simulated by 2D FEM for the proposed 8/14 TPSRM and\nthe conventional 8/12 TPSRM under the same mechanical load on the shaft to get\na current hysteresis reference of 15A at the nominal speed of 600 rpm. To\nverify the novelty and advantages of the suggested TPSRM, it is compared with\nthe conventional 8/12 TPSRM in terms of mean and peak torque, torque density,\nand core and copper losses were compared. Lastly, the proposed 8/14 TPSRM is\nshown to have better performance than the conventional 8/12 TPSRM.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.06161v4", "cate": "eess.SY", "date": "2024-11-09", "updated": "2025-07-13"}
{"id": "2501.06793", "title": "Differentially Private Gradient-Tracking-Based Distributed Stochastic Optimization over Directed Graphs", "authors": ["Jialong Chen", "Jimin Wang", "Ji-Feng Zhang"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.06793v2", "summary": "This paper proposes a differentially private gradient-tracking-based\ndistributed stochastic optimization algorithm over directed graphs. In\nparticular, privacy noises are incorporated into each agent's state and\ntracking variable to mitigate information leakage, after which the perturbed\nstates and tracking variables are transmitted to neighbors. We design two novel\nschemes for the step-sizes and the sampling number within the algorithm. The\nsampling parameter-controlled subsampling method employed by both schemes\nenhances the differential privacy level, and ensures a finite cumulative\nprivacy budget even over infinite iterations. The algorithm achieves both\nalmost sure and mean square convergence for nonconvex objectives. Furthermore,\nwhen nonconvex objectives satisfy the Polyak-Lojasiewicz condition, Scheme (S1)\nachieves a polynomial mean square convergence rate, and Scheme (S2) achieves an\nexponential mean square convergence rate. The trade-off between privacy and\nconvergence is presented. The effectiveness of the algorithm and its superior\nperformance compared to existing works are illustrated through numerical\nexamples of distributed training on the benchmark datasets \"MNIST\" and\n\"CIFAR-10\".", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.06793v2", "cate": "eess.SY", "date": "2025-01-12", "updated": "2025-07-14"}
{"id": "2507.09535", "title": "Reframing SAR Target Recognition as Visual Reasoning: A Chain-of-Thought Dataset with Multimodal LLMs", "authors": ["Chaoran Li", "Xingguo Xu", "Siyuan Mu"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09535v1", "summary": "In the context of Synthetic Aperture Radar (SAR) image recognition,\ntraditional methods often struggle with the intrinsic limitations of SAR data,\nsuch as weak texture, high noise, and ambiguous object boundaries. This work\nexplores a novel perspective by reformulating SAR target recognition as a\nmultimodal reasoning task. We leverage multimodal large language models\n(MLLMs), specifically GPT-4o, to perform target classification based on SAR\nimagery, guided by candidate categories and enhanced with Chain-of-Thought\n(CoT) reasoning. A new dataset is constructed based on the FAIR-CSAR benchmark,\ncomprising raw SAR images, structured target annotations, candidate label sets,\nand GPT-generated CoT reasoning chains. Experimental results show that the\nMLLMs are capable of generating logically coherent and interpretable inferences\nin most scenarios. Our analysis highlights both the strengths and current\nlimitations of MLLMs in interpreting SAR imagery, and we provide detailed\ninsights into model behavior through failure case analysis. This work\ndemonstrates the feasibility of incorporating MLLMs into SAR analysis pipelines\nand establishes a foundation for future research in SAR-oriented visual\nreasoning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09535v1", "cate": "eess.SP", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09393", "title": "Deep Image Prior Assisted ISAR Imaging for Missing Data Case", "authors": ["Necmettin Bayar", "Isin Erer", "Deniz Kumlu"], "categories": ["eess.IV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09393v1", "summary": "In Inverse Synthetic Aperture Radar (ISAR), random missing entries of the\nreceived radar echo matrix deteriorate the imaging quality, compromising target\ndistinction from the background. Compressive sensing techniques or matrix\ncompletion prior to conventional imaging have been used in recent years to\nsolve this issue. However, while the former techniques fail to preserve target\ncontinuity due to the sparsity constraint, the latter fails for high missing\nratios. This paper proposes to use deep image prior (DIP) to complete the\ncomplex radar data and then obtain the radar image by conventional Fourier\nimaging. Real and imaginary parts are separately completed by independent deep\nstructures and then put together for the imaging part. The proposed DIP based\nimaging method has been compared with IALM, 2D-SL0 and NNM methods visually and\nquantitatively for both simulated and real data. The results demonstrate an\nincrease of 100% for some extreme cases in terms of RMSE, 50% increase on\nCorrelation and 30% increase on IC metrics quantitatively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09393v1", "cate": "eess.IV", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09350", "title": "Microphone Occlusion Mitigation for Own-Voice Enhancement in Head-Worn Microphone Arrays Using Switching-Adaptive Beamforming", "authors": ["Wiebke Middelberg", "Jung-Suk Lee", "Saeed Bagheri Sereshki", "Ali Aroudi", "Vladimir Tourbabin", "Daniel D. E. Wong"], "categories": ["eess.AS"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Accepted for publication at WASPAA 2025", "url": "http://arxiv.org/abs/2507.09350v1", "summary": "Enhancing the user's own-voice for head-worn microphone arrays is an\nimportant task in noisy environments to allow for easier speech communication\nand user-device interaction. However, a rarely addressed challenge is the\nchange of the microphones' transfer functions when one or more of the\nmicrophones gets occluded by skin, clothes or hair. The underlying problem for\nbeamforming-based speech enhancement is the (potentially rapidly) changing\ntransfer functions of both the own-voice and the noise component that have to\nbe accounted for to achieve optimal performance. In this paper, we address the\nproblem of an occluded microphone in a head-worn microphone array. We\ninvestigate three alternative mitigation approaches by means of (i)\nconventional adaptive beamforming, (ii) switching between a-priori estimates of\nthe beamformer coefficients for the occluded and unoccluded state, and (iii) a\nhybrid approach using a switching-adaptive beamformer. In an evaluation with\nreal-world recordings and simulated occlusion, we demonstrate the advantages of\nthe different approaches in terms of noise reduction, own-voice distortion and\nrobustness against voice activity detection errors.", "comment": "Accepted for publication at WASPAA 2025", "pdf_url": "http://arxiv.org/pdf/2507.09350v1", "cate": "eess.AS", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.09766", "title": "Toward accurate RUL and SOH estimation using reinforced graph-based PINNs enhanced with dynamic weights", "authors": ["Mohamadreza Akbari Pour", "Ali Ghasemzadeh", "MohamadAli Bijarchi", "Mohammad Behshad Shafii"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09766v1", "summary": "Accurate estimation of Remaining Useful Life (RUL) and State of Health (SOH)\nis essential for Prognostics and Health Management (PHM) across a wide range of\nindustrial applications. We propose a novel framework -- Reinforced Graph-Based\nPhysics-Informed Neural Networks Enhanced with Dynamic Weights (RGPD) -- that\ncombines physics-based supervision with advanced spatio-temporal learning.\nGraph Convolutional Recurrent Networks (GCRNs) embed graph-convolutional\nfilters within recurrent units to capture how node representations evolve over\ntime. Graph Attention Convolution (GATConv) leverages a self-attention\nmechanism to compute learnable, edge-wise attention coefficients, dynamically\nweighting neighbor contributions for adaptive spatial aggregation. A Soft\nActor-Critic (SAC) module is positioned between the Temporal Attention Unit\n(TAU) and GCRN to further improve the spatio-temporal learning. This module\nimproves attention and prediction accuracy by dynamically scaling hidden\nrepresentations to minimize noise and highlight informative features. To\nidentify the most relevant physical constraints in each area, Q-learning agents\ndynamically assign weights to physics-informed loss terms, improving\ngeneralization across real-time industrial systems and reducing the need for\nmanual tuning. In both RUL and SOH estimation tasks, the proposed method\nconsistently outperforms state-of-the-art models, demonstrating strong\nrobustness and predictive accuracy across varied degradation patterns across\nthree diverse industrial benchmark datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09766v1", "cate": "cs.LG", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.10009", "title": "Binomial Self-Compensation: Mechanism and Suppression of Motion Error in Phase-Shifting Profilometry", "authors": ["Geyou Zhang", "Kai Liu", "Ce Zhu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10009v1", "summary": "Phase shifting profilometry (PSP) is widely used in high-precision 3D\nscanning due to its high accuracy, robustness, and pixel-wise handling.\nHowever, a fundamental assumption of PSP that the object should remain static\ndoes not hold in dynamic measurement, making PSP susceptible to object motion.\nTo address this challenge, our proposed solution, phase-sequential binomial\nself-compensation (P-BSC), sums successive motion-affected phase frames\nweighted by binomial coefficients. This approach exponentially reduces the\nmotion error in a pixel-wise and frame-wise loopable manner. Despite its\nefficacy, P-BSC suffers from high computational overhead and error accumulation\ndue to its reliance on multi-frame phase calculations and weighted summations.\nInspired by P-BSC, we propose an image-sequential binomial self-compensation\n(I-BSC) to weight sum the homogeneous fringe images instead of successive phase\nframes, which generalizes the BSC concept from phase sequences to image\nsequences. I-BSC computes the arctangent function only once, resolving both\nlimitations in P-BSC. Extensive analysis, simulations, and experiments show\nthat 1) the proposed BSC outperforms existing methods in reducing motion error\nwhile achieving a quasi-single-shot frame rate, i.e., depth map frame rate\nequals to the camera's acquisition rate, enabling 3D reconstruction with high\npixel-depth-temporal resolution; 2) compared to P-BSC, our I-BSC reduces the\ncomputational complexity by one polynomial order, thereby accelerating the\ncomputational frame rate by several to dozen times, while also reaching faster\nmotion error convergence.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10009v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2502.08782", "title": "Balancing service provision by EV aggregator in different TSO-DSO coordination schemes", "authors": ["Hang Nguyen", "Phuong Nguyen", "Koen Kok"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.08782v2", "summary": "The increasing penetration of Distributed Energy Resources (DERs) in the\ndistribution system has led to the emergence of a new market actor - the\naggregator. The aggregator serves as a facilitator, enabling flexibility asset\nowners to get access to different markets. In which, EVs aggregators are\ngaining more attention due to their expanding use and potential to provide\nservices in various types of markets, particularly in the reserve market.\nCurrently, TSO indirectly utilizes these resources under the management of the\ndistribution system operators (DSO), which can negatively impact the\ndistribution grid. Conversely, adjustments from DSOs can impact service\nprovision to TSO due to the shortage of TSO usage information. These factors\nhighlight the importance of evaluating the service provision from aggregators\nunder different TSO-DSO coordination schemes. This paper focuses on the\nprovision of flexibility from electric vehicles (EVs) aggregators for balancing\nservice in the TSO-DSO hybrid-managed and compares it with the DSO-managed\ncoordination schemes. The behavior of aggregators reacting to price\nfluctuations and TSO requests under different coordination schemes and\nsimulation scenarios is thoroughly evaluated. Additionally, their impact on the\ngrid is analyzed through the DSO's congestion management process and validated\nusing data from a real part of the Dutch distribution network. Results find\nthat the hybrid-managed coordination scheme gives more benefit to the\naggregator than the DSO-managed scheme and the EVs aggregator will gain more\nprofit in winter than summer due to more upward regulation service is needed.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.08782v2", "cate": "eess.SY", "date": "2025-02-12", "updated": "2025-07-13"}
{"id": "2503.13407", "title": "Kernel-based error bounds of bilinear Koopman surrogate models for nonlinear data-driven control", "authors": ["Robin Strässer", "Manuel Schaller", "Julian Berberich", "Karl Worthmann", "Frank Allgöwer"], "categories": ["eess.SY", "cs.SY", "math.OC"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      Accepted for publication in IEEE Control Systems Letters (L-CSS)", "url": "http://arxiv.org/abs/2503.13407v4", "summary": "We derive novel deterministic bounds on the approximation error of data-based\nbilinear surrogate models for unknown nonlinear systems. The surrogate models\nare constructed using kernel-based extended dynamic mode decomposition to\napproximate the Koopman operator in a reproducing kernel Hilbert space. Unlike\nprevious methods that require restrictive assumptions on the invariance of the\ndictionary, our approach leverages kernel-based dictionaries that allow us to\ncontrol the projection error via pointwise error bounds, overcoming a\nsignificant limitation of existing theoretical guarantees. The derived state-\nand input-dependent error bounds allow for direct integration into\nKoopman-based robust controller designs with closed-loop guarantees for the\nunknown nonlinear system. Numerical examples illustrate the effectiveness of\nthe proposed framework.", "comment": "Accepted for publication in IEEE Control Systems Letters (L-CSS)", "pdf_url": "http://arxiv.org/pdf/2503.13407v4", "cate": "eess.SY", "date": "2025-03-17", "updated": "2025-07-14"}
{"id": "2507.09561", "title": "Novel Physics-Aware Attention-Based Machine Learning Approach for Mutual Coupling Modeling", "authors": ["Can Wang", "Wei Liu", "Hanzhi Ma", "Xiaonan Jiang", "Erping Li", "Steven Gao"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      This work has been submitted to the IEEE for possible publication", "url": "http://arxiv.org/abs/2507.09561v1", "summary": "This article presents a physics-aware convolutional long short-term memory\n(PC-LSTM) network for efficient and accurate extraction of mutual impedance\nmatrices in dipole antenna arrays. By reinterpreting the Green's function\nthrough a physics-aware neural network and embedding it into an adaptive loss\nfunction, the proposed machine learning-based approach achieves enhanced\nphysical interpretability in mutual coupling modeling. Also, an attention\nmechanism is carefully designed to calibrate complex-valued features by fusing\nthe real and imaginary parts of the Green's function matrix. These fused\nrepresentations are then processed by a convolutional long short-term memory\nnetwork, and the impedance matrix of the linear antenna array can be finally\nderived. Validation against five benchmarks underscores the efficacy of the\nproposed approach, demonstrating accurate impedance extraction with up to a 7x\nspeedup compared to CST Microwave Studio, making it a fast alternative to\nfull-wave simulations for mutual coupling characterization.", "comment": "This work has been submitted to the IEEE for possible publication", "pdf_url": "http://arxiv.org/pdf/2507.09561v1", "cate": "eess.SP", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09608", "title": "prNet: Data-Driven Phase Retrieval via Stochastic Refinement", "authors": ["Mehmet Onurcan Kaya", "Figen S. Oktem"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09608v1", "summary": "We propose a novel framework for phase retrieval that leverages Langevin\ndynamics to enable efficient posterior sampling, yielding reconstructions that\nexplicitly balance distortion and perceptual quality. Unlike conventional\napproaches that prioritize pixel-wise accuracy, our method navigates the\nperception-distortion tradeoff through a principled combination of stochastic\nsampling, learned denoising, and model-based updates. The framework comprises\nthree variants of increasing complexity, integrating theoretically grounded\nLangevin inference, adaptive noise schedule learning, parallel reconstruction\nsampling, and warm-start initialization from classical solvers. Extensive\nexperiments demonstrate that our method achieves state-of-the-art performance\nacross multiple benchmarks, both in terms of fidelity and perceptual quality.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09608v1", "cate": "eess.IV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09929", "title": "Aligning Generative Speech Enhancement with Human Preferences via Direct Preference Optimization", "authors": ["Haoyang Li", "Nana Hou", "Yuchen Hu", "Jixun Yao", "Sabato Marco Siniscalchi", "Eng Siong Chng"], "categories": ["eess.AS", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09929v1", "summary": "This work investigates speech enhancement (SE) from the perspective of\nlanguage models (LMs). We propose a novel method that leverages Direct\nPreference Optimization (DPO) to improve the perceptual quality of enhanced\nspeech. Using UTMOS, a neural MOS prediction model, as a proxy for human\nratings, our approach guides optimization toward perceptually preferred\noutputs. This differs from existing LM-based SE methods that focus on\nmaximizing the likelihood of clean speech tokens, which may misalign with human\nperception and degrade quality despite low prediction error. Experiments on the\n2020 Deep Noise Suppression Challenge test sets demonstrate that applying DPO\nto a pretrained LM-based SE model yields consistent improvements across various\nspeech quality metrics, with relative gains of up to 56%. To our knowledge,\nthis is the first application of DPO to SE and the first to incorporate proxy\nperceptual feedback into LM-based SE training, pointing to a promising\ndirection for perceptually aligned SE.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09929v1", "cate": "eess.AS", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09805", "title": "Federated Learning with Graph-Based Aggregation for Traffic Forecasting", "authors": ["Audri Banik", "Glaucio Haroldo Silva de Carvalho", "Renata Dividino"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted at FedKDD 2025: International Joint Workshop on Federated Learning for Data Mining and Graph Analytics. 6 pages, 1 figure", "url": "http://arxiv.org/abs/2507.09805v1", "summary": "In traffic prediction, the goal is to estimate traffic speed or flow in\nspecific regions or road segments using historical data collected by devices\ndeployed in each area. Each region or road segment can be viewed as an\nindividual client that measures local traffic flow, making Federated Learning\n(FL) a suitable approach for collaboratively training models without sharing\nraw data. In centralized FL, a central server collects and aggregates model\nupdates from multiple clients to build a shared model while preserving each\nclient's data privacy. Standard FL methods, such as Federated Averaging\n(FedAvg), assume that clients are independent, which can limit performance in\ntraffic prediction tasks where spatial relationships between clients are\nimportant. Federated Graph Learning methods can capture these dependencies\nduring server-side aggregation, but they often introduce significant\ncomputational overhead. In this paper, we propose a lightweight graph-aware FL\napproach that blends the simplicity of FedAvg with key ideas from graph\nlearning. Rather than training full models, our method applies basic\nneighbourhood aggregation principles to guide parameter updates, weighting\nclient models based on graph connectivity. This approach captures spatial\nrelationships effectively while remaining computationally efficient. We\nevaluate our method on two benchmark traffic datasets, METR-LA and PEMS-BAY,\nand show that it achieves competitive performance compared to standard\nbaselines and recent graph-based federated learning techniques.", "comment": "Accepted at FedKDD 2025: International Joint Workshop on Federated\n  Learning for Data Mining and Graph Analytics. 6 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2507.09805v1", "cate": "cs.LG", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.10013", "title": "Cross-modal Associations in Vision and Language Models: Revisiting the bouba-kiki effect", "authors": ["Tom Kouwenhoven", "Kiana Shahrasbi", "Tessa Verhoef"], "categories": ["cs.CV", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10013v1", "summary": "Recent advances in multimodal models have raised questions about whether\nvision-and-language models (VLMs) integrate cross-modal information in ways\nthat reflect human cognition. One well-studied test case in this domain is the\nbouba-kiki effect, where humans reliably associate pseudowords like \"bouba\"\nwith round shapes and \"kiki\" with jagged ones. Given the mixed evidence found\nin prior studies for this effect in VLMs, we present a comprehensive\nre-evaluation focused on two variants of CLIP, ResNet and Vision Transformer\n(ViT), given their centrality in many state-of-the-art VLMs. We apply two\ncomplementary methods closely modelled after human experiments: a prompt-based\nevaluation that uses probabilities as model preference, and we use Grad-CAM as\na novel way to interpret visual attention in shape-word matching tasks. Our\nfindings show that these models do not consistently exhibit the bouba-kiki\neffect. While ResNet shows a preference for round shapes, overall performance\nacross both models lacks the expected associations. Moreover, direct comparison\nwith prior human data on the same task shows that the models' responses fall\nmarkedly short of the robust, modality-integrated behaviour characteristic of\nhuman cognition. These results contribute to the ongoing debate about the\nextent to which VLMs truly understand cross-modal concepts, highlighting\nlimitations in their internal representations and alignment with human\nintuitions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10013v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2504.00735", "title": "Reinforcement learning for robust dynamic metabolic control", "authors": ["Sebastián Espinel-Ríos", "River Walser", "Dongda Zhang"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.00735v3", "summary": "Dynamic metabolic control allows key metabolic fluxes to be modulated in real\ntime, enhancing bioprocess flexibility and expanding available optimization\ndegrees of freedom. This is achieved, e.g., via targeted modulation of\nmetabolic enzyme expression. However, identifying optimal dynamic control\npolicies is challenging due to the generally high-dimensional solution space\nand the need to manage metabolic burden and cytotoxic effects arising from\ninducible enzyme expression. The task is further complicated by stochastic\ndynamics, which reduce bioprocess reproducibility. We propose a reinforcement\nlearning framework} to derive optimal policies by allowing an agent (the\ncontroller) to interact with a surrogate dynamic model. To promote robustness,\nwe apply domain randomization, enabling the controller to generalize across\nuncertainties. When transferred to an experimental system, the agent can in\nprinciple continue fine-tuning the policy. Our framework provides an\nalternative to conventional model-based control such as model predictive\ncontrol, which requires model differentiation with respect to decision\nvariables; often impractical for complex stochastic, nonlinear, stiff, and\npiecewise-defined dynamics. In contrast, our approach relies on forward\nintegration of the model, thereby simplifying the task. We demonstrate the\nframework in two $\\textit{Escherichia coli}$ bioprocesses: dynamic control of\nacetyl-CoA carboxylase for fatty-acid synthesis and of adenosine triphosphatase\nfor lactate synthesis.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.00735v3", "cate": "eess.SY", "date": "2025-04-01", "updated": "2025-07-14"}
{"id": "2505.18912", "title": "Robust Stability Analysis of Positive Lure System with Neural Network Feedback", "authors": ["Hamidreza Montazeri Hedesh", "Moh. Kamalul Wafi", "Bahram Shafai", "Milad Siami"], "categories": ["eess.SY", "cs.AI", "cs.SY", "93D09, 93D20, 93C10, 68T07", "B.1.3; G.1; I.2; I.2.3; I.2.8; I.2.1; J.2"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      Accepted at the 9th IEEE Conference on Control Technology and Applications (CCTA) 2025, San Diego, California", "url": "http://arxiv.org/abs/2505.18912v2", "summary": "This paper investigates the robustness of the Lur'e problem under positivity\nconstraints, drawing on results from the positive Aizerman conjecture and\nrobustness properties of Metzler matrices. Specifically, we consider a control\nsystem of Lur'e type in which not only the linear part includes parametric\nuncertainty but also the nonlinear sector bound is unknown. We investigate\ntools from positive linear systems to effectively solve the problems in\ncomplicated and uncertain nonlinear systems. By leveraging the positivity\ncharacteristic of the system, we derive an explicit formula for the stability\nradius of Lur'e systems. Furthermore, we extend our analysis to systems with\nneural network (NN) feedback loops. Building on this approach, we also propose\na refinement method for sector bounds of NNs. This study introduces a scalable\nand efficient approach for robustness analysis of both Lur'e and NN-controlled\nsystems. Finally, the proposed results are supported by illustrative examples.", "comment": "Accepted at the 9th IEEE Conference on Control Technology and\n  Applications (CCTA) 2025, San Diego, California", "pdf_url": "http://arxiv.org/pdf/2505.18912v2", "cate": "eess.SY", "date": "2025-05-25", "updated": "2025-07-14"}
{"id": "2507.09713", "title": "A New Wireless Image Transmission System Using Code Index Modulation and Image Enhancement for High-Rate Next Generation Networks", "authors": ["Burak Ahmet Ozden", "Erdogan Aydin", "Ahmet Elbir", "Filiz Gurkan"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      17 pages, 14 figures", "url": "http://arxiv.org/abs/2507.09713v1", "summary": "With the development of wireless network technologies, the wireless image\ntransmission area has become prominent. The need for high resolution, data\ntraffic density, widespread use of multimedia applications, and the importance\nof high rate and reliable image transmission in medical and military fields\nnecessitate the design of novel and high-performance wireless image\ntransmission systems. This paper proposes a code index modulation (CIM)-based\nimage transmission (CIM-IT) system that utilizes spreading code index and\nquadrature amplitude modulation (QAM) symbol for image transmission over a\nwireless channel. The proposed CIM-IT system maps bits to each pixel value of\nthe image to be transmitted and transmits these bits over a wireless channel\nusing a single-input and multiple-output system comprising code index\nmodulation and QAM techniques. At the receiver, the active spreading code index\nand the selected QAM symbol are estimated using a despreading-based maximum\nlikelihood detector, and the corresponding bits are obtained. The image\nconveyed from the transmitter is then reconstructed at the receiver side using\nthe pixel values corresponding to the bits. The obtained noisy image is\nenhanced using important enhancement filters. In addition, an advanced filter\nis proposed to improve the transmitted degraded image with optimum results.\nFurthermore, error performance, spectral efficiency, energy efficiency, and\nthroughputof the CIM-IT system are performed and the results are compared with\ntraditional wireless communication techniques.", "comment": "17 pages, 14 figures", "pdf_url": "http://arxiv.org/pdf/2507.09713v1", "cate": "eess.SP", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09609", "title": "I2I-PR: Deep Iterative Refinement for Phase Retrieval using Image-to-Image Diffusion Models", "authors": ["Mehmet Onurcan Kaya", "Figen S. Oktem"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09609v1", "summary": "Phase retrieval involves recovering a signal from intensity-only\nmeasurements, crucial in many fields such as imaging, holography, optical\ncomputing, crystallography, and microscopy. Although there are several\nwell-known phase retrieval algorithms, including classical iterative solvers,\nthe reconstruction performance often remains sensitive to initialization and\nmeasurement noise. Recently, image-to-image diffusion models have gained\ntraction in various image reconstruction tasks, yielding significant\ntheoretical insights and practical breakthroughs. In this work, we introduce a\nnovel phase retrieval approach based on an image-to-image diffusion framework\ncalled Inversion by Direct Iteration. Our method begins with an enhanced\ninitialization stage that leverages a hybrid iterative technique, combining the\nHybrid Input-Output and Error Reduction methods and incorporating a novel\nacceleration mechanism to obtain a robust crude estimate. Then, it iteratively\nrefines this initial crude estimate using the learned image-to-image pipeline.\nOur method achieves substantial improvements in both training efficiency and\nreconstruction quality. Furthermore, our approach utilizes aggregation\ntechniques to refine quality metrics and demonstrates superior results compared\nto both classical and contemporary techniques. This highlights its potential\nfor effective and efficient phase retrieval across various applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09609v1", "cate": "eess.IV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.10159", "title": "Cyclic Multichannel Wiener Filter for Acoustic Beamforming", "authors": ["Giovanni Bologni", "Richard Heusdens", "Richard C. Hendriks"], "categories": ["eess.AS", "eess.SP"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Comments: Accepted for publication at the 2025 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA 2025). IEEE retains copyright", "url": "http://arxiv.org/abs/2507.10159v1", "summary": "Acoustic beamforming models typically assume wide-sense stationarity of\nspeech signals within short time frames. However, voiced speech is better\nmodeled as a cyclostationary (CS) process, a random process whose mean and\nautocorrelation are $T_1$-periodic, where $\\alpha_1=1/T_1$ corresponds to the\nfundamental frequency of vowels. Higher harmonic frequencies are found at\ninteger multiples of the fundamental. This work introduces a cyclic\nmultichannel Wiener filter (cMWF) for speech enhancement derived from a\ncyclostationary model. This beamformer exploits spectral correlation across the\nharmonic frequencies of the signal to further reduce the mean-squared error\n(MSE) between the target and the processed input. The proposed cMWF is optimal\nin the MSE sense and reduces to the MWF when the target is wide-sense\nstationary. Experiments on simulated data demonstrate considerable improvements\nin scale-invariant signal-to-distortion ratio (SI-SDR) on synthetic data but\nalso indicate high sensitivity to the accuracy of the estimated fundamental\nfrequency $\\alpha_1$, which limits effectiveness on real data.", "comment": "Comments: Accepted for publication at the 2025 IEEE Workshop on\n  Applications of Signal Processing to Audio and Acoustics (WASPAA 2025). IEEE\n  retains copyright", "pdf_url": "http://arxiv.org/pdf/2507.10159v1", "cate": "eess.AS", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09816", "title": "Compressed Computation: Dense Circuits in a Toy Model of the Universal-AND Problem", "authors": ["Adam Newgas"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      9 pages, 9 figures", "url": "http://arxiv.org/abs/2507.09816v1", "summary": "Neural networks are capable of superposition -- representing more features\nthan there are dimensions. Recent work considers the analogous concept for\ncomputation instead of storage, proposing theoretical constructions. But there\nhas been little investigation into whether these circuits can be learned in\npractice. In this work, we investigate a toy model for the Universal-AND\nproblem which computes the AND of all $m\\choose 2$ pairs of $m$ sparse inputs.\nThe hidden dimension that determines the number of non-linear activations is\nrestricted to pressure the model to find a compute-efficient circuit, called\ncompressed computation. We find that the training process finds a simple\nsolution that does not correspond to theoretical constructions. It is fully\ndense -- every neuron contributes to every output. The solution circuit\nnaturally scales with dimension, trading off error rates for neuron efficiency.\nIt is similarly robust to changes in sparsity and other key parameters, and\nextends naturally to other boolean operations and boolean circuits. We explain\nthe found solution in detail and compute why it is more efficient than the\ntheoretical constructions at low sparsity. Our findings shed light on the types\nof circuits that models like to form and the flexibility of the superposition\nrepresentation. This contributes to a broader understanding of network\ncircuitry and interpretability.", "comment": "9 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2507.09816v1", "cate": "cs.LG", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.10015", "title": "(Almost) Free Modality Stitching of Foundation Models", "authors": ["Jaisidh Singh", "Diganta Misra", "Boris Knyazev", "Antonio Orvieto"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Pre-print", "url": "http://arxiv.org/abs/2507.10015v1", "summary": "Foundation multi-modal models are often designed by stitching of multiple\nexisting pretrained uni-modal models: for example, an image classifier with an\nautoregressive text model. This stitching process is performed by training a\nconnector module that aims to align the representation-representation or\nrepresentation-input spaces of these uni-modal models. However, given the\ncomplexity of training such connectors on large scale web-based datasets\ncoupled with the ever-increasing number of available pretrained uni-modal\nmodels, the task of uni-modal models selection and subsequent connector module\ntraining becomes computationally demanding. To address this under-studied\ncritical problem, we propose Hypernetwork Model Alignment (Hyma), a novel\nall-in-one solution for optimal uni-modal model selection and connector\ntraining by leveraging hypernetworks. Specifically, our framework utilizes the\nparameter prediction capability of a hypernetwork to obtain jointly trained\nconnector modules for $N \\times M$ combinations of uni-modal models. In our\nexperiments, Hyma reduces the optimal uni-modal model pair search cost by\n$10\\times$ (averaged across all experiments), while matching the ranking and\ntrained connector performance obtained via grid search across a suite of\ndiverse multi-modal benchmarks.", "comment": "Pre-print", "pdf_url": "http://arxiv.org/pdf/2507.10015v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2301.00922", "title": "Faster Reinforcement Learning by Freezing Slow States", "authors": ["Yijia Wang", "Daniel R. Jiang"], "categories": ["cs.AI", "cs.LG", "cs.SY", "eess.SY", "math.OC"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      70 pages, 10 figures", "url": "http://arxiv.org/abs/2301.00922v3", "summary": "We study infinite horizon Markov decision processes (MDPs) with \"fast-slow\"\nstructure, where some state variables evolve rapidly (\"fast states\") while\nothers change more gradually (\"slow states\"). This structure commonly arises in\npractice when decisions must be made at high frequencies over long horizons,\nand where slowly changing information still plays a critical role in\ndetermining optimal actions. Examples include inventory control under slowly\nchanging demand indicators or dynamic pricing with gradually shifting consumer\nbehavior. Modeling the problem at the natural decision frequency leads to MDPs\nwith discount factors close to one, making them computationally challenging. We\npropose a novel approximation strategy that \"freezes\" slow states during phases\nof lower-level planning and subsequently applies value iteration to an\nauxiliary upper-level MDP that evolves on a slower timescale. Freezing states\nfor short periods of time leads to easier-to-solve lower-level problems, while\na slower upper-level timescale allows for a more favorable discount factor. On\nthe theoretical side, we analyze the regret incurred by our frozen-state\napproach, which leads to simple insights on how to trade off regret versus\ncomputational cost. Empirically, we benchmark our new frozen-state methods on\nthree domains, (i) inventory control with fixed order costs, (ii) a gridworld\nproblem with spatial tasks, and (iii) dynamic pricing with reference-price\neffects. We demonstrate that the new methods produce high-quality policies with\nsignificantly less computation, and we show that simply omitting slow states is\noften a poor heuristic.", "comment": "70 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2301.00922v3", "cate": "cs.AI", "date": "2023-01-03", "updated": "2025-07-14"}
{"id": "2403.08553", "title": "Regret Analysis of Policy Optimization over Submanifolds for Linearly Constrained Online LQG", "authors": ["Ting-Jui Chang", "Shahin Shahrampour"], "categories": ["math.OC", "cs.LG", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2403.08553v2", "summary": "Recent advancement in online optimization and control has provided novel\ntools to study online linear quadratic regulator (LQR) problems, where cost\nmatrices are time-varying and unknown in advance. In this work, we study the\nonline linear quadratic Gaussian (LQG) problem over the manifold of stabilizing\ncontrollers that are linearly constrained to impose physical conditions such as\nsparsity. By adopting a Riemannian perspective, we propose the online Newton on\nmanifold (ONM) algorithm, which generates an online controller on-the-fly based\non the second-order information of the cost function sequence. To quantify the\nalgorithm performance, we use the notion of regret, defined as the\nsub-optimality of the algorithm cumulative cost against a (locally) minimizing\ncontroller sequence. We establish a regret bound in terms of the path-length of\nthe benchmark minimizer sequence, and we further verify the effectiveness of\nONM via simulations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2403.08553v2", "cate": "math.OC", "date": "2024-03-13", "updated": "2025-07-13"}
{"id": "2507.09895", "title": "AI-Enhanced Wide-Area Data Imaging via Massive Non-Orthogonal Direct Device-to-HAPS Transmission", "authors": ["Hyung-Joo Moon", "Chan-Byoung Chae", "Kai-Kit Wong", "Robert W. Heath Jr"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      7 pages, 6 figures, IEEE Communications Magazine (under revision)", "url": "http://arxiv.org/abs/2507.09895v1", "summary": "Massive Aerial Processing for X MAP-X is an innovative framework for\nreconstructing spatially correlated ground data, such as environmental or\nindustrial measurements distributed across a wide area, into data maps using a\nsingle high altitude pseudo-satellite (HAPS) and a large number of distributed\nsensors. With subframe-level data reconstruction, MAP-X provides a\ntransformative solution for latency-sensitive IoT applications. This article\nexplores two distinct approaches for AI integration in the post-processing\nstage of MAP-X. The DNN-based pointwise estimation approach enables real-time,\nadaptive reconstruction through online training, while the CNN-based image\nreconstruction approach improves reconstruction accuracy through offline\ntraining with non-real-time data. Simulation results show that both approaches\nsignificantly outperform the conventional inverse discrete Fourier transform\n(IDFT)-based linear post-processing method. Furthermore, to enable AI-enhanced\nMAP-X, we propose a ground-HAPS cooperation framework, where terrestrial\nstations collect, process, and relay training data to the HAPS. With its\nenhanced capability in reconstructing field data, AI-enhanced MAP-X is\napplicable to various real-world use cases, including disaster response and\nnetwork management.", "comment": "7 pages, 6 figures, IEEE Communications Magazine (under revision)", "pdf_url": "http://arxiv.org/pdf/2507.09895v1", "cate": "eess.SP", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09731", "title": "Pre-trained Under Noise: A Framework for Robust Bone Fracture Detection in Medical Imaging", "authors": ["Robby Hoover", "Nelly Elsayed", "Zag ElSayed", "Chengcheng Li"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      7 pages, under review", "url": "http://arxiv.org/abs/2507.09731v1", "summary": "Medical Imagings are considered one of the crucial diagnostic tools for\ndifferent bones-related diseases, especially bones fractures. This paper\ninvestigates the robustness of pre-trained deep learning models for classifying\nbone fractures in X-ray images and seeks to address global healthcare disparity\nthrough the lens of technology. Three deep learning models have been tested\nunder varying simulated equipment quality conditions. ResNet50, VGG16 and\nEfficientNetv2 are the three pre-trained architectures which are compared.\nThese models were used to perform bone fracture classification as images were\nprogressively degraded using noise. This paper specifically empirically studies\nhow the noise can affect the bone fractures detection and how the pre-trained\nmodels performance can be changes due to the noise that affect the quality of\nthe X-ray images. This paper aims to help replicate real world challenges\nexperienced by medical imaging technicians across the world. Thus, this paper\nestablishes a methodological framework for assessing AI model degradation using\ntransfer learning and controlled noise augmentation. The findings provide\npractical insight into how robust and generalizable different pre-trained deep\nlearning powered computer vision models can be when used in different contexts.", "comment": "7 pages, under review", "pdf_url": "http://arxiv.org/pdf/2507.09731v1", "cate": "eess.IV", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.10176", "title": "Harmonics to the Rescue: Why Voiced Speech is Not a Wss Process", "authors": ["Giovanni Bologni", "Richard Heusdens", "Richard C. Hendriks"], "categories": ["eess.AS", "eess.SP"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Comments: Accepted at the 2024 International Workshop on Acoustic Signal Enhancement (IWAENC 2024)", "url": "http://arxiv.org/abs/2507.10176v1", "summary": "Speech processing algorithms often rely on statistical knowledge of the\nunderlying process. Despite many years of research, however, the debate on the\nmost appropriate statistical model for speech still continues. Speech is\ncommonly modeled as a wide-sense stationary (WSS) process. However, the use of\nthe WSS model for spectrally correlated processes is fundamentally wrong, as\nWSS implies spectral uncorrelation. In this paper, we demonstrate that voiced\nspeech can be more accurately represented as a cyclostationary (CS) process. By\nemploying the CS rather than the WSS model for processes that are inherently\ncorrelated across frequency, it is possible to improve the estimation of\ncross-power spectral densities (PSDs), source separation, and beamforming. We\nillustrate how the correlation between harmonic frequencies of CS processes can\nenhance system identification, and validate our findings using both simulated\nand real speech data.", "comment": "Comments: Accepted at the 2024 International Workshop on Acoustic\n  Signal Enhancement (IWAENC 2024)", "pdf_url": "http://arxiv.org/pdf/2507.10176v1", "cate": "eess.AS", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09826", "title": "Bridging Neural Networks and Dynamic Time Warping for Adaptive Time Series Classification", "authors": ["Jintao Qu", "Zichong Wang", "Chenhao Wu", "Wenbin Zhang"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09826v1", "summary": "Neural networks have achieved remarkable success in time series\nclassification, but their reliance on large amounts of labeled data for\ntraining limits their applicability in cold-start scenarios. Moreover, they\nlack interpretability, reducing transparency in decision-making. In contrast,\ndynamic time warping (DTW) combined with a nearest neighbor classifier is\nwidely used for its effectiveness in limited-data settings and its inherent\ninterpretability. However, as a non-parametric method, it is not trainable and\ncannot leverage large amounts of labeled data, making it less effective than\nneural networks in rich-resource scenarios. In this work, we aim to develop a\nversatile model that adapts to cold-start conditions and becomes trainable with\nlabeled data, while maintaining interpretability. We propose a dynamic\nlength-shortening algorithm that transforms time series into prototypes while\npreserving key structural patterns, thereby enabling the reformulation of the\nDTW recurrence relation into an equivalent recurrent neural network. Based on\nthis, we construct a trainable model that mimics DTW's alignment behavior. As a\nneural network, it becomes trainable when sufficient labeled data is available,\nwhile still retaining DTW's inherent interpretability. We apply the model to\nseveral benchmark time series classification tasks and observe that it\nsignificantly outperforms previous approaches in low-resource settings and\nremains competitive in rich-resource settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09826v1", "cate": "cs.LG", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.10029", "title": "Memory-Efficient Personalization of Text-to-Image Diffusion Models via Selective Optimization Strategies", "authors": ["Seokeon Choi", "Sunghyun Park", "Hyoungwoo Park", "Jeongho Kim", "Sungrack Yun"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10029v1", "summary": "Memory-efficient personalization is critical for adapting text-to-image\ndiffusion models while preserving user privacy and operating within the limited\ncomputational resources of edge devices. To this end, we propose a selective\noptimization framework that adaptively chooses between backpropagation on\nlow-resolution images (BP-low) and zeroth-order optimization on high-resolution\nimages (ZO-high), guided by the characteristics of the diffusion process. As\nobserved in our experiments, BP-low efficiently adapts the model to\ntarget-specific features, but suffers from structural distortions due to\nresolution mismatch. Conversely, ZO-high refines high-resolution details with\nminimal memory overhead but faces slow convergence when applied without prior\nadaptation. By complementing both methods, our framework leverages BP-low for\neffective personalization while using ZO-high to maintain structural\nconsistency, achieving memory-efficient and high-quality fine-tuning. To\nmaximize the efficacy of both BP-low and ZO-high, we introduce a timestep-aware\nprobabilistic function that dynamically selects the appropriate optimization\nstrategy based on diffusion timesteps. This function mitigates the overfitting\nfrom BP-low at high timesteps, where structural information is critical, while\nensuring ZO-high is applied more effectively as training progresses.\nExperimental results demonstrate that our method achieves competitive\nperformance while significantly reducing memory consumption, enabling scalable,\nhigh-quality on-device personalization without increasing inference latency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10029v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2412.02811", "title": "Kernel-based Koopman approximants for control: Flexible sampling, error analysis, and stability", "authors": ["Lea Bold", "Friedrich M. Philipp", "Manuel Schaller", "Karl Worthmann"], "categories": ["math.OC", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      29 pages, 5 figures", "url": "http://arxiv.org/abs/2412.02811v2", "summary": "Data-driven techniques for analysis, modeling, and control of complex\ndynamical systems are on the uptake. Koopman theory provides the theoretical\nfoundation for the popular kernel extended dynamic mode decomposition (kEDMD).\nIn this work, we propose a novel kEDMD scheme to approximate nonlinear control\nsystems accompanied by an in-depth error analysis. Key features are\nregularization-based robustness and an adroit decomposition into micro and\nmacro grids enabling flexible sampling. But foremost, we prove proportionality,\ni.e., explicit dependence on the distance to the (controlled) equilibrium, of\nthe derived bound on the full approximation error. Leveraging this key\nproperty, we rigorously show that asymptotic stability of the data-driven\nsurrogate (control) system implies asymptotic stability of the original\n(control) system and vice versa.", "comment": "29 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2412.02811v2", "cate": "math.OC", "date": "2024-12-03", "updated": "2025-07-14"}
{"id": "2507.09987", "title": "VoxelRF: Voxelized Radiance Field for Fast Wireless Channel Modeling", "authors": ["Zihang Zeng", "Shu Sun", "Meixia Tao", "Yin Xu", "Xianghao Yu"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09987v1", "summary": "Wireless channel modeling in complex environments is crucial for wireless\ncommunication system design and deployment. Traditional channel modeling\napproaches face challenges in balancing accuracy, efficiency, and scalability,\nwhile recent neural approaches such as neural radiance field (NeRF) suffer from\nlong training and slow inference. To tackle these challenges, we propose\nvoxelized radiance field (VoxelRF), a novel neural representation for wireless\nchannel modeling that enables fast and accurate synthesis of spatial spectra.\nVoxelRF replaces the costly multilayer perception (MLP) used in NeRF-based\nmethods with trilinear interpolation of voxel grid-based representation, and\ntwo shallow MLPs to model both propagation and transmitter-dependent effects.\nTo further accelerate training and improve generalization, we introduce\nprogressive learning, empty space skipping, and an additional background\nentropy loss function. Experimental results demonstrate that VoxelRF achieves\ncompetitive accuracy with significantly reduced computation and limited\ntraining data, making it more practical for real-time and resource-constrained\nwireless applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09987v1", "cate": "eess.SP", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09872", "title": "Resolution Revolution: A Physics-Guided Deep Learning Framework for Spatiotemporal Temperature Reconstruction", "authors": ["Shengjie Liu", "Lu Zhang", "Siqin Wang"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      ICCV 2025 Workshop SEA -- International Conference on Computer Vision 2025 Workshop on Sustainability with Earth Observation and AI", "url": "http://arxiv.org/abs/2507.09872v1", "summary": "Central to Earth observation is the trade-off between spatial and temporal\nresolution. For temperature, this is especially critical because real-world\napplications require high spatiotemporal resolution data. Current technology\nallows for hourly temperature observations at 2 km, but only every 16 days at\n100 m, a gap further exacerbated by cloud cover. Earth system models offer\ncontinuous hourly temperature data, but at a much coarser spatial resolution\n(9-31 km). Here, we present a physics-guided deep learning framework for\ntemperature data reconstruction that integrates these two data sources. The\nproposed framework uses a convolutional neural network that incorporates the\nannual temperature cycle and includes a linear term to amplify the coarse Earth\nsystem model output into fine-scale temperature values observed from\nsatellites. We evaluated this framework using data from two satellites, GOES-16\n(2 km, hourly) and Landsat (100 m, every 16 days), and demonstrated effective\ntemperature reconstruction with hold-out and in situ data across four datasets.\nThis physics-guided deep learning framework opens new possibilities for\ngenerating high-resolution temperature data across spatial and temporal scales,\nunder all weather conditions and globally.", "comment": "ICCV 2025 Workshop SEA -- International Conference on Computer Vision\n  2025 Workshop on Sustainability with Earth Observation and AI", "pdf_url": "http://arxiv.org/pdf/2507.09872v1", "cate": "eess.IV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10200", "title": "Natural Language-based Assessment of L2 Oral Proficiency using LLMs", "authors": ["Stefano Bannò", "Rao Ma", "Mengjie Qian", "Siyuan Tang", "Kate Knill", "Mark Gales"], "categories": ["eess.AS", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Accepted for the 10th Workshop on Speech and Language Technology in Education (SLaTE 2025)", "url": "http://arxiv.org/abs/2507.10200v1", "summary": "Natural language-based assessment (NLA) is an approach to second language\nassessment that uses instructions - expressed in the form of can-do descriptors\n- originally intended for human examiners, aiming to determine whether large\nlanguage models (LLMs) can interpret and apply them in ways comparable to human\nassessment. In this work, we explore the use of such descriptors with an\nopen-source LLM, Qwen 2.5 72B, to assess responses from the publicly available\nS&I Corpus in a zero-shot setting. Our results show that this approach -\nrelying solely on textual information - achieves competitive performance: while\nit does not outperform state-of-the-art speech LLMs fine-tuned for the task, it\nsurpasses a BERT-based model trained specifically for this purpose. NLA proves\nparticularly effective in mismatched task settings, is generalisable to other\ndata types and languages, and offers greater interpretability, as it is\ngrounded in clearly explainable, widely applicable language descriptors.", "comment": "Accepted for the 10th Workshop on Speech and Language Technology in\n  Education (SLaTE 2025)", "pdf_url": "http://arxiv.org/pdf/2507.10200v1", "cate": "eess.AS", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09837", "title": "A Pre-training Framework for Relational Data with Information-theoretic Principles", "authors": ["Quang Truong", "Zhikai Chen", "Mingxuan Ju", "Tong Zhao", "Neil Shah", "Jiliang Tang"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09837v1", "summary": "Relational databases underpin critical infrastructure across a wide range of\ndomains, yet the design of generalizable pre-training strategies for learning\nfrom relational databases remains an open challenge due to task heterogeneity.\nSpecifically, there exist infinitely many possible downstream tasks, as tasks\nare defined based on relational schema graphs, temporal dependencies, and\nSQL-defined label logics. An effective pre-training framework is desired to\ntake these factors into account in order to obtain task-aware representations.\nBy incorporating knowledge of the underlying distribution that drives label\ngeneration, downstream tasks can benefit from relevant side-channel\ninformation. To bridge this gap, we introduce Task Vector Estimation (TVE), a\nnovel pre-training framework that constructs predictive supervisory signals via\nset-based aggregation over schema traversal graphs, explicitly modeling\nnext-window relational dynamics. We formalize our approach through an\ninformation-theoretic lens, demonstrating that task-informed representations\nretain more relevant signals than those obtained without task priors. Extensive\nexperiments on the RelBench benchmark show that TVE consistently outperforms\ntraditional pre-training baselines. Our findings advocate for pre-training\nobjectives that encode task heterogeneity and temporal structure as design\nprinciples for predictive modeling on relational databases.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09837v1", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10053", "title": "CoSMo: A Multimodal Transformer for Page Stream Segmentation in Comic Books", "authors": ["Marc Serra Ortega", "Emanuele Vivoli", "Artemis Llabrés", "Dimosthenis Karatzas"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10053v1", "summary": "This paper introduces CoSMo, a novel multimodal Transformer for Page Stream\nSegmentation (PSS) in comic books, a critical task for automated content\nunderstanding, as it is a necessary first stage for many downstream tasks like\ncharacter analysis, story indexing, or metadata enrichment. We formalize PSS\nfor this unique medium and curate a new 20,800-page annotated dataset. CoSMo,\ndeveloped in vision-only and multimodal variants, consistently outperforms\ntraditional baselines and significantly larger general-purpose vision-language\nmodels across F1-Macro, Panoptic Quality, and stream-level metrics. Our\nfindings highlight the dominance of visual features for comic PSS\nmacro-structure, yet demonstrate multimodal benefits in resolving challenging\nambiguities. CoSMo establishes a new state-of-the-art, paving the way for\nscalable comic book analysis.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10053v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09999", "title": "Sparsity-Aware Extended Kalman Filter for Tracking Dynamic Graphs", "authors": ["Lital Dabush", "Nir Shlezinger", "Tirza Routtenberg"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      This work has been submitted to the IEEE for possible publication", "url": "http://arxiv.org/abs/2507.09999v1", "summary": "A broad range of applications involve signals with irregular structures that\ncan be represented as a graph. As the underlying structures can change over\ntime, the tracking dynamic graph topologies from observed signals is a\nfundamental challenge in graph signal processing (GSP), with applications in\nvarious domains, such as power systems, the brain-machine interface, and\ncommunication systems. In this paper, we propose a method for tracking dynamic\nchanges in graph topologies. Our approach builds on a representation of the\ndynamics as a graph-based nonlinear state-space model (SSM), where the\nobservations are graph signals generated through graph filtering, and the\nunderlying evolving topology serves as the latent states. In our formulation,\nthe graph Laplacian matrix is parameterized using the incidence matrix and edge\nweights, enabling a structured representation of the state. In order to track\nthe evolving topology in the resulting SSM, we develop a sparsity-aware\nextended Kalman filter (EKF) that integrates $\\ell_1$-regularized updates\nwithin the filtering process. Furthermore, a dynamic programming scheme to\nefficiently compute the Jacobian of the graph filter is introduced. Our\nnumerical study demonstrates the ability of the proposed method to accurately\ntrack sparse and time-varying graphs under realistic conditions, with highly\nnonlinear measurements, various noise levels, and different change rates, while\nmaintaining low computational complexity.", "comment": "This work has been submitted to the IEEE for possible publication", "pdf_url": "http://arxiv.org/pdf/2507.09999v1", "cate": "eess.SP", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09898", "title": "Advanced U-Net Architectures with CNN Backbones for Automated Lung Cancer Detection and Segmentation in Chest CT Images", "authors": ["Alireza Golkarieha", "Kiana Kiashemshakib", "Sajjad Rezvani Boroujenic", "Nasibeh Asadi Isakand"], "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      This manuscript has 20 pages and 10 figures. It is submitted to the Journal 'Scientific Reports'", "url": "http://arxiv.org/abs/2507.09898v1", "summary": "This study investigates the effectiveness of U-Net architectures integrated\nwith various convolutional neural network (CNN) backbones for automated lung\ncancer detection and segmentation in chest CT images, addressing the critical\nneed for accurate diagnostic tools in clinical settings. A balanced dataset of\n832 chest CT images (416 cancerous and 416 non-cancerous) was preprocessed\nusing Contrast Limited Adaptive Histogram Equalization (CLAHE) and resized to\n128x128 pixels. U-Net models were developed with three CNN backbones: ResNet50,\nVGG16, and Xception, to segment lung regions. After segmentation, CNN-based\nclassifiers and hybrid models combining CNN feature extraction with traditional\nmachine learning classifiers (Support Vector Machine, Random Forest, and\nGradient Boosting) were evaluated using 5-fold cross-validation. Metrics\nincluded accuracy, precision, recall, F1-score, Dice coefficient, and ROC-AUC.\nU-Net with ResNet50 achieved the best performance for cancerous lungs (Dice:\n0.9495, Accuracy: 0.9735), while U-Net with VGG16 performed best for\nnon-cancerous segmentation (Dice: 0.9532, Accuracy: 0.9513). For\nclassification, the CNN model using U-Net with Xception achieved 99.1 percent\naccuracy, 99.74 percent recall, and 99.42 percent F1-score. The hybrid\nCNN-SVM-Xception model achieved 96.7 percent accuracy and 97.88 percent\nF1-score. Compared to prior methods, our framework consistently outperformed\nexisting models. In conclusion, combining U-Net with advanced CNN backbones\nprovides a powerful method for both segmentation and classification of lung\ncancer in CT scans, supporting early diagnosis and clinical decision-making.", "comment": "This manuscript has 20 pages and 10 figures. It is submitted to the\n  Journal 'Scientific Reports'", "pdf_url": "http://arxiv.org/pdf/2507.09898v1", "cate": "eess.IV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10264", "title": "ASDKit: A Toolkit for Comprehensive Evaluation of Anomalous Sound Detection Methods", "authors": ["Takuya Fujimura", "Kevin Wilkinghoff", "Keisuke Imoto", "Tomoki Toda"], "categories": ["eess.AS"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10264v1", "summary": "In this paper, we introduce ASDKit, a toolkit for anomalous sound detection\n(ASD) task. Our aim is to facilitate ASD research by providing an open-source\nframework that collects and carefully evaluates various ASD methods. First,\nASDKit provides training and evaluation scripts for a wide range of ASD\nmethods, all handled within a unified framework. For instance, it includes the\nautoencoder-based official DCASE baseline, representative discriminative\nmethods, and self-supervised learning-based methods. Second, it supports\ncomprehensive evaluation on the DCASE 2020--2024 datasets, enabling careful\nassessment of ASD performance, which is highly sensitive to factors such as\ndatasets and random seeds. In our experiments, we re-evaluate various ASD\nmethods using ASDKit and identify consistently effective techniques across\nmultiple datasets and trials. We also demonstrate that ASDKit reproduces the\nstate-of-the-art-level performance on the considered datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10264v1", "cate": "eess.AS", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09846", "title": "Through the River: Understanding the Benefit of Schedule-Free Methods for Language Model Training", "authors": ["Minhak Song", "Beomhan Baek", "Kwangjun Ahn", "Chulhee Yun"], "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Comments would be appreciated!", "url": "http://arxiv.org/abs/2507.09846v1", "summary": "As both model and dataset sizes continue to scale rapidly, conventional\npretraining strategies with fixed compute budgets-such as cosine learning rate\nschedules-are increasingly inadequate for large-scale training. Recent\nalternatives, including warmup-stable-decay (WSD) schedules and weight\naveraging, offer greater flexibility. However, WSD relies on explicit decay\nphases to track progress, while weight averaging addresses this limitation at\nthe cost of additional memory. In search of a more principled and scalable\nalternative, we revisit the Schedule-Free (SF) method [Defazio et al., 2024],\nwhich has shown strong empirical performance across diverse settings. We show\nthat SF-AdamW effectively navigates the \"river\" structure of the loss landscape\nwithout decay phases or auxiliary averaging, making it particularly suitable\nfor continuously scaling training workloads. To understand this behavior, we\nconduct a theoretical and empirical analysis of SF dynamics, revealing that it\nimplicitly performs weight averaging without memory overhead. Guided by this\nanalysis, we propose a refined variant of SF that improves robustness to\nmomentum and performs better under large batch sizes, addressing key\nlimitations of the original method. Together, these results establish SF as a\npractical, scalable, and theoretically grounded approach for language model\ntraining.", "comment": "Comments would be appreciated!", "pdf_url": "http://arxiv.org/pdf/2507.09846v1", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10056", "title": "Lightweight Model for Poultry Disease Detection from Fecal Images Using Multi-Color Space Feature Optimization and Machine Learning", "authors": ["A. K. M. Shoriful Islam", "Md. Rakib Hassan", "Macbah Uddin", "Md. Shahidur Rahman"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10056v1", "summary": "Poultry farming is a vital component of the global food supply chain, yet it\nremains highly vulnerable to infectious diseases such as coccidiosis,\nsalmonellosis, and Newcastle disease. This study proposes a lightweight machine\nlearning-based approach to detect these diseases by analyzing poultry fecal\nimages. We utilize multi-color space feature extraction (RGB, HSV, LAB) and\nexplore a wide range of color, texture, and shape-based descriptors, including\ncolor histograms, local binary patterns (LBP), wavelet transforms, and edge\ndetectors. Through a systematic ablation study and dimensionality reduction\nusing PCA and XGBoost feature selection, we identify a compact global feature\nset that balances accuracy and computational efficiency. An artificial neural\nnetwork (ANN) classifier trained on these features achieved 95.85% accuracy\nwhile requiring no GPU and only 638 seconds of execution time in Google Colab.\nCompared to deep learning models such as Xception and MobileNetV3, our proposed\nmodel offers comparable accuracy with drastically lower resource usage. This\nwork demonstrates a cost-effective, interpretable, and scalable alternative to\ndeep learning for real-time poultry disease detection in low-resource\nagricultural settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10056v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10063", "title": "Deep Learning-Based Beamforming Design Using Target Beam Patterns", "authors": ["Hongpu Zhang", "Shu Sun", "Hangsong Yan", "Jianhua Mo"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      6 pages, 5 figures", "url": "http://arxiv.org/abs/2507.10063v1", "summary": "This paper proposes a deep learning-based beamforming design framework that\ndirectly maps a target beam pattern to optimal beamforming vectors across\nmultiple antenna array architectures, including digital, analog, and hybrid\nbeamforming. The proposed method employs a lightweight encoder-decoder network\nwhere the encoder compresses the complex beam pattern into a low-dimensional\nfeature vector and the decoder reconstructs the beamforming vector while\nsatisfying hardware constraints. To address training challenges under diverse\nand limited channel station information (CSI) conditions, a two-stage training\nprocess is introduced, which consists of an offline pre-training for robust\nfeature extraction using an auxiliary module, followed by online training of\nthe decoder with a composite loss function that ensures alignment between the\nsynthesized and target beam patterns in terms of the main lobe shape and side\nlobe suppression. Simulation results based on NYUSIM-generated channels show\nthat the proposed method can achieve spectral efficiency close to that of fully\ndigital beamforming under limited CSI and outperforms representative existing\nmethods.", "comment": "6 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.10063v1", "cate": "eess.SP", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09923", "title": "IM-LUT: Interpolation Mixing Look-Up Tables for Image Super-Resolution", "authors": ["Sejin Park", "Sangmin Lee", "Kyong Hwan Jin", "Seung-Won Jung"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      ICCV 2025", "url": "http://arxiv.org/abs/2507.09923v1", "summary": "Super-resolution (SR) has been a pivotal task in image processing, aimed at\nenhancing image resolution across various applications. Recently, look-up table\n(LUT)-based approaches have attracted interest due to their efficiency and\nperformance. However, these methods are typically designed for fixed scale\nfactors, making them unsuitable for arbitrary-scale image SR (ASISR). Existing\nASISR techniques often employ implicit neural representations, which come with\nconsiderable computational cost and memory demands. To address these\nlimitations, we propose Interpolation Mixing LUT (IM-LUT), a novel framework\nthat operates ASISR by learning to blend multiple interpolation functions to\nmaximize their representational capacity. Specifically, we introduce IM-Net, a\nnetwork trained to predict mixing weights for interpolation functions based on\nlocal image patterns and the target scale factor. To enhance efficiency of\ninterpolation-based methods, IM-Net is transformed into IM-LUT, where LUTs are\nemployed to replace computationally expensive operations, enabling lightweight\nand fast inference on CPUs while preserving reconstruction quality.\nExperimental results on several benchmark datasets demonstrate that IM-LUT\nconsistently achieves a superior balance between image quality and efficiency\ncompared to existing methods, highlighting its potential as a promising\nsolution for resource-constrained applications.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.09923v1", "cate": "eess.IV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.06917", "title": "Musical Source Separation Bake-Off: Comparing Objective Metrics with Human Perception", "authors": ["Noah Jaffe", "John Ashley Burgoyne"], "categories": ["eess.AS"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.06917v2", "summary": "Music source separation aims to extract individual sound sources (e.g.,\nvocals, drums, guitar) from a mixed music recording. However, evaluating the\nquality of separated audio remains challenging, as commonly used metrics like\nthe source-to-distortion ratio (SDR) do not always align with human perception.\nIn this study, we conducted a large-scale listener evaluation on the MUSDB18\ntest set, collecting approximately 30 ratings per track from seven distinct\nlistener groups. We compared several objective energy-ratio metrics, including\nlegacy measures (BSSEval v4, SI-SDR variants), and embedding-based alternatives\n(Frechet Audio Distance using CLAP-LAION-music, EnCodec, VGGish, Wave2Vec2, and\nHuBERT). While SDR remains the best-performing metric for vocal estimates, our\nresults show that the scale-invariant signal-to-artifacts ratio (SI-SAR) better\npredicts listener ratings for drums and bass stems. Frechet Audio Distance\n(FAD) computed with the CLAP-LAION-music embedding also performs\ncompetitively--achieving Kendall's tau values of 0.25 for drums and 0.19 for\nbass--matching or surpassing energy-based metrics for those stems. However,\nnone of the embedding-based metrics, including CLAP, correlate positively with\nhuman perception for vocal estimates. These findings highlight the need for\nstem-specific evaluation strategies and suggest that no single metric reliably\nreflects perceptual quality across all source types. We release our raw\nlistener ratings to support reproducibility and further research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.06917v2", "cate": "eess.AS", "date": "2025-07-09", "updated": "2025-07-14"}
{"id": "2507.09871", "title": "Task Priors: Enhancing Model Evaluation by Considering the Entire Space of Downstream Tasks", "authors": ["Niket Patel", "Randall Balestriero"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09871v1", "summary": "The grand goal of AI research, and particularly Self Supervised Learning\n(SSL), is to produce systems that can successfully solve any possible task. In\ncontrast, current evaluation methods available to AI researchers typically rely\non a fixed collection of hand-picked downstream benchmarks. Hence, a large\namount of effort is put into designing and searching for large collection of\nevaluation tasks that can serve as a proxy of our grand goal. We argue that\nsuch a rigid evaluation protocol creates a silent bottleneck in AI research. To\nremedy that, we define a probabilistic space of downstream tasks obtained by\nadopting a distribution of tasks and by defining Task Priors. Under this view,\none can evaluate a model's performance over the set of all possible downstream\ntasks. Our framework is the first to provide answers to key questions such as\n(i) what is the average performance of my model over all possible downstream\ntasks weighted by the probability to encounter each task? or (ii) what is the\nvariance of my model's performance across all downstream tasks under the\ndefined Task Priors? Beyond establishing a new standard for evaluation, we\nbelieve that Task Priors will accelerate the pace of research in SSL - where\ndownstream task evaluation is the sole qualitative signal that researchers have\naccess to.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09871v1", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10065", "title": "MoVieS: Motion-Aware 4D Dynamic View Synthesis in One Second", "authors": ["Chenguo Lin", "Yuchen Lin", "Panwang Pan", "Yifan Yu", "Honglei Yan", "Katerina Fragkiadaki", "Yadong Mu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project page: this https URL", "url": "http://arxiv.org/abs/2507.10065v1", "summary": "We present MoVieS, a novel feed-forward model that synthesizes 4D dynamic\nnovel views from monocular videos in one second. MoVieS represents dynamic 3D\nscenes using pixel-aligned grids of Gaussian primitives, explicitly supervising\ntheir time-varying motion. This allows, for the first time, the unified\nmodeling of appearance, geometry and motion, and enables view synthesis,\nreconstruction and 3D point tracking within a single learning-based framework.\nBy bridging novel view synthesis with dynamic geometry reconstruction, MoVieS\nenables large-scale training on diverse datasets with minimal dependence on\ntask-specific supervision. As a result, it also naturally supports a wide range\nof zero-shot applications, such as scene flow estimation and moving object\nsegmentation. Extensive experiments validate the effectiveness and efficiency\nof MoVieS across multiple tasks, achieving competitive performance while\noffering several orders of magnitude speedups.", "comment": "Project page: https://chenguolin.github.io/projects/MoVieS", "pdf_url": "http://arxiv.org/pdf/2507.10065v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10145", "title": "Intrinsic frequency distribution characterises neural dynamics", "authors": ["Ryohei Fukuma", "Yoshinobu Kawahara", "Okito Yamashita", "Kei Majima", "Haruhiko Kishima", "Takufumi Yanagisawa"], "categories": ["eess.SP", "q-bio.NC"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10145v1", "summary": "Decomposing multivariate time series with certain basic dynamics is crucial\nfor understanding, predicting and controlling nonlinear spatiotemporally\ndynamic systems such as the brain. Dynamic mode decomposition (DMD) is a method\nfor decomposing nonlinear spatiotemporal dynamics into several basic dynamics\n(dynamic modes; DMs) with intrinsic frequencies and decay rates. In particular,\nunlike Fourier transform-based methods, which are used to decompose a\nsingle-channel signal into the amplitudes of sinusoidal waves with discrete\nfrequencies at a regular interval, DMD can derive the intrinsic frequencies of\na multichannel signal on the basis of the available data; furthermore, it can\ncapture nonstationary components such as alternations between states with\ndifferent intrinsic frequencies. Here, we propose the use of the distribution\nof intrinsic frequencies derived from DMDs (DM frequencies) to characterise\nneural activities. The distributions of DM frequencies in the\nelectroencephalograms of healthy subjects and patients with dementia or\nParkinson's disease in a resting state were evaluated. By using the\ndistributions, these patients were distinguished from healthy subjects with\nsignificantly greater accuracy than when using amplitude spectra derived by\ndiscrete Fourier transform. This finding suggests that the distribution of DM\nfrequencies exhibits distinct behaviour from amplitude spectra, and therefore,\nthe distribution may serve as a new biomarker by characterising the nonlinear\nspatiotemporal dynamics of electrophysiological signals.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10145v1", "cate": "eess.SP", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09966", "title": "A Brain Tumor Segmentation Method Based on CLIP and 3D U-Net with Cross-Modal Semantic Guidance and Multi-Level Feature Fusion", "authors": ["Mingda Zhang"], "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      13 pages,6 figures", "url": "http://arxiv.org/abs/2507.09966v1", "summary": "Precise segmentation of brain tumors from magnetic resonance imaging (MRI) is\nessential for neuro-oncology diagnosis and treatment planning. Despite advances\nin deep learning methods, automatic segmentation remains challenging due to\ntumor morphological heterogeneity and complex three-dimensional spatial\nrelationships. Current techniques primarily rely on visual features extracted\nfrom MRI sequences while underutilizing semantic knowledge embedded in medical\nreports. This research presents a multi-level fusion architecture that\nintegrates pixel-level, feature-level, and semantic-level information,\nfacilitating comprehensive processing from low-level data to high-level\nconcepts. The semantic-level fusion pathway combines the semantic understanding\ncapabilities of Contrastive Language-Image Pre-training (CLIP) models with the\nspatial feature extraction advantages of 3D U-Net through three mechanisms:\n3D-2D semantic bridging, cross-modal semantic guidance, and semantic-based\nattention mechanisms. Experimental validation on the BraTS 2020 dataset\ndemonstrates that the proposed model achieves an overall Dice coefficient of\n0.8567, representing a 4.8% improvement compared to traditional 3D U-Net, with\na 7.3% Dice coefficient increase in the clinically important enhancing tumor\n(ET) region.", "comment": "13 pages,6 figures", "pdf_url": "http://arxiv.org/pdf/2507.09966v1", "cate": "eess.IV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09875", "title": "Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition", "authors": ["Qinyuan Ye", "Robin Jia", "Xiang Ren"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Code: this https URL", "url": "http://arxiv.org/abs/2507.09875v1", "summary": "Large language models demonstrate the intriguing ability to perform unseen\ntasks via in-context learning. However, it remains unclear what mechanisms\ninside the model drive such task-level generalization. In this work, we\napproach this question through the lens of off-by-one addition (i.e., 1+1=3,\n2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function\nas a second step. Leveraging circuit-style interpretability techniques such as\npath patching, we analyze the models' internal computations behind their\nnotable performance and present three key findings. First, we uncover a\nfunction induction mechanism that explains the model's generalization from\nstandard addition to off-by-one addition. This mechanism resembles the\nstructure of the induction head mechanism found in prior work and elevates it\nto a higher level of abstraction. Second, we show that the induction of the +1\nfunction is governed by multiple attention heads in parallel, each of which\nemits a distinct piece of the +1 function. Finally, we find that this function\ninduction mechanism is reused in a broader range of tasks, including synthetic\ntasks such as shifted multiple-choice QA and algorithmic tasks such as base-8\naddition. Overall, our findings offer deeper insights into how reusable and\ncomposable structures within language models enable task-level generalization.", "comment": "Code: https://github.com/INK-USC/function-induction", "pdf_url": "http://arxiv.org/pdf/2507.09875v1", "cate": "cs.CL", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10072", "title": "Frequency Regulation for Exposure Bias Mitigation in Diffusion Models", "authors": ["Meng Yu", "Kun Zhan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ACM Multimedia 2025 accepted!", "url": "http://arxiv.org/abs/2507.10072v1", "summary": "Diffusion models exhibit impressive generative capabilities but are\nsignificantly impacted by exposure bias. In this paper, we make a key\nobservation: the energy of the predicted noisy images decreases during the\ndiffusion process. Building on this, we identify two important findings: 1) The\nreduction in energy follows distinct patterns in the low-frequency and\nhigh-frequency subbands; 2) This energy reduction results in amplitude\nvariations between the network-reconstructed clean data and the real clean\ndata. Based on the first finding, we introduce a frequency-domain regulation\nmechanism utilizing wavelet transforms, which separately adjusts the low- and\nhigh-frequency subbands. Leveraging the second insight, we provide a more\naccurate analysis of exposure bias in the two subbands. Our method is\ntraining-free and plug-and-play, significantly improving the generative quality\nof various diffusion models and providing a robust solution to exposure bias\nacross different model architectures. The source code is available at\nhttps://github.com/kunzhan/wpp.", "comment": "ACM Multimedia 2025 accepted!", "pdf_url": "http://arxiv.org/pdf/2507.10072v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10167", "title": "Pinching-Antenna Systems for Physical Layer Security", "authors": ["Kaidi Wang", "Zhiguo Ding", "Naofal Al-Dhahir"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10167v1", "summary": "This letter investigates the potential of pinching-antenna systems for\nenhancing physical layer security. By pre-installing multiple pinching antennas\nat discrete positions along a waveguide, the capability of the considered\nsystem to perform amplitude and phase adjustment is validated through the\nformulation of a secrecy rate maximization problem. Specifically, amplitude\ncontrol is applied to enhance the signal quality at the legitimate user, while\nphase alignment is designed to degrade the received signal quality at the\neavesdropper. This cooperation among pinching antennas is modeled as a\ncoalitional game, and a corresponding antenna activation algorithm is proposed.\nThe individual impact of each antenna is quantified based on the Shapley value\nand marginal contribution, providing a fair and efficient method for\nperformance evaluation. Simulation results show that the considered\npinching-antenna system achieves significant improvements in secrecy rate, and\nthat the Shapley value based algorithm outperforms conventional coalition value\nbased solutions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10167v1", "cate": "eess.SP", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09995", "title": "Graph-based Multi-Modal Interaction Lightweight Network for Brain Tumor Segmentation (GMLN-BTS) in Edge Iterative MRI Lesion Localization System (EdgeIMLocSys)", "authors": ["Guohao Huo", "Ruiting Dai", "Hao Tang"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09995v1", "summary": "Brain tumor segmentation plays a critical role in clinical diagnosis and\ntreatment planning, yet the variability in imaging quality across different MRI\nscanners presents significant challenges to model generalization. To address\nthis, we propose the Edge Iterative MRI Lesion Localization System\n(EdgeIMLocSys), which integrates Continuous Learning from Human Feedback to\nadaptively fine-tune segmentation models based on clinician feedback, thereby\nenhancing robustness to scanner-specific imaging characteristics. Central to\nthis system is the Graph-based Multi-Modal Interaction Lightweight Network for\nBrain Tumor Segmentation (GMLN-BTS), which employs a Modality-Aware Adaptive\nEncoder (M2AE) to extract multi-scale semantic features efficiently, and a\nGraph-based Multi-Modal Collaborative Interaction Module (G2MCIM) to model\ncomplementary cross-modal relationships via graph structures. Additionally, we\nintroduce a novel Voxel Refinement UpSampling Module (VRUM) that\nsynergistically combines linear interpolation and multi-scale transposed\nconvolutions to suppress artifacts while preserving high-frequency details,\nimproving segmentation boundary accuracy. Our proposed GMLN-BTS model achieves\na Dice score of 85.1% on the BraTS2017 dataset with only 4.58 million\nparameters, representing a 98% reduction compared to mainstream 3D Transformer\nmodels, and significantly outperforms existing lightweight approaches. This\nwork demonstrates a synergistic breakthrough in achieving high-accuracy,\nresource-efficient brain tumor segmentation suitable for deployment in\nresource-constrained clinical environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09995v1", "cate": "eess.IV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09887", "title": "TolerantECG: A Foundation Model for Imperfect Electrocardiogram", "authors": ["Huynh Nguyen Dang", "Thang Pham", "Ngan Le", "Van Nguyen"], "categories": ["cs.LG", "cs.AI", "eess.SP"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      10 pages, 6 figures. Accepted to ACM Multimedia 2025", "url": "http://arxiv.org/abs/2507.09887v1", "summary": "The electrocardiogram (ECG) is an essential and effective tool for diagnosing\nheart diseases. However, its effectiveness can be compromised by noise or\nunavailability of one or more leads of the standard 12-lead recordings,\nresulting in diagnostic errors or uncertainty. To address these challenges, we\npropose TolerantECG, a foundation model for ECG signals that is robust to noise\nand capable of functioning with arbitrary subsets of the standard 12-lead ECG.\nTolerantECG training combines contrastive and self-supervised learning\nframeworks to jointly learn ECG signal representations alongside their\ncorresponding knowledge-retrieval-based text report descriptions and corrupted\nor lead-missing signals. Comprehensive benchmarking results demonstrate that\nTolerantECG consistently ranks as the best or second-best performer across\nvarious ECG signal conditions and class levels in the PTB-XL dataset, and\nachieves the highest performance on the MIT-BIH Arrhythmia Database.", "comment": "10 pages, 6 figures. Accepted to ACM Multimedia 2025", "pdf_url": "http://arxiv.org/pdf/2507.09887v1", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10084", "title": "A Transfer Learning-Based Method for Water Body Segmentation in Remote Sensing Imagery: A Case Study of the Zhada Tulin Area", "authors": ["Haonan Chen", "Xin Tong"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      13 pages, 6 figures, 2 tables", "url": "http://arxiv.org/abs/2507.10084v1", "summary": "To address the prevalent challenges of domain shift and small sample sizes in\nremote sensing image water body segmentation, this study proposes and validates\na two-stage transfer learning strategy based on the SegFormer model. The\napproach begins by training a foundational segmentation model on a diverse\nsource domain, where it achieves an Intersection over Union (IoU) of 68.80% on\nits validation set, followed by fine-tuning on data from the distinct target\ndomain. Focusing on the Zhada Tulin area in Tibet -- a region characterized by\nhighly complex topography and spectral features -- the experimental results\ndemonstrate that this strategy significantly boosts the IoU for the water body\nsegmentation task from 25.50% (for direct transfer) to 64.84%. This not only\neffectively resolves the model performance degradation caused by domain\ndiscrepancy but also provides an effective technical paradigm for\nhigh-precision thematic information extraction in data-scarce and\nenvironmentally unique remote sensing scenarios.", "comment": "13 pages, 6 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2507.10084v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10173", "title": "Pinching-Antenna Systems with LoS Blockages", "authors": ["Kaidi Wang", "Chongjun Ouyang", "Yuanwei Liu", "Zhiguo Ding"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10173v1", "summary": "The aim of this letter is to explore the capability of pinching-antenna\nsystems to construct line-of-sight (LoS) links in the presence of LoS\nblockages. Specifically, pinching antennas are pre-installed at preconfigured\npositions along waveguides and can be selectively activated to create LoS links\nfor enhancing desired signals and non-line-of-sight (NLoS) links for\neliminating inter-user interference. On this basis, a sum-rate maximization\nproblem is formulated by jointly optimizing waveguide assignment and antenna\nactivation. To solve this problem, a matching based algorithm is proposed using\ntwo distinct preference designs. Simulation results demonstrate that the\nconsidered pinching-antenna system and proposed solutions can dynamically\nestablish LoS links and effectively exploit LoS blockages to mitigate\ninterference, thereby significantly improving system throughput.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10173v1", "cate": "eess.SP", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10250", "title": "DepViT-CAD: Deployable Vision Transformer-Based Cancer Diagnosis in Histopathology", "authors": ["Ashkan Shakarami", "Lorenzo Nicole", "Rocco Cappellesso", "Angelo Paolo Dei Tos", "Stefano Ghidoni"], "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      25 pages, 15 figures", "url": "http://arxiv.org/abs/2507.10250v1", "summary": "Accurate and timely cancer diagnosis from histopathological slides is vital\nfor effective clinical decision-making. This paper introduces DepViT-CAD, a\ndeployable AI system for multi-class cancer diagnosis in histopathology. At its\ncore is MAViT, a novel Multi-Attention Vision Transformer designed to capture\nfine-grained morphological patterns across diverse tumor types. MAViT was\ntrained on expert-annotated patches from 1008 whole-slide images, covering 11\ndiagnostic categories, including 10 major cancers and non-tumor tissue.\nDepViT-CAD was validated on two independent cohorts: 275 WSIs from The Cancer\nGenome Atlas and 50 routine clinical cases from pathology labs, achieving\ndiagnostic sensitivities of 94.11% and 92%, respectively. By combining\nstate-of-the-art transformer architecture with large-scale real-world\nvalidation, DepViT-CAD offers a robust and scalable approach for AI-assisted\ncancer diagnostics. To support transparency and reproducibility, software and\ncode will be made publicly available at GitHub.", "comment": "25 pages, 15 figures", "pdf_url": "http://arxiv.org/pdf/2507.10250v1", "cate": "eess.IV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09888", "title": "NeuTSFlow: Modeling Continuous Functions Behind Time Series Forecasting", "authors": ["Huibo Xu", "Likang Wu", "Xianquan Wang", "Haoning Dang", "Chun-Wun Cheng", "Angelica I Aviles-Rivero", "Qi Liu"], "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09888v1", "summary": "Time series forecasting is a fundamental task with broad applications, yet\nconventional methods often treat data as discrete sequences, overlooking their\norigin as noisy samples of continuous processes. Crucially, discrete noisy\nobservations cannot uniquely determine a continuous function; instead, they\ncorrespond to a family of plausible functions. Mathematically, time series can\nbe viewed as noisy observations of a continuous function family governed by a\nshared probability measure. Thus, the forecasting task can be framed as\nlearning the transition from the historical function family to the future\nfunction family. This reframing introduces two key challenges: (1) How can we\nleverage discrete historical and future observations to learn the relationships\nbetween their underlying continuous functions? (2) How can we model the\ntransition path in function space from the historical function family to the\nfuture function family? To address these challenges, we propose NeuTSFlow, a\nnovel framework that leverages Neural Operators to facilitate flow matching for\nlearning path of measure between historical and future function families. By\nparameterizing the velocity field of the flow in infinite-dimensional function\nspaces, NeuTSFlow moves beyond traditional methods that focus on dependencies\nat discrete points, directly modeling function-level features instead.\nExperiments on diverse forecasting tasks demonstrate NeuTSFlow's superior\naccuracy and robustness, validating the effectiveness of the function-family\nperspective.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09888v1", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10095", "title": "FIX-CLIP: Dual-Branch Hierarchical Contrastive Learning via Synthetic Captions for Better Understanding of Long Text", "authors": ["Bingchao Wang", "Zhiwei Ning", "Jianyu Ding", "Xuanang Gao", "Yin Li", "Dongsheng Jiang", "Jie Yang", "Wei Liu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10095v1", "summary": "CLIP has shown promising performance across many short-text tasks in a\nzero-shot manner. However, limited by the input length of the text encoder,\nCLIP struggles on under-stream tasks with long-text inputs (>77 tokens). To\nremedy this issue, we propose FIX-CLIP which includes three novel modules: (1)\nA dual-branch training pipeline that aligns short and long texts with masked\nand raw images respectively, which boosts the long-text representation while\npreserving the short-text ability. (2) Multiple learnable regional prompts with\nunidirectional masks in Transformer layers for regional information extraction.\n(3) A hierarchical feature alignment module in the intermediate encoder layers\nto promote the consistency of multi-scale features. Furthermore, we collect 30M\nimages and utilize existing MLLMs to synthesize long-text captions for\ntraining. Extensive experiments show that FIX-CLIP achieves state-of-the-art\nperformance on both long-text and short-text retrieval benchmarks. For\ndownstream applications, we reveal that FIX-CLIP's text encoder delivers\npromising performance in a plug-and-play manner for diffusion models with\nlong-text input.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10095v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10308", "title": "Enhanced Throughput and Seamless Handover Solutions for Urban 5G-Vehicle C-Band Integrated Satellite-Terrestrial Networks", "authors": ["Hung Nguyen-Kha", "Vu Nguyen Ha", "Eva Lagunas", "Symeon Chatzinotas", "Joel Grotz"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      ACCEPTED FOR PUBLICATION IN IEEE TRANSACTIONS ON COMMUNICATIONS", "url": "http://arxiv.org/abs/2507.10308v1", "summary": "This paper investigates downlink transmission in 5G Integrated\nSatellite-Terrestrial Networks (ISTNs) supporting automotive users (UEs) in\nurban environments, where base stations (BSs) and Low Earth Orbit (LEO)\nsatellites (LSats) cooperate to serve moving UEs over shared C-band frequency\ncarriers. Urban settings, characterized by dense obstructions, together with UE\nmobility, and the dynamic movement and coverage of LSats pose significant\nchallenges to user association and resource allocation. To address these\nchallenges, we formulate a multi-objective optimization problem designed to\nimprove both throughput and seamless handover (HO). Particularly, the\nformulated problem balances sum-rate (SR) maximization and connection change\n(CC) minimization through a weighted trade-off by jointly optimizing power\nallocation and BS-UE/LSat-UE associations over a given time window. This is a\nmixed-integer and non-convex problem which is inherently difficult to solve. To\nsolve this problem efficiently, we propose an iterative algorithm based on the\nSuccessive Convex Approximation (SCA) technique. Furthermore, we introduce a\npractical prediction-based algorithm capable of providing efficient solutions\nin real-world implementations. Especially, the simulations use a realistic 3D\nmap of London and UE routes obtained from the Google Navigator application to\nensure practical examination. Thanks to these realistic data, the simulation\nresults can show valuable insights into the link budget assessment in urban\nareas due to the impact of buildings on transmission links under the blockage,\nreflection, and diffraction effects. Furthermore, the numerical results\ndemonstrate the effectiveness of our proposed algorithms in terms of SR and the\nCC-number compared to the greedy and benchmark algorithms.", "comment": "ACCEPTED FOR PUBLICATION IN IEEE TRANSACTIONS ON COMMUNICATIONS", "pdf_url": "http://arxiv.org/pdf/2507.10308v1", "cate": "eess.SP", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09697", "title": "Curvature-adaptive gigapixel microscopy at submicron resolution and centimeter scale", "authors": ["Xi Yang", "Haitao Chen", "Lucas Kreiss", "Clare B. Cook", "Genevieve Kuczewski", "Mark Harfouche", "Martin O. Bohlen", "Roarke Horstmeyer"], "categories": ["physics.optics", "eess.IV"], "primary_category": "Subjects:       Optics (physics.optics)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09697v1", "summary": "Large-area microscopy with submicron resolution is limited by tradeoffs\nbetween field of view (FOV), resolution, and imaging speed. Samples are rarely\nflat across centimeter-scale FOV, which often requires existing solutions to\nuse mechanical scanning to ensure focused capture at reduced throughput. Here,\nwe present PANORAMA, a single-shot, re-imaging microscope that achieves\nseamless, gigapixel imaging over a 16.3$\\times$18.8 $\\text{mm}^2$ FOV at 0.84\num resolution without mechanical scanning. By using a telecentric\nphotolithography lens, a large-aperture tube lens, and a flat micro-camera\narray with adaptive per-camera focus control, PANORAMA maintains submicron\nfocus across flat, curved or uneven samples that span centimeters. This\napproach improves imaging throughput and adaptability, enabling gigapixel\nmulti-modal microscopy of large flat and non-flat samples in one shot, thus\nbroadening its applications in biomedical and materials imaging.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09697v1", "cate": "physics.optics", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.09890", "title": "Soft Graph Clustering for single-cell RNA Sequencing Data", "authors": ["Ping Xu", "Pengfei Wang", "Zhiyuan Ning", "Meng Xiao", "Min Wu", "Yuanchun Zhou"], "categories": ["cs.LG", "cs.AI", "q-bio.GN"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09890v1", "summary": "Clustering analysis is fundamental in single-cell RNA sequencing (scRNA-seq)\ndata analysis for elucidating cellular heterogeneity and diversity. Recent\ngraph-based scRNA-seq clustering methods, particularly graph neural networks\n(GNNs), have significantly improved in tackling the challenges of\nhigh-dimension, high-sparsity, and frequent dropout events that lead to\nambiguous cell population boundaries. However, their reliance on hard graph\nconstructions derived from thresholded similarity matrices presents\nchallenges:(i) The simplification of intercellular relationships into binary\nedges (0 or 1) by applying thresholds, which restricts the capture of\ncontinuous similarity features among cells and leads to significant information\nloss.(ii) The presence of significant inter-cluster connections within hard\ngraphs, which can confuse GNN methods that rely heavily on graph structures,\npotentially causing erroneous message propagation and biased clustering\noutcomes. To tackle these challenges, we introduce scSGC, a Soft Graph\nClustering for single-cell RNA sequencing data, which aims to more accurately\ncharacterize continuous similarities among cells through non-binary edge\nweights, thereby mitigating the limitations of rigid data structures. The scSGC\nframework comprises three core components: (i) a zero-inflated negative\nbinomial (ZINB)-based feature autoencoder; (ii) a dual-channel cut-informed\nsoft graph embedding module; and (iii) an optimal transport-based clustering\noptimization module. Extensive experiments across ten datasets demonstrate that\nscSGC outperforms 13 state-of-the-art clustering models in clustering accuracy,\ncell type annotation, and computational efficiency. These results highlight its\nsubstantial potential to advance scRNA-seq data analysis and deepen our\nunderstanding of cellular heterogeneity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09890v1", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10115", "title": "Glance-MCMT: A General MCMT Framework with Glance Initialization and Progressive Association", "authors": ["Hamidreza Hashempoor"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10115v1", "summary": "We propose a multi-camera multi-target (MCMT) tracking framework that ensures\nconsistent global identity assignment across views using trajectory and\nappearance cues. The pipeline starts with BoT-SORT-based single-camera\ntracking, followed by an initial glance phase to initialize global IDs via\ntrajectory-feature matching. In later frames, new tracklets are matched to\nexisting global identities through a prioritized global matching strategy. New\nglobal IDs are only introduced when no sufficiently similar trajectory or\nfeature match is found. 3D positions are estimated using depth maps and\ncalibration for spatial validation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10115v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09091", "title": "Continuous-Time Signal Decomposition: An Implicit Neural Generalization of PCA and ICA", "authors": ["Shayan K. Azmoodeh", "Krishna Subramani", "Paris Smaragdis"], "categories": ["cs.LG", "eess.SP", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      6 pages, 3 figures, 1 table. MLSP 2025", "url": "http://arxiv.org/abs/2507.09091v1", "summary": "We generalize the low-rank decomposition problem, such as principal and\nindependent component analysis (PCA, ICA) for continuous-time vector-valued\nsignals and provide a model-agnostic implicit neural signal representation\nframework to learn numerical approximations to solve the problem. Modeling\nsignals as continuous-time stochastic processes, we unify the approaches to\nboth the PCA and ICA problems in the continuous setting through a contrast\nfunction term in the network loss, enforcing the desired statistical properties\nof the source signals (decorrelation, independence) learned in the\ndecomposition. This extension to a continuous domain allows the application of\nsuch decompositions to point clouds and irregularly sampled signals where\nstandard techniques are not applicable.", "comment": "6 pages, 3 figures, 1 table. MLSP 2025", "pdf_url": "http://arxiv.org/pdf/2507.09091v1", "cate": "cs.LG", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2507.10222", "title": "Spatial Lifting for Dense Prediction", "authors": ["Mingzhi Xu", "Yizhe Zhang"], "categories": ["cs.CV", "cs.LG", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Preprint. Under review", "url": "http://arxiv.org/abs/2507.10222v1", "summary": "We present Spatial Lifting (SL), a novel methodology for dense prediction\ntasks. SL operates by lifting standard inputs, such as 2D images, into a\nhigher-dimensional space and subsequently processing them using networks\ndesigned for that higher dimension, such as a 3D U-Net. Counterintuitively,\nthis dimensionality lifting allows us to achieve good performance on benchmark\ntasks compared to conventional approaches, while reducing inference costs and\nsignificantly lowering the number of model parameters. The SL framework\nproduces intrinsically structured outputs along the lifted dimension. This\nemergent structure facilitates dense supervision during training and enables\nrobust, near-zero-additional-cost prediction quality assessment at test time.\nWe validate our approach across 19 benchmark datasets (13 for semantic\nsegmentation and 6 for depth estimation), demonstrating competitive dense\nprediction performance while reducing the model parameter count by over 98% (in\nthe U-Net case) and lowering inference costs. Spatial Lifting introduces a new\nvision modeling paradigm that offers a promising path toward more efficient,\naccurate, and reliable deep networks for dense prediction tasks in vision.", "comment": "Preprint. Under review", "pdf_url": "http://arxiv.org/pdf/2507.10222v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09891", "title": "Sequence-Model-Guided Measurement Selection for Quantum State Learning", "authors": ["Jiaxin Huang", "Yan Zhu", "Giulio Chiribella", "Ya-Dong Wu"], "categories": ["quant-ph", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09891v1", "summary": "Characterization of quantum systems from experimental data is a central\nproblem in quantum science and technology. But which measurements should be\nused to gather data in the first place? While optimal measurement choices can\nbe worked out for small quantum systems, the optimization becomes intractable\nas the system size grows large. To address this problem, we introduce a deep\nneural network with a sequence model architecture that searches for efficient\nmeasurement choices in a data-driven, adaptive manner. The model can be applied\nto a variety of tasks, including the prediction of linear and nonlinear\nproperties of quantum states, as well as state clustering and state tomography\ntasks. In all these tasks, we find that the measurement choices identified by\nour neural network consistently outperform the uniformly random choice.\nIntriguingly, for topological quantum systems, our model tends to recommend\nmeasurements at the system's boundaries, even when the task is to predict bulk\nproperties. This behavior suggests that the neural network may have\nindependently discovered a connection between boundaries and bulk, without\nhaving been provided any built-in knowledge of quantum physics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09891v1", "cate": "quant-ph", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10118", "title": "DEARLi: Decoupled Enhancement of Recognition and Localization for Semi-supervised Panoptic Segmentation", "authors": ["Ivan Martinović", "Josip Šarić", "Marin Oršić", "Matej Kristan", "Siniša Šegvić"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025 Findings Workshop", "url": "http://arxiv.org/abs/2507.10118v1", "summary": "Pixel-level annotation is expensive and time-consuming. Semi-supervised\nsegmentation methods address this challenge by learning models on few labeled\nimages alongside a large corpus of unlabeled images. Although foundation models\ncould further account for label scarcity, effective mechanisms for their\nexploitation remain underexplored. We address this by devising a novel\nsemi-supervised panoptic approach fueled by two dedicated foundation models. We\nenhance recognition by complementing unsupervised mask-transformer consistency\nwith zero-shot classification of CLIP features. We enhance localization by\nclass-agnostic decoder warm-up with respect to SAM pseudo-labels. The resulting\ndecoupled enhancement of recognition and localization (DEARLi) particularly\nexcels in the most challenging semi-supervised scenarios with large taxonomies\nand limited labeled data. Moreover, DEARLi outperforms the state of the art in\nsemi-supervised semantic segmentation by a large margin while requiring 8x less\nGPU memory, in spite of being trained only for the panoptic objective. We\nobserve 29.9 PQ and 38.9 mIoU on ADE20K with only 158 labeled images. The\nsource code is available at https://github.com/helen1c/DEARLi.", "comment": "ICCV 2025 Findings Workshop", "pdf_url": "http://arxiv.org/pdf/2507.10118v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09717", "title": "Signed Graph Learning: Algorithms and Theory", "authors": ["Abdullah Karaaslanli", "Bisakh Banerjee", "Tapabrata Maiti", "Selin Aviyente"], "categories": ["stat.ML", "cs.LG", "eess.SP"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09717v1", "summary": "Real-world data is often represented through the relationships between data\nsamples, forming a graph structure. In many applications, it is necessary to\nlearn this graph structure from the observed data. Current graph learning\nresearch has primarily focused on unsigned graphs, which consist only of\npositive edges. However, many biological and social systems are better\ndescribed by signed graphs that account for both positive and negative\ninteractions, capturing similarity and dissimilarity between samples. In this\npaper, we develop a method for learning signed graphs from a set of smooth\nsigned graph signals. Specifically, we employ the net Laplacian as a graph\nshift operator (GSO) to define smooth signed graph signals as the outputs of a\nlow-pass signed graph filter defined by the net Laplacian. The signed graph is\nthen learned by formulating a non-convex optimization problem where the total\nvariation of the observed signals is minimized with respect to the net\nLaplacian. The proposed problem is solved using alternating direction method of\nmultipliers (ADMM) and a fast algorithm reducing the per-ADMM iteration\ncomplexity from quadratic to linear in the number of nodes is introduced.\nFurthermore, theoretical proofs of convergence for the algorithm and a bound on\nthe estimation error of the learned net Laplacian as a function of sample size,\nnumber of nodes, and graph topology are provided. Finally, the proposed method\nis evaluated on simulated data and gene regulatory network inference problem\nand compared to existing signed graph learning methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09717v1", "cate": "stat.ML", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2507.10230", "title": "Exogeneous PpIX model for brain tumour assessment", "authors": ["John Raschke", "Jean Pierre Ndabakuranye", "Bobbi Fleiss", "Arman Ahnood"], "categories": ["physics.bio-ph", "eess.IV"], "primary_category": "Subjects:       Biological Physics (physics.bio-ph)", "pdf_link": null, "comments": "Comments:      20 pages, 7 figures", "url": "http://arxiv.org/abs/2507.10230v1", "summary": "Reliable in-vitro models are used for optoelectronic device development such\nas fluorescence detection devices for fluorescence-guided surgery of gliomas. A\ncommon approach is based on inducing gliomas in animal models. This is followed\nby a dosage of 5-ALA to induce Protoporphyrin IX (PpIX) in the glioma and which\nfluoresces. Although these approaches excel in capturing key biomolecular and\nphysiological features of the tumour, they are inherently indeterministic. This\nlimits the scope of their use for preclinical device development, where\nconsistent and controllable tumour reproduction across multiple animals is\nneeded. Approaches using fluorescence markers in gelatine provide a simple\nreplication but fail to capture the complexities of in-vivo models. In this\nstudy, we introduce an exogenous brain tumour model for assessing PpIX\nfluorescence detection. The model was developed by injecting a PpIX solution\ninto the cortical region of a resected adult rat brain, the injection site\nsimulated a tumoral region with elevated PpIX concentration. The tumoral region\nhad a gradient of concentrations, with a peak at the centre and a decrease\ntowards the margins, akin to in-vivo gliomas. The fluorescence profile was\ncompared to in-vivo conditions using 5-ALA and correlated well with other\nreported works, achieving a correlation of R2>0.93. The model's validity was\ntested by examining the effect of the solvent, DMSO, on the Autofluorescence\n(AF) of the brain sample and the short-term effect of storage on AF was\nanalysed. Examinations confirmed the solvent did not alter AF, and the brain\nsample should be stored in Hanks Balanced Salt Solution and refrigerated to\nmaintain moisture and preserve AF. The model accurately replicated surgical\nfluorescence conditions and offers a suitable alternative to glioma induction,\nbenefiting the development of fluorescence detection devices across design\niterations.", "comment": "20 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.10230v1", "cate": "physics.bio-ph", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09931", "title": "Mechanistic Interpretability of LoRA-Adapted Language Models for Nuclear Reactor Safety Applications", "authors": ["Yoon Pyo Lee"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Submitted to Nuclear Technology. 22 pages, 2 tables, 4 figures", "url": "http://arxiv.org/abs/2507.09931v1", "summary": "The integration of Large Language Models (LLMs) into safety-critical domains,\nsuch as nuclear engineering, necessitates a deep understanding of their\ninternal reasoning processes. This paper presents a novel methodology for\ninterpreting how an LLM encodes and utilizes domain-specific knowledge, using a\nBoiling Water Reactor system as a case study. We adapted a general-purpose LLM\n(Gemma-3-1b-it) to the nuclear domain using a parameter-efficient fine-tuning\ntechnique known as Low-Rank Adaptation. By comparing the neuron activation\npatterns of the base model to those of the fine-tuned model, we identified a\nsparse set of neurons whose behavior was significantly altered during the\nadaptation process. To probe the causal role of these specialized neurons, we\nemployed a neuron silencing technique. Our results demonstrate that while\nsilencing most of these specialized neurons individually did not produce a\nstatistically significant effect, deactivating the entire group collectively\nled to a statistically significant degradation in task performance. Qualitative\nanalysis further revealed that silencing these neurons impaired the model's\nability to generate detailed, contextually accurate technical information. This\npaper provides a concrete methodology for enhancing the transparency of an\nopaque black-box model, allowing domain expertise to be traced to verifiable\nneural circuits. This offers a pathway towards achieving nuclear-grade\nartificial intelligence (AI) assurance, addressing the verification and\nvalidation challenges mandated by nuclear regulatory frameworks (e.g., 10 CFR\n50 Appendix B), which have limited AI deployment in safety-critical nuclear\noperations.", "comment": "Submitted to Nuclear Technology. 22 pages, 2 tables, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.09931v1", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10127", "title": "Taming Modern Point Tracking for Speckle Tracking Echocardiography via Impartial Motion", "authors": ["Md Abulkalam Azad", "John Nyberg", "Håvard Dalen", "Bjørnar Grenne", "Lasse Lovstakken", "Andreas Østvik"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to CVAMD workshop at ICCV 2025", "url": "http://arxiv.org/abs/2507.10127v1", "summary": "Accurate motion estimation for tracking deformable tissues in\nechocardiography is essential for precise cardiac function measurements. While\ntraditional methods like block matching or optical flow struggle with intricate\ncardiac motion, modern point tracking approaches remain largely underexplored\nin this domain. This work investigates the potential of state-of-the-art (SOTA)\npoint tracking methods for ultrasound, with a focus on echocardiography.\nAlthough these novel approaches demonstrate strong performance in general\nvideos, their effectiveness and generalizability in echocardiography remain\nlimited. By analyzing cardiac motion throughout the heart cycle in real B-mode\nultrasound videos, we identify that a directional motion bias across different\nviews is affecting the existing training strategies. To mitigate this, we\nrefine the training procedure and incorporate a set of tailored augmentations\nto reduce the bias and enhance tracking robustness and generalization through\nimpartial cardiac motion. We also propose a lightweight network leveraging\nmulti-scale cost volumes from spatial context alone to challenge the advanced\nspatiotemporal point tracking models. Experiments demonstrate that fine-tuning\nwith our strategies significantly improves models' performances over their\nbaselines, even for out-of-distribution (OOD) cases. For instance, EchoTracker\nboosts overall position accuracy by 60.7% and reduces median trajectory error\nby 61.5% across heart cycle phases. Interestingly, several point tracking\nmodels fail to outperform our proposed simple model in terms of tracking\naccuracy and generalization, reflecting their limitations when applied to\nechocardiography. Nevertheless, clinical evaluation reveals that these methods\nimprove GLS measurements, aligning more closely with expert-validated,\nsemi-automated tools and thus demonstrating better reproducibility in\nreal-world applications.", "comment": "Accepted to CVAMD workshop at ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.10127v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2402.09007", "title": "Enhancing Hemodynamic Parameter Estimations: Nonlinear Blood Behavior in 4D Flow MRI", "authors": ["Hernán Mella", "Felipe Galarce", "Tetsuro Sekine", "Julio Sotelo", "Ernesto Castillo"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2402.09007v3", "summary": "Hemodynamic parameters are often estimated assuming a constant Newtonian\nviscosity, even though blood exhibits shear-thinning behavior. This article\ninvestigates the influence of blood rheology and hematocrit (Hct) percentage on\nthe estimation of Wall Shear Stress (WSS), rate of viscous Energy Loss\n($\\dot{E}_L$) at different points in the cardiac cycle, and the Oscillatory\nShear Index (OSI). We focus on a hematocrit-dependent power-law non-Newtonian\nmodel, considering a wide range of Hct values at physiological temperature,\nwith rheological parameters obtained from previously reported experimental\ndata. In all cases, we systematically compared WSS, $\\dot{E}_L$, and OSI using\nboth Newtonian and power-law models, underscoring the crucial role of blood\nrheology in accurately assessing cardiovascular diseases. Our results show\nthat, in in-silico experiments, differences in WSS and $\\dot{E}_L$ across a\nwide range of Hct values can reach as high as 190\\% and 113\\% at systole, and\nas low as -72\\% and -74\\% at diastole, respectively. In in-vivo data,\ndifferences in WSS and $\\dot{E}_L$ can reach up to -45\\% and -60\\% at systole,\nand range from -69\\% to 73\\% at diastole. This study enhances our understanding\nof the impact of blood rheology on hemodynamic parameter estimations using both\nin-silico and in-vivo aortic 4D Flow MRI data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2402.09007v3", "cate": "eess.SP", "date": "2024-02-14", "updated": "2025-07-13"}
{"id": "2507.10461", "title": "RAPNet: A Receptive-Field Adaptive Convolutional Neural Network for Pansharpening", "authors": ["Tao Tang", "Chengxu Yang"], "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      To appear in the proceedings of the 6th International Conference on Artificial Intelligence and Electromechanical Automation (AIEA 2025). 5 pages, 6 figures", "url": "http://arxiv.org/abs/2507.10461v1", "summary": "Pansharpening refers to the process of integrating a high resolution\npanchromatic (PAN) image with a lower resolution multispectral (MS) image to\ngenerate a fused product, which is pivotal in remote sensing. Despite the\neffectiveness of CNNs in addressing this challenge, they are inherently\nconstrained by the uniform application of convolutional kernels across all\nspatial positions, overlooking local content variations. To overcome this\nissue, we introduce RAPNet, a new architecture that leverages content-adaptive\nconvolution. At its core, RAPNet employs the Receptive-field Adaptive\nPansharpening Convolution (RAPConv), designed to produce spatially adaptive\nkernels responsive to local feature context, thereby enhancing the precision of\nspatial detail extraction. Additionally, the network integrates the\nPansharpening Dynamic Feature Fusion (PAN-DFF) module, which incorporates an\nattention mechanism to achieve an optimal balance between spatial detail\nenhancement and spectral fidelity. Comprehensive evaluations on publicly\navailable datasets confirm that RAPNet delivers superior performance compared\nto existing approaches, as demonstrated by both quantitative metrics and\nqualitative assessments. Ablation analyses further substantiate the\neffectiveness of the proposed adaptive components.", "comment": "To appear in the proceedings of the 6th International Conference on\n  Artificial Intelligence and Electromechanical Automation (AIEA 2025). 5\n  pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.10461v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.09935", "title": "Enhancing Retrieval Augmented Generation with Hierarchical Text Segmentation Chunking", "authors": ["Hai Toan Nguyen", "Tien Dat Nguyen", "Viet Ha Nguyen"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09935v1", "summary": "Retrieval-Augmented Generation (RAG) systems commonly use chunking strategies\nfor retrieval, which enhance large language models (LLMs) by enabling them to\naccess external knowledge, ensuring that the retrieved information is\nup-to-date and domain-specific. However, traditional methods often fail to\ncreate chunks that capture sufficient semantic meaning, as they do not account\nfor the underlying textual structure. This paper proposes a novel framework\nthat enhances RAG by integrating hierarchical text segmentation and clustering\nto generate more meaningful and semantically coherent chunks. During inference,\nthe framework retrieves information by leveraging both segment-level and\ncluster-level vector representations, thereby increasing the likelihood of\nretrieving more precise and contextually relevant information. Evaluations on\nthe NarrativeQA, QuALITY, and QASPER datasets indicate that the proposed method\nachieved improved results compared to traditional chunking techniques.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09935v1", "cate": "cs.CL", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10143", "title": "Deep Recurrence for Dynamical Segmentation Models", "authors": ["David Calhas", "Arlindo L. Oliveira"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      12 pages", "url": "http://arxiv.org/abs/2507.10143v1", "summary": "While biological vision systems rely heavily on feedback connections to\niteratively refine perception, most artificial neural networks remain purely\nfeedforward, processing input in a single static pass. In this work, we propose\na predictive coding inspired feedback mechanism that introduces a recurrent\nloop from output to input, allowing the model to refine its internal state over\ntime. We implement this mechanism within a standard U-Net architecture and\nintroduce two biologically motivated operations, softmax projection and\nexponential decay, to ensure stability of the feedback loop. Through controlled\nexperiments on a synthetic segmentation task, we show that the feedback model\nsignificantly outperforms its feedforward counterpart in noisy conditions and\ngeneralizes more effectively with limited supervision. Notably, feedback\nachieves above random performance with just two training examples, while the\nfeedforward model requires at least four. Our findings demonstrate that\nfeedback enhances robustness and data efficiency, and offer a path toward more\nadaptive and biologically inspired neural architectures. Code is available at:\ngithub.com/DCalhas/feedback_segmentation.", "comment": "12 pages", "pdf_url": "http://arxiv.org/pdf/2507.10143v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2407.20793", "title": "Detecting $\\sim$10 mK Face Temperature Change Based on Lock-in Thermography Referencing Heartbeat", "authors": ["Nanami Kotani", "Yasuaki Monnai"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      Accepted for publication in SICE Festival with Annual Conference 2024 (SICE FES 2024) organized by the Society of Instrument and Control Engineers this https URL", "url": "http://arxiv.org/abs/2407.20793v2", "summary": "Infrared thermography, which has widely spread particularly during the\nCOVID-19 period, has been effectively used for research on health monitoring\nand emotion estimation. Nevertheless, detecting minute temperature changes with\nthermography is challenging as it is disturbed by not only noise but also\noutside temperature surrounding the object. In this study, we demonstrate\ndetecting face temperature variation by implementing lock-in thermography using\nheartbeat signals as a reference. It allows us to detect minute temperature\nchanges, as low as $\\sim$10 mK, on the forehead with a commercially available\nthermal camera. The proposed approach enables stable measurement of body\ntemperature variation, showing potential for non-contact emotion estimation.", "comment": "Accepted for publication in SICE Festival with Annual Conference 2024\n  (SICE FES 2024) organized by the Society of Instrument and Control Engineers\n  https://ieeexplore.ieee.org/document/10805151", "pdf_url": "http://arxiv.org/pdf/2407.20793v2", "cate": "eess.SP", "date": "2024-07-30", "updated": "2025-07-12"}
{"id": "2210.14231", "title": "Neural Architecture Search generated Phase Retrieval Net for Real-time Off-axis Quantitative Phase Imaging", "authors": ["Xin Shu", "Mengxuan Niu", "Yi Zhang", "Wei Luo", "Renjie Zhou"], "categories": ["eess.IV", "cs.LG"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2210.14231v2", "summary": "In off-axis Quantitative Phase Imaging (QPI), artificial neural networks have\nbeen recently applied for phase retrieval with aberration compensation and\nphase unwrapping. However, the involved neural network architectures are\nlargely unoptimized and inefficient with low inference speed, which hinders the\nrealization of real-time imaging. Here, we propose a Neural Architecture Search\n(NAS) generated Phase Retrieval Net (NAS-PRNet) for accurate and fast phase\nretrieval. NAS-PRNet is an encoder-decoder style neural network, automatically\nfound from a large neural network architecture search space through NAS. By\nmodifying the differentiable NAS scheme from SparseMask, we learn the optimized\nskip connections through gradient descent. Specifically, we implement\nMobileNet-v2 as the encoder and define a synthesized loss that incorporates\nphase reconstruction loss and network sparsity loss. NAS-PRNet has achieved\nhigh-fidelity phase retrieval by achieving a peak Signal-to-Noise Ratio (PSNR)\nof 36.7 dB and a Structural SIMilarity (SSIM) of 86.6% as tested on\ninterferograms of biological cells. Notably, NAS-PRNet achieves phase retrieval\nin only 31 ms, representing 15x speedup over the most recent Mamba-UNet with\nonly a slightly lower phase retrieval accuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2210.14231v2", "cate": "eess.IV", "date": "2022-10-25", "updated": "2025-07-13"}
{"id": "2507.09937", "title": "Memorization Sinks: Isolating Memorization during LLM Training", "authors": ["Gaurav R. Ghosal", "Pratyush Maini", "Aditi Raghunathan"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted at the 2025 International Conference of Machine Learning", "url": "http://arxiv.org/abs/2507.09937v1", "summary": "Large language models are susceptible to memorizing repeated sequences,\nposing privacy and copyright concerns. A popular mitigation strategy is to\nremove memorized information from specific neurons post-hoc. However, such\napproaches have shown limited success so far. In a controlled setting, we show\nthat the memorization of natural sequences (those that resemble linguistically\nplausible text) become mechanistically entangled with general language\nabilities, thereby becoming challenging to remove post-hoc. In this work, we\nput forward a new paradigm of MemSinks that promotes isolation of memorization\nby design. We leverage a sequence identifier that activates a unique set of\nmemorization neurons for each sequence across repetitions. By analyzing the\ndynamics of learning and forgetting, we argue that MemSinks facilitates\nisolation of memorized content, making it easier to remove without compromising\ngeneral language capabilities. We implement MemSinks at the billion-parameter\nand billion-token scale, and observe both effective isolation and strong\ngeneralization. To our knowledge, this is the first proof-of-concept on real\ndata demonstrating that simultaneous generalization and isolation is\nachievable. We open-source our code at http://github.com/grghosal/MemSinks.", "comment": "Accepted at the 2025 International Conference of Machine Learning", "pdf_url": "http://arxiv.org/pdf/2507.09937v1", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10171", "title": "SlumpGuard: An AI-Powered Real-Time System for Automated Concrete Slump Prediction via Video Analysis", "authors": ["Youngmin Kim", "Giyeong Oh", "Kwangsoo Youm", "Youngjae Yu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10171v1", "summary": "Concrete workability is essential for construction quality, with the slump\ntest being the most common on-site method for its assessment. However,\ntraditional slump testing is manual, time-consuming, and prone to\ninconsistency, limiting its applicability for real-time monitoring. To address\nthese challenges, we propose SlumpGuard, an AI-powered, video-based system that\nautomatically analyzes concrete flow from the truck chute to assess workability\nin real time. Our system enables full-batch inspection without manual\nintervention, improving both the accuracy and efficiency of quality control. We\npresent the system design, a the construction of a dedicated dataset, and\nempirical results from real-world deployment, demonstrating the effectiveness\nof SlumpGuard as a practical solution for modern concrete quality assurance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10171v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2409.00062", "title": "HiFAKES: Synthetic High-Frequency NILM Data for NILM Models Diagnostics and Generalization Testing", "authors": ["Ilia Kamyshev", "Sahar Moghimian", "Henni Ouerdane"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.00062v2", "summary": "Monitoring electricity consumption at the appliance level is crucial for\nincreasing energy efficiency in residential and commercial buildings. Using a\nsingle meter, the non-intrusive load monitoring (NILM) breaks down household\nconsumption down to appliance-level, providing comprehensive insights into\nend-user electricity behavior. NILM models are trained on a household's total\npower consumption paired with submetered appliance labels. When sampled at high\nfrequencies ($\\geq$ 1 kHz), these datasets capture the full waveform\ncharacteristics, significantly improving disaggregation accuracy and model\ngeneralization. Nevertheless, such datasets are scarce, collected from a\nlimited number of households, and rarely include labels for power estimation,\nwhich complicates their use for model training, evaluation, or debugging. We\npropose HiFAKES, a pre-trained synthetic data generator that can instantly\ngenerate unlimited amounts of fully labeled high-frequency NILM data, including\naggregated and submetered current signatures. The data is ready-to-use and\nannotated for load identification (classification) and power estimation\n(regression). It allows simulating seen and completely unseen scenarios of\nappliances' behavior with full control over the number of appliance classes,\noperational modes, class similarity, brand diversity, and the number of\nconcurrently running devices. We propose a structured methodology to test the\ngeneralization of NILM models on simulated unseen households. The reliability\nof the HiFAKES synthetic data is assessed using a domain-agnostic 3-dimensional\nmetric. The generated signatures achieve high realism (93\\% authenticity),\nclosely resemble real-world data (84\\% fidelity), and include a reasonable\nportion of unseen signatures (5\\%).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.00062v2", "cate": "eess.SP", "date": "2024-08-22", "updated": "2025-07-12"}
{"id": "2312.05357", "title": "Unmixing Optical Signals from Undersampled Volumetric Measurements by Filtering the Pixel Latent Variables", "authors": ["Catherine Bouchard", "Andréanne Deschênes", "Vincent Boulanger", "Jean-Michel Bellavance", "Julia Chabbert", "Alexy Pelletier-Rioux", "Flavie Lavoie-Cardinal", "Christian Gagné"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      42 pages, 9 figures (main paper) + 22 pages, 15 figures (supplementary material)", "url": "http://arxiv.org/abs/2312.05357v4", "summary": "The development of signal unmixing algorithms is essential for leveraging\nmultimodal datasets acquired through a wide array of scientific imaging\ntechnologies, including hyperspectral or time-resolved acquisitions. In\nexperimental physics, enhancing the spatio-temporal resolution or expanding the\nnumber of detection channels often leads to diminished sampling rate and\nsignal-to-noise ratio, significantly affecting the efficacy of signal unmixing\nalgorithms. We propose Latent Unmixing, a new approach which applies bandpass\nfilters to the latent space of a multidimensional convolutional neural network\nto disentangle overlapping signal components. It enables better isolation and\nquantification of individual signal contributions, especially in the context of\nundersampled distributions. Using multidimensional convolution kernels to\nprocess all dimensions simultaneously enhances the network's ability to extract\ninformation from adjacent pixels, and time or spectral bins. This approach\nenables more effective separation of components in cases where individual\npixels do not provide clear, well-resolved information. We showcase the\nmethod's practical use in experimental physics through two test cases that\nhighlight the versatility of our approach: fluorescence lifetime microscopy and\nmode decomposition in optical fibers. The latent unmixing method extracts\nvaluable information from complex signals that cannot be resolved by standard\nmethods. It opens up new possibilities in optics and photonics for multichannel\nseparation at an increased sampling rate.", "comment": "42 pages, 9 figures (main paper) + 22 pages, 15 figures\n  (supplementary material)", "pdf_url": "http://arxiv.org/pdf/2312.05357v4", "cate": "eess.IV", "date": "2023-12-08", "updated": "2025-07-11"}
{"id": "2507.09973", "title": "Tiny Reward Models", "authors": ["Sarah Pan"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      2025 ICML Efficient Systems for Foundation Models Workshop", "url": "http://arxiv.org/abs/2507.09973v1", "summary": "Large decoder-based language models have become the dominant architecture for\nreward modeling in reinforcement learning from human feedback (RLHF). However,\nas reward models are increasingly deployed in test-time strategies, their\ninference costs become a growing concern. We present TinyRM, a family of small,\nbidirectional masked language models (MLMs) with as few as 400 million\nparameters, that rival the capabilities of models over 175 times larger on\nreasoning and safety preference modeling tasks. TinyRM combines FLAN-style\nprompting, Directional Low-Rank Adaptation (DoRA), and layer freezing to\nachieve strong performance on RewardBench, despite using significantly fewer\nresources. Our experiments suggest that small models benefit from\ndomain-specific tuning strategies, particularly in reasoning, where lightweight\nfinetuning methods are especially effective. While challenges remain in\nbuilding generalist models and conversational preference modeling, our\npreliminary results highlight the promise of lightweight bidirectional\narchitectures as efficient, scalable alternatives for preference modeling.", "comment": "2025 ICML Efficient Systems for Foundation Models Workshop", "pdf_url": "http://arxiv.org/pdf/2507.09973v1", "cate": "cs.CL", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10195", "title": "Minimizing the Pretraining Gap: Domain-aligned Text-Based Person Retrieval", "authors": ["Shuyu Yang", "Yaxiong Wang", "Yongrui Li", "Li Zhu", "Zhedong Zheng"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10195v1", "summary": "In this work, we focus on text-based person retrieval, which aims to identify\nindividuals based on textual descriptions. Given the significant privacy issues\nand the high cost associated with manual annotation, synthetic data has become\na popular choice for pretraining models, leading to notable advancements.\nHowever, the considerable domain gap between synthetic pretraining datasets and\nreal-world target datasets, characterized by differences in lighting, color,\nand viewpoint, remains a critical obstacle that hinders the effectiveness of\nthe pretrain-finetune paradigm. To bridge this gap, we introduce a unified\ntext-based person retrieval pipeline considering domain adaptation at both\nimage and region levels. In particular, it contains two primary components,\ni.e., Domain-aware Diffusion (DaD) for image-level adaptation and\nMulti-granularity Relation Alignment (MRA) for region-level adaptation. As the\nname implies, Domain-aware Diffusion is to migrate the distribution of images\nfrom the pretraining dataset domain to the target real-world dataset domain,\ne.g., CUHK-PEDES. Subsequently, MRA performs a meticulous region-level\nalignment by establishing correspondences between visual regions and their\ndescriptive sentences, thereby addressing disparities at a finer granularity.\nExtensive experiments show that our dual-level adaptation method has achieved\nstate-of-the-art results on the CUHK-PEDES, ICFG-PEDES, and RSTPReid datasets,\noutperforming existing methodologies. The dataset, model, and code are\navailable at https://github.com/Shuyu-XJTU/MRA.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10195v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2502.01077", "title": "A Framework for Fractional Matrix Programming Problems with Applications in FBL MU-MIMO", "authors": ["Mohammad Soleymani", "Eduard Jorswieck", "Robert Schober", "Lajos Hanzo"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      Accepted at IEEE Transactions on Wireless Communications", "url": "http://arxiv.org/abs/2502.01077v2", "summary": "An efficient framework is conceived for fractional matrix programming (FMP)\noptimization problems (OPs) namely for minimization and maximization. In each\ngeneric OP, either the objective or the constraints are functions of multiple\narbitrary continuous-domain fractional functions (FFs). This ensures the\nframework's versatility, enabling it to solve a broader range of OPs than\nclassical FMP solvers, like Dinkelbach-based algorithms. Specifically, the\ngeneralized Dinkelbach algorithm can only solve multiple-ratio FMP problems. By\ncontrast, our framework solves OPs associated with a sum or product of multiple\nFFs as the objective or constraint functions. Additionally, our framework\nprovides a single-loop solution, while most FMP solvers require twin-loop\nalgorithms.\n  Many popular performance metrics of wireless communications are FFs. For\ninstance, latency has a fractional structure, and minimizing the sum delay\nleads to an FMP problem. Moreover, the mean square error (MSE) and energy\nefficiency (EE) metrics have fractional structures. Thus, optimizing EE-related\nmetrics such as the sum or geometric mean of EEs and enhancing the metrics\nrelated to spectral-versus-energy-efficiency tradeoff yield FMP problems.\nFurthermore, both the signal-to-interference-plus-noise ratio and the channel\ndispersion are FFs. In this paper, we also develop resource allocation schemes\nfor multi-user multiple-input multiple-output (MU-MIMO) systems, using finite\nblock length (FBL) coding, demonstrating attractive practical applications of\nFMP by optimizing the aforementioned metrics.", "comment": "Accepted at IEEE Transactions on Wireless Communications", "pdf_url": "http://arxiv.org/pdf/2502.01077v2", "cate": "eess.SP", "date": "2025-02-03", "updated": "2025-07-14"}
{"id": "2404.13693", "title": "Advancing Automatic Photovoltaic Defect Detection using Semi-Supervised Semantic Segmentation of Electroluminescence Images", "authors": ["Abhishek Jha", "Yogesh Rawat", "Shruti Vyas"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      19 pages, 10 figures", "url": "http://arxiv.org/abs/2404.13693v4", "summary": "Photovoltaic (PV) systems allow us to tap into all abundant solar energy,\nhowever they require regular maintenance for high efficiency and to prevent\ndegradation. Traditional manual health check, using Electroluminescence (EL)\nimaging, is expensive and logistically challenging which makes automated defect\ndetection essential. Current automation approaches require extensive manual\nexpert labeling, which is time-consuming, expensive, and prone to errors. We\npropose PV-S3 (Photovoltaic-Semi-supervised Semantic Segmentation), a\nSemi-Supervised Learning approach for semantic segmentation of defects in EL\nimages that reduces reliance on extensive labeling. PV-S3 is an artificial\nintelligence (AI) model trained using a few labeled images along with numerous\nunlabeled images. We introduce a novel Semi Cross-Entropy loss function to deal\nwith class imbalance. We evaluate PV-S3 on multiple datasets and demonstrate\nits effectiveness and adaptability. With merely 20% labeled samples, we achieve\nan absolute improvement of 9.7% in mean Intersection-over-Union (mIoU), 13.5%\nin Precision, 29.15% in Recall, and 20.42% in F1-Score over prior\nstate-of-the-art supervised method (which uses 100% labeled samples) on\nUniversity of Central Florida-Electroluminescence (UCF-EL) dataset (largest\ndataset available for semantic segmentation of EL images) showing improvement\nin performance while reducing the annotation costs by 80%. For more details,\nvisit our GitHub repository: https://github.com/abj247/PV-S3.", "comment": "19 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2404.13693v4", "cate": "eess.IV", "date": "2024-04-21", "updated": "2025-07-14"}
{"id": "2507.10073", "title": "Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires", "authors": ["Simon Münker"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      15pages, 1 figure, 2 tables", "url": "http://arxiv.org/abs/2507.10073v1", "summary": "Are AI systems truly representing human values, or merely averaging across\nthem? Our study suggests a concerning reality: Large Language Models (LLMs)\nfail to represent diverse cultural moral frameworks despite their linguistic\ncapabilities. We expose significant gaps between AI-generated and human moral\nintuitions by applying the Moral Foundations Questionnaire across 19 cultural\ncontexts. Comparing multiple state-of-the-art LLMs' origins against human\nbaseline data, we find these models systematically homogenize moral diversity.\nSurprisingly, increased model size doesn't consistently improve cultural\nrepresentation fidelity. Our findings challenge the growing use of LLMs as\nsynthetic populations in social science research and highlight a fundamental\nlimitation in current AI alignment approaches. Without data-driven alignment\nbeyond prompting, these systems cannot capture the nuanced, culturally-specific\nmoral intuitions. Our results call for more grounded alignment objectives and\nevaluation metrics to ensure AI systems represent diverse human values rather\nthan flattening the moral landscape.", "comment": "15pages, 1 figure, 2 tables", "pdf_url": "http://arxiv.org/pdf/2507.10073v1", "cate": "cs.CL", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10202", "title": "A Training-Free, Task-Agnostic Framework for Enhancing MLLM Performance on High-Resolution Images", "authors": ["Jaeseong Lee", "Yeeun Choi", "Heechan Choi", "Hanjung Kim", "Seonjoo Kim"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at CVPR 2025 Workshop on Emergent Visual Abilities and Limits of Foundation Models", "url": "http://arxiv.org/abs/2507.10202v1", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in vision-language understanding, reasoning, and generation.\nHowever, they struggle with tasks requiring fine-grained localization and\nreasoning in high-resolution images. This constraint stems from the fact that\nMLLMs are fine-tuned with fixed image resolution to align with the pre-trained\nimage encoder used in MLLM. Consequently, feeding high-resolution images\ndirectly into MLLMs leads to poor generalization due to a train-test resolution\ndiscrepancy, while downsampling these images-although ensuring\nconsistency-compromises fine-grained visual details and ultimately degrades\nperformance. To address this challenge, we propose Extract Candidate then\nPredict (ECP), a novel training-free, task-agnostic two-stage framework\ndesigned to enhance MLLM performance on high-resolution images. The key\nintuition behind ECP is that while MLLMs struggle with high-resolution images,\ntheir predictions on downsampled images still contain implicit localization\ncues. By first identifying candidate region using the coarse prediction and\nthen predicting the final output based on candidate region, ECP effectively\npreserves fine-grained details while mitigating the challenges posed by\nhigh-resolution data. We validate our framework on 4K GUI grounding and 4K, 8K\nMLLM perception, achieving +21.3%, +5.8%, +5.2% absolute improvement compared\nto baseline respectively, demonstrating its effectiveness. Code is available at\nhttps://github.com/yenncye/ECP.", "comment": "Accepted at CVPR 2025 Workshop on Emergent Visual Abilities and\n  Limits of Foundation Models", "pdf_url": "http://arxiv.org/pdf/2507.10202v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2502.05952", "title": "Comprehensive Review of Deep Unfolding Techniques for Next-Generation Wireless Communication Systems", "authors": ["Sukanya Deka", "Kuntal Deka", "Nhan Thanh Nguyen", "Sanjeev Sharma", "Vimal Bhatia", "Nandana Rajatheva"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.05952v3", "summary": "The application of machine learning in wireless communications has been\nextensively explored, with deep unfolding emerging as a powerful model-based\ntechnique. Deep unfolding enhances interpretability by transforming complex\niterative algorithms into structured layers of deep neural networks (DNNs).\nThis approach seamlessly integrates domain knowledge with deep learning (DL),\nleveraging the strengths of both methods to simplify complex signal processing\ntasks in communication systems. To provide a solid foundation, we first present\na brief overview of DL and deep unfolding. We then explore the applications of\ndeep unfolding in key areas, including signal detection, channel estimation,\nbeamforming design, decoding for error-correcting codes, sensing and\ncommunication, power allocation, and security. Each section focuses on a\nspecific task, highlighting its significance in emerging 6G technologies and\nreviewing recent advancements in deep unfolding-based solutions. Finally, we\ndiscuss the challenges associated with developing deep unfolding techniques and\npropose potential improvements to enhance their applicability across diverse\nwireless communication scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.05952v3", "cate": "eess.SP", "date": "2025-02-09", "updated": "2025-07-12"}
{"id": "2406.17709", "title": "MGA-Net: A Novel Mask-Guided Attention Neural Network for Precision Neonatal Brain Imaging", "authors": ["Bahram Jafrasteh", "Simon Pedro Lubian-Lopez", "Emiliano Trimarco", "Macarena Roman Ruiz", "Carmen Rodriguez Barrios", "Yolanda Marin Almagro", "Isabel Benavente-Fernandez"], "categories": ["eess.IV", "cs.CV", "stat.CO"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2406.17709v3", "summary": "In this study, we introduce MGA-Net, a novel mask-guided attention neural\nnetwork, which extends the U-net model for precision neonatal brain imaging.\nMGA-Net is designed to extract the brain from other structures and reconstruct\nhigh-quality brain images. The network employs a common encoder and two\ndecoders: one for brain mask extraction and the other for brain region\nreconstruction. A key feature of MGA-Net is its high-level mask-guided\nattention module, which leverages features from the brain mask decoder to\nenhance image reconstruction. To enable the same encoder and decoder to process\nboth MRI and ultrasound (US) images, MGA-Net integrates sinusoidal positional\nencoding. This encoding assigns distinct positional values to MRI and US\nimages, allowing the model to effectively learn from both modalities.\nConsequently, features learned from a single modality can aid in learning a\nmodality with less available data, such as US. We extensively validated the\nproposed MGA-Net on diverse and independent datasets from varied clinical\nsettings and neonatal age groups. The metrics used for assessment included the\nDICE similarity coefficient, recall, and accuracy for image segmentation;\nstructural similarity for image reconstruction; and root mean squared error for\ntotal brain volume estimation from 3D ultrasound images. Our results\ndemonstrate that MGA-Net significantly outperforms traditional methods,\noffering superior performance in brain extraction and segmentation while\nachieving high precision in image reconstruction and volumetric analysis. Thus,\nMGA-Net represents a robust and effective preprocessing tool for MRI and 3D\nultrasound images, marking a significant advance in neuroimaging that enhances\nboth research and clinical diagnostics in the neonatal period and beyond.Our\ncode is available at https://github.com/BahramJafrasteh/MGA-Net", "comment": null, "pdf_url": "http://arxiv.org/pdf/2406.17709v3", "cate": "eess.IV", "date": "2024-06-25", "updated": "2025-07-14"}
{"id": "2507.10085", "title": "Enhancing Chain-of-Thought Reasoning with Critical Representation Fine-tuning", "authors": ["Chenxi Huang", "Shaotian Yan", "Liang Xie", "Binbin Lin", "Sinan Fan", "Yue Xin", "Deng Cai", "Chen Shen", "Jieping Ye"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted by ACL 2025", "url": "http://arxiv.org/abs/2507.10085v1", "summary": "Representation Fine-tuning (ReFT), a recently proposed Parameter-Efficient\nFine-Tuning (PEFT) method, has attracted widespread attention for significantly\nimproving parameter efficiency by editing representation space alone. In this\nwork, we investigate applying ReFT to complex reasoning tasks. However,\ndirectly using the native ReFT method, which modifies fixed representations at\nthe beginning and end of each layer, yields suboptimal performance, as these\nfixed-position representations have uncertain impact on the outputs. We observe\nthat, in complex reasoning tasks, there often exist certain critical\nrepresentations. These representations either integrate significant information\nfrom preceding layers or regulate subsequent layer representations. Through\nlayer-by-layer propagation, they exert a substantial influence on the final\noutput. Naturally, fine-tuning these critical representations has the potential\nto greatly enhance reasoning performance. Building upon these insights, we\npropose Critical Representation Fine-Tuning (CRFT), a novel method that\nidentifies and optimizes these critical representations through information\nflow analysis. CRFT operates within a supervised learning framework,\ndynamically optimizing critical representations in a low-rank linear subspace\nwhile freezing the base model. The effectiveness and efficiency of our method\nare validated across eight benchmarks for arithmetic and commonsense reasoning,\nusing LLaMA and Mistral model families. Furthermore, our method also adapts\neffectively to few-shot settings, boosting one-shot accuracy by 16.4%. Our work\nhighlights the untapped potential of representation-level optimization for CoT\nreasoning, offering a lightweight yet powerful alternative to traditional PEFT\nmethods.", "comment": "Accepted by ACL 2025", "pdf_url": "http://arxiv.org/pdf/2507.10085v1", "cate": "cs.CL", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10203", "title": "Improving Multimodal Learning via Imbalanced Learning", "authors": ["Shicai Wei", "Chunbo Luo", "Yang Luo"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV2025", "url": "http://arxiv.org/abs/2507.10203v1", "summary": "Multimodal learning often encounters the under-optimized problem and may\nperform worse than unimodal learning. Existing approaches attribute this issue\nto imbalanced learning across modalities and tend to address it through\ngradient balancing. However, this paper argues that balanced learning is not\nthe optimal setting for multimodal learning. With bias-variance analysis, we\nprove that imbalanced dependency on each modality obeying the inverse ratio of\ntheir variances contributes to optimal performance. To this end, we propose the\nAsymmetric Representation Learning(ARL) strategy to assist multimodal learning\nvia imbalanced optimization. ARL introduces auxiliary regularizers for each\nmodality encoder to calculate their prediction variance. ARL then calculates\ncoefficients via the unimodal variance to re-weight the optimization of each\nmodality, forcing the modality dependence ratio to be inversely proportional to\nthe modality variance ratio. Moreover, to minimize the generalization error,\nARL further introduces the prediction bias of each modality and jointly\noptimizes them with multimodal loss. Notably, all auxiliary regularizers share\nparameters with the multimodal model and rely only on the modality\nrepresentation. Thus the proposed ARL strategy introduces no extra parameters\nand is independent of the structures and fusion methods of the multimodal\nmodel. Finally, extensive experiments on various datasets validate the\neffectiveness and versatility of ARL. Code is available at\n\\href{https://github.com/shicaiwei123/ICCV2025-ARL}{https://github.com/shicaiwei123/ICCV2025-ARL}", "comment": "Accepted to ICCV2025", "pdf_url": "http://arxiv.org/pdf/2507.10203v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2503.00285", "title": "Advances in Anti-Deception Jamming Strategies for Radar Systems: A Survey", "authors": ["Helena Calatrava", "Shuo Tang", "Pau Closas"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      22 pages, 10 figures, 6 tables", "url": "http://arxiv.org/abs/2503.00285v2", "summary": "Deception jamming has long been a significant threat to radar systems,\ninterfering with search, acquisition, and tracking by introducing false\ninformation that diverts attention from the targets of interest. As deception\nstrategies become more sophisticated, the vulnerability of radar systems to\nthese attacks continues to escalate. This paper offers a comprehensive review\nof the evolution of anti-deception jamming techniques, starting with legacy\nsolutions and progressing to the latest advancements. Current research is\ncategorized into three key areas: prevention strategies, which hinder the\nability of jammers to alter radar processing; detection strategies, which alert\nthe system to deception and may classify the type of attack; and mitigation\nstrategies, which aim to reduce or suppress the impact of jamming.\nAdditionally, key avenues for further research are highlighted, with a\nparticular emphasis on distributed, cognitive, and AI-enabled radar systems. We\nenvision this paper as a gateway to the existing literature on anti-deception\njamming, a critical area for safeguarding radar systems against evolving\nthreats.", "comment": "22 pages, 10 figures, 6 tables", "pdf_url": "http://arxiv.org/pdf/2503.00285v2", "cate": "eess.SP", "date": "2025-03-01", "updated": "2025-07-13"}
{"id": "2501.11854", "title": "WaveNet-SF: A Hybrid Network for Retinal Disease Detection Based on Wavelet Transform in the Spatial-Frequency Domain", "authors": ["Jilan Cheng", "Guoli Long", "Zeyu Zhang", "Zhenjia Qi", "Hanyu Wang", "Libin Lu", "Shuihua Wang", "Yudong Zhang", "Jin Hong"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.11854v2", "summary": "Retinal diseases are a leading cause of vision impairment and blindness, with\ntimely diagnosis being critical for effective treatment. Optical Coherence\nTomography (OCT) has become a standard imaging modality for retinal disease\ndiagnosis, but OCT images often suffer from issues such as speckle noise,\ncomplex lesion shapes, and varying lesion sizes, making interpretation\nchallenging. In this paper, we propose a novel framework, WaveNet-SF, to\nenhance retinal disease detection by integrating the spatial-domain and\nfrequency-domain learning. The framework utilizes wavelet transforms to\ndecompose OCT images into low- and high-frequency components, enabling the\nmodel to extract both global structural features and fine-grained details. To\nimprove lesion detection, we introduce a Multi-Scale Wavelet Spatial Attention\n(MSW-SA) module, which enhances the model's focus on regions of interest at\nmultiple scales. Additionally, a High-Frequency Feature Compensation (HFFC)\nblock is incorporated to recover edge information lost during wavelet\ndecomposition, suppress noise, and preserve fine details crucial for lesion\ndetection. Our approach achieves state-of-the-art (SOTA) classification\naccuracies of 97.82% and 99.58% on the OCT-C8 and OCT2017 datasets,\nrespectively, surpassing existing methods. These results demonstrate the\nefficacy of WaveNet-SF in addressing the challenges of OCT image analysis and\nits potential as a powerful tool for retinal disease diagnosis.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.11854v2", "cate": "eess.IV", "date": "2025-01-21", "updated": "2025-07-12"}
{"id": "2507.10120", "title": "A Variance-Reduced Cubic-Regularized Newton for Policy Optimization", "authors": ["Cheng Sun", "Zhen Zhang", "Shaofu Yang"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      13 pages, 1 figure", "url": "http://arxiv.org/abs/2507.10120v1", "summary": "In this paper, we study a second-order approach to policy optimization in\nreinforcement learning. Existing second-order methods often suffer from\nsuboptimal sample complexity or rely on unrealistic assumptions about\nimportance sampling. To overcome these limitations, we propose VR-CR-PN, a\nvariance-reduced cubic-regularized policy Newton algorithm. To the best of our\nknowledge, this is the first algorithm that integrates Hessian-aided variance\nreduction with second-order policy optimization, effectively addressing the\ndistribution shift problem and achieving best-known sample complexity under\ngeneral nonconvex conditions but without the need for importance sampling. We\ntheoretically establish that VR-CR-PN achieves a sample complexity of\n$\\tilde{\\mathcal{O}}(\\epsilon^{-3})$ to reach an $\\epsilon$-second-order\nstationary point, significantly improving upon the previous best result of\n$\\tilde{\\mathcal{O}}(\\epsilon^{-3.5})$ under comparable assumptions. As an\nadditional contribution, we introduce a novel Hessian estimator for the\nexpected return function, which admits a uniform upper bound independent of the\nhorizon length $H$, allowing the algorithm to achieve horizon-independent\nsample complexity.", "comment": "13 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2507.10120v1", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10209", "title": "Is Micro-expression Ethnic Leaning?", "authors": ["Huai-Qian Khor", "Yante Li", "Xingxun Jiang", "Guoying Zhao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10209v1", "summary": "How much does ethnicity play its part in emotional expression? Emotional\nexpression and micro-expression research probe into understanding human\npsychological responses to emotional stimuli, thereby revealing substantial\nhidden yet authentic emotions that can be useful in the event of diagnosis and\ninterviews. While increased attention had been provided to micro-expression\nanalysis, the studies were done under Ekman's assumption of emotion\nuniversality, where emotional expressions are identical across cultures and\nsocial contexts. Our computational study uncovers some of the influences of\nethnic background in expression analysis, leading to an argument that the\nemotional universality hypothesis is an overgeneralization from the perspective\nof manual psychological analysis. In this research, we propose to investigate\nthe level of influence of ethnicity in a simulated micro-expression scenario.\nWe construct a cross-cultural micro-expression database and algorithmically\nannotate the ethnic labels to facilitate the investigation. With the ethnically\nannotated dataset, we perform a prima facie study to compare mono-ethnicity and\nstereo-ethnicity in a controlled environment, which uncovers a certain\ninfluence of ethnic bias via an experimental way. Building on this finding, we\npropose a framework that integrates ethnic context into the emotional feature\nlearning process, yielding an ethnically aware framework that recognises\nethnicity differences in micro-expression recognition. For improved\nunderstanding, qualitative analyses have been done to solidify the preliminary\ninvestigation into this new realm of research. Code is publicly available at\nhttps://github.com/IcedDoggie/ICMEW2025_EthnicMER", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10209v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2503.12962", "title": "Goal-Oriented Remote Tracking Through Correlated Observations in Pull-based Communications", "authors": ["Abolfazl Zakeri", "Mohammad Moltafet", "Marian Codreanu"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      This is a full version of an IEEE COML paper (under revision)", "url": "http://arxiv.org/abs/2503.12962v2", "summary": "We address the real-time remote tracking problem in a status update system\ncomprising two sensors, two independent information sources, and a remote\nmonitor. The status updating follows a pull-based communication, where the\nmonitor commands/pulls the sensors for status updates, i.e., the actual state\nof the sources. We consider that the observations are correlated, meaning that\neach sensor sent data could also include the state of the other source due to,\ne.g., inter-sensor communication or proximity-based monitoring. The\neffectiveness of data communication is measured by a generic distortion,\ncapturing the underlying application goal. We provide optimal command/pulling\npolicies for the monitor that minimize the average weighted sum distortion and\ntransmission cost. Since the monitor cannot fully observe the exact state of\neach source, we propose a partially observable Markov decision process (POMDP)\nand reformulate it as a belief MDP problem. We then effectively truncate the\ninfinite belief space and transform it into a finite-state MDP problem, which\nis solved via relative value iteration. Simulation results show the\neffectiveness of the derived policy over age-based and deep-Q network baseline\npolicies.", "comment": "This is a full version of an IEEE COML paper (under revision)", "pdf_url": "http://arxiv.org/pdf/2503.12962v2", "cate": "eess.SP", "date": "2025-03-17", "updated": "2025-07-14"}
{"id": "2501.14171", "title": "Guided Neural Schrödinger bridge for Brain MR image synthesis with Limited Data", "authors": ["Hanyeol Yang", "Sunggyu Kim", "Mi Kyung Kim", "Yongseon Yoo", "Yu-Mi Kim", "Min-Ho Shin", "Insung Chung", "Sang Baek Koh", "Hyeon Chang Kim", "Jong-Min Lee"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Single column, 28 pages, 7 figures", "url": "http://arxiv.org/abs/2501.14171v2", "summary": "Multi-modal brain MRI provides essential complementary information for\nclinical diagnosis. However, acquiring all modalities in practice is often\nconstrained by time and cost. To address this, various methods have been\nproposed to generate missing modalities from available ones. Traditional\napproaches can be broadly categorized into two main types: paired and unpaired\nmethods. While paired methods for synthesizing missing modalities achieve high\naccuracy, obtaining large-scale paired datasets is typically impractical. In\ncontrast, unpaired methods, though scalable, often fail to preserve critical\nanatomical features, such as lesions. In this paper, we propose Fully Guided\nSchr\\\"odinger Bridge (FGSB), a novel framework designed to overcome these\nlimitations by enabling high-fidelity generation with extremely limited paired\ndata. Furthermore, when provided with lesion-specific information such as\nexpert annotations, segmentation tools, or simple intensity thresholds for\ncritical regions, FGSB can generate missing modalities while preserving these\nsignificant lesion with reduced data requirements. Our model comprises two\nstages: 1) Generation Phase: Iteratively refines synthetic images using paired\ntarget image and Gaussian noise. Training Phase: Learns optimal transformation\npathways from source to target modality by mapping all intermediate states,\nensuring consistent and high-fidelity synthesis. Experimental results across\nmultiple datasets demonstrate that FGSB achieved performance comparable to\nlarge-data-trained models, while using only two subjects. Incorporating\nlesion-specific priors further improves the preservation of clinical features.", "comment": "Single column, 28 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2501.14171v2", "cate": "eess.IV", "date": "2025-01-24", "updated": "2025-07-14"}
{"id": "2507.10132", "title": "Wavelet-Enhanced Neural ODE and Graph Attention for Interpretable Energy Forecasting", "authors": ["Usman Gani Joy"], "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10132v1", "summary": "Accurate forecasting of energy demand and supply is critical for optimizing\nsustainable energy systems, yet it is challenged by the variability of\nrenewable sources and dynamic consumption patterns. This paper introduces a\nneural framework that integrates continuous-time Neural Ordinary Differential\nEquations (Neural ODEs), graph attention, multi-resolution wavelet\ntransformations, and adaptive learning of frequencies to address the issues of\ntime series prediction. The model employs a robust ODE solver, using the\nRunge-Kutta method, paired with graph-based attention and residual connections\nto better understand both structural and temporal patterns. Through\nwavelet-based feature extraction and adaptive frequency modulation, it adeptly\ncaptures and models diverse, multi-scale temporal dynamics. When evaluated\nacross seven diverse datasets: ETTh1, ETTh2, ETTm1, ETTm2 (electricity\ntransformer temperature), and Waste, Solar, and Hydro (renewable energy), this\narchitecture consistently outperforms state-of-the-art baselines in various\nforecasting metrics, proving its robustness in capturing complex temporal\ndependencies. Furthermore, the model enhances interpretability through SHAP\nanalysis, making it suitable for sustainable energy applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10132v1", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10213", "title": "Boosting Multimodal Learning via Disentangled Gradient Learning", "authors": ["Shicai Wei", "Chunbo Luo", "Yang Luo"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV2025", "url": "http://arxiv.org/abs/2507.10213v1", "summary": "Multimodal learning often encounters the under-optimized problem and may have\nworse performance than unimodal learning. Existing methods attribute this\nproblem to the imbalanced learning between modalities and rebalance them\nthrough gradient modulation. However, they fail to explain why the dominant\nmodality in multimodal models also underperforms that in unimodal learning. In\nthis work, we reveal the optimization conflict between the modality encoder and\nmodality fusion module in multimodal models. Specifically, we prove that the\ncross-modal fusion in multimodal models decreases the gradient passed back to\neach modality encoder compared with unimodal models. Consequently, the\nperformance of each modality in the multimodal model is inferior to that in the\nunimodal model. To this end, we propose a disentangled gradient learning (DGL)\nframework to decouple the optimization of the modality encoder and modality\nfusion module in the multimodal model. DGL truncates the gradient\nback-propagated from the multimodal loss to the modality encoder and replaces\nit with the gradient from unimodal loss. Besides, DGL removes the gradient\nback-propagated from the unimodal loss to the modality fusion module. This\nhelps eliminate the gradient interference between the modality encoder and\nmodality fusion module while ensuring their respective optimization processes.\nFinally, extensive experiments on multiple types of modalities, tasks, and\nframeworks with dense cross-modal interaction demonstrate the effectiveness and\nversatility of the proposed DGL. Code is available at\n\\href{https://github.com/shicaiwei123/ICCV2025-GDL}{https://github.com/shicaiwei123/ICCV2025-GDL}", "comment": "Accepted to ICCV2025", "pdf_url": "http://arxiv.org/pdf/2507.10213v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2503.23883", "title": "Algorithm Design and Prototype Validation for Reconfigurable Intelligent Sensing Surface: Forward-Only Transmission", "authors": ["Cheng Luo", "Luping Xiang", "Jie Hu", "Kun Yang"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.23883v4", "summary": "Sensing-assisted communication schemes have recently garnered significant\nresearch attention. In this work, we design a dual-function reconfigurable\nintelligent surface (RIS), integrating both active and passive elements,\nreferred to as the reconfigurable intelligent sensing surface (RISS), to\nenhance communication. By leveraging sensing results from the active elements,\nwe propose communication enhancement and robust interference suppression\nschemes for both near-field and far-field models, implemented through the\npassive elements. These schemes remove the need for base station (BS) feedback\nfor RISS control, simplifying the communication process by replacing\ntraditional channel state information (CSI) feedback with real-time sensing\nfrom the active elements. The proposed schemes are theoretically analyzed and\nthen validated using software-defined radio (SDR). Experimental results\ndemonstrate the effectiveness of the sensing algorithms in real-world\nscenarios, such as direction of arrival (DOA) estimation and radio frequency\n(RF) identification recognition. Moreover, the RISS-assisted communication\nsystem shows strong performance in communication enhancement and interference\nsuppression, particularly in near-field models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.23883v4", "cate": "eess.SP", "date": "2025-03-31", "updated": "2025-07-13"}
{"id": "2503.01248", "title": "Comprehensive Evaluation of OCT-based Automated Segmentation of Retinal Layer, Fluid and Hyper-Reflective Foci: Impact on Clinical Assessment of Diabetic Retinopathy Severity", "authors": ["S. Chen", "D. Ma", "M. Raviselvan", "S. Sundaramoorthy", "K. Popuri", "M. J. Ju", "M. V. Sarunic", "D. Ratra", "M. F. Beg"], "categories": ["eess.IV", "cs.CV", "cs.LG", "q-bio.TO"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      18 pages, 11 figures", "url": "http://arxiv.org/abs/2503.01248v4", "summary": "Diabetic retinopathy (DR) is a leading cause of vision loss, requiring early\nand accurate assessment to prevent irreversible damage. Spectral Domain Optical\nCoherence Tomography (SD-OCT) enables high-resolution retinal imaging, but\nautomated segmentation performance varies, especially in cases with complex\nfluid and hyperreflective foci (HRF) patterns. This study proposes an\nactive-learning-based deep learning pipeline for automated segmentation of\nretinal layers, fluid, and HRF, using four state-of-the-art models: U-Net,\nSegFormer, SwinUNETR, and VM-UNet, trained on expert-annotated SD-OCT volumes.\nSegmentation accuracy was evaluated with five-fold cross-validation, and\nretinal thickness was quantified using a K-nearest neighbors algorithm and\nvisualized with Early Treatment Diabetic Retinopathy Study (ETDRS) maps.\nSwinUNETR achieved the highest overall accuracy (DSC = 0.7719; NSD = 0.8149),\nwhile VM-UNet excelled in specific layers. Structural differences were observed\nbetween non-proliferative and proliferative DR, with layer-specific thickening\ncorrelating with visual acuity impairment. The proposed framework enables\nrobust, clinically relevant DR assessment while reducing the need for manual\nannotation, supporting improved disease monitoring and treatment planning.", "comment": "18 pages, 11 figures", "pdf_url": "http://arxiv.org/pdf/2503.01248v4", "cate": "eess.IV", "date": "2025-03-03", "updated": "2025-07-13"}
{"id": "2507.10133", "title": "Extending Defeasibility for Propositional Standpoint Logics", "authors": ["Nicholas Leisegang", "Thomas Meyer", "Ivan Varzinczak"], "categories": ["cs.LO", "cs.AI"], "primary_category": "Subjects:       Logic in Computer Science (cs.LO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10133v1", "summary": "In this paper, we introduce a new defeasible version of propositional\nstandpoint logic by integrating Kraus et al.'s defeasible conditionals, Britz\nand Varzinczak's notions of defeasible necessity and distinct possibility,\nalong with Leisegang et al.'s approach to defeasibility into the standpoint\nlogics of G\\'omez \\'Alvarez and Rudolph. The resulting logical framework allows\nfor the expression of defeasibility on the level of implications, standpoint\nmodal operators, and standpoint-sharpening statements. We provide a\npreferential semantics for this extended language and propose a tableaux\ncalculus, which is shown to be sound and complete with respect to preferential\nentailment. We also establish the computational complexity of the tableaux\nprocedure to be in PSpace.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10133v1", "cate": "cs.LO", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10217", "title": "From Wardrobe to Canvas: Wardrobe Polyptych LoRA for Part-level Controllable Human Image Generation", "authors": ["Jeongho Kim", "Sunghyun Park", "Hyoungwoo Park", "Sungrack Yun", "Jaegul Choo", "Seokeon Cho"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 pages, 8 figures", "url": "http://arxiv.org/abs/2507.10217v1", "summary": "Recent diffusion models achieve personalization by learning specific\nsubjects, allowing learned attributes to be integrated into generated images.\nHowever, personalized human image generation remains challenging due to the\nneed for precise and consistent attribute preservation (e.g., identity,\nclothing details). Existing subject-driven image generation methods often\nrequire either (1) inference-time fine-tuning with few images for each new\nsubject or (2) large-scale dataset training for generalization. Both approaches\nare computationally expensive and impractical for real-time applications. To\naddress these limitations, we present Wardrobe Polyptych LoRA, a novel\npart-level controllable model for personalized human image generation. By\ntraining only LoRA layers, our method removes the computational burden at\ninference while ensuring high-fidelity synthesis of unseen subjects. Our key\nidea is to condition the generation on the subject's wardrobe and leverage\nspatial references to reduce information loss, thereby improving fidelity and\nconsistency. Additionally, we introduce a selective subject region loss, which\nencourages the model to disregard some of reference images during training. Our\nloss ensures that generated images better align with text prompts while\nmaintaining subject integrity. Notably, our Wardrobe Polyptych LoRA requires no\nadditional parameters at the inference stage and performs generation using a\nsingle model trained on a few training samples. We construct a new dataset and\nbenchmark tailored for personalized human image generation. Extensive\nexperiments show that our approach significantly outperforms existing\ntechniques in fidelity and consistency, enabling realistic and\nidentity-preserving full-body synthesis.", "comment": "10 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.10217v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2505.01625", "title": "Embracing Diffraction: A Paradigm Shift in Wireless Sensing and Communication", "authors": ["Anurag Pallaprolu", "Winston Hurst", "Yasamin Mostofi"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.01625v2", "summary": "Wireless signals are integral to modern society, enabling both communication\nand increasingly, environmental sensing. While various propagation models\nexist, ranging from empirical methods to full-wave simulations, the phenomenon\nof electromagnetic diffraction is often treated as a secondary effect or a\ncorrection factor. This paper positions diffraction as a fundamentally\nimportant and underutilized mechanism that is rich with information about the\nphysical environment. Specifically, diffraction-inducing elements generate\ndistinct signatures that are rich with information about their underlying\nproperties such as their geometries. We then argue that by understanding and\nexploiting these relationships, diffraction can be harnessed strategically. We\nintroduce a general optimization framework to formalize this concept,\nillustrating how diffraction can be leveraged for both inverse problems\n(sensing scene details such as object geometries from measured fields) and\ndesign problems (shaping radio frequency (RF) fields for communication\nobjectives by configuring diffracting elements). Focusing primarily on edge\ndiffraction and Keller's Geometrical Theory of Diffraction (GTD), we discuss\nspecific applications in RF sensing for scene understanding and in\ncommunications for RF field programming, drawing upon recent work. Overall,\nthis paper lays out a vision for systematically incorporating diffraction into\nthe design and operation of future wireless systems, paving the way for\nenhanced sensing capabilities and more robust communication strategies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.01625v2", "cate": "eess.SP", "date": "2025-05-02", "updated": "2025-07-14"}
{"id": "2503.12642", "title": "COVID-19 Pneumonia Diagnosis Using Medical Images: Deep Learning-Based Transfer Learning Approach", "authors": ["Anjali Dharmik"], "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.12642v3", "summary": "SARS-CoV-2, the causative agent of COVID-19, remains a global health concern\ndue to its high transmissibility and evolving variants. Although vaccination\nefforts and therapeutic advancements have mitigated disease severity, emerging\nmutations continue to challenge diagnostics and containment strategies. As of\nmid-February 2025, global test positivity has risen to 11%, marking the highest\nlevel in over six months despite widespread immunization efforts. Newer\nvariants demonstrate enhanced host cell binding, increasing both infectivity\nand diagnostic complexity. This study evaluates the effectiveness of deep\ntransfer learning in delivering rapid, accurate, and mutation-resilient\nCOVID-19 diagnosis from medical imaging, with a focus on scalability and\naccessibility. We developed an automated detection system using\nstate-of-the-art CNNs, including VGG16, ResNet50, ConvNetXtTiny, MobileNet,\nNASNetMobile, and DenseNet121 among others, to detect COVID-19 from chest X-ray\nand CT images. Among all the models evaluated, DenseNet121 emerged as the\nbest-performing architecture for COVID-19 diagnosis using CT and X-ray images.\nIt achieved an impressive accuracy of 98%, with 96.9% precision, 98.9% recall,\n97.9% F1-score and 99.8% AUC score, indicating a high degree of consistency and\nreliability in both detecting positive and negative cases. The confusion matrix\nshowed minimal false positives and false negatives, underscoring the model's\nrobustness in real-world diagnostic scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.12642v3", "cate": "eess.IV", "date": "2025-03-16", "updated": "2025-07-13"}
{"id": "2507.10136", "title": "A PBN-RL-XAI Framework for Discovering a \"Hit-and-Run'' Therapeutic Strategy in Melanoma", "authors": ["Zhonglin Liu"], "categories": ["q-bio.QM", "cs.AI"], "primary_category": "Subjects:       Quantitative Methods (q-bio.QM)", "pdf_link": null, "comments": "Comments:      9 pages, 5 figures. Submitted to the IEEE International Conference on Bioinformatics and Biomedicine (BIBM) 2025. Code is available at this https URL", "url": "http://arxiv.org/abs/2507.10136v1", "summary": "Innate resistance to anti-PD-1 immunotherapy remains a major clinical\nchallenge in metastatic melanoma, with the underlying molecular networks being\npoorly understood. To address this, we constructed a dynamic Probabilistic\nBoolean Network model using transcriptomic data from patient tumor biopsies to\nelucidate the regulatory logic governing therapy response. We then employed a\nreinforcement learning agent to systematically discover optimal, multi-step\ntherapeutic interventions and used explainable artificial intelligence to\nmechanistically interpret the agent's control policy. The analysis revealed\nthat a precisely timed, 4-step temporary inhibition of the lysyl oxidase like 2\nprotein (LOXL2) was the most effective strategy. Our explainable analysis\nshowed that this ``hit-and-run\" intervention is sufficient to erase the\nmolecular signature driving resistance, allowing the network to self-correct\nwithout requiring sustained intervention. This study presents a novel,\ntime-dependent therapeutic hypothesis for overcoming immunotherapy resistance\nand provides a powerful computational framework for identifying non-obvious\nintervention protocols in complex biological systems.", "comment": "9 pages, 5 figures. Submitted to the IEEE International Conference on\n  Bioinformatics and Biomedicine (BIBM) 2025. Code is available at\n  https://github.com/Liu-Zhonglin/pbn-melanoma-project", "pdf_url": "http://arxiv.org/pdf/2507.10136v1", "cate": "q-bio.QM", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10218", "title": "Straighten Viscous Rectified Flow via Noise Optimization", "authors": ["Jimin Dai", "Jiexi Yan", "Jian Yang", "Lei Luo"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10218v1", "summary": "The Reflow operation aims to straighten the inference trajectories of the\nrectified flow during training by constructing deterministic couplings between\nnoises and images, thereby improving the quality of generated images in\nsingle-step or few-step generation. However, we identify critical limitations\nin Reflow, particularly its inability to rapidly generate high-quality images\ndue to a distribution gap between images in its constructed deterministic\ncouplings and real images. To address these shortcomings, we propose a novel\nalternative called Straighten Viscous Rectified Flow via Noise Optimization\n(VRFNO), which is a joint training framework integrating an encoder and a\nneural velocity field. VRFNO introduces two key innovations: (1) a historical\nvelocity term that enhances trajectory distinction, enabling the model to more\naccurately predict the velocity of the current trajectory, and (2) the noise\noptimization through reparameterization to form optimized couplings with real\nimages which are then utilized for training, effectively mitigating errors\ncaused by Reflow's limitations. Comprehensive experiments on synthetic data and\nreal datasets with varying resolutions show that VRFNO significantly mitigates\nthe limitations of Reflow, achieving state-of-the-art performance in both\none-step and few-step generation tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10218v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2506.07267", "title": "Real-Time High-Accuracy Digital Wireless Time, Frequency, and Phase Calibration For Coherent Distributed Antenna Arrays", "authors": ["Jason M. Merlo", "Samuel Wagner", "John Lancaster", "Jeffrey A. Nanzer"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.07267v3", "summary": "This work presents a fully-digital high-accuracy real-time calibration\nprocedure for frequency and time alignment of open-loop wirelessly coordinated\ncoherent distributed antenna array (CDA) modems, enabling radio frequency (RF)\nphase coherence of spatially separated commercial off-the-shelf (COTS)\nsoftware-defined radios (SDRs) without cables or external references such as\nglobal navigation satellite system (GNSS). Building on previous work using\nhigh-accuracy spectrally-sparse time of arrival (ToA) waveforms and a\nmulti-step ToA refinement process, a high-accuracy two-way time transfer\n(TWTT)-based timefrequency coordination approach is demonstrated. Due to the\ntwo-way nature of the high-accuracy TWTT approach, the time and frequency\nestimates are Doppler and multi-path tolerant, so long as the channel is\nreciprocal over the synchronization epoch. This technique is experimentally\nverified using COTS SDRs in a lab environment in static and dynamic scenarios\nand with significant multipath scatterers. Time, frequency, and phase stability\nwere evaluated by beamforming over coaxial cables to an oscilloscope which\nachieved time and phase precisions of ~60 ps-70 ps, with median coherent gains\nabove 99 % using optimized coordination parameters, and a beamforming frequency\nroot-mean-square error (RMSE) of 3.73 ppb in a dynamic scenario. Finally,\nexperiments were conducted to compare the performance of this technique with\nprevious works using an analog continuous-wave two-tone (CWTT) frequency\nreference technique in both static and dynamic settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.07267v3", "cate": "eess.SP", "date": "2025-06-08", "updated": "2025-07-11"}
{"id": "2503.13257", "title": "Anatomically and Metabolically Informed Diffusion for Unified Denoising and Segmentation in Low-Count PET Imaging", "authors": ["Menghua Xia", "Kuan-Yin Ko", "Der-Shiun Wang", "Ming-Kai Chen", "Qiong Liu", "Huidong Xie", "Liang Guo", "Wei Ji", "Jinsong Ouyang", "Reimund Bayerlein", "Benjamin A. Spencer", "Quanzheng Li", "Ramsey D. Badawi", "Georges El Fakhri", "Chi Liu"], "categories": ["eess.IV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      19 pages", "url": "http://arxiv.org/abs/2503.13257v2", "summary": "Positron emission tomography (PET) image denoising, along with lesion and\norgan segmentation, are critical steps in PET-aided diagnosis. However,\nexisting methods typically treat these tasks independently, overlooking\ninherent synergies between them as correlated steps in the analysis pipeline.\nIn this work, we present the anatomically and metabolically informed diffusion\n(AMDiff) model, a unified framework for denoising and lesion/organ segmentation\nin low-count PET imaging. By integrating multi-task functionality and\nexploiting the mutual benefits of these tasks, AMDiff enables direct\nquantification of clinical metrics, such as total lesion glycolysis (TLG), from\nlow-count inputs. The AMDiff model incorporates a semantic-informed denoiser\nbased on diffusion strategy and a denoising-informed segmenter utilizing\nnnMamba architecture. The segmenter constrains denoised outputs via a\nlesion-organ-specific regularizer, while the denoiser enhances the segmenter by\nproviding enriched image information through a denoising revision module. These\ncomponents are connected via a warming-up mechanism to optimize multi-task\ninteractions. Experiments on multi-vendor, multi-center, and multi-noise-level\ndatasets demonstrate the superior performance of AMDiff. For test cases below\n20% of the clinical count levels from participating sites, AMDiff achieves TLG\nquantification biases of -21.60%, outperforming its ablated versions which\nyield biases of -30.83% (without the lesion-organ-specific regularizer) and\n-35.63% (without the denoising revision module).", "comment": "19 pages", "pdf_url": "http://arxiv.org/pdf/2503.13257v2", "cate": "eess.IV", "date": "2025-03-17", "updated": "2025-07-14"}
{"id": "2507.10172", "title": "Play Style Identification Using Low-Level Representations of Play Traces in MicroRTS", "authors": ["Ruizhe Yu Xia", "Jeremy Gow", "Simon Lucas"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted as Short Paper for IEEE CoG", "url": "http://arxiv.org/abs/2507.10172v1", "summary": "Play style identification can provide valuable game design insights and\nenable adaptive experiences, with the potential to improve game playing agents.\nPrevious work relies on domain knowledge to construct play trace\nrepresentations using handcrafted features. More recent approaches incorporate\nthe sequential structure of play traces but still require some level of domain\nabstraction. In this study, we explore the use of unsupervised CNN-LSTM\nautoencoder models to obtain latent representations directly from low-level\nplay trace data in MicroRTS. We demonstrate that this approach yields a\nmeaningful separation of different game playing agents in the latent space,\nreducing reliance on domain expertise and its associated biases. This latent\nspace is then used to guide the exploration of diverse play styles within\nstudied AI players.", "comment": "Accepted as Short Paper for IEEE CoG", "pdf_url": "http://arxiv.org/pdf/2507.10172v1", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10223", "title": "ProGait: A Multi-Purpose Video Dataset and Benchmark for Transfemoral Prosthesis Users", "authors": ["Xiangyu Yin", "Boyuan Yang", "Weichen Liu", "Qiyao Xue", "Abrar Alamri", "Goeran Fiedler", "Wei Gao"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV'25", "url": "http://arxiv.org/abs/2507.10223v1", "summary": "Prosthetic legs play a pivotal role in clinical rehabilitation, allowing\nindividuals with lower-limb amputations the ability to regain mobility and\nimprove their quality of life. Gait analysis is fundamental for optimizing\nprosthesis design and alignment, directly impacting the mobility and life\nquality of individuals with lower-limb amputations. Vision-based machine\nlearning (ML) methods offer a scalable and non-invasive solution to gait\nanalysis, but face challenges in correctly detecting and analyzing prosthesis,\ndue to their unique appearances and new movement patterns. In this paper, we\naim to bridge this gap by introducing a multi-purpose dataset, namely ProGait,\nto support multiple vision tasks including Video Object Segmentation, 2D Human\nPose Estimation, and Gait Analysis (GA). ProGait provides 412 video clips from\nfour above-knee amputees when testing multiple newly-fitted prosthetic legs\nthrough walking trials, and depicts the presence, contours, poses, and gait\npatterns of human subjects with transfemoral prosthetic legs. Alongside the\ndataset itself, we also present benchmark tasks and fine-tuned baseline models\nto illustrate the practical application and performance of the ProGait dataset.\nWe compared our baseline models against pre-trained vision models,\ndemonstrating improved generalizability when applying the ProGait dataset for\nprosthesis-specific tasks. Our code is available at\nhttps://github.com/pittisl/ProGait and dataset at\nhttps://huggingface.co/datasets/ericyxy98/ProGait.", "comment": "Accepted by ICCV'25", "pdf_url": "http://arxiv.org/pdf/2507.10223v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2503.20815", "title": "D2SA: Dual-Stage Distribution and Slice Adaptation for Efficient Test-Time Adaptation in MRI Reconstruction", "authors": ["Lipei Zhang", "Rui Sun", "Zhongying Deng", "Yanqi Cheng", "Carola-Bibiane Schönlieb", "Angelica I Aviles-Rivero"], "categories": ["eess.IV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      10 pages, 12 pages (supplementary)", "url": "http://arxiv.org/abs/2503.20815v2", "summary": "Variations in Magnetic resonance imaging (MRI) scanners and acquisition\nprotocols cause distribution shifts that degrade reconstruction performance on\nunseen data. Test-time adaptation (TTA) offers a promising solution to address\nthis discrepancies. However, previous single-shot TTA approaches are\ninefficient due to repeated training and suboptimal distributional models.\nSelf-supervised learning methods may risk over-smoothing in scarce data\nscenarios. To address these challenges, we propose a novel Dual-Stage\nDistribution and Slice Adaptation (D2SA) via MRI implicit neural representation\n(MR-INR) to improve MRI reconstruction performance and efficiency, which\nfeatures two stages. In the first stage, an MR-INR branch performs patient-wise\ndistribution adaptation by learning shared representations across slices and\nmodelling patient-specific shifts with mean and variance adjustments. In the\nsecond stage, single-slice adaptation refines the output from frozen\nconvolutional layers with a learnable anisotropic diffusion module, preventing\nover-smoothing and reducing computation. Experiments across five MRI\ndistribution shifts demonstrate that our method can integrate well with various\nself-supervised learning (SSL) framework, improving performance and\naccelerating convergence under diverse conditions.", "comment": "10 pages, 12 pages (supplementary)", "pdf_url": "http://arxiv.org/pdf/2503.20815v2", "cate": "eess.IV", "date": "2025-03-25", "updated": "2025-07-14"}
{"id": "2507.10177", "title": "Abusive text transformation using LLMs", "authors": ["Rohitash Chandra", "Jiyong Choi"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10177v1", "summary": "Although Large Language Models (LLMs) have demonstrated significant\nadvancements in natural language processing tasks, their effectiveness in the\nclassification and transformation of abusive text into non-abusive versions\nremains an area for exploration. In this study, we aim to use LLMs to transform\nabusive text (tweets and reviews) featuring hate speech and swear words into\nnon-abusive text, while retaining the intent of the text. We evaluate the\nperformance of two state-of-the-art LLMs, such as Gemini, GPT-4o, DeekSeek and\nGroq, on their ability to identify abusive text. We them to transform and\nobtain a text that is clean from abusive and inappropriate content but\nmaintains a similar level of sentiment and semantics, i.e. the transformed text\nneeds to maintain its message. Afterwards, we evaluate the raw and transformed\ndatasets with sentiment analysis and semantic analysis. Our results show Groq\nprovides vastly different results when compared with other LLMs. We have\nidentified similarities between GPT-4o and DeepSeek-V3.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10177v1", "cate": "cs.CL", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10225", "title": "Synthesizing Near-Boundary OOD Samples for Out-of-Distribution Detection", "authors": ["Jinglun Li", "Kaixun Jiang", "Zhaoyu Chen", "Bo Lin", "Yao Tang", "Weifeng Ge", "Wenqiang Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10225v1", "summary": "Pre-trained vision-language models have exhibited remarkable abilities in\ndetecting out-of-distribution (OOD) samples. However, some challenging OOD\nsamples, which lie close to in-distribution (InD) data in image feature space,\ncan still lead to misclassification. The emergence of foundation models like\ndiffusion models and multimodal large language models (MLLMs) offers a\npotential solution to this issue. In this work, we propose SynOOD, a novel\napproach that harnesses foundation models to generate synthetic, challenging\nOOD data for fine-tuning CLIP models, thereby enhancing boundary-level\ndiscrimination between InD and OOD samples. Our method uses an iterative\nin-painting process guided by contextual prompts from MLLMs to produce nuanced,\nboundary-aligned OOD samples. These samples are refined through noise\nadjustments based on gradients from OOD scores like the energy score,\neffectively sampling from the InD/OOD boundary. With these carefully\nsynthesized images, we fine-tune the CLIP image encoder and negative label\nfeatures derived from the text encoder to strengthen connections between\nnear-boundary OOD samples and a set of negative labels. Finally, SynOOD\nachieves state-of-the-art performance on the large-scale ImageNet benchmark,\nwith minimal increases in parameters and runtime. Our approach significantly\nsurpasses existing methods, improving AUROC by 2.80% and reducing FPR95 by\n11.13%. Codes are available in https://github.com/Jarvisgivemeasuit/SynOOD.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10225v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2504.21612", "title": "Selective Variable Convolution Meets Dynamic Content-Guided Attention for Infrared Small Target Detection", "authors": ["Yirui Chen", "Yiming Zhu", "Yuxin Jing", "Tianpei Zhang", "Jufeng Zhao"], "categories": ["eess.IV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.21612v2", "summary": "Infrared Small Target Detection (IRSTD) system aims to identify small targets\nin complex backgrounds. Due to the convolution operation in Convolutional\nNeural Networks (CNNs), applying traditional CNNs to IRSTD presents challenges,\nsince the feature extraction of small targets is often insufficient, resulting\nin the loss of critical features. To address these issues, we propose a dynamic\ncontent-guided attention multiscale feature aggregation network (DCGANet),\nwhich adheres to the attention principle of 'coarse-to-fine' and achieves high\ndetection accuracy. First, we propose a selective variable convolution (SVC)\nmodule that integrates the benefits of standard convolution, irregular\ndeformable convolution, and multi-rate dilated convolution. This module is\ndesigned to expand the receptive field and enhance non-local features, thereby\neffectively improving the discrimination between targets and backgrounds.\nSecond, the core component of DCGANet is a two-stage content-guided attention\nmodule. This module employs a two-stage attention mechanism to initially direct\nthe network's focus to salient regions within the feature maps and subsequently\ndetermine whether these regions correspond to targets or background\ninterference. By retaining the most significant responses, this mechanism\neffectively suppresses false alarms. Additionally, we propose an Adaptive\nDynamic Feature Fusion (ADFF) module to substitute for static feature\ncascading. This dynamic feature fusion strategy enables DCGANet to adaptively\nintegrate contextual features, thereby enhancing its ability to discriminate\ntrue targets from false alarms. DCGANet has achieved new benchmarks across\nmultiple datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.21612v2", "cate": "eess.IV", "date": "2025-04-30", "updated": "2025-07-13"}
{"id": "2507.10179", "title": "The Second Machine Turn: From Checking Proofs to Creating Concepts", "authors": ["Asvin G"], "categories": ["math.HO", "cs.AI"], "primary_category": "Subjects:       History and Overview (math.HO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10179v1", "summary": "We identify a second machine turn in the process of mathematical discovery:\nafter automating proof-checking, AI is now poised to automate the *creation* of\nmathematical concepts themselves. We discuss the current state of the art,\nobstacles and potential solutions as well as a preliminary attempt at\nmathematizing the creation of concepts itself. The paper ends with an\nassessment of how these capabilities could reshape mathematics and\nhuman-machine collaboration, and a few different futures we might find\nourselves in.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10179v1", "cate": "math.HO", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10236", "title": "Navigating the Challenges of AI-Generated Image Detection in the Wild: What Truly Matters?", "authors": ["Despina Konstantinidou", "Dimitrios Karageorgiou", "Christos Koutlis", "Olga Papadopoulou", "Emmanouil Schinas", "Symeon Papadopoulos"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      35 pages, 4 figures", "url": "http://arxiv.org/abs/2507.10236v1", "summary": "The rapid advancement of generative technologies presents both unprecedented\ncreative opportunities and significant challenges, particularly in maintaining\nsocial trust and ensuring the integrity of digital information. Following these\nconcerns, the challenge of AI-Generated Image Detection (AID) becomes\nincreasingly critical. As these technologies become more sophisticated, the\nquality of AI-generated images has reached a level that can easily deceive even\nthe most discerning observers. Our systematic evaluation highlights a critical\nweakness in current AI-Generated Image Detection models: while they perform\nexceptionally well on controlled benchmark datasets, they struggle\nsignificantly with real-world variations. To assess this, we introduce ITW-SM,\na new dataset of real and AI-generated images collected from major social media\nplatforms. In this paper, we identify four key factors that influence AID\nperformance in real-world scenarios: backbone architecture, training data\ncomposition, pre-processing strategies and data augmentation combinations. By\nsystematically analyzing these components, we shed light on their impact on\ndetection efficacy. Our modifications result in an average AUC improvement of\n26.87% across various AID models under real-world conditions.", "comment": "35 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.10236v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2506.23121", "title": "CRISP-SAM2: SAM2 with Cross-Modal Interaction and Semantic Prompting for Multi-Organ Segmentation", "authors": ["Xinlei Yu", "Changmiao Wang", "Hui Jin", "Ahmed Elazab", "Gangyong Jia", "Xiang Wan", "Changqing Zou", "Ruiquan Ge"], "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Accepted By ACMMM25", "url": "http://arxiv.org/abs/2506.23121v3", "summary": "Multi-organ medical segmentation is a crucial component of medical image\nprocessing, essential for doctors to make accurate diagnoses and develop\neffective treatment plans. Despite significant progress in this field, current\nmulti-organ segmentation models often suffer from inaccurate details,\ndependence on geometric prompts and loss of spatial information. Addressing\nthese challenges, we introduce a novel model named CRISP-SAM2 with CRoss-modal\nInteraction and Semantic Prompting based on SAM2. This model represents a\npromising approach to multi-organ medical segmentation guided by textual\ndescriptions of organs. Our method begins by converting visual and textual\ninputs into cross-modal contextualized semantics using a progressive\ncross-attention interaction mechanism. These semantics are then injected into\nthe image encoder to enhance the detailed understanding of visual information.\nTo eliminate reliance on geometric prompts, we use a semantic prompting\nstrategy, replacing the original prompt encoder to sharpen the perception of\nchallenging targets. In addition, a similarity-sorting self-updating strategy\nfor memory and a mask-refining process is applied to further adapt to medical\nimaging and enhance localized details. Comparative experiments conducted on\nseven public datasets indicate that CRISP-SAM2 outperforms existing models.\nExtensive analysis also demonstrates the effectiveness of our method, thereby\nconfirming its superior performance, especially in addressing the limitations\nmentioned earlier. Our code is available at:\nhttps://github.com/YU-deep/CRISP_SAM2.git.", "comment": "Accepted By ACMMM25", "pdf_url": "http://arxiv.org/pdf/2506.23121v3", "cate": "eess.IV", "date": "2025-06-29", "updated": "2025-07-14"}
{"id": "2507.10194", "title": "Learning Private Representations through Entropy-based Adversarial Training", "authors": ["Tassilo Klein", "Moin Nabi"], "categories": ["cs.LG", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10194v1", "summary": "How can we learn a representation with high predictive power while preserving\nuser privacy? We present an adversarial representation learning method for\nsanitizing sensitive content from the learned representation. Specifically, we\nintroduce a variant of entropy - focal entropy, which mitigates the potential\ninformation leakage of the existing entropy-based approaches. We showcase\nfeasibility on multiple benchmarks. The results suggest high target utility at\nmoderate privacy leakage.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10194v1", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10239", "title": "Transferring Styles for Reduced Texture Bias and Improved Robustness in Semantic Segmentation Networks", "authors": ["Ben Hamscher", "Edgar Heinert", "Annika Mütze", "Kira Maag", "Matthias Rottmann"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      accepted at ECAI 2025", "url": "http://arxiv.org/abs/2507.10239v1", "summary": "Recent research has investigated the shape and texture biases of deep neural\nnetworks (DNNs) in image classification which influence their generalization\ncapabilities and robustness. It has been shown that, in comparison to regular\nDNN training, training with stylized images reduces texture biases in image\nclassification and improves robustness with respect to image corruptions. In an\neffort to advance this line of research, we examine whether style transfer can\nlikewise deliver these two effects in semantic segmentation. To this end, we\nperform style transfer with style varying across artificial image areas. Those\nrandom areas are formed by a chosen number of Voronoi cells. The resulting\nstyle-transferred data is then used to train semantic segmentation DNNs with\nthe objective of reducing their dependence on texture cues while enhancing\ntheir reliance on shape-based features. In our experiments, it turns out that\nin semantic segmentation, style transfer augmentation reduces texture bias and\nstrongly increases robustness with respect to common image corruptions as well\nas adversarial attacks. These observations hold for convolutional neural\nnetworks and transformer architectures on the Cityscapes dataset as well as on\nPASCAL Context, showing the generality of the proposed method.", "comment": "accepted at ECAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.10239v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.01564", "title": "Multi Source COVID-19 Detection via Kernel-Density-based Slice Sampling", "authors": ["Chia-Ming Lee", "Bo-Cheng Qiu", "Ting-Yao Chen", "Ming-Han Sun", "Fang-Ying Lin", "Jung-Tse Tsai", "I-An Tsai", "Yu-Fan Lin", "Chih-Chung Hsu"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.01564v2", "summary": "We present our solution for the Multi-Source COVID-19 Detection Challenge,\nwhich classifies chest CT scans from four distinct medical centers. To address\nmulti-source variability, we employ the Spatial-Slice Feature Learning (SSFL)\nframework with Kernel-Density-based Slice Sampling (KDS). Our preprocessing\npipeline combines lung region extraction, quality control, and adaptive slice\nsampling to select eight representative slices per scan. We compare\nEfficientNet and Swin Transformer architectures on the validation set. The\nEfficientNet model achieves an F1-score of 94.68%, compared to the Swin\nTransformer's 93.34%. The results demonstrate the effectiveness of our\nKDS-based pipeline on multi-source data and highlight the importance of dataset\nbalance in multi-institutional medical imaging evaluation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.01564v2", "cate": "eess.IV", "date": "2025-07-02", "updated": "2025-07-12"}
{"id": "2507.10216", "title": "Absher: A Benchmark for Evaluating Large Language Models Understanding of Saudi Dialects", "authors": ["Renad Al-Monef", "Hassan Alhuzali", "Nora Alturayeif", "Ashwag Alasmari"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10216v1", "summary": "As large language models (LLMs) become increasingly central to Arabic NLP\napplications, evaluating their understanding of regional dialects and cultural\nnuances is essential, particularly in linguistically diverse settings like\nSaudi Arabia. This paper introduces \\texttt{Absher}, a comprehensive benchmark\nspecifically designed to assess LLMs performance across major Saudi dialects.\n\\texttt{Absher} comprises over 18,000 multiple-choice questions spanning six\ndistinct categories: Meaning, True/False, Fill-in-the-Blank, Contextual Usage,\nCultural Interpretation, and Location Recognition. These questions are derived\nfrom a curated dataset of dialectal words, phrases, and proverbs sourced from\nvarious regions of Saudi Arabia. We evaluate several state-of-the-art LLMs,\nincluding multilingual and Arabic-specific models. We also provide detailed\ninsights into their capabilities and limitations. Our results reveal notable\nperformance gaps, particularly in tasks requiring cultural inference or\ncontextual understanding. Our findings highlight the urgent need for\ndialect-aware training and culturally aligned evaluation methodologies to\nimprove LLMs performance in real-world Arabic applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10216v1", "cate": "cs.CL", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10265", "title": "Kaleidoscopic Background Attack: Disrupting Pose Estimation with Multi-Fold Radial Symmetry Textures", "authors": ["Xinlong Ding", "Hongwei Yu", "Jiawei Li", "Feifan Li", "Yu Shang", "Bochao Zou", "Huimin Ma", "Jiansheng Chen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at ICCV 2025. Project page is available at this https URL", "url": "http://arxiv.org/abs/2507.10265v1", "summary": "Camera pose estimation is a fundamental computer vision task that is\nessential for applications like visual localization and multi-view stereo\nreconstruction. In the object-centric scenarios with sparse inputs, the\naccuracy of pose estimation can be significantly influenced by background\ntextures that occupy major portions of the images across different viewpoints.\nIn light of this, we introduce the Kaleidoscopic Background Attack (KBA), which\nuses identical segments to form discs with multi-fold radial symmetry. These\ndiscs maintain high similarity across different viewpoints, enabling effective\nattacks on pose estimation models even with natural texture segments.\nAdditionally, a projected orientation consistency loss is proposed to optimize\nthe kaleidoscopic segments, leading to significant enhancement in the attack\neffectiveness. Experimental results show that optimized adversarial\nkaleidoscopic backgrounds can effectively attack various camera pose estimation\nmodels.", "comment": "Accepted at ICCV 2025. Project page is available at\n  https://wakuwu.github.io/KBA", "pdf_url": "http://arxiv.org/pdf/2507.10265v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.02668", "title": "MEGANet-W: A Wavelet-Driven Edge-Guided Attention Framework for Weak Boundary Polyp Detection", "authors": ["Zhe Yee Tan"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      7 pages, 3 figures", "url": "http://arxiv.org/abs/2507.02668v2", "summary": "Colorectal polyp segmentation is critical for early detection of colorectal\ncancer, yet weak and low contrast boundaries significantly limit automated\naccuracy. Existing deep models either blur fine edge details or rely on\nhandcrafted filters that perform poorly under variable imaging conditions. We\npropose MEGANet-W, a Wavelet Driven Edge Guided Attention Network that injects\ndirectional, parameter free Haar wavelet edge maps into each decoder stage to\nrecalibrate semantic features. Our two main contributions are: (1) a two-level\nHaar wavelet head for multi orientation edge extraction; and (2) Wavelet Edge\nGuided Attention (WEGA) modules that fuse wavelet cues with boundary and input\nbranches. On five public polyp datasets, MEGANet-W consistently outperforms\nexisting methods, improving mIoU by up to 2.3% and mDice by 1.2%, while\nintroducing no additional learnable parameters.", "comment": "7 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2507.02668v2", "cate": "eess.IV", "date": "2025-07-03", "updated": "2025-07-13"}
{"id": "2507.10300", "title": "FaceLLM: A Multimodal Large Language Model for Face Understanding", "authors": ["Hatef Otroshi Shahreza", "Sébastien Marcel"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted in ICCV 2025 workshops", "url": "http://arxiv.org/abs/2507.10300v1", "summary": "Multimodal large language models (MLLMs) have shown remarkable performance in\nvision-language tasks. However, existing MLLMs are primarily trained on generic\ndatasets, limiting their ability to reason on domain-specific visual cues such\nas those in facial images. In particular, tasks that require detailed\nunderstanding of facial structure, expression, emotion, and demographic\nfeatures remain underexplored by MLLMs due to the lack of large-scale annotated\nface image-text datasets. In this work, we introduce FaceLLM, a multimodal\nlarge language model trained specifically for facial image understanding. To\nconstruct the training data, we propose a novel weakly supervised pipeline that\nuses ChatGPT with attribute-aware prompts to generate high-quality\nquestion-answer pairs based on images from the FairFace dataset. The resulting\ncorpus, called FairFaceGPT, covers a diverse set of attributes including\nexpression, pose, skin texture, and forensic information. Our experiments\ndemonstrate that FaceLLM improves the performance of MLLMs on various\nface-centric tasks and achieves state-of-the-art performance. This work\nhighlights the potential of synthetic supervision via language models for\nbuilding domain-specialized MLLMs, and sets a precedent for trustworthy,\nhuman-centric multimodal AI systems. FairFaceGPT dataset and pretrained FaceLLM\nmodels are publicly available in the project page.", "comment": "Accepted in ICCV 2025 workshops", "pdf_url": "http://arxiv.org/pdf/2507.10300v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10283", "title": "FTCFormer: Fuzzy Token Clustering Transformer for Image Classification", "authors": ["Muyi Bao", "Changyu Zeng", "Yifan Wang", "Zhengni Yang", "Zimu Wang", "Guangliang Cheng", "Jun Qi", "Wei Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10283v1", "summary": "Transformer-based deep neural networks have achieved remarkable success\nacross various computer vision tasks, largely attributed to their long-range\nself-attention mechanism and scalability. However, most transformer\narchitectures embed images into uniform, grid-based vision tokens, neglecting\nthe underlying semantic meanings of image regions, resulting in suboptimal\nfeature representations. To address this issue, we propose Fuzzy Token\nClustering Transformer (FTCFormer), which incorporates a novel clustering-based\ndownsampling module to dynamically generate vision tokens based on the semantic\nmeanings instead of spatial positions. It allocates fewer tokens to less\ninformative regions and more to represent semantically important regions,\nregardless of their spatial adjacency or shape irregularity. To further enhance\nfeature extraction and representation, we propose a Density Peak\nClustering-Fuzzy K-Nearest Neighbor (DPC-FKNN) mechanism for clustering center\ndetermination, a Spatial Connectivity Score (SCS) for token assignment, and a\nchannel-wise merging (Cmerge) strategy for token merging. Extensive experiments\non 32 datasets across diverse domains validate the effectiveness of FTCFormer\non image classification, showing consistent improvements over the TCFormer\nbaseline, achieving gains of improving 1.43% on five fine-grained datasets,\n1.09% on six natural image datasets, 0.97% on three medical datasets and 0.55%\non four remote sensing datasets. The code is available at:\nhttps://github.com/BaoBao0926/FTCFormer/tree/main.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10283v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.08282", "title": "MetaH2: A Snapshot Metasurface HDR Hyperspectral Camera", "authors": ["Yuxuan Liu", "Qi Guo"], "categories": ["eess.IV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      To appear in IEEE ICIP 2025", "url": "http://arxiv.org/abs/2507.08282v2", "summary": "We present a metasurface camera that jointly performs high-dynamic range\n(HDR) and hyperspectral imaging in a snapshot. The system integrates exposure\nbracketing and computed tomography imaging spectrometry (CTIS) by\nsimultaneously forming multiple spatially multiplexed projections with unique\npower ratios and chromatic aberrations on a photosensor. The measurements are\nsubsequently processed through a deep reconstruction model to generate an HDR\nimage and a hyperspectral datacube. Our simulation studies show that the\nproposed system achieves higher reconstruction accuracy than previous snapshot\nhyperspectral imaging methods on benchmark datasets. We assemble a working\nprototype and demonstrate snapshot reconstruction of 60 dB dynamic range and 10\nnm spectral resolution from 600 nm to 700 nm on real-world scenes from a\nmonochrome photosensor.", "comment": "To appear in IEEE ICIP 2025", "pdf_url": "http://arxiv.org/pdf/2507.08282v2", "cate": "eess.IV", "date": "2025-07-11", "updated": "2025-07-14"}
{"id": "2507.10311", "title": "Recognizing Dementia from Neuropsychological Tests with State Space Models", "authors": ["Liming Wang", "Saurabhchand Bhati", "Cody Karjadi", "Rhoda Au", "James Glass"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10311v1", "summary": "Early detection of dementia is critical for timely medical intervention and\nimproved patient outcomes. Neuropsychological tests are widely used for\ncognitive assessment but have traditionally relied on manual scoring. Automatic\ndementia classification (ADC) systems aim to infer cognitive decline directly\nfrom speech recordings of such tests. We propose Demenba, a novel ADC framework\nbased on state space models, which scale linearly in memory and computation\nwith sequence length. Trained on over 1,000 hours of cognitive assessments\nadministered to Framingham Heart Study participants, some of whom were\ndiagnosed with dementia through adjudicated review, our method outperforms\nprior approaches in fine-grained dementia classification by 21\\%, while using\nfewer parameters. We further analyze its scaling behavior and demonstrate that\nour model gains additional improvement when fused with large language models,\npaving the way for more transparent and scalable dementia assessment tools.\nCode: https://anonymous.4open.science/r/Demenba-0861", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10311v1", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10293", "title": "Show and Polish: Reference-Guided Identity Preservation in Face Video Restoration", "authors": ["Wenkang Han", "Wang Lin", "Yiyun Zhou", "Qi Liu", "Shulei Wang", "Chang Yao", "Jingyuan Chen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by MM 2025", "url": "http://arxiv.org/abs/2507.10293v1", "summary": "Face Video Restoration (FVR) aims to recover high-quality face videos from\ndegraded versions. Traditional methods struggle to preserve fine-grained,\nidentity-specific features when degradation is severe, often producing\naverage-looking faces that lack individual characteristics. To address these\nchallenges, we introduce IP-FVR, a novel method that leverages a high-quality\nreference face image as a visual prompt to provide identity conditioning during\nthe denoising process. IP-FVR incorporates semantically rich identity\ninformation from the reference image using decoupled cross-attention\nmechanisms, ensuring detailed and identity consistent results. For intra-clip\nidentity drift (within 24 frames), we introduce an identity-preserving feedback\nlearning method that combines cosine similarity-based reward signals with\nsuffix-weighted temporal aggregation. This approach effectively minimizes drift\nwithin sequences of frames. For inter-clip identity drift, we develop an\nexponential blending strategy that aligns identities across clips by\niteratively blending frames from previous clips during the denoising process.\nThis method ensures consistent identity representation across different clips.\nAdditionally, we enhance the restoration process with a multi-stream negative\nprompt, guiding the model's attention to relevant facial attributes and\nminimizing the generation of low-quality or incorrect features. Extensive\nexperiments on both synthetic and real-world datasets demonstrate that IP-FVR\noutperforms existing methods in both quality and identity preservation,\nshowcasing its substantial potential for practical applications in face video\nrestoration.", "comment": "Accepted by MM 2025", "pdf_url": "http://arxiv.org/pdf/2507.10293v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2306.11341", "title": "MSVD-Indonesian: A Benchmark for Multimodal Video-Text Tasks in Indonesian", "authors": ["Willy Fitra Hendria"], "categories": ["cs.MM", "cs.CL", "cs.CV", "cs.LG", "eess.IV"], "primary_category": "Subjects:       Multimedia (cs.MM)", "pdf_link": null, "comments": "Comments:      10 pages, 5 figures, 5 tables", "url": "http://arxiv.org/abs/2306.11341v2", "summary": "Multimodal learning on video and text has seen significant progress,\nparticularly in tasks like text-to-video retrieval, video-to-text retrieval,\nand video captioning. However, most existing methods and datasets focus\nexclusively on English. Despite Indonesian being one of the most widely spoken\nlanguages, multimodal research in Indonesian remains under-explored, largely\ndue to the lack of benchmark datasets. To address this gap, we introduce the\nfirst public Indonesian video-text dataset by translating the English captions\nin the MSVD dataset into Indonesian. Using this dataset, we evaluate neural\nnetwork models which were developed for the English video-text dataset on three\ntasks, i.e., text-to-video retrieval, video-to-text retrieval, and video\ncaptioning. Most existing models rely on feature extractors pretrained on\nEnglish vision-language datasets, raising concerns about their applicability to\nIndonesian, given the scarcity of large-scale pretraining resources in the\nlanguage. We apply a cross-lingual transfer learning approach by leveraging\nEnglish-pretrained extractors and fine-tuning models on our Indonesian dataset.\nExperimental results demonstrate that this strategy improves performance across\nall tasks and metrics. We release our dataset publicly to support future\nresearch and hope it will inspire further progress in Indonesian multimodal\nlearning.", "comment": "10 pages, 5 figures, 5 tables", "pdf_url": "http://arxiv.org/pdf/2306.11341v2", "cate": "cs.MM", "date": "2023-06-20", "updated": "2025-07-12"}
{"id": "2507.10348", "title": "Feature Distillation is the Better Choice for Model-Heterogeneous Federated Learning", "authors": ["Yichen Li"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10348v1", "summary": "Model-Heterogeneous Federated Learning (Hetero-FL) has attracted growing\nattention for its ability to aggregate knowledge from heterogeneous models\nwhile keeping private data locally. To better aggregate knowledge from clients,\nensemble distillation, as a widely used and effective technique, is often\nemployed after global aggregation to enhance the performance of the global\nmodel. However, simply combining Hetero-FL and ensemble distillation does not\nalways yield promising results and can make the training process unstable. The\nreason is that existing methods primarily focus on logit distillation, which,\nwhile being model-agnostic with softmax predictions, fails to compensate for\nthe knowledge bias arising from heterogeneous models. To tackle this challenge,\nwe propose a stable and efficient Feature Distillation for model-heterogeneous\nFederated learning, dubbed FedFD, that can incorporate aligned feature\ninformation via orthogonal projection to integrate knowledge from heterogeneous\nmodels better. Specifically, a new feature-based ensemble federated knowledge\ndistillation paradigm is proposed. The global model on the server needs to\nmaintain a projection layer for each client-side model architecture to align\nthe features separately. Orthogonal techniques are employed to re-parameterize\nthe projection layer to mitigate knowledge bias from heterogeneous models and\nthus maximize the distilled knowledge. Extensive experiments show that FedFD\nachieves superior performance compared to state-of-the-art methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10348v1", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10302", "title": "DisCo: Towards Distinct and Coherent Visual Encapsulation in Video MLLMs", "authors": ["Jiahe Zhao", "Rongkun Zheng", "Yi Wang", "Helin Wang", "Hengshuang Zhao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025", "url": "http://arxiv.org/abs/2507.10302v1", "summary": "In video Multimodal Large Language Models (video MLLMs), the visual\nencapsulation process plays a pivotal role in converting video contents into\nrepresentative tokens for LLM input. While linear projectors are widely\nemployed for encapsulation, they introduce semantic indistinctness and temporal\nincoherence when applied to videos. Conversely, the structure of resamplers\nshows promise in tackling these challenges, but an effective solution remains\nunexplored. Drawing inspiration from resampler structures, we introduce DisCo,\na novel visual encapsulation method designed to yield semantically distinct and\ntemporally coherent visual tokens for video MLLMs. DisCo integrates two key\ncomponents: (1) A Visual Concept Discriminator (VCD) module, assigning unique\nsemantics for visual tokens by associating them in pair with discriminative\nconcepts in the video. (2) A Temporal Focus Calibrator (TFC) module, ensuring\nconsistent temporal focus of visual tokens to video elements across every video\nframe. Through extensive experiments on multiple video MLLM frameworks, we\ndemonstrate that DisCo remarkably outperforms previous state-of-the-art methods\nacross a variety of video understanding benchmarks, while also achieving higher\ntoken efficiency thanks to the reduction of semantic indistinctness. The code:\nhttps://github.com/ZJHTerry18/DisCo.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.10302v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2404.14435", "title": "Frenet-Serret Frame-based Decomposition for Part Segmentation of 3D Curvilinear Structures", "authors": ["Leslie Gu", "Jason Ken Adhinarta", "Mikhail Bessmeltsev", "Jiancheng Yang", "Yongjie Jessica Zhang", "Wenjie Yin", "Daniel Berger", "Jeff Lichtman", "Hanspeter Pfister", "Donglai Wei"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 pages, 4 figures", "url": "http://arxiv.org/abs/2404.14435v3", "summary": "Accurately segmenting 3D curvilinear structures in medical imaging remains\nchallenging due to their complex geometry and the scarcity of diverse,\nlarge-scale datasets for algorithm development and evaluation. In this paper,\nwe use dendritic spine segmentation as a case study and address these\nchallenges by introducing a novel Frenet--Serret Frame-based Decomposition,\nwhich decomposes 3D curvilinear structures into a globally \\( C^2 \\) continuous\ncurve that captures the overall shape, and a cylindrical primitive that encodes\nlocal geometric properties. This approach leverages Frenet--Serret Frames and\narc length parameterization to preserve essential geometric features while\nreducing representational complexity, facilitating data-efficient learning,\nimproved segmentation accuracy, and generalization on 3D curvilinear\nstructures. To rigorously evaluate our method, we introduce two datasets:\nCurviSeg, a synthetic dataset for 3D curvilinear structure segmentation that\nvalidates our method's key properties, and DenSpineEM, a benchmark for\ndendritic spine segmentation, which comprises 4,476 manually annotated spines\nfrom 70 dendrites across three public electron microscopy datasets, covering\nmultiple brain regions and species. Our experiments on DenSpineEM demonstrate\nexceptional cross-region and cross-species generalization: models trained on\nthe mouse somatosensory cortex subset achieve 91.9\\% Dice, maintaining strong\nperformance in zero-shot segmentation on both mouse visual cortex (94.1\\% Dice)\nand human frontal lobe (81.8\\% Dice) subsets. Moreover, we test the\ngeneralizability of our method on the IntrA dataset, where it achieves 77.08\\%\nDice (5.29\\% higher than prior arts) on intracranial aneurysm segmentation.\nThese findings demonstrate the potential of our approach for accurately\nanalyzing complex curvilinear structures across diverse medical imaging fields.", "comment": "10 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2404.14435v3", "cate": "cs.CV", "date": "2024-04-19", "updated": "2025-07-14"}
{"id": "2507.10349", "title": "TAT: Temporal-Aligned Transformer for Multi-Horizon Peak Demand Forecasting", "authors": ["Zhiyuan Zhao", "Sitan Yang", "Kin G. Olivares", "Boris N. Oreshkin", "Stan Vitebsky", "Michael W. Mahoney", "B. Aditya Prakash", "Dmitry Efimov"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      9 pages, 4 figures, 7 tables, published at KDD 2025 workshop on AI for Supply Chain: Today and Future", "url": "http://arxiv.org/abs/2507.10349v1", "summary": "Multi-horizon time series forecasting has many practical applications such as\ndemand forecasting. Accurate demand prediction is critical to help make buying\nand inventory decisions for supply chain management of e-commerce and physical\nretailers, and such predictions are typically required for future horizons\nextending tens of weeks. This is especially challenging during high-stake sales\nevents when demand peaks are particularly difficult to predict accurately.\nHowever, these events are important not only for managing supply chain\noperations but also for ensuring a seamless shopping experience for customers.\nTo address this challenge, we propose Temporal-Aligned Transformer (TAT), a\nmulti-horizon forecaster leveraging apriori-known context variables such as\nholiday and promotion events information for improving predictive performance.\nOur model consists of an encoder and decoder, both embedded with a novel\nTemporal Alignment Attention (TAA), designed to learn context-dependent\nalignment for peak demand forecasting. We conduct extensive empirical analysis\non two large-scale proprietary datasets from a large e-commerce retailer. We\ndemonstrate that TAT brings up to 30% accuracy improvement on peak demand\nforecasting while maintaining competitive overall performance compared to other\nstate-of-the-art methods.", "comment": "9 pages, 4 figures, 7 tables, published at KDD 2025 workshop on AI\n  for Supply Chain: Today and Future", "pdf_url": "http://arxiv.org/pdf/2507.10349v1", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10306", "title": "Contrastive Pretraining with Dual Visual Encoders for Gloss-Free Sign Language Translation", "authors": ["Ozge Mercanoglu Sincan", "Richard Bowden"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at 9th Workshop on Sign Language Translation and Avatar Technologies (SLTAT), will be held in conjunction with IVA'25", "url": "http://arxiv.org/abs/2507.10306v1", "summary": "Sign Language Translation (SLT) aims to convert sign language videos into\nspoken or written text. While early systems relied on gloss annotations as an\nintermediate supervision, such annotations are costly to obtain and often fail\nto capture the full complexity of continuous signing. In this work, we propose\na two-phase, dual visual encoder framework for gloss-free SLT, leveraging\ncontrastive visual-language pretraining. During pretraining, our approach\nemploys two complementary visual backbones whose outputs are jointly aligned\nwith each other and with sentence-level text embeddings via a contrastive\nobjective. During the downstream SLT task, we fuse the visual features and\ninput them into an encoder-decoder model. On the Phoenix-2014T benchmark, our\ndual encoder architecture consistently outperforms its single stream variants\nand achieves the highest BLEU-4 score among existing gloss-free SLT approaches.", "comment": "Accepted at 9th Workshop on Sign Language Translation and Avatar\n  Technologies (SLTAT), will be held in conjunction with IVA'25", "pdf_url": "http://arxiv.org/pdf/2507.10306v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2408.09241", "title": "Re-boosting Self-Collaboration Parallel Prompt GAN for Unsupervised Image Restoration", "authors": ["Xin Lin", "Yuyan Zhou", "Jingtong Yue", "Chao Ren", "Kelvin C. K. Chan", "Lu Qi", "Ming-Hsuan Yang"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted in IEEE T-PAMI", "url": "http://arxiv.org/abs/2408.09241v2", "summary": "Unsupervised restoration approaches based on generative adversarial networks\n(GANs) offer a promising solution without requiring paired datasets. Yet, these\nGAN-based approaches struggle to surpass the performance of conventional\nunsupervised GAN-based frameworks without significantly modifying model\nstructures or increasing the computational complexity. To address these issues,\nwe propose a self-collaboration (SC) strategy for existing restoration models.\nThis strategy utilizes information from the previous stage as feedback to guide\nsubsequent stages, achieving significant performance improvement without\nincreasing the framework's inference complexity. The SC strategy comprises a\nprompt learning (PL) module and a restorer ($Res$). It iteratively replaces the\nprevious less powerful fixed restorer $\\overline{Res}$ in the PL module with a\nmore powerful $Res$. The enhanced PL module generates better\npseudo-degraded/clean image pairs, leading to a more powerful $Res$ for the\nnext iteration. Our SC can significantly improve the $Res$'s performance by\nover 1.5 dB without adding extra parameters or computational complexity during\ninference. Meanwhile, existing self-ensemble (SE) and our SC strategies enhance\nthe performance of pre-trained restorers from different perspectives. As SE\nincreases computational complexity during inference, we propose a re-boosting\nmodule to the SC (Reb-SC) to improve the SC strategy further by incorporating\nSE into SC without increasing inference time. This approach further enhances\nthe restorer's performance by approximately 0.3 dB. Extensive experimental\nresults on restoration tasks demonstrate that the proposed model performs\nfavorably against existing state-of-the-art unsupervised restoration methods.\nSource code and trained models are publicly available at:\nhttps://github.com/linxin0/RSCP2GAN.", "comment": "Accepted in IEEE T-PAMI", "pdf_url": "http://arxiv.org/pdf/2408.09241v2", "cate": "cs.CV", "date": "2024-08-17", "updated": "2025-07-14"}
{"id": "2507.10398", "title": "Devanagari Handwritten Character Recognition using Convolutional Neural Network", "authors": ["Diksha Mehta", "Prateek Mehta"], "categories": ["cs.CV", "cs.AI", "cs.CL", "14J60", "I.2.7; I.4; I.5; I.7.5"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      9 pages, 6 figures", "url": "http://arxiv.org/abs/2507.10398v1", "summary": "Handwritten character recognition is getting popular among researchers\nbecause of its possible applications in facilitating technological search\nengines, social media, recommender systems, etc. The Devanagari script is one\nof the oldest language scripts in India that does not have proper digitization\ntools. With the advancement of computing and technology, the task of this\nresearch is to extract handwritten Hindi characters from an image of Devanagari\nscript with an automated approach to save time and obsolete data. In this\npaper, we present a technique to recognize handwritten Devanagari characters\nusing two deep convolutional neural network layers. This work employs a\nmethodology that is useful to enhance the recognition rate and configures a\nconvolutional neural network for effective Devanagari handwritten text\nrecognition (DHTR). This approach uses the Devanagari handwritten character\ndataset (DHCD), an open dataset with 36 classes of Devanagari characters. Each\nof these classes has 1700 images for training and testing purposes. This\napproach obtains promising results in terms of accuracy by achieving 96.36%\naccuracy in testing and 99.55% in training time.", "comment": "9 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.10398v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10318", "title": "Mind the Gap: Aligning Vision Foundation Models to Image Feature Matching", "authors": ["Yuhan Liu", "Jingwen Fu", "Yang Wu", "Kangyi Wu", "Pengna Li", "Jiayi Wu", "Sanping Zhou", "Jingmin Xin"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2507.10318v1", "summary": "Leveraging the vision foundation models has emerged as a mainstream paradigm\nthat improves the performance of image feature matching. However, previous\nworks have ignored the misalignment when introducing the foundation models into\nfeature matching. The misalignment arises from the discrepancy between the\nfoundation models focusing on single-image understanding and the cross-image\nunderstanding requirement of feature matching. Specifically, 1) the embeddings\nderived from commonly used foundation models exhibit discrepancies with the\noptimal embeddings required for feature matching; 2) lacking an effective\nmechanism to leverage the single-image understanding ability into cross-image\nunderstanding. A significant consequence of the misalignment is they struggle\nwhen addressing multi-instance feature matching problems. To address this, we\nintroduce a simple but effective framework, called IMD (Image feature Matching\nwith a pre-trained Diffusion model) with two parts: 1) Unlike the dominant\nsolutions employing contrastive-learning based foundation models that emphasize\nglobal semantics, we integrate the generative-based diffusion models to\neffectively capture instance-level details. 2) We leverage the prompt mechanism\nin generative model as a natural tunnel, propose a novel cross-image\ninteraction prompting module to facilitate bidirectional information\ninteraction between image pairs. To more accurately measure the misalignment,\nwe propose a new benchmark called IMIM, which focuses on multi-instance\nscenarios. Our proposed IMD establishes a new state-of-the-art in commonly\nevaluated benchmarks, and the superior improvement 12% in IMIM indicates our\nmethod efficiently mitigates the misalignment.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.10318v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2408.09554", "title": "Screen Them All: High-Throughput Pan-Cancer Genetic and Phenotypic Biomarker Screening from H&E Whole Slide Images", "authors": ["Yi Kan Wang", "Ludmila Tydlitatova", "Jeremy D. Kunz", "Gerard Oakley", "Bonnie Kar Bo Chow", "Ran A. Godrich", "Matthew C. H. Lee", "Hamed Aghdam", "Alican Bozkurt", "Michal Zelechowski", "Chad Vanderbilt", "Christopher Kanan", "Juan A. Retamero", "Peter Hamilton", "Razik Yousfi", "Thomas J. Fuchs", "David S. Klimstra", "Siqi Liu"], "categories": ["q-bio.QM", "cs.CV", "eess.IV"], "primary_category": "Subjects:       Quantitative Methods (q-bio.QM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2408.09554v4", "summary": "Molecular assays are standard of care for detecting genomic alterations in\ncancer prognosis and therapy selection but are costly, tissue-destructive and\ntime-consuming. Artificial intelligence (AI) applied to routine hematoxylin and\neosin (H&E)-stained whole slide images (WSIs) offers a fast and economical\nalternative for screening molecular biomarkers. We introduce OmniScreen, a\nhigh-throughput AI-based system leveraging Virchow2 embeddings extracted from\n60,529 cancer patients with paired 489-gene MSK-IMPACT targeted biomarker panel\nand WSIs. Unlike conventional approaches that train separate models for each\nbiomarker, OmniScreen employs a unified model to predict a broad range of\nclinically relevant biomarkers across cancers, including low-prevalence targets\nimpractical to model individually. OmniScreen reliably identifies therapeutic\ntargets and shared phenotypic features across common and rare tumors. We\ninvestigate the biomarker prediction probabilities and accuracies of OmniScreen\nin relation to tumor area, cohort size, histologic subtype alignment, and\npathway-level morphological patterns. These findings underscore the potential\nof OmniScreen for routine clinical screening.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2408.09554v4", "cate": "q-bio.QM", "date": "2024-08-18", "updated": "2025-07-14"}
{"id": "2507.10409", "title": "Energy Efficiency in AI for 5G and Beyond: A DeepRx Case Study", "authors": ["Amine Lbath", "Ibtissam Labriji"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10409v1", "summary": "This study addresses the challenge of balancing energy efficiency with\nperformance in AI/ML models, focusing on DeepRX, a deep learning receiver based\non a fully convolutional ResNet architecture. We evaluate the energy\nconsumption of DeepRX, considering factors including FLOPs/Watt and\nFLOPs/clock, and find consistency between estimated and actual energy usage,\ninfluenced by memory access patterns. The research extends to comparing energy\ndynamics during training and inference phases. A key contribution is the\napplication of knowledge distillation (KD) to train a compact DeepRX\n\\textit{student} model that emulates the performance of the \\textit{teacher}\nmodel but with reduced energy consumption. We experiment with different student\nmodel sizes, optimal teacher sizes, and KD hyperparameters. Performance is\nmeasured by comparing the Bit Error Rate (BER) performance versus\nSignal-to-Interference \\& Noise Ratio (SINR) values of the distilled model and\na model trained from scratch. The distilled models demonstrate a lower error\nfloor across SINR levels, highlighting the effectiveness of KD in achieving\nenergy-efficient AI solutions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10409v1", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10340", "title": "Text Embedding Knows How to Quantize Text-Guided Diffusion Models", "authors": ["Hongjae Lee", "Myungjun Son", "Dongjea Kang", "Seung-Won Jung"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025", "url": "http://arxiv.org/abs/2507.10340v1", "summary": "Despite the success of diffusion models in image generation tasks such as\ntext-to-image, the enormous computational complexity of diffusion models limits\ntheir use in resource-constrained environments. To address this, network\nquantization has emerged as a promising solution for designing efficient\ndiffusion models. However, existing diffusion model quantization methods do not\nconsider input conditions, such as text prompts, as an essential source of\ninformation for quantization. In this paper, we propose a novel quantization\nmethod dubbed Quantization of Language-to-Image diffusion models using text\nPrompts (QLIP). QLIP leverages text prompts to guide the selection of bit\nprecision for every layer at each time step. In addition, QLIP can be\nseamlessly integrated into existing quantization methods to enhance\nquantization efficiency. Our extensive experiments demonstrate the\neffectiveness of QLIP in reducing computational complexity and improving the\nquality of the generated images across various datasets.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.10340v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2408.17339", "title": "Enhancing Underwater Imaging with 4-D Light Fields: Dataset and Method", "authors": ["Yuji Lin", "Junhui Hou", "Xianqiang Lyu", "Qian Zhao", "Deyu Meng"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      20 pages, 22 figures", "url": "http://arxiv.org/abs/2408.17339v2", "summary": "In this paper, we delve into the realm of 4-D light fields (LFs) to enhance\nunderwater imaging plagued by light absorption, scattering, and other\nchallenges. Contrasting with conventional 2-D RGB imaging, 4-D LF imaging\nexcels in capturing scenes from multiple perspectives, thereby indirectly\nembedding geometric information. This intrinsic property is anticipated to\neffectively address the challenges associated with underwater imaging. By\nleveraging both explicit and implicit depth cues present in 4-D LF images, we\npropose a progressive, mutually reinforcing framework for underwater 4-D LF\nimage enhancement and depth estimation. Specifically, our framework explicitly\nutilizes estimated depth information alongside implicit depth-related dynamic\nconvolutional kernels to modulate output features. The entire framework\ndecomposes this complex task, iteratively optimizing the enhanced image and\ndepth information to progressively achieve optimal enhancement results. More\nimportantly, we construct the first 4-D LF-based underwater image dataset for\nquantitative evaluation and supervised training of learning-based methods,\ncomprising 75 underwater scenes and 3675 high-resolution 2K pairs. To craft\nvibrant and varied underwater scenes, we build underwater environments with\nvarious objects and adopt several types of degradation. Through extensive\nexperimentation, we showcase the potential and superiority of 4-D LF-based\nunderwater imaging vis-a-vis traditional 2-D RGB-based approaches. Moreover,\nour method effectively corrects color bias and achieves state-of-the-art\nperformance. The dataset and code will be publicly available at\nhttps://github.com/linlos1234/LFUIE.", "comment": "20 pages, 22 figures", "pdf_url": "http://arxiv.org/pdf/2408.17339v2", "cate": "cs.CV", "date": "2024-08-30", "updated": "2025-07-12"}
{"id": "2507.10419", "title": "Multiple Choice Learning of Low Rank Adapters for Language Modeling", "authors": ["Victor Letzelter", "Hugo Malard", "Mathieu Fontaine", "Gaël Richard", "Slim Essid", "Andrei Bursuc", "Patrick Pérez"], "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10419v1", "summary": "We propose LoRA-MCL, a training scheme that extends next-token prediction in\nlanguage models with a method designed to decode diverse, plausible sentence\ncontinuations at inference time. Traditional language modeling is an\nintrinsically ill-posed problem: given a context, multiple futures may be\nequally plausible. Our approach leverages Multiple Choice Learning (MCL) and\nthe Winner-Takes-All (WTA) loss to efficiently handle ambiguity through\nLow-Rank Adaptation (LoRA). We provide a theoretical interpretation of applying\nMultiple Choice Learning to Language Modeling, assuming the data is generated\nfrom a mixture of distributions. To illustrate the proposed approach, we use\ndata sampled from mixtures of Markov chains. We then demonstrate with extensive\nexperiments on real-world visual and audio captioning tasks that our method\nachieves high diversity and relevance in generated outputs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10419v1", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10343", "title": "FGSSNet: Feature-Guided Semantic Segmentation of Real World Floorplans", "authors": ["Hugo Norrby", "Gabriel Färm", "Kevin Hernandez-Diaz", "Fernando Alonso-Fernandez"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at International Workshop on Artificial Intelligence and Pattern Recognition, IWAIPR 2025", "url": "http://arxiv.org/abs/2507.10343v1", "summary": "We introduce FGSSNet, a novel multi-headed feature-guided semantic\nsegmentation (FGSS) architecture designed to improve the generalization ability\nof wall segmentation on floorplans. FGSSNet features a U-Net segmentation\nbackbone with a multi-headed dedicated feature extractor used to extract\ndomain-specific feature maps which are injected into the latent space of U-Net\nto guide the segmentation process. This dedicated feature extractor is trained\nas an encoder-decoder with selected wall patches, representative of the walls\npresent in the input floorplan, to produce a compressed latent representation\nof wall patches while jointly trained to predict the wall width. In doing so,\nwe expect that the feature extractor encodes texture and width features of wall\npatches that are useful to guide the wall segmentation process. Our experiments\nshow increased performance by the use of such injected features in comparison\nto the vanilla U-Net, highlighting the validity of the proposed approach.", "comment": "Accepted at International Workshop on Artificial Intelligence and\n  Pattern Recognition, IWAIPR 2025", "pdf_url": "http://arxiv.org/pdf/2507.10343v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2410.09135", "title": "Enabling Advanced Land Cover Analytics: An Integrated Data Extraction Pipeline for Predictive Modeling with the Dynamic World Dataset", "authors": ["Victor Radermecker", "Andrea Zanon", "Nancy Thomas", "Annita Vapsi", "Saba Rahimi", "Rama Ramakrishnan", "Daniel Borrajo"], "categories": ["cs.CV", "cs.LG", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.09135v2", "summary": "Understanding land cover holds considerable potential for a myriad of\npractical applications, particularly as data accessibility transitions from\nbeing exclusive to governmental and commercial entities to now including the\nbroader research community. Nevertheless, although the data is accessible to\nany community member interested in exploration, there exists a formidable\nlearning curve and no standardized process for accessing, pre-processing, and\nleveraging the data for subsequent tasks. In this study, we democratize this\ndata by presenting a flexible and efficient end to end pipeline for working\nwith the Dynamic World dataset, a cutting-edge near-real-time land use/land\ncover (LULC) dataset. This includes a pre-processing and representation\nframework which tackles noise removal, efficient extraction of large amounts of\ndata, and re-representation of LULC data in a format well suited for several\ndownstream tasks. To demonstrate the power of our pipeline, we use it to\nextract data for an urbanization prediction problem and build a suite of\nmachine learning models with excellent performance. This task is easily\ngeneralizable to the prediction of any type of land cover and our pipeline is\nalso compatible with a series of other downstream tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.09135v2", "cate": "cs.CV", "date": "2024-10-11", "updated": "2025-07-14"}
{"id": "2507.10435", "title": "From Sequence to Structure: Uncovering Substructure Reasoning in Transformers", "authors": ["Xinnan Dai", "Kai Yang", "Jay Revolinsky", "Kai Guo", "Aoran Wang", "Bohang Zhang", "Jiliang Tang"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10435v1", "summary": "Recent studies suggest that large language models (LLMs) possess the\ncapability to solve graph reasoning tasks. Notably, even when graph structures\nare embedded within textual descriptions, LLMs can still effectively answer\nrelated questions. This raises a fundamental question: How can a decoder-only\nTransformer architecture understand underlying graph structures? To address\nthis, we start with the substructure extraction task, interpreting the inner\nmechanisms inside the transformers and analyzing the impact of the input\nqueries. Specifically, through both empirical results and theoretical analysis,\nwe present Induced Substructure Filtration (ISF), a perspective that captures\nthe substructure identification in the multi-layer transformers. We further\nvalidate the ISF process in LLMs, revealing consistent internal dynamics across\nlayers. Building on these insights, we explore the broader capabilities of\nTransformers in handling diverse graph types. Specifically, we introduce the\nconcept of thinking in substructures to efficiently extract complex composite\npatterns, and demonstrate that decoder-only Transformers can successfully\nextract substructures from attributed graphs, such as molecular graphs.\nTogether, our findings offer a new insight on how sequence-based Transformers\nperform the substructure extraction task over graph data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10435v1", "cate": "cs.CL", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2507.10355", "title": "Beyond Graph Model: Reliable VLM Fine-Tuning via Random Graph Adapter", "authors": ["Bo Jiang", "Xueyang Ze", "Beibei Wang", "Xixi Wang", "Xixi Wan", "Bin Luo"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10355v1", "summary": "Textual adapter-based tuning methods have shown significant potential in\ntransferring knowledge from pre-trained Vision-Language Models (VLMs) to\ndownstream tasks. Existing works generally employ the deterministic textual\nfeature adapter to refine each category textual representation. However, due to\ninherent factors such as different attributes and contexts, there exists\nsignificant diversity in textual descriptions for each category. Such\ndescription diversity offers rich discriminative semantic knowledge that can\nbenefit downstream visual learning tasks. Obviously, traditional deterministic\nadapter model cannot adequately capture this varied semantic information. Also,\nit is desirable to exploit the inter-class relationships in VLM adapter. To\naddress these issues, we propose to exploit random graph model into VLM adapter\nand develop a novel Vertex Random Graph Adapter (VRGAdapter). VRGAdapter first\nmodels the inherent diverse descriptions of each category and inter-class\nrelationships of different categories simultaneously by leveraging a Vertex\nRandom Knowledge Graph (VRKG) model. Then, it employs probabilistic message\npropagation on VRKG to learn context-aware distribution representation for each\nclass node. Finally, it adopts a reparameterized sampling function to achieve\ntextual adapter learning. Note that, VRGAdapter provides a more general adapter\nsolution that encompasses traditional graph-based adapter as a special case. In\naddition, to enable more robust performance for downstream tasks, we also\nintroduce a new Uncertainty-guided Multi-branch Fusion (UMF) scheme that\ndynamically integrates multiple pre-trained models for ensemble prediction.\nExtensive experiments on multiple benchmark datasets demonstrate the\neffectiveness of our approach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10355v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2505.12860", "title": "Towards a Universal Image Degradation Model via Content-Degradation Disentanglement", "authors": ["Wenbo Yang", "Zhongling Wang", "Zhou Wang"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.12860v2", "summary": "Image degradation synthesis is highly desirable in a wide variety of\napplications ranging from image restoration to simulating artistic effects.\nExisting models are designed to generate one specific or a narrow set of\ndegradations, which often require user-provided degradation parameters. As a\nresult, they lack the generalizability to synthesize degradations beyond their\ninitial design or adapt to other applications. Here we propose the first\nuniversal degradation model that can synthesize a broad spectrum of complex and\nrealistic degradations containing both homogeneous (global) and inhomogeneous\n(spatially varying) components. Our model automatically extracts and\ndisentangles homogeneous and inhomogeneous degradation features, which are\nlater used for degradation synthesis without user intervention. A\ndisentangle-by-compression method is proposed to separate degradation\ninformation from images. Two novel modules for extracting and incorporating\ninhomogeneous degradations are created to model inhomogeneous components in\ncomplex degradations. We demonstrate the model's accuracy and adaptability in\nfilm-grain simulation and blind image restoration tasks. The demo video, code,\nand dataset of this project will be released at\ngithub.com/yangwenbo99/content-degradation-disentanglement.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.12860v2", "cate": "cs.CV", "date": "2025-05-19", "updated": "2025-07-13"}
{"id": "2507.10442", "title": "Response Wide Shut? Surprising Observations in Basic Vision Language Model Capabilities", "authors": ["Shivam Chandhok", "Wan-Cyuan Fan", "Vered Shwartz", "Vineeth N Balasubramanian", "Leonid Sigal"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted at ACL 2025 (Main Conference)", "url": "http://arxiv.org/abs/2507.10442v1", "summary": "Vision-language Models (VLMs) have emerged as general-purpose tools for\naddressing a variety of complex computer vision problems. Such models have been\nshown to be highly capable, but, at the same time, lacking some basic visual\nunderstanding skills. In this paper, we set out to understand the limitations\nof SoTA VLMs on fundamental visual tasks by constructing a series of tests that\nprobe which components of design, specifically, may be lacking. Importantly, we\ngo significantly beyond the current benchmarks, which simply measure the final\nperformance of VLM response, by also comparing and contrasting it to the\nperformance of probes trained directly on features obtained from the visual\nencoder, intermediate vision-language projection and LLM-decoder output. In\ndoing so, we uncover shortcomings in VLMs and make a number of important\nobservations about their capabilities, robustness and how they process visual\ninformation. We hope our insights will guide progress in further improving\nVLMs.", "comment": "Accepted at ACL 2025 (Main Conference)", "pdf_url": "http://arxiv.org/pdf/2507.10442v1", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.10358", "title": "Fine-Grained Zero-Shot Object Detection", "authors": ["Hongxu Ma", "Chenbo Zhang", "Lu Zhang", "Jiaogen Zhou", "Jihong Guan", "Shuigeng Zhou"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ACM MM'25", "url": "http://arxiv.org/abs/2507.10358v1", "summary": "Zero-shot object detection (ZSD) aims to leverage semantic descriptions to\nlocalize and recognize objects of both seen and unseen classes. Existing ZSD\nworks are mainly coarse-grained object detection, where the classes are\nvisually quite different, thus are relatively easy to distinguish. However, in\nreal life we often have to face fine-grained object detection scenarios, where\nthe classes are too similar to be easily distinguished. For example, detecting\ndifferent kinds of birds, fishes, and flowers.\n  In this paper, we propose and solve a new problem called Fine-Grained\nZero-Shot Object Detection (FG-ZSD for short), which aims to detect objects of\ndifferent classes with minute differences in details under the ZSD paradigm. We\ndevelop an effective method called MSHC for the FG-ZSD task, which is based on\nan improved two-stage detector and employs a multi-level semantics-aware\nembedding alignment loss, ensuring tight coupling between the visual and\nsemantic spaces. Considering that existing ZSD datasets are not suitable for\nthe new FG-ZSD task, we build the first FG-ZSD benchmark dataset FGZSD-Birds,\nwhich contains 148,820 images falling into 36 orders, 140 families, 579 genera\nand 1432 species. Extensive experiments on FGZSD-Birds show that our method\noutperforms existing ZSD models.", "comment": "Accepted by ACM MM'25", "pdf_url": "http://arxiv.org/pdf/2507.10358v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10445", "title": "Referential ambiguity and clarification requests: comparing human and LLM behaviour", "authors": ["Chris Madge", "Matthew Purver", "Massimo Poesio"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10445v1", "summary": "In this work we examine LLMs' ability to ask clarification questions in\ntask-oriented dialogues that follow the asynchronous\ninstruction-giver/instruction-follower format. We present a new corpus that\ncombines two existing annotations of the Minecraft Dialogue Corpus -- one for\nreference and ambiguity in reference, and one for SDRT including clarifications\n-- into a single common format providing the necessary information to\nexperiment with clarifications and their relation to ambiguity. With this\ncorpus we compare LLM actions with original human-generated clarification\nquestions, examining how both humans and LLMs act in the case of ambiguity. We\nfind that there is only a weak link between ambiguity and humans producing\nclarification questions in these dialogues, and low correlation between humans\nand LLMs. Humans hardly ever produce clarification questions for referential\nambiguity, but often do so for task-based uncertainty. Conversely, LLMs produce\nmore clarification questions for referential ambiguity, but less so for task\nuncertainty. We question if LLMs' ability to ask clarification questions is\npredicated on their recent ability to simulate reasoning, and test this with\ndifferent reasoning approaches, finding that reasoning does appear to increase\nquestion frequency and relevancy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10445v1", "cate": "cs.CL", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10375", "title": "Test-Time Canonicalization by Foundation Models for Robust Perception", "authors": ["Utkarsh Singhal", "Ryan Feng", "Stella X. Yu", "Atul Prakash"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Published at ICML 2025", "url": "http://arxiv.org/abs/2507.10375v1", "summary": "Real-world visual perception requires invariance to diverse transformations,\nyet current methods rely heavily on specialized architectures or training on\npredefined augmentations, limiting generalization. We propose FOCAL, a\ntest-time, data-driven framework that achieves robust perception by leveraging\ninternet-scale visual priors from foundation models. By generating and\noptimizing candidate transformations toward visually typical, \"canonical\"\nviews, FOCAL enhances robustness without re-training or architectural changes.\nOur experiments demonstrate improved robustness of CLIP and SAM across\nchallenging transformations, including 2D/3D rotations, illumination shifts\n(contrast and color), and day-night variations. We also highlight potential\napplications in active vision. Our approach challenges the assumption that\ntransform-specific training is necessary, instead offering a scalable path to\ninvariance. Our code is available at: https://github.com/sutkarsh/focal.", "comment": "Published at ICML 2025", "pdf_url": "http://arxiv.org/pdf/2507.10375v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10449", "title": "CoralVQA: A Large-Scale Visual Question Answering Dataset for Coral Reef Image Understanding", "authors": ["Hongyong Han", "Wei Wang", "Gaowei Zhang", "Mingjie Li", "Yi Wang"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10449v1", "summary": "Coral reefs are vital yet vulnerable ecosystems that require continuous\nmonitoring to support conservation. While coral reef images provide essential\ninformation in coral monitoring, interpreting such images remains challenging\ndue to the need for domain expertise. Visual Question Answering (VQA), powered\nby Large Vision-Language Models (LVLMs), has great potential in user-friendly\ninteraction with coral reef images. However, applying VQA to coral imagery\ndemands a dedicated dataset that addresses two key challenges: domain-specific\nannotations and multidimensional questions. In this work, we introduce\nCoralVQA, the first large-scale VQA dataset for coral reef analysis. It\ncontains 12,805 real-world coral images from 67 coral genera collected from 3\noceans, along with 277,653 question-answer pairs that comprehensively assess\necological and health-related conditions. To construct this dataset, we develop\na semi-automatic data construction pipeline in collaboration with marine\nbiologists to ensure both scalability and professional-grade data quality.\nCoralVQA presents novel challenges and provides a comprehensive benchmark for\nstudying vision-language reasoning in the context of coral reef images. By\nevaluating several state-of-the-art LVLMs, we reveal key limitations and\nopportunities. These insights form a foundation for future LVLM development,\nwith a particular emphasis on supporting coral conservation efforts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10449v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10381", "title": "Improving Remote Sensing Classification using Topological Data Analysis and Convolutional Neural Networks", "authors": ["Aaryam Sharma"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      9 pages, 8 figures", "url": "http://arxiv.org/abs/2507.10381v1", "summary": "Topological data analysis (TDA) is a relatively new field that is gaining\nrapid adoption due to its robustness and ability to effectively describe\ncomplex datasets by quantifying geometric information. In imaging contexts, TDA\ntypically models data as filtered cubical complexes from which we can extract\ndiscriminative features using persistence homology. Meanwhile, convolutional\nneural networks (CNNs) have been shown to be biased towards texture based local\nfeatures. To address this limitation, we propose a TDA feature engineering\npipeline and a simple method to integrate topological features with deep\nlearning models on remote sensing classification. Our method improves the\nperformance of a ResNet18 model on the EuroSAT dataset by 1.44% achieving\n99.33% accuracy, which surpasses all previously reported single-model\naccuracies, including those with larger architectures, such as ResNet50 (2x\nlarger) and XL Vision Transformers (197x larger). We additionally show that our\nmethod's accuracy is 1.82% higher than our ResNet18 baseline on the RESISC45\ndataset. To our knowledge, this is the first application of TDA features in\nsatellite scene classification with deep learning. This demonstrates that TDA\nfeatures can be integrated with deep learning models, even on datasets without\nexplicit topological structures, thereby increasing the applicability of TDA. A\nclean implementation of our method will be made publicly available upon\npublication.", "comment": "9 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.10381v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10475", "title": "Can You Detect the Difference?", "authors": ["İsmail Tarım", "Aytuğ Onan"], "categories": ["cs.CL", "cs.AI", "I.2.7; H.3.3"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      11 pages, 3 figures, 2 tables. Code and data: this https URL . Cross-list requested to cs.AI for AI-safety relevance", "url": "http://arxiv.org/abs/2507.10475v1", "summary": "The rapid advancement of large language models (LLMs) has raised concerns\nabout reliably detecting AI-generated text. Stylometric metrics work well on\nautoregressive (AR) outputs, but their effectiveness on diffusion-based models\nis unknown. We present the first systematic comparison of diffusion-generated\ntext (LLaDA) and AR-generated text (LLaMA) using 2 000 samples. Perplexity,\nburstiness, lexical diversity, readability, and BLEU/ROUGE scores show that\nLLaDA closely mimics human text in perplexity and burstiness, yielding high\nfalse-negative rates for AR-oriented detectors. LLaMA shows much lower\nperplexity but reduced lexical fidelity. Relying on any single metric fails to\nseparate diffusion outputs from human writing. We highlight the need for\ndiffusion-aware detectors and outline directions such as hybrid models,\ndiffusion-specific stylometric signatures, and robust watermarking.", "comment": "11 pages, 3 figures, 2 tables. Code and data:\n  https://github.com/ismailtrm/ceng_404. Cross-list requested to cs.AI for\n  AI-safety relevance", "pdf_url": "http://arxiv.org/pdf/2507.10475v1", "cate": "cs.CL", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10432", "title": "Text-Visual Semantic Constrained AI-Generated Image Quality Assessment", "authors": ["Qiang Li", "Qingsen Yan", "Haojian Huang", "Peng Wu", "Haokui Zhang", "Yanning Zhang"], "categories": ["cs.CV", "I.4.7"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      9 pages, 5 figures, Accepted at ACMMM 2025", "url": "http://arxiv.org/abs/2507.10432v1", "summary": "With the rapid advancements in Artificial Intelligence Generated Image (AGI)\ntechnology, the accurate assessment of their quality has become an increasingly\nvital requirement. Prevailing methods typically rely on cross-modal models like\nCLIP or BLIP to evaluate text-image alignment and visual quality. However, when\napplied to AGIs, these methods encounter two primary challenges: semantic\nmisalignment and details perception missing. To address these limitations, we\npropose Text-Visual Semantic Constrained AI-Generated Image Quality Assessment\n(SC-AGIQA), a unified framework that leverages text-visual semantic constraints\nto significantly enhance the comprehensive evaluation of both text-image\nconsistency and perceptual distortion in AI-generated images. Our approach\nintegrates key capabilities from multiple models and tackles the aforementioned\nchallenges by introducing two core modules: the Text-assisted Semantic\nAlignment Module (TSAM), which leverages Multimodal Large Language Models\n(MLLMs) to bridge the semantic gap by generating an image description and\ncomparing it against the original prompt for a refined consistency check, and\nthe Frequency-domain Fine-Grained Degradation Perception Module (FFDPM), which\ndraws inspiration from Human Visual System (HVS) properties by employing\nfrequency domain analysis combined with perceptual sensitivity weighting to\nbetter quantify subtle visual distortions and enhance the capture of\nfine-grained visual quality details in images. Extensive experiments conducted\non multiple benchmark datasets demonstrate that SC-AGIQA outperforms existing\nstate-of-the-art methods. The code is publicly available at\nhttps://github.com/mozhu1/SC-AGIQA.", "comment": "9 pages, 5 figures, Accepted at ACMMM 2025", "pdf_url": "http://arxiv.org/pdf/2507.10432v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10492", "title": "BenchReAD: A systematic benchmark for retinal anomaly detection", "authors": ["Chenyu Lian", "Hong-Yu Zhou", "Zhanli Hu", "Jing Qin"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      MICCAI 2025", "url": "http://arxiv.org/abs/2507.10492v1", "summary": "Retinal anomaly detection plays a pivotal role in screening ocular and\nsystemic diseases. Despite its significance, progress in the field has been\nhindered by the absence of a comprehensive and publicly available benchmark,\nwhich is essential for the fair evaluation and advancement of methodologies.\nDue to this limitation, previous anomaly detection work related to retinal\nimages has been constrained by (1) a limited and overly simplistic set of\nanomaly types, (2) test sets that are nearly saturated, and (3) a lack of\ngeneralization evaluation, resulting in less convincing experimental setups.\nFurthermore, existing benchmarks in medical anomaly detection predominantly\nfocus on one-class supervised approaches (training only with negative samples),\noverlooking the vast amounts of labeled abnormal data and unlabeled data that\nare commonly available in clinical practice. To bridge these gaps, we introduce\na benchmark for retinal anomaly detection, which is comprehensive and\nsystematic in terms of data and algorithm. Through categorizing and\nbenchmarking previous methods, we find that a fully supervised approach\nleveraging disentangled representations of abnormalities (DRA) achieves the\nbest performance but suffers from significant drops in performance when\nencountering certain unseen anomalies. Inspired by the memory bank mechanisms\nin one-class supervised learning, we propose NFM-DRA, which integrates DRA with\na Normal Feature Memory to mitigate the performance degradation, establishing a\nnew SOTA. The benchmark is publicly available at\nhttps://github.com/DopamineLcy/BenchReAD.", "comment": "MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.10492v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10437", "title": "4D-Animal: Freely Reconstructing Animatable 3D Animals from Videos", "authors": ["Shanshan Zhong", "Jiawei Peng", "Zehan Zheng", "Zhongzhan Huang", "Wufei Ma", "Guofeng Zhang", "Qihao Liu", "Alan Yuille", "Jieneng Chen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10437v1", "summary": "Existing methods for reconstructing animatable 3D animals from videos\ntypically rely on sparse semantic keypoints to fit parametric models. However,\nobtaining such keypoints is labor-intensive, and keypoint detectors trained on\nlimited animal data are often unreliable. To address this, we propose\n4D-Animal, a novel framework that reconstructs animatable 3D animals from\nvideos without requiring sparse keypoint annotations. Our approach introduces a\ndense feature network that maps 2D representations to SMAL parameters,\nenhancing both the efficiency and stability of the fitting process.\nFurthermore, we develop a hierarchical alignment strategy that integrates\nsilhouette, part-level, pixel-level, and temporal cues from pre-trained 2D\nvisual models to produce accurate and temporally coherent reconstructions\nacross frames. Extensive experiments demonstrate that 4D-Animal outperforms\nboth model-based and model-free baselines. Moreover, the high-quality 3D assets\ngenerated by our method can benefit other 3D tasks, underscoring its potential\nfor large-scale applications. The code is released at\nhttps://github.com/zhongshsh/4D-Animal.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10437v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10496", "title": "Cameras as Relative Positional Encoding", "authors": ["Ruilong Li", "Brent Yi", "Junchen Liu", "Hang Gao", "Yi Ma", "Angjoo Kanazawa"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project Page: this https URL", "url": "http://arxiv.org/abs/2507.10496v1", "summary": "Transformers are increasingly prevalent for multi-view computer vision tasks,\nwhere geometric relationships between viewpoints are critical for 3D\nperception. To leverage these relationships, multi-view transformers must use\ncamera geometry to ground visual tokens in 3D space. In this work, we compare\ntechniques for conditioning transformers on cameras: token-level raymap\nencodings, attention-level relative pose encodings, and a new relative encoding\nwe propose -- Projective Positional Encoding (PRoPE) -- that captures complete\ncamera frustums, both intrinsics and extrinsics, as a relative positional\nencoding. Our experiments begin by showing how relative camera conditioning\nimproves performance in feedforward novel view synthesis, with further gains\nfrom PRoPE. This holds across settings: scenes with both shared and varying\nintrinsics, when combining token- and attention-level conditioning, and for\ngeneralization to inputs with out-of-distribution sequence lengths and camera\nintrinsics. We then verify that these benefits persist for different tasks,\nstereo depth estimation and discriminative spatial cognition, as well as larger\nmodel sizes.", "comment": "Project Page: https://www.liruilong.cn/prope/", "pdf_url": "http://arxiv.org/pdf/2507.10496v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10470", "title": "RefSTAR: Blind Facial Image Restoration with Reference Selection, Transfer, and Reconstruction", "authors": ["Zhicun Yin", "Junjie Chen", "Ming Liu", "Zhixin Wang", "Fan Li", "Renjing Pei", "Xiaoming Li", "Rynson W. H. Lau", "Wangmeng Zuo"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10470v1", "summary": "Blind facial image restoration is highly challenging due to unknown complex\ndegradations and the sensitivity of humans to faces. Although existing methods\nintroduce auxiliary information from generative priors or high-quality\nreference images, they still struggle with identity preservation problems,\nmainly due to improper feature introduction on detailed textures. In this\npaper, we focus on effectively incorporating appropriate features from\nhigh-quality reference images, presenting a novel blind facial image\nrestoration method that considers reference selection, transfer, and\nreconstruction (RefSTAR). In terms of selection, we construct a reference\nselection (RefSel) module. For training the RefSel module, we construct a\nRefSel-HQ dataset through a mask generation pipeline, which contains annotating\nmasks for 10,000 ground truth-reference pairs. As for the transfer, due to the\ntrivial solution in vanilla cross-attention operations, a feature fusion\nparadigm is designed to force the features from the reference to be integrated.\nFinally, we propose a reference image reconstruction mechanism that further\nensures the presence of reference image features in the output image. The cycle\nconsistency loss is also redesigned in conjunction with the mask. Extensive\nexperiments on various backbone models demonstrate superior performance,\nshowing better identity preservation ability and reference feature transfer\nquality. Source code, dataset, and pre-trained models are available at\nhttps://github.com/yinzhicun/RefSTAR.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10470v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10502", "title": "Benchmarking and Evaluation of AI Models in Biology: Outcomes and Recommendations from the CZI Virtual Cells Workshop", "authors": ["Elizabeth Fahsbender", "Alma Andersson", "Jeremy Ash", "Polina Binder", "Daniel Burkhardt", "Benjamin Chang", "Georg K. Gerber", "Anthony Gitter", "Patrick Godau", "Ankit Gupta", "Genevieve Haliburton", "Siyu He", "Trey Ideker", "Ivana Jelic", "Aly Khan", "Yang-Joon Kim", "Aditi Krishnapriyan", "Jon M. Laurent", "Tianyu Liu 28", "Emma Lundberg", "Shalin B. Mehta", "Rob Moccia", "Angela Oliveira Pisco", "Katherine S. Pollard", "Suresh Ramani", "Julio Saez-Rodriguez", "Yasin Senbabaoglu", "Elana Simon", "Srinivasan Sivanandan", "Gustavo Stolovitzky", "Marc Valer", "Bo Wang", "Xikun Zhang", "James Zou", "Katrina Kalantar"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10502v1", "summary": "Artificial intelligence holds immense promise for transforming biology, yet a\nlack of standardized, cross domain, benchmarks undermines our ability to build\nrobust, trustworthy models. Here, we present insights from a recent workshop\nthat convened machine learning and computational biology experts across\nimaging, transcriptomics, proteomics, and genomics to tackle this gap. We\nidentify major technical and systemic bottlenecks such as data heterogeneity\nand noise, reproducibility challenges, biases, and the fragmented ecosystem of\npublicly available resources and propose a set of recommendations for building\nbenchmarking frameworks that can efficiently compare ML models of biological\nsystems across tasks and data modalities. By promoting high quality data\ncuration, standardized tooling, comprehensive evaluation metrics, and open,\ncollaborative platforms, we aim to accelerate the development of robust\nbenchmarks for AI driven Virtual Cells. These benchmarks are crucial for\nensuring rigor, reproducibility, and biological relevance, and will ultimately\nadvance the field toward integrated models that drive new discoveries,\ntherapeutic insights, and a deeper understanding of cellular systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10502v1", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10473", "title": "GT-Loc: Unifying When and Where in Images Through a Joint Embedding Space", "authors": ["David G. Shatwell", "Ishan Rajendrakumar Dave", "Sirnam Swetha", "Mubarak Shah"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted in ICCV2025", "url": "http://arxiv.org/abs/2507.10473v1", "summary": "Timestamp prediction aims to determine when an image was captured using only\nvisual information, supporting applications such as metadata correction,\nretrieval, and digital forensics. In outdoor scenarios, hourly estimates rely\non cues like brightness, hue, and shadow positioning, while seasonal changes\nand weather inform date estimation. However, these visual cues significantly\ndepend on geographic context, closely linking timestamp prediction to\ngeo-localization. To address this interdependence, we introduce GT-Loc, a novel\nretrieval-based method that jointly predicts the capture time (hour and month)\nand geo-location (GPS coordinates) of an image. Our approach employs separate\nencoders for images, time, and location, aligning their embeddings within a\nshared high-dimensional feature space. Recognizing the cyclical nature of time,\ninstead of conventional contrastive learning with hard positives and negatives,\nwe propose a temporal metric-learning objective providing soft targets by\nmodeling pairwise time differences over a cyclical toroidal surface. We present\nnew benchmarks demonstrating that our joint optimization surpasses previous\ntime prediction methods, even those using the ground-truth geo-location as an\ninput during inference. Additionally, our approach achieves competitive results\non standard geo-localization tasks, and the unified embedding space facilitates\ncompositional and text-based image retrieval.", "comment": "Accepted in ICCV2025", "pdf_url": "http://arxiv.org/pdf/2507.10473v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10530", "title": "Accurate generation of chemical reaction transition states by conditional flow matching", "authors": ["Ping Tuo", "Jiale Chen", "Ju Li"], "categories": ["physics.chem-ph", "cs.AI"], "primary_category": "Subjects:       Chemical Physics (physics.chem-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10530v1", "summary": "Transition state (TS) structures define the critical geometries and energy\nbarriers underlying chemical reactivity, yet their fleeting nature renders them\nexperimentally elusive and drives the reliance on costly, high-throughput\ndensity functional theory (DFT) calculations. Here, we introduce TS-GEN, a\nconditional flow-matching generative model that maps samples from a simple\nGaussian prior directly to transition-state saddle-point geometries in a\nsingle, deterministic pass. By embedding both reactant and product\nconformations as conditioning information, TS-GEN learns to transport latent\nnoise to true TS structures via an optimal-transport path, effectively\nreplacing the iterative optimization common in nudged-elastic band or\nstring-method algorithms. TS-GEN delivers unprecedented accuracy, achieving a\nroot-mean-square deviation of $0.004\\ \\rm{\\mathring{A}}$ (vs. $0.103\\\n\\rm{\\mathring{A}}$ for prior state-of-the-art) and a mean barrier-height error\nof $1.019\\ {\\rm kcal/mol}$ (vs. $2.864\\ {\\rm kcal/mol}$), while requiring only\n$0.06\\ {\\rm s}$ GPU time per inference. Over 87% of generated TSs meet\nchemical-accuracy criteria ($<1.58\\ {\\rm kcal/mol}$ error), substantially\noutpacing existing methods. TS-GEN also exhibits strong transferability to\nout-of-distribution reactions from a larger database. By uniting sub-angstrom\nprecision, sub-second speed, and broad applicability, TS-GEN will be highly\nuseful for high-throughput exploration of complex reaction networks, paving the\nway to the exploration of novel chemical reaction mechanisms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10530v1", "cate": "physics.chem-ph", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10490", "title": "The Power of Certainty: How Confident Models Lead to Better Segmentation", "authors": ["Tugberk Erol", "Tuba Caglikantar", "Duygu Sarikaya"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      9 pages, 3 figures", "url": "http://arxiv.org/abs/2507.10490v1", "summary": "Deep learning models have been proposed for automatic polyp detection and\nprecise segmentation of polyps during colonoscopy procedures. Although these\nstate-of-the-art models achieve high performance, they often require a large\nnumber of parameters. Their complexity can make them prone to overfitting,\nparticularly when trained on biased datasets, and can result in poor\ngeneralization across diverse datasets. Knowledge distillation and\nself-distillation are proposed as promising strategies to mitigate the\nlimitations of large, over-parameterized models. These approaches, however, are\nresource-intensive, often requiring multiple models and significant memory\nduring training. We propose a confidence-based self-distillation approach that\noutperforms state-of-the-art models by utilizing only previous iteration data\nstorage during training, without requiring extra computation or memory usage\nduring testing. Our approach calculates the loss between the previous and\ncurrent iterations within a batch using a dynamic confidence coefficient. To\nevaluate the effectiveness of our approach, we conduct comprehensive\nexperiments on the task of polyp segmentation. Our approach outperforms\nstate-of-the-art models and generalizes well across datasets collected from\nmultiple clinical centers. The code will be released to the public once the\npaper is accepted.", "comment": "9 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2507.10490v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10532", "title": "Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination", "authors": ["Mingqi Wu", "Zhihao Zhang", "Qiaole Dong", "Zhiheng Xi", "Jun Zhao", "Senjie Jin", "Xiaoran Fan", "Yuhao Zhou", "Yanwei Fu", "Qin Liu", "Songyang Zhang", "Qi Zhang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      26 pages", "url": "http://arxiv.org/abs/2507.10532v1", "summary": "The reasoning capabilities of large language models (LLMs) have been a\nlongstanding focus of research. Recent works have further enhanced these\ncapabilities using reinforcement learning (RL), with many new methods claiming\nsignificant improvements with minimal or no external supervision. Surprisingly,\nsome studies even suggest that random or incorrect reward signals can enhance\nreasoning performance. However, these breakthroughs are mostly reported on the\nQwen2.5 model family and evaluated on well-known benchmarks such as MATH-500,\nAMC, and AIME, while failing to achieve similar gains on other models like\nLlama, which warrants further investigation. Our analysis shows that although\nQwen2.5 achieves strong mathematical reasoning performance, its pretraining on\nlarge-scale web corpora makes it vulnerable to data contamination in popular\nbenchmarks. As a result, results derived from these benchmarks may be\nunreliable. To address this, we introduce a generator that produces fully\nsynthetic arithmetic problems of arbitrary length and difficulty, yielding a\nclean dataset we call RandomCalculation. Using these leakage-free datasets, we\nshow that only accurate reward signals consistently improve performance, while\nnoisy or incorrect signals do not. We advocate for evaluating RL methods on\nuncontaminated benchmarks and across diverse model families to ensure\ntrustworthy conclusions.", "comment": "26 pages", "pdf_url": "http://arxiv.org/pdf/2507.10532v1", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10499", "title": "National level satellite-based crop field inventories in smallholder landscapes", "authors": ["Philippe Rufin", "Pauline Lucie Hammer", "Leon-Friedrich Thomas", "Sá Nogueira Lisboa", "Natasha Ribeiro", "Almeida Sitoe", "Patrick Hostert", "Patrick Meyfroidt"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10499v1", "summary": "The design of science-based policies to improve the sustainability of\nsmallholder agriculture is challenged by a limited understanding of fundamental\nsystem properties, such as the spatial distribution of active cropland and\nfield size. We integrate very high spatial resolution (1.5 m) Earth observation\ndata and deep transfer learning to derive crop field delineations in complex\nagricultural systems at the national scale, while maintaining minimum reference\ndata requirements and enhancing transferability. We provide the first\nnational-level dataset of 21 million individual fields for Mozambique (covering\n~800,000 km2) for 2023. Our maps separate active cropland from non-agricultural\nland use with an overall accuracy of 93% and balanced omission and commission\nerrors. Field-level spatial agreement reached median intersection over union\n(IoU) scores of 0.81, advancing the state-of-the-art in large-area field\ndelineation in complex smallholder systems. The active cropland maps capture\nfragmented rural regions with low cropland shares not yet identified in global\nland cover or cropland maps. These regions are mostly located in agricultural\nfrontier regions which host 7-9% of the Mozambican population. Field size in\nMozambique is very low overall, with half of the fields being smaller than 0.16\nha, and 83% smaller than 0.5 ha. Mean field size at aggregate spatial\nresolution (0.05{\\deg}) is 0.32 ha, but it varies strongly across gradients of\naccessibility, population density, and net forest cover change. This variation\nreflects a diverse set of actors, ranging from semi-subsistence smallholder\nfarms to medium-scale commercial farming, and large-scale farming operations.\nOur results highlight that field size is a key indicator relating to\nsocio-economic and environmental outcomes of agriculture (e.g., food\nproduction, livelihoods, deforestation, biodiversity), as well as their\ntrade-offs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10499v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10546", "title": "Disentangling Neural Disjunctive Normal Form Models", "authors": ["Kexin Gu Baugh", "Vincent Perreault", "Matthew Baugh", "Luke Dickens", "Katsumi Inoue", "Alessandra Russo"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted at NeSy 2025", "url": "http://arxiv.org/abs/2507.10546v1", "summary": "Neural Disjunctive Normal Form (DNF) based models are powerful and\ninterpretable approaches to neuro-symbolic learning and have shown promising\nresults in classification and reinforcement learning settings without prior\nknowledge of the tasks. However, their performance is degraded by the\nthresholding of the post-training symbolic translation process. We show here\nthat part of the performance degradation during translation is due to its\nfailure to disentangle the learned knowledge represented in the form of the\nnetworks' weights. We address this issue by proposing a new disentanglement\nmethod; by splitting nodes that encode nested rules into smaller independent\nnodes, we are able to better preserve the models' performance. Through\nexperiments on binary, multiclass, and multilabel classification tasks\n(including those requiring predicate invention), we demonstrate that our\ndisentanglement method provides compact and interpretable logical\nrepresentations for the neural DNF-based models, with performance closer to\nthat of their pre-translation counterparts. Our code is available at\nhttps://github.com/kittykg/disentangling-ndnf-classification.", "comment": "Accepted at NeSy 2025", "pdf_url": "http://arxiv.org/pdf/2507.10546v1", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10547", "title": "Quantize-then-Rectify: Efficient VQ-VAE Training", "authors": ["Borui Zhang", "Qihang Rao", "Wenzhao Zheng", "Jie Zhou", "Jiwen Lu"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10547v1", "summary": "Visual tokenizers are pivotal in multimodal large models, acting as bridges\nbetween continuous inputs and discrete tokens. Nevertheless, training\nhigh-compression-rate VQ-VAEs remains computationally demanding, often\nnecessitating thousands of GPU hours. This work demonstrates that a pre-trained\nVAE can be efficiently transformed into a VQ-VAE by controlling quantization\nnoise within the VAE's tolerance threshold. We present\n\\textbf{Quantize-then-Rectify (ReVQ)}, a framework leveraging pre-trained VAEs\nto enable rapid VQ-VAE training with minimal computational overhead. By\nintegrating \\textbf{channel multi-group quantization} to enlarge codebook\ncapacity and a \\textbf{post rectifier} to mitigate quantization errors, ReVQ\ncompresses ImageNet images into at most 512 tokens while sustaining competitive\nreconstruction quality (rFID = 1.06). Significantly, ReVQ reduces training\ncosts by over two orders of magnitude relative to state-of-the-art approaches:\nReVQ finishes full training on a single NVIDIA 4090 in approximately 22 hours,\nwhereas comparable methods require 4.5 days on 32 A100 GPUs. Experimental\nresults show that ReVQ achieves superior efficiency-reconstruction trade-offs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10547v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10548", "title": "EmbRACE-3K: Embodied Reasoning and Action in Complex Environments", "authors": ["Mingxian Lin", "Wei Huang", "Yitang Li", "Chengjie Jiang", "Kui Wu", "Fangwei Zhong", "Shengju Qian", "Xin Wang", "Xiaojuan Qi"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project page: this https URL", "url": "http://arxiv.org/abs/2507.10548v1", "summary": "Recent advanced vision-language models(VLMs) have demonstrated strong\nperformance on passive, offline image and video understanding tasks. However,\ntheir effectiveness in embodied settings, which require online interaction and\nactive scene understanding remains limited. In such scenarios, an agent\nperceives the environment from a first-person perspective, with each action\ndynamically shaping subsequent observations. Even state-of-the-art models such\nas GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment\ninteractions, exhibiting clear limitations in spatial reasoning and\nlong-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset\nof over 3,000 language-guided tasks situated in diverse, photorealistic\nenvironments constructed using Unreal Engine and the UnrealCV-Zoo framework.\nThe tasks encompass a wide range of embodied challenges, including navigation,\nobject manipulation, and multi-stage goal execution. Each task unfolds as a\nmulti-step trajectory, pairing first-person visual observations with high-level\ninstructions, grounded actions, and natural language rationales that express\nthe agent's intent at every step. Using EmRACE-3K, we establish a benchmark to\nevaluate the embodied reasoning capabilities of VLMs across three key\ndimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage\nGoal Execution. In zero-shot settings, all models achieve success rates below\n20%, underscoring the challenge posed by our benchmark and the current\nlimitations of VLMs in interactive environments. To demonstrate the utility of\nEmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning\nfollowed by reinforcement learning. This approach yields substantial\nimprovements across all three challenge categories, highlighting the dataset's\neffectiveness in enabling the development of embodied reasoning capabilities.", "comment": "Project page: https://mxllc.github.io/EmbRACE-3K/", "pdf_url": "http://arxiv.org/pdf/2507.10548v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2507.10552", "title": "Self-supervised Learning on Camera Trap Footage Yields a Strong Universal Face Embedder", "authors": ["Vladimir Iashin", "Horace Lee", "Dan Schofield", "Andrew Zisserman"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted for publication. Project page, code and weights: this https URL", "url": "http://arxiv.org/abs/2507.10552v1", "summary": "Camera traps are revolutionising wildlife monitoring by capturing vast\namounts of visual data; however, the manual identification of individual\nanimals remains a significant bottleneck. This study introduces a fully\nself-supervised approach to learning robust chimpanzee face embeddings from\nunlabeled camera-trap footage. Leveraging the DINOv2 framework, we train Vision\nTransformers on automatically mined face crops, eliminating the need for\nidentity labels. Our method demonstrates strong open-set re-identification\nperformance, surpassing supervised baselines on challenging benchmarks such as\nBossou, despite utilising no labelled data during training. This work\nunderscores the potential of self-supervised learning in biodiversity\nmonitoring and paves the way for scalable, non-invasive population studies.", "comment": "Accepted for publication. Project page, code and weights:\n  https://www.robots.ox.ac.uk/~vgg/research/ChimpUFE/", "pdf_url": "http://arxiv.org/pdf/2507.10552v1", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2405.20725", "title": "GI-NAS: Boosting Gradient Inversion Attacks through Adaptive Neural Architecture Search", "authors": ["Wenbo Yu", "Hao Fang", "Bin Chen", "Xiaohang Sui", "Chuan Chen", "Hao Wu", "Shu-Tao Xia", "Ke Xu"], "categories": ["cs.AI", "cs.CV"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      accepted by IEEE Transactions on Information Forensics and Security (TIFS)", "url": "http://arxiv.org/abs/2405.20725v3", "summary": "Gradient Inversion Attacks invert the transmitted gradients in Federated\nLearning (FL) systems to reconstruct the sensitive data of local clients and\nhave raised considerable privacy concerns. A majority of gradient inversion\nmethods rely heavily on explicit prior knowledge (e.g., a well pre-trained\ngenerative model), which is often unavailable in realistic scenarios. This is\nbecause real-world client data distributions are often highly heterogeneous,\ndomain-specific, and unavailable to attackers, making it impractical for\nattackers to obtain perfectly matched pre-trained models, which inevitably\nsuffer from fundamental distribution shifts relative to target private data. To\nalleviate this issue, researchers have proposed to leverage the implicit prior\nknowledge of an over-parameterized network. However, they only utilize a fixed\nneural architecture for all the attack settings. This would hinder the adaptive\nuse of implicit architectural priors and consequently limit the\ngeneralizability. In this paper, we further exploit such implicit prior\nknowledge by proposing Gradient Inversion via Neural Architecture Search\n(GI-NAS), which adaptively searches the network and captures the implicit\npriors behind neural architectures. Extensive experiments verify that our\nproposed GI-NAS can achieve superior attack performance compared to\nstate-of-the-art gradient inversion methods, even under more practical settings\nwith high-resolution images, large-sized batches, and advanced defense\nstrategies. To the best of our knowledge, we are the first to successfully\nintroduce NAS to the gradient inversion community. We believe that this work\nexposes critical vulnerabilities in real-world federated learning by\ndemonstrating high-fidelity reconstruction of sensitive data without requiring\ndomain-specific priors, forcing urgent reassessment of FL privacy safeguards.", "comment": "accepted by IEEE Transactions on Information Forensics and Security\n  (TIFS)", "pdf_url": "http://arxiv.org/pdf/2405.20725v3", "cate": "cs.AI", "date": "2024-05-31", "updated": "2025-07-13"}
{"id": "2507.09024", "title": "CNeuroMod-THINGS, a densely-sampled fMRI dataset for visual neuroscience", "authors": ["Marie St-Laurent", "Basile Pinsard", "Oliver Contier", "Elizabeth DuPre", "Katja Seeliger", "Valentina Borghesani", "Julie A. Boyle", "Lune Bellec", "Martin N. Hebart"], "categories": ["q-bio.NC", "cs.CV"], "primary_category": "Subjects:       Neurons and Cognition (q-bio.NC)", "pdf_link": null, "comments": "Comments:      29 pages manuscript, 5 figures, 12 pages supplementary material", "url": "http://arxiv.org/abs/2507.09024v1", "summary": "Data-hungry neuro-AI modelling requires ever larger neuroimaging datasets.\nCNeuroMod-THINGS meets this need by capturing neural representations for a wide\nset of semantic concepts using well-characterized stimuli in a new\ndensely-sampled, large-scale fMRI dataset. Importantly, CNeuroMod-THINGS\nexploits synergies between two existing projects: the THINGS initiative\n(THINGS) and the Courtois Project on Neural Modelling (CNeuroMod). THINGS has\ndeveloped a common set of thoroughly annotated images broadly sampling natural\nand man-made objects which is used to acquire a growing collection of\nlarge-scale multimodal neural responses. Meanwhile, CNeuroMod is acquiring\nhundreds of hours of fMRI data from a core set of participants during\ncontrolled and naturalistic tasks, including visual tasks like movie watching\nand videogame playing. For CNeuroMod-THINGS, four CNeuroMod participants each\ncompleted 33-36 sessions of a continuous recognition paradigm using\napproximately 4000 images from the THINGS stimulus set spanning 720 categories.\nWe report behavioural and neuroimaging metrics that showcase the quality of the\ndata. By bridging together large existing resources, CNeuroMod-THINGS expands\nour capacity to model broad slices of the human visual experience.", "comment": "29 pages manuscript, 5 figures, 12 pages supplementary material", "pdf_url": "http://arxiv.org/pdf/2507.09024v1", "cate": "q-bio.NC", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2410.15595", "title": "A Comprehensive Survey of Direct Preference Optimization: Datasets, Theories, Variants, and Applications", "authors": ["Wenyi Xiao", "Zechuan Wang", "Leilei Gan", "Shuai Zhao", "Zongrui Li", "Ruirui Lei", "Wanggui He", "Luu Anh Tuan", "Long Chen", "Hao Jiang", "Zhou Zhao", "Fei Wu"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      45 pages, 12 Figures. Project page: this https URL", "url": "http://arxiv.org/abs/2410.15595v3", "summary": "With the rapid advancement of large language models (LLMs), aligning policy\nmodels with human preferences has become increasingly critical. Direct\nPreference Optimization (DPO) has emerged as a promising approach for\nalignment, acting as an RL-free alternative to Reinforcement Learning from\nHuman Feedback (RLHF). Despite DPO's various advancements and inherent\nlimitations, an in-depth review of these aspects is currently lacking in the\nliterature. In this work, we present a comprehensive review of the challenges\nand opportunities in DPO, covering theoretical analyses, variants, relevant\npreference datasets, and applications. Specifically, we categorize recent\nstudies on DPO based on key research questions to provide a thorough\nunderstanding of DPO's current landscape. Additionally, we propose several\nfuture research directions to offer insights on model alignment for the\nresearch community. An updated collection of relevant papers can be found on\nhttps://github.com/Mr-Loevan/DPO-Survey.", "comment": "45 pages, 12 Figures. Project page:\n  https://github.com/Mr-Loevan/DPO-Survey", "pdf_url": "http://arxiv.org/pdf/2410.15595v3", "cate": "cs.AI", "date": "2024-10-21", "updated": "2025-07-14"}
{"id": "2507.09031", "title": "Confounder-Free Continual Learning via Recursive Feature Normalization", "authors": ["Yash Shah", "Camila Gonzalez", "Mohammad H. Abbasi", "Qingyu Zhao", "Kilian M. Pohl", "Ehsan Adeli"], "categories": ["cs.LG", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09031v1", "summary": "Confounders are extraneous variables that affect both the input and the\ntarget, resulting in spurious correlations and biased predictions. There are\nrecent advances in dealing with or removing confounders in traditional models,\nsuch as metadata normalization (MDN), where the distribution of the learned\nfeatures is adjusted based on the study confounders. However, in the context of\ncontinual learning, where a model learns continuously from new data over time\nwithout forgetting, learning feature representations that are invariant to\nconfounders remains a significant challenge. To remove their influence from\nintermediate feature representations, we introduce the Recursive MDN (R-MDN)\nlayer, which can be integrated into any deep learning architecture, including\nvision transformers, and at any model stage. R-MDN performs statistical\nregression via the recursive least squares algorithm to maintain and\ncontinually update an internal model state with respect to changing\ndistributions of data and confounding variables. Our experiments demonstrate\nthat R-MDN promotes equitable predictions across population groups, both within\nstatic learning and across different stages of continual learning, by reducing\ncatastrophic forgetting caused by confounder effects changing over time.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09031v1", "cate": "cs.LG", "date": "2025-07-11", "updated": "2025-07-11"}
{"id": "2410.19546", "title": "Bongard in Wonderland: Visual Puzzles that Still Make AI Go Mad?", "authors": ["Antonia Wüst", "Tim Woydt", "Lukas Helff", "Inga Ibs", "Wolfgang Stammer", "Devendra S. Dhami", "Constantin A. Rothkopf", "Kristian Kersting"], "categories": ["cs.AI", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.19546v4", "summary": "Recently, newly developed Vision-Language Models (VLMs), such as OpenAI's o1,\nhave emerged, seemingly demonstrating advanced reasoning capabilities across\ntext and image modalities. However, the depth of these advances in\nlanguage-guided perception and abstract reasoning remains underexplored, and it\nis unclear whether these models can truly live up to their ambitious promises.\nTo assess the progress and identify shortcomings, we enter the wonderland of\nBongard problems, a set of classic visual reasoning puzzles that require\nhuman-like abilities of pattern recognition and abstract reasoning. With our\nextensive evaluation setup, we show that while VLMs occasionally succeed in\nidentifying discriminative concepts and solving some of the problems, they\nfrequently falter. Surprisingly, even elementary concepts that may seem trivial\nto humans, such as simple spirals, pose significant challenges. Moreover, when\nexplicitly asked to recognize ground truth concepts, they continue to falter,\nsuggesting not only a lack of understanding of these elementary visual concepts\nbut also an inability to generalize to unseen concepts. We compare the results\nof VLMs to human performance and observe that a significant gap remains between\nhuman visual reasoning capabilities and machine cognition.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.19546v4", "cate": "cs.AI", "date": "2024-10-25", "updated": "2025-07-12"}
{"id": "2507.09212", "title": "Warm Starts Accelerate Generative Modelling", "authors": ["Jonas Scholz", "Richard E. Turner"], "categories": ["cs.LG", "cs.CV", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      10 pages, 6 figures", "url": "http://arxiv.org/abs/2507.09212v1", "summary": "Iterative generative models, like diffusion and flow-matching, create\nhigh-fidelity samples by progressively refining a noise vector into data.\nHowever, this process is notoriously slow, often requiring hundreds of function\nevaluations. We introduce the warm-start model, a simple, deterministic model\nthat dramatically accelerates conditional generation by providing a better\nstarting point. Instead of starting generation from an uninformed N(0, I)\nprior, our warm-start model predicts an informed prior N(mu, sigma), whose\nmoments are conditioned on the input context. This \"warm start\" substantially\nreduces the distance the generative process must traverse, particularly when\nthe conditioning information is strongly informative. On tasks like image\ninpainting, our method achieves results competitive with a 1000-step DDPM\nbaseline using only 11 total function evaluations (1 for the warm start, 10 for\ngeneration). A simple conditional normalization trick makes our method\ncompatible with any standard generative model and sampler without modification,\nallowing it to be combined with other efficient sampling techniques for further\nacceleration. Our implementation is available at\nhttps://github.com/jonas-scholz123/warm-start-model.", "comment": "10 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.09212v1", "cate": "cs.LG", "date": "2025-07-12", "updated": "2025-07-12"}
{"id": "2411.16313", "title": "CATP-LLM: Empowering Large Language Models for Cost-Aware Tool Planning", "authors": ["Duo Wu", "Jinghe Wang", "Yuan Meng", "Yanning Zhang", "Le Sun", "Zhi Wang"], "categories": ["cs.AI", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025. Codes and dataset are available at: this https URL", "url": "http://arxiv.org/abs/2411.16313v3", "summary": "Utilizing large language models (LLMs) for tool planning has emerged as a\npromising avenue for developing general AI systems, where LLMs automatically\nschedule external tools (e.g., vision models) to tackle complex tasks based on\ntask descriptions. To push this paradigm toward practical applications, it is\ncrucial for LLMs to consider tool execution costs (e.g., execution time) for\ntool planning. Unfortunately, prior studies overlook the tool execution costs,\nleading to the generation of expensive plans whose costs outweigh their\nbenefits in terms of task performance. To fill this gap, we propose the\nCost-Aware Tool Planning with LLMs (CATP-LLM) framework, which for the first\ntime provides a coherent design to empower LLMs for cost-aware tool planning.\nSpecifically, To facilitate efficient concurrent tool execution and cost\nreduction, we design a tool planning language to enhance the LLM for creating\nmulti-branch non-sequential plans. Moreover, we propose a cost-aware offline\nreinforcement learning algorithm to fine-tune the LLM to optimize the\nperformance-cost trade-off in tool planning. In the lack of public cost-related\ndatasets, we further present OpenCATP, the first dataset for cost-aware\nplanning, which comprises 11,100 evaluation samples from diverse tasks.\nExtensive experiments show that CATP-LLM outperforms GPT-4 even when using\nLlama2-7B as its backbone, with the average improvement of 1.5%-93.9% in terms\nof plan quality. Codes and dataset are available at:\nhttps://github.com/duowuyms/OpenCATP-LLM.", "comment": "Accepted to ICCV 2025. Codes and dataset are available at:\n  https://github.com/duowuyms/OpenCATP-LLM", "pdf_url": "http://arxiv.org/pdf/2411.16313v3", "cate": "cs.AI", "date": "2024-11-25", "updated": "2025-07-13"}
{"id": "2507.09448", "title": "TRACER: Efficient Object Re-Identification in Networked Cameras through Adaptive Query Processing", "authors": ["Pramod Chunduri", "Yao Lu", "Joy Arulraj"], "categories": ["cs.DB", "cs.CV"], "primary_category": "Subjects:       Databases (cs.DB)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09448v1", "summary": "Efficiently re-identifying and tracking objects across a network of cameras\nis crucial for applications like traffic surveillance. Spatula is the\nstate-of-the-art video database management system (VDBMS) for processing Re-ID\nqueries. However, it suffers from two limitations. Its spatio-temporal\nfiltering scheme has limited accuracy on large camera networks due to localized\ncamera history. It is not suitable for critical video analytics applications\nthat require high recall due to a lack of support for adaptive query\nprocessing.\n  In this paper, we present Tracer, a novel VDBMS for efficiently processing\nRe-ID queries using an adaptive query processing framework. Tracer selects the\noptimal camera to process at each time step by training a recurrent network to\nmodel long-term historical correlations. To accelerate queries under a high\nrecall constraint, Tracer incorporates a probabilistic adaptive search model\nthat processes camera feeds in incremental search windows and dynamically\nupdates the sampling probabilities using an exploration-exploitation strategy.\nTo address the paucity of benchmarks for the Re-ID task due to privacy\nconcerns, we present a novel synthetic benchmark for generating multi-camera\nRe-ID datasets based on real-world traffic distribution. Our evaluation shows\nthat Tracer outperforms the state-of-the-art cross-camera analytics system by\n3.9x on average across diverse datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09448v1", "cate": "cs.DB", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2412.17739", "title": "Fourier Position Embedding: Enhancing Attention's Periodic Extension for Length Generalization", "authors": ["Ermo Hua", "Che Jiang", "Xingtai Lv", "Kaiyan Zhang", "Youbang Sun", "Yuchen Fan", "Xuekai Zhu", "Biqing Qi", "Ning Ding", "Bowen Zhou"], "categories": ["cs.AI", "cs.CL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Accepted to ICML 2025", "url": "http://arxiv.org/abs/2412.17739v4", "summary": "Extending the context length of Language Models (LMs) by improving Rotary\nPosition Embedding (RoPE) has become a trend. While prior works mainly address\nRoPE's limitations within attention, this paper uncovers the adverse effects on\nlength generalization from nearly all parts of LMs. Using Discrete Signal\nProcessing theory, we show that RoPE enables periodic attention by implicitly\nachieving Non-Uniform Discrete Fourier Transform. However, this periodicity is\nundermined by the spectrum damage caused by: 1) linear layers and activation\nfunctions; 2) insufficiently trained frequency components brought by\ntime-domain truncation. Building on our observations, we propose Fourier\nPosition Embedding (FoPE), which enhances attention's frequency-domain\nproperties to improve both its periodic extension and length generalization.\nFoPE constructs \\textit{Fourier Series} and zero-outs the destructive frequency\ncomponents, increasing model robustness against the spectrum damage.\nExperiments across various model scales and benchmarks show that, within\nvarying context windows, FoPE maintains a more stable performance compared to\nother baselines. Several analyses and ablations bring further support to our\nmethod and theoretical modeling.", "comment": "Accepted to ICML 2025", "pdf_url": "http://arxiv.org/pdf/2412.17739v4", "cate": "cs.AI", "date": "2024-12-23", "updated": "2025-07-14"}
{"id": "2507.09513", "title": "Self-supervised pretraining of vision transformers for animal behavioral analysis and neural encoding", "authors": ["Yanchen Wang", "Han Yu", "Ari Blau", "Yizi Zhang", "The International Brain Laboratory", "Liam Paninski", "Cole Hurwitz", "Matt Whiteway"], "categories": ["q-bio.NC", "cs.CV"], "primary_category": "Subjects:       Neurons and Cognition (q-bio.NC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09513v1", "summary": "The brain can only be fully understood through the lens of the behavior it\ngenerates -- a guiding principle in modern neuroscience research that\nnevertheless presents significant technical challenges. Many studies capture\nbehavior with cameras, but video analysis approaches typically rely on\nspecialized models requiring extensive labeled data. We address this limitation\nwith BEAST (BEhavioral Analysis via Self-supervised pretraining of\nTransformers), a novel and scalable framework that pretrains\nexperiment-specific vision transformers for diverse neuro-behavior analyses.\nBEAST combines masked autoencoding with temporal contrastive learning to\neffectively leverage unlabeled video data. Through comprehensive evaluation\nacross multiple species, we demonstrate improved performance in three critical\nneuro-behavioral tasks: extracting behavioral features that correlate with\nneural activity, and pose estimation and action segmentation in both the\nsingle- and multi-animal settings. Our method establishes a powerful and\nversatile backbone model that accelerates behavioral analysis in scenarios\nwhere labeled data remains scarce.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09513v1", "cate": "q-bio.NC", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2501.16961", "title": "Instantiation-based Formalization of Logical Reasoning Tasks using Language Models and Logical Solvers", "authors": ["Mohammad Raza", "Natasa Milic-Frayling"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      IJCAI 2025", "url": "http://arxiv.org/abs/2501.16961v3", "summary": "Robustness of reasoning remains a significant challenge for large language\nmodels, and addressing it is essential for the practical applicability of\nAI-driven reasoning systems. We introduce Semantic Self-Verification (SSV), a\nnovel approach that addresses the key challenge in combining language models\nwith the rigor of logical solvers: to accurately formulate the reasoning\nproblem from natural language to the formal language of the solver. SSV uses a\nconsistency-based approach to produce strong abstract formalizations of\nproblems using concrete instantiations that are generated by the model and\nverified by the solver. In addition to significantly advancing the overall\nreasoning accuracy over the state-of-the-art, a key novelty that this approach\npresents is a feature of verification that has near-perfect precision over a\nsignificant coverage of cases, as we demonstrate on open reasoning benchmarks.\nWe propose such *near-certain reasoning* as a new approach to reduce the need\nfor manual verification in many cases, taking us closer to more dependable and\nautonomous AI reasoning systems.", "comment": "IJCAI 2025", "pdf_url": "http://arxiv.org/pdf/2501.16961v3", "cate": "cs.AI", "date": "2025-01-28", "updated": "2025-07-12"}
{"id": "2507.09616", "title": "MLoRQ: Bridging Low-Rank and Quantization for Transformer Compression", "authors": ["Ofir Gordon", "Ariel Lapid", "Elad Cohen", "Yarden Yagil", "Arnon Netzer", "Hai Victor Habi"], "categories": ["cs.LG", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09616v1", "summary": "Deploying transformer-based neural networks on resource-constrained edge\ndevices presents a significant challenge. This challenge is often addressed\nthrough various techniques, such as low-rank approximation and mixed-precision\nquantization. In this work, we introduce Mixed Low-Rank and Quantization\n(MLoRQ), a novel method that integrates both techniques. MLoRQ employs a\ntwo-stage optimization process to determine optimal bit-width and rank\nassignments for each layer, adhering to predefined memory constraints. This\nprocess includes: (i) an intra-layer optimization that identifies potentially\noptimal compression solutions out of all low-rank and quantization\ncombinations; (ii) an inter-layer optimization that assigns bit-width precision\nand rank to each layer while ensuring the memory constraint is met. An optional\nfinal step applies a sequential optimization process using a modified adaptive\nrounding technique to mitigate compression-induced errors in joint low-rank\napproximation and quantization. The method is compatible and can be seamlessly\nintegrated with most existing quantization algorithms. MLoRQ shows\nstate-of-the-art results with up to 15\\% performance improvement, evaluated on\nVision Transformers for image classification, object detection, and instance\nsegmentation tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09616v1", "cate": "cs.LG", "date": "2025-07-13", "updated": "2025-07-13"}
{"id": "2502.18439", "title": "MAPoRL: Multi-Agent Post-Co-Training for Collaborative Large Language Models with Reinforcement Learning", "authors": ["Chanwoo Park", "Seungju Han", "Xingzhi Guo", "Asuman Ozdaglar", "Kaiqing Zhang", "Joo-Kyung Kim"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      version for ACL", "url": "http://arxiv.org/abs/2502.18439v2", "summary": "Leveraging multiple large language models (LLMs) to build collaborative\nmulti-agentic workflows has demonstrated significant potential. However, most\nprevious studies focus on prompting the out-of-the-box LLMs, relying on their\ninnate capability for collaboration, which may not improve LLMs' performance as\nshown recently. In this paper, we introduce a new post-training paradigm MAPoRL\n(Multi-Agent Post-co-training for collaborative LLMs with Reinforcement\nLearning), to explicitly elicit the collaborative behaviors and further unleash\nthe power of multi-agentic LLM frameworks. In MAPoRL, multiple LLMs first\ngenerate their own responses independently and engage in a multi-turn\ndiscussion to collaboratively improve the final answer. In the end, a MAPoRL\nverifier evaluates both the answer and the discussion, by assigning a score\nthat verifies the correctness of the answer, while adding incentives to\nencourage corrective and persuasive discussions. The score serves as the\nco-training reward, and is then maximized through multi-agent RL. Unlike\nexisting LLM post-training paradigms, MAPoRL advocates the co-training of\nmultiple LLMs together using RL for better generalization. Accompanied by\nanalytical insights, our experiments demonstrate that training individual LLMs\nalone is insufficient to induce effective collaboration. In contrast,\nmulti-agent co-training can boost the collaboration performance across\nbenchmarks, with generalization to unseen domains.", "comment": "version for ACL", "pdf_url": "http://arxiv.org/pdf/2502.18439v2", "cate": "cs.AI", "date": "2025-02-25", "updated": "2025-07-12"}
{"id": "2507.09945", "title": "ESG-Net: Event-Aware Semantic Guided Network for Dense Audio-Visual Event Localization", "authors": ["Huilai Li", "Yonghao Dang", "Ying Xing", "Yiming Wang", "Jianqin Yin"], "categories": ["cs.MM", "cs.CV"], "primary_category": "Subjects:       Multimedia (cs.MM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09945v1", "summary": "Dense audio-visual event localization (DAVE) aims to identify event\ncategories and locate the temporal boundaries in untrimmed videos. Most studies\nonly employ event-related semantic constraints on the final outputs, lacking\ncross-modal semantic bridging in intermediate layers. This causes modality\nsemantic gap for further fusion, making it difficult to distinguish between\nevent-related content and irrelevant background content. Moreover, they rarely\nconsider the correlations between events, which limits the model to infer\nconcurrent events among complex scenarios. In this paper, we incorporate\nmulti-stage semantic guidance and multi-event relationship modeling, which\nrespectively enable hierarchical semantic understanding of audio-visual events\nand adaptive extraction of event dependencies, thereby better focusing on\nevent-related information. Specifically, our eventaware semantic guided network\n(ESG-Net) includes a early semantics interaction (ESI) module and a mixture of\ndependency experts (MoDE) module. ESI applys multi-stage semantic guidance to\nexplicitly constrain the model in learning semantic information through\nmulti-modal early fusion and several classification loss functions, ensuring\nhierarchical understanding of event-related content. MoDE promotes the\nextraction of multi-event dependencies through multiple serial mixture of\nexperts with adaptive weight allocation. Extensive experiments demonstrate that\nour method significantly surpasses the state-of-the-art methods, while greatly\nreducing parameters and computational load. Our code will be released on\nhttps://github.com/uchiha99999/ESG-Net.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09945v1", "cate": "cs.MM", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2503.20124", "title": "Synthesizing world models for bilevel planning", "authors": ["Zergham Ahmed", "Joshua B. Tenenbaum", "Christopher J. Bates", "Samuel J. Gershman"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Accepted to TMLR", "url": "http://arxiv.org/abs/2503.20124v2", "summary": "Modern reinforcement learning (RL) systems have demonstrated remarkable\ncapabilities in complex environments, such as video games. However, they still\nfall short of achieving human-like sample efficiency and adaptability when\nlearning new domains. Theory-based reinforcement learning (TBRL) is an\nalgorithmic framework specifically designed to address this gap. Modeled on\ncognitive theories, TBRL leverages structured, causal world models - \"theories\"\n- as forward simulators for use in planning, generalization and exploration.\nAlthough current TBRL systems provide compelling explanations of how humans\nlearn to play video games, they face several technical limitations: their\ntheory languages are restrictive, and their planning algorithms are not\nscalable. To address these challenges, we introduce TheoryCoder, an\ninstantiation of TBRL that exploits hierarchical representations of theories\nand efficient program synthesis methods for more powerful learning and\nplanning. TheoryCoder equips agents with general-purpose abstractions (e.g.,\n\"move to\"), which are then grounded in a particular environment by learning a\nlow-level transition model (a Python program synthesized from observations by a\nlarge language model). A bilevel planning algorithm can exploit this\nhierarchical structure to solve large domains. We demonstrate that this\napproach can be successfully applied to diverse and challenging grid-world\ngames, where approaches based on directly synthesizing a policy perform poorly.\nAblation studies demonstrate the benefits of using hierarchical abstractions.", "comment": "Accepted to TMLR", "pdf_url": "http://arxiv.org/pdf/2503.20124v2", "cate": "cs.AI", "date": "2025-03-26", "updated": "2025-07-13"}
{"id": "2507.10066", "title": "LayLens: Improving Deepfake Understanding through Simplified Explanations", "authors": ["Abhijeet Narang", "Parul Gupta", "Liuyijia Su", "Abhinav Dhall"], "categories": ["cs.MM", "cs.CV"], "primary_category": "Subjects:       Multimedia (cs.MM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10066v1", "summary": "This demonstration paper presents $\\mathbf{LayLens}$, a tool aimed to make\ndeepfake understanding easier for users of all educational backgrounds. While\nprior works often rely on outputs containing technical jargon, LayLens bridges\nthe gap between model reasoning and human understanding through a three-stage\npipeline: (1) explainable deepfake detection using a state-of-the-art forgery\nlocalization model, (2) natural language simplification of technical\nexplanations using a vision-language model, and (3) visual reconstruction of a\nplausible original image via guided image editing. The interface presents both\ntechnical and layperson-friendly explanations in addition to a side-by-side\ncomparison of the uploaded and reconstructed images. A user study with 15\nparticipants shows that simplified explanations significantly improve clarity\nand reduce cognitive load, with most users expressing increased confidence in\nidentifying deepfakes. LayLens offers a step toward transparent, trustworthy,\nand user-centric deepfake forensics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10066v1", "cate": "cs.MM", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2504.06122", "title": "Leanabell-Prover: Posttraining Scaling in Formal Reasoning", "authors": ["Jingyuan Zhang", "Qi Wang", "Xingguang Ji", "Yahui Liu", "Yang Yue", "Fuzheng Zhang", "Di Zhang", "Guorui Zhou", "Kun Gai"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      23 pages, 6 figures", "url": "http://arxiv.org/abs/2504.06122v3", "summary": "Recent advances in automated theorem proving (ATP) through LLMs have\nhighlighted the potential of formal reasoning with Lean 4 codes. However, ATP\nhas not yet be revolutionized by the recent posttraining scaling as\ndemonstrated by Open AI O1/O3 and Deepseek R1. In this work, we investigate the\nentire posttraining of ATP, aiming to align it with breakthroughs in reasoning\nmodels in natural languages. To begin, we continual train current ATP models\nwith a hybrid dataset, which consists of numerous statement-proof pairs, and\nadditional data aimed at incorporating cognitive behaviors that emulate human\nreasoning and hypothesis refinement. Next, we explore reinforcement learning\nwith the use of outcome reward returned by Lean 4 compiler. Through our\ndesigned continual training and reinforcement learning processes, we have\nsuccessfully improved existing formal provers, including both\nDeepSeek-Prover-v1.5 and Goedel-Prover, achieving state-of-the-art performance\nin the field of whole-proof generation. For example, we achieve a 59.8% pass\nrate (pass@32) on MiniF2F. This is an on-going project and we will\nprogressively update our findings, release our data and training details.", "comment": "23 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2504.06122v3", "cate": "cs.AI", "date": "2025-04-08", "updated": "2025-07-14"}
{"id": "2507.10434", "title": "CLA: Latent Alignment for Online Continual Self-Supervised Learning", "authors": ["Giacomo Cignoni", "Andrea Cossu", "Alexandra Gomez-Villa", "Joost van de Weijer", "Antonio Carta"], "categories": ["cs.LG", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted at CoLLAs 2025 conference", "url": "http://arxiv.org/abs/2507.10434v1", "summary": "Self-supervised learning (SSL) is able to build latent representations that\ngeneralize well to unseen data. However, only a few SSL techniques exist for\nthe online CL setting, where data arrives in small minibatches, the model must\ncomply with a fixed computational budget, and task boundaries are absent. We\nintroduce Continual Latent Alignment (CLA), a novel SSL strategy for Online CL\nthat aligns the representations learned by the current model with past\nrepresentations to mitigate forgetting. We found that our CLA is able to speed\nup the convergence of the training process in the online scenario,\noutperforming state-of-the-art approaches under the same computational budget.\nSurprisingly, we also discovered that using CLA as a pretraining protocol in\nthe early stages of pretraining leads to a better final performance when\ncompared to a full i.i.d. pretraining.", "comment": "Accepted at CoLLAs 2025 conference", "pdf_url": "http://arxiv.org/pdf/2507.10434v1", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-14"}
{"id": "2505.05602", "title": "HiBayES: A Hierarchical Bayesian Modeling Framework for AI Evaluation Statistics", "authors": ["Lennart Luettgau", "Harry Coppock", "Magda Dubois", "Christopher Summerfield", "Cozmin Ududec"], "categories": ["cs.AI", "stat.AP"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      23 pages, 9 figures", "url": "http://arxiv.org/abs/2505.05602v3", "summary": "As Large Language Models (LLMs) and other AI systems evolve, robustly\nestimating their capabilities from inherently stochastic outputs while\nsystematically quantifying uncertainty in these estimates becomes increasingly\nimportant. Further, advanced AI evaluations often have a nested hierarchical\nstructure, exhibit high levels of complexity, and come with high costs in\ntesting the most advanced AI systems. To address these challenges, we introduce\nHiBayES, a generalizable Hierarchical Bayesian modeling framework for AI\nEvaluation Statistics. HiBayES supports robust inferences in classical\nquestion-answer benchmarks and advanced agentic evaluations, particularly in\nlow-data scenarios (e.g., < 20 data points per evaluation). Built on\nGeneralized Linear Models (GLMs), Bayesian data analysis, and formal model\ncomparison, HiBayES provides principled uncertainty quantification and robust\nparameter estimation. This paper offers a comprehensive introduction to\nHiBayES, including illustrative examples, comparisons to conventional\nstatistical methods, and practical guidance for implementing multilevel\nBayesian GLMs. Additionally, we provide a HiBayES software package [4] (Beta\nversion) for out-of-the-box implementation.", "comment": "23 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2505.05602v3", "cate": "cs.AI", "date": "2025-05-08", "updated": "2025-07-13"}
{"id": "2204.01298", "title": "Capsule Networks Do Not Need to Model Everything", "authors": ["Riccardo Renzulli", "Enzo Tartaglione", "Marco Grangetto"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at Pattern Recognition", "url": "http://arxiv.org/abs/2204.01298v2", "summary": "Capsule networks are biologically inspired neural networks that group neurons\ninto vectors called capsules, each explicitly representing an object or one of\nits parts. The routing mechanism connects capsules in consecutive layers,\nforming a hierarchical structure between parts and objects, also known as a\nparse tree. Capsule networks often attempt to model all elements in an image,\nrequiring large network sizes to handle complexities such as intricate\nbackgrounds or irrelevant objects. However, this comprehensive modeling leads\nto increased parameter counts and computational inefficiencies. Our goal is to\nenable capsule networks to focus only on the object of interest, reducing the\nnumber of parse trees. We accomplish this with REM (Routing Entropy\nMinimization), a technique that minimizes the entropy of the parse tree-like\nstructure. REM drives the model parameters distribution towards low entropy\nconfigurations through a pruning mechanism, significantly reducing the\ngeneration of intra-class parse trees. This empowers capsules to learn more\nstable and succinct representations with fewer parameters and negligible\nperformance loss.", "comment": "Accepted at Pattern Recognition", "pdf_url": "http://arxiv.org/pdf/2204.01298v2", "cate": "cs.CV", "date": "2022-04-04", "updated": "2025-07-12"}
{"id": "2505.09341", "title": "Access Controls Will Solve the Dual-Use Dilemma", "authors": ["Evžen Wybitul"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Accepted at ICML 2025 Workshop on Technical AI Governance (TAIG)", "url": "http://arxiv.org/abs/2505.09341v3", "summary": "AI safety systems face the dual-use dilemma. It is unclear whether to answer\ndual-use requests, since the same query could be either harmless or harmful\ndepending on who made it and why. To make better decisions, such systems would\nneed to examine requests' real-world context, but currently, they lack access\nto this information. Instead, they sometimes end up making arbitrary choices\nthat result in refusing legitimate queries and allowing harmful ones, which\nhurts both utility and safety. To address this, we propose a conceptual\nframework based on access controls where only verified users can access\ndual-use outputs. We describe the framework's components, analyse its\nfeasibility, and explain how it addresses both over-refusals and\nunder-refusals. While only a high-level proposal, our work takes the first step\ntoward giving model providers more granular tools for managing dual-use\ncontent. Such tools would enable users to access more capabilities without\nsacrificing safety, and offer regulators new options for targeted policies.", "comment": "Accepted at ICML 2025 Workshop on Technical AI Governance (TAIG)", "pdf_url": "http://arxiv.org/pdf/2505.09341v3", "cate": "cs.AI", "date": "2025-05-14", "updated": "2025-07-14"}
{"id": "2212.08983", "title": "Adaptive deep learning framework for robust unsupervised underwater image enhancement", "authors": ["Alzayat Saleh", "Marcus Sheaves", "Dean Jerry", "Mostafa Rahimi Azghadi"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      25 pages, 7 figures, 6 tables, accepted for publication in Expert Systems with Applications", "url": "http://arxiv.org/abs/2212.08983v3", "summary": "One of the main challenges in deep learning-based underwater image\nenhancement is the limited availability of high-quality training data.\nUnderwater images are difficult to capture and are often of poor quality due to\nthe distortion and loss of colour and contrast in water. This makes it\ndifficult to train supervised deep learning models on large and diverse\ndatasets, which can limit the model's performance. In this paper, we explore an\nalternative approach to supervised underwater image enhancement. Specifically,\nwe propose a novel unsupervised underwater image enhancement framework that\nemploys a conditional variational autoencoder (cVAE) to train a deep learning\nmodel with probabilistic adaptive instance normalization (PAdaIN) and\nstatistically guided multi-colour space stretch that produces realistic\nunderwater images. The resulting framework is composed of a U-Net as a feature\nextractor and a PAdaIN to encode the uncertainty, which we call UDnet. To\nimprove the visual quality of the images generated by UDnet, we use a\nstatistically guided multi-colour space stretch module that ensures visual\nconsistency with the input image and provides an alternative to training using\na ground truth image. The proposed model does not need manual human annotation\nand can learn with a limited amount of data and achieves state-of-the-art\nresults on underwater images. We evaluated our proposed framework on eight\npublicly-available datasets. The results show that our proposed framework\nyields competitive performance compared to other state-of-the-art approaches in\nquantitative as well as qualitative metrics. Code available at\nhttps://github.com/alzayats/UDnet .", "comment": "25 pages, 7 figures, 6 tables, accepted for publication in Expert\n  Systems with Applications", "pdf_url": "http://arxiv.org/pdf/2212.08983v3", "cate": "cs.CV", "date": "2022-12-18", "updated": "2025-07-12"}
{"id": "2505.14403", "title": "Unearthing Gems from Stones: Policy Optimization with Negative Sample Augmentation for LLM Reasoning", "authors": ["Zhaohui Yang", "Yuxiao Ye", "Shilei Jiang", "Chen Hu", "Linjing Li", "Shihong Deng", "Daxin Jiang"], "categories": ["cs.AI", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.14403v3", "summary": "Recent advances in reasoning language models have witnessed a paradigm shift\nfrom short to long CoT pattern. Given the substantial computational cost of\nrollouts in long CoT models, maximizing the utility of fixed training datasets\nbecomes crucial. Our analysis reveals that negative responses contain valuable\ncomponents such as self-reflection and error-correction steps, yet primary\nexisting methods either completely discard negative samples (RFT) or apply\nequal penalization across all tokens (RL), failing to leverage these potential\nlearning signals. In light of this, we propose Behavior Constrained Policy\nGradient with Negative Sample Augmentation (BCPG-NSA), a fine-grained offline\nRL framework that encompasses three stages: 1) sample segmentation, 2)\nconsensus-based step correctness assessment combining LLM and PRM judgers, and\n3) policy optimization with NSA designed to effectively mine positive steps\nwithin negative samples. Experimental results show that BCPG-NSA outperforms\nbaselines on several challenging math/coding reasoning benchmarks using the\nsame training dataset, achieving improved sample efficiency and demonstrating\nrobustness and scalability when extended to multiple iterations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.14403v3", "cate": "cs.AI", "date": "2025-05-20", "updated": "2025-07-14"}
{"id": "2312.00700", "title": "WeGeFT: Weight-Generative Fine-Tuning for Multi-Faceted Efficient Adaptation of Large Models", "authors": ["Chinmay Savadikar", "Xi Song", "Tianfu Wu"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICML25", "url": "http://arxiv.org/abs/2312.00700v5", "summary": "Fine-tuning large pretrained Transformer models can focus on either\nintroducing a small number of new learnable parameters (parameter efficiency)\nor editing representations of a small number of tokens using lightweight\nmodules (representation efficiency). While the pioneering method LoRA (Low-Rank\nAdaptation) inherently balances parameter, compute, and memory efficiency, many\nsubsequent variants trade off compute and memory efficiency and/or performance\nto further reduce fine-tuning parameters. To address this limitation and unify\nparameter-efficient and representation-efficient fine-tuning, we propose\nWeight-Generative Fine-Tuning (WeGeFT, pronounced wee-gift), a novel approach\nthat learns to generate fine-tuning weights directly from the pretrained\nweights. WeGeFT employs a simple low-rank formulation consisting of two linear\nlayers, either shared across multiple layers of the pretrained model or\nindividually learned for different layers. This design achieves multi-faceted\nefficiency in parameters, representations, compute, and memory, while\nmaintaining or exceeding the performance of LoRA and its variants. Extensive\nexperiments on commonsense reasoning, arithmetic reasoning, instruction\nfollowing, code generation, and visual recognition verify the effectiveness of\nour proposed WeGeFT. Our code is available at\nhttps://github.com/savadikarc/wegeft", "comment": "Accepted to ICML25", "pdf_url": "http://arxiv.org/pdf/2312.00700v5", "cate": "cs.CV", "date": "2023-12-01", "updated": "2025-07-13"}
{"id": "2505.22050", "title": "Reinforced Reasoning for Embodied Planning", "authors": ["Di Wu", "Jiaxin Fan", "Junzhe Zang", "Guanbo Wang", "Wei Yin", "Wenhao Li", "Bo Jin"], "categories": ["cs.AI", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.22050v2", "summary": "Embodied planning requires agents to make coherent multi-step decisions based\non dynamic visual observations and natural language goals. While recent\nvision-language models (VLMs) excel at static perception tasks, they struggle\nwith the temporal reasoning, spatial understanding, and commonsense grounding\nneeded for planning in interactive environments. In this work, we introduce a\nreinforcement fine-tuning framework that brings R1-style reasoning enhancement\ninto embodied planning. We first distill a high-quality dataset from a powerful\nclosed-source model and perform supervised fine-tuning (SFT) to equip the model\nwith structured decision-making priors. We then design a rule-based reward\nfunction tailored to multi-step action quality and optimize the policy via\nGeneralized Reinforced Preference Optimization (GRPO). Our approach is\nevaluated on Embench, a recent benchmark for interactive embodied tasks,\ncovering both in-domain and out-of-domain scenarios. Experimental results show\nthat our method significantly outperforms models of similar or larger scale,\nincluding GPT-4o-mini and 70B+ open-source baselines, and exhibits strong\ngeneralization to unseen environments. This work highlights the potential of\nreinforcement-driven reasoning to advance long-horizon planning in embodied AI.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.22050v2", "cate": "cs.AI", "date": "2025-05-28", "updated": "2025-07-13"}
{"id": "2312.10872", "title": "Evaluating the Role of Training Data Origin for Country-Scale Cropland Mapping in Data-Scarce Regions: A Case Study of Nigeria", "authors": ["Joaquin Gajardo", "Michele Volpi", "Daniel Onwude", "Thijs Defraeye"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      This article is published in ISPRS Open Journal of Photogrammetry and Remote Sensing under a CC BY 4.0 license: this https URL . Code repository: this https URL", "url": "http://arxiv.org/abs/2312.10872v2", "summary": "Cropland maps are essential for remote sensing-based agricultural monitoring,\nproviding timely insights without extensive field surveys. Machine learning\nenables large-scale mapping but depends on geo-referenced ground-truth data,\nwhich is costly to collect, motivating the use of global datasets in\ndata-scarce regions. A key challenge is understanding how the quantity,\nquality, and proximity of the training data to the target region influences\nmodel performance. We evaluate this in Nigeria, using 1,827 manually labelled\nsamples covering the whole country, and subsets of the Geowiki dataset:\nNigeria-only, regional (Nigeria and neighbouring countries), and global. We\nextract pixel-wise multi-source time series arrays from Sentinel-1, Sentinel-2,\nERA5 climate, and a digital elevation model using Google Earth Engine,\ncomparing Random Forests with LSTMs, including a lightweight multi-headed LSTM\nvariant. Results show local data significantly boosts performance, with\naccuracy gains up to 0.246 (RF) and 0.178 (LSTM). Nigeria-only or regional data\noutperformed global data despite the lower amount of labels, with the exception\nof the multi-headed LSTM, which benefited from global data when local samples\nwere absent. Sentinel-1, climate, and topographic data are critical data\nsources, with their removal reducing F1-score by up to 0.593. Addressing class\nimbalance also improved LSTM accuracy by up to 0.071. Our top-performing model\n(Nigeria-only LSTM) achieved an F1-score of 0.814 and accuracy of 0.842,\nmatching the best global land cover product while offering stronger recall,\ncritical for food security. We release code, data, maps, and an interactive web\napp to support future work.", "comment": "This article is published in ISPRS Open Journal of Photogrammetry and\n  Remote Sensing under a CC BY 4.0 license:\n  https://www.sciencedirect.com/science/article/pii/S2667393225000109. Code\n  repository: https://github.com/Joaquin-Gajardo/nigeria-crop-mask", "pdf_url": "http://arxiv.org/pdf/2312.10872v2", "cate": "cs.CV", "date": "2023-12-18", "updated": "2025-07-13"}
{"id": "2506.10521", "title": "Scientists' First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning", "authors": ["Yuhao Zhou", "Yiheng Wang", "Xuming He", "Ruoyao Xiao", "Zhiwei Li", "Qiantai Feng", "Zijie Guo", "Yuejin Yang", "Hao Wu", "Wenxuan Huang", "Jiaqi Wei", "Dan Si", "Xiuqi Yao", "Jia Bu", "Haiwen Huang", "Tianfan Fu", "Shixiang Tang", "Ben Fei", "Dongzhan Zhou", "Fenghua Ling", "Yan Lu", "Siqi Sun", "Chenhui Li", "Guanjie Zheng", "Jiancheng Lv", "Wenlong Zhang", "Lei Bai"], "categories": ["cs.AI", "cs.CL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      82 pages", "url": "http://arxiv.org/abs/2506.10521v4", "summary": "Scientific discoveries increasingly rely on complex multimodal reasoning\nbased on information-intensive scientific data and domain-specific expertise.\nEmpowered by expert-level scientific benchmarks, scientific Multimodal Large\nLanguage Models (MLLMs) hold the potential to significantly enhance this\ndiscovery process in realistic workflows. However, current scientific\nbenchmarks mostly focus on evaluating the knowledge understanding capabilities\nof MLLMs, leading to an inadequate assessment of their perception and reasoning\nabilities. To address this gap, we present the Scientists' First Exam (SFE)\nbenchmark, designed to evaluate the scientific cognitive capacities of MLLMs\nthrough three interconnected levels: scientific signal perception, scientific\nattribute understanding, scientific comparative reasoning. Specifically, SFE\ncomprises 830 expert-verified VQA pairs across three question types, spanning\n66 multimodal tasks across five high-value disciplines. Extensive experiments\nreveal that current state-of-the-art GPT-o3 and InternVL-3 achieve only 34.08%\nand 26.52% on SFE, highlighting significant room for MLLMs to improve in\nscientific realms. We hope the insights obtained in SFE will facilitate further\ndevelopments in AI-enhanced scientific discoveries.", "comment": "82 pages", "pdf_url": "http://arxiv.org/pdf/2506.10521v4", "cate": "cs.AI", "date": "2025-06-12", "updated": "2025-07-14"}
{"id": "2401.11406", "title": "Adversarial Augmentation Training Makes Action Recognition Models More Robust to Realistic Video Distribution Shifts", "authors": ["Kiyoon Kim", "Shreyank N Gowda", "Panagiotis Eustratiadis", "Antreas Antoniou", "Robert B Fisher"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICPRAI 2024", "url": "http://arxiv.org/abs/2401.11406v2", "summary": "Despite recent advances in video action recognition achieving strong\nperformance on existing benchmarks, these models often lack robustness when\nfaced with natural distribution shifts between training and test data. We\npropose two novel evaluation methods to assess model resilience to such\ndistribution disparity. One method uses two different datasets collected from\ndifferent sources and uses one for training and validation, and the other for\ntesting. More precisely, we created dataset splits of HMDB-51 or UCF-101 for\ntraining, and Kinetics-400 for testing, using the subset of the classes that\nare overlapping in both train and test datasets. The other proposed method\nextracts the feature mean of each class from the target evaluation dataset's\ntraining data (i.e. class prototype) and estimates test video prediction as a\ncosine similarity score between each sample to the class prototypes of each\ntarget class. This procedure does not alter model weights using the target\ndataset and it does not require aligning overlapping classes of two different\ndatasets, thus is a very efficient method to test the model robustness to\ndistribution shifts without prior knowledge of the target distribution. We\naddress the robustness problem by adversarial augmentation training -\ngenerating augmented views of videos that are \"hard\" for the classification\nmodel by applying gradient ascent on the augmentation parameters - as well as\n\"curriculum\" scheduling the strength of the video augmentations. We\nexperimentally demonstrate the superior performance of the proposed adversarial\naugmentation approach over baselines across three state-of-the-art action\nrecognition models - TSM, Video Swin Transformer, and Uniformer. The presented\nwork provides critical insight into model robustness to distribution shifts and\npresents effective techniques to enhance video action recognition performance\nin a real-world deployment.", "comment": "Accepted to ICPRAI 2024", "pdf_url": "http://arxiv.org/pdf/2401.11406v2", "cate": "cs.CV", "date": "2024-01-21", "updated": "2025-07-14"}
{"id": "2506.17300", "title": "Individual Causal Inference with Structural Causal Model", "authors": ["Daniel T. Chang"], "categories": ["cs.AI", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.17300v2", "summary": "Individual causal inference (ICI) uses causal inference methods to understand\nand predict the effects of interventions on individuals, considering their\nspecific characteristics / facts. It aims to estimate individual causal effect\n(ICE), which varies across individuals. Estimating ICE can be challenging due\nto the limited data available for individuals, and the fact that most causal\ninference methods are population-based. Structural Causal Model (SCM) is\nfundamentally population-based. Therefore, causal discovery (structural\nlearning and parameter learning), association queries and intervention queries\nare all naturally population-based. However, exogenous variables (U) in SCM can\nencode individual variations and thus provide the mechanism for individualized\npopulation per specific individual characteristics / facts. Based on this, we\npropose ICI with SCM as a \"rung 3\" causal inference, because it involves\n\"imagining\" what would be the causal effect of a hypothetical intervention on\nan individual, given the individual's observed characteristics / facts.\nSpecifically, we propose the indiv-operator, indiv(W), to formalize/represent\nthe population individualization process, and the individual causal query, P(Y\n| indiv(W), do(X), Z), to formalize/represent ICI. We show and argue that ICI\nwith SCM is inference on individual alternatives (possible), not individual\ncounterfactuals (non-actual).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.17300v2", "cate": "cs.AI", "date": "2025-06-17", "updated": "2025-07-11"}
{"id": "2403.06759", "title": "Average Calibration Error: A Differentiable Loss for Improved Reliability in Image Segmentation", "authors": ["Theodore Barfoot", "Luis Garcia-Peraza-Herrera", "Ben Glocker", "Tom Vercauteren"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Camera ready version as in https://doi.org/10.1007/978-3-031-72114-4_14", "url": "http://arxiv.org/abs/2403.06759v4", "summary": "Deep neural networks for medical image segmentation often produce\noverconfident results misaligned with empirical observations. Such\nmiscalibration, challenges their clinical translation. We propose to use\nmarginal L1 average calibration error (mL1-ACE) as a novel auxiliary loss\nfunction to improve pixel-wise calibration without compromising segmentation\nquality. We show that this loss, despite using hard binning, is directly\ndifferentiable, bypassing the need for approximate but differentiable surrogate\nor soft binning approaches. Our work also introduces the concept of dataset\nreliability histograms which generalises standard reliability diagrams for\nrefined visual assessment of calibration in semantic segmentation aggregated at\nthe dataset level. Using mL1-ACE, we reduce average and maximum calibration\nerror by 45% and 55% respectively, maintaining a Dice score of 87% on the BraTS\n2021 dataset. We share our code here: https://github.com/cai4cai/ACE-DLIRIS", "comment": "Camera ready version as in 10.1007/978-3-031-72114-4_14", "pdf_url": "http://arxiv.org/pdf/2403.06759v4", "cate": "cs.CV", "date": "2024-03-11", "updated": "2025-07-14"}
{"id": "2507.00951", "title": "Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive Foundations for Artificial General Intelligence and its Societal Impact", "authors": ["Rizwan Qureshi", "Ranjan Sapkota", "Abbas Shah", "Amgad Muneer", "Anas Zafar", "Ashmal Vayani", "Maged Shoman", "Abdelrahman B. M. Eldaly", "Kai Zhang", "Ferhat Sadak", "Shaina Raza", "Xinqi Fan", "Ravid Shwartz-Ziv", "Hong Yan", "Vinjia Jain", "Aman Chadha", "Manoj Karkee", "Jia Wu", "Seyedali Mirjalili"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.00951v3", "summary": "Can machines truly think, reason and act in domains like humans? This\nenduring question continues to shape the pursuit of Artificial General\nIntelligence (AGI). Despite the growing capabilities of models such as GPT-4.5,\nDeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal\nfluency and partial reasoning, these systems remain fundamentally limited by\ntheir reliance on token-level prediction and lack of grounded agency. This\npaper offers a cross-disciplinary synthesis of AGI development, spanning\nartificial intelligence, cognitive neuroscience, psychology, generative models,\nand agent-based systems. We analyze the architectural and cognitive foundations\nof general intelligence, highlighting the role of modular reasoning, persistent\nmemory, and multi-agent coordination. In particular, we emphasize the rise of\nAgentic RAG frameworks that combine retrieval, planning, and dynamic tool use\nto enable more adaptive behavior. We discuss generalization strategies,\nincluding information compression, test-time adaptation, and training-free\nmethods, as critical pathways toward flexible, domain-agnostic intelligence.\nVision-Language Models (VLMs) are reexamined not just as perception modules but\nas evolving interfaces for embodied understanding and collaborative task\ncompletion. We also argue that true intelligence arises not from scale alone\nbut from the integration of memory and reasoning: an orchestration of modular,\ninteractive, and self-improving components where compression enables adaptive\nbehavior. Drawing on advances in neurosymbolic systems, reinforcement learning,\nand cognitive scaffolding, we explore how recent architectures begin to bridge\nthe gap between statistical learning and goal-directed cognition. Finally, we\nidentify key scientific, technical, and ethical challenges on the path to AGI.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00951v3", "cate": "cs.AI", "date": "2025-07-01", "updated": "2025-07-12"}
{"id": "2403.08142", "title": "FieldNet: Efficient Real-Time Shadow Removal for Enhanced Vision in Field Robotics", "authors": ["Alzayat Saleh", "Alex Olsen", "Jake Wood", "Bronson Philippa", "Mostafa Rahimi Azghadi"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      22 pages, 9 figures, 8 tables. Published at Expert Systems with Applications", "url": "http://arxiv.org/abs/2403.08142v3", "summary": "Shadows significantly hinder computer vision tasks in outdoor environments,\nparticularly in field robotics, where varying lighting conditions complicate\nobject detection and localisation. We present FieldNet, a novel deep learning\nframework for real-time shadow removal, optimised for resource-constrained\nhardware. FieldNet introduces a probabilistic enhancement module and a novel\nloss function to address challenges of inconsistent shadow boundary supervision\nand artefact generation, achieving enhanced accuracy and simplicity without\nrequiring shadow masks during inference. Trained on a dataset of 10,000 natural\nimages augmented with synthetic shadows, FieldNet outperforms state-of-the-art\nmethods on benchmark datasets (ISTD, ISTD+, SRD), with up to $9$x speed\nimprovements (66 FPS on Nvidia 2080Ti) and superior shadow removal quality\n(PSNR: 38.67, SSIM: 0.991). Real-world case studies in precision agriculture\nrobotics demonstrate the practical impact of FieldNet in enhancing weed\ndetection accuracy. These advancements establish FieldNet as a robust,\nefficient solution for real-time vision tasks in field robotics and beyond.", "comment": "22 pages, 9 figures, 8 tables. Published at Expert Systems with\n  Applications", "pdf_url": "http://arxiv.org/pdf/2403.08142v3", "cate": "cs.CV", "date": "2024-03-13", "updated": "2025-07-12"}
{"id": "2507.01410", "title": "A Fuzzy Approach to the Specification, Verification and Validation of Risk-Based Ethical Decision Making Models", "authors": ["Abeer Dyoub", "Francesca A. Lisi"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.01410v2", "summary": "The ontological and epistemic complexities inherent in the moral domain make\nit challenging to establish clear standards for evaluating the performance of a\nmoral machine. In this paper, we present a formal method to describe Ethical\nDecision Making models based on ethical risk assessment. Then, we show how\nthese models that are specified as fuzzy rules can be verified and validated\nusing fuzzy Petri nets. A case study from the medical field is considered to\nillustrate the proposed approach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.01410v2", "cate": "cs.AI", "date": "2025-07-02", "updated": "2025-07-13"}
{"id": "2405.03546", "title": "CCDM: Continuous Conditional Diffusion Models for Image Generation", "authors": ["Xin Ding", "Yongwei Wang", "Kao Zhang", "Z. Jane Wang"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2405.03546v3", "summary": "Continuous Conditional Generative Modeling (CCGM) estimates high-dimensional\ndata distributions, such as images, conditioned on scalar continuous variables\n(aka regression labels). While Continuous Conditional Generative Adversarial\nNetworks (CcGANs) were designed for this task, their instability during\nadversarial learning often leads to suboptimal results. Conditional Diffusion\nModels (CDMs) offer a promising alternative, generating more realistic images,\nbut their diffusion processes, label conditioning, and model fitting procedures\nare either not optimized for or incompatible with CCGM, making it difficult to\nintegrate CcGANs' vicinal approach. To address these issues, we introduce\nContinuous Conditional Diffusion Models (CCDMs), the first CDM specifically\ntailored for CCGM. CCDMs address existing limitations with specially designed\nconditional diffusion processes, a novel hard vicinal image denoising loss, a\ncustomized label embedding method, and efficient conditional sampling\nprocedures. Through comprehensive experiments on four datasets with resolutions\nranging from 64x64 to 192x192, we demonstrate that CCDMs outperform\nstate-of-the-art CCGM models, establishing a new benchmark. Ablation studies\nfurther validate the model design and implementation, highlighting that some\nwidely used CDM implementations are ineffective for the CCGM task. Our code is\npublicly available at https://github.com/UBCDingXin/CCDM.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2405.03546v3", "cate": "cs.CV", "date": "2024-05-06", "updated": "2025-07-13"}
{"id": "2507.02083", "title": "Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab", "authors": ["Haonan Duan", "Stephen Zhewen Lu", "Caitlin Fiona Harrigan", "Nishkrit Desai", "Jiarui Lu", "Michał Koziarski", "Leonardo Cotta", "Chris J. Maddison"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02083v2", "summary": "Designing experiments and result interpretations are core scientific\ncompetencies, particularly in biology, where researchers perturb complex\nsystems to uncover the underlying systems. Recent efforts to evaluate the\nscientific capabilities of large language models (LLMs) fail to test these\ncompetencies because wet-lab experimentation is prohibitively expensive: in\nexpertise, time and equipment. We introduce SciGym, a first-in-class benchmark\nthat assesses LLMs' iterative experiment design and analysis abilities in\nopen-ended scientific discovery tasks. SciGym overcomes the challenge of\nwet-lab costs by running a dry lab of biological systems. These models, encoded\nin Systems Biology Markup Language, are efficient for generating simulated\ndata, making them ideal testbeds for experimentation on realistically complex\nsystems. We evaluated six frontier LLMs on 137 small systems, and released a\ntotal of 350 systems. Our evaluation shows that while more capable models\ndemonstrated superior performance, all models' performance declined\nsignificantly as system complexity increased, suggesting substantial room for\nimprovement in the scientific capabilities of LLM agents.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02083v2", "cate": "cs.AI", "date": "2025-07-02", "updated": "2025-07-14"}
{"id": "2405.11467", "title": "AdaAugment: A Tuning-Free and Adaptive Approach to Enhance Data Augmentation", "authors": ["Suorong Yang", "Peijia Li", "Xin Xiong", "Furao Shen", "Jian Zhao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      IEEE Transactions on Image Processing", "url": "http://arxiv.org/abs/2405.11467v3", "summary": "Data augmentation (DA) is widely employed to improve the generalization\nperformance of deep models. However, most existing DA methods employ\naugmentation operations with fixed or random magnitudes throughout the training\nprocess. While this fosters data diversity, it can also inevitably introduce\nuncontrolled variability in augmented data, which could potentially cause\nmisalignment with the evolving training status of the target models. Both\ntheoretical and empirical findings suggest that this misalignment increases the\nrisks of both underfitting and overfitting. To address these limitations, we\npropose AdaAugment, an innovative and tuning-free adaptive augmentation method\nthat leverages reinforcement learning to dynamically and adaptively adjust\naugmentation magnitudes for individual training samples based on real-time\nfeedback from the target network. Specifically, AdaAugment features a\ndual-model architecture consisting of a policy network and a target network,\nwhich are jointly optimized to adapt augmentation magnitudes in accordance with\nthe model's training progress effectively. The policy network optimizes the\nvariability within the augmented data, while the target network utilizes the\nadaptively augmented samples for training. These two networks are jointly\noptimized and mutually reinforce each other. Extensive experiments across\nbenchmark datasets and deep architectures demonstrate that AdaAugment\nconsistently outperforms other state-of-the-art DA methods in effectiveness\nwhile maintaining remarkable efficiency. Code is available at\nhttps://github.com/Jackbrocp/AdaAugment.", "comment": "IEEE Transactions on Image Processing", "pdf_url": "http://arxiv.org/pdf/2405.11467v3", "cate": "cs.CV", "date": "2024-05-19", "updated": "2025-07-13"}
{"id": "2507.05201", "title": "MedGemma Technical Report", "authors": ["Andrew Sellergren", "Sahar Kazemzadeh", "Tiam Jaroensri", "Atilla Kiraly", "Madeleine Traverse", "Timo Kohlberger", "Shawn Xu", "Fayaz Jamil", "Cían Hughes", "Charles Lau", "Justin Chen", "Fereshteh Mahvar", "Liron Yatziv", "Tiffany Chen", "Bram Sterling", "Stefanie Anna Baby", "Susanna Maria Baby", "Jeremy Lai", "Samuel Schmidgall", "Lu Yang", "Kejia Chen", "Per Bjornsson", "Shashir Reddy", "Ryan Brush", "Kenneth Philbrick", "Mercy Asiedu", "Ines Mezerreg", "Howard Hu", "Howard Yang", "Richa Tiwari", "Sunny Jansen", "Preeti Singh", "Yun Liu", "Shekoofeh Azizi", "Aishwarya Kamath", "Johan Ferret", "Shreya Pathak", "Nino Vieillard", "Ramona Merhej", "Sarah Perrin", "Tatiana Matejovicova", "Alexandre Ramé", "Morgane Riviere", "Louis Rouillard", "Thomas Mesnard", "Geoffrey Cideron", "Jean-bastien Grill", "Sabela Ramos", "Edouard Yvinec", "Michelle Casbon", "Elena Buchatskaya", "Jean-Baptiste Alayrac", "Dmitry Lepikhin", "Vlad Feinberg", "Sebastian Borgeaud", "Alek Andreev", "Cassidy Hardin", "Robert Dadashi", "Léonard Hussenot", "Armand Joulin", "Olivier Bachem", "Yossi Matias", "Katherine Chou", "Avinatan Hassidim", "Kavi Goel", "Clement Farabet", "Joelle Barral", "Tris Warkentin", "Jonathon Shlens", "David Fleet", "Victor Cotruta", "Omar Sanseviero", "Gus Martins", "Phoebe Kirk", "Anand Rao", "Shravya Shetty", "David F. Steiner", "Can Kirmizibayrak", "Rory Pilgrim", "Daniel Golden", "Lin Yang"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.05201v3", "summary": "Artificial intelligence (AI) has significant potential in healthcare\napplications, but its training and deployment faces challenges due to\nhealthcare's diverse data, complex tasks, and the need to preserve privacy.\nFoundation models that perform well on medical tasks and require less\ntask-specific tuning data are critical to accelerate the development of\nhealthcare AI applications. We introduce MedGemma, a collection of medical\nvision-language foundation models based on Gemma 3 4B and 27B. MedGemma\ndemonstrates advanced medical understanding and reasoning on images and text,\nsignificantly exceeding the performance of similar-sized generative models and\napproaching the performance of task-specific models, while maintaining the\ngeneral capabilities of the Gemma 3 base models. For out-of-distribution tasks,\nMedGemma achieves 2.6-10% improvement on medical multimodal question answering,\n15.5-18.1% improvement on chest X-ray finding classification, and 10.8%\nimprovement on agentic evaluations compared to the base models. Fine-tuning\nMedGemma further improves performance in subdomains, reducing errors in\nelectronic health record information retrieval by 50% and reaching comparable\nperformance to existing specialized state-of-the-art methods for pneumothorax\nclassification and histopathology patch classification. We additionally\nintroduce MedSigLIP, a medically-tuned vision encoder derived from SigLIP.\nMedSigLIP powers the visual understanding capabilities of MedGemma and as an\nencoder achieves comparable or better performance than specialized medical\nimage encoders. Taken together, the MedGemma collection provides a strong\nfoundation of medical image and text capabilities, with potential to\nsignificantly accelerate medical research and development of downstream\napplications. The MedGemma collection, including tutorials and model weights,\ncan be found at https://goo.gle/medgemma.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.05201v3", "cate": "cs.AI", "date": "2025-07-07", "updated": "2025-07-12"}
{"id": "2406.01069", "title": "UniQA: Unified Vision-Language Pre-training for Image Quality and Aesthetic Assessment", "authors": ["Hantao Zhou", "Longxiang Tang", "Rui Yang", "Guanyi Qin", "Yan Zhang", "Yutao Li", "Xiu Li", "Runze Hu", "Guangtao Zhai"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2406.01069v2", "summary": "Image Quality Assessment (IQA) and Image Aesthetic Assessment (IAA) aim to\nsimulate human subjective perception of image visual quality and aesthetic\nappeal. Despite distinct learning objectives, they have underlying\ninterconnectedness due to consistent human assessment perception. In this\npaper, we propose Unified vision-language pre-training of Quality and\nAesthetics (UniQA}), to extract useful and common representations from two\ntasks, thereby benefiting them simultaneously. However, the lack of text in the\nIQA datasets and the textual noise in the IAA datasets pose severe challenges\nfor multimodal pre-training. To address this, we (1) utilize multimodal large\nlanguage models (MLLMs) to generate high-quality text descriptions; (2) use the\ngenerated text for IAA as metadata to purify noisy IAA data. To effectively\nadapt the pre-trained UniQA to downstream tasks, we further propose a\nlightweight adapter that utilizes versatile cues to fully exploit the extensive\nknowledge of the pre-trained model. UniQA demonstrates high competitiveness in\nvarious image assessment tasks, including classical IQA and IAA tasks,\nfew-label IQA, and other downstream tasks, showing promise as a foundational\nassessment model. Codes are available at https://github.com/zht8506/UniQA.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2406.01069v2", "cate": "cs.CV", "date": "2024-06-03", "updated": "2025-07-14"}
{"id": "2507.05297", "title": "Continuous Classification Aggregation", "authors": ["Zijun Meng"], "categories": ["cs.AI", "econ.TH"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      9 pages; 2 figures", "url": "http://arxiv.org/abs/2507.05297v4", "summary": "We prove that any optimal, independent, and zero unanimous fuzzy\nclassification aggregation function of a continuum of individual\nclassifications of $m\\ge 3$ objects into $2\\le p\\le m$ types must be a weighted\narithmetic mean. We also provide a characterization for the case when $m=p=2$.", "comment": "9 pages; 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.05297v4", "cate": "cs.AI", "date": "2025-07-06", "updated": "2025-07-14"}
{"id": "2407.15228", "title": "3D Reconstruction of the Human Colon from Capsule Endoscope Video", "authors": ["Pål Anders Floor", "Ivar Farup", "Marius Pedersen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      11 pages, 12 figures", "url": "http://arxiv.org/abs/2407.15228v2", "summary": "As the number of people affected by diseases in the gastrointestinal system\nis ever-increasing, a higher demand on preventive screening is inevitable. This\nwill significantly increase the workload on gastroenterologists. To help reduce\nthe workload, tools from computer vision may be helpful. In this paper, we\ninvestigate the possibility of constructing 3D models of whole sections of the\nhuman colon using image sequences from wireless capsule endoscope video,\nproviding enhanced viewing for gastroenterologists. As capsule endoscope images\ncontain distortion and artifacts non-ideal for many 3D reconstruction\nalgorithms, the problem is challenging. However, recent developments of virtual\ngraphics-based models of the human gastrointestinal system, where distortion\nand artifacts can be enabled or disabled, makes it possible to ``dissect'' the\nproblem. The graphical model also provides a ground truth, enabling computation\nof geometric distortion introduced by the 3D reconstruction method. In this\npaper, most distortions and artifacts are left out to determine if it is\nfeasible to reconstruct whole sections of the human gastrointestinal system by\nexisting methods. We demonstrate that 3D reconstruction is possible using\nsimultaneous localization and mapping. Further, to reconstruct the\ngastrointestinal wall surface from resulting point clouds, varying greatly in\ndensity, Poisson surface reconstruction is a good option. The results are\npromising, encouraging further research on this problem.", "comment": "11 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2407.15228v2", "cate": "cs.CV", "date": "2024-07-21", "updated": "2025-07-13"}
{"id": "2507.07426", "title": "DrugMCTS: a drug repurposing framework combining multi-agent, RAG and Monte Carlo Tree Search", "authors": ["Zerui Yang", "Yuwei Wan", "Yinqiao Li", "Yudai Matsuda", "Tong Xie", "Linqi Song"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07426v2", "summary": "Recent advances in large language models have demonstrated considerable\npotential in scientific domains such as drug discovery. However, their\neffectiveness remains constrained when reasoning extends beyond the knowledge\nacquired during pretraining. Conventional approaches, such as fine-tuning or\nretrieval-augmented generation, face limitations in either imposing high\ncomputational overhead or failing to fully exploit structured scientific data.\nTo overcome these challenges, we propose DrugMCTS, a novel framework that\nsynergistically integrates RAG, multi-agent collaboration, and Monte Carlo Tree\nSearch for drug repurposing. The framework employs five specialized agents\ntasked with retrieving and analyzing molecular and protein information, thereby\nenabling structured and iterative reasoning. Without requiring domain-specific\nfine-tuning, DrugMCTS empowers Qwen2.5-7B-Instruct to outperform Deepseek-R1 by\nover 20\\%. Extensive experiments on the DrugBank and KIBA datasets demonstrate\nthat DrugMCTS achieves substantially higher recall and robustness compared to\nboth general-purpose LLMs and deep learning baselines. Our results highlight\nthe importance of structured reasoning, agent-based collaboration, and\nfeedback-driven search mechanisms in advancing LLM applications for drug\ndiscovery.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07426v2", "cate": "cs.AI", "date": "2025-07-10", "updated": "2025-07-12"}
{"id": "2407.19547", "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization", "authors": ["Yushi Huang", "Ruihao Gong", "Xianglong Liu", "Jing Liu", "Yuhang Li", "Jiwen Lu", "Dacheng Tao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by TPAMI 2025. arXiv admin note: substantial text overlap with arXiv:2311.16503", "url": "http://arxiv.org/abs/2407.19547v4", "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.", "comment": "Accepted by TPAMI 2025. arXiv admin note: substantial text overlap\n  with arXiv:2311.16503", "pdf_url": "http://arxiv.org/pdf/2407.19547v4", "cate": "cs.CV", "date": "2024-07-28", "updated": "2025-07-11"}
{"id": "2306.04979", "title": "CoCo: A Coupled Contrastive Framework for Unsupervised Domain Adaptive Graph Classification", "authors": ["Nan Yin", "Li Shen", "Mengzhu Wang", "Long Lan", "Zeyu Ma", "Chong Chen", "Xian-Sheng Hua", "Xiao Luo"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2306.04979v4", "summary": "Although graph neural networks (GNNs) have achieved impressive achievements\nin graph classification, they often need abundant task-specific labels, which\ncould be extensively costly to acquire. A credible solution is to explore\nadditional labeled graphs to enhance unsupervised learning on the target\ndomain. However, how to apply GNNs to domain adaptation remains unsolved owing\nto the insufficient exploration of graph topology and the significant domain\ndiscrepancy. In this paper, we propose Coupled Contrastive Graph Representation\nLearning (CoCo), which extracts the topological information from coupled\nlearning branches and reduces the domain discrepancy with coupled contrastive\nlearning. CoCo contains a graph convolutional network branch and a hierarchical\ngraph kernel network branch, which explore graph topology in implicit and\nexplicit manners. Besides, we incorporate coupled branches into a holistic\nmulti-view contrastive learning framework, which not only incorporates graph\nrepresentations learned from complementary views for enhanced understanding,\nbut also encourages the similarity between cross-domain example pairs with the\nsame semantics for domain alignment. Extensive experiments on popular datasets\nshow that our CoCo outperforms these competing baselines in different settings\ngenerally.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2306.04979v4", "cate": "cs.LG", "date": "2023-06-08", "updated": "2025-07-12"}
{"id": "2408.11447", "title": "GaussianOcc: Fully Self-supervised and Efficient 3D Occupancy Estimation with Gaussian Splatting", "authors": ["Wanshui Gan", "Fang Liu", "Hongbin Xu", "Ningkai Mo", "Naoto Yokoya"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project page: this https URL", "url": "http://arxiv.org/abs/2408.11447v4", "summary": "We introduce GaussianOcc, a systematic method that investigates the two\nusages of Gaussian splatting for fully self-supervised and efficient 3D\noccupancy estimation in surround views. First, traditional methods for\nself-supervised 3D occupancy estimation still require ground truth 6D poses\nfrom sensors during training. To address this limitation, we propose Gaussian\nSplatting for Projection (GSP) module to provide accurate scale information for\nfully self-supervised training from adjacent view projection. Additionally,\nexisting methods rely on volume rendering for final 3D voxel representation\nlearning using 2D signals (depth maps, semantic maps), which is both\ntime-consuming and less effective. We propose Gaussian Splatting from Voxel\nspace (GSV) to leverage the fast rendering properties of Gaussian splatting. As\na result, the proposed GaussianOcc method enables fully self-supervised (no\nground truth pose) 3D occupancy estimation in competitive performance with low\ncomputational cost (2.7 times faster in training and 5 times faster in\nrendering). The relevant code is available in\nhttps://github.com/GANWANSHUI/GaussianOcc.git.", "comment": "Project page: https://ganwanshui.github.io/GaussianOcc/", "pdf_url": "http://arxiv.org/pdf/2408.11447v4", "cate": "cs.CV", "date": "2024-08-21", "updated": "2025-07-14"}
{"id": "2310.14890", "title": "Bounding the Worst-class Error: A Boosting Approach", "authors": ["Yuya Saito", "Shinnosuke Matsuo", "Seiichi Uchida", "Daiki Suehiro"], "categories": ["stat.ML", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2310.14890v2", "summary": "This paper tackles the problem of the worst-class error rate, instead of the\nstandard error rate averaged over all classes. For example, a three-class\nclassification task with class-wise error rates of 10%, 10%, and 40% has a\nworst-class error rate of 40%, whereas the average is 20% under the\nclass-balanced condition. The worst-class error is important in many\napplications. For example, in a medical image classification task, it would not\nbe acceptable for the malignant tumor class to have a 40% error rate, while the\nbenign and healthy classes have a 10% error rates. To avoid overfitting in\nworst-class error minimization using Deep Neural Networks (DNNs), we design a\nproblem formulation for bounding the worst-class error instead of achieving\nzero worst-class error. Moreover, to correctly bound the worst-class error, we\npropose a boosting approach which ensembles DNNs. We give training and\ngeneralization worst-class-error bound. Experimental results show that the\nalgorithm lowers worst-class test error rates while avoiding overfitting to the\ntraining set.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2310.14890v2", "cate": "stat.ML", "date": "2023-10-20", "updated": "2025-07-12"}
{"id": "2408.12429", "title": "FlexEdit: Marrying Free-Shape Masks to VLLM for Flexible Image Editing", "authors": ["Tianshuo Yuan", "Yuxiang Lin", "Jue Wang", "Zhi-Qi Cheng", "Xiaolong Wang", "Jiao GH", "Wei Chen", "Xiaojiang Peng"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      15 pages, 14 figures", "url": "http://arxiv.org/abs/2408.12429v2", "summary": "Combining Vision Large Language Models (VLLMs) with diffusion models offers a\npowerful method for executing image editing tasks based on human language\ninstructions. However, language instructions alone often fall short in\naccurately conveying user requirements, particularly when users want to add,\nreplace elements in specific areas of an image. Luckily, masks can effectively\nindicate the exact locations or elements to be edited, while they require users\nto precisely draw the shapes at the desired locations, which is highly\nuser-unfriendly. To address this, we propose FlexEdit, an end-to-end image\nediting method that leverages both free-shape masks and language instructions\nfor Flexible Editing. Our approach employs a VLLM in comprehending the image\ncontent, mask, and user instructions. Additionally, we introduce the Mask\nEnhance Adapter (MEA) that fuses the embeddings of the VLLM with the image\ndata, ensuring a seamless integration of mask information and model output\nembeddings. Furthermore, we construct FSMI-Edit, a benchmark specifically\ntailored for free-shape mask, including 8 types of free-shape mask. Extensive\nexperiments show that our method achieves state-of-the-art (SOTA) performance\nin LLM-based image editing, and our simple prompting technique stands out in\nits effectiveness. The code and data can be found at\nhttps://github.com/A-new-b/flex_edit.", "comment": "15 pages, 14 figures", "pdf_url": "http://arxiv.org/pdf/2408.12429v2", "cate": "cs.CV", "date": "2024-08-22", "updated": "2025-07-12"}
{"id": "2403.04963", "title": "An In-depth Evaluation of Large Language Models in Sentence Simplification with Error-based Human Assessment", "authors": ["Xuanxin Wu", "Yuki Arase"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted by ACM Transactions on Intelligent Systems and Technology. Our human evaluation corpus is available at: this https URL", "url": "http://arxiv.org/abs/2403.04963v4", "summary": "Recent studies have used both automatic metrics and human evaluations to\nassess the simplification abilities of LLMs. However, the suitability of\nexisting evaluation methodologies for LLMs remains in question. First, the\nsuitability of current automatic metrics on LLMs' simplification evaluation is\nstill uncertain. Second, current human evaluation approaches in sentence\nsimplification often fall into two extremes: they are either too superficial,\nfailing to offer a clear understanding of the models' performance, or overly\ndetailed, making the annotation process complex and prone to inconsistency,\nwhich in turn affects the evaluation's reliability. To address these problems,\nthis study provides in-depth insights into LLMs' performance while ensuring the\nreliability of the evaluation. We design an error-based human annotation\nframework to assess the LLMs' simplification capabilities. We select both\nclosed-source and open-source LLMs, including GPT-4, Qwen2.5-72B, and\nLlama-3.2-3B. We believe that these models offer a representative selection\nacross large, medium, and small sizes of LLMs. Results show that LLMs generally\ngenerate fewer erroneous simplification outputs compared to the previous\nstate-of-the-art. However, LLMs have their limitations, as seen in GPT-4's and\nQwen2.5-72B's struggle with lexical paraphrasing. Furthermore, we conduct\nmeta-evaluations on widely used automatic metrics using our human annotations.\nWe find that these metrics lack sufficient sensitivity to assess the overall\nhigh-quality simplifications, particularly those generated by high-performance\nLLMs.", "comment": "Accepted by ACM Transactions on Intelligent Systems and Technology.\n  Our human evaluation corpus is available at:\n  https://github.com/WuXuanxin/human-eval-llm-simplification", "pdf_url": "http://arxiv.org/pdf/2403.04963v4", "cate": "cs.CL", "date": "2024-03-08", "updated": "2025-07-12"}
{"id": "2409.08824", "title": "Pathfinder for Low-altitude Aircraft with Binary Neural Network", "authors": ["Kaijie Yin", "Tian Gao", "Hui Kong"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.08824v4", "summary": "A prior global topological map (e.g., the OpenStreetMap, OSM) can boost the\nperformance of autonomous mapping by a ground mobile robot. However, the prior\nmap is usually incomplete due to lacking labeling in partial paths. To solve\nthis problem, this paper proposes an OSM maker using airborne sensors carried\nby low-altitude aircraft, where the core of the OSM maker is a novel efficient\npathfinder approach based on LiDAR and camera data, i.e., a binary dual-stream\nroad segmentation model. Specifically, a multi-scale feature extraction based\non the UNet architecture is implemented for images and point clouds. To reduce\nthe effect caused by the sparsity of point cloud, an attention-guided gated\nblock is designed to integrate image and point-cloud features. To optimize the\nmodel for edge deployment that significantly reduces storage footprint and\ncomputational demands, we propose a binarization streamline to each model\ncomponent, including a variant of vision transformer (ViT) architecture as the\nencoder of the image branch, and new focal and perception losses to optimize\nthe model training. The experimental results on two datasets demonstrate that\nour pathfinder method achieves SOTA accuracy with high efficiency in finding\npaths from the low-level airborne sensors, and we can create complete OSM prior\nmaps based on the segmented road skeletons. Code and data are available at:\n\\href{https://github.com/IMRL/Pathfinder}{https://github.com/IMRL/Pathfinder}.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.08824v4", "cate": "cs.CV", "date": "2024-09-13", "updated": "2025-07-14"}
{"id": "2404.13910", "title": "Integrated Gradient Correlation: a Dataset-wise Attribution Method", "authors": ["Pierre Lelièvre", "Chien-Chung Chen"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      16 pages, 6 figures, source code at this https URL", "url": "http://arxiv.org/abs/2404.13910v2", "summary": "Attribution methods are primarily designed to study input component\ncontributions to individual model predictions. However, some research\napplications require a summary of attribution patterns across the entire\ndataset to facilitate the interpretability of the scrutinized models at a\ntask-level rather than an instance-level. It specifically applies when the\nlocalization of important input information is supposed to be stable for a\nspecific problem but remains unidentified among numerous components. In this\npaper, we present a dataset-wise attribution method called Integrated Gradient\nCorrelation (IGC) that enables region-specific analysis by a direct summation\nover associated components, and further relates the sum of all attributions to\na model prediction score (correlation). We demonstrate IGC on synthetic data\nand fMRI neural signals (NSD dataset) with the study of the representation of\nimage features in the brain and the estimation of the visual receptive field of\nneural populations. The resulting IGC attributions reveal selective patterns,\ncoherent with respective model objectives.", "comment": "16 pages, 6 figures, source code at\n  https://github.com/plelievre/int_grad_corr", "pdf_url": "http://arxiv.org/pdf/2404.13910v2", "cate": "cs.LG", "date": "2024-04-22", "updated": "2025-07-14"}
{"id": "2409.10925", "title": "HGSLoc: 3DGS-based Heuristic Camera Pose Refinement", "authors": ["Zhongyan Niu", "Zhen Tan", "Jinpu Zhang", "Xueliang Yang", "Dewen Hu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.10925v3", "summary": "Visual localization refers to the process of determining camera poses and\norientation within a known scene representation. This task is often complicated\nby factors such as changes in illumination and variations in viewing angles. In\nthis paper, we propose HGSLoc, a novel lightweight plug-and-play pose\noptimization framework, which integrates 3D reconstruction with a heuristic\nrefinement strategy to achieve higher pose estimation accuracy. Specifically,\nwe introduce an explicit geometric map for 3D representation and high-fidelity\nrendering, allowing the generation of high-quality synthesized views to support\naccurate visual localization. Our method demonstrates higher localization\naccuracy compared to NeRF-based neural rendering localization approaches. We\nintroduce a heuristic refinement strategy, its efficient optimization\ncapability can quickly locate the target node, while we set the step level\noptimization step to enhance the pose accuracy in the scenarios with small\nerrors. With carefully designed heuristic functions, it offers efficient\noptimization capabilities, enabling rapid error reduction in rough localization\nestimations. Our method mitigates the dependence on complex neural network\nmodels while demonstrating improved robustness against noise and higher\nlocalization accuracy in challenging environments, as compared to neural\nnetwork joint optimization strategies. The optimization framework proposed in\nthis paper introduces novel approaches to visual localization by integrating\nthe advantages of 3D reconstruction and the heuristic refinement strategy,\nwhich demonstrates strong performance across multiple benchmark datasets,\nincluding 7Scenes and Deep Blending dataset. The implementation of our method\nhas been released at https://github.com/anchang699/HGSLoc.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.10925v3", "cate": "cs.CV", "date": "2024-09-17", "updated": "2025-07-14"}
{"id": "2405.07344", "title": "TKAN: Temporal Kolmogorov-Arnold Networks", "authors": ["Remi Genet", "Hugo Inzirillo"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2405.07344v4", "summary": "Recurrent Neural Networks (RNNs) have revolutionized many areas of machine\nlearning, particularly in natural language and data sequence processing. Long\nShort-Term Memory (LSTM) has demonstrated its ability to capture long-term\ndependencies in sequential data. Inspired by the Kolmogorov-Arnold Networks\n(KANs) a promising alternatives to Multi-Layer Perceptrons (MLPs), we proposed\na new neural networks architecture inspired by KAN and the LSTM, the Temporal\nKolomogorov-Arnold Networks (TKANs). TKANs combined the strenght of both\nnetworks, it is composed of Recurring Kolmogorov-Arnold Networks (RKANs) Layers\nembedding memory management. This innovation enables us to perform multi-step\ntime series forecasting with enhanced accuracy and efficiency. By addressing\nthe limitations of traditional models in handling complex sequential patterns,\nthe TKAN architecture offers significant potential for advancements in fields\nrequiring more than one step ahead forecasting.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2405.07344v4", "cate": "cs.LG", "date": "2024-05-12", "updated": "2025-07-14"}
{"id": "2410.02072", "title": "A Practical Approach to Underwater Depth and Surface Normals Estimation", "authors": ["Alzayat Saleh", "Melanie Olsen", "Bouchra Senadji", "Mostafa Rahimi Azghadi"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      18 pages, 6 figures, 8 tables. Submitted to Elsevier", "url": "http://arxiv.org/abs/2410.02072v2", "summary": "Monocular Depth and Surface Normals Estimation (MDSNE) is crucial for tasks\nsuch as 3D reconstruction, autonomous navigation, and underwater exploration.\nCurrent methods rely either on discriminative models, which struggle with\ntransparent or reflective surfaces, or generative models, which, while\naccurate, are computationally expensive. This paper presents a novel deep\nlearning model for MDSNE, specifically tailored for underwater environments,\nusing a hybrid architecture that integrates Convolutional Neural Networks\n(CNNs) with Transformers, leveraging the strengths of both approaches. Training\neffective MDSNE models is often hampered by noisy real-world datasets and the\nlimited generalization of synthetic datasets. To address this, we generate\npseudo-labeled real data using multiple pre-trained MDSNE models. To ensure the\nquality of this data, we propose the Depth Normal Evaluation and Selection\nAlgorithm (DNESA), which evaluates and selects the most reliable pseudo-labeled\nsamples using domain-specific metrics. A lightweight student model is then\ntrained on this curated dataset. Our model reduces parameters by 90% and\ntraining costs by 80%, allowing real-time 3D perception on resource-constrained\ndevices. Key contributions include: a novel and efficient MDSNE model, the\nDNESA algorithm, a domain-specific data pipeline, and a focus on real-time\nperformance and scalability. Designed for real-world underwater applications,\nour model facilitates low-cost deployments in underwater robots and autonomous\nvehicles, bridging the gap between research and practical implementation.", "comment": "18 pages, 6 figures, 8 tables. Submitted to Elsevier", "pdf_url": "http://arxiv.org/pdf/2410.02072v2", "cate": "cs.CV", "date": "2024-10-02", "updated": "2025-07-12"}
{"id": "2405.11870", "title": "Intuitive Fine-Tuning: Towards Simplifying Alignment into a Single Process", "authors": ["Ermo Hua", "Biqing Qi", "Kaiyan Zhang", "Kai Tian", "Xingtai Lv", "Ning Ding", "Bowen Zhou"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted to ACL 2025, Oral & Panel Discussion", "url": "http://arxiv.org/abs/2405.11870v3", "summary": "Supervised Fine-Tuning (SFT) and Preference Optimization (PO) are key\nprocesses for aligning Language Models (LMs) with human preferences post\npre-training. While SFT excels in efficiency and PO in effectiveness, they are\noften combined sequentially without integrating their optimization objectives.\nThis approach ignores the opportunities to bridge their paradigm gap and take\nthe strengths from both. In this paper, we interpret SFT and PO with two\nsub-processes -- Preference Estimation and Transition Optimization -- defined\nat token level within the Markov Decision Process (MDP). This modeling shows\nthat SFT is only a special case of PO with inferior estimation and\noptimization. PO estimates the model's preference by its entire generation,\nwhile SFT only scores model's subsequent predicted tokens based on prior tokens\nfrom ground truth answer. These priors deviates from model's distribution,\nhindering the preference estimation and transition optimization. Building on\nthis view, we introduce Intuitive Fine-Tuning (IFT) to integrate SFT and PO\ninto a single process. Through a temporal residual connection, IFT brings\nbetter estimation and optimization by capturing LMs' intuitive sense of its\nentire answers. But it solely relies on a single policy and the same volume of\nnon-preference-labeled data as SFT. Our experiments show that IFT performs\ncomparably or even superiorly to SFT and some typical PO methods across several\ntasks, particularly those require generation, reasoning, and fact-following\nabilities. An explainable Frozen Lake game further validates the effectiveness\nof IFT for getting competitive policy.", "comment": "Accepted to ACL 2025, Oral & Panel Discussion", "pdf_url": "http://arxiv.org/pdf/2405.11870v3", "cate": "cs.CL", "date": "2024-05-20", "updated": "2025-07-14"}
{"id": "2410.07151", "title": "DH-FaceVid-1K: A Large-Scale High-Quality Dataset for Face Video Generation", "authors": ["Donglin Di", "He Feng", "Wenzhang Sun", "Yongjia Ma", "Hao Li", "Wei Chen", "Lei Fan", "Tonghua Su", "Xun Yang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.07151v2", "summary": "Human-centric generative models are becoming increasingly popular, giving\nrise to various innovative tools and applications, such as talking face videos\nconditioned on text or audio prompts. The core of these capabilities lies in\npowerful pre-trained foundation models, trained on large-scale, high-quality\ndatasets. However, many advanced methods rely on in-house data subject to\nvarious constraints, and other current studies fail to generate high-resolution\nface videos, which is mainly attributed to the significant lack of large-scale,\nhigh-quality face video datasets. In this paper, we introduce a human face\nvideo dataset, \\textbf{DH-FaceVid-1K}. Our collection spans 1,200 hours in\ntotal, encompassing 270,043 video clips from over 20,000 individuals. Each\nsample includes corresponding speech audio, facial keypoints, and text\nannotations. Compared to other publicly available datasets, ours distinguishes\nitself through its multi-ethnic coverage and high-quality, comprehensive\nindividual attributes. We establish multiple face video generation models\nsupporting tasks such as text-to-video and image-to-video generation. In\naddition, we develop comprehensive benchmarks to validate the scaling law when\nusing different proportions of proposed dataset. Our primary aim is to\ncontribute a face video dataset, particularly addressing the\nunderrepresentation of Asian faces in existing curated datasets and thereby\nenriching the global spectrum of face-centric data and mitigating demographic\nbiases. \\textbf{Project Page:} https://luna-ai-lab.github.io/DH-FaceVid-1K/", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.07151v2", "cate": "cs.CV", "date": "2024-09-23", "updated": "2025-07-13"}
{"id": "2407.07290", "title": "Causal Discovery-Driven Change Point Detection in Time Series", "authors": ["Shanyun Gao", "Raghavendra Addanki", "Tong Yu", "Ryan A. Rossi", "Murat Kocaoglu"], "categories": ["cs.LG", "cs.AI", "stat.ML", "I.2.6; G.3"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Proceedings of the 28th International Conference on Artificial Intelligence and Statistics (AISTATS) 2025, Mai Khao, Thailand. PMLR: Volume 258", "url": "http://arxiv.org/abs/2407.07290v2", "summary": "Change point detection in time series aims to identify moments when the\nprobability distribution of time series changes. It is widely applied in many\nareas, such as human activity sensing and medical science. In the context of\nmultivariate time series, this typically involves examining the joint\ndistribution of multiple variables: If the distribution of any one variable\nchanges, the entire time series undergoes a distribution shift. However, in\npractical applications, we may be interested only in certain components of the\ntime series, exploring abrupt changes in their distributions while accounting\nfor the presence of other components. Here, assuming an underlying structural\ncausal model that governs the time-series data generation, we address this task\nby proposing a two-stage non-parametric algorithm that first learns parts of\nthe causal structure through constraint-based discovery methods, and then\nemploys conditional relative Pearson divergence estimation to identify the\nchange points. The conditional relative Pearson divergence quantifies the\ndistribution difference between consecutive segments in the time series, while\nthe causal discovery method allows a focus on the causal mechanism,\nfacilitating access to independent and identically distributed (IID) samples.\nTheoretically, the typical assumption of samples being IID in conventional\nchange point detection methods can be relaxed based on the Causal Markov\nCondition. Through experiments on both synthetic and real-world datasets, we\nvalidate the correctness and utility of our approach.", "comment": "Proceedings of the 28th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2025, Mai Khao, Thailand. PMLR: Volume\n  258", "pdf_url": "http://arxiv.org/pdf/2407.07290v2", "cate": "cs.LG", "date": "2024-07-10", "updated": "2025-07-12"}
{"id": "2410.10563", "title": "MEGA-Bench: Scaling Multimodal Evaluation to over 500 Real-World Tasks", "authors": ["Jiacheng Chen", "Tianhao Liang", "Sherman Siu", "Zhengqing Wang", "Kai Wang", "Yubo Wang", "Yuansheng Ni", "Wang Zhu", "Ziyan Jiang", "Bohan Lyu", "Dongfu Jiang", "Xuan He", "Yuan Liu", "Hexiang Hu", "Xiang Yue", "Wenhu Chen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICLR 2025 camera-ready version. Project page: this https URL", "url": "http://arxiv.org/abs/2410.10563v3", "summary": "We present MEGA-Bench, an evaluation suite that scales multimodal evaluation\nto over 500 real-world tasks, to address the highly heterogeneous daily use\ncases of end users. Our objective is to optimize for a set of high-quality data\nsamples that cover a highly diverse and rich set of multimodal tasks, while\nenabling cost-effective and accurate model evaluation. In particular, we\ncollected 505 realistic tasks encompassing over 8,000 samples from 16 expert\nannotators to extensively cover the multimodal task space. Instead of unifying\nthese problems into standard multi-choice questions (like MMMU, MMBench, and\nMMT-Bench), we embrace a wide range of output formats like numbers, phrases,\ncode, \\LaTeX, coordinates, JSON, free-form, etc. To accommodate these formats,\nwe developed over 40 metrics to evaluate these tasks. Unlike existing\nbenchmarks, MEGA-Bench offers a fine-grained capability report across multiple\ndimensions (e.g., application, input type, output format, skill), allowing\nusers to interact with and visualize model capabilities in depth. We evaluate a\nwide variety of frontier vision-language models on MEGA-Bench to understand\ntheir capabilities across these dimensions.", "comment": "ICLR 2025 camera-ready version. Project page:\n  https://tiger-ai-lab.github.io/MEGA-Bench/", "pdf_url": "http://arxiv.org/pdf/2410.10563v3", "cate": "cs.CV", "date": "2024-10-14", "updated": "2025-07-13"}
{"id": "2408.11415", "title": "Political Bias in LLMs: Unaligned Moral Values in Agent-centric Simulations", "authors": ["Simon Münker"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      14 pages, 2 tables", "url": "http://arxiv.org/abs/2408.11415v2", "summary": "Contemporary research in social sciences increasingly utilizes\nstate-of-the-art generative language models to annotate or generate content.\nWhile these models achieve benchmark-leading performance on common language\ntasks, their application to novel out-of-domain tasks remains insufficiently\nexplored. To address this gap, we investigate how personalized language models\nalign with human responses on the Moral Foundation Theory Questionnaire. We\nadapt open-source generative language models to different political personas\nand repeatedly survey these models to generate synthetic data sets where\nmodel-persona combinations define our sub-populations. Our analysis reveals\nthat models produce inconsistent results across multiple repetitions, yielding\nhigh response variance. Furthermore, the alignment between synthetic data and\ncorresponding human data from psychological studies shows a weak correlation,\nwith conservative persona-prompted models particularly failing to align with\nactual conservative populations. These results suggest that language models\nstruggle to coherently represent ideologies through in-context prompting due to\ntheir alignment process. Thus, using language models to simulate social\ninteractions requires measurable improvements in in-context optimization or\nparameter manipulation to align with psychological and sociological stereotypes\nproperly.", "comment": "14 pages, 2 tables", "pdf_url": "http://arxiv.org/pdf/2408.11415v2", "cate": "cs.CL", "date": "2024-08-21", "updated": "2025-07-14"}
{"id": "2411.07625", "title": "Unraveling the Connections between Flow Matching and Diffusion Probabilistic Models in Training-free Conditional Generation", "authors": ["Kaiyu Song", "Hanjiang Lai"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.07625v2", "summary": "Training-free conditional generation based on flow matching aims to leverage\npre-trained unconditional flow matching models to perform conditional\ngeneration without retraining. Recently, a successful training-free conditional\ngeneration approach incorporates conditions via posterior sampling, which\nrelies on the availability of a score function in the unconditional diffusion\nmodel. However, flow matching models do not possess an explicit score function,\nrendering such a strategy inapplicable. Approximate posterior sampling for flow\nmatching has been explored, but it is limited to linear inverse problems. In\nthis paper, we propose Flow Matching-based Posterior Sampling (FMPS) to expand\nits application scope. We introduce a correction term by steering the velocity\nfield. This correction term can be reformulated to incorporate a surrogate\nscore function, thereby bridging the gap between flow matching models and\nscore-based posterior sampling. Hence, FMPS enables the posterior sampling to\nbe adjusted within the flow matching framework. Further, we propose two\npractical implementations of the correction mechanism: one aimed at improving\ngeneration quality, and the other focused on computational efficiency.\nExperimental results on diverse conditional generation tasks demonstrate that\nour method achieves superior generation quality compared to existing\nstate-of-the-art approaches, validating the effectiveness and generality of\nFMPS.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.07625v2", "cate": "cs.CV", "date": "2024-11-12", "updated": "2025-07-14"}
{"id": "2409.06241", "title": "DiPT: Enhancing LLM reasoning through diversified perspective-taking", "authors": ["Hoang Anh Just", "Mahavir Dabas", "Lifu Huang", "Ming Jin", "Ruoxi Jia"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      LLM Reasoning with Perspectives, NAACL 2025 Findings", "url": "http://arxiv.org/abs/2409.06241v2", "summary": "Existing work on improving language model reasoning typically explores a\nsingle solution path, which can be prone to errors. Inspired by\nperspective-taking in social studies, this paper introduces DiPT, a novel\napproach that complements current reasoning methods by explicitly incorporating\ndiversified viewpoints. This approach allows the model to gain a deeper\nunderstanding of the problem's context and identify the most effective solution\npath during the inference stage. Additionally, it provides a general\ndata-centric AI recipe for augmenting existing data to improve their quality\nfor fine-tuning.\n  Our empirical results demonstrate that DiPT can be flexibly integrated into\nexisting methods that focus on a single reasoning approach, enhancing their\nreasoning performance and stability when presented with paraphrased problems.\nFurthermore, we illustrate improved context understanding by maintaining the\nmodel's safe outputs against \"jailbreaking\" prompts intentionally designed to\nbypass safeguards built into deployed models. Lastly, we show that fine-tuning\nwith data enriched with diverse perspectives can boost the reasoning\ncapabilities of the model compared to fine-tuning with raw data alone.", "comment": "LLM Reasoning with Perspectives, NAACL 2025 Findings", "pdf_url": "http://arxiv.org/pdf/2409.06241v2", "cate": "cs.LG", "date": "2024-09-10", "updated": "2025-07-13"}
{"id": "2411.10086", "title": "CorrCLIP: Reconstructing Patch Correlations in CLIP for Open-Vocabulary Semantic Segmentation", "authors": ["Dengke Zhang", "Fagui Liu", "Quan Tang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2411.10086v2", "summary": "Open-vocabulary semantic segmentation aims to assign semantic labels to each\npixel without being constrained by a predefined set of categories. While\nContrastive Language-Image Pre-training (CLIP) excels in zero-shot\nclassification, it struggles to align image patches with category embeddings\nbecause of its incoherent patch correlations. This study reveals that\ninter-class correlations are the main reason for impairing CLIP's segmentation\nperformance. Accordingly, we propose CorrCLIP, which reconstructs the scope and\nvalue of patch correlations. Specifically, CorrCLIP leverages the Segment\nAnything Model (SAM) to define the scope of patch interactions, reducing\ninter-class correlations. To mitigate the problem that SAM-generated masks may\ncontain patches belonging to different classes, CorrCLIP incorporates\nself-supervised models to compute coherent similarity values, suppressing the\nweight of inter-class correlations. Additionally, we introduce two additional\nbranches to strengthen patch features' spatial details and semantic\nrepresentation. Finally, we update segmentation maps with SAM-generated masks\nto improve spatial consistency. Based on the improvement across patch\ncorrelations, feature representations, and segmentation maps, CorrCLIP achieves\nsuperior performance across eight benchmarks. Codes are available at:\nhttps://github.com/zdk258/CorrCLIP.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2411.10086v2", "cate": "cs.CV", "date": "2024-11-15", "updated": "2025-07-13"}
{"id": "2409.17517", "title": "Dataset Distillation-based Hybrid Federated Learning on Non-IID Data", "authors": ["Xiufang Shi", "Wei Zhang", "Mincheng Wu", "Guangyi Liu", "Zhenyu Wen", "Shibo He", "Tejal Shah", "Rajiv Ranjan"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.17517v2", "summary": "With the development of edge computing, Federated Learning (FL) has emerged\nas a promising solution for the intelligent Internet of Things (IoT). However,\napplying FL in mobile edge-cloud networks is greatly challenged by statistical\nheterogeneity and high communication overhead. To address it, we propose a\nhybrid federated learning framework called HFLDD, which integrates dataset\ndistillation to generate approximately independent and equally distributed\n(IID) data, thereby improving the performance of model training. In particular,\nwe partition the clients into heterogeneous clusters, where the data labels\namong different clients within a cluster are unbalanced while the data labels\namong different clusters are balanced. The cluster heads collect distilled data\nfrom the corresponding cluster members, and conduct model training in\ncollaboration with the server. This training process is like traditional\nfederated learning on IID data, and hence effectively alleviates the impact of\nnon-IID data on model training. We perform a comprehensive analysis of the\nconvergence behavior, communication overhead, and computational complexity of\nthe proposed HFLDD. Extensive experimental results based on multiple public\ndatasets demonstrate that when data labels are severely imbalanced, the\nproposed HFLDD outperforms the baseline methods in terms of both test accuracy\nand communication cost.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.17517v2", "cate": "cs.LG", "date": "2024-09-26", "updated": "2025-07-14"}
{"id": "2411.10440", "title": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step", "authors": ["Guowei Xu", "Peng Jin", "Ziang Wu", "Hao Li", "Yibing Song", "Lichao Sun", "Li Yuan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      17 pages, ICCV 2025", "url": "http://arxiv.org/abs/2411.10440v5", "summary": "Large language models have demonstrated substantial advancements in reasoning\ncapabilities. However, current Vision-Language Models (VLMs) often struggle to\nperform systematic and structured reasoning, especially when handling complex\nvisual question-answering tasks. In this work, we introduce LLaVA-CoT, a large\nVLM designed to conduct autonomous multistage reasoning. Unlike\nchain-of-thought prompting, LLaVA-CoT independently engages in sequential\nstages of summarization, visual interpretation, logical reasoning, and\nconclusion generation. This structured approach enables LLaVA-CoT to achieve\nmarked improvements on reasoning-intensive tasks. To accomplish this, we\nconstruct the LLaVA-CoT-100k dataset, integrating samples from various visual\nquestion answering sources and providing structured reasoning annotations.\nBesides, we propose a test-time stage-wise retracing search method (SWIRES),\nwhich enables effective and efficient test-time scaling. Remarkably, with only\n100k training samples and test-time scaling, LLaVA-CoT not only outperforms its\nbase model by 9.4% on a wide range of multimodal reasoning benchmarks, but also\nsurpasses the performance of larger and even closed-source models, such as\nGemini-1.5-pro, GPT-4o-mini, and Llama-3.2-90B-Vision-Instruct. The code,\ndataset, and pre-trained weights are publicly available at\nhttps://github.com/PKU-YuanGroup/LLaVA-CoT.", "comment": "17 pages, ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2411.10440v5", "cate": "cs.CV", "date": "2024-11-15", "updated": "2025-07-13"}
{"id": "2409.19949", "title": "Task-Agnostic Pre-training and Task-Guided Fine-tuning for Versatile Diffusion Planner", "authors": ["Chenyou Fan", "Chenjia Bai", "Zhao Shan", "Haoran He", "Yang Zhang", "Zhen Wang"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Published at ICML 2025", "url": "http://arxiv.org/abs/2409.19949v3", "summary": "Diffusion models have demonstrated their capabilities in modeling\ntrajectories of multi-tasks. However, existing multi-task planners or policies\ntypically rely on task-specific demonstrations via multi-task imitation, or\nrequire task-specific reward labels to facilitate policy optimization via\nReinforcement Learning (RL). They are costly due to the substantial human\nefforts required to collect expert data or design reward functions. To address\nthese challenges, we aim to develop a versatile diffusion planner capable of\nleveraging large-scale inferior data that contains task-agnostic sub-optimal\ntrajectories, with the ability to fast adapt to specific tasks. In this paper,\nwe propose SODP, a two-stage framework that leverages Sub-Optimal data to learn\na Diffusion Planner, which is generalizable for various downstream tasks.\nSpecifically, in the pre-training stage, we train a foundation diffusion\nplanner that extracts general planning capabilities by modeling the versatile\ndistribution of multi-task trajectories, which can be sub-optimal and has wide\ndata coverage. Then for downstream tasks, we adopt RL-based fine-tuning with\ntask-specific rewards to quickly refine the diffusion planner, which aims to\ngenerate action sequences with higher task-specific returns. Experimental\nresults from multi-task domains including Meta-World and Adroit demonstrate\nthat SODP outperforms state-of-the-art methods with only a small amount of data\nfor reward-guided fine-tuning.", "comment": "Published at ICML 2025", "pdf_url": "http://arxiv.org/pdf/2409.19949v3", "cate": "cs.LG", "date": "2024-09-30", "updated": "2025-07-13"}
{"id": "2411.15260", "title": "VIVID-10M: A Dataset and Baseline for Versatile and Interactive Video Local Editing", "authors": ["Jiahao Hu", "Tianxiong Zhong", "Xuebo Wang", "Boyuan Jiang", "Xingye Tian", "Fei Yang", "Pengfei Wan", "Di Zhang"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 pages, 10 figures", "url": "http://arxiv.org/abs/2411.15260v2", "summary": "Diffusion-based image editing models have made remarkable progress in recent\nyears. However, achieving high-quality video editing remains a significant\nchallenge. One major hurdle is the absence of open-source, large-scale video\nediting datasets based on real-world data, as constructing such datasets is\nboth time-consuming and costly. Moreover, video data requires a significantly\nlarger number of tokens for representation, which substantially increases the\ntraining costs for video editing models. Lastly, current video editing models\noffer limited interactivity, often making it difficult for users to express\ntheir editing requirements effectively in a single attempt. To address these\nchallenges, this paper introduces a dataset VIVID-10M and a baseline model\nVIVID. VIVID-10M is the first large-scale hybrid image-video local editing\ndataset aimed at reducing data construction and model training costs, which\ncomprises 9.7M samples that encompass a wide range of video editing tasks.\nVIVID is a Versatile and Interactive VIdeo local eDiting model trained on\nVIVID-10M, which supports entity addition, modification, and deletion. At its\ncore, a keyframe-guided interactive video editing mechanism is proposed,\nenabling users to iteratively edit keyframes and propagate it to other frames,\nthereby reducing latency in achieving desired outcomes. Extensive experimental\nevaluations show that our approach achieves state-of-the-art performance in\nvideo local editing, surpassing baseline methods in both automated metrics and\nuser studies. The VIVID-10M dataset are open-sourced at\nhttps://kwaivgi.github.io/VIVID/.", "comment": "10 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2411.15260v2", "cate": "cs.CV", "date": "2024-11-22", "updated": "2025-07-14"}
{"id": "2410.06238", "title": "EVOLvE: Evaluating and Optimizing LLMs For In-Context Exploration", "authors": ["Allen Nie", "Yi Su", "Bo Chang", "Jonathan N. Lee", "Ed H. Chi", "Quoc V. Le", "Minmin Chen"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      28 pages. Published at ICML 2025", "url": "http://arxiv.org/abs/2410.06238v2", "summary": "Despite their success in many domains, large language models (LLMs) remain\nunder-studied in scenarios requiring optimal decision-making under uncertainty.\nThis is crucial as many real-world applications, ranging from personalized\nrecommendations to healthcare interventions, demand that LLMs not only predict\nbut also actively learn to make optimal decisions through exploration. In this\nwork, we measure LLMs' (in)ability to make optimal decisions in bandits, a\nstate-less reinforcement learning setting relevant to many applications. We\ndevelop a comprehensive suite of environments, including both context-free and\ncontextual bandits with varying task difficulties, to benchmark LLMs'\nperformance. Motivated by the existence of optimal exploration algorithms, we\npropose efficient ways to integrate this algorithmic knowledge into LLMs: by\nproviding explicit algorithm-guided support during inference; and through\nalgorithm distillation via in-context demonstrations and fine-tuning, using\nsynthetic data generated from these algorithms. Impressively, these techniques\nallow us to achieve superior exploration performance with smaller models,\nsurpassing larger models on various tasks. We conducted an extensive ablation\nstudy to shed light on various factors, such as task difficulty and data\nrepresentation, that influence the efficiency of LLM exploration. Additionally,\nwe conduct a rigorous analysis of the LLM's exploration efficiency using the\nconcept of regret, linking its ability to explore to the model size and\nunderlying algorithm.", "comment": "28 pages. Published at ICML 2025", "pdf_url": "http://arxiv.org/pdf/2410.06238v2", "cate": "cs.LG", "date": "2024-10-08", "updated": "2025-07-14"}
{"id": "2411.19700", "title": "Explaining the Impact of Training on Vision Models via Activation Clustering", "authors": ["Ahcène Boubekki", "Samuel G. Fadel", "Sebastian Mair"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.19700v4", "summary": "This paper introduces Neuro-Activated Vision Explanations (NAVE), a method\nfor extracting and visualizing the internal representations of vision model\nencoders. By clustering feature activations, NAVE provides insights into\nlearned semantics without fine-tuning. Using object localization, we show that\nNAVE's concepts align with image semantics. Through extensive experiments, we\nanalyze the impact of training strategies and architectures on encoder\nrepresentation capabilities. Additionally, we apply NAVE to study training\nartifacts in vision transformers and reveal how weak training strategies and\nspurious correlations degrade model performance. Our findings establish NAVE as\na valuable tool for post-hoc model inspection and improving transparency in\nvision models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.19700v4", "cate": "cs.CV", "date": "2024-11-29", "updated": "2025-07-14"}
{"id": "2410.18076", "title": "Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration", "authors": ["Max Wilcoxson", "Qiyang Li", "Kevin Frans", "Sergey Levine"], "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      28 pages, 20 figures, 42nd International Conference on Machine Learning (ICML 2025)", "url": "http://arxiv.org/abs/2410.18076v4", "summary": "Unsupervised pretraining has been transformative in many supervised domains.\nHowever, applying such ideas to reinforcement learning (RL) presents a unique\nchallenge in that fine-tuning does not involve mimicking task-specific data,\nbut rather exploring and locating the solution through iterative\nself-improvement. In this work, we study how unlabeled offline trajectory data\ncan be leveraged to learn efficient exploration strategies. While prior data\ncan be used to pretrain a set of low-level skills, or as additional off-policy\ndata for online RL, it has been unclear how to combine these ideas effectively\nfor online exploration. Our method SUPE (Skills from Unlabeled Prior data for\nExploration) demonstrates that a careful combination of these ideas compounds\ntheir benefits. Our method first extracts low-level skills using a variational\nautoencoder (VAE), and then pseudo-labels unlabeled trajectories with\noptimistic rewards and high-level action labels, transforming prior data into\nhigh-level, task-relevant examples that encourage novelty-seeking behavior.\nFinally, SUPE uses these transformed examples as additional off-policy data for\nonline RL to learn a high-level policy that composes pretrained low-level\nskills to explore efficiently. In our experiments, SUPE consistently\noutperforms prior strategies across a suite of 42 long-horizon, sparse-reward\ntasks. Code: https://github.com/rail-berkeley/supe.", "comment": "28 pages, 20 figures, 42nd International Conference on Machine\n  Learning (ICML 2025)", "pdf_url": "http://arxiv.org/pdf/2410.18076v4", "cate": "cs.LG", "date": "2024-10-23", "updated": "2025-07-11"}
{"id": "2412.03409", "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following Models Need for Efficient Generation", "authors": ["Ao Wang", "Hui Chen", "Jiaxin Li", "Jianchao Tan", "Kefeng Zhang", "Xunliang Cai", "Zijia Lin", "Jungong Han", "Guiguang Ding"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      12 pages, 5 figures;", "url": "http://arxiv.org/abs/2412.03409v3", "summary": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at https://github.com/THU-MIG/PrefixKV.", "comment": "12 pages, 5 figures;", "pdf_url": "http://arxiv.org/pdf/2412.03409v3", "cate": "cs.CV", "date": "2024-12-04", "updated": "2025-07-14"}
{"id": "2410.18164", "title": "TabDPT: Scaling Tabular Foundation Models on Real Data", "authors": ["Junwei Ma", "Valentin Thomas", "Rasa Hosseinzadeh", "Hamidreza Kamkari", "Alex Labach", "Jesse C. Cresswell", "Keyvan Golestan", "Guangwei Yu", "Anthony L. Caterini", "Maksims Volkovs"], "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Inference repo: this http URL Training repo: this http URL", "url": "http://arxiv.org/abs/2410.18164v2", "summary": "Tabular data is one of the most ubiquitous sources of information worldwide,\nspanning a wide variety of domains. This inherent heterogeneity has slowed the\ndevelopment of Tabular Foundation Models (TFMs) capable of fast generalization\nto unseen datasets. In-Context Learning (ICL) has recently emerged as a\npromising solution for TFMs, enabling dynamic adaptation to new tasks without\nadditional tuning. While many studies have attempted to re-purpose large\nlanguage models for tabular ICL, they have had limited success, so recent works\nhave focused on developing tabular-specific foundation models. In this work, we\npropose an approach to combine ICL-based retrieval with self supervised\nlearning to train tabular foundation models. We also investigate the utility of\nreal vs. synthetic data for model pre-training, and show that real data can\ncontain useful signal not easily captured in synthetic training. Specifically,\nwe show that incorporating real data during the pre-training phase can lead to\nsignificantly faster training and better downstream generalization to unseen\ndata. Our resulting model, TabDPT, achieves top performance on both regression\n(CTR23) and classification (CC18) benchmarks. Importantly, we also demonstrate\nthat with our pre-training procedure, scaling both model and data size leads to\nconsistent performance improvements that follow power laws. This echoes scaling\nlaws in LLMs and other foundation models, and suggests that Internet-scale TFMs\ncan be achievable. We open-source our full pipeline: inference code including\ntrained model weights can be found at\ngithub.com/layer6ai-labs/TabDPT-inference, and the training code to reproduce\nexperiments can be found at github.com/layer6ai-labs/TabDPT-training.", "comment": "Inference repo: github.com/layer6ai-labs/TabDPT-inference; Training\n  repo: github.com/layer6ai-labs/TabDPT-training", "pdf_url": "http://arxiv.org/pdf/2410.18164v2", "cate": "cs.LG", "date": "2024-10-23", "updated": "2025-07-12"}
{"id": "2412.04189", "title": "HANDI: Hand-Centric Text-and-Image Conditioned Video Generation", "authors": ["Yayuan Li", "Zhi Cao", "Jason J. Corso"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      16 pages, 7 figures and 4 tables", "url": "http://arxiv.org/abs/2412.04189v5", "summary": "Despite the recent strides in video generation, state-of-the-art methods\nstill struggle with elements of visual detail. One particularly challenging\ncase is the class of videos in which the intricate motion of the hand coupled\nwith a mostly stable and otherwise distracting environment is necessary to\nconvey the execution of some complex action and its effects. To address these\nchallenges, we introduce a new method for video generation that focuses on\nhand-centric actions. Our diffusion-based method incorporates two distinct\ninnovations. First, we propose an automatic method to generate the motion area\n-- the region in the video in which the detailed activities occur -- guided by\nboth the visual context and the action text prompt, rather than assuming this\nregion can be provided manually as is now commonplace. Second, we introduce a\ncritical Hand Refinement Loss to guide the diffusion model to focus on smooth\nand consistent hand poses. We evaluate our method on challenging augmented\ndatasets based on EpicKitchens and Ego4D, demonstrating significant\nimprovements over state-of-the-art methods in terms of action clarity,\nespecially of the hand motion in the target region, across diverse environments\nand actions. Video results can be found in\nhttps://excitedbutter.github.io/project_page", "comment": "16 pages, 7 figures and 4 tables", "pdf_url": "http://arxiv.org/pdf/2412.04189v5", "cate": "cs.CV", "date": "2024-12-05", "updated": "2025-07-14"}
{"id": "2410.19919", "title": "Provably Adaptive Average Reward Reinforcement Learning for Metric Spaces", "authors": ["Avik Kar", "Rahul Singh"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted in the 41st Conference on Uncertainty in Artificial Intelligence", "url": "http://arxiv.org/abs/2410.19919v2", "summary": "We study infinite-horizon average-reward reinforcement learning (RL) for\nLipschitz MDPs, a broad class that subsumes several important classes such as\nlinear and RKHS MDPs, function approximation frameworks, and develop an\nadaptive algorithm $\\text{ZoRL}$ with regret bounded as $\\mathcal{O}\\big(T^{1 -\nd_{\\text{eff.}}^{-1}}\\big)$, where $d_{\\text{eff.}}= 2d_\\mathcal{S} + d_z + 3$,\n$d_\\mathcal{S}$ is the dimension of the state space and $d_z$ is the zooming\ndimension. In contrast, algorithms with fixed discretization yield\n$d_{\\text{eff.}} = 2(d_\\mathcal{S} + d_\\mathcal{A}) + 2$, $d_\\mathcal{A}$ being\nthe dimension of action space. $\\text{ZoRL}$ achieves this by discretizing the\nstate-action space adaptively and zooming into ''promising regions'' of the\nstate-action space. $d_z$, a problem-dependent quantity bounded by the\nstate-action space's dimension, allows us to conclude that if an MDP is benign,\nthen the regret of $\\text{ZoRL}$ will be small. The zooming dimension and\n$\\text{ZoRL}$ are truly adaptive, i.e., the current work shows how to capture\nadaptivity gains for infinite-horizon average-reward RL. $\\text{ZoRL}$\noutperforms other state-of-the-art algorithms in experiments, thereby\ndemonstrating the gains arising due to adaptivity.", "comment": "Accepted in the 41st Conference on Uncertainty in Artificial\n  Intelligence", "pdf_url": "http://arxiv.org/pdf/2410.19919v2", "cate": "cs.LG", "date": "2024-10-25", "updated": "2025-07-13"}
{"id": "2412.08331", "title": "SLGaussian: Fast Language Gaussian Splatting in Sparse Views", "authors": ["Kangjie Chen", "BingQuan Dai", "Minghan Qin", "Dongbin Zhang", "Peihao Li", "Yingshuang Zou", "Haoqian Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ACM MM 2025. Project page: this https URL", "url": "http://arxiv.org/abs/2412.08331v2", "summary": "3D semantic field learning is crucial for applications like autonomous\nnavigation, AR/VR, and robotics, where accurate comprehension of 3D scenes from\nlimited viewpoints is essential. Existing methods struggle under sparse view\nconditions, relying on inefficient per-scene multi-view optimizations, which\nare impractical for many real-world tasks. To address this, we propose\nSLGaussian, a feed-forward method for constructing 3D semantic fields from\nsparse viewpoints, allowing direct inference of 3DGS-based scenes. By ensuring\nconsistent SAM segmentations through video tracking and using low-dimensional\nindexing for high-dimensional CLIP features, SLGaussian efficiently embeds\nlanguage information in 3D space, offering a robust solution for accurate 3D\nscene understanding under sparse view conditions. In experiments on two-view\nsparse 3D object querying and segmentation in the LERF and 3D-OVS datasets,\nSLGaussian outperforms existing methods in chosen IoU, Localization Accuracy,\nand mIoU. Moreover, our model achieves scene inference in under 30 seconds and\nopen-vocabulary querying in just 0.011 seconds per query.", "comment": "Accepted by ACM MM 2025. Project page:\n  https://chenkangjie1123.github.io/SLGaussian.github.io/", "pdf_url": "http://arxiv.org/pdf/2412.08331v2", "cate": "cs.CV", "date": "2024-12-11", "updated": "2025-07-14"}
{"id": "2411.07611", "title": "Knowledge-Augmented Multimodal Clinical Rationale Generation for Disease Diagnosis with Small Language Models", "authors": ["Shuai Niu", "Jing Ma", "Hongzhan Lin", "Liang Bai", "Zhihua Wang", "Yida Xu", "Yunya Song", "Xian Yang"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      13 pages. 7 figures", "url": "http://arxiv.org/abs/2411.07611v5", "summary": "Interpretation is critical for disease diagnosis, but existing models\nstruggle to balance predictive accuracy with human-understandable rationales.\nWhile large language models (LLMs) offer strong reasoning abilities, their\nclinical use is limited by high computational costs and restricted multimodal\nreasoning ability. Small language models (SLMs) are efficient but lack advanced\nreasoning for integrating multimodal medical data. In addition, both LLMs and\nSLMs lack domain knowledge for trustworthy reasoning. Therefore, we propose\nClinRaGen, enhancing SLMs by leveraging LLM-derived reasoning ability via\nrationale distillation and domain knowledge injection for trustworthy\nmultimodal rationale generation. Key innovations include a sequential rationale\ndistillation framework that equips SLMs with LLM-comparable multimodal\nreasoning abilities, and a knowledge-augmented attention mechanism that jointly\nunifies multimodal representation from time series and textual data in the same\nencoding space, enabling it to be naturally interpreted by SLMs while\nincorporating domain knowledge for reliable rationale generation. Experiments\non real-world medical datasets show that ClinRaGen achieves state-of-the-art\nperformance in disease diagnosis and rationale generation, demonstrating the\neffectiveness of combining LLM-driven reasoning with knowledge augmentation for\nimproved interpretability.", "comment": "13 pages. 7 figures", "pdf_url": "http://arxiv.org/pdf/2411.07611v5", "cate": "cs.CL", "date": "2024-11-12", "updated": "2025-07-13"}
{"id": "2412.14379", "title": "HA-RDet: Hybrid Anchor Rotation Detector for Oriented Object Detection", "authors": ["Phuc D. A. Nguyen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Bachelor thesis, Accepted to ICCV'25 SEA", "url": "http://arxiv.org/abs/2412.14379v2", "summary": "Oriented object detection in aerial images poses a significant challenge due\nto their varying sizes and orientations. Current state-of-the-art detectors\ntypically rely on either two-stage or one-stage approaches, often employing\nAnchor-based strategies, which can result in computationally expensive\noperations due to the redundant number of generated anchors during training. In\ncontrast, Anchor-free mechanisms offer faster processing but suffer from a\nreduction in the number of training samples, potentially impacting detection\naccuracy. To address these limitations, we propose the Hybrid-Anchor Rotation\nDetector (HA-RDet), which combines the advantages of both anchor-based and\nanchor-free schemes for oriented object detection. By utilizing only one preset\nanchor for each location on the feature maps and refining these anchors with\nour Orientation-Aware Convolution technique, HA-RDet achieves competitive\naccuracies, including 75.41 mAP on DOTA-v1, 65.3 mAP on DIOR-R, and 90.2 mAP on\nHRSC2016, against current anchor-based state-of-the-art methods, while\nsignificantly reducing computational resources.", "comment": "Bachelor thesis, Accepted to ICCV'25 SEA", "pdf_url": "http://arxiv.org/pdf/2412.14379v2", "cate": "cs.CV", "date": "2024-12-18", "updated": "2025-07-12"}
{"id": "2411.10000", "title": "DuSEGO: Dual Second-order Equivariant Graph Ordinary Differential Equation", "authors": ["Yingxu Wang", "Nan Yin", "Mingyan Xiao", "Xinhao Yi", "Siwei Liu", "Shangsong Liang"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.10000v2", "summary": "Graph Neural Networks (GNNs) with equivariant properties have achieved\nsignificant success in modeling complex dynamic systems and molecular\nproperties. However, their expressiveness ability is limited by: (1) Existing\nmethods often overlook the over-smoothing issue caused by traditional GNN\nmodels, as well as the gradient explosion or vanishing problems in deep GNNs.\n(2) Most models operate on first-order information, neglecting that the real\nworld often consists of second-order systems, which further limits the model's\nrepresentation capabilities. To address these issues, we propose the\n\\textbf{Du}al \\textbf{S}econd-order \\textbf{E}quivariant \\textbf{G}raph\n\\textbf{O}rdinary Differential Equation (\\method{}) for equivariant\nrepresentation. Specifically, \\method{} apply the dual second-order equivariant\ngraph ordinary differential equations (Graph ODEs) on graph embeddings and node\ncoordinates, simultaneously. Theoretically, we first prove that \\method{}\nmaintains the equivariant property. Furthermore, we provide theoretical\ninsights showing that \\method{} effectively alleviates the over-smoothing\nproblem in both feature representation and coordinate update. Additionally, we\ndemonstrate that the proposed \\method{} mitigates the exploding and vanishing\ngradients problem, facilitating the training of deep multi-layer GNNs.\nExtensive experiments on benchmark datasets validate the superiority of the\nproposed \\method{} compared to baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.10000v2", "cate": "cs.LG", "date": "2024-11-15", "updated": "2025-07-12"}
{"id": "2412.19021", "title": "Relation-aware Hierarchical Prompt for Open-vocabulary Scene Graph Generation", "authors": ["Tao Liu", "Rongjie Li", "Chongyu Wang", "Xuming He"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by AAAI-25", "url": "http://arxiv.org/abs/2412.19021v2", "summary": "Open-vocabulary Scene Graph Generation (OV-SGG) overcomes the limitations of\nthe closed-set assumption by aligning visual relationship representations with\nopen-vocabulary textual representations. This enables the identification of\nnovel visual relationships, making it applicable to real-world scenarios with\ndiverse relationships. However, existing OV-SGG methods are constrained by\nfixed text representations, limiting diversity and accuracy in image-text\nalignment. To address these challenges, we propose the Relation-Aware\nHierarchical Prompting (RAHP) framework, which enhances text representation by\nintegrating subject-object and region-specific relation information. Our\napproach utilizes entity clustering to address the complexity of relation\ntriplet categories, enabling the effective integration of subject-object\ninformation. Additionally, we utilize a large language model (LLM) to generate\ndetailed region-aware prompts, capturing fine-grained visual interactions and\nimproving alignment between visual and textual modalities. RAHP also introduces\na dynamic selection mechanism within Vision-Language Models (VLMs), which\nadaptively selects relevant text prompts based on the visual content, reducing\nnoise from irrelevant prompts. Extensive experiments on the Visual Genome and\nOpen Images v6 datasets demonstrate that our framework consistently achieves\nstate-of-the-art performance, demonstrating its effectiveness in addressing the\nchallenges of open-vocabulary scene graph generation. The code is available at:\nhttps://github.com/Leon022/RAHP", "comment": "Accepted by AAAI-25", "pdf_url": "http://arxiv.org/pdf/2412.19021v2", "cate": "cs.CV", "date": "2024-12-26", "updated": "2025-07-13"}
{"id": "2412.10454", "title": "An Interoperable Machine Learning Pipeline for Pediatric Obesity Risk Estimation", "authors": ["Hamed Fayyaz", "Mehak Gupta", "Alejandra Perez Ramirez", "Claudine Jurkovitz", "H. Timothy Bunnell", "Thao-Ly T. Phan", "Rahmatollah Beheshti"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      This paper has been accepted in Machine Learning for Health (ML4H) Symposium. Link: this https URL", "url": "http://arxiv.org/abs/2412.10454v2", "summary": "Reliable prediction of pediatric obesity can offer a valuable resource to\nproviders, helping them engage in timely preventive interventions before the\ndisease is established. Many efforts have been made to develop ML-based\npredictive models of obesity, and some studies have reported high predictive\nperformances. However, no commonly used clinical decision support tool based on\nexisting ML models currently exists. This study presents a novel end-to-end\npipeline specifically designed for pediatric obesity prediction, which supports\nthe entire process of data extraction, inference, and communication via an API\nor a user interface. While focusing only on routinely recorded data in\npediatric electronic health records (EHRs), our pipeline uses a diverse\nexpert-curated list of medical concepts to predict the 1-3 years risk of\ndeveloping obesity. Furthermore, by using the Fast Healthcare Interoperability\nResources (FHIR) standard in our design procedure, we specifically target\nfacilitating low-effort integration of our pipeline with different EHR systems.\nIn our experiments, we report the effectiveness of the predictive model as well\nas its alignment with the feedback from various stakeholders, including ML\nscientists, providers, health IT personnel, health administration\nrepresentatives, and patient group representatives.", "comment": "This paper has been accepted in Machine Learning for Health (ML4H)\n  Symposium. Link: https://proceedings.mlr.press/v259/fayyaz25a.html", "pdf_url": "http://arxiv.org/pdf/2412.10454v2", "cate": "cs.LG", "date": "2024-12-12", "updated": "2025-07-14"}
{"id": "2501.00574", "title": "VideoChat-Flash: Hierarchical Compression for Long-Context Video Modeling", "authors": ["Xinhao Li", "Yi Wang", "Jiashuo Yu", "Xiangyu Zeng", "Yuhan Zhu", "Haian Huang", "Jianfei Gao", "Kunchang Li", "Yinan He", "Chenting Wang", "Yu Qiao", "Yali Wang", "Limin Wang"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.00574v4", "summary": "Long-context video modeling is critical for multimodal large language models\n(MLLMs), enabling them to process movies, online video streams, and so on.\nDespite its advances, handling long videos remains challenging due to the\ndifficulty in efficiently understanding the extremely long video context. This\npaper aims to address this issue from aspects of model architecture, training\ndata, training strategy and evaluation benchmark. First, we propose a novel\nHierarchical video token Compression (HiCo) method, which leverages visual\nredundancy in long videos to compress long video context from Clip-level to\nVideo-level, reducing the computation significantly while preserving essential\ndetails, achieving an extreme compression ratio of approximately 1/50 with\nalmost no performance loss. Second, we introduce a multi-stage short-to-long\nlearning scheme, a large-scale dataset of real-world long videos named LongVid,\nand a challenging ``Multi-Hop Needle-In-A-Video-Haystack'' benchmark. Finally,\nwe build a powerful video MLLM named VideoChat-Flash, which shows a leading\nperformance on both mainstream long and short video benchmarks at the 2B and 7B\nmodel scale. It first gets 99.1% accuracy over 10,000 frames in NIAH among\nopen-source models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.00574v4", "cate": "cs.CV", "date": "2024-12-31", "updated": "2025-07-13"}
{"id": "2501.00555", "title": "Prune 'n Predict: Optimizing LLM Decision-making with Conformal Prediction", "authors": ["Harit Vishwakarma", "Alan Mishler", "Thomas Cook", "Niccolò Dalmasso", "Natraj Raman", "Sumitra Ganesh"], "categories": ["cs.LG", "cs.AI", "stat.AP", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.00555v2", "summary": "Large language models (LLMs) are empowering decision-making in several\napplications, including tool or API usage and answering multiple-choice\nquestions (MCQs). However, incorrect outputs pose significant risks in\nhigh-stakes domains like healthcare and finance. To quantify LLM uncertainty\nand thereby mitigate these risks, recent works employ conformal prediction\n(CP), a model- and distribution-agnostic framework that uses LLM outputs to\ngenerate a \\emph{prediction set} containing the true answer with high\nprobability. Leveraging CP, we propose \\emph{conformal revision of questions}\n(CROQ), which revises the question by narrowing down the available choices to\nthose in the prediction set and asking the LLM the revised question. We expect\nLLMs to be more accurate on revised questions with fewer choices. Furthermore,\nwe expect CROQ to be effective when the prediction sets from CP are small.\nCommonly used logit scores often lead to large sets, diminishing CROQ's\neffectiveness. To overcome this, we propose CP-OPT, an optimization framework\nto learn scores that minimize set sizes while maintaining coverage. Our\nextensive experiments on MMLU, ToolAlpaca, and TruthfulQA datasets with\nmultiple LLMs show that CROQ improves accuracy over the standard inference,\nwith more pronounced gains when paired with CP-OPT.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.00555v2", "cate": "cs.LG", "date": "2024-12-31", "updated": "2025-07-12"}
{"id": "2501.05242", "title": "SEGS-SLAM: Structure-enhanced 3D Gaussian Splatting SLAM with Appearance Embedding", "authors": ["Tianci Wen", "Zhiang Liu", "Yongchun Fang"], "categories": ["cs.CV", "68T40(Primary)68T45, 68U99 (Secondary)", "I.4.8; I.3.7"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025 accept;code, video, demos, and project are available at Project page this https URL", "url": "http://arxiv.org/abs/2501.05242v3", "summary": "3D Gaussian splatting (3D-GS) has recently revolutionized novel view\nsynthesis in the simultaneous localization and mapping (SLAM) problem. However,\nmost existing algorithms fail to fully capture the underlying structure,\nresulting in structural inconsistency. Additionally, they struggle with abrupt\nappearance variations, leading to inconsistent visual quality. To address these\nproblems, we propose SEGS-SLAM, a structure-enhanced 3D Gaussian Splatting\nSLAM, which achieves high-quality photorealistic mapping. Our main\ncontributions are two-fold. First, we propose a structure-enhanced\nphotorealistic mapping (SEPM) framework that, for the first time, leverages\nhighly structured point cloud to initialize structured 3D Gaussians, leading to\nsignificant improvements in rendering quality. Second, we propose\nAppearance-from-Motion embedding (AfME), enabling 3D Gaussians to better model\nimage appearance variations across different camera poses. Extensive\nexperiments on monocular, stereo, and RGB-D datasets demonstrate that SEGS-SLAM\nsignificantly outperforms state-of-the-art (SOTA) methods in photorealistic\nmapping quality, e.g., an improvement of $19.86\\%$ in PSNR over MonoGS on the\nTUM RGB-D dataset for monocular cameras. The project page is available at\nhttps://segs-slam.github.io/.", "comment": "ICCV 2025 accept;code, video, demos, and project are available at\n  Project page https://segs-slam.github.io/", "pdf_url": "http://arxiv.org/pdf/2501.05242v3", "cate": "cs.CV", "date": "2025-01-09", "updated": "2025-07-13"}
{"id": "2501.03940", "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks for AI generated text detection", "authors": ["Pablo Miralles-González", "Javier Huertas-Tato", "Alejandro Martín", "David Camacho"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.03940v3", "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.03940v3", "cate": "cs.CL", "date": "2025-01-07", "updated": "2025-07-14"}
{"id": "2501.12386", "title": "InternVideo2.5: Empowering Video MLLMs with Long and Rich Context Modeling", "authors": ["Yi Wang", "Xinhao Li", "Ziang Yan", "Yinan He", "Jiashuo Yu", "Xiangyu Zeng", "Chenting Wang", "Changlian Ma", "Haian Huang", "Jianfei Gao", "Min Dou", "Kai Chen", "Wenhai Wang", "Yu Qiao", "Yali Wang", "Limin Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      technical report", "url": "http://arxiv.org/abs/2501.12386v3", "summary": "This paper aims to improve the performance of video multimodal large language\nmodels (MLLM) via long and rich context (LRC) modeling. As a result, we develop\na new version of InternVideo2.5 with a focus on enhancing the original MLLMs'\nability to perceive fine-grained details and capture long-form temporal\nstructure in videos. Specifically, our approach incorporates dense vision task\nannotations into MLLMs using direct preference optimization and develops\ncompact spatiotemporal representations through adaptive hierarchical token\ncompression. Experimental results demonstrate this unique design of LRC greatly\nimproves the results of video MLLM in mainstream video understanding benchmarks\n(short & long), enabling the MLLM to memorize significantly longer video inputs\n(at least 6x longer than the original), and master specialized vision\ncapabilities like object tracking and segmentation. Our work highlights the\nimportance of multimodal context richness (length and fineness) in empowering\nMLLM's innate abilites (focus and memory), providing new insights for future\nresearch on video MLLM. Code and models are available at\nhttps://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2.5", "comment": "technical report", "pdf_url": "http://arxiv.org/pdf/2501.12386v3", "cate": "cs.CV", "date": "2025-01-21", "updated": "2025-07-13"}
{"id": "2501.08411", "title": "BiDepth: A Bidirectional-Depth Neural Network for Spatio-Temporal Prediction", "authors": ["Sina Ehsani", "Fenglian Pan", "Qingpei Hu", "Jian Liu"], "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.AP"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      21 pages, 6 figures. Submitted to ACM TKDD", "url": "http://arxiv.org/abs/2501.08411v3", "summary": "Accurate spatial-temporal (ST) prediction for dynamic systems, such as urban\nmobility and weather patterns, is crucial but hindered by complex ST\ncorrelations and the challenge of concurrently modeling long-term trends with\nshort-term fluctuations. Existing methods often falter in these areas. This\npaper proposes the BiDepth Multimodal Neural Network (BDMNN), which integrates\ntwo key innovations: 1) a bidirectional depth modulation mechanism that\ndynamically adjusts network depth to comprehensively capture both long-term\nseasonality and immediate short-term events; and 2) a novel convolutional\nself-attention cell (CSAC). Critically, unlike many attention mechanisms that\ncan lose spatial acuity, our CSAC is specifically designed to preserve crucial\nspatial relationships throughout the network, akin to standard convolutional\nlayers, while simultaneously capturing temporal dependencies. Evaluated on\nreal-world urban traffic and precipitation datasets, BDMNN demonstrates\nsignificant accuracy improvements, achieving a 12% Mean Squared Error (MSE)\nreduction in urban traffic prediction and a 15% improvement in precipitation\nforecasting over leading deep learning benchmarks like ConvLSTM, using\ncomparable computational resources. These advancements offer robust ST\nforecasting for smart city management, disaster prevention, and resource\noptimization.", "comment": "21 pages, 6 figures. Submitted to ACM TKDD", "pdf_url": "http://arxiv.org/pdf/2501.08411v3", "cate": "cs.LG", "date": "2025-01-14", "updated": "2025-07-13"}
{"id": "2501.12596", "title": "Adapting OpenAI's CLIP Model for Few-Shot Image Inspection in Manufacturing Quality Control: An Expository Case Study with Multiple Application Examples", "authors": ["Fadel M. Megahed", "Ying-Ju Chen", "Bianca Maria Colosimo", "Marco Luigi Giuseppe Grasso", "L. Allison Jones-Farmer", "Sven Knoth", "Hongyue Sun", "Inez Zwetsloot"], "categories": ["cs.CV", "stat.AP", "stat.OT"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      36 pages, 13 figures", "url": "http://arxiv.org/abs/2501.12596v2", "summary": "This expository paper introduces a simplified approach to image-based quality\ninspection in manufacturing using OpenAI's CLIP (Contrastive Language-Image\nPretraining) model adapted for few-shot learning. While CLIP has demonstrated\nimpressive capabilities in general computer vision tasks, its direct\napplication to manufacturing inspection presents challenges due to the domain\ngap between its training data and industrial applications. We evaluate CLIP's\neffectiveness through five case studies: metallic pan surface inspection, 3D\nprinting extrusion profile analysis, stochastic textured surface evaluation,\nautomotive assembly inspection, and microstructure image classification. Our\nresults show that CLIP can achieve high classification accuracy with relatively\nsmall learning sets (50-100 examples per class) for single-component and\ntexture-based applications. However, the performance degrades with complex\nmulti-component scenes. We provide a practical implementation framework that\nenables quality engineers to quickly assess CLIP's suitability for their\nspecific applications before pursuing more complex solutions. This work\nestablishes CLIP-based few-shot learning as an effective baseline approach that\nbalances implementation simplicity with robust performance, demonstrated in\nseveral manufacturing quality control applications.", "comment": "36 pages, 13 figures", "pdf_url": "http://arxiv.org/pdf/2501.12596v2", "cate": "cs.CV", "date": "2025-01-22", "updated": "2025-07-14"}
{"id": "2502.01391", "title": "Learning Traffic Anomalies from Generative Models on Real-Time Observations", "authors": ["Fotis I. Giasemis", "Alexandros Sopasakis"], "categories": ["cs.LG", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.01391v5", "summary": "Accurate detection of traffic anomalies is crucial for effective urban\ntraffic management and congestion mitigation. We use the Spatiotemporal\nGenerative Adversarial Network (STGAN) framework combining Graph Neural\nNetworks and Long Short-Term Memory networks to capture complex spatial and\ntemporal dependencies in traffic data. We apply STGAN to real-time,\nminute-by-minute observations from 42 traffic cameras across Gothenburg,\nSweden, collected over several months in 2020. The images are processed to\ncompute a flow metric representing vehicle density, which serves as input for\nthe model. Training is conducted on data from April to November 2020, and\nvalidation is performed on a separate dataset from November 14 to 23, 2020. Our\nresults demonstrate that the model effectively detects traffic anomalies with\nhigh precision and low false positive rates. The detected anomalies include\ncamera signal interruptions, visual artifacts, and extreme weather conditions\naffecting traffic flow.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.01391v5", "cate": "cs.LG", "date": "2025-02-03", "updated": "2025-07-12"}
{"id": "2501.13667", "title": "MPG-SAM 2: Adapting SAM 2 with Mask Priors and Global Context for Referring Video Object Segmentation", "authors": ["Fu Rong", "Meng Lan", "Qian Zhang", "Lefei Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025", "url": "http://arxiv.org/abs/2501.13667v4", "summary": "Referring video object segmentation (RVOS) aims to segment objects in a video\naccording to textual descriptions, which requires the integration of multimodal\ninformation and temporal dynamics perception. The Segment Anything Model 2 (SAM\n2) has shown great effectiveness across various video segmentation tasks.\nHowever, its application to offline RVOS is challenged by the translation of\nthe text into effective prompts and a lack of global context awareness. In this\npaper, we propose a novel RVOS framework, termed MPG-SAM 2, to address these\nchallenges. Specifically, MPG-SAM 2 employs a unified multimodal encoder to\njointly encode video and textual features, generating semantically aligned\nvideo and text embeddings, along with multimodal class tokens. A mask prior\ngenerator utilizes the video embeddings and class tokens to create pseudo masks\nof target objects and global context. These masks are fed into the prompt\nencoder as dense prompts along with multimodal class tokens as sparse prompts\nto generate accurate prompts for SAM 2. To provide the online SAM 2 with a\nglobal view, we introduce a hierarchical global-historical aggregator, which\nallows SAM 2 to aggregate global and historical information of target objects\nat both pixel and object levels, enhancing the target representation and\ntemporal consistency. Extensive experiments on several RVOS benchmarks\ndemonstrate the superiority of MPG-SAM 2 and the effectiveness of our proposed\nmodules. The code is available at https://github.com/rongfu-dsb/MPG-SAM2.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2501.13667v4", "cate": "cs.CV", "date": "2025-01-23", "updated": "2025-07-12"}
{"id": "2502.05310", "title": "Oracular Programming: A Modular Foundation for Building LLM-Enabled Software", "authors": ["Jonathan Laurent", "André Platzer"], "categories": ["cs.PL", "cs.AI"], "primary_category": "Subjects:       Programming Languages (cs.PL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.05310v2", "summary": "Large Language Models have proven surprisingly effective at solving a wide\nrange of tasks from just a handful of examples. However, their lack of\nreliability and modularity limits their capacity to tackle large problems that\nrequire many steps of reasoning. In response, researchers have proposed\nadvanced pipelines that leverage domain-specific knowledge to chain smaller\nprompts, provide intermediate feedback and improve performance through search.\nHowever, the current complexity of writing, tuning, maintaining and improving\nsuch pipelines has limited their sophistication. We propose oracular\nprogramming, a foundational paradigm for building LLM-enabled applications that\nlets domain experts express high-level problem-solving strategies as programs\nwith unresolved choice points. These choice points are resolved at runtime by\nLLMs, which generalize from user-provided examples of correct and incorrect\ndecisions. An oracular program is composed of three orthogonal components: a\nstrategy that consists in a nondeterministic program with choice points that\ncan be reified into a search tree, a policy that specifies how to navigate this\ntree with the help of LLM oracles, and a set of demonstrations that describe\nsuccessful and unsuccessful search tree navigation scenarios across diverse\nproblem instances. Each component is expressed in a dedicated programming\nlanguage and can be independently improved or substituted. We address the key\nprogramming language design challenges of modularly composing oracular programs\nand enforcing consistency between their components as they evolve.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.05310v2", "cate": "cs.PL", "date": "2025-02-07", "updated": "2025-07-13"}
{"id": "2501.19066", "title": "Concept Steerers: Leveraging K-Sparse Autoencoders for Test-Time Controllable Generations", "authors": ["Dahye Kim", "Deepti Ghadiyaram"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      23 pages, 18 figures", "url": "http://arxiv.org/abs/2501.19066v2", "summary": "Despite the remarkable progress in text-to-image generative models, they are\nprone to adversarial attacks and inadvertently generate unsafe, unethical\ncontent. Existing approaches often rely on fine-tuning models to remove\nspecific concepts, which is computationally expensive, lacks scalability,\nand/or compromises generation quality. In this work, we propose a novel\nframework leveraging k-sparse autoencoders (k-SAEs) to enable efficient and\ninterpretable concept manipulation in diffusion models. Specifically, we first\nidentify interpretable monosemantic concepts in the latent space of text\nembeddings and leverage them to precisely steer the generation away or towards\na given concept (e.g., nudity) or to introduce a new concept (e.g.,\nphotographic style) -- all during test time. Through extensive experiments, we\ndemonstrate that our approach is very simple, requires no retraining of the\nbase model nor LoRA adapters, does not compromise the generation quality, and\nis robust to adversarial prompt manipulations. Our method yields an improvement\nof $\\mathbf{20.01\\%}$ in unsafe concept removal, is effective in style\nmanipulation, and is $\\mathbf{\\sim5}$x faster than the current\nstate-of-the-art. Code is available at: https://github.com/kim-dahye/steerers", "comment": "23 pages, 18 figures", "pdf_url": "http://arxiv.org/pdf/2501.19066v2", "cate": "cs.CV", "date": "2025-01-31", "updated": "2025-07-14"}
{"id": "2502.06806", "title": "Logits are All We Need to Adapt Closed Models", "authors": ["Gaurush Hiranandani", "Haolun Wu", "Subhojyoti Mukherjee", "Sanmi Koyejo"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      29 pages, 8 figures", "url": "http://arxiv.org/abs/2502.06806v4", "summary": "Many commercial Large Language Models (LLMs) are often closed-source,\nlimiting developers to prompt tuning for aligning content generation with\nspecific applications. While these models currently do not provide access to\ntoken logits, we argue that if such access were available, it would enable more\npowerful adaptation techniques beyond prompt engineering. In this paper, we\npropose a token-level probability reweighting framework that, given access to\nlogits and a small amount of task-specific data, can effectively steer\nblack-box LLMs toward application-specific content generation. Our approach\nviews next-token prediction through the lens of supervised classification. We\nshow that aligning black-box LLMs with task-specific data can be formulated as\na label noise correction problem, leading to Plugin model -- an autoregressive\nprobability reweighting model that operates solely on logits. We provide\ntheoretical justification for why reweighting logits alone is sufficient for\ntask adaptation. Extensive experiments with multiple datasets, LLMs, and\nreweighting models demonstrate the effectiveness of our method, advocating for\nbroader access to token logits in closed-source models.", "comment": "29 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2502.06806v4", "cate": "cs.LG", "date": "2025-02-03", "updated": "2025-07-12"}
{"id": "2502.08560", "title": "Brain Latent Progression: Individual-based Spatiotemporal Disease Progression on 3D Brain MRIs via Latent Diffusion", "authors": ["Lemuel Puglisi", "Daniel C. Alexander", "Daniele Ravì"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      arXiv admin note: text overlap with arXiv:2405.03328", "url": "http://arxiv.org/abs/2502.08560v2", "summary": "The growing availability of longitudinal Magnetic Resonance Imaging (MRI)\ndatasets has facilitated Artificial Intelligence (AI)-driven modeling of\ndisease progression, making it possible to predict future medical scans for\nindividual patients. However, despite significant advancements in AI, current\nmethods continue to face challenges including achieving patient-specific\nindividualization, ensuring spatiotemporal consistency, efficiently utilizing\nlongitudinal data, and managing the substantial memory demands of 3D scans. To\naddress these challenges, we propose Brain Latent Progression (BrLP), a novel\nspatiotemporal model designed to predict individual-level disease progression\nin 3D brain MRIs. The key contributions in BrLP are fourfold: (i) it operates\nin a small latent space, mitigating the computational challenges posed by\nhigh-dimensional imaging data; (ii) it explicitly integrates subject metadata\nto enhance the individualization of predictions; (iii) it incorporates prior\nknowledge of disease dynamics through an auxiliary model, facilitating the\nintegration of longitudinal data; and (iv) it introduces the Latent Average\nStabilization (LAS) algorithm, which (a) enforces spatiotemporal consistency in\nthe predicted progression at inference time and (b) allows us to derive a\nmeasure of the uncertainty for the prediction at the global and voxel level. We\ntrain and evaluate BrLP on 11,730 T1-weighted (T1w) brain MRIs from 2,805\nsubjects and validate its generalizability on an external test set comprising\n2,257 MRIs from 962 subjects. Our experiments compare BrLP-generated MRI scans\nwith real follow-up MRIs, demonstrating state-of-the-art accuracy compared to\nexisting methods. The code is publicly available at:\nhttps://github.com/LemuelPuglisi/BrLP.", "comment": "arXiv admin note: text overlap with arXiv:2405.03328", "pdf_url": "http://arxiv.org/pdf/2502.08560v2", "cate": "cs.CV", "date": "2025-02-12", "updated": "2025-07-13"}
{"id": "2502.12992", "title": "B-cos LM: Efficiently Transforming Pre-trained Language Models for Improved Explainability", "authors": ["Yifan Wang", "Sukrut Rao", "Ji-Ung Lee", "Mayank Jobanputra", "Vera Demberg"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.12992v2", "summary": "Post-hoc explanation methods for black-box models often struggle with\nfaithfulness and human interpretability due to the lack of explainability in\ncurrent neural architectures. Meanwhile, B-cos networks have been introduced to\nimprove model explainability by proposing an architecture that removes bias\nterms and promotes input-weight alignment. Although B-cos networks have shown\nsuccess in building explainable systems, their application has so far been\nlimited to computer vision models and their associated training pipelines. In\nthis work, we introduce B-cos LMs, i.e., B-cos language models (LMs) empowered\nfor natural language processing (NLP) tasks. Our approach directly transforms\npre-trained language models into B-cos LMs by combining B-cos conversion and\ntask fine-tuning, improving efficiency compared to previous methods. Our\nautomatic and human evaluation results demonstrate that B-cos LMs produce more\nfaithful and human interpretable explanations than post-hoc methods, while\nmaintaining task performance comparable to conventional fine-tuning. Our\nin-depth analysis explores how B-cos LMs differ from conventionally fine-tuned\nmodels in their learning processes and explanation patterns. Finally, we are\nalso the first to explore the transformation of decoder-only models to B-cos\nLMs for generation tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.12992v2", "cate": "cs.CL", "date": "2025-02-18", "updated": "2025-07-14"}
{"id": "2502.09873", "title": "Compression-Aware One-Step Diffusion Model for JPEG Artifact Removal", "authors": ["Jinpei Guo", "Zheng Chen", "Wenbo Li", "Yong Guo", "Yulun Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.09873v3", "summary": "Diffusion models have demonstrated remarkable success in image restoration\ntasks. However, their multi-step denoising process introduces significant\ncomputational overhead, limiting their practical deployment. Furthermore,\nexisting methods struggle to effectively remove severe JPEG artifact,\nespecially in highly compressed images. To address these challenges, we propose\nCODiff, a compression-aware one-step diffusion model for JPEG artifact removal.\nThe core of CODiff is the compression-aware visual embedder (CaVE), which\nextracts and leverages JPEG compression priors to guide the diffusion model. We\npropose a dual learning strategy that combines explicit and implicit learning.\nSpecifically, explicit learning enforces a quality prediction objective to\ndifferentiate low-quality images with different compression levels. Implicit\nlearning employs a reconstruction objective that enhances the model's\ngeneralization. This dual learning allows for a deeper and more comprehensive\nunderstanding of JPEG compression. Experimental results demonstrate that CODiff\nsurpasses recent leading methods in both quantitative and visual quality\nmetrics. The code is released at https://github.com/jp-guo/CODiff.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.09873v3", "cate": "cs.CV", "date": "2025-02-14", "updated": "2025-07-13"}
{"id": "2502.15902", "title": "IPAD: Inverse Prompt for AI Detection -- A Robust and Explainable LLM-Generated Text Detector", "authors": ["Zheng Chen", "Yushi Feng", "Changyang He", "Yue Deng", "Hongxi Pu", "Bo Li"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.15902v2", "summary": "Large Language Models (LLMs) have attained human-level fluency in text\ngeneration, which complicates the distinction between human-written and\nLLM-generated texts. This increases the risk of misuse and highlights the need\nfor reliable detectors. Yet, existing detectors exhibit poor robustness on\nout-of-distribution (OOD) data and attacked data, which is critical for\nreal-world scenarios. Also, they struggle to provide interpretable evidence to\nsupport their decisions, thus undermining the reliability. In light of these\nchallenges, we propose IPAD (Inverse Prompt for AI Detection), a novel\nframework consisting of a Prompt Inverter that identifies predicted prompts\nthat could have generated the input text, and two Distinguishers that examine\nthe probability that the input texts align with the predicted prompts.\nEmpirical evaluations demonstrate that IPAD outperforms the strongest baselines\nby 9.05% (Average Recall) on in-distribution data, 12.93% (AUROC) on\nout-of-distribution (OOD) data, and 5.48% (AUROC) on attacked data. IPAD also\nperforms robustly on structured datasets. Furthermore, an interpretability\nassessment is conducted to illustrate that IPAD enhances the AI detection\ntrustworthiness by allowing users to directly examine the decision-making\nevidence, which provides interpretable support for its state-of-the-art\ndetection results.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.15902v2", "cate": "cs.LG", "date": "2025-02-21", "updated": "2025-07-14"}
{"id": "2502.10059", "title": "RealCam-I2V: Real-World Image-to-Video Generation with Interactive Complex Camera Control", "authors": ["Teng Li", "Guangcong Zheng", "Rui Jiang", "Shuigen Zhan", "Tao Wu", "Yehao Lu", "Yining Lin", "Chuanyun Deng", "Yepan Xiong", "Min Chen", "Lin Cheng", "Xi Li"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2502.10059v2", "summary": "Recent advancements in camera-trajectory-guided image-to-video generation\noffer higher precision and better support for complex camera control compared\nto text-based approaches. However, they also introduce significant usability\nchallenges, as users often struggle to provide precise camera parameters when\nworking with arbitrary real-world images without knowledge of their depth nor\nscene scale. To address these real-world application issues, we propose\nRealCam-I2V, a novel diffusion-based video generation framework that integrates\nmonocular metric depth estimation to establish 3D scene reconstruction in a\npreprocessing step. During training, the reconstructed 3D scene enables scaling\ncamera parameters from relative to metric scales, ensuring compatibility and\nscale consistency across diverse real-world images. In inference, RealCam-I2V\noffers an intuitive interface where users can precisely draw camera\ntrajectories by dragging within the 3D scene. To further enhance precise camera\ncontrol and scene consistency, we propose scene-constrained noise shaping,\nwhich shapes high-level noise and also allows the framework to maintain dynamic\nand coherent video generation in lower noise stages. RealCam-I2V achieves\nsignificant improvements in controllability and video quality on the\nRealEstate10K and out-of-domain images. We further enables applications like\ncamera-controlled looping video generation and generative frame interpolation.\nProject page: https://zgctroy.github.io/RealCam-I2V.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2502.10059v2", "cate": "cs.CV", "date": "2025-02-14", "updated": "2025-07-13"}
{"id": "2502.18448", "title": "Disambiguate First, Parse Later: Generating Interpretations for Ambiguity Resolution in Semantic Parsing", "authors": ["Irina Saparina", "Mirella Lapata"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Findings of ACL 2025", "url": "http://arxiv.org/abs/2502.18448v2", "summary": "Handling ambiguity and underspecification is an important challenge in\nnatural language interfaces, particularly for tasks like text-to-SQL semantic\nparsing. We propose a modular approach that resolves ambiguity using natural\nlanguage interpretations before mapping these to logical forms (e.g., SQL\nqueries). Although LLMs excel at parsing unambiguous utterances, they show\nstrong biases for ambiguous ones, typically predicting only preferred\ninterpretations. We constructively exploit this bias to generate an initial set\nof preferred disambiguations and then apply a specialized infilling model to\nidentify and generate missing interpretations. To train the infilling model, we\nintroduce an annotation method that uses SQL execution to validate different\nmeanings. Our approach improves interpretation coverage and generalizes across\ndatasets with different annotation styles, database structures, and ambiguity\ntypes.", "comment": "Findings of ACL 2025", "pdf_url": "http://arxiv.org/pdf/2502.18448v2", "cate": "cs.CL", "date": "2025-02-25", "updated": "2025-07-12"}
{"id": "2502.12377", "title": "Alignment and Adversarial Robustness: Are More Human-Like Models More Secure?", "authors": ["Blaine Hoak", "Kunyang Li", "Patrick McDaniel"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to International Workshop on Security and Privacy-Preserving AI/ML (SPAIML) 2025", "url": "http://arxiv.org/abs/2502.12377v2", "summary": "A small but growing body of work has shown that machine learning models which\nbetter align with human vision have also exhibited higher robustness to\nadversarial examples, raising the question: can human-like perception make\nmodels more secure? If true generally, such mechanisms would offer new avenues\ntoward robustness. In this work, we conduct a large-scale empirical analysis to\nsystematically investigate the relationship between representational alignment\nand adversarial robustness. We evaluate 114 models spanning diverse\narchitectures and training paradigms, measuring their neural and behavioral\nalignment and engineering task performance across 105 benchmarks as well as\ntheir adversarial robustness via AutoAttack. Our findings reveal that while\naverage alignment and robustness exhibit a weak overall correlation, specific\nalignment benchmarks serve as strong predictors of adversarial robustness,\nparticularly those that measure selectivity toward texture or shape. These\nresults suggest that different forms of alignment play distinct roles in model\nrobustness, motivating further investigation into how alignment-driven\napproaches can be leveraged to build more secure and perceptually-grounded\nvision models.", "comment": "Accepted to International Workshop on Security and Privacy-Preserving\n  AI/ML (SPAIML) 2025", "pdf_url": "http://arxiv.org/pdf/2502.12377v2", "cate": "cs.CV", "date": "2025-02-17", "updated": "2025-07-14"}
{"id": "2503.02951", "title": "KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for Coding", "authors": ["Zhangchen Xu", "Yang Liu", "Yueqin Yin", "Mingyuan Zhou", "Radha Poovendran"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted by ACL 2025. Codes and Data: this https URL", "url": "http://arxiv.org/abs/2503.02951v2", "summary": "We introduce KodCode, a synthetic dataset that addresses the persistent\nchallenge of acquiring high-quality, verifiable training data across diverse\ndifficulties and domains for training Large Language Models for coding.\nExisting code-focused resources typically fail to ensure either the breadth of\ncoverage (e.g., spanning simple coding tasks to advanced algorithmic problems)\nor verifiable correctness (e.g., unit tests). In contrast, KodCode comprises\nquestion-solution-test triplets that are systematically validated via a\nself-verification procedure. Our pipeline begins by synthesizing a broad range\nof coding questions, then generates solutions and test cases with additional\nattempts allocated to challenging problems. Finally, post-training data\nsynthesis is done by rewriting questions into diverse formats and generating\nresponses under a test-based reject sampling procedure from a reasoning model\n(DeepSeek R1). This pipeline yields a large-scale, robust and diverse coding\ndataset. KodCode is suitable for supervised fine-tuning and the paired unit\ntests also provide great potential for RL tuning. Fine-tuning experiments on\ncoding benchmarks (HumanEval(+), MBPP(+), BigCodeBench, and LiveCodeBench)\ndemonstrate that KodCode-tuned models achieve state-of-the-art performance,\nsurpassing models like Qwen2.5-Coder-32B-Instruct and\nDeepSeek-R1-Distill-Llama-70B.", "comment": "Accepted by ACL 2025. Codes and Data: https://kodcode-ai.github.io/", "pdf_url": "http://arxiv.org/pdf/2503.02951v2", "cate": "cs.LG", "date": "2025-03-04", "updated": "2025-07-12"}
{"id": "2502.15438", "title": "Deflickering Vision-Based Occupancy Networks through Lightweight Spatio-Temporal Correlation", "authors": ["Fengcheng Yu", "Haoran Xu", "Canming Xia", "Ziyang Zong", "Guang Tan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.15438v3", "summary": "Vision-based occupancy networks (VONs) provide an end-to-end solution for\nreconstructing 3D environments in autonomous driving. However, existing methods\noften suffer from temporal inconsistencies, manifesting as flickering effects\nthat compromise visual experience and adversely affect decision-making. While\nrecent approaches have incorporated historical data to mitigate the issue, they\noften incur high computational costs and may introduce noisy information that\ninterferes with object detection. We propose OccLinker, a novel plugin\nframework designed to seamlessly integrate with existing VONs for boosting\nperformance. Our method efficiently consolidates historical static and motion\ncues, learns sparse latent correlations with current features through a dual\ncross-attention mechanism, and produces correction occupancy components to\nrefine the base network's predictions. We propose a new temporal consistency\nmetric to quantitatively identify flickering effects. Extensive experiments on\ntwo benchmark datasets demonstrate that our method delivers superior\nperformance with negligible computational overhead, while effectively\neliminating flickering artifacts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.15438v3", "cate": "cs.CV", "date": "2025-02-21", "updated": "2025-07-14"}
{"id": "2503.05979", "title": "Learning-Order Autoregressive Models with Application to Molecular Graph Generation", "authors": ["Zhe Wang", "Jiaxin Shi", "Nicolas Heess", "Arthur Gretton", "Michalis K. Titsias"], "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.05979v2", "summary": "Autoregressive models (ARMs) have become the workhorse for sequence\ngeneration tasks, since many problems can be modeled as next-token prediction.\nWhile there appears to be a natural ordering for text (i.e., left-to-right),\nfor many data types, such as graphs, the canonical ordering is less obvious. To\naddress this problem, we introduce a variant of ARM that generates\nhigh-dimensional data using a probabilistic ordering that is sequentially\ninferred from data. This model incorporates a trainable probability\ndistribution, referred to as an order-policy, that dynamically decides the\nautoregressive order in a state-dependent manner. To train the model, we\nintroduce a variational lower bound on the log-likelihood, which we optimize\nwith stochastic gradient estimation. We demonstrate experimentally that our\nmethod can learn meaningful autoregressive orderings in image and graph\ngeneration. On the challenging domain of molecular graph generation, we achieve\nstate-of-the-art results on the QM9 and ZINC250k benchmarks, evaluated across\nkey metrics for distribution similarity and drug-likeless.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.05979v2", "cate": "cs.LG", "date": "2025-03-07", "updated": "2025-07-13"}
{"id": "2502.21291", "title": "MIGE: Mutually Enhanced Multimodal Instruction-Based Image Generation and Editing", "authors": ["Xueyun Tian", "Wei Li", "Bingbing Xu", "Yige Yuan", "Yuanzhuo Wang", "Huawei Shen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      This paper have been accepted by ACM MM25", "url": "http://arxiv.org/abs/2502.21291v3", "summary": "Despite significant progress in diffusion-based image generation,\nsubject-driven generation and instruction-based editing remain challenging.\nExisting methods typically treat them separately, struggling with limited\nhigh-quality data and poor generalization. However, both tasks require\ncapturing complex visual variations while maintaining consistency between\ninputs and outputs. Inspired by this, we propose MIGE, a unified framework that\nstandardizes task representations using multimodal instructions. It first\ntreats subject-driven generation as creation on a blank canvas and\ninstruction-based editing as modification of an existing image, establishing a\nshared input-output formulation, then introduces a novel multimodal encoder\nthat maps free-form multimodal instructions into a unified vision-language\nspace, integrating visual and semantic features through a feature fusion\nmechanism. This unification enables joint training of both tasks, providing two\nkey advantages: (1) Cross-Task Enhancement: by leveraging shared visual and\nsemantic representations, joint training improves instruction adherence and\nvisual consistency in both subject-driven generation and instruction-based\nediting. (2) Generalization: learning in a unified format facilitates\ncross-task knowledge transfer, enabling MIGE to generalize to novel\ncompositional tasks, including instruction-based subject-driven editing.\nExperiments show that MIGE excels in both subject-driven generation and\ninstruction-based editing while setting a SOTA in the new task of\ninstruction-based subject-driven editing. Code and model have been publicly\navailable at https://github.com/Eureka-Maggie/MIGE/tree/main.", "comment": "This paper have been accepted by ACM MM25", "pdf_url": "http://arxiv.org/pdf/2502.21291v3", "cate": "cs.CV", "date": "2025-02-28", "updated": "2025-07-13"}
{"id": "2504.08161", "title": "Rethinking the Foundations for Continual Reinforcement Learning", "authors": ["Michael Bowling", "Esraa Elelimy"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.08161v2", "summary": "In the traditional view of reinforcement learning, the agent's goal is to\nfind an optimal policy that maximizes its expected sum of rewards. Once the\nagent finds this policy, the learning ends. This view contrasts with\n\\emph{continual reinforcement learning}, where learning does not end, and\nagents are expected to continually learn and adapt indefinitely. Despite the\nclear distinction between these two paradigms of learning, much of the progress\nin continual reinforcement learning has been shaped by foundations rooted in\nthe traditional view of reinforcement learning. In this paper, we first examine\nwhether the foundations of traditional reinforcement learning are suitable for\nthe continual reinforcement learning paradigm. We identify four key pillars of\nthe traditional reinforcement learning foundations that are antithetical to the\ngoals of continual learning: the Markov decision process formalism, the focus\non atemporal artifacts, the expected sum of rewards as an evaluation metric,\nand episodic benchmark environments that embrace the other three foundations.\nWe then propose a new formalism that sheds the first and the third foundations\nand replaces them with the history process as a mathematical formalism and a\nnew definition of deviation regret, adapted for continual learning, as an\nevaluation metric. Finally, we discuss possible approaches to shed the other\ntwo foundations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.08161v2", "cate": "cs.LG", "date": "2025-04-10", "updated": "2025-07-12"}
{"id": "2503.02481", "title": "A Novel Streamline-based diffusion MRI Tractography Registration Method with Probabilistic Keypoint Detection", "authors": ["Junyi Wang", "Mubai Du", "Ye Wu", "Yijie Li", "William M. Wells III", "Lauren J. O'Donnell", "Fan Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.02481v2", "summary": "Registration of diffusion MRI tractography is an essential step for analyzing\ngroup similarities and variations in the brain's white matter (WM).\nStreamline-based registration approaches can leverage the 3D geometric\ninformation of fiber pathways to enable spatial alignment after registration.\nExisting methods usually rely on the optimization of the spatial distances to\nidentify the optimal transformation. However, such methods overlook point\nconnectivity patterns within the streamline itself, limiting their ability to\nidentify anatomical correspondences across tractography datasets. In this work,\nwe propose a novel unsupervised approach using deep learning to perform\nstreamline-based dMRI tractography registration. The overall idea is to\nidentify corresponding keypoint pairs across subjects for spatial alignment of\ntractography datasets. We model tractography as point clouds to leverage the\ngraph connectivity along streamlines. We propose a novel keypoint detection\nmethod for streamlines, framed as a probabilistic classification task to\nidentify anatomically consistent correspondences across unstructured streamline\nsets. In the experiments, we compare several existing methods and show highly\neffective and efficient tractography registration performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.02481v2", "cate": "cs.CV", "date": "2025-03-04", "updated": "2025-07-12"}
{"id": "2504.11130", "title": "Divergence of Empirical Neural Tangent Kernel in Classification Problems", "authors": ["Zixiong Yu", "Songtao Tian", "Guhan Chen"], "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      This is the revised version of our paper accepted at ICLR 2025, originally titled \"Divergence of Neural Tangent Kernel in Classification Problems\"", "url": "http://arxiv.org/abs/2504.11130v2", "summary": "This paper demonstrates that in classification problems, fully connected\nneural networks (FCNs) and residual neural networks (ResNets) cannot be\napproximated by kernel logistic regression based on the Neural Tangent Kernel\n(NTK) under overtraining (i.e., when training time approaches infinity).\nSpecifically, when using the cross-entropy loss, regardless of how large the\nnetwork width is (as long as it is finite), the empirical NTK diverges from the\nNTK on the training samples as training time increases. To establish this\nresult, we first demonstrate the strictly positive definiteness of the NTKs for\nmulti-layer FCNs and ResNets. Then, we prove that during training, % with the\ncross-entropy loss, the neural network parameters diverge if the smallest\neigenvalue of the empirical NTK matrix (Gram matrix) with respect to training\nsamples is bounded below by a positive constant. This behavior contrasts\nsharply with the lazy training regime commonly observed in regression problems.\nConsequently, using a proof by contradiction, we show that the empirical NTK\ndoes not uniformly converge to the NTK across all times on the training samples\nas the network width increases. We validate our theoretical results through\nexperiments on both synthetic data and the MNIST classification task. This\nfinding implies that NTK theory is not applicable in this context, with\nsignificant theoretical implications for understanding neural networks in\nclassification problems.", "comment": "This is the revised version of our paper accepted at ICLR 2025,\n  originally titled \"Divergence of Neural Tangent Kernel in Classification\n  Problems\"", "pdf_url": "http://arxiv.org/pdf/2504.11130v2", "cate": "cs.LG", "date": "2025-04-15", "updated": "2025-07-12"}
{"id": "2503.05332", "title": "CoMoGaussian: Continuous Motion-Aware Gaussian Splatting from Motion-Blurred Images", "authors": ["Jungho Lee", "Donghyeong Kim", "Dogyoon Lee", "Suhwan Cho", "Minhyeok Lee", "Wonjoon Lee", "Taeoh Kim", "Dongyoon Wee", "Sangyoun Lee"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Revised Version of CRiM-GS, Project Page: this https URL", "url": "http://arxiv.org/abs/2503.05332v2", "summary": "3D Gaussian Splatting (3DGS) has gained significant attention due to its\nhigh-quality novel view rendering, motivating research to address real-world\nchallenges. A critical issue is the camera motion blur caused by movement\nduring exposure, which hinders accurate 3D scene reconstruction. In this study,\nwe propose CoMoGaussian, a Continuous Motion-Aware Gaussian Splatting that\nreconstructs precise 3D scenes from motion-blurred images while maintaining\nreal-time rendering speed. Considering the complex motion patterns inherent in\nreal-world camera movements, we predict continuous camera trajectories using\nneural ordinary differential equations (ODEs). To ensure accurate modeling, we\nemploy rigid body transformations, preserving the shape and size of the object\nbut rely on the discrete integration of sampled frames. To better approximate\nthe continuous nature of motion blur, we introduce a continuous motion\nrefinement (CMR) transformation that refines rigid transformations by\nincorporating additional learnable parameters. By revisiting fundamental camera\ntheory and leveraging advanced neural ODE techniques, we achieve precise\nmodeling of continuous camera trajectories, leading to improved reconstruction\naccuracy. Extensive experiments demonstrate state-of-the-art performance both\nquantitatively and qualitatively on benchmark datasets, which include a wide\nrange of motion blur scenarios, from moderate to extreme blur.", "comment": "Revised Version of CRiM-GS, Project Page:\n  https://Jho-Yonsei.github.io/CoMoGaussian", "pdf_url": "http://arxiv.org/pdf/2503.05332v2", "cate": "cs.CV", "date": "2025-03-07", "updated": "2025-07-14"}
{"id": "2504.13414", "title": "Adaptive Non-local Observable on Quantum Neural Networks", "authors": ["Hsin-Yi Lin", "Huan-Hsin Tseng", "Samuel Yen-Chi Chen", "Shinjae Yoo"], "categories": ["quant-ph", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      Accepted at IEEE International Conference on Quantum Computing and Engineering (QCE), 2025", "url": "http://arxiv.org/abs/2504.13414v3", "summary": "Conventional Variational Quantum Circuits (VQCs) for Quantum Machine Learning\ntypically rely on a fixed Hermitian observable, often built from Pauli\noperators. Inspired by the Heisenberg picture, we propose an adaptive non-local\nmeasurement framework that substantially increases the model complexity of the\nquantum circuits. Our introduction of dynamical Hermitian observables with\nevolving parameters shows that optimizing VQC rotations corresponds to tracing\na trajectory in the observable space. This viewpoint reveals that standard VQCs\nare merely a special case of the Heisenberg representation.\n  Furthermore, we show that properly incorporating variational rotations with\nnon-local observables enhances qubit interaction and information mixture,\nadmitting flexible circuit designs. Two non-local measurement schemes are\nintroduced, and numerical simulations on classification tasks confirm that our\napproach outperforms conventional VQCs, yielding a more powerful and\nresource-efficient approach as a Quantum Neural Network.", "comment": "Accepted at IEEE International Conference on Quantum Computing and\n  Engineering (QCE), 2025", "pdf_url": "http://arxiv.org/pdf/2504.13414v3", "cate": "quant-ph", "date": "2025-04-18", "updated": "2025-07-11"}
{"id": "2503.06471", "title": "Online Dense Point Tracking with Streaming Memory", "authors": ["Qiaole Dong", "Yanwei Fu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025", "url": "http://arxiv.org/abs/2503.06471v2", "summary": "Dense point tracking is a challenging task requiring the continuous tracking\nof every point in the initial frame throughout a substantial portion of a\nvideo, even in the presence of occlusions. Traditional methods use optical flow\nmodels to directly estimate long-range motion, but they often suffer from\nappearance drifting without considering temporal consistency. Recent point\ntracking algorithms usually depend on sliding windows for indirect information\npropagation from the first frame to the current one, which is slow and less\neffective for long-range tracking. To account for temporal consistency and\nenable efficient information propagation, we present a lightweight and fast\nmodel with \\textbf{S}treaming memory for dense \\textbf{PO}int \\textbf{T}racking\nand online video processing. The \\textbf{SPOT} framework features three core\ncomponents: a customized memory reading module for feature enhancement, a\nsensory memory for short-term motion dynamics modeling, and a visibility-guided\nsplatting module for accurate information propagation. This combination enables\nSPOT to perform dense point tracking with state-of-the-art accuracy on the CVO\nbenchmark, as well as comparable or superior performance to offline models on\nsparse tracking benchmarks such as TAP-Vid and RoboTAP. Notably, SPOT with\n10$\\times$ smaller parameter numbers operates at least 2$\\times$ faster than\nprevious state-of-the-art models while maintaining the best performance on CVO.\nWe will release the models and codes at: https://dqiaole.github.io/SPOT/.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2503.06471v2", "cate": "cs.CV", "date": "2025-03-09", "updated": "2025-07-13"}
{"id": "2504.15266", "title": "Roll the dice & look before you leap: Going beyond the creative limits of next-token prediction", "authors": ["Vaishnavh Nagarajan", "Chen Henry Wu", "Charles Ding", "Aditi Raghunathan"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      ICML 2025 (oral)", "url": "http://arxiv.org/abs/2504.15266v3", "summary": "We design a suite of minimal algorithmic tasks that are a loose abstraction\nof open-ended real-world tasks. This allows us to cleanly and controllably\nquantify the creative limits of the present-day language model. Much like\nreal-world tasks that require a creative, far-sighted leap of thought, our\ntasks require an implicit, open-ended stochastic planning step that either (a)\ndiscovers new connections in an abstract knowledge graph (like in wordplay,\ndrawing analogies, or research) or (b) constructs new patterns (like in\ndesigning math problems or new proteins). In these tasks, we empirically and\nconceptually argue how next-token learning is myopic; multi-token approaches,\nnamely teacherless training and diffusion models, comparatively excel in\nproducing diverse and original output. Secondly, to elicit randomness without\nhurting coherence, we find that injecting noise at the input layer (dubbed\nseed-conditioning) works surprisingly as well as (and in some conditions,\nbetter than) temperature sampling from the output layer. Thus, our work offers\na principled, minimal test-bed for analyzing open-ended creative skills, and\noffers new arguments for going beyond next-token learning and temperature\nsampling. We make part of the code available under\nhttps://github.com/chenwu98/algorithmic-creativity", "comment": "ICML 2025 (oral)", "pdf_url": "http://arxiv.org/pdf/2504.15266v3", "cate": "cs.LG", "date": "2025-04-21", "updated": "2025-07-14"}
{"id": "2503.06678", "title": "Gamma: Toward Generic Image Assessment with Mixture of Assessment Experts", "authors": ["Hantao Zhou", "Rui Yang", "Longxiang Tang", "Guanyi Qin", "Runze Hu", "Xiu Li"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ACMMM 2025", "url": "http://arxiv.org/abs/2503.06678v2", "summary": "Image assessment aims to evaluate the quality and aesthetics of images and\nhas been applied across various scenarios, such as natural and AIGC scenes.\nExisting methods mostly address these sub-tasks or scenes individually. While\nsome works attempt to develop unified image assessment models, they have\nstruggled to achieve satisfactory performance or cover a broad spectrum of\nassessment scenarios. In this paper, we present \\textbf{Gamma}, a\n\\textbf{G}eneric im\\textbf{A}ge assess\\textbf{M}ent model using\n\\textbf{M}ixture of \\textbf{A}ssessment Experts, which can effectively assess\nimages from diverse scenes through mixed-dataset training. Achieving unified\ntraining in image assessment presents significant challenges due to annotation\nbiases across different datasets. To address this issue, we first propose a\nMixture of Assessment Experts (MoAE) module, which employs shared and adaptive\nexperts to dynamically learn common and specific knowledge for different\ndatasets, respectively. In addition, we introduce a Scene-based Differential\nPrompt (SDP) strategy, which uses scene-specific prompts to provide prior\nknowledge and guidance during the learning process, further boosting adaptation\nfor various scenes. Our Gamma model is trained and evaluated on 12 datasets\nspanning 6 image assessment scenarios. Extensive experiments show that our\nunified Gamma outperforms other state-of-the-art mixed-training methods by\nsignificant margins while covering more scenes. Codes are available at\nhttps://github.com/zht8506/Gamma.", "comment": "Accepted to ACMMM 2025", "pdf_url": "http://arxiv.org/pdf/2503.06678v2", "cate": "cs.CV", "date": "2025-03-09", "updated": "2025-07-14"}
{"id": "2505.00268", "title": "Consistency in Language Models: Current Landscape, Challenges, and Future Directions", "authors": ["Jekaterina Novikova", "Carol Anderson", "Borhane Blili-Hamelin", "Domenic Rosati", "Subhabrata Majumdar"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted in ICML 2025 Workshop on Reliable and Responsible Foundation Models", "url": "http://arxiv.org/abs/2505.00268v2", "summary": "The hallmark of effective language use lies in consistency: expressing\nsimilar meanings in similar contexts and avoiding contradictions. While human\ncommunication naturally demonstrates this principle, state-of-the-art language\nmodels (LMs) struggle to maintain reliable consistency across task- and\ndomain-specific applications. Here we examine the landscape of consistency\nresearch in LMs, analyze current approaches to measure aspects of consistency,\nand identify critical research gaps. Our findings point to an urgent need for\nquality benchmarks to measure and interdisciplinary approaches to ensure\nconsistency while preserving utility.", "comment": "Accepted in ICML 2025 Workshop on Reliable and Responsible Foundation\n  Models", "pdf_url": "http://arxiv.org/pdf/2505.00268v2", "cate": "cs.CL", "date": "2025-05-01", "updated": "2025-07-12"}
{"id": "2503.09394", "title": "Bidirectional Prototype-Reward co-Evolution for Test-Time Adaptation of Vision-Language Models", "authors": ["Xiaozhen Qiao", "Peng Huang", "Jiakang Yuan", "Xianda Guo", "Bowen Ye", "Chaocan Xue", "Ye Zheng", "Zhe Sun", "Xuelong Li"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.09394v2", "summary": "Test-time adaptation (TTA) is crucial in maintaining performance of Vision\nLanguage Models (VLMs) when facing distribution shifts, particularly when the\nsource data or target labels are inaccessible. Existing TTA methods\npredominantly leverage the output probability distribution of CLIP for feature\nevaluation, resulting in biases under domain shifts, which cause misclassified\nfeatures due to text priors or incorrect textual associations. To address these\nissues, we propose \\underline{B}idirectional Prototype-Reward co-Evolution\n(BPRE), a novel VLMs framework with TTA that integrates feature quality\nassessment with prototype evolution via a synergistic feedback loop. First, the\nMulti-dimensional Quality-aware Reward Module (MQRM) is designed to evaluate\nfeature quality and guide prototype refinement precisely. The continuous\nrefinement of prototype quality via Prototype-Reward Interactive Evolution\n(PRIE) enhances the computation more robust. Through this bidirectional\ninteraction, the precision of rewards and prototype evolution mutually\nreinforce each other, forming a self-evolving feedback cycle. Extensive\nexperiments conducted on 15 diverse recognition datasets demonstrate that our\nmodel consistently achieves superior performance compared to other SOTA\nmethods, and advances VLM generalization capabilities through emphasizing\ncomprehensive feature evaluation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.09394v2", "cate": "cs.CV", "date": "2025-03-12", "updated": "2025-07-12"}
{"id": "2505.00684", "title": "Visual Test-time Scaling for GUI Agent Grounding", "authors": ["Tiange Luo", "Lajanugen Logeswaran", "Justin Johnson", "Honglak Lee"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV2025, this https URL", "url": "http://arxiv.org/abs/2505.00684v2", "summary": "We introduce RegionFocus, a visual test-time scaling approach for Vision\nLanguage Model Agents. Understanding webpages is challenging due to the visual\ncomplexity of GUI images and the large number of interface elements, making\naccurate action selection difficult. Our approach dynamically zooms in on\nrelevant regions, reducing background clutter and improving grounding accuracy.\nTo support this process, we propose an image-as-map mechanism that visualizes\nkey landmarks at each step, providing a transparent action record and enables\nthe agent to effectively choose among action candidates. Even with a simple\nregion selection strategy, we observe significant performance gains of 28+\\% on\nScreenspot-pro and 24+\\% on WebVoyager benchmarks on top of two\nstate-of-the-art open vision language model agents, UI-TARS and Qwen2.5-VL,\nhighlighting the effectiveness of visual test-time scaling in interactive\nsettings. We achieve a new state-of-the-art grounding performance of 61.6\\% on\nthe ScreenSpot-Pro benchmark by applying RegionFocus to a Qwen2.5-VL-72B model.\nOur code will be released publicly at https://github.com/tiangeluo/RegionFocus.", "comment": "ICCV2025, https://github.com/tiangeluo/RegionFocus", "pdf_url": "http://arxiv.org/pdf/2505.00684v2", "cate": "cs.CV", "date": "2025-05-01", "updated": "2025-07-14"}
{"id": "2503.10200", "title": "LVAgent: Long Video Understanding by Multi-Round Dynamical Collaboration of MLLM Agents", "authors": ["Boyu Chen", "Zhengrong Yue", "Siran Chen", "Zikang Wang", "Yang Liu", "Peng Li", "Yali Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      accepted in ICCV 2025", "url": "http://arxiv.org/abs/2503.10200v3", "summary": "Existing MLLMs encounter significant challenges in modeling the temporal\ncontext within long videos. Currently, mainstream Agent-based methods use\nexternal tools to assist a single MLLM in answering long video questions.\nDespite such tool-based support, a solitary MLLM still offers only a partial\nunderstanding of long videos, resulting in limited performance. In order to\nbetter address long video tasks, we introduce LVAgent, the first framework\nenabling multi-round dynamic collaboration of MLLM agents in long video\nunderstanding. Our method consists of four key steps: 1) Selection: We\npre-select appropriate agents from the model library to form optimal agent\nteams based on different tasks. 2) Perception: We design an effective retrieval\nscheme for long videos to improve the coverage of critical temporal segments\nwhile maintaining computational efficiency. 3) Action: Agents answer long video\nquestions and exchange reasons. 4) Reflection: We evaluate each agent's\nperformance in each round of discussion and optimize the agent team for dynamic\ncollaboration. The agents iteratively refine their answers by multi-round\ndynamical collaboration of MLLM agents. LVAgent is the first agent system\nmethod that outperforms all closed-source models (like GPT-4o) and open-source\nmodels (like InternVL-2.5 and Qwen2-VL) in the long video understanding tasks.\nOur LVAgent achieves an accuracy of 80\\% on four mainstream long video\nunderstanding tasks. Notably, LVAgent improves accuracy by 13.3\\% on\nLongVideoBench. Code is available at https://github.com/64327069/LVAgent.", "comment": "accepted in ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2503.10200v3", "cate": "cs.CV", "date": "2025-03-13", "updated": "2025-07-14"}
{"id": "2505.06799", "title": "Quantum Observers: A NISQ Hardware Demonstration of Chaotic State Prediction Using Quantum Echo-state Networks", "authors": ["Erik L. Connerty", "Ethan N. Evans", "Gerasimos Angelatos", "Vignesh Narayanan"], "categories": ["quant-ph", "cs.AI"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      14 pages, 12 figures", "url": "http://arxiv.org/abs/2505.06799v2", "summary": "Recent advances in artificial intelligence have highlighted the remarkable\ncapabilities of neural network (NN)-powered systems on classical computers.\nHowever, these systems face significant computational challenges that limit\nscalability and efficiency. Quantum computers hold the potential to overcome\nthese limitations and increase processing power beyond classical systems.\nDespite this, integrating quantum computing with NNs remains largely unrealized\ndue to challenges posed by noise, decoherence, and high error rates in current\nquantum hardware. Here, we propose a novel quantum echo-state network (QESN)\ndesign and implementation algorithm that can operate within the presence of\nnoise on current IBM hardware. We apply classical control-theoretic response\nanalysis to characterize the QESN, emphasizing its rich nonlinear dynamics and\nmemory, as well as its ability to be fine-tuned with sparsity and re-uploading\nblocks. We validate our approach through a comprehensive demonstration of QESNs\nfunctioning as quantum observers, applied in both high-fidelity simulations and\nhardware experiments utilizing data from a prototypical chaotic Lorenz system.\nOur results show that the QESN can predict long time-series with persistent\nmemory, running over 100 times longer than the median T1 and T2 of the IBM\nMarrakesh QPU, achieving state-of-the-art time-series performance on\nsuperconducting hardware.", "comment": "14 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2505.06799v2", "cate": "quant-ph", "date": "2025-05-11", "updated": "2025-07-12"}
{"id": "2503.12720", "title": "Towards Open-World Generation of Stereo Images and Unsupervised Matching", "authors": ["Feng Qiao", "Zhexiao Xiong", "Eric Xing", "Nathan Jacobs"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2503.12720v2", "summary": "Stereo images are fundamental to numerous applications, including extended\nreality (XR) devices, autonomous driving, and robotics. Unfortunately,\nacquiring high-quality stereo images remains challenging due to the precise\ncalibration requirements of dual-camera setups and the complexity of obtaining\naccurate, dense disparity maps. Existing stereo image generation methods\ntypically focus on either visual quality for viewing or geometric accuracy for\nmatching, but not both. We introduce GenStereo, a diffusion-based approach, to\nbridge this gap. The method includes two primary innovations (1) conditioning\nthe diffusion process on a disparity-aware coordinate embedding and a warped\ninput image, allowing for more precise stereo alignment than previous methods,\nand (2) an adaptive fusion mechanism that intelligently combines the\ndiffusion-generated image with a warped image, improving both realism and\ndisparity consistency. Through extensive training on 11 diverse stereo\ndatasets, GenStereo demonstrates strong generalization ability. GenStereo\nachieves state-of-the-art performance in both stereo image generation and\nunsupervised stereo matching tasks. Project page is available at\nhttps://qjizhi.github.io/genstereo.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2503.12720v2", "cate": "cs.CV", "date": "2025-03-17", "updated": "2025-07-12"}
{"id": "2505.12864", "title": "LEXam: Benchmarking Legal Reasoning on 340 Law Exams", "authors": ["Yu Fan", "Jingwei Ni", "Jakob Merane", "Etienne Salimbeni", "Yang Tian", "Yoan Hermstrüwer", "Yinya Huang", "Mubashara Akhtar", "Florian Geering", "Oliver Dreyer", "Daniel Brunner", "Markus Leippold", "Mrinmaya Sachan", "Alexander Stremitzer", "Christoph Engel", "Elliott Ash", "Joel Niklaus"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.12864v3", "summary": "Long-form legal reasoning remains a key challenge for large language models\n(LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a\nnovel benchmark derived from 340 law exams spanning 116 law school courses\nacross a range of subjects and degree levels. The dataset comprises 4,886 law\nexam questions in English and German, including 2,841 long-form, open-ended\nquestions and 2,045 multiple-choice questions. Besides reference answers, the\nopen questions are also accompanied by explicit guidance outlining the expected\nlegal reasoning approach such as issue spotting, rule recall, or rule\napplication. Our evaluation on both open-ended and multiple-choice questions\npresent significant challenges for current LLMs; in particular, they notably\nstruggle with open questions that require structured, multi-step legal\nreasoning. Moreover, our results underscore the effectiveness of the dataset in\ndifferentiating between models with varying capabilities. Adopting an\nLLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate\nhow model-generated reasoning steps can be evaluated consistently and\naccurately. Our evaluation setup provides a scalable method to assess legal\nreasoning quality beyond simple accuracy metrics. Project page:\nhttps://lexam-benchmark.github.io/", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.12864v3", "cate": "cs.CL", "date": "2025-05-19", "updated": "2025-07-14"}
{"id": "2503.23519", "title": "BoundMatch: Boundary detection applied to semi-supervised segmentation for urban-driving scenes", "authors": ["Haruya Ishikawa", "Yoshimitsu Aoki"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      20 pages, 18 figures", "url": "http://arxiv.org/abs/2503.23519v3", "summary": "Semi-supervised semantic segmentation (SS-SS) aims to mitigate the heavy\nannotation burden of dense pixel labeling by leveraging abundant unlabeled\nimages alongside a small labeled set. While current consistency regularization\nmethods achieve strong results, they often overlook a critical challenge: the\nprecise delineation of object boundaries. In this paper, we propose BoundMatch,\na novel multi-task SS-SS framework that explicitly integrates semantic boundary\ndetection into a teacher-student consistency regularization pipeline. Our core\nmechanism, Boundary Consistency Regularized Multi-Task Learning (BCRM),\nenforces prediction agreement between teacher and student models on both\nsegmentation masks and detailed semantic boundaries. To further enhance\nperformance and sharpen boundaries, BoundMatch incorporates two lightweight\nfusion modules: Boundary-Semantic Fusion (BSF) injects learned boundary cues\ninto the segmentation decoder, while Spatial Gradient Fusion (SGF) refines\nboundary predictions using mask gradients, leading to higher-quality boundary\npseudo-labels. This framework is built upon SAMTH, a strong teacher-student\nbaseline featuring a Harmonious Batch Normalization (HBN) update strategy for\nimproved stability. Extensive experiments on diverse urban-driving scene\ndatasets including Cityscapes, BDD100K, and SYNTHIA show that BoundMatch\nachieves competitive performance against current state-of-the-art methods. Our\napproach achieves state-of-the-art results on the new benchmark with DINOv2\nfoundation model. We further validate our approach's generalizability on Pascal\nVOC and ADE20K datasets. Ablation studies highlight BoundMatch's ability to\nimprove boundary-specific evaluation metrics, its effectiveness in realistic\nlarge-scale unlabeled data scenarios, and applicability to lightweight\narchitectures for mobile deployment.", "comment": "20 pages, 18 figures", "pdf_url": "http://arxiv.org/pdf/2503.23519v3", "cate": "cs.CV", "date": "2025-03-30", "updated": "2025-07-13"}
{"id": "2505.16790", "title": "Learning Flexible Forward Trajectories for Masked Molecular Diffusion", "authors": ["Hyunjin Seo", "Taewon Kim", "Sihyun Yu", "SungSoo Ahn"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.16790v3", "summary": "Masked diffusion models (MDMs) have achieved notable progress in modeling\ndiscrete data, while their potential in molecular generation remains\nunderexplored. In this work, we explore their potential and introduce the\nsurprising result that naively applying standards MDMs severely degrades the\nperformance. We identify the critical cause of this issue as a state-clashing\nproblem-where the forward diffusion of distinct molecules collapse into a\ncommon state, resulting in a mixture of reconstruction targets that cannot be\nlearned using typical reverse diffusion process with unimodal predictions. To\nmitigate this, we propose Masked Element-wise Learnable Diffusion (MELD) that\norchestrates per-element corruption trajectories to avoid collision between\ndistinct molecular graphs. This is achieved through a parameterized noise\nscheduling network that assigns distinct corruption rates to individual graph\nelements, i.e., atoms and bonds. Extensive experiments on diverse molecular\nbenchmarks reveal that MELD markedly enhances overall generation quality\ncompared to element-agnostic noise scheduling, increasing the chemical validity\nof vanilla MDMs on ZINC250K from 15% to 93%, Furthermore, it achieves\nstate-of-the-art property alignment in conditional generation tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.16790v3", "cate": "cs.LG", "date": "2025-05-22", "updated": "2025-07-13"}
{"id": "2503.24391", "title": "Easi3R: Estimating Disentangled Motion from DUSt3R Without Training", "authors": ["Xingyu Chen", "Yue Chen", "Yuliang Xiu", "Andreas Geiger", "Anpei Chen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Page: this https URL Code: this https URL", "url": "http://arxiv.org/abs/2503.24391v2", "summary": "Recent advances in DUSt3R have enabled robust estimation of dense point\nclouds and camera parameters of static scenes, leveraging Transformer network\narchitectures and direct supervision on large-scale 3D datasets. In contrast,\nthe limited scale and diversity of available 4D datasets present a major\nbottleneck for training a highly generalizable 4D model. This constraint has\ndriven conventional 4D methods to fine-tune 3D models on scalable dynamic video\ndata with additional geometric priors such as optical flow and depths. In this\nwork, we take an opposite path and introduce Easi3R, a simple yet efficient\ntraining-free method for 4D reconstruction. Our approach applies attention\nadaptation during inference, eliminating the need for from-scratch pre-training\nor network fine-tuning. We find that the attention layers in DUSt3R inherently\nencode rich information about camera and object motion. By carefully\ndisentangling these attention maps, we achieve accurate dynamic region\nsegmentation, camera pose estimation, and 4D dense point map reconstruction.\nExtensive experiments on real-world dynamic videos demonstrate that our\nlightweight attention adaptation significantly outperforms previous\nstate-of-the-art methods that are trained or finetuned on extensive dynamic\ndatasets. Our code is publicly available for research purpose at\nhttps://easi3r.github.io/", "comment": "Page: https://easi3r.github.io/ Code:\n  https://github.com/Inception3D/Easi3R", "pdf_url": "http://arxiv.org/pdf/2503.24391v2", "cate": "cs.CV", "date": "2025-03-31", "updated": "2025-07-14"}
{"id": "2505.16801", "title": "A modular framework for automated evaluation of procedural content generation in serious games with deep reinforcement learning agents", "authors": ["Eleftherios Kalafatis", "Konstantinos Mitsis", "Konstantia Zarkogianni", "Maria Athanasiou", "Konstantina Nikita"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.16801v2", "summary": "Serious Games (SGs) are nowadays shifting focus to include procedural content\ngeneration (PCG) in the development process as a means of offering personalized\nand enhanced player experience. However, the development of a framework to\nassess the impact of PCG techniques when integrated into SGs remains\nparticularly challenging. This study proposes a methodology for automated\nevaluation of PCG integration in SGs, incorporating deep reinforcement learning\n(DRL) game testing agents. To validate the proposed framework, a previously\nintroduced SG featuring card game mechanics and incorporating three different\nversions of PCG for nonplayer character (NPC) creation has been deployed.\nVersion 1 features random NPC creation, while versions 2 and 3 utilize a\ngenetic algorithm approach. These versions are used to test the impact of\ndifferent dynamic SG environments on the proposed framework's agents. The\nobtained results highlight the superiority of the DRL game testing agents\ntrained on Versions 2 and 3 over those trained on Version 1 in terms of win\nrate (i.e. number of wins per played games) and training time. More\nspecifically, within the execution of a test emulating regular gameplay, both\nVersions 2 and 3 peaked at a 97% win rate and achieved statistically\nsignificant higher (p=0009) win rates compared to those achieved in Version 1\nthat peaked at 94%. Overall, results advocate towards the proposed framework's\ncapability to produce meaningful data for the evaluation of procedurally\ngenerated content in SGs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.16801v2", "cate": "cs.LG", "date": "2025-05-22", "updated": "2025-07-13"}
{"id": "2504.15085", "title": "Hierarchical Attention Fusion of Visual and Textual Representations for Cross-Domain Sequential Recommendation", "authors": ["Wangyu Wu", "Zhenhong Chen", "Siqi Song", "Xianglin Qiu", "Xiaowei Huang", "Fei Ma", "Jimin Xiao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at CogSCI 2025. arXiv admin note: text overlap with arXiv:2502.15694", "url": "http://arxiv.org/abs/2504.15085v3", "summary": "Cross-Domain Sequential Recommendation (CDSR) predicts user behavior by\nleveraging historical interactions across multiple domains, focusing on\nmodeling cross-domain preferences through intra- and inter-sequence item\nrelationships. Inspired by human cognitive processes, we propose Hierarchical\nAttention Fusion of Visual and Textual Representations (HAF-VT), a novel\napproach integrating visual and textual data to enhance cognitive modeling.\nUsing the frozen CLIP model, we generate image and text embeddings, enriching\nitem representations with multimodal data. A hierarchical attention mechanism\njointly learns single-domain and cross-domain preferences, mimicking human\ninformation integration. Evaluated on four e-commerce datasets, HAF-VT\noutperforms existing methods in capturing cross-domain user interests, bridging\ncognitive principles with computational models and highlighting the role of\nmultimodal data in sequential decision-making.", "comment": "Accepted at CogSCI 2025. arXiv admin note: text overlap with\n  arXiv:2502.15694", "pdf_url": "http://arxiv.org/pdf/2504.15085v3", "cate": "cs.CV", "date": "2025-04-21", "updated": "2025-07-13"}
{"id": "2505.18373", "title": "Next-token pretraining implies in-context learning", "authors": ["Paul M. Riechers", "Henry R. Bigelow", "Eric A. Alt", "Adam Shai"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.18373v2", "summary": "We argue that in-context learning (ICL) predictably arises from standard\nself-supervised next-token pretraining, rather than being an exotic emergent\nproperty. This work establishes the foundational principles of this emergence\nby focusing on in-distribution ICL, demonstrating how models necessarily adapt\nto context when trained on token sequences, especially from non-ergodic\nsources. Our information-theoretic framework precisely predicts these\nin-distribution ICL dynamics (i.e., context-dependent loss reduction). We\nverify this with experiments using synthetic datasets of differing types of\ncorrelational structure, reproducing characteristic phenomena like phase\ntransitions in training loss for induction head formation and power-law scaling\nof in-context loss. We further show that a model's in-context performance on\nany task is mathematically coupled to the ensemble of tasks seen in\npretraining, offering a fundamental explanation, grounded in architecture- and\nmodality-independent principles, for such inference-time learning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.18373v2", "cate": "cs.LG", "date": "2025-05-23", "updated": "2025-07-13"}
{"id": "2504.21771", "title": "WASABI: A Metric for Evaluating Morphometric Plausibility of Synthetic Brain MRIs", "authors": ["Bahram Jafrasteh", "Wei Peng", "Cheng Wan", "Yimin Luo", "Ehsan Adeli", "Qingyu Zhao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.21771v3", "summary": "Generative models enhance neuroimaging through data augmentation, quality\nimprovement, and rare condition studies. Despite advances in realistic\nsynthetic MRIs, evaluations focus on texture and perception, lacking\nsensitivity to crucial anatomical fidelity. This study proposes a new metric,\ncalled WASABI (Wasserstein-Based Anatomical Brain Index), to assess the\nanatomical realism of synthetic brain MRIs. WASABI leverages \\textit{SynthSeg},\na deep learning-based brain parcellation tool, to derive volumetric measures of\nbrain regions in each MRI and uses the multivariate Wasserstein distance to\ncompare distributions between real and synthetic anatomies. Based on controlled\nexperiments on two real datasets and synthetic MRIs from five generative\nmodels, WASABI demonstrates higher sensitivity in quantifying anatomical\ndiscrepancies compared to traditional image-level metrics, even when synthetic\nimages achieve near-perfect visual quality. Our findings advocate for shifting\nthe evaluation paradigm beyond visual inspection and conventional metrics,\nemphasizing anatomical fidelity as a crucial benchmark for clinically\nmeaningful brain MRI synthesis. Our code is available at\nhttps://github.com/BahramJafrasteh/wasabi-mri.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.21771v3", "cate": "cs.CV", "date": "2025-04-30", "updated": "2025-07-14"}
{"id": "2505.21363", "title": "Subgroups Matter for Robust Bias Mitigation", "authors": ["Anissa Alloula", "Charles Jones", "Ben Glocker", "Bartłomiej W. Papież"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.21363v3", "summary": "Despite the constant development of new bias mitigation methods for machine\nlearning, no method consistently succeeds, and a fundamental question remains\nunanswered: when and why do bias mitigation techniques fail? In this paper, we\nhypothesise that a key factor may be the often-overlooked but crucial step\nshared by many bias mitigation methods: the definition of subgroups. To\ninvestigate this, we conduct a comprehensive evaluation of state-of-the-art\nbias mitigation methods across multiple vision and language classification\ntasks, systematically varying subgroup definitions, including coarse,\nfine-grained, intersectional, and noisy subgroups. Our results reveal that\nsubgroup choice significantly impacts performance, with certain groupings\nparadoxically leading to worse outcomes than no mitigation at all. Our findings\nsuggest that observing a disparity between a set of subgroups is not a\nsufficient reason to use those subgroups for mitigation. Through theoretical\nanalysis, we explain these phenomena and uncover a counter-intuitive insight\nthat, in some cases, improving fairness with respect to a particular set of\nsubgroups is best achieved by using a different set of subgroups for\nmitigation. Our work highlights the importance of careful subgroup definition\nin bias mitigation and presents it as an alternative lever for improving the\nrobustness and fairness of machine learning models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.21363v3", "cate": "cs.LG", "date": "2025-05-27", "updated": "2025-07-14"}
{"id": "2505.01837", "title": "CVVNet: A Cross-Vertical-View Network for Gait Recognition", "authors": ["Xiangru Li", "Wei Song", "Yingda Huang", "Wei Meng", "Le Chang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.01837v2", "summary": "Gait recognition enables contact-free, long-range person identification that\nis robust to clothing variations and non-cooperative scenarios. While existing\nmethods perform well in controlled indoor environments, they struggle with\ncross-vertical view scenarios, where surveillance angles vary significantly in\nelevation. Our experiments show up to 60\\% accuracy degradation in low-to-high\nvertical view settings due to severe deformations and self-occlusions of key\nanatomical features. Current CNN and self-attention-based methods fail to\neffectively handle these challenges, due to their reliance on single-scale\nconvolutions or simplistic attention mechanisms that lack effective\nmulti-frequency feature integration. To tackle this challenge, we propose\nCVVNet (Cross-Vertical-View Network), a frequency aggregation architecture\nspecifically designed for robust cross-vertical-view gait recognition. CVVNet\nemploys a High-Low Frequency Extraction module (HLFE) that adopts parallel\nmulti-scale convolution/max-pooling path and self-attention path as high- and\nlow-frequency mixers for effective multi-frequency feature extraction from\ninput silhouettes. We also introduce the Dynamic Gated Aggregation (DGA)\nmechanism to adaptively adjust the fusion ratio of high- and low-frequency\nfeatures. The integration of our core Multi-Scale Attention Gated Aggregation\n(MSAGA) module, HLFE and DGA enables CVVNet to effectively handle distortions\nfrom view changes, significantly improving the recognition robustness across\ndifferent vertical views. Experimental results show that our CVVNet achieves\nstate-of-the-art performance, with $8.6\\%$ improvement on DroneGait and $2\\%$\non Gait3D compared with the best existing methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.01837v2", "cate": "cs.CV", "date": "2025-05-03", "updated": "2025-07-14"}
{"id": "2506.05821", "title": "FuseUNet: A Multi-Scale Feature Fusion Method for U-like Networks", "authors": ["Quansong He", "Xiangde Min", "Kaishen Wang", "Tao He"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICML2025", "url": "http://arxiv.org/abs/2506.05821v2", "summary": "Medical image segmentation is a critical task in computer vision, with UNet\nserving as a milestone architecture. The typical component of UNet family is\nthe skip connection, however, their skip connections face two significant\nlimitations: (1) they lack effective interaction between features at different\nscales, and (2) they rely on simple concatenation or addition operations, which\nconstrain efficient information integration. While recent improvements to UNet\nhave focused on enhancing encoder and decoder capabilities, these limitations\nremain overlooked. To overcome these challenges, we propose a novel multi-scale\nfeature fusion method that reimagines the UNet decoding process as solving an\ninitial value problem (IVP), treating skip connections as discrete nodes. By\nleveraging principles from the linear multistep method, we propose an adaptive\nordinary differential equation method to enable effective multi-scale feature\nfusion. Our approach is independent of the encoder and decoder architectures,\nmaking it adaptable to various U-Net-like networks. Experiments on ACDC,\nKiTS2023, MSD brain tumor, and ISIC2017/2018 skin lesion segmentation datasets\ndemonstrate improved feature utilization, reduced network parameters, and\nmaintained high performance. The code is available at\nhttps://github.com/nayutayuki/FuseUNet.", "comment": "ICML2025", "pdf_url": "http://arxiv.org/pdf/2506.05821v2", "cate": "cs.CV", "date": "2025-06-06", "updated": "2025-07-13"}
{"id": "2505.02178", "title": "Sparfels: Fast Reconstruction from Sparse Unposed Imagery", "authors": ["Shubhendu Jena", "Amine Ouasfi", "Mae Younes", "Adnane Boukhayma"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025. Project page : this https URL", "url": "http://arxiv.org/abs/2505.02178v2", "summary": "We present a method for Sparse view reconstruction with surface element\nsplatting that runs within 3 minutes on a consumer grade GPU. While few methods\naddress sparse radiance field learning from noisy or unposed sparse cameras,\nshape recovery remains relatively underexplored in this setting. Several\nradiance and shape learning test-time optimization methods address the sparse\nposed setting by learning data priors or using combinations of external\nmonocular geometry priors. Differently, we propose an efficient and simple\npipeline harnessing a single recent 3D foundation model. We leverage its\nvarious task heads, notably point maps and camera initializations to\ninstantiate a bundle adjusting 2D Gaussian Splatting (2DGS) model, and image\ncorrespondences to guide camera optimization midst 2DGS training. Key to our\ncontribution is a novel formulation of splatted color variance along rays,\nwhich can be computed efficiently. Reducing this moment in training leads to\nmore accurate shape reconstructions. We demonstrate state-of-the-art\nperformances in the sparse uncalibrated setting in reconstruction and novel\nview benchmarks based on established multi-view datasets.", "comment": "ICCV 2025. Project page :\n  https://shubhendu-jena.github.io/Sparfels-web/", "pdf_url": "http://arxiv.org/pdf/2505.02178v2", "cate": "cs.CV", "date": "2025-05-04", "updated": "2025-07-14"}
{"id": "2506.06955", "title": "BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning", "authors": ["Ha-Thanh Nguyen", "Chaoran Liu", "Qianying Liu", "Hideyuki Tachibana", "Su Myat Noe", "Yusuke Miyao", "Koichi Takeda", "Sadao Kurohashi"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      This version includes minor typo corrections in the example image", "url": "http://arxiv.org/abs/2506.06955v4", "summary": "We present BIS Reasoning 1.0, the first large-scale Japanese dataset of\nsyllogistic reasoning problems explicitly designed to evaluate\nbelief-inconsistent reasoning in large language models (LLMs). Unlike prior\ndatasets such as NeuBAROCO and JFLD, which focus on general or belief-aligned\nreasoning, BIS Reasoning 1.0 introduces logically valid yet belief-inconsistent\nsyllogisms to uncover reasoning biases in LLMs trained on human-aligned\ncorpora. We benchmark state-of-the-art models - including GPT models, Claude\nmodels, and leading Japanese LLMs - revealing significant variance in\nperformance, with GPT-4o achieving 79.54% accuracy. Our analysis identifies\ncritical weaknesses in current LLMs when handling logically valid but\nbelief-conflicting inputs. These findings have important implications for\ndeploying LLMs in high-stakes domains such as law, healthcare, and scientific\nliterature, where truth must override intuitive belief to ensure integrity and\nsafety.", "comment": "This version includes minor typo corrections in the example image", "pdf_url": "http://arxiv.org/pdf/2506.06955v4", "cate": "cs.CL", "date": "2025-06-08", "updated": "2025-07-14"}
{"id": "2505.02704", "title": "VGLD: Visually-Guided Linguistic Disambiguation for Monocular Depth Scale Recovery", "authors": ["Bojin Wu", "Jing Chen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      19 pages, conference", "url": "http://arxiv.org/abs/2505.02704v3", "summary": "Monocular depth estimation can be broadly categorized into two directions:\nrelative depth estimation, which predicts normalized or inverse depth without\nabsolute scale, and metric depth estimation, which aims to recover depth with\nreal-world scale. While relative methods are flexible and data-efficient, their\nlack of metric scale limits their utility in downstream tasks. A promising\nsolution is to infer absolute scale from textual descriptions. However, such\nlanguage-based recovery is highly sensitive to natural language ambiguity, as\nthe same image may be described differently across perspectives and styles. To\naddress this, we introduce VGLD (Visually-Guided Linguistic Disambiguation), a\nframework that incorporates high-level visual semantics to resolve ambiguity in\ntextual inputs. By jointly encoding both image and text, VGLD predicts a set of\nglobal linear transformation parameters that align relative depth maps with\nmetric scale. This visually grounded disambiguation improves the stability and\naccuracy of scale estimation. We evaluate VGLD on representative models,\nincluding MiDaS and DepthAnything, using standard indoor (NYUv2) and outdoor\n(KITTI) benchmarks. Results show that VGLD significantly mitigates scale\nestimation bias caused by inconsistent or ambiguous language, achieving robust\nand accurate metric predictions. Moreover, when trained on multiple datasets,\nVGLD functions as a universal and lightweight alignment module, maintaining\nstrong performance even in zero-shot settings. Code will be released upon\nacceptance.", "comment": "19 pages, conference", "pdf_url": "http://arxiv.org/pdf/2505.02704v3", "cate": "cs.CV", "date": "2025-05-05", "updated": "2025-07-13"}
{"id": "2506.10395", "title": "Pisces: An Auto-regressive Foundation Model for Image Understanding and Generation", "authors": ["Zhiyang Xu", "Jiuhai Chen", "Zhaojiang Lin", "Xichen Pan", "Lifu Huang", "Tianyi Zhou", "Madian Khabsa", "Qifan Wang", "Di Jin", "Michihiro Yasunaga", "Lili Yu", "Xi Victoria Lin", "Shaoliang Nie"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Unified image understanding and generation model", "url": "http://arxiv.org/abs/2506.10395v2", "summary": "Recent advances in large language models (LLMs) have enabled multimodal\nfoundation models to tackle both image understanding and generation within a\nunified framework. Despite these gains, unified models often underperform\ncompared to specialized models in either task. A key challenge in developing\nunified models lies in the inherent differences between the visual features\nneeded for image understanding versus generation, as well as the distinct\ntraining processes required for each modality. In this work, we introduce\nPisces, an auto-regressive multimodal foundation model that addresses this\nchallenge through a novel decoupled visual encoding architecture and tailored\ntraining techniques optimized for multimodal generation. Combined with\nmeticulous data curation, pretraining, and finetuning, Pisces achieves\ncompetitive performance in both image understanding and image generation. We\nevaluate Pisces on over 20 public benchmarks for image understanding, where it\ndemonstrates strong performance across a wide range of tasks. Additionally, on\nGenEval, a widely adopted benchmark for image generation, Pisces exhibits\nrobust generative capabilities. Our extensive analysis reveals the synergistic\nrelationship between image understanding and generation, and the benefits of\nusing separate visual encoders, advancing the field of unified multimodal\nmodels.", "comment": "Unified image understanding and generation model", "pdf_url": "http://arxiv.org/pdf/2506.10395v2", "cate": "cs.CV", "date": "2025-06-12", "updated": "2025-07-12"}
{"id": "2505.05759", "title": "A review of advancements in low-light image enhancement using deep learning", "authors": ["Fangxue Liu", "Lei Fan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.05759v2", "summary": "In low-light environments, the performance of computer vision algorithms\noften deteriorates significantly, adversely affecting key vision tasks such as\nsegmentation, detection, and classification. With the rapid advancement of deep\nlearning, its application to low-light image processing has attracted\nwidespread attention and seen significant progress in recent years. However,\nthere remains a lack of comprehensive surveys that systematically examine how\nrecent deep-learning-based low-light image enhancement methods function and\nevaluate their effectiveness in enhancing downstream vision tasks. To address\nthis gap, this review provides detailed elaboration on how various recent\napproaches (from 2020) operate and their enhancement mechanisms, supplemented\nwith clear illustrations. It also investigates the impact of different\nenhancement techniques on subsequent vision tasks, critically analyzing their\nstrengths and limitations. Our review found that image enhancement improved the\nperformance of downstream vision tasks to varying degrees. Although supervised\nmethods often produced images with high perceptual quality, they typically\nproduced modest improvements in vision tasks. In contrast, zero-shot learning,\ndespite achieving lower scores in image quality metrics, showed consistently\nboosted performance across various vision tasks. These suggest a disconnect\nbetween image quality metrics and those evaluating vision task performance.\nAdditionally, unsupervised domain adaptation techniques demonstrated\nsignificant gains in segmentation tasks, highlighting their potential in\npractical low-light scenarios where labelled data is scarce. Observed\nlimitations of existing studies are analyzed, and directions for future\nresearch are proposed. This review serves as a useful reference for determining\nlow-light image enhancement techniques and optimizing vision task performance\nin low-light conditions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.05759v2", "cate": "cs.CV", "date": "2025-05-09", "updated": "2025-07-14"}
{"id": "2506.12190", "title": "BreastDCEDL: A Comprehensive Breast Cancer DCE-MRI Dataset and Transformer Implementation for Treatment Response Prediction", "authors": ["Naomi Fridman", "Bubby Solway", "Tomer Fridman", "Itamar Barnea", "Anat Goldstein"], "categories": ["cs.CV", "cs.AI", "68T07, 68U10, 92C55", "I.2.0; I.2.10; I.4.5; J.3"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.12190v3", "summary": "Breast cancer remains a leading cause of cancer-related mortality worldwide,\nmaking early detection and accurate treatment response monitoring critical\npriorities. We present BreastDCEDL, a curated, deep learning-ready dataset\ncomprising pre-treatment 3D Dynamic Contrast-Enhanced MRI (DCE-MRI) scans from\n2,070 breast cancer patients drawn from the I-SPY1, I-SPY2, and Duke cohorts,\nall sourced from The Cancer Imaging Archive. The raw DICOM imaging data were\nrigorously converted into standardized 3D NIfTI volumes with preserved signal\nintegrity, accompanied by unified tumor annotations and harmonized clinical\nmetadata including pathologic complete response (pCR), hormone receptor (HR),\nand HER2 status. Although DCE-MRI provides essential diagnostic information and\ndeep learning offers tremendous potential for analyzing such complex data,\nprogress has been limited by lack of accessible, public, multicenter datasets.\nBreastDCEDL addresses this gap by enabling development of advanced models,\nincluding state-of-the-art transformer architectures that require substantial\ntraining data. To demonstrate its capacity for robust modeling, we developed\nthe first transformer-based model for breast DCE-MRI, leveraging Vision\nTransformer (ViT) architecture trained on RGB-fused images from three contrast\nphases (pre-contrast, early post-contrast, and late post-contrast). Our ViT\nmodel achieved state-of-the-art pCR prediction performance in HR+/HER2-\npatients (AUC 0.94, accuracy 0.93). BreastDCEDL includes predefined benchmark\nsplits, offering a framework for reproducible research and enabling clinically\nmeaningful modeling in breast cancer imaging.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.12190v3", "cate": "cs.CV", "date": "2025-06-13", "updated": "2025-07-13"}
{"id": "2505.08527", "title": "Leveraging Segment Anything Model for Source-Free Domain Adaptation via Dual Feature Guided Auto-Prompting", "authors": ["Zheang Huai", "Hui Tang", "Yi Li", "Zhuangzhuang Chen", "Xiaomeng Li"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted in TMI 2025", "url": "http://arxiv.org/abs/2505.08527v3", "summary": "Source-free domain adaptation (SFDA) for segmentation aims at adapting a\nmodel trained in the source domain to perform well in the target domain with\nonly the source model and unlabeled target data. Inspired by the recent success\nof Segment Anything Model (SAM) which exhibits the generality of segmenting\nimages of various modalities and in different domains given human-annotated\nprompts like bounding boxes or points, we for the first time explore the\npotentials of Segment Anything Model for SFDA via automatedly finding an\naccurate bounding box prompt. We find that the bounding boxes directly\ngenerated with existing SFDA approaches are defective due to the domain gap. To\ntackle this issue, we propose a novel Dual Feature Guided (DFG) auto-prompting\napproach to search for the box prompt. Specifically, the source model is first\ntrained in a feature aggregation phase, which not only preliminarily adapts the\nsource model to the target domain but also builds a feature distribution\nwell-prepared for box prompt search. In the second phase, based on two feature\ndistribution observations, we gradually expand the box prompt with the guidance\nof the target model feature and the SAM feature to handle the class-wise\nclustered target features and the class-wise dispersed target features,\nrespectively. To remove the potentially enlarged false positive regions caused\nby the over-confident prediction of the target model, the refined pseudo-labels\nproduced by SAM are further postprocessed based on connectivity analysis.\nExperiments on 3D and 2D datasets indicate that our approach yields superior\nperformance compared to conventional methods. Code is available at\nhttps://github.com/xmed-lab/DFG.", "comment": "Accepted in TMI 2025", "pdf_url": "http://arxiv.org/pdf/2505.08527v3", "cate": "cs.CV", "date": "2025-05-13", "updated": "2025-07-12"}
{"id": "2506.15686", "title": "Learning from M-Tuple Dominant Positive and Unlabeled Data", "authors": ["Jiahe Qin", "Junpeng Li", "Changchun Hua", "Yana Yang"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.15686v2", "summary": "Label Proportion Learning (LLP) addresses the classification problem where\nmultiple instances are grouped into bags and each bag contains information\nabout the proportion of each class. However, in practical applications,\nobtaining precise supervisory information regarding the proportion of instances\nin a specific class is challenging. To better align with real-world application\nscenarios and effectively leverage the proportional constraints of instances\nwithin tuples, this paper proposes a generalized learning framework\n\\emph{MDPU}. Specifically, we first mathematically model the distribution of\ninstances within tuples of arbitrary size, under the constraint that the number\nof positive instances is no less than that of negative instances. Then we\nderive an unbiased risk estimator that satisfies risk consistency based on the\nempirical risk minimization (ERM) method. To mitigate the inevitable\noverfitting issue during training, a risk correction method is introduced,\nleading to the development of a corrected risk estimator. The generalization\nerror bounds of the unbiased risk estimator theoretically demonstrate the\nconsistency of the proposed method. Extensive experiments on multiple datasets\nand comparisons with other relevant baseline methods comprehensively validate\nthe effectiveness of the proposed learning framework.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.15686v2", "cate": "cs.LG", "date": "2025-05-25", "updated": "2025-07-12"}
{"id": "2505.13327", "title": "Benchmarking Unified Face Attack Detection via Hierarchical Prompt Tuning", "authors": ["Ajian Liu", "Haocheng Yuan", "Xiao Guo", "Hui Ma", "Wanyi Zhuang", "Changtao Miao", "Yan Hong", "Chuanbiao Song", "Jun Lan", "Qi Chu", "Tao Gong", "Yanyan Liang", "Weiqiang Wang", "Jun Wan", "Xiaoming Liu", "Zhen Lei"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.13327v3", "summary": "PAD and FFD are proposed to protect face data from physical media-based\nPresentation Attacks and digital editing-based DeepFakes, respectively.\nHowever, isolated training of these two models significantly increases\nvulnerability towards unknown attacks, burdening deployment environments. The\nlack of a Unified Face Attack Detection model to simultaneously handle attacks\nin these two categories is mainly attributed to two factors: (1) A benchmark\nthat is sufficient for models to explore is lacking. Existing UAD datasets only\ncontain limited attack types and samples, leading to the model's confined\nability to address abundant advanced threats. In light of these, through an\nexplainable hierarchical way, we propose the most extensive and sophisticated\ncollection of forgery techniques available to date, namely UniAttackDataPlus.\nOur UniAttackData+ encompasses 2,875 identities and their 54 kinds of\ncorresponding falsified samples, in a total of 697,347 videos. (2) The absence\nof a trustworthy classification criterion. Current methods endeavor to explore\nan arbitrary criterion within the same semantic space, which fails to exist\nwhen encountering diverse attacks. Thus, we present a novel Visual-Language\nModel-based Hierarchical Prompt Tuning Framework that adaptively explores\nmultiple classification criteria from different semantic spaces. Specifically,\nwe construct a VP-Tree to explore various classification rules hierarchically.\nThen, by adaptively pruning the prompts, the model can select the most suitable\nprompts guiding the encoder to extract discriminative features at different\nlevels in a coarse-to-fine manner. Finally, to help the model understand the\nclassification criteria in visual space, we propose a DPI module to project the\nvisual prompts to the text encoder to help obtain a more accurate semantics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.13327v3", "cate": "cs.CV", "date": "2025-05-19", "updated": "2025-07-13"}
{"id": "2506.18421", "title": "TReB: A Comprehensive Benchmark for Evaluating Table Reasoning Capabilities of Large Language Models", "authors": ["Ce Li", "Xiaofan Liu", "Zhiyan Song", "Ce Chi", "Chen Zhao", "Jingjing Yang", "Zhendong Wang", "Kexin Yang", "Boshen Shi", "Xing Wang", "Chao Deng", "Junlan Feng"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Benmark report v1.1", "url": "http://arxiv.org/abs/2506.18421v2", "summary": "The majority of data in businesses and industries is stored in tables,\ndatabases, and data warehouses. Reasoning with table-structured data poses\nsignificant challenges for large language models (LLMs) due to its hidden\nsemantics, inherent complexity, and structured nature. One of these challenges\nis lacking an effective evaluation benchmark fairly reflecting the performances\nof LLMs on broad table reasoning abilities. In this paper, we fill in this gap,\npresenting a comprehensive table reasoning evolution benchmark, TReB, which\nmeasures both shallow table understanding abilities and deep table reasoning\nabilities, a total of 26 sub-tasks. We construct a high quality dataset through\nan iterative data processing procedure. We create an evaluation framework to\nrobustly measure table reasoning capabilities with three distinct inference\nmodes, TCoT, PoT and ICoT. Further, we benchmark over 20 state-of-the-art LLMs\nusing this frame work and prove its effectiveness. Experimental results reveal\nthat existing LLMs still have significant room for improvement in addressing\nthe complex and real world Table related tasks. Both the dataset and evaluation\nframework are publicly available, with the dataset hosted on\nhuggingface.co/datasets/JT-LM/JIUTIAN-TReB and the framework on\ngithub.com/JT-LM/jiutian-treb.", "comment": "Benmark report v1.1", "pdf_url": "http://arxiv.org/pdf/2506.18421v2", "cate": "cs.CL", "date": "2025-06-23", "updated": "2025-07-14"}
{"id": "2505.14359", "title": "Dual Data Alignment Makes AI-Generated Image Detector Easier Generalizable", "authors": ["Ruoxin Chen", "Junwei Xi", "Zhiyuan Yan", "Ke-Yue Zhang", "Shuang Wu", "Jingyi Xie", "Xu Chen", "Lei Xu", "Isabel Guan", "Taiping Yao", "Shouhong Ding"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      12 Pages, 9 figures", "url": "http://arxiv.org/abs/2505.14359v4", "summary": "Existing detectors are often trained on biased datasets, leading to the\npossibility of overfitting on non-causal image attributes that are spuriously\ncorrelated with real/synthetic labels. While these biased features enhance\nperformance on the training data, they result in substantial performance\ndegradation when applied to unbiased datasets. One common solution is to\nperform dataset alignment through generative reconstruction, matching the\nsemantic content between real and synthetic images. However, we revisit this\napproach and show that pixel-level alignment alone is insufficient. The\nreconstructed images still suffer from frequency-level misalignment, which can\nperpetuate spurious correlations. To illustrate, we observe that reconstruction\nmodels tend to restore the high-frequency details lost in real images (possibly\ndue to JPEG compression), inadvertently creating a frequency-level\nmisalignment, where synthetic images appear to have richer high-frequency\ncontent than real ones. This misalignment leads to models associating\nhigh-frequency features with synthetic labels, further reinforcing biased cues.\nTo resolve this, we propose Dual Data Alignment (DDA), which aligns both the\npixel and frequency domains. Moreover, we introduce two new test sets:\nDDA-COCO, containing DDA-aligned synthetic images for testing detector\nperformance on the most aligned dataset, and EvalGEN, featuring the latest\ngenerative models for assessing detectors under new generative architectures\nsuch as visual auto-regressive generators. Finally, our extensive evaluations\ndemonstrate that a detector trained exclusively on DDA-aligned MSCOCO could\nimprove across 8 diverse benchmarks by a non-trivial margin, showing a +7.2% on\nin-the-wild benchmarks, highlighting the improved generalizability of unbiased\ndetectors.", "comment": "12 Pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2505.14359v4", "cate": "cs.CV", "date": "2025-05-20", "updated": "2025-07-14"}
{"id": "2506.21278", "title": "Hyperspherical Variational Autoencoders Using Efficient Spherical Cauchy Distribution", "authors": ["Lukas Sablica", "Kurt Hornik"], "categories": ["stat.ML", "cs.AI", "cs.LG", "math.ST", "stat.TH"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.21278v2", "summary": "We propose a novel variational autoencoder (VAE) architecture that employs a\nspherical Cauchy (spCauchy) latent distribution. Unlike traditional Gaussian\nlatent spaces or the widely used von Mises-Fisher (vMF) distribution, spCauchy\nprovides a more natural hyperspherical representation of latent variables,\nbetter capturing directional data while maintaining flexibility. Its\nheavy-tailed nature prevents over-regularization, ensuring efficient latent\nspace utilization while offering a more expressive representation.\nAdditionally, spCauchy circumvents the numerical instabilities inherent to vMF,\nwhich arise from computing normalization constants involving Bessel functions.\nInstead, it enables a fully differentiable and efficient reparameterization\ntrick via M\\\"obius transformations, allowing for stable and scalable training.\nThe KL divergence can be computed through a rapidly converging power series,\neliminating concerns of underflow or overflow associated with evaluation of\nratios of hypergeometric functions. These properties make spCauchy a compelling\nalternative for VAEs, offering both theoretical advantages and practical\nefficiency in high-dimensional generative modeling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21278v2", "cate": "stat.ML", "date": "2025-06-26", "updated": "2025-07-14"}
{"id": "2505.15137", "title": "Multispectral Detection Transformer with Infrared-Centric Feature Fusion", "authors": ["Seongmin Hwang", "Daeyoung Han", "Moongu Jeon"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Under Review", "url": "http://arxiv.org/abs/2505.15137v2", "summary": "Multispectral object detection aims to leverage complementary information\nfrom visible (RGB) and infrared (IR) modalities to enable robust performance\nunder diverse environmental conditions. Our key insight, derived from wavelet\nanalysis and empirical observations, is that IR images contain structurally\nrich high-frequency information critical for object detection, making an\ninfrared-centric approach highly effective. To capitalize on this finding, we\npropose Infrared-Centric Fusion (IC-Fusion), a lightweight and modality-aware\nsensor fusion method that prioritizes infrared features while effectively\nintegrating complementary RGB semantic context. IC-Fusion adopts a compact RGB\nbackbone and designs a novel fusion module comprising a Multi-Scale Feature\nDistillation (MSFD) block to enhance RGB features and a three-stage fusion\nblock with a Cross-Modal Channel Shuffle Gate (CCSG), a Cross-Modal Large\nKernel Gate (CLKG), and a Channel Shuffle Projection (CSP) to facilitate\neffective cross-modal interaction. Experiments on the FLIR and LLVIP benchmarks\ndemonstrate the superior effectiveness and efficiency of our IR-centric fusion\nstrategy, further validating its benefits. Our code is available at\nhttps://github.com/smin-hwang/IC-Fusion.", "comment": "Under Review", "pdf_url": "http://arxiv.org/pdf/2505.15137v2", "cate": "cs.CV", "date": "2025-05-21", "updated": "2025-07-14"}
{"id": "2506.22566", "title": "Exploration Behavior of Untrained Policies", "authors": ["Jacob Adamczyk"], "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      High-dimensional Learning Dynamics Workshop at ICML-2025", "url": "http://arxiv.org/abs/2506.22566v2", "summary": "Exploration remains a fundamental challenge in reinforcement learning (RL),\nparticularly in environments with sparse or adversarial reward structures. In\nthis work, we study how the architecture of deep neural policies implicitly\nshapes exploration before training. We theoretically and empirically\ndemonstrate strategies for generating ballistic or diffusive trajectories from\nuntrained policies in a toy model. Using the theory of infinite-width networks\nand a continuous-time limit, we show that untrained policies return correlated\nactions and result in non-trivial state-visitation distributions. We discuss\nthe distributions of the corresponding trajectories for a standard\narchitecture, revealing insights into inductive biases for tackling\nexploration. Our results establish a theoretical and experimental framework for\nusing policy initialization as a design tool to understand exploration behavior\nin early training.", "comment": "High-dimensional Learning Dynamics Workshop at ICML-2025", "pdf_url": "http://arxiv.org/pdf/2506.22566v2", "cate": "cs.LG", "date": "2025-06-27", "updated": "2025-07-11"}
{"id": "2505.18608", "title": "Spiking Transformers Need High Frequency Information", "authors": ["Yuetong Fang", "Deming Zhou", "Ziqing Wang", "Hongwei Ren", "ZeCui Zeng", "Lusong Li", "Shibo Zhou", "Renjing Xu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.18608v2", "summary": "Spiking Transformers offer an energy-efficient alternative to conventional\ndeep learning by transmitting information solely through binary (0/1) spikes.\nHowever, there remains a substantial performance gap compared to artificial\nneural networks. A common belief is that their binary and sparse activation\ntransmission leads to information loss, thus degrading feature representation\nand accuracy. In this work, however, we reveal for the first time that spiking\nneurons preferentially propagate low-frequency information. We hypothesize that\nthe rapid dissipation of high-frequency components is the primary cause of\nperformance degradation. For example, on Cifar-100, adopting Avg-Pooling\n(low-pass) for token mixing lowers performance to 76.73%; interestingly,\nreplacing it with Max-Pooling (high-pass) pushes the top-1 accuracy to 79.12%,\nsurpassing the well-tuned Spikformer baseline by 0.97%. Accordingly, we\nintroduce Max-Former that restores high-frequency signals through two\nfrequency-enhancing operators: extra Max-Pooling in patch embedding and\nDepth-Wise Convolution in place of self-attention. Notably, our Max-Former\n(63.99 M) hits the top-1 accuracy of 82.39% on ImageNet, showing a +7.58%\nimprovement over Spikformer with comparable model size (74.81%, 66.34 M). We\nhope this simple yet effective solution inspires future research to explore the\ndistinctive nature of spiking neural networks, beyond the established practice\nin standard deep learning.\n\\href{https://github.com/bic-L/Spiking-Transformers-Need-High-Frequency-Information}{Code}\nis available.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.18608v2", "cate": "cs.CV", "date": "2025-05-24", "updated": "2025-07-13"}
{"id": "2506.22777", "title": "Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning", "authors": ["Miles Turpin", "Andy Arditi", "Marvin Li", "Joe Benton", "Julian Michael"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Published at ICML 2025 Workshop on Reliable and Responsible Foundation Models", "url": "http://arxiv.org/abs/2506.22777v2", "summary": "Language models trained with reinforcement learning (RL) can engage in reward\nhacking--the exploitation of unintended strategies for high reward--without\nrevealing this behavior in their chain-of-thought reasoning. This makes the\ndetection of reward hacking difficult, posing risks for high-stakes\napplications. We propose verbalization fine-tuning (VFT), a pre-RL fine-tuning\nintervention that trains models to explicitly acknowledge when they are\ninfluenced by prompt cues--hints which point to incorrect answers (e.g., \"a\nStanford professor thinks the answer is A\"). To evaluate VFT, we subsequently\ntrain models with RL on environments where held-out prompt cues signal which\nincorrect answers will receive high reward, incentivizing models to exploit\nthese cues instead of reasoning correctly. We measure how often models exploit\nthese cues without verbalizing it. After RL, only 6% of the VFT-trained model's\nresponses consist of undetected reward hacks. In comparison, when we perform RL\nwithout VFT, the rate of undetected reward hacks goes up to 88%; with a\ndebiasing baseline intervention, this increases further to 99%. VFT achieves\nthis by substantially increasing how often models verbalize the influence of\ncues, from 8% to 43% after VFT, and up to 94% after RL. Baselines remain low\neven after RL (11% and 1%). Our results show that teaching models to explicitly\nverbalize reward hacking behavior before RL significantly improves their\ndetection, offering a practical path toward more transparent and safe AI\nsystems.", "comment": "Published at ICML 2025 Workshop on Reliable and Responsible\n  Foundation Models", "pdf_url": "http://arxiv.org/pdf/2506.22777v2", "cate": "cs.CL", "date": "2025-06-28", "updated": "2025-07-13"}
{"id": "2505.19319", "title": "Holistic White-light Polyp Classification via Alignment-free Dense Distillation of Auxiliary Optical Chromoendoscopy", "authors": ["Qiang Hu", "Qimei Wang", "Jia Chen", "Xuantao Ji", "Mei Liu", "Qiang Li", "Zhiwei Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Early Accepted by MICCAI 2025", "url": "http://arxiv.org/abs/2505.19319v3", "summary": "White Light Imaging (WLI) and Narrow Band Imaging (NBI) are the two main\ncolonoscopic modalities for polyp classification. While NBI, as optical\nchromoendoscopy, offers valuable vascular details, WLI remains the most common\nand often the only available modality in resource-limited settings. However,\nWLI-based methods typically underperform, limiting their clinical\napplicability. Existing approaches transfer knowledge from NBI to WLI through\nglobal feature alignment but often rely on cropped lesion regions, which are\nsusceptible to detection errors and neglect contextual and subtle diagnostic\ncues. To address this, this paper proposes a novel holistic classification\nframework that leverages full-image diagnosis without requiring polyp\nlocalization. The key innovation lies in the Alignment-free Dense Distillation\n(ADD) module, which enables fine-grained cross-domain knowledge distillation\nregardless of misalignment between WLI and NBI images. Without resorting to\nexplicit image alignment, ADD learns pixel-wise cross-domain affinities to\nestablish correspondences between feature maps, guiding the distillation along\nthe most relevant pixel connections. To further enhance distillation\nreliability, ADD incorporates Class Activation Mapping (CAM) to filter\ncross-domain affinities, ensuring the distillation path connects only those\nsemantically consistent regions with equal contributions to polyp diagnosis.\nExtensive results on public and in-house datasets show that our method achieves\nstate-of-the-art performance, relatively outperforming the other approaches by\nat least 2.5% and 16.2% in AUC, respectively. Code is available at:\nhttps://github.com/Huster-Hq/ADD.", "comment": "Early Accepted by MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2505.19319v3", "cate": "cs.CV", "date": "2025-05-25", "updated": "2025-07-12"}
{"id": "2506.22895", "title": "Interpretable Time Series Autoregression for Periodicity Quantification", "authors": ["Xinyu Chen", "Vassilis Digalakis Jr", "Lijun Ding", "Dingyi Zhuang", "Jinhua Zhao"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.22895v2", "summary": "Time series autoregression (AR) is a classical tool for modeling\nauto-correlations and periodic structures in real-world systems. We revisit\nthis model from an interpretable machine learning perspective by introducing\nsparse autoregression (SAR), where $\\ell_0$-norm constraints are used to\nisolate dominant periodicities. We formulate exact mixed-integer optimization\n(MIO) approaches for both stationary and non-stationary settings and introduce\ntwo scalable extensions: a decision variable pruning (DVP) strategy for\ntemporally-varying SAR (TV-SAR), and a two-stage optimization scheme for\nspatially- and temporally-varying SAR (STV-SAR). These models enable scalable\ninference on real-world spatiotemporal datasets. We validate our framework on\nlarge-scale mobility and climate time series. On NYC ridesharing data, TV-SAR\nreveals interpretable daily and weekly cycles as well as long-term shifts due\nto COVID-19. On climate datasets, STV-SAR uncovers the evolving spatial\nstructure of temperature and precipitation seasonality across four decades in\nNorth America and detects global sea surface temperature dynamics, including El\nNi\\~no. Together, our results demonstrate the interpretability, flexibility,\nand scalability of sparse autoregression for periodicity quantification in\ncomplex time series.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22895v2", "cate": "cs.LG", "date": "2025-06-28", "updated": "2025-07-13"}
{"id": "2506.01758", "title": "Many-for-Many: Unify the Training of Multiple Video and Image Generation and Manipulation Tasks", "authors": ["Tao Yang", "Ruibin Li", "Yangming Shi", "Yuqi Zhang", "Qide Dong", "Haoran Cheng", "Weiguo Feng", "Shilei Wen", "Bingyue Peng", "Lei Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.01758v2", "summary": "Diffusion models have shown impressive performance in many visual generation\nand manipulation tasks. Many existing methods focus on training a model for a\nspecific task, especially, text-to-video (T2V) generation, while many other\nworks focus on finetuning the pretrained T2V model for image-to-video (I2V),\nvideo-to-video (V2V), image and video manipulation tasks, etc. However,\ntraining a strong T2V foundation model requires a large amount of high-quality\nannotations, which is very costly. In addition, many existing models can\nperform only one or several tasks. In this work, we introduce a unified\nframework, namely many-for-many, which leverages the available training data\nfrom many different visual generation and manipulation tasks to train a single\nmodel for those different tasks. Specifically, we design a lightweight adapter\nto unify the different conditions in different tasks, then employ a joint\nimage-video learning strategy to progressively train the model from scratch.\nOur joint learning leads to a unified visual generation and manipulation model\nwith improved video generation performance. In addition, we introduce depth\nmaps as a condition to help our model better perceive the 3D space in visual\ngeneration. Two versions of our model are trained with different model sizes\n(8B and 2B), each of which can perform more than 10 different tasks. In\nparticular, our 8B model demonstrates highly competitive performance in video\ngeneration tasks compared to open-source and even commercial engines. Our\nmodels and source codes are available at https://github.com/leeruibin/MfM.git.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.01758v2", "cate": "cs.CV", "date": "2025-06-02", "updated": "2025-07-12"}
{"id": "2506.23377", "title": "Perspective Dial: Measuring Perspective of Text and Guiding LLM Outputs", "authors": ["Taejin Kim", "Siun-Chuon Mau", "Konrad Vesey"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      7 pages, 5 main pages of text, 5 figures, 2 tables. Research work performed at CACI INTL INC", "url": "http://arxiv.org/abs/2506.23377v2", "summary": "Large language models (LLMs) are used in a variety of mission-critical roles.\nDue to the rapidly developing nature of LLMs, there is a lack of quantifiable\nunderstanding of the bias and perspective associated with LLM output. Inspired\nby this need, this paper considers the broader issue of perspective or\nviewpoint of general text and perspective control of large-language model (LLM)\noutput. Perspective-Dial consists of two main components: a (1) metric space,\ndubbed Perspective Space, that enables quantitative measurements of different\nperspectives regarding a topic, and the use of (2) Systematic Prompt\nEngineering that utilizes greedy-coordinate descent to control LLM output\nperspective based on measurement feedback from the Perspective Space. The\nempirical nature of the approach allows progress to side step a principled\nunderstanding of perspective or bias -- effectively quantifying and adjusting\noutputs for a variety of topics. Potential applications include detection,\ntracking and mitigation of LLM bias, narrative detection, sense making and\ntracking in public discourse, and debate bot advocating given perspective.", "comment": "7 pages, 5 main pages of text, 5 figures, 2 tables. Research work\n  performed at CACI INTL INC", "pdf_url": "http://arxiv.org/pdf/2506.23377v2", "cate": "cs.CL", "date": "2025-06-29", "updated": "2025-07-12"}
{"id": "2506.04526", "title": "EECD-Net: Energy-Efficient Crack Detection with Spiking Neural Networks and Gated Attention", "authors": ["Shuo Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Withdrawn by the authors due to a critical bug in our energy consumption analysis. The script for calculating synaptic operations (SOPs) for baseline models was flawed, leading to an incorrect overestimation of our method's energy efficiency", "url": "http://arxiv.org/abs/2506.04526v2", "summary": "Crack detection on road surfaces is a critical measurement technology in the\ninstrumentation domain, essential for ensuring infrastructure safety and\ntransportation reliability. However, due to limited energy and low-resolution\nimaging, smart terminal devices struggle to maintain real-time monitoring\nperformance. To overcome these challenges, this paper proposes a multi-stage\ndetection approach for road crack detection, EECD-Net, to enhance accuracy and\nenergy efficiency of instrumentation. Specifically, the sophisticated\nSuper-Resolution Convolutional Neural Network (SRCNN) is employed to address\nthe inherent challenges of low-quality images, which effectively enhance image\nresolution while preserving critical structural details. Meanwhile, a Spike\nConvolution Unit (SCU) with Continuous Integrate-and-Fire (CIF) neurons is\nproposed to convert these images into sparse pulse sequences, significantly\nreducing power consumption. Additionally, a Gated Attention Transformer (GAT)\nmodule is designed to strategically fuse multi-scale feature representations\nthrough adaptive attention mechanisms, effectively capturing both long-range\ndependencies and intricate local crack patterns, and significantly enhancing\ndetection robustness across varying crack morphologies. The experiments on the\nCrackVision12K benchmark demonstrate that EECD-Net achieves a remarkable 98.6\\%\ndetection accuracy, surpassing state-of-the-art counterparts such as\nHybrid-Segmentor by a significant 1.5\\%. Notably, the EECD-Net maintains\nexceptional energy efficiency, consuming merely 5.6 mJ, which is a substantial\n33\\% reduction compared to baseline implementations. This work pioneers a\ntransformative approach in instrumentation-based crack detection, offering a\nscalable, low-power solution for real-time, large-scale infrastructure\nmonitoring in resource-constrained environments.", "comment": "Withdrawn by the authors due to a critical bug in our energy\n  consumption analysis. The script for calculating synaptic operations (SOPs)\n  for baseline models was flawed, leading to an incorrect overestimation of our\n  method's energy efficiency", "pdf_url": "http://arxiv.org/pdf/2506.04526v2", "cate": "cs.CV", "date": "2025-06-05", "updated": "2025-07-12"}
{"id": "2506.23724", "title": "When Small Guides Large: Cross-Model Co-Learning for Test-Time Adaptation", "authors": ["Chang'an Yi", "Xiaohui Deng", "Guohao Chen", "Yan Zhou", "Qinghua Lu", "Shuaicheng Niu"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      15 pages, 5 figures", "url": "http://arxiv.org/abs/2506.23724v2", "summary": "Test-time Adaptation (TTA) adapts a given model to testing domain data with\npotential domain shifts through online unsupervised learning, yielding\nimpressive performance. However, to date, existing TTA methods primarily focus\non single-model adaptation. In this work, we investigate an intriguing\nquestion: how does cross-model knowledge influence the TTA process? Our\nfindings reveal that, in TTA's unsupervised online setting, each model can\nprovide complementary, confident knowledge to the others, even when there are\nsubstantial differences in model size. For instance, a smaller model like\nMobileViT (10.6M parameters) can effectively guide a larger model like ViT-Base\n(86.6M parameters). In light of this, we propose COCA, a Cross-Model\nCo-Learning framework for TTA, which mainly consists of two main strategies. 1)\nCo-adaptation adaptively integrates complementary knowledge from other models\nthroughout the TTA process, reducing individual model biases. 2)\nSelf-adaptation enhances each model's unique strengths via unsupervised\nlearning, enabling diverse adaptation to the target domain. Extensive\nexperiments show that COCA, which can also serve as a plug-and-play module,\nsignificantly boosts existing SOTAs, on models with various sizes--including\nResNets, ViTs, and Mobile-ViTs--via cross-model co-learned TTA. For example,\nwith Mobile-ViT's guidance, COCA raises ViT-Base's average adaptation accuracy\non ImageNet-C from 51.7% to 64.5%. The code is publicly available at\nhttps://github.com/ycarobot/COCA.", "comment": "15 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.23724v2", "cate": "cs.CV", "date": "2025-06-30", "updated": "2025-07-12"}
{"id": "2506.07966", "title": "SpaCE-10: A Comprehensive Benchmark for Multimodal Large Language Models in Compositional Spatial Intelligence", "authors": ["Ziyang Gong", "Wenhao Li", "Oliver Ma", "Songyuan Li", "Jiayi Ji", "Xue Yang", "Gen Luo", "Junchi Yan", "Rongrong Ji"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.07966v2", "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable progress in\nvarious multimodal tasks. To pursue higher intelligence in space, MLLMs require\nintegrating multiple atomic spatial capabilities to handle complex and dynamic\ntasks. However, existing benchmarks struggle to comprehensively evaluate the\nspatial intelligence of common MLLMs from the atomic level to the compositional\nlevel. To fill this gap, we present SpaCE-10, a comprehensive benchmark for\ncompositional spatial evaluations. In SpaCE-10, we define 10 atomic spatial\ncapabilities, which are combined to form 8 compositional capabilities. Based on\nthese definitions, we propose a novel hierarchical annotation pipeline to\ngenerate high-quality and diverse question-answer (QA) pairs. With over 150+\nhours of human expert effort, we obtain over 5k QA pairs for 811 real indoor\nscenes in SpaCE-10, which covers various evaluation settings like point cloud\ninput and multi-choice QA. We conduct an extensive evaluation of common MLLMs\non SpaCE-10 and find that even the most advanced MLLM still lags behind humans\nby large margins. Through our careful study, we also draw several significant\nfindings that benefit the MLLM community. For example, we reveal that the\nshortcoming of counting capability greatly limits the compositional spatial\ncapabilities of existing MLLMs. The evaluation code and benchmark datasets are\navailable at https://github.com/Cuzyoung/SpaCE-10.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.07966v2", "cate": "cs.CV", "date": "2025-06-09", "updated": "2025-07-13"}
{"id": "2506.24085", "title": "Imagine for Me: Creative Conceptual Blending of Real Images and Text via Blended Attention", "authors": ["Wonwoong Cho", "Yanxia Zhang", "Yan-Ying Chen", "David I. Inouye"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project website is available at this https URL", "url": "http://arxiv.org/abs/2506.24085v2", "summary": "Blending visual and textual concepts into a new visual concept is a unique\nand powerful trait of human beings that can fuel creativity. However, in\npractice, cross-modal conceptual blending for humans is prone to cognitive\nbiases, like design fixation, which leads to local minima in the design space.\nIn this paper, we propose a T2I diffusion adapter \"IT-Blender\" that can\nautomate the blending process to enhance human creativity. Prior works related\nto cross-modal conceptual blending are limited in encoding a real image without\nloss of details or in disentangling the image and text inputs. To address these\ngaps, IT-Blender leverages pretrained diffusion models (SD and FLUX) to blend\nthe latent representations of a clean reference image with those of the noisy\ngenerated image. Combined with our novel blended attention, IT-Blender encodes\nthe real reference image without loss of details and blends the visual concept\nwith the object specified by the text in a disentangled way. Our experiment\nresults show that IT-Blender outperforms the baselines by a large margin in\nblending visual and textual concepts, shedding light on the new application of\nimage generative models to augment human creativity.", "comment": "Project website is available at https://imagineforme.github.io/", "pdf_url": "http://arxiv.org/pdf/2506.24085v2", "cate": "cs.CV", "date": "2025-06-30", "updated": "2025-07-14"}
{"id": "2506.10344", "title": "RealKeyMorph: Keypoints in Real-world Coordinates for Resolution-agnostic Image Registration", "authors": ["Mina C. Moghadam", "Alan Q. Wang", "Omer Taub", "Martin R. Prince", "Mert R. Sabuncu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      23 pages, 8 figures", "url": "http://arxiv.org/abs/2506.10344v2", "summary": "Many real-world settings require registration of a pair of medical images\nthat differ in spatial resolution, which may arise from differences in image\nacquisition parameters like pixel spacing, slice thickness, and field-of-view.\nHowever, all previous machine learning-based registration techniques resample\nimages onto a fixed resolution. This is suboptimal because resampling can\nintroduce artifacts due to interpolation. To address this, we present\nRealKeyMorph (RKM), a resolution-agnostic method for image registration. RKM is\nan extension of KeyMorph, a registration framework which works by training a\nnetwork to learn corresponding keypoints for a given pair of images, after\nwhich a closed-form keypoint matching step is used to derive the transformation\nthat aligns them. To avoid resampling and enable operating on the raw data, RKM\noutputs keypoints in real-world coordinates of the scanner. To do this, we\nleverage the affine matrix produced by the scanner (e.g., MRI machine) that\nencodes the mapping from voxel coordinates to real world coordinates. By\ntransforming keypoints into real-world space and integrating this into the\ntraining process, RKM effectively enables the extracted keypoints to be\nresolution-agnostic. In our experiments, we demonstrate the advantages of RKM\non the registration task for orthogonal 2D stacks of abdominal MRIs, as well as\n3D volumes with varying resolutions in brain datasets.", "comment": "23 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2506.10344v2", "cate": "cs.CV", "date": "2025-06-12", "updated": "2025-07-14"}
{"id": "2507.00038", "title": "Quality over Quantity: An Effective Large-Scale Data Reduction Strategy Based on Pointwise V-Information", "authors": ["Fei Chen", "Wenchi Zhou"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.00038v2", "summary": "In order to increase the effectiveness of model training, data reduction is\nessential to data-centric AI. It does this by locating the most instructive\nexamples in massive datasets. To increase data quality and training efficiency,\nthe main difficulty is to choose the best examples rather than the complete\ndatasets. In this paper, we propose an effective data reduction strategy based\non Pointwise -Information (PVI). To enable a static method, we first use PVI to\nquantify instance difficulty and remove instances with low difficulty.\nExperiments show that the classifier performance is maintained with only a\n0.0001% to 0.76% reduction in accuracy when 10%-30% of the data is removed.\nSecond, we train the classifiers using a progressive learning strategy on\nexamples sorted by increasing PVI, accelerating convergence and achieving a\n0.8% accuracy gain over conventional training. Our findings imply that training\na classifier on the chosen optimal subset may improve model performance and\nincrease training efficiency when combined with an efficient data reduction\nstrategy. Furthermore, we have adapted the PVI framework, which was previously\nlimited to English datasets, to a variety of Chinese NLP tasks and base models,\nyielding insightful results for faster training and cross-lingual data\nreduction. The codes are released at\nhttps://github.com/zhouwenchi/DatasetReductionStrategy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00038v2", "cate": "cs.LG", "date": "2025-06-19", "updated": "2025-07-14"}
{"id": "2506.11143", "title": "On the development of an AI performance and behavioural measures for teaching and classroom management", "authors": ["Andreea I. Niculescu", "Jochen Ehnes", "Chen Yi", "Du Jiawei", "Tay Chiat Pin", "Joey Tianyi Zhou", "Vigneshwaran Subbaraju", "Teh Kah Kuan", "Tran Huy Dat", "John Komar", "Gi Soong Chee", "Kenneth Kwok"], "categories": ["cs.CV", "H.5; J.4; I.2.7; I.2.10"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      7 pages, 10 figures, A video demonstration of the teacher trainer dashboard can be accessed here: this https URL", "url": "http://arxiv.org/abs/2506.11143v2", "summary": "This paper presents a two-year research project focused on developing\nAI-driven measures to analyze classroom dynamics, with particular emphasis on\nteacher actions captured through multimodal sensor data. We applied real-time\ndata from classroom sensors and AI techniques to extract meaningful insights\nand support teacher development. Key outcomes include a curated audio-visual\ndataset, novel behavioral measures, and a proof-of-concept teaching review\ndashboard. An initial evaluation with eight researchers from the National\nInstitute for Education (NIE) highlighted the system's clarity, usability, and\nits non-judgmental, automated analysis approach -- which reduces manual\nworkloads and encourages constructive reflection. Although the current version\ndoes not assign performance ratings, it provides an objective snapshot of\nin-class interactions, helping teachers recognize and improve their\ninstructional strategies. Designed and tested in an Asian educational context,\nthis work also contributes a culturally grounded methodology to the growing\nfield of AI-based educational analytics.", "comment": "7 pages, 10 figures, A video demonstration of the teacher trainer\n  dashboard can be accessed here: https://vimeo.com/1076482827", "pdf_url": "http://arxiv.org/pdf/2506.11143v2", "cate": "cs.CV", "date": "2025-06-11", "updated": "2025-07-14"}
{"id": "2507.01003", "title": "Description of the Training Process of Neural Networks via Ergodic Theorem : Ghost nodes", "authors": ["Eun-Ji Park", "Sangwon Yun"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      16 pages, 9 figures", "url": "http://arxiv.org/abs/2507.01003v3", "summary": "Recent studies have proposed interpreting the training process from an\nergodic perspective. Building on this foundation, we present a unified\nframework for understanding and accelerating the training of deep neural\nnetworks via stochastic gradient descent (SGD). By analyzing the geometric\nlandscape of the objective function we introduce a practical diagnostic, the\nrunning estimate of the largest Lyapunov exponent, which provably distinguishes\ngenuine convergence toward stable minimizers from mere statistical\nstabilization near saddle points. We then propose a ghost category extension\nfor standard classifiers that adds auxiliary ghost output nodes so the model\ngains extra descent directions that open a lateral corridor around narrow loss\nbarriers and enable the optimizer to bypass poor basins during the early\ntraining phase. We show that this extension strictly reduces the approximation\nerror and that after sufficient convergence the ghost dimensions collapse so\nthat the extended model coincides with the original one and there exists a path\nin the enlarged parameter space along which the total loss does not increase.\nTaken together, these results provide a principled architecture level\nintervention that accelerates early stage trainability while preserving\nasymptotic behavior and simultaneously serves as an architecture-friendly\nregularizer.", "comment": "16 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2507.01003v3", "cate": "cs.LG", "date": "2025-07-01", "updated": "2025-07-13"}
{"id": "2506.15285", "title": "AI-driven visual monitoring of industrial assembly tasks", "authors": ["Mattia Nardon", "Stefano Messelodi", "Antonio Granata", "Fabio Poiesi", "Alberto Danese", "Davide Boscaini"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.15285v2", "summary": "Visual monitoring of industrial assembly tasks is critical for preventing\nequipment damage due to procedural errors and ensuring worker safety. Although\ncommercial solutions exist, they typically require rigid workspace setups or\nthe application of visual markers to simplify the problem. We introduce ViMAT,\na novel AI-driven system for real-time visual monitoring of assembly tasks that\noperates without these constraints. ViMAT combines a perception module that\nextracts visual observations from multi-view video streams with a reasoning\nmodule that infers the most likely action being performed based on the observed\nassembly state and prior task knowledge. We validate ViMAT on two assembly\ntasks, involving the replacement of LEGO components and the reconfiguration of\nhydraulic press molds, demonstrating its effectiveness through quantitative and\nqualitative analysis in challenging real-world scenarios characterized by\npartial and uncertain visual observations. Project page:\nhttps://tev-fbk.github.io/ViMAT", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.15285v2", "cate": "cs.CV", "date": "2025-06-18", "updated": "2025-07-14"}
{"id": "2507.01504", "title": "Following the Clues: Experiments on Person Re-ID using Cross-Modal Intelligence", "authors": ["Robert Aufschläger", "Youssef Shoeb", "Azarm Nowzad", "Michael Heigl", "Fabian Bally", "Martin Schramm"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      accepted for publication at the 2025 IEEE 28th International Conference on Intelligent Transportation Systems (ITSC 2025), taking place during November 18-21, 2025 in Gold Coast, Australia", "url": "http://arxiv.org/abs/2507.01504v2", "summary": "The collection and release of street-level recordings as Open Data play a\nvital role in advancing autonomous driving systems and AI research. However,\nthese datasets pose significant privacy risks, particularly for pedestrians,\ndue to the presence of Personally Identifiable Information (PII) that extends\nbeyond biometric traits such as faces. In this paper, we present cRID, a novel\ncross-modal framework combining Large Vision-Language Models, Graph Attention\nNetworks, and representation learning to detect textual describable clues of\nPII and enhance person re-identification (Re-ID). Our approach focuses on\nidentifying and leveraging interpretable features, enabling the detection of\nsemantically meaningful PII beyond low-level appearance cues. We conduct a\nsystematic evaluation of PII presence in person image datasets. Our experiments\nshow improved performance in practical cross-dataset Re-ID scenarios, notably\nfrom Market-1501 to CUHK03-np (detected), highlighting the framework's\npractical utility. Code is available at https://github.com/RAufschlaeger/cRID.", "comment": "accepted for publication at the 2025 IEEE 28th International\n  Conference on Intelligent Transportation Systems (ITSC 2025), taking place\n  during November 18-21, 2025 in Gold Coast, Australia", "pdf_url": "http://arxiv.org/pdf/2507.01504v2", "cate": "cs.CV", "date": "2025-07-02", "updated": "2025-07-14"}
{"id": "2506.18527", "title": "Auto-Regressively Generating Multi-View Consistent Images", "authors": ["JiaKui Hu", "Yuxiao Yang", "Jialun Liu", "Jinbo Wu", "Chen Zhao", "Yanye Lu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025. Code is at this https URL", "url": "http://arxiv.org/abs/2506.18527v2", "summary": "Generating multi-view images from human instructions is crucial for 3D\ncontent creation. The primary challenges involve maintaining consistency across\nmultiple views and effectively synthesizing shapes and textures under diverse\nconditions. In this paper, we propose the Multi-View Auto-Regressive\n(\\textbf{MV-AR}) method, which leverages an auto-regressive model to\nprogressively generate consistent multi-view images from arbitrary prompts.\nFirstly, the next-token-prediction capability of the AR model significantly\nenhances its effectiveness in facilitating progressive multi-view synthesis.\nWhen generating widely-separated views, MV-AR can utilize all its preceding\nviews to extract effective reference information. Subsequently, we propose a\nunified model that accommodates various prompts via architecture designing and\ntraining strategies. To address multiple conditions, we introduce condition\ninjection modules for text, camera pose, image, and shape. To manage\nmulti-modal conditions simultaneously, a progressive training strategy is\nemployed. This strategy initially adopts the text-to-multi-view (t2mv) model as\na baseline to enhance the development of a comprehensive X-to-multi-view (X2mv)\nmodel through the randomly dropping and combining conditions. Finally, to\nalleviate the overfitting problem caused by limited high-quality data, we\npropose the ``Shuffle View\" data augmentation technique, thus significantly\nexpanding the training data by several magnitudes. Experiments demonstrate the\nperformance and versatility of our MV-AR, which consistently generates\nconsistent multi-view images across a range of conditions and performs on par\nwith leading diffusion-based multi-view image generation models. The code and\nmodels are released at https://github.com/MILab-PKU/MVAR.", "comment": "Accepted by ICCV 2025. Code is at https://github.com/MILab-PKU/MVAR", "pdf_url": "http://arxiv.org/pdf/2506.18527v2", "cate": "cs.CV", "date": "2025-06-23", "updated": "2025-07-13"}
{"id": "2507.02342", "title": "DeltaSHAP: Explaining Prediction Evolutions in Online Patient Monitoring with Shapley Values", "authors": ["Changhun Kim", "Yechan Mun", "Sangchul Hahn", "Eunho Yang"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted to ICML 2025 Workshop on Actionable Interpretability. Code is available at this https URL", "url": "http://arxiv.org/abs/2507.02342v2", "summary": "This study proposes DeltaSHAP, a novel explainable artificial intelligence\n(XAI) algorithm specifically designed for online patient monitoring systems. In\nclinical environments, discovering the causes driving patient risk evolution is\ncritical for timely intervention, yet existing XAI methods fail to address the\nunique requirements of clinical time series explanation tasks. To this end,\nDeltaSHAP addresses three key clinical needs: explaining the changes in the\nconsecutive predictions rather than isolated prediction scores, providing both\nmagnitude and direction of feature attributions, and delivering these insights\nin real time. By adapting Shapley values to temporal settings, our approach\naccurately captures feature coalition effects. It further attributes prediction\nchanges using only the actually observed feature combinations, making it\nefficient and practical for time-sensitive clinical applications. We also\nintroduce new evaluation metrics to evaluate the faithfulness of the\nattributions for online time series, and demonstrate through experiments on\nonline patient monitoring tasks that DeltaSHAP outperforms state-of-the-art XAI\nmethods in both explanation quality as 62% and computational efficiency as 33%\ntime reduction on the MIMIC-III decompensation benchmark. We release our code\nat https://github.com/AITRICS/DeltaSHAP.", "comment": "Accepted to ICML 2025 Workshop on Actionable Interpretability. Code\n  is available at https://github.com/AITRICS/DeltaSHAP", "pdf_url": "http://arxiv.org/pdf/2507.02342v2", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-12"}
{"id": "2506.22027", "title": "Cross-modal Ship Re-Identification via Optical and SAR Imagery: A Novel Dataset and Method", "authors": ["Han Wang", "Shengyang Li", "Jian Yang", "Yuxuan Liu", "Yixuan Lv", "Zhuang Zhou"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2506.22027v2", "summary": "Detecting and tracking ground objects using earth observation imagery remains\na significant challenge in the field of remote sensing. Continuous maritime\nship tracking is crucial for applications such as maritime search and rescue,\nlaw enforcement, and shipping analysis. However, most current ship tracking\nmethods rely on geostationary satellites or video satellites. The former offer\nlow resolution and are susceptible to weather conditions, while the latter have\nshort filming durations and limited coverage areas, making them less suitable\nfor the real-world requirements of ship tracking. To address these limitations,\nwe present the Hybrid Optical and Synthetic Aperture Radar (SAR) Ship\nRe-Identification Dataset (HOSS ReID dataset), designed to evaluate the\neffectiveness of ship tracking using low-Earth orbit constellations of optical\nand SAR sensors. This approach ensures shorter re-imaging cycles and enables\nall-weather tracking. HOSS ReID dataset includes images of the same ship\ncaptured over extended periods under diverse conditions, using different\nsatellites of different modalities at varying times and angles. Furthermore, we\npropose a baseline method for cross-modal ship re-identification, TransOSS,\nwhich is built on the Vision Transformer architecture. It refines the patch\nembedding structure to better accommodate cross-modal tasks, incorporates\nadditional embeddings to introduce more reference information, and employs\ncontrastive learning to pre-train on large-scale optical-SAR image pairs,\nensuring the model's ability to extract modality-invariant features. Our\ndataset and baseline method are publicly available on\nhttps://github.com/Alioth2000/Hoss-ReID.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.22027v2", "cate": "cs.CV", "date": "2025-06-27", "updated": "2025-07-14"}
{"id": "2507.03152", "title": "Expert-level validation of AI-generated medical text with scalable language models", "authors": ["Asad Aali", "Vasiliki Bikia", "Maya Varma", "Nicole Chiou", "Sophie Ostmeier", "Arnav Singhvi", "Magdalini Paschali", "Ashwin Kumar", "Andrew Johnston", "Karimar Amador-Martinez", "Eduardo Juan Perez Guerrero", "Paola Naovi Cruz Rivera", "Sergios Gatidis", "Christian Bluethgen", "Eduardo Pontes Reis", "Eddy D. Zandee van Rilland", "Poonam Laxmappa Hosamani", "Kevin R Keet", "Minjoung Go", "Evelyn Ling", "David B. Larson", "Curtis Langlotz", "Roxana Daneshjou", "Jason Hom", "Sanmi Koyejo", "Emily Alsentzer", "Akshay S. Chaudhari"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.03152v2", "summary": "With the growing use of language models (LMs) in clinical environments, there\nis an immediate need to evaluate the accuracy and safety of LM-generated\nmedical text. Currently, such evaluation relies solely on manual physician\nreview. However, detecting errors in LM-generated text is challenging because\n1) manual review is costly and 2) expert-composed reference outputs are often\nunavailable in real-world settings. While the \"LM-as-judge\" paradigm (a LM\nevaluating another LM) offers scalable evaluation, even frontier LMs can miss\nsubtle but clinically significant errors. To address these challenges, we\npropose MedVAL, a self-supervised framework that leverages synthetic data to\ntrain evaluator LMs to assess whether LM-generated medical outputs are\nfactually consistent with inputs, without requiring physician labels or\nreference outputs. To evaluate LM performance, we introduce MedVAL-Bench, a\ndataset containing 840 outputs annotated by physicians, following a\nphysician-defined taxonomy of risk levels and error categories. Across 6\ndiverse medical tasks and 10 state-of-the-art LMs spanning open-source,\nproprietary, and medically adapted models, MedVAL fine-tuning significantly\nimproves (p < 0.001) alignment with physicians on both seen and unseen tasks,\nincreasing average F1 scores from 66% to 83%, with per-sample safety\nclassification scores up to 86%. MedVAL improves the performance of even the\nbest-performing proprietary LM (GPT-4o) by 8%. To support a scalable,\nrisk-aware pathway towards clinical integration, we open-source the 1) codebase\n(https://github.com/StanfordMIMI/MedVAL), 2) MedVAL-Bench\n(https://huggingface.co/datasets/stanfordmimi/MedVAL-Bench), and 3) MedVAL-4B\n(https://huggingface.co/stanfordmimi/MedVAL-4B), the best-performing\nopen-source LM. Our research provides the first evidence of LMs approaching\nexpert-level validation ability for medical text.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.03152v2", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-14"}
{"id": "2506.22589", "title": "LIGHT: Multi-Modal Text Linking on Historical Maps", "authors": ["Yijun Lin", "Rhett Olson", "Junhan Wu", "Yao-Yi Chiang", "Jerod Weinman"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at ICDAR2025", "url": "http://arxiv.org/abs/2506.22589v2", "summary": "Text on historical maps provides valuable information for studies in history,\neconomics, geography, and other related fields. Unlike structured or\nsemi-structured documents, text on maps varies significantly in orientation,\nreading order, shape, and placement. Many modern methods can detect and\ntranscribe text regions, but they struggle to effectively ``link'' the\nrecognized text fragments, e.g., determining a multi-word place name. Existing\nlayout analysis methods model word relationships to improve text understanding\nin structured documents, but they primarily rely on linguistic features and\nneglect geometric information, which is essential for handling map text. To\naddress these challenges, we propose LIGHT, a novel multi-modal approach that\nintegrates linguistic, image, and geometric features for linking text on\nhistorical maps. In particular, LIGHT includes a geometry-aware embedding\nmodule that encodes the polygonal coordinates of text regions to capture\npolygon shapes and their relative spatial positions on an image. LIGHT unifies\nthis geometric information with the visual and linguistic token embeddings from\nLayoutLMv3, a pretrained layout analysis model. LIGHT uses the cross-modal\ninformation to predict the reading-order successor of each text instance\ndirectly with a bi-directional learning strategy that enhances sequence\nrobustness. Experimental results show that LIGHT outperforms existing methods\non the ICDAR 2024/2025 MapText Competition data, demonstrating the\neffectiveness of multi-modal learning for historical map text linking.", "comment": "Accepted at ICDAR2025", "pdf_url": "http://arxiv.org/pdf/2506.22589v2", "cate": "cs.CV", "date": "2025-06-27", "updated": "2025-07-11"}
{"id": "2507.03334", "title": "De-Fake: Style based Anomaly Deepfake Detection", "authors": ["Sudev Kumar Padhi", "Harshit Kumar", "Umesh Kashyap", "Sk. Subidh Ali"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.03334v2", "summary": "Detecting deepfakes involving face-swaps presents a significant challenge,\nparticularly in real-world scenarios where anyone can perform face-swapping\nwith freely available tools and apps without any technical knowledge. Existing\ndeepfake detection methods rely on facial landmarks or inconsistencies in\npixel-level features and often struggle with face-swap deepfakes, where the\nsource face is seamlessly blended into the target image or video. The\nprevalence of face-swap is evident in everyday life, where it is used to spread\nfalse information, damage reputations, manipulate political opinions, create\nnon-consensual intimate deepfakes (NCID), and exploit children by enabling the\ncreation of child sexual abuse material (CSAM). Even prominent public figures\nare not immune to its impact, with numerous deepfakes of them circulating\nwidely across social media platforms. Another challenge faced by deepfake\ndetection methods is the creation of datasets that encompass a wide range of\nvariations, as training models require substantial amounts of data. This raises\nprivacy concerns, particularly regarding the processing and storage of personal\nfacial data, which could lead to unauthorized access or misuse. Our key idea is\nto identify these style discrepancies to detect face-swapped images effectively\nwithout accessing the real facial image. We perform comprehensive evaluations\nusing multiple datasets and face-swapping methods, which showcases the\neffectiveness of SafeVision in detecting face-swap deepfakes across diverse\nscenarios. SafeVision offers a reliable and scalable solution for detecting\nface-swaps in a privacy preserving manner, making it particularly effective in\nchallenging real-world applications. To the best of our knowledge, SafeVision\nis the first deepfake detection using style features while providing inherent\nprivacy protection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.03334v2", "cate": "cs.CV", "date": "2025-07-04", "updated": "2025-07-14"}
{"id": "2506.23502", "title": "LLM-enhanced Action-aware Multi-modal Prompt Tuning for Image-Text Matching", "authors": ["Mengxiao Tian", "Xinxiao Wu", "Shuo Yang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      accepted by ICCV 2025", "url": "http://arxiv.org/abs/2506.23502v2", "summary": "Driven by large-scale contrastive vision-language pre-trained models such as\nCLIP, recent advancements in the image-text matching task have achieved\nremarkable success in representation learning. Due to image-level\nvisual-language alignment, CLIP falls short in understanding fine-grained\ndetails such as object attributes and spatial relationships between objects.\nRecent efforts have attempted to compel CLIP to acquire structured visual\nrepresentations by introducing prompt learning to achieve object-level\nalignment. While achieving promising results, they still lack the capability to\nperceive actions, which are crucial for describing the states or relationships\nbetween objects. Therefore, we propose to endow CLIP with fine-grained\naction-level understanding by introducing an LLM-enhanced action-aware\nmulti-modal prompt-tuning method, incorporating the action-related external\nknowledge generated by large language models (LLMs). Specifically, we design an\naction triplet prompt and an action state prompt to exploit compositional\nsemantic knowledge and state-related causal knowledge implicitly stored in\nLLMs. Subsequently, we propose an adaptive interaction module to aggregate\nattentive visual features conditioned on action-aware prompted knowledge for\nestablishing discriminative and action-aware visual representations, which\nfurther improves the performance. Comprehensive experimental results on two\nbenchmark datasets demonstrate the effectiveness of our method.", "comment": "accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.23502v2", "cate": "cs.CV", "date": "2025-06-30", "updated": "2025-07-12"}
{"id": "2507.03558", "title": "An Efficient Deep Learning Framework for Brain Stroke Diagnosis Using Computed Tomography (CT) Images", "authors": ["Md. Sabbir Hossen", "Eshat Ahmed Shuvo", "Shibbir Ahmed Arif", "Pabon Shaha", "Md. Saiduzzaman", "Mostofa Kamal Nasir"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Preprint version. Submitted for peer review", "url": "http://arxiv.org/abs/2507.03558v2", "summary": "Brain stroke is a leading cause of mortality and long-term disability\nworldwide, underscoring the need for precise and rapid prediction techniques.\nComputed Tomography (CT) scan is considered one of the most effective methods\nfor diagnosing brain strokes. Most stroke classification techniques use a\nsingle slice-level prediction mechanism, requiring radiologists to manually\nselect the most critical CT slice from the original CT volume. Although\nclinical evaluations are often used in traditional diagnostic procedures,\nmachine learning (ML) has opened up new avenues for improving stroke diagnosis.\nTo supplement traditional diagnostic techniques, this study investigates\nmachine learning models for early brain stroke prediction using CT scan images.\nThis research proposes a novel machine learning approach to brain stroke\ndetection, focusing on optimizing classification performance with pre-trained\ndeep learning models and advanced optimization strategies. Pre-trained models,\nincluding DenseNet201, InceptionV3, MobileNetV2, ResNet50, and Xception, are\nused for feature extraction. Feature engineering techniques, including BFO,\nPCA, and LDA, further enhance model performance. These features are then\nclassified using machine learning algorithms, including SVC, RF, XGB, DT, LR,\nKNN, and GNB. Our experiments demonstrate that the combination of MobileNetV2,\nLDA, and SVC achieved the highest classification accuracy of 97.93%,\nsignificantly outperforming other model-optimizer-classifier combinations. The\nresults underline the effectiveness of integrating lightweight pre-trained\nmodels with robust optimization and classification techniques for brain stroke\ndiagnosis.", "comment": "Preprint version. Submitted for peer review", "pdf_url": "http://arxiv.org/pdf/2507.03558v2", "cate": "cs.CV", "date": "2025-07-04", "updated": "2025-07-13"}
{"id": "2507.02222", "title": "High-Fidelity Differential-information Driven Binary Vision Transformer", "authors": ["Tian Gao", "Zhiyuan Zhang", "Kaijie Yin", "Xu-Cheng Zhong", "Hui Kong"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02222v2", "summary": "The binarization of vision transformers (ViTs) offers a promising approach to\naddressing the trade-off between high computational/storage demands and the\nconstraints of edge-device deployment. However, existing binary ViT methods\noften suffer from severe performance degradation or rely heavily on\nfull-precision modules. To address these issues, we propose DIDB-ViT, a novel\nbinary ViT that is highly informative while maintaining the original ViT\narchitecture and computational efficiency. Specifically, we design an\ninformative attention module incorporating differential information to mitigate\ninformation loss caused by binarization and enhance high-frequency retention.\nTo preserve the fidelity of the similarity calculations between binary Q and K\ntensors, we apply frequency decomposition using the discrete Haar wavelet and\nintegrate similarities across different frequencies. Additionally, we introduce\nan improved RPReLU activation function to restructure the activation\ndistribution, expanding the model's representational capacity. Experimental\nresults demonstrate that our DIDB-ViT significantly outperforms\nstate-of-the-art network quantization methods in multiple ViT architectures,\nachieving superior image classification and segmentation performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02222v2", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-13"}
{"id": "2507.03633", "title": "From Video to EEG: Adapting Joint Embedding Predictive Architecture to Uncover Visual Concepts in Brain Signal Analysis", "authors": ["Amirabbas Hojjati", "Lu Li", "Ibrahim Hameed", "Anis Yazidi", "Pedro G. Lind", "Rabindra Khadka"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.03633v4", "summary": "EEG signals capture brain activity with high temporal and low spatial\nresolution, supporting applications such as neurological diagnosis, cognitive\nmonitoring, and brain-computer interfaces. However, effective analysis is\nhindered by limited labeled data, high dimensionality, and the absence of\nscalable models that fully capture spatiotemporal dependencies. Existing\nself-supervised learning (SSL) methods often focus on either spatial or\ntemporal features, leading to suboptimal representations. To this end, we\npropose EEG-VJEPA, a novel adaptation of the Video Joint Embedding Predictive\nArchitecture (V-JEPA) for EEG classification. By treating EEG as video-like\nsequences, EEG-VJEPA learns semantically meaningful spatiotemporal\nrepresentations using joint embeddings and adaptive masking. To our knowledge,\nthis is the first work that exploits V-JEPA for EEG classification and explores\nthe visual concepts learned by the model. Evaluations on the publicly available\nTemple University Hospital (TUH) Abnormal EEG dataset show that EEG-VJEPA\noutperforms existing state-of-the-art models in classification accuracy. Beyond\nclassification accuracy, EEG-VJEPA captures physiologically relevant spatial\nand temporal signal patterns, offering interpretable embeddings that may\nsupport human-AI collaboration in diagnostic workflows. These findings position\nEEG-VJEPA as a promising framework for scalable, trustworthy EEG analysis in\nreal-world clinical settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.03633v4", "cate": "cs.CV", "date": "2025-07-04", "updated": "2025-07-11"}
{"id": "2507.03504", "title": "Information-Bottleneck Driven Binary Neural Network for Change Detection", "authors": ["Kaijie Yin", "Zhiyuan Zhang", "Shu Kong", "Tian Gao", "Chengzhong Xu", "Hui Kong"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025 Accepted", "url": "http://arxiv.org/abs/2507.03504v2", "summary": "In this paper, we propose Binarized Change Detection (BiCD), the first binary\nneural network (BNN) designed specifically for change detection. Conventional\nnetwork binarization approaches, which directly quantize both weights and\nactivations in change detection models, severely limit the network's ability to\nrepresent input data and distinguish between changed and unchanged regions.\nThis results in significantly lower detection accuracy compared to real-valued\nnetworks. To overcome these challenges, BiCD enhances both the representational\npower and feature separability of BNNs, improving detection performance.\nSpecifically, we introduce an auxiliary objective based on the Information\nBottleneck (IB) principle, guiding the encoder to retain essential input\ninformation while promoting better feature discrimination. Since directly\ncomputing mutual information under the IB principle is intractable, we design a\ncompact, learnable auxiliary module as an approximation target, leading to a\nsimple yet effective optimization strategy that minimizes both reconstruction\nloss and standard change detection loss. Extensive experiments on street-view\nand remote sensing datasets demonstrate that BiCD establishes a new benchmark\nfor BNN-based change detection, achieving state-of-the-art performance in this\ndomain.", "comment": "ICCV 2025 Accepted", "pdf_url": "http://arxiv.org/pdf/2507.03504v2", "cate": "cs.CV", "date": "2025-07-04", "updated": "2025-07-14"}
{"id": "2507.04219", "title": "Model Collapse Is Not a Bug but a Feature in Machine Unlearning for LLMs", "authors": ["Yan Scholten", "Sophie Xhonneux", "Leo Schwinn", "Stephan Günnemann"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.04219v2", "summary": "Current unlearning methods for LLMs optimize on the private information they\nseek to remove by incorporating it into their training objectives. We argue\nthis not only risks reinforcing exposure to sensitive data, it also\nfundamentally contradicts the principle of minimizing its use. As a remedy, we\npropose a novel unlearning method - Partial Model Collapse (PMC), which does\nnot require unlearning targets in the unlearning objective. Our approach is\ninspired by recent observations that training generative models on their own\ngenerations leads to distribution collapse, effectively removing information\nfrom the model. Our core idea is to leverage this collapse for unlearning by\ntriggering collapse partially on the sensitive data. We theoretically analyze\nthat our approach converges to the desired outcome, i.e. the LLM unlearns the\ninformation in the forget set. We empirically demonstrate that PMC overcomes\ntwo key limitations of existing unlearning approaches that explicitly optimize\non unlearning targets, and more effectively removes private information from\nmodel outputs. Overall, our contributions represent an important step toward\nmore comprehensive unlearning that aligns with real-world privacy constraints.\nCode available at https://www.cs.cit.tum.de/daml/partial-model-collapse/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.04219v2", "cate": "cs.LG", "date": "2025-07-06", "updated": "2025-07-11"}
{"id": "2507.03924", "title": "DNF-Intrinsic: Deterministic Noise-Free Diffusion for Indoor Inverse Rendering", "authors": ["Rongjia Zheng", "Qing Zhang", "Chengjiang Long", "Wei-Shi Zheng"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV2025", "url": "http://arxiv.org/abs/2507.03924v2", "summary": "Recent methods have shown that pre-trained diffusion models can be fine-tuned\nto enable generative inverse rendering by learning image-conditioned\nnoise-to-intrinsic mapping. Despite their remarkable progress, they struggle to\nrobustly produce high-quality results as the noise-to-intrinsic paradigm\nessentially utilizes noisy images with deteriorated structure and appearance\nfor intrinsic prediction, while it is common knowledge that structure and\nappearance information in an image are crucial for inverse rendering. To\naddress this issue, we present DNF-Intrinsic, a robust yet efficient inverse\nrendering approach fine-tuned from a pre-trained diffusion model, where we\npropose to take the source image rather than Gaussian noise as input to\ndirectly predict deterministic intrinsic properties via flow matching.\nMoreover, we design a generative renderer to constrain that the predicted\nintrinsic properties are physically faithful to the source image. Experiments\non both synthetic and real-world datasets show that our method clearly\noutperforms existing state-of-the-art methods.", "comment": "Accepted to ICCV2025", "pdf_url": "http://arxiv.org/pdf/2507.03924v2", "cate": "cs.CV", "date": "2025-07-05", "updated": "2025-07-14"}
{"id": "2507.04225", "title": "Zero-Shot Cyclic Peptide Design via Composable Geometric Constraints", "authors": ["Dapeng Jiang", "Xiangzhe Kong", "Jiaqi Han", "Mingyu Li", "Rui Jiao", "Wenbing Huang", "Stefano Ermon", "Jianzhu Ma", "Yang Liu"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.04225v2", "summary": "Cyclic peptides, characterized by geometric constraints absent in linear\npeptides, offer enhanced biochemical properties, presenting new opportunities\nto address unmet medical needs. However, designing target-specific cyclic\npeptides remains underexplored due to limited training data. To bridge the gap,\nwe propose CP-Composer, a novel generative framework that enables zero-shot\ncyclic peptide generation via composable geometric constraints. Our approach\ndecomposes complex cyclization patterns into unit constraints, which are\nincorporated into a diffusion model through geometric conditioning on nodes and\nedges. During training, the model learns from unit constraints and their random\ncombinations in linear peptides, while at inference, novel constraint\ncombinations required for cyclization are imposed as input. Experiments show\nthat our model, despite trained with linear peptides, is capable of generating\ndiverse target-binding cyclic peptides, reaching success rates from 38% to 84%\non different cyclization strategies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.04225v2", "cate": "cs.LG", "date": "2025-07-06", "updated": "2025-07-14"}
{"id": "2507.04681", "title": "Colorectal Cancer Tumor Grade Segmentation in Digital Histopathology Images: From Giga to Mini Challenge", "authors": ["Alper Bahcekapili", "Duygu Arslan", "Umut Ozdemir", "Berkay Ozkirli", "Emre Akbas", "Ahmet Acar", "Gozde B. Akar", "Bingdou He", "Shuoyu Xu", "Umit Mert Caglar", "Alptekin Temizel", "Guillaume Picaud", "Marc Chaumont", "Gérard Subsol", "Luc Téot", "Fahad Alsharekh", "Shahad Alghannam", "Hexiang Mao", "Wenhua Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted Grand Challenge Paper ICIP 2025", "url": "http://arxiv.org/abs/2507.04681v2", "summary": "Colorectal cancer (CRC) is the third most diagnosed cancer and the second\nleading cause of cancer-related death worldwide. Accurate histopathological\ngrading of CRC is essential for prognosis and treatment planning but remains a\nsubjective process prone to observer variability and limited by global\nshortages of trained pathologists. To promote automated and standardized\nsolutions, we organized the ICIP Grand Challenge on Colorectal Cancer Tumor\nGrading and Segmentation using the publicly available METU CCTGS dataset. The\ndataset comprises 103 whole-slide images with expert pixel-level annotations\nfor five tissue classes. Participants submitted segmentation masks via Codalab,\nevaluated using metrics such as macro F-score and mIoU. Among 39 participating\nteams, six outperformed the Swin Transformer baseline (62.92 F-score). This\npaper presents an overview of the challenge, dataset, and the top-performing\nmethods", "comment": "Accepted Grand Challenge Paper ICIP 2025", "pdf_url": "http://arxiv.org/pdf/2507.04681v2", "cate": "cs.CV", "date": "2025-07-07", "updated": "2025-07-12"}
{"id": "2507.04607", "title": "PRIME: Large Language Model Personalization with Cognitive Memory and Thought Processes", "authors": ["Xinliang Frederick Zhang", "Nick Beauchamp", "Lu Wang"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.04607v2", "summary": "Large language model (LLM) personalization aims to align model outputs with\nindividuals' unique preferences and opinions. While recent efforts have\nimplemented various personalization methods, a unified theoretical framework\nthat can systematically understand the drivers of effective personalization is\nstill lacking. In this work, we integrate the well-established cognitive\ndual-memory model into LLM personalization, by mirroring episodic memory to\nhistorical user engagements and semantic memory to long-term, evolving user\nbeliefs. Specifically, we systematically investigate memory instantiations and\nintroduce a unified framework, PRIME, using episodic and semantic memory\nmechanisms. We further augment PRIME with a novel personalized thinking\ncapability inspired by the slow thinking strategy. Moreover, recognizing the\nabsence of suitable benchmarks, we introduce a dataset using Change My View\n(CMV) from Reddit, specifically designed to evaluate long-context\npersonalization. Extensive experiments validate PRIME's effectiveness across\nboth long- and short-context scenarios. Further analysis confirms that PRIME\neffectively captures dynamic personalization beyond mere popularity biases.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.04607v2", "cate": "cs.CL", "date": "2025-07-07", "updated": "2025-07-14"}
{"id": "2507.04692", "title": "Structure-Guided Diffusion Models for High-Fidelity Portrait Shadow Removal", "authors": ["Wanchang Yu", "Qing Zhang", "Rongjia Zheng", "Wei-Shi Zheng"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV2025", "url": "http://arxiv.org/abs/2507.04692v2", "summary": "We present a diffusion-based portrait shadow removal approach that can\nrobustly produce high-fidelity results. Unlike previous methods, we cast shadow\nremoval as diffusion-based inpainting. To this end, we first train a\nshadow-independent structure extraction network on a real-world portrait\ndataset with various synthetic lighting conditions, which allows to generate a\nshadow-independent structure map including facial details while excluding the\nunwanted shadow boundaries. The structure map is then used as condition to\ntrain a structure-guided inpainting diffusion model for removing shadows in a\ngenerative manner. Finally, to restore the fine-scale details (e.g., eyelashes,\nmoles and spots) that may not be captured by the structure map, we take the\ngradients inside the shadow regions as guidance and train a detail restoration\ndiffusion model to refine the shadow removal result. Extensive experiments on\nthe benchmark datasets show that our method clearly outperforms existing\nmethods, and is effective to avoid previously common issues such as facial\nidentity tampering, shadow residual, color distortion, structure blurring, and\nloss of details. Our code is available at\nhttps://github.com/wanchang-yu/Structure-Guided-Diffusion-for-Portrait-Shadow-Removal.", "comment": "Accepted by ICCV2025", "pdf_url": "http://arxiv.org/pdf/2507.04692v2", "cate": "cs.CV", "date": "2025-07-07", "updated": "2025-07-14"}
{"id": "2507.06269", "title": "BayesSDF: Surface-Based Laplacian Uncertainty Estimation for 3D Geometry with Neural Signed Distance Fields", "authors": ["Rushil Desai"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025 Workshops (8 Pages, 6 Figures, 2 Tables)", "url": "http://arxiv.org/abs/2507.06269v2", "summary": "Quantifying uncertainty in neural implicit 3D representations, particularly\nthose utilizing Signed Distance Functions (SDFs), remains a substantial\nchallenge due to computational inefficiencies, scalability issues, and\ngeometric inconsistencies. Existing methods typically neglect direct geometric\nintegration, leading to poorly calibrated uncertainty maps. We introduce\nBayesSDF, a novel probabilistic framework for uncertainty quantification in\nneural implicit SDF models, motivated by scientific simulation applications\nwith 3D environments (e.g., forests) such as modeling fluid flow through\nforests, where precise surface geometry and reliable uncertainty estimates are\nessential. Unlike radiance-based models such as Neural Radiance Fields (NeRF)\nor 3D Gaussian splatting, which lack explicit surface formulations, Signed\nDistance Functions (SDFs) define continuous and differentiable geometry, making\nthem better suited for physical modeling and analysis. BayesSDF leverages a\nLaplace approximation to quantify local surface instability using Hessian-based\nmetrics, enabling efficient, surfaceaware uncertainty estimation. Our method\nshows that uncertainty predictions correspond closely with poorly reconstructed\ngeometry, providing actionable confidence measures for downstream use.\nExtensive evaluations on synthetic and real-world datasets demonstrate that\nBayesSDF outperforms existing methods in both calibration and geometric\nconsistency, establishing a strong foundation for uncertainty-aware 3D scene\nreconstruction, simulation, and robotic decision-making.", "comment": "ICCV 2025 Workshops (8 Pages, 6 Figures, 2 Tables)", "pdf_url": "http://arxiv.org/pdf/2507.06269v2", "cate": "cs.CV", "date": "2025-07-08", "updated": "2025-07-14"}
{"id": "2507.04839", "title": "RIPE: Reinforcement Learning on Unlabeled Image Pairs for Robust Keypoint Extraction", "authors": ["Johannes Künzel", "Anna Hilsmann", "Peter Eisert"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025", "url": "http://arxiv.org/abs/2507.04839v2", "summary": "We introduce RIPE, an innovative reinforcement learning-based framework for\nweakly-supervised training of a keypoint extractor that excels in both\ndetection and description tasks. In contrast to conventional training regimes\nthat depend heavily on artificial transformations, pre-generated models, or 3D\ndata, RIPE requires only a binary label indicating whether paired images\nrepresent the same scene. This minimal supervision significantly expands the\npool of training data, enabling the creation of a highly generalized and robust\nkeypoint extractor.\n  RIPE utilizes the encoder's intermediate layers for the description of the\nkeypoints with a hyper-column approach to integrate information from different\nscales. Additionally, we propose an auxiliary loss to enhance the\ndiscriminative capability of the learned descriptors.\n  Comprehensive evaluations on standard benchmarks demonstrate that RIPE\nsimplifies data preparation while achieving competitive performance compared to\nstate-of-the-art techniques, marking a significant advancement in robust\nkeypoint extraction and description. To support further research, we have made\nour code publicly available at https://github.com/fraunhoferhhi/RIPE.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.04839v2", "cate": "cs.CV", "date": "2025-07-07", "updated": "2025-07-14"}
{"id": "2507.06272", "title": "LIRA: Inferring Segmentation in Large Multi-modal Models with Local Interleaved Region Assistance", "authors": ["Zhang Li", "Biao Yang", "Qiang Liu", "Shuo Zhang", "Zhiyin Ma", "Shuo Zhang", "Liang Yin", "Linger Deng", "Yabo Sun", "Yuliang Liu", "Xiang Bai"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025", "url": "http://arxiv.org/abs/2507.06272v2", "summary": "While large multi-modal models (LMMs) demonstrate promising capabilities in\nsegmentation and comprehension, they still struggle with two limitations:\ninaccurate segmentation and hallucinated comprehension. These challenges stem\nprimarily from constraints in weak visual comprehension and a lack of\nfine-grained perception. To alleviate these limitations, we propose LIRA, a\nframework that capitalizes on the complementary relationship between visual\ncomprehension and segmentation via two key components: (1) Semantic-Enhanced\nFeature Extractor (SEFE) improves object attribute inference by fusing semantic\nand pixel-level features, leading to more accurate segmentation; (2)\nInterleaved Local Visual Coupling (ILVC) autoregressively generates local\ndescriptions after extracting local features based on segmentation masks,\noffering fine-grained supervision to mitigate hallucinations. Furthermore, we\nfind that the precision of object segmentation is positively correlated with\nthe latent related semantics of the <seg> token. To quantify this relationship\nand the model's potential semantic inferring ability, we introduce the\nAttributes Evaluation (AttrEval) dataset. Our experiments show that LIRA\nachieves state-of-the-art performance in both segmentation and comprehension\ntasks. Code will be available at https://github.com/echo840/LIRA.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.06272v2", "cate": "cs.CV", "date": "2025-07-08", "updated": "2025-07-14"}
{"id": "2507.05948", "title": "Beyond Appearance: Geometric Cues for Robust Video Instance Segmentation", "authors": ["Quanzhu Niu", "Yikang Zhou", "Shihao Chen", "Tao Zhang", "Shunping Ji"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025 Workshop LSVOS", "url": "http://arxiv.org/abs/2507.05948v2", "summary": "Video Instance Segmentation (VIS) fundamentally struggles with pervasive\nchallenges including object occlusions, motion blur, and appearance variations\nduring temporal association. To overcome these limitations, this work\nintroduces geometric awareness to enhance VIS robustness by strategically\nleveraging monocular depth estimation. We systematically investigate three\ndistinct integration paradigms. Expanding Depth Channel (EDC) method\nconcatenates the depth map as input channel to segmentation networks; Sharing\nViT (SV) designs a uniform ViT backbone, shared between depth estimation and\nsegmentation branches; Depth Supervision (DS) makes use of depth prediction as\nan auxiliary training guide for feature learning. Though DS exhibits limited\neffectiveness, benchmark evaluations demonstrate that EDC and SV significantly\nenhance the robustness of VIS. When with Swin-L backbone, our EDC method gets\n56.2 AP, which sets a new state-of-the-art result on OVIS benchmark. This work\nconclusively establishes depth cues as critical enablers for robust video\nunderstanding.", "comment": "Accepted by ICCV 2025 Workshop LSVOS", "pdf_url": "http://arxiv.org/pdf/2507.05948v2", "cate": "cs.CV", "date": "2025-07-08", "updated": "2025-07-14"}
{"id": "2507.06812", "title": "Democratizing High-Fidelity Co-Speech Gesture Video Generation", "authors": ["Xu Yang", "Shaoli Huang", "Shenbo Xie", "Xuelin Chen", "Yifei Liu", "Changxing Ding"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025", "url": "http://arxiv.org/abs/2507.06812v2", "summary": "Co-speech gesture video generation aims to synthesize realistic,\naudio-aligned videos of speakers, complete with synchronized facial expressions\nand body gestures. This task presents challenges due to the significant\none-to-many mapping between audio and visual content, further complicated by\nthe scarcity of large-scale public datasets and high computational demands. We\npropose a lightweight framework that utilizes 2D full-body skeletons as an\nefficient auxiliary condition to bridge audio signals with visual outputs. Our\napproach introduces a diffusion model conditioned on fine-grained audio\nsegments and a skeleton extracted from the speaker's reference image,\npredicting skeletal motions through skeleton-audio feature fusion to ensure\nstrict audio coordination and body shape consistency. The generated skeletons\nare then fed into an off-the-shelf human video generation model with the\nspeaker's reference image to synthesize high-fidelity videos. To democratize\nresearch, we present CSG-405-the first public dataset with 405 hours of\nhigh-resolution videos across 71 speech types, annotated with 2D skeletons and\ndiverse speaker demographics. Experiments show that our method exceeds\nstate-of-the-art approaches in visual quality and synchronization while\ngeneralizing across speakers and contexts. Code, models, and CSG-405 are\npublicly released at https://mpi-lab.github.io/Democratizing-CSG/", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.06812v2", "cate": "cs.CV", "date": "2025-07-09", "updated": "2025-07-14"}
{"id": "2507.06071", "title": "MEDTalk: Multimodal Controlled 3D Facial Animation with Dynamic Emotions by Disentangled Embedding", "authors": ["Chang Liu", "Ye Pan", "Chenyang Ding", "Susanto Rahardja", "Xiaokang Yang"], "categories": ["cs.CV", "cs.MM"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      11 pages, 8 figures", "url": "http://arxiv.org/abs/2507.06071v2", "summary": "Audio-driven emotional 3D facial animation aims to generate synchronized lip\nmovements and vivid facial expressions. However, most existing approaches focus\non static and predefined emotion labels, limiting their diversity and\nnaturalness. To address these challenges, we propose MEDTalk, a novel framework\nfor fine-grained and dynamic emotional talking head generation. Our approach\nfirst disentangles content and emotion embedding spaces from motion sequences\nusing a carefully designed cross-reconstruction process, enabling independent\ncontrol over lip movements and facial expressions. Beyond conventional\naudio-driven lip synchronization, we integrate audio and speech text,\npredicting frame-wise intensity variations and dynamically adjusting static\nemotion features to generate realistic emotional expressions. Furthermore, to\nenhance control and personalization, we incorporate multimodal inputs-including\ntext descriptions and reference expression images-to guide the generation of\nuser-specified facial expressions. With MetaHuman as the priority, our\ngenerated results can be conveniently integrated into the industrial production\npipeline.", "comment": "11 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.06071v2", "cate": "cs.CV", "date": "2025-07-08", "updated": "2025-07-13"}
{"id": "2507.07024", "title": "FlexOlmo: Open Language Models for Flexible Data Use", "authors": ["Weijia Shi", "Akshita Bhagia", "Kevin Farhat", "Niklas Muennighoff", "Pete Walsh", "Jacob Morrison", "Dustin Schwenk", "Shayne Longpre", "Jake Poznanski", "Allyson Ettinger", "Daogao Liu", "Margaret Li", "Dirk Groeneveld", "Mike Lewis", "Wen-tau Yih", "Luca Soldaini", "Kyle Lo", "Noah A. Smith", "Luke Zettlemoyer", "Pang Wei Koh", "Hannaneh Hajishirzi", "Ali Farhadi", "Sewon Min"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07024v2", "summary": "We introduce FlexOlmo, a new class of language models (LMs) that supports (1)\ndistributed training without data sharing, where different model parameters are\nindependently trained on closed datasets, and (2) data-flexible inference,\nwhere these parameters along with their associated data can be flexibly\nincluded or excluded from model inferences with no further training. FlexOlmo\nemploys a mixture-of-experts (MoE) architecture where each expert is trained\nindependently on closed datasets and later integrated through a new\ndomain-informed routing without any joint training. FlexOlmo is trained on\nFlexMix, a corpus we curate comprising publicly available datasets alongside\nseven domain-specific sets, representing realistic approximations of closed\nsets. We evaluate models with up to 37 billion parameters (20 billion active)\non 31 diverse downstream tasks. We show that a general expert trained on public\ndata can be effectively combined with independently trained experts from other\ndata owners, leading to an average 41% relative improvement while allowing\nusers to opt out of certain data based on data licensing or permission\nrequirements. Our approach also outperforms prior model merging methods by\n10.1% on average and surpasses the standard MoE trained without data\nrestrictions using the same training FLOPs. Altogether, this research presents\na solution for both data owners and researchers in regulated industries with\nsensitive or protected data. FlexOlmo enables benefiting from closed data while\nrespecting data owners' preferences by keeping their data local and supporting\nfine-grained control of data access during inference.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07024v2", "cate": "cs.CL", "date": "2025-07-09", "updated": "2025-07-11"}
{"id": "2507.07776", "title": "SCOOTER: A Human Evaluation Framework for Unrestricted Adversarial Examples", "authors": ["Dren Fazlija", "Monty-Maximilian Zühlke", "Johanna Schrader", "Arkadij Orlov", "Clara Stein", "Iyiola E. Olatunji", "Daniel Kudenko"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      42 pages, 16 figures, 11 tables, Under Review, Code: this https URL , Data: this https URL", "url": "http://arxiv.org/abs/2507.07776v2", "summary": "Unrestricted adversarial attacks aim to fool computer vision models without\nbeing constrained by $\\ell_p$-norm bounds to remain imperceptible to humans,\nfor example, by changing an object's color. This allows attackers to circumvent\ntraditional, norm-bounded defense strategies such as adversarial training or\ncertified defense strategies. However, due to their unrestricted nature, there\nare also no guarantees of norm-based imperceptibility, necessitating human\nevaluations to verify just how authentic these adversarial examples look. While\nsome related work assesses this vital quality of adversarial attacks, none\nprovide statistically significant insights. This issue necessitates a unified\nframework that supports and streamlines such an assessment for evaluating and\ncomparing unrestricted attacks. To close this gap, we introduce SCOOTER - an\nopen-source, statistically powered framework for evaluating unrestricted\nadversarial examples. Our contributions are: $(i)$ best-practice guidelines for\ncrowd-study power, compensation, and Likert equivalence bounds to measure\nimperceptibility; $(ii)$ the first large-scale human vs. model comparison\nacross 346 human participants showing that three color-space attacks and three\ndiffusion-based attacks fail to produce imperceptible images. Furthermore, we\nfound that GPT-4o can serve as a preliminary test for imperceptibility, but it\nonly consistently detects adversarial examples for four out of six tested\nattacks; $(iii)$ open-source software tools, including a browser-based task\ntemplate to collect annotations and analysis scripts in Python and R; $(iv)$ an\nImageNet-derived benchmark dataset containing 3K real images, 7K adversarial\nexamples, and over 34K human ratings. Our findings demonstrate that automated\nvision systems do not align with human perception, reinforcing the need for a\nground-truth SCOOTER benchmark.", "comment": "42 pages, 16 figures, 11 tables, Under Review, Code:\n  https://github.com/DrenFazlija/Scooter, Data:\n  https://doi.org/10.5281/zenodo.15771501", "pdf_url": "http://arxiv.org/pdf/2507.07776v2", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-14"}
{"id": "2507.07186", "title": "Planted in Pretraining, Swayed by Finetuning: A Case Study on the Origins of Cognitive Biases in LLMs", "authors": ["Itay Itzhak", "Yonatan Belinkov", "Gabriel Stanovsky"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      CoLM 2025", "url": "http://arxiv.org/abs/2507.07186v2", "summary": "Large language models (LLMs) exhibit cognitive biases -- systematic\ntendencies of irrational decision-making, similar to those seen in humans.\nPrior work has found that these biases vary across models and can be amplified\nby instruction tuning. However, it remains unclear if these differences in\nbiases stem from pretraining, finetuning, or even random noise due to training\nstochasticity. We propose a two-step causal experimental approach to\ndisentangle these factors. First, we finetune models multiple times using\ndifferent random seeds to study how training randomness affects over $30$\ncognitive biases. Second, we introduce \\emph{cross-tuning} -- swapping\ninstruction datasets between models to isolate bias sources. This swap uses\ndatasets that led to different bias patterns, directly testing whether biases\nare dataset-dependent. Our findings reveal that while training randomness\nintroduces some variability, biases are mainly shaped by pretraining: models\nwith the same pretrained backbone exhibit more similar bias patterns than those\nsharing only finetuning data. These insights suggest that understanding biases\nin finetuned models requires considering their pretraining origins beyond\nfinetuning effects. This perspective can guide future efforts to develop\nprincipled strategies for evaluating and mitigating bias in LLMs.", "comment": "CoLM 2025", "pdf_url": "http://arxiv.org/pdf/2507.07186v2", "cate": "cs.CL", "date": "2025-07-09", "updated": "2025-07-12"}
{"id": "2507.07997", "title": "MGVQ: Could VQ-VAE Beat VAE? A Generalizable Tokenizer with Multi-group Quantization", "authors": ["Mingkai Jia", "Wei Yin", "Xiaotao Hu", "Jiaxin Guo", "Xiaoyang Guo", "Qian Zhang", "Xiao-Xiao Long", "Ping Tan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07997v2", "summary": "Vector Quantized Variational Autoencoders (VQ-VAEs) are fundamental models\nthat compress continuous visual data into discrete tokens. Existing methods\nhave tried to improve the quantization strategy for better reconstruction\nquality, however, there still exists a large gap between VQ-VAEs and VAEs. To\nnarrow this gap, we propose MGVQ, a novel method to augment the representation\ncapability of discrete codebooks, facilitating easier optimization for\ncodebooks and minimizing information loss, thereby enhancing reconstruction\nquality. Specifically, we propose to retain the latent dimension to preserve\nencoded features and incorporate a set of sub-codebooks for quantization.\nFurthermore, we construct comprehensive zero-shot benchmarks featuring\nresolutions of 512p and 2k to evaluate the reconstruction performance of\nexisting methods rigorously. MGVQ achieves the state-of-the-art performance on\nboth ImageNet and 8 zero-shot benchmarks across all VQ-VAEs. Notably, compared\nwith SD-VAE, we outperform them on ImageNet significantly, with rFID 0.49 v.s.\n0.91, and achieve superior PSNR on all zero-shot benchmarks. These results\nhighlight the superiority of MGVQ in reconstruction and pave the way for\npreserving fidelity in HD image processing tasks. Code will be publicly\navailable at https://github.com/MKJia/MGVQ.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07997v2", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-14"}
{"id": "2507.07192", "title": "Bridging the Last Mile of Prediction: Enhancing Time Series Forecasting with Conditional Guided Flow Matching", "authors": ["Huibo Xu", "Runlong Yu", "Likang Wu", "Xianquan Wang", "Qi Liu"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07192v2", "summary": "Diffusion models, a type of generative model, have shown promise in time\nseries forecasting. But they face limitations like rigid source distributions\nand limited sampling paths, which hinder their performance. Flow matching\noffers faster generation, higher-quality outputs, and greater flexibility,\nwhile also possessing the ability to utilize valuable information from the\nprediction errors of prior models, which were previously inaccessible yet\ncritically important. To address these challenges and fully unlock the untapped\npotential of flow matching, we propose Conditional Guided Flow Matching (CGFM).\nCGFM extends flow matching by incorporating the outputs of an auxiliary model,\nenabling a previously unattainable capability in the field: learning from the\nerrors of the auxiliary model. For time series forecasting tasks, it integrates\nhistorical data as conditions and guidance, constructs two-sided conditional\nprobability paths, and uses a general affine path to expand the space of\nprobability paths, ultimately leading to improved predictions. Extensive\nexperiments show that CGFM consistently enhances and outperforms\nstate-of-the-art models, highlighting its effectiveness in advancing\nforecasting methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07192v2", "cate": "cs.LG", "date": "2025-07-09", "updated": "2025-07-14"}
{"id": "2507.08307", "title": "M2DAO-Talker: Harmonizing Multi-granular Motion Decoupling and Alternating Optimization for Talking-head Generation", "authors": ["Kui Jiang", "Shiyu Liu", "Junjun Jiang", "Xin Yang", "Hongxun Yao", "Xiaopeng Fan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08307v2", "summary": "Audio-driven talking head generation holds significant potential for film\nproduction. While existing 3D methods have advanced motion modeling and content\nsynthesis, they often produce rendering artifacts, such as motion blur,\ntemporal jitter, and local penetration, due to limitations in representing\nstable, fine-grained motion fields. Through systematic analysis, we reformulate\ntalking head generation into a unified framework comprising three steps: video\npreprocessing, motion representation, and rendering reconstruction. This\nframework underpins our proposed M2DAO-Talker, which addresses current\nlimitations via multi-granular motion decoupling and alternating optimization.\nSpecifically, we devise a novel 2D portrait preprocessing pipeline to extract\nframe-wise deformation control conditions (motion region segmentation masks,\nand camera parameters) to facilitate motion representation. To ameliorate\nmotion modeling, we elaborate a multi-granular motion decoupling strategy,\nwhich independently models non-rigid (oral and facial) and rigid (head) motions\nfor improved reconstruction accuracy. Meanwhile, a motion consistency\nconstraint is developed to ensure head-torso kinematic consistency, thereby\nmitigating penetration artifacts caused by motion aliasing. In addition, an\nalternating optimization strategy is designed to iteratively refine facial and\noral motion parameters, enabling more realistic video generation. Experiments\nacross multiple datasets show that M2DAO-Talker achieves state-of-the-art\nperformance, with the 2.43 dB PSNR improvement in generation quality and 0.64\ngain in user-evaluated video realness versus TalkingGaussian while with 150 FPS\ninference speed. Our project homepage is\nhttps://m2dao-talker.github.io/M2DAO-Talk.github.io.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08307v2", "cate": "cs.CV", "date": "2025-07-11", "updated": "2025-07-14"}
{"id": "2507.07586", "title": "Your Absorbing Discrete Diffusion Secretly Models the Bayesian Posterior", "authors": ["Cooper Doyle"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      12 pages, 2 figures, 2 tables", "url": "http://arxiv.org/abs/2507.07586v2", "summary": "Discrete diffusion language models learn to reconstruct text from randomly\nmasked inputs, yet under mild assumptions their denoiser already implements the\nexact Bayesian posterior over the original tokens. We prove that the expected\ndenoiser output under the forward corruption distribution recovers the true\nposterior, and that a simple Monte Carlo estimator converges to this posterior\nat rate O(1/sqrt(K)) with finite-sample concentration bounds. Building on this\ninsight, we introduce an inference-time ensemble that runs K independent\ndenoising passes and aggregates both posterior means and variances without any\nextra training. On WikiText-2, our MC-marginal sampler recovers the analytic\nlambda-DCE zero-shot perplexity (approximately 39) to within a few points at\nK=128, and its per-token variance shows a strong rank correlation with\nreconstruction error (Spearman rho = 0.996). This cost-proportional procedure\nyields calibrated uncertainty estimates and a direct trade-off between compute\nand posterior fidelity in discrete diffusion LMs.", "comment": "12 pages, 2 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2507.07586v2", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-13"}
{"id": "2507.08776", "title": "CLiFT: Compressive Light-Field Tokens for Compute-Efficient and Adaptive Neural Rendering", "authors": ["Zhengqing Wang", "Yuefan Wu", "Jiacheng Chen", "Fuyang Zhang", "Yasutaka Furukawa"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project page: this https URL", "url": "http://arxiv.org/abs/2507.08776v2", "summary": "This paper proposes a neural rendering approach that represents a scene as\n\"compressed light-field tokens (CLiFTs)\", retaining rich appearance and\ngeometric information of a scene. CLiFT enables compute-efficient rendering by\ncompressed tokens, while being capable of changing the number of tokens to\nrepresent a scene or render a novel view with one trained network. Concretely,\ngiven a set of images, multi-view encoder tokenizes the images with the camera\nposes. Latent-space K-means selects a reduced set of rays as cluster centroids\nusing the tokens. The multi-view ``condenser'' compresses the information of\nall the tokens into the centroid tokens to construct CLiFTs. At test time,\ngiven a target view and a compute budget (i.e., the number of CLiFTs), the\nsystem collects the specified number of nearby tokens and synthesizes a novel\nview using a compute-adaptive renderer. Extensive experiments on RealEstate10K\nand DL3DV datasets quantitatively and qualitatively validate our approach,\nachieving significant data reduction with comparable rendering quality and the\nhighest overall rendering score, while providing trade-offs of data size,\nrendering quality, and rendering speed.", "comment": "Project page: https://clift-nvs.github.io", "pdf_url": "http://arxiv.org/pdf/2507.08776v2", "cate": "cs.CV", "date": "2025-07-11", "updated": "2025-07-14"}
{"id": "2507.07947", "title": "Low Resource Reconstruction Attacks Through Benign Prompts", "authors": ["Sol Yarkoni", "Roi Livni"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07947v2", "summary": "The recent advances in generative models such as diffusion models have raised\nseveral risks and concerns related to privacy, copyright infringements and data\nstewardship. To better understand and control the risks, various researchers\nhave created techniques, experiments and attacks that reconstruct images, or\npart of images, from the training set. While these techniques already establish\nthat data from the training set can be reconstructed, they often rely on\nhigh-resources, excess to the training set as well as well-engineered and\ndesigned prompts.\n  In this work, we devise a new attack that requires low resources, assumes\nlittle to no access to the actual training set, and identifies, seemingly,\nbenign prompts that lead to potentially-risky image reconstruction. This\nhighlights the risk that images might even be reconstructed by an uninformed\nuser and unintentionally. For example, we identified that, with regard to one\nexisting model, the prompt ``blue Unisex T-Shirt'' can generate the face of a\nreal-life human model. Our method builds on an intuition from previous works\nwhich leverages domain knowledge and identifies a fundamental vulnerability\nthat stems from the use of scraped data from e-commerce platforms, where\ntemplated layouts and images are tied to pattern-like prompts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07947v2", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-14"}
{"id": "2310.02718", "title": "Understanding Pan-Sharpening via Generalized Inverse", "authors": ["Shiqi Liu", "Yutong Bai", "Xinyang Han", "Alan Yuille"], "categories": ["cs.LG", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2310.02718v2", "summary": "Pan-sharpening algorithm utilizes panchromatic image and multispectral image\nto obtain a high spatial and high spectral image. However, the optimizations of\nthe algorithms are designed with different standards. We adopt the simple\nmatrix equation to describe the Pan-sharpening problem. The solution existence\ncondition and the acquirement of spectral and spatial resolution are discussed.\nA down-sampling enhancement method was introduced for better acquiring the\nspatial and spectral down-sample matrices. By the generalized inverse theory,\nwe derived two forms of general inverse matrix formulations that can correspond\nto the two prominent classes of Pan-sharpening methods, that is, component\nsubstitution and multi-resolution analysis methods. Specifically, the Gram\nSchmidt Adaptive(GSA) was proved to follow the general inverse matrix\nformulation of component substitution. A model prior to the general inverse\nmatrix of the spectral function was rendered. The theoretical errors are\nanalyzed. Synthetic experiments and real data experiments are implemented. The\nproposed methods are better and sharper than other methods qualitatively in\nboth synthetic and real experiments. The down-sample enhancement effect is\nshown of better results both quantitatively and qualitatively in real\nexperiments. The generalized inverse matrix theory help us better understand\nthe Pan-sharpening.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2310.02718v2", "cate": "cs.LG", "date": "2023-10-04", "updated": "2025-07-12"}
{"id": "2507.07998", "title": "PyVision: Agentic Vision with Dynamic Tooling", "authors": ["Shitian Zhao", "Haoquan Zhang", "Shaoheng Lin", "Ming Li", "Qilong Wu", "Kaipeng Zhang", "Chen Wei"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      26 Pages, 10 Figures, Technical report", "url": "http://arxiv.org/abs/2507.07998v2", "summary": "LLMs are increasingly deployed as agents, systems capable of planning,\nreasoning, and dynamically calling external tools. However, in visual\nreasoning, prior approaches largely remain limited by predefined workflows and\nstatic toolsets. In this report, we present PyVision, an interactive,\nmulti-turn framework that enables MLLMs to autonomously generate, execute, and\nrefine Python-based tools tailored to the task at hand, unlocking flexible and\ninterpretable problem-solving. We develop a taxonomy of the tools created by\nPyVision and analyze their usage across a diverse set of benchmarks.\nQuantitatively, PyVision achieves consistent performance gains, boosting\nGPT-4.1 by +7.8% on V* and Claude-4.0-Sonnet by +31.1% on VLMsAreBlind-mini.\nThese results point to a broader shift: dynamic tooling allows models not just\nto use tools, but to invent them, advancing toward more agentic visual\nreasoning.", "comment": "26 Pages, 10 Figures, Technical report", "pdf_url": "http://arxiv.org/pdf/2507.07998v2", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-14"}
{"id": "2312.05348", "title": "High-Quality Live Video Streaming via Transcoding Time Prediction and Preset Selection", "authors": ["Zahra Nabizadeh Shahre-Babak", "Nader Karimi", "Krishna Rapaka", "Tarek Amara", "Shadrokh Samavi", "Shahram Shirani"], "categories": ["cs.MM", "cs.CV"], "primary_category": "Subjects:       Multimedia (cs.MM)", "pdf_link": null, "comments": "Comments:      After further review, we found major flaws in the paper that need extensive revision", "url": "http://arxiv.org/abs/2312.05348v2", "summary": "Video streaming often requires transcoding content into different resolutions\nand bitrates to match the recipient's internet speed and screen capabilities.\nVideo encoders like x264 offer various presets, each with different tradeoffs\nbetween transcoding time and rate-distortion performance. Choosing the best\npreset for video transcoding is difficult, especially for live streaming, as\ntrying all the presets and choosing the best one is not feasible. One solution\nis to predict each preset's transcoding time and select the preset that ensures\nthe highest quality while adhering to live streaming time constraints.\nPrediction of video transcoding time is also critical in minimizing streaming\ndelays, deploying resource management algorithms, and load balancing. We\npropose a learning-based framework for predicting the transcoding time of\nvideos across various presets. Our predictor's features for video transcoding\ntime prediction are derived directly from the ingested stream, primarily from\nthe header or metadata. As a result, only minimal additional delay is incurred\nfor feature extraction, rendering our approach ideal for live-streaming\napplications. We evaluated our learning-based transcoding time prediction using\na dataset of videos. The results demonstrate that our framework can accurately\npredict the transcoding time for different presets, with a mean absolute\npercentage error (MAPE) of nearly 5.0%. Leveraging these predictions, we then\nselect the most suitable transcoding preset for live video streaming. Utilizing\nour transcoding time prediction-based preset selection improved Peak\nSignal-to-Noise Ratio (PSNR) of up to 5 dB.", "comment": "After further review, we found major flaws in the paper that need\n  extensive revision", "pdf_url": "http://arxiv.org/pdf/2312.05348v2", "cate": "cs.MM", "date": "2023-12-08", "updated": "2025-07-13"}
{"id": "2507.08017", "title": "Mechanistic Indicators of Understanding in Large Language Models", "authors": ["Pierre Beckmann", "Matthieu Queloz"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      32 pages", "url": "http://arxiv.org/abs/2507.08017v2", "summary": "Recent findings in mechanistic interpretability (MI), the field probing the\ninner workings of Large Language Models (LLMs), challenge the view that these\nmodels rely solely on superficial statistics. We offer an accessible synthesis\nof these findings that doubles as an introduction to MI while integrating these\nfindings within a novel theoretical framework for thinking about machine\nunderstanding. We argue that LLMs develop internal structures that are\nfunctionally analogous to the kind of understanding that consists in seeing\nconnections. To sharpen this idea, we propose a three-tiered conception of\nunderstanding. First, conceptual understanding emerges when a model forms\n\"features\" as directions in latent space, learning the connections between\ndiverse manifestations of something. Second, state-of-the-world understanding\nemerges when a model learns contingent factual connections between features and\ndynamically tracks changes in the world. Third, principled understanding\nemerges when a model ceases to rely on a collection of memorized facts and\ndiscovers a \"circuit\" connecting these facts. However, these forms of\nunderstanding remain radically different from human understanding, as the\nphenomenon of \"parallel mechanisms\" shows. We conclude that the debate should\nmove beyond the yes-or-no question of whether LLMs understand to investigate\nhow their strange minds work and forge conceptions that fit them.", "comment": "32 pages", "pdf_url": "http://arxiv.org/pdf/2507.08017v2", "cate": "cs.CL", "date": "2025-07-07", "updated": "2025-07-14"}
{"id": "2409.05137", "title": "READoc: A Unified Benchmark for Realistic Document Structured Extraction", "authors": ["Zichao Li", "Aizier Abulaiti", "Yaojie Lu", "Xuanang Chen", "Jia Zheng", "Hongyu Lin", "Xianpei Han", "Le Sun"], "categories": ["cs.CL", "cs.CV"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      ACL 2025 Findings", "url": "http://arxiv.org/abs/2409.05137v3", "summary": "Document Structured Extraction (DSE) aims to extract structured content from\nraw documents. Despite the emergence of numerous DSE systems, their unified\nevaluation remains inadequate, significantly hindering the field's advancement.\nThis problem is largely attributed to existing benchmark paradigms, which\nexhibit fragmented and localized characteristics. To address these limitations\nand offer a thorough evaluation of DSE systems, we introduce a novel benchmark\nnamed READoc, which defines DSE as a realistic task of converting unstructured\nPDFs into semantically rich Markdown. The READoc dataset is derived from 3,576\ndiverse and real-world documents from arXiv, GitHub, and Zenodo. In addition,\nwe develop a DSE Evaluation S$^3$uite comprising Standardization, Segmentation\nand Scoring modules, to conduct a unified evaluation of state-of-the-art DSE\napproaches. By evaluating a range of pipeline tools, expert visual models, and\ngeneral VLMs, we identify the gap between current work and the unified,\nrealistic DSE objective for the first time. We aspire that READoc will catalyze\nfuture research in DSE, fostering more comprehensive and practical solutions.", "comment": "ACL 2025 Findings", "pdf_url": "http://arxiv.org/pdf/2409.05137v3", "cate": "cs.CL", "date": "2024-09-08", "updated": "2025-07-13"}
{"id": "2412.00947", "title": "VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information", "authors": ["Ryo Kamoi", "Yusen Zhang", "Sarkar Snigdha Sarathi Das", "Ranran Haoran Zhang", "Rui Zhang"], "categories": ["cs.CL", "cs.CV"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      COLM 2025. VisOnlyQA dataset, code, and model responses are provided at this https URL . Please also refer to our project website at this https URL", "url": "http://arxiv.org/abs/2412.00947v3", "summary": "Large Vision Language Models (LVLMs) have achieved remarkable performance in\nvarious vision-language tasks. However, it is still unclear how accurately\nLVLMs can perceive visual information in images. In particular, the capability\nof LVLMs to perceive geometric information, such as shape, angle, and size,\nremains insufficiently analyzed, although the perception of these properties is\ncrucial for tasks that require a detailed visual understanding. In this work,\nwe introduce VisOnlyQA, a dataset for evaluating the geometric perception of\nLVLMs, and reveal that LVLMs often cannot accurately perceive basic geometric\ninformation in images, while human performance is nearly perfect. VisOnlyQA\nconsists of 12 tasks that directly ask about geometric information in geometric\nshapes, charts, chemical structures, and 3D shapes. Our experiments highlight\nthe following findings: (i) State-of-the-art LVLMs struggle with basic\ngeometric perception. 23 LVLMs we evaluate, including GPT-4o and Gemini 2.5\nPro, work poorly on VisOnlyQA. (ii) Additional training data does not resolve\nthis issue. Fine-tuning on the training set of VisOnlyQA is not always\neffective, even for in-distribution tasks. (iii) LLM may be the bottleneck.\nLVLMs using stronger LLMs exhibit better geometric perception on VisOnlyQA,\nwhile it does not require complex reasoning, suggesting that the way LVLMs\nprocess information from visual encoders is a bottleneck. The datasets, code,\nand model responses are provided at https://github.com/psunlpgroup/VisOnlyQA.", "comment": "COLM 2025. VisOnlyQA dataset, code, and model responses are provided\n  at https://github.com/psunlpgroup/VisOnlyQA. Please also refer to our project\n  website at https://visonlyqa.github.io/", "pdf_url": "http://arxiv.org/pdf/2412.00947v3", "cate": "cs.CL", "date": "2024-12-01", "updated": "2025-07-13"}
{"id": "2501.06848", "title": "A General Framework for Inference-time Scaling and Steering of Diffusion Models", "authors": ["Raghav Singhal", "Zachary Horvitz", "Ryan Teehan", "Mengye Ren", "Zhou Yu", "Kathleen McKeown", "Rajesh Ranganath"], "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.06848v4", "summary": "Diffusion models produce impressive results in modalities ranging from images\nand video to protein design and text. However, generating samples with\nuser-specified properties remains a challenge. Recent research proposes\nfine-tuning models to maximize rewards that capture desired properties, but\nthese methods require expensive training and are prone to mode collapse. In\nthis work, we present Feynman-Kac (FK) steering, an inference-time framework\nfor steering diffusion models with reward functions. FK steering works by\nsampling a system of multiple interacting diffusion processes, called\nparticles, and resampling particles at intermediate steps based on scores\ncomputed using functions called potentials. Potentials are defined using\nrewards for intermediate states and are selected such that a high value\nindicates that the particle will yield a high-reward sample. We explore various\nchoices of potentials, intermediate rewards, and samplers. We evaluate FK\nsteering on text-to-image and text diffusion models. For steering text-to-image\nmodels with a human preference reward, we find that FK steering a 0.8B\nparameter model outperforms a 2.6B parameter fine-tuned model on prompt\nfidelity, with faster sampling and no training. For steering text diffusion\nmodels with rewards for text quality and specific text attributes, we find that\nFK steering generates lower perplexity, more linguistically acceptable outputs\nand enables gradient-free control of attributes like toxicity. Our results\ndemonstrate that inference-time scaling and steering of diffusion models - even\nwith off-the-shelf rewards - can provide significant sample quality gains and\ncontrollability benefits. Code is available at\nhttps://github.com/zacharyhorvitz/Fk-Diffusion-Steering .", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.06848v4", "cate": "cs.LG", "date": "2025-01-12", "updated": "2025-07-14"}
{"id": "2503.14836", "title": "On the Robustness Tradeoff in Fine-Tuning", "authors": ["Kunyang Li", "Jean-Charles Noirot Ferrand", "Ryan Sheatsley", "Blaine Hoak", "Yohan Beugin", "Eric Pauley", "Patrick McDaniel"], "categories": ["cs.LG", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted to International Conference on Computer Vision, ICCV 2025", "url": "http://arxiv.org/abs/2503.14836v2", "summary": "Fine-tuning has become the standard practice for adapting pre-trained models\nto downstream tasks. However, the impact on model robustness is not well\nunderstood. In this work, we characterize the robustness-accuracy trade-off in\nfine-tuning. We evaluate the robustness and accuracy of fine-tuned models over\n6 benchmark datasets and 7 different fine-tuning strategies. We observe a\nconsistent trade-off between adversarial robustness and accuracy. Peripheral\nupdates such as BitFit are more effective for simple tasks -- over 75% above\nthe average measured by the area under the Pareto frontiers on CIFAR-10 and\nCIFAR-100. In contrast, fine-tuning information-heavy layers, such as attention\nlayers via Compacter, achieves a better Pareto frontier on more complex tasks\n-- 57.5% and 34.6% above the average on Caltech-256 and CUB-200, respectively.\nLastly, we observe that the robustness of fine-tuning against\nout-of-distribution data closely tracks accuracy. These insights emphasize the\nneed for robustness-aware fine-tuning to ensure reliable real-world\ndeployments.", "comment": "Accepted to International Conference on Computer Vision, ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2503.14836v2", "cate": "cs.LG", "date": "2025-03-19", "updated": "2025-07-14"}
{"id": "2507.05823", "title": "Fair Domain Generalization: An Information-Theoretic View", "authors": ["Tangzheng Lian", "Guanyu Hu", "Dimitrios Kollias", "Xinyu Yang", "Oya Celiktutan"], "categories": ["cs.LG", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.05823v2", "summary": "Domain generalization (DG) and algorithmic fairness are two critical\nchallenges in machine learning. However, most DG methods focus only on\nminimizing expected risk in the unseen target domain without considering\nalgorithmic fairness. Conversely, fairness methods typically do not account for\ndomain shifts, so the fairness achieved during training may not generalize to\nunseen test domains. In this work, we bridge these gaps by studying the problem\nof Fair Domain Generalization (FairDG), which aims to minimize both expected\nrisk and fairness violations in unseen target domains. We derive novel mutual\ninformation-based upper bounds for expected risk and fairness violations in\nmulti-class classification tasks with multi-group sensitive attributes. These\nbounds provide key insights for algorithm design from an information-theoretic\nperspective. Guided by these insights, we introduce PAFDG (Pareto-Optimal\nFairness for Domain Generalization), a practical framework that solves the\nFairDG problem and models the utility-fairness trade-off through Pareto\noptimization. Experiments on real-world vision and language datasets show that\nPAFDG achieves superior utility-fairness trade-offs compared to existing\nmethods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.05823v2", "cate": "cs.LG", "date": "2025-07-08", "updated": "2025-07-13"}
{"id": "2507.08036", "title": "Barriers in Integrating Medical Visual Question Answering into Radiology Workflows: A Scoping Review and Clinicians' Insights", "authors": ["Deepali Mishra", "Chaklam Silpasuwanchai", "Ashutosh Modi", "Madhumita Sushil", "Sorayouth Chumnanvej"], "categories": ["cs.CL", "cs.CV"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      29 pages, 5 figures (1 in supplementary), 3 tables (1 in main text, 2 in supplementary). Scoping review and clinician survey", "url": "http://arxiv.org/abs/2507.08036v2", "summary": "Medical Visual Question Answering (MedVQA) is a promising tool to assist\nradiologists by automating medical image interpretation through question\nanswering. Despite advances in models and datasets, MedVQA's integration into\nclinical workflows remains limited. This study systematically reviews 68\npublications (2018-2024) and surveys 50 clinicians from India and Thailand to\nexamine MedVQA's practical utility, challenges, and gaps. Following the Arksey\nand O'Malley scoping review framework, we used a two-pronged approach: (1)\nreviewing studies to identify key concepts, advancements, and research gaps in\nradiology workflows, and (2) surveying clinicians to capture their perspectives\non MedVQA's clinical relevance. Our review reveals that nearly 60% of QA pairs\nare non-diagnostic and lack clinical relevance. Most datasets and models do not\nsupport multi-view, multi-resolution imaging, EHR integration, or domain\nknowledge, features essential for clinical diagnosis. Furthermore, there is a\nclear mismatch between current evaluation metrics and clinical needs. The\nclinician survey confirms this disconnect: only 29.8% consider MedVQA systems\nhighly useful. Key concerns include the absence of patient history or domain\nknowledge (87.2%), preference for manually curated datasets (51.1%), and the\nneed for multi-view image support (78.7%). Additionally, 66% favor models\nfocused on specific anatomical regions, and 89.4% prefer dialogue-based\ninteractive systems. While MedVQA shows strong potential, challenges such as\nlimited multimodal analysis, lack of patient context, and misaligned evaluation\napproaches must be addressed for effective clinical integration.", "comment": "29 pages, 5 figures (1 in supplementary), 3 tables (1 in main text, 2\n  in supplementary). Scoping review and clinician survey", "pdf_url": "http://arxiv.org/pdf/2507.08036v2", "cate": "cs.CL", "date": "2025-07-09", "updated": "2025-07-14"}
