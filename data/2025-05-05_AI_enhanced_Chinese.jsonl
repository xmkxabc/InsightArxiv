{"id": "2505.00734", "pdf": "https://arxiv.org/pdf/2505.00734", "abs": "https://arxiv.org/abs/2505.00734", "authors": ["Neil Joshi", "Joshua Carney", "Nathanael Kuo", "Homer Li", "Cheng Peng", "Myron Brown"], "title": "Unconstrained Large-scale 3D Reconstruction and Rendering across Altitudes", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Production of photorealistic, navigable 3D site models requires a large\nvolume of carefully collected images that are often unavailable to first\nresponders for disaster relief or law enforcement. Real-world challenges\ninclude limited numbers of images, heterogeneous unposed cameras, inconsistent\nlighting, and extreme viewpoint differences for images collected from varying\naltitudes. To promote research aimed at addressing these challenges, we have\ndeveloped the first public benchmark dataset for 3D reconstruction and novel\nview synthesis based on multiple calibrated ground-level, security-level, and\nairborne cameras. We present datasets that pose real-world challenges,\nindependently evaluate calibration of unposed cameras and quality of novel\nrendered views, demonstrate baseline performance using recent state-of-practice\nmethods, and identify challenges for further research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u516c\u5f00\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u89e3\u51b33D\u91cd\u5efa\u548c\u65b0\u89c6\u89d2\u5408\u6210\u4e2d\u7684\u5b9e\u9645\u6311\u6218\uff0c\u5982\u56fe\u50cf\u6570\u91cf\u6709\u9650\u3001\u76f8\u673a\u672a\u6807\u5b9a\u3001\u5149\u7167\u4e0d\u4e00\u81f4\u548c\u6781\u7aef\u89c6\u89d2\u5dee\u5f02\u3002", "motivation": "\u4e3a\u707e\u5bb3\u6551\u63f4\u6216\u6267\u6cd5\u7b49\u573a\u666f\u63d0\u4f9b\u903c\u771f\u3001\u53ef\u5bfc\u822a\u76843D\u573a\u666f\u6a21\u578b\uff0c\u4f46\u73b0\u6709\u56fe\u50cf\u6570\u636e\u5f80\u5f80\u4e0d\u8db3\u4e14\u8d28\u91cf\u53c2\u5dee\u4e0d\u9f50\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u591a\u6807\u5b9a\u76f8\u673a\u7684\u516c\u5171\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u62ec\u5730\u9762\u3001\u5b89\u9632\u548c\u7a7a\u4e2d\u89c6\u89d2\uff0c\u5e76\u8bc4\u4f30\u4e86\u672a\u6807\u5b9a\u76f8\u673a\u7684\u6821\u51c6\u548c\u65b0\u89c6\u89d2\u6e32\u67d3\u8d28\u91cf\u3002", "result": "\u5c55\u793a\u4e86\u5f53\u524d\u5148\u8fdb\u65b9\u6cd5\u7684\u57fa\u7ebf\u6027\u80fd\uff0c\u5e76\u6307\u51fa\u4e86\u8fdb\u4e00\u6b65\u7814\u7a76\u7684\u6311\u6218\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a3D\u91cd\u5efa\u548c\u65b0\u89c6\u89d2\u5408\u6210\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u9645\u6311\u6218\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u63a8\u52a8\u4e86\u76f8\u5173\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.01319", "pdf": "https://arxiv.org/pdf/2505.01319", "abs": "https://arxiv.org/abs/2505.01319", "authors": ["Yifang Pan", "Karan Singh", "Luiz Gustavo Hafemann"], "title": "Model See Model Do: Speech-Driven Facial Animation with Style Control", "categories": ["cs.GR", "cs.LG", "I.3.7; I.3.8"], "comment": "10 pages, 7 figures, SIGGRAPH Conference Papers '25", "summary": "Speech-driven 3D facial animation plays a key role in applications such as\nvirtual avatars, gaming, and digital content creation. While existing methods\nhave made significant progress in achieving accurate lip synchronization and\ngenerating basic emotional expressions, they often struggle to capture and\neffectively transfer nuanced performance styles. We propose a novel\nexample-based generation framework that conditions a latent diffusion model on\na reference style clip to produce highly expressive and temporally coherent\nfacial animations. To address the challenge of accurately adhering to the style\nreference, we introduce a novel conditioning mechanism called style basis,\nwhich extracts key poses from the reference and additively guides the diffusion\ngeneration process to fit the style without compromising lip synchronization\nquality. This approach enables the model to capture subtle stylistic cues while\nensuring that the generated animations align closely with the input speech.\nExtensive qualitative, quantitative, and perceptual evaluations demonstrate the\neffectiveness of our method in faithfully reproducing the desired style while\nachieving superior lip synchronization across various speech scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u793a\u4f8b\u7684\u751f\u6210\u6846\u67b6\uff0c\u5229\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\u548c\u98ce\u683c\u53c2\u8003\u7247\u6bb5\u751f\u6210\u9ad8\u8868\u73b0\u529b\u4e14\u65f6\u95f4\u8fde\u8d2f\u76843D\u9762\u90e8\u52a8\u753b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u548c\u8f6c\u79fb\u7ec6\u5fae\u8868\u6f14\u98ce\u683c\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5b9e\u73b0\u51c6\u786e\u7684\u5507\u540c\u6b65\u548c\u57fa\u672c\u60c5\u611f\u8868\u8fbe\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u96be\u4ee5\u6355\u6349\u548c\u8f6c\u79fb\u7ec6\u5fae\u7684\u8868\u6f14\u98ce\u683c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u793a\u4f8b\u7684\u751f\u6210\u6846\u67b6\uff0c\u5229\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5f15\u5165\u98ce\u683c\u57fa\u5e95\u7684\u673a\u5236\u4ece\u53c2\u8003\u7247\u6bb5\u4e2d\u63d0\u53d6\u5173\u952e\u59ff\u52bf\uff0c\u4ee5\u6307\u5bfc\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6355\u6349\u7ec6\u5fae\u7684\u98ce\u683c\u7ebf\u7d22\uff0c\u540c\u65f6\u786e\u4fdd\u751f\u6210\u7684\u52a8\u753b\u4e0e\u8f93\u5165\u8bed\u97f3\u7d27\u5bc6\u5bf9\u9f50\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u5507\u540c\u6b65\u6548\u679c\u3002", "conclusion": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u5fe0\u5b9e\u518d\u73b0\u76ee\u6807\u98ce\u683c\u548c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u5507\u540c\u6b65\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2505.00739", "pdf": "https://arxiv.org/pdf/2505.00739", "abs": "https://arxiv.org/abs/2505.00739", "authors": ["Qiushi Yang", "Yuan Yao", "Miaomiao Cui", "Liefeng Bo"], "title": "MoSAM: Motion-Guided Segment Anything Model with Spatial-Temporal Memory Selection", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "The recent Segment Anything Model 2 (SAM2) has demonstrated exceptional\ncapabilities in interactive object segmentation for both images and videos.\nHowever, as a foundational model on interactive segmentation, SAM2 performs\nsegmentation directly based on mask memory from the past six frames, leading to\ntwo significant challenges. Firstly, during inference in videos, objects may\ndisappear since SAM2 relies solely on memory without accounting for object\nmotion information, which limits its long-range object tracking capabilities.\nSecondly, its memory is constructed from fixed past frames, making it\nsusceptible to challenges associated with object disappearance or occlusion,\ndue to potentially inaccurate segmentation results in memory. To address these\nproblems, we present MoSAM, incorporating two key strategies to integrate\nobject motion cues into the model and establish more reliable feature memory.\nFirstly, we propose Motion-Guided Prompting (MGP), which represents the object\nmotion in both sparse and dense manners, then injects them into SAM2 through a\nset of motion-guided prompts. MGP enables the model to adjust its focus towards\nthe direction of motion, thereby enhancing the object tracking capabilities.\nFurthermore, acknowledging that past segmentation results may be inaccurate, we\ndevise a Spatial-Temporal Memory Selection (ST-MS) mechanism that dynamically\nidentifies frames likely to contain accurate segmentation in both pixel- and\nframe-level. By eliminating potentially inaccurate mask predictions from\nmemory, we can leverage more reliable memory features to exploit similar\nregions for improving segmentation results. Extensive experiments on various\nbenchmarks of video object segmentation and video instance segmentation\ndemonstrate that our MoSAM achieves state-of-the-art results compared to other\ncompetitors.", "AI": {"tldr": "MoSAM\u901a\u8fc7\u5f15\u5165\u8fd0\u52a8\u5f15\u5bfc\u63d0\u793a\u548c\u52a8\u6001\u65f6\u7a7a\u8bb0\u5fc6\u9009\u62e9\u673a\u5236\uff0c\u89e3\u51b3\u4e86SAM2\u5728\u89c6\u9891\u5206\u5272\u4e2d\u4f9d\u8d56\u56fa\u5b9a\u5e27\u8bb0\u5fc6\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5bf9\u8c61\u8ddf\u8e2a\u548c\u5206\u5272\u7cbe\u5ea6\u3002", "motivation": "SAM2\u4ec5\u4f9d\u8d56\u8fc7\u53bb\u516d\u5e27\u7684\u56fa\u5b9a\u8bb0\u5fc6\u8fdb\u884c\u5206\u5272\uff0c\u5bfc\u81f4\u5bf9\u8c61\u6d88\u5931\u6216\u906e\u6321\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u7f3a\u4e4f\u8fd0\u52a8\u4fe1\u606f\u652f\u6301\u957f\u7a0b\u8ddf\u8e2a\u3002", "method": "\u63d0\u51faMotion-Guided Prompting (MGP)\u6ce8\u5165\u8fd0\u52a8\u4fe1\u606f\uff0c\u5e76\u8bbe\u8ba1Spatial-Temporal Memory Selection (ST-MS)\u52a8\u6001\u7b5b\u9009\u53ef\u9760\u8bb0\u5fc6\u5e27\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u548c\u5b9e\u4f8b\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMoSAM\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "MoSAM\u901a\u8fc7\u7ed3\u5408\u8fd0\u52a8\u4fe1\u606f\u548c\u52a8\u6001\u8bb0\u5fc6\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u5206\u5272\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2505.01425", "pdf": "https://arxiv.org/pdf/2505.01425", "abs": "https://arxiv.org/abs/2505.01425", "authors": ["Jiefeng Li", "Jinkun Cao", "Haotian Zhang", "Davis Rempe", "Jan Kautz", "Umar Iqbal", "Ye Yuan"], "title": "GENMO: A GENeralist Model for Human MOtion", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG", "cs.RO"], "comment": "Project page: https://research.nvidia.com/labs/dair/genmo/", "summary": "Human motion modeling traditionally separates motion generation and\nestimation into distinct tasks with specialized models. Motion generation\nmodels focus on creating diverse, realistic motions from inputs like text,\naudio, or keyframes, while motion estimation models aim to reconstruct accurate\nmotion trajectories from observations like videos. Despite sharing underlying\nrepresentations of temporal dynamics and kinematics, this separation limits\nknowledge transfer between tasks and requires maintaining separate models. We\npresent GENMO, a unified Generalist Model for Human Motion that bridges motion\nestimation and generation in a single framework. Our key insight is to\nreformulate motion estimation as constrained motion generation, where the\noutput motion must precisely satisfy observed conditioning signals. Leveraging\nthe synergy between regression and diffusion, GENMO achieves accurate global\nmotion estimation while enabling diverse motion generation. We also introduce\nan estimation-guided training objective that exploits in-the-wild videos with\n2D annotations and text descriptions to enhance generative diversity.\nFurthermore, our novel architecture handles variable-length motions and mixed\nmultimodal conditions (text, audio, video) at different time intervals,\noffering flexible control. This unified approach creates synergistic benefits:\ngenerative priors improve estimated motions under challenging conditions like\nocclusions, while diverse video data enhances generation capabilities.\nExtensive experiments demonstrate GENMO's effectiveness as a generalist\nframework that successfully handles multiple human motion tasks within a single\nmodel.", "AI": {"tldr": "GENMO\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u4eba\u7c7b\u8fd0\u52a8\u901a\u7528\u6a21\u578b\uff0c\u5c06\u8fd0\u52a8\u751f\u6210\u548c\u4f30\u8ba1\u7ed3\u5408\u5728\u4e00\u4e2a\u6846\u67b6\u4e2d\uff0c\u901a\u8fc7\u7ea6\u675f\u751f\u6210\u548c\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u591a\u6837\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c06\u8fd0\u52a8\u751f\u6210\u548c\u4f30\u8ba1\u5206\u5f00\uff0c\u9650\u5236\u4e86\u77e5\u8bc6\u5171\u4eab\u548c\u6a21\u578b\u6548\u7387\uff0cGENMO\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "GENMO\u5c06\u8fd0\u52a8\u4f30\u8ba1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u7ea6\u675f\u751f\u6210\uff0c\u7ed3\u5408\u56de\u5f52\u548c\u6269\u6563\u6a21\u578b\uff0c\u5e76\u5229\u7528\u89c6\u9891\u548c\u6587\u672c\u6570\u636e\u589e\u5f3a\u591a\u6837\u6027\u3002", "result": "GENMO\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u5904\u7406\u590d\u6742\u6761\u4ef6\u548c\u591a\u6a21\u6001\u8f93\u5165\u3002", "conclusion": "GENMO\u8bc1\u660e\u4e86\u7edf\u4e00\u6846\u67b6\u5728\u8fd0\u52a8\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u751f\u6210\u548c\u4f30\u8ba1\u7684\u534f\u540c\u63d0\u5347\u3002"}}
{"id": "2505.00740", "pdf": "https://arxiv.org/pdf/2505.00740", "abs": "https://arxiv.org/abs/2505.00740", "authors": ["Zhengbin Zhang", "Yan Wu", "Hongkun Zhang"], "title": "Fast2comm:Collaborative perception combined with prior knowledge", "categories": ["cs.CV", "cs.MA"], "comment": "8pages,8figures", "summary": "Collaborative perception has the potential to significantly enhance\nperceptual accuracy through the sharing of complementary information among\nagents. However, real-world collaborative perception faces persistent\nchallenges, particularly in balancing perception performance and bandwidth\nlimitations, as well as coping with localization errors. To address these\nchallenges, we propose Fast2comm, a prior knowledge-based collaborative\nperception framework. Specifically, (1)we propose a prior-supervised confidence\nfeature generation method, that effectively distinguishes foreground from\nbackground by producing highly discriminative confidence features; (2)we\npropose GT Bounding Box-based spatial prior feature selection strategy to\nensure that only the most informative prior-knowledge features are selected and\nshared, thereby minimizing background noise and optimizing bandwidth efficiency\nwhile enhancing adaptability to localization inaccuracies; (3)we decouple the\nfeature fusion strategies between model training and testing phases, enabling\ndynamic bandwidth adaptation. To comprehensively validate our framework, we\nconduct extensive experiments on both real-world and simulated datasets. The\nresults demonstrate the superior performance of our model and highlight the\nnecessity of the proposed methods. Our code is available at\nhttps://github.com/Zhangzhengbin-TJ/Fast2comm.", "AI": {"tldr": "Fast2comm\u662f\u4e00\u4e2a\u57fa\u4e8e\u5148\u9a8c\u77e5\u8bc6\u7684\u534f\u4f5c\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u9ad8\u533a\u5206\u5ea6\u7684\u7f6e\u4fe1\u7279\u5f81\u548c\u4f18\u5316\u5e26\u5bbd\u6548\u7387\uff0c\u89e3\u51b3\u4e86\u534f\u4f5c\u611f\u77e5\u4e2d\u7684\u6027\u80fd\u4e0e\u5e26\u5bbd\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u534f\u4f5c\u611f\u77e5\u901a\u8fc7\u5171\u4eab\u4fe1\u606f\u63d0\u5347\u51c6\u786e\u6027\uff0c\u4f46\u9762\u4e34\u6027\u80fd\u4e0e\u5e26\u5bbd\u5e73\u8861\u4ee5\u53ca\u5b9a\u4f4d\u8bef\u5dee\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u5148\u9a8c\u76d1\u7763\u7684\u7f6e\u4fe1\u7279\u5f81\u751f\u6210\u65b9\u6cd5\u3001\u57fa\u4e8eGT Bounding Box\u7684\u7a7a\u95f4\u5148\u9a8c\u7279\u5f81\u9009\u62e9\u7b56\u7565\uff0c\u5e76\u89e3\u8026\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u9636\u6bb5\u7684\u7279\u5f81\u878d\u5408\u7b56\u7565\u3002", "result": "\u5728\u771f\u5b9e\u548c\u6a21\u62df\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u578b\u6027\u80fd\u4f18\u8d8a\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "Fast2comm\u6709\u6548\u89e3\u51b3\u4e86\u534f\u4f5c\u611f\u77e5\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u611f\u77e5\u51c6\u786e\u6027\u548c\u5e26\u5bbd\u6548\u7387\u3002"}}
{"id": "2505.01001", "pdf": "https://arxiv.org/pdf/2505.01001", "abs": "https://arxiv.org/abs/2505.01001", "authors": ["Tessa De La Fuente"], "title": "Photoshop Batch Rendering Using Actions for Stylistic Video Editing", "categories": ["cs.MM", "cs.GR", "cs.HC"], "comment": "11 pages, 12 figures", "summary": "My project looks at an efficient workflow for creative image/video editing\nusing Adobe Photoshop Actions tool and Batch Processing System. This innovative\napproach to video editing through Photoshop creates a fundamental shift to\ncreative workflow management through the integration of industry-leading image\nmanipulation with video editing techniques. Through systematic automation of\nActions, users can achieve a simple and consistent application of visual edits\nacross a string of images. This approach provides an alternative method to\noptimize productivity while ensuring uniform results across image collections\nthrough a post-processing pipeline.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAdobe Photoshop Actions\u5de5\u5177\u548c\u6279\u5904\u7406\u7cfb\u7edf\u7684\u9ad8\u6548\u521b\u610f\u56fe\u50cf/\u89c6\u9891\u7f16\u8f91\u5de5\u4f5c\u6d41\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u5b9e\u73b0\u4e00\u81f4\u6027\u548c\u751f\u4ea7\u529b\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u89c6\u9891\u7f16\u8f91\u5de5\u4f5c\u6d41\u6548\u7387\u8f83\u4f4e\uff0c\u7f3a\u4e4f\u4e00\u81f4\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u521b\u65b0\u65b9\u6cd5\u6765\u4f18\u5316\u521b\u610f\u5de5\u4f5c\u6d41\u7ba1\u7406\u3002", "method": "\u5229\u7528Photoshop Actions\u5de5\u5177\u548c\u6279\u5904\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u81ea\u52a8\u5316\u6280\u672f\u5b9e\u73b0\u89c6\u89c9\u7f16\u8f91\u7684\u4e00\u81f4\u5e94\u7528\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u7b80\u5316\u7f16\u8f91\u6d41\u7a0b\uff0c\u786e\u4fdd\u56fe\u50cf\u96c6\u5408\u7684\u7edf\u4e00\u7ed3\u679c\uff0c\u5e76\u63d0\u9ad8\u751f\u4ea7\u529b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u6d41\u4e3a\u521b\u610f\u56fe\u50cf/\u89c6\u9891\u7f16\u8f91\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u4e00\u81f4\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002"}}
{"id": "2505.00741", "pdf": "https://arxiv.org/pdf/2505.00741", "abs": "https://arxiv.org/abs/2505.00741", "authors": ["Srinivas Kanakala", "Sneha Ningappa"], "title": "Detection and Classification of Diseases in Multi-Crop Leaves using LSTM and CNN Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Plant diseases pose a serious challenge to agriculture by reducing crop yield\nand affecting food quality. Early detection and classification of these\ndiseases are essential for minimising losses and improving crop management\npractices. This study applies Convolutional Neural Networks (CNN) and Long\nShort-Term Memory (LSTM) models to classify plant leaf diseases using a dataset\ncontaining 70,295 training images and 17,572 validation images across 38\ndisease classes. The CNN model was trained using the Adam optimiser with a\nlearning rate of 0.0001 and categorical cross-entropy as the loss function.\nAfter 10 training epochs, the model achieved a training accuracy of 99.1% and a\nvalidation accuracy of 96.4%. The LSTM model reached a validation accuracy of\n93.43%. Performance was evaluated using precision, recall, F1-score, and\nconfusion matrix, confirming the reliability of the CNN-based approach. The\nresults suggest that deep learning models, particularly CNN, enable an\neffective solution for accurate and scalable plant disease classification,\nsupporting practical applications in agricultural monitoring.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528CNN\u548cLSTM\u6a21\u578b\u5bf9\u690d\u7269\u53f6\u7247\u75be\u75c5\u8fdb\u884c\u5206\u7c7b\uff0cCNN\u6a21\u578b\u8868\u73b0\u4f18\u4e8eLSTM\uff0c\u9a8c\u8bc1\u51c6\u786e\u7387\u8fbe96.4%\uff0c\u8868\u660e\u6df1\u5ea6\u5b66\u4e60\u5728\u519c\u4e1a\u76d1\u6d4b\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002", "motivation": "\u690d\u7269\u75c5\u5bb3\u4e25\u91cd\u5f71\u54cd\u519c\u4e1a\u4ea7\u91cf\u548c\u98df\u54c1\u8d28\u91cf\uff0c\u65e9\u671f\u68c0\u6d4b\u548c\u5206\u7c7b\u5bf9\u51cf\u5c11\u635f\u5931\u548c\u4f18\u5316\u4f5c\u7269\u7ba1\u7406\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528CNN\u548cLSTM\u6a21\u578b\uff0c\u57fa\u4e8e\u5305\u542b70,295\u5f20\u8bad\u7ec3\u56fe\u50cf\u548c17,572\u5f20\u9a8c\u8bc1\u56fe\u50cf\u7684\u6570\u636e\u96c6\uff0c\u8bad\u7ec3CNN\u65f6\u91c7\u7528Adam\u4f18\u5316\u5668\u548c\u5206\u7c7b\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570\u3002", "result": "CNN\u6a21\u578b\u8bad\u7ec3\u51c6\u786e\u7387\u8fbe99.1%\uff0c\u9a8c\u8bc1\u51c6\u786e\u738796.4%\uff1bLSTM\u9a8c\u8bc1\u51c6\u786e\u7387\u4e3a93.43%\u3002\u6027\u80fd\u8bc4\u4f30\u6307\u6807\u8bc1\u5b9e\u4e86CNN\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5c24\u5176\u662fCNN\uff09\u4e3a\u690d\u7269\u75c5\u5bb3\u5206\u7c7b\u63d0\u4f9b\u4e86\u51c6\u786e\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u519c\u4e1a\u76d1\u6d4b\u5b9e\u8df5\u3002"}}
{"id": "2505.00725", "pdf": "https://arxiv.org/pdf/2505.00725", "abs": "https://arxiv.org/abs/2505.00725", "authors": ["Bithiah Yuan"], "title": "FinBERT-QA: Financial Question Answering with pre-trained BERT Language Models", "categories": ["cs.CL", "cs.IR", "cs.LG", "I.2.7; I.5.1; H.3.3"], "comment": "Submitted in partial fulfillment of the requirements for the Master\n  of Science degree in Computer Science at the University of Freiburg, July 31,\n  2020", "summary": "Motivated by the emerging demand in the financial industry for the automatic\nanalysis of unstructured and structured data at scale, Question Answering (QA)\nsystems can provide lucrative and competitive advantages to companies by\nfacilitating the decision making of financial advisers. Consequently, we\npropose a novel financial QA system using the transformer-based pre-trained\nBERT language model to address the limitations of data scarcity and language\nspecificity in the financial domain. Our system focuses on financial\nnon-factoid answer selection, which retrieves a set of passage-level texts and\nselects the most relevant as the answer. To increase efficiency, we formulate\nthe answer selection task as a re-ranking problem, in which our system consists\nof an Answer Retriever using BM25, a simple information retrieval approach, to\nfirst return a list of candidate answers, and an Answer Re-ranker built with\nvariants of pre-trained BERT language models to re-rank and select the most\nrelevant answers. We investigate various learning, further pre-training, and\nfine-tuning approaches for BERT. Our experiments suggest that FinBERT-QA, a\nmodel built from applying the Transfer and Adapt further fine-tuning and\npointwise learning approach, is the most effective, improving the\nstate-of-the-art results of task 2 of the FiQA dataset by 16% on MRR, 17% on\nNDCG, and 21% on Precision@1.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eBERT\u7684\u91d1\u878dQA\u7cfb\u7edf\uff0c\u7528\u4e8e\u975e\u4e8b\u5b9e\u6027\u7b54\u6848\u9009\u62e9\uff0c\u901a\u8fc7\u91cd\u6392\u5e8f\u65b9\u6cd5\u63d0\u5347\u6548\u7387\uff0c\u5e76\u5728FiQA\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u91d1\u878d\u884c\u4e1a\u5bf9\u5927\u89c4\u6a21\u975e\u7ed3\u6784\u5316\u548c\u7ed3\u6784\u5316\u6570\u636e\u7684\u81ea\u52a8\u5206\u6790\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0cQA\u7cfb\u7edf\u53ef\u4e3a\u91d1\u878d\u987e\u95ee\u7684\u51b3\u7b56\u63d0\u4f9b\u7ade\u4e89\u4f18\u52bf\u3002", "method": "\u7cfb\u7edf\u7ed3\u5408BM25\u68c0\u7d22\u548cBERT\u91cd\u6392\u5e8f\uff0c\u7814\u7a76\u4e86\u591a\u79cdBERT\u7684\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u65b9\u6cd5\uff0c\u6700\u7ec8\u91c7\u7528Transfer and Adapt\u5fae\u8c03\u548c\u70b9\u5b66\u4e60\u7b56\u7565\u3002", "result": "FinBERT-QA\u6a21\u578b\u5728FiQA\u6570\u636e\u96c6\u7684\u4efb\u52a12\u4e0a\uff0cMRR\u63d0\u534716%\uff0cNDCG\u63d0\u534717%\uff0cPrecision@1\u63d0\u534721%\u3002", "conclusion": "\u63d0\u51fa\u7684FinBERT-QA\u7cfb\u7edf\u5728\u91d1\u878dQA\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2505.00742", "pdf": "https://arxiv.org/pdf/2505.00742", "abs": "https://arxiv.org/abs/2505.00742", "authors": ["Jiaxu Qian", "Chendong Wang", "Yifan Yang", "Chaoyun Zhang", "Huiqiang Jiang", "Xufang Luo", "Yu Kang", "Qingwei Lin", "Anlan Zhang", "Shiqi Jiang", "Ting Cao", "Tianjun Mao", "Suman Banerjee", "Guyue Liu", "Saravan Rajmohan", "Dongmei Zhang", "Yuqing Yang", "Qi Zhang", "Lili Qiu"], "title": "Zoomer: Adaptive Image Focus Optimization for Black-box MLLM", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "Recent advancements in multimodal large language models (MLLMs) have\nbroadened the scope of vision-language tasks, excelling in applications like\nimage captioning and interactive question-answering. However, these models\nstruggle with accurately processing visual data, particularly in tasks\nrequiring precise object recognition and fine visual details. Stringent token\nlimits often result in the omission of critical information, hampering\nperformance. To address these limitations, we introduce \\SysName, a novel\nvisual prompting mechanism designed to enhance MLLM performance while\npreserving essential visual details within token limits. \\SysName features\nthree key innovations: a prompt-aware strategy that dynamically highlights\nrelevant image regions, a spatial-preserving orchestration schema that\nmaintains object integrity, and a budget-aware prompting method that balances\nglobal context with crucial visual details. Comprehensive evaluations across\nmultiple datasets demonstrate that \\SysName consistently outperforms baseline\nmethods, achieving up to a $26.9\\%$ improvement in accuracy while significantly\nreducing token consumption.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\\SysName\u7684\u65b0\u578b\u89c6\u89c9\u63d0\u793a\u673a\u5236\uff0c\u65e8\u5728\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u7559\u5173\u952e\u89c6\u89c9\u7ec6\u8282\u3002", "motivation": "\u73b0\u6709\u7684MLLM\u5728\u7cbe\u786e\u5904\u7406\u89c6\u89c9\u6570\u636e\uff08\u5982\u7269\u4f53\u8bc6\u522b\u548c\u7ec6\u8282\u6355\u6349\uff09\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u4e25\u683c\u7684token\u9650\u5236\u5e38\u5bfc\u81f4\u5173\u952e\u4fe1\u606f\u4e22\u5931\u3002", "method": "\\SysName\u5305\u542b\u4e09\u9879\u521b\u65b0\uff1a\u52a8\u6001\u7a81\u51fa\u76f8\u5173\u56fe\u50cf\u533a\u57df\u7684\u63d0\u793a\u611f\u77e5\u7b56\u7565\u3001\u4fdd\u6301\u7269\u4f53\u5b8c\u6574\u6027\u7684\u7a7a\u95f4\u4fdd\u7559\u7f16\u6392\u6a21\u5f0f\uff0c\u4ee5\u53ca\u5e73\u8861\u5168\u5c40\u4e0a\u4e0b\u6587\u4e0e\u5173\u952e\u89c6\u89c9\u7ec6\u8282\u7684\u9884\u7b97\u611f\u77e5\u63d0\u793a\u65b9\u6cd5\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\\SysName\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe26.9%\uff0c\u540c\u65f6\u5927\u5e45\u51cf\u5c11token\u6d88\u8017\u3002", "conclusion": "\\SysName\u6709\u6548\u89e3\u51b3\u4e86MLLM\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u74f6\u9888\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.00753", "pdf": "https://arxiv.org/pdf/2505.00753", "abs": "https://arxiv.org/abs/2505.00753", "authors": ["Henry Peng Zou", "Wei-Chieh Huang", "Yaozu Wu", "Yankai Chen", "Chunyu Miao", "Hoang Nguyen", "Yue Zhou", "Weizhi Zhang", "Liancheng Fang", "Langzhou He", "Yangning Li", "Yuwei Cao", "Dongyuan Li", "Renhe Jiang", "Philip S. Yu"], "title": "A Survey on Large Language Model based Human-Agent Systems", "categories": ["cs.CL", "cs.LG"], "comment": "Paper lists and resources are available at\n  \\url{https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-System-Papers}", "summary": "Recent advances in large language models (LLMs) have sparked growing interest\nin building fully autonomous agents. However, fully autonomous LLM-based agents\nstill face significant challenges, including limited reliability due to\nhallucinations, difficulty in handling complex tasks, and substantial safety\nand ethical risks, all of which limit their feasibility and trustworthiness in\nreal-world applications. To overcome these limitations, LLM-based human-agent\nsystems (LLM-HAS) incorporate human-provided information, feedback, or control\ninto the agent system to enhance system performance, reliability and safety.\nThis paper provides the first comprehensive and structured survey of LLM-HAS.\nIt clarifies fundamental concepts, systematically presents core components\nshaping these systems, including environment & profiling, human feedback,\ninteraction types, orchestration and communication, explores emerging\napplications, and discusses unique challenges and opportunities. By\nconsolidating current knowledge and offering a structured overview, we aim to\nfoster further research and innovation in this rapidly evolving\ninterdisciplinary field. Paper lists and resources are available at\nhttps://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-System-Papers.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4eba\u673a\u534f\u4f5c\u7cfb\u7edf\uff08LLM-HAS\uff09\uff0c\u63a2\u8ba8\u4e86\u5176\u6838\u5fc3\u7ec4\u4ef6\u3001\u5e94\u7528\u53ca\u6311\u6218\uff0c\u65e8\u5728\u63a8\u52a8\u8fd9\u4e00\u8de8\u5b66\u79d1\u9886\u57df\u7684\u7814\u7a76\u3002", "motivation": "\u5b8c\u5168\u81ea\u4e3b\u7684LLM\u4ee3\u7406\u5b58\u5728\u53ef\u9760\u6027\u3001\u590d\u6742\u4efb\u52a1\u5904\u7406\u53ca\u4f26\u7406\u98ce\u9669\u7b49\u95ee\u9898\uff0cLLM-HAS\u901a\u8fc7\u5f15\u5165\u4eba\u7c7b\u53c2\u4e0e\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u4e0e\u5b89\u5168\u6027\u3002", "method": "\u7cfb\u7edf\u68b3\u7406\u4e86LLM-HAS\u7684\u57fa\u672c\u6982\u5ff5\u3001\u6838\u5fc3\u7ec4\u4ef6\uff08\u5982\u73af\u5883\u4e0e\u753b\u50cf\u3001\u4eba\u7c7b\u53cd\u9988\u3001\u4ea4\u4e92\u7c7b\u578b\u7b49\uff09\u53ca\u65b0\u5174\u5e94\u7528\u3002", "result": "\u63d0\u4f9b\u4e86LLM-HAS\u7684\u5168\u9762\u7efc\u8ff0\uff0c\u660e\u786e\u4e86\u7814\u7a76\u65b9\u5411\u4e0e\u6311\u6218\u3002", "conclusion": "LLM-HAS\u901a\u8fc7\u4eba\u673a\u534f\u4f5c\u514b\u670d\u4e86LLM\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u6846\u67b6\u3002"}}
{"id": "2505.00743", "pdf": "https://arxiv.org/pdf/2505.00743", "abs": "https://arxiv.org/abs/2505.00743", "authors": ["Yinfeng Yu", "Dongsheng Yang"], "title": "DOPE: Dual Object Perception-Enhancement Network for Vision-and-Language Navigation", "categories": ["cs.CV", "cs.RO"], "comment": "Main paper (10 pages). Accepted for publication by ICMR(International\n  Conference on Multimedia Retrieval) 2025", "summary": "Vision-and-Language Navigation (VLN) is a challenging task where an agent\nmust understand language instructions and navigate unfamiliar environments\nusing visual cues. The agent must accurately locate the target based on visual\ninformation from the environment and complete tasks through interaction with\nthe surroundings. Despite significant advancements in this field, two major\nlimitations persist: (1) Many existing methods input complete language\ninstructions directly into multi-layer Transformer networks without fully\nexploiting the detailed information within the instructions, thereby limiting\nthe agent's language understanding capabilities during task execution; (2)\nCurrent approaches often overlook the modeling of object relationships across\ndifferent modalities, failing to effectively utilize latent clues between\nobjects, which affects the accuracy and robustness of navigation decisions. We\npropose a Dual Object Perception-Enhancement Network (DOPE) to address these\nissues to improve navigation performance. First, we design a Text Semantic\nExtraction (TSE) to extract relatively essential phrases from the text and\ninput them into the Text Object Perception-Augmentation (TOPA) to fully\nleverage details such as objects and actions within the instructions. Second,\nwe introduce an Image Object Perception-Augmentation (IOPA), which performs\nadditional modeling of object information across different modalities, enabling\nthe model to more effectively utilize latent clues between objects in images\nand text, enhancing decision-making accuracy. Extensive experiments on the R2R\nand REVERIE datasets validate the efficacy of the proposed approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cdDOPE\u7f51\u7edc\uff0c\u901a\u8fc7\u589e\u5f3a\u6587\u672c\u548c\u56fe\u50cf\u4e2d\u7684\u5bf9\u8c61\u611f\u77e5\u80fd\u529b\uff0c\u89e3\u51b3\u4e86VLN\u4efb\u52a1\u4e2d\u8bed\u8a00\u7406\u89e3\u4e0d\u8db3\u548c\u8de8\u6a21\u6001\u5bf9\u8c61\u5173\u7cfb\u5efa\u6a21\u7f3a\u5931\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728VLN\u4efb\u52a1\u4e2d\u672a\u80fd\u5145\u5206\u5229\u7528\u8bed\u8a00\u6307\u4ee4\u7684\u7ec6\u8282\u4fe1\u606f\uff0c\u4e14\u5ffd\u89c6\u4e86\u8de8\u6a21\u6001\u5bf9\u8c61\u5173\u7cfb\u7684\u5efa\u6a21\uff0c\u5f71\u54cd\u4e86\u5bfc\u822a\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86TSE\u6a21\u5757\u63d0\u53d6\u5173\u952e\u6587\u672c\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7TOPA\u548cIOPA\u6a21\u5757\u5206\u522b\u589e\u5f3a\u6587\u672c\u548c\u56fe\u50cf\u4e2d\u7684\u5bf9\u8c61\u611f\u77e5\u80fd\u529b\u3002", "result": "\u5728R2R\u548cREVERIE\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86DOPE\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "DOPE\u901a\u8fc7\u589e\u5f3a\u5bf9\u8c61\u611f\u77e5\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86VLN\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2505.00776", "pdf": "https://arxiv.org/pdf/2505.00776", "abs": "https://arxiv.org/abs/2505.00776", "authors": ["Alessandro Raganato", "Rafael Pe\u00f1aloza", "Marco Viviani", "Gabriella Pasi"], "title": "Reasoning Capabilities and Invariability of Large Language Models", "categories": ["cs.CL"], "comment": "Accepted for publication in the Proceedings of the 23rd IEEE/WIC\n  International Conference on Web Intelligence and Intelligent Agent Technology\n  (WI-IAT 2024)", "summary": "Large Language Models (LLMs) have shown remarkable capabilities in\nmanipulating natural language across multiple applications, but their ability\nto handle simple reasoning tasks is often questioned. In this work, we aim to\nprovide a comprehensive analysis of LLMs' reasoning competence, specifically\nfocusing on their prompt dependency. In particular, we introduce a new\nbenchmark dataset with a series of simple reasoning questions demanding shallow\nlogical reasoning. Aligned with cognitive psychology standards, the questions\nare confined to a basic domain revolving around geometric figures, ensuring\nthat responses are independent of any pre-existing intuition about the world\nand rely solely on deduction. An empirical analysis involving zero-shot and\nfew-shot prompting across 24 LLMs of different sizes reveals that, while LLMs\nwith over 70 billion parameters perform better in the zero-shot setting, there\nis still a large room for improvement. An additional test with chain-of-thought\nprompting over 22 LLMs shows that this additional prompt can aid or damage the\nperformance of models, depending on whether the rationale is required before or\nafter the answer.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7b80\u5355\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u91cd\u70b9\u5173\u6ce8\u5176\u5bf9\u63d0\u793a\u7684\u4f9d\u8d56\u6027\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6570\u636e\u96c6\u8fdb\u884c\u6d4b\u8bd5\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u7b80\u5355\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u4ecd\u53d7\u8d28\u7591\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u5176\u63a8\u7406\u80fd\u529b\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u57fa\u4e8e\u51e0\u4f55\u56fe\u5f62\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6d4b\u8bd5\u4e8624\u79cd\u4e0d\u540c\u89c4\u6a21\u7684LLMs\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u63d0\u793a\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u8fdb\u4e00\u6b65\u6d4b\u8bd5\u4e86\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u7684\u6548\u679c\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u53c2\u6570\u91cf\u8d85\u8fc7700\u4ebf\u7684LLMs\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e2d\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff1b\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u7684\u6548\u679c\u53d6\u51b3\u4e8e\u63d0\u793a\u7684\u65f6\u673a\u3002", "conclusion": "LLMs\u5728\u7b80\u5355\u63a8\u7406\u4efb\u52a1\u4e2d\u4ecd\u6709\u4e0d\u8db3\uff0c\u63d0\u793a\u7684\u8bbe\u8ba1\u5bf9\u5176\u8868\u73b0\u6709\u663e\u8457\u5f71\u54cd\u3002"}}
{"id": "2505.00744", "pdf": "https://arxiv.org/pdf/2505.00744", "abs": "https://arxiv.org/abs/2505.00744", "authors": ["Dung Nguyen", "Minh Khoi Ho", "Huy Ta", "Thanh Tam Nguyen", "Qi Chen", "Kumar Rav", "Quy Duong Dang", "Satwik Ramchandre", "Son Lam Phung", "Zhibin Liao", "Minh-Son To", "Johan Verjans", "Phi Le Nguyen", "Vu Minh Hieu Phan"], "title": "Localizing Before Answering: A Benchmark for Grounded Medical Visual Question Answering", "categories": ["cs.CV"], "comment": "Accepted at Joint Conference on Artificial Intelligence (IJCAI) 2025", "summary": "Medical Large Multi-modal Models (LMMs) have demonstrated remarkable\ncapabilities in medical data interpretation. However, these models frequently\ngenerate hallucinations contradicting source evidence, particularly due to\ninadequate localization reasoning. This work reveals a critical limitation in\ncurrent medical LMMs: instead of analyzing relevant pathological regions, they\noften rely on linguistic patterns or attend to irrelevant image areas when\nresponding to disease-related queries. To address this, we introduce\nHEAL-MedVQA (Hallucination Evaluation via Localization MedVQA), a comprehensive\nbenchmark designed to evaluate LMMs' localization abilities and hallucination\nrobustness. HEAL-MedVQA features (i) two innovative evaluation protocols to\nassess visual and textual shortcut learning, and (ii) a dataset of 67K VQA\npairs, with doctor-annotated anatomical segmentation masks for pathological\nregions. To improve visual reasoning, we propose the Localize-before-Answer\n(LobA) framework, which trains LMMs to localize target regions of interest and\nself-prompt to emphasize segmented pathological areas, generating grounded and\nreliable answers. Experimental results demonstrate that our approach\nsignificantly outperforms state-of-the-art biomedical LMMs on the challenging\nHEAL-MedVQA benchmark, advancing robustness in medical VQA.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\u4e86\u5f53\u524d\u533b\u5b66\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08LMMs\uff09\u5728\u5b9a\u4f4d\u75c5\u7406\u533a\u57df\u65f6\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86HEAL-MedVQA\u57fa\u51c6\u548cLocalize-before-Answer\u6846\u67b6\u4ee5\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u533b\u5b66LMMs\u5728\u89e3\u91ca\u533b\u5b66\u6570\u636e\u65f6\u7ecf\u5e38\u4ea7\u751f\u4e0e\u6e90\u8bc1\u636e\u77db\u76fe\u7684\u5e7b\u89c9\uff0c\u4e3b\u8981\u7531\u4e8e\u7f3a\u4e4f\u5b9a\u4f4d\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86HEAL-MedVQA\u57fa\u51c6\uff0c\u5305\u542b\u4e24\u79cd\u8bc4\u4f30\u534f\u8bae\u548c67K\u533b\u751f\u6807\u6ce8\u7684VQA\u5bf9\uff0c\u5e76\u8bbe\u8ba1\u4e86Localize-before-Answer\u6846\u67b6\u6765\u8bad\u7ec3\u6a21\u578b\u5b9a\u4f4d\u75c5\u7406\u533a\u57df\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728HEAL-MedVQA\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u751f\u7269\u533b\u5b66LMMs\uff0c\u63d0\u5347\u4e86\u533b\u5b66VQA\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u6539\u8fdb\u5b9a\u4f4d\u80fd\u529b\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u4e86\u533b\u5b66LMMs\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u4e3a\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.00814", "pdf": "https://arxiv.org/pdf/2505.00814", "abs": "https://arxiv.org/abs/2505.00814", "authors": ["Mario S\u00e4nger", "Ulf Leser"], "title": "Knowledge-augmented Pre-trained Language Models for Biomedical Relation Extraction", "categories": ["cs.CL"], "comment": null, "summary": "Automatic relationship extraction (RE) from biomedical literature is critical\nfor managing the vast amount of scientific knowledge produced each year. In\nrecent years, utilizing pre-trained language models (PLMs) has become the\nprevalent approach in RE. Several studies report improved performance when\nincorporating additional context information while fine-tuning PLMs for RE.\nHowever, variations in the PLMs applied, the databases used for augmentation,\nhyper-parameter optimization, and evaluation methods complicate direct\ncomparisons between studies and raise questions about the generalizability of\nthese findings. Our study addresses this research gap by evaluating PLMs\nenhanced with contextual information on five datasets spanning four relation\nscenarios within a consistent evaluation framework. We evaluate three baseline\nPLMs and first conduct extensive hyperparameter optimization. After selecting\nthe top-performing model, we enhance it with additional data, including textual\nentity descriptions, relational information from knowledge graphs, and\nmolecular structure encodings. Our findings illustrate the importance of i) the\nchoice of the underlying language model and ii) a comprehensive hyperparameter\noptimization for achieving strong extraction performance. Although inclusion of\ncontext information yield only minor overall improvements, an ablation study\nreveals substantial benefits for smaller PLMs when such external data was\nincluded during fine-tuning.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PLMs\uff09\u5728\u751f\u7269\u533b\u5b66\u5173\u7cfb\u62bd\u53d6\uff08RE\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u7edf\u4e00\u8bc4\u4f30\u6846\u67b6\u6bd4\u8f83\u4e86\u4e0d\u540c\u4e0a\u4e0b\u6587\u4fe1\u606f\u589e\u5f3a\u7684PLMs\u6027\u80fd\uff0c\u53d1\u73b0\u6a21\u578b\u9009\u62e9\u548c\u8d85\u53c2\u6570\u4f18\u5316\u5bf9\u6027\u80fd\u5f71\u54cd\u663e\u8457\u3002", "motivation": "\u751f\u7269\u533b\u5b66\u6587\u732e\u4e2d\u7684\u5173\u7cfb\u62bd\u53d6\u5bf9\u7ba1\u7406\u5927\u91cf\u79d1\u5b66\u77e5\u8bc6\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u56e0\u6a21\u578b\u3001\u6570\u636e\u548c\u8bc4\u4f30\u65b9\u6cd5\u5dee\u5f02\u96be\u4ee5\u76f4\u63a5\u6bd4\u8f83\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u4e09\u79cd\u57fa\u7ebfPLMs\uff0c\u5e76\u8fdb\u884c\u8d85\u53c2\u6570\u4f18\u5316\uff0c\u968f\u540e\u7528\u5b9e\u4f53\u63cf\u8ff0\u3001\u77e5\u8bc6\u56fe\u8c31\u548c\u5206\u5b50\u7ed3\u6784\u7f16\u7801\u589e\u5f3a\u6027\u80fd\u6700\u4f73\u6a21\u578b\u3002", "result": "\u6a21\u578b\u9009\u62e9\u548c\u8d85\u53c2\u6570\u4f18\u5316\u5bf9\u6027\u80fd\u5f71\u54cd\u663e\u8457\uff0c\u4e0a\u4e0b\u6587\u4fe1\u606f\u5bf9\u5c0f\u578bPLMs\u6709\u8f83\u5927\u63d0\u5347\uff0c\u4f46\u5bf9\u6574\u4f53\u6027\u80fd\u6539\u5584\u6709\u9650\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u6a21\u578b\u9009\u62e9\u548c\u8d85\u53c2\u6570\u4f18\u5316\u7684\u91cd\u8981\u6027\uff0c\u4e0a\u4e0b\u6587\u4fe1\u606f\u5bf9\u5c0f\u578bPLMs\u6709\u76ca\uff0c\u4f46\u6574\u4f53\u6539\u8fdb\u6709\u9650\u3002"}}
{"id": "2505.00745", "pdf": "https://arxiv.org/pdf/2505.00745", "abs": "https://arxiv.org/abs/2505.00745", "authors": ["Maozhe Zhao", "Shengzhong Liu", "Fan Wu", "Guihai Chen"], "title": "Responsive DNN Adaptation for Video Analytics against Environment Shift via Hierarchical Mobile-Cloud Collaborations", "categories": ["cs.CV", "cs.LG"], "comment": "Sensys 2025 final version", "summary": "Mobile video analysis systems often encounter various deploying environments,\nwhere environment shifts present greater demands for responsiveness in\nadaptations of deployed \"expert DNN models\". Existing model adaptation\nframeworks primarily operate in a cloud-centric way, exhibiting degraded\nperformance during adaptation and delayed reactions to environment shifts.\nInstead, this paper proposes MOCHA, a novel framework optimizing the\nresponsiveness of continuous model adaptation through hierarchical\ncollaborations between mobile and cloud resources. Specifically, MOCHA (1)\nreduces adaptation response delays by performing on-device model reuse and fast\nfine-tuning before requesting cloud model retrieval and end-to-end retraining;\n(2) accelerates history expert model retrieval by organizing them into a\nstructured taxonomy utilizing domain semantics analyzed by a cloud foundation\nmodel as indices; (3) enables efficient local model reuse by maintaining\nonboard expert model caches for frequent scenes, which proactively prefetch\nmodel weights from the cloud model database. Extensive evaluations with\nreal-world videos on three DNN tasks show MOCHA improves the model accuracy\nduring adaptation by up to 6.8% while saving the response delay and retraining\ntime by up to 35.5x and 3.0x respectively.", "AI": {"tldr": "MOCHA\u662f\u4e00\u4e2a\u4f18\u5316\u79fb\u52a8\u548c\u4e91\u8d44\u6e90\u534f\u4f5c\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u6a21\u578b\u9002\u5e94\u73af\u5883\u53d8\u5316\u7684\u54cd\u5e94\u901f\u5ea6\uff0c\u901a\u8fc7\u8bbe\u5907\u7aef\u6a21\u578b\u91cd\u7528\u548c\u5feb\u901f\u5fae\u8c03\u51cf\u5c11\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u6709\u4e91\u4e2d\u5fc3\u5316\u6a21\u578b\u9002\u5e94\u6846\u67b6\u5728\u73af\u5883\u53d8\u5316\u65f6\u6027\u80fd\u4e0b\u964d\u4e14\u53cd\u5e94\u5ef6\u8fdf\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "MOCHA\u901a\u8fc7\u8bbe\u5907\u7aef\u6a21\u578b\u91cd\u7528\u3001\u5feb\u901f\u5fae\u8c03\u548c\u7ed3\u6784\u5316\u4e13\u5bb6\u6a21\u578b\u5206\u7c7b\u7d22\u5f15\uff0c\u4f18\u5316\u54cd\u5e94\u901f\u5ea6\u548c\u6a21\u578b\u68c0\u7d22\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMOCHA\u5728\u9002\u5e94\u671f\u95f4\u6a21\u578b\u51c6\u786e\u7387\u63d0\u53476.8%\uff0c\u54cd\u5e94\u5ef6\u8fdf\u548c\u91cd\u8bad\u7ec3\u65f6\u95f4\u5206\u522b\u51cf\u5c1135.5\u500d\u548c3.0\u500d\u3002", "conclusion": "MOCHA\u901a\u8fc7\u5206\u5c42\u534f\u4f5c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u9002\u5e94\u73af\u5883\u53d8\u5316\u7684\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2505.00931", "pdf": "https://arxiv.org/pdf/2505.00931", "abs": "https://arxiv.org/abs/2505.00931", "authors": ["Timur Jaganov", "John Blake", "Juli\u00e1n Villegas", "Nicholas Carr"], "title": "Large Language Model-Driven Dynamic Assessment of Grammatical Accuracy in English Language Learner Writing", "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 8 Figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "This study investigates the potential for Large Language Models (LLMs) to\nscale-up Dynamic Assessment (DA). To facilitate such an investigation, we first\ndeveloped DynaWrite-a modular, microservices-based grammatical tutoring\napplication which supports multiple LLMs to generate dynamic feedback to\nlearners of English. Initial testing of 21 LLMs, revealed GPT-4o and neural\nchat to have the most potential to scale-up DA in the language learning\nclassroom. Further testing of these two candidates found both models performed\nsimilarly in their ability to accurately identify grammatical errors in user\nsentences. However, GPT-4o consistently outperformed neural chat in the quality\nof its DA by generating clear, consistent, and progressively explicit hints.\nReal-time responsiveness and system stability were also confirmed through\ndetailed performance testing, with GPT-4o exhibiting sufficient speed and\nstability. This study shows that LLMs can be used to scale-up dynamic\nassessment and thus enable dynamic assessment to be delivered to larger groups\nthan possible in traditional teacher-learner settings.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u53ef\u4ee5\u6269\u5c55\u52a8\u6001\u8bc4\u4f30\uff08DA\uff09\uff0c\u5176\u4e2dGPT-4o\u5728\u751f\u6210\u6e05\u6670\u3001\u4e00\u81f4\u7684\u63d0\u793a\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u8bed\u8a00\u5b66\u4e60\u8bfe\u5802\u4e2d\u6269\u5c55\u52a8\u6001\u8bc4\u4f30\u7684\u6f5c\u529b\u3002", "method": "\u5f00\u53d1DynaWrite\u5e94\u7528\uff0c\u6d4b\u8bd521\u79cdLLMs\uff0c\u91cd\u70b9\u8bc4\u4f30GPT-4o\u548cNeural Chat\u7684\u8868\u73b0\u3002", "result": "GPT-4o\u5728\u9519\u8bef\u8bc6\u522b\u548c\u63d0\u793a\u8d28\u91cf\u4e0a\u4f18\u4e8eNeural Chat\uff0c\u4e14\u7cfb\u7edf\u54cd\u5e94\u901f\u5ea6\u548c\u7a33\u5b9a\u6027\u826f\u597d\u3002", "conclusion": "LLMs\u80fd\u591f\u6709\u6548\u6269\u5c55\u52a8\u6001\u8bc4\u4f30\uff0c\u9002\u7528\u4e8e\u66f4\u5927\u89c4\u6a21\u7684\u8bed\u8a00\u5b66\u4e60\u573a\u666f\u3002"}}
{"id": "2505.00746", "pdf": "https://arxiv.org/pdf/2505.00746", "abs": "https://arxiv.org/abs/2505.00746", "authors": ["Alexei Kaltchenko"], "title": "Entropy Heat-Mapping: Localizing GPT-Based OCR Errors with Sliding-Window Shannon Analysis", "categories": ["cs.CV"], "comment": "22 pages", "summary": "Vision-language models such as OpenAI GPT-4o can transcribe mathematical\ndocuments directly from images, yet their token-level confidence signals are\nseldom used to pinpoint local recognition mistakes. We present an\nentropy-heat-mapping proof-of-concept that turns per-token Shannon entropy into\na visual ''uncertainty landscape''. By scanning the entropy sequence with a\nfixed-length sliding window, we obtain hotspots that are likely to contain OCR\nerrors such as missing symbols, mismatched braces, or garbled prose. Using a\nsmall, curated set of scanned research pages rendered at several resolutions,\nwe compare the highlighted hotspots with the actual transcription errors\nproduced by GPT-4o. Our analysis shows that the vast majority of true errors\nare indeed concentrated inside the high-entropy regions. This study\ndemonstrates--in a minimally engineered setting--that sliding-window entropy\ncan serve as a practical, lightweight aid for post-editing GPT-based OCR. All\ncode, sample data, and annotation guidelines are released to encourage\nreplication and further research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u71b5\u70ed\u56fe\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790GPT-4o\u7684token\u7ea7\u7f6e\u4fe1\u5ea6\u4fe1\u53f7\uff0c\u5b9a\u4f4dOCR\u9519\u8bef\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u9ad8\u71b5\u533a\u57df\u4e0e\u771f\u5b9e\u9519\u8bef\u9ad8\u5ea6\u76f8\u5173\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-4o\uff09\u5728\u6570\u5b66\u6587\u6863\u8f6c\u5f55\u4e2d\u5f88\u5c11\u5229\u7528token\u7ea7\u7f6e\u4fe1\u5ea6\u4fe1\u53f7\u6765\u5b9a\u4f4d\u8bc6\u522b\u9519\u8bef\u3002", "method": "\u4f7f\u7528\u6ed1\u52a8\u7a97\u53e3\u626b\u63cftoken\u7684Shannon\u71b5\u5e8f\u5217\uff0c\u751f\u6210\u89c6\u89c9\u5316\u7684\u201c\u4e0d\u786e\u5b9a\u6027\u666f\u89c2\u201d\uff0c\u5e76\u6807\u8bb0\u9ad8\u71b5\u533a\u57df\u4f5c\u4e3a\u6f5c\u5728\u9519\u8bef\u70b9\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5927\u591a\u6570\u771f\u5b9e\u9519\u8bef\u96c6\u4e2d\u5728\u9ad8\u71b5\u533a\u57df\u3002", "conclusion": "\u6ed1\u52a8\u7a97\u53e3\u71b5\u5206\u6790\u53ef\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u5de5\u5177\uff0c\u8f85\u52a9GPT-based OCR\u7684\u540e\u7f16\u8f91\u5de5\u4f5c\u3002"}}
{"id": "2505.00949", "pdf": "https://arxiv.org/pdf/2505.00949", "abs": "https://arxiv.org/abs/2505.00949", "authors": ["Akhiad Bercovich", "Itay Levy", "Izik Golan", "Mohammad Dabbah", "Ran El-Yaniv", "Omri Puny", "Ido Galil", "Zach Moshe", "Tomer Ronen", "Najeeb Nabwani", "Ido Shahaf", "Oren Tropp", "Ehud Karpas", "Ran Zilberstein", "Jiaqi Zeng", "Soumye Singhal", "Alexander Bukharin", "Yian Zhang", "Tugrul Konuk", "Gerald Shen", "Ameya Sunil Mahabaleshwarkar", "Bilal Kartal", "Yoshi Suhara", "Olivier Delalleau", "Zijia Chen", "Zhilin Wang", "David Mosallanezhad", "Adi Renduchintala", "Haifeng Qian", "Dima Rekesh", "Fei Jia", "Somshubra Majumdar", "Vahid Noroozi", "Wasi Uddin Ahmad", "Sean Narenthiran", "Aleksander Ficek", "Mehrzad Samadi", "Jocelyn Huang", "Siddhartha Jain", "Igor Gitman", "Ivan Moshkov", "Wei Du", "Shubham Toshniwal", "George Armstrong", "Branislav Kisacanin", "Matvei Novikov", "Daria Gitman", "Evelina Bakhturina", "Jane Polak Scowcroft", "John Kamalu", "Dan Su", "Kezhi Kong", "Markus Kliegl", "Rabeeh Karimi", "Ying Lin", "Sanjeev Satheesh", "Jupinder Parmar", "Pritam Gundecha", "Brandon Norick", "Joseph Jennings", "Shrimai Prabhumoye", "Syeda Nahida Akter", "Mostofa Patwary", "Abhinav Khattar", "Deepak Narayanan", "Roger Waleffe", "Jimmy Zhang", "Bor-Yiing Su", "Guyue Huang", "Terry Kong", "Parth Chadha", "Sahil Jain", "Christine Harvey", "Elad Segal", "Jining Huang", "Sergey Kashirsky", "Robert McQueen", "Izzy Putterman", "George Lam", "Arun Venkatesan", "Sherry Wu", "Vinh Nguyen", "Manoj Kilaru", "Andrew Wang", "Anna Warno", "Abhilash Somasamudramath", "Sandip Bhaskar", "Maka Dong", "Nave Assaf", "Shahar Mor", "Omer Ullman Argov", "Scot Junkin", "Oleksandr Romanenko", "Pedro Larroy", "Monika Katariya", "Marco Rovinelli", "Viji Balas", "Nicholas Edelman", "Anahita Bhiwandiwalla", "Muthu Subramaniam", "Smita Ithape", "Karthik Ramamoorthy", "Yuting Wu", "Suguna Varshini Velury", "Omri Almog", "Joyjit Daw", "Denys Fridman", "Erick Galinkin", "Michael Evans", "Katherine Luna", "Leon Derczynski", "Nikki Pope", "Eileen Long", "Seth Schneider", "Guillermo Siman", "Tomasz Grzegorzek", "Pablo Ribalta", "Monika Katariya", "Joey Conway", "Trisha Saar", "Ann Guan", "Krzysztof Pawelec", "Shyamala Prayaga", "Oleksii Kuchaiev", "Boris Ginsburg", "Oluwatobi Olabiyi", "Kari Briski", "Jonathan Cohen", "Bryan Catanzaro", "Jonah Alben", "Yonatan Geifman", "Eric Chung"], "title": "Llama-Nemotron: Efficient Reasoning Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce the Llama-Nemotron series of models, an open family of\nheterogeneous reasoning models that deliver exceptional reasoning capabilities,\ninference efficiency, and an open license for enterprise use. The family comes\nin three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs\ncompetitively with state-of-the-art reasoning models such as DeepSeek-R1 while\noffering superior inference throughput and memory efficiency. In this report,\nwe discuss the training procedure for these models, which entails using neural\narchitecture search from Llama 3 models for accelerated inference, knowledge\ndistillation, and continued pretraining, followed by a reasoning-focused\npost-training stage consisting of two main parts: supervised fine-tuning and\nlarge scale reinforcement learning. Llama-Nemotron models are the first\nopen-source models to support a dynamic reasoning toggle, allowing users to\nswitch between standard chat and reasoning modes during inference. To further\nsupport open research and facilitate model development, we provide the\nfollowing resources: 1. We release the Llama-Nemotron reasoning models --\nLN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA\nOpen Model License Agreement. 2. We release the complete post-training dataset:\nLlama-Nemotron-Post-Training-Dataset. 3. We also release our training\ncodebases: NeMo, NeMo-Aligner, and Megatron-LM.", "AI": {"tldr": "Llama-Nemotron\u7cfb\u5217\u6a21\u578b\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5f02\u6784\u63a8\u7406\u6a21\u578b\u5bb6\u65cf\uff0c\u63d0\u4f9b\u5353\u8d8a\u7684\u63a8\u7406\u80fd\u529b\u3001\u9ad8\u6548\u7684\u63a8\u7406\u901f\u5ea6\u548c\u5546\u4e1a\u53cb\u597d\u7684\u8bb8\u53ef\u534f\u8bae\u3002", "motivation": "\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u5f00\u6e90\u3001\u9ad8\u6548\u7684\u63a8\u7406\u6a21\u578b\u5bb6\u65cf\uff0c\u4e0e\u73b0\u6709\u5148\u8fdb\u6a21\u578b\u7ade\u4e89\uff0c\u540c\u65f6\u63d0\u4f9b\u66f4\u9ad8\u7684\u63a8\u7406\u541e\u5410\u91cf\u548c\u5185\u5b58\u6548\u7387\u3002", "method": "\u901a\u8fc7\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u3001\u77e5\u8bc6\u84b8\u998f\u548c\u6301\u7eed\u9884\u8bad\u7ec3\u4f18\u5316\u6a21\u578b\uff0c\u968f\u540e\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u548c\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u7684\u540e\u8bad\u7ec3\u9636\u6bb5\u3002", "result": "\u6a21\u578b\u5728\u63a8\u7406\u6548\u7387\u548c\u6027\u80fd\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u652f\u6301\u52a8\u6001\u63a8\u7406\u5207\u6362\u529f\u80fd\uff0c\u5e76\u5f00\u6e90\u4e86\u6a21\u578b\u3001\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u4ee3\u7801\u3002", "conclusion": "Llama-Nemotron\u7cfb\u5217\u6a21\u578b\u4e3a\u5f00\u6e90\u793e\u533a\u548c\u4f01\u4e1a\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u63a8\u7406\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u5f00\u653e\u7814\u7a76\u548c\u6a21\u578b\u5f00\u53d1\u3002"}}
{"id": "2505.00751", "pdf": "https://arxiv.org/pdf/2505.00751", "abs": "https://arxiv.org/abs/2505.00751", "authors": ["Xingxi Yin", "Jingfeng Zhang", "Zhi Li", "Yicheng Li", "Yin Zhang"], "title": "InstructAttribute: Fine-grained Object Attributes editing with Instruction", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image (T2I) diffusion models, renowned for their advanced generative\nabilities, are extensively utilized in image editing applications,\ndemonstrating remarkable effectiveness. However, achieving precise control over\nfine-grained attributes still presents considerable challenges. Existing image\nediting techniques either fail to modify the attributes of an object or\nstruggle to preserve its structure and maintain consistency in other areas of\nthe image. To address these challenges, we propose the Structure-Preserving and\nAttribute Amplification (SPAA), a training-free method which enables precise\ncontrol over the color and material transformations of objects by editing the\nself-attention maps and cross-attention values. Furthermore, we constructed the\nAttribute Dataset, which encompasses nearly all colors and materials associated\nwith various objects, by integrating multimodal large language models (MLLM) to\ndevelop an automated pipeline for data filtering and instruction labeling.\nTraining on this dataset, we present our InstructAttribute, an\ninstruction-based model designed to facilitate fine-grained editing of color\nand material attributes. Extensive experiments demonstrate that our method\nachieves superior performance in object-level color and material editing,\noutperforming existing instruction-based image editing approaches.", "AI": {"tldr": "SPAA\u65b9\u6cd5\u901a\u8fc7\u7f16\u8f91\u81ea\u6ce8\u610f\u529b\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u56fe\uff0c\u5b9e\u73b0\u4e86\u5bf9\u7269\u4f53\u989c\u8272\u548c\u6750\u8d28\u7684\u7cbe\u786e\u63a7\u5236\uff0c\u65e0\u9700\u8bad\u7ec3\u3002\u7ed3\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u7684\u5c5e\u6027\u6570\u636e\u96c6\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u7ec6\u7c92\u5ea6\u7f16\u8f91\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u50cf\u7f16\u8f91\u6280\u672f\u5728\u4fee\u6539\u7269\u4f53\u5c5e\u6027\u65f6\u96be\u4ee5\u4fdd\u6301\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u4e14\u5bf9\u7ec6\u7c92\u5ea6\u5c5e\u6027\u7684\u63a7\u5236\u4e0d\u8db3\u3002", "method": "\u63d0\u51faSPAA\u65b9\u6cd5\uff0c\u901a\u8fc7\u7f16\u8f91\u81ea\u6ce8\u610f\u529b\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u56fe\u5b9e\u73b0\u989c\u8272\u548c\u6750\u8d28\u7684\u7cbe\u786e\u63a7\u5236\uff1b\u6784\u5efa\u5c5e\u6027\u6570\u636e\u96c6\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u6807\u6ce8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSPAA\u5728\u7269\u4f53\u7ea7\u989c\u8272\u548c\u6750\u8d28\u7f16\u8f91\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8e\u6307\u4ee4\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u3002", "conclusion": "SPAA\u65b9\u6cd5\u5728\u4fdd\u6301\u56fe\u50cf\u7ed3\u6784\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u5bf9\u7ec6\u7c92\u5ea6\u5c5e\u6027\u7684\u9ad8\u6548\u7f16\u8f91\uff0c\u4e3a\u56fe\u50cf\u7f16\u8f91\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.00977", "pdf": "https://arxiv.org/pdf/2505.00977", "abs": "https://arxiv.org/abs/2505.00977", "authors": ["Yingquan Chen", "Qianmu Li", "Xiaocong Wu", "Huifeng Li", "Qing Chang"], "title": "A Character-based Diffusion Embedding Algorithm for Enhancing the Generation Quality of Generative Linguistic Steganographic Texts", "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "Generating high-quality steganographic text is a fundamental challenge in the\nfield of generative linguistic steganography. This challenge arises primarily\nfrom two aspects: firstly, the capabilities of existing models in text\ngeneration are limited; secondly, embedding algorithms fail to effectively\nmitigate the negative impacts of sensitive information's properties, such as\nsemantic content or randomness. Specifically, to ensure that the recipient can\naccurately extract hidden information, embedding algorithms often have to\nconsider selecting candidate words with relatively low probabilities. This\nphenomenon leads to a decrease in the number of high-probability candidate\nwords and an increase in low-probability candidate words, thereby compromising\nthe semantic coherence and logical fluency of the steganographic text and\ndiminishing the overall quality of the generated steganographic material. To\naddress this issue, this paper proposes a novel embedding algorithm,\ncharacter-based diffusion embedding algorithm (CDEA). Unlike existing embedding\nalgorithms that strive to eliminate the impact of sensitive information's\nproperties on the generation process, CDEA leverages sensitive information's\nproperties. It enhances the selection frequency of high-probability candidate\nwords in the candidate pool based on general statistical properties at the\ncharacter level and grouping methods based on power-law distributions, while\nreducing the selection frequency of low-probability candidate words in the\ncandidate pool. Furthermore, to ensure the effective transformation of\nsensitive information in long sequences, we also introduce the XLNet model.\nExperimental results demonstrate that the combination of CDEA and XLNet\nsignificantly improves the quality of generated steganographic text,\nparticularly in terms of perceptual-imperceptibility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b57\u7b26\u7684\u6269\u6563\u5d4c\u5165\u7b97\u6cd5\uff08CDEA\uff09\uff0c\u7ed3\u5408XLNet\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9690\u5199\u6587\u672c\u7684\u751f\u6210\u8d28\u91cf\uff0c\u7279\u522b\u662f\u5728\u611f\u77e5\u4e0d\u53ef\u5bdf\u89c9\u6027\u65b9\u9762\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u6587\u672c\u751f\u6210\u80fd\u529b\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u4ee5\u53ca\u5d4c\u5165\u7b97\u6cd5\u672a\u80fd\u6709\u6548\u7f13\u89e3\u654f\u611f\u4fe1\u606f\u5c5e\u6027\uff08\u5982\u8bed\u4e49\u5185\u5bb9\u6216\u968f\u673a\u6027\uff09\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u5bfc\u81f4\u9690\u5199\u6587\u672c\u7684\u8bed\u4e49\u8fde\u8d2f\u6027\u548c\u903b\u8f91\u6d41\u7545\u6027\u4e0b\u964d\u3002", "method": "\u63d0\u51faCDEA\u7b97\u6cd5\uff0c\u5229\u7528\u654f\u611f\u4fe1\u606f\u7684\u5c5e\u6027\uff0c\u901a\u8fc7\u5b57\u7b26\u7ea7\u7edf\u8ba1\u7279\u6027\u548c\u57fa\u4e8e\u5e42\u5f8b\u5206\u5e03\u7684\u5206\u7ec4\u65b9\u6cd5\uff0c\u589e\u52a0\u9ad8\u6982\u7387\u5019\u9009\u8bcd\u7684\u9009\u62e9\u9891\u7387\uff0c\u51cf\u5c11\u4f4e\u6982\u7387\u5019\u9009\u8bcd\u7684\u9009\u62e9\u9891\u7387\uff0c\u5e76\u7ed3\u5408XLNet\u6a21\u578b\u5904\u7406\u957f\u5e8f\u5217\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCDEA\u4e0eXLNet\u7684\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u9690\u5199\u6587\u672c\u7684\u751f\u6210\u8d28\u91cf\uff0c\u5c24\u5176\u662f\u5728\u611f\u77e5\u4e0d\u53ef\u5bdf\u89c9\u6027\u65b9\u9762\u3002", "conclusion": "CDEA\u7b97\u6cd5\u901a\u8fc7\u5229\u7528\u654f\u611f\u4fe1\u606f\u7684\u5c5e\u6027\uff0c\u6709\u6548\u63d0\u5347\u4e86\u9690\u5199\u6587\u672c\u7684\u8d28\u91cf\uff0c\u4e3a\u751f\u6210\u9ad8\u8d28\u91cf\u9690\u5199\u6587\u672c\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.00752", "pdf": "https://arxiv.org/pdf/2505.00752", "abs": "https://arxiv.org/abs/2505.00752", "authors": ["Xuzhao Li", "Xuchen Li", "Shiyu Hu"], "title": "DARTer: Dynamic Adaptive Representation Tracker for Nighttime UAV Tracking", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ICMR 2025", "summary": "Nighttime UAV tracking presents significant challenges due to extreme\nillumination variations and viewpoint changes, which severely degrade tracking\nperformance. Existing approaches either rely on light enhancers with high\ncomputational costs or introduce redundant domain adaptation mechanisms,\nfailing to fully utilize the dynamic features in varying perspectives. To\naddress these issues, we propose \\textbf{DARTer} (\\textbf{D}ynamic\n\\textbf{A}daptive \\textbf{R}epresentation \\textbf{T}racker), an end-to-end\ntracking framework designed for nighttime UAV scenarios. DARTer leverages a\nDynamic Feature Blender (DFB) to effectively fuse multi-perspective nighttime\nfeatures from static and dynamic templates, enhancing representation\nrobustness. Meanwhile, a Dynamic Feature Activator (DFA) adaptively activates\nVision Transformer layers based on extracted features, significantly improving\nefficiency by reducing redundant computations. Our model eliminates the need\nfor complex multi-task loss functions, enabling a streamlined training process.\nExtensive experiments on multiple nighttime UAV tracking benchmarks demonstrate\nthe superiority of DARTer over state-of-the-art trackers. These results confirm\nthat DARTer effectively balances tracking accuracy and efficiency, making it a\npromising solution for real-world nighttime UAV tracking applications.", "AI": {"tldr": "DARTer\u662f\u4e00\u79cd\u7528\u4e8e\u591c\u95f4\u65e0\u4eba\u673a\u8ddf\u8e2a\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u7279\u5f81\u878d\u5408\u548c\u81ea\u9002\u5e94\u6fc0\u6d3b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ddf\u8e2a\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u591c\u95f4\u65e0\u4eba\u673a\u8ddf\u8e2a\u56e0\u5149\u7167\u53d8\u5316\u548c\u89c6\u89d2\u53d8\u5316\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u73b0\u6709\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u6216\u672a\u80fd\u5145\u5206\u5229\u7528\u52a8\u6001\u7279\u5f81\u3002", "method": "\u63d0\u51faDARTer\u6846\u67b6\uff0c\u5305\u542b\u52a8\u6001\u7279\u5f81\u6df7\u5408\u5668\uff08DFB\uff09\u548c\u52a8\u6001\u7279\u5f81\u6fc0\u6d3b\u5668\uff08DFA\uff09\uff0c\u4f18\u5316\u7279\u5f81\u878d\u5408\u548c\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5728\u591a\u4e2a\u591c\u95f4\u65e0\u4eba\u673a\u8ddf\u8e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e73\u8861\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "conclusion": "DARTer\u662f\u591c\u95f4\u65e0\u4eba\u673a\u8ddf\u8e2a\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2505.00979", "pdf": "https://arxiv.org/pdf/2505.00979", "abs": "https://arxiv.org/abs/2505.00979", "authors": ["Xuhui Jiang", "Shengjie Ma", "Chengjin Xu", "Cehao Yang", "Liyu Zhang", "Jian Guo"], "title": "Synthesize-on-Graph: Knowledgeable Synthetic Data Generation for Continue Pre-training of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success but remain\ndata-inefficient, especially when learning from small, specialized corpora with\nlimited and proprietary data. Existing synthetic data generation methods for\ncontinue pre-training focus on intra-document content and overlook\ncross-document knowledge associations, limiting content diversity and depth. We\npropose Synthetic-on-Graph (SoG), a synthetic data generation framework that\nincorporates cross-document knowledge associations for efficient corpus\nexpansion. SoG constructs a context graph by extracting entities and concepts\nfrom the original corpus, representing cross-document associations, and\nemploying a graph walk strategy for knowledge-associated sampling. This\nenhances synthetic data diversity and coherence, enabling models to learn\ncomplex knowledge structures and handle rare knowledge. To further improve\nsynthetic data quality, we integrate Chain-of-Thought (CoT) and Contrastive\nClarifying (CC) synthetic, enhancing reasoning processes and discriminative\npower. Experiments show that SoG outperforms the state-of-the-art (SOTA) method\nin a multi-hop document Q&A dataset while performing comparably to the SOTA\nmethod on the reading comprehension task datasets, which also underscores the\nbetter generalization capability of SoG. Our work advances synthetic data\ngeneration and provides practical solutions for efficient knowledge acquisition\nin LLMs, especially in domains with limited data availability.", "AI": {"tldr": "SoG\u6846\u67b6\u901a\u8fc7\u6784\u5efa\u4e0a\u4e0b\u6587\u56fe\u6574\u5408\u8de8\u6587\u6863\u77e5\u8bc6\u5173\u8054\uff0c\u63d0\u5347\u5408\u6210\u6570\u636e\u7684\u591a\u6837\u6027\u548c\u8fde\u8d2f\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5c0f\u89c4\u6a21\u3001\u4e13\u4e1a\u5316\u8bed\u6599\u4e0a\u6570\u636e\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u8de8\u6587\u6863\u77e5\u8bc6\u5173\u8054\u3002", "method": "SoG\u6846\u67b6\u63d0\u53d6\u5b9e\u4f53\u548c\u6982\u5ff5\u6784\u5efa\u4e0a\u4e0b\u6587\u56fe\uff0c\u7ed3\u5408\u56fe\u6e38\u8d70\u7b56\u7565\u548cCoT\u3001CC\u65b9\u6cd5\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\u3002", "result": "SoG\u5728\u591a\u8df3\u6587\u6863\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u4f18\u4e8eSOTA\u65b9\u6cd5\uff0c\u5728\u9605\u8bfb\u7406\u89e3\u4efb\u52a1\u4e0a\u8868\u73b0\u76f8\u5f53\uff0c\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\u3002", "conclusion": "SoG\u4e3a\u6709\u9650\u6570\u636e\u9886\u57df\u7684\u77e5\u8bc6\u83b7\u53d6\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u5408\u6210\u6570\u636e\u751f\u6210\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.00755", "pdf": "https://arxiv.org/pdf/2505.00755", "abs": "https://arxiv.org/abs/2505.00755", "authors": ["Atsuya Watanabe", "Ratna Aisuwarya", "Lei Jing"], "title": "P2P-Insole: Human Pose Estimation Using Foot Pressure Distribution and Motion Sensors", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This work presents P2P-Insole, a low-cost approach for estimating and\nvisualizing 3D human skeletal data using insole-type sensors integrated with\nIMUs. Each insole, fabricated with e-textile garment techniques, costs under\nUSD 1, making it significantly cheaper than commercial alternatives and ideal\nfor large-scale production. Our approach uses foot pressure distribution,\nacceleration, and rotation data to overcome limitations, providing a\nlightweight, minimally intrusive, and privacy-aware solution. The system\nemploys a Transformer model for efficient temporal feature extraction, enriched\nby first and second derivatives in the input stream. Including multimodal\ninformation, such as accelerometers and rotational measurements, improves the\naccuracy of complex motion pattern recognition. These facts are demonstrated\nexperimentally, while error metrics show the robustness of the approach in\nvarious posture estimation tasks. This work could be the foundation for a\nlow-cost, practical application in rehabilitation, injury prevention, and\nhealth monitoring while enabling further development through sensor\noptimization and expanded datasets.", "AI": {"tldr": "P2P-Insole\u662f\u4e00\u79cd\u4f4e\u6210\u672c\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210IMU\u7684\u978b\u57ab\u4f20\u611f\u5668\u4f30\u8ba1\u548c\u53ef\u89c6\u53163D\u4eba\u4f53\u9aa8\u9abc\u6570\u636e\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u751f\u4ea7\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5546\u4e1a\u65b9\u6848\u6210\u672c\u9ad8\u3001\u4fb5\u5165\u6027\u5f3a\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u8f7b\u91cf\u3001\u9690\u79c1\u53cb\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u978b\u57ab\u538b\u529b\u5206\u5e03\u3001\u52a0\u901f\u5ea6\u548c\u65cb\u8f6c\u6570\u636e\uff0c\u7ed3\u5408Transformer\u6a21\u578b\u63d0\u53d6\u65f6\u95f4\u7279\u5f81\uff0c\u5e76\u5229\u7528\u591a\u6a21\u6001\u4fe1\u606f\u63d0\u9ad8\u8bc6\u522b\u7cbe\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u8fd0\u52a8\u6a21\u5f0f\u8bc6\u522b\u548c\u59ff\u6001\u4f30\u8ba1\u4efb\u52a1\u4e2d\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "P2P-Insole\u4e3a\u5eb7\u590d\u3001\u4f24\u5bb3\u9884\u9632\u548c\u5065\u5eb7\u76d1\u6d4b\u63d0\u4f9b\u4e86\u4f4e\u6210\u672c\u5b9e\u7528\u65b9\u6848\uff0c\u5e76\u53ef\u901a\u8fc7\u4f20\u611f\u5668\u4f18\u5316\u548c\u6570\u636e\u96c6\u6269\u5c55\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2505.00985", "pdf": "https://arxiv.org/pdf/2505.00985", "abs": "https://arxiv.org/abs/2505.00985", "authors": ["Ayan Sengupta", "Yash Goel", "Tanmoy Chakraborty"], "title": "Position: Enough of Scaling LLMs! Lets Focus on Downscaling", "categories": ["cs.CL"], "comment": null, "summary": "We challenge the dominant focus on neural scaling laws and advocate for a\nparadigm shift toward downscaling in the development of large language models\n(LLMs). While scaling laws have provided critical insights into performance\nimprovements through increasing model and dataset size, we emphasize the\nsignificant limitations of this approach, particularly in terms of\ncomputational inefficiency, environmental impact, and deployment constraints.\nTo address these challenges, we propose a holistic framework for downscaling\nLLMs that seeks to maintain performance while drastically reducing resource\ndemands. This paper outlines practical strategies for transitioning away from\ntraditional scaling paradigms, advocating for a more sustainable, efficient,\nand accessible approach to LLM development.", "AI": {"tldr": "\u672c\u6587\u4e3b\u5f20\u4ece\u795e\u7ecf\u6269\u5c55\u5b9a\u5f8b\u8f6c\u5411\u7f29\u5c0f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5f00\u53d1\u8303\u5f0f\uff0c\u5f3a\u8c03\u4f20\u7edf\u6269\u5c55\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u8d44\u6e90\u9700\u6c42\u5927\u5e45\u964d\u4f4e\u7684\u6846\u67b6\u3002", "motivation": "\u4f20\u7edf\u6269\u5c55\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u3001\u73af\u5883\u5f71\u54cd\u548c\u90e8\u7f72\u9650\u5236\u65b9\u9762\u5b58\u5728\u663e\u8457\u95ee\u9898\uff0c\u9700\u8981\u66f4\u53ef\u6301\u7eed\u548c\u9ad8\u6548\u7684\u5f00\u53d1\u65b9\u5f0f\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5168\u9762\u7684\u7f29\u5c0fLLM\u6846\u67b6\uff0c\u65e8\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u51cf\u5c11\u8d44\u6e90\u9700\u6c42\uff0c\u5e76\u6982\u8ff0\u4e86\u4ece\u4f20\u7edf\u6269\u5c55\u8303\u5f0f\u8fc7\u6e21\u7684\u5b9e\u9645\u7b56\u7565\u3002", "result": "\u901a\u8fc7\u7f29\u5c0fLLM\uff0c\u53ef\u4ee5\u5728\u51cf\u5c11\u8d44\u6e90\u6d88\u8017\u7684\u540c\u65f6\u7ef4\u6301\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u7f29\u5c0fLLM\u662f\u4e00\u79cd\u66f4\u53ef\u6301\u7eed\u3001\u9ad8\u6548\u4e14\u53ef\u8bbf\u95ee\u7684\u5f00\u53d1\u65b9\u6cd5\uff0c\u503c\u5f97\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u63a8\u5e7f\u3002"}}
{"id": "2505.00757", "pdf": "https://arxiv.org/pdf/2505.00757", "abs": "https://arxiv.org/abs/2505.00757", "authors": ["Woong-Chan Byun", "Dong-Hee Paek", "Seung-Hyun Song", "Seung-Hyun Kong"], "title": "Efficient On-Chip Implementation of 4D Radar-Based 3D Object Detection on Hailo-8L", "categories": ["cs.CV", "cs.AI"], "comment": "4pages, 2 figures", "summary": "4D radar has attracted attention in autonomous driving due to its ability to\nenable robust 3D object detection even under adverse weather conditions. To\npractically deploy such technologies, it is essential to achieve real-time\nprocessing within low-power embedded environments. Addressing this, we present\nthe first on-chip implementation of a 4D radar-based 3D object detection model\non the Hailo-8L AI accelerator. Although conventional 3D convolutional neural\nnetwork (CNN) architectures require 5D inputs, the Hailo-8L only supports 4D\ntensors, posing a significant challenge. To overcome this limitation, we\nintroduce a tensor transformation method that reshapes 5D inputs into 4D\nformats during the compilation process, enabling direct deployment without\naltering the model structure. The proposed system achieves 46.47% AP_3D and\n52.75% AP_BEV, maintaining comparable accuracy to GPU-based models while\nachieving an inference speed of 13.76 Hz. These results demonstrate the\napplicability of 4D radar-based perception technologies to autonomous driving\nsystems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728Hailo-8L AI\u52a0\u901f\u5668\u4e0a\u5b9e\u73b04D\u96f7\u8fbe3D\u7269\u4f53\u68c0\u6d4b\u7684\u82af\u7247\u7ea7\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f20\u91cf\u53d8\u6362\u89e3\u51b35D\u8f93\u5165\u4e0e4D\u652f\u6301\u7684\u517c\u5bb9\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u5904\u7406\u4e0e\u9ad8\u7cbe\u5ea6\u3002", "motivation": "4D\u96f7\u8fbe\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u5728\u4f4e\u529f\u8017\u5d4c\u5165\u5f0f\u73af\u5883\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u5904\u7406\u3002", "method": "\u5f15\u5165\u5f20\u91cf\u53d8\u6362\u65b9\u6cd5\uff0c\u5c065D\u8f93\u5165\u91cd\u5851\u4e3a4D\u683c\u5f0f\uff0c\u9002\u914dHailo-8L\u52a0\u901f\u5668\u3002", "result": "\u7cfb\u7edf\u8fbe\u523046.47% AP_3D\u548c52.75% AP_BEV\uff0c\u63a8\u7406\u901f\u5ea613.76 Hz\uff0c\u4e0eGPU\u6a21\u578b\u7cbe\u5ea6\u76f8\u5f53\u3002", "conclusion": "\u8bc1\u660e\u4e864D\u96f7\u8fbe\u611f\u77e5\u6280\u672f\u5728\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2505.00989", "pdf": "https://arxiv.org/pdf/2505.00989", "abs": "https://arxiv.org/abs/2505.00989", "authors": ["Sijin Sun", "Liangbin Zhao", "Ming Deng", "Xiuju Fu"], "title": "VTS-LLM: Domain-Adaptive LLM Agent for Enhancing Awareness in Vessel Traffic Services through Natural Language", "categories": ["cs.CL"], "comment": "8 pages, 5 figures, 7 tablels, submitted to ITSC2025", "summary": "Vessel Traffic Services (VTS) are essential for maritime safety and\nregulatory compliance through real-time traffic management. However, with\nincreasing traffic complexity and the prevalence of heterogeneous, multimodal\ndata, existing VTS systems face limitations in spatiotemporal reasoning and\nintuitive human interaction. In this work, we propose VTS-LLM Agent, the first\ndomain-adaptive large LLM agent tailored for interactive decision support in\nVTS operations. We formalize risk-prone vessel identification as a\nknowledge-augmented Text-to-SQL task, combining structured vessel databases\nwith external maritime knowledge. To support this, we construct a curated\nbenchmark dataset consisting of a custom schema, domain-specific corpus, and a\nquery-SQL test set in multiple linguistic styles. Our framework incorporates\nNER-based relational reasoning, agent-based domain knowledge injection,\nsemantic algebra intermediate representation, and query rethink mechanisms to\nenhance domain grounding and context-aware understanding. Experimental results\nshow that VTS-LLM outperforms both general-purpose and SQL-focused baselines\nunder command-style, operational-style, and formal natural language queries,\nrespectively. Moreover, our analysis provides the first empirical evidence that\nlinguistic style variation introduces systematic performance challenges in\nText-to-SQL modeling. This work lays the foundation for natural language\ninterfaces in vessel traffic services and opens new opportunities for\nproactive, LLM-driven maritime real-time traffic management.", "AI": {"tldr": "\u63d0\u51faVTS-LLM Agent\uff0c\u9996\u4e2a\u9488\u5bf9VTS\u64cd\u4f5c\u7684\u4ea4\u4e92\u5f0f\u51b3\u7b56\u652f\u6301\u9886\u57df\u81ea\u9002\u5e94\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\uff0c\u901a\u8fc7\u77e5\u8bc6\u589e\u5f3a\u7684Text-to-SQL\u4efb\u52a1\u8bc6\u522b\u98ce\u9669\u8239\u8236\uff0c\u5e76\u5728\u591a\u8bed\u8a00\u98ce\u683c\u67e5\u8be2\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709VTS\u7cfb\u7edf\u5728\u65f6\u7a7a\u63a8\u7406\u548c\u76f4\u89c2\u4eba\u673a\u4ea4\u4e92\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u5e94\u5bf9\u65e5\u76ca\u590d\u6742\u7684\u4ea4\u901a\u548c\u591a\u6a21\u6001\u6570\u636e\u3002", "method": "\u7ed3\u5408\u7ed3\u6784\u5316\u8239\u8236\u6570\u636e\u5e93\u4e0e\u5916\u90e8\u6d77\u4e8b\u77e5\u8bc6\uff0c\u6784\u5efa\u5b9a\u5236\u6570\u636e\u96c6\uff0c\u91c7\u7528NER\u5173\u7cfb\u63a8\u7406\u3001\u9886\u57df\u77e5\u8bc6\u6ce8\u5165\u3001\u8bed\u4e49\u4ee3\u6570\u4e2d\u95f4\u8868\u793a\u548c\u67e5\u8be2\u91cd\u601d\u673a\u5236\u3002", "result": "VTS-LLM\u5728\u547d\u4ee4\u5f0f\u3001\u64cd\u4f5c\u5f0f\u548c\u6b63\u5f0f\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u4e2d\u5747\u4f18\u4e8e\u901a\u7528\u548cSQL\u4e13\u7528\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u4e3aVTS\u81ea\u7136\u8bed\u8a00\u63a5\u53e3\u5960\u5b9a\u57fa\u7840\uff0c\u63a8\u52a8LLM\u9a71\u52a8\u7684\u4e3b\u52a8\u6d77\u4e8b\u5b9e\u65f6\u4ea4\u901a\u7ba1\u7406\u3002"}}
{"id": "2505.00759", "pdf": "https://arxiv.org/pdf/2505.00759", "abs": "https://arxiv.org/abs/2505.00759", "authors": ["Jiahui Chen", "Candace Ross", "Reyhane Askari-Hemmat", "Koustuv Sinha", "Melissa Hall", "Michal Drozdzal", "Adriana Romero-Soriano"], "title": "Multi-Modal Language Models as Text-to-Image Model Evaluators", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "The steady improvements of text-to-image (T2I) generative models lead to slow\ndeprecation of automatic evaluation benchmarks that rely on static datasets,\nmotivating researchers to seek alternative ways to evaluate the T2I progress.\nIn this paper, we explore the potential of multi-modal large language models\n(MLLMs) as evaluator agents that interact with a T2I model, with the objective\nof assessing prompt-generation consistency and image aesthetics. We present\nMultimodal Text-to-Image Eval (MT2IE), an evaluation framework that iteratively\ngenerates prompts for evaluation, scores generated images and matches T2I\nevaluation of existing benchmarks with a fraction of the prompts used in\nexisting static benchmarks. Moreover, we show that MT2IE's prompt-generation\nconsistency scores have higher correlation with human judgment than scores\npreviously introduced in the literature. MT2IE generates prompts that are\nefficient at probing T2I model performance, producing the same relative T2I\nmodel rankings as existing benchmarks while using only 1/80th the number of\nprompts for evaluation.", "AI": {"tldr": "MT2IE\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5176\u751f\u6210\u7684\u63d0\u793a\u66f4\u9ad8\u6548\u4e14\u4e0e\u4eba\u7c7b\u5224\u65ad\u76f8\u5173\u6027\u66f4\u9ad8\u3002", "motivation": "\u968f\u7740\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u8fdb\u6b65\uff0c\u4f20\u7edf\u9759\u6001\u6570\u636e\u96c6\u7684\u8bc4\u4f30\u65b9\u6cd5\u9010\u6e10\u5931\u6548\uff0c\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u65b9\u5f0f\u3002", "method": "\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4f5c\u4e3a\u8bc4\u4f30\u4ee3\u7406\uff0c\u901a\u8fc7\u8fed\u4ee3\u751f\u6210\u63d0\u793a\u3001\u8bc4\u5206\u56fe\u50cf\uff0c\u5e76\u4e0e\u73b0\u6709\u57fa\u51c6\u6bd4\u8f83\u3002", "result": "MT2IE\u4ec5\u97001/80\u7684\u63d0\u793a\u6570\u91cf\u5373\u53ef\u8fbe\u5230\u4e0e\u73b0\u6709\u57fa\u51c6\u76f8\u540c\u7684\u6a21\u578b\u6392\u540d\uff0c\u4e14\u5176\u8bc4\u5206\u4e0e\u4eba\u7c7b\u5224\u65ad\u76f8\u5173\u6027\u66f4\u9ad8\u3002", "conclusion": "MT2IE\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4f18\u4e8e\u4f20\u7edf\u9759\u6001\u57fa\u51c6\u3002"}}
{"id": "2505.01006", "pdf": "https://arxiv.org/pdf/2505.01006", "abs": "https://arxiv.org/abs/2505.01006", "authors": ["Sumit Mamtani", "Maitreya Sonawane", "Kanika Agarwal", "Nishanth Sanjeev"], "title": "Token-free Models for Sarcasm Detection", "categories": ["cs.CL"], "comment": null, "summary": "Tokenization is a foundational step in most natural language processing (NLP)\npipelines, yet it introduces challenges such as vocabulary mismatch and\nout-of-vocabulary issues. Recent work has shown that models operating directly\non raw text at the byte or character level can mitigate these limitations. In\nthis paper, we evaluate two token-free models, ByT5 and CANINE, on the task of\nsarcasm detection in both social media (Twitter) and non-social media (news\nheadlines) domains. We fine-tune and benchmark these models against token-based\nbaselines and state-of-the-art approaches. Our results show that ByT5-small and\nCANINE outperform token-based counterparts and achieve new state-of-the-art\nperformance, improving accuracy by 0.77% and 0.49% on the News Headlines and\nTwitter Sarcasm datasets, respectively. These findings underscore the potential\nof token-free models for robust NLP in noisy and informal domains such as\nsocial media.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u4e24\u79cd\u65e0\u6807\u8bb0\u6a21\u578b\uff08ByT5\u548cCANINE\uff09\u5728\u793e\u4ea4\u5a92\u4f53\u548c\u975e\u793e\u4ea4\u5a92\u4f53\u9886\u57df\u7684\u8bbd\u523a\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5b83\u4eec\u4f18\u4e8e\u57fa\u4e8e\u6807\u8bb0\u7684\u6a21\u578b\uff0c\u5e76\u5237\u65b0\u4e86\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u6807\u8bb0\u5316\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u5f15\u5165\u8bcd\u6c47\u4e0d\u5339\u914d\u548c\u8bcd\u6c47\u8868\u5916\u95ee\u9898\uff0c\u65e0\u6807\u8bb0\u6a21\u578b\u53ef\u80fd\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u901a\u8fc7\u5fae\u8c03\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6bd4\u8f83ByT5\u548cCANINE\u4e0e\u57fa\u4e8e\u6807\u8bb0\u7684\u6a21\u578b\u5728\u8bbd\u523a\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "ByT5-small\u548cCANINE\u5728\u65b0\u95fb\u6807\u9898\u548cTwitter\u8bbd\u523a\u6570\u636e\u96c6\u4e0a\u5206\u522b\u63d0\u9ad8\u4e860.77%\u548c0.49%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u65e0\u6807\u8bb0\u6a21\u578b\u5728\u5608\u6742\u548c\u975e\u6b63\u5f0f\u9886\u57df\uff08\u5982\u793e\u4ea4\u5a92\u4f53\uff09\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2505.00772", "pdf": "https://arxiv.org/pdf/2505.00772", "abs": "https://arxiv.org/abs/2505.00772", "authors": ["Branko Brklja\u010d", "Milan Brklja\u010d"], "title": "Person detection and re-identification in open-world settings of retail stores and public spaces", "categories": ["cs.CV", "68T10, 68T07, 68T45, 94A08, 94A13,", "I.4.9; I.4.8; I.5.4; I.5.5; I.2.10; C.3; J.4; J.7"], "comment": "6 pages, 3 figures, 1 table, associated code implementation and\n  accompanying test videos with experimental results are available at the\n  following link: https://github.com/brkljac/personReID , paper submitted to\n  the 2nd International Scientific Conference 'ALFATECH - Smart Cities and\n  modern technologies - 2025', Belgrade, Serbia, Feb. 28, 2025", "summary": "Practical applications of computer vision in smart cities usually assume\nsystem integration and operation in challenging open-world environments. In the\ncase of person re-identification task the main goal is to retrieve information\nwhether the specific person has appeared in another place at a different time\ninstance of the same video, or over multiple camera feeds. This typically\nassumes collecting raw data from video surveillance cameras in different places\nand under varying illumination conditions. In the considered open-world setting\nit also requires detection and localization of the person inside the analyzed\nvideo frame before the main re-identification step. With multi-person and\nmulti-camera setups the system complexity becomes higher, requiring\nsophisticated tracking solutions and re-identification models. In this work we\nwill discuss existing challenges in system design architectures, consider\npossible solutions based on different computer vision techniques, and describe\napplications of such systems in retail stores and public spaces for improved\nmarketing analytics. In order to analyse sensitivity of person\nre-identification task under different open-world environments, a performance\nof one close to real-time solution will be demonstrated over several video\ncaptures and live camera feeds. Finally, based on conducted experiments we will\nindicate further research directions and possible system improvements.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u667a\u80fd\u57ce\u5e02\u4e2d\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e0b\u7684\u4eba\u5458\u91cd\u8bc6\u522b\u4efb\u52a1\uff0c\u63d0\u51fa\u4e86\u7cfb\u7edf\u8bbe\u8ba1\u6311\u6218\u3001\u89e3\u51b3\u65b9\u6848\u53ca\u5728\u96f6\u552e\u548c\u516c\u5171\u7a7a\u95f4\u7684\u5e94\u7528\u3002", "motivation": "\u89e3\u51b3\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u4eba\u5458\u91cd\u8bc6\u522b\u7684\u590d\u6742\u6027\u548c\u7cfb\u7edf\u8bbe\u8ba1\u6311\u6218\uff0c\u4ee5\u63d0\u5347\u89c6\u9891\u76d1\u63a7\u548c\u591a\u6444\u50cf\u5934\u73af\u5883\u4e0b\u7684\u6027\u80fd\u3002", "method": "\u8ba8\u8bba\u4e86\u73b0\u6709\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\uff0c\u5e76\u6d4b\u8bd5\u4e86\u4e00\u79cd\u63a5\u8fd1\u5b9e\u65f6\u7684\u4eba\u5458\u91cd\u8bc6\u522b\u89e3\u51b3\u65b9\u6848\u5728\u4e0d\u540c\u89c6\u9891\u548c\u5b9e\u65f6\u6444\u50cf\u5934\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u5c55\u793a\u4e86\u8be5\u89e3\u51b3\u65b9\u6848\u7684\u6027\u80fd\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5728\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u654f\u611f\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u8fdb\u4e00\u6b65\u7814\u7a76\u65b9\u5411\u548c\u6539\u8fdb\u7cfb\u7edf\u7684\u53ef\u80fd\u6027\uff0c\u4ee5\u4f18\u5316\u4eba\u5458\u91cd\u8bc6\u522b\u4efb\u52a1\u7684\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2505.01015", "pdf": "https://arxiv.org/pdf/2505.01015", "abs": "https://arxiv.org/abs/2505.01015", "authors": ["Jongwook Han", "Dongmin Choi", "Woojung Song", "Eun-Ju Lee", "Yohan Jo"], "title": "Value Portrait: Understanding Values of LLMs with Human-aligned Benchmark", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "32 pages, 7 figures", "summary": "The importance of benchmarks for assessing the values of language models has\nbeen pronounced due to the growing need of more authentic, human-aligned\nresponses. However, existing benchmarks rely on human or machine annotations\nthat are vulnerable to value-related biases. Furthermore, the tested scenarios\noften diverge from real-world contexts in which models are commonly used to\ngenerate text and express values. To address these issues, we propose the Value\nPortrait benchmark, a reliable framework for evaluating LLMs' value\norientations with two key characteristics. First, the benchmark consists of\nitems that capture real-life user-LLM interactions, enhancing the relevance of\nassessment results to real-world LLM usage and thus ecological validity.\nSecond, each item is rated by human subjects based on its similarity to their\nown thoughts, and correlations between these ratings and the subjects' actual\nvalue scores are derived. This psychometrically validated approach ensures that\nitems strongly correlated with specific values serve as reliable items for\nassessing those values. Through evaluating 27 LLMs with our benchmark, we find\nthat these models prioritize Benevolence, Security, and Self-Direction values\nwhile placing less emphasis on Tradition, Power, and Achievement values. Also,\nour analysis reveals biases in how LLMs perceive various demographic groups,\ndeviating from real human data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aValue Portrait\u7684\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u7684\u4ef7\u503c\u53d6\u5411\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u504f\u5dee\u548c\u751f\u6001\u6548\u5ea6\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4f9d\u8d56\u4eba\u5de5\u6216\u673a\u5668\u6807\u6ce8\uff0c\u6613\u53d7\u4ef7\u503c\u504f\u5dee\u5f71\u54cd\uff0c\u4e14\u6d4b\u8bd5\u573a\u666f\u4e0e\u73b0\u5b9e\u4f7f\u7528\u573a\u666f\u8131\u8282\u3002", "method": "\u8bbe\u8ba1\u5305\u542b\u771f\u5b9e\u7528\u6237-LLM\u4ea4\u4e92\u7684\u8bc4\u4f30\u9879\uff0c\u5e76\u901a\u8fc7\u4eba\u7c7b\u8bc4\u5206\u4e0e\u5fc3\u7406\u6d4b\u91cf\u5b66\u9a8c\u8bc1\u786e\u4fdd\u53ef\u9760\u6027\u3002", "result": "\u8bc4\u4f3027\u4e2aLLM\u53d1\u73b0\u5176\u66f4\u91cd\u89c6Benevolence\u3001Security\u548cSelf-Direction\u4ef7\u503c\uff0c\u800c\u5bf9Tradition\u3001Power\u548cAchievement\u91cd\u89c6\u4e0d\u8db3\uff0c\u5e76\u63ed\u793a\u4e86\u6a21\u578b\u5bf9\u67d0\u4e9b\u4eba\u53e3\u7fa4\u4f53\u7684\u504f\u89c1\u3002", "conclusion": "Value Portrait\u57fa\u51c6\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u3001\u53ef\u9760\u7684\u4ef7\u503c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86LLM\u7684\u4ef7\u503c\u53d6\u5411\u53ca\u5176\u504f\u5dee\u3002"}}
{"id": "2505.00786", "pdf": "https://arxiv.org/pdf/2505.00786", "abs": "https://arxiv.org/abs/2505.00786", "authors": ["Oluwanisola Ibikunle", "Hara Talasila", "Debvrat Varshney", "Jilu Li", "John Paden", "Maryam Rahnemoonfar"], "title": "AI-ready Snow Radar Echogram Dataset (SRED) for climate change monitoring", "categories": ["cs.CV"], "comment": null, "summary": "Tracking internal layers in radar echograms with high accuracy is essential\nfor understanding ice sheet dynamics and quantifying the impact of accelerated\nice discharge in Greenland and other polar regions due to contemporary global\nclimate warming. Deep learning algorithms have become the leading approach for\nautomating this task, but the absence of a standardized and well-annotated\nechogram dataset has hindered the ability to test and compare algorithms\nreliably, limiting the advancement of state-of-the-art methods for the radar\nechogram layer tracking problem. This study introduces the first comprehensive\n``deep learning ready'' radar echogram dataset derived from Snow Radar airborne\ndata collected during the National Aeronautics and Space Administration\nOperation Ice Bridge (OIB) mission in 2012. The dataset contains 13,717 labeled\nand 57,815 weakly-labeled echograms covering diverse snow zones (dry, ablation,\nwet) with varying along-track resolutions. To demonstrate its utility, we\nevaluated the performance of five deep learning models on the dataset. Our\nresults show that while current computer vision segmentation algorithms can\nidentify and track snow layer pixels in echogram images, advanced end-to-end\nmodels are needed to directly extract snow depth and annual accumulation from\nechograms, reducing or eliminating post-processing. The dataset and\naccompanying benchmarking framework provide a valuable resource for advancing\nradar echogram layer tracking and snow accumulation estimation, advancing our\nunderstanding of polar ice sheets response to climate warming.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u9996\u4e2a\u6df1\u5ea6\u5b66\u4e60\u53ef\u7528\u7684\u96f7\u8fbe\u56de\u6ce2\u56fe\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u6539\u8fdb\u51b0\u5c42\u8ffd\u8e2a\u6280\u672f\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e94\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u8868\u73b0\u3002", "motivation": "\u9ad8\u7cbe\u5ea6\u8ffd\u8e2a\u96f7\u8fbe\u56de\u6ce2\u56fe\u4e2d\u7684\u51b0\u5c42\u5bf9\u7406\u89e3\u51b0\u76d6\u52a8\u6001\u548c\u5168\u7403\u6c14\u5019\u53d8\u6696\u7684\u5f71\u54cd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u6807\u51c6\u5316\u6570\u636e\u96c6\u9650\u5236\u4e86\u7b97\u6cd5\u7684\u53d1\u5c55\u3002", "method": "\u7814\u7a76\u57fa\u4e8eNASA OIB\u4efb\u52a1\u7684Snow Radar\u6570\u636e\uff0c\u6784\u5efa\u4e86\u5305\u542b13,717\u6807\u6ce8\u548c57,815\u5f31\u6807\u6ce8\u56de\u6ce2\u56fe\u7684\u6570\u636e\u96c6\uff0c\u5e76\u6d4b\u8bd5\u4e86\u4e94\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "result": "\u5f53\u524d\u8ba1\u7b97\u673a\u89c6\u89c9\u5206\u5272\u7b97\u6cd5\u80fd\u8bc6\u522b\u51b0\u5c42\u50cf\u7d20\uff0c\u4f46\u9700\u66f4\u5148\u8fdb\u7684\u7aef\u5230\u7aef\u6a21\u578b\u4ee5\u76f4\u63a5\u63d0\u53d6\u96ea\u6df1\u548c\u5e74\u79ef\u7d2f\u91cf\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6846\u67b6\u4e3a\u51b0\u5c42\u8ffd\u8e2a\u548c\u96ea\u79ef\u7d2f\u4f30\u7b97\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u6781\u5730\u51b0\u76d6\u5bf9\u6c14\u5019\u53d8\u6696\u7684\u54cd\u5e94\u3002"}}
{"id": "2505.01035", "pdf": "https://arxiv.org/pdf/2505.01035", "abs": "https://arxiv.org/abs/2505.01035", "authors": ["Lui Yoshida"], "title": "Do We Need a Detailed Rubric for Automated Essay Scoring using Large Language Models?", "categories": ["cs.CL"], "comment": "Accepted in AIED 2025. This preprint has not undergone any\n  post-submission improvements or corrections", "summary": "This study investigates the necessity and impact of a detailed rubric in\nautomated essay scoring (AES) using large language models (LLMs). While using\nrubrics are standard in LLM-based AES, creating detailed rubrics requires\nsubstantial ef-fort and increases token usage. We examined how different levels\nof rubric detail affect scoring accuracy across multiple LLMs using the TOEFL11\ndataset. Our experiments compared three conditions: a full rubric, a simplified\nrubric, and no rubric, using four different LLMs (Claude 3.5 Haiku, Gemini 1.5\nFlash, GPT-4o-mini, and Llama 3 70B Instruct). Results showed that three out of\nfour models maintained similar scoring accuracy with the simplified rubric\ncompared to the detailed one, while significantly reducing token usage.\nHowever, one model (Gemini 1.5 Flash) showed decreased performance with more\ndetailed rubrics. The findings suggest that simplified rubrics may be\nsufficient for most LLM-based AES applications, offering a more efficient\nalternative without compromis-ing scoring accuracy. However, model-specific\nevaluation remains crucial as per-formance patterns vary across different LLMs.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\uff08AES\uff09\u4e2d\u8be6\u7ec6\u8bc4\u5206\u6807\u51c6\u7684\u5fc5\u8981\u6027\u548c\u5f71\u54cd\uff0c\u53d1\u73b0\u7b80\u5316\u6807\u51c6\u5728\u591a\u6570\u60c5\u51b5\u4e0b\u80fd\u4fdd\u6301\u8bc4\u5206\u51c6\u786e\u6027\u5e76\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u3002", "motivation": "\u8be6\u7ec6\u8bc4\u5206\u6807\u51c6\u5728LLM-based AES\u4e2d\u867d\u5e38\u7528\uff0c\u4f46\u5236\u4f5c\u8017\u65f6\u4e14\u589e\u52a0\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff0c\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u7b80\u5316\u6807\u51c6\u662f\u5426\u53ef\u884c\u3002", "method": "\u4f7f\u7528TOEFL11\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u4e86\u56db\u79cdLLM\uff08Claude 3.5 Haiku\u3001Gemini 1.5 Flash\u3001GPT-4o-mini\u548cLlama 3 70B Instruct\uff09\u5728\u5b8c\u6574\u3001\u7b80\u5316\u53ca\u65e0\u8bc4\u5206\u6807\u51c6\u4e09\u79cd\u6761\u4ef6\u4e0b\u7684\u8bc4\u5206\u51c6\u786e\u6027\u3002", "result": "\u56db\u5206\u4e4b\u4e09\u7684\u6a21\u578b\u5728\u7b80\u5316\u6807\u51c6\u4e0b\u4fdd\u6301\u4e0e\u8be6\u7ec6\u6807\u51c6\u76f8\u8fd1\u7684\u51c6\u786e\u6027\uff0c\u4e14\u663e\u8457\u51cf\u5c11\u8d44\u6e90\u6d88\u8017\uff1b\u4f46Gemini 1.5 Flash\u5728\u8be6\u7ec6\u6807\u51c6\u4e0b\u8868\u73b0\u4e0b\u964d\u3002", "conclusion": "\u7b80\u5316\u8bc4\u5206\u6807\u51c6\u5bf9\u591a\u6570LLM-based AES\u8db3\u591f\u9ad8\u6548\uff0c\u4f46\u9700\u6839\u636e\u6a21\u578b\u7279\u6027\u8bc4\u4f30\uff0c\u56e0\u6027\u80fd\u8868\u73b0\u56e0\u6a21\u578b\u800c\u5f02\u3002"}}
{"id": "2505.00788", "pdf": "https://arxiv.org/pdf/2505.00788", "abs": "https://arxiv.org/abs/2505.00788", "authors": ["Wufei Ma", "Luoxin Ye", "Nessa McWeeney", "Celso M de Melo", "Alan Yuille", "Jieneng Chen"], "title": "SpatialLLM: A Compound 3D-Informed Design towards Spatially-Intelligent Large Multimodal Models", "categories": ["cs.CV"], "comment": "CVPR 2025 highlight, camera ready version", "summary": "Humans naturally understand 3D spatial relationships, enabling complex\nreasoning like predicting collisions of vehicles from different directions.\nCurrent large multimodal models (LMMs), however, lack of this capability of 3D\nspatial reasoning. This limitation stems from the scarcity of 3D training data\nand the bias in current model designs toward 2D data. In this paper, we\nsystematically study the impact of 3D-informed data, architecture, and training\nsetups, introducing SpatialLLM, a large multi-modal model with advanced 3D\nspatial reasoning abilities. To address data limitations, we develop two types\nof 3D-informed training datasets: (1) 3D-informed probing data focused on\nobject's 3D location and orientation, and (2) 3D-informed conversation data for\ncomplex spatial relationships. Notably, we are the first to curate VQA data\nthat incorporate 3D orientation relationships on real images. Furthermore, we\nsystematically integrate these two types of training data with the\narchitectural and training designs of LMMs, providing a roadmap for optimal\ndesign aimed at achieving superior 3D reasoning capabilities. Our SpatialLLM\nadvances machines toward highly capable 3D-informed reasoning, surpassing\nGPT-4o performance by 8.7%. Our systematic empirical design and the resulting\nfindings offer valuable insights for future research in this direction.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSpatialLLM\uff0c\u4e00\u79cd\u5177\u6709\u5148\u8fdb3D\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u901a\u8fc73D\u6570\u636e\u589e\u5f3a\u548c\u67b6\u6784\u4f18\u5316\uff0c\u6027\u80fd\u8d85\u8d8aGPT-4o 8.7%\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u6a21\u578b\u7f3a\u4e4f3D\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u4e3b\u8981\u7531\u4e8e3D\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u548c\u6a21\u578b\u8bbe\u8ba1\u504f\u54112D\u6570\u636e\u3002", "method": "\u5f00\u53d1\u4e24\u79cd3D\u8bad\u7ec3\u6570\u636e\u96c6\uff083D\u63a2\u6d4b\u6570\u636e\u548c3D\u5bf9\u8bdd\u6570\u636e\uff09\uff0c\u5e76\u7cfb\u7edf\u6574\u5408\u5230\u6a21\u578b\u67b6\u6784\u4e0e\u8bad\u7ec3\u8bbe\u8ba1\u4e2d\u3002", "result": "SpatialLLM\u57283D\u63a8\u7406\u80fd\u529b\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u6027\u80fd\u8d85\u8d8aGPT-4o 8.7%\u3002", "conclusion": "\u7814\u7a76\u4e3a\u672a\u67653D\u63a8\u7406\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u65b9\u6cd5\u548c\u5b9d\u8d35\u89c1\u89e3\u3002"}}
{"id": "2505.01068", "pdf": "https://arxiv.org/pdf/2505.01068", "abs": "https://arxiv.org/abs/2505.01068", "authors": ["Yijie Jin", "Junjie Peng", "Xuanchao Lin", "Haochen Yuan", "Lan Wang", "Cangzhi Zheng"], "title": "Multimodal Transformers are Hierarchical Modal-wise Heterogeneous Graphs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multimodal Sentiment Analysis (MSA) is a rapidly developing field that\nintegrates multimodal information to recognize sentiments, and existing models\nhave made significant progress in this area. The central challenge in MSA is\nmultimodal fusion, which is predominantly addressed by Multimodal Transformers\n(MulTs). Although act as the paradigm, MulTs suffer from efficiency concerns.\nIn this work, from the perspective of efficiency optimization, we propose and\nprove that MulTs are hierarchical modal-wise heterogeneous graphs (HMHGs), and\nwe introduce the graph-structured representation pattern of MulTs. Based on\nthis pattern, we propose an Interlaced Mask (IM) mechanism to design the\nGraph-Structured and Interlaced-Masked Multimodal Transformer (GsiT). It is\nformally equivalent to MulTs which achieves an efficient weight-sharing\nmechanism without information disorder through IM, enabling All-Modal-In-One\nfusion with only 1/3 of the parameters of pure MulTs. A Triton kernel called\nDecomposition is implemented to ensure avoiding additional computational\noverhead. Moreover, it achieves significantly higher performance than\ntraditional MulTs. To further validate the effectiveness of GsiT itself and the\nHMHG concept, we integrate them into multiple state-of-the-art models and\ndemonstrate notable performance improvements and parameter reduction on widely\nused MSA datasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7ed3\u6784\u7684GsiT\u6a21\u578b\uff0c\u901a\u8fc7Interlaced Mask\u673a\u5236\u4f18\u5316\u591a\u6a21\u6001Transformer\u7684\u6548\u7387\uff0c\u53c2\u6570\u51cf\u5c112/3\u4e14\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u591a\u6a21\u6001Transformer\uff08MulTs\uff09\u5728\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u4e2d\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u4f18\u5316\u3002", "method": "\u5c06MulTs\u5efa\u6a21\u4e3a\u5c42\u6b21\u5316\u6a21\u6001\u5f02\u6784\u56fe\uff08HMHGs\uff09\uff0c\u63d0\u51faGsiT\u6a21\u578b\uff0c\u91c7\u7528Interlaced Mask\u673a\u5236\u5b9e\u73b0\u53c2\u6570\u5171\u4eab\u3002", "result": "GsiT\u53c2\u6570\u4ec5\u4e3a\u4f20\u7edfMulTs\u76841/3\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u5e76\u5728\u591a\u4e2aSOTA\u6a21\u578b\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "GsiT\u548cHMHG\u6982\u5ff5\u5728\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u4e2d\u5177\u6709\u9ad8\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002"}}
{"id": "2505.00805", "pdf": "https://arxiv.org/pdf/2505.00805", "abs": "https://arxiv.org/abs/2505.00805", "authors": ["Fadi Abdeladhim Zidi", "Abdelkrim Ouafi", "Fares Bougourzi", "Cosimo Distante", "Abdelmalik Taleb-Ahmed"], "title": "Advancing Wheat Crop Analysis: A Survey of Deep Learning Approaches Using Hyperspectral Imaging", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "As one of the most widely cultivated and consumed crops, wheat is essential\nto global food security. However, wheat production is increasingly challenged\nby pests, diseases, climate change, and water scarcity, threatening yields.\nTraditional crop monitoring methods are labor-intensive and often ineffective\nfor early issue detection. Hyperspectral imaging (HSI) has emerged as a\nnon-destructive and efficient technology for remote crop health assessment.\nHowever, the high dimensionality of HSI data and limited availability of\nlabeled samples present notable challenges. In recent years, deep learning has\nshown great promise in addressing these challenges due to its ability to\nextract and analysis complex structures. Despite advancements in applying deep\nlearning methods to HSI data for wheat crop analysis, no comprehensive survey\ncurrently exists in this field. This review addresses this gap by summarizing\nbenchmark datasets, tracking advancements in deep learning methods, and\nanalyzing key applications such as variety classification, disease detection,\nand yield estimation. It also highlights the strengths, limitations, and future\nopportunities in leveraging deep learning methods for HSI-based wheat crop\nanalysis. We have listed the current state-of-the-art papers and will continue\ntracking updating them in the following\nhttps://github.com/fadi-07/Awesome-Wheat-HSI-DeepLearning.", "AI": {"tldr": "\u7efc\u8ff0\u63a2\u8ba8\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u9ad8\u5149\u8c31\u6210\u50cf\uff08HSI\uff09\u5c0f\u9ea6\u4f5c\u7269\u5206\u6790\u4e2d\u7684\u5e94\u7528\uff0c\u603b\u7ed3\u4e86\u6570\u636e\u96c6\u3001\u65b9\u6cd5\u8fdb\u5c55\u53ca\u5173\u952e\u5e94\u7528\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u673a\u4f1a\u3002", "motivation": "\u5c0f\u9ea6\u751f\u4ea7\u9762\u4e34\u75c5\u866b\u5bb3\u3001\u6c14\u5019\u53d8\u5316\u7b49\u6311\u6218\uff0c\u4f20\u7edf\u76d1\u6d4b\u65b9\u6cd5\u6548\u7387\u4f4e\uff0cHSI\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u6709\u671b\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7efc\u8ff0\u73b0\u6709\u6587\u732e\uff0c\u603b\u7ed3HSI\u6570\u636e\u96c6\u3001\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u8fdb\u5c55\uff0c\u5206\u6790\u5176\u5728\u54c1\u79cd\u5206\u7c7b\u3001\u75c5\u5bb3\u68c0\u6d4b\u548c\u4ea7\u91cf\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u7efc\u8ff0\u586b\u8865\u4e86\u8be5\u9886\u57df\u7a7a\u767d\uff0c\u5217\u51fa\u4e86\u6700\u65b0\u7814\u7a76\uff0c\u5e76\u63d0\u4f9b\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u5728HSI\u5c0f\u9ea6\u5206\u6790\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u6570\u636e\u9ad8\u7ef4\u6027\u548c\u6837\u672c\u4e0d\u8db3\u7b49\u95ee\u9898\u3002"}}
{"id": "2505.01110", "pdf": "https://arxiv.org/pdf/2505.01110", "abs": "https://arxiv.org/abs/2505.01110", "authors": ["Murtadha Ahmed", "Wenbo", "Liu yunfeng"], "title": "MateICL: Mitigating Attention Dispersion in Large-Scale In-Context Learning", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nIn-Context Learning (ICL). However, the fixed position length constraints in\npre-trained models limit the number of demonstration examples. Recent efforts\nto extend context suffer from attention dispersion as the number of\ndemonstrations increases. In this paper, we introduce Mitigating Attention\nDispersion in large-scale ICL (MateICL) that enables LLMs to maintain effective\nself-attention as the context size grows. We first split the context into\nmultiple windows, each filled to the model's context capacity, which are\nprocessed separately. Then, we introduce an additional layer to recalibrate the\nattention weights, prioritizing the query tokens as the number of\ndemonstrations increases. Our empirical results show that MateICL can\neffectively leverage larger contexts to improve ICL performance. Compared to\nretrieval-based baselines, MateICL consistently achieves better performance\nwithout requiring an externally trained retrieval model. Despite recent\nadvances in inference strategies (e.g., 32k token contexts), our results\ndemonstrate that MateICL remains beneficial in computationally\nresource-constrained settings. The code is publicly available at\nhttps://github.com/amurtadha/MateICL.", "AI": {"tldr": "MateICL\u901a\u8fc7\u5206\u7a97\u5904\u7406\u548c\u6ce8\u610f\u529b\u6743\u91cd\u91cd\u65b0\u6821\u51c6\uff0c\u89e3\u51b3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u6ce8\u610f\u529b\u5206\u6563\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u56fa\u5b9a\u4f4d\u7f6e\u957f\u5ea6\u9650\u5236\u548c\u6ce8\u610f\u529b\u5206\u6563\u95ee\u9898\u9650\u5236\u4e86\u5927\u89c4\u6a21\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u6709\u6548\u6027\u3002", "method": "\u5c06\u4e0a\u4e0b\u6587\u5206\u7a97\u5904\u7406\uff0c\u5e76\u5f15\u5165\u989d\u5916\u5c42\u91cd\u65b0\u6821\u51c6\u6ce8\u610f\u529b\u6743\u91cd\u3002", "result": "MateICL\u80fd\u6709\u6548\u5229\u7528\u66f4\u5927\u4e0a\u4e0b\u6587\u63d0\u5347\u6027\u80fd\uff0c\u4f18\u4e8e\u68c0\u7d22\u57fa\u7ebf\u4e14\u65e0\u9700\u5916\u90e8\u68c0\u7d22\u6a21\u578b\u3002", "conclusion": "MateICL\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u4ecd\u5177\u4f18\u52bf\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.00836", "pdf": "https://arxiv.org/pdf/2505.00836", "abs": "https://arxiv.org/abs/2505.00836", "authors": ["Conor Flynn", "Christopher Ebersole", "Edmund Zelnio"], "title": "The Comparability of Model Fusion to Measured Data in Confuser Rejection", "categories": ["cs.CV"], "comment": "Conference paper for SPIE Defense and Commercial Sensing Algorithms\n  for Synthetic Aperture Radar Imagery XXXII. 14 pages, 9 figures", "summary": "Data collection has always been a major issue in the modeling and training of\nlarge deep learning networks, as no dataset can account for every slight\ndeviation we might see in live usage. Collecting samples can be especially\ncostly for Synthetic Aperture Radar (SAR), limiting the amount of unique\ntargets and operating conditions we are able to observe from. To counter this\nlack of data, simulators have been developed utilizing the shooting and\nbouncing ray method to allow for the generation of synthetic SAR data on 3D\nmodels. While effective, the synthetically generated data does not perfectly\ncorrelate to the measured data leading to issues when training models solely on\nsynthetic data. We aim to use computational power as a substitution for this\nlack of quality measured data, by ensembling many models trained on synthetic\ndata. Synthetic data is also not complete, as we do not know what targets might\nbe present in a live environment. Therefore we need to have our ensembling\ntechniques account for these unknown targets by applying confuser rejection in\nwhich our models will reject unknown targets it is presented with, and only\nclassify those it has been trained on.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u96c6\u6210\u591a\u4e2a\u57fa\u4e8e\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u6765\u89e3\u51b3\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\uff08SAR\uff09\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u5e72\u6270\u7269\u62d2\u7edd\u6280\u672f\u4ee5\u5e94\u5bf9\u672a\u77e5\u76ee\u6807\u3002", "motivation": "\u7531\u4e8e\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\uff08SAR\uff09\u6570\u636e\u91c7\u96c6\u6210\u672c\u9ad8\u4e14\u5408\u6210\u6570\u636e\u4e0e\u5b9e\u6d4b\u6570\u636e\u4e0d\u5b8c\u5168\u4e00\u81f4\uff0c\u5bfc\u81f4\u6a21\u578b\u8bad\u7ec3\u6548\u679c\u53d7\u9650\u3002", "method": "\u5229\u7528\u8ba1\u7b97\u80fd\u529b\u96c6\u6210\u591a\u4e2a\u57fa\u4e8e\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u5e76\u91c7\u7528\u5e72\u6270\u7269\u62d2\u7edd\u6280\u672f\u5904\u7406\u672a\u77e5\u76ee\u6807\u3002", "result": "\u901a\u8fc7\u96c6\u6210\u6a21\u578b\u548c\u5e72\u6270\u7269\u62d2\u7edd\u6280\u672f\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u5408\u6210\u6570\u636e\u8bad\u7ec3\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aSAR\u6570\u636e\u4e0d\u8db3\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u6a21\u578b\u5bf9\u672a\u77e5\u76ee\u6807\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.01162", "pdf": "https://arxiv.org/pdf/2505.01162", "abs": "https://arxiv.org/abs/2505.01162", "authors": ["Chebrolu Niranjan", "Kokil Jaidka", "Gerard Christopher Yeo"], "title": "On the Limitations of Steering in Language Model Alignment", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Steering vectors are a promising approach to aligning language model behavior\nat inference time. In this paper, we propose a framework to assess the\nlimitations of steering vectors as alignment mechanisms. Using a framework of\ntransformer hook interventions and antonym-based function vectors, we evaluate\nthe role of prompt structure and context complexity in steering effectiveness.\nOur findings indicate that steering vectors are promising for specific\nalignment tasks, such as value alignment, but may not provide a robust\nfoundation for general-purpose alignment in LLMs, particularly in complex\nscenarios. We establish a methodological foundation for future investigations\ninto steering capabilities of reasoning models.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u5f15\u5bfc\u5411\u91cf\u5728\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u53d1\u73b0\u5176\u5728\u7279\u5b9a\u4efb\u52a1\uff08\u5982\u4ef7\u503c\u89c2\u5bf9\u9f50\uff09\u4e2d\u6709\u6548\uff0c\u4f46\u5728\u590d\u6742\u573a\u666f\u4e0b\u53ef\u80fd\u4e0d\u8db3\u3002", "motivation": "\u7814\u7a76\u5f15\u5bfc\u5411\u91cf\u4f5c\u4e3a\u5bf9\u9f50\u673a\u5236\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u63a8\u7406\u6a21\u578b\u7684\u5f15\u5bfc\u80fd\u529b\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002", "method": "\u4f7f\u7528\u53d8\u538b\u5668\u94a9\u5e72\u9884\u548c\u53cd\u4e49\u8bcd\u529f\u80fd\u5411\u91cf\u6846\u67b6\uff0c\u8bc4\u4f30\u63d0\u793a\u7ed3\u6784\u548c\u4e0a\u4e0b\u6587\u590d\u6742\u6027\u5bf9\u5f15\u5bfc\u6548\u679c\u7684\u5f71\u54cd\u3002", "result": "\u5f15\u5bfc\u5411\u91cf\u5728\u7279\u5b9a\u5bf9\u9f50\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u590d\u6742\u573a\u666f\u4e0b\u7f3a\u4e4f\u9c81\u68d2\u6027\u3002", "conclusion": "\u5f15\u5bfc\u5411\u91cf\u9002\u7528\u4e8e\u7279\u5b9a\u5bf9\u9f50\u4efb\u52a1\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u63d0\u5347\u5176\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2505.00866", "pdf": "https://arxiv.org/pdf/2505.00866", "abs": "https://arxiv.org/abs/2505.00866", "authors": ["Viktor Kocur", "Charalambos Tzamos", "Yaqing Ding", "Zuzana Berger Haladova", "Torsten Sattler", "Zuzana Kukelova"], "title": "Are Minimal Radial Distortion Solvers Really Necessary for Relative Pose Estimation?", "categories": ["cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2410.05984", "summary": "Estimating the relative pose between two cameras is a fundamental step in\nmany applications such as Structure-from-Motion. The common approach to\nrelative pose estimation is to apply a minimal solver inside a RANSAC loop.\nHighly efficient solvers exist for pinhole cameras. Yet, (nearly) all cameras\nexhibit radial distortion. Not modeling radial distortion leads to\n(significantly) worse results. However, minimal radial distortion solvers are\nsignificantly more complex than pinhole solvers, both in terms of run-time and\nimplementation efforts. This paper compares radial distortion solvers with two\nsimple-to-implement approaches that do not use minimal radial distortion\nsolvers: The first approach combines an efficient pinhole solver with sampled\nradial undistortion parameters, where the sampled parameters are used for\nundistortion prior to applying the pinhole solver. The second approach uses a\nstate-of-the-art neural network to estimate the distortion parameters rather\nthan sampling them from a set of potential values. Extensive experiments on\nmultiple datasets, and different camera setups, show that complex minimal\nradial distortion solvers are not necessary in practice. We discuss under which\nconditions a simple sampling of radial undistortion parameters is preferable\nover calibrating cameras using a learning-based prior approach. Code and newly\ncreated benchmark for relative pose estimation under radial distortion are\navailable at https://github.com/kocurvik/rdnet.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86\u4e24\u79cd\u7b80\u5355\u5b9e\u73b0\u65b9\u6cd5\u4e0e\u4f20\u7edf\u5f84\u5411\u7578\u53d8\u6c42\u89e3\u5668\u7684\u6548\u679c\uff0c\u53d1\u73b0\u590d\u6742\u6c42\u89e3\u5668\u5728\u5b9e\u9645\u4e2d\u5e76\u975e\u5fc5\u8981\u3002", "motivation": "\u89e3\u51b3\u76f8\u673a\u5f84\u5411\u7578\u53d8\u5bf9\u76f8\u5bf9\u4f4d\u59ff\u4f30\u8ba1\u7684\u5f71\u54cd\uff0c\u907f\u514d\u590d\u6742\u6c42\u89e3\u5668\u7684\u9ad8\u6210\u672c\u548c\u5b9e\u73b0\u96be\u5ea6\u3002", "method": "1. \u7ed3\u5408\u9ad8\u6548\u9488\u5b54\u6c42\u89e3\u5668\u4e0e\u91c7\u6837\u5f84\u5411\u53bb\u7578\u53d8\u53c2\u6570\uff1b2. \u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u4f30\u8ba1\u7578\u53d8\u53c2\u6570\u800c\u975e\u91c7\u6837\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u590d\u6742\u5f84\u5411\u7578\u53d8\u6c42\u89e3\u5668\u5728\u5b9e\u9645\u4e2d\u4e0d\u5fc5\u8981\uff0c\u7b80\u5355\u65b9\u6cd5\u6548\u679c\u66f4\u4f18\u3002", "conclusion": "\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0c\u91c7\u6837\u5f84\u5411\u53bb\u7578\u53d8\u53c2\u6570\u4f18\u4e8e\u5b66\u4e60\u578b\u65b9\u6cd5\uff0c\u590d\u6742\u6c42\u89e3\u5668\u53ef\u88ab\u66ff\u4ee3\u3002"}}
{"id": "2505.01198", "pdf": "https://arxiv.org/pdf/2505.01198", "abs": "https://arxiv.org/abs/2505.01198", "authors": ["Mahdi Dhaini", "Ege Erdogan", "Nils Feldhus", "Gjergji Kasneci"], "title": "Gender Bias in Explainability: Investigating Performance Disparity in Post-hoc Methods", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACM Conference on Fairness, Accountability, and\n  Transparency (FAccT) 2025", "summary": "While research on applications and evaluations of explanation methods\ncontinues to expand, fairness of the explanation methods concerning disparities\nin their performance across subgroups remains an often overlooked aspect. In\nthis paper, we address this gap by showing that, across three tasks and five\nlanguage models, widely used post-hoc feature attribution methods exhibit\nsignificant gender disparity with respect to their faithfulness, robustness,\nand complexity. These disparities persist even when the models are pre-trained\nor fine-tuned on particularly unbiased datasets, indicating that the\ndisparities we observe are not merely consequences of biased training data. Our\nresults highlight the importance of addressing disparities in explanations when\ndeveloping and applying explainability methods, as these can lead to biased\noutcomes against certain subgroups, with particularly critical implications in\nhigh-stakes contexts. Furthermore, our findings underscore the importance of\nincorporating the fairness of explanations, alongside overall model fairness\nand explainability, as a requirement in regulatory frameworks.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5e7f\u6cdb\u4f7f\u7528\u7684\u540e\u9a8c\u7279\u5f81\u5f52\u56e0\u65b9\u6cd5\u5728\u5fe0\u5b9e\u6027\u3001\u9c81\u68d2\u6027\u548c\u590d\u6742\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u7684\u6027\u522b\u5dee\u5f02\uff0c\u5373\u4f7f\u6a21\u578b\u5728\u65e0\u504f\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u3002", "motivation": "\u63a2\u8ba8\u89e3\u91ca\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u7684\u516c\u5e73\u6027\u95ee\u9898\uff0c\u5c24\u5176\u662f\u8de8\u5b50\u7fa4\u4f53\u95f4\u7684\u5dee\u5f02\u3002", "method": "\u5206\u6790\u4e09\u79cd\u4efb\u52a1\u548c\u4e94\u79cd\u8bed\u8a00\u6a21\u578b\u4e2d\u540e\u9a8c\u7279\u5f81\u5f52\u56e0\u65b9\u6cd5\u7684\u6027\u522b\u5dee\u5f02\u3002", "result": "\u53d1\u73b0\u8fd9\u4e9b\u65b9\u6cd5\u5728\u65e0\u504f\u6570\u636e\u8bad\u7ec3\u4e0b\u4ecd\u5b58\u5728\u6027\u522b\u5dee\u5f02\uff0c\u53ef\u80fd\u5bfc\u81f4\u5bf9\u67d0\u4e9b\u5b50\u7fa4\u4f53\u7684\u504f\u89c1\u3002", "conclusion": "\u5f3a\u8c03\u5728\u5f00\u53d1\u548c\u5e94\u7528\u89e3\u91ca\u65b9\u6cd5\u65f6\u9700\u5173\u6ce8\u516c\u5e73\u6027\uff0c\u5e76\u5c06\u5176\u7eb3\u5165\u76d1\u7ba1\u6846\u67b6\u3002"}}
{"id": "2505.00938", "pdf": "https://arxiv.org/pdf/2505.00938", "abs": "https://arxiv.org/abs/2505.00938", "authors": ["Boyuan Meng", "Xiaohan Zhang", "Peilin Li", "Zhe Wu", "Yiming Li", "Wenkai Zhao", "Beinan Yu", "Hui-Liang Shen"], "title": "CDFormer: Cross-Domain Few-Shot Object Detection Transformer Against Feature Confusion", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Cross-domain few-shot object detection (CD-FSOD) aims to detect novel objects\nacross different domains with limited class instances. Feature confusion,\nincluding object-background confusion and object-object confusion, presents\nsignificant challenges in both cross-domain and few-shot settings. In this\nwork, we introduce CDFormer, a cross-domain few-shot object detection\ntransformer against feature confusion, to address these challenges. The method\nspecifically tackles feature confusion through two key modules:\nobject-background distinguishing (OBD) and object-object distinguishing (OOD).\nThe OBD module leverages a learnable background token to differentiate between\nobjects and background, while the OOD module enhances the distinction between\nobjects of different classes. Experimental results demonstrate that CDFormer\noutperforms previous state-of-the-art approaches, achieving 12.9% mAP, 11.0%\nmAP, and 10.4% mAP improvements under the 1/5/10 shot settings, respectively,\nwhen fine-tuned.", "AI": {"tldr": "CDFormer\u662f\u4e00\u79cd\u9488\u5bf9\u8de8\u57df\u5c11\u6837\u672c\u76ee\u6807\u68c0\u6d4b\uff08CD-FSOD\uff09\u7684Transformer\u65b9\u6cd5\uff0c\u901a\u8fc7OBD\u548cOOD\u6a21\u5757\u89e3\u51b3\u7279\u5f81\u6df7\u6dc6\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u8de8\u57df\u5c11\u6837\u672c\u76ee\u6807\u68c0\u6d4b\u4e2d\uff0c\u7279\u5f81\u6df7\u6dc6\uff08\u5982\u7269\u4f53-\u80cc\u666f\u6df7\u6dc6\u548c\u7269\u4f53-\u7269\u4f53\u6df7\u6dc6\uff09\u662f\u4e3b\u8981\u6311\u6218\u3002", "method": "\u63d0\u51faCDFormer\uff0c\u5305\u542bOBD\u6a21\u5757\uff08\u533a\u5206\u7269\u4f53\u4e0e\u80cc\u666f\uff09\u548cOOD\u6a21\u5757\uff08\u533a\u5206\u4e0d\u540c\u7c7b\u522b\u7269\u4f53\uff09\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cCDFormer\u57281/5/10 shot\u8bbe\u7f6e\u4e0b\u5206\u522b\u63d0\u5347\u4e8612.9%\u300111.0%\u548c10.4%\u7684mAP\u3002", "conclusion": "CDFormer\u6709\u6548\u89e3\u51b3\u4e86\u7279\u5f81\u6df7\u6dc6\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u57df\u5c11\u6837\u672c\u76ee\u6807\u68c0\u6d4b\u7684\u6027\u80fd\u3002"}}
{"id": "2505.01238", "pdf": "https://arxiv.org/pdf/2505.01238", "abs": "https://arxiv.org/abs/2505.01238", "authors": ["Mahdi Dhaini", "Kafaite Zahra Hussain", "Efstratios Zaradoukas", "Gjergji Kasneci"], "title": "EvalxNLP: A Framework for Benchmarking Post-Hoc Explainability Methods on NLP Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to the xAI World Conference (2025) - System Demonstration", "summary": "As Natural Language Processing (NLP) models continue to evolve and become\nintegral to high-stakes applications, ensuring their interpretability remains a\ncritical challenge. Given the growing variety of explainability methods and\ndiverse stakeholder requirements, frameworks that help stakeholders select\nappropriate explanations tailored to their specific use cases are increasingly\nimportant. To address this need, we introduce EvalxNLP, a Python framework for\nbenchmarking state-of-the-art feature attribution methods for transformer-based\nNLP models. EvalxNLP integrates eight widely recognized explainability\ntechniques from the Explainable AI (XAI) literature, enabling users to generate\nand evaluate explanations based on key properties such as faithfulness,\nplausibility, and complexity. Our framework also provides interactive,\nLLM-based textual explanations, facilitating user understanding of the\ngenerated explanations and evaluation outcomes. Human evaluation results\nindicate high user satisfaction with EvalxNLP, suggesting it is a promising\nframework for benchmarking explanation methods across diverse user groups. By\noffering a user-friendly and extensible platform, EvalxNLP aims at\ndemocratizing explainability tools and supporting the systematic comparison and\nadvancement of XAI techniques in NLP.", "AI": {"tldr": "EvalxNLP\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30NLP\u6a21\u578b\u89e3\u91ca\u65b9\u6cd5\u7684Python\u6846\u67b6\uff0c\u6574\u5408\u4e86\u591a\u79cdXAI\u6280\u672f\uff0c\u652f\u6301\u751f\u6210\u548c\u8bc4\u4f30\u89e3\u91ca\uff0c\u7528\u6237\u6ee1\u610f\u5ea6\u9ad8\u3002", "motivation": "\u968f\u7740NLP\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u7684\u666e\u53ca\uff0c\u89e3\u91ca\u6027\u6210\u4e3a\u5173\u952e\u6311\u6218\uff0c\u9700\u8981\u9488\u5bf9\u4e0d\u540c\u7528\u4f8b\u5b9a\u5236\u89e3\u91ca\u65b9\u6cd5\u3002", "method": "EvalxNLP\u6574\u5408\u4e86\u516b\u79cdXAI\u6280\u672f\uff0c\u652f\u6301\u57fa\u4e8e\u5fe0\u5b9e\u6027\u3001\u5408\u7406\u6027\u548c\u590d\u6742\u6027\u7b49\u5c5e\u6027\u7684\u89e3\u91ca\u751f\u6210\u4e0e\u8bc4\u4f30\uff0c\u5e76\u63d0\u4f9b\u4ea4\u4e92\u5f0fLLM\u6587\u672c\u89e3\u91ca\u3002", "result": "\u7528\u6237\u8bc4\u4f30\u663e\u793aEvalxNLP\u6ee1\u610f\u5ea6\u9ad8\uff0c\u8868\u660e\u5176\u662f\u8de8\u7528\u6237\u7fa4\u4f53\u89e3\u91ca\u65b9\u6cd5\u8bc4\u4f30\u7684\u6709\u524d\u666f\u6846\u67b6\u3002", "conclusion": "EvalxNLP\u65e8\u5728\u666e\u53ca\u89e3\u91ca\u5de5\u5177\uff0c\u652f\u6301XAI\u6280\u672f\u7684\u7cfb\u7edf\u6bd4\u8f83\u4e0e\u8fdb\u6b65\u3002"}}
{"id": "2505.00975", "pdf": "https://arxiv.org/pdf/2505.00975", "abs": "https://arxiv.org/abs/2505.00975", "authors": ["Yeonsang Shin", "Jihwan Kim", "Yumin Song", "Kyungseung Lee", "Hyunhee Chung", "Taeyoung Na"], "title": "Generating Animated Layouts as Structured Text Representations", "categories": ["cs.CV"], "comment": "AI for Content Creation (AI4CC) Workshop at CVPR 2025", "summary": "Despite the remarkable progress in text-to-video models, achieving precise\ncontrol over text elements and animated graphics remains a significant\nchallenge, especially in applications such as video advertisements. To address\nthis limitation, we introduce Animated Layout Generation, a novel approach to\nextend static graphic layouts with temporal dynamics. We propose a Structured\nText Representation for fine-grained video control through hierarchical visual\nelements. To demonstrate the effectiveness of our approach, we present VAKER\n(Video Ad maKER), a text-to-video advertisement generation pipeline that\ncombines a three-stage generation process with Unstructured Text Reasoning for\nseamless integration with LLMs. VAKER fully automates video advertisement\ngeneration by incorporating dynamic layout trajectories for objects and\ngraphics across specific video frames. Through extensive evaluations, we\ndemonstrate that VAKER significantly outperforms existing methods in generating\nvideo advertisements. Project Page:\nhttps://yeonsangshin.github.io/projects/Vaker", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAnimated Layout Generation\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6587\u672c\u8868\u793a\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u89c6\u9891\u63a7\u5236\uff0c\u5e76\u5f00\u53d1\u4e86VAKER\u5de5\u5177\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u89c6\u9891\u5e7f\u544a\u751f\u6210\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u5728\u63a7\u5236\u6587\u672c\u5143\u7d20\u548c\u52a8\u6001\u56fe\u5f62\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u89c6\u9891\u5e7f\u544a\u5e94\u7528\u4e2d\u3002", "method": "\u63d0\u51faAnimated Layout Generation\u65b9\u6cd5\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u6587\u672c\u8868\u793a\u548c\u4e09\u9636\u6bb5\u751f\u6210\u6d41\u7a0b\uff0c\u5f00\u53d1VAKER\u5de5\u5177\u3002", "result": "VAKER\u5728\u89c6\u9891\u5e7f\u544a\u751f\u6210\u4e2d\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u52a8\u6001\u56fe\u5f62\u548c\u6587\u672c\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5b9e\u73b0\u4e86\u89c6\u9891\u5e7f\u544a\u751f\u6210\u7684\u81ea\u52a8\u5316\u3002"}}
{"id": "2505.01255", "pdf": "https://arxiv.org/pdf/2505.01255", "abs": "https://arxiv.org/abs/2505.01255", "authors": ["Wei Han", "Hui Chen", "Soujanya Poria"], "title": "PREMISE: Matching-based Prediction for Accurate Review Recommendation", "categories": ["cs.CL", "cs.IR", "cs.MM"], "comment": "19 pages, 16 figures", "summary": "We present PREMISE (PREdict with Matching ScorEs), a new architecture for the\nmatching-based learning in the multimodal fields for the multimodal review\nhelpfulness (MRHP) task. Distinct to previous fusion-based methods which\nobtains multimodal representations via cross-modal attention for downstream\ntasks, PREMISE computes the multi-scale and multi-field representations,\nfilters duplicated semantics, and then obtained a set of matching scores as\nfeature vectors for the downstream recommendation task. This new architecture\nsignificantly boosts the performance for such multimodal tasks whose context\nmatching content are highly correlated to the targets of that task, compared to\nthe state-of-the-art fusion-based methods. Experimental results on two publicly\navailable datasets show that PREMISE achieves promising performance with less\ncomputational cost.", "AI": {"tldr": "PREMISE\u662f\u4e00\u79cd\u57fa\u4e8e\u5339\u914d\u7684\u591a\u6a21\u6001\u5b66\u4e60\u67b6\u6784\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u548c\u591a\u9886\u57df\u8868\u793a\u53ca\u5339\u914d\u5206\u6570\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u878d\u5408\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u56e0\u91cd\u590d\u8bed\u4e49\u548c\u4f4e\u6548\u8868\u793a\u5bfc\u81f4\u6027\u80fd\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u8ba1\u7b97\u591a\u5c3a\u5ea6\u548c\u591a\u9886\u57df\u8868\u793a\uff0c\u8fc7\u6ee4\u91cd\u590d\u8bed\u4e49\uff0c\u751f\u6210\u5339\u914d\u5206\u6570\u4f5c\u4e3a\u7279\u5f81\u5411\u91cf\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u3002", "conclusion": "PREMISE\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u878d\u5408\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u4e0a\u4e0b\u6587\u5339\u914d\u4e0e\u4efb\u52a1\u76ee\u6807\u9ad8\u5ea6\u76f8\u5173\u7684\u573a\u666f\u3002"}}
{"id": "2505.00980", "pdf": "https://arxiv.org/pdf/2505.00980", "abs": "https://arxiv.org/abs/2505.00980", "authors": ["Jiahuan Long", "Xin Zhou"], "title": "LMDepth: Lightweight Mamba-based Monocular Depth Estimation for Real-World Deployment", "categories": ["cs.CV"], "comment": null, "summary": "Monocular depth estimation provides an additional depth dimension to RGB\nimages, making it widely applicable in various fields such as virtual reality,\nautonomous driving and robotic navigation. However, existing depth estimation\nalgorithms often struggle to effectively balance performance and computational\nefficiency, which poses challenges for deployment on resource-constrained\ndevices. To address this, we propose LMDepth, a lightweight Mamba-based\nmonocular depth estimation network, designed to reconstruct high-precision\ndepth information while maintaining low computational overhead. Specifically,\nwe propose a modified pyramid spatial pooling module that serves as a\nmulti-scale feature aggregator and context extractor, ensuring global spatial\ninformation for accurate depth estimation. Moreover, we integrate multiple\ndepth Mamba blocks into the decoder. Designed with linear computations, the\nMamba Blocks enable LMDepth to efficiently decode depth information from global\nfeatures, providing a lightweight alternative to Transformer-based\narchitectures that depend on complex attention mechanisms. Extensive\nexperiments on the NYUDv2 and KITTI datasets demonstrate the effectiveness of\nour proposed LMDepth. Compared to previous lightweight depth estimation\nmethods, LMDepth achieves higher performance with fewer parameters and lower\ncomputational complexity (measured by GFLOPs). We further deploy LMDepth on an\nembedded platform with INT8 quantization, validating its practicality for\nreal-world edge applications.", "AI": {"tldr": "LMDepth\u662f\u4e00\u79cd\u57fa\u4e8eMamba\u7684\u8f7b\u91cf\u7ea7\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7f51\u7edc\uff0c\u901a\u8fc7\u6539\u8fdb\u7684\u91d1\u5b57\u5854\u7a7a\u95f4\u6c60\u5316\u6a21\u5757\u548c\u591a\u6df1\u5ea6Mamba\u5757\uff0c\u5728\u4fdd\u6301\u4f4e\u8ba1\u7b97\u5f00\u9500\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u6df1\u5ea6\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u4f30\u8ba1\u7b97\u6cd5\u5728\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0c\u9650\u5236\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u3002", "method": "\u63d0\u51fa\u6539\u8fdb\u7684\u91d1\u5b57\u5854\u7a7a\u95f4\u6c60\u5316\u6a21\u5757\u4f5c\u4e3a\u591a\u5c3a\u5ea6\u7279\u5f81\u805a\u5408\u5668\u548c\u4e0a\u4e0b\u6587\u63d0\u53d6\u5668\uff0c\u5e76\u96c6\u6210\u591a\u4e2a\u6df1\u5ea6Mamba\u5757\u4e8e\u89e3\u7801\u5668\u4e2d\uff0c\u5229\u7528\u7ebf\u6027\u8ba1\u7b97\u9ad8\u6548\u89e3\u7801\u6df1\u5ea6\u4fe1\u606f\u3002", "result": "\u5728NYUDv2\u548cKITTI\u6570\u636e\u96c6\u4e0a\uff0cLMDepth\u4ee5\u66f4\u5c11\u7684\u53c2\u6570\u548c\u66f4\u4f4e\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff08GFLOPs\uff09\u4f18\u4e8e\u73b0\u6709\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u5e76\u5728\u5d4c\u5165\u5f0f\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u4e86\u5b9e\u7528\u6027\u3002", "conclusion": "LMDepth\u4e3a\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.01273", "pdf": "https://arxiv.org/pdf/2505.01273", "abs": "https://arxiv.org/abs/2505.01273", "authors": ["Xuan Li", "Zhe Yin", "Xiaodong Gu", "Beijun Shen"], "title": "Anti-adversarial Learning: Desensitizing Prompts for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With the widespread use of LLMs, preserving privacy in user prompts has\nbecome crucial, as prompts risk exposing privacy and sensitive data to the\ncloud LLMs. Traditional techniques like homomorphic encryption, secure\nmulti-party computation, and federated learning face challenges due to heavy\ncomputational costs and user participation requirements, limiting their\napplicability in LLM scenarios. In this paper, we propose PromptObfus, a novel\nmethod for desensitizing LLM prompts. The core idea of PromptObfus is\n\"anti-adversarial\" learning, which perturbs privacy words in the prompt to\nobscure sensitive information while retaining the stability of model\npredictions. Specifically, PromptObfus frames prompt desensitization as a\nmasked language modeling task, replacing privacy-sensitive terms with a [MASK]\ntoken. A desensitization model is trained to generate candidate replacements\nfor each masked position. These candidates are subsequently selected based on\ngradient feedback from a surrogate model, ensuring minimal disruption to the\ntask output. We demonstrate the effectiveness of our approach on three NLP\ntasks. Results show that PromptObfus effectively prevents privacy inference\nfrom remote LLMs while preserving task performance.", "AI": {"tldr": "PromptObfus\u662f\u4e00\u79cd\u901a\u8fc7\u6297\u5bf9\u6297\u5b66\u4e60\u6270\u52a8\u9690\u79c1\u8bcd\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4fdd\u62a4LLM\u63d0\u793a\u4e2d\u7684\u9690\u79c1\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u9884\u6d4b\u7684\u7a33\u5b9a\u6027\u3002", "motivation": "\u968f\u7740LLM\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u7528\u6237\u63d0\u793a\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u4f20\u7edf\u65b9\u6cd5\u56e0\u8ba1\u7b97\u6210\u672c\u548c\u7528\u6237\u53c2\u4e0e\u9700\u6c42\u9ad8\u800c\u53d7\u9650\u3002", "method": "\u5c06\u63d0\u793a\u8131\u654f\u89c6\u4e3a\u63a9\u7801\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\uff0c\u7528[MASK]\u66ff\u6362\u9690\u79c1\u8bcd\uff0c\u8bad\u7ec3\u8131\u654f\u6a21\u578b\u751f\u6210\u5019\u9009\u66ff\u6362\uff0c\u5e76\u901a\u8fc7\u68af\u5ea6\u53cd\u9988\u9009\u62e9\u6700\u4f73\u66ff\u6362\u3002", "result": "\u5728\u4e09\u4e2aNLP\u4efb\u52a1\u4e2d\uff0cPromptObfus\u6709\u6548\u9632\u6b62\u4e86\u8fdc\u7a0bLLM\u7684\u9690\u79c1\u63a8\u65ad\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "PromptObfus\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684LLM\u63d0\u793a\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\u3002"}}
{"id": "2505.00998", "pdf": "https://arxiv.org/pdf/2505.00998", "abs": "https://arxiv.org/abs/2505.00998", "authors": ["Yu Hua", "Weiming Liu", "Gui Xu", "Yaqing Hou", "Yew-Soon Ong", "Qiang Zhang"], "title": "Deterministic-to-Stochastic Diverse Latent Feature Mapping for Human Motion Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Human motion synthesis aims to generate plausible human motion sequences,\nwhich has raised widespread attention in computer animation. Recent score-based\ngenerative models (SGMs) have demonstrated impressive results on this task.\nHowever, their training process involves complex curvature trajectories,\nleading to unstable training process. In this paper, we propose a\nDeterministic-to-Stochastic Diverse Latent Feature Mapping (DSDFM) method for\nhuman motion synthesis. DSDFM consists of two stages. The first human motion\nreconstruction stage aims to learn the latent space distribution of human\nmotions. The second diverse motion generation stage aims to build connections\nbetween the Gaussian distribution and the latent space distribution of human\nmotions, thereby enhancing the diversity and accuracy of the generated human\nmotions. This stage is achieved by the designed deterministic feature mapping\nprocedure with DerODE and stochastic diverse output generation procedure with\nDivSDE.DSDFM is easy to train compared to previous SGMs-based methods and can\nenhance diversity without introducing additional training parameters.Through\nqualitative and quantitative experiments, DSDFM achieves state-of-the-art\nresults surpassing the latest methods, validating its superiority in human\nmotion synthesis.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u786e\u5b9a\u6027\u5230\u968f\u673a\u6027\u7684\u591a\u6837\u5316\u6f5c\u5728\u7279\u5f81\u6620\u5c04\uff08DSDFM\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u4eba\u4f53\u8fd0\u52a8\u5408\u6210\uff0c\u89e3\u51b3\u4e86\u57fa\u4e8e\u5206\u6570\u751f\u6210\u6a21\u578b\uff08SGMs\uff09\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u4e86\u751f\u6210\u8fd0\u52a8\u7684\u591a\u6837\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u57fa\u4e8e\u5206\u6570\u751f\u6210\u6a21\u578b\uff08SGMs\uff09\u5728\u4eba\u4f53\u8fd0\u52a8\u5408\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u8bad\u7ec3\u8fc7\u7a0b\u590d\u6742\u4e14\u4e0d\u7a33\u5b9a\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7a33\u5b9a\u4e14\u80fd\u63d0\u5347\u591a\u6837\u6027\u7684\u65b9\u6cd5\u3002", "method": "DSDFM\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a1\uff09\u4eba\u4f53\u8fd0\u52a8\u91cd\u5efa\u9636\u6bb5\uff0c\u5b66\u4e60\u8fd0\u52a8\u6f5c\u5728\u7a7a\u95f4\u5206\u5e03\uff1b2\uff09\u591a\u6837\u5316\u8fd0\u52a8\u751f\u6210\u9636\u6bb5\uff0c\u901a\u8fc7\u786e\u5b9a\u6027\u7279\u5f81\u6620\u5c04\uff08DerODE\uff09\u548c\u968f\u673a\u591a\u6837\u5316\u8f93\u51fa\u751f\u6210\uff08DivSDE\uff09\u8fde\u63a5\u9ad8\u65af\u5206\u5e03\u4e0e\u6f5c\u5728\u7a7a\u95f4\u5206\u5e03\u3002", "result": "DSDFM\u5728\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u751f\u6210\u591a\u6837\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "DSDFM\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u4f53\u8fd0\u52a8\u5408\u6210\u7684\u591a\u6837\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2505.01311", "pdf": "https://arxiv.org/pdf/2505.01311", "abs": "https://arxiv.org/abs/2505.01311", "authors": ["Svenja Kenneweg", "J\u00f6rg Deigm\u00f6ller", "Julian Eggert", "Philipp Cimiano"], "title": "A Factorized Probabilistic Model of the Semantics of Vague Temporal Adverbials Relative to Different Event Types", "categories": ["cs.CL"], "comment": "7 pages, 1 figure, to be published in CogSci Proceedings 2025", "summary": "Vague temporal adverbials, such as recently, just, and a long time ago,\ndescribe the temporal distance between a past event and the utterance time but\nleave the exact duration underspecified. In this paper, we introduce a\nfactorized model that captures the semantics of these adverbials as\nprobabilistic distributions. These distributions are composed with\nevent-specific distributions to yield a contextualized meaning for an adverbial\napplied to a specific event. We fit the model's parameters using existing data\ncapturing judgments of native speakers regarding the applicability of these\nvague temporal adverbials to events that took place a given time ago. Comparing\nour approach to a non-factorized model based on a single Gaussian distribution\nfor each pair of event and temporal adverbial, we find that while both models\nhave similar predictive power, our model is preferable in terms of Occam's\nrazor, as it is simpler and has better extendability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u56e0\u5b50\u5316\u6a21\u578b\uff0c\u7528\u4e8e\u6355\u6349\u6a21\u7cca\u65f6\u95f4\u526f\u8bcd\u7684\u8bed\u4e49\uff0c\u5e76\u5c06\u5176\u4e0e\u4e8b\u4ef6\u7279\u5b9a\u5206\u5e03\u7ed3\u5408\uff0c\u751f\u6210\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u542b\u4e49\u3002\u4e0e\u975e\u56e0\u5b50\u5316\u6a21\u578b\u76f8\u6bd4\uff0c\u8be5\u6a21\u578b\u5728\u9884\u6d4b\u80fd\u529b\u76f8\u4f3c\u7684\u60c5\u51b5\u4e0b\u66f4\u7b80\u5355\u4e14\u66f4\u5177\u6269\u5c55\u6027\u3002", "motivation": "\u6a21\u7cca\u65f6\u95f4\u526f\u8bcd\uff08\u5982\u201c\u6700\u8fd1\u201d\u3001\u201c\u521a\u521a\u201d\u548c\u201c\u5f88\u4e45\u4ee5\u524d\u201d\uff09\u63cf\u8ff0\u4e86\u4e8b\u4ef6\u4e0e\u8bf4\u8bdd\u65f6\u95f4\u4e4b\u95f4\u7684\u65f6\u95f4\u8ddd\u79bb\uff0c\u4f46\u672a\u660e\u786e\u5177\u4f53\u65f6\u957f\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6982\u7387\u5206\u5e03\u6a21\u578b\u66f4\u51c6\u786e\u5730\u6355\u6349\u8fd9\u4e9b\u526f\u8bcd\u7684\u8bed\u4e49\u3002", "method": "\u5f15\u5165\u56e0\u5b50\u5316\u6a21\u578b\uff0c\u5c06\u6a21\u7cca\u65f6\u95f4\u526f\u8bcd\u7684\u8bed\u4e49\u5efa\u6a21\u4e3a\u6982\u7387\u5206\u5e03\uff0c\u5e76\u4e0e\u4e8b\u4ef6\u7279\u5b9a\u5206\u5e03\u7ed3\u5408\u3002\u6a21\u578b\u53c2\u6570\u901a\u8fc7\u73b0\u6709\u6570\u636e\uff08\u6bcd\u8bed\u8005\u5bf9\u526f\u8bcd\u9002\u7528\u6027\u7684\u5224\u65ad\uff09\u62df\u5408\u3002", "result": "\u4e0e\u975e\u56e0\u5b50\u5316\u6a21\u578b\uff08\u57fa\u4e8e\u9ad8\u65af\u5206\u5e03\uff09\u76f8\u6bd4\uff0c\u56e0\u5b50\u5316\u6a21\u578b\u9884\u6d4b\u80fd\u529b\u76f8\u4f3c\uff0c\u4f46\u66f4\u7b80\u5355\u4e14\u66f4\u5177\u6269\u5c55\u6027\u3002", "conclusion": "\u56e0\u5b50\u5316\u6a21\u578b\u5728\u6355\u6349\u6a21\u7cca\u65f6\u95f4\u526f\u8bcd\u8bed\u4e49\u65b9\u9762\u66f4\u4f18\uff0c\u7b26\u5408\u5965\u5361\u59c6\u5243\u5200\u539f\u5219\uff0c\u9002\u5408\u8fdb\u4e00\u6b65\u6269\u5c55\u3002"}}
{"id": "2505.01003", "pdf": "https://arxiv.org/pdf/2505.01003", "abs": "https://arxiv.org/abs/2505.01003", "authors": ["Kamel Aouaidjia", "Aofan Li", "Wenhao Zhang", "Chongsheng Zhang"], "title": "3D Human Pose Estimation via Spatial Graph Order Attention and Temporal Body Aware Transformer", "categories": ["cs.CV"], "comment": "16 pages, 9 figures, 7 tables", "summary": "Nowadays, Transformers and Graph Convolutional Networks (GCNs) are the\nprevailing techniques for 3D human pose estimation. However, Transformer-based\nmethods either ignore the spatial neighborhood relationships between the joints\nwhen used for skeleton representations or disregard the local temporal patterns\nof the local joint movements in skeleton sequence modeling, while GCN-based\nmethods often neglect the need for pose-specific representations. To address\nthese problems, we propose a new method that exploits the graph modeling\ncapability of GCN to represent each skeleton with multiple graphs of different\norders, incorporated with a newly introduced Graph Order Attention module that\ndynamically emphasizes the most representative orders for each joint. The\nresulting spatial features of the sequence are further processed using a\nproposed temporal Body Aware Transformer that models the global body feature\ndependencies in the sequence with awareness of the local inter-skeleton feature\ndependencies of joints. Given that our 3D pose output aligns with the central\n2D pose in the sequence, we improve the self-attention mechanism to be aware of\nthe central pose while diminishing its focus gradually towards the first and\nthe last poses. Extensive experiments on Human3.6m, MPIINF-3DHP, and HumanEva-I\ndatasets demonstrate the effectiveness of the proposed method. Code and models\nare made available on Github.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408GCN\u548cTransformer\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u9636\u56fe\u8868\u793a\u9aa8\u67b6\u5e76\u5f15\u5165\u56fe\u9636\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u540c\u65f6\u5229\u7528\u6539\u8fdb\u7684\u65f6\u7a7aTransformer\u5efa\u6a21\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5f81\u4f9d\u8d56\u3002", "motivation": "\u73b0\u6709Transformer\u548cGCN\u65b9\u6cd5\u57283D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u4e2d\u5206\u522b\u5ffd\u7565\u4e86\u7a7a\u95f4\u90bb\u57df\u5173\u7cfb\u6216\u5c40\u90e8\u65f6\u95f4\u6a21\u5f0f\uff0c\u4e14GCN\u65b9\u6cd5\u7f3a\u4e4f\u59ff\u6001\u7279\u5b9a\u8868\u793a\u3002", "method": "\u4f7f\u7528GCN\u7684\u591a\u9636\u56fe\u8868\u793a\u9aa8\u67b6\uff0c\u7ed3\u5408\u56fe\u9636\u6ce8\u610f\u529b\u6a21\u5757\u52a8\u6001\u9009\u62e9\u4ee3\u8868\u6027\u9636\u6570\uff0c\u5e76\u8bbe\u8ba1\u65f6\u7a7aBody Aware Transformer\u5efa\u6a21\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5f81\u4f9d\u8d56\u3002", "result": "\u5728Human3.6m\u3001MPIINF-3DHP\u548cHumanEva-I\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408GCN\u548cTransformer\u7684\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u6027\u80fd\u3002"}}
{"id": "2505.01314", "pdf": "https://arxiv.org/pdf/2505.01314", "abs": "https://arxiv.org/abs/2505.01314", "authors": ["Shang Wang", "Huanrong Tang", "Jianquan Ouyang"], "title": "A Transformer-based Neural Architecture Search Method", "categories": ["cs.CL", "cs.NE"], "comment": "GECCO 2023", "summary": "This paper presents a neural architecture search method based on Transformer\narchitecture, searching cross multihead attention computation ways for\ndifferent number of encoder and decoder combinations. In order to search for\nneural network structures with better translation results, we considered\nperplexity as an auxiliary evaluation metric for the algorithm in addition to\nBLEU scores and iteratively improved each individual neural network within the\npopulation by a multi-objective genetic algorithm. Experimental results show\nthat the neural network structures searched by the algorithm outperform all the\nbaseline models, and that the introduction of the auxiliary evaluation metric\ncan find better models than considering only the BLEU score as an evaluation\nmetric.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u67b6\u6784\u7684\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u7684\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\uff0c\u7ed3\u5408BLEU\u5206\u6570\u548c\u56f0\u60d1\u5ea6\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u4e3a\u4e86\u5728\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u641c\u7d22\u51fa\u6027\u80fd\u66f4\u597d\u7684\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6307\u6807\u548c\u9ad8\u6548\u7684\u641c\u7d22\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u57fa\u4e8eTransformer\u67b6\u6784\u7684\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff0c\u7ed3\u5408\u591a\u76ee\u6807\u9057\u4f20\u7b97\u6cd5\uff0c\u540c\u65f6\u4f18\u5316BLEU\u5206\u6570\u548c\u56f0\u60d1\u5ea6\u3002", "result": "\u641c\u7d22\u51fa\u7684\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u6a21\u578b\uff0c\u4e14\u5f15\u5165\u56f0\u60d1\u5ea6\u4f5c\u4e3a\u8f85\u52a9\u6307\u6807\u80fd\u53d1\u73b0\u6bd4\u4ec5\u7528BLEU\u5206\u6570\u66f4\u597d\u7684\u6a21\u578b\u3002", "conclusion": "\u591a\u76ee\u6807\u9057\u4f20\u7b97\u6cd5\u7ed3\u5408Transformer\u67b6\u6784\u641c\u7d22\u80fd\u6709\u6548\u63d0\u5347\u7ffb\u8bd1\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u8f85\u52a9\u8bc4\u4f30\u6307\u6807\u7684\u5f15\u5165\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86\u6a21\u578b\u9009\u62e9\u3002"}}
{"id": "2505.01016", "pdf": "https://arxiv.org/pdf/2505.01016", "abs": "https://arxiv.org/abs/2505.01016", "authors": ["Vishal Gandhi", "Sagar Gandhi"], "title": "Fine-Tuning Without Forgetting: Adaptation of YOLOv8 Preserves COCO Performance", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The success of large pre-trained object detectors hinges on their\nadaptability to diverse downstream tasks. While fine-tuning is the standard\nadaptation method, specializing these models for challenging fine-grained\ndomains necessitates careful consideration of feature granularity. The critical\nquestion remains: how deeply should the pre-trained backbone be fine-tuned to\noptimize for the specialized task without incurring catastrophic forgetting of\nthe original general capabilities? Addressing this, we present a systematic\nempirical study evaluating the impact of fine-tuning depth. We adapt a standard\nYOLOv8n model to a custom, fine-grained fruit detection dataset by\nprogressively unfreezing backbone layers (freeze points at layers 22, 15, and\n10) and training. Performance was rigorously evaluated on both the target fruit\ndataset and, using a dual-head evaluation architecture, on the original COCO\nvalidation set. Our results demonstrate unequivocally that deeper fine-tuning\n(unfreezing down to layer 10) yields substantial performance gains (e.g., +10\\%\nabsolute mAP50) on the fine-grained fruit task compared to only training the\nhead. Strikingly, this significant adaptation and specialization resulted in\nnegligible performance degradation (<0.1\\% absolute mAP difference) on the COCO\nbenchmark across all tested freeze levels. We conclude that adapting\nmid-to-late backbone features is highly effective for fine-grained\nspecialization. Critically, our results demonstrate this adaptation can be\nachieved without the commonly expected penalty of catastrophic forgetting,\npresenting a compelling case for exploring deeper fine-tuning strategies,\nparticularly when targeting complex domains or when maximizing specialized\nperformance is paramount.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u6df1\u5ea6\u5fae\u8c03\u9884\u8bad\u7ec3\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff08\u5982YOLOv8n\uff09\u7684\u4e2d\u540e\u671f\u9aa8\u5e72\u5c42\u53ef\u663e\u8457\u63d0\u5347\u7ec6\u7c92\u5ea6\u4efb\u52a1\u6027\u80fd\uff08\u5982\u6c34\u679c\u68c0\u6d4b\uff09\uff0c\u4e14\u4e0d\u4f1a\u5bfc\u81f4\u539f\u6709\u901a\u7528\u80fd\u529b\uff08\u5982COCO\u57fa\u51c6\uff09\u7684\u663e\u8457\u9000\u5316\u3002", "motivation": "\u63a2\u8ba8\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u4efb\u52a1\u4e2d\u7684\u9002\u5e94\u6027\uff0c\u7279\u522b\u662f\u5fae\u8c03\u6df1\u5ea6\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4ee5\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\u3002", "method": "\u901a\u8fc7\u9010\u6b65\u89e3\u51bb\u9aa8\u5e72\u5c42\uff0822\u300115\u300110\u5c42\uff09\u5fae\u8c03YOLOv8n\u6a21\u578b\uff0c\u5e76\u5728\u7ec6\u7c92\u5ea6\u6c34\u679c\u6570\u636e\u96c6\u548cCOCO\u9a8c\u8bc1\u96c6\u4e0a\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u6df1\u5ea6\u5fae\u8c03\uff08\u89e3\u51bb\u81f310\u5c42\uff09\u5728\u6c34\u679c\u4efb\u52a1\u4e0a\u6027\u80fd\u63d0\u5347\u663e\u8457\uff08+10% mAP50\uff09\uff0c\u4e14\u5bf9COCO\u57fa\u51c6\u5f71\u54cd\u6781\u5c0f\uff08<0.1% mAP\u5dee\u5f02\uff09\u3002", "conclusion": "\u4e2d\u540e\u671f\u9aa8\u5e72\u5c42\u5fae\u8c03\u5bf9\u7ec6\u7c92\u5ea6\u4efb\u52a1\u9ad8\u6548\u4e14\u5b89\u5168\uff0c\u65e0\u9700\u62c5\u5fc3\u707e\u96be\u6027\u9057\u5fd8\uff0c\u9002\u5408\u590d\u6742\u9886\u57df\u6216\u6027\u80fd\u4f18\u5148\u573a\u666f\u3002"}}
{"id": "2505.01315", "pdf": "https://arxiv.org/pdf/2505.01315", "abs": "https://arxiv.org/abs/2505.01315", "authors": ["Sheikh Samit Muhaimin", "Spyridon Mastorakis"], "title": "Helping Big Language Models Protect Themselves: An Enhanced Filtering and Summarization System", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The recent growth in the use of Large Language Models has made them\nvulnerable to sophisticated adversarial assaults, manipulative prompts, and\nencoded malicious inputs. Existing countermeasures frequently necessitate\nretraining models, which is computationally costly and impracticable for\ndeployment. Without the need for retraining or fine-tuning, this study presents\na unique defense paradigm that allows LLMs to recognize, filter, and defend\nagainst adversarial or malicious inputs on their own. There are two main parts\nto the suggested framework: (1) A prompt filtering module that uses\nsophisticated Natural Language Processing (NLP) techniques, including zero-shot\nclassification, keyword analysis, and encoded content detection (e.g. base64,\nhexadecimal, URL encoding), to detect, decode, and classify harmful inputs; and\n(2) A summarization module that processes and summarizes adversarial research\nliterature to give the LLM context-aware defense knowledge. This approach\nstrengthens LLMs' resistance to adversarial exploitation by fusing text\nextraction, summarization, and harmful prompt analysis. According to\nexperimental results, this integrated technique has a 98.71% success rate in\nidentifying harmful patterns, manipulative language structures, and encoded\nprompts. By employing a modest amount of adversarial research literature as\ncontext, the methodology also allows the model to react correctly to harmful\ninputs with a larger percentage of jailbreak resistance and refusal rate. While\nmaintaining the quality of LLM responses, the framework dramatically increases\nLLM's resistance to hostile misuse, demonstrating its efficacy as a quick and\neasy substitute for time-consuming, retraining-based defenses.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3LLM\u7684\u9632\u5fa1\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u793a\u8fc7\u6ee4\u548c\u4e0a\u4e0b\u6587\u603b\u7ed3\u6a21\u5757\uff0c\u6709\u6548\u8bc6\u522b\u548c\u62b5\u5fa1\u6076\u610f\u8f93\u5165\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6613\u53d7\u5bf9\u6297\u6027\u653b\u51fb\u548c\u6076\u610f\u8f93\u5165\u5f71\u54cd\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\uff0c\u6210\u672c\u9ad8\u4e14\u4e0d\u5b9e\u7528\u3002", "method": "\u6846\u67b6\u5305\u542b\u4e24\u90e8\u5206\uff1a1) \u63d0\u793a\u8fc7\u6ee4\u6a21\u5757\uff0c\u5229\u7528NLP\u6280\u672f\u68c0\u6d4b\u6709\u5bb3\u8f93\u5165\uff1b2) \u603b\u7ed3\u6a21\u5757\uff0c\u63d0\u4f9b\u4e0a\u4e0b\u6587\u9632\u5fa1\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u8bc6\u522b\u6709\u5bb3\u6a21\u5f0f\u7684\u6210\u529f\u7387\u8fbe98.71%\uff0c\u5e76\u663e\u8457\u63d0\u5347\u6a21\u578b\u5bf9\u6076\u610f\u8f93\u5165\u7684\u62b5\u6297\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u9632\u5fa1\u65b9\u6848\uff0c\u663e\u8457\u589e\u5f3aLLM\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2505.01032", "pdf": "https://arxiv.org/pdf/2505.01032", "abs": "https://arxiv.org/abs/2505.01032", "authors": ["Ruyu Yan", "Da-Qing Zhang"], "title": "Edge-preserving Image Denoising via Multi-scale Adaptive Statistical Independence Testing", "categories": ["cs.CV"], "comment": null, "summary": "Edge detection is crucial in image processing, but existing methods often\nproduce overly detailed edge maps, affecting clarity. Fixed-window statistical\ntesting faces issues like scale mismatch and computational redundancy. To\naddress these, we propose a novel Multi-scale Adaptive Independence\nTesting-based Edge Detection and Denoising (EDD-MAIT), a Multi-scale Adaptive\nStatistical Testing-based edge detection and denoising method that integrates a\nchannel attention mechanism with independence testing. A gradient-driven\nadaptive window strategy adjusts window sizes dynamically, improving detail\npreservation and noise suppression. EDD-MAIT achieves better robustness,\naccuracy, and efficiency, outperforming traditional and learning-based methods\non BSDS500 and BIPED datasets, with improvements in F-score, MSE, PSNR, and\nreduced runtime. It also shows robustness against Gaussian noise, generating\naccurate and clean edge maps in noisy environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u5c3a\u5ea6\u81ea\u9002\u5e94\u72ec\u7acb\u6027\u6d4b\u8bd5\u7684\u8fb9\u7f18\u68c0\u6d4b\u4e0e\u53bb\u566a\u65b9\u6cd5\uff08EDD-MAIT\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u7a97\u53e3\u5927\u5c0f\u548c\u7ed3\u5408\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fb9\u7f18\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u3001\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u8fb9\u7f18\u68c0\u6d4b\u65b9\u6cd5\u751f\u6210\u7684\u8fb9\u7f18\u56fe\u8fc7\u4e8e\u8be6\u7ec6\uff0c\u5f71\u54cd\u6e05\u6670\u5ea6\uff0c\u4e14\u56fa\u5b9a\u7a97\u53e3\u7edf\u8ba1\u6d4b\u8bd5\u5b58\u5728\u5c3a\u5ea6\u4e0d\u5339\u914d\u548c\u8ba1\u7b97\u5197\u4f59\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\u4e0e\u72ec\u7acb\u6027\u6d4b\u8bd5\uff0c\u91c7\u7528\u68af\u5ea6\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u7a97\u53e3\u7b56\u7565\u52a8\u6001\u8c03\u6574\u7a97\u53e3\u5927\u5c0f\u3002", "result": "\u5728BSDS500\u548cBIPED\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\uff0cF-score\u3001MSE\u3001PSNR\u7b49\u6307\u6807\u5747\u6709\u63d0\u5347\uff0c\u4e14\u8fd0\u884c\u65f6\u95f4\u51cf\u5c11\u3002\u5bf9\u9ad8\u65af\u566a\u58f0\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "EDD-MAIT\u80fd\u591f\u751f\u6210\u51c6\u786e\u4e14\u5e72\u51c0\u7684\u8fb9\u7f18\u56fe\uff0c\u9002\u7528\u4e8e\u566a\u58f0\u73af\u5883\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2505.01325", "pdf": "https://arxiv.org/pdf/2505.01325", "abs": "https://arxiv.org/abs/2505.01325", "authors": ["Svenja Kenneweg", "J\u00f6rg Deigm\u00f6ller", "Philipp Cimiano", "Julian Eggert"], "title": "TRAVELER: A Benchmark for Evaluating Temporal Reasoning across Vague, Implicit and Explicit References", "categories": ["cs.CL"], "comment": "24 pages, 6 figures, submitted to Springer Nature Computer Science", "summary": "Understanding and resolving temporal references is essential in Natural\nLanguage Understanding as we often refer to the past or future in daily\ncommunication. Although existing benchmarks address a system's ability to\nreason about and resolve temporal references, systematic evaluation of specific\ntemporal references remains limited. Towards closing this gap, we introduce\nTRAVELER, a novel synthetic benchmark dataset that follows a Question Answering\nparadigm and consists of questions involving temporal references with the\ncorresponding correct answers. TRAVELER assesses models' abilities to resolve\nexplicit, implicit relative to speech time, and vague temporal references.\nBeyond investigating the performance of state-of-the-art LLMs depending on the\ntype of temporal reference, our benchmark also allows evaluation of performance\nin relation to the length of the set of events. For the category of vague\ntemporal references, ground-truth answers were established via human surveys on\nProlific, following a procedure similar to the one from Kenneweg et al. To\ndemonstrate the benchmark's applicability, we evaluate four state-of-the-art\nLLMs using a question-answering task encompassing 3,300 questions. Our findings\nshow that while the benchmarked LLMs can answer questions over event sets with\na handful of events and explicit temporal references successfully, performance\nclearly deteriorates with larger event set length and when temporal references\nget less explicit. Notably, the vague question category exhibits the lowest\nperformance across all models.\n  The benchmark is publicly available at:\nhttps://gitlab.ub.uni-bielefeld.de/s.kenneweg/TRAVELER", "AI": {"tldr": "TRAVELER\u662f\u4e00\u4e2a\u65b0\u7684\u5408\u6210\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u5728\u89e3\u51b3\u65f6\u95f4\u5f15\u7528\u95ee\u9898\u4e0a\u7684\u80fd\u529b\uff0c\u5305\u62ec\u663e\u5f0f\u3001\u9690\u5f0f\u548c\u6a21\u7cca\u65f6\u95f4\u5f15\u7528\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u5bf9\u65f6\u95f4\u5f15\u7528\u7684\u7cfb\u7edf\u8bc4\u4f30\u6709\u9650\uff0cTRAVELER\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u95ee\u7b54\u8303\u5f0f\u6784\u5efa\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u6a21\u578b\u5bf9\u4e0d\u540c\u7c7b\u578b\u65f6\u95f4\u5f15\u7528\u548c\u4e8b\u4ef6\u96c6\u957f\u5ea6\u7684\u5904\u7406\u80fd\u529b\u3002", "result": "\u6d4b\u8bd5\u7684LLMs\u5728\u5c11\u91cf\u4e8b\u4ef6\u548c\u663e\u5f0f\u65f6\u95f4\u5f15\u7528\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u4e8b\u4ef6\u96c6\u8f83\u957f\u6216\u65f6\u95f4\u5f15\u7528\u6a21\u7cca\u65f6\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "TRAVELER\u4e3a\u65f6\u95f4\u5f15\u7528\u95ee\u9898\u63d0\u4f9b\u4e86\u7cfb\u7edf\u8bc4\u4f30\u5de5\u5177\uff0c\u63ed\u793a\u4e86LLMs\u5728\u6b64\u4efb\u52a1\u4e0a\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2505.01040", "pdf": "https://arxiv.org/pdf/2505.01040", "abs": "https://arxiv.org/abs/2505.01040", "authors": ["Ru-yu Yan", "Da-Qing Zhang"], "title": "Edge Detection based on Channel Attention and Inter-region Independence Test", "categories": ["cs.CV"], "comment": null, "summary": "Existing edge detection methods often suffer from noise amplification and\nexcessive retention of non-salient details, limiting their applicability in\nhigh-precision industrial scenarios. To address these challenges, we propose\nCAM-EDIT, a novel framework that integrates Channel Attention Mechanism (CAM)\nand Edge Detection via Independence Testing (EDIT). The CAM module adaptively\nenhances discriminative edge features through multi-channel fusion, while the\nEDIT module employs region-wise statistical independence analysis (using\nFisher's exact test and chi-square test) to suppress uncorrelated\nnoise.Extensive experiments on BSDS500 and NYUDv2 datasets demonstrate\nstate-of-the-art performance. Among the nine comparison algorithms, the\nF-measure scores of CAM-EDIT are 0.635 and 0.460, representing improvements of\n19.2\\% to 26.5\\% over traditional methods (Canny, CannySR), and better than the\nlatest learning based methods (TIP2020, MSCNGP). Noise robustness evaluations\nfurther reveal a 2.2\\% PSNR improvement under Gaussian noise compared to\nbaseline methods. Qualitative results exhibit cleaner edge maps with reduced\nartifacts, demonstrating its potential for high-precision industrial\napplications.", "AI": {"tldr": "CAM-EDIT\u7ed3\u5408\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\u548c\u72ec\u7acb\u6027\u6d4b\u8bd5\u7684\u8fb9\u7f18\u68c0\u6d4b\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u566a\u58f0\u9c81\u68d2\u6027\u548c\u8fb9\u7f18\u68c0\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u8fb9\u7f18\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u566a\u58f0\u653e\u5927\u548c\u975e\u663e\u8457\u7ec6\u8282\u4fdd\u7559\u8fc7\u591a\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5728\u9ad8\u7cbe\u5ea6\u5de5\u4e1a\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faCAM-EDIT\u6846\u67b6\uff0c\u96c6\u6210\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\uff08CAM\uff09\u548c\u57fa\u4e8e\u72ec\u7acb\u6027\u6d4b\u8bd5\u7684\u8fb9\u7f18\u68c0\u6d4b\uff08EDIT\uff09\uff0c\u901a\u8fc7\u591a\u901a\u9053\u878d\u5408\u548c\u7edf\u8ba1\u72ec\u7acb\u6027\u5206\u6790\u6291\u5236\u566a\u58f0\u3002", "result": "\u5728BSDS500\u548cNYUDv2\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cF-measure\u5206\u6570\u5206\u522b\u4e3a0.635\u548c0.460\uff0c\u4f18\u4e8e\u4f20\u7edf\u548c\u6700\u65b0\u5b66\u4e60\u65b9\u6cd5\uff0c\u566a\u58f0\u9c81\u68d2\u6027\u63d0\u53472.2% PSNR\u3002", "conclusion": "CAM-EDIT\u5728\u9ad8\u7cbe\u5ea6\u5de5\u4e1a\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u751f\u6210\u66f4\u5e72\u51c0\u7684\u8fb9\u7f18\u56fe\u5e76\u51cf\u5c11\u4f2a\u5f71\u3002"}}
{"id": "2505.01050", "pdf": "https://arxiv.org/pdf/2505.01050", "abs": "https://arxiv.org/abs/2505.01050", "authors": ["Kai Hu", "Weichen Yu", "Li Zhang", "Alexander Robey", "Andy Zou", "Chengming Xu", "Haoqi Hu", "Matt Fredrikson"], "title": "Transferable Adversarial Attacks on Black-Box Vision-Language Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Vision Large Language Models (VLLMs) are increasingly deployed to offer\nadvanced capabilities on inputs comprising both text and images. While prior\nresearch has shown that adversarial attacks can transfer from open-source to\nproprietary black-box models in text-only and vision-only contexts, the extent\nand effectiveness of such vulnerabilities remain underexplored for VLLMs. We\npresent a comprehensive analysis demonstrating that targeted adversarial\nexamples are highly transferable to widely-used proprietary VLLMs such as\nGPT-4o, Claude, and Gemini. We show that attackers can craft perturbations to\ninduce specific attacker-chosen interpretations of visual information, such as\nmisinterpreting hazardous content as safe, overlooking sensitive or restricted\nmaterial, or generating detailed incorrect responses aligned with the\nattacker's intent. Furthermore, we discover that universal perturbations --\nmodifications applicable to a wide set of images -- can consistently induce\nthese misinterpretations across multiple proprietary VLLMs. Our experimental\nresults on object recognition, visual question answering, and image captioning\nshow that this vulnerability is common across current state-of-the-art models,\nand underscore an urgent need for robust mitigations to ensure the safe and\nsecure deployment of VLLMs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u9488\u5bf9\u5f00\u6e90\u6a21\u578b\u7684\u5bf9\u6297\u653b\u51fb\u53ef\u4ee5\u8f6c\u79fb\u5230\u4e13\u6709\u7684\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b\uff08VLLMs\uff09\u4e0a\uff0c\u5bfc\u81f4\u6a21\u578b\u5bf9\u89c6\u89c9\u4fe1\u606f\u7684\u9519\u8bef\u89e3\u8bfb\u3002", "motivation": "\u63a2\u7d22VLLMs\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u8106\u5f31\u6027\uff0c\u5c24\u5176\u662f\u5728\u591a\u6a21\u6001\u8f93\u5165\uff08\u6587\u672c\u548c\u56fe\u50cf\uff09\u573a\u666f\u4e2d\u3002", "method": "\u901a\u8fc7\u751f\u6210\u76ee\u6807\u5bf9\u6297\u6837\u672c\u548c\u901a\u7528\u6270\u52a8\uff0c\u6d4b\u8bd5\u5176\u5728\u4e13\u6709VLLMs\uff08\u5982GPT-4o\u3001Claude\u548cGemini\uff09\u4e0a\u7684\u53ef\u8f6c\u79fb\u6027\u548c\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5bf9\u6297\u653b\u51fb\u80fd\u8bf1\u5bfc\u6a21\u578b\u9519\u8bef\u89e3\u8bfb\u89c6\u89c9\u4fe1\u606f\uff0c\u4e14\u901a\u7528\u6270\u52a8\u5728\u591a\u4e2a\u6a21\u578b\u4e2d\u5747\u6709\u6548\u3002", "conclusion": "\u5f53\u524dVLLMs\u666e\u904d\u5b58\u5728\u5bf9\u6297\u653b\u51fb\u6f0f\u6d1e\uff0c\u4e9f\u9700\u9c81\u68d2\u7684\u9632\u5fa1\u63aa\u65bd\u4ee5\u786e\u4fdd\u5b89\u5168\u90e8\u7f72\u3002"}}
{"id": "2505.00808", "pdf": "https://arxiv.org/pdf/2505.00808", "abs": "https://arxiv.org/abs/2505.00808", "authors": ["Kola Ayonrinde", "Louis Jaburi"], "title": "A Mathematical Philosophy of Explanations in Mechanistic Interpretability -- The Strange Science Part I.i", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "15 pages (plus appendices), 2 figures", "summary": "Mechanistic Interpretability aims to understand neural networks through\ncausal explanations. We argue for the Explanatory View Hypothesis: that\nMechanistic Interpretability research is a principled approach to understanding\nmodels because neural networks contain implicit explanations which can be\nextracted and understood. We hence show that Explanatory Faithfulness, an\nassessment of how well an explanation fits a model, is well-defined. We propose\na definition of Mechanistic Interpretability (MI) as the practice of producing\nModel-level, Ontic, Causal-Mechanistic, and Falsifiable explanations of neural\nnetworks, allowing us to distinguish MI from other interpretability paradigms\nand detail MI's inherent limits. We formulate the Principle of Explanatory\nOptimism, a conjecture which we argue is a necessary precondition for the\nsuccess of Mechanistic Interpretability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u89e3\u91ca\u6027\u89c6\u89d2\u5047\u8bf4\uff0c\u8ba4\u4e3a\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u80fd\u63d0\u53d6\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u9690\u542b\u89e3\u91ca\uff0c\u5e76\u5b9a\u4e49\u4e86\u89e3\u91ca\u5fe0\u5b9e\u6027\u548c\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7684\u6846\u67b6\u3002", "motivation": "\u63a2\u8ba8\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u7684\u7406\u8bba\u57fa\u7840\uff0c\u660e\u786e\u5176\u4e0e\u5176\u4ed6\u53ef\u89e3\u91ca\u6027\u8303\u5f0f\u7684\u533a\u522b\u53ca\u5176\u56fa\u6709\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u89e3\u91ca\u5fe0\u5b9e\u6027\u7684\u5b9a\u4e49\uff0c\u5e76\u5b9a\u4e49\u673a\u5236\u53ef\u89e3\u91ca\u6027\u4e3a\u6a21\u578b\u7ea7\u3001\u672c\u4f53\u6027\u3001\u56e0\u679c\u673a\u5236\u6027\u4e14\u53ef\u8bc1\u4f2a\u7684\u89e3\u91ca\u3002", "result": "\u786e\u7acb\u4e86\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7684\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u4e86\u89e3\u91ca\u4e50\u89c2\u4e3b\u4e49\u539f\u5219\u4f5c\u4e3a\u5176\u6210\u529f\u7684\u524d\u63d0\u6761\u4ef6\u3002", "conclusion": "\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u662f\u4e00\u79cd\u6709\u539f\u5219\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u63d0\u53d6\u548c\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u9690\u542b\u89e3\u91ca\uff0c\u4f46\u5176\u6210\u529f\u4f9d\u8d56\u4e8e\u89e3\u91ca\u4e50\u89c2\u4e3b\u4e49\u539f\u5219\u3002"}}
{"id": "2505.01057", "pdf": "https://arxiv.org/pdf/2505.01057", "abs": "https://arxiv.org/abs/2505.01057", "authors": ["Boris Kriuk", "Matey Yordanov"], "title": "GeloVec: Higher Dimensional Geometric Smoothing for Coherent Visual Feature Extraction in Image Segmentation", "categories": ["cs.CV"], "comment": "13 pages, 3 figures, 3 tables", "summary": "This paper introduces GeloVec, a new CNN-based attention smoothing framework\nfor semantic segmentation that addresses critical limitations in conventional\napproaches. While existing attention-backed segmentation methods suffer from\nboundary instability and contextual discontinuities during feature mapping, our\nframework implements a higher-dimensional geometric smoothing method to\nestablish a robust manifold relationships between visually coherent regions.\nGeloVec combines modified Chebyshev distance metrics with multispatial\ntransformations to enhance segmentation accuracy through stabilized feature\nextraction. The core innovation lies in the adaptive sampling weights system\nthat calculates geometric distances in n-dimensional feature space, achieving\nsuperior edge preservation while maintaining intra-class homogeneity. The\nmultispatial transformation matrix incorporates tensorial projections with\northogonal basis vectors, creating more discriminative feature representations\nwithout sacrificing computational efficiency. Experimental validation across\nmultiple benchmark datasets demonstrates significant improvements in\nsegmentation performance, with mean Intersection over Union (mIoU) gains of\n2.1%, 2.7%, and 2.4% on Caltech Birds-200, LSDSC, and FSSD datasets\nrespectively compared to state-of-the-art methods. GeloVec's mathematical\nfoundation in Riemannian geometry provides theoretical guarantees on\nsegmentation stability. Importantly, our framework maintains computational\nefficiency through parallelized implementation of geodesic transformations and\nexhibits strong generalization capabilities across disciplines due to the\nabsence of information loss during transformations.", "AI": {"tldr": "GeloVec\u662f\u4e00\u79cd\u57fa\u4e8eCNN\u7684\u6ce8\u610f\u529b\u5e73\u6ed1\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u7ef4\u51e0\u4f55\u5e73\u6ed1\u65b9\u6cd5\u89e3\u51b3\u4f20\u7edf\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u8fb9\u754c\u4e0d\u7a33\u5b9a\u548c\u4e0a\u4e0b\u6587\u4e0d\u8fde\u7eed\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u6ce8\u610f\u529b\u5206\u5272\u65b9\u6cd5\u5b58\u5728\u8fb9\u754c\u4e0d\u7a33\u5b9a\u548c\u4e0a\u4e0b\u6587\u4e0d\u8fde\u7eed\u95ee\u9898\uff0cGeloVec\u65e8\u5728\u901a\u8fc7\u51e0\u4f55\u5e73\u6ed1\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "\u7ed3\u5408\u6539\u8fdb\u7684Chebyshev\u8ddd\u79bb\u5ea6\u91cf\u548c\u591a\u7a7a\u95f4\u53d8\u6362\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u91c7\u6837\u6743\u91cd\u7cfb\u7edf\u5728n\u7ef4\u7279\u5f81\u7a7a\u95f4\u4e2d\u8ba1\u7b97\u51e0\u4f55\u8ddd\u79bb\uff0c\u540c\u65f6\u5229\u7528\u5f20\u91cf\u6295\u5f71\u548c\u6b63\u4ea4\u57fa\u5411\u91cf\u589e\u5f3a\u7279\u5f81\u8868\u793a\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cmIoU\u5206\u522b\u63d0\u53472.1%\u30012.7%\u548c2.4%\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u3002", "conclusion": "GeloVec\u901a\u8fc7\u51e0\u4f55\u5e73\u6ed1\u548c\u9ad8\u6548\u8ba1\u7b97\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u4e49\u5206\u5272\u7684\u7a33\u5b9a\u6027\u548c\u6027\u80fd\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u548c\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.00831", "pdf": "https://arxiv.org/pdf/2505.00831", "abs": "https://arxiv.org/abs/2505.00831", "authors": ["Quang P. M. Pham", "Khoi T. N. Nguyen", "Nhi H. Doan", "Cuong A. Pham", "Kentaro Inui", "Dezhen Song"], "title": "SmallPlan: Leverage Small Language Models for Sequential Path Planning with Simulation-Powered, LLM-Guided Distillation", "categories": ["cs.RO", "cs.CL"], "comment": "Paper is under review", "summary": "Efficient path planning in robotics, particularly within large-scale, dynamic\nenvironments, remains a significant hurdle. While Large Language Models (LLMs)\noffer strong reasoning capabilities, their high computational cost and limited\nadaptability in dynamic scenarios hinder real-time deployment on edge devices.\nWe present SmallPlan -- a novel framework leveraging LLMs as teacher models to\ntrain lightweight Small Language Models (SLMs) for high-level path planning\ntasks. In SmallPlan, the SLMs provide optimal action sequences to navigate\nacross scene graphs that compactly represent full-scaled 3D scenes. The SLMs\nare trained in a simulation-powered, interleaved manner with LLM-guided\nsupervised fine-tuning (SFT) and reinforcement learning (RL). This strategy not\nonly enables SLMs to successfully complete navigation tasks but also makes them\naware of important factors like travel distance and number of trials. Through\nexperiments, we demonstrate that the fine-tuned SLMs perform competitively with\nlarger models like GPT-4o on sequential path planning, without suffering from\nhallucination and overfitting. SmallPlan is resource-efficient, making it\nwell-suited for edge-device deployment and advancing practical autonomous\nrobotics.", "AI": {"tldr": "SmallPlan\u5229\u7528LLMs\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\u8bad\u7ec3\u8f7b\u91cf\u7ea7SLMs\uff0c\u7528\u4e8e\u9ad8\u6548\u8def\u5f84\u89c4\u5212\uff0c\u9002\u5408\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u52a8\u6001\u73af\u5883\u4e2d\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u5b9e\u65f6\u6027\u95ee\u9898\u3002", "method": "\u901a\u8fc7LLM\u5f15\u5bfc\u7684\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3SLMs\uff0c\u751f\u6210\u6700\u4f18\u52a8\u4f5c\u5e8f\u5217\u3002", "result": "SLMs\u5728\u8def\u5f84\u89c4\u5212\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0eGPT-4o\u76f8\u5f53\uff0c\u4e14\u907f\u514d\u4e86\u5e7b\u89c9\u548c\u8fc7\u62df\u5408\u3002", "conclusion": "SmallPlan\u8d44\u6e90\u9ad8\u6548\uff0c\u9002\u5408\u8fb9\u7f18\u8bbe\u5907\uff0c\u63a8\u52a8\u81ea\u4e3b\u673a\u5668\u4eba\u53d1\u5c55\u3002"}}
{"id": "2505.01064", "pdf": "https://arxiv.org/pdf/2505.01064", "abs": "https://arxiv.org/abs/2505.01064", "authors": ["Hari Chandana Kuchibhotla", "Sai Srinivas Kancheti", "Abbavaram Gowtham Reddy", "Vineeth N Balasubramanian"], "title": "Efficient Vocabulary-Free Fine-Grained Visual Recognition in the Age of Multimodal LLMs", "categories": ["cs.CV", "cs.LG"], "comment": "preprint; earlier version accepted at NeurIPS 2024 Workshop on\n  Adaptive Foundation Models", "summary": "Fine-grained Visual Recognition (FGVR) involves distinguishing between\nvisually similar categories, which is inherently challenging due to subtle\ninter-class differences and the need for large, expert-annotated datasets. In\ndomains like medical imaging, such curated datasets are unavailable due to\nissues like privacy concerns and high annotation costs. In such scenarios\nlacking labeled data, an FGVR model cannot rely on a predefined set of training\nlabels, and hence has an unconstrained output space for predictions. We refer\nto this task as Vocabulary-Free FGVR (VF-FGVR), where a model must predict\nlabels from an unconstrained output space without prior label information.\nWhile recent Multimodal Large Language Models (MLLMs) show potential for\nVF-FGVR, querying these models for each test input is impractical because of\nhigh costs and prohibitive inference times. To address these limitations, we\nintroduce \\textbf{Nea}rest-Neighbor Label \\textbf{R}efinement (NeaR), a novel\napproach that fine-tunes a downstream CLIP model using labels generated by an\nMLLM. Our approach constructs a weakly supervised dataset from a small,\nunlabeled training set, leveraging MLLMs for label generation. NeaR is designed\nto handle the noise, stochasticity, and open-endedness inherent in labels\ngenerated by MLLMs, and establishes a new benchmark for efficient VF-FGVR.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNeaR\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u65e0\u6807\u7b7e\u6570\u636e\u4e0b\u7684\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8bc6\u522b\uff08VF-FGVR\uff09\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u751f\u6210\u6807\u7b7e\u5e76\u5fae\u8c03CLIP\u6a21\u578b\u3002", "motivation": "\u5728\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u7684\u9886\u57df\uff08\u5982\u533b\u5b66\u5f71\u50cf\uff09\uff0c\u4f20\u7edf\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8bc6\u522b\u65b9\u6cd5\u65e0\u6cd5\u5e94\u7528\uff0c\u800c\u76f4\u63a5\u4f7f\u7528MLLM\u6210\u672c\u9ad8\u4e14\u6548\u7387\u4f4e\u3002", "method": "NeaR\u65b9\u6cd5\u5229\u7528MLLM\u4e3a\u5c11\u91cf\u672a\u6807\u6ce8\u8bad\u7ec3\u6570\u636e\u751f\u6210\u6807\u7b7e\uff0c\u6784\u5efa\u5f31\u76d1\u7763\u6570\u636e\u96c6\uff0c\u5e76\u5fae\u8c03\u4e0b\u6e38CLIP\u6a21\u578b\u4ee5\u5904\u7406\u6807\u7b7e\u566a\u58f0\u548c\u5f00\u653e\u6027\u3002", "result": "NeaR\u4e3a\u9ad8\u6548VF-FGVR\u5efa\u7acb\u4e86\u65b0\u57fa\u51c6\uff0c\u89e3\u51b3\u4e86MLLM\u76f4\u63a5\u4f7f\u7528\u7684\u9ad8\u6210\u672c\u548c\u63a8\u7406\u65f6\u95f4\u95ee\u9898\u3002", "conclusion": "NeaR\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u65e0\u6807\u7b7e\u6570\u636e\u7684\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8bc6\u522b\u4efb\u52a1\u3002"}}
{"id": "2505.00903", "pdf": "https://arxiv.org/pdf/2505.00903", "abs": "https://arxiv.org/abs/2505.00903", "authors": ["Daria Gitman", "Igor Gitman", "Evelina Bakhturina"], "title": "NeMo-Inspector: A Visualization Tool for LLM Generation Analysis", "categories": ["cs.LG", "cs.CL"], "comment": "Presented at the NAACL 2025 conference", "summary": "Adapting Large Language Models (LLMs) to novel tasks and enhancing their\noverall capabilities often requires large, high-quality training datasets.\nSynthetic data, generated at scale, serves a valuable alternative when\nreal-world data is scarce or difficult to obtain. However, ensuring the quality\nof synthetic datasets is challenging, as developers must manually inspect and\nrefine numerous samples to identify errors and areas for improvement. This\nprocess is time-consuming and requires specialized tools. We introduce\nNeMo-Inspector, an open-source tool designed to simplify the analysis of\nsynthetic datasets with integrated inference capabilities. We demonstrate its\neffectiveness through two real-world cases. Analysis and cleaning of the\nsynthetically generated GSM-Plus dataset with NeMo-Inspector led to a\nsignificant decrease in low-quality samples from 46.99% to 19.51%. The tool\nalso helped identify and correct generation errors in OpenMath models,\nimproving accuracy by 1.92% on the MATH dataset and by 4.17% on the GSM8K\ndataset for a Meta-Llama-3-8B model fine-tuned on synthetic data generated from\nNemotron-4-340B.", "AI": {"tldr": "NeMo-Inspector\u662f\u4e00\u4e2a\u5f00\u6e90\u5de5\u5177\uff0c\u7528\u4e8e\u7b80\u5316\u5408\u6210\u6570\u636e\u96c6\u7684\u5206\u6790\u548c\u6e05\u7406\uff0c\u663e\u8457\u63d0\u5347\u6570\u636e\u8d28\u91cf\u548c\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u5408\u6210\u6570\u636e\u8d28\u91cf\u96be\u4ee5\u4fdd\u8bc1\u4e14\u624b\u52a8\u68c0\u67e5\u8017\u65f6\u8d39\u529b\uff0c\u9700\u8981\u4e13\u7528\u5de5\u5177\u6765\u9ad8\u6548\u5206\u6790\u548c\u6539\u8fdb\u6570\u636e\u96c6\u3002", "method": "\u5f00\u53d1\u4e86NeMo-Inspector\u5de5\u5177\uff0c\u5177\u5907\u96c6\u6210\u63a8\u7406\u80fd\u529b\uff0c\u7528\u4e8e\u5206\u6790\u548c\u6e05\u7406\u5408\u6210\u6570\u636e\u96c6\u3002", "result": "\u4f7f\u7528\u8be5\u5de5\u5177\u540e\uff0cGSM-Plus\u6570\u636e\u96c6\u7684\u4f4e\u8d28\u91cf\u6837\u672c\u4ece46.99%\u964d\u81f319.51%\uff0cOpenMath\u6a21\u578b\u7684\u51c6\u786e\u6027\u5728MATH\u548cGSM8K\u6570\u636e\u96c6\u4e0a\u5206\u522b\u63d0\u9ad8\u4e861.92%\u548c4.17%\u3002", "conclusion": "NeMo-Inspector\u80fd\u6709\u6548\u63d0\u5347\u5408\u6210\u6570\u636e\u96c6\u8d28\u91cf\uff0c\u8fdb\u800c\u6539\u5584\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2505.01079", "pdf": "https://arxiv.org/pdf/2505.01079", "abs": "https://arxiv.org/abs/2505.01079", "authors": ["Daneul Kim", "Jaeah Lee", "Jaesik Park"], "title": "Improving Editability in Image Generation with Layer-wise Memory", "categories": ["cs.CV", "eess.IV"], "comment": "CVPR 2025. Project page :\n  https://carpedkm.github.io/projects/improving_edit/index.html", "summary": "Most real-world image editing tasks require multiple sequential edits to\nachieve desired results. Current editing approaches, primarily designed for\nsingle-object modifications, struggle with sequential editing: especially with\nmaintaining previous edits along with adapting new objects naturally into the\nexisting content. These limitations significantly hinder complex editing\nscenarios where multiple objects need to be modified while preserving their\ncontextual relationships. We address this fundamental challenge through two key\nproposals: enabling rough mask inputs that preserve existing content while\nnaturally integrating new elements and supporting consistent editing across\nmultiple modifications. Our framework achieves this through layer-wise memory,\nwhich stores latent representations and prompt embeddings from previous edits.\nWe propose Background Consistency Guidance that leverages memorized latents to\nmaintain scene coherence and Multi-Query Disentanglement in cross-attention\nthat ensures natural adaptation to existing content. To evaluate our method, we\npresent a new benchmark dataset incorporating semantic alignment metrics and\ninteractive editing scenarios. Through comprehensive experiments, we\ndemonstrate superior performance in iterative image editing tasks with minimal\nuser effort, requiring only rough masks while maintaining high-quality results\nthroughout multiple editing steps.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u652f\u6301\u591a\u6b65\u56fe\u50cf\u7f16\u8f91\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c42\u8bb0\u5fc6\u548c\u4e00\u81f4\u6027\u5f15\u5bfc\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u8fde\u7eed\u7f16\u8f91\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5355\u5bf9\u8c61\u4fee\u6539\uff0c\u96be\u4ee5\u5904\u7406\u591a\u6b65\u7f16\u8f91\u4efb\u52a1\uff0c\u5c24\u5176\u662f\u4fdd\u6301\u5148\u524d\u7f16\u8f91\u5185\u5bb9\u5e76\u81ea\u7136\u878d\u5165\u65b0\u5bf9\u8c61\u3002", "method": "\u63d0\u51fa\u5c42\u8bb0\u5fc6\u5b58\u50a8\u6f5c\u5728\u8868\u793a\u548c\u63d0\u793a\u5d4c\u5165\uff0c\u7ed3\u5408\u80cc\u666f\u4e00\u81f4\u6027\u5f15\u5bfc\u548c\u591a\u67e5\u8be2\u89e3\u8026\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u8fed\u4ee3\u7f16\u8f91\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4ec5\u9700\u7c97\u7565\u63a9\u7801\u5373\u53ef\u4fdd\u6301\u9ad8\u8d28\u91cf\u7ed3\u679c\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u591a\u6b65\u56fe\u50cf\u7f16\u8f91\u7684\u6548\u7387\u548c\u6548\u679c\u3002"}}
{"id": "2505.00926", "pdf": "https://arxiv.org/pdf/2505.00926", "abs": "https://arxiv.org/abs/2505.00926", "authors": ["Ruiquan Huang", "Yingbin Liang", "Jing Yang"], "title": "How Transformers Learn Regular Language Recognition: A Theoretical Study on Training Dynamics and Implicit Bias", "categories": ["cs.LG", "cs.CL", "stat.ML"], "comment": "accepted by ICML 2025", "summary": "Language recognition tasks are fundamental in natural language processing\n(NLP) and have been widely used to benchmark the performance of large language\nmodels (LLMs). These tasks also play a crucial role in explaining the working\nmechanisms of transformers. In this work, we focus on two representative tasks\nin the category of regular language recognition, known as `even pairs' and\n`parity check', the aim of which is to determine whether the occurrences of\ncertain subsequences in a given sequence are even. Our goal is to explore how a\none-layer transformer, consisting of an attention layer followed by a linear\nlayer, learns to solve these tasks by theoretically analyzing its training\ndynamics under gradient descent. While even pairs can be solved directly by a\none-layer transformer, parity check need to be solved by integrating\nChain-of-Thought (CoT), either into the inference stage of a transformer\nwell-trained for the even pairs task, or into the training of a one-layer\ntransformer. For both problems, our analysis shows that the joint training of\nattention and linear layers exhibits two distinct phases. In the first phase,\nthe attention layer grows rapidly, mapping data sequences into separable\nvectors. In the second phase, the attention layer becomes stable, while the\nlinear layer grows logarithmically and approaches in direction to a max-margin\nhyperplane that correctly separates the attention layer outputs into positive\nand negative samples, and the loss decreases at a rate of $O(1/t)$. Our\nexperiments validate those theoretical results.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5355\u5c42Transformer\u5728\u89e3\u51b3\u6b63\u5219\u8bed\u8a00\u8bc6\u522b\u4efb\u52a1\uff08\u5982\u2018even pairs\u2019\u548c\u2018parity check\u2019\uff09\u4e2d\u7684\u5b66\u4e60\u52a8\u6001\uff0c\u53d1\u73b0\u5176\u8bad\u7ec3\u8fc7\u7a0b\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u6ce8\u610f\u529b\u5c42\u5feb\u901f\u6620\u5c04\u6570\u636e\uff0c\u7ebf\u6027\u5c42\u9010\u6b65\u5206\u79bb\u6837\u672c\u3002", "motivation": "\u63a2\u7d22Transformer\u5728\u6b63\u5219\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u5b66\u4e60\u673a\u5236\uff0c\u7279\u522b\u662f\u5355\u5c42\u6a21\u578b\u5982\u4f55\u901a\u8fc7\u68af\u5ea6\u4e0b\u964d\u89e3\u51b3\u2018even pairs\u2019\u548c\u2018parity check\u2019\u4efb\u52a1\u3002", "method": "\u7406\u8bba\u5206\u6790\u5355\u5c42Transformer\uff08\u6ce8\u610f\u529b\u5c42+\u7ebf\u6027\u5c42\uff09\u5728\u68af\u5ea6\u4e0b\u964d\u4e0b\u7684\u8bad\u7ec3\u52a8\u6001\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u8bad\u7ec3\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a\u6ce8\u610f\u529b\u5c42\u5feb\u901f\u6620\u5c04\u6570\u636e\uff0c\u7ebf\u6027\u5c42\u9010\u6b65\u5206\u79bb\u6837\u672c\uff1b\u635f\u5931\u4ee5$O(1/t)$\u901f\u7387\u4e0b\u964d\u3002", "conclusion": "\u5355\u5c42Transformer\u80fd\u76f4\u63a5\u89e3\u51b3\u2018even pairs\u2019\uff0c\u800c\u2018parity check\u2019\u9700\u7ed3\u5408Chain-of-Thought\uff1b\u8bad\u7ec3\u52a8\u6001\u63ed\u793a\u4e86\u6a21\u578b\u7684\u5206\u9636\u6bb5\u5b66\u4e60\u884c\u4e3a\u3002"}}
{"id": "2505.01091", "pdf": "https://arxiv.org/pdf/2505.01091", "abs": "https://arxiv.org/abs/2505.01091", "authors": ["Daniele Molino", "Francesco di Feola", "Linlin Shen", "Paolo Soda", "Valerio Guarrasi"], "title": "Any-to-Any Vision-Language Model for Multimodal X-ray Imaging and Radiological Report Generation", "categories": ["cs.CV", "cs.AI"], "comment": "arXiv admin note: substantial text overlap with arXiv:2501.04614", "summary": "Generative models have revolutionized Artificial Intelligence (AI),\nparticularly in multimodal applications. However, adapting these models to the\nmedical domain poses unique challenges due to the complexity of medical data\nand the stringent need for clinical accuracy. In this work, we introduce a\nframework specifically designed for multimodal medical data generation. By\nenabling the generation of multi-view chest X-rays and their associated\nclinical report, it bridges the gap between general-purpose vision-language\nmodels and the specialized requirements of healthcare. Leveraging the MIMIC-CXR\ndataset, the proposed framework shows superior performance in generating\nhigh-fidelity images and semantically coherent reports. Our quantitative\nevaluation reveals significant results in terms of FID and BLEU scores,\nshowcasing the quality of the generated data. Notably, our framework achieves\ncomparable or even superior performance compared to real data on downstream\ndisease classification tasks, underlining its potential as a tool for medical\nresearch and diagnostics. This study highlights the importance of\ndomain-specific adaptations in enhancing the relevance and utility of\ngenerative models for clinical applications, paving the way for future\nadvancements in synthetic multimodal medical data generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u591a\u6a21\u6001\u533b\u5b66\u6570\u636e\u751f\u6210\u7684\u6846\u67b6\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u80f8\u7247\u548c\u4e34\u5e8a\u62a5\u544a\uff0c\u6027\u80fd\u4f18\u4e8e\u901a\u7528\u6a21\u578b\uff0c\u5e76\u5728\u4e0b\u6e38\u75be\u75c5\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u533b\u5b66\u6570\u636e\u590d\u6742\u4e14\u9700\u9ad8\u4e34\u5e8a\u51c6\u786e\u6027\uff0c\u901a\u7528\u751f\u6210\u6a21\u578b\u96be\u4ee5\u6ee1\u8db3\u9700\u6c42\uff0c\u9700\u4e13\u95e8\u8bbe\u8ba1\u6846\u67b6\u3002", "method": "\u57fa\u4e8eMIMIC-CXR\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u591a\u6a21\u6001\u751f\u6210\u6846\u67b6\uff0c\u751f\u6210\u80f8\u7247\u548c\u4e34\u5e8a\u62a5\u544a\u3002", "result": "\u751f\u6210\u6570\u636e\u5728FID\u548cBLEU\u5206\u6570\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u63a5\u8fd1\u6216\u4f18\u4e8e\u771f\u5b9e\u6570\u636e\u3002", "conclusion": "\u9886\u57df\u7279\u5b9a\u9002\u914d\u5bf9\u63d0\u5347\u751f\u6210\u6a21\u578b\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u76f8\u5173\u6027\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2505.00976", "pdf": "https://arxiv.org/pdf/2505.00976", "abs": "https://arxiv.org/abs/2505.00976", "authors": ["Zhiyu Liao", "Kang Chen", "Yuanguo Lin", "Kangkang Li", "Yunxuan Liu", "Hefeng Chen", "Xingwang Huang", "Yuanhui Yu"], "title": "Attack and defense techniques in large language models: A survey and new perspectives", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have become central to numerous natural language\nprocessing tasks, but their vulnerabilities present significant security and\nethical challenges. This systematic survey explores the evolving landscape of\nattack and defense techniques in LLMs. We classify attacks into adversarial\nprompt attack, optimized attacks, model theft, as well as attacks on\napplication of LLMs, detailing their mechanisms and implications. Consequently,\nwe analyze defense strategies, including prevention-based and detection-based\ndefense methods. Although advances have been made, challenges remain to adapt\nto the dynamic threat landscape, balance usability with robustness, and address\nresource constraints in defense implementation. We highlight open problems,\nincluding the need for adaptive scalable defenses, explainable security\ntechniques, and standardized evaluation frameworks. This survey provides\nactionable insights and directions for developing secure and resilient LLMs,\nemphasizing the importance of interdisciplinary collaboration and ethical\nconsiderations to mitigate risks in real-world applications.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8c03\u67e5\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u653b\u51fb\u4e0e\u9632\u5fa1\u6280\u672f\uff0c\u5206\u7c7b\u4e86\u653b\u51fb\u7c7b\u578b\u5e76\u5206\u6790\u4e86\u9632\u5fa1\u7b56\u7565\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u5f53\u524d\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u5b89\u5168\u6f0f\u6d1e\u548c\u4f26\u7406\u95ee\u9898\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u653b\u51fb\u4e0e\u9632\u5fa1\u6280\u672f\u3002", "method": "\u901a\u8fc7\u5206\u7c7b\u653b\u51fb\u7c7b\u578b\uff08\u5982\u5bf9\u6297\u6027\u63d0\u793a\u653b\u51fb\u3001\u4f18\u5316\u653b\u51fb\u3001\u6a21\u578b\u7a83\u53d6\u7b49\uff09\u548c\u9632\u5fa1\u7b56\u7565\uff08\u9884\u9632\u6027\u548c\u68c0\u6d4b\u6027\u65b9\u6cd5\uff09\uff0c\u7cfb\u7edf\u5206\u6790\u4e86LLMs\u7684\u5b89\u5168\u95ee\u9898\u3002", "result": "\u5c3d\u7ba1\u5df2\u6709\u8fdb\u5c55\uff0c\u4f46\u52a8\u6001\u5a01\u80c1\u73af\u5883\u3001\u9632\u5fa1\u5b9e\u65bd\u4e2d\u7684\u8d44\u6e90\u9650\u5236\u7b49\u95ee\u9898\u4ecd\u9700\u89e3\u51b3\u3002", "conclusion": "\u672a\u6765\u9700\u5f00\u53d1\u81ea\u9002\u5e94\u9632\u5fa1\u3001\u53ef\u89e3\u91ca\u5b89\u5168\u6280\u672f\u548c\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u5f3a\u8c03\u8de8\u5b66\u79d1\u5408\u4f5c\u548c\u4f26\u7406\u8003\u91cf\u4ee5\u964d\u4f4e\u5b9e\u9645\u5e94\u7528\u98ce\u9669\u3002"}}
{"id": "2505.01096", "pdf": "https://arxiv.org/pdf/2505.01096", "abs": "https://arxiv.org/abs/2505.01096", "authors": ["Marco Salm\u00e8", "Rosa Sicilia", "Paolo Soda", "Valerio Guarrasi"], "title": "Evaluating Vision Language Model Adaptations for Radiology Report Generation in Low-Resource Languages", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "The integration of artificial intelligence in healthcare has opened new\nhorizons for improving medical diagnostics and patient care. However,\nchallenges persist in developing systems capable of generating accurate and\ncontextually relevant radiology reports, particularly in low-resource\nlanguages. In this study, we present a comprehensive benchmark to evaluate the\nperformance of instruction-tuned Vision-Language Models (VLMs) in the\nspecialized task of radiology report generation across three low-resource\nlanguages: Italian, German, and Spanish. Employing the LLaVA architectural\nframework, we conducted a systematic evaluation of pre-trained models utilizing\ngeneral datasets, domain-specific datasets, and low-resource language-specific\ndatasets. In light of the unavailability of models that possess prior knowledge\nof both the medical domain and low-resource languages, we analyzed various\nadaptations to determine the most effective approach for these contexts. The\nresults revealed that language-specific models substantially outperformed both\ngeneral and domain-specific models in generating radiology reports, emphasizing\nthe critical role of linguistic adaptation. Additionally, models fine-tuned\nwith medical terminology exhibited enhanced performance across all languages\ncompared to models with generic knowledge, highlighting the importance of\ndomain-specific training. We also explored the influence of the temperature\nparameter on the coherence of report generation, providing insights for optimal\nmodel settings. Our findings highlight the importance of tailored language and\ndomain-specific training for improving the quality and accuracy of radiological\nreports in multilingual settings. This research not only advances our\nunderstanding of VLMs adaptability in healthcare but also points to significant\navenues for future investigations into model tuning and language-specific\nadaptations.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u6307\u4ee4\u8c03\u4f18\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u610f\u5927\u5229\u8bed\u3001\u5fb7\u8bed\u3001\u897f\u73ed\u7259\u8bed\uff09\u4e2d\u751f\u6210\u653e\u5c04\u5b66\u62a5\u544a\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u8bed\u8a00\u548c\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u5bf9\u63d0\u5347\u62a5\u544a\u8d28\u91cf\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u751f\u6210\u51c6\u786e\u4e14\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u653e\u5c04\u5b66\u62a5\u544a\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528LLaVA\u67b6\u6784\uff0c\u7cfb\u7edf\u8bc4\u4f30\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u901a\u7528\u3001\u9886\u57df\u7279\u5b9a\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u8bed\u8a00\u7279\u5b9a\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u533b\u5b66\u672f\u8bed\u5fae\u8c03\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u6e29\u5ea6\u53c2\u6570\u5f71\u54cd\u62a5\u544a\u8fde\u8d2f\u6027\u3002", "conclusion": "\u8bed\u8a00\u548c\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u5bf9\u63d0\u5347\u591a\u8bed\u8a00\u653e\u5c04\u5b66\u62a5\u544a\u8d28\u91cf\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u8c03\u4f18\u548c\u8bed\u8a00\u9002\u5e94\u7814\u7a76\u6307\u660e\u65b9\u5411\u3002"}}
{"id": "2505.01007", "pdf": "https://arxiv.org/pdf/2505.01007", "abs": "https://arxiv.org/abs/2505.01007", "authors": ["Ling Tang", "Yuefeng Chen", "Hui Xue", "Quanshi Zhang"], "title": "Towards the Resistance of Neural Network Watermarking to Fine-tuning", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "This paper proves a new watermarking method to embed the ownership\ninformation into a deep neural network (DNN), which is robust to fine-tuning.\nSpecifically, we prove that when the input feature of a convolutional layer\nonly contains low-frequency components, specific frequency components of the\nconvolutional filter will not be changed by gradient descent during the\nfine-tuning process, where we propose a revised Fourier transform to extract\nfrequency components from the convolutional filter. Additionally, we also prove\nthat these frequency components are equivariant to weight scaling and weight\npermutations. In this way, we design a watermark module to encode the watermark\ninformation to specific frequency components in a convolutional filter.\nPreliminary experiments demonstrate the effectiveness of our method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6c34\u5370\u65b9\u6cd5\uff0c\u5c06\u6240\u6709\u6743\u4fe1\u606f\u5d4c\u5165\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u4e2d\uff0c\u5e76\u5bf9\u5fae\u8c03\u5177\u6709\u9c81\u68d2\u6027\u3002", "motivation": "\u4fdd\u62a4DNN\u6a21\u578b\u7684\u6240\u6709\u6743\uff0c\u9632\u6b62\u672a\u7ecf\u6388\u6743\u7684\u5fae\u8c03\u7be1\u6539\u6c34\u5370\u4fe1\u606f\u3002", "method": "\u901a\u8fc7\u6539\u8fdb\u7684\u5085\u91cc\u53f6\u53d8\u6362\u63d0\u53d6\u5377\u79ef\u6ee4\u6ce2\u5668\u7684\u7279\u5b9a\u9891\u7387\u6210\u5206\uff0c\u8bbe\u8ba1\u6c34\u5370\u6a21\u5757\u5c06\u4fe1\u606f\u7f16\u7801\u5230\u8fd9\u4e9b\u6210\u5206\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u80fd\u4fdd\u6301\u6c34\u5370\u4fe1\u606f\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aDNN\u6a21\u578b\u7684\u6240\u6709\u6743\u4fdd\u62a4\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.01104", "pdf": "https://arxiv.org/pdf/2505.01104", "abs": "https://arxiv.org/abs/2505.01104", "authors": ["Do Huu Dat", "Nam Hyeonu", "Po-Yuan Mao", "Tae-Hyun Oh"], "title": "VSC: Visual Search Compositional Text-to-Image Diffusion Model", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image diffusion models have shown impressive capabilities in\ngenerating realistic visuals from natural-language prompts, yet they often\nstruggle with accurately binding attributes to corresponding objects,\nespecially in prompts containing multiple attribute-object pairs. This\nchallenge primarily arises from the limitations of commonly used text encoders,\nsuch as CLIP, which can fail to encode complex linguistic relationships and\nmodifiers effectively. Existing approaches have attempted to mitigate these\nissues through attention map control during inference and the use of layout\ninformation or fine-tuning during training, yet they face performance drops\nwith increased prompt complexity. In this work, we introduce a novel\ncompositional generation method that leverages pairwise image embeddings to\nimprove attribute-object binding. Our approach decomposes complex prompts into\nsub-prompts, generates corresponding images, and computes visual prototypes\nthat fuse with text embeddings to enhance representation. By applying\nsegmentation-based localization training, we address cross-attention\nmisalignment, achieving improved accuracy in binding multiple attributes to\nobjects. Our approaches outperform existing compositional text-to-image\ndiffusion models on the benchmark T2I CompBench, achieving better image\nquality, evaluated by humans, and emerging robustness under scaling number of\nbinding pairs in the prompt.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ec4\u5408\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u6210\u5bf9\u56fe\u50cf\u5d4c\u5165\u6765\u6539\u8fdb\u5c5e\u6027-\u5bf9\u8c61\u7ed1\u5b9a\uff0c\u89e3\u51b3\u4e86\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u591a\u5c5e\u6027-\u5bf9\u8c61\u5bf9\u63d0\u793a\u7684\u7ed1\u5b9a\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u590d\u6742\u63d0\u793a\u4e2d\u96be\u4ee5\u51c6\u786e\u7ed1\u5b9a\u5c5e\u6027\u4e0e\u5bf9\u8c61\uff0c\u4e3b\u8981\u7531\u4e8e\u6587\u672c\u7f16\u7801\u5668\uff08\u5982CLIP\uff09\u7684\u5c40\u9650\u6027\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5206\u89e3\u590d\u6742\u63d0\u793a\u4e3a\u5b50\u63d0\u793a\u3001\u751f\u6210\u5bf9\u5e94\u56fe\u50cf\u3001\u8ba1\u7b97\u89c6\u89c9\u539f\u578b\u5e76\u4e0e\u6587\u672c\u5d4c\u5165\u878d\u5408\uff0c\u540c\u65f6\u91c7\u7528\u57fa\u4e8e\u5206\u5272\u7684\u5b9a\u4f4d\u8bad\u7ec3\u89e3\u51b3\u4ea4\u53c9\u6ce8\u610f\u529b\u9519\u4f4d\u3002", "result": "\u5728T2I CompBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7ec4\u5408\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff0c\u63d0\u5347\u4e86\u56fe\u50cf\u8d28\u91cf\u5e76\u5728\u591a\u7ed1\u5b9a\u5bf9\u60c5\u51b5\u4e0b\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u89c6\u89c9\u539f\u578b\u548c\u5206\u5272\u8bad\u7ec3\u663e\u8457\u6539\u5584\u4e86\u591a\u5c5e\u6027-\u5bf9\u8c61\u7ed1\u5b9a\u7684\u51c6\u786e\u6027\uff0c\u5177\u6709\u66f4\u597d\u7684\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.01109", "pdf": "https://arxiv.org/pdf/2505.01109", "abs": "https://arxiv.org/abs/2505.01109", "authors": ["Ali Mammadov", "Loic Le Folgoc", "Julien Adam", "Anne Buronfosse", "Gilles Hayem", "Guillaume Hocquet", "Pietro Gori"], "title": "Self-Supervision Enhances Instance-based Multiple Instance Learning Methods in Digital Pathology: A Benchmark Study", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted for publication in the Journal of Medical Imaging (SPIE)", "summary": "Multiple Instance Learning (MIL) has emerged as the best solution for Whole\nSlide Image (WSI) classification. It consists of dividing each slide into\npatches, which are treated as a bag of instances labeled with a global label.\nMIL includes two main approaches: instance-based and embedding-based. In the\nformer, each patch is classified independently, and then the patch scores are\naggregated to predict the bag label. In the latter, bag classification is\nperformed after aggregating patch embeddings. Even if instance-based methods\nare naturally more interpretable, embedding-based MILs have usually been\npreferred in the past due to their robustness to poor feature extractors.\nHowever, recently, the quality of feature embeddings has drastically increased\nusing self-supervised learning (SSL). Nevertheless, many authors continue to\nendorse the superiority of embedding-based MIL. To investigate this further, we\nconduct 710 experiments across 4 datasets, comparing 10 MIL strategies, 6\nself-supervised methods with 4 backbones, 4 foundation models, and various\npathology-adapted techniques. Furthermore, we introduce 4 instance-based MIL\nmethods never used before in the pathology domain. Through these extensive\nexperiments, we show that with a good SSL feature extractor, simple\ninstance-based MILs, with very few parameters, obtain similar or better\nperformance than complex, state-of-the-art (SOTA) embedding-based MIL methods,\nsetting new SOTA results on the BRACS and Camelyon16 datasets. Since simple\ninstance-based MIL methods are naturally more interpretable and explainable to\nclinicians, our results suggest that more effort should be put into\nwell-adapted SSL methods for WSI rather than into complex embedding-based MIL\nmethods.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u9ad8\u8d28\u91cf\u81ea\u76d1\u7763\u5b66\u4e60\u7279\u5f81\u63d0\u53d6\u5668\u7684\u652f\u6301\u4e0b\uff0c\u7b80\u5355\u7684\u57fa\u4e8e\u5b9e\u4f8b\u7684\u591a\u5b9e\u4f8b\u5b66\u4e60\u65b9\u6cd5\uff08MIL\uff09\u53ef\u4ee5\u5ab2\u7f8e\u6216\u4f18\u4e8e\u590d\u6742\u7684\u57fa\u4e8e\u5d4c\u5165\u7684MIL\u65b9\u6cd5\uff0c\u4e14\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u63a2\u8ba8\u57fa\u4e8e\u5b9e\u4f8b\u7684MIL\u65b9\u6cd5\u5728\u9ad8\u8d28\u91cf\u7279\u5f81\u63d0\u53d6\u5668\u4e0b\u7684\u8868\u73b0\uff0c\u6311\u6218\u57fa\u4e8e\u5d4c\u5165\u7684MIL\u65b9\u6cd5\u7684\u4f20\u7edf\u4f18\u52bf\u3002", "method": "\u901a\u8fc7710\u4e2a\u5b9e\u9a8c\uff0c\u6bd4\u8f8310\u79cdMIL\u7b56\u7565\u30016\u79cd\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u30014\u79cd\u57fa\u7840\u6a21\u578b\u53ca\u591a\u79cd\u75c5\u7406\u5b66\u9002\u5e94\u6280\u672f\uff0c\u5e76\u5f15\u51654\u79cd\u65b0\u7684\u57fa\u4e8e\u5b9e\u4f8b\u7684MIL\u65b9\u6cd5\u3002", "result": "\u5728BRACS\u548cCamelyon16\u6570\u636e\u96c6\u4e0a\uff0c\u7b80\u5355\u7684\u57fa\u4e8e\u5b9e\u4f8b\u7684MIL\u65b9\u6cd5\u53d6\u5f97\u4e86\u4e0e\u590d\u6742\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u597d\u7684\u6027\u80fd\uff0c\u5e76\u5237\u65b0\u4e86SOTA\u7ed3\u679c\u3002", "conclusion": "\u5efa\u8bae\u66f4\u591a\u5173\u6ce8\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5728WSI\u4e2d\u7684\u5e94\u7528\uff0c\u800c\u975e\u590d\u6742\u7684\u57fa\u4e8e\u5d4c\u5165\u7684MIL\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2505.01372", "pdf": "https://arxiv.org/pdf/2505.01372", "abs": "https://arxiv.org/abs/2505.01372", "authors": ["Kola Ayonrinde", "Louis Jaburi"], "title": "Evaluating Explanations: An Explanatory Virtues Framework for Mechanistic Interpretability -- The Strange Science Part I.ii", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.HC"], "comment": "13 pages (plus appendices), 5 figures", "summary": "Mechanistic Interpretability (MI) aims to understand neural networks through\ncausal explanations. Though MI has many explanation-generating methods,\nprogress has been limited by the lack of a universal approach to evaluating\nexplanations. Here we analyse the fundamental question \"What makes a good\nexplanation?\" We introduce a pluralist Explanatory Virtues Framework drawing on\nfour perspectives from the Philosophy of Science - the Bayesian, Kuhnian,\nDeutschian, and Nomological - to systematically evaluate and improve\nexplanations in MI. We find that Compact Proofs consider many explanatory\nvirtues and are hence a promising approach. Fruitful research directions\nimplied by our framework include (1) clearly defining explanatory simplicity,\n(2) focusing on unifying explanations and (3) deriving universal principles for\nneural networks. Improved MI methods enhance our ability to monitor, predict,\nand steer AI systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u54f2\u5b66\u79d1\u5b66\u89c6\u89d2\u7684\u591a\u5143\u89e3\u91ca\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdb\u795e\u7ecf\u7f51\u7edc\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u5e76\u6307\u51fa\u7d27\u51d1\u8bc1\u660e\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u5411\u3002", "motivation": "\u5f53\u524d\u795e\u7ecf\u7f51\u7edc\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u7f3a\u4e4f\u7edf\u4e00\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u9650\u5236\u4e86\u8fdb\u5c55\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u8d1d\u53f6\u65af\u3001\u5e93\u6069\u3001\u5fb7\u610f\u5fd7\u548c\u89c4\u8303\u56db\u79cd\u54f2\u5b66\u89c6\u89d2\u7684\u591a\u5143\u89e3\u91ca\u6846\u67b6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u548c\u6539\u8fdb\u89e3\u91ca\u3002", "result": "\u7d27\u51d1\u8bc1\u660e\u65b9\u6cd5\u56e0\u5176\u7efc\u5408\u8003\u8651\u591a\u79cd\u89e3\u91ca\u4f18\u70b9\u800c\u88ab\u8ba4\u4e3a\u662f\u6709\u524d\u666f\u7684\u65b9\u5411\u3002", "conclusion": "\u6539\u8fdb\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u76d1\u63a7\u3001\u9884\u6d4b\u548c\u5f15\u5bfcAI\u7cfb\u7edf\u3002"}}
{"id": "2505.01172", "pdf": "https://arxiv.org/pdf/2505.01172", "abs": "https://arxiv.org/abs/2505.01172", "authors": ["Jiangtong Tan", "Hu Yu", "Jie Huang", "Jie Xiao", "Feng Zhao"], "title": "FreePCA: Integrating Consistency Information across Long-short Frames in Training-free Long Video Generation via Principal Component Analysis", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Long video generation involves generating extended videos using models\ntrained on short videos, suffering from distribution shifts due to varying\nframe counts. It necessitates the use of local information from the original\nshort frames to enhance visual and motion quality, and global information from\nthe entire long frames to ensure appearance consistency. Existing training-free\nmethods struggle to effectively integrate the benefits of both, as appearance\nand motion in videos are closely coupled, leading to motion inconsistency and\nvisual quality. In this paper, we reveal that global and local information can\nbe precisely decoupled into consistent appearance and motion intensity\ninformation by applying Principal Component Analysis (PCA), allowing for\nrefined complementary integration of global consistency and local quality. With\nthis insight, we propose FreePCA, a training-free long video generation\nparadigm based on PCA that simultaneously achieves high consistency and\nquality. Concretely, we decouple consistent appearance and motion intensity\nfeatures by measuring cosine similarity in the principal component space.\nCritically, we progressively integrate these features to preserve original\nquality and ensure smooth transitions, while further enhancing consistency by\nreusing the mean statistics of the initial noise. Experiments demonstrate that\nFreePCA can be applied to various video diffusion models without requiring\ntraining, leading to substantial improvements. Code is available at\nhttps://github.com/JosephTiTan/FreePCA.", "AI": {"tldr": "FreePCA\u662f\u4e00\u79cd\u57fa\u4e8ePCA\u7684\u65e0\u8bad\u7ec3\u957f\u89c6\u9891\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u5168\u5c40\u4e00\u81f4\u6027\u548c\u5c40\u90e8\u8d28\u91cf\uff0c\u663e\u8457\u63d0\u5347\u89c6\u9891\u751f\u6210\u6548\u679c\u3002", "motivation": "\u957f\u89c6\u9891\u751f\u6210\u56e0\u5e27\u6570\u53d8\u5316\u5bfc\u81f4\u5206\u5e03\u504f\u79fb\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u517c\u987e\u5168\u5c40\u4e00\u81f4\u6027\u548c\u5c40\u90e8\u8d28\u91cf\u3002", "method": "\u5229\u7528PCA\u5c06\u5168\u5c40\u548c\u5c40\u90e8\u4fe1\u606f\u89e3\u8026\u4e3a\u4e00\u81f4\u5916\u89c2\u548c\u8fd0\u52a8\u5f3a\u5ea6\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u4f59\u5f26\u76f8\u4f3c\u5ea6\u6d4b\u91cf\u548c\u6e10\u8fdb\u5f0f\u6574\u5408\u5b9e\u73b0\u9ad8\u8d28\u91cf\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFreePCA\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5e94\u7528\u4e8e\u591a\u79cd\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u6548\u679c\u3002", "conclusion": "FreePCA\u901a\u8fc7PCA\u89e3\u8026\u548c\u6574\u5408\u5168\u5c40\u4e0e\u5c40\u90e8\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u7684\u957f\u89c6\u9891\u751f\u6210\u3002"}}
{"id": "2505.01182", "pdf": "https://arxiv.org/pdf/2505.01182", "abs": "https://arxiv.org/abs/2505.01182", "authors": ["Ziyan Guo", "Haoxuan Qu", "Hossein Rahmani", "Dewen Soh", "Ping Hu", "Qiuhong Ke", "Jun Liu"], "title": "TSTMotion: Training-free Scene-awarenText-to-motion Generation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ICME2025", "summary": "Text-to-motion generation has recently garnered significant research\ninterest, primarily focusing on generating human motion sequences in blank\nbackgrounds. However, human motions commonly occur within diverse 3D scenes,\nwhich has prompted exploration into scene-aware text-to-motion generation\nmethods. Yet, existing scene-aware methods often rely on large-scale\nground-truth motion sequences in diverse 3D scenes, which poses practical\nchallenges due to the expensive cost. To mitigate this challenge, we are the\nfirst to propose a \\textbf{T}raining-free \\textbf{S}cene-aware\n\\textbf{T}ext-to-\\textbf{Motion} framework, dubbed as \\textbf{TSTMotion}, that\nefficiently empowers pre-trained blank-background motion generators with the\nscene-aware capability. Specifically, conditioned on the given 3D scene and\ntext description, we adopt foundation models together to reason, predict and\nvalidate a scene-aware motion guidance. Then, the motion guidance is\nincorporated into the blank-background motion generators with two\nmodifications, resulting in scene-aware text-driven motion sequences. Extensive\nexperiments demonstrate the efficacy and generalizability of our proposed\nframework. We release our code in \\href{https://tstmotion.github.io/}{Project\nPage}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u573a\u666f\u611f\u77e5\u6587\u672c\u5230\u8fd0\u52a8\u751f\u6210\u6846\u67b6TSTMotion\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u573a\u666f\u4fe1\u606f\u751f\u6210\u7b26\u5408\u573a\u666f\u7684\u8fd0\u52a8\u5e8f\u5217\u3002", "motivation": "\u73b0\u6709\u573a\u666f\u611f\u77e5\u65b9\u6cd5\u4f9d\u8d56\u5927\u89c4\u6a21\u771f\u5b9e\u8fd0\u52a8\u6570\u636e\uff0c\u6210\u672c\u9ad8\u6602\uff0c\u56e0\u6b64\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u63a8\u7406\u3001\u9884\u6d4b\u548c\u9a8c\u8bc1\u573a\u666f\u611f\u77e5\u8fd0\u52a8\u6307\u5bfc\uff0c\u5e76\u5c06\u5176\u878d\u5165\u9884\u8bad\u7ec3\u7684\u8fd0\u52a8\u751f\u6210\u5668\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u6846\u67b6\u6709\u6548\u4e14\u5177\u6709\u901a\u7528\u6027\u3002", "conclusion": "TSTMotion\u4e3a\u573a\u666f\u611f\u77e5\u6587\u672c\u5230\u8fd0\u52a8\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\u3002"}}
{"id": "2505.01203", "pdf": "https://arxiv.org/pdf/2505.01203", "abs": "https://arxiv.org/abs/2505.01203", "authors": ["Andrej Macko", "Luk\u00e1\u0161 Gajdo\u0161ech", "Viktor Kocur"], "title": "Efficient Vision-based Vehicle Speed Estimation", "categories": ["cs.CV", "68T45", "I.4.9"], "comment": "Submitted to Journal of Real-Time Image Processing (JRTIP)", "summary": "This paper presents a computationally efficient method for vehicle speed\nestimation from traffic camera footage. Building upon previous work that\nutilizes 3D bounding boxes derived from 2D detections and vanishing point\ngeometry, we introduce several improvements to enhance real-time performance.\nWe evaluate our method in several variants on the BrnoCompSpeed dataset in\nterms of vehicle detection and speed estimation accuracy. Our extensive\nevaluation across various hardware platforms, including edge devices,\ndemonstrates significant gains in frames per second (FPS) compared to the prior\nstate-of-the-art, while maintaining comparable or improved speed estimation\naccuracy. We analyze the trade-off between accuracy and computational cost,\nshowing that smaller models utilizing post-training quantization offer the best\nbalance for real-world deployment. Our best performing model beats previous\nstate-of-the-art in terms of median vehicle speed estimation error (0.58 km/h\nvs. 0.60 km/h), detection precision (91.02% vs 87.08%) and recall (91.14% vs.\n83.32%) while also being 5.5 times faster.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u8ba1\u7b97\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ea4\u901a\u6444\u50cf\u5934\u89c6\u9891\u4f30\u8ba1\u8f66\u8f86\u901f\u5ea6\uff0c\u6539\u8fdb\u4e86\u73b0\u6709\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5b9e\u65f6\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u8f66\u8f86\u901f\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e3D\u8fb9\u754c\u6846\u548c\u6d88\u5931\u70b9\u51e0\u4f55\u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u7ed3\u5408\u540e\u8bad\u7ec3\u91cf\u5316\u6280\u672f\uff0c\u4f18\u5316\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5728BrnoCompSpeed\u6570\u636e\u96c6\u4e0a\uff0c\u901f\u5ea6\u4f30\u8ba1\u8bef\u5dee\uff080.58 km/h\uff09\u3001\u68c0\u6d4b\u7cbe\u5ea6\uff0891.02%\uff09\u548c\u53ec\u56de\u7387\uff0891.14%\uff09\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4e14\u901f\u5ea6\u5feb5.5\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u5408\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2505.01207", "pdf": "https://arxiv.org/pdf/2505.01207", "abs": "https://arxiv.org/abs/2505.01207", "authors": ["Qingyu Xian", "Weiqin Jiao", "Hao Cheng", "Berend Jan van der Zwaag", "Yanqiu Huang"], "title": "T-Graph: Enhancing Sparse-view Camera Pose Estimation by Pairwise Translation Graph", "categories": ["cs.CV"], "comment": null, "summary": "Sparse-view camera pose estimation, which aims to estimate the\n6-Degree-of-Freedom (6-DoF) poses from a limited number of images captured from\ndifferent viewpoints, is a fundamental yet challenging problem in remote\nsensing applications. Existing methods often overlook the translation\ninformation between each pair of viewpoints, leading to suboptimal performance\nin sparse-view scenarios. To address this limitation, we introduce T-Graph, a\nlightweight, plug-and-play module to enhance camera pose estimation in\nsparse-view settings. T-graph takes paired image features as input and maps\nthem through a Multilayer Perceptron (MLP). It then constructs a fully\nconnected translation graph, where nodes represent cameras and edges encode\ntheir translation relationships. It can be seamlessly integrated into existing\nmodels as an additional branch in parallel with the original prediction,\nmaintaining efficiency and ease of use. Furthermore, we introduce two pairwise\ntranslation representations, relative-t and pair-t, formulated under different\nlocal coordinate systems. While relative-t captures intuitive spatial\nrelationships, pair-t offers a rotation-disentangled alternative. The two\nrepresentations contribute to enhanced adaptability across diverse application\nscenarios, further improving our module's robustness. Extensive experiments on\ntwo state-of-the-art methods (RelPose++ and Forge) using public datasets (C03D\nand IMC PhotoTourism) validate both the effectiveness and generalizability of\nT-Graph. The results demonstrate consistent improvements across various\nmetrics, notably camera center accuracy, which improves by 1% to 6% from 2 to 8\nviewpoints.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faT-Graph\u6a21\u5757\uff0c\u901a\u8fc7\u6784\u5efa\u5168\u8fde\u63a5\u7684\u5e73\u79fb\u56fe\u548c\u591a\u5c42\u611f\u77e5\u673a\uff0c\u63d0\u5347\u7a00\u758f\u89c6\u56fe\u4e0b\u7684\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u6027\u80fd\u3002", "motivation": "\u7a00\u758f\u89c6\u56fe\u4e0b\u7684\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u5b58\u5728\u6027\u80fd\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5e38\u5ffd\u7565\u89c6\u89d2\u95f4\u7684\u5e73\u79fb\u4fe1\u606f\u3002", "method": "T-Graph\u901a\u8fc7MLP\u5904\u7406\u6210\u5bf9\u56fe\u50cf\u7279\u5f81\uff0c\u6784\u5efa\u5e73\u79fb\u56fe\uff0c\u5e76\u5f15\u5165\u4e24\u79cd\u5e73\u79fb\u8868\u793a\uff08relative-t\u548cpair-t\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cT-Graph\u5728RelPose++\u548cForge\u65b9\u6cd5\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u76f8\u673a\u4e2d\u5fc3\u7cbe\u5ea6\u63d0\u9ad81%\u81f36%\u3002", "conclusion": "T-Graph\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u5373\u63d2\u5373\u7528\u7684\u6a21\u5757\uff0c\u80fd\u6709\u6548\u63d0\u5347\u7a00\u758f\u89c6\u56fe\u4e0b\u7684\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u6027\u80fd\u3002"}}
{"id": "2505.01212", "pdf": "https://arxiv.org/pdf/2505.01212", "abs": "https://arxiv.org/abs/2505.01212", "authors": ["Kaixuan Zhang", "Hu Wang", "Minxian Li", "Mingwu Ren", "Mao Ye", "Xiatian Zhu"], "title": "High Dynamic Range Novel View Synthesis with Single Exposure", "categories": ["cs.CV", "eess.IV"], "comment": "It has been accepted by ICML 2025", "summary": "High Dynamic Range Novel View Synthesis (HDR-NVS) aims to establish a 3D\nscene HDR model from Low Dynamic Range (LDR) imagery. Typically,\nmultiple-exposure LDR images are employed to capture a wider range of\nbrightness levels in a scene, as a single LDR image cannot represent both the\nbrightest and darkest regions simultaneously. While effective, this\nmultiple-exposure HDR-NVS approach has significant limitations, including\nsusceptibility to motion artifacts (e.g., ghosting and blurring), high capture\nand storage costs. To overcome these challenges, we introduce, for the first\ntime, the single-exposure HDR-NVS problem, where only single exposure LDR\nimages are available during training. We further introduce a novel approach,\nMono-HDR-3D, featuring two dedicated modules formulated by the LDR image\nformation principles, one for converting LDR colors to HDR counterparts, and\nthe other for transforming HDR images to LDR format so that unsupervised\nlearning is enabled in a closed loop. Designed as a meta-algorithm, our\napproach can be seamlessly integrated with existing NVS models. Extensive\nexperiments show that Mono-HDR-3D significantly outperforms previous methods.\nSource code will be released.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5355\u66dd\u5149HDR-NVS\u65b9\u6cd5Mono-HDR-3D\uff0c\u89e3\u51b3\u4e86\u591a\u66dd\u5149HDR-NVS\u7684\u5c40\u9650\u6027\uff0c\u5982\u8fd0\u52a8\u4f2a\u5f71\u548c\u9ad8\u6210\u672c\u3002", "motivation": "\u591a\u66dd\u5149HDR-NVS\u5b58\u5728\u8fd0\u52a8\u4f2a\u5f71\u548c\u9ad8\u6210\u672c\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u4ec5\u4f9d\u8d56\u5355\u66dd\u5149LDR\u56fe\u50cf\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMono-HDR-3D\uff0c\u5305\u542b\u4e24\u4e2a\u6a21\u5757\uff1aLDR\u8f6cHDR\u548cHDR\u8f6cLDR\uff0c\u652f\u6301\u65e0\u76d1\u7763\u95ed\u73af\u5b66\u4e60\uff0c\u5e76\u53ef\u96c6\u6210\u5230\u73b0\u6709NVS\u6a21\u578b\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMono-HDR-3D\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Mono-HDR-3D\u4e3a\u5355\u66dd\u5149HDR-NVS\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2505.01224", "pdf": "https://arxiv.org/pdf/2505.01224", "abs": "https://arxiv.org/abs/2505.01224", "authors": ["Kui Jiang", "Yan Luo", "Junjun Jiang", "Xin Xu", "Fei Ma", "Fei Yu"], "title": "RD-UIE: Relation-Driven State Space Modeling for Underwater Image Enhancement", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Underwater image enhancement (UIE) is a critical preprocessing step for\nmarine vision applications, where wavelength-dependent attenuation causes\nsevere content degradation and color distortion. While recent state space\nmodels like Mamba show potential for long-range dependency modeling, their\nunfolding operations and fixed scan paths on 1D sequences fail to adapt to\nlocal object semantics and global relation modeling, limiting their efficacy in\ncomplex underwater environments. To address this, we enhance conventional Mamba\nwith the sorting-based scanning mechanism that dynamically reorders scanning\nsequences based on statistical distribution of spatial correlation of all\npixels. In this way, it encourages the network to prioritize the most\ninformative components--structural and semantic features. Upon building this\nmechanism, we devise a Visually Self-adaptive State Block (VSSB) that\nharmonizes dynamic sorting of Mamba with input-dependent dynamic convolution,\nenabling coherent integration of global context and local relational cues. This\nexquisite design helps eliminate global focus bias, especially for widely\ndistributed contents, which greatly weakens the statistical frequency. For\nrobust feature extraction and refinement, we design a cross-feature bridge\n(CFB) to adaptively fuse multi-scale representations. These efforts compose the\nnovel relation-driven Mamba framework for effective UIE (RD-UIE). Extensive\nexperiments on underwater enhancement benchmarks demonstrate RD-UIE outperforms\nthe state-of-the-art approach WMamba in both quantitative metrics and visual\nfidelity, averagely achieving 0.55 dB performance gain on the three benchmarks.\nOur code is available at https://github.com/kkoucy/RD-UIE/tree/main", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6392\u5e8f\u626b\u63cf\u673a\u5236\u7684\u6539\u8fdbMamba\u6a21\u578b\uff08RD-UIE\uff09\uff0c\u7528\u4e8e\u6c34\u4e0b\u56fe\u50cf\u589e\u5f3a\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u626b\u63cf\u987a\u5e8f\u548c\u878d\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u6c34\u4e0b\u56fe\u50cf\u56e0\u6ce2\u957f\u8870\u51cf\u5bfc\u81f4\u5185\u5bb9\u9000\u5316\u548c\u989c\u8272\u5931\u771f\uff0c\u73b0\u6709Mamba\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e2d\u56e0\u56fa\u5b9a\u626b\u63cf\u8def\u5f84\u548c\u5c40\u90e8\u8bed\u4e49\u9002\u5e94\u6027\u4e0d\u8db3\u800c\u53d7\u9650\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u6392\u5e8f\u626b\u63cf\u673a\u5236\u548c\u89c6\u89c9\u81ea\u9002\u5e94\u72b6\u6001\u5757\uff08VSSB\uff09\uff0c\u7ed3\u5408\u8de8\u7279\u5f81\u6865\uff08CFB\uff09\u878d\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u6784\u5efaRD-UIE\u6846\u67b6\u3002", "result": "\u5728\u591a\u4e2a\u6c34\u4e0b\u589e\u5f3a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRD-UIE\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5WMamba\uff0c\u5e73\u5747\u6027\u80fd\u63d0\u53470.55 dB\u3002", "conclusion": "RD-UIE\u901a\u8fc7\u52a8\u6001\u626b\u63cf\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6c34\u4e0b\u56fe\u50cf\u589e\u5f3a\u4e2d\u7684\u5168\u5c40\u548c\u5c40\u90e8\u5173\u7cfb\u5efa\u6a21\u95ee\u9898\u3002"}}
{"id": "2505.01225", "pdf": "https://arxiv.org/pdf/2505.01225", "abs": "https://arxiv.org/abs/2505.01225", "authors": ["Keiller Nogueira", "Akram Zaytar", "Wanli Ma", "Ribana Roscher", "Ronny H\u00e4nsch", "Caleb Robinson", "Anthony Ortiz", "Simone Nsutezo", "Rahul Dodhia", "Juan M. Lavista Ferres", "Oktay Karaku\u015f", "Paul L. Rosin"], "title": "Core-Set Selection for Data-efficient Land Cover Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "The increasing accessibility of remotely sensed data and the potential of\nsuch data to inform large-scale decision-making has driven the development of\ndeep learning models for many Earth Observation tasks. Traditionally, such\nmodels must be trained on large datasets. However, the common assumption that\nbroadly larger datasets lead to better outcomes tends to overlook the\ncomplexities of the data distribution, the potential for introducing biases and\nnoise, and the computational resources required for processing and storing vast\ndatasets. Therefore, effective solutions should consider both the quantity and\nquality of data. In this paper, we propose six novel core-set selection methods\nfor selecting important subsets of samples from remote sensing image\nsegmentation datasets that rely on imagery only, labels only, and a combination\nof each. We benchmark these approaches against a random-selection baseline on\nthree commonly used land cover classification datasets: DFC2022, Vaihingen, and\nPotsdam. In each of the datasets, we demonstrate that training on a subset of\nsamples outperforms the random baseline, and some approaches outperform\ntraining on all available data. This result shows the importance and potential\nof data-centric learning for the remote sensing domain. The code is available\nat https://github.com/keillernogueira/data-centric-rs-classification/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u516d\u79cd\u65b0\u9896\u7684\u6838\u5fc3\u96c6\u9009\u62e9\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u9065\u611f\u56fe\u50cf\u5206\u5272\u6570\u636e\u96c6\u4e2d\u9009\u62e9\u91cd\u8981\u5b50\u96c6\uff0c\u5b9e\u9a8c\u8868\u660e\u8fd9\u4e9b\u65b9\u6cd5\u4f18\u4e8e\u968f\u673a\u9009\u62e9\u57fa\u7ebf\uff0c\u751a\u81f3\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4f18\u4e8e\u4f7f\u7528\u5168\u90e8\u6570\u636e\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u5927\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u4f46\u5927\u6570\u636e\u96c6\u53ef\u80fd\u5f15\u5165\u504f\u5dee\u548c\u566a\u58f0\uff0c\u4e14\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u5927\u3002\u56e0\u6b64\uff0c\u9700\u5173\u6ce8\u6570\u636e\u8d28\u91cf\u548c\u6570\u91cf\u3002", "method": "\u63d0\u51fa\u516d\u79cd\u6838\u5fc3\u96c6\u9009\u62e9\u65b9\u6cd5\uff0c\u57fa\u4e8e\u56fe\u50cf\u3001\u6807\u7b7e\u6216\u4e24\u8005\u7ed3\u5408\uff0c\u5e76\u5728\u4e09\u4e2a\u5e38\u7528\u571f\u5730\u5206\u7c7b\u6570\u636e\u96c6\uff08DFC2022\u3001Vaihingen\u3001Potsdam\uff09\u4e0a\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u4f7f\u7528\u5b50\u96c6\u8bad\u7ec3\u4f18\u4e8e\u968f\u673a\u9009\u62e9\u57fa\u7ebf\uff0c\u90e8\u5206\u65b9\u6cd5\u751a\u81f3\u4f18\u4e8e\u4f7f\u7528\u5168\u90e8\u6570\u636e\u3002", "conclusion": "\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u5b66\u4e60\u5728\u9065\u611f\u9886\u57df\u5177\u6709\u91cd\u8981\u6f5c\u529b\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.01235", "pdf": "https://arxiv.org/pdf/2505.01235", "abs": "https://arxiv.org/abs/2505.01235", "authors": ["Youngsik Yun", "Jeongmin Bae", "Hyunseung Son", "Seoha Kim", "Hahyun Lee", "Gun Bang", "Youngjung Uh"], "title": "Compensating Spatiotemporally Inconsistent Observations for Online Dynamic 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": "SIGGRAPH 2025, Project page: https://bbangsik13.github.io/OR2", "summary": "Online reconstruction of dynamic scenes is significant as it enables learning\nscenes from live-streaming video inputs, while existing offline dynamic\nreconstruction methods rely on recorded video inputs. However, previous online\nreconstruction approaches have primarily focused on efficiency and rendering\nquality, overlooking the temporal consistency of their results, which often\ncontain noticeable artifacts in static regions. This paper identifies that\nerrors such as noise in real-world recordings affect temporal inconsistency in\nonline reconstruction. We propose a method that enhances temporal consistency\nin online reconstruction from observations with temporal inconsistency which is\ninevitable in cameras. We show that our method restores the ideal observation\nby subtracting the learned error. We demonstrate that applying our method to\nvarious baselines significantly enhances both temporal consistency and\nrendering quality across datasets. Code, video results, and checkpoints are\navailable at https://bbangsik13.github.io/OR2.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u7ebf\u52a8\u6001\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d88\u9664\u5b66\u4e60\u5230\u7684\u8bef\u5dee\u6765\u63d0\u5347\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u9759\u6001\u533a\u57df\u4e2d\u7684\u660e\u663e\u4f2a\u5f71\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5728\u7ebf\u91cd\u5efa\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6548\u7387\u548c\u6e32\u67d3\u8d28\u91cf\uff0c\u5ffd\u7565\u4e86\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u5bfc\u81f4\u9759\u6001\u533a\u57df\u51fa\u73b0\u4f2a\u5f71\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u5e76\u51cf\u53bb\u8bef\u5dee\u6765\u589e\u5f3a\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u9002\u7528\u4e8e\u5b58\u5728\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\u7684\u76f8\u673a\u89c2\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u6e32\u67d3\u8d28\u91cf\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5728\u7ebf\u52a8\u6001\u573a\u666f\u91cd\u5efa\u4e2d\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6574\u4f53\u8d28\u91cf\u3002"}}
{"id": "2505.01249", "pdf": "https://arxiv.org/pdf/2505.01249", "abs": "https://arxiv.org/abs/2505.01249", "authors": ["Christopher K. I. Williams"], "title": "Fusing Foveal Fixations Using Linear Retinal Transformations and Bayesian Experimental Design", "categories": ["cs.CV", "cs.LG"], "comment": "19 pages, 4 figures", "summary": "Humans (and many vertebrates) face the problem of fusing together multiple\nfixations of a scene in order to obtain a representation of the whole, where\neach fixation uses a high-resolution fovea and decreasing resolution in the\nperiphery. In this paper we explicitly represent the retinal transformation of\na fixation as a linear downsampling of a high-resolution latent image of the\nscene, exploiting the known geometry. This linear transformation allows us to\ncarry out exact inference for the latent variables in factor analysis (FA) and\nmixtures of FA models of the scene. Further, this allows us to formulate and\nsolve the choice of \"where to look next\" as a Bayesian experimental design\nproblem using the Expected Information Gain criterion. Experiments on the Frey\nfaces and MNIST datasets demonstrate the effectiveness of our models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ebf\u6027\u4e0b\u91c7\u6837\u7684\u65b9\u6cd5\uff0c\u6a21\u62df\u4eba\u773c\u6ce8\u89c6\u573a\u666f\u65f6\u7684\u89c6\u7f51\u819c\u53d8\u6362\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u6ce8\u89c6\u70b9\u878d\u5408\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u8d1d\u53f6\u65af\u5b9e\u9a8c\u8bbe\u8ba1\u4f18\u5316\u6ce8\u89c6\u70b9\u9009\u62e9\u3002", "motivation": "\u4eba\u7c7b\u548c\u8bb8\u591a\u810a\u690e\u52a8\u7269\u9700\u8981\u901a\u8fc7\u591a\u4e2a\u6ce8\u89c6\u70b9\u878d\u5408\u573a\u666f\u4fe1\u606f\uff0c\u6bcf\u4e2a\u6ce8\u89c6\u70b9\u4f7f\u7528\u9ad8\u5206\u8fa8\u7387\u4e2d\u592e\u51f9\u548c\u4f4e\u5206\u8fa8\u7387\u5468\u8fb9\u89c6\u89c9\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u5df2\u77e5\u51e0\u4f55\u5173\u7cfb\uff0c\u5c06\u89c6\u7f51\u819c\u53d8\u6362\u5efa\u6a21\u4e3a\u7ebf\u6027\u4e0b\u91c7\u6837\uff0c\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5c06\u89c6\u7f51\u819c\u53d8\u6362\u8868\u793a\u4e3a\u9ad8\u5206\u8fa8\u7387\u6f5c\u5728\u573a\u666f\u56fe\u50cf\u7684\u7ebf\u6027\u4e0b\u91c7\u6837\uff0c\u5229\u7528\u56e0\u5b50\u5206\u6790\uff08FA\uff09\u548cFA\u6df7\u5408\u6a21\u578b\u8fdb\u884c\u7cbe\u786e\u63a8\u65ad\uff0c\u5e76\u901a\u8fc7\u8d1d\u53f6\u65af\u5b9e\u9a8c\u8bbe\u8ba1\uff08\u57fa\u4e8e\u9884\u671f\u4fe1\u606f\u589e\u76ca\uff09\u4f18\u5316\u6ce8\u89c6\u70b9\u9009\u62e9\u3002", "result": "\u5728Frey\u4eba\u8138\u548cMNIST\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u6a21\u62df\u591a\u6ce8\u89c6\u70b9\u878d\u5408\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u8d1d\u53f6\u65af\u4f18\u5316\u5b9e\u73b0\u6ce8\u89c6\u70b9\u9009\u62e9\u7684\u81ea\u52a8\u5316\u3002"}}
{"id": "2505.01257", "pdf": "https://arxiv.org/pdf/2505.01257", "abs": "https://arxiv.org/abs/2505.01257", "authors": ["Vladimir Somers", "Baptiste Standaert", "Victor Joos", "Alexandre Alahi", "Christophe De Vleeschouwer"], "title": "CAMELTrack: Context-Aware Multi-cue ExpLoitation for Online Multi-Object Tracking", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Online multi-object tracking has been recently dominated by\ntracking-by-detection (TbD) methods, where recent advances rely on increasingly\nsophisticated heuristics for tracklet representation, feature fusion, and\nmulti-stage matching. The key strength of TbD lies in its modular design,\nenabling the integration of specialized off-the-shelf models like motion\npredictors and re-identification. However, the extensive usage of human-crafted\nrules for temporal associations makes these methods inherently limited in their\nability to capture the complex interplay between various tracking cues. In this\nwork, we introduce CAMEL, a novel association module for Context-Aware\nMulti-Cue ExpLoitation, that learns resilient association strategies directly\nfrom data, breaking free from hand-crafted heuristics while maintaining TbD's\nvaluable modularity. At its core, CAMEL employs two transformer-based modules\nand relies on a novel association-centric training scheme to effectively model\nthe complex interactions between tracked targets and their various association\ncues. Unlike end-to-end detection-by-tracking approaches, our method remains\nlightweight and fast to train while being able to leverage external\noff-the-shelf models. Our proposed online tracking pipeline, CAMELTrack,\nachieves state-of-the-art performance on multiple tracking benchmarks. Our code\nis available at https://github.com/TrackingLaboratory/CAMELTrack.", "AI": {"tldr": "CAMEL\u662f\u4e00\u79cd\u65b0\u578b\u7684\u5173\u8054\u6a21\u5757\uff0c\u901a\u8fc7\u5b66\u4e60\u6570\u636e\u9a71\u52a8\u7684\u5173\u8054\u7b56\u7565\uff0c\u6446\u8131\u4e86\u4f20\u7edf\u624b\u5de5\u8bbe\u8ba1\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u5757\u5316\u8bbe\u8ba1\u7684\u4f18\u52bf\u3002", "motivation": "\u73b0\u6709\u7684\u8ddf\u8e2a\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u8bbe\u8ba1\u7684\u89c4\u5219\u8fdb\u884c\u65f6\u95f4\u5173\u8054\uff0c\u9650\u5236\u4e86\u5176\u6355\u6349\u590d\u6742\u8ddf\u8e2a\u7ebf\u7d22\u4e4b\u95f4\u4ea4\u4e92\u7684\u80fd\u529b\u3002", "method": "CAMEL\u91c7\u7528\u4e24\u4e2a\u57fa\u4e8eTransformer\u7684\u6a21\u5757\u548c\u4e00\u79cd\u65b0\u7684\u4ee5\u5173\u8054\u4e3a\u4e2d\u5fc3\u7684\u8bad\u7ec3\u65b9\u6848\uff0c\u6709\u6548\u5efa\u6a21\u76ee\u6807\u4e0e\u5176\u5173\u8054\u7ebf\u7d22\u4e4b\u95f4\u7684\u590d\u6742\u4ea4\u4e92\u3002", "result": "CAMELTrack\u5728\u591a\u4e2a\u8ddf\u8e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "CAMEL\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u5feb\u901f\u8bad\u7ec3\u4e14\u80fd\u5229\u7528\u5916\u90e8\u73b0\u6210\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u76ee\u6807\u8ddf\u8e2a\u7684\u6027\u80fd\u3002"}}
{"id": "2505.01267", "pdf": "https://arxiv.org/pdf/2505.01267", "abs": "https://arxiv.org/abs/2505.01267", "authors": ["Gaozheng Pei", "Ke Ma", "Yingfei Sun", "Qianqian Xu", "Qingming Huang"], "title": "Diffusion-based Adversarial Purification from the Perspective of the Frequency Domain", "categories": ["cs.CV"], "comment": null, "summary": "The diffusion-based adversarial purification methods attempt to drown\nadversarial perturbations into a part of isotropic noise through the forward\nprocess, and then recover the clean images through the reverse process. Due to\nthe lack of distribution information about adversarial perturbations in the\npixel domain, it is often unavoidable to damage normal semantics. We turn to\nthe frequency domain perspective, decomposing the image into amplitude spectrum\nand phase spectrum. We find that for both spectra, the damage caused by\nadversarial perturbations tends to increase monotonically with frequency. This\nmeans that we can extract the content and structural information of the\noriginal clean sample from the frequency components that are less damaged.\nMeanwhile, theoretical analysis indicates that existing purification methods\nindiscriminately damage all frequency components, leading to excessive damage\nto the image. Therefore, we propose a purification method that can eliminate\nadversarial perturbations while maximizing the preservation of the content and\nstructure of the original image. Specifically, at each time step during the\nreverse process, for the amplitude spectrum, we replace the low-frequency\ncomponents of the estimated image's amplitude spectrum with the corresponding\nparts of the adversarial image. For the phase spectrum, we project the phase of\nthe estimated image into a designated range of the adversarial image's phase\nspectrum, focusing on the low frequencies. Empirical evidence from extensive\nexperiments demonstrates that our method significantly outperforms most current\ndefense methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9891\u7387\u57df\u7684\u5bf9\u6297\u51c0\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u632f\u5e45\u548c\u76f8\u4f4d\u8c31\uff0c\u9009\u62e9\u6027\u5730\u4fdd\u7559\u4f4e\u9891\u4fe1\u606f\u4ee5\u6d88\u9664\u5bf9\u6297\u6270\u52a8\uff0c\u540c\u65f6\u4fdd\u62a4\u56fe\u50cf\u7684\u539f\u59cb\u5185\u5bb9\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u5bf9\u6297\u51c0\u5316\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u5bf9\u6297\u6270\u52a8\u7684\u5206\u5e03\u4fe1\u606f\uff0c\u5bb9\u6613\u7834\u574f\u56fe\u50cf\u7684\u6b63\u5e38\u8bed\u4e49\u3002\u4f5c\u8005\u4ece\u9891\u7387\u57df\u89c6\u89d2\u51fa\u53d1\uff0c\u53d1\u73b0\u5bf9\u6297\u6270\u52a8\u5bf9\u9ad8\u9891\u90e8\u5206\u7684\u7834\u574f\u66f4\u5927\uff0c\u4ece\u800c\u63d0\u51fa\u9009\u62e9\u6027\u4fdd\u7559\u4f4e\u9891\u4fe1\u606f\u7684\u51c0\u5316\u65b9\u6cd5\u3002", "method": "\u5728\u53cd\u5411\u8fc7\u7a0b\u4e2d\uff0c\u5bf9\u632f\u5e45\u8c31\u7684\u4f4e\u9891\u90e8\u5206\u7528\u5bf9\u6297\u56fe\u50cf\u7684\u5bf9\u5e94\u90e8\u5206\u66ff\u6362\uff0c\u540c\u65f6\u5bf9\u76f8\u4f4d\u8c31\u7684\u4f4e\u9891\u90e8\u5206\u8fdb\u884c\u6295\u5f71\uff0c\u4ee5\u6d88\u9664\u6270\u52a8\u5e76\u4fdd\u7559\u539f\u59cb\u56fe\u50cf\u5185\u5bb9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u6d88\u9664\u5bf9\u6297\u6270\u52a8\u5e76\u4fdd\u62a4\u56fe\u50cf\u5185\u5bb9\u3002", "conclusion": "\u901a\u8fc7\u9891\u7387\u57df\u5206\u6790\u9009\u62e9\u6027\u5904\u7406\u4f4e\u9891\u4fe1\u606f\uff0c\u8be5\u65b9\u6cd5\u5728\u5bf9\u6297\u51c0\u5316\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u548c\u56fe\u50cf\u4fdd\u62a4\u3002"}}
{"id": "2505.01322", "pdf": "https://arxiv.org/pdf/2505.01322", "abs": "https://arxiv.org/abs/2505.01322", "authors": ["Chenxi Li", "Weijie Wang", "Qiang Li", "Bruno Lepri", "Nicu Sebe", "Weizhi Nie"], "title": "FreeInsert: Disentangled Text-Guided Object Insertion in 3D Gaussian Scene without Spatial Priors", "categories": ["cs.CV"], "comment": null, "summary": "Text-driven object insertion in 3D scenes is an emerging task that enables\nintuitive scene editing through natural language. However, existing 2D\nediting-based methods often rely on spatial priors such as 2D masks or 3D\nbounding boxes, and they struggle to ensure consistency of the inserted object.\nThese limitations hinder flexibility and scalability in real-world\napplications. In this paper, we propose FreeInsert, a novel framework that\nleverages foundation models including MLLMs, LGMs, and diffusion models to\ndisentangle object generation from spatial placement. This enables unsupervised\nand flexible object insertion in 3D scenes without spatial priors. FreeInsert\nstarts with an MLLM-based parser that extracts structured semantics, including\nobject types, spatial relationships, and attachment regions, from user\ninstructions. These semantics guide both the reconstruction of the inserted\nobject for 3D consistency and the learning of its degrees of freedom. We\nleverage the spatial reasoning capabilities of MLLMs to initialize object pose\nand scale. A hierarchical, spatially aware refinement stage further integrates\nspatial semantics and MLLM-inferred priors to enhance placement. Finally, the\nappearance of the object is improved using the inserted-object image to enhance\nvisual fidelity. Experimental results demonstrate that FreeInsert achieves\nsemantically coherent, spatially precise, and visually realistic 3D insertions\nwithout relying on spatial priors, offering a user-friendly and flexible\nediting experience.", "AI": {"tldr": "FreeInsert\u662f\u4e00\u79cd\u65e0\u9700\u7a7a\u95f4\u5148\u9a8c\u7684\u6587\u672c\u9a71\u52a83D\u573a\u666f\u5bf9\u8c61\u63d2\u5165\u6846\u67b6\uff0c\u5229\u7528\u57fa\u7840\u6a21\u578b\u5b9e\u73b0\u7075\u6d3b\u4e14\u4e00\u81f4\u7684\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7a7a\u95f4\u5148\u9a8c\uff08\u59822D\u63a9\u7801\u62163D\u8fb9\u754c\u6846\uff09\uff0c\u9650\u5236\u4e86\u7075\u6d3b\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u7ed3\u5408MLLM\u3001LGM\u548c\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u8bed\u4e49\u89e3\u6790\u3001\u7a7a\u95f4\u63a8\u7406\u548c\u5206\u5c42\u7ec6\u5316\u5b9e\u73b0\u5bf9\u8c61\u63d2\u5165\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFreeInsert\u80fd\u5b9e\u73b0\u8bed\u4e49\u4e00\u81f4\u3001\u7a7a\u95f4\u7cbe\u786e\u4e14\u89c6\u89c9\u903c\u771f\u76843D\u63d2\u5165\u3002", "conclusion": "FreeInsert\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u7a7a\u95f4\u5148\u9a8c\u7684\u7528\u6237\u53cb\u597d\u7f16\u8f91\u65b9\u6848\u3002"}}
{"id": "2505.01364", "pdf": "https://arxiv.org/pdf/2505.01364", "abs": "https://arxiv.org/abs/2505.01364", "authors": ["Enamundram Naga Karthik", "Sandrine B\u00e9dard", "Jan Valo\u0161ek", "Christoph S. Aigner", "Elise Bannier", "Josef Bedna\u0159\u00edk", "Virginie Callot", "Anna Combes", "Armin Curt", "Gergely David", "Falk Eippert", "Lynn Farner", "Michael G Fehlings", "Patrick Freund", "Tobias Granberg", "Cristina Granziera", "RHSCIR Network Imaging Group", "Ulrike Horn", "Tom\u00e1\u0161 Hor\u00e1k", "Suzanne Humphreys", "Markus Hupp", "Anne Kerbrat", "Nawal Kinany", "Shannon Kolind", "Petr Kudli\u010dka", "Anna Lebret", "Lisa Eunyoung Lee", "Caterina Mainero", "Allan R. Martin", "Megan McGrath", "Govind Nair", "Kristin P. O'Grady", "Jiwon Oh", "Russell Ouellette", "Nikolai Pfender", "Dario Pfyffer", "Pierre-Fran\u00e7ois Pradat", "Alexandre Prat", "Emanuele Pravat\u00e0", "Daniel S. Reich", "Ilaria Ricchi", "Naama Rotem-Kohavi", "Simon Schading-Sassenhausen", "Maryam Seif", "Andrew Smith", "Seth A Smith", "Grace Sweeney", "Roger Tam", "Anthony Traboulsee", "Constantina Andrada Treaba", "Charidimos Tsagkas", "Zachary Vavasour", "Dimitri Van De Ville", "Kenneth Arnold Weber II", "Sarath Chandar", "Julien Cohen-Adad"], "title": "Monitoring morphometric drift in lifelong learning segmentation of the spinal cord", "categories": ["cs.CV"], "comment": null, "summary": "Morphometric measures derived from spinal cord segmentations can serve as\ndiagnostic and prognostic biomarkers in neurological diseases and injuries\naffecting the spinal cord. While robust, automatic segmentation methods to a\nwide variety of contrasts and pathologies have been developed over the past few\nyears, whether their predictions are stable as the model is updated using new\ndatasets has not been assessed. This is particularly important for deriving\nnormative values from healthy participants. In this study, we present a spinal\ncord segmentation model trained on a multisite $(n=75)$ dataset, including 9\ndifferent MRI contrasts and several spinal cord pathologies. We also introduce\na lifelong learning framework to automatically monitor the morphometric drift\nas the model is updated using additional datasets. The framework is triggered\nby an automatic GitHub Actions workflow every time a new model is created,\nrecording the morphometric values derived from the model's predictions over\ntime. As a real-world application of the proposed framework, we employed the\nspinal cord segmentation model to update a recently-introduced normative\ndatabase of healthy participants containing commonly used measures of spinal\ncord morphometry. Results showed that: (i) our model outperforms previous\nversions and pathology-specific models on challenging lumbar spinal cord cases,\nachieving an average Dice score of $0.95 \\pm 0.03$; (ii) the automatic workflow\nfor monitoring morphometric drift provides a quick feedback loop for developing\nfuture segmentation models; and (iii) the scaling factor required to update the\ndatabase of morphometric measures is nearly constant among slices across the\ngiven vertebral levels, showing minimum drift between the current and previous\nversions of the model monitored by the framework. The model is freely available\nin Spinal Cord Toolbox v7.0.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u810a\u9ad3\u5206\u5272\u6a21\u578b\u53ca\u7ec8\u8eab\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u76d1\u6d4b\u6a21\u578b\u66f4\u65b0\u65f6\u7684\u5f62\u6001\u6d4b\u91cf\u6f02\u79fb\uff0c\u5e76\u5e94\u7528\u4e8e\u66f4\u65b0\u5065\u5eb7\u53c2\u4e0e\u8005\u7684\u89c4\u8303\u6570\u636e\u5e93\u3002", "motivation": "\u8bc4\u4f30\u810a\u9ad3\u5206\u5272\u6a21\u578b\u5728\u66f4\u65b0\u65f6\u7684\u7a33\u5b9a\u6027\uff0c\u7279\u522b\u662f\u7528\u4e8e\u5065\u5eb7\u53c2\u4e0e\u8005\u89c4\u8303\u503c\u7684\u63a8\u5bfc\u3002", "method": "\u8bad\u7ec3\u591a\u7ad9\u70b9\u6570\u636e\u96c6\u4e0a\u7684\u810a\u9ad3\u5206\u5272\u6a21\u578b\uff0c\u5f15\u5165\u7ec8\u8eab\u5b66\u4e60\u6846\u67b6\u76d1\u6d4b\u5f62\u6001\u6d4b\u91cf\u6f02\u79fb\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8GitHub Actions\u5de5\u4f5c\u6d41\u5b9e\u73b0\u3002", "result": "\u6a21\u578b\u5728\u8170\u690e\u810a\u9ad3\u75c5\u4f8b\u4e0a\u8868\u73b0\u4f18\u5f02\uff08Dice\u5f97\u52060.95\u00b10.03\uff09\uff0c\u5f62\u6001\u6d4b\u91cf\u6f02\u79fb\u76d1\u6d4b\u63d0\u4f9b\u4e86\u5feb\u901f\u53cd\u9988\uff0c\u89c4\u8303\u6570\u636e\u5e93\u66f4\u65b0\u6240\u9700\u7684\u7f29\u653e\u56e0\u5b50\u7a33\u5b9a\u3002", "conclusion": "\u6a21\u578b\u548c\u6846\u67b6\u4e3a\u810a\u9ad3\u5f62\u6001\u6d4b\u91cf\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\uff0c\u652f\u6301\u672a\u6765\u5206\u5272\u6a21\u578b\u7684\u5f00\u53d1\u4e0e\u89c4\u8303\u6570\u636e\u5e93\u7684\u66f4\u65b0\u3002"}}
{"id": "2505.01385", "pdf": "https://arxiv.org/pdf/2505.01385", "abs": "https://arxiv.org/abs/2505.01385", "authors": ["Fahong Zhang", "Yilei Shi", "Xiao Xiang Zhu"], "title": "Global Collinearity-aware Polygonizer for Polygonal Building Mapping in Remote Sensing", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "This paper addresses the challenge of mapping polygonal buildings from remote\nsensing images and introduces a novel algorithm, the Global Collinearity-aware\nPolygonizer (GCP). GCP, built upon an instance segmentation framework,\nprocesses binary masks produced by any instance segmentation model. The\nalgorithm begins by collecting polylines sampled along the contours of the\nbinary masks. These polylines undergo a refinement process using a\ntransformer-based regression module to ensure they accurately fit the contours\nof the targeted building instances. Subsequently, a collinearity-aware polygon\nsimplification module simplifies these refined polylines and generate the final\npolygon representation. This module employs dynamic programming technique to\noptimize an objective function that balances the simplicity and fidelity of the\npolygons, achieving globally optimal solutions. Furthermore, the optimized\ncollinearity-aware objective is seamlessly integrated into network training,\nenhancing the cohesiveness of the entire pipeline. The effectiveness of GCP has\nbeen validated on two public benchmarks for polygonal building mapping. Further\nexperiments reveal that applying the collinearity-aware polygon simplification\nmodule to arbitrary polylines, without prior knowledge, enhances accuracy over\ntraditional methods such as the Douglas-Peucker algorithm. This finding\nunderscores the broad applicability of GCP. The code for the proposed method\nwill be made available at https://github.com/zhu-xlab.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGCP\u7684\u65b0\u7b97\u6cd5\uff0c\u7528\u4e8e\u4ece\u9065\u611f\u56fe\u50cf\u4e2d\u6620\u5c04\u591a\u8fb9\u5f62\u5efa\u7b51\uff0c\u901a\u8fc7\u5168\u5c40\u5171\u7ebf\u6027\u611f\u77e5\u7684\u591a\u8fb9\u5f62\u5316\u65b9\u6cd5\u4f18\u5316\u591a\u8fb9\u5f62\u751f\u6210\u3002", "motivation": "\u89e3\u51b3\u4ece\u9065\u611f\u56fe\u50cf\u4e2d\u51c6\u786e\u6620\u5c04\u591a\u8fb9\u5f62\u5efa\u7b51\u7684\u6311\u6218\uff0c\u63d0\u9ad8\u591a\u8fb9\u5f62\u751f\u6210\u7684\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "method": "\u57fa\u4e8e\u5b9e\u4f8b\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u91c7\u6837\u8f6e\u5ed3\u7ebf\u3001Transformer\u56de\u5f52\u6a21\u5757\u4f18\u5316\u8f6e\u5ed3\u62df\u5408\uff0c\u518d\u901a\u8fc7\u5171\u7ebf\u6027\u611f\u77e5\u7684\u591a\u8fb9\u5f62\u7b80\u5316\u6a21\u5757\u751f\u6210\u6700\u7ec8\u591a\u8fb9\u5f62\u3002", "result": "\u5728\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86GCP\u7684\u6709\u6548\u6027\uff0c\u5176\u7b80\u5316\u6a21\u5757\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u5982Douglas-Peucker\u7b97\u6cd5\u3002", "conclusion": "GCP\u7b97\u6cd5\u5728\u5efa\u7b51\u591a\u8fb9\u5f62\u6620\u5c04\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.01390", "pdf": "https://arxiv.org/pdf/2505.01390", "abs": "https://arxiv.org/abs/2505.01390", "authors": ["Alice Natalina Caragliano", "Claudia Tacconi", "Carlo Greco", "Lorenzo Nibid", "Edy Ippolito", "Michele Fiore", "Giuseppe Perrone", "Sara Ramella", "Paolo Soda", "Valerio Guarrasi"], "title": "Multimodal Doctor-in-the-Loop: A Clinically-Guided Explainable Framework for Predicting Pathological Response in Non-Small Cell Lung Cancer", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "arXiv admin note: substantial text overlap with arXiv:2502.17503", "summary": "This study proposes a novel approach combining Multimodal Deep Learning with\nintrinsic eXplainable Artificial Intelligence techniques to predict\npathological response in non-small cell lung cancer patients undergoing\nneoadjuvant therapy. Due to the limitations of existing radiomics and unimodal\ndeep learning approaches, we introduce an intermediate fusion strategy that\nintegrates imaging and clinical data, enabling efficient interaction between\ndata modalities. The proposed Multimodal Doctor-in-the-Loop method further\nenhances clinical relevance by embedding clinicians' domain knowledge directly\ninto the training process, guiding the model's focus gradually from broader\nlung regions to specific lesions. Results demonstrate improved predictive\naccuracy and explainability, providing insights into optimal data integration\nstrategies for clinical applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u548c\u53ef\u89e3\u91caAI\u6280\u672f\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u975e\u5c0f\u7ec6\u80de\u80ba\u764c\u60a3\u8005\u7684\u75c5\u7406\u53cd\u5e94\uff0c\u901a\u8fc7\u4e2d\u95f4\u878d\u5408\u7b56\u7565\u6574\u5408\u5f71\u50cf\u548c\u4e34\u5e8a\u6570\u636e\uff0c\u5e76\u5f15\u5165\u533b\u751f\u53c2\u4e0e\u7684\u8bad\u7ec3\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u6709\u653e\u5c04\u7ec4\u5b66\u548c\u5355\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u6709\u6548\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\uff0c\u4e14\u7f3a\u4e4f\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u91c7\u7528\u4e2d\u95f4\u878d\u5408\u7b56\u7565\u6574\u5408\u5f71\u50cf\u548c\u4e34\u5e8a\u6570\u636e\uff0c\u63d0\u51fa\u591a\u6a21\u6001\u533b\u751f\u53c2\u4e0e\u5faa\u73af\u65b9\u6cd5\uff0c\u5c06\u4e34\u5e8a\u77e5\u8bc6\u5d4c\u5165\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u4e34\u5e8a\u6570\u636e\u6574\u5408\u63d0\u4f9b\u4e86\u4f18\u5316\u7b56\u7565\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u6570\u636e\u6574\u5408\u548c\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u9002\u7528\u4e8e\u4e34\u5e8a\u9884\u6d4b\u4efb\u52a1\u3002"}}
{"id": "2505.01406", "pdf": "https://arxiv.org/pdf/2505.01406", "abs": "https://arxiv.org/abs/2505.01406", "authors": ["Mohammadreza Teymoorianfard", "Shiqing Ma", "Amir Houmansadr"], "title": "VIDSTAMP: A Temporally-Aware Watermark for Ownership and Integrity in Video Diffusion Models", "categories": ["cs.CV", "cs.CR", "cs.LG"], "comment": null, "summary": "The rapid rise of video diffusion models has enabled the generation of highly\nrealistic and temporally coherent videos, raising critical concerns about\ncontent authenticity, provenance, and misuse. Existing watermarking approaches,\nwhether passive, post-hoc, or adapted from image-based techniques, often\nstruggle to withstand video-specific manipulations such as frame insertion,\ndropping, or reordering, and typically degrade visual quality. In this work, we\nintroduce VIDSTAMP, a watermarking framework that embeds per-frame or\nper-segment messages directly into the latent space of temporally-aware video\ndiffusion models. By fine-tuning the model's decoder through a two-stage\npipeline, first on static image datasets to promote spatial message separation,\nand then on synthesized video sequences to restore temporal consistency,\nVIDSTAMP learns to embed high-capacity, flexible watermarks with minimal\nperceptual impact. Leveraging architectural components such as 3D convolutions\nand temporal attention, our method imposes no additional inference cost and\noffers better perceptual quality than prior methods, while maintaining\ncomparable robustness against common distortions and tampering. VIDSTAMP embeds\n768 bits per video (48 bits per frame) with a bit accuracy of 95.0%, achieves a\nlog P-value of -166.65 (lower is better), and maintains a video quality score\nof 0.836, comparable to unwatermarked outputs (0.838) and surpassing prior\nmethods in capacity-quality tradeoffs. Code: Code:\n\\url{https://github.com/SPIN-UMass/VidStamp}", "AI": {"tldr": "VIDSTAMP\u662f\u4e00\u79cd\u65b0\u578b\u89c6\u9891\u6c34\u5370\u6846\u67b6\uff0c\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u5d4c\u5165\u9ad8\u5bb9\u91cf\u6c34\u5370\uff0c\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf\u5e76\u62b5\u6297\u89c6\u9891\u64cd\u4f5c\u3002", "motivation": "\u73b0\u6709\u6c34\u5370\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u89c6\u9891\u7279\u5b9a\u64cd\u4f5c\uff08\u5982\u5e27\u63d2\u5165\u3001\u5220\u9664\u6216\u91cd\u6392\u5e8f\uff09\u4e14\u5f71\u54cd\u89c6\u89c9\u8d28\u91cf\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u5fae\u8c03\u89e3\u7801\u5668\uff0c\u5148\u5728\u9759\u6001\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4ee5\u5206\u79bb\u7a7a\u95f4\u4fe1\u606f\uff0c\u518d\u5728\u5408\u6210\u89c6\u9891\u5e8f\u5217\u4e0a\u6062\u590d\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u5d4c\u5165768\u6bd4\u7279/\u89c6\u9891\uff0848\u6bd4\u7279/\u5e27\uff09\uff0c\u6bd4\u7279\u51c6\u786e\u738795.0%\uff0c\u89c6\u9891\u8d28\u91cf\u63a5\u8fd1\u672a\u52a0\u6c34\u5370\u7248\u672c\u3002", "conclusion": "VIDSTAMP\u5728\u5bb9\u91cf\u3001\u8d28\u91cf\u548c\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u4e0d\u589e\u52a0\u63a8\u7406\u6210\u672c\u3002"}}
{"id": "2505.00735", "pdf": "https://arxiv.org/pdf/2505.00735", "abs": "https://arxiv.org/abs/2505.00735", "authors": ["Jin Hyun Park", "Harine Choi", "Praewa Pitiphat"], "title": "Leveraging Depth and Attention Mechanisms for Improved RGB Image Inpainting", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Existing deep learning-based image inpainting methods typically rely on\nconvolutional networks with RGB images to reconstruct images. However, relying\nexclusively on RGB images may neglect important depth information, which plays\na critical role in understanding the spatial and structural context of a scene.\nJust as human vision leverages stereo cues to perceive depth, incorporating\ndepth maps into the inpainting process can enhance the model's ability to\nreconstruct images with greater accuracy and contextual awareness. In this\npaper, we propose a novel approach that incorporates both RGB and depth images\nfor enhanced image inpainting. Our models employ a dual encoder architecture,\nwhere one encoder processes the RGB image and the other handles the depth\nimage. The encoded features from both encoders are then fused in the decoder\nusing an attention mechanism, effectively integrating the RGB and depth\nrepresentations. We use two different masking strategies, line and square, to\ntest the robustness of the model under different types of occlusions. To\nfurther analyze the effectiveness of our approach, we use Gradient-weighted\nClass Activation Mapping (Grad-CAM) visualizations to examine the regions of\ninterest the model focuses on during inpainting. We show that incorporating\ndepth information alongside the RGB image significantly improves the\nreconstruction quality. Through both qualitative and quantitative comparisons,\nwe demonstrate that the depth-integrated model outperforms the baseline, with\nattention mechanisms further enhancing inpainting performance, as evidenced by\nmultiple evaluation metrics and visualization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408RGB\u548c\u6df1\u5ea6\u56fe\u50cf\u7684\u53cc\u7f16\u7801\u5668\u67b6\u6784\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u4fee\u590d\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u4f9d\u8d56RGB\u56fe\u50cf\uff0c\u5ffd\u7565\u4e86\u6df1\u5ea6\u4fe1\u606f\u5bf9\u7a7a\u95f4\u548c\u7ed3\u6784\u7406\u89e3\u7684\u91cd\u8981\u6027\u3002", "method": "\u91c7\u7528\u53cc\u7f16\u7801\u5668\u5206\u522b\u5904\u7406RGB\u548c\u6df1\u5ea6\u56fe\u50cf\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u4e0d\u540c\u63a9\u7801\u7b56\u7565\u6d4b\u8bd5\u6a21\u578b\u9c81\u68d2\u6027\u3002", "result": "\u7ed3\u5408\u6df1\u5ea6\u4fe1\u606f\u7684\u6a21\u578b\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e2d\u5747\u4f18\u4e8e\u57fa\u7ebf\uff0c\u6ce8\u610f\u529b\u673a\u5236\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "\u6df1\u5ea6\u4fe1\u606f\u7684\u5f15\u5165\u548c\u6ce8\u610f\u529b\u673a\u5236\u7684\u8bbe\u8ba1\u6709\u6548\u63d0\u5347\u4e86\u56fe\u50cf\u4fee\u590d\u7684\u51c6\u786e\u6027\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2505.00737", "pdf": "https://arxiv.org/pdf/2505.00737", "abs": "https://arxiv.org/abs/2505.00737", "authors": ["Jiajia Li", "Xinda Qi", "Seyed Hamidreza Nabaei", "Meiqi Liu", "Dong Chen", "Xin Zhang", "Xunyuan Yin", "Zhaojian Li"], "title": "A Survey on 3D Reconstruction Techniques in Plant Phenotyping: From Classical Methods to Neural Radiance Fields (NeRF), 3D Gaussian Splatting (3DGS), and Beyond", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "17 pages, 7 figures, 4 tables", "summary": "Plant phenotyping plays a pivotal role in understanding plant traits and\ntheir interactions with the environment, making it crucial for advancing\nprecision agriculture and crop improvement. 3D reconstruction technologies have\nemerged as powerful tools for capturing detailed plant morphology and\nstructure, offering significant potential for accurate and automated\nphenotyping. This paper provides a comprehensive review of the 3D\nreconstruction techniques for plant phenotyping, covering classical\nreconstruction methods, emerging Neural Radiance Fields (NeRF), and the novel\n3D Gaussian Splatting (3DGS) approach. Classical methods, which often rely on\nhigh-resolution sensors, are widely adopted due to their simplicity and\nflexibility in representing plant structures. However, they face challenges\nsuch as data density, noise, and scalability. NeRF, a recent advancement,\nenables high-quality, photorealistic 3D reconstructions from sparse viewpoints,\nbut its computational cost and applicability in outdoor environments remain\nareas of active research. The emerging 3DGS technique introduces a new paradigm\nin reconstructing plant structures by representing geometry through Gaussian\nprimitives, offering potential benefits in both efficiency and scalability. We\nreview the methodologies, applications, and performance of these approaches in\nplant phenotyping and discuss their respective strengths, limitations, and\nfuture prospects (https://github.com/JiajiaLi04/3D-Reconstruction-Plants).\nThrough this review, we aim to provide insights into how these diverse 3D\nreconstruction techniques can be effectively leveraged for automated and\nhigh-throughput plant phenotyping, contributing to the next generation of\nagricultural technology.", "AI": {"tldr": "\u7efc\u8ff0\u4e86\u690d\u7269\u8868\u578b\u5206\u6790\u4e2d\u76843D\u91cd\u5efa\u6280\u672f\uff0c\u5305\u62ec\u7ecf\u5178\u65b9\u6cd5\u3001NeRF\u548c3DGS\uff0c\u63a2\u8ba8\u4e86\u5b83\u4eec\u7684\u4f18\u7f3a\u70b9\u53ca\u672a\u6765\u524d\u666f\u3002", "motivation": "\u690d\u7269\u8868\u578b\u5206\u6790\u5bf9\u7cbe\u51c6\u519c\u4e1a\u548c\u4f5c\u7269\u6539\u826f\u81f3\u5173\u91cd\u8981\uff0c3D\u91cd\u5efa\u6280\u672f\u4e3a\u81ea\u52a8\u5316\u8868\u578b\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "method": "\u56de\u987e\u4e86\u7ecf\u5178\u91cd\u5efa\u65b9\u6cd5\u3001NeRF\u548c3DGS\u7684\u6280\u672f\u539f\u7406\u3001\u5e94\u7528\u53ca\u6027\u80fd\u3002", "result": "\u7ecf\u5178\u65b9\u6cd5\u7b80\u5355\u7075\u6d3b\u4f46\u9762\u4e34\u6570\u636e\u5bc6\u5ea6\u548c\u566a\u58f0\u95ee\u9898\uff1bNeRF\u9ad8\u8d28\u91cf\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff1b3DGS\u5728\u6548\u7387\u548c\u6269\u5c55\u6027\u4e0a\u5177\u6f5c\u529b\u3002", "conclusion": "\u4e0d\u540c3D\u91cd\u5efa\u6280\u672f\u5404\u6709\u4f18\u52a3\uff0c\u672a\u6765\u9700\u7ed3\u5408\u5e94\u7528\u573a\u666f\u4f18\u5316\uff0c\u63a8\u52a8\u519c\u4e1a\u6280\u672f\u53d1\u5c55\u3002"}}
{"id": "2505.00747", "pdf": "https://arxiv.org/pdf/2505.00747", "abs": "https://arxiv.org/abs/2505.00747", "authors": ["Zhiying Song", "Tenghui Xie", "Fuxi Wen", "Jun Li"], "title": "Wireless Communication as an Information Sensor for Multi-agent Cooperative Perception: A Survey", "categories": ["cs.OH", "cs.CV", "cs.MA", "cs.RO"], "comment": null, "summary": "Cooperative perception extends the perception capabilities of autonomous\nvehicles by enabling multi-agent information sharing via Vehicle-to-Everything\n(V2X) communication. Unlike traditional onboard sensors, V2X acts as a dynamic\n\"information sensor\" characterized by limited communication, heterogeneity,\nmobility, and scalability. This survey provides a comprehensive review of\nrecent advancements from the perspective of information-centric cooperative\nperception, focusing on three key dimensions: information representation,\ninformation fusion, and large-scale deployment. We categorize information\nrepresentation into data-level, feature-level, and object-level schemes, and\nhighlight emerging methods for reducing data volume and compressing messages\nunder communication constraints. In information fusion, we explore techniques\nunder both ideal and non-ideal conditions, including those addressing\nheterogeneity, localization errors, latency, and packet loss. Finally, we\nsummarize system-level approaches to support scalability in dense traffic\nscenarios. Compared with existing surveys, this paper introduces a new\nperspective by treating V2X communication as an information sensor and\nemphasizing the challenges of deploying cooperative perception in real-world\nintelligent transportation systems.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u534f\u4f5c\u611f\u77e5\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u91cd\u70b9\u8ba8\u8bba\u4e86\u4fe1\u606f\u8868\u793a\u3001\u4fe1\u606f\u878d\u5408\u548c\u5927\u89c4\u6a21\u90e8\u7f72\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u5e76\u63d0\u51fa\u4e86\u5c06V2X\u901a\u4fe1\u89c6\u4e3a\u52a8\u6001\u4fe1\u606f\u4f20\u611f\u5668\u7684\u65b0\u89c6\u89d2\u3002", "motivation": "\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u4fe1\u606f\u5171\u4eab\u6269\u5c55\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u611f\u77e5\u80fd\u529b\uff0c\u89e3\u51b3\u4f20\u7edf\u8f66\u8f7d\u4f20\u611f\u5668\u7684\u5c40\u9650\u6027\u3002", "method": "\u4ece\u4fe1\u606f\u4e2d\u5fc3\u5316\u7684\u89d2\u5ea6\u5206\u7c7b\u4fe1\u606f\u8868\u793a\uff08\u6570\u636e\u7ea7\u3001\u7279\u5f81\u7ea7\u3001\u5bf9\u8c61\u7ea7\uff09\uff0c\u63a2\u8ba8\u4fe1\u606f\u878d\u5408\u6280\u672f\uff08\u7406\u60f3\u4e0e\u975e\u7406\u60f3\u6761\u4ef6\uff09\uff0c\u5e76\u603b\u7ed3\u652f\u6301\u5927\u89c4\u6a21\u90e8\u7f72\u7684\u7cfb\u7edf\u7ea7\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u51cf\u5c11\u6570\u636e\u91cf\u548c\u538b\u7f29\u6d88\u606f\u7684\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5f02\u6784\u6027\u3001\u5b9a\u4f4d\u8bef\u5dee\u3001\u5ef6\u8fdf\u548c\u6570\u636e\u5305\u4e22\u5931\u7b49\u95ee\u9898\uff0c\u5e76\u652f\u6301\u5bc6\u96c6\u4ea4\u901a\u573a\u666f\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u5c06V2X\u901a\u4fe1\u89c6\u4e3a\u4fe1\u606f\u4f20\u611f\u5668\uff0c\u4e3a\u534f\u4f5c\u611f\u77e5\u5728\u73b0\u5b9e\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u548c\u6311\u6218\u3002"}}
{"id": "2505.00935", "pdf": "https://arxiv.org/pdf/2505.00935", "abs": "https://arxiv.org/abs/2505.00935", "authors": ["Roberto Bigazzi"], "title": "Autonomous Embodied Agents: When Robotics Meets Deep Learning Reasoning", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Ph.D. Dissertation", "summary": "The increase in available computing power and the Deep Learning revolution\nhave allowed the exploration of new topics and frontiers in Artificial\nIntelligence research. A new field called Embodied Artificial Intelligence,\nwhich places at the intersection of Computer Vision, Robotics, and Decision\nMaking, has been gaining importance during the last few years, as it aims to\nfoster the development of smart autonomous robots and their deployment in\nsociety. The recent availability of large collections of 3D models for\nphotorealistic robotic simulation has allowed faster and safe training of\nlearning-based agents for millions of frames and a careful evaluation of their\nbehavior before deploying the models on real robotic platforms. These\nintelligent agents are intended to perform a certain task in a possibly unknown\nenvironment. To this end, during the training in simulation, the agents learn\nto perform continuous interactions with the surroundings, such as gathering\ninformation from the environment, encoding and extracting useful cues for the\ntask, and performing actions towards the final goal; where every action of the\nagent influences the interactions. This dissertation follows the complete\ncreation process of embodied agents for indoor environments, from their concept\nto their implementation and deployment. We aim to contribute to research in\nEmbodied AI and autonomous agents, in order to foster future work in this\nfield. We present a detailed analysis of the procedure behind implementing an\nintelligent embodied agent, comprehending a thorough description of the current\nstate-of-the-art in literature, technical explanations of the proposed methods,\nand accurate experimental studies on relevant robotic tasks.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u8ba1\u7b97\u80fd\u529b\u63d0\u5347\u548c\u6df1\u5ea6\u5b66\u4e60\u9769\u547d\u7684\u80cc\u666f\u4e0b\uff0c\u5177\u8eab\u4eba\u5de5\u667a\u80fd\uff08Embodied AI\uff09\u7684\u53d1\u5c55\uff0c\u91cd\u70b9\u7814\u7a76\u4e86\u667a\u80fd\u81ea\u4e3b\u673a\u5668\u4eba\u5728\u5ba4\u5185\u73af\u5883\u4e2d\u7684\u8bad\u7ec3\u4e0e\u90e8\u7f72\u3002", "motivation": "\u968f\u7740\u8ba1\u7b97\u80fd\u529b\u7684\u589e\u5f3a\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u8fdb\u6b65\uff0c\u5177\u8eab\u4eba\u5de5\u667a\u80fd\u6210\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u673a\u5668\u4eba\u548c\u51b3\u7b56\u9886\u57df\u7684\u4ea4\u53c9\u7814\u7a76\u65b9\u5411\uff0c\u76ee\u6807\u662f\u63a8\u52a8\u667a\u80fd\u81ea\u4e3b\u673a\u5668\u4eba\u7684\u53d1\u5c55\u53ca\u5176\u5728\u793e\u4f1a\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5229\u7528\u5927\u89c4\u6a213D\u6a21\u578b\u8fdb\u884c\u903c\u771f\u7684\u673a\u5668\u4eba\u4eff\u771f\uff0c\u901a\u8fc7\u6570\u767e\u4e07\u5e27\u7684\u8bad\u7ec3\uff0c\u4f7f\u5b66\u4e60\u578b\u667a\u80fd\u4f53\u5728\u6a21\u62df\u73af\u5883\u4e2d\u5b66\u4e60\u4e0e\u73af\u5883\u7684\u8fde\u7eed\u4ea4\u4e92\uff0c\u5305\u62ec\u4fe1\u606f\u6536\u96c6\u3001\u4efb\u52a1\u76f8\u5173\u7ebf\u7d22\u63d0\u53d6\u548c\u76ee\u6807\u5bfc\u5411\u884c\u52a8\u3002", "result": "\u8bba\u6587\u8be6\u7ec6\u5206\u6790\u4e86\u667a\u80fd\u5177\u8eab\u4ee3\u7406\u7684\u5b9e\u73b0\u8fc7\u7a0b\uff0c\u5305\u62ec\u6587\u732e\u7efc\u8ff0\u3001\u6280\u672f\u65b9\u6cd5\u89e3\u91ca\u548c\u76f8\u5173\u673a\u5668\u4eba\u4efb\u52a1\u7684\u5b9e\u9a8c\u7814\u7a76\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5177\u8eab\u4eba\u5de5\u667a\u80fd\u548c\u81ea\u4e3b\u4ee3\u7406\u9886\u57df\u63d0\u4f9b\u4e86\u8d21\u732e\uff0c\u65e8\u5728\u63a8\u52a8\u672a\u6765\u5728\u8fd9\u4e00\u9886\u57df\u7684\u5de5\u4f5c\u3002"}}
{"id": "2505.00986", "pdf": "https://arxiv.org/pdf/2505.00986", "abs": "https://arxiv.org/abs/2505.00986", "authors": ["Xiao Ma", "Young D. Kwon", "Dong Ma"], "title": "On-demand Test-time Adaptation for Edge Devices", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Continual Test-time adaptation (CTTA) continuously adapts the deployed model\non every incoming batch of data. While achieving optimal accuracy, existing\nCTTA approaches present poor real-world applicability on resource-constrained\nedge devices, due to the substantial memory overhead and energy consumption. In\nthis work, we first introduce a novel paradigm -- on-demand TTA -- which\ntriggers adaptation only when a significant domain shift is detected. Then, we\npresent OD-TTA, an on-demand TTA framework for accurate and efficient\nadaptation on edge devices. OD-TTA comprises three innovative techniques: 1) a\nlightweight domain shift detection mechanism to activate TTA only when it is\nneeded, drastically reducing the overall computation overhead, 2) a source\ndomain selection module that chooses an appropriate source model for\nadaptation, ensuring high and robust accuracy, 3) a decoupled Batch\nNormalization (BN) update scheme to enable memory-efficient adaptation with\nsmall batch sizes. Extensive experiments show that OD-TTA achieves comparable\nand even better performance while reducing the energy and computation overhead\nremarkably, making TTA a practical reality.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOD-TTA\u7684\u6309\u9700\u6d4b\u8bd5\u65f6\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u57df\u504f\u79fb\u68c0\u6d4b\u3001\u6e90\u57df\u9009\u62e9\u6a21\u5757\u548c\u89e3\u8026BN\u66f4\u65b0\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6027\u80fd\u3002", "motivation": "\u73b0\u6709CTTA\u65b9\u6cd5\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b58\u5728\u5185\u5b58\u548c\u80fd\u8017\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "1) \u8f7b\u91cf\u7ea7\u57df\u504f\u79fb\u68c0\u6d4b\u673a\u5236\uff1b2) \u6e90\u57df\u9009\u62e9\u6a21\u5757\uff1b3) \u89e3\u8026BN\u66f4\u65b0\u65b9\u6848\u3002", "result": "OD-TTA\u5728\u964d\u4f4e\u80fd\u8017\u548c\u8ba1\u7b97\u5f00\u9500\u7684\u540c\u65f6\uff0c\u6027\u80fd\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u3002", "conclusion": "OD-TTA\u4f7fTTA\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u66f4\u5177\u5b9e\u7528\u6027\u3002"}}
{"id": "2505.00995", "pdf": "https://arxiv.org/pdf/2505.00995", "abs": "https://arxiv.org/abs/2505.00995", "authors": ["Taewook Park", "Jinwoo Lee", "Hyondong Oh", "Won-Jae Yun", "Kyu-Wha Lee"], "title": "Optimizing Indoor Farm Monitoring Efficiency Using UAV: Yield Estimation in a GNSS-Denied Cherry Tomato Greenhouse", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted at 2025 ICRA workshop on field robotics", "summary": "As the agricultural workforce declines and labor costs rise, robotic yield\nestimation has become increasingly important. While unmanned ground vehicles\n(UGVs) are commonly used for indoor farm monitoring, their deployment in\ngreenhouses is often constrained by infrastructure limitations, sensor\nplacement challenges, and operational inefficiencies. To address these issues,\nwe develop a lightweight unmanned aerial vehicle (UAV) equipped with an RGB-D\ncamera, a 3D LiDAR, and an IMU sensor. The UAV employs a LiDAR-inertial\nodometry algorithm for precise navigation in GNSS-denied environments and\nutilizes a 3D multi-object tracking algorithm to estimate the count and weight\nof cherry tomatoes. We evaluate the system using two dataset: one from a\nharvesting row and another from a growing row. In the harvesting-row dataset,\nthe proposed system achieves 94.4\\% counting accuracy and 87.5\\% weight\nestimation accuracy within a 13.2-meter flight completed in 10.5 seconds. For\nthe growing-row dataset, which consists of occluded unripened fruits, we\nqualitatively analyze tracking performance and highlight future research\ndirections for improving perception in greenhouse with strong occlusions. Our\nfindings demonstrate the potential of UAVs for efficient robotic yield\nestimation in commercial greenhouses.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65e0\u4eba\u673a\u7cfb\u7edf\uff0c\u7528\u4e8e\u6e29\u5ba4\u4e2d\u7684\u756a\u8304\u4ea7\u91cf\u4f30\u8ba1\uff0c\u89e3\u51b3\u4e86\u5730\u9762\u673a\u5668\u4eba\u5728\u6e29\u5ba4\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u968f\u7740\u519c\u4e1a\u52b3\u52a8\u529b\u51cf\u5c11\u548c\u6210\u672c\u4e0a\u5347\uff0c\u673a\u5668\u4eba\u4ea7\u91cf\u4f30\u8ba1\u53d8\u5f97\u91cd\u8981\u3002\u5730\u9762\u673a\u5668\u4eba\u5728\u6e29\u5ba4\u4e2d\u90e8\u7f72\u53d7\u9650\uff0c\u56e0\u6b64\u9700\u8981\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86\u914d\u5907RGB-D\u76f8\u673a\u30013D LiDAR\u548cIMU\u4f20\u611f\u5668\u7684\u65e0\u4eba\u673a\uff0c\u91c7\u7528LiDAR\u60ef\u6027\u91cc\u7a0b\u8ba1\u7b97\u6cd5\u5bfc\u822a\uff0c\u5e76\u4f7f\u75283D\u591a\u76ee\u6807\u8ddf\u8e2a\u7b97\u6cd5\u4f30\u8ba1\u756a\u8304\u6570\u91cf\u548c\u91cd\u91cf\u3002", "result": "\u5728\u6536\u83b7\u884c\u6570\u636e\u96c6\u4e2d\uff0c\u7cfb\u7edf\u8ba1\u6570\u51c6\u786e\u738794.4%\uff0c\u91cd\u91cf\u4f30\u8ba1\u51c6\u786e\u738787.5%\uff1b\u5728\u751f\u957f\u884c\u6570\u636e\u96c6\u4e2d\uff0c\u5b9a\u6027\u5206\u6790\u4e86\u906e\u6321\u60c5\u51b5\u4e0b\u7684\u8ddf\u8e2a\u6027\u80fd\u3002", "conclusion": "\u65e0\u4eba\u673a\u7cfb\u7edf\u5728\u6e29\u5ba4\u4ea7\u91cf\u4f30\u8ba1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u5f3a\u906e\u6321\u73af\u5883\u4e0b\u7684\u611f\u77e5\u6539\u8fdb\u3002"}}
{"id": "2505.01113", "pdf": "https://arxiv.org/pdf/2505.01113", "abs": "https://arxiv.org/abs/2505.01113", "authors": ["Xun Li", "Jian Yang", "Fenli Jia", "Muyu Wang", "Qi Wu", "Jun Wu", "Jinpeng Mi", "Jilin Hu", "Peidong Liang", "Xuan Tang", "Ke Li", "Xiong You", "Xian Wei"], "title": "NeuroLoc: Encoding Navigation Cells for 6-DOF Camera Localization", "categories": ["cs.RO", "cs.CV", "cs.NE"], "comment": null, "summary": "Recently, camera localization has been widely adopted in autonomous robotic\nnavigation due to its efficiency and convenience. However, autonomous\nnavigation in unknown environments often suffers from scene ambiguity,\nenvironmental disturbances, and dynamic object transformation in camera\nlocalization. To address this problem, inspired by the biological brain\nnavigation mechanism (such as grid cells, place cells, and head direction\ncells), we propose a novel neurobiological camera location method, namely\nNeuroLoc. Firstly, we designed a Hebbian learning module driven by place cells\nto save and replay historical information, aiming to restore the details of\nhistorical representations and solve the issue of scene fuzziness. Secondly, we\nutilized the head direction cell-inspired internal direction learning as\nmulti-head attention embedding to help restore the true orientation in similar\nscenes. Finally, we added a 3D grid center prediction in the pose regression\nmodule to reduce the final wrong prediction. We evaluate the proposed NeuroLoc\non commonly used benchmark indoor and outdoor datasets. The experimental\nresults show that our NeuroLoc can enhance the robustness in complex\nenvironments and improve the performance of pose regression by using only a\nsingle image.", "AI": {"tldr": "NeuroLoc\u662f\u4e00\u79cd\u53d7\u751f\u7269\u8111\u5bfc\u822a\u673a\u5236\u542f\u53d1\u7684\u76f8\u673a\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7Hebbian\u5b66\u4e60\u6a21\u5757\u3001\u65b9\u5411\u5b66\u4e60\u5d4c\u5165\u548c3D\u7f51\u683c\u4e2d\u5fc3\u9884\u6d4b\uff0c\u89e3\u51b3\u4e86\u573a\u666f\u6a21\u7cca\u548c\u65b9\u5411\u6062\u590d\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b9a\u4f4d\u6027\u80fd\u3002", "motivation": "\u81ea\u4e3b\u5bfc\u822a\u5728\u672a\u77e5\u73af\u5883\u4e2d\u5e38\u56e0\u573a\u666f\u6a21\u7cca\u3001\u73af\u5883\u5e72\u6270\u548c\u52a8\u6001\u7269\u4f53\u53d8\u6362\u800c\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u76f8\u673a\u5b9a\u4f4d\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u4f4d\u7f6e\u7ec6\u80de\u7684Hebbian\u5b66\u4e60\u6a21\u5757\u3001\u65b9\u5411\u7ec6\u80de\u542f\u53d1\u7684\u591a\u6ce8\u610f\u529b\u5d4c\u5165\uff0c\u4ee5\u53ca3D\u7f51\u683c\u4e2d\u5fc3\u9884\u6d4b\uff0c\u7ed3\u5408\u5355\u56fe\u50cf\u8fdb\u884c\u59ff\u6001\u56de\u5f52\u3002", "result": "\u5728\u5ba4\u5185\u5916\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cNeuroLoc\u63d0\u5347\u4e86\u590d\u6742\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u6539\u5584\u4e86\u5355\u56fe\u50cf\u7684\u59ff\u6001\u56de\u5f52\u6027\u80fd\u3002", "conclusion": "NeuroLoc\u901a\u8fc7\u4eff\u751f\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u76f8\u673a\u5b9a\u4f4d\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u4e3a\u81ea\u4e3b\u5bfc\u822a\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.01237", "pdf": "https://arxiv.org/pdf/2505.01237", "abs": "https://arxiv.org/abs/2505.01237", "authors": ["Edson Araujo", "Andrew Rouditchenko", "Yuan Gong", "Saurabhchand Bhati", "Samuel Thomas", "Brian Kingsbury", "Leonid Karlinsky", "Rogerio Feris", "James R. Glass"], "title": "CAV-MAE Sync: Improving Contrastive Audio-Visual Mask Autoencoders via Fine-Grained Alignment", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "comment": "To be published at CVPR 2025, code available at\n  https://github.com/edsonroteia/cav-mae-sync", "summary": "Recent advances in audio-visual learning have shown promising results in\nlearning representations across modalities. However, most approaches rely on\nglobal audio representations that fail to capture fine-grained temporal\ncorrespondences with visual frames. Additionally, existing methods often\nstruggle with conflicting optimization objectives when trying to jointly learn\nreconstruction and cross-modal alignment. In this work, we propose CAV-MAE Sync\nas a simple yet effective extension of the original CAV-MAE framework for\nself-supervised audio-visual learning. We address three key challenges: First,\nwe tackle the granularity mismatch between modalities by treating audio as a\ntemporal sequence aligned with video frames, rather than using global\nrepresentations. Second, we resolve conflicting optimization goals by\nseparating contrastive and reconstruction objectives through dedicated global\ntokens. Third, we improve spatial localization by introducing learnable\nregister tokens that reduce semantic load on patch tokens. We evaluate the\nproposed approach on AudioSet, VGG Sound, and the ADE20K Sound dataset on\nzero-shot retrieval, classification and localization tasks demonstrating\nstate-of-the-art performance and outperforming more complex architectures.", "AI": {"tldr": "CAV-MAE Sync\u6269\u5c55\u4e86CAV-MAE\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u5e8f\u5bf9\u9f50\u97f3\u9891\u4e0e\u89c6\u9891\u5e27\u3001\u5206\u79bb\u5bf9\u6bd4\u548c\u91cd\u5efa\u76ee\u6807\u3001\u5f15\u5165\u53ef\u5b66\u4e60\u6ce8\u518c\u4ee4\u724c\uff0c\u89e3\u51b3\u4e86\u97f3\u9891-\u89c6\u89c9\u5b66\u4e60\u4e2d\u7684\u7c92\u5ea6\u4e0d\u5339\u914d\u548c\u4f18\u5316\u51b2\u7a81\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u97f3\u9891-\u89c6\u89c9\u5b66\u4e60\u4e2d\u96be\u4ee5\u6355\u6349\u7ec6\u7c92\u5ea6\u65f6\u5e8f\u5bf9\u5e94\u5173\u7cfb\uff0c\u4e14\u4f18\u5316\u76ee\u6807\u51b2\u7a81\u3002", "method": "\u5c06\u97f3\u9891\u4f5c\u4e3a\u65f6\u5e8f\u5e8f\u5217\u4e0e\u89c6\u9891\u5e27\u5bf9\u9f50\uff0c\u5206\u79bb\u5bf9\u6bd4\u548c\u91cd\u5efa\u76ee\u6807\uff0c\u5f15\u5165\u53ef\u5b66\u4e60\u6ce8\u518c\u4ee4\u724c\u3002", "result": "\u5728AudioSet\u3001VGG Sound\u548cADE20K Sound\u6570\u636e\u96c6\u4e0a\u7684\u96f6\u6837\u672c\u68c0\u7d22\u3001\u5206\u7c7b\u548c\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "CAV-MAE Sync\u901a\u8fc7\u7b80\u5355\u6709\u6548\u7684\u6539\u8fdb\uff0c\u89e3\u51b3\u4e86\u97f3\u9891-\u89c6\u89c9\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5e76\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2505.01239", "pdf": "https://arxiv.org/pdf/2505.01239", "abs": "https://arxiv.org/abs/2505.01239", "authors": ["Elena Mulero Ayll\u00f3n", "Massimiliano Mantegna", "Linlin Shen", "Paolo Soda", "Valerio Guarrasi", "Matteo Tortora"], "title": "Can Foundation Models Really Segment Tumors? A Benchmarking Odyssey in Lung CT Imaging", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Accurate lung tumor segmentation is crucial for improving diagnosis,\ntreatment planning, and patient outcomes in oncology. However, the complexity\nof tumor morphology, size, and location poses significant challenges for\nautomated segmentation. This study presents a comprehensive benchmarking\nanalysis of deep learning-based segmentation models, comparing traditional\narchitectures such as U-Net and DeepLabV3, self-configuring models like nnUNet,\nand foundation models like MedSAM, and MedSAM~2. Evaluating performance across\ntwo lung tumor segmentation datasets, we assess segmentation accuracy and\ncomputational efficiency under various learning paradigms, including few-shot\nlearning and fine-tuning. The results reveal that while traditional models\nstruggle with tumor delineation, foundation models, particularly MedSAM~2,\noutperform them in both accuracy and computational efficiency. These findings\nunderscore the potential of foundation models for lung tumor segmentation,\nhighlighting their applicability in improving clinical workflows and patient\noutcomes.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5bf9\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u80ba\u80bf\u7624\u5206\u5272\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u57fa\u7840\u6a21\u578b\uff08\u5982MedSAM~2\uff09\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\u3002", "motivation": "\u80ba\u80bf\u7624\u5206\u5272\u7684\u51c6\u786e\u6027\u5bf9\u8bca\u65ad\u548c\u6cbb\u7597\u89c4\u5212\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u80bf\u7624\u5f62\u6001\u3001\u5927\u5c0f\u548c\u4f4d\u7f6e\u7684\u590d\u6742\u6027\u7ed9\u81ea\u52a8\u5316\u5206\u5272\u5e26\u6765\u6311\u6218\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4f20\u7edf\u67b6\u6784\uff08\u5982U-Net\u3001DeepLabV3\uff09\u3001\u81ea\u914d\u7f6e\u6a21\u578b\uff08\u5982nnUNet\uff09\u548c\u57fa\u7840\u6a21\u578b\uff08\u5982MedSAM\u3001MedSAM~2\uff09\uff0c\u5e76\u5728\u4e24\u4e2a\u80ba\u80bf\u7624\u5206\u5272\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u6027\u80fd\u3002", "result": "\u57fa\u7840\u6a21\u578b\uff08\u5c24\u5176\u662fMedSAM~2\uff09\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u5728\u80ba\u80bf\u7624\u5206\u5272\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u6539\u5584\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u548c\u60a3\u8005\u9884\u540e\u3002"}}
{"id": "2505.01263", "pdf": "https://arxiv.org/pdf/2505.01263", "abs": "https://arxiv.org/abs/2505.01263", "authors": ["Gaoxiang Cong", "Liang Li", "Jiadong Pan", "Zhedong Zhang", "Amin Beheshti", "Anton van den Hengel", "Yuankai Qi", "Qingming Huang"], "title": "FlowDubber: Movie Dubbing with LLM-based Semantic-aware Learning and Flow Matching based Voice Enhancing", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "comment": null, "summary": "Movie Dubbing aims to convert scripts into speeches that align with the given\nmovie clip in both temporal and emotional aspects while preserving the vocal\ntimbre of a given brief reference audio. Existing methods focus primarily on\nreducing the word error rate while ignoring the importance of lip-sync and\nacoustic quality. To address these issues, we propose a large language model\n(LLM) based flow matching architecture for dubbing, named FlowDubber, which\nachieves high-quality audio-visual sync and pronunciation by incorporating a\nlarge speech language model and dual contrastive aligning while achieving\nbetter acoustic quality via the proposed voice-enhanced flow matching than\nprevious works. First, we introduce Qwen2.5 as the backbone of LLM to learn the\nin-context sequence from movie scripts and reference audio. Then, the proposed\nsemantic-aware learning focuses on capturing LLM semantic knowledge at the\nphoneme level. Next, dual contrastive aligning (DCA) boosts mutual alignment\nwith lip movement, reducing ambiguities where similar phonemes might be\nconfused. Finally, the proposed Flow-based Voice Enhancing (FVE) improves\nacoustic quality in two aspects, which introduces an LLM-based acoustics flow\nmatching guidance to strengthen clarity and uses affine style prior to enhance\nidentity when recovering noise into mel-spectrograms via gradient vector field\nprediction. Extensive experiments demonstrate that our method outperforms\nseveral state-of-the-art methods on two primary benchmarks. The demos are\navailable at\n{\\href{https://galaxycong.github.io/LLM-Flow-Dubber/}{\\textcolor{red}{https://galaxycong.github.io/LLM-Flow-Dubber/}}}.", "AI": {"tldr": "FlowDubber\u662f\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7535\u5f71\u914d\u97f3\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u548c\u53cc\u91cd\u5bf9\u6bd4\u5bf9\u9f50\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u97f3\u9891-\u89c6\u89c9\u540c\u6b65\u548c\u53d1\u97f3\uff0c\u540c\u65f6\u901a\u8fc7\u58f0\u97f3\u589e\u5f3a\u6d41\u5339\u914d\u63d0\u5347\u4e86\u97f3\u8d28\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u964d\u4f4e\u5355\u8bcd\u9519\u8bef\u7387\uff0c\u5ffd\u89c6\u4e86\u5507\u540c\u6b65\u548c\u97f3\u8d28\u7684\u91cd\u8981\u6027\u3002FlowDubber\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528Qwen2.5\u4f5c\u4e3aLLM\u9aa8\u5e72\uff0c\u5b66\u4e60\u7535\u5f71\u811a\u672c\u548c\u53c2\u8003\u97f3\u9891\u7684\u4e0a\u4e0b\u6587\u5e8f\u5217\uff1b\u63d0\u51fa\u8bed\u4e49\u611f\u77e5\u5b66\u4e60\u3001\u53cc\u91cd\u5bf9\u6bd4\u5bf9\u9f50\uff08DCA\uff09\u548c\u57fa\u4e8e\u6d41\u7684\u8bed\u97f3\u589e\u5f3a\uff08FVE\uff09\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FlowDubber\u901a\u8fc7LLM\u548c\u6d41\u5339\u914d\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7535\u5f71\u914d\u97f3\u7684\u540c\u6b65\u6027\u548c\u97f3\u8d28\u3002"}}
{"id": "2505.01313", "pdf": "https://arxiv.org/pdf/2505.01313", "abs": "https://arxiv.org/abs/2505.01313", "authors": ["Shang Wang", "Huanrong Tang", "Jianquan Ouyang"], "title": "A Neural Architecture Search Method using Auxiliary Evaluation Metric based on ResNet Architecture", "categories": ["cs.NE", "cs.CV"], "comment": "GECCO 2023", "summary": "This paper proposes a neural architecture search space using ResNet as a\nframework, with search objectives including parameters for convolution,\npooling, fully connected layers, and connectivity of the residual network. In\naddition to recognition accuracy, this paper uses the loss value on the\nvalidation set as a secondary objective for optimization. The experimental\nresults demonstrate that the search space of this paper together with the\noptimisation approach can find competitive network architectures on the MNIST,\nFashion-MNIST and CIFAR100 datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eResNet\u7684\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u7a7a\u95f4\uff0c\u4f18\u5316\u76ee\u6807\u5305\u62ec\u5377\u79ef\u3001\u6c60\u5316\u548c\u5168\u8fde\u63a5\u5c42\u53c2\u6570\u4ee5\u53ca\u6b8b\u5dee\u7f51\u7edc\u8fde\u63a5\u6027\uff0c\u5e76\u4f7f\u7528\u9a8c\u8bc1\u96c6\u635f\u5931\u503c\u4f5c\u4e3a\u6b21\u8981\u4f18\u5316\u76ee\u6807\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728MNIST\u3001Fashion-MNIST\u548cCIFAR100\u6570\u636e\u96c6\u4e0a\u80fd\u627e\u5230\u6709\u7ade\u4e89\u529b\u7684\u7f51\u7edc\u67b6\u6784\u3002", "motivation": "\u901a\u8fc7\u7ed3\u5408ResNet\u6846\u67b6\u548c\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff0c\u4f18\u5316\u7f51\u7edc\u7ed3\u6784\u548c\u53c2\u6570\uff0c\u4ee5\u63d0\u9ad8\u8bc6\u522b\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eResNet\u7684\u641c\u7d22\u7a7a\u95f4\uff0c\u4f18\u5316\u5377\u79ef\u3001\u6c60\u5316\u3001\u5168\u8fde\u63a5\u5c42\u53c2\u6570\u53ca\u6b8b\u5dee\u7f51\u7edc\u8fde\u63a5\u6027\uff0c\u5e76\u5f15\u5165\u9a8c\u8bc1\u96c6\u635f\u5931\u503c\u4f5c\u4e3a\u6b21\u8981\u4f18\u5316\u76ee\u6807\u3002", "result": "\u5728MNIST\u3001Fashion-MNIST\u548cCIFAR100\u6570\u636e\u96c6\u4e0a\u627e\u5230\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u7f51\u7edc\u67b6\u6784\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408ResNet\u548c\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7f51\u7edc\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9645\u6570\u636e\u96c6\u4e0a\u7684\u6709\u6548\u6027\u3002"}}
