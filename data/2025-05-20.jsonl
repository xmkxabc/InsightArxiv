{"id": "2505.11521", "pdf": "https://arxiv.org/pdf/2505.11521", "abs": "https://arxiv.org/abs/2505.11521", "authors": ["Wang Fang", "Shirin Rahimi", "Olivia Bennett", "Sophie Carter", "Mitra Hassani", "Xu Lan", "Omid Javadi", "Lucas Mitchell"], "title": "Improving Open-Set Semantic Segmentation in 3D Point Clouds by Conditional Channel Capacity Maximization: Preliminary Results", "categories": ["cs.CV", "eess.SP"], "comment": null, "summary": "Point-cloud semantic segmentation underpins a wide range of critical\napplications. Although recent deep architectures and large-scale datasets have\ndriven impressive closed-set performance, these models struggle to recognize or\nproperly segment objects outside their training classes. This gap has sparked\ninterest in Open-Set Semantic Segmentation (O3S), where models must both\ncorrectly label known categories and detect novel, unseen classes. In this\npaper, we propose a plug and play framework for O3S. By modeling the\nsegmentation pipeline as a conditional Markov chain, we derive a novel\nregularizer term dubbed Conditional Channel Capacity Maximization (3CM), that\nmaximizes the mutual information between features and predictions conditioned\non each class. When incorporated into standard loss functions, 3CM encourages\nthe encoder to retain richer, label-dependent features, thereby enhancing the\nnetwork's ability to distinguish and segment previously unseen categories.\nExperimental results demonstrate effectiveness of proposed method on detecting\nunseen objects. We further outline future directions for dynamic open-world\nadaptation and efficient information-theoretic estimation."}
{"id": "2505.11581", "pdf": "https://arxiv.org/pdf/2505.11581", "abs": "https://arxiv.org/abs/2505.11581", "authors": ["Akarsh Kumar", "Jeff Clune", "Joel Lehman", "Kenneth O. Stanley"], "title": "Questioning Representational Optimism in Deep Learning: The Fractured Entangled Representation Hypothesis", "categories": ["cs.CV", "cs.LG", "cs.NE"], "comment": "43 pages, 25 figures", "summary": "Much of the excitement in modern AI is driven by the observation that scaling\nup existing systems leads to better performance. But does better performance\nnecessarily imply better internal representations? While the representational\noptimist assumes it must, this position paper challenges that view. We compare\nneural networks evolved through an open-ended search process to networks\ntrained via conventional stochastic gradient descent (SGD) on the simple task\nof generating a single image. This minimal setup offers a unique advantage:\neach hidden neuron's full functional behavior can be easily visualized as an\nimage, thus revealing how the network's output behavior is internally\nconstructed neuron by neuron. The result is striking: while both networks\nproduce the same output behavior, their internal representations differ\ndramatically. The SGD-trained networks exhibit a form of disorganization that\nwe term fractured entangled representation (FER). Interestingly, the evolved\nnetworks largely lack FER, even approaching a unified factored representation\n(UFR). In large models, FER may be degrading core model capacities like\ngeneralization, creativity, and (continual) learning. Therefore, understanding\nand mitigating FER could be critical to the future of representation learning."}
{"id": "2505.11620", "pdf": "https://arxiv.org/pdf/2505.11620", "abs": "https://arxiv.org/abs/2505.11620", "authors": ["Aaron Wilhelm", "Nils Napp"], "title": "Improved Bag-of-Words Image Retrieval with Geometric Constraints for Ground Texture Localization", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted to ICRA 2025", "summary": "Ground texture localization using a downward-facing camera offers a low-cost,\nhigh-precision localization solution that is robust to dynamic environments and\nrequires no environmental modification. We present a significantly improved\nbag-of-words (BoW) image retrieval system for ground texture localization,\nachieving substantially higher accuracy for global localization and higher\nprecision and recall for loop closure detection in SLAM. Our approach leverages\nan approximate $k$-means (AKM) vocabulary with soft assignment, and exploits\nthe consistent orientation and constant scale constraints inherent to ground\ntexture localization. Identifying the different needs of global localization\nvs. loop closure detection for SLAM, we present both high-accuracy and\nhigh-speed versions of our algorithm. We test the effect of each of our\nproposed improvements through an ablation study and demonstrate our method's\neffectiveness for both global localization and loop closure detection. With\nnumerous ground texture localization systems already using BoW, our method can\nreadily replace other generic BoW systems in their pipeline and immediately\nimprove their results."}
{"id": "2505.11640", "pdf": "https://arxiv.org/pdf/2505.11640", "abs": "https://arxiv.org/abs/2505.11640", "authors": ["Pandula Thennakoon", "Avishka Ranasinghe", "Mario De Silva", "Buwaneka Epakanda", "Roshan Godaliyadda", "Parakrama Ekanayake", "Vijitha Herath"], "title": "BandRC: Band Shifted Raised Cosine Activated Implicit Neural Representations", "categories": ["cs.CV"], "comment": "Submitted as a conference paper to ICCV 2025", "summary": "In recent years, implicit neural representations(INRs) have gained popularity\nin the computer vision community. This is mainly due to the strong performance\nof INRs in many computer vision tasks. These networks can extract a continuous\nsignal representation given a discrete signal representation. In previous\nstudies, it has been repeatedly shown that INR performance has a strong\ncorrelation with the activation functions used in its multilayer perceptrons.\nAlthough numerous activation functions have been proposed that are competitive\nwith one another, they share some common set of challenges such as spectral\nbias(Lack of sensitivity to high-frequency content in signals), limited\nrobustness to signal noise and difficulties in simultaneous capturing both\nlocal and global features. and furthermore, the requirement for manual\nparameter tuning. To address these issues, we introduce a novel activation\nfunction, Band Shifted Raised Cosine Activated Implicit Neural Networks\n\\textbf{(BandRC)} tailored to enhance signal representation capacity further.\nWe also incorporate deep prior knowledge extracted from the signal to adjust\nthe activation functions through a task-specific model. Through a mathematical\nanalysis and a series of experiments which include image reconstruction (with a\n+8.93 dB PSNR improvement over the nearest counterpart), denoising (with a\n+0.46 dB increase in PSNR), super-resolution (with a +1.03 dB improvement over\nthe nearest State-Of-The-Art (SOTA) method for 6X super-resolution),\ninpainting, and 3D shape reconstruction we demonstrate the dominance of BandRC\nover existing state of the art activation functions."}
{"id": "2505.11533", "pdf": "https://arxiv.org/pdf/2505.11533", "abs": "https://arxiv.org/abs/2505.11533", "authors": ["Jinqiang Wang", "Huansheng Ning", "Tao Zhu", "Jianguo Ding"], "title": "A Data Synthesis Method Driven by Large Language Models for Proactive Mining of Implicit User Intentions in Tourism", "categories": ["cs.CL"], "comment": null, "summary": "In the tourism domain, Large Language Models (LLMs) often struggle to mine\nimplicit user intentions from tourists' ambiguous inquiries and lack the\ncapacity to proactively guide users toward clarifying their needs. A critical\nbottleneck is the scarcity of high-quality training datasets that facilitate\nproactive questioning and implicit intention mining. While recent advances\nleverage LLM-driven data synthesis to generate such datasets and transfer\nspecialized knowledge to downstream models, existing approaches suffer from\nseveral shortcomings: (1) lack of adaptation to the tourism domain, (2) skewed\ndistributions of detail levels in initial inquiries, (3) contextual redundancy\nin the implicit intention mining module, and (4) lack of explicit thinking\nabout tourists' emotions and intention values. Therefore, we propose SynPT (A\nData Synthesis Method Driven by LLMs for Proactive Mining of Implicit User\nIntentions in the Tourism), which constructs an LLM-driven user agent and\nassistant agent to simulate dialogues based on seed data collected from Chinese\ntourism websites. This approach addresses the aforementioned limitations and\ngenerates SynPT-Dialog, a training dataset containing explicit reasoning. The\ndataset is utilized to fine-tune a general LLM, enabling it to proactively mine\nimplicit user intentions. Experimental evaluations, conducted from both human\nand LLM perspectives, demonstrate the superiority of SynPT compared to existing\nmethods. Furthermore, we analyze key hyperparameters and present case studies\nto illustrate the practical applicability of our method, including discussions\non its adaptability to English-language scenarios. All code and data are\npublicly available."}
{"id": "2505.11729", "pdf": "https://arxiv.org/pdf/2505.11729", "abs": "https://arxiv.org/abs/2505.11729", "authors": ["Pedro Figueiredo", "Qihao He", "Steve Bako", "Nima Khademi Kalantari"], "title": "Neural Importance Sampling of Many Lights", "categories": ["cs.GR", "cs.LG"], "comment": "11 pages, 11 figures. Accepted for publication in SIGGRAPH Conference\n  Papers '25; to be presented at SIGGRAPH 2025", "summary": "We propose a neural approach for estimating spatially varying light selection\ndistributions to improve importance sampling in Monte Carlo rendering,\nparticularly for complex scenes with many light sources. Our method uses a\nneural network to predict the light selection distribution at each shading\npoint based on local information, trained by minimizing the KL-divergence\nbetween the learned and target distributions in an online manner. To\nefficiently manage hundreds or thousands of lights, we integrate our neural\napproach with light hierarchy techniques, where the network predicts\ncluster-level distributions and existing methods sample lights within clusters.\nAdditionally, we introduce a residual learning strategy that leverages initial\ndistributions from existing techniques, accelerating convergence during\ntraining. Our method achieves superior performance across diverse and\nchallenging scenes."}
{"id": "2505.11676", "pdf": "https://arxiv.org/pdf/2505.11676", "abs": "https://arxiv.org/abs/2505.11676", "authors": ["Ziyu Zhao", "Xiaoguang Li", "Linjia Shi", "Nasrin Imanpour", "Song Wang"], "title": "DPSeg: Dual-Prompt Cost Volume Learning for Open-Vocabulary Semantic Segmentation", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Open-vocabulary semantic segmentation aims to segment images into distinct\nsemantic regions for both seen and unseen categories at the pixel level.\nCurrent methods utilize text embeddings from pre-trained vision-language models\nlike CLIP but struggle with the inherent domain gap between image and text\nembeddings, even after extensive alignment during training. Additionally,\nrelying solely on deep text-aligned features limits shallow-level feature\nguidance, which is crucial for detecting small objects and fine details,\nultimately reducing segmentation accuracy. To address these limitations, we\npropose a dual prompting framework, DPSeg, for this task. Our approach combines\ndual-prompt cost volume generation, a cost volume-guided decoder, and a\nsemantic-guided prompt refinement strategy that leverages our dual prompting\nscheme to mitigate alignment issues in visual prompt generation. By\nincorporating visual embeddings from a visual prompt encoder, our approach\nreduces the domain gap between text and image embeddings while providing\nmulti-level guidance through shallow features. Extensive experiments\ndemonstrate that our method significantly outperforms existing state-of-the-art\napproaches on multiple public datasets."}
{"id": "2505.11550", "pdf": "https://arxiv.org/pdf/2505.11550", "abs": "https://arxiv.org/abs/2505.11550", "authors": ["Harika Abburi", "Sanmitra Bhattacharya", "Edward Bowen", "Nirmala Pudota"], "title": "AI-generated Text Detection: A Multifaceted Approach to Binary and Multiclass Classification", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ngenerating text that closely resembles human writing across a wide range of\nstyles and genres. However, such capabilities are prone to potential misuse,\nsuch as fake news generation, spam email creation, and misuse in academic\nassignments. As a result, accurate detection of AI-generated text and\nidentification of the model that generated it are crucial for maintaining the\nresponsible use of LLMs. In this work, we addressed two sub-tasks put forward\nby the Defactify workshop under AI-Generated Text Detection shared task at the\nAssociation for the Advancement of Artificial Intelligence (AAAI 2025): Task A\ninvolved distinguishing between human-authored or AI-generated text, while Task\nB focused on attributing text to its originating language model. For each task,\nwe proposed two neural architectures: an optimized model and a simpler variant.\nFor Task A, the optimized neural architecture achieved fifth place with $F1$\nscore of 0.994, and for Task B, the simpler neural architecture also ranked\nfifth place with $F1$ score of 0.627."}
{"id": "2505.11799", "pdf": "https://arxiv.org/pdf/2505.11799", "abs": "https://arxiv.org/abs/2505.11799", "authors": ["Rushan Ziatdinov", "Rifkat Nabiyev"], "title": "Generating Digital Models Using Text-to-3D and Image-to-3D Prompts: Critical Case Study", "categories": ["cs.GR", "cs.MM"], "comment": "6 pages, 11 figures", "summary": "In the world of technology and AI, digital models play an important role in\nour lives and are an essential part of the digital twins of real-world objects.\nThey can be created by designers, artists, or game developers using spline\ncurves and surfaces, meshes, and voxels, but making such models is too\ntime-consuming. With the growth of AI tools, there is interest in the automated\ngeneration of 3D models, such as generative design approaches, which can save\ncreators valuable time. This paper reviews several online 3D model generators\nand critically analyses the results, hoping to see higher-quality results from\ndifferent prompts."}
{"id": "2505.11703", "pdf": "https://arxiv.org/pdf/2505.11703", "abs": "https://arxiv.org/abs/2505.11703", "authors": ["Jae Myung Kim", "Stephan Alaniz", "Cordelia Schmid", "Zeynep Akata"], "title": "LoFT: LoRA-fused Training Dataset Generation with Few-shot Guidance", "categories": ["cs.CV"], "comment": null, "summary": "Despite recent advances in text-to-image generation, using synthetically\ngenerated data seldom brings a significant boost in performance for supervised\nlearning. Oftentimes, synthetic datasets do not faithfully recreate the data\ndistribution of real data, i.e., they lack the fidelity or diversity needed for\neffective downstream model training. While previous work has employed few-shot\nguidance to address this issue, existing methods still fail to capture and\ngenerate features unique to specific real images. In this paper, we introduce a\nnovel dataset generation framework named LoFT, LoRA-Fused Training-data\nGeneration with Few-shot Guidance. Our method fine-tunes LoRA weights on\nindividual real images and fuses them at inference time, producing synthetic\nimages that combine the features of real images for improved diversity and\nfidelity of generated data. We evaluate the synthetic data produced by LoFT on\n10 datasets, using 8 to 64 real images per class as guidance and scaling up to\n1000 images per class. Our experiments show that training on LoFT-generated\ndata consistently outperforms other synthetic dataset methods, significantly\nincreasing accuracy as the dataset size increases. Additionally, our analysis\ndemonstrates that LoFT generates datasets with high fidelity and sufficient\ndiversity, which contribute to the performance improvement. The code is\navailable at https://github.com/ExplainableML/LoFT."}
{"id": "2505.11556", "pdf": "https://arxiv.org/pdf/2505.11556", "abs": "https://arxiv.org/abs/2505.11556", "authors": ["Yuxuan Li", "Aoi Naito", "Hirokazu Shirado"], "title": "Assessing Collective Reasoning in Multi-Agent LLMs via Hidden Profile Tasks", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": null, "summary": "Multi-agent systems built on large language models (LLMs) promise enhanced\nproblem-solving through distributed information integration, but also risk\nreplicating collective reasoning failures observed in human groups. Yet, no\ntheory-grounded benchmark exists to systematically evaluate such failures. In\nthis paper, we introduce the Hidden Profile paradigm from social psychology as\na diagnostic testbed for multi-agent LLM systems. By distributing critical\ninformation asymmetrically across agents, the paradigm reveals how inter-agent\ndynamics support or hinder collective reasoning. We first formalize the\nparadigm for multi-agent decision-making under distributed knowledge and\ninstantiate it as a benchmark with nine tasks spanning diverse scenarios,\nincluding adaptations from prior human studies. We then conduct experiments\nwith GPT-4.1 and five other leading LLMs, including reasoning-enhanced\nvariants, showing that multi-agent systems across all models fail to match the\naccuracy of single agents given complete information. While agents' collective\nperformance is broadly comparable to that of human groups, nuanced behavioral\ndifferences emerge, such as increased sensitivity to social desirability.\nFinally, we demonstrate the paradigm's diagnostic utility by exploring a\ncooperation-contradiction trade-off in multi-agent LLM systems. We find that\nwhile cooperative agents are prone to over-coordination in collective settings,\nincreased contradiction impairs group convergence. This work contributes a\nreproducible framework for evaluating multi-agent LLM systems and motivates\nfuture research on artificial collective intelligence and human-AI interaction."}
{"id": "2505.12373", "pdf": "https://arxiv.org/pdf/2505.12373", "abs": "https://arxiv.org/abs/2505.12373", "authors": ["Kapil Dev"], "title": "Modeling Aesthetic Preferences in 3D Shapes: A Large-Scale Paired Comparison Study Across Object Categories", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "11 pages, 8 figures, submitted to IEEE Transactions on Visualization\n  and Computer Graphics (TVCG)", "summary": "Human aesthetic preferences for 3D shapes are central to industrial design,\nvirtual reality, and consumer product development. However, most computational\nmodels of 3D aesthetics lack empirical grounding in large-scale human\njudgments, limiting their practical relevance. We present a large-scale study\nof human preferences. We collected 22,301 pairwise comparisons across five\nobject categories (chairs, tables, mugs, lamps, and dining chairs) via Amazon\nMechanical Turk. Building on a previously published\ndataset~\\cite{dev2020learning}, we introduce new non-linear modeling and\ncross-category analysis to uncover the geometric drivers of aesthetic\npreference. We apply the Bradley-Terry model to infer latent aesthetic scores\nand use Random Forests with SHAP analysis to identify and interpret the most\ninfluential geometric features (e.g., symmetry, curvature, compactness). Our\ncross-category analysis reveals both universal principles and domain-specific\ntrends in aesthetic preferences. We focus on human interpretable geometric\nfeatures to ensure model transparency and actionable design insights, rather\nthan relying on black-box deep learning approaches. Our findings bridge\ncomputational aesthetics and cognitive science, providing practical guidance\nfor designers and a publicly available dataset to support reproducibility. This\nwork advances the understanding of 3D shape aesthetics through a human-centric,\ndata-driven framework."}
{"id": "2505.11707", "pdf": "https://arxiv.org/pdf/2505.11707", "abs": "https://arxiv.org/abs/2505.11707", "authors": ["Haipeng Fang", "Sheng Tang", "Juan Cao", "Enshuo Zhang", "Fan Tang", "Tong-Yee Lee"], "title": "Attend to Not Attended: Structure-then-Detail Token Merging for Post-training DiT Acceleration", "categories": ["cs.CV"], "comment": "Comments: 14 pages, 14 figures. Accepted by the Proceedings of the\n  42nd IEEE/CVF Conference on Computer Vision and Pattern Recognition", "summary": "Diffusion transformers have shown exceptional performance in visual\ngeneration but incur high computational costs. Token reduction techniques that\ncompress models by sharing the denoising process among similar tokens have been\nintroduced. However, existing approaches neglect the denoising priors of the\ndiffusion models, leading to suboptimal acceleration and diminished image\nquality. This study proposes a novel concept: attend to prune feature\nredundancies in areas not attended by the diffusion process. We analyze the\nlocation and degree of feature redundancies based on the structure-then-detail\ndenoising priors. Subsequently, we introduce SDTM, a structure-then-detail\ntoken merging approach that dynamically compresses feature redundancies.\nSpecifically, we design dynamic visual token merging, compression ratio\nadjusting, and prompt reweighting for different stages. Served in a\npost-training way, the proposed method can be integrated seamlessly into any\nDiT architecture. Extensive experiments across various backbones, schedulers,\nand datasets showcase the superiority of our method, for example, it achieves\n1.55 times acceleration with negligible impact on image quality. Project page:\nhttps://github.com/ICTMCG/SDTM."}
{"id": "2505.11604", "pdf": "https://arxiv.org/pdf/2505.11604", "abs": "https://arxiv.org/abs/2505.11604", "authors": ["Kyudan Jung", "Hojun Cho", "Jooyeol Yun", "Jaehyeok Jang", "Jagul Choo"], "title": "Talk to Your Slides: Efficient Slide Editing Agent with Large Language Models", "categories": ["cs.CL"], "comment": "14 pages, 6 figures", "summary": "Existing research on large language models (LLMs) for PowerPoint\npredominantly focuses on slide generation, overlooking the common yet tedious\ntask of editing existing slides. We introduce Talk-to-Your-Slides, an\nLLM-powered agent that directly edits slides within active PowerPoint sessions\nthrough COM communication. Our system employs a two-level approach: (1)\nhigh-level processing where an LLM agent interprets instructions and formulates\nediting plans, and (2) low-level execution where Python scripts directly\nmanipulate PowerPoint objects. Unlike previous methods relying on predefined\noperations, our approach enables more flexible and contextually-aware editing.\nTo facilitate evaluation, we present TSBench, a human-annotated dataset of 379\ndiverse editing instructions with corresponding slide variations. Experimental\nresults demonstrate that Talk-to-Your-Slides significantly outperforms baseline\nmethods in execution success rate, instruction fidelity, and editing\nefficiency. Our code and benchmark are available at\nhttps://anonymous.4open.science/r/talk-to-your-slides/"}
{"id": "2505.12539", "pdf": "https://arxiv.org/pdf/2505.12539", "abs": "https://arxiv.org/abs/2505.12539", "authors": ["Jinyuan Liu", "Yuchen Sun", "Yin Yang", "Chenfanfu Jiang", "Minchen Li", "Bo Zhu"], "title": "Penetration-free Solid-Fluid Interaction on Shells and Rods", "categories": ["cs.GR"], "comment": null, "summary": "We introduce a novel approach to simulate the interaction between fluids and\nthin elastic solids without any penetration. Our approach is centered around an\noptimization system augmented with barriers, which aims to find a configuration\nthat ensures the absence of penetration while enforcing incompressibility for\nthe fluids and minimizing elastic potentials for the solids. Unlike previous\nmethods that primarily focus on velocity coherence at the fluid-solid\ninterfaces, we demonstrate the effectiveness and flexibility of explicitly\nresolving positional constraints, including both explicit representation of\nsolid positions and the implicit representation of fluid level-set interface.\nTo preserve the volume of the fluid, we propose a simple yet efficient approach\nthat adjusts the associated level-set values. Additionally, we develop a\ndistance metric capable of measuring the separation between an implicitly\nrepresented surface and a Lagrangian object of arbitrary codimension. By\nintegrating the inertia, solid elastic potential, damping, barrier potential,\nand fluid incompressibility within a unified system, we are able to robustly\nsimulate a wide range of processes involving fluid interactions with\nlower-dimensional objects such as shells and rods. These processes include\ntopology changes, bouncing, splashing, sliding, rolling, floating, and more."}
{"id": "2505.11709", "pdf": "https://arxiv.org/pdf/2505.11709", "abs": "https://arxiv.org/abs/2505.11709", "authors": ["Ryan Hoque", "Peide Huang", "David J. Yoon", "Mouli Sivapurapu", "Jian Zhang"], "title": "EgoDex: Learning Dexterous Manipulation from Large-Scale Egocentric Video", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "Imitation learning for manipulation has a well-known data scarcity problem.\nUnlike natural language and 2D computer vision, there is no Internet-scale\ncorpus of data for dexterous manipulation. One appealing option is egocentric\nhuman video, a passively scalable data source. However, existing large-scale\ndatasets such as Ego4D do not have native hand pose annotations and do not\nfocus on object manipulation. To this end, we use Apple Vision Pro to collect\nEgoDex: the largest and most diverse dataset of dexterous human manipulation to\ndate. EgoDex has 829 hours of egocentric video with paired 3D hand and finger\ntracking data collected at the time of recording, where multiple calibrated\ncameras and on-device SLAM can be used to precisely track the pose of every\njoint of each hand. The dataset covers a wide range of diverse manipulation\nbehaviors with everyday household objects in 194 different tabletop tasks\nranging from tying shoelaces to folding laundry. Furthermore, we train and\nsystematically evaluate imitation learning policies for hand trajectory\nprediction on the dataset, introducing metrics and benchmarks for measuring\nprogress in this increasingly important area. By releasing this large-scale\ndataset, we hope to push the frontier of robotics, computer vision, and\nfoundation models."}
{"id": "2505.11613", "pdf": "https://arxiv.org/pdf/2505.11613", "abs": "https://arxiv.org/abs/2505.11613", "authors": ["Xiaomin Li", "Mingye Gao", "Yuexing Hao", "Taoran Li", "Guangya Wan", "Zihan Wang", "Yijun Wang"], "title": "MedGUIDE: Benchmarking Clinical Decision-Making in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Clinical guidelines, typically structured as decision trees, are central to\nevidence-based medical practice and critical for ensuring safe and accurate\ndiagnostic decision-making. However, it remains unclear whether Large Language\nModels (LLMs) can reliably follow such structured protocols. In this work, we\nintroduce MedGUIDE, a new benchmark for evaluating LLMs on their ability to\nmake guideline-consistent clinical decisions. MedGUIDE is constructed from 55\ncurated NCCN decision trees across 17 cancer types and uses clinical scenarios\ngenerated by LLMs to create a large pool of multiple-choice diagnostic\nquestions. We apply a two-stage quality selection process, combining\nexpert-labeled reward models and LLM-as-a-judge ensembles across ten clinical\nand linguistic criteria, to select 7,747 high-quality samples. We evaluate 25\nLLMs spanning general-purpose, open-source, and medically specialized models,\nand find that even domain-specific LLMs often underperform on tasks requiring\nstructured guideline adherence. We also test whether performance can be\nimproved via in-context guideline inclusion or continued pretraining. Our\nfindings underscore the importance of MedGUIDE in assessing whether LLMs can\noperate safely within the procedural frameworks expected in real-world clinical\nsettings."}
{"id": "2505.12619", "pdf": "https://arxiv.org/pdf/2505.12619", "abs": "https://arxiv.org/abs/2505.12619", "authors": ["Jiashun Wang", "Yifeng Jiang", "Haotian Zhang", "Chen Tessler", "Davis Rempe", "Jessica Hodgins", "Xue Bin Peng"], "title": "HIL: Hybrid Imitation Learning of Diverse Parkour Skills from Videos", "categories": ["cs.GR"], "comment": "14 pages, 10 figures", "summary": "Recent data-driven methods leveraging deep reinforcement learning have been\nan effective paradigm for developing controllers that enable physically\nsimulated characters to produce natural human-like behaviors. However, these\ndata-driven methods often struggle to adapt to novel environments and compose\ndiverse skills coherently to perform more complex tasks. To address these\nchallenges, we propose a hybrid imitation learning (HIL) framework that\ncombines motion tracking, for precise skill replication, with adversarial\nimitation learning, to enhance adaptability and skill composition. This hybrid\nlearning framework is implemented through parallel multi-task environments and\na unified observation space, featuring an agent-centric scene representation to\nfacilitate effective learning from the hybrid parallel environments. Our\nframework trains a unified controller on parkour data sourced from Internet\nvideos, enabling a simulated character to traverse through new environments\nusing diverse and life-like parkour skills. Evaluations across challenging\nparkour environments demonstrate that our method improves motion quality,\nincreases skill diversity, and achieves competitive task completion compared to\nprevious learning-based methods."}
{"id": "2505.11720", "pdf": "https://arxiv.org/pdf/2505.11720", "abs": "https://arxiv.org/abs/2505.11720", "authors": ["Shijun Liang", "Ismail R. Alkhouri", "Siddhant Gautam", "Qing Qu", "Saiprasad Ravishankar"], "title": "UGoDIT: Unsupervised Group Deep Image Prior Via Transferable Weights", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Recent advances in data-centric deep generative models have led to\nsignificant progress in solving inverse imaging problems. However, these models\n(e.g., diffusion models (DMs)) typically require large amounts of fully sampled\n(clean) training data, which is often impractical in medical and scientific\nsettings such as dynamic imaging.\n  On the other hand, training-data-free approaches like the Deep Image Prior\n(DIP) do not require clean ground-truth images but suffer from noise\noverfitting and can be computationally expensive as the network parameters need\nto be optimized for each measurement set independently. Moreover, DIP-based\nmethods often overlook the potential of learning a prior using a small number\nof sub-sampled measurements (or degraded images) available during training. In\nthis paper, we propose UGoDIT, an Unsupervised Group DIP via Transferable\nweights, designed for the low-data regime where only a very small number, M, of\nsub-sampled measurement vectors are available during training. Our method\nlearns a set of transferable weights by optimizing a shared encoder and M\ndisentangled decoders. At test time, we reconstruct the unseen degraded image\nusing a DIP network, where part of the parameters are fixed to the learned\nweights, while the remaining are optimized to enforce measurement consistency.\nWe evaluate UGoDIT on both medical (multi-coil MRI) and natural (super\nresolution and non-linear deblurring) image recovery tasks under various\nsettings. Compared to recent standalone DIP methods, UGoDIT provides\naccelerated convergence and notable improvement in reconstruction quality.\nFurthermore, our method achieves performance competitive with SOTA DM-based and\nsupervised approaches, despite not requiring large amounts of clean training\ndata."}
{"id": "2505.11615", "pdf": "https://arxiv.org/pdf/2505.11615", "abs": "https://arxiv.org/abs/2505.11615", "authors": ["Jian-Qiao Zhu", "Haijiang Yan", "Thomas L. Griffiths"], "title": "Steering Risk Preferences in Large Language Models by Aligning Behavioral and Neural Representations", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Changing the behavior of large language models (LLMs) can be as\nstraightforward as editing the Transformer's residual streams using\nappropriately constructed \"steering vectors.\" These modifications to internal\nneural activations, a form of representation engineering, offer an effective\nand targeted means of influencing model behavior without retraining or\nfine-tuning the model. But how can such steering vectors be systematically\nidentified? We propose a principled approach for uncovering steering vectors by\naligning latent representations elicited through behavioral methods\n(specifically, Markov chain Monte Carlo with LLMs) with their neural\ncounterparts. To evaluate this approach, we focus on extracting latent risk\npreferences from LLMs and steering their risk-related outputs using the aligned\nrepresentations as steering vectors. We show that the resulting steering\nvectors successfully and reliably modulate LLM outputs in line with the\ntargeted behavior."}
{"id": "2505.12774", "pdf": "https://arxiv.org/pdf/2505.12774", "abs": "https://arxiv.org/abs/2505.12774", "authors": ["Zichen Geng", "Zeeshan Hayder", "Wei Liu", "Ajmal Mian"], "title": "UniHM: Universal Human Motion Generation with Object Interactions in Indoor Scenes", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": null, "summary": "Human motion synthesis in complex scenes presents a fundamental challenge,\nextending beyond conventional Text-to-Motion tasks by requiring the integration\nof diverse modalities such as static environments, movable objects, natural\nlanguage prompts, and spatial waypoints. Existing language-conditioned motion\nmodels often struggle with scene-aware motion generation due to limitations in\nmotion tokenization, which leads to information loss and fails to capture the\ncontinuous, context-dependent nature of 3D human movement. To address these\nissues, we propose UniHM, a unified motion language model that leverages\ndiffusion-based generation for synthesizing scene-aware human motion. UniHM is\nthe first framework to support both Text-to-Motion and Text-to-Human-Object\nInteraction (HOI) in complex 3D scenes. Our approach introduces three key\ncontributions: (1) a mixed-motion representation that fuses continuous 6DoF\nmotion with discrete local motion tokens to improve motion realism; (2) a novel\nLook-Up-Free Quantization VAE (LFQ-VAE) that surpasses traditional VQ-VAEs in\nboth reconstruction accuracy and generative performance; and (3) an enriched\nversion of the Lingo dataset augmented with HumanML3D annotations, providing\nstronger supervision for scene-specific motion learning. Experimental results\ndemonstrate that UniHM achieves comparative performance on the OMOMO benchmark\nfor text-to-HOI synthesis and yields competitive results on HumanML3D for\ngeneral text-conditioned motion generation."}
{"id": "2505.11724", "pdf": "https://arxiv.org/pdf/2505.11724", "abs": "https://arxiv.org/abs/2505.11724", "authors": ["Kai Zhu", "Vignesh Edithal", "Le Zhang", "Ilia Blank", "Imran Junejo"], "title": "Semantically-Aware Game Image Quality Assessment", "categories": ["cs.CV", "eess.IV"], "comment": "16 pages, 12 figures", "summary": "Assessing the visual quality of video game graphics presents unique\nchallenges due to the absence of reference images and the distinct types of\ndistortions, such as aliasing, texture blur, and geometry level of detail (LOD)\nissues, which differ from those in natural images or user-generated content.\nExisting no-reference image and video quality assessment (NR-IQA/VQA) methods\nfail to generalize to gaming environments as they are primarily designed for\ndistortions like compression artifacts. This study introduces a\nsemantically-aware NR-IQA model tailored to gaming. The model employs a\nknowledge-distilled Game distortion feature extractor (GDFE) to detect and\nquantify game-specific distortions, while integrating semantic gating via CLIP\nembeddings to dynamically weight feature importance based on scene content.\nTraining on gameplay data recorded across graphical quality presets enables the\nmodel to produce quality scores that align with human perception. Our results\ndemonstrate that the GDFE, trained through knowledge distillation from binary\nclassifiers, generalizes effectively to intermediate distortion levels unseen\nduring training. Semantic gating further improves contextual relevance and\nreduces prediction variance. In the absence of in-domain NR-IQA baselines, our\nmodel outperforms out-of-domain methods and exhibits robust, monotonic quality\ntrends across unseen games in the same genre. This work establishes a\nfoundation for automated graphical quality assessment in gaming, advancing\nNR-IQA methods in this domain."}
{"id": "2505.11626", "pdf": "https://arxiv.org/pdf/2505.11626", "abs": "https://arxiv.org/abs/2505.11626", "authors": ["Udita Patel", "Rutu Mulkar", "Jay Roberts", "Cibi Chakravarthy Senthilkumar", "Sujay Gandhi", "Xiaofei Zheng", "Naumaan Nayyar", "Rafael Castrillo"], "title": "THELMA: Task Based Holistic Evaluation of Large Language Model Applications-RAG Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "We propose THELMA (Task Based Holistic Evaluation of Large Language Model\nApplications), a reference free framework for RAG (Retrieval Augmented\ngeneration) based question answering (QA) applications. THELMA consist of six\ninterdependent metrics specifically designed for holistic, fine grained\nevaluation of RAG QA applications. THELMA framework helps developers and\napplication owners evaluate, monitor and improve end to end RAG QA pipelines\nwithout requiring labelled sources or reference responses.We also present our\nfindings on the interplay of the proposed THELMA metrics, which can be\ninterpreted to identify the specific RAG component needing improvement in QA\napplications."}
{"id": "2505.12782", "pdf": "https://arxiv.org/pdf/2505.12782", "abs": "https://arxiv.org/abs/2505.12782", "authors": ["Kai Zhang", "Xingyu Chen", "Xiaofeng Zhang"], "title": "AdaToken-3D: Dynamic Spatial Gating for Efficient 3D Large Multimodal-Models Reasoning", "categories": ["cs.GR", "cs.CV", "cs.IR", "cs.IT", "math.IT"], "comment": null, "summary": "Large Multimodal Models (LMMs) have become a pivotal research focus in deep\nlearning, demonstrating remarkable capabilities in 3D scene understanding.\nHowever, current 3D LMMs employing thousands of spatial tokens for multimodal\nreasoning suffer from critical inefficiencies: excessive computational overhead\nand redundant information flows. Unlike 2D VLMs processing single images, 3D\nLMMs exhibit inherent architectural redundancy due to the heterogeneous\nmechanisms between spatial tokens and visual tokens. To address this challenge,\nwe propose AdaToken-3D, an adaptive spatial token optimization framework that\ndynamically prunes redundant tokens through spatial contribution analysis. Our\nmethod automatically tailors pruning strategies to different 3D LMM\narchitectures by quantifying token-level information flows via attention\npattern mining. Extensive experiments on LLaVA-3D (a 7B parameter 3D-LMM)\ndemonstrate that AdaToken-3D achieves 21\\% faster inference speed and 63\\%\nFLOPs reduction while maintaining original task accuracy. Beyond efficiency\ngains, this work systematically investigates redundancy patterns in multimodal\nspatial information flows through quantitative token interaction analysis. Our\nfindings reveal that over 60\\% of spatial tokens contribute minimally ($<$5\\%)\nto the final predictions, establishing theoretical foundations for efficient 3D\nmultimodal learning."}
{"id": "2505.11753", "pdf": "https://arxiv.org/pdf/2505.11753", "abs": "https://arxiv.org/abs/2505.11753", "authors": ["Valentina Bazyleva", "Nicolo Bonettini", "Gaurav Bharaj"], "title": "X-Edit: Detecting and Localizing Edits in Images Altered by Text-Guided Diffusion Models", "categories": ["cs.CV"], "comment": "CVPR (XAI4CV) 2025", "summary": "Text-guided diffusion models have significantly advanced image editing,\nenabling highly realistic and local modifications based on textual prompts.\nWhile these developments expand creative possibilities, their malicious use\nposes substantial challenges for detection of such subtle deepfake edits. To\nthis end, we introduce Explain Edit (X-Edit), a novel method for localizing\ndiffusion-based edits in images. To localize the edits for an image, we invert\nthe image using a pretrained diffusion model, then use these inverted features\nas input to a segmentation network that explicitly predicts the edited masked\nregions via channel and spatial attention. Further, we finetune the model using\na combined segmentation and relevance loss. The segmentation loss ensures\naccurate mask prediction by balancing pixel-wise errors and perceptual\nsimilarity, while the relevance loss guides the model to focus on low-frequency\nregions and mitigate high-frequency artifacts, enhancing the localization of\nsubtle edits. To the best of our knowledge, we are the first to address and\nmodel the problem of localizing diffusion-based modified regions in images. We\nadditionally contribute a new dataset of paired original and edited images\naddressing the current lack of resources for this task. Experimental results\ndemonstrate that X-Edit accurately localizes edits in images altered by\ntext-guided diffusion models, outperforming baselines in PSNR and SSIM metrics.\nThis highlights X-Edit's potential as a robust forensic tool for detecting and\npinpointing manipulations introduced by advanced image editing techniques."}
{"id": "2505.11628", "pdf": "https://arxiv.org/pdf/2505.11628", "abs": "https://arxiv.org/abs/2505.11628", "authors": ["Berkcan Kapusuzoglu", "Supriyo Chakraborty", "Chia-Hsuan Lee", "Sambit Sahu"], "title": "Critique-Guided Distillation: Improving Supervised Fine-tuning via Better Distillation", "categories": ["cs.CL", "cs.LG"], "comment": "Submitted to NeurIPS 2025", "summary": "Supervised fine-tuning (SFT) using expert demonstrations often suffer from\nthe imitation problem, where the model learns to reproduce the correct\nresponses without \\emph{understanding} the underlying rationale. To address\nthis limitation, we propose \\textsc{Critique-Guided Distillation (CGD)}, a\nnovel multi-stage framework that integrates teacher model generated\n\\emph{explanatory critiques} and \\emph{refined responses} into the SFT process.\nA student model is then trained to map the triplet of prompt, teacher critique,\nand its own initial response to the corresponding refined teacher response,\nthereby learning both \\emph{what} to imitate and \\emph{why}. Using\nentropy-based analysis, we show that \\textsc{CGD} reduces refinement\nuncertainty and can be interpreted as a Bayesian posterior update. We perform\nextensive empirical evaluation of \\textsc{CGD}, on variety of benchmark tasks,\nand demonstrate significant gains on both math (AMC23 +17.5%) and language\nunderstanding tasks (MMLU-Pro +6.3%), while successfully mitigating the format\ndrift issues observed in previous critique fine-tuning (CFT) techniques."}
{"id": "2505.13390", "pdf": "https://arxiv.org/pdf/2505.13390", "abs": "https://arxiv.org/abs/2505.13390", "authors": ["Chunlei Li", "Peng Yu", "Tiantian Liu", "Siyuan Yu", "Yuting Xiao", "Shuai Li", "Aimin Hao", "Yang Gao", "Qinping Zhao"], "title": "MGPBD: A Multigrid Accelerated Global XPBD Solver", "categories": ["cs.GR", "I.3.6"], "comment": "SIGGRAPH 2025", "summary": "We introduce a novel Unsmoothed Aggregation (UA) Algebraic Multigrid (AMG)\nmethod combined with Preconditioned Conjugate Gradient (PCG) to overcome the\nlimitations of Extended Position-Based Dynamics (XPBD) in high-resolution and\nhigh-stiffness simulations. While XPBD excels in simulating deformable objects\ndue to its speed and simplicity, its nonlinear Gauss-Seidel (GS) solver often\nstruggles with low-frequency errors, leading to instability and stalling\nissues, especially in high-resolution, high-stiffness simulations. Our\nmultigrid approach addresses these issues efficiently by leveraging AMG. To\nreduce the computational overhead of traditional AMG, where prolongator\nconstruction can consume up to two-thirds of the runtime, we propose a lazy\nsetup strategy that reuses prolongators across iterations based on matrix\nstructure and physical significance. Furthermore, we introduce a simplified\nmethod for constructing near-kernel components by applying a few sweeps of\niterative methods to the homogeneous equation, achieving convergence rates\ncomparable to adaptive smoothed aggregation (adaptive-SA) at a lower\ncomputational cost. Experimental results demonstrate that our method\nsignificantly improves convergence rates and numerical stability, enabling\nefficient and stable high-resolution simulations of deformable objects."}
{"id": "2505.11758", "pdf": "https://arxiv.org/pdf/2505.11758", "abs": "https://arxiv.org/abs/2505.11758", "authors": ["Sriram Mandalika"], "title": "Generalizable Vision-Language Few-Shot Adaptation with Predictive Prompts and Negative Learning", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.RO"], "comment": null, "summary": "Few-shot adaptation remains a core challenge for vision-language models\n(VLMs), especially under limited supervision and noisy support samples. We\npropose PromptFuseNL, a unified framework that enhances few-shot generalization\nby combining predictive prompt tuning with dual-branch positive and negative\nlearning. The method refines class prototypes through task-conditioned\nresiduals, multi-stage cross-modal coordination, and semantic hard negative\nmining. To address label noise, we introduce an unsupervised instance\nreweighting strategy that downweights unreliable support examples without\nrequiring additional labels or structural changes. PromptFuseNL fuses visual\nand textual cues through lightweight modules for efficient and discriminative\nprediction. Evaluated across 15 benchmarks, it consistently surpasses existing\nprompt- and adapter-based methods in all shot settings while remaining highly\nefficient, achieving up to 300x faster training and 1000x lower FLOPs compared\nto full prompt tuning, achieving a new state-of-the-art for robust and scalable\nfew-shot vision-language adaptation."}
{"id": "2505.11643", "pdf": "https://arxiv.org/pdf/2505.11643", "abs": "https://arxiv.org/abs/2505.11643", "authors": ["Xiang Fu"], "title": "Can an Easy-to-Hard Curriculum Make Reasoning Emerge in Small Language Models? Evidence from a Four-Stage Curriculum on GPT-2", "categories": ["cs.CL"], "comment": null, "summary": "We demonstrate that a developmentally ordered curriculum markedly improves\nreasoning transparency and sample-efficiency in small language models (SLMs).\nConcretely, we train Cognivolve, a 124 M-parameter GPT-2 model, on a four-stage\nsyllabus that ascends from lexical matching to multi-step symbolic inference\nand then evaluate it without any task-specific fine-tuning. Cognivolve reaches\ntarget accuracy in half the optimization steps of a single-phase baseline,\nactivates an order-of-magnitude more gradient-salient reasoning heads, and\nshifts those heads toward deeper layers, yielding higher-entropy attention that\nbalances local and long-range context. The same curriculum applied out of order\nor with optimizer resets fails to reproduce these gains, confirming that\nprogression--not extra compute--drives the effect. We also identify open\nchallenges: final-answer success still lags a conventional run by about 30%,\nand our saliency probe under-detects verbal-knowledge heads in the hardest\nstage, suggesting directions for mixed-stage fine-tuning and probe expansion."}
{"id": "2505.11758", "pdf": "https://arxiv.org/pdf/2505.11758", "abs": "https://arxiv.org/abs/2505.11758", "authors": ["Sriram Mandalika"], "title": "Generalizable Vision-Language Few-Shot Adaptation with Predictive Prompts and Negative Learning", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.RO"], "comment": null, "summary": "Few-shot adaptation remains a core challenge for vision-language models\n(VLMs), especially under limited supervision and noisy support samples. We\npropose PromptFuseNL, a unified framework that enhances few-shot generalization\nby combining predictive prompt tuning with dual-branch positive and negative\nlearning. The method refines class prototypes through task-conditioned\nresiduals, multi-stage cross-modal coordination, and semantic hard negative\nmining. To address label noise, we introduce an unsupervised instance\nreweighting strategy that downweights unreliable support examples without\nrequiring additional labels or structural changes. PromptFuseNL fuses visual\nand textual cues through lightweight modules for efficient and discriminative\nprediction. Evaluated across 15 benchmarks, it consistently surpasses existing\nprompt- and adapter-based methods in all shot settings while remaining highly\nefficient, achieving up to 300x faster training and 1000x lower FLOPs compared\nto full prompt tuning, achieving a new state-of-the-art for robust and scalable\nfew-shot vision-language adaptation."}
{"id": "2505.11769", "pdf": "https://arxiv.org/pdf/2505.11769", "abs": "https://arxiv.org/abs/2505.11769", "authors": ["Wonjune Kim", "Lae-kyoung Lee", "Su-Yong An"], "title": "Technical Report for ICRA 2025 GOOSE 2D Semantic Segmentation Challenge: Boosting Off-Road Segmentation via Photometric Distortion and Exponential Moving Average", "categories": ["cs.CV"], "comment": "Winners of the GOOSE 2D Semantic Segmentation Challenge at the IEEE\n  ICRA Workshop on Field Robotics 2025", "summary": "We report on the application of a high-capacity semantic segmentation\npipeline to the GOOSE 2D Semantic Segmentation Challenge for unstructured\noff-road environments. Using a FlashInternImage-B backbone together with a\nUPerNet decoder, we adapt established techniques, rather than designing new\nones, to the distinctive conditions of off-road scenes. Our training recipe\ncouples strong photometric distortion augmentation (to emulate the wide\nlighting variations of outdoor terrain) with an Exponential Moving Average\n(EMA) of weights for better generalization. Using only the GOOSE training\ndataset, we achieve 88.8\\% mIoU on the validation set."}
{"id": "2505.11665", "pdf": "https://arxiv.org/pdf/2505.11665", "abs": "https://arxiv.org/abs/2505.11665", "authors": ["Shubham Vatsal", "Harsh Dubey", "Aditi Singh"], "title": "Multilingual Prompt Engineering in Large Language Models: A Survey Across NLP Tasks", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive performance across\na wide range of Natural Language Processing (NLP) tasks. However, ensuring\ntheir effectiveness across multiple languages presents unique challenges.\nMultilingual prompt engineering has emerged as a key approach to enhance LLMs'\ncapabilities in diverse linguistic settings without requiring extensive\nparameter re-training or fine-tuning. With growing interest in multilingual\nprompt engineering over the past two to three years, researchers have explored\nvarious strategies to improve LLMs' performance across languages and NLP tasks.\nBy crafting structured natural language prompts, researchers have successfully\nextracted knowledge from LLMs across different languages, making these\ntechniques an accessible pathway for a broader audience, including those\nwithout deep expertise in machine learning, to harness the capabilities of\nLLMs. In this paper, we survey and categorize different multilingual prompting\ntechniques based on the NLP tasks they address across a diverse set of datasets\nthat collectively span around 250 languages. We further highlight the LLMs\nemployed, present a taxonomy of approaches and discuss potential\nstate-of-the-art (SoTA) methods for specific multilingual datasets.\nAdditionally, we derive a range of insights across language families and\nresource levels (high-resource vs. low-resource), including analyses such as\nthe distribution of NLP tasks by language resource type and the frequency of\nprompting methods across different language families. Our survey reviews 36\nresearch papers covering 39 prompting techniques applied to 30 multilingual NLP\ntasks, with the majority of these studies published in the last two years."}
{"id": "2505.12267", "pdf": "https://arxiv.org/pdf/2505.12267", "abs": "https://arxiv.org/abs/2505.12267", "authors": ["Pengdi Huang", "Mingyang Wang", "Huan Tian", "Minglun Gong", "Hao Zhang", "Hui Huang"], "title": "Real-Time Spatial Reasoning by Mobile Robots for Reconstruction and Navigation in Dynamic LiDAR Scenes", "categories": ["cs.RO", "cs.GR"], "comment": null, "summary": "Our brain has an inner global positioning system which enables us to sense\nand navigate 3D spaces in real time. Can mobile robots replicate such a\nbiological feat in a dynamic environment? We introduce the first spatial\nreasoning framework for real-time surface reconstruction and navigation that is\ndesigned for outdoor LiDAR scanning data captured by ground mobile robots and\ncapable of handling moving objects such as pedestrians. Our\nreconstruction-based approach is well aligned with the critical cellular\nfunctions performed by the border vector cells (BVCs) over all layers of the\nmedial entorhinal cortex (MEC) for surface sensing and tracking. To address the\nchallenges arising from blurred boundaries resulting from sparse single-frame\nLiDAR points and outdated data due to object movements, we integrate real-time\nsingle-frame mesh reconstruction, via visibility reasoning, with robot\nnavigation assistance through on-the-fly 3D free space determination. This\nenables continuous and incremental updates of the scene and free space across\nmultiple frames. Key to our method is the utilization of line-of-sight (LoS)\nvectors from LiDAR, which enable real-time surface normal estimation, as well\nas robust and instantaneous per-voxel free space updates. We showcase two\npractical applications: real-time 3D scene reconstruction and autonomous\noutdoor robot navigation in real-world conditions. Comprehensive experiments on\nboth synthetic and real scenes highlight our method's superiority in speed and\nquality over existing real-time LiDAR processing approaches."}
{"id": "2505.11777", "pdf": "https://arxiv.org/pdf/2505.11777", "abs": "https://arxiv.org/abs/2505.11777", "authors": ["Fu-Yun Wang", "Keqiang Sun", "Yao Teng", "Xihui Liu", "Jiaming Song", "Hongsheng Li"], "title": "Self-NPO: Negative Preference Optimization of Diffusion Models by Simply Learning from Itself without Explicit Preference Annotations", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have demonstrated remarkable success in various visual\ngeneration tasks, including image, video, and 3D content generation. Preference\noptimization (PO) is a prominent and growing area of research that aims to\nalign these models with human preferences. While existing PO methods primarily\nconcentrate on producing favorable outputs, they often overlook the\nsignificance of classifier-free guidance (CFG) in mitigating undesirable\nresults. Diffusion-NPO addresses this gap by introducing negative preference\noptimization (NPO), training models to generate outputs opposite to human\npreferences and thereby steering them away from unfavorable outcomes. However,\nprior NPO approaches, including Diffusion-NPO, rely on costly and fragile\nprocedures for obtaining explicit preference annotations (e.g., manual pairwise\nlabeling or reward model training), limiting their practicality in domains\nwhere such data are scarce or difficult to acquire. In this work, we introduce\nSelf-NPO, a Negative Preference Optimization approach that learns exclusively\nfrom the model itself, thereby eliminating the need for manual data labeling or\nreward model training. Moreover, our method is highly efficient and does not\nrequire exhaustive data sampling. We demonstrate that Self-NPO integrates\nseamlessly into widely used diffusion models, including SD1.5, SDXL, and\nCogVideoX, as well as models already optimized for human preferences,\nconsistently enhancing both their generation quality and alignment with human\npreferences."}
{"id": "2505.11679", "pdf": "https://arxiv.org/pdf/2505.11679", "abs": "https://arxiv.org/abs/2505.11679", "authors": ["Zhibo Hu", "Chen Wang", "Yanfeng Shu", "Hye-Young Paik", "Liming Zhu"], "title": "Ambiguity Resolution in Text-to-Structured Data Mapping", "categories": ["cs.CL", "cs.LG", "I.2.7"], "comment": "15 pages, 11 figures", "summary": "Ambiguity in natural language is a significant obstacle for achieving\naccurate text to structured data mapping through large language models (LLMs),\nwhich affects the performance of tasks such as mapping text to agentic tool\ncalling and text-to-SQL queries. Existing methods of ambiguity handling either\nexploit ReACT framework to produce the correct mapping through trial and error,\nor supervised fine tuning to guide models to produce a biased mapping to\nimprove certain tasks. In this paper, we adopt a different approach that\ncharacterizes the representation difference of ambiguous text in the latent\nspace and leverage the difference to identify ambiguity before mapping them to\nstructured data. To detect ambiguity of a sentence, we focused on the\nrelationship between ambiguous questions and their interpretations and what\ncause the LLM ignore multiple interpretations. Different to the distance\ncalculated by dense embedding vectors, we utilize the observation that\nambiguity is caused by concept missing in latent space of LLM to design a new\ndistance measurement, computed through the path kernel by the integral of\ngradient values for each concepts from sparse-autoencoder (SAE) under each\nstate. We identify patterns to distinguish ambiguous questions with this\nmeasurement. Based on our observation, We propose a new framework to improve\nthe performance of LLMs on ambiguous agentic tool calling through missing\nconcepts prediction."}
{"id": "2505.12734", "pdf": "https://arxiv.org/pdf/2505.12734", "abs": "https://arxiv.org/abs/2505.12734", "authors": ["Junbo Wang", "Haofeng Tan", "Bowen Liao", "Albert Jiang", "Teng Fei", "Qixing Huang", "Zhengzhong Tu", "Shan Ye", "Yuhao Kang"], "title": "SounDiT: Geo-Contextual Soundscape-to-Landscape Generation", "categories": ["cs.SD", "cs.AI", "cs.GR", "cs.HC", "eess.AS"], "comment": "14 pages, 5 figures", "summary": "We present a novel and practically significant problem-Geo-Contextual\nSoundscape-to-Landscape (GeoS2L) generation-which aims to synthesize\ngeographically realistic landscape images from environmental soundscapes. Prior\naudio-to-image generation methods typically rely on general-purpose datasets\nand overlook geographic and environmental contexts, resulting in unrealistic\nimages that are misaligned with real-world environmental settings. To address\nthis limitation, we introduce a novel geo-contextual computational framework\nthat explicitly integrates geographic knowledge into multimodal generative\nmodeling. We construct two large-scale geo-contextual multimodal datasets,\nSoundingSVI and SonicUrban, pairing diverse soundscapes with real-world\nlandscape images. We propose SounDiT, a novel Diffusion Transformer (DiT)-based\nmodel that incorporates geo-contextual scene conditioning to synthesize\ngeographically coherent landscape images. Furthermore, we propose a\npractically-informed geo-contextual evaluation framework, the Place Similarity\nScore (PSS), across element-, scene-, and human perception-levels to measure\nconsistency between input soundscapes and generated landscape images. Extensive\nexperiments demonstrate that SounDiT outperforms existing baselines in both\nvisual fidelity and geographic settings. Our work not only establishes\nfoundational benchmarks for GeoS2L generation but also highlights the\nimportance of incorporating geographic domain knowledge in advancing multimodal\ngenerative models, opening new directions at the intersection of generative AI,\ngeography, urban planning, and environmental sciences."}
{"id": "2505.11793", "pdf": "https://arxiv.org/pdf/2505.11793", "abs": "https://arxiv.org/abs/2505.11793", "authors": ["Jianing Wang", "Siying Guo", "Zheng Hua", "Runhu Huang", "Jinyu Hu", "Maoguo Gong"], "title": "CL-CaGAN: Capsule differential adversarial continuous learning for cross-domain hyperspectral anomaly detection", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "Anomaly detection (AD) has attracted remarkable attention in hyperspectral\nimage (HSI) processing fields, and most existing deep learning (DL)-based\nalgorithms indicate dramatic potential for detecting anomaly samples through\nspecific training process under current scenario. However, the limited prior\ninformation and the catastrophic forgetting problem indicate crucial challenges\nfor existing DL structure in open scenarios cross-domain detection. In order to\nimprove the detection performance, a novel continual learning-based capsule\ndifferential generative adversarial network (CL-CaGAN) is proposed to elevate\nthe cross-scenario learning performance for facilitating the real application\nof DL-based structure in hyperspectral AD (HAD) task. First, a modified capsule\nstructure with adversarial learning network is constructed to estimate the\nbackground distribution for surmounting the deficiency of prior information. To\nmitigate the catastrophic forgetting phenomenon, clustering-based sample replay\nstrategy and a designed extra self-distillation regularization are integrated\nfor merging the history and future knowledge in continual AD task, while the\ndiscriminative learning ability from previous detection scenario to current\nscenario is retained by the elaborately designed structure with continual\nlearning (CL) strategy. In addition, the differentiable enhancement is enforced\nto augment the generation performance of the training data. This further\nstabilizes the training process with better convergence and efficiently\nconsolidates the reconstruction ability of background samples. To verify the\neffectiveness of our proposed CL-CaGAN, we conduct experiments on several real\nHSIs, and the results indicate that the proposed CL-CaGAN demonstrates higher\ndetection performance and continuous learning capacity for mitigating the\ncatastrophic forgetting under cross-domain scenarios."}
{"id": "2505.11683", "pdf": "https://arxiv.org/pdf/2505.11683", "abs": "https://arxiv.org/abs/2505.11683", "authors": ["Susanna Rücker", "Alan Akbik"], "title": "Evaluating Design Decisions for Dual Encoder-based Entity Disambiguation", "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 (The 63rd Annual Meeting of the Association for\n  Computational Linguistics)", "summary": "Entity disambiguation (ED) is the task of linking mentions in text to\ncorresponding entries in a knowledge base. Dual Encoders address this by\nembedding mentions and label candidates in a shared embedding space and\napplying a similarity metric to predict the correct label. In this work, we\nfocus on evaluating key design decisions for Dual Encoder-based ED, such as its\nloss function, similarity metric, label verbalization format, and negative\nsampling strategy. We present the resulting model VerbalizED, a document-level\nDual Encoder model that includes contextual label verbalizations and efficient\nhard negative sampling. Additionally, we explore an iterative prediction\nvariant that aims to improve the disambiguation of challenging data points.\nComprehensive experiments on AIDA-Yago validate the effectiveness of our\napproach, offering insights into impactful design choices that result in a new\nState-of-the-Art system on the ZELDA benchmark."}
{"id": "2505.11796", "pdf": "https://arxiv.org/pdf/2505.11796", "abs": "https://arxiv.org/abs/2505.11796", "authors": ["Jianing Wang", "Zheng Hua", "Wan Zhang", "Shengjia Hao", "Yuqiong Yao", "Maoguo Gong"], "title": "CL-BioGAN: Biologically-Inspired Cross-Domain Continual Learning for Hyperspectral Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Memory stability and learning flexibility in continual learning (CL) is a\ncore challenge for cross-scene Hyperspectral Anomaly Detection (HAD) task.\nBiological neural networks can actively forget history knowledge that conflicts\nwith the learning of new experiences by regulating learning-triggered synaptic\nexpansion and synaptic convergence. Inspired by this phenomenon, we propose a\nnovel Biologically-Inspired Continual Learning Generative Adversarial Network\n(CL-BioGAN) for augmenting continuous distribution fitting ability for\ncross-domain HAD task, where Continual Learning Bio-inspired Loss (CL-Bio Loss)\nand self-attention Generative Adversarial Network (BioGAN) are incorporated to\nrealize forgetting history knowledge as well as involving replay strategy in\nthe proposed BioGAN. Specifically, a novel Bio-Inspired Loss composed with an\nActive Forgetting Loss (AF Loss) and a CL loss is designed to realize\nparameters releasing and enhancing between new task and history tasks from a\nBayesian perspective. Meanwhile, BioGAN loss with L2-Norm enhances\nself-attention (SA) to further balance the stability and flexibility for better\nfitting background distribution for open scenario HAD (OHAD) tasks. Experiment\nresults underscore that the proposed CL-BioGAN can achieve more robust and\nsatisfying accuracy for cross-domain HAD with fewer parameters and computation\ncost. This dual contribution not only elevates CL performance but also offers\nnew insights into neural adaptation mechanisms in OHAD task."}
{"id": "2505.11690", "pdf": "https://arxiv.org/pdf/2505.11690", "abs": "https://arxiv.org/abs/2505.11690", "authors": ["Sukairaj Hafiz Imam", "Babangida Sani", "Dawit Ketema Gete", "Bedru Yimam Ahamed", "Ibrahim Said Ahmad", "Idris Abdulmumin", "Seid Muhie Yimam", "Muhammad Yahuza Bello", "Shamsuddeen Hassan Muhammad"], "title": "Automatic Speech Recognition for African Low-Resource Languages: Challenges and Future Directions", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Automatic Speech Recognition (ASR) technologies have transformed\nhuman-computer interaction; however, low-resource languages in Africa remain\nsignificantly underrepresented in both research and practical applications.\nThis study investigates the major challenges hindering the development of ASR\nsystems for these languages, which include data scarcity, linguistic\ncomplexity, limited computational resources, acoustic variability, and ethical\nconcerns surrounding bias and privacy. The primary goal is to critically\nanalyze these barriers and identify practical, inclusive strategies to advance\nASR technologies within the African context. Recent advances and case studies\nemphasize promising strategies such as community-driven data collection,\nself-supervised and multilingual learning, lightweight model architectures, and\ntechniques that prioritize privacy. Evidence from pilot projects involving\nvarious African languages showcases the feasibility and impact of customized\nsolutions, which encompass morpheme-based modeling and domain-specific ASR\napplications in sectors like healthcare and education. The findings highlight\nthe importance of interdisciplinary collaboration and sustained investment to\ntackle the distinct linguistic and infrastructural challenges faced by the\ncontinent. This study offers a progressive roadmap for creating ethical,\nefficient, and inclusive ASR systems that not only safeguard linguistic\ndiversity but also improve digital accessibility and promote socioeconomic\nparticipation for speakers of African languages."}
{"id": "2505.11800", "pdf": "https://arxiv.org/pdf/2505.11800", "abs": "https://arxiv.org/abs/2505.11800", "authors": ["Jian Zhu", "He Wang", "Yang Xu", "Zebin Wu", "Zhihui Wei"], "title": "Self-Learning Hyperspectral and Multispectral Image Fusion via Adaptive Residual Guided Subspace Diffusion Model", "categories": ["cs.CV", "eess.IV"], "comment": "cvpr", "summary": "Hyperspectral and multispectral image (HSI-MSI) fusion involves combining a\nlow-resolution hyperspectral image (LR-HSI) with a high-resolution\nmultispectral image (HR-MSI) to generate a high-resolution hyperspectral image\n(HR-HSI). Most deep learning-based methods for HSI-MSI fusion rely on large\namounts of hyperspectral data for supervised training, which is often scarce in\npractical applications. In this paper, we propose a self-learning Adaptive\nResidual Guided Subspace Diffusion Model (ARGS-Diff), which only utilizes the\nobserved images without any extra training data. Specifically, as the LR-HSI\ncontains spectral information and the HR-MSI contains spatial information, we\ndesign two lightweight spectral and spatial diffusion models to separately\nlearn the spectral and spatial distributions from them. Then, we use these two\nmodels to reconstruct HR-HSI from two low-dimensional components, i.e, the\nspectral basis and the reduced coefficient, during the reverse diffusion\nprocess. Furthermore, we introduce an Adaptive Residual Guided Module (ARGM),\nwhich refines the two components through a residual guided function at each\nsampling step, thereby stabilizing the sampling process. Extensive experimental\nresults demonstrate that ARGS-Diff outperforms existing state-of-the-art\nmethods in terms of both performance and computational efficiency in the field\nof HSI-MSI fusion. Code is available at https://github.com/Zhu1116/ARGS-Diff."}
{"id": "2505.11693", "pdf": "https://arxiv.org/pdf/2505.11693", "abs": "https://arxiv.org/abs/2505.11693", "authors": ["Ana Ezquerro", "David Vilares", "Anssi Yli-Jyrä", "Carlos Gómez-Rodríguez"], "title": "Hierarchical Bracketing Encodings for Dependency Parsing as Tagging", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025. Original submission; camera-ready coming soon", "summary": "We present a family of encodings for sequence labeling dependency parsing,\nbased on the concept of hierarchical bracketing. We prove that the existing\n4-bit projective encoding belongs to this family, but it is suboptimal in the\nnumber of labels used to encode a tree. We derive an optimal hierarchical\nbracketing, which minimizes the number of symbols used and encodes projective\ntrees using only 12 distinct labels (vs. 16 for the 4-bit encoding). We also\nextend optimal hierarchical bracketing to support arbitrary non-projectivity in\na more compact way than previous encodings. Our new encodings yield competitive\naccuracy on a diverse set of treebanks."}
{"id": "2505.11804", "pdf": "https://arxiv.org/pdf/2505.11804", "abs": "https://arxiv.org/abs/2505.11804", "authors": ["Xi Wang", "Eric Nalisnick"], "title": "Are vision language models robust to uncertain inputs?", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Robustness against uncertain and ambiguous inputs is a critical challenge for\ndeep learning models. While recent advancements in large scale vision language\nmodels (VLMs, e.g. GPT4o) might suggest that increasing model and training\ndataset size would mitigate this issue, our empirical evaluation shows a more\ncomplicated picture. Testing models using two classic uncertainty\nquantification tasks, anomaly detection and classification under inherently\nambiguous conditions, we find that newer and larger VLMs indeed exhibit\nimproved robustness compared to earlier models, but still suffer from a\ntendency to strictly follow instructions, often causing them to hallucinate\nconfident responses even when faced with unclear or anomalous inputs.\nRemarkably, for natural images such as ImageNet, this limitation can be\novercome without pipeline modifications: simply prompting models to abstain\nfrom uncertain predictions enables significant reliability gains, achieving\nnear-perfect robustness in several settings. However, for domain-specific tasks\nsuch as galaxy morphology classification, a lack of specialized knowledge\nprevents reliable uncertainty estimation. Finally, we propose a novel mechanism\nbased on caption diversity to reveal a model's internal uncertainty, enabling\npractitioners to predict when models will successfully abstain without relying\non labeled data."}
{"id": "2505.11726", "pdf": "https://arxiv.org/pdf/2505.11726", "abs": "https://arxiv.org/abs/2505.11726", "authors": ["Shun Inadumi", "Nobuhiro Ueda", "Koichiro Yoshino"], "title": "Disambiguating Reference in Visually Grounded Dialogues through Joint Modeling of Textual and Multimodal Semantic Structures", "categories": ["cs.CL"], "comment": "ACL2025 main. Code available at https://github.com/SInadumi/mmrr", "summary": "Multimodal reference resolution, including phrase grounding, aims to\nunderstand the semantic relations between mentions and real-world objects.\nPhrase grounding between images and their captions is a well-established task.\nIn contrast, for real-world applications, it is essential to integrate textual\nand multimodal reference resolution to unravel the reference relations within\ndialogue, especially in handling ambiguities caused by pronouns and ellipses.\nThis paper presents a framework that unifies textual and multimodal reference\nresolution by mapping mention embeddings to object embeddings and selecting\nmentions or objects based on their similarity. Our experiments show that\nlearning textual reference resolution, such as coreference resolution and\npredicate-argument structure analysis, positively affects performance in\nmultimodal reference resolution. In particular, our model with coreference\nresolution performs better in pronoun phrase grounding than representative\nmodels for this task, MDETR and GLIP. Our qualitative analysis demonstrates\nthat incorporating textual reference relations strengthens the confidence\nscores between mentions, including pronouns and predicates, and objects, which\ncan reduce the ambiguities that arise in visually grounded dialogues."}
{"id": "2505.11809", "pdf": "https://arxiv.org/pdf/2505.11809", "abs": "https://arxiv.org/abs/2505.11809", "authors": ["Zicheng Fan", "Kunihiko Fujiwara", "Pengyuan Liu", "Fan Zhang", "Filip Biljecki"], "title": "Image-based Visibility Analysis Replacing Line-of-Sight Simulation: An Urban Landmark Perspective", "categories": ["cs.CV"], "comment": null, "summary": "Visibility analysis is one of the fundamental analytics methods in urban\nplanning and landscape research, traditionally conducted through computational\nsimulations based on the Line-of-Sight (LoS) principle. However, when assessing\nthe visibility of named urban objects such as landmarks, geometric intersection\nalone fails to capture the contextual and perceptual dimensions of visibility\nas experienced in the real world. The study challenges the traditional\nLoS-based approaches by introducing a new, image-based visibility analysis\nmethod. Specifically, a Vision Language Model (VLM) is applied to detect the\ntarget object within a direction-zoomed Street View Image (SVI). Successful\ndetection represents the object's visibility at the corresponding SVI location.\nFurther, a heterogeneous visibility graph is constructed to address the complex\ninteraction between observers and target objects. In the first case study, the\nmethod proves its reliability in detecting the visibility of six tall landmark\nconstructions in global cities, with an overall accuracy of 87%. Furthermore,\nit reveals broader contextual differences when the landmarks are perceived and\nexperienced. In the second case, the proposed visibility graph uncovers the\nform and strength of connections for multiple landmarks along the River Thames\nin London, as well as the places where these connections occur. Notably,\nbridges on the River Thames account for approximately 30% of total connections.\nOur method complements and enhances traditional LoS-based visibility analysis,\nand showcases the possibility of revealing the prevalent connection of any\nvisual objects in the urban environment. It opens up new research perspectives\nfor urban planning, heritage conservation, and computational social science."}
{"id": "2505.11733", "pdf": "https://arxiv.org/pdf/2505.11733", "abs": "https://arxiv.org/abs/2505.11733", "authors": ["Kevin Wu", "Eric Wu", "Rahul Thapa", "Kevin Wei", "Angela Zhang", "Arvind Suresh", "Jacqueline J. Tao", "Min Woo Sun", "Alejandro Lozano", "James Zou"], "title": "MedCaseReasoning: Evaluating and learning diagnostic reasoning from clinical case reports", "categories": ["cs.CL"], "comment": null, "summary": "Doctors and patients alike increasingly use Large Language Models (LLMs) to\ndiagnose clinical cases. However, unlike domains such as math or coding, where\ncorrectness can be objectively defined by the final answer, medical diagnosis\nrequires both the outcome and the reasoning process to be accurate. Currently,\nwidely used medical benchmarks like MedQA and MMLU assess only accuracy in the\nfinal answer, overlooking the quality and faithfulness of the clinical\nreasoning process. To address this limitation, we introduce MedCaseReasoning,\nthe first open-access dataset for evaluating LLMs on their ability to align\nwith clinician-authored diagnostic reasoning. The dataset includes 14,489\ndiagnostic question-and-answer cases, each paired with detailed reasoning\nstatements derived from open-access medical case reports. We evaluate\nstate-of-the-art reasoning LLMs on MedCaseReasoning and find significant\nshortcomings in their diagnoses and reasoning: for instance, the top-performing\nopen-source model, DeepSeek-R1, achieves only 48% 10-shot diagnostic accuracy\nand mentions only 64% of the clinician reasoning statements (recall). However,\nwe demonstrate that fine-tuning LLMs on the reasoning traces derived from\nMedCaseReasoning significantly improves diagnostic accuracy and clinical\nreasoning recall by an average relative gain of 29% and 41%, respectively. The\nopen-source dataset, code, and models are available at\nhttps://github.com/kevinwu23/Stanford-MedCaseReasoning."}
{"id": "2505.11813", "pdf": "https://arxiv.org/pdf/2505.11813", "abs": "https://arxiv.org/abs/2505.11813", "authors": ["Yixuan Dong", "Fang-Yi Su", "Jung-Hsien Chiang"], "title": "SGD-Mix: Enhancing Domain-Specific Image Classification with Label-Preserving Data Augmentation", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 6 figures, 6 tables", "summary": "Data augmentation for domain-specific image classification tasks often\nstruggles to simultaneously address diversity, faithfulness, and label clarity\nof generated data, leading to suboptimal performance in downstream tasks. While\nexisting generative diffusion model-based methods aim to enhance augmentation,\nthey fail to cohesively tackle these three critical aspects and often overlook\nintrinsic challenges of diffusion models, such as sensitivity to model\ncharacteristics and stochasticity under strong transformations. In this paper,\nwe propose a novel framework that explicitly integrates diversity,\nfaithfulness, and label clarity into the augmentation process. Our approach\nemploys saliency-guided mixing and a fine-tuned diffusion model to preserve\nforeground semantics, enrich background diversity, and ensure label\nconsistency, while mitigating diffusion model limitations. Extensive\nexperiments across fine-grained, long-tail, few-shot, and background robustness\ntasks demonstrate our method's superior performance over state-of-the-art\napproaches."}
{"id": "2505.11739", "pdf": "https://arxiv.org/pdf/2505.11739", "abs": "https://arxiv.org/abs/2505.11739", "authors": ["Feijiang Han", "Xiaodong Yu", "Jianheng Tang", "Lyle Ungar"], "title": "ZeroTuning: Unlocking the Initial Token's Power to Enhance Large Language Models Without Training", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, training-free methods for improving large language models (LLMs)\nhave attracted growing interest, with token-level attention tuning emerging as\na promising and interpretable direction. However, existing methods typically\nrely on auxiliary mechanisms to identify important or irrelevant task-specific\ntokens, introducing potential bias and limiting applicability. In this paper,\nwe uncover a surprising and elegant alternative: the semantically empty initial\ntoken is a powerful and underexplored control point for optimizing model\nbehavior. Through theoretical analysis, we show that tuning the initial token's\nattention sharpens or flattens the attention distribution over subsequent\ntokens, and its role as an attention sink amplifies this effect. Empirically,\nwe find that: (1) tuning its attention improves LLM performance more\neffectively than tuning other task-specific tokens; (2) the effect follows a\nconsistent trend across layers, with earlier layers having greater impact, but\nvaries across attention heads, with different heads showing distinct\npreferences in how they attend to this token. Based on these findings, we\npropose ZeroTuning, a training-free approach that improves LLM performance by\napplying head-specific attention adjustments to this special token. Despite\ntuning only one token, ZeroTuning achieves higher performance on text\nclassification, multiple-choice, and multi-turn conversation tasks across\nmodels such as Llama, Qwen, and DeepSeek. For example, ZeroTuning improves\nLlama-3.1-8B by 11.71% on classification, 2.64% on QA tasks, and raises its\nmulti-turn score from 7.804 to 7.966. The method is also robust to limited\nresources, few-shot settings, long contexts, quantization, decoding strategies,\nand prompt variations. Our work sheds light on a previously overlooked control\npoint in LLMs, offering new insights into both inference-time tuning and model\ninterpretability."}
{"id": "2505.11815", "pdf": "https://arxiv.org/pdf/2505.11815", "abs": "https://arxiv.org/abs/2505.11815", "authors": ["Jiajun Qin", "Yuan Pu", "Zhuolun He", "Seunggeun Kim", "David Z. Pan", "Bei Yu"], "title": "UniMoCo: Unified Modality Completion for Robust Multi-Modal Embeddings", "categories": ["cs.CV"], "comment": null, "summary": "Current research has explored vision-language models for multi-modal\nembedding tasks, such as information retrieval, visual grounding, and\nclassification. However, real-world scenarios often involve diverse modality\ncombinations between queries and targets, such as text and image to text, text\nand image to text and image, and text to text and image. These diverse\ncombinations pose significant challenges for existing models, as they struggle\nto align all modality combinations within a unified embedding space during\ntraining, which degrades performance at inference. To address this limitation,\nwe propose UniMoCo, a novel vision-language model architecture designed for\nmulti-modal embedding tasks. UniMoCo introduces a modality-completion module\nthat generates visual features from textual inputs, ensuring modality\ncompleteness for both queries and targets. Additionally, we develop a\nspecialized training strategy to align embeddings from both original and\nmodality-completed inputs, ensuring consistency within the embedding space.\nThis enables the model to robustly handle a wide range of modality combinations\nacross embedding tasks. Experiments show that UniMoCo outperforms previous\nmethods while demonstrating consistent robustness across diverse settings. More\nimportantly, we identify and quantify the inherent bias in conventional\napproaches caused by imbalance of modality combinations in training data, which\ncan be mitigated through our modality-completion paradigm. The code is\navailable at https://github.com/HobbitQia/UniMoCo."}
{"id": "2505.11746", "pdf": "https://arxiv.org/pdf/2505.11746", "abs": "https://arxiv.org/abs/2505.11746", "authors": ["Xianglong Xu", "John Bowen", "Rojin Taheri"], "title": "Token Masking Improves Transformer-Based Text Classification", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While transformer-based models achieve strong performance on text\nclassification, we explore whether masking input tokens can further enhance\ntheir effectiveness. We propose token masking regularization, a simple yet\ntheoretically motivated method that randomly replaces input tokens with a\nspecial [MASK] token at probability p. This introduces stochastic perturbations\nduring training, leading to implicit gradient averaging that encourages the\nmodel to capture deeper inter-token dependencies. Experiments on language\nidentification and sentiment analysis -- across diverse models (mBERT,\nQwen2.5-0.5B, TinyLlama-1.1B) -- show consistent improvements over standard\nregularization techniques. We identify task-specific optimal masking rates,\nwith p = 0.1 as a strong general default. We attribute the gains to two key\neffects: (1) input perturbation reduces overfitting, and (2) gradient-level\nsmoothing acts as implicit ensembling."}
{"id": "2505.11816", "pdf": "https://arxiv.org/pdf/2505.11816", "abs": "https://arxiv.org/abs/2505.11816", "authors": ["Quan Cheng", "Yuanyu Wan", "Lingyu Wu", "Chenping Hou", "Lijun Zhang"], "title": "Continuous Subspace Optimization for Continual Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Continual learning aims to learn multiple tasks sequentially while preserving\nprior knowledge, but faces the challenge of catastrophic forgetting when\nacquiring new knowledge. Recently, approaches leveraging pre-trained models\nhave gained increasing popularity to mitigate this issue, due to the strong\ngeneralization ability of foundation models. To adjust pre-trained models for\nnew tasks, existing methods usually employ low-rank adaptation, which restricts\nparameter updates to a fixed low-rank subspace. However, constraining the\noptimization space inherently compromises the model's learning capacity,\nresulting in inferior performance. To address the limitation, we propose\nContinuous Subspace Optimization for Continual Learning (CoSO) to fine-tune the\nmodel in a series of subspaces rather than a single one. These sequential\nsubspaces are dynamically determined through the singular value decomposition\nof gradients. CoSO updates the model by projecting gradients into these\nsubspaces, ensuring memory-efficient optimization. To mitigate forgetting, the\noptimization subspaces of each task are set to be orthogonal to the historical\ntask subspace. During task learning, CoSO maintains a task-specific component\nthat captures the critical update directions associated with the current task.\nUpon completing a task, this component is used to update the historical task\nsubspace, laying the groundwork for subsequent learning. Extensive experiments\non multiple datasets demonstrate that CoSO significantly outperforms\nstate-of-the-art methods, especially in challenging scenarios with long task\nsequences."}
{"id": "2505.11754", "pdf": "https://arxiv.org/pdf/2505.11754", "abs": "https://arxiv.org/abs/2505.11754", "authors": ["Wenyu Huang", "Pavlos Vougiouklis", "Mirella Lapata", "Jeff Z. Pan"], "title": "Masking in Multi-hop QA: An Analysis of How Language Models Perform with Context Permutation", "categories": ["cs.CL"], "comment": "ACL 2025 main", "summary": "Multi-hop Question Answering (MHQA) adds layers of complexity to question\nanswering, making it more challenging. When Language Models (LMs) are prompted\nwith multiple search results, they are tasked not only with retrieving relevant\ninformation but also employing multi-hop reasoning across the information\nsources. Although LMs perform well on traditional question-answering tasks, the\ncausal mask can hinder their capacity to reason across complex contexts. In\nthis paper, we explore how LMs respond to multi-hop questions by permuting\nsearch results (retrieved documents) under various configurations. Our study\nreveals interesting findings as follows: 1) Encoder-decoder models, such as the\nones in the Flan-T5 family, generally outperform causal decoder-only LMs in\nMHQA tasks, despite being significantly smaller in size; 2) altering the order\nof gold documents reveals distinct trends in both Flan T5 models and fine-tuned\ndecoder-only models, with optimal performance observed when the document order\naligns with the reasoning chain order; 3) enhancing causal decoder-only models\nwith bi-directional attention by modifying the causal mask can effectively\nboost their end performance. In addition to the above, we conduct a thorough\ninvestigation of the distribution of LM attention weights in the context of\nMHQA. Our experiments reveal that attention weights tend to peak at higher\nvalues when the resulting answer is correct. We leverage this finding to\nheuristically improve LMs' performance on this task. Our code is publicly\navailable at https://github.com/hwy9855/MultiHopQA-Reasoning."}
{"id": "2505.11822", "pdf": "https://arxiv.org/pdf/2505.11822", "abs": "https://arxiv.org/abs/2505.11822", "authors": ["Ke Li", "Di Wang", "Xiaowei Wang", "Zhihong Wu", "Yiming Zhang", "Yifeng Wang", "Quan Wang"], "title": "Robust Cross-View Geo-Localization via Content-Viewpoint Disentanglement", "categories": ["cs.CV"], "comment": null, "summary": "Cross-view geo-localization (CVGL) aims to match images of the same\ngeographic location captured from different perspectives, such as drones and\nsatellites. Despite recent advances, CVGL remains highly challenging due to\nsignificant appearance changes and spatial distortions caused by viewpoint\nvariations. Existing methods typically assume that cross-view images can be\ndirectly aligned within a shared feature space by maximizing feature similarity\nthrough contrastive learning. Nonetheless, this assumption overlooks the\ninherent conflicts induced by viewpoint discrepancies, resulting in extracted\nfeatures containing inconsistent information that hinders precise localization.\nIn this study, we take a manifold learning perspective and model the feature\nspace of cross-view images as a composite manifold jointly governed by content\nand viewpoint information. Building upon this insight, we propose\n$\\textbf{CVD}$, a new CVGL framework that explicitly disentangles\n$\\textit{content}$ and $\\textit{viewpoint}$ factors. To promote effective\ndisentanglement, we introduce two constraints: $\\textit{(i)}$ An intra-view\nindependence constraint, which encourages statistical independence between the\ntwo factors by minimizing their mutual information. $\\textit{(ii)}$ An\ninter-view reconstruction constraint that reconstructs each view by\ncross-combining $\\textit{content}$ and $\\textit{viewpoint}$ from paired images,\nensuring factor-specific semantics are preserved. As a plug-and-play module,\nCVD can be seamlessly integrated into existing geo-localization pipelines.\nExtensive experiments on four benchmarks, i.e., University-1652, SUES-200,\nCVUSA, and CVACT, demonstrate that CVD consistently improves both localization\naccuracy and generalization across multiple baselines."}
{"id": "2505.11764", "pdf": "https://arxiv.org/pdf/2505.11764", "abs": "https://arxiv.org/abs/2505.11764", "authors": ["Raymond Baartmans", "Matthew Raffel", "Rahul Vikram", "Aiden Deringer", "Lizhong Chen"], "title": "Towards Universal Semantics With Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The Natural Semantic Metalanguage (NSM) is a linguistic theory based on a\nuniversal set of semantic primes: simple, primitive word-meanings that have\nbeen shown to exist in most, if not all, languages of the world. According to\nthis framework, any word, regardless of complexity, can be paraphrased using\nthese primes, revealing a clear and universally translatable meaning. These\nparaphrases, known as explications, can offer valuable applications for many\nnatural language processing (NLP) tasks, but producing them has traditionally\nbeen a slow, manual process. In this work, we present the first study of using\nlarge language models (LLMs) to generate NSM explications. We introduce\nautomatic evaluation methods, a tailored dataset for training and evaluation,\nand fine-tuned models for this task. Our 1B and 8B models outperform GPT-4o in\nproducing accurate, cross-translatable explications, marking a significant step\ntoward universal semantic representation with LLMs and opening up new\npossibilities for applications in semantic analysis, translation, and beyond."}
{"id": "2505.11825", "pdf": "https://arxiv.org/pdf/2505.11825", "abs": "https://arxiv.org/abs/2505.11825", "authors": ["Xudong Ma"], "title": "Bootstrapping Diffusion: Diffusion Model Training Leveraging Partial and Corrupted Data", "categories": ["cs.CV", "cs.AI"], "comment": "21 pages, 1 figure", "summary": "Training diffusion models requires large datasets. However, acquiring large\nvolumes of high-quality data can be challenging, for example, collecting large\nnumbers of high-resolution images and long videos. On the other hand, there are\nmany complementary data that are usually considered corrupted or partial, such\nas low-resolution images and short videos. Other examples of corrupted data\ninclude videos that contain subtitles, watermarks, and logos. In this study, we\ninvestigate the theoretical problem of whether the above partial data can be\nutilized to train conventional diffusion models. Motivated by our theoretical\nanalysis in this study, we propose a straightforward approach of training\ndiffusion models utilizing partial data views, where we consider each form of\ncomplementary data as a view of conventional data. Our proposed approach first\ntrains one separate diffusion model for each individual view, and then trains a\nmodel for predicting the residual score function. We prove generalization error\nbounds, which show that the proposed diffusion model training approach can\nachieve lower generalization errors if proper regularizations are adopted in\nthe residual score function training. In particular, we prove that the\ndifficulty in training the residual score function scales proportionally with\nthe signal correlations not captured by partial data views. Consequently, the\nproposed approach achieves near first-order optimal data efficiency."}
{"id": "2505.11807", "pdf": "https://arxiv.org/pdf/2505.11807", "abs": "https://arxiv.org/abs/2505.11807", "authors": ["Yufei Xiang", "Yiqun Shen", "Yeqin Zhang", "Cam-Tu Nguyen"], "title": "Retrospex: Language Agent Meets Offline Reinforcement Learning Critic", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages", "summary": "Large Language Models (LLMs) possess extensive knowledge and commonsense\nreasoning capabilities, making them valuable for creating powerful agents.\nHowever, existing LLM agent frameworks have not fully utilized past experiences\nfor improvement. This work introduces a new LLM-based agent framework called\nRetrospex, which addresses this challenge by analyzing past experiences in\ndepth. Unlike previous approaches, Retrospex does not directly integrate\nexperiences into the LLM's context. Instead, it combines the LLM's action\nlikelihood with action values estimated by a Reinforcement Learning (RL)\nCritic, which is trained on past experiences through an offline\n''retrospection'' process. Additionally, Retrospex employs a dynamic action\nrescoring mechanism that increases the importance of experience-based values\nfor tasks that require more interaction with the environment. We evaluate\nRetrospex in ScienceWorld, ALFWorld and Webshop environments, demonstrating its\nadvantages over strong, contemporary baselines."}
{"id": "2505.11830", "pdf": "https://arxiv.org/pdf/2505.11830", "abs": "https://arxiv.org/abs/2505.11830", "authors": ["Hongbo Jin", "Ruyang Liu", "Wenhao Zhang", "Guibo Luo", "Ge Li"], "title": "CoT-Vid: Dynamic Chain-of-Thought Routing with Self Verification for Training-Free Video Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": "19 pages, 7 figures", "summary": "System2 reasoning is developing rapidly these days with the emergence of\nDeep- Thinking Models and chain-of-thought technology, which has become a\ncentralized discussion point in the AI community. However, there is a relative\ngap in the research on complex video reasoning at present. In this work, we\npropose CoT-Vid, a novel training-free paradigm for the video domain with a\nmultistage complex reasoning design. Distinguishing from existing video LLMs,\nwhich rely heavily on perceptual abilities, it achieved surprising performance\ngain with explicit reasoning mechanism. The paradigm consists of three main\ncomponents: dynamic inference path routing, problem decoupling strategy, and\nvideo self-consistency verification. In addition, we propose a new standard for\ncategorization of video questions. CoT- Vid showed outstanding results on a\nwide range of benchmarks, and outperforms its base model by 9.3% on Egochema\nand 5.6% on VideoEspresso, rivalling or even surpassing larger and proprietary\nmodels, such as GPT-4V, GPT-4o and Gemini-1.5-flash. Our codebase will be\npublicly available soon."}
{"id": "2505.11810", "pdf": "https://arxiv.org/pdf/2505.11810", "abs": "https://arxiv.org/abs/2505.11810", "authors": ["Shen Li", "Renfen Hu", "Lijun Wang"], "title": "Efficiently Building a Domain-Specific Large Language Model from Scratch: A Case Study of a Classical Chinese Large Language Model", "categories": ["cs.CL"], "comment": null, "summary": "General-purpose large language models demonstrate notable capabilities in\nlanguage comprehension and generation, achieving results that are comparable\nto, or even surpass, human performance in many language information processing\ntasks. Nevertheless, when general models are applied to some specific domains,\ne.g., Classical Chinese texts, their effectiveness is often unsatisfactory, and\nfine-tuning open-source foundational models similarly struggles to adequately\nincorporate domain-specific knowledge. To address this challenge, this study\ndeveloped a large language model, AI Taiyan, specifically designed for\nunderstanding and generating Classical Chinese. Experiments show that with a\nreasonable model design, data processing, foundational training, and\nfine-tuning, satisfactory results can be achieved with only 1.8 billion\nparameters. In key tasks related to Classical Chinese information processing\nsuch as punctuation, identification of allusions, explanation of word meanings,\nand translation between ancient and modern Chinese, this model exhibits a clear\nadvantage over both general-purpose large models and domain-specific\ntraditional models, achieving levels close to or surpassing human baselines.\nThis research provides a reference for the efficient construction of\nspecialized domain-specific large language models. Furthermore, the paper\ndiscusses the application of this model in fields such as the collation of\nancient texts, dictionary editing, and language research, combined with case\nstudies."}
{"id": "2505.11838", "pdf": "https://arxiv.org/pdf/2505.11838", "abs": "https://arxiv.org/abs/2505.11838", "authors": ["Yiqing Shen", "Chenjia Li", "Chenxiao Fan", "Mathias Unberath"], "title": "RVTBench: A Benchmark for Visual Reasoning Tasks", "categories": ["cs.CV"], "comment": null, "summary": "Visual reasoning, the capability to interpret visual input in response to\nimplicit text query through multi-step reasoning, remains a challenge for deep\nlearning models due to the lack of relevant benchmarks. Previous work in visual\nreasoning has primarily focused on reasoning segmentation, where models aim to\nsegment objects based on implicit text queries. This paper introduces reasoning\nvisual tasks (RVTs), a unified formulation that extends beyond traditional\nvideo reasoning segmentation to a diverse family of visual language reasoning\nproblems, which can therefore accommodate multiple output formats including\nbounding boxes, natural language descriptions, and question-answer pairs.\nCorrespondingly, we identify the limitations in current benchmark construction\nmethods that rely solely on large language models (LLMs), which inadequately\ncapture complex spatial-temporal relationships and multi-step reasoning chains\nin video due to their reliance on token representation, resulting in benchmarks\nwith artificially limited reasoning complexity. To address this limitation, we\npropose a novel automated RVT benchmark construction pipeline that leverages\ndigital twin (DT) representations as structured intermediaries between\nperception and the generation of implicit text queries. Based on this method,\nwe construct RVTBench, a RVT benchmark containing 3,896 queries of over 1.2\nmillion tokens across four types of RVT (segmentation, grounding, VQA and\nsummary), three reasoning categories (semantic, spatial, and temporal), and\nfour increasing difficulty levels, derived from 200 video sequences. Finally,\nwe propose RVTagent, an agent framework for RVT that allows for zero-shot\ngeneralization across various types of RVT without task-specific fine-tuning."}
{"id": "2505.11811", "pdf": "https://arxiv.org/pdf/2505.11811", "abs": "https://arxiv.org/abs/2505.11811", "authors": ["Taolin Zhang", "Dongyang Li", "Qizhou Chen", "Chengyu Wang", "Xiaofeng He"], "title": "BELLE: A Bi-Level Multi-Agent Reasoning Framework for Multi-Hop Question Answering", "categories": ["cs.CL"], "comment": "Accepted by ACL2025 main track", "summary": "Multi-hop question answering (QA) involves finding multiple relevant passages\nand performing step-by-step reasoning to answer complex questions. Previous\nworks on multi-hop QA employ specific methods from different modeling\nperspectives based on large language models (LLMs), regardless of the question\ntypes. In this paper, we first conduct an in-depth analysis of public multi-hop\nQA benchmarks, dividing the questions into four types and evaluating five types\nof cutting-edge methods for multi-hop QA: Chain-of-Thought (CoT), Single-step,\nIterative-step, Sub-step, and Adaptive-step. We find that different types of\nmulti-hop questions have varying degrees of sensitivity to different types of\nmethods. Thus, we propose a Bi-levEL muLti-agEnt reasoning (BELLE) framework to\naddress multi-hop QA by specifically focusing on the correspondence between\nquestion types and methods, where each type of method is regarded as an\n''operator'' by prompting LLMs differently. The first level of BELLE includes\nmultiple agents that debate to obtain an executive plan of combined\n''operators'' to address the multi-hop QA task comprehensively. During the\ndebate, in addition to the basic roles of affirmative debater, negative\ndebater, and judge, at the second level, we further leverage fast and slow\ndebaters to monitor whether changes in viewpoints are reasonable. Extensive\nexperiments demonstrate that BELLE significantly outperforms strong baselines\nin various datasets. Additionally, the model consumption of BELLE is higher\ncost-effectiveness than that of single models in more complex multi-hop QA\nscenarios."}
{"id": "2505.11842", "pdf": "https://arxiv.org/pdf/2505.11842", "abs": "https://arxiv.org/abs/2505.11842", "authors": ["Xuannan Liu", "Zekun Li", "Zheqi He", "Peipei Li", "Shuhan Xia", "Xing Cui", "Huaibo Huang", "Xi Yang", "Ran He"], "title": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs", "categories": ["cs.CV", "cs.CL"], "comment": "Project page:\n  https://liuxuannan.github.io/Video-SafetyBench.github.io/", "summary": "The increasing deployment of Large Vision-Language Models (LVLMs) raises\nsafety concerns under potential malicious inputs. However, existing multimodal\nsafety evaluations primarily focus on model vulnerabilities exposed by static\nimage inputs, ignoring the temporal dynamics of video that may induce distinct\nsafety risks. To bridge this gap, we introduce Video-SafetyBench, the first\ncomprehensive benchmark designed to evaluate the safety of LVLMs under\nvideo-text attacks. It comprises 2,264 video-text pairs spanning 48\nfine-grained unsafe categories, each pairing a synthesized video with either a\nharmful query, which contains explicit malice, or a benign query, which appears\nharmless but triggers harmful behavior when interpreted alongside the video. To\ngenerate semantically accurate videos for safety evaluation, we design a\ncontrollable pipeline that decomposes video semantics into subject images (what\nis shown) and motion text (how it moves), which jointly guide the synthesis of\nquery-relevant videos. To effectively evaluate uncertain or borderline harmful\noutputs, we propose RJScore, a novel LLM-based metric that incorporates the\nconfidence of judge models and human-aligned decision threshold calibration.\nExtensive experiments show that benign-query video composition achieves average\nattack success rates of 67.2%, revealing consistent vulnerabilities to\nvideo-induced attacks. We believe Video-SafetyBench will catalyze future\nresearch into video-based safety evaluation and defense strategies."}
{"id": "2505.11820", "pdf": "https://arxiv.org/pdf/2505.11820", "abs": "https://arxiv.org/abs/2505.11820", "authors": ["Kaitao Song", "Xiaohua Wang", "Xu Tan", "Huiqiang Jiang", "Chengruidong Zhang", "Yongliang Shen", "Cen LU", "Zihao Li", "Zifan Song", "Caihua Shan", "Yansen Wang", "Kan Ren", "Xiaoqing Zheng", "Tao Qin", "Yuqing Yang", "Dongsheng Li", "Lili Qiu"], "title": "Chain-of-Model Learning for Language Model", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we propose a novel learning paradigm, termed Chain-of-Model\n(CoM), which incorporates the causal relationship into the hidden states of\neach layer as a chain style, thereby introducing great scaling efficiency in\nmodel training and inference flexibility in deployment. We introduce the\nconcept of Chain-of-Representation (CoR), which formulates the hidden states at\neach layer as a combination of multiple sub-representations (i.e., chains) at\nthe hidden dimension level. In each layer, each chain from the output\nrepresentations can only view all of its preceding chains in the input\nrepresentations. Consequently, the model built upon CoM framework can\nprogressively scale up the model size by increasing the chains based on the\nprevious models (i.e., chains), and offer multiple sub-models at varying sizes\nfor elastic inference by using different chain numbers. Based on this\nprinciple, we devise Chain-of-Language-Model (CoLM), which incorporates the\nidea of CoM into each layer of Transformer architecture. Based on CoLM, we\nfurther introduce CoLM-Air by introducing a KV sharing mechanism, that computes\nall keys and values within the first chain and then shares across all chains.\nThis design demonstrates additional extensibility, such as enabling seamless LM\nswitching, prefilling acceleration and so on. Experimental results demonstrate\nour CoLM family can achieve comparable performance to the standard Transformer,\nwhile simultaneously enabling greater flexiblity, such as progressive scaling\nto improve training efficiency and offer multiple varying model sizes for\nelastic inference, paving a a new way toward building language models. Our code\nwill be released in the future at: https://github.com/microsoft/CoLM."}
{"id": "2505.11845", "pdf": "https://arxiv.org/pdf/2505.11845", "abs": "https://arxiv.org/abs/2505.11845", "authors": ["Tasrifur Riahi", "Md. Azizul Hakim Bappy", "Md. Mehedi Islam"], "title": "ElderFallGuard: Real-Time IoT and Computer Vision-Based Fall Detection System for Elderly Safety", "categories": ["cs.CV"], "comment": "9 page, 1 table, 5 figure", "summary": "For the elderly population, falls pose a serious and increasing risk of\nserious injury and loss of independence. In order to overcome this difficulty,\nwe present ElderFallGuard: A Computer Vision Based IoT Solution for Elderly\nFall Detection and Notification, a cutting-edge, non-invasive system intended\nfor quick caregiver alerts and real-time fall detection. Our approach leverages\nthe power of computer vision, utilizing MediaPipe for accurate human pose\nestimation from standard video streams. We developed a custom dataset\ncomprising 7200 samples across 12 distinct human poses to train and evaluate\nvarious machine learning classifiers, with Random Forest ultimately selected\nfor its superior performance. ElderFallGuard employs a specific detection\nlogic, identifying a fall when a designated prone pose (\"Pose6\") is held for\nover 3 seconds coupled with a significant drop in motion detected for more than\n2 seconds. Upon confirmation, the system instantly dispatches an alert,\nincluding a snapshot of the event, to a designated Telegram group via a custom\nbot, incorporating cooldown logic to prevent notification overload. Rigorous\ntesting on our dataset demonstrated exceptional results, achieving 100%\naccuracy, precision, recall, and F1-score. ElderFallGuard offers a promising,\nvision-based IoT solution to enhance elderly safety and provide peace of mind\nfor caregivers through intelligent, timely alerts."}
{"id": "2505.11827", "pdf": "https://arxiv.org/pdf/2505.11827", "abs": "https://arxiv.org/abs/2505.11827", "authors": ["Yansong Ning", "Wei Li", "Jun Fang", "Naiqiang Tan", "Hao Liu"], "title": "Not All Thoughts are Generated Equal: Efficient LLM Reasoning via Multi-Turn Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": "In progress", "summary": "Compressing long chain-of-thought (CoT) from large language models (LLMs) is\nan emerging strategy to improve the reasoning efficiency of LLMs. Despite its\npromising benefits, existing studies equally compress all thoughts within a\nlong CoT, hindering more concise and effective reasoning. To this end, we first\ninvestigate the importance of different thoughts by examining their\neffectiveness and efficiency in contributing to reasoning through automatic\nlong CoT chunking and Monte Carlo rollouts. Building upon the insights, we\npropose a theoretically bounded metric to jointly measure the effectiveness and\nefficiency of different thoughts. We then propose Long$\\otimes$Short, an\nefficient reasoning framework that enables two LLMs to collaboratively solve\nthe problem: a long-thought LLM for more effectively generating important\nthoughts, while a short-thought LLM for efficiently generating remaining\nthoughts. Specifically, we begin by synthesizing a small amount of cold-start\ndata to fine-tune LLMs for long-thought and short-thought reasoning styles,\nrespectively. Furthermore, we propose a synergizing-oriented multi-turn\nreinforcement learning, focusing on the model self-evolution and collaboration\nbetween long-thought and short-thought LLMs. Experimental results show that our\nmethod enables Qwen2.5-7B and Llama3.1-8B to achieve comparable performance\ncompared to DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B, while\nreducing token length by over 80% across the MATH500, AIME24/25, AMC23, and\nGPQA Diamond benchmarks. Our data and code are available at\nhttps://github.com/yasNing/Long-otimes-Short/."}
{"id": "2505.11852", "pdf": "https://arxiv.org/pdf/2505.11852", "abs": "https://arxiv.org/abs/2505.11852", "authors": ["Jingkun Yue", "Siqi Zhang", "Zinan Jia", "Huihuan Xu", "Zongbo Han", "Xiaohong Liu", "Guangyu Wang"], "title": "MedSG-Bench: A Benchmark for Medical Image Sequences Grounding", "categories": ["cs.CV"], "comment": null, "summary": "Visual grounding is essential for precise perception and reasoning in\nmultimodal large language models (MLLMs), especially in medical imaging\ndomains. While existing medical visual grounding benchmarks primarily focus on\nsingle-image scenarios, real-world clinical applications often involve\nsequential images, where accurate lesion localization across different\nmodalities and temporal tracking of disease progression (e.g., pre- vs.\npost-treatment comparison) require fine-grained cross-image semantic alignment\nand context-aware reasoning. To remedy the underrepresentation of image\nsequences in existing medical visual grounding benchmarks, we propose\nMedSG-Bench, the first benchmark tailored for Medical Image Sequences\nGrounding. It comprises eight VQA-style tasks, formulated into two paradigms of\nthe grounding tasks, including 1) Image Difference Grounding, which focuses on\ndetecting change regions across images, and 2) Image Consistency Grounding,\nwhich emphasizes detection of consistent or shared semantics across sequential\nimages. MedSG-Bench covers 76 public datasets, 10 medical imaging modalities,\nand a wide spectrum of anatomical structures and diseases, totaling 9,630\nquestion-answer pairs. We benchmark both general-purpose MLLMs (e.g.,\nQwen2.5-VL) and medical-domain specialized MLLMs (e.g., HuatuoGPT-vision),\nobserving that even the advanced models exhibit substantial limitations in\nmedical sequential grounding tasks. To advance this field, we construct\nMedSG-188K, a large-scale instruction-tuning dataset tailored for sequential\nvisual grounding, and further develop MedSeq-Grounder, an MLLM designed to\nfacilitate future research on fine-grained understanding across medical\nsequential images. The benchmark, dataset, and model are available at\nhttps://huggingface.co/MedSG-Bench"}
{"id": "2505.11829", "pdf": "https://arxiv.org/pdf/2505.11829", "abs": "https://arxiv.org/abs/2505.11829", "authors": ["Chenlu Wang", "Weimin Lyu", "Ritwik Banerjee"], "title": "Class Distillation with Mahalanobis Contrast: An Efficient Training Paradigm for Pragmatic Language Understanding Tasks", "categories": ["cs.CL"], "comment": null, "summary": "Detecting deviant language such as sexism, or nuanced language such as\nmetaphors or sarcasm, is crucial for enhancing the safety, clarity, and\ninterpretation of online social discourse. While existing classifiers deliver\nstrong results on these tasks, they often come with significant computational\ncost and high data demands. In this work, we propose \\textbf{Cla}ss\n\\textbf{D}istillation (ClaD), a novel training paradigm that targets the core\nchallenge: distilling a small, well-defined target class from a highly diverse\nand heterogeneous background. ClaD integrates two key innovations: (i) a loss\nfunction informed by the structural properties of class distributions, based on\nMahalanobis distance, and (ii) an interpretable decision algorithm optimized\nfor class separation. Across three benchmark detection tasks -- sexism,\nmetaphor, and sarcasm -- ClaD outperforms competitive baselines, and even with\nsmaller language models and orders of magnitude fewer parameters, achieves\nperformance comparable to several large language models (LLMs). These results\ndemonstrate ClaD as an efficient tool for pragmatic language understanding\ntasks that require gleaning a small target class from a larger heterogeneous\nbackground."}
{"id": "2505.11868", "pdf": "https://arxiv.org/pdf/2505.11868", "abs": "https://arxiv.org/abs/2505.11868", "authors": ["Hongyi Zhou", "Xiaogang Wang", "Yulan Guo", "Kai Xu"], "title": "MonoMobility: Zero-Shot 3D Mobility Analysis from Monocular Videos", "categories": ["cs.CV"], "comment": null, "summary": "Accurately analyzing the motion parts and their motion attributes in dynamic\nenvironments is crucial for advancing key areas such as embodied intelligence.\nAddressing the limitations of existing methods that rely on dense multi-view\nimages or detailed part-level annotations, we propose an innovative framework\nthat can analyze 3D mobility from monocular videos in a zero-shot manner. This\nframework can precisely parse motion parts and motion attributes only using a\nmonocular video, completely eliminating the need for annotated training data.\nSpecifically, our method first constructs the scene geometry and roughly\nanalyzes the motion parts and their initial motion attributes combining depth\nestimation, optical flow analysis and point cloud registration method, then\nemploys 2D Gaussian splatting for scene representation. Building on this, we\nintroduce an end-to-end dynamic scene optimization algorithm specifically\ndesigned for articulated objects, refining the initial analysis results to\nensure the system can handle 'rotation', 'translation', and even complex\nmovements ('rotation+translation'), demonstrating high flexibility and\nversatility. To validate the robustness and wide applicability of our method,\nwe created a comprehensive dataset comprising both simulated and real-world\nscenarios. Experimental results show that our framework can effectively analyze\narticulated object motions in an annotation-free manner, showcasing its\nsignificant potential in future embodied intelligence applications."}
{"id": "2505.11835", "pdf": "https://arxiv.org/pdf/2505.11835", "abs": "https://arxiv.org/abs/2505.11835", "authors": ["Hongliang Li", "Jinan Xu", "Gengping Cui", "Changhao Guan", "Fengran Mo", "Kaiyu Huang"], "title": "Multilingual Collaborative Defense for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages, 4figures", "summary": "The robustness and security of large language models (LLMs) has become a\nprominent research area. One notable vulnerability is the ability to bypass LLM\nsafeguards by translating harmful queries into rare or underrepresented\nlanguages, a simple yet effective method of \"jailbreaking\" these models.\nDespite the growing concern, there has been limited research addressing the\nsafeguarding of LLMs in multilingual scenarios, highlighting an urgent need to\nenhance multilingual safety. In this work, we investigate the correlation\nbetween various attack features across different languages and propose\nMultilingual Collaborative Defense (MCD), a novel learning method that\noptimizes a continuous, soft safety prompt automatically to facilitate\nmultilingual safeguarding of LLMs. The MCD approach offers three advantages:\nFirst, it effectively improves safeguarding performance across multiple\nlanguages. Second, MCD maintains strong generalization capabilities while\nminimizing false refusal rates. Third, MCD mitigates the language safety\nmisalignment caused by imbalances in LLM training corpora. To evaluate the\neffectiveness of MCD, we manually construct multilingual versions of commonly\nused jailbreak benchmarks, such as MaliciousInstruct and AdvBench, to assess\nvarious safeguarding methods. Additionally, we introduce these datasets in\nunderrepresented (zero-shot) languages to verify the language transferability\nof MCD. The results demonstrate that MCD outperforms existing approaches in\nsafeguarding against multilingual jailbreak attempts while also exhibiting\nstrong language transfer capabilities. Our code is available at\nhttps://github.com/HLiang-Lee/MCD."}
{"id": "2505.11872", "pdf": "https://arxiv.org/pdf/2505.11872", "abs": "https://arxiv.org/abs/2505.11872", "authors": ["Quoc-Huy Trinh", "Minh-Van Nguyen", "Jung Peng", "Ulas Bagci", "Debesh Jha"], "title": "PRS-Med: Position Reasoning Segmentation with Vision-Language Model in Medical Imaging", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in prompt-based medical image segmentation have enabled\nclinicians to identify tumors using simple input like bounding boxes or text\nprompts. However, existing methods face challenges when doctors need to\ninteract through natural language or when position reasoning is required -\nunderstanding spatial relationships between anatomical structures and\npathologies. We present PRS-Med, a framework that integrates vision-language\nmodels with segmentation capabilities to generate both accurate segmentation\nmasks and corresponding spatial reasoning outputs. Additionally, we introduce\nthe MMRS dataset (Multimodal Medical in Positional Reasoning Segmentation),\nwhich provides diverse, spatially-grounded question-answer pairs to address the\nlack of position reasoning data in medical imaging. PRS-Med demonstrates\nsuperior performance across six imaging modalities (CT, MRI, X-ray, ultrasound,\nendoscopy, RGB), significantly outperforming state-of-the-art methods in both\nsegmentation accuracy and position reasoning. Our approach enables intuitive\ndoctor-system interaction through natural language, facilitating more efficient\ndiagnoses. Our dataset pipeline, model, and codebase will be released to foster\nfurther research in spatially-aware multimodal reasoning for medical\napplications."}
{"id": "2505.11855", "pdf": "https://arxiv.org/pdf/2505.11855", "abs": "https://arxiv.org/abs/2505.11855", "authors": ["Guijin Son", "Jiwoo Hong", "Honglu Fan", "Heejeong Nam", "Hyunwoo Ko", "Seungwon Lim", "Jinyeop Song", "Jinha Choi", "Gonçalo Paulo", "Youngjae Yu", "Stella Biderman"], "title": "When AI Co-Scientists Fail: SPOT-a Benchmark for Automated Verification of Scientific Research", "categories": ["cs.CL"], "comment": "work in progress", "summary": "Recent advances in large language models (LLMs) have fueled the vision of\nautomated scientific discovery, often called AI Co-Scientists. To date, prior\nwork casts these systems as generative co-authors responsible for crafting\nhypotheses, synthesizing code, or drafting manuscripts. In this work, we\nexplore a complementary application: using LLMs as verifiers to automate the\n\\textbf{academic verification of scientific manuscripts}. To that end, we\nintroduce SPOT, a dataset of 83 published papers paired with 91 errors\nsignificant enough to prompt errata or retraction, cross-validated with actual\nauthors and human annotators. Evaluating state-of-the-art LLMs on SPOT, we find\nthat none surpasses 21.1\\% recall or 6.1\\% precision (o3 achieves the best\nscores, with all others near zero). Furthermore, confidence estimates are\nuniformly low, and across eight independent runs, models rarely rediscover the\nsame errors, undermining their reliability. Finally, qualitative analysis with\ndomain experts reveals that even the strongest models make mistakes resembling\nstudent-level misconceptions derived from misunderstandings. These findings\nhighlight the substantial gap between current LLM capabilities and the\nrequirements for dependable AI-assisted academic verification."}
{"id": "2505.11881", "pdf": "https://arxiv.org/pdf/2505.11881", "abs": "https://arxiv.org/abs/2505.11881", "authors": ["Giyeong Oh", "Woohyun Cho", "Siyeol Kim", "Suhwan Choi", "Younjae Yu"], "title": "Revisiting Residual Connections: Orthogonal Updates for Stable and Efficient Deep Networks", "categories": ["cs.CV", "cs.AI"], "comment": "27 pages, WIP", "summary": "Residual connections are pivotal for deep neural networks, enabling greater\ndepth by mitigating vanishing gradients. However, in standard residual updates,\nthe module's output is directly added to the input stream. This can lead to\nupdates that predominantly reinforce or modulate the existing stream direction,\npotentially underutilizing the module's capacity for learning entirely novel\nfeatures. In this work, we introduce Orthogonal Residual Update: we decompose\nthe module's output relative to the input stream and add only the component\northogonal to this stream. This design aims to guide modules to contribute\nprimarily new representational directions, fostering richer feature learning\nwhile promoting more efficient training. We demonstrate that our orthogonal\nupdate strategy improves generalization accuracy and training stability across\ndiverse architectures (ResNetV2, Vision Transformers) and datasets (CIFARs,\nTinyImageNet, ImageNet-1k), achieving, for instance, a +4.3\\%p top-1 accuracy\ngain for ViT-B on ImageNet-1k."}
{"id": "2505.11876", "pdf": "https://arxiv.org/pdf/2505.11876", "abs": "https://arxiv.org/abs/2505.11876", "authors": ["Yanbo Dai", "Zhenlan Ji", "Zongjie Li", "Shuai Wang"], "title": "NAMET: Robust Massive Model Editing via Noise-Aware Memory Optimization", "categories": ["cs.CL"], "comment": null, "summary": "Model editing techniques are essential for efficiently updating knowledge in\nlarge language models (LLMs). However, the effectiveness of existing approaches\ndegrades in massive editing scenarios, particularly when evaluated with\npractical metrics or in context-rich settings. We attribute these failures to\nembedding collisions among knowledge items, which undermine editing reliability\nat scale. To address this, we propose NAMET (Noise-aware Model Editing in\nTransformers), a simple yet effective method that introduces noise during\nmemory extraction via a one-line modification to MEMIT. Extensive experiments\nacross six LLMs and three datasets demonstrate that NAMET consistently\noutperforms existing methods when editing thousands of facts."}
{"id": "2505.11882", "pdf": "https://arxiv.org/pdf/2505.11882", "abs": "https://arxiv.org/abs/2505.11882", "authors": ["Shiming Chen", "Dingjie Fu", "Salman Khan", "Fahad Shahbaz Khan"], "title": "GenZSL: Generative Zero-Shot Learning Via Inductive Variational Autoencoder", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to ICML'25", "summary": "Remarkable progress in zero-shot learning (ZSL) has been achieved using\ngenerative models. However, existing generative ZSL methods merely generate\n(imagine) the visual features from scratch guided by the strong class semantic\nvectors annotated by experts, resulting in suboptimal generative performance\nand limited scene generalization. To address these and advance ZSL, we propose\nan inductive variational autoencoder for generative zero-shot learning, dubbed\nGenZSL. Mimicking human-level concept learning, GenZSL operates by inducting\nnew class samples from similar seen classes using weak class semantic vectors\nderived from target class names (i.e., CLIP text embedding). To ensure the\ngeneration of informative samples for training an effective ZSL classifier, our\nGenZSL incorporates two key strategies. Firstly, it employs class diversity\npromotion to enhance the diversity of class semantic vectors. Secondly, it\nutilizes target class-guided information boosting criteria to optimize the\nmodel. Extensive experiments conducted on three popular benchmark datasets\nshowcase the superiority and potential of our GenZSL with significant efficacy\nand efficiency over f-VAEGAN, e.g., 24.7% performance gains and more than\n$60\\times$ faster training speed on AWA2. Codes are available at\nhttps://github.com/shiming-chen/GenZSL."}
{"id": "2505.11887", "pdf": "https://arxiv.org/pdf/2505.11887", "abs": "https://arxiv.org/abs/2505.11887", "authors": ["Xiechi Zhang", "Zetian Ouyang", "Linlin Wang", "Gerard de Melo", "Zhu Cao", "Xiaoling Wang", "Ya Zhang", "Yanfeng Wang", "Liang He"], "title": "AutoMedEval: Harnessing Language Models for Automatic Medical Capability Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "With the proliferation of large language models (LLMs) in the medical domain,\nthere is increasing demand for improved evaluation techniques to assess their\ncapabilities. However, traditional metrics like F1 and ROUGE, which rely on\ntoken overlaps to measure quality, significantly overlook the importance of\nmedical terminology. While human evaluation tends to be more reliable, it can\nbe very costly and may as well suffer from inaccuracies due to limits in human\nexpertise and motivation. Although there are some evaluation methods based on\nLLMs, their usability in the medical field is limited due to their proprietary\nnature or lack of expertise. To tackle these challenges, we present\nAutoMedEval, an open-sourced automatic evaluation model with 13B parameters\nspecifically engineered to measure the question-answering proficiency of\nmedical LLMs. The overarching objective of AutoMedEval is to assess the quality\nof responses produced by diverse models, aspiring to significantly reduce the\ndependence on human evaluation. Specifically, we propose a hierarchical\ntraining method involving curriculum instruction tuning and an iterative\nknowledge introspection mechanism, enabling AutoMedEval to acquire professional\nmedical assessment capabilities with limited instructional data. Human\nevaluations indicate that AutoMedEval surpasses other baselines in terms of\ncorrelation with human judgments."}
{"id": "2505.11884", "pdf": "https://arxiv.org/pdf/2505.11884", "abs": "https://arxiv.org/abs/2505.11884", "authors": ["Zhongwen Li", "Zongwei Li", "Xiaoqi Li"], "title": "Facial Recognition Leveraging Generative Adversarial Networks", "categories": ["cs.CV", "cs.CR"], "comment": null, "summary": "Face recognition performance based on deep learning heavily relies on\nlarge-scale training data, which is often difficult to acquire in practical\napplications. To address this challenge, this paper proposes a GAN-based data\naugmentation method with three key contributions: (1) a residual-embedded\ngenerator to alleviate gradient vanishing/exploding problems, (2) an Inception\nResNet-V1 based FaceNet discriminator for improved adversarial training, and\n(3) an end-to-end framework that jointly optimizes data generation and\nrecognition performance. Experimental results demonstrate that our approach\nachieves stable training dynamics and significantly improves face recognition\naccuracy by 12.7% on the LFW benchmark compared to baseline methods, while\nmaintaining good generalization capability with limited training samples."}
{"id": "2505.11891", "pdf": "https://arxiv.org/pdf/2505.11891", "abs": "https://arxiv.org/abs/2505.11891", "authors": ["Weikai Xu", "Zhizheng Jiang", "Yuxuan Liu", "Wei Liu", "Jian Luan", "Yuanchun Li", "Yunxin Liu", "Bin Wang", "Bo An"], "title": "Mobile-Bench-v2: A More Realistic and Comprehensive Benchmark for VLM-based Mobile Agents", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "VLM-based mobile agents are increasingly popular due to their capabilities to\ninteract with smartphone GUIs and XML-structured texts and to complete daily\ntasks. However, existing online benchmarks struggle with obtaining stable\nreward signals due to dynamic environmental changes. Offline benchmarks\nevaluate the agents through single-path trajectories, which stands in contrast\nto the inherently multi-solution characteristics of GUI tasks. Additionally,\nboth types of benchmarks fail to assess whether mobile agents can handle noise\nor engage in proactive interactions due to a lack of noisy apps or overly full\ninstructions during the evaluation process. To address these limitations, we\nuse a slot-based instruction generation method to construct a more realistic\nand comprehensive benchmark named Mobile-Bench-v2. Mobile-Bench-v2 includes a\ncommon task split, with offline multi-path evaluation to assess the agent's\nability to obtain step rewards during task execution. It contains a noisy split\nbased on pop-ups and ads apps, and a contaminated split named AITZ-Noise to\nformulate a real noisy environment. Furthermore, an ambiguous instruction split\nwith preset Q\\&A interactions is released to evaluate the agent's proactive\ninteraction capabilities. We conduct evaluations on these splits using the\nsingle-agent framework AppAgent-v1, the multi-agent framework Mobile-Agent-v2,\nas well as other mobile agents such as UI-Tars and OS-Atlas. Code and data are\navailable at https://huggingface.co/datasets/xwk123/MobileBench-v2."}
{"id": "2505.11895", "pdf": "https://arxiv.org/pdf/2505.11895", "abs": "https://arxiv.org/abs/2505.11895", "authors": ["Chih-Ting Liao", "Bin Ren", "Guofeng Mei", "Xu Zheng"], "title": "Adversarial Robustness for Unified Multi-Modal Encoders via Efficient Calibration", "categories": ["cs.CV"], "comment": null, "summary": "Recent unified multi-modal encoders align a wide range of modalities into a\nshared representation space, enabling diverse cross-modal tasks. Despite their\nimpressive capabilities, the robustness of these models under adversarial\nperturbations remains underexplored, which is a critical concern for\nsafety-sensitive applications. In this work, we present the first comprehensive\nstudy of adversarial vulnerability in unified multi-modal encoders. We find\nthat even mild adversarial perturbations lead to substantial performance drops\nacross all modalities. Non-visual inputs, such as audio and point clouds, are\nespecially fragile, while visual inputs like images and videos also degrade\nsignificantly. To address this, we propose an efficient adversarial calibration\nframework that improves robustness across modalities without modifying\npretrained encoders or semantic centers, ensuring compatibility with existing\nfoundation models. Our method introduces modality-specific projection heads\ntrained solely on adversarial examples, while keeping the backbone and\nembeddings frozen. We explore three training objectives: fixed-center\ncross-entropy, clean-to-adversarial L2 alignment, and clean-adversarial\nInfoNCE, and we introduce a regularization strategy to ensure\nmodality-consistent alignment under attack. Experiments on six modalities and\nthree Bind-style models show that our method improves adversarial robustness by\nup to 47.3 percent at epsilon = 4/255, while preserving or even improving clean\nzero-shot and retrieval performance with less than 1 percent trainable\nparameters."}
{"id": "2505.11893", "pdf": "https://arxiv.org/pdf/2505.11893", "abs": "https://arxiv.org/abs/2505.11893", "authors": ["Zepeng Ding", "Dixuan Wang", "Ziqin Luo", "Guochao Jiang", "Deqing Yang", "Jiaqing Liang"], "title": "RLAP: A Reinforcement Learning Enhanced Adaptive Planning Framework for Multi-step NLP Task Solving", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multi-step planning has been widely employed to enhance the performance of\nlarge language models (LLMs) on downstream natural language processing (NLP)\ntasks, which decomposes the original task into multiple subtasks and guide LLMs\nto solve them sequentially without additional training. When addressing task\ninstances, existing methods either preset the order of steps or attempt\nmultiple paths at each step. However, these methods overlook instances'\nlinguistic features and rely on the intrinsic planning capabilities of LLMs to\nevaluate intermediate feedback and then select subtasks, resulting in\nsuboptimal outcomes. To better solve multi-step NLP tasks with LLMs, in this\npaper we propose a Reinforcement Learning enhanced Adaptive Planning framework\n(RLAP). In our framework, we model an NLP task as a Markov decision process\n(MDP) and employ an LLM directly into the environment. In particular, a\nlightweight Actor model is trained to estimate Q-values for natural language\nsequences consisting of states and actions through reinforcement learning.\nTherefore, during sequential planning, the linguistic features of each sequence\nin the MDP can be taken into account, and the Actor model interacts with the\nLLM to determine the optimal order of subtasks for each task instance. We apply\nRLAP on three different types of NLP tasks and conduct extensive experiments on\nmultiple datasets to verify RLAP's effectiveness and robustness."}
{"id": "2505.11897", "pdf": "https://arxiv.org/pdf/2505.11897", "abs": "https://arxiv.org/abs/2505.11897", "authors": ["Seonghak Kim"], "title": "FiGKD: Fine-Grained Knowledge Distillation via High-Frequency Detail Transfer", "categories": ["cs.CV"], "comment": "14 pages, 6 figures. This work has been submitted to the Elsevier for\n  possible publication", "summary": "Knowledge distillation (KD) is a widely adopted technique for transferring\nknowledge from a high-capacity teacher model to a smaller student model by\naligning their output distributions. However, existing methods often\nunderperform in fine-grained visual recognition tasks, where distinguishing\nsubtle differences between visually similar classes is essential. This\nperformance gap stems from the fact that conventional approaches treat the\nteacher's output logits as a single, undifferentiated signal-assuming all\ncontained information is equally beneficial to the student. Consequently,\nstudent models may become overloaded with redundant signals and fail to capture\nthe teacher's nuanced decision boundaries. To address this issue, we propose\nFine-Grained Knowledge Distillation (FiGKD), a novel frequency-aware framework\nthat decomposes a model's logits into low-frequency (content) and\nhigh-frequency (detail) components using the discrete wavelet transform (DWT).\nFiGKD selectively transfers only the high-frequency components, which encode\nthe teacher's semantic decision patterns, while discarding redundant\nlow-frequency content already conveyed through ground-truth supervision. Our\napproach is simple, architecture-agnostic, and requires no access to\nintermediate feature maps. Extensive experiments on CIFAR-100, TinyImageNet,\nand multiple fine-grained recognition benchmarks show that FiGKD consistently\noutperforms state-of-the-art logit-based and feature-based distillation methods\nacross a variety of teacher-student configurations. These findings confirm that\nfrequency-aware logit decomposition enables more efficient and effective\nknowledge transfer, particularly in resource-constrained settings."}
{"id": "2505.11900", "pdf": "https://arxiv.org/pdf/2505.11900", "abs": "https://arxiv.org/abs/2505.11900", "authors": ["Philipp Christmann", "Gerhard Weikum"], "title": "Recursive Question Understanding for Complex Question Answering over Heterogeneous Personal Data", "categories": ["cs.CL", "cs.IR"], "comment": "Accepted at ACL 2025 (Findings)", "summary": "Question answering over mixed sources, like text and tables, has been\nadvanced by verbalizing all contents and encoding it with a language model. A\nprominent case of such heterogeneous data is personal information: user devices\nlog vast amounts of data every day, such as calendar entries, workout\nstatistics, shopping records, streaming history, and more. Information needs\nrange from simple look-ups to queries of analytical nature. The challenge is to\nprovide humans with convenient access with small footprint, so that all\npersonal data stays on the user devices. We present ReQAP, a novel method that\ncreates an executable operator tree for a given question, via recursive\ndecomposition. Operators are designed to enable seamless integration of\nstructured and unstructured sources, and the execution of the operator tree\nyields a traceable answer. We further release the PerQA benchmark, with\npersona-based data and questions, covering a diverse spectrum of realistic user\nneeds."}
{"id": "2505.11905", "pdf": "https://arxiv.org/pdf/2505.11905", "abs": "https://arxiv.org/abs/2505.11905", "authors": ["Takuya Ikeda", "Sergey Zakharov", "Muhammad Zubair Irshad", "Istvan Balazs Opra", "Shun Iwase", "Dian Chen", "Mark Tjersland", "Robert Lee", "Alexandre Dilly", "Rares Ambrus", "Koichi Nishiwaki"], "title": "GTR: Gaussian Splatting Tracking and Reconstruction of Unknown Objects Based on Appearance and Geometric Complexity", "categories": ["cs.CV", "cs.RO"], "comment": "main contains 10 pages, 9 figures. And supplementary material\n  contains 10 pages, 27 figures", "summary": "We present a novel method for 6-DoF object tracking and high-quality 3D\nreconstruction from monocular RGBD video. Existing methods, while achieving\nimpressive results, often struggle with complex objects, particularly those\nexhibiting symmetry, intricate geometry or complex appearance. To bridge these\ngaps, we introduce an adaptive method that combines 3D Gaussian Splatting,\nhybrid geometry/appearance tracking, and key frame selection to achieve robust\ntracking and accurate reconstructions across a diverse range of objects.\nAdditionally, we present a benchmark covering these challenging object classes,\nproviding high-quality annotations for evaluating both tracking and\nreconstruction performance. Our approach demonstrates strong capabilities in\nrecovering high-fidelity object meshes, setting a new standard for\nsingle-sensor 3D reconstruction in open-world environments."}
{"id": "2505.11908", "pdf": "https://arxiv.org/pdf/2505.11908", "abs": "https://arxiv.org/abs/2505.11908", "authors": ["Zhangyu Wang", "Siyuan Gao", "Rong Zhou", "Hao Wang", "Li Ning"], "title": "ELITE: Embedding-Less retrieval with Iterative Text Exploration", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved impressive progress in natural\nlanguage processing, but their limited ability to retain long-term context\nconstrains performance on document-level or multi-turn tasks.\nRetrieval-Augmented Generation (RAG) mitigates this by retrieving relevant\ninformation from an external corpus. However, existing RAG systems often rely\non embedding-based retrieval trained on corpus-level semantic similarity, which\ncan lead to retrieving content that is semantically similar in form but\nmisaligned with the question's true intent. Furthermore, recent RAG variants\nconstruct graph- or hierarchy-based structures to improve retrieval accuracy,\nresulting in significant computation and storage overhead. In this paper, we\npropose an embedding-free retrieval framework. Our method leverages the logical\ninferencing ability of LLMs in retrieval using iterative search space\nrefinement guided by our novel importance measure and extend our retrieval\nresults with logically related information without explicit graph construction.\nExperiments on long-context QA benchmarks, including NovelQA and Marathon, show\nthat our approach outperforms strong baselines while reducing storage and\nruntime by over an order of magnitude."}
{"id": "2505.11907", "pdf": "https://arxiv.org/pdf/2505.11907", "abs": "https://arxiv.org/abs/2505.11907", "authors": ["Zihao Dongfang", "Xu Zheng", "Ziqiao Weng", "Yuanhuiyi Lyu", "Danda Pani Paudel", "Luc Van Gool", "Kailun Yang", "Xuming Hu"], "title": "Are Multimodal Large Language Models Ready for Omnidirectional Spatial Reasoning?", "categories": ["cs.CV"], "comment": null, "summary": "The 180x360 omnidirectional field of view captured by 360-degree cameras\nenables their use in a wide range of applications such as embodied AI and\nvirtual reality. Although recent advances in multimodal large language models\n(MLLMs) have shown promise in visual-spatial reasoning, most studies focus on\nstandard pinhole-view images, leaving omnidirectional perception largely\nunexplored. In this paper, we ask: Are MLLMs ready for omnidirectional spatial\nreasoning? To investigate this, we introduce OSR-Bench, the first benchmark\nspecifically designed for this setting. OSR-Bench includes over 153,000 diverse\nquestion-answer pairs grounded in high-fidelity panoramic indoor scene maps. It\ncovers key reasoning types including object counting, relative distance, and\ndirection. We also propose a negative sampling strategy that inserts\nnon-existent objects into prompts to evaluate hallucination and grounding\nrobustness. For fine-grained analysis, we design a two-stage evaluation\nframework assessing both cognitive map generation and QA accuracy using\nrotation-invariant matching and a combination of rule-based and LLM-based\nmetrics. We evaluate eight state-of-the-art MLLMs, including GPT-4o, Gemini 1.5\nPro, and leading open-source models under zero-shot settings. Results show that\ncurrent models struggle with spatial reasoning in panoramic contexts,\nhighlighting the need for more perceptually grounded MLLMs. OSR-Bench and code\nwill be released at: https://huggingface.co/datasets/UUUserna/OSR-Bench"}
{"id": "2505.11922", "pdf": "https://arxiv.org/pdf/2505.11922", "abs": "https://arxiv.org/abs/2505.11922", "authors": ["Yuheng Lu", "ZiMeng Bai", "Caixia Yuan", "Huixing Jiang", "Xiaojie Wang"], "title": "Enhancing Complex Instruction Following for Large Language Models with Mixture-of-Contexts Fine-tuning", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) exhibit remarkable capabilities in handling\nnatural language tasks; however, they may struggle to consistently follow\ncomplex instructions including those involve multiple constraints.\nPost-training LLMs using supervised fine-tuning (SFT) is a standard approach to\nimprove their ability to follow instructions. In addressing complex instruction\nfollowing, existing efforts primarily focus on data-driven methods that\nsynthesize complex instruction-output pairs for SFT. However, insufficient\nattention allocated to crucial sub-contexts may reduce the effectiveness of\nSFT. In this work, we propose transforming sequentially structured input\ninstruction into multiple parallel instructions containing subcontexts. To\nsupport processing this multi-input, we propose MISO (Multi-Input\nSingle-Output), an extension to currently dominant decoder-only\ntransformer-based LLMs. MISO introduces a mixture-of-contexts paradigm that\njointly considers the overall instruction-output alignment and the influence of\nindividual sub-contexts to enhance SFT effectiveness. We apply MISO fine-tuning\nto complex instructionfollowing datasets and evaluate it with standard LLM\ninference. Empirical results demonstrate the superiority of MISO as a\nfine-tuning method for LLMs, both in terms of effectiveness in complex\ninstruction-following scenarios and its potential for training efficiency."}
{"id": "2505.11921", "pdf": "https://arxiv.org/pdf/2505.11921", "abs": "https://arxiv.org/abs/2505.11921", "authors": ["Haitao Li", "Ziyu Li", "Yiheng Mao", "Zhengyao Ding", "Zhengxing Huang"], "title": "DC-Seg: Disentangled Contrastive Learning for Brain Tumor Segmentation with Missing Modalities", "categories": ["cs.CV"], "comment": null, "summary": "Accurate segmentation of brain images typically requires the integration of\ncomplementary information from multiple image modalities. However, clinical\ndata for all modalities may not be available for every patient, creating a\nsignificant challenge. To address this, previous studies encode multiple\nmodalities into a shared latent space. While somewhat effective, it remains\nsuboptimal, as each modality contains distinct and valuable information. In\nthis study, we propose DC-Seg (Disentangled Contrastive Learning for\nSegmentation), a new method that explicitly disentangles images into\nmodality-invariant anatomical representation and modality-specific\nrepresentation, by using anatomical contrastive learning and modality\ncontrastive learning respectively. This solution improves the separation of\nanatomical and modality-specific features by considering the modality gaps,\nleading to more robust representations. Furthermore, we introduce a\nsegmentation-based regularizer that enhances the model's robustness to missing\nmodalities. Extensive experiments on the BraTS 2020 and a private white matter\nhyperintensity(WMH) segmentation dataset demonstrate that DC-Seg outperforms\nstate-of-the-art methods in handling incomplete multimodal brain tumor\nsegmentation tasks with varying missing modalities, while also demonstrate\nstrong generalizability in WMH segmentation. The code is available at\nhttps://github.com/CuCl-2/DC-Seg."}
{"id": "2505.11924", "pdf": "https://arxiv.org/pdf/2505.11924", "abs": "https://arxiv.org/abs/2505.11924", "authors": ["Yu-Ting Lee", "Hui-Ying Shih", "Fu-Chieh Chang", "Pei-Yuan Wu"], "title": "An Explanation of Intrinsic Self-Correction via Linear Representations and Latent Concepts", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We provide an explanation for the performance gains of intrinsic\nself-correction, a process where a language model iteratively refines its\noutputs without external feedback. More precisely, we investigate how prompting\ninduces interpretable changes in hidden states and thus affects the output\ndistributions. We hypothesize that each prompt-induced shift lies in a linear\nspan of some linear representation vectors, naturally separating tokens based\non individual concept alignment. Building around this idea, we give a\nmathematical formulation of self-correction and derive a concentration result\nfor output tokens based on alignment magnitudes. Our experiments on text\ndetoxification with zephyr-7b-sft reveal a substantial gap in the inner\nproducts of the prompt-induced shifts and the unembeddings of the top-100 most\ntoxic tokens vs. those of the unembeddings of the bottom-100 least toxic\ntokens, under toxic instructions. This suggests that self-correction prompts\nenhance a language model's capability of latent concept recognition. Our\nanalysis offers insights into the underlying mechanism of self-correction by\ncharacterizing how prompting works explainably. For reproducibility, our code\nis available."}
{"id": "2505.11926", "pdf": "https://arxiv.org/pdf/2505.11926", "abs": "https://arxiv.org/abs/2505.11926", "authors": ["Yixu Wang", "Jiaxin Song", "Yifeng Gao", "Xin Wang", "Yang Yao", "Yan Teng", "Xingjun Ma", "Yingchun Wang", "Yu-Gang Jiang"], "title": "SafeVid: Toward Safety Aligned Video Large Multimodal Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "As Video Large Multimodal Models (VLMMs) rapidly advance, their inherent\ncomplexity introduces significant safety challenges, particularly the issue of\nmismatched generalization where static safety alignments fail to transfer to\ndynamic video contexts. We introduce SafeVid, a framework designed to instill\nvideo-specific safety principles in VLMMs. SafeVid uniquely transfers robust\ntextual safety alignment capabilities to the video domain by employing detailed\ntextual video descriptions as an interpretive bridge, facilitating LLM-based\nrule-driven safety reasoning. This is achieved through a closed-loop system\ncomprising: 1) generation of SafeVid-350K, a novel 350,000-pair video-specific\nsafety preference dataset; 2) targeted alignment of VLMMs using Direct\nPreference Optimization (DPO); and 3) comprehensive evaluation via our new\nSafeVidBench benchmark. Alignment with SafeVid-350K significantly enhances VLMM\nsafety, with models like LLaVA-NeXT-Video demonstrating substantial\nimprovements (e.g., up to 42.39%) on SafeVidBench. SafeVid provides critical\nresources and a structured approach, demonstrating that leveraging textual\ndescriptions as a conduit for safety reasoning markedly improves the safety\nalignment of VLMMs. We have made SafeVid-350K dataset\n(https://huggingface.co/datasets/yxwang/SafeVid-350K) publicly available."}
{"id": "2505.11932", "pdf": "https://arxiv.org/pdf/2505.11932", "abs": "https://arxiv.org/abs/2505.11932", "authors": ["Yuyao Zhang", "Zhicheng Dou", "Xiaoxi Li", "Jiajie Jin", "Yongkang Wu", "Zhonghua Li", "Qi Ye", "Ji-Rong Wen"], "title": "Neuro-Symbolic Query Compiler", "categories": ["cs.CL", "cs.IR"], "comment": "Findings of ACL2025, codes are available at this url:\n  https://github.com/YuyaoZhangQAQ/Query_Compiler", "summary": "Precise recognition of search intent in Retrieval-Augmented Generation (RAG)\nsystems remains a challenging goal, especially under resource constraints and\nfor complex queries with nested structures and dependencies. This paper\npresents QCompiler, a neuro-symbolic framework inspired by linguistic grammar\nrules and compiler design, to bridge this gap. It theoretically designs a\nminimal yet sufficient Backus-Naur Form (BNF) grammar $G[q]$ to formalize\ncomplex queries. Unlike previous methods, this grammar maintains completeness\nwhile minimizing redundancy. Based on this, QCompiler includes a Query\nExpression Translator, a Lexical Syntax Parser, and a Recursive Descent\nProcessor to compile queries into Abstract Syntax Trees (ASTs) for execution.\nThe atomicity of the sub-queries in the leaf nodes ensures more precise\ndocument retrieval and response generation, significantly improving the RAG\nsystem's ability to address complex queries."}
{"id": "2505.11934", "pdf": "https://arxiv.org/pdf/2505.11934", "abs": "https://arxiv.org/abs/2505.11934", "authors": ["Yian Zhao", "Wanshi Xu", "Ruochong Zheng", "Pengchong Qiao", "Chang Liu", "Jie Chen"], "title": "iSegMan: Interactive Segment-and-Manipulate 3D Gaussians", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "The efficient rendering and explicit nature of 3DGS promote the advancement\nof 3D scene manipulation. However, existing methods typically encounter\nchallenges in controlling the manipulation region and are unable to furnish the\nuser with interactive feedback, which inevitably leads to unexpected results.\nIntuitively, incorporating interactive 3D segmentation tools can compensate for\nthis deficiency. Nevertheless, existing segmentation frameworks impose a\npre-processing step of scene-specific parameter training, which limits the\nefficiency and flexibility of scene manipulation. To deliver a 3D region\ncontrol module that is well-suited for scene manipulation with reliable\nefficiency, we propose interactive Segment-and-Manipulate 3D Gaussians\n(iSegMan), an interactive segmentation and manipulation framework that only\nrequires simple 2D user interactions in any view. To propagate user\ninteractions to other views, we propose Epipolar-guided Interaction Propagation\n(EIP), which innovatively exploits epipolar constraint for efficient and robust\ninteraction matching. To avoid scene-specific training to maintain efficiency,\nwe further propose the novel Visibility-based Gaussian Voting (VGV), which\nobtains 2D segmentations from SAM and models the region extraction as a voting\ngame between 2D Pixels and 3D Gaussians based on Gaussian visibility. Taking\nadvantage of the efficient and precise region control of EIP and VGV, we put\nforth a Manipulation Toolbox to implement various functions on selected\nregions, enhancing the controllability, flexibility and practicality of scene\nmanipulation. Extensive results on 3D scene manipulation and segmentation tasks\nfully demonstrate the significant advantages of iSegMan. Project page is\navailable at https://zhao-yian.github.io/iSegMan."}
{"id": "2505.11935", "pdf": "https://arxiv.org/pdf/2505.11935", "abs": "https://arxiv.org/abs/2505.11935", "authors": ["Xuanle Zhao", "Xuexin Liu", "Haoyue Yang", "Xianzhen Luo", "Fanhu Zeng", "Jianling Li", "Qi Shi", "Chi Chen"], "title": "ChartEdit: How Far Are MLLMs From Automating Chart Analysis? Evaluating MLLMs' Capability via Chart Editing", "categories": ["cs.CL"], "comment": "Accept by ACL2025 Findings, preprint version", "summary": "Although multimodal large language models (MLLMs) show promise in generating\nchart rendering code, chart editing presents a greater challenge. This\ndifficulty stems from its nature as a labor-intensive task for humans that also\ndemands MLLMs to integrate chart understanding, complex reasoning, and precise\nintent interpretation. While many MLLMs claim such editing capabilities,\ncurrent assessments typically rely on limited case studies rather than robust\nevaluation methodologies, highlighting the urgent need for a comprehensive\nevaluation framework. In this work, we propose ChartEdit, a new high-quality\nbenchmark designed for chart editing tasks. This benchmark comprises $1,405$\ndiverse editing instructions applied to $233$ real-world charts, with each\ninstruction-chart instance having been manually annotated and validated for\naccuracy. Utilizing ChartEdit, we evaluate the performance of 10 mainstream\nMLLMs across two types of experiments, assessing them at both the code and\nchart levels. The results suggest that large-scale models can generate code to\nproduce images that partially match the reference images. However, their\nability to generate accurate edits according to the instructions remains\nlimited. The state-of-the-art (SOTA) model achieves a score of only $59.96$,\nhighlighting significant challenges in precise modification. In contrast,\nsmall-scale models, including chart-domain models, struggle both with following\nediting instructions and generating overall chart images, underscoring the need\nfor further development in this area. Code is available at\nhttps://github.com/xxlllz/ChartEdit."}
{"id": "2505.11945", "pdf": "https://arxiv.org/pdf/2505.11945", "abs": "https://arxiv.org/abs/2505.11945", "authors": ["Bonan li", "Zicheng Zhang", "Songhua Liu", "Weihao Yu", "Xinchao Wang"], "title": "Top-Down Compression: Revisit Efficient Vision Token Projection for Visual Instruction Tuning", "categories": ["cs.CV"], "comment": "Under Review", "summary": "Visual instruction tuning aims to enable large language models to comprehend\nthe visual world, with a pivotal challenge lying in establishing an effective\nvision-to-language projection. However, existing methods often grapple with the\nintractable trade-off between accuracy and efficiency. In this paper, we\npresent LLaVA-Meteor, a novel approach designed to break this deadlock,\nequipped with a novel Top-Down Compression paradigm that strategically\ncompresses visual tokens without compromising core information. Specifically,\nwe construct a trainable Flash Global Fusion module based on efficient\nselective state space operators, which aligns the feature space while enabling\neach token to perceive holistic visual context and instruction preference at\nlow cost. Furthermore, a local-to-single scanning manner is employed to\neffectively capture local dependencies, thereby enhancing the model's\ncapability in vision modeling. To alleviate computational overhead, we explore\na Visual-Native Selection mechanism that independently assesses token\nsignificance by both the visual and native experts, followed by aggregation to\nretain the most critical subset. Extensive experiments show that our approach\nreduces visual tokens by 75--95% while achieving comparable or superior\nperformance across 12 benchmarks, significantly improving efficiency."}
{"id": "2505.11958", "pdf": "https://arxiv.org/pdf/2505.11958", "abs": "https://arxiv.org/abs/2505.11958", "authors": ["Aswini Kumar Padhi", "Anil Bandhakavi", "Tanmoy Chakraborty"], "title": "Counterspeech the ultimate shield! Multi-Conditioned Counterspeech Generation through Attributed Prefix Learning", "categories": ["cs.CL"], "comment": null, "summary": "Counterspeech has proven to be a powerful tool to combat hate speech online.\nPrevious studies have focused on generating counterspeech conditioned only on\nspecific intents (single attributed). However, a holistic approach considering\nmultiple attributes simultaneously can yield more nuanced and effective\nresponses. Here, we introduce HiPPrO, Hierarchical Prefix learning with\nPreference Optimization, a novel two-stage framework that utilizes the\neffectiveness of attribute-specific prefix embedding spaces hierarchically\noptimized during the counterspeech generation process in the first phase.\nThereafter, we incorporate both reference and reward-free preference\noptimization to generate more constructive counterspeech. Furthermore, we\nextend IntentCONANv2 by annotating all 13,973 counterspeech instances with\nemotion labels by five annotators. HiPPrO leverages hierarchical prefix\noptimization to integrate these dual attributes effectively. An extensive\nevaluation demonstrates that HiPPrO achieves a ~38 % improvement in intent\nconformity and a ~3 %, ~2 %, ~3 % improvement in Rouge-1, Rouge-2, and Rouge-L,\nrespectively, compared to several baseline models. Human evaluations further\nsubstantiate the superiority of our approach, highlighting the enhanced\nrelevance and appropriateness of the generated counterspeech. This work\nunderscores the potential of multi-attribute conditioning in advancing the\nefficacy of counterspeech generation systems."}
{"id": "2505.11976", "pdf": "https://arxiv.org/pdf/2505.11976", "abs": "https://arxiv.org/abs/2505.11976", "authors": ["Soumya Swarup Prusty", "Astha Agarwal", "Srinivasan Iyenger"], "title": "Advanced Integration of Discrete Line Segments in Digitized P&ID for Continuous Instrument Connectivity", "categories": ["cs.CV"], "comment": "6 pages, 13 figures", "summary": "Piping and Instrumentation Diagrams (P&IDs) constitute the foundational\nblueprint of a plant, depicting the interconnections among process equipment,\ninstrumentation for process control, and the flow of fluids and control\nsignals. In their existing setup, the manual mapping of information from P&ID\nsheets holds a significant challenge. This is a time-consuming process, taking\naround 3-6 months, and is susceptible to errors. It also depends on the\nexpertise of the domain experts and often requires multiple rounds of review.\nThe digitization of P&IDs entails merging detected line segments, which is\nessential for linking various detected instruments, thereby creating a\ncomprehensive digitized P&ID. This paper focuses on explaining how line\nsegments which are detected using a computer vision model are merged and\neventually building the connection between equipment and merged lines. Hence\npresenting a digitized form of information stating the interconnection between\nprocess equipment, instrumentation, flow of fluids and control signals.\nEventually, which can be stored in a knowledge graph and that information along\nwith the help of advanced algorithms can be leveraged for tasks like finding\noptimal routes, detecting system cycles, computing transitive closures, and\nmore."}
{"id": "2505.11959", "pdf": "https://arxiv.org/pdf/2505.11959", "abs": "https://arxiv.org/abs/2505.11959", "authors": ["Md. Rafiul Biswas", "Wajdi Zaghouani"], "title": "EmoHopeSpeech: An Annotated Dataset of Emotions and Hope Speech in English", "categories": ["cs.CL"], "comment": null, "summary": "This research introduces a bilingual dataset comprising 23,456 entries for\nArabic and 10,036 entries for English, annotated for emotions and hope speech,\naddressing the scarcity of multi-emotion (Emotion and hope) datasets. The\ndataset provides comprehensive annotations capturing emotion intensity,\ncomplexity, and causes, alongside detailed classifications and subcategories\nfor hope speech. To ensure annotation reliability, Fleiss' Kappa was employed,\nrevealing 0.75-0.85 agreement among annotators both for Arabic and English\nlanguage. The evaluation metrics (micro-F1-Score=0.67) obtained from the\nbaseline model (i.e., using a machine learning model) validate that the data\nannotations are worthy. This dataset offers a valuable resource for advancing\nnatural language processing in underrepresented languages, fostering better\ncross-linguistic analysis of emotions and hope speech."}
{"id": "2505.11980", "pdf": "https://arxiv.org/pdf/2505.11980", "abs": "https://arxiv.org/abs/2505.11980", "authors": ["Yi Chen", "Mu-Young Son", "Chuanbo Hua", "Joo-Young Kim"], "title": "AoP-SAM: Automation of Prompts for Efficient Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at AAAI 2025", "summary": "The Segment Anything Model (SAM) is a powerful foundation model for image\nsegmentation, showing robust zero-shot generalization through prompt\nengineering. However, relying on manual prompts is impractical for real-world\napplications, particularly in scenarios where rapid prompt provision and\nresource efficiency are crucial. In this paper, we propose the Automation of\nPrompts for SAM (AoP-SAM), a novel approach that learns to generate essential\nprompts in optimal locations automatically. AoP-SAM enhances SAM's efficiency\nand usability by eliminating manual input, making it better suited for\nreal-world tasks. Our approach employs a lightweight yet efficient Prompt\nPredictor model that detects key entities across images and identifies the\noptimal regions for placing prompt candidates. This method leverages SAM's\nimage embeddings, preserving its zero-shot generalization capabilities without\nrequiring fine-tuning. Additionally, we introduce a test-time instance-level\nAdaptive Sampling and Filtering mechanism that generates prompts in a\ncoarse-to-fine manner. This notably enhances both prompt and mask generation\nefficiency by reducing computational overhead and minimizing redundant mask\nrefinements. Evaluations of three datasets demonstrate that AoP-SAM\nsubstantially improves both prompt generation efficiency and mask generation\naccuracy, making SAM more effective for automated segmentation tasks."}
{"id": "2505.11965", "pdf": "https://arxiv.org/pdf/2505.11965", "abs": "https://arxiv.org/abs/2505.11965", "authors": ["Xu Liu", "Guanyi Chen"], "title": "CCNU at SemEval-2025 Task 3: Leveraging Internal and External Knowledge of Large Language Models for Multilingual Hallucination Annotation", "categories": ["cs.CL"], "comment": "SemEval-2025 Task 3", "summary": "We present the system developed by the Central China Normal University (CCNU)\nteam for the Mu-SHROOM shared task, which focuses on identifying hallucinations\nin question-answering systems across 14 different languages. Our approach\nleverages multiple Large Language Models (LLMs) with distinct areas of\nexpertise, employing them in parallel to annotate hallucinations, effectively\nsimulating a crowdsourcing annotation process. Furthermore, each LLM-based\nannotator integrates both internal and external knowledge related to the input\nduring the annotation process. Using the open-source LLM DeepSeek-V3, our\nsystem achieves the top ranking (\\#1) for Hindi data and secures a Top-5\nposition in seven other languages. In this paper, we also discuss unsuccessful\napproaches explored during our development process and share key insights\ngained from participating in this shared task."}
{"id": "2505.11983", "pdf": "https://arxiv.org/pdf/2505.11983", "abs": "https://arxiv.org/abs/2505.11983", "authors": ["Ting Xiao", "Lei Shi", "Yang Zhang", "HaoFeng Yang", "Zhe Wang", "Chenjia Bai"], "title": "Online Iterative Self-Alignment for Radiology Report Generation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ACL 2025 Main", "summary": "Radiology Report Generation (RRG) is an important research topic for\nrelieving radiologist' heavy workload. Existing RRG models mainly rely on\nsupervised fine-tuning (SFT) based on different model architectures using data\npairs of radiological images and corresponding radiologist-annotated reports.\nRecent research has shifted focus to post-training improvements, aligning RRG\nmodel outputs with human preferences using reinforcement learning (RL).\nHowever, the limited data coverage of high-quality annotated data poses risks\nof overfitting and generalization. This paper proposes a novel Online Iterative\nSelf-Alignment (OISA) method for RRG that consists of four stages:\nself-generation of diverse data, self-evaluation for multi-objective preference\ndata,self-alignment for multi-objective optimization and self-iteration for\nfurther improvement. Our approach allows for generating varied reports tailored\nto specific clinical objectives, enhancing the overall performance of the RRG\nmodel iteratively. Unlike existing methods, our frame-work significantly\nincreases data quality and optimizes performance through iterative\nmulti-objective optimization. Experimental results demonstrate that our method\nsurpasses previous approaches, achieving state-of-the-art performance across\nmultiple evaluation metrics."}
{"id": "2505.11969", "pdf": "https://arxiv.org/pdf/2505.11969", "abs": "https://arxiv.org/abs/2505.11969", "authors": ["Md. Rafiul Biswas", "Wajdi Zaghouani"], "title": "An Annotated Corpus of Arabic Tweets for Hate Speech Analysis", "categories": ["cs.CL"], "comment": null, "summary": "Identifying hate speech content in the Arabic language is challenging due to\nthe rich quality of dialectal variations. This study introduces a multilabel\nhate speech dataset in the Arabic language. We have collected 10000 Arabic\ntweets and annotated each tweet, whether it contains offensive content or not.\nIf a text contains offensive content, we further classify it into different\nhate speech targets such as religion, gender, politics, ethnicity, origin, and\nothers. A text can contain either single or multiple targets. Multiple\nannotators are involved in the data annotation task. We calculated the\ninter-annotator agreement, which was reported to be 0.86 for offensive content\nand 0.71 for multiple hate speech targets. Finally, we evaluated the data\nannotation task by employing a different transformers-based model in which\nAraBERTv2 outperformed with a micro-F1 score of 0.7865 and an accuracy of\n0.786."}
{"id": "2505.11992", "pdf": "https://arxiv.org/pdf/2505.11992", "abs": "https://arxiv.org/abs/2505.11992", "authors": ["Songchun Zhang", "Huiyao Xu", "Sitong Guo", "Zhongwei Xie", "Pengwei Liu", "Hujun Bao", "Weiwei Xu", "Changqing Zou"], "title": "SpatialCrafter: Unleashing the Imagination of Video Diffusion Models for Scene Reconstruction from Limited Observations", "categories": ["cs.CV"], "comment": "18 pages, 16 figures", "summary": "Novel view synthesis (NVS) boosts immersive experiences in computer vision\nand graphics. Existing techniques, though progressed, rely on dense multi-view\nobservations, restricting their application. This work takes on the challenge\nof reconstructing photorealistic 3D scenes from sparse or single-view inputs.\nWe introduce SpatialCrafter, a framework that leverages the rich knowledge in\nvideo diffusion models to generate plausible additional observations, thereby\nalleviating reconstruction ambiguity. Through a trainable camera encoder and an\nepipolar attention mechanism for explicit geometric constraints, we achieve\nprecise camera control and 3D consistency, further reinforced by a unified\nscale estimation strategy to handle scale discrepancies across datasets.\nFurthermore, by integrating monocular depth priors with semantic features in\nthe video latent space, our framework directly regresses 3D Gaussian primitives\nand efficiently processes long-sequence features using a hybrid network\nstructure. Extensive experiments show our method enhances sparse view\nreconstruction and restores the realistic appearance of 3D scenes."}
{"id": "2505.11995", "pdf": "https://arxiv.org/pdf/2505.11995", "abs": "https://arxiv.org/abs/2505.11995", "authors": ["Yuhao Wang", "Ruiyang Ren", "Yucheng Wang", "Wayne Xin Zhao", "Jing Liu", "Hua Wu", "Haifeng Wang"], "title": "Unveiling Knowledge Utilization Mechanisms in LLM-based Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": "SIGIR 2025", "summary": "Considering the inherent limitations of parametric knowledge in large\nlanguage models (LLMs), retrieval-augmented generation (RAG) is widely employed\nto expand their knowledge scope. Since RAG has shown promise in\nknowledge-intensive tasks like open-domain question answering, its broader\napplication to complex tasks and intelligent assistants has further advanced\nits utility. Despite this progress, the underlying knowledge utilization\nmechanisms of LLM-based RAG remain underexplored. In this paper, we present a\nsystematic investigation of the intrinsic mechanisms by which LLMs integrate\ninternal (parametric) and external (retrieved) knowledge in RAG scenarios.\nSpecially, we employ knowledge stream analysis at the macroscopic level, and\ninvestigate the function of individual modules at the microscopic level.\nDrawing on knowledge streaming analyses, we decompose the knowledge utilization\nprocess into four distinct stages within LLM layers: knowledge refinement,\nknowledge elicitation, knowledge expression, and knowledge contestation. We\nfurther demonstrate that the relevance of passages guides the streaming of\nknowledge through these stages. At the module level, we introduce a new method,\nknowledge activation probability entropy (KAPE) for neuron identification\nassociated with either internal or external knowledge. By selectively\ndeactivating these neurons, we achieve targeted shifts in the LLM's reliance on\none knowledge source over the other. Moreover, we discern complementary roles\nfor multi-head attention and multi-layer perceptron layers during knowledge\nformation. These insights offer a foundation for improving interpretability and\nreliability in retrieval-augmented LLMs, paving the way for more robust and\ntransparent generative solutions in knowledge-intensive domains."}
{"id": "2505.11997", "pdf": "https://arxiv.org/pdf/2505.11997", "abs": "https://arxiv.org/abs/2505.11997", "authors": ["Mingcheng Qu", "Guang Yang", "Donglin", "Tonghua Su", "Yue Gao", "Yang Song", "Lei Fan"], "title": "Multimodal Cancer Survival Analysis via Hypergraph Learning with Cross-Modality Rebalance", "categories": ["cs.CV"], "comment": "Code: https://github.com/MCPathology/MRePath", "summary": "Multimodal pathology-genomic analysis has become increasingly prominent in\ncancer survival prediction. However, existing studies mainly utilize\nmulti-instance learning to aggregate patch-level features, neglecting the\ninformation loss of contextual and hierarchical details within pathology\nimages. Furthermore, the disparity in data granularity and dimensionality\nbetween pathology and genomics leads to a significant modality imbalance. The\nhigh spatial resolution inherent in pathology data renders it a dominant role\nwhile overshadowing genomics in multimodal integration. In this paper, we\npropose a multimodal survival prediction framework that incorporates hypergraph\nlearning to effectively capture both contextual and hierarchical details from\npathology images. Moreover, it employs a modality rebalance mechanism and an\ninteractive alignment fusion strategy to dynamically reweight the contributions\nof the two modalities, thereby mitigating the pathology-genomics imbalance.\nQuantitative and qualitative experiments are conducted on five TCGA datasets,\ndemonstrating that our model outperforms advanced methods by over 3.4\\% in\nC-Index performance."}
{"id": "2505.12028", "pdf": "https://arxiv.org/pdf/2505.12028", "abs": "https://arxiv.org/abs/2505.12028", "authors": ["Yupei Ren", "Xinyi Zhou", "Ning Zhang", "Shangqing Zhao", "Man Lan", "Xiaopeng Bai"], "title": "Towards Comprehensive Argument Analysis in Education: Dataset, Tasks, and Method", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025; 13 pages, 3 figures", "summary": "Argument mining has garnered increasing attention over the years, with the\nrecent advancement of Large Language Models (LLMs) further propelling this\ntrend. However, current argument relations remain relatively simplistic and\nfoundational, struggling to capture the full scope of argument information,\nparticularly when it comes to representing complex argument structures in\nreal-world scenarios. To address this limitation, we propose 14 fine-grained\nrelation types from both vertical and horizontal dimensions, thereby capturing\nthe intricate interplay between argument components for a thorough\nunderstanding of argument structure. On this basis, we conducted extensive\nexperiments on three tasks: argument component detection, relation prediction,\nand automated essay grading. Additionally, we explored the impact of writing\nquality on argument component detection and relation prediction, as well as the\nconnections between discourse relations and argumentative features. The\nfindings highlight the importance of fine-grained argumentative annotations for\nargumentative writing quality assessment and encourage multi-dimensional\nargument analysis."}
{"id": "2505.12000", "pdf": "https://arxiv.org/pdf/2505.12000", "abs": "https://arxiv.org/abs/2505.12000", "authors": ["Tan-Hanh Pham", "Phu-Vinh Nguyen", "Dang The Hung", "Bui Trong Duong", "Vu Nguyen Thanh", "Chris Ngo", "Tri Quang Truong", "Truong-Son Hy"], "title": "IQBench: How \"Smart'' Are Vision-Language Models? A Study with Human IQ Tests", "categories": ["cs.CV"], "comment": "IQ Test for Multimodal Models", "summary": "Although large Vision-Language Models (VLMs) have demonstrated remarkable\nperformance in a wide range of multimodal tasks, their true reasoning\ncapabilities on human IQ tests remain underexplored. To advance research on the\nfluid intelligence of VLMs, we introduce **IQBench**, a new benchmark designed\nto evaluate VLMs on standardized visual IQ tests. We focus on evaluating the\nreasoning capabilities of VLMs, which we argue are more important than the\naccuracy of the final prediction. **Our benchmark is visually centric,\nminimizing the dependence on unnecessary textual content**, thus encouraging\nmodels to derive answers primarily from image-based information rather than\nlearned textual knowledge. To this end, we manually collected and annotated 500\nvisual IQ questions to **prevent unintentional data leakage during training**.\nUnlike prior work that focuses primarily on the accuracy of the final answer,\nwe evaluate the reasoning ability of the models by assessing their explanations\nand the patterns used to solve each problem, along with the accuracy of the\nfinal prediction and human evaluation. Our experiments show that there are\nsubstantial performance disparities between tasks, with models such as\n`o4-mini`, `gemini-2.5-flash`, and `claude-3.7-sonnet` achieving the highest\naverage accuracies of 0.615, 0.578, and 0.548, respectively. However, all\nmodels struggle with 3D spatial and anagram reasoning tasks, highlighting\nsignificant limitations in current VLMs' general reasoning abilities. In terms\nof reasoning scores, `o4-mini`, `gemini-2.5-flash`, and `claude-3.7-sonnet`\nachieved top averages of 0.696, 0.586, and 0.516, respectively. These results\nhighlight inconsistencies between the reasoning processes of the models and\ntheir final answers, emphasizing the importance of evaluating the accuracy of\nthe reasoning in addition to the final predictions."}
{"id": "2505.12043", "pdf": "https://arxiv.org/pdf/2505.12043", "abs": "https://arxiv.org/abs/2505.12043", "authors": ["Jingxue Chen", "Qingkun Tang", "Qianchun Lu", "Siyuan Fang"], "title": "MoL for LLMs: Dual-Loss Optimization to Enhance Domain Expertise While Preserving General Capabilities", "categories": ["cs.CL"], "comment": "12 pages, 2 figures", "summary": "Although LLMs perform well in general tasks, domain-specific applications\nsuffer from hallucinations and accuracy limitations. CPT approaches encounter\ntwo key issues: (1) domain-biased data degrades general language skills, and\n(2) improper corpus-mixture ratios limit effective adaptation. To address\nthese, we propose a novel framework, Mixture of Losses (MoL), which decouples\noptimization objectives for domain-specific and general corpora. Specifically,\ncross-entropy (CE) loss is applied to domain data to ensure knowledge\nacquisition, while Kullback-Leibler (KL) divergence aligns general-corpus\ntraining with the base model's foundational capabilities. This dual-loss\narchitecture preserves universal skills while enhancing domain expertise,\navoiding catastrophic forgetting. Empirically, we validate that a 1:1\ndomain-to-general corpus ratio optimally balances training and overfitting\nwithout the need for extensive tuning or resource-intensive experiments.\nFurthermore, our experiments demonstrate significant performance gains compared\nto traditional CPT approaches, which often suffer from degradation in general\nlanguage capabilities; our model achieves 27.9% higher accuracy on the Math-500\nbenchmark in the non-think reasoning mode, and an impressive 83.3% improvement\non the challenging AIME25 subset in the think mode, underscoring the\neffectiveness of our approach."}
{"id": "2505.12005", "pdf": "https://arxiv.org/pdf/2505.12005", "abs": "https://arxiv.org/abs/2505.12005", "authors": ["Dong Liu", "Yifan Yang", "Zixiong Huang", "Yuxin Gao", "Mingkui Tan"], "title": "CHRIS: Clothed Human Reconstruction with Side View Consistency", "categories": ["cs.CV", "cs.AI"], "comment": "ICME 2025", "summary": "Creating a realistic clothed human from a single-view RGB image is crucial\nfor applications like mixed reality and filmmaking. Despite some progress in\nrecent years, mainstream methods often fail to fully utilize side-view\ninformation, as the input single-view image contains front-view information\nonly. This leads to globally unrealistic topology and local surface\ninconsistency in side views. To address these, we introduce Clothed Human\nReconstruction with Side View Consistency, namely CHRIS, which consists of 1) A\nSide-View Normal Discriminator that enhances global visual reasonability by\ndistinguishing the generated side-view normals from the ground truth ones; 2) A\nMulti-to-One Gradient Computation (M2O) that ensures local surface consistency.\nM2O calculates the gradient of a sampling point by integrating the gradients of\nthe nearby points, effectively acting as a smooth operation. Experimental\nresults demonstrate that CHRIS achieves state-of-the-art performance on public\nbenchmarks and outperforms the prior work."}
{"id": "2505.12050", "pdf": "https://arxiv.org/pdf/2505.12050", "abs": "https://arxiv.org/abs/2505.12050", "authors": ["Vinod Raman", "Hilal Asi", "Satyen Kale"], "title": "ABoN: Adaptive Best-of-N Alignment", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "23 pages", "summary": "Recent advances in test-time alignment methods, such as Best-of-N sampling,\noffer a simple and effective way to steer language models (LMs) toward\npreferred behaviors using reward models (RM). However, these approaches can be\ncomputationally expensive, especially when applied uniformly across prompts\nwithout accounting for differences in alignment difficulty. In this work, we\npropose a prompt-adaptive strategy for Best-of-N alignment that allocates\ninference-time compute more efficiently. Motivated by latency concerns, we\ndevelop a two-stage algorithm: an initial exploratory phase estimates the\nreward distribution for each prompt using a small exploration budget, and a\nsecond stage adaptively allocates the remaining budget using these estimates.\nOur method is simple, practical, and compatible with any LM/RM combination.\nEmpirical results on the AlpacaEval dataset for 12 LM/RM pairs and 50 different\nbatches of prompts show that our adaptive strategy consistently outperforms the\nuniform allocation with the same inference budget. Moreover, our experiments\nshow that our adaptive strategy remains competitive against uniform allocations\nwith 20% larger inference budgets and even improves in performance as the batch\nsize grows."}
{"id": "2505.12007", "pdf": "https://arxiv.org/pdf/2505.12007", "abs": "https://arxiv.org/abs/2505.12007", "authors": ["Runduo Han", "Xiuping Liu", "Shangxuan Yi", "Yi Zhang", "Hongchen Tan"], "title": "Multi-modal Collaborative Optimization and Expansion Network for Event-assisted Single-eye Expression Recognition", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we proposed a Multi-modal Collaborative Optimization and\nExpansion Network (MCO-E Net), to use event modalities to resist challenges\nsuch as low light, high exposure, and high dynamic range in single-eye\nexpression recognition tasks. The MCO-E Net introduces two innovative designs:\nMulti-modal Collaborative Optimization Mamba (MCO-Mamba) and Heterogeneous\nCollaborative and Expansion Mixture-of-Experts (HCE-MoE). MCO-Mamba, building\nupon Mamba, leverages dual-modal information to jointly optimize the model,\nfacilitating collaborative interaction and fusion of modal semantics. This\napproach encourages the model to balance the learning of both modalities and\nharness their respective strengths. HCE-MoE, on the other hand, employs a\ndynamic routing mechanism to distribute structurally varied experts (deep,\nattention, and focal), fostering collaborative learning of complementary\nsemantics. This heterogeneous architecture systematically integrates diverse\nfeature extraction paradigms to comprehensively capture expression semantics.\nExtensive experiments demonstrate that our proposed network achieves\ncompetitive performance in the task of single-eye expression recognition,\nespecially under poor lighting conditions."}
{"id": "2505.12054", "pdf": "https://arxiv.org/pdf/2505.12054", "abs": "https://arxiv.org/abs/2505.12054", "authors": ["Matúš Pikuliak"], "title": "GenderBench: Evaluation Suite for Gender Biases in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "We present GenderBench -- a comprehensive evaluation suite designed to\nmeasure gender biases in LLMs. GenderBench includes 14 probes that quantify 19\ngender-related harmful behaviors exhibited by LLMs. We release GenderBench as\nan open-source and extensible library to improve the reproducibility and\nrobustness of benchmarking across the field. We also publish our evaluation of\n12 LLMs. Our measurements reveal consistent patterns in their behavior. We show\nthat LLMs struggle with stereotypical reasoning, equitable gender\nrepresentation in generated texts, and occasionally also with discriminatory\nbehavior in high-stakes scenarios, such as hiring."}
{"id": "2505.12009", "pdf": "https://arxiv.org/pdf/2505.12009", "abs": "https://arxiv.org/abs/2505.12009", "authors": ["Zhiying Li", "Guanggang Geng", "Yeying Jin", "Zhizhi Guo", "Bruce Gu", "Jidong Huo", "Zhaoxin Fan", "Wenjun Wu"], "title": "Black-box Adversaries from Latent Space: Unnoticeable Attacks on Human Pose and Shape Estimation", "categories": ["cs.CV"], "comment": "17 pages, 6 figures", "summary": "Expressive human pose and shape (EHPS) estimation is vital for digital human\ngeneration, particularly in live-streaming applications. However, most existing\nEHPS models focus primarily on minimizing estimation errors, with limited\nattention on potential security vulnerabilities. Current adversarial attacks on\nEHPS models often require white-box access (e.g., model details or gradients)\nor generate visually conspicuous perturbations, limiting their practicality and\nability to expose real-world security threats. To address these limitations, we\npropose a novel Unnoticeable Black-Box Attack (UBA) against EHPS models. UBA\nleverages the latent-space representations of natural images to generate an\noptimal adversarial noise pattern and iteratively refine its attack potency\nalong an optimized direction in digital space. Crucially, this process relies\nsolely on querying the model's output, requiring no internal knowledge of the\nEHPS architecture, while guiding the noise optimization toward greater stealth\nand effectiveness. Extensive experiments and visual analyses demonstrate the\nsuperiority of UBA. Notably, UBA increases the pose estimation errors of EHPS\nmodels by 17.27%-58.21% on average, revealing critical vulnerabilities. These\nfindings underscore the urgent need to address and mitigate security risks\nassociated with digital human generation systems."}
{"id": "2505.12060", "pdf": "https://arxiv.org/pdf/2505.12060", "abs": "https://arxiv.org/abs/2505.12060", "authors": ["Peng Ding", "Jun Kuang", "Zongyu Wang", "Xuezhi Cao", "Xunliang Cai", "Jiajun Chen", "Shujian Huang"], "title": "Why Not Act on What You Know? Unleashing Safety Potential of LLMs via Self-Aware Guard Enhancement", "categories": ["cs.CL"], "comment": "Acccepted by ACL 2025 Findings, 21 pages, 9 figures, 14 tables", "summary": "Large Language Models (LLMs) have shown impressive capabilities across\nvarious tasks but remain vulnerable to meticulously crafted jailbreak attacks.\nIn this paper, we identify a critical safety gap: while LLMs are adept at\ndetecting jailbreak prompts, they often produce unsafe responses when directly\nprocessing these inputs. Inspired by this insight, we propose SAGE (Self-Aware\nGuard Enhancement), a training-free defense strategy designed to align LLMs'\nstrong safety discrimination performance with their relatively weaker safety\ngeneration ability. SAGE consists of two core components: a Discriminative\nAnalysis Module and a Discriminative Response Module, enhancing resilience\nagainst sophisticated jailbreak attempts through flexible safety discrimination\ninstructions. Extensive experiments demonstrate SAGE's effectiveness and\nrobustness across various open-source and closed-source LLMs of different sizes\nand architectures, achieving an average 99% defense success rate against\nnumerous complex and covert jailbreak methods while maintaining helpfulness on\ngeneral benchmarks. We further conduct mechanistic interpretability analysis\nthrough hidden states and attention distributions, revealing the underlying\nmechanisms of this detection-generation discrepancy. Our work thus contributes\nto developing future LLMs with coherent safety awareness and generation\nbehavior. Our code and datasets are publicly available at\nhttps://github.com/NJUNLP/SAGE."}
{"id": "2505.12021", "pdf": "https://arxiv.org/pdf/2505.12021", "abs": "https://arxiv.org/abs/2505.12021", "authors": ["Kazuhiko Kawamoto", "Atsuhiro Endo", "Hiroshi Kera"], "title": "Cross-Model Transfer of Task Vectors via Few-Shot Orthogonal Alignment", "categories": ["cs.CV"], "comment": "8 pages", "summary": "Task arithmetic enables efficient model editing by representing task-specific\nchanges as vectors in parameter space. Task arithmetic typically assumes that\nthe source and target models are initialized from the same pre-trained\nparameters. This assumption limits its applicability in cross-model transfer\nsettings, where models are independently pre-trained on different datasets. To\naddress this challenge, we propose a method based on few-shot orthogonal\nalignment, which aligns task vectors to the parameter space of a differently\npre-trained target model. These transformations preserve key properties of task\nvectors, such as norm and rank, and are learned using only a small number of\nlabeled examples. We evaluate the method using two Vision Transformers\npre-trained on YFCC100M and LAION400M, and test on eight classification\ndatasets. Experimental results show that our method improves transfer accuracy\nover direct task vector application and achieves performance comparable to\nfew-shot fine-tuning, while maintaining the modularity and reusability of task\nvectors. Our code is available at\nhttps://github.com/kawakera-lab/CrossModelTransfer."}
{"id": "2505.12071", "pdf": "https://arxiv.org/pdf/2505.12071", "abs": "https://arxiv.org/abs/2505.12071", "authors": ["Harald Baayen", "Kristian Berg", "Maziyah Mohamed"], "title": "Historical and psycholinguistic perspectives on morphological productivity: A sketch of an integrative approach", "categories": ["cs.CL"], "comment": "35 pages, 11 figures", "summary": "In this study, we approach morphological productivity from two perspectives:\na cognitive-computational perspective, and a diachronic perspective zooming in\non an actual speaker, Thomas Mann. For developing the first perspective, we\nmake use of a cognitive computational model of the mental lexicon, the\ndiscriminative lexicon model. For computational mappings between form and\nmeaning to be productive, in the sense that novel, previously unencountered\nwords, can be understood and produced, there must be systematicities between\nthe form space and the semantic space. If the relation between form and meaning\nwould be truly arbitrary, a model could memorize form and meaning pairings, but\nthere is no way in which the model would be able to generalize to novel test\ndata. For Finnish nominal inflection, Malay derivation, and English\ncompounding, we explore, using the Discriminative Lexicon Model as a\ncomputational tool, to trace differences in the degree to which inflectional\nand word formation patterns are productive. We show that the DLM tends to\nassociate affix-like sublexical units with the centroids of the embeddings of\nthe words with a given affix. For developing the second perspective, we study\nhow the intake and output of one prolific writer, Thomas Mann, changes over\ntime. We show by means of an examination of what Thomas Mann is likely to have\nread, and what he wrote, that the rate at which Mann produces novel derived\nwords is extremely low. There are far more novel words in his input than in his\noutput. We show that Thomas Mann is less likely to produce a novel derived word\nwith a given suffix the greater the average distance is of the embeddings of\nall derived words to the corresponding centroid, and discuss the challenges of\nusing speaker-specific embeddings for low-frequency and novel words."}
{"id": "2505.12045", "pdf": "https://arxiv.org/pdf/2505.12045", "abs": "https://arxiv.org/abs/2505.12045", "authors": ["Shuai Yuan", "Guowen Xu", "Hongwei Li", "Rui Zhang", "Xinyuan Qian", "Wenbo Jiang", "Hangcheng Cao", "Qingchuan Zhao"], "title": "FIGhost: Fluorescent Ink-based Stealthy and Flexible Backdoor Attacks on Physical Traffic Sign Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Traffic sign recognition (TSR) systems are crucial for autonomous driving but\nare vulnerable to backdoor attacks. Existing physical backdoor attacks either\nlack stealth, provide inflexible attack control, or ignore emerging\nVision-Large-Language-Models (VLMs). In this paper, we introduce FIGhost, the\nfirst physical-world backdoor attack leveraging fluorescent ink as triggers.\nFluorescent triggers are invisible under normal conditions and activated\nstealthily by ultraviolet light, providing superior stealthiness, flexibility,\nand untraceability. Inspired by real-world graffiti, we derive realistic\ntrigger shapes and enhance their robustness via an interpolation-based\nfluorescence simulation algorithm. Furthermore, we develop an automated\nbackdoor sample generation method to support three attack objectives. Extensive\nevaluations in the physical world demonstrate FIGhost's effectiveness against\nstate-of-the-art detectors and VLMs, maintaining robustness under environmental\nvariations and effectively evading existing defenses."}
{"id": "2505.12075", "pdf": "https://arxiv.org/pdf/2505.12075", "abs": "https://arxiv.org/abs/2505.12075", "authors": ["Guy Davidson", "Todd M. Gureckis", "Brenden M. Lake", "Adina Williams"], "title": "Do different prompting methods yield a common task representation in language models?", "categories": ["cs.CL", "cs.LG"], "comment": "9 pages, 4 figures; under review", "summary": "Demonstrations and instructions are two primary approaches for prompting\nlanguage models to perform in-context learning (ICL) tasks. Do identical tasks\nelicited in different ways result in similar representations of the task? An\nimproved understanding of task representation mechanisms would offer\ninterpretability insights and may aid in steering models. We study this through\nfunction vectors, recently proposed as a mechanism to extract few-shot ICL task\nrepresentations. We generalize function vectors to alternative task\npresentations, focusing on short textual instruction prompts, and successfully\nextract instruction function vectors that promote zero-shot task accuracy. We\nfind evidence that demonstration- and instruction-based function vectors\nleverage different model components, and offer several controls to dissociate\ntheir contributions to task performance. Our results suggest that different\ntask presentations do not induce a common task representation but elicit\ndifferent, partly overlapping mechanisms. Our findings offer principled support\nto the practice of combining textual instructions and task demonstrations,\nimply challenges in universally monitoring task inference across presentation\nforms, and encourage further examinations of LLM task inference mechanisms."}
{"id": "2505.12048", "pdf": "https://arxiv.org/pdf/2505.12048", "abs": "https://arxiv.org/abs/2505.12048", "authors": ["Rui Qin", "Qijie Wang", "Ming Sun", "Haowei Zhu", "Chao Zhou", "Bin Wang"], "title": "Accelerating Diffusion-based Super-Resolution with Dynamic Time-Spatial Sampling", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have gained attention for their success in modeling complex\ndistributions, achieving impressive perceptual quality in SR tasks. However,\nexisting diffusion-based SR methods often suffer from high computational costs,\nrequiring numerous iterative steps for training and inference. Existing\nacceleration techniques, such as distillation and solver optimization, are\ngenerally task-agnostic and do not fully leverage the specific characteristics\nof low-level tasks like super-resolution (SR). In this study, we analyze the\nfrequency- and spatial-domain properties of diffusion-based SR methods,\nrevealing key insights into the temporal and spatial dependencies of\nhigh-frequency signal recovery. Specifically, high-frequency details benefit\nfrom concentrated optimization during early and late diffusion iterations,\nwhile spatially textured regions demand adaptive denoising strategies. Building\non these observations, we propose the Time-Spatial-aware Sampling strategy\n(TSS) for the acceleration of Diffusion SR without any extra training cost. TSS\ncombines Time Dynamic Sampling (TDS), which allocates more iterations to\nrefining textures, and Spatial Dynamic Sampling (SDS), which dynamically\nadjusts strategies based on image content. Extensive evaluations across\nmultiple benchmarks demonstrate that TSS achieves state-of-the-art (SOTA)\nperformance with significantly fewer iterations, improving MUSIQ scores by 0.2\n- 3.0 and outperforming the current acceleration methods with only half the\nnumber of steps."}
{"id": "2505.12082", "pdf": "https://arxiv.org/pdf/2505.12082", "abs": "https://arxiv.org/abs/2505.12082", "authors": ["Yunshui Li", "Yiyuan Ma", "Shen Yan", "Chaoyi Zhang", "Jing Liu", "Jianqiao Lu", "Ziwen Xu", "Mengzhao Chen", "Minrui Wang", "Shiyi Zhan", "Jin Ma", "Xunhao Lai", "Yao Luo", "Xingyan Bin", "Hongbin Ren", "Mingji Han", "Wenhao Hao", "Bairen Yi", "LingJun Liu", "Bole Ma", "Xiaoying Jia", "Zhou Xun", "Liang Xiang", "Yonghui Wu"], "title": "Model Merging in Pre-training of Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Model merging has emerged as a promising technique for enhancing large\nlanguage models, though its application in large-scale pre-training remains\nrelatively unexplored. In this paper, we present a comprehensive investigation\nof model merging techniques during the pre-training process. Through extensive\nexperiments with both dense and Mixture-of-Experts (MoE) architectures ranging\nfrom millions to over 100 billion parameters, we demonstrate that merging\ncheckpoints trained with constant learning rates not only achieves significant\nperformance improvements but also enables accurate prediction of annealing\nbehavior. These improvements lead to both more efficient model development and\nsignificantly lower training costs. Our detailed ablation studies on merging\nstrategies and hyperparameters provide new insights into the underlying\nmechanisms while uncovering novel applications. Through comprehensive\nexperimental analysis, we offer the open-source community practical\npre-training guidelines for effective model merging."}
{"id": "2505.12053", "pdf": "https://arxiv.org/pdf/2505.12053", "abs": "https://arxiv.org/abs/2505.12053", "authors": ["Tianxiong Zhong", "Xingye Tian", "Boyuan Jiang", "Xuebo Wang", "Xin Tao", "Pengfei Wan", "Zhiwei Zhang"], "title": "VFRTok: Variable Frame Rates Video Tokenizer with Duration-Proportional Information Assumption", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 10 figures", "summary": "Modern video generation frameworks based on Latent Diffusion Models suffer\nfrom inefficiencies in tokenization due to the Frame-Proportional Information\nAssumption. Existing tokenizers provide fixed temporal compression rates,\ncausing the computational cost of the diffusion model to scale linearly with\nthe frame rate. The paper proposes the Duration-Proportional Information\nAssumption: the upper bound on the information capacity of a video is\nproportional to the duration rather than the number of frames. Based on this\ninsight, the paper introduces VFRTok, a Transformer-based video tokenizer, that\nenables variable frame rate encoding and decoding through asymmetric frame rate\ntraining between the encoder and decoder. Furthermore, the paper proposes\nPartial Rotary Position Embeddings (RoPE) to decouple position and content\nmodeling, which groups correlated patches into unified tokens. The Partial RoPE\neffectively improves content-awareness, enhancing the video generation\ncapability. Benefiting from the compact and continuous spatio-temporal\nrepresentation, VFRTok achieves competitive reconstruction quality and\nstate-of-the-art generation fidelity while using only 1/8 tokens compared to\nexisting tokenizers."}
{"id": "2505.12090", "pdf": "https://arxiv.org/pdf/2505.12090", "abs": "https://arxiv.org/abs/2505.12090", "authors": ["Mohammad Shokri", "Sarah Ita Levitan", "Rivka Levitan"], "title": "Personalized Author Obfuscation with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this paper, we investigate the efficacy of large language models (LLMs) in\nobfuscating authorship by paraphrasing and altering writing styles. Rather than\nadopting a holistic approach that evaluates performance across the entire\ndataset, we focus on user-wise performance to analyze how obfuscation\neffectiveness varies across individual authors. While LLMs are generally\neffective, we observe a bimodal distribution of efficacy, with performance\nvarying significantly across users. To address this, we propose a personalized\nprompting method that outperforms standard prompting techniques and partially\nmitigates the bimodality issue."}
{"id": "2505.12066", "pdf": "https://arxiv.org/pdf/2505.12066", "abs": "https://arxiv.org/abs/2505.12066", "authors": ["Yijie Zheng", "Jinxuan Yang", "Yu Chen", "Yaxuan Wang", "Yihang Lu", "Guoqing Li"], "title": "Beluga Whale Detection from Satellite Imagery with Point Labels", "categories": ["cs.CV"], "comment": "Accepted for oral presentation at IGARSS 2025. Session at\n  https://www.2025.ieeeigarss.org/view_paper.php?PaperNum=2430&SessionID=1426", "summary": "Very high-resolution (VHR) satellite imagery has emerged as a powerful tool\nfor monitoring marine animals on a large scale. However, existing deep\nlearning-based whale detection methods usually require manually created,\nhigh-quality bounding box annotations, which are labor-intensive to produce.\nMoreover, existing studies often exclude ``uncertain whales'', individuals that\nhave ambiguous appearances in satellite imagery, limiting the applicability of\nthese models in real-world scenarios. To address these limitations, this study\nintroduces an automated pipeline for detecting beluga whales and harp seals in\nVHR satellite imagery. The pipeline leverages point annotations and the Segment\nAnything Model (SAM) to generate precise bounding box annotations, which are\nused to train YOLOv8 for multiclass detection of certain whales, uncertain\nwhales, and harp seals. Experimental results demonstrated that SAM-generated\nannotations significantly improved detection performance, achieving higher\n$\\text{F}_\\text{1}$-scores compared to traditional buffer-based annotations.\nYOLOv8 trained on SAM-labeled boxes achieved an overall\n$\\text{F}_\\text{1}$-score of 72.2% for whales overall and 70.3% for harp seals,\nwith superior performance in dense scenes. The proposed approach not only\nreduces the manual effort required for annotation but also enhances the\ndetection of uncertain whales, offering a more comprehensive solution for\nmarine animal monitoring. This method holds great potential for extending to\nother species, habitats, and remote sensing platforms, as well as for\nestimating whale biometrics, thereby advancing ecological monitoring and\nconservation efforts. The codes for our label and detection pipeline are\npublicly available at http://github.com/voyagerxvoyagerx/beluga-seeker ."}
{"id": "2505.12100", "pdf": "https://arxiv.org/pdf/2505.12100", "abs": "https://arxiv.org/abs/2505.12100", "authors": ["Isabela Pereira Gregio", "Ian Pons", "Anna Helena Reali Costa", "Artur Jordão"], "title": "Improving Fairness in LLMs Through Testing-Time Adversaries", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) push the bound-aries in natural language\nprocessing and generative AI, driving progress across various aspects of modern\nsociety. Unfortunately, the pervasive issue of bias in LLMs responses (i.e.,\npredictions) poses a significant and open challenge, hindering their\napplication in tasks involving ethical sensitivity and responsible\ndecision-making. In this work, we propose a straightforward, user-friendly and\npractical method to mitigate such biases, enhancing the reliability and\ntrustworthiness of LLMs. Our method creates multiple variations of a given\nsentence by modifying specific attributes and evaluates the corresponding\nprediction behavior compared to the original, unaltered, prediction/sentence.\nThe idea behind this process is that critical ethical predictions often exhibit\nnotable inconsistencies, indicating the presence of bias. Unlike previous\napproaches, our method relies solely on forward passes (i.e., testing-time\nadversaries), eliminating the need for training, fine-tuning, or prior\nknowledge of the training data distribution. Through extensive experiments on\nthe popular Llama family, we demonstrate the effectiveness of our method in\nimproving various fairness metrics, focusing on the reduction of disparities in\nhow the model treats individuals from different racial groups. Specifically,\nusing standard metrics, we improve the fairness in Llama3 in up to 27\npercentage points. Overall, our approach significantly enhances fairness,\nequity, and reliability in LLM-generated results without parameter tuning or\ntraining data modifications, confirming its effectiveness in practical\nscenarios. We believe our work establishes an important step toward enabling\nthe use of LLMs in tasks that require ethical considerations and responsible\ndecision-making."}
{"id": "2505.12069", "pdf": "https://arxiv.org/pdf/2505.12069", "abs": "https://arxiv.org/abs/2505.12069", "authors": ["Shenzhou Liu", "Di Wang", "Haonan Guo", "Chengxi Han", "Wenzhi Zeng"], "title": "MT-CYP-Net: Multi-Task Network for Pixel-Level Crop Yield Prediction Under Very Few Samples", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate and fine-grained crop yield prediction plays a crucial role in\nadvancing global agriculture. However, the accuracy of pixel-level yield\nestimation based on satellite remote sensing data has been constrained by the\nscarcity of ground truth data. To address this challenge, we propose a novel\napproach called the Multi-Task Crop Yield Prediction Network (MT-CYP-Net). This\nframework introduces an effective multi-task feature-sharing strategy, where\nfeatures extracted from a shared backbone network are simultaneously utilized\nby both crop yield prediction decoders and crop classification decoders with\nthe ability to fuse information between them. This design allows MT-CYP-Net to\nbe trained with extremely sparse crop yield point labels and crop type labels,\nwhile still generating detailed pixel-level crop yield maps. Concretely, we\ncollected 1,859 yield point labels along with corresponding crop type labels\nand satellite images from eight farms in Heilongjiang Province, China, in 2023,\ncovering soybean, maize, and rice crops, and constructed a sparse crop yield\nlabel dataset. MT-CYP-Net is compared with three classical machine learning and\ndeep learning benchmark methods in this dataset. Experimental results not only\nindicate the superiority of MT-CYP-Net compared to previous methods on multiple\ntypes of crops but also demonstrate the potential of deep networks on precise\npixel-level crop yield prediction, especially with limited data labels."}
{"id": "2505.12116", "pdf": "https://arxiv.org/pdf/2505.12116", "abs": "https://arxiv.org/abs/2505.12116", "authors": ["Fitsum Gaim", "Hoyun Song", "Huije Lee", "Changgeon Ko", "Eui Jun Hwang", "Jong C. Park"], "title": "A Multi-Task Benchmark for Abusive Language Detection in Low-Resource Settings", "categories": ["cs.CL", "I.2.7"], "comment": null, "summary": "Content moderation research has recently made significant advances, but still\nfails to serve the majority of the world's languages due to the lack of\nresources, leaving millions of vulnerable users to online hostility. This work\npresents a large-scale human-annotated multi-task benchmark dataset for abusive\nlanguage detection in Tigrinya social media with joint annotations for three\ntasks: abusiveness, sentiment, and topic classification. The dataset comprises\n13,717 YouTube comments annotated by nine native speakers, collected from 7,373\nvideos with a total of over 1.2 billion views across 51 channels. We developed\nan iterative term clustering approach for effective data selection. Recognizing\nthat around 64% of Tigrinya social media content uses Romanized\ntransliterations rather than native Ge'ez script, our dataset accommodates both\nwriting systems to reflect actual language use. We establish strong baselines\nacross the tasks in the benchmark, while leaving significant challenges for\nfuture contributions. Our experiments reveal that small, specialized multi-task\nmodels outperform the current frontier models in the low-resource setting,\nachieving up to 86% accuracy (+7 points) in abusiveness detection. We make the\nresources publicly available to promote research on online safety."}
{"id": "2505.12074", "pdf": "https://arxiv.org/pdf/2505.12074", "abs": "https://arxiv.org/abs/2505.12074", "authors": ["Chen Shu", "Boyu Fu", "Yiman Li", "Ting Yin", "Wenchuan Zhang", "Jie Chen", "Yuhao Yi", "Hong Bu"], "title": "Denoising Mutual Knowledge Distillation in Bi-Directional Multiple Instance Learning", "categories": ["cs.CV"], "comment": "12 pages, 2 figures", "summary": "Multiple Instance Learning is the predominant method for Whole Slide Image\nclassification in digital pathology, enabling the use of slide-level labels to\nsupervise model training. Although MIL eliminates the tedious fine-grained\nannotation process for supervised learning, whether it can learn accurate bag-\nand instance-level classifiers remains a question. To address the issue,\ninstance-level classifiers and instance masks were incorporated to ground the\nprediction on supporting patches. These methods, while practically improving\nthe performance of MIL methods, may potentially introduce noisy labels. We\npropose to bridge the gap between commonly used MIL and fully supervised\nlearning by augmenting both the bag- and instance-level learning processes with\npseudo-label correction capabilities elicited from weak to strong\ngeneralization techniques. The proposed algorithm improves the performance of\ndual-level MIL algorithms on both bag- and instance-level predictions.\nExperiments on public pathology datasets showcase the advantage of the proposed\nmethods."}
{"id": "2505.12158", "pdf": "https://arxiv.org/pdf/2505.12158", "abs": "https://arxiv.org/abs/2505.12158", "authors": ["Elisa Bassignana", "Amanda Cercas Curry", "Dirk Hovy"], "title": "The AI Gap: How Socioeconomic Status Affects Language Technology Interactions", "categories": ["cs.CL"], "comment": "Accepted at ACL Main 2025", "summary": "Socioeconomic status (SES) fundamentally influences how people interact with\neach other and more recently, with digital technologies like Large Language\nModels (LLMs). While previous research has highlighted the interaction between\nSES and language technology, it was limited by reliance on proxy metrics and\nsynthetic data. We survey 1,000 individuals from diverse socioeconomic\nbackgrounds about their use of language technologies and generative AI, and\ncollect 6,482 prompts from their previous interactions with LLMs. We find\nsystematic differences across SES groups in language technology usage (i.e.,\nfrequency, performed tasks), interaction styles, and topics. Higher SES entails\na higher level of abstraction, convey requests more concisely, and topics like\n'inclusivity' and 'travel'. Lower SES correlates with higher\nanthropomorphization of LLMs (using ''hello'' and ''thank you'') and more\nconcrete language. Our findings suggest that while generative language\ntechnologies are becoming more accessible to everyone, socioeconomic linguistic\ndifferences still stratify their use to exacerbate the digital divide. These\ndifferences underscore the importance of considering SES in developing language\ntechnologies to accommodate varying linguistic needs rooted in socioeconomic\nfactors and limit the AI Gap across SES groups."}
{"id": "2505.12081", "pdf": "https://arxiv.org/pdf/2505.12081", "abs": "https://arxiv.org/abs/2505.12081", "authors": ["Yuqi Liu", "Tianyuan Qu", "Zhisheng Zhong", "Bohao Peng", "Shu Liu", "Bei Yu", "Jiaya Jia"], "title": "VisionReasoner: Unified Visual Perception and Reasoning via Reinforcement Learning", "categories": ["cs.CV"], "comment": null, "summary": "Large vision-language models exhibit inherent capabilities to handle diverse\nvisual perception tasks. In this paper, we introduce VisionReasoner, a unified\nframework capable of reasoning and solving multiple visual perception tasks\nwithin a shared model. Specifically, by designing novel multi-object cognitive\nlearning strategies and systematic task reformulation, VisionReasoner enhances\nits reasoning capabilities to analyze visual inputs, and addresses diverse\nperception tasks in a unified framework. The model generates a structured\nreasoning process before delivering the desired outputs responding to user\nqueries. To rigorously assess unified visual perception capabilities, we\nevaluate VisionReasoner on ten diverse tasks spanning three critical domains:\ndetection, segmentation, and counting. Experimental results show that\nVisionReasoner achieves superior performance as a unified model, outperforming\nQwen2.5VL by relative margins of 29.1% on COCO (detection), 22.1% on ReasonSeg\n(segmentation), and 15.3% on CountBench (counting)."}
{"id": "2505.12160", "pdf": "https://arxiv.org/pdf/2505.12160", "abs": "https://arxiv.org/abs/2505.12160", "authors": ["Darmawan Wicaksono", "Hasri Akbar Awal Rozaq", "Nevfel Boz"], "title": "Emotion Recognition for Low-Resource Turkish: Fine-Tuning BERTurk on TREMO and Testing on Xenophobic Political Discourse", "categories": ["cs.CL"], "comment": null, "summary": "Social media platforms like X (formerly Twitter) play a crucial role in\nshaping public discourse and societal norms. This study examines the term\nSessiz Istila (Silent Invasion) on Turkish social media, highlighting the rise\nof anti-refugee sentiment amidst the Syrian refugee influx. Using BERTurk and\nthe TREMO dataset, we developed an advanced Emotion Recognition Model (ERM)\ntailored for Turkish, achieving 92.62% accuracy in categorizing emotions such\nas happiness, fear, anger, sadness, disgust, and surprise. By applying this\nmodel to large-scale X data, the study uncovers emotional nuances in Turkish\ndiscourse, contributing to computational social science by advancing sentiment\nanalysis in underrepresented languages and enhancing our understanding of\nglobal digital discourse and the unique linguistic challenges of Turkish. The\nfindings underscore the transformative potential of localized NLP tools, with\nour ERM model offering practical applications for real-time sentiment analysis\nin Turkish-language contexts. By addressing critical areas, including\nmarketing, public relations, and crisis management, these models facilitate\nimproved decision-making through timely and accurate sentiment tracking. This\nhighlights the significance of advancing research that accounts for regional\nand linguistic nuances."}
{"id": "2505.12098", "pdf": "https://arxiv.org/pdf/2505.12098", "abs": "https://arxiv.org/abs/2505.12098", "authors": ["Jiarui Wang", "Huiyu Duan", "Ziheng Jia", "Yu Zhao", "Woo Yi Yang", "Zicheng Zhang", "Zijian Chen", "Juntong Wang", "Yuke Xing", "Guangtao Zhai", "Xiongkuo Min"], "title": "LOVE: Benchmarking and Evaluating Text-to-Video Generation and Video-to-Text Interpretation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in large multimodal models (LMMs) have driven substantial\nprogress in both text-to-video (T2V) generation and video-to-text (V2T)\ninterpretation tasks. However, current AI-generated videos (AIGVs) still\nexhibit limitations in terms of perceptual quality and text-video alignment.\nTherefore, a reliable and scalable automatic model for AIGV evaluation is\ndesirable, which heavily relies on the scale and quality of human annotations.\nTo this end, we present AIGVE-60K, a comprehensive dataset and benchmark for\nAI-Generated Video Evaluation, which features (i) comprehensive tasks,\nencompassing 3,050 extensive prompts across 20 fine-grained task dimensions,\n(ii) the largest human annotations, including 120K mean-opinion scores (MOSs)\nand 60K question-answering (QA) pairs annotated on 58,500 videos generated from\n30 T2V models, and (iii) bidirectional benchmarking and evaluating for both T2V\ngeneration and V2T interpretation capabilities. Based on AIGVE-60K, we propose\nLOVE, a LMM-based metric for AIGV Evaluation from multiple dimensions including\nperceptual preference, text-video correspondence, and task-specific accuracy in\nterms of both instance level and model level. Comprehensive experiments\ndemonstrate that LOVE not only achieves state-of-the-art performance on the\nAIGVE-60K dataset, but also generalizes effectively to a wide range of other\nAIGV evaluation benchmarks. These findings highlight the significance of the\nAIGVE-60K dataset. Database and codes are anonymously available at\nhttps://github.com/IntMeGroup/LOVE."}
{"id": "2505.12182", "pdf": "https://arxiv.org/pdf/2505.12182", "abs": "https://arxiv.org/abs/2505.12182", "authors": ["Haohang Li", "Yupeng Cao", "Yangyang Yu", "Jordan W. Suchow", "Zining Zhu"], "title": "Truth Neurons", "categories": ["cs.CL"], "comment": null, "summary": "Despite their remarkable success and deployment across diverse workflows,\nlanguage models sometimes produce untruthful responses. Our limited\nunderstanding of how truthfulness is mechanistically encoded within these\nmodels jeopardizes their reliability and safety. In this paper, we propose a\nmethod for identifying representations of truthfulness at the neuron level. We\nshow that language models contain truth neurons, which encode truthfulness in a\nsubject-agnostic manner. Experiments conducted across models of varying scales\nvalidate the existence of truth neurons, confirming that the encoding of\ntruthfulness at the neuron level is a property shared by many language models.\nThe distribution patterns of truth neurons over layers align with prior\nfindings on the geometry of truthfulness. Selectively suppressing the\nactivations of truth neurons found through the TruthfulQA dataset degrades\nperformance both on TruthfulQA and on other benchmarks, showing that the\ntruthfulness mechanisms are not tied to a specific dataset. Our results offer\nnovel insights into the mechanisms underlying truthfulness in language models\nand highlight potential directions toward improving their trustworthiness and\nreliability."}
{"id": "2505.12099", "pdf": "https://arxiv.org/pdf/2505.12099", "abs": "https://arxiv.org/abs/2505.12099", "authors": ["Aybora Koksal", "A. Aydin Alatan"], "title": "TinyRS-R1: Compact Multimodal Language Model for Remote Sensing", "categories": ["cs.CV"], "comment": "Submitted to BMVC 2025. Code, models, and the captions for datasets\n  will be released", "summary": "Remote-sensing applications often run on edge hardware that cannot host\ntoday's 7B-parameter multimodal language models. This paper introduces TinyRS,\nthe first 2B-parameter multimodal small language model (MSLM) optimized for\nremote sensing tasks, and TinyRS-R1, its reasoning-augmented variant. Built\nupon Qwen2-VL-2B, TinyRS is trained through a four-stage pipeline: pre-training\non million satellite images, instruction tuning on visual instruction examples,\nfine-tuning with Chain-of-Thought (CoT) annotations from the proposed reasoning\ndataset, and alignment via Group Relative Policy Optimization (GRPO). TinyRS-R1\nachieves or surpasses the performance of recent 7B-parameter remote sensing\nmodels across classification, VQA, visual grounding, and open-ended question\nanswering-while requiring just one-third of the memory and latency. Our\nanalysis shows that CoT reasoning substantially benefits spatial grounding and\nscene understanding, while the non-reasoning TinyRS excels in concise,\nlatency-sensitive VQA tasks. TinyRS-R1 represents the first domain-specialized\nMSLM with GRPO-aligned CoT reasoning for general-purpose remote sensing."}
{"id": "2505.12183", "pdf": "https://arxiv.org/pdf/2505.12183", "abs": "https://arxiv.org/abs/2505.12183", "authors": ["Manari Hirose", "Masato Uchida"], "title": "Decoding the Mind of Large Language Models: A Quantitative Evaluation of Ideology and Biases", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": "23 pages, 5 figures, 17 tables", "summary": "The widespread integration of Large Language Models (LLMs) across various\nsectors has highlighted the need for empirical research to understand their\nbiases, thought patterns, and societal implications to ensure ethical and\neffective use. In this study, we propose a novel framework for evaluating LLMs,\nfocusing on uncovering their ideological biases through a quantitative analysis\nof 436 binary-choice questions, many of which have no definitive answer. By\napplying our framework to ChatGPT and Gemini, findings revealed that while LLMs\ngenerally maintain consistent opinions on many topics, their ideologies differ\nacross models and languages. Notably, ChatGPT exhibits a tendency to change\ntheir opinion to match the questioner's opinion. Both models also exhibited\nproblematic biases, unethical or unfair claims, which might have negative\nsocietal impacts. These results underscore the importance of addressing both\nideological and ethical considerations when evaluating LLMs. The proposed\nframework offers a flexible, quantitative method for assessing LLM behavior,\nproviding valuable insights for the development of more socially aligned AI\nsystems."}
{"id": "2505.12108", "pdf": "https://arxiv.org/pdf/2505.12108", "abs": "https://arxiv.org/abs/2505.12108", "authors": ["Jiancheng Pan", "Shiye Lei", "Yuqian Fu", "Jiahao Li", "Yanxing Liu", "Yuze Sun", "Xiao He", "Long Peng", "Xiaomeng Huang", "Bo Zhao"], "title": "EarthSynth: Generating Informative Earth Observation with Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": "23 pages", "summary": "Remote sensing image (RSI) interpretation typically faces challenges due to\nthe scarcity of labeled data, which limits the performance of RSI\ninterpretation tasks. To tackle this challenge, we propose EarthSynth, a\ndiffusion-based generative foundation model that enables synthesizing\nmulti-category, cross-satellite labeled Earth observation for downstream RSI\ninterpretation tasks. To the best of our knowledge, EarthSynth is the first to\nexplore multi-task generation for remote sensing. EarthSynth, trained on the\nEarthSynth-180K dataset, employs the Counterfactual Composition training\nstrategy to improve training data diversity and enhance category control.\nFurthermore, a rule-based method of R-Filter is proposed to filter more\ninformative synthetic data for downstream tasks. We evaluate our EarthSynth on\nscene classification, object detection, and semantic segmentation in open-world\nscenarios, offering a practical solution for advancing RSI interpretation."}
{"id": "2505.12196", "pdf": "https://arxiv.org/pdf/2505.12196", "abs": "https://arxiv.org/abs/2505.12196", "authors": ["Yi-Chien Lin", "Hongao Zhu", "William Schuler"], "title": "Vectors from Larger Language Models Predict Human Reading Time and fMRI Data More Poorly when Dimensionality Expansion is Controlled", "categories": ["cs.CL"], "comment": null, "summary": "The impressive linguistic abilities of large language models (LLMs) have\nrecommended them as models of human sentence processing, with some conjecturing\na positive 'quality-power' relationship (Wilcox et al., 2023), in which\nlanguage models' (LMs') fit to psychometric data continues to improve as their\nability to predict words in context increases. This is important because it\nsuggests that elements of LLM architecture, such as veridical attention to\ncontext and a unique objective of predicting upcoming words, reflect the\narchitecture of the human sentence processing faculty, and that any\ninadequacies in predicting human reading time and brain imaging data may be\nattributed to insufficient model complexity, which recedes as larger models\nbecome available. Recent studies (Oh and Schuler, 2023) have shown this scaling\ninverts after a point, as LMs become excessively large and accurate, when word\nprediction probability (as information-theoretic surprisal) is used as a\npredictor. Other studies propose the use of entire vectors from differently\nsized LLMs, still showing positive scaling (Schrimpf et al., 2021), casting\ndoubt on the value of surprisal as a predictor, but do not control for the\nlarger number of predictors in vectors from larger LMs. This study evaluates\nLLM scaling using entire LLM vectors, while controlling for the larger number\nof predictors in vectors from larger LLMs. Results show that inverse scaling\nobtains, suggesting that inadequacies in predicting human reading time and\nbrain imaging data may be due to substantial misalignment between LLMs and\nhuman sentence processing, which worsens as larger models are used."}
{"id": "2505.12130", "pdf": "https://arxiv.org/pdf/2505.12130", "abs": "https://arxiv.org/abs/2505.12130", "authors": ["Niaz Ahmad", "Jawad Khan", "Kang G. Shin", "Youngmoon Lee", "Guanghui Wang"], "title": "Keypoints as Dynamic Centroids for Unified Human Pose and Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The dynamic movement of the human body presents a fundamental challenge for\nhuman pose estimation and body segmentation. State-of-the-art approaches\nprimarily rely on combining keypoint heatmaps with segmentation masks but often\nstruggle in scenarios involving overlapping joints or rapidly changing poses\nduring instance-level segmentation. To address these limitations, we propose\nKeypoints as Dynamic Centroid (KDC), a new centroid-based representation for\nunified human pose estimation and instance-level segmentation. KDC adopts a\nbottom-up paradigm to generate keypoint heatmaps for both easily\ndistinguishable and complex keypoints and improves keypoint detection and\nconfidence scores by introducing KeyCentroids using a keypoint disk. It\nleverages high-confidence keypoints as dynamic centroids in the embedding space\nto generate MaskCentroids, allowing for swift clustering of pixels to specific\nhuman instances during rapid body movements in live environments. Our\nexperimental evaluations on the CrowdPose, OCHuman, and COCO benchmarks\ndemonstrate KDC's effectiveness and generalizability in challenging scenarios\nin terms of both accuracy and runtime performance. The implementation is\navailable at: https://sites.google.com/view/niazahmad/projects/kdc."}
{"id": "2505.12201", "pdf": "https://arxiv.org/pdf/2505.12201", "abs": "https://arxiv.org/abs/2505.12201", "authors": ["Xiyan Fu", "Wei Liu"], "title": "How Reliable is Multilingual LLM-as-a-Judge?", "categories": ["cs.CL"], "comment": null, "summary": "LLM-as-a-Judge has emerged as a popular evaluation strategy, where advanced\nlarge language models assess generation results in alignment with human\ninstructions. While these models serve as a promising alternative to human\nannotators, their reliability in multilingual evaluation remains uncertain. To\nbridge this gap, we conduct a comprehensive analysis of multilingual\nLLM-as-a-Judge. Specifically, we evaluate five models from different model\nfamilies across five diverse tasks involving 25 languages. Our findings reveal\nthat LLMs struggle to achieve consistent judgment results across languages,\nwith an average Fleiss' Kappa of approximately 0.3, and some models performing\neven worse. To investigate the cause of inconsistency, we analyze various\ninfluencing factors. We observe that consistency varies significantly across\nlanguages, with particularly poor performance in low-resource languages.\nAdditionally, we find that neither training on multilingual data nor increasing\nmodel scale directly improves judgment consistency. These findings suggest that\nLLMs are not yet reliable for evaluating multilingual predictions. We finally\npropose an ensemble strategy which improves the consistency of the multilingual\njudge in real-world applications."}
{"id": "2505.12154", "pdf": "https://arxiv.org/pdf/2505.12154", "abs": "https://arxiv.org/abs/2505.12154", "authors": ["Chao Huang", "Ruohan Gao", "J. M. F. Tsang", "Jan Kurcius", "Cagdas Bilen", "Chenliang Xu", "Anurag Kumar", "Sanjeel Parekh"], "title": "Learning to Highlight Audio by Watching Movies", "categories": ["cs.CV", "cs.SD", "eess.AS"], "comment": "CVPR 2025. Project page: https://wikichao.github.io/VisAH/", "summary": "Recent years have seen a significant increase in video content creation and\nconsumption. Crafting engaging content requires the careful curation of both\nvisual and audio elements. While visual cue curation, through techniques like\noptimal viewpoint selection or post-editing, has been central to media\nproduction, its natural counterpart, audio, has not undergone equivalent\nadvancements. This often results in a disconnect between visual and acoustic\nsaliency. To bridge this gap, we introduce a novel task: visually-guided\nacoustic highlighting, which aims to transform audio to deliver appropriate\nhighlighting effects guided by the accompanying video, ultimately creating a\nmore harmonious audio-visual experience. We propose a flexible,\ntransformer-based multimodal framework to solve this task. To train our model,\nwe also introduce a new dataset -- the muddy mix dataset, leveraging the\nmeticulous audio and video crafting found in movies, which provides a form of\nfree supervision. We develop a pseudo-data generation process to simulate\npoorly mixed audio, mimicking real-world scenarios through a three-step process\n-- separation, adjustment, and remixing. Our approach consistently outperforms\nseveral baselines in both quantitative and subjective evaluation. We also\nsystematically study the impact of different types of contextual guidance and\ndifficulty levels of the dataset. Our project page is here:\nhttps://wikichao.github.io/VisAH/."}
{"id": "2505.12212", "pdf": "https://arxiv.org/pdf/2505.12212", "abs": "https://arxiv.org/abs/2505.12212", "authors": ["Shaobo Wang", "Ziming Wang", "Xiangqi Jin", "Jize Wang", "Jiajun Zhang", "Kaixin Li", "Zichen Wen", "Zhong Li", "Conghui He", "Xuming Hu", "Linfeng Zhang"], "title": "Data Whisperer: Efficient Data Selection for Task-Specific LLM Fine-Tuning via Few-Shot In-Context Learning", "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 main, 18 pages, 8 figures, 6 tables", "summary": "Fine-tuning large language models (LLMs) on task-specific data is essential\nfor their effective deployment. As dataset sizes grow, efficiently selecting\noptimal subsets for training becomes crucial to balancing performance and\ncomputational costs. Traditional data selection methods often require\nfine-tuning a scoring model on the target dataset, which is time-consuming and\nresource-intensive, or rely on heuristics that fail to fully leverage the\nmodel's predictive capabilities. To address these challenges, we propose Data\nWhisperer, an efficient, training-free, attention-based method that leverages\nfew-shot in-context learning with the model to be fine-tuned. Comprehensive\nevaluations were conducted on both raw and synthetic datasets across diverse\ntasks and models. Notably, Data Whisperer achieves superior performance\ncompared to the full GSM8K dataset on the Llama-3-8B-Instruct model, using just\n10% of the data, and outperforms existing methods with a 3.1-point improvement\nand a 7.4$\\times$ speedup."}
{"id": "2505.12155", "pdf": "https://arxiv.org/pdf/2505.12155", "abs": "https://arxiv.org/abs/2505.12155", "authors": ["Ranit Karmakar", "Simon F. Nørrelykke"], "title": "SoftPQ: Robust Instance Segmentation Evaluation via Soft Matching and Tunable Thresholds", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Segmentation evaluation metrics traditionally rely on binary decision logic:\npredictions are either correct or incorrect, based on rigid IoU thresholds.\nDetection--based metrics such as F1 and mAP determine correctness at the object\nlevel using fixed overlap cutoffs, while overlap--based metrics like\nIntersection over Union (IoU) and Dice operate at the pixel level, often\noverlooking instance--level structure. Panoptic Quality (PQ) attempts to unify\ndetection and segmentation assessment, but it remains dependent on\nhard-threshold matching--treating predictions below the threshold as entirely\nincorrect. This binary framing obscures important distinctions between\nqualitatively different errors and fails to reward gradual model improvements.\nWe propose SoftPQ, a flexible and interpretable instance segmentation metric\nthat redefines evaluation as a graded continuum rather than a binary\nclassification. SoftPQ introduces tunable upper and lower IoU thresholds to\ndefine a partial matching region and applies a sublinear penalty function to\nambiguous or fragmented predictions. These extensions allow SoftPQ to exhibit\nsmoother score behavior, greater robustness to structural segmentation errors,\nand more informative feedback for model development and evaluation. Through\ncontrolled perturbation experiments, we show that SoftPQ captures meaningful\ndifferences in segmentation quality that existing metrics overlook, making it a\npractical and principled alternative for both benchmarking and iterative model\nrefinement."}
{"id": "2505.12215", "pdf": "https://arxiv.org/pdf/2505.12215", "abs": "https://arxiv.org/abs/2505.12215", "authors": ["Jiwei Tang", "Zhicheng Zhang", "Shunlong Wu", "Jingheng Ye", "Lichen Bai", "Zitai Wang", "Tingwei Lu", "Jiaqi Chen", "Lin Hai", "Hai-Tao Zheng", "Hong-Gee Kim"], "title": "GMSA: Enhancing Context Compression via Group Merging and Layer Semantic Alignment", "categories": ["cs.CL"], "comment": "19 pages, 7 figures", "summary": "Large language models (LLMs) have achieved impressive performance in a\nvariety of natural language processing (NLP) tasks. However, when applied to\nlong-context scenarios, they face two challenges, i.e., low computational\nefficiency and much redundant information. This paper introduces GMSA, a\ncontext compression framework based on the encoder-decoder architecture, which\naddresses these challenges by reducing input sequence length and redundant\ninformation. Structurally, GMSA has two key components: Group Merging and Layer\nSemantic Alignment (LSA). Group merging is used to effectively and efficiently\nextract summary vectors from the original context. Layer semantic alignment, on\nthe other hand, aligns the high-level summary vectors with the low-level\nprimary input semantics, thus bridging the semantic gap between different\nlayers. In the training process, GMSA first learns soft tokens that contain\ncomplete semantics through autoencoder training. To furtherly adapt GMSA to\ndownstream tasks, we propose Knowledge Extraction Fine-tuning (KEFT) to extract\nknowledge from the soft tokens for downstream tasks. We train GMSA by randomly\nsampling the compression rate for each sample in the dataset. Under this\ncondition, GMSA not only significantly outperforms the traditional compression\nparadigm in context restoration but also achieves stable and significantly\nfaster convergence with only a few encoder layers. In downstream\nquestion-answering (QA) tasks, GMSA can achieve approximately a 2x speedup in\nend-to-end inference while outperforming both the original input prompts and\nvarious state-of-the-art (SOTA) methods by a large margin."}
{"id": "2505.12191", "pdf": "https://arxiv.org/pdf/2505.12191", "abs": "https://arxiv.org/abs/2505.12191", "authors": ["Wenquan Lu", "Jiaqi Zhang", "Hugues Van Assel", "Randall Balestriero"], "title": "Ditch the Denoiser: Emergence of Noise Robustness in Self-Supervised Learning from Data Curriculum", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Self-Supervised Learning (SSL) has become a powerful solution to extract rich\nrepresentations from unlabeled data. Yet, SSL research is mostly focused on\nclean, curated and high-quality datasets. As a result, applying SSL on noisy\ndata remains a challenge, despite being crucial to applications such as\nastrophysics, medical imaging, geophysics or finance. In this work, we present\na fully self-supervised framework that enables noise-robust representation\nlearning without requiring a denoiser at inference or downstream fine-tuning.\nOur method first trains an SSL denoiser on noisy data, then uses it to\nconstruct a denoised-to-noisy data curriculum (i.e., training first on\ndenoised, then noisy samples) for pretraining a SSL backbone (e.g., DINOv2),\ncombined with a teacher-guided regularization that anchors noisy embeddings to\ntheir denoised counterparts. This process encourages the model to internalize\nnoise robustness. Notably, the denoiser can be discarded after pretraining,\nsimplifying deployment. On ImageNet-1k with ViT-B under extreme Gaussian noise\n($\\sigma=255$, SNR = 0.72 dB), our method improves linear probing accuracy by\n4.8% over DINOv2, demonstrating that denoiser-free robustness can emerge from\nnoise-aware pretraining. The code is available at\nhttps://github.com/wenquanlu/noisy_dinov2."}
{"id": "2505.12216", "pdf": "https://arxiv.org/pdf/2505.12216", "abs": "https://arxiv.org/abs/2505.12216", "authors": ["Rongguang Ye", "Ming Tang"], "title": "One-for-All Pruning: A Universal Model for Customized Compression of Large Language Models", "categories": ["cs.CL"], "comment": "ACL Findings", "summary": "Existing pruning methods for large language models (LLMs) focus on achieving\nhigh compression rates while maintaining model performance. Although these\nmethods have demonstrated satisfactory performance in handling a single user's\ncompression request, their processing time increases linearly with the number\nof requests, making them inefficient for real-world scenarios with multiple\nsimultaneous requests. To address this limitation, we propose a Univeral Model\nfor Customized Compression (UniCuCo) for LLMs, which introduces a StratNet that\nlearns to map arbitrary requests to their optimal pruning strategy. The\nchallenge in training StratNet lies in the high computational cost of\nevaluating pruning strategies and the non-differentiable nature of the pruning\nprocess, which hinders gradient backpropagation for StratNet updates. To\novercome these challenges, we leverage a Gaussian process to approximate the\nevaluation process. Since the gradient of the Gaussian process is computable,\nwe can use it to approximate the gradient of the non-differentiable pruning\nprocess, thereby enabling StratNet updates. Experimental results show that\nUniCuCo is 28 times faster than baselines in processing 64 requests, while\nmaintaining comparable accuracy to baselines."}
{"id": "2505.12199", "pdf": "https://arxiv.org/pdf/2505.12199", "abs": "https://arxiv.org/abs/2505.12199", "authors": ["Kui Jiang", "Jing Cao", "Zhaocheng Yu", "Junjun Jiang", "Jingchun Zhou"], "title": "Always Clear Depth: Robust Monocular Depth Estimation under Adverse Weather", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Monocular depth estimation is critical for applications such as autonomous\ndriving and scene reconstruction. While existing methods perform well under\nnormal scenarios, their performance declines in adverse weather, due to\nchallenging domain shifts and difficulties in extracting scene information. To\naddress this issue, we present a robust monocular depth estimation method\ncalled \\textbf{ACDepth} from the perspective of high-quality training data\ngeneration and domain adaptation. Specifically, we introduce a one-step\ndiffusion model for generating samples that simulate adverse weather\nconditions, constructing a multi-tuple degradation dataset during training. To\nensure the quality of the generated degradation samples, we employ LoRA\nadapters to fine-tune the generation weights of diffusion model. Additionally,\nwe integrate circular consistency loss and adversarial training to guarantee\nthe fidelity and naturalness of the scene contents. Furthermore, we elaborate\non a multi-granularity knowledge distillation strategy (MKD) that encourages\nthe student network to absorb knowledge from both the teacher model and\npretrained Depth Anything V2. This strategy guides the student model in\nlearning degradation-agnostic scene information from various degradation\ninputs. In particular, we introduce an ordinal guidance distillation mechanism\n(OGD) that encourages the network to focus on uncertain regions through\ndifferential ranking, leading to a more precise depth estimation. Experimental\nresults demonstrate that our ACDepth surpasses md4all-DD by 2.50\\% for night\nscene and 2.61\\% for rainy scene on the nuScenes dataset in terms of the absRel\nmetric."}
{"id": "2505.12218", "pdf": "https://arxiv.org/pdf/2505.12218", "abs": "https://arxiv.org/abs/2505.12218", "authors": ["Tong Bao", "Yi Zhao", "Jin Mao", "Chengzhi Zhang"], "title": "Examining Linguistic Shifts in Academic Writing Before and After the Launch of ChatGPT: A Study on Preprint Papers", "categories": ["cs.CL", "68T50", "I.2.7"], "comment": null, "summary": "Large Language Models (LLMs), such as ChatGPT, have prompted academic\nconcerns about their impact on academic writing. Existing studies have\nprimarily examined LLM usage in academic writing through quantitative\napproaches, such as word frequency statistics and probability-based analyses.\nHowever, few have systematically examined the potential impact of LLMs on the\nlinguistic characteristics of academic writing. To address this gap, we\nconducted a large-scale analysis across 823,798 abstracts published in last\ndecade from arXiv dataset. Through the linguistic analysis of features such as\nthe frequency of LLM-preferred words, lexical complexity, syntactic complexity,\ncohesion, readability and sentiment, the results indicate a significant\nincrease in the proportion of LLM-preferred words in abstracts, revealing the\nwidespread influence of LLMs on academic writing. Additionally, we observed an\nincrease in lexical complexity and sentiment in the abstracts, but a decrease\nin syntactic complexity, suggesting that LLMs introduce more new vocabulary and\nsimplify sentence structure. However, the significant decrease in cohesion and\nreadability indicates that abstracts have fewer connecting words and are\nbecoming more difficult to read. Moreover, our analysis reveals that scholars\nwith weaker English proficiency were more likely to use the LLMs for academic\nwriting, and focused on improving the overall logic and fluency of the\nabstracts. Finally, at discipline level, we found that scholars in Computer\nScience showed more pronounced changes in writing style, while the changes in\nMathematics were minimal."}
{"id": "2505.12200", "pdf": "https://arxiv.org/pdf/2505.12200", "abs": "https://arxiv.org/abs/2505.12200", "authors": ["Bohan Jia", "Wenxuan Huang", "Yuntian Tang", "Junbo Qiao", "Jincheng Liao", "Shaosheng Cao", "Fei Zhao", "Zhaopeng Feng", "Zhouhong Gu", "Zhenfei Yin", "Lei Bai", "Wanli Ouyang", "Lin Chen", "Fei Zhao", "Zihan Wang", "Yuan Xie", "Shaohui Lin"], "title": "CompBench: Benchmarking Complex Instruction-guided Image Editing", "categories": ["cs.CV"], "comment": null, "summary": "While real-world applications increasingly demand intricate scene\nmanipulation, existing instruction-guided image editing benchmarks often\noversimplify task complexity and lack comprehensive, fine-grained instructions.\nTo bridge this gap, we introduce, a large-scale benchmark specifically designed\nfor complex instruction-guided image editing. CompBench features challenging\nediting scenarios that incorporate fine-grained instruction following, spatial\nand contextual reasoning, thereby enabling comprehensive evaluation of image\nediting models' precise manipulation capabilities. To construct CompBench, We\npropose an MLLM-human collaborative framework with tailored task pipelines.\nFurthermore, we propose an instruction decoupling strategy that disentangles\nediting intents into four key dimensions: location, appearance, dynamics, and\nobjects, ensuring closer alignment between instructions and complex editing\nrequirements. Extensive evaluations reveal that CompBench exposes fundamental\nlimitations of current image editing models and provides critical insights for\nthe development of next-generation instruction-guided image editing systems."}
{"id": "2505.12236", "pdf": "https://arxiv.org/pdf/2505.12236", "abs": "https://arxiv.org/abs/2505.12236", "authors": ["Quanjiang Guo", "Jinchuan Zhang", "Sijie Wang", "Ling Tian", "Zhao Kang", "Bin Yan", "Weidong Xiao"], "title": "Bridging Generative and Discriminative Learning: Few-Shot Relation Extraction via Two-Stage Knowledge-Guided Pre-training", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "13 pages, 6 figures, Appear on IJCAI 2025", "summary": "Few-Shot Relation Extraction (FSRE) remains a challenging task due to the\nscarcity of annotated data and the limited generalization capabilities of\nexisting models. Although large language models (LLMs) have demonstrated\npotential in FSRE through in-context learning (ICL), their general-purpose\ntraining objectives often result in suboptimal performance for task-specific\nrelation extraction. To overcome these challenges, we propose TKRE (Two-Stage\nKnowledge-Guided Pre-training for Relation Extraction), a novel framework that\nsynergistically integrates LLMs with traditional relation extraction models,\nbridging generative and discriminative learning paradigms. TKRE introduces two\nkey innovations: (1) leveraging LLMs to generate explanation-driven knowledge\nand schema-constrained synthetic data, addressing the issue of data scarcity;\nand (2) a two-stage pre-training strategy combining Masked Span Language\nModeling (MSLM) and Span-Level Contrastive Learning (SCL) to enhance relational\nreasoning and generalization. Together, these components enable TKRE to\neffectively tackle FSRE tasks. Comprehensive experiments on benchmark datasets\ndemonstrate the efficacy of TKRE, achieving new state-of-the-art performance in\nFSRE and underscoring its potential for broader application in low-resource\nscenarios. \\footnote{The code and data are released on\nhttps://github.com/UESTC-GQJ/TKRE."}
{"id": "2505.12206", "pdf": "https://arxiv.org/pdf/2505.12206", "abs": "https://arxiv.org/abs/2505.12206", "authors": ["Mathanesh Vellingiri Ramasamy", "Dimas Rizky Kurniasalim"], "title": "Road Segmentation for ADAS/AD Applications", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Accurate road segmentation is essential for autonomous driving and ADAS,\nenabling effective navigation in complex environments. This study examines how\nmodel architecture and dataset choice affect segmentation by training a\nmodified VGG-16 on the Comma10k dataset and a modified U-Net on the KITTI Road\ndataset. Both models achieved high accuracy, with cross-dataset testing showing\nVGG-16 outperforming U-Net despite U-Net being trained for more epochs. We\nanalyze model performance using metrics such as F1-score, mean intersection\nover union, and precision, discussing how architecture and dataset impact\nresults."}
{"id": "2505.12238", "pdf": "https://arxiv.org/pdf/2505.12238", "abs": "https://arxiv.org/abs/2505.12238", "authors": ["Sriram Selvam", "Anneswa Ghosh"], "title": "PANORAMA: A synthetic PII-laced dataset for studying sensitive data memorization in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The memorization of sensitive and personally identifiable information (PII)\nby large language models (LLMs) poses growing privacy risks as models scale and\nare increasingly deployed in real-world applications. Existing efforts to study\nsensitive and PII data memorization and develop mitigation strategies are\nhampered by the absence of comprehensive, realistic, and ethically sourced\ndatasets reflecting the diversity of sensitive information found on the web. We\nintroduce PANORAMA - Profile-based Assemblage for Naturalistic Online\nRepresentation and Attribute Memorization Analysis, a large-scale synthetic\ncorpus of 384,789 samples derived from 9,674 synthetic profiles designed to\nclosely emulate the distribution, variety, and context of PII and sensitive\ndata as it naturally occurs in online environments. Our data generation\npipeline begins with the construction of internally consistent, multi-attribute\nhuman profiles using constrained selection to reflect real-world demographics\nsuch as education, health attributes, financial status, etc. Using a\ncombination of zero-shot prompting and OpenAI o3-mini, we generate diverse\ncontent types - including wiki-style articles, social media posts, forum\ndiscussions, online reviews, comments, and marketplace listings - each\nembedding realistic, contextually appropriate PII and other sensitive\ninformation. We validate the utility of PANORAMA by fine-tuning the Mistral-7B\nmodel on 1x, 5x, 10x, and 25x data replication rates with a subset of data and\nmeasure PII memorization rates - revealing not only consistent increases with\nrepetition but also variation across content types, highlighting PANORAMA's\nability to model how memorization risks differ by context. Our dataset and code\nare publicly available, providing a much-needed resource for privacy risk\nassessment, model auditing, and the development of privacy-preserving LLMs."}
{"id": "2505.12207", "pdf": "https://arxiv.org/pdf/2505.12207", "abs": "https://arxiv.org/abs/2505.12207", "authors": ["Qingmei Li", "Yang Zhang", "Zurong Mai", "Yuhang Chen", "Shuohong Lou", "Henglian Huang", "Jiarui Zhang", "Zhiwei Zhang", "Yibin Wen", "Weijia Li", "Haohuan Fu", "Jianxi Huang", "Juepeng Zheng"], "title": "Can Large Multimodal Models Understand Agricultural Scenes? Benchmarking with AgroMind", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Multimodal Models (LMMs) has demonstrated capabilities across various\ndomains, but comprehensive benchmarks for agricultural remote sensing (RS)\nremain scarce. Existing benchmarks designed for agricultural RS scenarios\nexhibit notable limitations, primarily in terms of insufficient scene diversity\nin the dataset and oversimplified task design. To bridge this gap, we introduce\nAgroMind, a comprehensive agricultural remote sensing benchmark covering four\ntask dimensions: spatial perception, object understanding, scene understanding,\nand scene reasoning, with a total of 13 task types, ranging from crop\nidentification and health monitoring to environmental analysis. We curate a\nhigh-quality evaluation set by integrating eight public datasets and one\nprivate farmland plot dataset, containing 25,026 QA pairs and 15,556 images.\nThe pipeline begins with multi-source data preprocessing, including collection,\nformat standardization, and annotation refinement. We then generate a diverse\nset of agriculturally relevant questions through the systematic definition of\ntasks. Finally, we employ LMMs for inference, generating responses, and\nperforming detailed examinations. We evaluated 18 open-source LMMs and 3\nclosed-source models on AgroMind. Experiments reveal significant performance\ngaps, particularly in spatial reasoning and fine-grained recognition, it is\nnotable that human performance lags behind several leading LMMs. By\nestablishing a standardized evaluation framework for agricultural RS, AgroMind\nreveals the limitations of LMMs in domain knowledge and highlights critical\nchallenges for future work. Data and code can be accessed at\nhttps://rssysu.github.io/AgroMind/."}
{"id": "2505.12244", "pdf": "https://arxiv.org/pdf/2505.12244", "abs": "https://arxiv.org/abs/2505.12244", "authors": ["Haojin Wang", "Zining Zhu", "Freda Shi"], "title": "Distribution Prompting: Understanding the Expressivity of Language Models Through the Next-Token Distributions They Can Produce", "categories": ["cs.CL"], "comment": null, "summary": "Autoregressive neural language models (LMs) generate a probability\ndistribution over tokens at each time step given a prompt. In this work, we\nattempt to systematically understand the probability distributions that LMs can\nproduce, showing that some distributions are significantly harder to elicit\nthan others. Specifically, for any target next-token distribution over the\nvocabulary, we attempt to find a prompt that induces the LM to output a\ndistribution as close as possible to the target, using either soft or hard\ngradient-based prompt tuning. We find that (1) in general, distributions with\nvery low or very high entropy are easier to approximate than those with\nmoderate entropy; (2) among distributions with the same entropy, those\ncontaining ''outlier tokens'' are easier to approximate; (3) target\ndistributions generated by LMs -- even LMs with different tokenizers -- are\neasier to approximate than randomly chosen targets. These results offer\ninsights into the expressiveness of LMs and the challenges of using them as\nprobability distribution proposers."}
{"id": "2505.12217", "pdf": "https://arxiv.org/pdf/2505.12217", "abs": "https://arxiv.org/abs/2505.12217", "authors": ["Aryan Das", "Tanishq Rachamalla", "Pravendra Singh", "Koushik Biswas", "Vinay Kumar Verma", "Swalpa Kumar Roy"], "title": "Hyperspectral Image Land Cover Captioning Dataset for Vision Language Models", "categories": ["cs.CV"], "comment": null, "summary": "We introduce HyperCap, the first large-scale hyperspectral captioning dataset\ndesigned to enhance model performance and effectiveness in remote sensing\napplications. Unlike traditional hyperspectral imaging (HSI) datasets that\nfocus solely on classification tasks, HyperCap integrates spectral data with\npixel-wise textual annotations, enabling deeper semantic understanding of\nhyperspectral imagery. This dataset enhances model performance in tasks like\nclassification and feature extraction, providing a valuable resource for\nadvanced remote sensing applications. HyperCap is constructed from four\nbenchmark datasets and annotated through a hybrid approach combining automated\nand manual methods to ensure accuracy and consistency. Empirical evaluations\nusing state-of-the-art encoders and diverse fusion techniques demonstrate\nsignificant improvements in classification performance. These results\nunderscore the potential of vision-language learning in HSI and position\nHyperCap as a foundational dataset for future research in the field."}
{"id": "2505.12250", "pdf": "https://arxiv.org/pdf/2505.12250", "abs": "https://arxiv.org/abs/2505.12250", "authors": ["Chi Zhang", "Huaping Zhong", "Hongtao Li", "Chengliang Chai", "Jiawei Hong", "Yuhao Deng", "Jiacheng Wang", "Tian Tan", "Yizhou Yan", "Jiantao Qiu", "Ye Yuan", "Guoren Wang", "Conghui He", "Lei Cao"], "title": "Not All Documents Are What You Need for Extracting Instruction Tuning Data", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Instruction tuning improves the performance of large language models (LLMs),\nbut it heavily relies on high-quality training data. Recently, LLMs have been\nused to synthesize instruction data using seed question-answer (QA) pairs.\nHowever, these synthesized instructions often lack diversity and tend to be\nsimilar to the input seeds, limiting their applicability in real-world\nscenarios. To address this, we propose extracting instruction tuning data from\nweb corpora that contain rich and diverse knowledge. A naive solution is to\nretrieve domain-specific documents and extract all QA pairs from them, but this\nfaces two key challenges: (1) extracting all QA pairs using LLMs is\nprohibitively expensive, and (2) many extracted QA pairs may be irrelevant to\nthe downstream tasks, potentially degrading model performance. To tackle these\nissues, we introduce EQUAL, an effective and scalable data extraction framework\nthat iteratively alternates between document selection and high-quality QA pair\nextraction to enhance instruction tuning. EQUAL first clusters the document\ncorpus based on embeddings derived from contrastive learning, then uses a\nmulti-armed bandit strategy to efficiently identify clusters that are likely to\ncontain valuable QA pairs. This iterative approach significantly reduces\ncomputational cost while boosting model performance. Experiments on\nAutoMathText and StackOverflow across four downstream tasks show that EQUAL\nreduces computational costs by 5-10x and improves accuracy by 2.5 percent on\nLLaMA-3.1-8B and Mistral-7B"}
{"id": "2505.12228", "pdf": "https://arxiv.org/pdf/2505.12228", "abs": "https://arxiv.org/abs/2505.12228", "authors": ["Karthik Gopinath", "Annabel Sorby-Adams", "Jonathan W. Ramirez", "Dina Zemlyanker", "Jennifer Guo", "David Hunt", "Christine L. Mac Donald", "C. Dirk Keene", "Timothy Coalson", "Matthew F. Glasser", "David Van Essen", "Matthew S. Rosen", "Oula Puonti", "W. Taylor Kimberly", "Juan Eugenio Iglesias"], "title": "From Low Field to High Value: Robust Cortical Mapping from Low-Field MRI", "categories": ["cs.CV", "cs.LG"], "comment": "32 pages", "summary": "Three-dimensional reconstruction of cortical surfaces from MRI for\nmorphometric analysis is fundamental for understanding brain structure. While\nhigh-field MRI (HF-MRI) is standard in research and clinical settings, its\nlimited availability hinders widespread use. Low-field MRI (LF-MRI),\nparticularly portable systems, offers a cost-effective and accessible\nalternative. However, existing cortical surface analysis tools are optimized\nfor high-resolution HF-MRI and struggle with the lower signal-to-noise ratio\nand resolution of LF-MRI. In this work, we present a machine learning method\nfor 3D reconstruction and analysis of portable LF-MRI across a range of\ncontrasts and resolutions. Our method works \"out of the box\" without\nretraining. It uses a 3D U-Net trained on synthetic LF-MRI to predict signed\ndistance functions of cortical surfaces, followed by geometric processing to\nensure topological accuracy. We evaluate our method using paired HF/LF-MRI\nscans of the same subjects, showing that LF-MRI surface reconstruction accuracy\ndepends on acquisition parameters, including contrast type (T1 vs T2),\norientation (axial vs isotropic), and resolution. A 3mm isotropic T2-weighted\nscan acquired in under 4 minutes, yields strong agreement with HF-derived\nsurfaces: surface area correlates at r=0.96, cortical parcellations reach\nDice=0.98, and gray matter volume achieves r=0.93. Cortical thickness remains\nmore challenging with correlations up to r=0.70, reflecting the difficulty of\nsub-mm precision with 3mm voxels. We further validate our method on challenging\npostmortem LF-MRI, demonstrating its robustness. Our method represents a step\ntoward enabling cortical surface analysis on portable LF-MRI. Code is available\nat https://surfer.nmr.mgh.harvard.edu/fswiki/ReconAny"}
{"id": "2505.12259", "pdf": "https://arxiv.org/pdf/2505.12259", "abs": "https://arxiv.org/abs/2505.12259", "authors": ["Yuhang Zhou", "Xutian Chen", "Yixin Cao", "Yuchen Ni", "Yu He", "Siyu Tian", "Xiang Liu", "Jian Zhang", "Chuanjun Ji", "Guangnan Ye", "Xipeng Qiu"], "title": "Teach2Eval: An Indirect Evaluation Method for LLM by Judging How It Teaches", "categories": ["cs.CL"], "comment": null, "summary": "Recent progress in large language models (LLMs) has outpaced the development\nof effective evaluation methods. Traditional benchmarks rely on task-specific\nmetrics and static datasets, which often suffer from fairness issues, limited\nscalability, and contamination risks. In this paper, we introduce Teach2Eval,\nan indirect evaluation framework inspired by the Feynman Technique. Instead of\ndirectly testing LLMs on predefined tasks, our method evaluates a model's\nmultiple abilities to teach weaker student models to perform tasks effectively.\nBy converting open-ended tasks into standardized multiple-choice questions\n(MCQs) through teacher-generated feedback, Teach2Eval enables scalable,\nautomated, and multi-dimensional assessment. Our approach not only avoids data\nleakage and memorization but also captures a broad range of cognitive abilities\nthat are orthogonal to current benchmarks. Experimental results across 26\nleading LLMs show strong alignment with existing human and model-based dynamic\nrankings, while offering additional interpretability for training guidance."}
{"id": "2505.12235", "pdf": "https://arxiv.org/pdf/2505.12235", "abs": "https://arxiv.org/abs/2505.12235", "authors": ["Jia Li", "Nan Gao", "Huaibo Huang", "Ran He"], "title": "NOFT: Test-Time Noise Finetune via Information Bottleneck for Highly Correlated Asset Creation", "categories": ["cs.CV"], "comment": null, "summary": "The diffusion model has provided a strong tool for implementing text-to-image\n(T2I) and image-to-image (I2I) generation. Recently, topology and texture\ncontrol are popular explorations, e.g., ControlNet, IP-Adapter, Ctrl-X, and\nDSG. These methods explicitly consider high-fidelity controllable editing based\non external signals or diffusion feature manipulations. As for diversity, they\ndirectly choose different noise latents. However, the diffused noise is capable\nof implicitly representing the topological and textural manifold of the\ncorresponding image. Moreover, it's an effective workbench to conduct the\ntrade-off between content preservation and controllable variations. Previous\nT2I and I2I diffusion works do not explore the information within the\ncompressed contextual latent. In this paper, we first propose a plug-and-play\nnoise finetune NOFT module employed by Stable Diffusion to generate highly\ncorrelated and diverse images. We fine-tune seed noise or inverse noise through\nan optimal-transported (OT) information bottleneck (IB) with around only 14K\ntrainable parameters and 10 minutes of training. Our test-time NOFT is good at\nproducing high-fidelity image variations considering topology and texture\nalignments. Comprehensive experiments demonstrate that NOFT is a powerful\ngeneral reimagine approach to efficiently fine-tune the 2D/3D AIGC assets with\ntext or image guidance."}
{"id": "2505.12265", "pdf": "https://arxiv.org/pdf/2505.12265", "abs": "https://arxiv.org/abs/2505.12265", "authors": ["Chengwei Qin", "Wenxuan Zhou", "Karthik Abinav Sankararaman", "Nanshu Wang", "Tengyu Xu", "Alexander Radovic", "Eryk Helenowski", "Arya Talebzadeh", "Aditya Tayade", "Sinong Wang", "Shafiq Joty", "Han Fang", "Hao Ma"], "title": "Learning Auxiliary Tasks Improves Reference-Free Hallucination Detection in Open-Domain Long-Form Generation", "categories": ["cs.CL"], "comment": null, "summary": "Hallucination, the generation of factually incorrect information, remains a\nsignificant challenge for large language models (LLMs), especially in\nopen-domain long-form generation. Existing approaches for detecting\nhallucination in long-form tasks either focus on limited domains or rely\nheavily on external fact-checking tools, which may not always be available.\n  In this work, we systematically investigate reference-free hallucination\ndetection in open-domain long-form responses. Our findings reveal that internal\nstates (e.g., model's output probability and entropy) alone are insufficient\nfor reliably (i.e., better than random guessing) distinguishing between factual\nand hallucinated content. To enhance detection, we explore various existing\napproaches, including prompting-based methods, probing, and fine-tuning, with\nfine-tuning proving the most effective. To further improve the accuracy, we\nintroduce a new paradigm, named RATE-FT, that augments fine-tuning with an\nauxiliary task for the model to jointly learn with the main task of\nhallucination detection. With extensive experiments and analysis using a\nvariety of model families & datasets, we demonstrate the effectiveness and\ngeneralizability of our method, e.g., +3% over general fine-tuning methods on\nLongFact."}
{"id": "2505.12237", "pdf": "https://arxiv.org/pdf/2505.12237", "abs": "https://arxiv.org/abs/2505.12237", "authors": ["Yuzhi Li", "Haojun Xu", "Feng Tian"], "title": "From Shots to Stories: LLM-Assisted Video Editing with Unified Language Representations", "categories": ["cs.CV"], "comment": null, "summary": "Large Language Models (LLMs) and Vision-Language Models (VLMs) have\ndemonstrated remarkable reasoning and generalization capabilities in video\nunderstanding; however, their application in video editing remains largely\nunderexplored. This paper presents the first systematic study of LLMs in the\ncontext of video editing. To bridge the gap between visual information and\nlanguage-based reasoning, we introduce L-Storyboard, an intermediate\nrepresentation that transforms discrete video shots into structured language\ndescriptions suitable for LLM processing. We categorize video editing tasks\ninto Convergent Tasks and Divergent Tasks, focusing on three core tasks: Shot\nAttributes Classification, Next Shot Selection, and Shot Sequence Ordering. To\naddress the inherent instability of divergent task outputs, we propose the\nStoryFlow strategy, which converts the divergent multi-path reasoning process\ninto a convergent selection mechanism, effectively enhancing task accuracy and\nlogical coherence. Experimental results demonstrate that L-Storyboard\nfacilitates a more robust mapping between visual information and language\ndescriptions, significantly improving the interpretability and privacy\nprotection of video editing tasks. Furthermore, StoryFlow enhances the logical\nconsistency and output stability in Shot Sequence Ordering, underscoring the\nsubstantial potential of LLMs in intelligent video editing."}
{"id": "2505.12268", "pdf": "https://arxiv.org/pdf/2505.12268", "abs": "https://arxiv.org/abs/2505.12268", "authors": ["Pratim Chowdhary"], "title": "$K$-MSHC: Unmasking Minimally Sufficient Head Circuits in Large Language Models with Experiments on Syntactic Classification Tasks", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Understanding which neural components drive specific capabilities in\nmid-sized language models ($\\leq$10B parameters) remains a key challenge. We\nintroduce the $(\\bm{K}, \\epsilon)$-Minimum Sufficient Head Circuit ($K$-MSHC),\na methodology to identify minimal sets of attention heads crucial for\nclassification tasks as well as Search-K-MSHC, an efficient algorithm for\ndiscovering these circuits. Applying our Search-K-MSHC algorithm to Gemma-9B,\nwe analyze three syntactic task families: grammar acceptability, arithmetic\nverification, and arithmetic word problems. Our findings reveal distinct\ntask-specific head circuits, with grammar tasks predominantly utilizing early\nlayers, word problems showing pronounced activity in both shallow and deep\nregions, and arithmetic verification demonstrating a more distributed pattern\nacross the network. We discover non-linear circuit overlap patterns, where\ndifferent task pairs share computational components at varying levels of\nimportance. While grammar and arithmetic share many \"weak\" heads, arithmetic\nand word problems share more consistently critical \"strong\" heads. Importantly,\nwe find that each task maintains dedicated \"super-heads\" with minimal\ncross-task overlap, suggesting that syntactic and numerical competencies emerge\nfrom specialized yet partially reusable head circuits."}
{"id": "2505.12246", "pdf": "https://arxiv.org/pdf/2505.12246", "abs": "https://arxiv.org/abs/2505.12246", "authors": ["Muleilan Pei", "Jiayao Shan", "Peiliang Li", "Jieqi Shi", "Jing Huo", "Yang Gao", "Shaojie Shen"], "title": "SEPT: Standard-Definition Map Enhanced Scene Perception and Topology Reasoning for Autonomous Driving", "categories": ["cs.CV"], "comment": "Accepted by IEEE Robotics and Automation Letters", "summary": "Online scene perception and topology reasoning are critical for autonomous\nvehicles to understand their driving environments, particularly for mapless\ndriving systems that endeavor to reduce reliance on costly High-Definition (HD)\nmaps. However, recent advances in online scene understanding still face\nlimitations, especially in long-range or occluded scenarios, due to the\ninherent constraints of onboard sensors. To address this challenge, we propose\na Standard-Definition (SD) Map Enhanced scene Perception and Topology reasoning\n(SEPT) framework, which explores how to effectively incorporate the SD map as\nprior knowledge into existing perception and reasoning pipelines. Specifically,\nwe introduce a novel hybrid feature fusion strategy that combines SD maps with\nBird's-Eye-View (BEV) features, considering both rasterized and vectorized\nrepresentations, while mitigating potential misalignment between SD maps and\nBEV feature spaces. Additionally, we leverage the SD map characteristics to\ndesign an auxiliary intersection-aware keypoint detection task, which further\nenhances the overall scene understanding performance. Experimental results on\nthe large-scale OpenLane-V2 dataset demonstrate that by effectively integrating\nSD map priors, our framework significantly improves both scene perception and\ntopology reasoning, outperforming existing methods by a substantial margin."}
{"id": "2505.12273", "pdf": "https://arxiv.org/pdf/2505.12273", "abs": "https://arxiv.org/abs/2505.12273", "authors": ["Md. Atiqur Rahman", "Sabrina Islam", "Mushfiqul Haque Omi"], "title": "LLM-Based Evaluation of Low-Resource Machine Translation: A Reference-less Dialect Guided Approach with a Refined Sylheti-English Benchmark", "categories": ["cs.CL"], "comment": null, "summary": "Evaluating machine translation (MT) for low-resource languages poses a\npersistent challenge, primarily due to the limited availability of high quality\nreference translations. This issue is further exacerbated in languages with\nmultiple dialects, where linguistic diversity and data scarcity hinder robust\nevaluation. Large Language Models (LLMs) present a promising solution through\nreference-free evaluation techniques; however, their effectiveness diminishes\nin the absence of dialect-specific context and tailored guidance. In this work,\nwe propose a comprehensive framework that enhances LLM-based MT evaluation\nusing a dialect guided approach. We extend the ONUBAD dataset by incorporating\nSylheti-English sentence pairs, corresponding machine translations, and Direct\nAssessment (DA) scores annotated by native speakers. To address the vocabulary\ngap, we augment the tokenizer vocabulary with dialect-specific terms. We\nfurther introduce a regression head to enable scalar score prediction and\ndesign a dialect-guided (DG) prompting strategy. Our evaluation across multiple\nLLMs shows that the proposed pipeline consistently outperforms existing\nmethods, achieving the highest gain of +0.1083 in Spearman correlation, along\nwith improvements across other evaluation settings. The dataset and the code\nare available at https://github.com/180041123-Atiq/MTEonLowResourceLanguage."}
{"id": "2505.12251", "pdf": "https://arxiv.org/pdf/2505.12251", "abs": "https://arxiv.org/abs/2505.12251", "authors": ["Haozhe Xiang", "Han Zhang", "Yu Cheng", "Xiongwen Quan", "Wanwan Huang"], "title": "SMFusion: Semantic-Preserving Fusion of Multimodal Medical Images for Enhanced Clinical Diagnosis", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal medical image fusion plays a crucial role in medical diagnosis by\nintegrating complementary information from different modalities to enhance\nimage readability and clinical applicability. However, existing methods mainly\nfollow computer vision standards for feature extraction and fusion strategy\nformulation, overlooking the rich semantic information inherent in medical\nimages. To address this limitation, we propose a novel semantic-guided medical\nimage fusion approach that, for the first time, incorporates medical prior\nknowledge into the fusion process. Specifically, we construct a publicly\navailable multimodal medical image-text dataset, upon which text descriptions\ngenerated by BiomedGPT are encoded and semantically aligned with image features\nin a high-dimensional space via a semantic interaction alignment module. During\nthis process, a cross attention based linear transformation automatically maps\nthe relationship between textual and visual features to facilitate\ncomprehensive learning. The aligned features are then embedded into a\ntext-injection module for further feature-level fusion. Unlike traditional\nmethods, we further generate diagnostic reports from the fused images to assess\nthe preservation of medical information. Additionally, we design a medical\nsemantic loss function to enhance the retention of textual cues from the source\nimages. Experimental results on test datasets demonstrate that the proposed\nmethod achieves superior performance in both qualitative and quantitative\nevaluations while preserving more critical medical information."}
{"id": "2505.12287", "pdf": "https://arxiv.org/pdf/2505.12287", "abs": "https://arxiv.org/abs/2505.12287", "authors": ["Linghan Huang", "Haolin Jin", "Zhaoge Bi", "Pengyue Yang", "Peizhou Zhao", "Taozhao Chen", "Xiongfei Wu", "Lei Ma", "Huaming Chen"], "title": "The Tower of Babel Revisited: Multilingual Jailbreak Prompts on Closed-Source Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have seen widespread applications across various\ndomains, yet remain vulnerable to adversarial prompt injections. While most\nexisting research on jailbreak attacks and hallucination phenomena has focused\nprimarily on open-source models, we investigate the frontier of closed-source\nLLMs under multilingual attack scenarios. We present a first-of-its-kind\nintegrated adversarial framework that leverages diverse attack techniques to\nsystematically evaluate frontier proprietary solutions, including GPT-4o,\nDeepSeek-R1, Gemini-1.5-Pro, and Qwen-Max. Our evaluation spans six categories\nof security contents in both English and Chinese, generating 38,400 responses\nacross 32 types of jailbreak attacks. Attack success rate (ASR) is utilized as\nthe quantitative metric to assess performance from three dimensions: prompt\ndesign, model architecture, and language environment. Our findings suggest that\nQwen-Max is the most vulnerable, while GPT-4o shows the strongest defense.\nNotably, prompts in Chinese consistently yield higher ASRs than their English\ncounterparts, and our novel Two-Sides attack technique proves to be the most\neffective across all models. This work highlights a dire need for\nlanguage-aware alignment and robust cross-lingual defenses in LLMs, and we hope\nit will inspire researchers, developers, and policymakers toward more robust\nand inclusive AI systems."}
{"id": "2505.12253", "pdf": "https://arxiv.org/pdf/2505.12253", "abs": "https://arxiv.org/abs/2505.12253", "authors": ["Hanyu Zhou", "Gim Hee Lee"], "title": "LLaVA-4D: Embedding SpatioTemporal Prompt into LMMs for 4D Scene Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Despite achieving significant progress in 2D image understanding, large\nmultimodal models (LMMs) struggle in the physical world due to the lack of\nspatial representation. Typically, existing 3D LMMs mainly embed 3D positions\nas fixed spatial prompts within visual features to represent the scene.\nHowever, these methods are limited to understanding the static background and\nfail to capture temporally varying dynamic objects. In this paper, we propose\nLLaVA-4D, a general LMM framework with a novel spatiotemporal prompt for visual\nrepresentation in 4D scene understanding. The spatiotemporal prompt is\ngenerated by encoding 3D position and 1D time into a dynamic-aware 4D\ncoordinate embedding. Moreover, we demonstrate that spatial and temporal\ncomponents disentangled from visual features are more effective in\ndistinguishing the background from objects. This motivates embedding the 4D\nspatiotemporal prompt into these features to enhance the dynamic scene\nrepresentation. By aligning visual spatiotemporal embeddings with language\nembeddings, LMMs gain the ability to understand both spatial and temporal\ncharacteristics of static background and dynamic objects in the physical world.\nAdditionally, we construct a 4D vision-language dataset with spatiotemporal\ncoordinate annotations for instruction fine-tuning LMMs. Extensive experiments\nhave been conducted to demonstrate the effectiveness of our method across\ndifferent tasks in 4D scene understanding."}
{"id": "2505.12299", "pdf": "https://arxiv.org/pdf/2505.12299", "abs": "https://arxiv.org/abs/2505.12299", "authors": ["Kun Huang", "Weikai Xu", "Yuxuan Liu", "Quandong Wang", "Pengzhi Gao", "Wei Liu", "Jian Luan", "Bin Wang", "Bo An"], "title": "Enhance Mobile Agents Thinking Process Via Iterative Preference Learning", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 8 figures, 7 tables", "summary": "The Chain of Action-Planning Thoughts (CoaT) paradigm has been shown to\nimprove the reasoning performance of VLM-based mobile agents in GUI tasks.\nHowever, the scarcity of diverse CoaT trajectories limits the expressiveness\nand generalization ability of such agents. While self-training is commonly\nemployed to address data scarcity, existing approaches either overlook the\ncorrectness of intermediate reasoning steps or depend on expensive\nprocess-level annotations to construct process reward models (PRM). To address\nthe above problems, we propose an Iterative Preference Learning (IPL) that\nconstructs a CoaT-tree through interative sampling, scores leaf nodes using\nrule-based reward, and backpropagates feedback to derive Thinking-level Direct\nPreference Optimization (T-DPO) pairs. To prevent overfitting during warm-up\nsupervised fine-tuning, we further introduce a three-stage instruction\nevolution, which leverages GPT-4o to generate diverse Q\\&A pairs based on real\nmobile UI screenshots, enhancing both generality and layout understanding.\nExperiments on three standard Mobile GUI-agent benchmarks demonstrate that our\nagent MobileIPL outperforms strong baselines, including continual pretraining\nmodels such as OS-ATLAS and UI-TARS. It achieves state-of-the-art performance\nacross three standard Mobile GUI-Agents benchmarks and shows strong\ngeneralization to out-of-domain scenarios."}
{"id": "2505.12254", "pdf": "https://arxiv.org/pdf/2505.12254", "abs": "https://arxiv.org/abs/2505.12254", "authors": ["Yiwei Ou", "Xiaobin Ren", "Ronggui Sun", "Guansong Gao", "Ziyi Jiang", "Kaiqi Zhao", "Manfredo Manfredini"], "title": "MMS-VPR: Multimodal Street-Level Visual Place Recognition Dataset and Benchmark", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Existing visual place recognition (VPR) datasets predominantly rely on\nvehicle-mounted imagery, lack multimodal diversity and underrepresent dense,\nmixed-use street-level spaces, especially in non-Western urban contexts. To\naddress these gaps, we introduce MMS-VPR, a large-scale multimodal dataset for\nstreet-level place recognition in complex, pedestrian-only environments. The\ndataset comprises 78,575 annotated images and 2,512 video clips captured across\n207 locations in a ~70,800 $\\mathrm{m}^2$ open-air commercial district in\nChengdu, China. Each image is labeled with precise GPS coordinates, timestamp,\nand textual metadata, and covers varied lighting conditions, viewpoints, and\ntimeframes. MMS-VPR follows a systematic and replicable data collection\nprotocol with minimal device requirements, lowering the barrier for scalable\ndataset creation. Importantly, the dataset forms an inherent spatial graph with\n125 edges, 81 nodes, and 1 subgraph, enabling structure-aware place\nrecognition. We further define two application-specific subsets --\nDataset_Edges and Dataset_Points -- to support fine-grained and graph-based\nevaluation tasks. Extensive benchmarks using conventional VPR models, graph\nneural networks, and multimodal baselines show substantial improvements when\nleveraging multimodal and structural cues. MMS-VPR facilitates future research\nat the intersection of computer vision, geospatial understanding, and\nmultimodal reasoning. The dataset is publicly available at\nhttps://huggingface.co/datasets/Yiwei-Ou/MMS-VPR."}
{"id": "2505.12300", "pdf": "https://arxiv.org/pdf/2505.12300", "abs": "https://arxiv.org/abs/2505.12300", "authors": ["Weixuan Wang", "Minghao Wu", "Barry Haddow", "Alexandra Birch"], "title": "HBO: Hierarchical Balancing Optimization for Fine-Tuning Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning large language models (LLMs) on a mixture of diverse datasets\nposes challenges due to data imbalance and heterogeneity. Existing methods\noften address these issues across datasets (globally) but overlook the\nimbalance and heterogeneity within individual datasets (locally), which limits\ntheir effectiveness. We introduce Hierarchical Balancing Optimization (HBO), a\nnovel method that enables LLMs to autonomously adjust data allocation during\nfine-tuning both across datasets (globally) and within each individual dataset\n(locally). HBO employs a bilevel optimization strategy with two types of\nactors: a Global Actor, which balances data sampling across different subsets\nof the training mixture, and several Local Actors, which optimizes data usage\nwithin each subset based on difficulty levels. These actors are guided by\nreward functions derived from the LLM's training state, which measure learning\nprogress and relative performance improvement. We evaluate HBO on three LLM\nbackbones across nine diverse tasks in multilingual and multitask setups.\nResults show that HBO consistently outperforms existing baselines, achieving\nsignificant accuracy gains. Our in-depth analysis further demonstrates that\nboth the global actor and local actors of HBO effectively adjust data usage\nduring fine-tuning. HBO provides a comprehensive solution to the challenges of\ndata imbalance and heterogeneity in LLM fine-tuning, enabling more effective\ntraining across diverse datasets."}
{"id": "2505.12266", "pdf": "https://arxiv.org/pdf/2505.12266", "abs": "https://arxiv.org/abs/2505.12266", "authors": ["ZhanFeng Feng", "Long Peng", "Xin Di", "Yong Guo", "Wenbo Li", "Yulun Zhang", "Renjing Pei", "Yang Wang", "Yang Cao", "Zheng-Jun Zha"], "title": "PMQ-VE: Progressive Multi-Frame Quantization for Video Enhancement", "categories": ["cs.CV"], "comment": null, "summary": "Multi-frame video enhancement tasks aim to improve the spatial and temporal\nresolution and quality of video sequences by leveraging temporal information\nfrom multiple frames, which are widely used in streaming video processing,\nsurveillance, and generation. Although numerous Transformer-based enhancement\nmethods have achieved impressive performance, their computational and memory\ndemands hinder deployment on edge devices. Quantization offers a practical\nsolution by reducing the bit-width of weights and activations to improve\nefficiency. However, directly applying existing quantization methods to video\nenhancement tasks often leads to significant performance degradation and loss\nof fine details. This stems from two limitations: (a) inability to allocate\nvarying representational capacity across frames, which results in suboptimal\ndynamic range adaptation; (b) over-reliance on full-precision teachers, which\nlimits the learning of low-bit student models. To tackle these challenges, we\npropose a novel quantization method for video enhancement: Progressive\nMulti-Frame Quantization for Video Enhancement (PMQ-VE). This framework\nfeatures a coarse-to-fine two-stage process: Backtracking-based Multi-Frame\nQuantization (BMFQ) and Progressive Multi-Teacher Distillation (PMTD). BMFQ\nutilizes a percentile-based initialization and iterative search with pruning\nand backtracking for robust clipping bounds. PMTD employs a progressive\ndistillation strategy with both full-precision and multiple high-bit (INT)\nteachers to enhance low-bit models' capacity and quality. Extensive experiments\ndemonstrate that our method outperforms existing approaches, achieving\nstate-of-the-art performance across multiple tasks and benchmarks.The code will\nbe made publicly available at: https://github.com/xiaoBIGfeng/PMQ-VE."}
{"id": "2505.12306", "pdf": "https://arxiv.org/pdf/2505.12306", "abs": "https://arxiv.org/abs/2505.12306", "authors": ["Yuwei Zhang", "Wenhao Yu", "Shangbin Feng", "Yifan Zhu", "Letian Peng", "Jayanth Srinivasa", "Gaowen Liu", "Jingbo Shang"], "title": "Bidirectional LMs are Better Knowledge Memorizers? A Benchmark for Real-world Knowledge Injection", "categories": ["cs.CL"], "comment": "Dataset is available at\n  https://huggingface.co/datasets/YWZBrandon/wikidyk", "summary": "Despite significant advances in large language models (LLMs), their knowledge\nmemorization capabilities remain underexplored, due to the lack of standardized\nand high-quality test ground. In this paper, we introduce a novel, real-world\nand large-scale knowledge injection benchmark that evolves continuously over\ntime without requiring human intervention. Specifically, we propose WikiDYK,\nwhich leverages recently-added and human-written facts from Wikipedia's \"Did\nYou Know...\" entries. These entries are carefully selected by expert Wikipedia\neditors based on criteria such as verifiability and clarity. Each entry is\nconverted into multiple question-answer pairs spanning diverse task formats\nfrom easy cloze prompts to complex multi-hop questions. WikiDYK contains 12,290\nfacts and 77,180 questions, which is also seamlessly extensible with future\nupdates from Wikipedia editors. Extensive experiments using continued\npre-training reveal a surprising insight: despite their prevalence in modern\nLLMs, Causal Language Models (CLMs) demonstrate significantly weaker knowledge\nmemorization capabilities compared to Bidirectional Language Models (BiLMs),\nexhibiting a 23% lower accuracy in terms of reliability. To compensate for the\nsmaller scales of current BiLMs, we introduce a modular collaborative framework\nutilizing ensembles of BiLMs as external knowledge repositories to integrate\nwith LLMs. Experiment shows that our framework further improves the reliability\naccuracy by up to 29.1%."}
{"id": "2505.12274", "pdf": "https://arxiv.org/pdf/2505.12274", "abs": "https://arxiv.org/abs/2505.12274", "authors": ["Yixiao Chen", "Zhiyuan Ma", "Guoli Jia", "Che Jiang", "Jianjun Li", "Bowen Zhou"], "title": "Context-Aware Autoregressive Models for Multi-Conditional Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Autoregressive transformers have recently shown impressive image generation\nquality and efficiency on par with state-of-the-art diffusion models. Unlike\ndiffusion architectures, autoregressive models can naturally incorporate\narbitrary modalities into a single, unified token sequence--offering a concise\nsolution for multi-conditional image generation tasks. In this work, we propose\n$\\textbf{ContextAR}$, a flexible and effective framework for multi-conditional\nimage generation. ContextAR embeds diverse conditions (e.g., canny edges, depth\nmaps, poses) directly into the token sequence, preserving modality-specific\nsemantics. To maintain spatial alignment while enhancing discrimination among\ndifferent condition types, we introduce hybrid positional encodings that fuse\nRotary Position Embedding with Learnable Positional Embedding. We design\nConditional Context-aware Attention to reduces computational complexity while\npreserving effective intra-condition perception. Without any fine-tuning,\nContextAR supports arbitrary combinations of conditions during inference time.\nExperimental results demonstrate the powerful controllability and versatility\nof our approach, and show that the competitive perpormance than diffusion-based\nmulti-conditional control approaches the existing autoregressive baseline\nacross diverse multi-condition driven scenarios. Project page:\n$\\href{https://context-ar.github.io/}{https://context-ar.github.io/.}$"}
{"id": "2505.12313", "pdf": "https://arxiv.org/pdf/2505.12313", "abs": "https://arxiv.org/abs/2505.12313", "authors": ["Weixuan Wang", "Minghao Wu", "Barry Haddow", "Alexandra Birch"], "title": "ExpertSteer: Intervening in LLMs through Expert Knowledge", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) exhibit remarkable capabilities across various\ntasks, yet guiding them to follow desired behaviours during inference remains a\nsignificant challenge. Activation steering offers a promising method to control\nthe generation process of LLMs by modifying their internal activations.\nHowever, existing methods commonly intervene in the model's behaviour using\nsteering vectors generated by the model itself, which constrains their\neffectiveness to that specific model and excludes the possibility of leveraging\npowerful external expert models for steering. To address these limitations, we\npropose ExpertSteer, a novel approach that leverages arbitrary specialized\nexpert models to generate steering vectors, enabling intervention in any LLMs.\nExpertSteer transfers the knowledge from an expert model to a target LLM\nthrough a cohesive four-step process: first aligning representation dimensions\nwith auto-encoders to enable cross-model transfer, then identifying\nintervention layer pairs based on mutual information analysis, next generating\nsteering vectors from the expert model using Recursive Feature Machines, and\nfinally applying these vectors on the identified layers during inference to\nselectively guide the target LLM without updating model parameters. We conduct\ncomprehensive experiments using three LLMs on 15 popular benchmarks across four\ndistinct domains. Experiments demonstrate that ExpertSteer significantly\noutperforms established baselines across diverse tasks at minimal cost."}
{"id": "2505.12280", "pdf": "https://arxiv.org/pdf/2505.12280", "abs": "https://arxiv.org/abs/2505.12280", "authors": ["Sijie Zhao", "Feng Liu", "Xueliang Zhang", "Hao Chen", "Pengfeng Xiao", "Lei Bai"], "title": "Temporal-Spectral-Spatial Unified Remote Sensing Dense Prediction", "categories": ["cs.CV"], "comment": "12 pages, 4 figures, Code\n  link:https://github.com/walking-shadow/Official_TSSUN", "summary": "The proliferation of diverse remote sensing data has spurred advancements in\ndense prediction tasks, yet significant challenges remain in handling data\nheterogeneity. Remote sensing imagery exhibits substantial variability across\ntemporal, spectral, and spatial (TSS) dimensions, complicating unified data\nprocessing. Current deep learning models for dense prediction tasks, such as\nsemantic segmentation and change detection, are typically tailored to specific\ninput-output configurations. Consequently, variations in data dimensionality or\ntask requirements often lead to significant performance degradation or model\nincompatibility, necessitating costly retraining or fine-tuning efforts for\ndifferent application scenarios. This paper introduces the\nTemporal-Spectral-Spatial Unified Network (TSSUN), a novel architecture\ndesigned for unified representation and modeling of remote sensing data across\ndiverse TSS characteristics and task types. TSSUN employs a\nTemporal-Spectral-Spatial Unified Strategy that leverages meta-information to\ndecouple and standardize input representations from varied temporal, spectral,\nand spatial configurations, and similarly unifies output structures for\ndifferent dense prediction tasks and class numbers. Furthermore, a Local-Global\nWindow Attention mechanism is proposed to efficiently capture both local\ncontextual details and global dependencies, enhancing the model's adaptability\nand feature extraction capabilities. Extensive experiments on multiple datasets\ndemonstrate that a single TSSUN model effectively adapts to heterogeneous\ninputs and unifies various dense prediction tasks. The proposed approach\nconsistently achieves or surpasses state-of-the-art performance, highlighting\nits robustness and generalizability for complex remote sensing applications\nwithout requiring task-specific modifications."}
{"id": "2505.12328", "pdf": "https://arxiv.org/pdf/2505.12328", "abs": "https://arxiv.org/abs/2505.12328", "authors": ["Xinye Li", "Mingqi Wan", "Dianbo Sui"], "title": "LLMSR@XLLM25: An Empirical Study of LLM for Structural Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "We present Team asdfo123's submission to the LLMSR@XLLM25 shared task, which\nevaluates large language models on producing fine-grained, controllable, and\ninterpretable reasoning processes. Systems must extract all problem conditions,\ndecompose a chain of thought into statement-evidence pairs, and verify the\nlogical validity of each pair. Leveraging only the off-the-shelf\nMeta-Llama-3-8B-Instruct, we craft a concise few-shot, multi-turn prompt that\nfirst enumerates all conditions and then guides the model to label, cite, and\nadjudicate every reasoning step. A lightweight post-processor based on regular\nexpressions normalises spans and enforces the official JSON schema. Without\nfine-tuning, external retrieval, or ensembling, our method ranks 5th overall,\nachieving macro F1 scores on par with substantially more complex and\nresource-consuming pipelines. We conclude by analysing the strengths and\nlimitations of our approach and outlining directions for future research in\nstructural reasoning with LLMs. Our code is available at\nhttps://github.com/asdfo123/LLMSR-asdfo123."}
{"id": "2505.12307", "pdf": "https://arxiv.org/pdf/2505.12307", "abs": "https://arxiv.org/abs/2505.12307", "authors": ["Maoyuan Ye", "Jing Zhang", "Juhua Liu", "Bo Du", "Dacheng Tao"], "title": "LogicOCR: Do Your Large Multimodal Models Excel at Logical Reasoning on Text-Rich Images?", "categories": ["cs.CV", "cs.CL"], "comment": "GitHub: \\url{https://github.com/MiliLab/LogicOCR}", "summary": "Recent advances in Large Multimodal Models (LMMs) have significantly improved\ntheir reasoning and Optical Character Recognition (OCR) capabilities. However,\ntheir performance on complex logical reasoning tasks involving text-rich images\nremains underexplored. To bridge this gap, we introduce LogicOCR, a benchmark\ncomprising 1,100 multiple-choice questions designed to evaluate LMMs' logical\nreasoning abilities on text-rich images, while minimizing reliance on\ndomain-specific knowledge (e.g., mathematics). We construct LogicOCR by\ncurating a text corpus from the Chinese National Civil Servant Examination and\ndevelop a scalable, automated pipeline to convert it into multimodal samples.\nFirst, we design prompt templates to steer GPT-Image-1 to generate images with\ndiverse backgrounds, interleaved text-illustration layouts, and varied fonts,\nensuring contextual relevance and visual realism. Then, the generated images\nare manually verified, with low-quality examples discarded. We evaluate a range\nof representative open-source and proprietary LMMs under both Chain-of-Thought\n(CoT) and direct-answer settings. Our multi-dimensional analysis reveals key\ninsights, such as the impact of test-time scaling, input modality differences,\nand sensitivity to visual-text orientation. Notably, LMMs still lag in\nmultimodal reasoning compared to text-only inputs, indicating that they have\nnot fully bridged visual reading with reasoning. We hope LogicOCR will serve as\na valuable resource for advancing multimodal reasoning research. The dataset is\navailable at https://github.com/MiliLab/LogicOCR."}
{"id": "2505.12345", "pdf": "https://arxiv.org/pdf/2505.12345", "abs": "https://arxiv.org/abs/2505.12345", "authors": ["Qizhou Chen", "Dakan Wang", "Taolin Zhang", "Zaoming Yan", "Chengsong You", "Chengyu Wang", "Xiaofeng He"], "title": "UniEdit: A Unified Knowledge Editing Benchmark for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Model editing aims to enhance the accuracy and reliability of large language\nmodels (LLMs) by efficiently adjusting their internal parameters. Currently,\nmost LLM editing datasets are confined to narrow knowledge domains and cover a\nlimited range of editing evaluation. They often overlook the broad scope of\nediting demands and the diversity of ripple effects resulting from edits. In\nthis context, we introduce UniEdit, a unified benchmark for LLM editing\ngrounded in open-domain knowledge. First, we construct editing samples by\nselecting entities from 25 common domains across five major categories,\nutilizing the extensive triple knowledge available in open-domain knowledge\ngraphs to ensure comprehensive coverage of the knowledge domains. To address\nthe issues of generality and locality in editing, we design an Neighborhood\nMulti-hop Chain Sampling (NMCS) algorithm to sample subgraphs based on a given\nknowledge piece to entail comprehensive ripple effects to evaluate. Finally, we\nemploy proprietary LLMs to convert the sampled knowledge subgraphs into natural\nlanguage text, guaranteeing grammatical accuracy and syntactical diversity.\nExtensive statistical analysis confirms the scale, comprehensiveness, and\ndiversity of our UniEdit benchmark. We conduct comprehensive experiments across\nmultiple LLMs and editors, analyzing their performance to highlight strengths\nand weaknesses in editing across open knowledge domains and various evaluation\ncriteria, thereby offering valuable insights for future research endeavors."}
{"id": "2505.12310", "pdf": "https://arxiv.org/pdf/2505.12310", "abs": "https://arxiv.org/abs/2505.12310", "authors": ["Shouyi Lu", "Huanyu Zhou", "Guirong Zhuo"], "title": "DNOI-4DRO: Deep 4D Radar Odometry with Differentiable Neural-Optimization Iterations", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "16 pages,10 figures", "summary": "A novel learning-optimization-combined 4D radar odometry model, named\nDNOI-4DRO, is proposed in this paper. The proposed model seamlessly integrates\ntraditional geometric optimization with end-to-end neural network training,\nleveraging an innovative differentiable neural-optimization iteration operator.\nIn this framework, point-wise motion flow is first estimated using a neural\nnetwork, followed by the construction of a cost function based on the\nrelationship between point motion and pose in 3D space. The radar pose is then\nrefined using Gauss-Newton updates. Additionally, we design a dual-stream 4D\nradar backbone that integrates multi-scale geometric features and\nclustering-based class-aware features to enhance the representation of sparse\n4D radar point clouds. Extensive experiments on the VoD and Snail-Radar\ndatasets demonstrate the superior performance of our model, which outperforms\nrecent classical and learning-based approaches. Notably, our method even\nachieves results comparable to A-LOAM with mapping optimization using LiDAR\npoint clouds as input. Our models and code will be publicly released."}
{"id": "2505.12349", "pdf": "https://arxiv.org/pdf/2505.12349", "abs": "https://arxiv.org/abs/2505.12349", "authors": ["Axel Abels", "Tom Lenaerts"], "title": "Wisdom from Diversity: Bias Mitigation Through Hybrid Human-LLM Crowds", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": "Accepted for publication in the Proceedings of the 34th International\n  Joint Conference on Artificial Intelligence (IJCAI 2025)", "summary": "Despite their performance, large language models (LLMs) can inadvertently\nperpetuate biases found in the data they are trained on. By analyzing LLM\nresponses to bias-eliciting headlines, we find that these models often mirror\nhuman biases. To address this, we explore crowd-based strategies for mitigating\nbias through response aggregation. We first demonstrate that simply averaging\nresponses from multiple LLMs, intended to leverage the \"wisdom of the crowd\",\ncan exacerbate existing biases due to the limited diversity within LLM crowds.\nIn contrast, we show that locally weighted aggregation methods more effectively\nleverage the wisdom of the LLM crowd, achieving both bias mitigation and\nimproved accuracy. Finally, recognizing the complementary strengths of LLMs\n(accuracy) and humans (diversity), we demonstrate that hybrid crowds containing\nboth significantly enhance performance and further reduce biases across ethnic\nand gender-related contexts."}
{"id": "2505.12312", "pdf": "https://arxiv.org/pdf/2505.12312", "abs": "https://arxiv.org/abs/2505.12312", "authors": ["Qi Feng", "Hidetoshi Shimodaira"], "title": "Visuospatial Cognitive Assistant", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "comment": "31 pages, 10 figures, 6 tables. The implementation and fine-tuned\n  model (ViCA-7B) are publicly available at https://huggingface.co/nkkbr/ViCA.\n  The ViCA-322K dataset can be found at\n  https://huggingface.co/datasets/nkkbr/ViCA-322K, and the ViCA-Thinking-2.68K\n  dataset is at https://huggingface.co/datasets/nkkbr/ViCA-thinking-2.68k", "summary": "Video-based spatial cognition is vital for robotics and embodied AI but\nchallenges current Vision-Language Models (VLMs). This paper makes two key\ncontributions. First, we introduce ViCA (Visuospatial Cognitive\nAssistant)-322K, a diverse dataset of 322,003 QA pairs from real-world indoor\nvideos (ARKitScenes, ScanNet, ScanNet++), offering supervision for 3D\nmetadata-grounded queries and video-based complex reasoning. Second, we develop\nViCA-7B, fine-tuned on ViCA-322K, which achieves new state-of-the-art on all\neight VSI-Bench tasks, outperforming existing models, including larger ones\n(e.g., +26.1 on Absolute Distance). For interpretability, we present\nViCA-Thinking-2.68K, a dataset with explicit reasoning chains, and fine-tune\nViCA-7B to create ViCA-7B-Thinking, a model that articulates its spatial\nreasoning. Our work highlights the importance of targeted data and suggests\npaths for improved temporal-spatial modeling. We release all resources to\nfoster research in robust visuospatial intelligence."}
{"id": "2505.12368", "pdf": "https://arxiv.org/pdf/2505.12368", "abs": "https://arxiv.org/abs/2505.12368", "authors": ["Gauri Kholkar", "Ratinder Ahuja"], "title": "CAPTURE: Context-Aware Prompt Injection Testing and Robustness Enhancement", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted in ACL LLMSec Workshop 2025", "summary": "Prompt injection remains a major security risk for large language models.\nHowever, the efficacy of existing guardrail models in context-aware settings\nremains underexplored, as they often rely on static attack benchmarks.\nAdditionally, they have over-defense tendencies. We introduce CAPTURE, a novel\ncontext-aware benchmark assessing both attack detection and over-defense\ntendencies with minimal in-domain examples. Our experiments reveal that current\nprompt injection guardrail models suffer from high false negatives in\nadversarial cases and excessive false positives in benign scenarios,\nhighlighting critical limitations."}
{"id": "2505.12317", "pdf": "https://arxiv.org/pdf/2505.12317", "abs": "https://arxiv.org/abs/2505.12317", "authors": ["Ruoqi Wang", "Haitao Wang", "Shaojie Guo", "Qiong Luo"], "title": "Improving Out-of-Domain Robustness with Targeted Augmentation in Frequency and Pixel Spaces", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Out-of-domain (OOD) robustness under domain adaptation settings, where\nlabeled source data and unlabeled target data come from different\ndistributions, is a key challenge in real-world applications. A common approach\nto improving OOD robustness is through data augmentations. However, in\nreal-world scenarios, models trained with generic augmentations can only\nimprove marginally when generalized under distribution shifts toward unlabeled\ntarget domains. While dataset-specific targeted augmentations can address this\nissue, they typically require expert knowledge and extensive prior data\nanalysis to identify the nature of the datasets and domain shift. To address\nthese challenges, we propose Frequency-Pixel Connect, a domain-adaptation\nframework that enhances OOD robustness by introducing a targeted augmentation\nin both the frequency space and pixel space. Specifically, we mix the amplitude\nspectrum and pixel content of a source image and a target image to generate\naugmented samples that introduce domain diversity while preserving the semantic\nstructure of the source image. Unlike previous targeted augmentation methods\nthat are both dataset-specific and limited to the pixel space, Frequency-Pixel\nConnect is dataset-agnostic, enabling broader and more flexible applicability\nbeyond natural image datasets. We further analyze the effectiveness of\nFrequency-Pixel Connect by evaluating the performance of our method connecting\nsame-class cross-domain samples while separating different-class examples. We\ndemonstrate that Frequency-Pixel Connect significantly improves cross-domain\nconnectivity and outperforms previous generic methods on four diverse\nreal-world benchmarks across vision, medical, audio, and astronomical domains,\nand it also outperforms other dataset-specific targeted augmentation methods."}
{"id": "2505.12381", "pdf": "https://arxiv.org/pdf/2505.12381", "abs": "https://arxiv.org/abs/2505.12381", "authors": ["Mohsinul Kabir", "Tasfia Tahsin", "Sophia Ananiadou"], "title": "From n-gram to Attention: How Model Architectures Learn and Propagate Bias in Language Modeling", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages", "summary": "Current research on bias in language models (LMs) predominantly focuses on\ndata quality, with significantly less attention paid to model architecture and\ntemporal influences of data. Even more critically, few studies systematically\ninvestigate the origins of bias. We propose a methodology grounded in\ncomparative behavioral theory to interpret the complex interaction between\ntraining data and model architecture in bias propagation during language\nmodeling. Building on recent work that relates transformers to n-gram LMs, we\nevaluate how data, model design choices, and temporal dynamics affect bias\npropagation. Our findings reveal that: (1) n-gram LMs are highly sensitive to\ncontext window size in bias propagation, while transformers demonstrate\narchitectural robustness; (2) the temporal provenance of training data\nsignificantly affects bias; and (3) different model architectures respond\ndifferentially to controlled bias injection, with certain biases (e.g. sexual\norientation) being disproportionately amplified. As language models become\nubiquitous, our findings highlight the need for a holistic approach -- tracing\nbias to its origins across both data and model dimensions, not just symptoms,\nto mitigate harm."}
{"id": "2505.12335", "pdf": "https://arxiv.org/pdf/2505.12335", "abs": "https://arxiv.org/abs/2505.12335", "authors": ["Ziqiang Li", "Jiazhen Yan", "Ziwen He", "Kai Zeng", "Weiwei Jiang", "Lizhi Xiong", "Zhangjie Fu"], "title": "Is Artificial Intelligence Generated Image Detection a Solved Problem?", "categories": ["cs.CV", "cs.CR"], "comment": "Under Review", "summary": "The rapid advancement of generative models, such as GANs and Diffusion\nmodels, has enabled the creation of highly realistic synthetic images, raising\nserious concerns about misinformation, deepfakes, and copyright infringement.\nAlthough numerous Artificial Intelligence Generated Image (AIGI) detectors have\nbeen proposed, often reporting high accuracy, their effectiveness in real-world\nscenarios remains questionable. To bridge this gap, we introduce AIGIBench, a\ncomprehensive benchmark designed to rigorously evaluate the robustness and\ngeneralization capabilities of state-of-the-art AIGI detectors. AIGIBench\nsimulates real-world challenges through four core tasks: multi-source\ngeneralization, robustness to image degradation, sensitivity to data\naugmentation, and impact of test-time pre-processing. It includes 23 diverse\nfake image subsets that span both advanced and widely adopted image generation\ntechniques, along with real-world samples collected from social media and AI\nart platforms. Extensive experiments on 11 advanced detectors demonstrate that,\ndespite their high reported accuracy in controlled settings, these detectors\nsuffer significant performance drops on real-world data, limited benefits from\ncommon augmentations, and nuanced effects of pre-processing, highlighting the\nneed for more robust detection strategies. By providing a unified and realistic\nevaluation framework, AIGIBench offers valuable insights to guide future\nresearch toward dependable and generalizable AIGI detection."}
{"id": "2505.12392", "pdf": "https://arxiv.org/pdf/2505.12392", "abs": "https://arxiv.org/abs/2505.12392", "authors": ["Yang Hu", "Xingyu Zhang", "Xueji Fang", "Zhiyang Chen", "Xiao Wang", "Huatian Zhang", "Guojun Qi"], "title": "SLOT: Sample-specific Language Model Optimization at Test-time", "categories": ["cs.CL"], "comment": null, "summary": "We propose SLOT (Sample-specific Language Model Optimization at Test-time), a\nnovel and parameter-efficient test-time inference approach that enhances a\nlanguage model's ability to more accurately respond to individual prompts.\nExisting Large Language Models (LLMs) often struggle with complex instructions,\nleading to poor performances on those not well represented among general\nsamples. To address this, SLOT conducts few optimization steps at test-time to\nupdate a light-weight sample-specific parameter vector. It is added to the\nfinal hidden layer before the output head, and enables efficient adaptation by\ncaching the last layer features during per-sample optimization. By minimizing\nthe cross-entropy loss on the input prompt only, SLOT helps the model better\naligned with and follow each given instruction. In experiments, we demonstrate\nthat our method outperforms the compared models across multiple benchmarks and\nLLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on\nGSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT\nachieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is\navailable at https://github.com/maple-research-lab/SLOT."}
{"id": "2505.12339", "pdf": "https://arxiv.org/pdf/2505.12339", "abs": "https://arxiv.org/abs/2505.12339", "authors": ["Midou Guo", "Qilin Yin", "Wei Lu", "Xiangyang Luo"], "title": "Towards Open-world Generalized Deepfake Detection: General Feature Extraction via Unsupervised Domain Adaptation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the development of generative artificial intelligence, new forgery\nmethods are rapidly emerging. Social platforms are flooded with vast amounts of\nunlabeled synthetic data and authentic data, making it increasingly challenging\nto distinguish real from fake. Due to the lack of labels, existing supervised\ndetection methods struggle to effectively address the detection of unknown\ndeepfake methods. Moreover, in open world scenarios, the amount of unlabeled\ndata greatly exceeds that of labeled data. Therefore, we define a new deepfake\ndetection generalization task which focuses on how to achieve efficient\ndetection of large amounts of unlabeled data based on limited labeled data to\nsimulate a open world scenario. To solve the above mentioned task, we propose a\nnovel Open-World Deepfake Detection Generalization Enhancement Training\nStrategy (OWG-DS) to improve the generalization ability of existing methods.\nOur approach aims to transfer deepfake detection knowledge from a small amount\nof labeled source domain data to large-scale unlabeled target domain data.\nSpecifically, we introduce the Domain Distance Optimization (DDO) module to\nalign different domain features by optimizing both inter-domain and\nintra-domain distances. Additionally, the Similarity-based Class Boundary\nSeparation (SCBS) module is used to enhance the aggregation of similar samples\nto ensure clearer class boundaries, while an adversarial training mechanism is\nadopted to learn the domain-invariant features. Extensive experiments show that\nthe proposed deepfake detection generalization enhancement training strategy\nexcels in cross-method and cross-dataset scenarios, improving the model's\ngeneralization."}
{"id": "2505.12398", "pdf": "https://arxiv.org/pdf/2505.12398", "abs": "https://arxiv.org/abs/2505.12398", "authors": ["Yepeng Weng", "Qiao Hu", "Xujie Chen", "Li Liu", "Dianwen Mei", "Huishi Qiu", "Jiang Tian", "Zhongchao Shi"], "title": "Traversal Verification for Speculative Tree Decoding", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Under review", "summary": "Speculative decoding is a promising approach for accelerating large language\nmodels. The primary idea is to use a lightweight draft model to speculate the\noutput of the target model for multiple subsequent timesteps, and then verify\nthem in parallel to determine whether the drafted tokens should be accepted or\nrejected. To enhance acceptance rates, existing frameworks typically construct\ntoken trees containing multiple candidates in each timestep. However, their\nreliance on token-level verification mechanisms introduces two critical\nlimitations: First, the probability distribution of a sequence differs from\nthat of individual tokens, leading to suboptimal acceptance length. Second,\ncurrent verification schemes begin from the root node and proceed layer by\nlayer in a top-down manner. Once a parent node is rejected, all its child nodes\nshould be discarded, resulting in inefficient utilization of speculative\ncandidates. This paper introduces Traversal Verification, a novel speculative\ndecoding algorithm that fundamentally rethinks the verification paradigm\nthrough leaf-to-root traversal. Our approach considers the acceptance of the\nentire token sequence from the current node to the root, and preserves\npotentially valid subsequences that would be prematurely discarded by existing\nmethods. We theoretically prove that the probability distribution obtained\nthrough Traversal Verification is identical to that of the target model,\nguaranteeing lossless inference while achieving substantial acceleration gains.\nExperimental results across different large language models and multiple tasks\nshow that our method consistently improves acceptance length and throughput\nover existing methods"}
{"id": "2505.12340", "pdf": "https://arxiv.org/pdf/2505.12340", "abs": "https://arxiv.org/abs/2505.12340", "authors": ["Jirong Zha", "Yuxuan Fan", "Kai Li", "Han Li", "Chen Gao", "Xinlei Chen", "Yong Li"], "title": "DIMM: Decoupled Multi-hierarchy Kalman Filter for 3D Object Tracking", "categories": ["cs.CV"], "comment": "10 pages", "summary": "State estimation is challenging for 3D object tracking with high\nmaneuverability, as the target's state transition function changes rapidly,\nirregularly, and is unknown to the estimator. Existing work based on\ninteracting multiple model (IMM) achieves more accurate estimation than\nsingle-filter approaches through model combination, aligning appropriate models\nfor different motion modes of the target object over time. However, two\nlimitations of conventional IMM remain unsolved. First, the solution space of\nthe model combination is constrained as the target's diverse kinematic\nproperties in different directions are ignored. Second, the model combination\nweights calculated by the observation likelihood are not accurate enough due to\nthe measurement uncertainty. In this paper, we propose a novel framework, DIMM,\nto effectively combine estimates from different motion models in each\ndirection, thus increasing the 3D object tracking accuracy. First, DIMM extends\nthe model combination solution space of conventional IMM from a hyperplane to a\nhypercube by designing a 3D-decoupled multi-hierarchy filter bank, which\ndescribes the target's motion with various-order linear models. Second, DIMM\ngenerates more reliable combination weight matrices through a differentiable\nadaptive fusion network for importance allocation rather than solely relying on\nthe observation likelihood; it contains an attention-based twin delayed deep\ndeterministic policy gradient (TD3) method with a hierarchical reward.\nExperiments demonstrate that DIMM significantly improves the tracking accuracy\nof existing state estimation methods by 31.61%~99.23%."}
{"id": "2505.12405", "pdf": "https://arxiv.org/pdf/2505.12405", "abs": "https://arxiv.org/abs/2505.12405", "authors": ["Konstantinos Xylogiannopoulos", "Petros Xanthopoulos", "Panagiotis Karampelas", "Georgios Bakamitsos"], "title": "The power of text similarity in identifying AI-LLM paraphrased documents: The case of BBC news articles and ChatGPT", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Generative AI paraphrased text can be used for copyright infringement and the\nAI paraphrased content can deprive substantial revenue from original content\ncreators. Despite this recent surge of malicious use of generative AI, there\nare few academic publications that research this threat. In this article, we\ndemonstrate the ability of pattern-based similarity detection for AI\nparaphrased news recognition. We propose an algorithmic scheme, which is not\nlimited to detect whether an article is an AI paraphrase, but, more\nimportantly, to identify that the source of infringement is the ChatGPT. The\nproposed method is tested with a benchmark dataset specifically created for\nthis task that incorporates real articles from BBC, incorporating a total of\n2,224 articles across five different news categories, as well as 2,224\nparaphrased articles created with ChatGPT. Results show that our pattern\nsimilarity-based method, that makes no use of deep learning, can detect ChatGPT\nassisted paraphrased articles at percentages 96.23% for accuracy, 96.25% for\nprecision, 96.21% for sensitivity, 96.25% for specificity and 96.23% for F1\nscore."}
{"id": "2505.12363", "pdf": "https://arxiv.org/pdf/2505.12363", "abs": "https://arxiv.org/abs/2505.12363", "authors": ["Qi Feng", "Hidetoshi Shimodaira"], "title": "Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "comment": "26 pages, 19 figures, 4 tables. Code, models, and dataset are\n  available at our project page: https://github.com/nkkbr/ViCA", "summary": "While Multimodal Large Language Models (MLLMs) excel at general\nvision-language tasks, visuospatial cognition - reasoning about spatial\nlayouts, relations, and dynamics - remains a significant challenge. Existing\nmodels often lack the necessary architectural components and specialized\ntraining data for fine-grained spatial understanding. We introduce ViCA2\n(Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial\nreasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP\nfor semantics and Hiera for spatial structure, coupled with a token ratio\ncontrol mechanism for efficiency. We also developed ViCA-322K, a new\nlarge-scale dataset with over 322,000 spatially grounded question-answer pairs\nfor targeted instruction tuning. On the challenging VSI-Bench benchmark, our\nViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly\nsurpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and\nleading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the\neffectiveness of our approach in achieving strong visuospatial intelligence\nwith a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset\nto facilitate further research."}
{"id": "2505.12415", "pdf": "https://arxiv.org/pdf/2505.12415", "abs": "https://arxiv.org/abs/2505.12415", "authors": ["Zhenhe Wu", "Jian Yang", "Jiaheng Liu", "Xianjie Wu", "Changzai Pan", "Jie Zhang", "Yu Zhao", "Shuangyong Song", "Yongxiang Li", "Zhoujun Li"], "title": "Table-R1: Region-based Reinforcement Learning for Table Understanding", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tables present unique challenges for language models due to their structured\nrow-column interactions, necessitating specialized approaches for effective\ncomprehension. While large language models (LLMs) have demonstrated potential\nin table reasoning through prompting and techniques like chain-of-thought (CoT)\nand program-of-thought (PoT), optimizing their performance for table question\nanswering remains underexplored. In this paper, we introduce region-based\nTable-R1, a novel reinforcement learning approach that enhances LLM table\nunderstanding by integrating region evidence into reasoning steps. Our method\nemploys Region-Enhanced Supervised Fine-Tuning (RE-SFT) to guide models in\nidentifying relevant table regions before generating answers, incorporating\ntextual, symbolic, and program-based reasoning. Additionally, Table-Aware Group\nRelative Policy Optimization (TARPO) introduces a mixed reward system to\ndynamically balance region accuracy and answer correctness, with decaying\nregion rewards and consistency penalties to align reasoning steps. Experiments\nshow that Table-R1 achieves an average performance improvement of 14.36 points\nacross multiple base models on three benchmark datasets, even outperforming\nbaseline models with ten times the parameters, while TARPO reduces response\ntoken consumption by 67.5% compared to GRPO, significantly advancing LLM\ncapabilities in efficient tabular reasoning."}
{"id": "2505.12391", "pdf": "https://arxiv.org/pdf/2505.12391", "abs": "https://arxiv.org/abs/2505.12391", "authors": ["Zhengyang Lu", "Qian Xia", "Weifan Wang", "Feng Wang"], "title": "CLIP-aware Domain-Adaptive Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "This work introduces CLIP-aware Domain-Adaptive Super-Resolution (CDASR), a\nnovel framework that addresses the critical challenge of domain generalization\nin single image super-resolution. By leveraging the semantic capabilities of\nCLIP (Contrastive Language-Image Pre-training), CDASR achieves unprecedented\nperformance across diverse domains and extreme scaling factors. The proposed\nmethod integrates CLIP-guided feature alignment mechanism with a meta-learning\ninspired few-shot adaptation strategy, enabling efficient knowledge transfer\nand rapid adaptation to target domains. A custom domain-adaptive module\nprocesses CLIP features alongside super-resolution features through a\nmulti-stage transformation process, including CLIP feature processing, spatial\nfeature generation, and feature fusion. This intricate process ensures\neffective incorporation of semantic information into the super-resolution\npipeline. Additionally, CDASR employs a multi-component loss function that\ncombines pixel-wise reconstruction, perceptual similarity, and semantic\nconsistency. Extensive experiments on benchmark datasets demonstrate CDASR's\nsuperiority, particularly in challenging scenarios. On the Urban100 dataset at\n$\\times$8 scaling, CDASR achieves a significant PSNR gain of 0.15dB over\nexisting methods, with even larger improvements of up to 0.30dB observed at\n$\\times$16 scaling."}
{"id": "2505.12423", "pdf": "https://arxiv.org/pdf/2505.12423", "abs": "https://arxiv.org/abs/2505.12423", "authors": ["Wenqiao Zhu", "Chao Xu", "Lulu Wang", "Jun Wu"], "title": "PSC: Extending Context Window of Large Language Models via Phase Shift Calibration", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Rotary Position Embedding (RoPE) is an efficient position encoding approach\nand is widely utilized in numerous large language models (LLMs). Recently, a\nlot of methods have been put forward to further expand the context window based\non RoPE. The core concept of those methods is to predefine or search for a set\nof factors to rescale the base frequencies of RoPE. Nevertheless, it is quite a\nchallenge for existing methods to predefine an optimal factor due to the\nexponential search space. In view of this, we introduce PSC (Phase Shift\nCalibration), a small module for calibrating the frequencies predefined by\nexisting methods. With the employment of PSC, we demonstrate that many existing\nmethods can be further enhanced, like PI, YaRN, and LongRoPE. We conducted\nextensive experiments across multiple models and tasks. The results demonstrate\nthat (1) when PSC is enabled, the comparative reductions in perplexity increase\nas the context window size is varied from 16k, to 32k, and up to 64k. (2) Our\napproach is broadly applicable and exhibits robustness across a variety of\nmodels and tasks. The code can be found at https://github.com/WNQzhu/PSC."}
{"id": "2505.12408", "pdf": "https://arxiv.org/pdf/2505.12408", "abs": "https://arxiv.org/abs/2505.12408", "authors": ["Minxu Liu", "Donghai Guan", "Chuhang Zheng", "Chunwei Tian", "Jie Wen", "Qi Zhu"], "title": "ViEEG: Hierarchical Neural Coding with Cross-Modal Progressive Enhancement for EEG-Based Visual Decoding", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "24 pages, 18 figures", "summary": "Understanding and decoding brain activity into visual representations is a\nfundamental challenge at the intersection of neuroscience and artificial\nintelligence. While EEG-based visual decoding has shown promise due to its\nnon-invasive, low-cost nature and millisecond-level temporal resolution,\nexisting methods are limited by their reliance on flat neural representations\nthat overlook the brain's inherent visual hierarchy. In this paper, we\nintroduce ViEEG, a biologically inspired hierarchical EEG decoding framework\nthat aligns with the Hubel-Wiesel theory of visual processing. ViEEG decomposes\neach visual stimulus into three biologically aligned components-contour,\nforeground object, and contextual scene-serving as anchors for a three-stream\nEEG encoder. These EEG features are progressively integrated via\ncross-attention routing, simulating cortical information flow from V1 to IT to\nthe association cortex. We further adopt hierarchical contrastive learning to\nalign EEG representations with CLIP embeddings, enabling zero-shot object\nrecognition. Extensive experiments on the THINGS-EEG dataset demonstrate that\nViEEG achieves state-of-the-art performance, with 40.9% Top-1 accuracy in\nsubject-dependent and 22.9% Top-1 accuracy in cross-subject settings,\nsurpassing existing methods by over 45%. Our framework not only advances the\nperformance frontier but also sets a new paradigm for biologically grounded\nbrain decoding in AI."}
{"id": "2505.12439", "pdf": "https://arxiv.org/pdf/2505.12439", "abs": "https://arxiv.org/abs/2505.12439", "authors": ["Jinming Zhang", "Yunfei Long"], "title": "Learning to Play Like Humans: A Framework for LLM Adaptation in Interactive Fiction Games", "categories": ["cs.CL"], "comment": null, "summary": "Interactive Fiction games (IF games) are where players interact through\nnatural language commands. While recent advances in Artificial Intelligence\nagents have reignited interest in IF games as a domain for studying\ndecision-making, existing approaches prioritize task-specific performance\nmetrics over human-like comprehension of narrative context and gameplay logic.\nThis work presents a cognitively inspired framework that guides Large Language\nModels (LLMs) to learn and play IF games systematically. Our proposed\n**L**earning to **P**lay **L**ike **H**umans (LPLH) framework integrates three\nkey components: (1) structured map building to capture spatial and narrative\nrelationships, (2) action learning to identify context-appropriate commands,\nand (3) feedback-driven experience analysis to refine decision-making over\ntime. By aligning LLMs-based agents' behavior with narrative intent and\ncommonsense constraints, LPLH moves beyond purely exploratory strategies to\ndeliver more interpretable, human-like performance. Crucially, this approach\ndraws on cognitive science principles to more closely simulate how human\nplayers read, interpret, and respond within narrative worlds. As a result, LPLH\nreframes the IF games challenge as a learning problem for LLMs-based agents,\noffering a new path toward robust, context-aware gameplay in complex text-based\nenvironments."}
{"id": "2505.12425", "pdf": "https://arxiv.org/pdf/2505.12425", "abs": "https://arxiv.org/abs/2505.12425", "authors": ["Edgar Riba", "Jian Shi", "Aditya Kumar", "Andrew Shen", "Gary Bradski"], "title": "Kornia-rs: A Low-Level 3D Computer Vision Library In Rust", "categories": ["cs.CV"], "comment": null, "summary": "We present \\textit{kornia-rs}, a high-performance 3D computer vision library\nwritten entirely in native Rust, designed for safety-critical and real-time\napplications. Unlike C++-based libraries like OpenCV or wrapper-based solutions\nlike OpenCV-Rust, \\textit{kornia-rs} is built from the ground up to leverage\nRust's ownership model and type system for memory and thread safety.\n\\textit{kornia-rs} adopts a statically-typed tensor system and a modular set of\ncrates, providing efficient image I/O, image processing and 3D operations. To\naid cross-platform compatibility, \\textit{kornia-rs} offers Python bindings,\nenabling seamless and efficient integration with Rust code. Empirical results\nshow that \\textit{kornia-rs} achieves a 3~ 5 times speedup in image\ntransformation tasks over native Rust alternatives, while offering comparable\nperformance to C++ wrapper-based libraries. In addition to 2D vision\ncapabilities, \\textit{kornia-rs} addresses a significant gap in the Rust\necosystem by providing a set of 3D computer vision operators. This paper\npresents the architecture and performance characteristics of\n\\textit{kornia-rs}, demonstrating its effectiveness in real-world computer\nvision applications."}
{"id": "2505.12452", "pdf": "https://arxiv.org/pdf/2505.12452", "abs": "https://arxiv.org/abs/2505.12452", "authors": ["Siyang Wu", "Honglin Bao", "Nadav Kunievsky", "James A. Evans"], "title": "Introspective Growth: Automatically Advancing LLM Expertise in Technology Judgment", "categories": ["cs.CL", "cs.CY", "cs.DL", "cs.IR"], "comment": "We commit to fully open-source our patent dataset", "summary": "Large language models (LLMs) increasingly demonstrate signs of conceptual\nunderstanding, yet much of their internal knowledge remains latent, loosely\nstructured, and difficult to access or evaluate. We propose self-questioning as\na lightweight and scalable strategy to improve LLMs' understanding,\nparticularly in domains where success depends on fine-grained semantic\ndistinctions. To evaluate this approach, we introduce a challenging new\nbenchmark of 1.3 million post-2015 computer science patent pairs, characterized\nby dense technical jargon and strategically complex writing. The benchmark\ncenters on a pairwise differentiation task: can a model distinguish between\nclosely related but substantively different inventions? We show that prompting\nLLMs to generate and answer their own questions - targeting the background\nknowledge required for the task - significantly improves performance. These\nself-generated questions and answers activate otherwise underutilized internal\nknowledge. Allowing LLMs to retrieve answers from external scientific texts\nfurther enhances performance, suggesting that model knowledge is compressed and\nlacks the full richness of the training data. We also find that\nchain-of-thought prompting and self-questioning converge, though\nself-questioning remains more effective for improving understanding of\ntechnical concepts. Notably, we uncover an asymmetry in prompting: smaller\nmodels often generate more fundamental, more open-ended, better-aligned\nquestions for mid-sized models than large models with better understanding do,\nrevealing a new strategy for cross-model collaboration. Altogether, our\nfindings establish self-questioning as both a practical mechanism for\nautomatically improving LLM comprehension, especially in domains with sparse\nand underrepresented knowledge, and a diagnostic probe of how internal and\nexternal knowledge are organized."}
{"id": "2505.12427", "pdf": "https://arxiv.org/pdf/2505.12427", "abs": "https://arxiv.org/abs/2505.12427", "authors": ["Siwei Xia", "Li Sun", "Tiantian Sun", "Qingli Li"], "title": "DragLoRA: Online Optimization of LoRA Adapters for Drag-based Image Editing in Diffusion Model", "categories": ["cs.CV"], "comment": "Accepted by ICML2025", "summary": "Drag-based editing within pretrained diffusion model provides a precise and\nflexible way to manipulate foreground objects. Traditional methods optimize the\ninput feature obtained from DDIM inversion directly, adjusting them iteratively\nto guide handle points towards target locations. However, these approaches\noften suffer from limited accuracy due to the low representation ability of the\nfeature in motion supervision, as well as inefficiencies caused by the large\nsearch space required for point tracking. To address these limitations, we\npresent DragLoRA, a novel framework that integrates LoRA (Low-Rank Adaptation)\nadapters into the drag-based editing pipeline. To enhance the training of LoRA\nadapters, we introduce an additional denoising score distillation loss which\nregularizes the online model by aligning its output with that of the original\nmodel. Additionally, we improve the consistency of motion supervision by\nadapting the input features using the updated LoRA, giving a more stable and\naccurate input feature for subsequent operations. Building on this, we design\nan adaptive optimization scheme that dynamically toggles between two modes,\nprioritizing efficiency without compromising precision. Extensive experiments\ndemonstrate that DragLoRA significantly enhances the control precision and\ncomputational efficiency for drag-based image editing. The Codes of DragLoRA\nare available at: https://github.com/Sylvie-X/DragLoRA."}
{"id": "2505.12454", "pdf": "https://arxiv.org/pdf/2505.12454", "abs": "https://arxiv.org/abs/2505.12454", "authors": ["Yuyang Ding", "Dan Qiao", "Juntao Li", "Jiajie Xu", "Pingfu Chao", "Xiaofang Zhou", "Min Zhang"], "title": "Towards DS-NER: Unveiling and Addressing Latent Noise in Distant Annotations", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Distantly supervised named entity recognition (DS-NER) has emerged as a cheap\nand convenient alternative to traditional human annotation methods, enabling\nthe automatic generation of training data by aligning text with external\nresources. Despite the many efforts in noise measurement methods, few works\nfocus on the latent noise distribution between different distant annotation\nmethods. In this work, we explore the effectiveness and robustness of DS-NER by\ntwo aspects: (1) distant annotation techniques, which encompasses both\ntraditional rule-based methods and the innovative large language model\nsupervision approach, and (2) noise assessment, for which we introduce a novel\nframework. This framework addresses the challenges by distinctly categorizing\nthem into the unlabeled-entity problem (UEP) and the noisy-entity problem\n(NEP), subsequently providing specialized solutions for each. Our proposed\nmethod achieves significant improvements on eight real-world distant\nsupervision datasets originating from three different data sources and\ninvolving four distinct annotation techniques, confirming its superiority over\ncurrent state-of-the-art methods."}
{"id": "2505.12431", "pdf": "https://arxiv.org/pdf/2505.12431", "abs": "https://arxiv.org/abs/2505.12431", "authors": ["Yating Liu", "Yujie Zhang", "Qi Yang", "Yiling Xu", "Zhu Li", "Ye-Kui Wang"], "title": "DPCD: A Quality Assessment Database for Dynamic Point Clouds", "categories": ["cs.CV", "cs.DB"], "comment": null, "summary": "Recently, the advancements in Virtual/Augmented Reality (VR/AR) have driven\nthe demand for Dynamic Point Clouds (DPC). Unlike static point clouds, DPCs are\ncapable of capturing temporal changes within objects or scenes, offering a more\naccurate simulation of the real world. While significant progress has been made\nin the quality assessment research of static point cloud, little study has been\ndone on Dynamic Point Cloud Quality Assessment (DPCQA), which hinders the\ndevelopment of quality-oriented applications, such as interframe compression\nand transmission in practical scenarios. In this paper, we introduce a\nlarge-scale DPCQA database, named DPCD, which includes 15 reference DPCs and\n525 distorted DPCs from seven types of lossy compression and noise distortion.\nBy rendering these samples to Processed Video Sequences (PVS), a comprehensive\nsubjective experiment is conducted to obtain Mean Opinion Scores (MOS) from 21\nviewers for analysis. The characteristic of contents, impact of various\ndistortions, and accuracy of MOSs are presented to validate the heterogeneity\nand reliability of the proposed database. Furthermore, we evaluate the\nperformance of several objective metrics on DPCD. The experiment results show\nthat DPCQA is more challenge than that of static point cloud. The DPCD, which\nserves as a catalyst for new research endeavors on DPCQA, is publicly available\nat https://huggingface.co/datasets/Olivialyt/DPCD."}
{"id": "2505.12474", "pdf": "https://arxiv.org/pdf/2505.12474", "abs": "https://arxiv.org/abs/2505.12474", "authors": ["Weixiao Zhou", "Junnan Zhu", "Gengyao Li", "Xianfu Cheng", "Xinnian Liang", "Feifei Zhai", "Zhoujun Li"], "title": "What are they talking about? Benchmarking Large Language Models for Knowledge-Grounded Discussion Summarization", "categories": ["cs.CL"], "comment": "Submitted to EMNLP 2025", "summary": "In this work, we investigate the performance of LLMs on a new task that\nrequires combining discussion with background knowledge for summarization. This\naims to address the limitation of outside observer confusion in existing\ndialogue summarization systems due to their reliance solely on discussion\ninformation. To achieve this, we model the task output as background and\nopinion summaries and define two standardized summarization patterns. To\nsupport assessment, we introduce the first benchmark comprising high-quality\nsamples consistently annotated by human experts and propose a novel\nhierarchical evaluation framework with fine-grained, interpretable metrics. We\nevaluate 12 LLMs under structured-prompt and self-reflection paradigms. Our\nfindings reveal: (1) LLMs struggle with background summary retrieval,\ngeneration, and opinion summary integration. (2) Even top LLMs achieve less\nthan 69% average performance across both patterns. (3) Current LLMs lack\nadequate self-evaluation and self-correction capabilities for this task."}
{"id": "2505.12433", "pdf": "https://arxiv.org/pdf/2505.12433", "abs": "https://arxiv.org/abs/2505.12433", "authors": ["Haodong Yang", "Lei Wang", "Md Zakir Hossain"], "title": "SRLoRA: Subspace Recomposition in Low-Rank Adaptation via Importance-Based Fusion and Reinitialization", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Research report", "summary": "Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient\nfine-tuning (PEFT) method that injects two trainable low-rank matrices (A and\nB) into frozen pretrained models. While efficient, LoRA constrains updates to a\nfixed low-rank subspace (Delta W = BA), which can limit representational\ncapacity and hinder downstream performance. We introduce Subspace Recomposition\nin Low-Rank Adaptation (SRLoRA) via importance-based fusion and\nreinitialization, a novel approach that enhances LoRA's expressiveness without\ncompromising its lightweight structure. SRLoRA assigns importance scores to\neach LoRA pair (a column of B and the corresponding row of A), and dynamically\nrecomposes the subspace during training. Less important pairs are fused into\nthe frozen backbone, freeing capacity to reinitialize new pairs along unused\nprincipal directions derived from the pretrained weight's singular value\ndecomposition. This mechanism enables continual subspace refreshment and richer\nadaptation over time, without increasing the number of trainable parameters. We\nevaluate SRLoRA on both language and vision tasks, including the GLUE benchmark\nand various image classification datasets. SRLoRA consistently achieves faster\nconvergence and improved accuracy over standard LoRA, demonstrating its\ngenerality, efficiency, and potential for broader PEFT applications."}
{"id": "2505.12476", "pdf": "https://arxiv.org/pdf/2505.12476", "abs": "https://arxiv.org/abs/2505.12476", "authors": ["Xiao Long", "Liansheng Zhuang", "Chen Shen", "Shaotian Yan", "Yifei Li", "Shafei Wang"], "title": "Enhancing Large Language Models with Reward-guided Tree Search for Knowledge Graph Question and Answering", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, large language models (LLMs) have demonstrated impressive\nperformance in Knowledge Graph Question Answering (KGQA) tasks, which aim to\nfind answers based on knowledge graphs (KGs) for natural language questions.\nExisting LLMs-based KGQA methods typically follow the Graph Retrieval-Augmented\nGeneration (GraphRAG) paradigm, which first retrieves reasoning paths from the\nlarge KGs, and then generates the answers based on them. However, these methods\nemphasize the exploration of new optimal reasoning paths in KGs while ignoring\nthe exploitation of historical reasoning paths, which may lead to sub-optimal\nreasoning paths. Additionally, the complex semantics contained in questions may\nlead to the retrieval of inaccurate reasoning paths. To address these issues,\nthis paper proposes a novel and training-free framework for KGQA tasks called\nReward-guided Tree Search on Graph (RTSoG). RTSoG decomposes an original\nquestion into a series of simpler and well-defined sub-questions to handle the\ncomplex semantics. Then, a Self-Critic Monte Carlo Tree Search (SC-MCTS) guided\nby a reward model is introduced to iteratively retrieve weighted reasoning\npaths as contextual knowledge. Finally, it stacks the weighted reasoning paths\naccording to their weights to generate the final answers. Extensive experiments\non four datasets demonstrate the effectiveness of RTSoG. Notably, it achieves\n8.7\\% and 7.0\\% performance improvement over the state-of-the-art method on the\nGrailQA and the WebQSP respectively."}
{"id": "2505.12434", "pdf": "https://arxiv.org/pdf/2505.12434", "abs": "https://arxiv.org/abs/2505.12434", "authors": ["Qi Wang", "Yanrui Yu", "Ye Yuan", "Rui Mao", "Tianfei Zhou"], "title": "VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning", "categories": ["cs.CV"], "comment": "Code: https://github.com/QiWang98/VideoRFT", "summary": "Reinforcement fine-tuning (RFT) has shown great promise in achieving\nhumanlevel reasoning capabilities of Large Language Models (LLMs), and has\nrecently been extended to MLLMs. Nevertheless, reasoning about videos, which is\na fundamental aspect of human intelligence, remains a persistent challenge due\nto the complex logic, temporal and causal structures inherent in video data. To\nfill this gap, we propose VIDEORFT, a novel approach that extends the RFT\nparadigm to cultivate human-like video reasoning capabilities in MLLMs.\nVIDEORFT follows the standard two-stage scheme in RFT: supervised fine-tuning\n(SFT) with chain-of-thought (CoT) annotations, followed by reinforcement\nlearning (RL) to improve generalization. A central challenge to achieve this in\nthe video domain lies in the scarcity of large-scale, high-quality video CoT\ndatasets. We address this by building a fully automatic CoT curation pipeline.\nFirst, we devise a cognitioninspired prompting strategy to elicit a reasoning\nLLM to generate preliminary CoTs based solely on rich, structured, and literal\nrepresentations of video content. Subsequently, these CoTs are revised by a\nvisual-language model conditioned on the actual video, ensuring visual\nconsistency and reducing visual hallucinations. This pipeline results in two\nnew datasets - VideoRFT-CoT-102K for SFT and VideoRFT-RL-310K for RL. To\nfurther strength the RL phase, we introduce a novel semantic-consistency reward\nthat explicitly promotes the alignment between textual reasoning with visual\nevidence. This reward encourages the model to produce coherent, context-aware\nreasoning outputs grounded in visual input. Extensive experiments show that\nVIDEORFT achieves state-of-the-art performance on six video reasoning\nbenchmarks."}
{"id": "2505.12495", "pdf": "https://arxiv.org/pdf/2505.12495", "abs": "https://arxiv.org/abs/2505.12495", "authors": ["Nikita Tatarinov", "Vidhyakshaya Kannan", "Haricharana Srinivasa", "Arnav Raj", "Harpreet Singh Anand", "Varun Singh", "Aditya Luthra", "Ravij Lade", "Agam Shah", "Sudheer Chava"], "title": "KG-QAGen: A Knowledge-Graph-Based Framework for Systematic Question Generation and Long-Context LLM Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "The increasing context length of modern language models has created a need\nfor evaluating their ability to retrieve and process information across\nextensive documents. While existing benchmarks test long-context capabilities,\nthey often lack a structured way to systematically vary question complexity. We\nintroduce KG-QAGen (Knowledge-Graph-based Question-Answer Generation), a\nframework that (1) extracts QA pairs at multiple complexity levels (2) by\nleveraging structured representations of financial agreements (3) along three\nkey dimensions -- multi-hop retrieval, set operations, and answer plurality --\nenabling fine-grained assessment of model performance across controlled\ndifficulty levels. Using this framework, we construct a dataset of 20,139 QA\npairs (the largest number among the long-context benchmarks) and open-source a\npart of it. We evaluate 13 proprietary and open-source LLMs and observe that\neven the best-performing models are struggling with set-based comparisons and\nmulti-hop logical inference. Our analysis reveals systematic failure modes tied\nto semantic misinterpretation and inability to handle implicit relations."}
{"id": "2505.12448", "pdf": "https://arxiv.org/pdf/2505.12448", "abs": "https://arxiv.org/abs/2505.12448", "authors": ["Yang Liu", "Ming Ma", "Xiaomin Yu", "Pengxiang Ding", "Han Zhao", "Mingyang Sun", "Siteng Huang", "Donglin Wang"], "title": "SSR: Enhancing Depth Perception in Vision-Language Models via Rationale-Guided Spatial Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Despite impressive advancements in Visual-Language Models (VLMs) for\nmulti-modal tasks, their reliance on RGB inputs limits precise spatial\nunderstanding. Existing methods for integrating spatial cues, such as point\nclouds or depth, either require specialized sensors or fail to effectively\nexploit depth information for higher-order reasoning. To this end, we propose a\nnovel Spatial Sense and Reasoning method, dubbed SSR, a novel framework that\ntransforms raw depth data into structured, interpretable textual rationales.\nThese textual rationales serve as meaningful intermediate representations to\nsignificantly enhance spatial reasoning capabilities. Additionally, we leverage\nknowledge distillation to compress the generated rationales into compact latent\nembeddings, which facilitate resource-efficient and plug-and-play integration\ninto existing VLMs without retraining. To enable comprehensive evaluation, we\nintroduce a new dataset named SSR-CoT, a million-scale visual-language\nreasoning dataset enriched with intermediate spatial reasoning annotations, and\npresent SSRBench, a comprehensive multi-task benchmark. Extensive experiments\non multiple benchmarks demonstrate SSR substantially improves depth utilization\nand enhances spatial reasoning, thereby advancing VLMs toward more human-like\nmulti-modal understanding. Our project page is at\nhttps://yliu-cs.github.io/SSR."}
{"id": "2505.12507", "pdf": "https://arxiv.org/pdf/2505.12507", "abs": "https://arxiv.org/abs/2505.12507", "authors": ["Xu Zheng", "Zhuomin Chen", "Esteban Schafir", "Sipeng Chen", "Hojat Allah Salehi", "Haifeng Chen", "Farhad Shirani", "Wei Cheng", "Dongsheng Luo"], "title": "LM$^2$otifs : An Explainable Framework for Machine-Generated Texts Detection", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "The impressive ability of large language models to generate natural text\nacross various tasks has led to critical challenges in authorship\nauthentication. Although numerous detection methods have been developed to\ndifferentiate between machine-generated texts (MGT) and human-generated texts\n(HGT), the explainability of these methods remains a significant gap.\nTraditional explainability techniques often fall short in capturing the complex\nword relationships that distinguish HGT from MGT. To address this limitation,\nwe present LM$^2$otifs, a novel explainable framework for MGT detection.\nInspired by probabilistic graphical models, we provide a theoretical rationale\nfor the effectiveness. LM$^2$otifs utilizes eXplainable Graph Neural Networks\nto achieve both accurate detection and interpretability. The LM$^2$otifs\npipeline operates in three key stages: first, it transforms text into graphs\nbased on word co-occurrence to represent lexical dependencies; second, graph\nneural networks are used for prediction; and third, a post-hoc explainability\nmethod extracts interpretable motifs, offering multi-level explanations from\nindividual words to sentence structures. Extensive experiments on multiple\nbenchmark datasets demonstrate the comparable performance of LM$^2$otifs. The\nempirical evaluation of the extracted explainable motifs confirms their\neffectiveness in differentiating HGT and MGT. Furthermore, qualitative analysis\nreveals distinct and visible linguistic fingerprints characteristic of MGT."}
{"id": "2505.12482", "pdf": "https://arxiv.org/pdf/2505.12482", "abs": "https://arxiv.org/abs/2505.12482", "authors": ["Wenchen Chen", "Yanmei Zhang", "Zhongwei Xiao", "Jianping Chu", "Xingbo Wang"], "title": "Spectral-Spatial Self-Supervised Learning for Few-Shot Hyperspectral Image Classification", "categories": ["cs.CV"], "comment": "https://github.com/Wenchen-Chen/S4L-FSC", "summary": "Few-shot classification of hyperspectral images (HSI) faces the challenge of\nscarce labeled samples. Self-Supervised learning (SSL) and Few-Shot Learning\n(FSL) offer promising avenues to address this issue. However, existing methods\noften struggle to adapt to the spatial geometric diversity of HSIs and lack\nsufficient spectral prior knowledge. To tackle these challenges, we propose a\nmethod, Spectral-Spatial Self-Supervised Learning for Few-Shot Hyperspectral\nImage Classification (S4L-FSC), aimed at improving the performance of few-shot\nHSI classification. Specifically, we first leverage heterogeneous datasets to\npretrain a spatial feature extractor using a designed Rotation-Mirror\nSelf-Supervised Learning (RM-SSL) method, combined with FSL. This approach\nenables the model to learn the spatial geometric diversity of HSIs using\nrotation and mirroring labels as supervisory signals, while acquiring\ntransferable spatial meta-knowledge through few-shot learning. Subsequently,\nhomogeneous datasets are utilized to pretrain a spectral feature extractor via\na combination of FSL and Masked Reconstruction Self-Supervised Learning\n(MR-SSL). The model learns to reconstruct original spectral information from\nrandomly masked spectral vectors, inferring spectral dependencies. In parallel,\nFSL guides the model to extract pixel-level discriminative features, thereby\nembedding rich spectral priors into the model. This spectral-spatial\npretraining method, along with the integration of knowledge from heterogeneous\nand homogeneous sources, significantly enhances model performance. Extensive\nexperiments on four HSI datasets demonstrate the effectiveness and superiority\nof the proposed S4L-FSC approach for few-shot HSI classification."}
{"id": "2505.12511", "pdf": "https://arxiv.org/pdf/2505.12511", "abs": "https://arxiv.org/abs/2505.12511", "authors": ["Yanting Li", "Jiyue Jiang", "Zikang Wang", "Ziqian Lin", "Dongchen He", "Yuheng Shan", "Yanruisheng Shao", "Jiayi Li", "Xiangyu Shi", "Jiuming Wang", "Yanyu Chen", "Yimin Fan", "Han Li", "Yu Li"], "title": "DS-ProGen: A Dual-Structure Deep Language Model for Functional Protein Design", "categories": ["cs.CL"], "comment": null, "summary": "Inverse Protein Folding (IPF) is a critical subtask in the field of protein\ndesign, aiming to engineer amino acid sequences capable of folding correctly\ninto a specified three-dimensional (3D) conformation. Although substantial\nprogress has been achieved in recent years, existing methods generally rely on\neither backbone coordinates or molecular surface features alone, which\nrestricts their ability to fully capture the complex chemical and geometric\nconstraints necessary for precise sequence prediction. To address this\nlimitation, we present DS-ProGen, a dual-structure deep language model for\nfunctional protein design, which integrates both backbone geometry and\nsurface-level representations. By incorporating backbone coordinates as well as\nsurface chemical and geometric descriptors into a next-amino-acid prediction\nparadigm, DS-ProGen is able to generate functionally relevant and structurally\nstable sequences while satisfying both global and local conformational\nconstraints. On the PRIDE dataset, DS-ProGen attains the current\nstate-of-the-art recovery rate of 61.47%, demonstrating the synergistic\nadvantage of multi-modal structural encoding in protein design. Furthermore,\nDS-ProGen excels in predicting interactions with a variety of biological\npartners, including ligands, ions, and RNA, confirming its robust functional\nretention capabilities."}
{"id": "2505.12486", "pdf": "https://arxiv.org/pdf/2505.12486", "abs": "https://arxiv.org/abs/2505.12486", "authors": ["Sangmin Jung", "Utkarsh Nath", "Yezhou Yang", "Giulia Pedrielli", "Joydeep Biswas", "Amy Zhang", "Hassan Ghasemzadeh", "Pavan Turaga"], "title": "Guiding Diffusion with Deep Geometric Moments: Balancing Fidelity and Variation", "categories": ["cs.CV"], "comment": "Accepted in CVPR Workshop GMCV 2025", "summary": "Text-to-image generation models have achieved remarkable capabilities in\nsynthesizing images, but often struggle to provide fine-grained control over\nthe output. Existing guidance approaches, such as segmentation maps and depth\nmaps, introduce spatial rigidity that restricts the inherent diversity of\ndiffusion models. In this work, we introduce Deep Geometric Moments (DGM) as a\nnovel form of guidance that encapsulates the subject's visual features and\nnuances through a learned geometric prior. DGMs focus specifically on the\nsubject itself compared to DINO or CLIP features, which suffer from\noveremphasis on global image features or semantics. Unlike ResNets, which are\nsensitive to pixel-wise perturbations, DGMs rely on robust geometric moments.\nOur experiments demonstrate that DGM effectively balance control and diversity\nin diffusion-based image generation, allowing a flexible control mechanism for\nsteering the diffusion process."}
{"id": "2505.12531", "pdf": "https://arxiv.org/pdf/2505.12531", "abs": "https://arxiv.org/abs/2505.12531", "authors": ["Navid Madani", "Rohini Srihari"], "title": "ESC-Judge: A Framework for Comparing Emotional Support Conversational Agents", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) increasingly power mental-health chatbots, yet\nthe field still lacks a scalable, theory-grounded way to decide which model is\nmost effective to deploy. We present ESC-Judge, the first end-to-end evaluation\nframework that (i) grounds head-to-head comparisons of emotional-support LLMs\nin Clara Hill's established Exploration-Insight-Action counseling model,\nproviding a structured and interpretable view of performance, and (ii) fully\nautomates the evaluation pipeline at scale. ESC-Judge operates in three stages:\nfirst, it synthesizes realistic help-seeker roles by sampling empirically\nsalient attributes such as stressors, personality, and life history; second, it\nhas two candidate support agents conduct separate sessions with the same role,\nisolating model-specific strategies; and third, it asks a specialized judge LLM\nto express pairwise preferences across rubric-anchored skills that span the\nExploration, Insight, and Action spectrum. In our study, ESC-Judge matched\nPhD-level annotators on 85 percent of Exploration, 83 percent of Insight, and\n86 percent of Action decisions, demonstrating human-level reliability at a\nfraction of the cost. All code, prompts, synthetic roles, transcripts, and\njudgment scripts are released to promote transparent progress in emotionally\nsupportive AI."}
{"id": "2505.12489", "pdf": "https://arxiv.org/pdf/2505.12489", "abs": "https://arxiv.org/abs/2505.12489", "authors": ["Shaobin Zhuang", "Zhipeng Huang", "Ying Zhang", "Fangyikang Wang", "Canmiao Fu", "Binxin Yang", "Chong Sun", "Chen Li", "Yali Wang"], "title": "Video-GPT via Next Clip Diffusion", "categories": ["cs.CV", "cs.AI"], "comment": "22 pages, 12 figures, 18 tables", "summary": "GPT has shown its remarkable success in natural language processing. However,\nthe language sequence is not sufficient to describe spatial-temporal details in\nthe visual world. Alternatively, the video sequence is good at capturing such\ndetails. Motivated by this fact, we propose a concise Video-GPT in this paper\nby treating video as new language for visual world modeling. By analogy to next\ntoken prediction in GPT, we introduce a novel next clip diffusion paradigm for\npretraining Video-GPT. Different from the previous works, this distinct\nparadigm allows Video-GPT to tackle both short-term generation and long-term\nprediction, by autoregressively denoising the noisy clip according to the clean\nclips in the history. Extensive experiments show our Video-GPT achieves the\nstate-of-the-art performance on video prediction, which is the key factor\ntowards world modeling (Physics-IQ Benchmark: Video-GPT 34.97 vs. Kling 23.64\nvs. Wan 20.89). Moreover, it can be well adapted on 6 mainstream video tasks in\nboth video generation and understanding, showing its great generalization\ncapacity in downstream. The project page is at https://Video-GPT.github.io."}
{"id": "2505.12533", "pdf": "https://arxiv.org/pdf/2505.12533", "abs": "https://arxiv.org/abs/2505.12533", "authors": ["Varvara Arzt", "Allan Hanbury", "Michael Wiegand", "Gábor Recski", "Terra Blevins"], "title": "Relation Extraction or Pattern Matching? Unravelling the Generalisation Limits of Language Models for Biographical RE", "categories": ["cs.CL"], "comment": null, "summary": "Analysing the generalisation capabilities of relation extraction (RE) models\nis crucial for assessing whether they learn robust relational patterns or rely\non spurious correlations. Our cross-dataset experiments find that RE models\nstruggle with unseen data, even within similar domains. Notably, higher\nintra-dataset performance does not indicate better transferability, instead\noften signaling overfitting to dataset-specific artefacts. Our results also\nshow that data quality, rather than lexical similarity, is key to robust\ntransfer, and the choice of optimal adaptation strategy depends on the quality\nof data available: while fine-tuning yields the best cross-dataset performance\nwith high-quality data, few-shot in-context learning (ICL) is more effective\nwith noisier data. However, even in these cases, zero-shot baselines\noccasionally outperform all cross-dataset results. Structural issues in RE\nbenchmarks, such as single-relation per sample constraints and non-standardised\nnegative class definitions, further hinder model transferability."}
{"id": "2505.12499", "pdf": "https://arxiv.org/pdf/2505.12499", "abs": "https://arxiv.org/abs/2505.12499", "authors": ["Jian Xiao", "Zijie Song", "Jialong Hu", "Hao Cheng", "Zhenzhen Hu", "Jia Li", "Richang Hong"], "title": "Rebalancing Contrastive Alignment with Learnable Semantic Gaps in Text-Video Retrieval", "categories": ["cs.CV", "cs.IR", "cs.MM"], "comment": null, "summary": "Recent advances in text-video retrieval have been largely driven by\ncontrastive learning frameworks. However, existing methods overlook a key\nsource of optimization tension: the separation between text and video\ndistributions in the representation space (referred to as the modality gap),\nand the prevalence of false negatives in batch sampling. These factors lead to\nconflicting gradients under the InfoNCE loss, impeding stable alignment. To\nmitigate this, we propose GARE, a Gap-Aware Retrieval framework that introduces\na learnable, pair-specific increment Delta_ij between text t_i and video v_j to\noffload the tension from the global anchor representation. We first derive the\nideal form of Delta_ij via a coupled multivariate first-order Taylor\napproximation of the InfoNCE loss under a trust-region constraint, revealing it\nas a mechanism for resolving gradient conflicts by guiding updates along a\nlocally optimal descent direction. Due to the high cost of directly computing\nDelta_ij, we introduce a lightweight neural module conditioned on the semantic\ngap between each video-text pair, enabling structure-aware correction guided by\ngradient supervision. To further stabilize learning and promote\ninterpretability, we regularize Delta using three components: a trust-region\nconstraint to prevent oscillation, a directional diversity term to promote\nsemantic coverage, and an information bottleneck to limit redundancy.\nExperiments across four retrieval benchmarks show that GARE consistently\nimproves alignment accuracy and robustness to noisy supervision, confirming the\neffectiveness of gap-aware tension mitigation."}
{"id": "2505.12543", "pdf": "https://arxiv.org/pdf/2505.12543", "abs": "https://arxiv.org/abs/2505.12543", "authors": ["Md Mehrab Tanjim", "Yeonjun In", "Xiang Chen", "Victor S. Bursztyn", "Ryan A. Rossi", "Sungchul Kim", "Guang-Jie Ren", "Vaishnavi Muppala", "Shun Jiang", "Yongsung Kim", "Chanyoung Park"], "title": "Disambiguation in Conversational Question Answering in the Era of LLM: A Survey", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Ambiguity remains a fundamental challenge in Natural Language Processing\n(NLP) due to the inherent complexity and flexibility of human language. With\nthe advent of Large Language Models (LLMs), addressing ambiguity has become\neven more critical due to their expanded capabilities and applications. In the\ncontext of Conversational Question Answering (CQA), this paper explores the\ndefinition, forms, and implications of ambiguity for language driven systems,\nparticularly in the context of LLMs. We define key terms and concepts,\ncategorize various disambiguation approaches enabled by LLMs, and provide a\ncomparative analysis of their advantages and disadvantages. We also explore\npublicly available datasets for benchmarking ambiguity detection and resolution\ntechniques and highlight their relevance for ongoing research. Finally, we\nidentify open problems and future research directions, proposing areas for\nfurther investigation. By offering a comprehensive review of current research\non ambiguities and disambiguation with LLMs, we aim to contribute to the\ndevelopment of more robust and reliable language systems."}
{"id": "2505.12513", "pdf": "https://arxiv.org/pdf/2505.12513", "abs": "https://arxiv.org/abs/2505.12513", "authors": ["Yang Mu", "Zhitong Xiong", "Yi Wang", "Muhammad Shahzad", "Franz Essl", "Mark van Kleunen", "Xiao Xiang Zhu"], "title": "GlobalGeoTree: A Multi-Granular Vision-Language Dataset for Global Tree Species Classification", "categories": ["cs.CV"], "comment": null, "summary": "Global tree species mapping using remote sensing data is vital for\nbiodiversity monitoring, forest management, and ecological research. However,\nprogress in this field has been constrained by the scarcity of large-scale,\nlabeled datasets. To address this, we introduce GlobalGeoTree, a comprehensive\nglobal dataset for tree species classification. GlobalGeoTree comprises 6.3\nmillion geolocated tree occurrences, spanning 275 families, 2,734 genera, and\n21,001 species across the hierarchical taxonomic levels. Each sample is paired\nwith Sentinel-2 image time series and 27 auxiliary environmental variables,\nencompassing bioclimatic, geographic, and soil data. The dataset is partitioned\ninto GlobalGeoTree-6M for model pretraining and curated evaluation subsets,\nprimarily GlobalGeoTree-10kEval for zero-shot and few-shot benchmarking. To\ndemonstrate the utility of the dataset, we introduce a baseline model,\nGeoTreeCLIP, which leverages paired remote sensing data and taxonomic text\nlabels within a vision-language framework pretrained on GlobalGeoTree-6M.\nExperimental results show that GeoTreeCLIP achieves substantial improvements in\nzero- and few-shot classification on GlobalGeoTree-10kEval over existing\nadvanced models. By making the dataset, models, and code publicly available, we\naim to establish a benchmark to advance tree species classification and foster\ninnovation in biodiversity research and ecological applications."}
{"id": "2505.12545", "pdf": "https://arxiv.org/pdf/2505.12545", "abs": "https://arxiv.org/abs/2505.12545", "authors": ["Yang Zhao", "Pu Wang", "Yibo Zhao", "Hongru Du", "Hao", "Yang"], "title": "Towards Reliable and Interpretable Traffic Crash Pattern Prediction and Safety Interventions Using Customized Large Language Models", "categories": ["cs.CL"], "comment": "Last revised 13 Feb 2025. Under review in Nature portfolio", "summary": "Predicting crash events is crucial for understanding crash distributions and\ntheir contributing factors, thereby enabling the design of proactive traffic\nsafety policy interventions. However, existing methods struggle to interpret\nthe complex interplay among various sources of traffic crash data, including\nnumeric characteristics, textual reports, crash imagery, environmental\nconditions, and driver behavior records. As a result, they often fail to\ncapture the rich semantic information and intricate interrelationships embedded\nin these diverse data sources, limiting their ability to identify critical\ncrash risk factors. In this research, we propose TrafficSafe, a framework that\nadapts LLMs to reframe crash prediction and feature attribution as text-based\nreasoning. A multi-modal crash dataset including 58,903 real-world reports\ntogether with belonged infrastructure, environmental, driver, and vehicle\ninformation is collected and textualized into TrafficSafe Event Dataset. By\ncustomizing and fine-tuning LLMs on this dataset, the TrafficSafe LLM achieves\na 42% average improvement in F1-score over baselines. To interpret these\npredictions and uncover contributing factors, we introduce TrafficSafe\nAttribution, a sentence-level feature attribution framework enabling\nconditional risk analysis. Findings show that alcohol-impaired driving is the\nleading factor in severe crashes, with aggressive and impairment-related\nbehaviors having nearly twice the contribution for severe crashes compared to\nother driver behaviors. Furthermore, TrafficSafe Attribution highlights pivotal\nfeatures during model training, guiding strategic crash data collection for\niterative performance improvements. The proposed TrafficSafe offers a\ntransformative leap in traffic safety research, providing a blueprint for\ntranslating advanced AI technologies into responsible, actionable, and\nlife-saving outcomes."}
{"id": "2505.12532", "pdf": "https://arxiv.org/pdf/2505.12532", "abs": "https://arxiv.org/abs/2505.12532", "authors": ["Ahmet Bilican", "M. Akın Yılmaz", "A. Murat Tekalp", "R. Gökberk Cinbiş"], "title": "Exploring Sparsity for Parameter Efficient Fine Tuning Using Wavelets", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV", "eess.SP"], "comment": null, "summary": "Efficiently adapting large foundation models is critical, especially with\ntight compute and memory budgets. Parameter-Efficient Fine-Tuning (PEFT)\nmethods such as LoRA offer limited granularity and effectiveness in\nfew-parameter regimes. We propose Wavelet Fine-Tuning (WaveFT), a novel PEFT\nmethod that learns highly sparse updates in the wavelet domain of residual\nmatrices. WaveFT allows precise control of trainable parameters, offering\nfine-grained capacity adjustment and excelling with remarkably low parameter\ncount, potentially far fewer than LoRA's minimum -- ideal for extreme\nparameter-efficient scenarios. In order to demonstrate the effect of the\nwavelet transform, we compare WaveFT with a special case, called SHiRA, that\nentails applying sparse updates directly in the weight domain. Evaluated on\npersonalized text-to-image generation using Stable Diffusion XL as baseline,\nWaveFT significantly outperforms LoRA and other PEFT methods, especially at low\nparameter counts; achieving superior subject fidelity, prompt alignment, and\nimage diversity."}
{"id": "2505.12546", "pdf": "https://arxiv.org/pdf/2505.12546", "abs": "https://arxiv.org/abs/2505.12546", "authors": ["A. Feder Cooper", "Aaron Gokaslan", "Amy B. Cyphert", "Christopher De Sa", "Mark A. Lemley", "Daniel E. Ho", "Percy Liang"], "title": "Extracting memorized pieces of (copyrighted) books from open-weight language models", "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "Plaintiffs and defendants in copyright lawsuits over generative AI often make\nsweeping, opposing claims about the extent to which large language models\n(LLMs) have memorized plaintiffs' protected expression. Drawing on adversarial\nML and copyright law, we show that these polarized positions dramatically\noversimplify the relationship between memorization and copyright. To do so, we\nleverage a recent probabilistic extraction technique to extract pieces of the\nBooks3 dataset from 13 open-weight LLMs. Through numerous experiments, we show\nthat it's possible to extract substantial parts of at least some books from\ndifferent LLMs. This is evidence that the LLMs have memorized the extracted\ntext; this memorized content is copied inside the model parameters. But the\nresults are complicated: the extent of memorization varies both by model and by\nbook. With our specific experiments, we find that the largest LLMs don't\nmemorize most books -- either in whole or in part. However, we also find that\nLlama 3.1 70B memorizes some books, like Harry Potter and 1984, almost\nentirely. We discuss why our results have significant implications for\ncopyright cases, though not ones that unambiguously favor either side."}
{"id": "2505.12547", "pdf": "https://arxiv.org/pdf/2505.12547", "abs": "https://arxiv.org/abs/2505.12547", "authors": ["Florent Chiaroni", "Ali Ayub", "Ola Ahmad"], "title": "ProMi: An Efficient Prototype-Mixture Baseline for Few-Shot Segmentation with Bounding-Box Annotations", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "In robotics applications, few-shot segmentation is crucial because it allows\nrobots to perform complex tasks with minimal training data, facilitating their\nadaptation to diverse, real-world environments. However, pixel-level\nannotations of even small amount of images is highly time-consuming and costly.\nIn this paper, we present a novel few-shot binary segmentation method based on\nbounding-box annotations instead of pixel-level labels. We introduce, ProMi, an\nefficient prototype-mixture-based method that treats the background class as a\nmixture of distributions. Our approach is simple, training-free, and effective,\naccommodating coarse annotations with ease. Compared to existing baselines,\nProMi achieves the best results across different datasets with significant\ngains, demonstrating its effectiveness. Furthermore, we present qualitative\nexperiments tailored to real-world mobile robot tasks, demonstrating the\napplicability of our approach in such scenarios. Our code:\nhttps://github.com/ThalesGroup/promi."}
{"id": "2505.12560", "pdf": "https://arxiv.org/pdf/2505.12560", "abs": "https://arxiv.org/abs/2505.12560", "authors": ["Hiram Ring"], "title": "The taggedPBC: Annotating a massive parallel corpus for crosslinguistic investigations", "categories": ["cs.CL"], "comment": null, "summary": "Existing datasets available for crosslinguistic investigations have tended to\nfocus on large amounts of data for a small group of languages or a small amount\nof data for a large number of languages. This means that claims based on these\ndatasets are limited in what they reveal about universal properties of the\nhuman language faculty. While this has begun to change through the efforts of\nprojects seeking to develop tagged corpora for a large number of languages,\nsuch efforts are still constrained by limits on resources. The current paper\nreports on a large automatically tagged parallel dataset which has been\ndeveloped to partially address this issue. The taggedPBC contains more than\n1,800 sentences of pos-tagged parallel text data from over 1,500 languages,\nrepresenting 133 language families and 111 isolates, dwarfing previously\navailable resources. The accuracy of tags in this dataset is shown to correlate\nwell with both existing SOTA taggers for high-resource languages (SpaCy,\nTrankit) as well as hand-tagged corpora (Universal Dependencies Treebanks).\nAdditionally, a novel measure derived from this dataset, the N1 ratio,\ncorrelates with expert determinations of word order in three typological\ndatabases (WALS, Grambank, Autotyp) such that a Gaussian Naive Bayes classifier\ntrained on this feature can accurately identify basic word order for languages\nnot in those databases. While much work is still needed to expand and develop\nthis dataset, the taggedPBC is an important step to enable corpus-based\ncrosslinguistic investigations, and is made available for research and\ncollaboration via GitHub."}
{"id": "2505.12549", "pdf": "https://arxiv.org/pdf/2505.12549", "abs": "https://arxiv.org/abs/2505.12549", "authors": ["Dominic Maggio", "Hyungtae Lim", "Luca Carlone"], "title": "VGGT-SLAM: Dense RGB SLAM Optimized on the SL(4) Manifold", "categories": ["cs.CV"], "comment": null, "summary": "We present VGGT-SLAM, a dense RGB SLAM system constructed by incrementally\nand globally aligning submaps created from the feed-forward scene\nreconstruction approach VGGT using only uncalibrated monocular cameras. While\nrelated works align submaps using similarity transforms (i.e., translation,\nrotation, and scale), we show that such approaches are inadequate in the case\nof uncalibrated cameras. In particular, we revisit the idea of reconstruction\nambiguity, where given a set of uncalibrated cameras with no assumption on the\ncamera motion or scene structure, the scene can only be reconstructed up to a\n15-degrees-of-freedom projective transformation of the true geometry. This\ninspires us to recover a consistent scene reconstruction across submaps by\noptimizing over the SL(4) manifold, thus estimating 15-degrees-of-freedom\nhomography transforms between sequential submaps while accounting for potential\nloop closure constraints. As verified by extensive experiments, we demonstrate\nthat VGGT-SLAM achieves improved map quality using long video sequences that\nare infeasible for VGGT due to its high GPU requirements."}
{"id": "2505.12568", "pdf": "https://arxiv.org/pdf/2505.12568", "abs": "https://arxiv.org/abs/2505.12568", "authors": ["Lekang Jiang", "Chengzu Li", "Stephan Goetz"], "title": "Enriching Patent Claim Generation with European Patent Dataset", "categories": ["cs.CL"], "comment": "18 pages, 13 tables, 4 figures", "summary": "Drafting patent claims is time-intensive, costly, and requires professional\nskill. Therefore, researchers have investigated large language models (LLMs) to\nassist inventors in writing claims. However, existing work has largely relied\non datasets from the United States Patent and Trademark Office (USPTO). To\nenlarge research scope regarding various jurisdictions, drafting conventions,\nand legal standards, we introduce EPD, a European patent dataset. EPD presents\nrich textual data and structured metadata to support multiple patent-related\ntasks, including claim generation. This dataset enriches the field in three\ncritical aspects: (1) Jurisdictional diversity: Patents from different offices\nvary in legal and drafting conventions. EPD fills a critical gap by providing a\nbenchmark for European patents to enable more comprehensive evaluation. (2)\nQuality improvement: EPD offers high-quality granted patents with finalized and\nlegally approved texts, whereas others consist of patent applications that are\nunexamined or provisional. Experiments show that LLMs fine-tuned on EPD\nsignificantly outperform those trained on previous datasets and even GPT-4o in\nclaim quality and cross-domain generalization. (3) Real-world simulation: We\npropose a difficult subset of EPD to better reflect real-world challenges of\nclaim generation. Results reveal that all tested LLMs perform substantially\nworse on these challenging samples, which highlights the need for future\nresearch."}
{"id": "2505.12580", "pdf": "https://arxiv.org/pdf/2505.12580", "abs": "https://arxiv.org/abs/2505.12580", "authors": ["Priyank Pathak", "Yogesh S Rawat"], "title": "Coarse Attribute Prediction with Task Agnostic Distillation for Real World Clothes Changing ReID", "categories": ["cs.CV"], "comment": null, "summary": "This work focuses on Clothes Changing Re-IDentification (CC-ReID) for the\nreal world. Existing works perform well with high-quality (HQ) images, but\nstruggle with low-quality (LQ) where we can have artifacts like pixelation,\nout-of-focus blur, and motion blur. These artifacts introduce noise to not only\nexternal biometric attributes (e.g. pose, body shape, etc.) but also corrupt\nthe model's internal feature representation. Models usually cluster LQ image\nfeatures together, making it difficult to distinguish between them, leading to\nincorrect matches. We propose a novel framework Robustness against Low-Quality\n(RLQ) to improve CC-ReID model on real-world data. RLQ relies on Coarse\nAttributes Prediction (CAP) and Task Agnostic Distillation (TAD) operating in\nalternate steps in a novel training mechanism. CAP enriches the model with\nexternal fine-grained attributes via coarse predictions, thereby reducing the\neffect of noisy inputs. On the other hand, TAD enhances the model's internal\nfeature representation by bridging the gap between HQ and LQ features, via an\nexternal dataset through task-agnostic self-supervision and distillation. RLQ\noutperforms the existing approaches by 1.6%-2.9% Top-1 on real-world datasets\nlike LaST, and DeepChange, while showing consistent improvement of 5.3%-6%\nTop-1 on PRCC with competitive performance on LTCC. *The code will be made\npublic soon.*"}
{"id": "2505.12572", "pdf": "https://arxiv.org/pdf/2505.12572", "abs": "https://arxiv.org/abs/2505.12572", "authors": ["Hanwen Shen", "Ting Ying"], "title": "Measuring Information Distortion in Hierarchical Ultra long Novel Generation:The Optimal Expansion Ratio", "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "Writing novels with Large Language Models (LLMs) raises a critical question:\nhow much human-authored outline is necessary to generate high-quality\nmillion-word novels? While frameworks such as DOME, Plan&Write, and Long Writer\nhave improved stylistic coherence and logical consistency, they primarily\ntarget shorter novels (10k--100k words), leaving ultra-long generation largely\nunexplored. Drawing on insights from recent text compression methods like\nLLMZip and LLM2Vec, we conduct an information-theoretic analysis that\nquantifies distortion occurring when LLMs compress and reconstruct ultra-long\nnovels under varying compression-expansion ratios. We introduce a hierarchical\ntwo-stage generation pipeline (outline -> detailed outline -> manuscript) and\nfind an optimal outline length that balances information preservation with\nhuman effort. Through extensive experimentation with Chinese novels, we\nestablish that a two-stage hierarchical outline approach significantly reduces\nsemantic distortion compared to single-stage methods. Our findings provide\nempirically-grounded guidance for authors and researchers collaborating with\nLLMs to create million-word novels."}
{"id": "2505.12588", "pdf": "https://arxiv.org/pdf/2505.12588", "abs": "https://arxiv.org/abs/2505.12588", "authors": ["Samya Bagchi", "Peter Anastasiou", "Matthew Tetlow", "Tat-Jun Chin", "Yasir Latif"], "title": "Event-based Star Tracking under Spacecraft Jitter: the e-STURT Dataset", "categories": ["cs.CV", "eess.SP"], "comment": null, "summary": "Jitter degrades a spacecraft's fine-pointing ability required for optical\ncommunication, earth observation, and space domain awareness. Development of\njitter estimation and compensation algorithms requires high-fidelity sensor\nobservations representative of on-board jitter. In this work, we present the\nEvent-based Star Tracking Under Jitter (e-STURT) dataset -- the first event\ncamera based dataset of star observations under controlled jitter conditions.\nSpecialized hardware employed for the dataset emulates an event-camera\nundergoing on-board jitter. While the event camera provides asynchronous, high\ntemporal resolution star observations, systematic and repeatable jitter is\nintroduced using a micrometer accurate piezoelectric actuator. Various jitter\nsources are simulated using distinct frequency bands and utilizing both axes of\nmotion. Ground-truth jitter is captured in hardware from the piezoelectric\nactuator. The resulting dataset consists of 200 sequences and is made publicly\navailable. This work highlights the dataset generation process, technical\nchallenges and the resulting limitations. To serve as a baseline, we propose a\nhigh-frequency jitter estimation algorithm that operates directly on the event\nstream. The e-STURT dataset will enable the development of jitter aware\nalgorithms for mission critical event-based space sensing applications."}
{"id": "2505.12584", "pdf": "https://arxiv.org/pdf/2505.12584", "abs": "https://arxiv.org/abs/2505.12584", "authors": ["Omar Mahmoud", "Buddhika Laknath Semage", "Thommen George Karimpanal", "Santu Rana"], "title": "Improving Multilingual Language Models by Aligning Representations through Steering", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we investigate how large language models (LLMS) process\nnon-English tokens within their layer representations, an open question despite\nsignificant advancements in the field. Using representation steering,\nspecifically by adding a learned vector to a single model layer's activations,\nwe demonstrate that steering a single model layer can notably enhance\nperformance. Our analysis shows that this approach achieves results comparable\nto translation baselines and surpasses state of the art prompt optimization\nmethods. Additionally, we highlight how advanced techniques like supervised\nfine tuning (\\textsc{sft}) and reinforcement learning from human feedback\n(\\textsc{rlhf}) improve multilingual capabilities by altering representation\nspaces. We further illustrate how these methods align with our approach to\nreshaping LLMS layer representations."}
{"id": "2505.12589", "pdf": "https://arxiv.org/pdf/2505.12589", "abs": "https://arxiv.org/abs/2505.12589", "authors": ["Bo Liu", "Pengfei Qiao", "Minhan Ma", "Xuange Zhang", "Yinan Tang", "Peng Xu", "Kun Liu", "Tongtong Yuan"], "title": "SurveillanceVQA-589K: A Benchmark for Comprehensive Surveillance Video-Language Understanding with Large Models", "categories": ["cs.CV"], "comment": "The dataset and code are publicly available at:\n  https://huggingface.co/datasets/fei213/SurveillanceVQA-589K", "summary": "Understanding surveillance video content remains a critical yet underexplored\nchallenge in vision-language research, particularly due to its real-world\ncomplexity, irregular event dynamics, and safety-critical implications. In this\nwork, we introduce SurveillanceVQA-589K, the largest open-ended video question\nanswering benchmark tailored to the surveillance domain. The dataset comprises\n589,380 QA pairs spanning 12 cognitively diverse question types, including\ntemporal reasoning, causal inference, spatial understanding, and anomaly\ninterpretation, across both normal and abnormal video scenarios. To construct\nthe benchmark at scale, we design a hybrid annotation pipeline that combines\ntemporally aligned human-written captions with Large Vision-Language\nModel-assisted QA generation using prompt-based techniques. We also propose a\nmulti-dimensional evaluation protocol to assess contextual, temporal, and\ncausal comprehension. We evaluate eight LVLMs under this framework, revealing\nsignificant performance gaps, especially in causal and anomaly-related tasks,\nunderscoring the limitations of current models in real-world surveillance\ncontexts. Our benchmark provides a practical and comprehensive resource for\nadvancing video-language understanding in safety-critical applications such as\nintelligent monitoring, incident analysis, and autonomous decision-making."}
{"id": "2505.12587", "pdf": "https://arxiv.org/pdf/2505.12587", "abs": "https://arxiv.org/abs/2505.12587", "authors": ["Aditeya Baral", "Allen George Ajith", "Roshan Nayak", "Mrityunjay Abhijeet Bhanja"], "title": "CMLFormer: A Dual Decoder Transformer with Switching Point Learning for Code-Mixed Language Modeling", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Code-mixed languages, characterized by frequent within-sentence language\ntransitions, present structural challenges that standard language models fail\nto address. In this work, we propose CMLFormer, an enhanced multi-layer\ndual-decoder Transformer with a shared encoder and synchronized decoder\ncross-attention, designed to model the linguistic and semantic dynamics of\ncode-mixed text. CMLFormer is pre-trained on an augmented Hinglish corpus with\nswitching point and translation annotations with multiple new objectives\nspecifically aimed at capturing switching behavior, cross-lingual structure,\nand code-mixing complexity. Our experiments show that CMLFormer improves F1\nscore, precision, and accuracy over other approaches on the HASOC-2021\nbenchmark under select pre-training setups. Attention analyses further show\nthat it can identify and attend to switching points, validating its sensitivity\nto code-mixed structure. These results demonstrate the effectiveness of\nCMLFormer's architecture and multi-task pre-training strategy for modeling\ncode-mixed languages."}
{"id": "2505.12593", "pdf": "https://arxiv.org/pdf/2505.12593", "abs": "https://arxiv.org/abs/2505.12593", "authors": ["Mia Thomas", "Trevor Ablett", "Jonathan Kelly"], "title": "Learning Cross-Spectral Point Features with Task-Oriented Training", "categories": ["cs.CV"], "comment": "Proceedings of the {IEEE} International Conference on Robotics and\n  Automation {(ICRA'25)} Thermal Infrared in Robotics (TIRO) Workshop, Atlanta,\n  Georgia, USA, May 19, 2025", "summary": "Unmanned aerial vehicles (UAVs) enable operations in remote and hazardous\nenvironments, yet the visible-spectrum, camera-based navigation systems often\nrelied upon by UAVs struggle in low-visibility conditions. Thermal cameras,\nwhich capture long-wave infrared radiation, are able to function effectively in\ndarkness and smoke, where visible-light cameras fail. This work explores\nlearned cross-spectral (thermal-visible) point features as a means to integrate\nthermal imagery into established camera-based navigation systems. Existing\nmethods typically train a feature network's detection and description outputs\ndirectly, which often focuses training on image regions where thermal and\nvisible-spectrum images exhibit similar appearance. Aiming to more fully\nutilize the available data, we propose a method to train the feature network on\nthe tasks of matching and registration. We run our feature network on\nthermal-visible image pairs, then feed the network response into a\ndifferentiable registration pipeline. Losses are applied to the matching and\nregistration estimates of this pipeline. Our selected model, trained on the\ntask of matching, achieves a registration error (corner error) below 10 pixels\nfor more than 75% of estimates on the MultiPoint dataset. We further\ndemonstrate that our model can also be used with a classical pipeline for\nmatching and registration."}
{"id": "2505.12592", "pdf": "https://arxiv.org/pdf/2505.12592", "abs": "https://arxiv.org/abs/2505.12592", "authors": ["Sullam Jeoung", "Yueyan Chen", "Yi Zhang", "Shuai Wang", "Haibo Ding", "Lin Lee Cheong"], "title": "PromptPrism: A Linguistically-Inspired Taxonomy for Prompts", "categories": ["cs.CL"], "comment": null, "summary": "Prompts are the interface for eliciting the capabilities of large language\nmodels (LLMs). Understanding their structure and components is critical for\nanalyzing LLM behavior and optimizing performance. However, the field lacks a\ncomprehensive framework for systematic prompt analysis and understanding. We\nintroduce PromptPrism, a linguistically-inspired taxonomy that enables prompt\nanalysis across three hierarchical levels: functional structure, semantic\ncomponent, and syntactic pattern. We show the practical utility of PromptPrism\nby applying it to three applications: (1) a taxonomy-guided prompt refinement\napproach that automatically improves prompt quality and enhances model\nperformance across a range of tasks; (2) a multi-dimensional dataset profiling\nmethod that extracts and aggregates structural, semantic, and syntactic\ncharacteristics from prompt datasets, enabling comprehensive analysis of prompt\ndistributions and patterns; (3) a controlled experimental framework for prompt\nsensitivity analysis by quantifying the impact of semantic reordering and\ndelimiter modifications on LLM performance. Our experimental results validate\nthe effectiveness of our taxonomy across these applications, demonstrating that\nPromptPrism provides a foundation for refining, profiling, and analyzing\nprompts."}
{"id": "2505.12605", "pdf": "https://arxiv.org/pdf/2505.12605", "abs": "https://arxiv.org/abs/2505.12605", "authors": ["Thong Nguyen", "Zhiyuan Hu", "Xu Lin", "Cong-Duy Nguyen", "See-Kiong Ng", "Luu Anh Tuan"], "title": "Temporal-Oriented Recipe for Transferring Large Vision-Language Model to Video Understanding", "categories": ["cs.CV"], "comment": "In Progress", "summary": "Recent years have witnessed outstanding advances of large vision-language\nmodels (LVLMs). In order to tackle video understanding, most of them depend\nupon their implicit temporal understanding capacity. As such, they have not\ndeciphered important components that contribute to temporal understanding\nability, which might limit the potential of these LVLMs for video\nunderstanding. In this work, we conduct a thorough empirical study to demystify\ncrucial components that influence the temporal understanding of LVLMs. Our\nempirical study reveals that significant impacts are centered around the\nintermediate interface between the visual encoder and the large language model.\nBuilding on these insights, we propose a temporal-oriented recipe that\nencompasses temporal-oriented training schemes and an upscaled interface. Our\nfinal model developed using our recipe significantly enhances previous LVLMs on\nstandard video understanding tasks."}
{"id": "2505.12594", "pdf": "https://arxiv.org/pdf/2505.12594", "abs": "https://arxiv.org/abs/2505.12594", "authors": ["Tiankai Yang", "Junjun Liu", "Wingchun Siu", "Jiahang Wang", "Zhuangzhuang Qian", "Chanjuan Song", "Cheng Cheng", "Xiyang Hu", "Yue Zhao"], "title": "AD-AGENT: A Multi-agent Framework for End-to-end Anomaly Detection", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Anomaly detection (AD) is essential in areas such as fraud detection, network\nmonitoring, and scientific research. However, the diversity of data modalities\nand the increasing number of specialized AD libraries pose challenges for\nnon-expert users who lack in-depth library-specific knowledge and advanced\nprogramming skills. To tackle this, we present AD-AGENT, an LLM-driven\nmulti-agent framework that turns natural-language instructions into fully\nexecutable AD pipelines. AD-AGENT coordinates specialized agents for intent\nparsing, data preparation, library and model selection, documentation mining,\nand iterative code generation and debugging. Using a shared short-term\nworkspace and a long-term cache, the agents integrate popular AD libraries like\nPyOD, PyGOD, and TSLib into a unified workflow. Experiments demonstrate that\nAD-AGENT produces reliable scripts and recommends competitive models across\nlibraries. The system is open-sourced to support further research and practical\napplications in AD."}
{"id": "2505.12606", "pdf": "https://arxiv.org/pdf/2505.12606", "abs": "https://arxiv.org/abs/2505.12606", "authors": ["Shiyu Xuan", "Zechao Li", "Jinhui Tang"], "title": "Diff-MM: Exploring Pre-trained Text-to-Image Generation Model for Unified Multi-modal Object Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Multi-modal object tracking integrates auxiliary modalities such as depth,\nthermal infrared, event flow, and language to provide additional information\nbeyond RGB images, showing great potential in improving tracking stabilization\nin complex scenarios. Existing methods typically start from an RGB-based\ntracker and learn to understand auxiliary modalities only from training data.\nConstrained by the limited multi-modal training data, the performance of these\nmethods is unsatisfactory. To alleviate this limitation, this work proposes a\nunified multi-modal tracker Diff-MM by exploiting the multi-modal understanding\ncapability of the pre-trained text-to-image generation model. Diff-MM leverages\nthe UNet of pre-trained Stable Diffusion as a tracking feature extractor\nthrough the proposed parallel feature extraction pipeline, which enables\npairwise image inputs for object tracking. We further introduce a multi-modal\nsub-module tuning method that learns to gain complementary information between\ndifferent modalities. By harnessing the extensive prior knowledge in the\ngeneration model, we achieve a unified tracker with uniform parameters for\nRGB-N/D/T/E tracking. Experimental results demonstrate the promising\nperformance of our method compared with recently proposed trackers, e.g., its\nAUC outperforms OneTracker by 8.3% on TNL2K."}
{"id": "2505.12616", "pdf": "https://arxiv.org/pdf/2505.12616", "abs": "https://arxiv.org/abs/2505.12616", "authors": ["Shujauddin Syed", "Ted Pedersen"], "title": "Duluth at SemEval-2025 Task 7: TF-IDF with Optimized Vector Dimensions for Multilingual Fact-Checked Claim Retrieval", "categories": ["cs.CL", "68T50"], "comment": "SemEval-2025", "summary": "This paper presents the Duluth approach to the SemEval-2025 Task 7 on\nMultilingual and Crosslingual Fact-Checked Claim Retrieval. We implemented a\nTF-IDF-based retrieval system with experimentation on vector dimensions and\ntokenization strategies. Our best-performing configuration used word-level\ntokenization with a vocabulary size of 15,000 features, achieving an average\nsuccess@10 score of 0.78 on the development set and 0.69 on the test set across\nten languages. Our system showed stronger performance on higher-resource\nlanguages but still lagged significantly behind the top-ranked system, which\nachieved 0.96 average success@10. Our findings suggest that though advanced\nneural architectures are increasingly dominant in multilingual retrieval tasks,\nproperly optimized traditional methods like TF-IDF remain competitive\nbaselines, especially in limited compute resource scenarios."}
{"id": "2505.12620", "pdf": "https://arxiv.org/pdf/2505.12620", "abs": "https://arxiv.org/abs/2505.12620", "authors": ["Haiquan Wen", "Yiwei He", "Zhenglin Huang", "Tianxiao Li", "Zihan YU", "Xingru Huang", "Lu Qi", "Baoyuan Wu", "Xiangtai Li", "Guangliang Cheng"], "title": "BusterX: MLLM-Powered AI-Generated Video Forgery Detection and Explanation", "categories": ["cs.CV"], "comment": null, "summary": "Advances in AI generative models facilitate super-realistic video synthesis,\namplifying misinformation risks via social media and eroding trust in digital\ncontent. Several research works have explored new deepfake detection methods on\nAI-generated images to alleviate these risks. However, with the fast\ndevelopment of video generation models, such as Sora and WanX, there is\ncurrently a lack of large-scale, high-quality AI-generated video datasets for\nforgery detection. In addition, existing detection approaches predominantly\ntreat the task as binary classification, lacking explainability in model\ndecision-making and failing to provide actionable insights or guidance for the\npublic. To address these challenges, we propose \\textbf{GenBuster-200K}, a\nlarge-scale AI-generated video dataset featuring 200K high-resolution video\nclips, diverse latest generative techniques, and real-world scenes. We further\nintroduce \\textbf{BusterX}, a novel AI-generated video detection and\nexplanation framework leveraging multimodal large language model (MLLM) and\nreinforcement learning for authenticity determination and explainable\nrationale. To our knowledge, GenBuster-200K is the {\\it \\textbf{first}}\nlarge-scale, high-quality AI-generated video dataset that incorporates the\nlatest generative techniques for real-world scenarios. BusterX is the {\\it\n\\textbf{first}} framework to integrate MLLM with reinforcement learning for\nexplainable AI-generated video detection. Extensive comparisons with\nstate-of-the-art methods and ablation studies validate the effectiveness and\ngeneralizability of BusterX. The code, models, and datasets will be released."}
{"id": "2505.12621", "pdf": "https://arxiv.org/pdf/2505.12621", "abs": "https://arxiv.org/abs/2505.12621", "authors": ["João Eduardo Batista", "Emil Vatai", "Mohamed Wahib"], "title": "Think Before You Attribute: Improving the Performance of LLMs Attribution Systems", "categories": ["cs.CL", "cs.IR"], "comment": "22 pages (9 pages of content, 4 pages of references, 9 pages of\n  supplementary material), 7 figures, 10 tables", "summary": "Large Language Models (LLMs) are increasingly applied in various science\ndomains, yet their broader adoption remains constrained by a critical\nchallenge: the lack of trustworthy, verifiable outputs. Current LLMs often\ngenerate answers without reliable source attribution, or worse, with incorrect\nattributions, posing a barrier to their use in scientific and high-stakes\nsettings, where traceability and accountability are non-negotiable. To be\nreliable, attribution systems need high accuracy and retrieve data with short\nlengths, i.e., attribute to a sentence within a document rather than a whole\ndocument. We propose a sentence-level pre-attribution step for\nRetrieve-Augmented Generation (RAG) systems that classify sentences into three\ncategories: not attributable, attributable to a single quote, and attributable\nto multiple quotes. By separating sentences before attribution, a proper\nattribution method can be selected for the type of sentence, or the attribution\ncan be skipped altogether. Our results indicate that classifiers are\nwell-suited for this task. In this work, we propose a pre-attribution step to\nreduce the computational complexity of attribution, provide a clean version of\nthe HAGRID dataset, and provide an end-to-end attribution system that works out\nof the box."}
{"id": "2505.12630", "pdf": "https://arxiv.org/pdf/2505.12630", "abs": "https://arxiv.org/abs/2505.12630", "authors": ["Xiangpeng Tian", "Xiangyu Liao", "Xiao Liu", "Meng Li", "Chao Ren"], "title": "Degradation-Aware Feature Perturbation for All-in-One Image Restoration", "categories": ["cs.CV", "cs.AI", "I.4.5"], "comment": "Accepted to CVPR 2025. 8 pages, 7 figures", "summary": "All-in-one image restoration aims to recover clear images from various\ndegradation types and levels with a unified model. Nonetheless, the significant\nvariations among degradation types present challenges for training a universal\nmodel, often resulting in task interference, where the gradient update\ndirections of different tasks may diverge due to shared parameters. To address\nthis issue, motivated by the routing strategy, we propose DFPIR, a novel\nall-in-one image restorer that introduces Degradation-aware Feature\nPerturbations(DFP) to adjust the feature space to align with the unified\nparameter space. In this paper, the feature perturbations primarily include\nchannel-wise perturbations and attention-wise perturbations. Specifically,\nchannel-wise perturbations are implemented by shuffling the channels in\nhigh-dimensional space guided by degradation types, while attention-wise\nperturbations are achieved through selective masking in the attention space. To\nachieve these goals, we propose a Degradation-Guided Perturbation Block (DGPB)\nto implement these two functions, positioned between the encoding and decoding\nstages of the encoder-decoder architecture. Extensive experimental results\ndemonstrate that DFPIR achieves state-of-the-art performance on several\nall-in-one image restoration tasks including image denoising, image dehazing,\nimage deraining, motion deblurring, and low-light image enhancement. Our codes\nare available at https://github.com/TxpHome/DFPIR."}
{"id": "2505.12625", "pdf": "https://arxiv.org/pdf/2505.12625", "abs": "https://arxiv.org/abs/2505.12625", "authors": ["Ali Naseh", "Harsh Chaudhari", "Jaechul Roh", "Mingshi Wu", "Alina Oprea", "Amir Houmansadr"], "title": "R1dacted: Investigating Local Censorship in DeepSeek's R1 Language Model", "categories": ["cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "DeepSeek recently released R1, a high-performing large language model (LLM)\noptimized for reasoning tasks. Despite its efficient training pipeline, R1\nachieves competitive performance, even surpassing leading reasoning models like\nOpenAI's o1 on several benchmarks. However, emerging reports suggest that R1\nrefuses to answer certain prompts related to politically sensitive topics in\nChina. While existing LLMs often implement safeguards to avoid generating\nharmful or offensive outputs, R1 represents a notable shift - exhibiting\ncensorship-like behavior on politically charged queries. In this paper, we\ninvestigate this phenomenon by first introducing a large-scale set of heavily\ncurated prompts that get censored by R1, covering a range of politically\nsensitive topics, but are not censored by other models. We then conduct a\ncomprehensive analysis of R1's censorship patterns, examining their\nconsistency, triggers, and variations across topics, prompt phrasing, and\ncontext. Beyond English-language queries, we explore censorship behavior in\nother languages. We also investigate the transferability of censorship to\nmodels distilled from the R1 language model. Finally, we propose techniques for\nbypassing or removing this censorship. Our findings reveal possible additional\ncensorship integration likely shaped by design choices during training or\nalignment, raising concerns about transparency, bias, and governance in\nlanguage model deployment."}
{"id": "2505.12631", "pdf": "https://arxiv.org/pdf/2505.12631", "abs": "https://arxiv.org/abs/2505.12631", "authors": ["Li Lin"], "title": "Multi-Resolution Haar Network: Enhancing human motion prediction via Haar transform", "categories": ["cs.CV"], "comment": null, "summary": "The 3D human pose is vital for modern computer vision and computer graphics,\nand its prediction has drawn attention in recent years. 3D human pose\nprediction aims at forecasting a human's future motion from the previous\nsequence. Ignoring that the arbitrariness of human motion sequences has a firm\norigin in transition in both temporal and spatial axes limits the performance\nof state-of-the-art methods, leading them to struggle with making precise\npredictions on complex cases, e.g., arbitrarily posing or greeting. To\nalleviate this problem, a network called HaarMoDic is proposed in this paper,\nwhich utilizes the 2D Haar transform to project joints to higher resolution\ncoordinates where the network can access spatial and temporal information\nsimultaneously. An ablation study proves that the significant contributing\nmodule within the HaarModic Network is the Multi-Resolution Haar (MR-Haar)\nblock. Instead of mining in one of two axes or extracting separately, the\nMR-Haar block projects whole motion sequences to a mixed-up coordinate in\nhigher resolution with 2D Haar Transform, allowing the network to give scope to\ninformation from both axes in different resolutions. With the MR-Haar block,\nthe HaarMoDic network can make predictions referring to a broader range of\ninformation. Experimental results demonstrate that HaarMoDic surpasses\nstate-of-the-art methods in every testing interval on the Human3.6M dataset in\nthe Mean Per Joint Position Error (MPJPE) metric."}
{"id": "2505.12636", "pdf": "https://arxiv.org/pdf/2505.12636", "abs": "https://arxiv.org/abs/2505.12636", "authors": ["Jiakuan Xie", "Pengfei Cao", "Yubo Chen", "Kang Liu", "Jun Zhao"], "title": "Revealing the Deceptiveness of Knowledge Editing: A Mechanistic Analysis of Superficial Editing", "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 main", "summary": "Knowledge editing, which aims to update the knowledge encoded in language\nmodels, can be deceptive. Despite the fact that many existing knowledge editing\nalgorithms achieve near-perfect performance on conventional metrics, the models\nedited by them are still prone to generating original knowledge. This paper\nintroduces the concept of \"superficial editing\" to describe this phenomenon.\nOur comprehensive evaluation reveals that this issue presents a significant\nchallenge to existing algorithms. Through systematic investigation, we identify\nand validate two key factors contributing to this issue: (1) the residual\nstream at the last subject position in earlier layers and (2) specific\nattention modules in later layers. Notably, certain attention heads in later\nlayers, along with specific left singular vectors in their output matrices,\nencapsulate the original knowledge and exhibit a causal relationship with\nsuperficial editing. Furthermore, we extend our analysis to the task of\nsuperficial unlearning, where we observe consistent patterns in the behavior of\nspecific attention heads and their corresponding left singular vectors, thereby\ndemonstrating the robustness and broader applicability of our methodology and\nconclusions. Our code is available here."}
{"id": "2505.12632", "pdf": "https://arxiv.org/pdf/2505.12632", "abs": "https://arxiv.org/abs/2505.12632", "authors": ["Yunseok Jang", "Yeda Song", "Sungryull Sohn", "Lajanugen Logeswaran", "Tiange Luo", "Dong-Ki Kim", "Kyunghoon Bae", "Honglak Lee"], "title": "Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "CVPR 2025", "summary": "Recent advancements in Large Language Models (LLMs) and Vision-Language\nModels (VLMs) have sparked significant interest in developing GUI visual\nagents. We introduce MONDAY (Mobile OS Navigation Task Dataset for Agents from\nYouTube), a large-scale dataset of 313K annotated frames from 20K instructional\nvideos capturing diverse real-world mobile OS navigation across multiple\nplatforms. Models that include MONDAY in their pre-training phases demonstrate\nrobust cross-platform generalization capabilities, consistently outperforming\nmodels trained on existing single OS datasets while achieving an average\nperformance gain of 18.11%p on an unseen mobile OS platform. To enable\ncontinuous dataset expansion as mobile platforms evolve, we present an\nautomated framework that leverages publicly available video content to create\ncomprehensive task datasets without manual annotation. Our framework comprises\nrobust OCR-based scene detection (95.04% F1score), near-perfect UI element\ndetection (99.87% hit ratio), and novel multi-step action identification to\nextract reliable action sequences across diverse interface configurations. We\ncontribute both the MONDAY dataset and our automated collection framework to\nfacilitate future research in mobile OS navigation."}
{"id": "2505.12654", "pdf": "https://arxiv.org/pdf/2505.12654", "abs": "https://arxiv.org/abs/2505.12654", "authors": ["Yuxin Lin", "Yinglin Zheng", "Ming Zeng", "Wangzheng Shi"], "title": "Predicting Turn-Taking and Backchannel in Human-Machine Conversations Using Linguistic, Acoustic, and Visual Signals", "categories": ["cs.CL", "cs.AI"], "comment": "Accepected by ACL 2025", "summary": "This paper addresses the gap in predicting turn-taking and backchannel\nactions in human-machine conversations using multi-modal signals (linguistic,\nacoustic, and visual). To overcome the limitation of existing datasets, we\npropose an automatic data collection pipeline that allows us to collect and\nannotate over 210 hours of human conversation videos. From this, we construct a\nMulti-Modal Face-to-Face (MM-F2F) human conversation dataset, including over\n1.5M words and corresponding turn-taking and backchannel annotations from\napproximately 20M frames. Additionally, we present an end-to-end framework that\npredicts the probability of turn-taking and backchannel actions from\nmulti-modal signals. The proposed model emphasizes the interrelation between\nmodalities and supports any combination of text, audio, and video inputs,\nmaking it adaptable to a variety of realistic scenarios. Our experiments show\nthat our approach achieves state-of-the-art performance on turn-taking and\nbackchannel prediction tasks, achieving a 10\\% increase in F1-score on\nturn-taking and a 33\\% increase on backchannel prediction. Our dataset and code\nare publicly available online to ease of subsequent research."}
{"id": "2505.12635", "pdf": "https://arxiv.org/pdf/2505.12635", "abs": "https://arxiv.org/abs/2505.12635", "authors": ["Mingqi Shao", "Feng Xiong", "Zhaoxu Sun", "Mu Xu"], "title": "MVPainter: Accurate and Detailed 3D Texture Generation via Multi-View Diffusion with Geometric Control", "categories": ["cs.CV"], "comment": "Project page: https://amap-cvlab.github.io/MV-Painter", "summary": "Recently, significant advances have been made in 3D object generation.\nBuilding upon the generated geometry, current pipelines typically employ image\ndiffusion models to generate multi-view RGB images, followed by UV texture\nreconstruction through texture baking. While 3D geometry generation has\nimproved significantly, supported by multiple open-source frameworks, 3D\ntexture generation remains underexplored. In this work, we systematically\ninvestigate 3D texture generation through the lens of three core dimensions:\nreference-texture alignment, geometry-texture consistency, and local texture\nquality. To tackle these issues, we propose MVPainter, which employs data\nfiltering and augmentation strategies to enhance texture fidelity and detail,\nand introduces ControlNet-based geometric conditioning to improve\ntexture-geometry alignment. Furthermore, we extract physically-based rendering\n(PBR) attributes from the generated views to produce PBR meshes suitable for\nreal-world rendering applications. MVPainter achieves state-of-the-art results\nacross all three dimensions, as demonstrated by human-aligned evaluations. To\nfacilitate further research and reproducibility, we also release our full\npipeline as an open-source system, including data construction, model\narchitecture, and evaluation tools."}
{"id": "2505.12662", "pdf": "https://arxiv.org/pdf/2505.12662", "abs": "https://arxiv.org/abs/2505.12662", "authors": ["Xukai Liu", "Ye Liu", "Shiwen Wu", "Yanghai Zhang", "Yihao Yuan", "Kai Zhang", "Qi Liu"], "title": "Know3-RAG: A Knowledge-aware RAG Framework with Adaptive Retrieval, Generation, and Filtering", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have led to impressive\nprogress in natural language generation, yet their tendency to produce\nhallucinated or unsubstantiated content remains a critical concern. To improve\nfactual reliability, Retrieval-Augmented Generation (RAG) integrates external\nknowledge during inference. However, existing RAG systems face two major\nlimitations: (1) unreliable adaptive control due to limited external knowledge\nsupervision, and (2) hallucinations caused by inaccurate or irrelevant\nreferences. To address these issues, we propose Know3-RAG, a knowledge-aware\nRAG framework that leverages structured knowledge from knowledge graphs (KGs)\nto guide three core stages of the RAG process, including retrieval, generation,\nand filtering. Specifically, we introduce a knowledge-aware adaptive retrieval\nmodule that employs KG embedding to assess the confidence of the generated\nanswer and determine retrieval necessity, a knowledge-enhanced reference\ngeneration strategy that enriches queries with KG-derived entities to improve\ngenerated reference relevance, and a knowledge-driven reference filtering\nmechanism that ensures semantic alignment and factual accuracy of references.\nExperiments on multiple open-domain QA benchmarks demonstrate that Know3-RAG\nconsistently outperforms strong baselines, significantly reducing\nhallucinations and enhancing answer reliability."}
{"id": "2505.12641", "pdf": "https://arxiv.org/pdf/2505.12641", "abs": "https://arxiv.org/abs/2505.12641", "authors": ["Yue Huang", "Zi'ang Li", "Tianle Hu", "Jie Wen", "Guanbin Li", "Jinglin Zhang", "Guoxu Zhou", "Xiaozhao Fang"], "title": "Single Image Reflection Removal via inter-layer Complementarity", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Although dual-stream architectures have achieved remarkable success in single\nimage reflection removal, they fail to fully exploit inter-layer\ncomplementarity in their physical modeling and network design, which limits the\nquality of image separation. To address this fundamental limitation, we propose\ntwo targeted improvements to enhance dual-stream architectures: First, we\nintroduce a novel inter-layer complementarity model where low-frequency\ncomponents extracted from the residual layer interact with the transmission\nlayer through dual-stream architecture to enhance inter-layer complementarity.\nMeanwhile, high-frequency components from the residual layer provide inverse\nmodulation to both streams, improving the detail quality of the transmission\nlayer. Second, we propose an efficient inter-layer complementarity attention\nmechanism which first cross-reorganizes dual streams at the channel level to\nobtain reorganized streams with inter-layer complementary structures, then\nperforms attention computation on the reorganized streams to achieve better\ninter-layer separation, and finally restores the original stream structure for\noutput. Experimental results demonstrate that our method achieves\nstate-of-the-art separation quality on multiple public datasets while\nsignificantly reducing both computational cost and model complexity."}
{"id": "2505.12716", "pdf": "https://arxiv.org/pdf/2505.12716", "abs": "https://arxiv.org/abs/2505.12716", "authors": ["Taiqiang Wu", "Runming Yang", "Jiayi Li", "Pengfei Hu", "Ngai Wong", "Yujiu Yang"], "title": "Shadow-FT: Tuning Instruct via Base", "categories": ["cs.CL", "cs.AI"], "comment": "Under review", "summary": "Large language models (LLMs) consistently benefit from further fine-tuning on\nvarious tasks. However, we observe that directly tuning the INSTRUCT (i.e.,\ninstruction tuned) models often leads to marginal improvements and even\nperformance degeneration. Notably, paired BASE models, the foundation for these\nINSTRUCT variants, contain highly similar weight values (i.e., less than 2% on\naverage for Llama 3.1 8B). Therefore, we propose a novel Shadow-FT framework to\ntune the INSTRUCT models by leveraging the corresponding BASE models. The key\ninsight is to fine-tune the BASE model, and then directly graft the learned\nweight updates to the INSTRUCT model. Our proposed Shadow-FT introduces no\nadditional parameters, is easy to implement, and significantly improves\nperformance. We conduct extensive experiments on tuning mainstream LLMs, such\nas Qwen 3 and Llama 3 series, and evaluate them across 19 benchmarks covering\ncoding, reasoning, and mathematical tasks. Experimental results demonstrate\nthat Shadow-FT consistently outperforms conventional full-parameter and\nparameter-efficient tuning approaches. Further analyses indicate that Shadow-FT\ncan be applied to multimodal large language models (MLLMs) and combined with\ndirect preference optimization (DPO). Codes and weights are available at\n\\href{https://github.com/wutaiqiang/Shadow-FT}{Github}."}
{"id": "2505.12644", "pdf": "https://arxiv.org/pdf/2505.12644", "abs": "https://arxiv.org/abs/2505.12644", "authors": ["Bo Yang", "Hengwei Zhang", "Jindong Wang", "Yuchen Ren", "Chenhao Lin", "Chao Shen", "Zhengyu Zhao"], "title": "Use as Many Surrogates as You Want: Selective Ensemble Attack to Unleash Transferability without Sacrificing Resource Efficiency", "categories": ["cs.CV"], "comment": null, "summary": "In surrogate ensemble attacks, using more surrogate models yields higher\ntransferability but lower resource efficiency. This practical trade-off between\ntransferability and efficiency has largely limited existing attacks despite\nmany pre-trained models are easily accessible online. In this paper, we argue\nthat such a trade-off is caused by an unnecessary common assumption, i.e., all\nmodels should be identical across iterations. By lifting this assumption, we\ncan use as many surrogates as we want to unleash transferability without\nsacrificing efficiency. Concretely, we propose Selective Ensemble Attack (SEA),\nwhich dynamically selects diverse models (from easily accessible pre-trained\nmodels) across iterations based on our new interpretation of decoupling\nwithin-iteration and cross-iteration model diversity.In this way, the number of\nwithin-iteration models is fixed for maintaining efficiency, while only\ncross-iteration model diversity is increased for higher transferability.\nExperiments on ImageNet demonstrate the superiority of SEA in various\nscenarios. For example, when dynamically selecting 4 from 20 accessible models,\nSEA yields 8.5% higher transferability than existing attacks under the same\nefficiency. The superiority of SEA also generalizes to real-world systems, such\nas commercial vision APIs and large vision-language models. Overall, SEA opens\nup the possibility of adaptively balancing transferability and efficiency\naccording to specific resource requirements."}
{"id": "2505.12717", "pdf": "https://arxiv.org/pdf/2505.12717", "abs": "https://arxiv.org/abs/2505.12717", "authors": ["Haoyuan Wu", "Xueyi Chen", "Rui Ming", "Jilong Gao", "Shoubo Hu", "Zhuolun He", "Bei Yu"], "title": "ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) demonstrate significant reasoning capabilities,\nparticularly through long chain-of-thought (CoT) processes, which can be\nelicited by reinforcement learning (RL). However, prolonged CoT reasoning\npresents limitations, primarily verbose outputs due to excessive introspection.\nThe reasoning process in these LLMs often appears to follow a trial-and-error\nmethodology rather than a systematic, logical deduction. In contrast,\ntree-of-thoughts (ToT) offers a conceptually more advanced approach by modeling\nreasoning as an exploration within a tree structure. This reasoning structure\nfacilitates the parallel generation and evaluation of multiple reasoning\nbranches, allowing for the active identification, assessment, and pruning of\nunproductive paths. This process can potentially lead to improved performance\nand reduced token costs. Building upon the long CoT capability of LLMs, we\nintroduce tree-of-thoughts RL (ToTRL), a novel on-policy RL framework with a\nrule-based reward. ToTRL is designed to guide LLMs in developing the parallel\nToT strategy based on the sequential CoT strategy. Furthermore, we employ LLMs\nas players in a puzzle game during the ToTRL training process. Solving puzzle\ngames inherently necessitates exploring interdependent choices and managing\nmultiple constraints, which requires the construction and exploration of a\nthought tree, providing challenging tasks for cultivating the ToT reasoning\ncapability. Our empirical evaluations demonstrate that our ToTQwen3-8B model,\ntrained with our ToTRL, achieves significant improvement in performance and\nreasoning efficiency on complex reasoning tasks."}
{"id": "2505.12650", "pdf": "https://arxiv.org/pdf/2505.12650", "abs": "https://arxiv.org/abs/2505.12650", "authors": ["Yaotian Yang", "Yiwen Tang", "Yizhe Chen", "Xiao Chen", "Jiangjie Qiu", "Hao Xiong", "Haoyu Yin", "Zhiyao Luo", "Yifei Zhang", "Sijia Tao", "Wentao Li", "Qinghua Zhang", "Yuqiang Li", "Wanli Ouyang", "Bin Zhao", "Xiaonan Wang", "Fei Wei"], "title": "AutoMat: Enabling Automated Crystal Structure Reconstruction from Microscopy via Agentic Tool Use", "categories": ["cs.CV", "cs.AI"], "comment": "The code and dataset are publicly available at\n  https://github.com/yyt-2378/AutoMat and\n  https://huggingface.co/datasets/yaotianvector/STEM2Mat", "summary": "Machine learning-based interatomic potentials and force fields depend\ncritically on accurate atomic structures, yet such data are scarce due to the\nlimited availability of experimentally resolved crystals. Although\natomic-resolution electron microscopy offers a potential source of structural\ndata, converting these images into simulation-ready formats remains\nlabor-intensive and error-prone, creating a bottleneck for model training and\nvalidation. We introduce AutoMat, an end-to-end, agent-assisted pipeline that\nautomatically transforms scanning transmission electron microscopy (STEM)\nimages into atomic crystal structures and predicts their physical properties.\nAutoMat combines pattern-adaptive denoising, physics-guided template retrieval,\nsymmetry-aware atomic reconstruction, fast relaxation and property prediction\nvia MatterSim, and coordinated orchestration across all stages. We propose the\nfirst dedicated STEM2Mat-Bench for this task and evaluate performance using\nlattice RMSD, formation energy MAE, and structure-matching success rate. By\norchestrating external tool calls, AutoMat enables a text-only LLM to\noutperform vision-language models in this domain, achieving closed-loop\nreasoning throughout the pipeline. In large-scale experiments over 450\nstructure samples, AutoMat substantially outperforms existing multimodal large\nlanguage models and tools. These results validate both AutoMat and\nSTEM2Mat-Bench, marking a key step toward bridging microscopy and atomistic\nsimulation in materials science.The code and dataset are publicly available at\nhttps://github.com/yyt-2378/AutoMat and\nhttps://huggingface.co/datasets/yaotianvector/STEM2Mat."}
{"id": "2505.12718", "pdf": "https://arxiv.org/pdf/2505.12718", "abs": "https://arxiv.org/abs/2505.12718", "authors": ["Jingyang Peng", "Wenyuan Shen", "Jiarui Rao", "Jionghao Lin"], "title": "Automated Bias Assessment in AI-Generated Educational Content Using CEAT Framework", "categories": ["cs.CL", "cs.HC"], "comment": "Accepted by AIED 2025: Late-Breaking Results (LBR) Track", "summary": "Recent advances in Generative Artificial Intelligence (GenAI) have\ntransformed educational content creation, particularly in developing tutor\ntraining materials. However, biases embedded in AI-generated content--such as\ngender, racial, or national stereotypes--raise significant ethical and\neducational concerns. Despite the growing use of GenAI, systematic methods for\ndetecting and evaluating such biases in educational materials remain limited.\nThis study proposes an automated bias assessment approach that integrates the\nContextualized Embedding Association Test with a prompt-engineered word\nextraction method within a Retrieval-Augmented Generation framework. We applied\nthis method to AI-generated texts used in tutor training lessons. Results show\na high alignment between the automated and manually curated word sets, with a\nPearson correlation coefficient of r = 0.993, indicating reliable and\nconsistent bias assessment. Our method reduces human subjectivity and enhances\nfairness, scalability, and reproducibility in auditing GenAI-produced\neducational content."}
{"id": "2505.12656", "pdf": "https://arxiv.org/pdf/2505.12656", "abs": "https://arxiv.org/abs/2505.12656", "authors": ["Yongchang Gao", "Meiling Jin", "Zhaofei Yu", "Tiejun Huang", "Guozhang Chen"], "title": "SPKLIP: Aligning Spike Video Streams with Natural Language", "categories": ["cs.CV"], "comment": null, "summary": "Spike cameras offer unique sensing capabilities but their sparse,\nasynchronous output challenges semantic understanding, especially for Spike\nVideo-Language Alignment (Spike-VLA) where models like CLIP underperform due to\nmodality mismatch. We introduce SPKLIP, the first architecture specifically for\nSpike-VLA. SPKLIP employs a hierarchical spike feature extractor that\nadaptively models multi-scale temporal dynamics in event streams, and uses\nspike-text contrastive learning to directly align spike video with language,\nenabling effective few-shot learning. A full-spiking visual encoder variant,\nintegrating SNN components into our pipeline, demonstrates enhanced energy\nefficiency. Experiments show state-of-the-art performance on benchmark spike\ndatasets and strong few-shot generalization on a newly contributed real-world\ndataset. SPKLIP's energy efficiency highlights its potential for neuromorphic\ndeployment, advancing event-based multimodal research. The source code and\ndataset are available at [link removed for anonymity]."}
{"id": "2505.12723", "pdf": "https://arxiv.org/pdf/2505.12723", "abs": "https://arxiv.org/abs/2505.12723", "authors": ["Haoyuan Wu", "Rui Ming", "Jilong Gao", "Hangyu Zhao", "Xueyi Chen", "Yikai Yang", "Haisheng Zheng", "Zhuolun He", "Bei Yu"], "title": "On-Policy Optimization with Group Equivalent Preference for Multi-Programming Language Understanding", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) achieve remarkable performance in code\ngeneration tasks. However, a significant performance disparity persists between\npopular programming languages (e.g., Python, C++) and others. To address this\ncapability gap, we leverage the code translation task to train LLMs, thereby\nfacilitating the transfer of coding proficiency across diverse programming\nlanguages. Moreover, we introduce OORL for training, a novel reinforcement\nlearning (RL) framework that integrates on-policy and off-policy strategies.\nWithin OORL, on-policy RL is applied during code translation, guided by a\nrule-based reward signal derived from unit tests. Complementing this\ncoarse-grained rule-based reward, we propose Group Equivalent Preference\nOptimization (GEPO), a novel preference optimization method. Specifically, GEPO\ntrains the LLM using intermediate representations (IRs) groups. LLMs can be\nguided to discern IRs equivalent to the source code from inequivalent ones,\nwhile also utilizing signals about the mutual equivalence between IRs within\nthe group. This process allows LLMs to capture nuanced aspects of code\nfunctionality. By employing OORL for training with code translation tasks, LLMs\nimprove their recognition of code functionality and their understanding of the\nrelationships between code implemented in different languages. Extensive\nexperiments demonstrate that our OORL for LLMs training with code translation\ntasks achieves significant performance improvements on code benchmarks across\nmultiple programming languages."}
{"id": "2505.12660", "pdf": "https://arxiv.org/pdf/2505.12660", "abs": "https://arxiv.org/abs/2505.12660", "authors": ["Ziqi Wen", "Jonathan Skaza", "Shravan Murlidaran", "William Y. Wang", "Miguel P. Eckstein"], "title": "Predicting Reaction Time to Comprehend Scenes with Foveated Scene Understanding Maps", "categories": ["cs.CV"], "comment": null, "summary": "Although models exist that predict human response times (RTs) in tasks such\nas target search and visual discrimination, the development of image-computable\npredictors for scene understanding time remains an open challenge. Recent\nadvances in vision-language models (VLMs), which can generate scene\ndescriptions for arbitrary images, combined with the availability of\nquantitative metrics for comparing linguistic descriptions, offer a new\nopportunity to model human scene understanding. We hypothesize that the primary\nbottleneck in human scene understanding and the driving source of variability\nin response times across scenes is the interaction between the foveated nature\nof the human visual system and the spatial distribution of task-relevant visual\ninformation within an image. Based on this assumption, we propose a novel\nimage-computable model that integrates foveated vision with VLMs to produce a\nspatially resolved map of scene understanding as a function of fixation\nlocation (Foveated Scene Understanding Map, or F-SUM), along with an aggregate\nF-SUM score. This metric correlates with average (N=17) human RTs (r=0.47) and\nnumber of saccades (r=0.51) required to comprehend a scene (across 277 scenes).\nThe F-SUM score also correlates with average (N=16) human description accuracy\n(r=-0.56) in time-limited presentations. These correlations significantly\nexceed those of standard image-based metrics such as clutter, visual\ncomplexity, and scene ambiguity based on language entropy. Together, our work\nintroduces a new image-computable metric for predicting human response times in\nscene understanding and demonstrates the importance of foveated visual\nprocessing in shaping comprehension difficulty."}
{"id": "2505.12727", "pdf": "https://arxiv.org/pdf/2505.12727", "abs": "https://arxiv.org/abs/2505.12727", "authors": ["Han Meng", "Yancan Chen", "Yunan Li", "Yitian Yang", "Jungup Lee", "Renwen Zhang", "Yi-Chieh Lee"], "title": "What is Stigma Attributed to? A Theory-Grounded, Expert-Annotated Interview Corpus for Demystifying Mental-Health Stigma", "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": "Accepted to ACL 2025 Main Conference, 35 Pages", "summary": "Mental-health stigma remains a pervasive social problem that hampers\ntreatment-seeking and recovery. Existing resources for training neural models\nto finely classify such stigma are limited, relying primarily on social-media\nor synthetic data without theoretical underpinnings. To remedy this gap, we\npresent an expert-annotated, theory-informed corpus of human-chatbot\ninterviews, comprising 4,141 snippets from 684 participants with documented\nsocio-cultural backgrounds. Our experiments benchmark state-of-the-art neural\nmodels and empirically unpack the challenges of stigma detection. This dataset\ncan facilitate research on computationally detecting, neutralizing, and\ncounteracting mental-health stigma."}
{"id": "2505.12667", "pdf": "https://arxiv.org/pdf/2505.12667", "abs": "https://arxiv.org/abs/2505.12667", "authors": ["Zihan Su", "Xuerui Qiu", "Hongbin Xu", "Tangyu Jiang", "Junhao Zhuang", "Chun Yuan", "Ming Li", "Shengfeng He", "Fei Richard Yu"], "title": "Safe-Sora: Safe Text-to-Video Generation via Graphical Watermarking", "categories": ["cs.CV"], "comment": null, "summary": "The explosive growth of generative video models has amplified the demand for\nreliable copyright preservation of AI-generated content. Despite its popularity\nin image synthesis, invisible generative watermarking remains largely\nunderexplored in video generation. To address this gap, we propose Safe-Sora,\nthe first framework to embed graphical watermarks directly into the video\ngeneration process. Motivated by the observation that watermarking performance\nis closely tied to the visual similarity between the watermark and cover\ncontent, we introduce a hierarchical coarse-to-fine adaptive matching\nmechanism. Specifically, the watermark image is divided into patches, each\nassigned to the most visually similar video frame, and further localized to the\noptimal spatial region for seamless embedding. To enable spatiotemporal fusion\nof watermark patches across video frames, we develop a 3D wavelet\ntransform-enhanced Mamba architecture with a novel spatiotemporal local\nscanning strategy, effectively modeling long-range dependencies during\nwatermark embedding and retrieval. To the best of our knowledge, this is the\nfirst attempt to apply state space models to watermarking, opening new avenues\nfor efficient and robust watermark protection. Extensive experiments\ndemonstrate that Safe-Sora achieves state-of-the-art performance in terms of\nvideo quality, watermark fidelity, and robustness, which is largely attributed\nto our proposals. We will release our code upon publication."}
{"id": "2505.12768", "pdf": "https://arxiv.org/pdf/2505.12768", "abs": "https://arxiv.org/abs/2505.12768", "authors": ["Yaxun Dai", "Wenxuan Xie", "Xialie Zhuang", "Tianyu Yang", "Yiying Yang", "Haiqin Yang", "Yuhang Zhao", "Pingfu Chao", "Wenhao Jiang"], "title": "ReEx-SQL: Reasoning with Execution-Aware Reinforcement Learning for Text-to-SQL", "categories": ["cs.CL"], "comment": null, "summary": "In Text-to-SQL, execution feedback is essential for guiding large language\nmodels (LLMs) to reason accurately and generate reliable SQL queries. However,\nexisting methods treat execution feedback solely as a post-hoc signal for\ncorrection or selection, failing to integrate it into the generation process.\nThis limitation hinders their ability to address reasoning errors as they\noccur, ultimately reducing query accuracy and robustness. To address this\nissue, we propose ReEx-SQL (Reasoning with Execution-Aware Reinforcement\nLearning), a framework for Text-to-SQL that enables models to interact with the\ndatabase during decoding and dynamically adjust their reasoning based on\nexecution feedback. ReEx-SQL introduces an execution-aware reasoning paradigm\nthat interleaves intermediate SQL execution into reasoning paths, facilitating\ncontext-sensitive revisions. It achieves this through structured prompts with\nmarkup tags and a stepwise rollout strategy that integrates execution feedback\ninto each stage of generation. To supervise policy learning, we develop a\ncomposite reward function that includes an exploration reward, explicitly\nencouraging effective database interaction. Additionally, ReEx-SQL adopts a\ntree-based decoding strategy to support exploratory reasoning, enabling dynamic\nexpansion of alternative reasoning paths. Notably, ReEx-SQL achieves 88.8% on\nSpider and 64.9% on BIRD at the 7B scale, surpassing the standard reasoning\nbaseline by 2.7% and 2.6%, respectively. It also shows robustness, achieving\n85.2% on Spider-Realistic with leading performance. In addition, its\ntree-structured decoding improves efficiency and performance over linear\ndecoding, reducing inference time by 51.9% on the BIRD development set."}
{"id": "2505.12670", "pdf": "https://arxiv.org/pdf/2505.12670", "abs": "https://arxiv.org/abs/2505.12670", "authors": ["Lihong Chen", "Hossein Hassani", "Soodeh Nikan"], "title": "TS-VLM: Text-Guided SoftSort Pooling for Vision-Language Models in Multi-View Driving Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) have shown remarkable potential in advancing\nautonomous driving by leveraging multi-modal fusion in order to enhance scene\nperception, reasoning, and decision-making. Despite their potential, existing\nmodels suffer from computational overhead and inefficient integration of\nmulti-view sensor data that make them impractical for real-time deployment in\nsafety-critical autonomous driving applications. To address these shortcomings,\nthis paper is devoted to designing a lightweight VLM called TS-VLM, which\nincorporates a novel Text-Guided SoftSort Pooling (TGSSP) module. By resorting\nto semantics of the input queries, TGSSP ranks and fuses visual features from\nmultiple views, enabling dynamic and query-aware multi-view aggregation without\nreliance on costly attention mechanisms. This design ensures the query-adaptive\nprioritization of semantically related views, which leads to improved\ncontextual accuracy in multi-view reasoning for autonomous driving. Extensive\nevaluations on the DriveLM benchmark demonstrate that, on the one hand, TS-VLM\noutperforms state-of-the-art models with a BLEU-4 score of 56.82, METEOR of\n41.91, ROUGE-L of 74.64, and CIDEr of 3.39. On the other hand, TS-VLM reduces\ncomputational cost by up to 90%, where the smallest version contains only 20.1\nmillion parameters, making it more practical for real-time deployment in\nautonomous vehicles."}
{"id": "2505.12781", "pdf": "https://arxiv.org/pdf/2505.12781", "abs": "https://arxiv.org/abs/2505.12781", "authors": ["Jitai Hao", "Qiang Huang", "Hao Liu", "Xinyan Xiao", "Zhaochun Ren", "Jun Yu"], "title": "A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation through Low-Rank Clone", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Training high-performing Small Language Models (SLMs) remains costly, even\nwith knowledge distillation and pruning from larger teacher models. Existing\nwork often faces three key challenges: (1) information loss from hard pruning,\n(2) inefficient alignment of representations, and (3) underutilization of\ninformative activations, particularly from Feed-Forward Networks (FFNs). To\naddress these challenges, we introduce Low-Rank Clone (LRC), an efficient\npre-training method that constructs SLMs aspiring to behavioral equivalence\nwith strong teacher models. LRC trains a set of low-rank projection matrices\nthat jointly enable soft pruning by compressing teacher weights, and activation\nclone by aligning student activations, including FFN signals, with those of the\nteacher. This unified design maximizes knowledge transfer while removing the\nneed for explicit alignment modules. Extensive experiments with open-source\nteachers (e.g., Llama-3.2-3B-Instruct, Qwen2.5-3B/7B-Instruct) show that LRC\nmatches or surpasses state-of-the-art models trained on trillions of\ntokens--while using only 20B tokens, achieving over 1,000x training efficiency.\nOur codes and model checkpoints are available at\nhttps://github.com/CURRENTF/LowRankClone and\nhttps://huggingface.co/collections/JitaiHao/low-rank-clone-lrc-6828389e96a93f1d4219dfaf."}
{"id": "2505.12674", "pdf": "https://arxiv.org/pdf/2505.12674", "abs": "https://arxiv.org/abs/2505.12674", "authors": ["Mingyuan Zhou", "Yi Gu", "Zhendong Wang"], "title": "Few-Step Diffusion via Score identity Distillation", "categories": ["cs.CV", "cs.LG", "stat.ML"], "comment": null, "summary": "Diffusion distillation has emerged as a promising strategy for accelerating\ntext-to-image (T2I) diffusion models by distilling a pretrained score network\ninto a one- or few-step generator. While existing methods have made notable\nprogress, they often rely on real or teacher-synthesized images to perform well\nwhen distilling high-resolution T2I diffusion models such as Stable Diffusion\nXL (SDXL), and their use of classifier-free guidance (CFG) introduces a\npersistent trade-off between text-image alignment and generation diversity. We\naddress these challenges by optimizing Score identity Distillation (SiD) -- a\ndata-free, one-step distillation framework -- for few-step generation. Backed\nby theoretical analysis that justifies matching a uniform mixture of outputs\nfrom all generation steps to the data distribution, our few-step distillation\nalgorithm avoids step-specific networks and integrates seamlessly into existing\npipelines, achieving state-of-the-art performance on SDXL at 1024x1024\nresolution. To mitigate the alignment-diversity trade-off when real text-image\npairs are available, we introduce a Diffusion GAN-based adversarial loss\napplied to the uniform mixture and propose two new guidance strategies:\nZero-CFG, which disables CFG in the teacher and removes text conditioning in\nthe fake score network, and Anti-CFG, which applies negative CFG in the fake\nscore network. This flexible setup improves diversity without sacrificing\nalignment. Comprehensive experiments on SD1.5 and SDXL demonstrate\nstate-of-the-art performance in both one-step and few-step generation settings,\nalong with robustness to the absence of real images. Our efficient PyTorch\nimplementation, along with the resulting one- and few-step distilled\ngenerators, will be released publicly as a separate branch at\nhttps://github.com/mingyuanzhou/SiD-LSG."}
{"id": "2505.12792", "pdf": "https://arxiv.org/pdf/2505.12792", "abs": "https://arxiv.org/abs/2505.12792", "authors": ["Wenhao Zhu", "Yuhang Xie", "Guojie Song", "Xin Zhang"], "title": "EAVIT: Efficient and Accurate Human Value Identification from Text data via LLMs", "categories": ["cs.CL"], "comment": null, "summary": "The rapid evolution of large language models (LLMs) has revolutionized\nvarious fields, including the identification and discovery of human values\nwithin text data. While traditional NLP models, such as BERT, have been\nemployed for this task, their ability to represent textual data is\nsignificantly outperformed by emerging LLMs like GPTs. However, the performance\nof online LLMs often degrades when handling long contexts required for value\nidentification, which also incurs substantial computational costs. To address\nthese challenges, we propose EAVIT, an efficient and accurate framework for\nhuman value identification that combines the strengths of both locally\nfine-tunable and online black-box LLMs. Our framework employs a value detector\n- a small, local language model - to generate initial value estimations. These\nestimations are then used to construct concise input prompts for online LLMs,\nenabling accurate final value identification. To train the value detector, we\nintroduce explanation-based training and data generation techniques\nspecifically tailored for value identification, alongside sampling strategies\nto optimize the brevity of LLM input prompts. Our approach effectively reduces\nthe number of input tokens by up to 1/6 compared to directly querying online\nLLMs, while consistently outperforming traditional NLP methods and other\nLLM-based strategies."}
{"id": "2505.12677", "pdf": "https://arxiv.org/pdf/2505.12677", "abs": "https://arxiv.org/abs/2505.12677", "authors": ["Shristi Das Biswas", "Arani Roy", "Kaushik Roy"], "title": "CURE: Concept Unlearning via Orthogonal Representation Editing in Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "As Text-to-Image models continue to evolve, so does the risk of generating\nunsafe, copyrighted, or privacy-violating content. Existing safety\ninterventions - ranging from training data curation and model fine-tuning to\ninference-time filtering and guidance - often suffer from incomplete concept\nremoval, susceptibility to jail-breaking, computational inefficiency, or\ncollateral damage to unrelated capabilities. In this paper, we introduce CURE,\na training-free concept unlearning framework that operates directly in the\nweight space of pre-trained diffusion models, enabling fast, interpretable, and\nhighly specific suppression of undesired concepts. At the core of our method is\nthe Spectral Eraser, a closed-form, orthogonal projection module that\nidentifies discriminative subspaces using Singular Value Decomposition over\ntoken embeddings associated with the concepts to forget and retain.\nIntuitively, the Spectral Eraser identifies and isolates features unique to the\nundesired concept while preserving safe attributes. This operator is then\napplied in a single step update to yield an edited model in which the target\nconcept is effectively unlearned - without retraining, supervision, or\niterative optimization. To balance the trade-off between filtering toxicity and\npreserving unrelated concepts, we further introduce an Expansion Mechanism for\nspectral regularization which selectively modulates singular vectors based on\ntheir relative significance to control the strength of forgetting. All the\nprocesses above are in closed-form, guaranteeing extremely efficient erasure in\nonly $2$ seconds. Benchmarking against prior approaches, CURE achieves a more\nefficient and thorough removal for targeted artistic styles, objects,\nidentities, or explicit content, with minor damage to original generation\nability and demonstrates enhanced robustness against red-teaming."}
{"id": "2505.12808", "pdf": "https://arxiv.org/pdf/2505.12808", "abs": "https://arxiv.org/abs/2505.12808", "authors": ["Yanbin Yin", "Kun Zhou", "Zhen Wang", "Xiangdong Zhang", "Yifei Shao", "Shibo Hao", "Yi Gu", "Jieyuan Liu", "Somanshu Singla", "Tianyang Liu", "Eric P. Xing", "Zhengzhong Liu", "Haojian Jin", "Zhiting Hu"], "title": "Decentralized Arena: Towards Democratic and Scalable Automatic Evaluation of Language Models", "categories": ["cs.CL", "cs.LG"], "comment": "20 pages, ongoing work", "summary": "The recent explosion of large language models (LLMs), each with its own\ngeneral or specialized strengths, makes scalable, reliable benchmarking more\nurgent than ever. Standard practices nowadays face fundamental trade-offs:\nclosed-ended question-based benchmarks (eg MMLU) struggle with saturation as\nnewer models emerge, while crowd-sourced leaderboards (eg Chatbot Arena) rely\non costly and slow human judges. Recently, automated methods (eg\nLLM-as-a-judge) shed light on the scalability, but risk bias by relying on one\nor a few \"authority\" models. To tackle these issues, we propose Decentralized\nArena (dearena), a fully automated framework leveraging collective intelligence\nfrom all LLMs to evaluate each other. It mitigates single-model judge bias by\ndemocratic, pairwise evaluation, and remains efficient at scale through two key\ncomponents: (1) a coarse-to-fine ranking algorithm for fast incremental\ninsertion of new models with sub-quadratic complexity, and (2) an automatic\nquestion selection strategy for the construction of new evaluation dimensions.\nAcross extensive experiments across 66 LLMs, dearena attains up to 97%\ncorrelation with human judgements, while significantly reducing the cost. Our\ncode and data will be publicly released on\nhttps://github.com/maitrix-org/de-arena."}
{"id": "2505.12685", "pdf": "https://arxiv.org/pdf/2505.12685", "abs": "https://arxiv.org/abs/2505.12685", "authors": ["Fei Xie", "Jiahao Nie", "Yujin Tang", "Wenkang Zhang", "Hongshen Zhao"], "title": "Mamba-Adaptor: State Space Model Adaptor for Visual Recognition", "categories": ["cs.CV"], "comment": "CVPR paper", "summary": "Recent State Space Models (SSM), especially Mamba, have demonstrated\nimpressive performance in visual modeling and possess superior model\nefficiency. However, the application of Mamba to visual tasks suffers inferior\nperformance due to three main constraints existing in the sequential model: 1)\nCasual computing is incapable of accessing global context; 2) Long-range\nforgetting when computing the current hidden states; 3) Weak spatial structural\nmodeling due to the transformed sequential input. To address these issues, we\ninvestigate a simple yet powerful vision task Adaptor for Mamba models, which\nconsists of two functional modules: Adaptor-T and Adaptor-S. When solving the\nhidden states for SSM, we apply a lightweight prediction module Adaptor-T to\nselect a set of learnable locations as memory augmentations to ease long-range\nforgetting issues. Moreover, we leverage Adapator-S, composed of multi-scale\ndilated convolutional kernels, to enhance the spatial modeling and introduce\nthe image inductive bias into the feature output. Both modules can enlarge the\ncontext modeling in casual computing, as the output is enhanced by the\ninaccessible features. We explore three usages of Mamba-Adaptor: A general\nvisual backbone for various vision tasks; A booster module to raise the\nperformance of pretrained backbones; A highly efficient fine-tuning module that\nadapts the base model for transfer learning tasks. Extensive experiments verify\nthe effectiveness of Mamba-Adaptor in three settings. Notably, our\nMamba-Adaptor achieves state-of the-art performance on the ImageNet and COCO\nbenchmarks."}
{"id": "2505.12814", "pdf": "https://arxiv.org/pdf/2505.12814", "abs": "https://arxiv.org/abs/2505.12814", "authors": ["Xilong Cheng", "Yunxiao Qin", "Yuting Tan", "Zhengnan Li", "Ye Wang", "Hongjiang Xiao", "Yuan Zhang"], "title": "PsyMem: Fine-grained psychological alignment and Explicit Memory Control for Advanced Role-Playing LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Existing LLM-based role-playing methods often rely on superficial textual\ndescriptions or simplistic metrics, inadequately modeling both intrinsic and\nextrinsic character dimensions. Additionally, they typically simulate character\nmemory with implicit model knowledge or basic retrieval augment generation\nwithout explicit memory alignment, compromising memory consistency. The two\nissues weaken reliability of role-playing LLMs in several applications, such as\ntrustworthy social simulation. To address these limitations, we propose PsyMem,\na novel framework integrating fine-grained psychological attributes and\nexplicit memory control for role-playing. PsyMem supplements textual\ndescriptions with 26 psychological indicators to detailed model character.\nAdditionally, PsyMem implements memory alignment training, explicitly trains\nthe model to align character's response with memory, thereby enabling dynamic\nmemory-controlled responding during inference. By training Qwen2.5-7B-Instruct\non our specially designed dataset (including 5,414 characters and 38,962\ndialogues extracted from novels), the resulting model, termed as PsyMem-Qwen,\noutperforms baseline models in role-playing, achieving the best performance in\nhuman-likeness and character fidelity."}
{"id": "2505.12693", "pdf": "https://arxiv.org/pdf/2505.12693", "abs": "https://arxiv.org/abs/2505.12693", "authors": ["Luyao Lei", "Shuo Xu", "Yifan Bai", "Xing Wei"], "title": "TACOcc:Target-Adaptive Cross-Modal Fusion with Volume Rendering for 3D Semantic Occupancy", "categories": ["cs.CV"], "comment": null, "summary": "The performance of multi-modal 3D occupancy prediction is limited by\nineffective fusion, mainly due to geometry-semantics mismatch from fixed fusion\nstrategies and surface detail loss caused by sparse, noisy annotations. The\nmismatch stems from the heterogeneous scale and distribution of point cloud and\nimage features, leading to biased matching under fixed neighborhood fusion. To\naddress this, we propose a target-scale adaptive, bidirectional symmetric\nretrieval mechanism. It expands the neighborhood for large targets to enhance\ncontext awareness and shrinks it for small ones to improve efficiency and\nsuppress noise, enabling accurate cross-modal feature alignment. This mechanism\nexplicitly establishes spatial correspondences and improves fusion accuracy.\nFor surface detail loss, sparse labels provide limited supervision, resulting\nin poor predictions for small objects. We introduce an improved volume\nrendering pipeline based on 3D Gaussian Splatting, which takes fused features\nas input to render images, applies photometric consistency supervision, and\njointly optimizes 2D-3D consistency. This enhances surface detail\nreconstruction while suppressing noise propagation. In summary, we propose\nTACOcc, an adaptive multi-modal fusion framework for 3D semantic occupancy\nprediction, enhanced by volume rendering supervision. Experiments on the\nnuScenes and SemanticKITTI benchmarks validate its effectiveness."}
{"id": "2505.12821", "pdf": "https://arxiv.org/pdf/2505.12821", "abs": "https://arxiv.org/abs/2505.12821", "authors": ["Han Sun", "Zhen Sun", "Zongmin Zhang", "Linzhao Jia", "Wei Shao", "Min Zhang"], "title": "SynDec: A Synthesize-then-Decode Approach for Arbitrary Textual Style Transfer via Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are emerging as dominant forces for textual\nstyle transfer. However, for arbitrary style transfer, LLMs face two key\nchallenges: (1) considerable reliance on manually-constructed prompts and (2)\nrigid stylistic biases inherent in LLMs. In this paper, we propose a novel\nSynthesize-then-Decode (SynDec) approach, which automatically synthesizes\nhigh-quality prompts and amplifies their roles during decoding process.\nSpecifically, our approach synthesizes prompts by selecting representative\nfew-shot samples, conducting a four-dimensional style analysis, and reranking\nthe candidates. At LLM decoding stage, the TST effect is amplified by\nmaximizing the contrast in output probabilities between scenarios with and\nwithout the synthesized prompt, as well as between prompts and negative\nsamples. We conduct extensive experiments and the results show that SynDec\noutperforms existing state-of-the-art LLM-based methods on five out of six\nbenchmarks (e.g., achieving up to a 9\\% increase in accuracy for\nmodern-to-Elizabethan English transfer). Detailed ablation studies further\nvalidate the effectiveness of SynDec."}
{"id": "2505.12702", "pdf": "https://arxiv.org/pdf/2505.12702", "abs": "https://arxiv.org/abs/2505.12702", "authors": ["Tianming Liang", "Haichao Jiang", "Yuting Yang", "Chaolei Tan", "Shuai Li", "Wei-Shi Zheng", "Jian-Fang Hu"], "title": "Long-RVOS: A Comprehensive Benchmark for Long-term Referring Video Object Segmentation", "categories": ["cs.CV"], "comment": "Project Page: \\url{https://isee-laboratory.github.io/Long-RVOS}", "summary": "Referring video object segmentation (RVOS) aims to identify, track and\nsegment the objects in a video based on language descriptions, which has\nreceived great attention in recent years. However, existing datasets remain\nfocus on short video clips within several seconds, with salient objects visible\nin most frames. To advance the task towards more practical scenarios, we\nintroduce \\textbf{Long-RVOS}, a large-scale benchmark for long-term referring\nvideo object segmentation. Long-RVOS contains 2,000+ videos of an average\nduration exceeding 60 seconds, covering a variety of objects that undergo\nocclusion, disappearance-reappearance and shot changing. The objects are\nmanually annotated with three different types of descriptions to individually\nevaluate the understanding of static attributes, motion patterns and\nspatiotemporal relationships. Moreover, unlike previous benchmarks that rely\nsolely on the per-frame spatial evaluation, we introduce two new metrics to\nassess the temporal and spatiotemporal consistency. We benchmark 6\nstate-of-the-art methods on Long-RVOS. The results show that current approaches\nstruggle severely with the long-video challenges. To address this, we further\npropose ReferMo, a promising baseline method that integrates motion information\nto expand the temporal receptive field, and employs a local-to-global\narchitecture to capture both short-term dynamics and long-term dependencies.\nDespite simplicity, ReferMo achieves significant improvements over current\nmethods in long-term scenarios. We hope that Long-RVOS and our baseline can\ndrive future RVOS research towards tackling more realistic and long-form\nvideos."}
{"id": "2505.12831", "pdf": "https://arxiv.org/pdf/2505.12831", "abs": "https://arxiv.org/abs/2505.12831", "authors": ["Zifeng Cheng", "Zhonghui Wang", "Yuchen Fu", "Zhiwei Jiang", "Yafeng Yin", "Cong Wang", "Qing Gu"], "title": "Contrastive Prompting Enhances Sentence Embeddings in LLMs through Inference-Time Steering", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Extracting sentence embeddings from large language models (LLMs) is a\npractical direction, as it requires neither additional data nor fine-tuning.\nPrevious studies usually focus on prompt engineering to guide LLMs to encode\nthe core semantic information of the sentence into the embedding of the last\ntoken. However, the last token in these methods still encodes an excess of\nnon-essential information, such as stop words, limiting its encoding capacity.\nTo this end, we propose a Contrastive Prompting (CP) method that introduces an\nextra auxiliary prompt to elicit better sentence embedding. By contrasting with\nthe auxiliary prompt, CP can steer existing prompts to encode the core\nsemantics of the sentence, rather than non-essential information. CP is a\nplug-and-play inference-time intervention method that can be combined with\nvarious prompt-based methods. Extensive experiments on Semantic Textual\nSimilarity (STS) tasks and downstream classification tasks demonstrate that our\nmethod can improve the performance of existing prompt-based methods across\ndifferent LLMs. Our code will be released at https://github.com/zifengcheng/CP."}
{"id": "2505.12703", "pdf": "https://arxiv.org/pdf/2505.12703", "abs": "https://arxiv.org/abs/2505.12703", "authors": ["Jiabin Chen", "Haiping Wang", "Jinpeng Li", "Yuan Liu", "Zhen Dong", "Bisheng Yang"], "title": "SpatialLLM: From Multi-modality Data to Urban Spatial Intelligence", "categories": ["cs.CV"], "comment": null, "summary": "We propose SpatialLLM, a novel approach advancing spatial intelligence tasks\nin complex urban scenes. Unlike previous methods requiring geographic analysis\ntools or domain expertise, SpatialLLM is a unified language model directly\naddressing various spatial intelligence tasks without any training,\nfine-tuning, or expert intervention. The core of SpatialLLM lies in\nconstructing detailed and structured scene descriptions from raw spatial data\nto prompt pre-trained LLMs for scene-based analysis. Extensive experiments show\nthat, with our designs, pretrained LLMs can accurately perceive spatial\ndistribution information and enable zero-shot execution of advanced spatial\nintelligence tasks, including urban planning, ecological analysis, traffic\nmanagement, etc. We argue that multi-field knowledge, context length, and\nreasoning ability are key factors influencing LLM performances in urban\nanalysis. We hope that SpatialLLM will provide a novel viable perspective for\nurban intelligent analysis and management. The code and dataset are available\nat https://github.com/WHU-USI3DV/SpatialLLM."}
{"id": "2505.12835", "pdf": "https://arxiv.org/pdf/2505.12835", "abs": "https://arxiv.org/abs/2505.12835", "authors": ["Hengxing Cai", "Jinhan Dong", "Jingjun Tan", "Jingcheng Deng", "Sihang Li", "Zhifeng Gao", "Haidong Wang", "Zicheng Su", "Agachai Sumalee", "Renxin Zhong"], "title": "FlightGPT: Towards Generalizable and Interpretable UAV Vision-and-Language Navigation with Vision-Language Models", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Unmanned Aerial Vehicle (UAV) Vision-and-Language Navigation (VLN) is vital\nfor applications such as disaster response, logistics delivery, and urban\ninspection. However, existing methods often struggle with insufficient\nmultimodal fusion, weak generalization, and poor interpretability. To address\nthese challenges, we propose FlightGPT, a novel UAV VLN framework built upon\nVision-Language Models (VLMs) with powerful multimodal perception capabilities.\nWe design a two-stage training pipeline: first, Supervised Fine-Tuning (SFT)\nusing high-quality demonstrations to improve initialization and structured\nreasoning; then, Group Relative Policy Optimization (GRPO) algorithm, guided by\na composite reward that considers goal accuracy, reasoning quality, and format\ncompliance, to enhance generalization and adaptability. Furthermore, FlightGPT\nintroduces a Chain-of-Thought (CoT)-based reasoning mechanism to improve\ndecision interpretability. Extensive experiments on the city-scale dataset\nCityNav demonstrate that FlightGPT achieves state-of-the-art performance across\nall scenarios, with a 9.22\\% higher success rate than the strongest baseline in\nunseen environments. Our implementation is publicly available."}
{"id": "2505.12711", "pdf": "https://arxiv.org/pdf/2505.12711", "abs": "https://arxiv.org/abs/2505.12711", "authors": ["Qichen Sun", "Zhengrui Guo", "Rui Peng", "Hao Chen", "Jinzhuo Wang"], "title": "Any-to-Any Learning in Computational Pathology via Triplet Multimodal Pretraining", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in computational pathology and artificial intelligence have\nsignificantly enhanced the utilization of gigapixel whole-slide images and and\nadditional modalities (e.g., genomics) for pathological diagnosis. Although\ndeep learning has demonstrated strong potential in pathology, several key\nchallenges persist: (1) fusing heterogeneous data types requires sophisticated\nstrategies beyond simple concatenation due to high computational costs; (2)\ncommon scenarios of missing modalities necessitate flexible strategies that\nallow the model to learn robustly in the absence of certain modalities; (3) the\ndownstream tasks in CPath are diverse, ranging from unimodal to multimodal,\ncnecessitating a unified model capable of handling all modalities. To address\nthese challenges, we propose ALTER, an any-to-any tri-modal pretraining\nframework that integrates WSIs, genomics, and pathology reports. The term \"any\"\nemphasizes ALTER's modality-adaptive design, enabling flexible pretraining with\nany subset of modalities, and its capacity to learn robust, cross-modal\nrepresentations beyond WSI-centric approaches. We evaluate ALTER across\nextensive clinical tasks including survival prediction, cancer subtyping, gene\nmutation prediction, and report generation, achieving superior or comparable\nperformance to state-of-the-art baselines."}
{"id": "2505.12837", "pdf": "https://arxiv.org/pdf/2505.12837", "abs": "https://arxiv.org/abs/2505.12837", "authors": ["Christian Braun", "Alexander Lilienbeck", "Daniel Mentjukov"], "title": "The Hidden Structure -- Improving Legal Document Understanding Through Explicit Text Formatting", "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, 3 figures", "summary": "Legal contracts possess an inherent, semantically vital structure (e.g.,\nsections, clauses) that is crucial for human comprehension but whose impact on\nLLM processing remains under-explored. This paper investigates the effects of\nexplicit input text structure and prompt engineering on the performance of\nGPT-4o and GPT-4.1 on a legal question-answering task using an excerpt of the\nCUAD. We compare model exact-match accuracy across various input formats:\nwell-structured plain-text (human-generated from CUAD), plain-text cleaned of\nline breaks, extracted plain-text from Azure OCR, plain-text extracted by\nGPT-4o Vision, and extracted (and interpreted) Markdown (MD) from GPT-4o\nVision. To give an indication of the impact of possible prompt engineering, we\nassess the impact of shifting task instructions to the system prompt and\nexplicitly informing the model about the structured nature of the input. Our\nfindings reveal that GPT-4o demonstrates considerable robustness to variations\nin input structure, but lacks in overall performance. Conversely, GPT-4.1's\nperformance is markedly sensitive; poorly structured inputs yield suboptimal\nresults (but identical with GPT-4o), while well-structured formats (original\nCUAD text, GPT-4o Vision text and GPT-4o MD) improve exact-match accuracy by\n~20 percentage points. Optimizing the system prompt to include task details and\nan advisory about structured input further elevates GPT-4.1's accuracy by an\nadditional ~10-13 percentage points, with Markdown ultimately achieving the\nhighest performance under these conditions (79 percentage points overall\nexact-match accuracy). This research empirically demonstrates that while newer\nmodels exhibit greater resilience, careful input structuring and strategic\nprompt design remain critical for optimizing the performance of LLMs, and can\nsignificantly affect outcomes in high-stakes legal applications."}
{"id": "2505.12714", "pdf": "https://arxiv.org/pdf/2505.12714", "abs": "https://arxiv.org/abs/2505.12714", "authors": ["Yinzhe Wang", "Yiwen Xiao", "Hu Wang", "Yiping Xu", "Yan Tian"], "title": "IA-MVS: Instance-Focused Adaptive Depth Sampling for Multi-View Stereo", "categories": ["cs.CV"], "comment": null, "summary": "Multi-view stereo (MVS) models based on progressive depth hypothesis\nnarrowing have made remarkable advancements. However, existing methods haven't\nfully utilized the potential that the depth coverage of individual instances is\nsmaller than that of the entire scene, which restricts further improvements in\ndepth estimation precision. Moreover, inevitable deviations in the initial\nstage accumulate as the process advances. In this paper, we propose\nInstance-Adaptive MVS (IA-MVS). It enhances the precision of depth estimation\nby narrowing the depth hypothesis range and conducting refinement on each\ninstance. Additionally, a filtering mechanism based on intra-instance depth\ncontinuity priors is incorporated to boost robustness. Furthermore, recognizing\nthat existing confidence estimation can degrade IA-MVS performance on point\nclouds. We have developed a detailed mathematical model for confidence\nestimation based on conditional probability. The proposed method can be widely\napplied in models based on MVSNet without imposing extra training burdens. Our\nmethod achieves state-of-the-art performance on the DTU benchmark. The source\ncode is available at https://github.com/KevinWang73106/IA-MVS."}
{"id": "2505.12859", "pdf": "https://arxiv.org/pdf/2505.12859", "abs": "https://arxiv.org/abs/2505.12859", "authors": ["Lucas Georges Gabriel Charpentier", "Pierre Lison"], "title": "Re-identification of De-identified Documents with Autoregressive Infilling", "categories": ["cs.CL"], "comment": "To be presented a ACL 2025, Main, Long paper", "summary": "Documents revealing sensitive information about individuals must typically be\nde-identified. This de-identification is often done by masking all mentions of\npersonally identifiable information (PII), thereby making it more difficult to\nuncover the identity of the person(s) in question. To investigate the\nrobustness of de-identification methods, we present a novel, RAG-inspired\napproach that attempts the reverse process of re-identification based on a\ndatabase of documents representing background knowledge. Given a text in which\npersonal identifiers have been masked, the re-identification proceeds in two\nsteps. A retriever first selects from the background knowledge passages deemed\nrelevant for the re-identification. Those passages are then provided to an\ninfilling model which seeks to infer the original content of each text span.\nThis process is repeated until all masked spans are replaced. We evaluate the\nre-identification on three datasets (Wikipedia biographies, court rulings and\nclinical notes). Results show that (1) as many as 80% of de-identified text\nspans can be successfully recovered and (2) the re-identification accuracy\nincreases along with the level of background knowledge."}
{"id": "2505.12715", "pdf": "https://arxiv.org/pdf/2505.12715", "abs": "https://arxiv.org/abs/2505.12715", "authors": ["Aditya Taparia", "Noel Ngu", "Mario Leiva", "Joshua Shay Kricheli", "John Corcoran", "Nathaniel D. Bastian", "Gerardo Simari", "Paulo Shakarian", "Ransalu Senanayake"], "title": "VLC Fusion: Vision-Language Conditioned Sensor Fusion for Robust Object Detection", "categories": ["cs.CV"], "comment": "12 pages, 19 figures", "summary": "Although fusing multiple sensor modalities can enhance object detection\nperformance, existing fusion approaches often overlook subtle variations in\nenvironmental conditions and sensor inputs. As a result, they struggle to\nadaptively weight each modality under such variations. To address this\nchallenge, we introduce Vision-Language Conditioned Fusion (VLC Fusion), a\nnovel fusion framework that leverages a Vision-Language Model (VLM) to\ncondition the fusion process on nuanced environmental cues. By capturing\nhigh-level environmental context such as as darkness, rain, and camera\nblurring, the VLM guides the model to dynamically adjust modality weights based\non the current scene. We evaluate VLC Fusion on real-world autonomous driving\nand military target detection datasets that include image, LIDAR, and mid-wave\ninfrared modalities. Our experiments show that VLC Fusion consistently\noutperforms conventional fusion baselines, achieving improved detection\naccuracy in both seen and unseen scenarios."}
{"id": "2505.12864", "pdf": "https://arxiv.org/pdf/2505.12864", "abs": "https://arxiv.org/abs/2505.12864", "authors": ["Yu Fan", "Jingwei Ni", "Jakob Merane", "Etienne Salimbeni", "Yang Tian", "Yoan Hermstrüwer", "Yinya Huang", "Mubashara Akhtar", "Florian Geering", "Oliver Dreyer", "Daniel Brunner", "Markus Leippold", "Mrinmaya Sachan", "Alexander Stremitzer", "Christoph Engel", "Elliott Ash", "Joel Niklaus"], "title": "LEXam: Benchmarking Legal Reasoning on 340 Law Exams", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2"], "comment": null, "summary": "Long-form legal reasoning remains a key challenge for large language models\n(LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a\nnovel benchmark derived from 340 law exams spanning 116 law school courses\nacross a range of subjects and degree levels. The dataset comprises 4,886 law\nexam questions in English and German, including 2,841 long-form, open-ended\nquestions and 2,045 multiple-choice questions. Besides reference answers, the\nopen questions are also accompanied by explicit guidance outlining the expected\nlegal reasoning approach such as issue spotting, rule recall, or rule\napplication. Our evaluation on both open-ended and multiple-choice questions\npresent significant challenges for current LLMs; in particular, they notably\nstruggle with open questions that require structured, multi-step legal\nreasoning. Moreover, our results underscore the effectiveness of the dataset in\ndifferentiating between models with varying capabilities. Adopting an\nLLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate\nhow model-generated reasoning steps can be evaluated consistently and\naccurately. Our evaluation setup provides a scalable method to assess legal\nreasoning quality beyond simple accuracy metrics. Project page:\nhttps://lexam-benchmark.github.io/"}
{"id": "2505.12728", "pdf": "https://arxiv.org/pdf/2505.12728", "abs": "https://arxiv.org/abs/2505.12728", "authors": ["Zihua Wang", "Ruibo Li", "Haozhe Du", "Joey Tianyi Zhou", "Yu Zhang", "Xu Yang"], "title": "FLASH: Latent-Aware Semi-Autoregressive Speculative Decoding for Multimodal Tasks", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Large language and multimodal models (LLMs and LMMs) exhibit strong inference\ncapabilities but are often limited by slow decoding speeds. This challenge is\nespecially acute in LMMs, where visual inputs typically comprise more tokens\nwith lower information density than text -- an issue exacerbated by recent\ntrends toward finer-grained visual tokenizations to boost performance.\nSpeculative decoding has been effective in accelerating LLM inference by using\na smaller draft model to generate candidate tokens, which are then selectively\nverified by the target model, improving speed without sacrificing output\nquality. While this strategy has been extended to LMMs, existing methods\nlargely overlook the unique properties of visual inputs and depend solely on\ntext-based draft models. In this work, we propose \\textbf{FLASH} (Fast\nLatent-Aware Semi-Autoregressive Heuristics), a speculative decoding framework\ndesigned specifically for LMMs, which leverages two key properties of\nmultimodal data to design the draft model. First, to address redundancy in\nvisual tokens, we propose a lightweight latent-aware token compression\nmechanism. Second, recognizing that visual objects often co-occur within a\nscene, we employ a semi-autoregressive decoding strategy to generate multiple\ntokens per forward pass. These innovations accelerate draft decoding while\nmaintaining high acceptance rates, resulting in faster overall inference.\nExperiments show that FLASH significantly outperforms prior speculative\ndecoding approaches in both unimodal and multimodal settings, achieving up to\n\\textbf{2.68$\\times$} speed-up on video captioning and \\textbf{2.55$\\times$} on\nvisual instruction tuning tasks compared to the original LMM."}
{"id": "2505.12888", "pdf": "https://arxiv.org/pdf/2505.12888", "abs": "https://arxiv.org/abs/2505.12888", "authors": ["Jialun Zhong", "Yanzeng Li", "Sen Hu", "Yang Zhang", "Teng Xu", "Lei Zou"], "title": "GAP: Graph-Assisted Prompts for Dialogue-based Medication Recommendation", "categories": ["cs.CL"], "comment": null, "summary": "Medication recommendations have become an important task in the healthcare\ndomain, especially in measuring the accuracy and safety of medical dialogue\nsystems (MDS). Different from the recommendation task based on electronic\nhealth records (EHRs), dialogue-based medication recommendations require\nresearch on the interaction details between patients and doctors, which is\ncrucial but may not exist in EHRs. Recent advancements in large language models\n(LLM) have extended the medical dialogue domain. These LLMs can interpret\npatients' intent and provide medical suggestions including medication\nrecommendations, but some challenges are still worth attention. During a\nmulti-turn dialogue, LLMs may ignore the fine-grained medical information or\nconnections across the dialogue turns, which is vital for providing accurate\nsuggestions. Besides, LLMs may generate non-factual responses when there is a\nlack of domain-specific knowledge, which is more risky in the medical domain.\nTo address these challenges, we propose a \\textbf{G}raph-\\textbf{A}ssisted\n\\textbf{P}rompts (\\textbf{GAP}) framework for dialogue-based medication\nrecommendation. It extracts medical concepts and corresponding states from\ndialogue to construct an explicitly patient-centric graph, which can describe\nthe neglected but important information. Further, combined with external\nmedical knowledge graphs, GAP can generate abundant queries and prompts, thus\nretrieving information from multiple sources to reduce the non-factual\nresponses. We evaluate GAP on a dialogue-based medication recommendation\ndataset and further explore its potential in a more difficult scenario,\ndynamically diagnostic interviewing. Extensive experiments demonstrate its\ncompetitive performance when compared with strong baselines."}
{"id": "2505.12742", "pdf": "https://arxiv.org/pdf/2505.12742", "abs": "https://arxiv.org/abs/2505.12742", "authors": ["Jinhua Zhang", "Wei Long", "Minghao Han", "Weiyi You", "Shuhang Gu"], "title": "MVAR: Visual Autoregressive Modeling with Scale and Spatial Markovian Conditioning", "categories": ["cs.CV"], "comment": null, "summary": "Essential to visual generation is efficient modeling of visual data priors.\nConventional next-token prediction methods define the process as learning the\nconditional probability distribution of successive tokens. Recently, next-scale\nprediction methods redefine the process to learn the distribution over\nmulti-scale representations, significantly reducing generation latency.\nHowever, these methods condition each scale on all previous scales and require\neach token to consider all preceding tokens, exhibiting scale and spatial\nredundancy. To better model the distribution by mitigating redundancy, we\npropose Markovian Visual AutoRegressive modeling (MVAR), a novel autoregressive\nframework that introduces scale and spatial Markov assumptions to reduce the\ncomplexity of conditional probability modeling. Specifically, we introduce a\nscale-Markov trajectory that only takes as input the features of adjacent\npreceding scale for next-scale prediction, enabling the adoption of a parallel\ntraining strategy that significantly reduces GPU memory consumption.\nFurthermore, we propose spatial-Markov attention, which restricts the attention\nof each token to a localized neighborhood of size k at corresponding positions\non adjacent scales, rather than attending to every token across these scales,\nfor the pursuit of reduced modeling complexity. Building on these improvements,\nwe reduce the computational complexity of attention calculation from O(N^2) to\nO(Nk), enabling training with just eight NVIDIA RTX 4090 GPUs and eliminating\nthe need for KV cache during inference. Extensive experiments on ImageNet\ndemonstrate that MVAR achieves comparable or superior performance with both\nsmall model trained from scratch and large fine-tuned models, while reducing\nthe average GPU memory footprint by 3.0x."}
{"id": "2505.12896", "pdf": "https://arxiv.org/pdf/2505.12896", "abs": "https://arxiv.org/abs/2505.12896", "authors": ["Chenxi Liu", "Yongqiang Chen", "Tongliang Liu", "James Cheng", "Bo Han", "Kun Zhang"], "title": "On the Thinking-Language Modeling Gap in Large Language Models", "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": "Chenxi and Yongqiang contributed equally; project page:\n  https://causalcoat.github.io/lot.html", "summary": "System 2 reasoning is one of the defining characteristics of intelligence,\nwhich requires slow and logical thinking. Human conducts System 2 reasoning via\nthe language of thoughts that organizes the reasoning process as a causal\nsequence of mental language, or thoughts. Recently, it has been observed that\nSystem 2 reasoning can be elicited from Large Language Models (LLMs)\npre-trained on large-scale natural languages. However, in this work, we show\nthat there is a significant gap between the modeling of languages and thoughts.\nAs language is primarily a tool for humans to share knowledge and thinking,\nmodeling human language can easily absorb language biases into LLMs deviated\nfrom the chain of thoughts in minds. Furthermore, we show that the biases will\nmislead the eliciting of \"thoughts\" in LLMs to focus only on a biased part of\nthe premise. To this end, we propose a new prompt technique termed\nLanguage-of-Thoughts (LoT) to demonstrate and alleviate this gap. Instead of\ndirectly eliciting the chain of thoughts from partial information, LoT\ninstructs LLMs to adjust the order and token used for the expressions of all\nthe relevant information. We show that the simple strategy significantly\nreduces the language modeling biases in LLMs and improves the performance of\nLLMs across a variety of reasoning tasks."}
{"id": "2505.12753", "pdf": "https://arxiv.org/pdf/2505.12753", "abs": "https://arxiv.org/abs/2505.12753", "authors": ["Martha Teiko Teye", "Ori Maoz", "Matthias Rottmann"], "title": "LiDAR MOT-DETR: A LiDAR-based Two-Stage Transformer for 3D Multiple Object Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Multi-object tracking from LiDAR point clouds presents unique challenges due\nto the sparse and irregular nature of the data, compounded by the need for\ntemporal coherence across frames. Traditional tracking systems often rely on\nhand-crafted features and motion models, which can struggle to maintain\nconsistent object identities in crowded or fast-moving scenes. We present a\nlidar-based two-staged DETR inspired transformer; a smoother and tracker. The\nsmoother stage refines lidar object detections, from any off-the-shelf\ndetector, across a moving temporal window. The tracker stage uses a DETR-based\nattention block to maintain tracks across time by associating tracked objects\nwith the refined detections using the point cloud as context. The model is\ntrained on the datasets nuScenes and KITTI in both online and offline (forward\npeeking) modes demonstrating strong performance across metrics such as\nID-switch and multiple object tracking accuracy (MOTA). The numerical results\nindicate that the online mode outperforms the lidar-only baseline and SOTA\nmodels on the nuScenes dataset, with an aMOTA of 0.722 and an aMOTP of 0.475,\nwhile the offline mode provides an additional 3 pp aMOTP"}
{"id": "2505.12920", "pdf": "https://arxiv.org/pdf/2505.12920", "abs": "https://arxiv.org/abs/2505.12920", "authors": ["Paul Van Eecke", "Katrien Beuls"], "title": "PyFCG: Fluid Construction Grammar in Python", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": null, "summary": "We present PyFCG, an open source software library that ports Fluid\nConstruction Grammar (FCG) to the Python programming language. PyFCG enables\nits users to seamlessly integrate FCG functionality into Python programs, and\nto use FCG in combination with other libraries within Python's rich ecosystem.\nApart from a general description of the library, this paper provides three\nwalkthrough tutorials that demonstrate example usage of PyFCG in typical use\ncases of FCG: (i) formalising and testing construction grammar analyses, (ii)\nlearning usage-based construction grammars from corpora, and (iii) implementing\nagent-based experiments on emergent communication."}
{"id": "2505.12758", "pdf": "https://arxiv.org/pdf/2505.12758", "abs": "https://arxiv.org/abs/2505.12758", "authors": ["Matias Quintana", "Youlong Gu", "Xiucheng Liang", "Yujun Hou", "Koichi Ito", "Yihan Zhu", "Mahmoud Abdelrahman", "Filip Biljecki"], "title": "It's not you, it's me -- Global urban visual perception varies across demographics and personalities", "categories": ["cs.CV", "cs.LG"], "comment": "Under review", "summary": "Understanding people's preferences and needs is crucial for urban planning\ndecisions, yet current approaches often combine them from multi-cultural and\nmulti-city populations, obscuring important demographic differences and risking\namplifying biases. We conducted a large-scale urban visual perception survey of\nstreetscapes worldwide using street view imagery, examining how demographics --\nincluding gender, age, income, education, race and ethnicity, and, for the\nfirst time, personality traits -- shape perceptions among 1,000 participants,\nwith balanced demographics, from five countries and 45 nationalities. This\ndataset, introduced as Street Perception Evaluation Considering Socioeconomics\n(SPECS), exhibits statistically significant differences in perception scores in\nsix traditionally used indicators (safe, lively, wealthy, beautiful, boring,\nand depressing) and four new ones we propose (live nearby, walk, cycle, green)\namong demographics and personalities. We revealed that location-based\nsentiments are carried over in people's preferences when comparing urban\nstreetscapes with other cities. Further, we compared the perception scores\nbased on where participants and streetscapes are from. We found that an\noff-the-shelf machine learning model trained on an existing global perception\ndataset tends to overestimate positive indicators and underestimate negative\nones compared to human responses, suggesting that targeted intervention should\nconsider locals' perception. Our study aspires to rectify the myopic treatment\nof street perception, which rarely considers demographics or personality\ntraits."}
{"id": "2505.12929", "pdf": "https://arxiv.org/pdf/2505.12929", "abs": "https://arxiv.org/abs/2505.12929", "authors": ["Zhihe Yang", "Xufang Luo", "Zilong Wang", "Dongqi Han", "Zhiyuan He", "Dongsheng Li", "Yunjian Xu"], "title": "Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "24 pages, 12 figures", "summary": "Reinforcement learning (RL) has become a cornerstone for enhancing the\nreasoning capabilities of large language models (LLMs), with recent innovations\nsuch as Group Relative Policy Optimization (GRPO) demonstrating exceptional\neffectiveness. In this study, we identify a critical yet underexplored issue in\nRL training: low-probability tokens disproportionately influence model updates\ndue to their large gradient magnitudes. This dominance hinders the effective\nlearning of high-probability tokens, whose gradients are essential for LLMs'\nperformance but are substantially suppressed. To mitigate this interference, we\npropose two novel methods: Advantage Reweighting and Low-Probability Token\nIsolation (Lopti), both of which effectively attenuate gradients from\nlow-probability tokens while emphasizing parameter updates driven by\nhigh-probability tokens. Our approaches promote balanced updates across tokens\nwith varying probabilities, thereby enhancing the efficiency of RL training.\nExperimental results demonstrate that they substantially improve the\nperformance of GRPO-trained LLMs, achieving up to a 46.2% improvement in K&K\nLogic Puzzle reasoning tasks. Our implementation is available at\nhttps://github.com/zhyang2226/AR-Lopti."}
{"id": "2505.12766", "pdf": "https://arxiv.org/pdf/2505.12766", "abs": "https://arxiv.org/abs/2505.12766", "authors": ["Haibin He", "Maoyuan Ye", "Jing Zhang", "Xiantao Cai", "Juhua Liu", "Bo Du", "Dacheng Tao"], "title": "Reasoning-OCR: Can Large Multimodal Models Solve Complex Logical Reasoning Problems from OCR Cues?", "categories": ["cs.CV"], "comment": null, "summary": "Large Multimodal Models (LMMs) have become increasingly versatile,\naccompanied by impressive Optical Character Recognition (OCR) related\ncapabilities. Existing OCR-related benchmarks emphasize evaluating LMMs'\nabilities of relatively simple visual question answering, visual-text parsing,\netc. However, the extent to which LMMs can deal with complex logical reasoning\nproblems based on OCR cues is relatively unexplored. To this end, we introduce\nthe Reasoning-OCR benchmark, which challenges LMMs to solve complex reasoning\nproblems based on the cues that can be extracted from rich visual-text.\nReasoning-OCR covers six visual scenarios and encompasses 150 meticulously\ndesigned questions categorized into six reasoning challenges. Additionally,\nReasoning-OCR minimizes the impact of field-specialized knowledge. Our\nevaluation offers some insights for proprietary and open-source LMMs in\ndifferent reasoning challenges, underscoring the urgent to improve the\nreasoning performance. We hope Reasoning-OCR can inspire and facilitate future\nresearch on enhancing complex reasoning ability based on OCR cues.\nReasoning-OCR is publicly available at\nhttps://github.com/Hxyz-123/ReasoningOCR."}
{"id": "2505.12942", "pdf": "https://arxiv.org/pdf/2505.12942", "abs": "https://arxiv.org/abs/2505.12942", "authors": ["Jeffrey T. H. Wong", "Cheng Zhang", "Xinye Cao", "Pedro Gimenes", "George A. Constantinides", "Wayne Luk", "Yiren Zhao"], "title": "A3 : an Analytical Low-Rank Approximation Framework for Attention", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance."}
{"id": "2505.12772", "pdf": "https://arxiv.org/pdf/2505.12772", "abs": "https://arxiv.org/abs/2505.12772", "authors": ["Junyi Hu", "Tian Bai", "Fengyi Wu", "Zhengming Peng", "Yi Zhang"], "title": "Pyramid Sparse Transformer: Enhancing Multi-Scale Feature Fusion with Dynamic Token Selection", "categories": ["cs.CV"], "comment": "13 pages, 5 figures", "summary": "Feature fusion is critical for high-performance vision models but often\nincurs prohibitive complexity. However, prevailing attention-based fusion\nmethods often involve significant computational complexity and implementation\nchallenges, limiting their efficiency in resource-constrained environments. To\naddress these issues, we introduce the Pyramid Sparse Transformer (PST), a\nlightweight, plug-and-play module that integrates coarse-to-fine token\nselection and shared attention parameters to reduce computation while\npreserving spatial detail. PST can be trained using only coarse attention and\nseamlessly activated at inference for further accuracy gains without\nretraining. When added to state-of-the-art real-time detection models, such as\nYOLOv11-N/S/M, PST yields mAP improvements of 0.9%, 0.5%, and 0.4% on MS COCO\nwith minimal latency impact. Likewise, embedding PST into ResNet-18/50/101 as\nbackbones, boosts ImageNet top-1 accuracy by 6.5%, 1.7%, and 1.0%,\nrespectively. These results demonstrate PST's effectiveness as a simple,\nhardware-friendly enhancement for both detection and classification tasks."}
{"id": "2505.12949", "pdf": "https://arxiv.org/pdf/2505.12949", "abs": "https://arxiv.org/abs/2505.12949", "authors": ["Cael Marquard", "Simbarashe Mawere", "Francois Meyer"], "title": "Neural Morphological Tagging for Nguni Languages", "categories": ["cs.CL"], "comment": null, "summary": "Morphological parsing is the task of decomposing words into morphemes, the\nsmallest units of meaning in a language, and labelling their grammatical roles.\nIt is a particularly challenging task for agglutinative languages, such as the\nNguni languages of South Africa, which construct words by concatenating\nmultiple morphemes. A morphological parsing system can be framed as a pipeline\nwith two separate components, a segmenter followed by a tagger. This paper\ninvestigates the use of neural methods to build morphological taggers for the\nfour Nguni languages. We compare two classes of approaches: training neural\nsequence labellers (LSTMs and neural CRFs) from scratch and finetuning\npretrained language models. We compare performance across these two categories,\nas well as to a traditional rule-based morphological parser. Neural taggers\ncomfortably outperform the rule-based baseline and models trained from scratch\ntend to outperform pretrained models. We also compare parsing results across\ndifferent upstream segmenters and with varying linguistic input features. Our\nfindings confirm the viability of employing neural taggers based on\npre-existing morphological segmenters for the Nguni languages."}
{"id": "2505.12789", "pdf": "https://arxiv.org/pdf/2505.12789", "abs": "https://arxiv.org/abs/2505.12789", "authors": ["Hemanth Saratchandran", "Simon Lucey"], "title": "Enhancing Transformers Through Conditioned Embedded Tokens", "categories": ["cs.CV"], "comment": null, "summary": "Transformers have transformed modern machine learning, driving breakthroughs\nin computer vision, natural language processing, and robotics. At the core of\ntheir success lies the attention mechanism, which enables the modeling of\nglobal dependencies among input tokens. However, we reveal that the attention\nblock in transformers suffers from inherent ill-conditioning, which hampers\ngradient-based optimization and leads to inefficient training. To address this,\nwe develop a theoretical framework that establishes a direct relationship\nbetween the conditioning of the attention block and that of the embedded\ntokenized data. Building on this insight, we introduce conditioned embedded\ntokens, a method that systematically modifies the embedded tokens to improve\nthe conditioning of the attention mechanism. Our analysis demonstrates that\nthis approach significantly mitigates ill-conditioning, leading to more stable\nand efficient training. We validate our methodology across various transformer\narchitectures, achieving consistent improvements in image classification,\nobject detection, instance segmentation, and natural language processing,\nhighlighting its broad applicability and effectiveness."}
{"id": "2505.12950", "pdf": "https://arxiv.org/pdf/2505.12950", "abs": "https://arxiv.org/abs/2505.12950", "authors": ["Daehee Kim", "Deokhyung Kang", "Jonghwi Kim", "Sangwon Ryu", "Gary Geunbae Lee"], "title": "GuRE:Generative Query REwriter for Legal Passage Retrieval", "categories": ["cs.CL"], "comment": "14 pages, 9 figures", "summary": "Legal Passage Retrieval (LPR) systems are crucial as they help practitioners\nsave time when drafting legal arguments. However, it remains an underexplored\navenue. One primary reason is the significant vocabulary mismatch between the\nquery and the target passage. To address this, we propose a simple yet\neffective method, the Generative query REwriter (GuRE). We leverage the\ngenerative capabilities of Large Language Models (LLMs) by training the LLM for\nquery rewriting. \"Rewritten queries\" help retrievers to retrieve target\npassages by mitigating vocabulary mismatch. Experimental results show that GuRE\nsignificantly improves performance in a retriever-agnostic manner,\noutperforming all baseline methods. Further analysis reveals that different\ntraining objectives lead to distinct retrieval behaviors, making GuRE more\nsuitable than direct retriever fine-tuning for real-world applications. Codes\nare avaiable at github.com/daehuikim/GuRE."}
{"id": "2505.12803", "pdf": "https://arxiv.org/pdf/2505.12803", "abs": "https://arxiv.org/abs/2505.12803", "authors": ["Jiawen Xu", "Odej Kao", "Margret Keuper"], "title": "Informed Mixing -- Improving Open Set Recognition via Attribution-based Augmentation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Open set recognition (OSR) is devised to address the problem of detecting\nnovel classes during model inference. Even in recent vision models, this\nremains an open issue which is receiving increasing attention. Thereby, a\ncrucial challenge is to learn features that are relevant for unseen categories\nfrom given data, for which these features might not be discriminative. To\nfacilitate this process and \"optimize to learn\" more diverse features, we\npropose GradMix, a data augmentation method that dynamically leverages\ngradient-based attribution maps of the model during training to mask out\nalready learned concepts. Thus GradMix encourages the model to learn a more\ncomplete set of representative features from the same data source. Extensive\nexperiments on open set recognition, close set classification, and\nout-of-distribution detection reveal that our method can often outperform the\nstate-of-the-art. GradMix can further increase model robustness to corruptions\nas well as downstream classification performance for self-supervised learning,\nindicating its benefit for model generalization."}
{"id": "2505.12964", "pdf": "https://arxiv.org/pdf/2505.12964", "abs": "https://arxiv.org/abs/2505.12964", "authors": ["Shanshan Liu", "Noriki Nishida", "Rumana Ferdous Munne", "Narumi Tokunaga", "Yuki Yamagata", "Kouji Kozaki", "Yuji Matsumoto"], "title": "MA-COIR: Leveraging Semantic Search Index and Generative Models for Ontology-Driven Biomedical Concept Recognition", "categories": ["cs.CL"], "comment": "preprint", "summary": "Recognizing biomedical concepts in the text is vital for ontology refinement,\nknowledge graph construction, and concept relationship discovery. However,\ntraditional concept recognition methods, relying on explicit mention\nidentification, often fail to capture complex concepts not explicitly stated in\nthe text. To overcome this limitation, we introduce MA-COIR, a framework that\nreformulates concept recognition as an indexing-recognition task. By assigning\nsemantic search indexes (ssIDs) to concepts, MA-COIR resolves ambiguities in\nontology entries and enhances recognition efficiency. Using a pretrained\nBART-based model fine-tuned on small datasets, our approach reduces\ncomputational requirements to facilitate adoption by domain experts.\nFurthermore, we incorporate large language models (LLMs)-generated queries and\nsynthetic data to improve recognition in low-resource settings. Experimental\nresults on three scenarios (CDR, HPO, and HOIP) highlight the effectiveness of\nMA-COIR in recognizing both explicit and implicit concepts without the need for\nmention-level annotations during inference, advancing ontology-driven concept\nrecognition in biomedical domain applications. Our code and constructed data\nare available at https://github.com/sl-633/macoir-master."}
{"id": "2505.12820", "pdf": "https://arxiv.org/pdf/2505.12820", "abs": "https://arxiv.org/abs/2505.12820", "authors": ["Hulin Li"], "title": "Rethinking Features-Fused-Pyramid-Neck for Object Detection", "categories": ["cs.CV"], "comment": "ECCV 2024", "summary": "Multi-head detectors typically employ a features-fused-pyramid-neck for\nmulti-scale detection and are widely adopted in the industry. However, this\napproach faces feature misalignment when representations from different\nhierarchical levels of the feature pyramid are forcibly fused point-to-point.\nTo address this issue, we designed an independent hierarchy pyramid (IHP)\narchitecture to evaluate the effectiveness of the features-unfused-pyramid-neck\nfor multi-head detectors. Subsequently, we introduced soft nearest neighbor\ninterpolation (SNI) with a weight downscaling factor to mitigate the impact of\nfeature fusion at different hierarchies while preserving key textures.\nFurthermore, we present a features adaptive selection method for down sampling\nin extended spatial windows (ESD) to retain spatial features and enhance\nlightweight convolutional techniques (GSConvE). These advancements culminate in\nour secondary features alignment solution (SA) for real-time detection,\nachieving state-of-the-art results on Pascal VOC and MS COCO. Code will be\nreleased at https://github.com/AlanLi1997/rethinking-fpn. This paper has been\naccepted by ECCV2024 and published on Springer Nature."}
{"id": "2505.12969", "pdf": "https://arxiv.org/pdf/2505.12969", "abs": "https://arxiv.org/abs/2505.12969", "authors": ["Yingzhi Wang", "Anas Alhmoud", "Saad Alsahly", "Muhammad Alqurishi", "Mirco Ravanelli"], "title": "Calm-Whisper: Reduce Whisper Hallucination On Non-Speech By Calming Crazy Heads Down", "categories": ["cs.CL"], "comment": "Accepted to Interspeech 2025", "summary": "OpenAI's Whisper has achieved significant success in Automatic Speech\nRecognition. However, it has consistently been found to exhibit hallucination\nissues, particularly in non-speech segments, which limits its broader\napplication in complex industrial settings.\n  In this paper, we introduce a novel method to reduce Whisper's hallucination\non non-speech segments without using any pre- or post-possessing techniques.\nSpecifically, we benchmark the contribution of each self-attentional head in\nthe Whisper-large-v3 decoder to the hallucination problem by performing a\nhead-wise mask. Our findings reveal that only 3 of the 20 heads account for\nover 75% of the hallucinations on the UrbanSound dataset. We then fine-tune\nthese three crazy heads using a collection of non-speech data. The results show\nthat our best fine-tuned model, namely Calm-Whisper, achieves over 80%\nreduction in non-speech hallucination with only less than 0.1% WER degradation\non LibriSpeech test-clean and test-other."}
{"id": "2505.12826", "pdf": "https://arxiv.org/pdf/2505.12826", "abs": "https://arxiv.org/abs/2505.12826", "authors": ["Jianfeng Cai", "Wengang Zhou", "Zongmeng Zhang", "Jiale Hong", "Nianji Zhan", "Houqiang Li"], "title": "Mitigating Hallucination in VideoLLMs via Temporal-Aware Activation Engineering", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal large language models (MLLMs) have achieved remarkable progress in\nvideo understanding.However, hallucination, where the model generates plausible\nyet incorrect outputs, persists as a significant and under-addressed challenge\nin the video domain. Among existing solutions, activation engineering has\nproven successful in mitigating hallucinations in LLMs and ImageLLMs, yet its\napplicability to VideoLLMs remains largely unexplored. In this work, we are the\nfirst to systematically investigate the effectiveness and underlying mechanisms\nof activation engineering for mitigating hallucinations in VideoLLMs. We\ninitially conduct an investigation of the key factors affecting the performance\nof activation engineering and find that a model's sensitivity to hallucination\ndepends on $\\textbf{temporal variation}$ rather than task type. Moreover,\nselecting appropriate internal modules and dataset for activation engineering\nis critical for reducing hallucination. Guided by these findings, we propose a\ntemporal-aware activation engineering framework for VideoLLMs, which adaptively\nidentifies and manipulates hallucination-sensitive modules based on the\ntemporal variation characteristic, substantially mitigating hallucinations\nwithout additional LLM fine-tuning. Experiments across multiple models and\nbenchmarks demonstrate that our method markedly reduces hallucination in\nVideoLLMs, thereby validating the robustness of our findings."}
{"id": "2505.12970", "pdf": "https://arxiv.org/pdf/2505.12970", "abs": "https://arxiv.org/abs/2505.12970", "authors": ["Robin Jegan", "Andreas Henrich"], "title": "A Structured Literature Review on Traditional Approaches in Current Natural Language Processing", "categories": ["cs.CL"], "comment": "14 pages, 1 figure", "summary": "The continued rise of neural networks and large language models in the more\nrecent past has altered the natural language processing landscape, enabling new\napproaches towards typical language tasks and achieving mainstream success.\nDespite the huge success of large language models, many disadvantages still\nremain and through this work we assess the state of the art in five application\nscenarios with a particular focus on the future perspectives and sensible\napplication scenarios of traditional and older approaches and techniques.\n  In this paper we survey recent publications in the application scenarios\nclassification, information and relation extraction, text simplification as\nwell as text summarization. After defining our terminology, i.e., which\nfeatures are characteristic for traditional techniques in our interpretation\nfor the five scenarios, we survey if such traditional approaches are still\nbeing used, and if so, in what way they are used. It turns out that all five\napplication scenarios still exhibit traditional models in one way or another,\nas part of a processing pipeline, as a comparison/baseline to the core model of\nthe respective paper, or as the main model(s) of the paper. For the complete\nstatistics, see https://zenodo.org/records/13683801"}
{"id": "2505.12834", "pdf": "https://arxiv.org/pdf/2505.12834", "abs": "https://arxiv.org/abs/2505.12834", "authors": ["Avinash Kumar", "Kyeolhee Kang", "Ammar ul Hassan", "Jaeyoung Choi"], "title": "A Study on the Refining Handwritten Font by Mixing Font Styles", "categories": ["cs.CV"], "comment": "4 pages, 3 figures, MITA 2023 (The 19th International Conference on\n  Multimedia Information Technology and Applications July. 11 ~ July 14, 2023,\n  Technical University of Ostrava, Ostrava, Czech)", "summary": "Handwritten fonts have a distinct expressive character, but they are often\ndifficult to read due to unclear or inconsistent handwriting. FontFusionGAN\n(FFGAN) is a novel method for improving handwritten fonts by combining them\nwith printed fonts. Our method implements generative adversarial network (GAN)\nto generate font that mix the desirable features of handwritten and printed\nfonts. By training the GAN on a dataset of handwritten and printed fonts, it\ncan generate legible and visually appealing font images. We apply our method to\na dataset of handwritten fonts and demonstrate that it significantly enhances\nthe readability of the original fonts while preserving their unique aesthetic.\nOur method has the potential to improve the readability of handwritten fonts,\nwhich would be helpful for a variety of applications including document\ncreation, letter writing, and assisting individuals with reading and writing\ndifficulties. In addition to addressing the difficulties of font creation for\nlanguages with complex character sets, our method is applicable to other\ntext-image-related tasks, such as font attribute control and multilingual font\nstyle transfer."}
{"id": "2505.12973", "pdf": "https://arxiv.org/pdf/2505.12973", "abs": "https://arxiv.org/abs/2505.12973", "authors": ["Mahta Fetrat Qharabagh", "Zahra Dehghanian", "Hamid R. Rabiee"], "title": "Fast, Not Fancy: Rethinking G2P with Rich Data and Rule-Based Models", "categories": ["cs.CL"], "comment": "8 main body pages, total 25 pages, 15 figures", "summary": "Homograph disambiguation remains a significant challenge in\ngrapheme-to-phoneme (G2P) conversion, especially for low-resource languages.\nThis challenge is twofold: (1) creating balanced and comprehensive homograph\ndatasets is labor-intensive and costly, and (2) specific disambiguation\nstrategies introduce additional latency, making them unsuitable for real-time\napplications such as screen readers and other accessibility tools. In this\npaper, we address both issues. First, we propose a semi-automated pipeline for\nconstructing homograph-focused datasets, introduce the HomoRich dataset\ngenerated through this pipeline, and demonstrate its effectiveness by applying\nit to enhance a state-of-the-art deep learning-based G2P system for Persian.\nSecond, we advocate for a paradigm shift - utilizing rich offline datasets to\ninform the development of fast, rule-based methods suitable for\nlatency-sensitive accessibility applications like screen readers. To this end,\nwe improve one of the most well-known rule-based G2P systems, eSpeak, into a\nfast homograph-aware version, HomoFast eSpeak. Our results show an approximate\n30% improvement in homograph disambiguation accuracy for the deep\nlearning-based and eSpeak systems."}
{"id": "2505.12849", "pdf": "https://arxiv.org/pdf/2505.12849", "abs": "https://arxiv.org/abs/2505.12849", "authors": ["Ben Liu", "Zhen Qin"], "title": "Accelerate TarFlow Sampling with GS-Jacobi Iteration", "categories": ["cs.CV"], "comment": "17 pages, 7 figures, 5 tables", "summary": "Image generation models have achieved widespread applications. As an\ninstance, the TarFlow model combines the transformer architecture with\nNormalizing Flow models, achieving state-of-the-art results on multiple\nbenchmarks. However, due to the causal form of attention requiring sequential\ncomputation, TarFlow's sampling process is extremely slow. In this paper, we\ndemonstrate that through a series of optimization strategies, TarFlow sampling\ncan be greatly accelerated by using the Gauss-Seidel-Jacobi (abbreviated as\nGS-Jacobi) iteration method. Specifically, we find that blocks in the TarFlow\nmodel have varying importance: a small number of blocks play a major role in\nimage generation tasks, while other blocks contribute relatively little; some\nblocks are sensitive to initial values and prone to numerical overflow, while\nothers are relatively robust. Based on these two characteristics, we propose\nthe Convergence Ranking Metric (CRM) and the Initial Guessing Metric (IGM): CRM\nis used to identify whether a TarFlow block is \"simple\" (converges in few\niterations) or \"tough\" (requires more iterations); IGM is used to evaluate\nwhether the initial value of the iteration is good. Experiments on four TarFlow\nmodels demonstrate that GS-Jacobi sampling can significantly enhance sampling\nefficiency while maintaining the quality of generated images (measured by FID),\nachieving speed-ups of 4.53x in Img128cond, 5.32x in AFHQ, 2.96x in\nImg64uncond, and 2.51x in Img64cond without degrading FID scores or sample\nquality. Code and checkpoints are accessible on\nhttps://github.com/encoreus/GS-Jacobi_for_TarFlow"}
{"id": "2505.12983", "pdf": "https://arxiv.org/pdf/2505.12983", "abs": "https://arxiv.org/abs/2505.12983", "authors": ["Jiaan Wang", "Fandong Meng", "Zengkui Sun", "Yunlong Liang", "Yuxuan Cao", "Jiarong Xu", "Haoxiang Shi", "Jie Zhou"], "title": "An Empirical Study of Many-to-Many Summarization with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 main conference", "summary": "Many-to-many summarization (M2MS) aims to process documents in any language\nand generate the corresponding summaries also in any language. Recently, large\nlanguage models (LLMs) have shown strong multi-lingual abilities, giving them\nthe potential to perform M2MS in real applications. This work presents a\nsystematic empirical study on LLMs' M2MS ability. Specifically, we first\nreorganize M2MS data based on eight previous domain-specific datasets. The\nreorganized data contains 47.8K samples spanning five domains and six\nlanguages, which could be used to train and evaluate LLMs. Then, we benchmark\n18 LLMs in a zero-shot manner and an instruction-tuning manner. Fine-tuned\ntraditional models (e.g., mBART) are also conducted for comparisons. Our\nexperiments reveal that, zero-shot LLMs achieve competitive results with\nfine-tuned traditional models. After instruct-tuning, open-source LLMs can\nsignificantly improve their M2MS ability, and outperform zero-shot LLMs\n(including GPT-4) in terms of automatic evaluations. In addition, we\ndemonstrate that this task-specific improvement does not sacrifice the LLMs'\ngeneral task-solving abilities. However, as revealed by our human evaluation,\nLLMs still face the factuality issue, and the instruction tuning might\nintensify the issue. Thus, how to control factual errors becomes the key when\nbuilding LLM summarizers in real applications, and is worth noting in future\nresearch."}
{"id": "2505.12854", "pdf": "https://arxiv.org/pdf/2505.12854", "abs": "https://arxiv.org/abs/2505.12854", "authors": ["Anna Maschek", "David C. Schedl"], "title": "The Way Up: A Dataset for Hold Usage Detection in Sport Climbing", "categories": ["cs.CV"], "comment": "accepted at the International Workshop on Computer Vision in Sports\n  (CVsports) at CVPR 2025", "summary": "Detecting an athlete's position on a route and identifying hold usage are\ncrucial in various climbing-related applications. However, no climbing dataset\nwith detailed hold usage annotations exists to our knowledge. To address this\nissue, we introduce a dataset of 22 annotated climbing videos, providing\nground-truth labels for hold locations, usage order, and time of use.\nFurthermore, we explore the application of keypoint-based 2D pose-estimation\nmodels for detecting hold usage in sport climbing. We determine usage by\nanalyzing the key points of certain joints and the corresponding overlap with\nclimbing holds. We evaluate multiple state-of-the-art models and analyze their\naccuracy on our dataset, identifying and highlighting climbing-specific\nchallenges. Our dataset and results highlight key challenges in\nclimbing-specific pose estimation and establish a foundation for future\nresearch toward AI-assisted systems for sports climbing."}
{"id": "2505.12996", "pdf": "https://arxiv.org/pdf/2505.12996", "abs": "https://arxiv.org/abs/2505.12996", "authors": ["Jiaan Wang", "Fandong Meng", "Jie Zhou"], "title": "ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 2 figures", "summary": "In recent years, the emergence of large reasoning models (LRMs), such as\nOpenAI-o1 and DeepSeek-R1, has shown impressive capabilities in complex\nproblems, e.g., mathematics and coding. Some pioneering studies attempt to\nbring the success of LRMs in neural machine translation (MT). They try to build\nLRMs with deep reasoning MT ability via reinforcement learning (RL). Despite\nsome progress that has been made, these attempts generally focus on several\nhigh-resource languages, e.g., English and Chinese, leaving the performance on\nother languages unclear. Besides, the reward modeling methods in previous work\ndo not fully unleash the potential of reinforcement learning in MT. In this\nwork, we first design a new reward modeling method that compares the\ntranslation results of the policy MT model with a strong LRM (i.e.,\nDeepSeek-R1-671B), and quantifies the comparisons to provide rewards.\nExperimental results demonstrate the superiority of the reward modeling method.\nUsing Qwen2.5-7B-Instruct as the backbone, the trained model achieves the new\nstate-of-the-art performance in literary translation, and outperforms strong\nLRMs including OpenAI-o1 and DeepSeeK-R1. Furthermore, we extend our method to\nthe multilingual settings with 11 languages. With a carefully designed\nlightweight reward modeling in RL, we can simply transfer the strong MT ability\nfrom a single direction into multiple (i.e., 90) translation directions and\nachieve impressive multilingual MT performance."}
{"id": "2505.12860", "pdf": "https://arxiv.org/pdf/2505.12860", "abs": "https://arxiv.org/abs/2505.12860", "authors": ["Wenbo Yang", "Zhongling Wang", "Zhou Wang"], "title": "Towards a Universal Image Degradation Model via Content-Degradation Disentanglement", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Image degradation synthesis is highly desirable in a wide variety of\napplications ranging from image restoration to simulating artistic effects.\nExisting models are designed to generate one specific or a narrow set of\ndegradations, which often require user-provided degradation parameters. As a\nresult, they lack the generalizability to synthesize degradations beyond their\ninitial design or adapt to other applications. Here we propose the first\nuniversal degradation model that can synthesize a broad spectrum of complex and\nrealistic degradations containing both homogeneous (global) and inhomogeneous\n(spatially varying) components. Our model automatically extracts and\ndisentangles homogeneous and inhomogeneous degradation features, which are\nlater used for degradation synthesis without user intervention. A\ndisentangle-by-compression method is proposed to separate degradation\ninformation from images. Two novel modules for extracting and incorporating\ninhomogeneous degradations are created to model inhomogeneous components in\ncomplex degradations. We demonstrate the model's accuracy and adaptability in\nfilm-grain simulation and blind image restoration tasks. The demo video, code,\nand dataset of this project will be released upon publication at\ngithub.com/yangwenbo99/content-degradation-disentanglement."}
{"id": "2505.13004", "pdf": "https://arxiv.org/pdf/2505.13004", "abs": "https://arxiv.org/abs/2505.13004", "authors": ["Yuhao Qing", "Boyu Zhu", "Mingzhe Du", "Zhijiang Guo", "Terry Yue Zhuo", "Qianru Zhang", "Jie M. Zhang", "Heming Cui", "Siu-Ming Yiu", "Dong Huang", "See-Kiong Ng", "Luu Anh Tuan"], "title": "EffiBench-X: A Multi-Language Benchmark for Measuring Efficiency of LLM-Generated Code", "categories": ["cs.CL"], "comment": "Under Review", "summary": "Existing code generation benchmarks primarily evaluate functional\ncorrectness, with limited focus on code efficiency and often restricted to a\nsingle language like Python. To address this gap, we introduce EffiBench-X, the\nfirst multi-language benchmark designed to measure the efficiency of\nLLM-generated code. EffiBench-X supports Python, C++, Java, JavaScript, Ruby,\nand Golang. It comprises competitive programming tasks with human-expert\nsolutions as efficiency baselines. Evaluating state-of-the-art LLMs on\nEffiBench-X reveals that while models generate functionally correct code, they\nconsistently underperform human experts in efficiency. Even the most efficient\nLLM-generated solutions (Qwen3-32B) achieve only around \\textbf{62\\%} of human\nefficiency on average, with significant language-specific variations. LLMs show\nbetter efficiency in Python, Ruby, and JavaScript than in Java, C++, and\nGolang. For instance, DeepSeek-R1's Python code is significantly more efficient\nthan its Java code. These results highlight the critical need for research into\nLLM optimization techniques to improve code efficiency across diverse\nlanguages. The dataset and evaluation infrastructure are submitted and\navailable at https://github.com/EffiBench/EffiBench-X.git and\nhttps://huggingface.co/datasets/EffiBench/effibench-x."}
{"id": "2505.12861", "pdf": "https://arxiv.org/pdf/2505.12861", "abs": "https://arxiv.org/abs/2505.12861", "authors": ["Jiaqi Tan", "Xu Zheng", "Yang Liu"], "title": "Robust Multimodal Segmentation with Representation Regularization and Hybrid Prototype Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Multi-modal semantic segmentation (MMSS) faces significant challenges in\nreal-world scenarios due to dynamic environments, sensor failures, and noise\ninterference, creating a gap between theoretical models and practical\nperformance. To address this, we propose a two-stage framework called\nRobustSeg, which enhances multi-modal robustness through two key components:\nthe Hybrid Prototype Distillation Module (HPDM) and the Representation\nRegularization Module (RRM). In the first stage, RobustSeg pre-trains a\nmulti-modal teacher model using complete modalities. In the second stage, a\nstudent model is trained with random modality dropout while learning from the\nteacher via HPDM and RRM. HPDM transforms features into compact prototypes,\nenabling cross-modal hybrid knowledge distillation and mitigating bias from\nmissing modalities. RRM reduces representation discrepancies between the\nteacher and student by optimizing functional entropy through the log-Sobolev\ninequality. Extensive experiments on three public benchmarks demonstrate that\nRobustSeg outperforms previous state-of-the-art methods, achieving improvements\nof +2.76%, +4.56%, and +0.98%, respectively. Code is available at:\nhttps://github.com/RobustSeg/RobustSeg."}
{"id": "2505.13006", "pdf": "https://arxiv.org/pdf/2505.13006", "abs": "https://arxiv.org/abs/2505.13006", "authors": ["Yuyang Li", "Philip J. M. Kerbusch", "Raimon H. R. Pruim", "Tobias Käfer"], "title": "Evaluating the Performance of RAG Methods for Conversational AI in the Airport Domain", "categories": ["cs.CL"], "comment": "Accepted by NAACL 2025 industry track", "summary": "Airports from the top 20 in terms of annual passengers are highly dynamic\nenvironments with thousands of flights daily, and they aim to increase the\ndegree of automation. To contribute to this, we implemented a Conversational AI\nsystem that enables staff in an airport to communicate with flight information\nsystems. This system not only answers standard airport queries but also\nresolves airport terminology, jargon, abbreviations, and dynamic questions\ninvolving reasoning. In this paper, we built three different\nRetrieval-Augmented Generation (RAG) methods, including traditional RAG, SQL\nRAG, and Knowledge Graph-based RAG (Graph RAG). Experiments showed that\ntraditional RAG achieved 84.84% accuracy using BM25 + GPT-4 but occasionally\nproduced hallucinations, which is risky to airport safety. In contrast, SQL RAG\nand Graph RAG achieved 80.85% and 91.49% accuracy respectively, with\nsignificantly fewer hallucinations. Moreover, Graph RAG was especially\neffective for questions that involved reasoning. Based on our observations, we\nthus recommend SQL RAG and Graph RAG are better for airport environments, due\nto fewer hallucinations and the ability to handle dynamic questions."}
{"id": "2505.12890", "pdf": "https://arxiv.org/pdf/2505.12890", "abs": "https://arxiv.org/abs/2505.12890", "authors": ["Ege Özsoy", "Chantal Pellegrini", "David Bani-Harouni", "Kun Yuan", "Matthias Keicher", "Nassir Navab"], "title": "ORQA: A Benchmark and Foundation Model for Holistic Operating Room Modeling", "categories": ["cs.CV"], "comment": null, "summary": "The real-world complexity of surgeries necessitates surgeons to have deep and\nholistic comprehension to ensure precision, safety, and effective\ninterventions. Computational systems are required to have a similar level of\ncomprehension within the operating room. Prior works, limited to single-task\nefforts like phase recognition or scene graph generation, lack scope and\ngeneralizability. In this work, we introduce ORQA, a novel OR question\nanswering benchmark and foundational multimodal model to advance OR\nintelligence. By unifying all four public OR datasets into a comprehensive\nbenchmark, we enable our approach to concurrently address a diverse range of OR\nchallenges. The proposed multimodal large language model fuses diverse OR\nsignals such as visual, auditory, and structured data, for a holistic modeling\nof the OR. Finally, we propose a novel, progressive knowledge distillation\nparadigm, to generate a family of models optimized for different speed and\nmemory requirements. We show the strong performance of ORQA on our proposed\nbenchmark, and its zero-shot generalization, paving the way for scalable,\nunified OR modeling and significantly advancing multimodal surgical\nintelligence. We will release our code and data upon acceptance."}
{"id": "2505.13010", "pdf": "https://arxiv.org/pdf/2505.13010", "abs": "https://arxiv.org/abs/2505.13010", "authors": ["Himel Ghosh", "Ahmed Mosharafa", "Georg Groh"], "title": "To Bias or Not to Bias: Detecting bias in News with bias-detector", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "7 pages, 5 figures, 2 tables", "summary": "Media bias detection is a critical task in ensuring fair and balanced\ninformation dissemination, yet it remains challenging due to the subjectivity\nof bias and the scarcity of high-quality annotated data. In this work, we\nperform sentence-level bias classification by fine-tuning a RoBERTa-based model\non the expert-annotated BABE dataset. Using McNemar's test and the 5x2\ncross-validation paired t-test, we show statistically significant improvements\nin performance when comparing our model to a domain-adaptively pre-trained\nDA-RoBERTa baseline. Furthermore, attention-based analysis shows that our model\navoids common pitfalls like oversensitivity to politically charged terms and\ninstead attends more meaningfully to contextually relevant tokens. For a\ncomprehensive examination of media bias, we present a pipeline that combines\nour model with an already-existing bias-type classifier. Our method exhibits\ngood generalization and interpretability, despite being constrained by\nsentence-level analysis and dataset size because of a lack of larger and more\nadvanced bias corpora. We talk about context-aware modeling, bias\nneutralization, and advanced bias type classification as potential future\ndirections. Our findings contribute to building more robust, explainable, and\nsocially responsible NLP systems for media bias detection."}
{"id": "2505.12897", "pdf": "https://arxiv.org/pdf/2505.12897", "abs": "https://arxiv.org/abs/2505.12897", "authors": ["Piotr Borycki", "Magdalena Trędowicz", "Szymon Janusz", "Jacek Tabor", "Przemysław Spurek", "Arkadiusz Lewicki", "Łukasz Struski"], "title": "EPIC: Explanation of Pretrained Image Classification Networks via Prototype", "categories": ["cs.CV"], "comment": null, "summary": "Explainable AI (XAI) methods generally fall into two categories. Post-hoc\napproaches generate explanations for pre-trained models and are compatible with\nvarious neural network architectures. These methods often use feature\nimportance visualizations, such as saliency maps, to indicate which input\nregions influenced the model's prediction. Unfortunately, they typically offer\na coarse understanding of the model's decision-making process. In contrast,\nante-hoc (inherently explainable) methods rely on specially designed model\narchitectures trained from scratch. A notable subclass of these methods\nprovides explanations through prototypes, representative patches extracted from\nthe training data. However, prototype-based approaches have limitations: they\nrequire dedicated architectures, involve specialized training procedures, and\nperform well only on specific datasets. In this work, we propose EPIC\n(Explanation of Pretrained Image Classification), a novel approach that bridges\nthe gap between these two paradigms. Like post-hoc methods, EPIC operates on\npre-trained models without architectural modifications. Simultaneously, it\ndelivers intuitive, prototype-based explanations inspired by ante-hoc\ntechniques. To the best of our knowledge, EPIC is the first post-hoc method\ncapable of fully replicating the core explanatory power of inherently\ninterpretable models. We evaluate EPIC on benchmark datasets commonly used in\nprototype-based explanations, such as CUB-200-2011 and Stanford Cars, alongside\nlarge-scale datasets like ImageNet, typically employed by post-hoc methods.\nEPIC uses prototypes to explain model decisions, providing a flexible and\neasy-to-understand tool for creating clear, high-quality explanations."}
{"id": "2505.13034", "pdf": "https://arxiv.org/pdf/2505.13034", "abs": "https://arxiv.org/abs/2505.13034", "authors": ["Márton Kardos", "Kenneth C. Enevoldsen", "Kristoffer Laigaard Nielbo"], "title": "topicwizard -- a Modern, Model-agnostic Framework for Topic Model Visualization and Interpretation", "categories": ["cs.CL"], "comment": "9 pages, 9 figures", "summary": "Topic models are statistical tools that allow their users to gain qualitative\nand quantitative insights into the contents of textual corpora without the need\nfor close reading. They can be applied in a wide range of settings from\ndiscourse analysis, through pretraining data curation, to text filtering. Topic\nmodels are typically parameter-rich, complex models, and interpreting these\nparameters can be challenging for their users. It is typical practice for users\nto interpret topics based on the top 10 highest ranking terms on a given topic.\nThis list-of-words approach, however, gives users a limited and biased picture\nof the content of topics. Thoughtful user interface design and visualizations\ncan help users gain a more complete and accurate understanding of topic models'\noutput. While some visualization utilities do exist for topic models, these are\ntypically limited to a certain type of topic model. We introduce topicwizard, a\nframework for model-agnostic topic model interpretation, that provides\nintuitive and interactive tools that help users examine the complex semantic\nrelations between documents, words and topics learned by topic models."}
{"id": "2505.12903", "pdf": "https://arxiv.org/pdf/2505.12903", "abs": "https://arxiv.org/abs/2505.12903", "authors": ["Shiao Wang", "Xiao Wang", "Liye Jin", "Bo Jiang", "Lin Zhu", "Lan Chen", "Yonghong Tian", "Bin Luo"], "title": "Towards Low-Latency Event Stream-based Visual Object Tracking: A Slow-Fast Approach", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Existing tracking algorithms typically rely on low-frame-rate RGB cameras\ncoupled with computationally intensive deep neural network architectures to\nachieve effective tracking. However, such frame-based methods inherently face\nchallenges in achieving low-latency performance and often fail in\nresource-constrained environments. Visual object tracking using bio-inspired\nevent cameras has emerged as a promising research direction in recent years,\noffering distinct advantages for low-latency applications. In this paper, we\npropose a novel Slow-Fast Tracking paradigm that flexibly adapts to different\noperational requirements, termed SFTrack. The proposed framework supports two\ncomplementary modes, i.e., a high-precision slow tracker for scenarios with\nsufficient computational resources, and an efficient fast tracker tailored for\nlatency-aware, resource-constrained environments. Specifically, our framework\nfirst performs graph-based representation learning from\nhigh-temporal-resolution event streams, and then integrates the learned\ngraph-structured information into two FlashAttention-based vision backbones,\nyielding the slow and fast trackers, respectively. The fast tracker achieves\nlow latency through a lightweight network design and by producing multiple\nbounding box outputs in a single forward pass. Finally, we seamlessly combine\nboth trackers via supervised fine-tuning and further enhance the fast tracker's\nperformance through a knowledge distillation strategy. Extensive experiments on\npublic benchmarks, including FE240, COESOT, and EventVOT, demonstrate the\neffectiveness and efficiency of our proposed method across different real-world\nscenarios. The source code has been released on\nhttps://github.com/Event-AHU/SlowFast_Event_Track."}
{"id": "2505.13036", "pdf": "https://arxiv.org/pdf/2505.13036", "abs": "https://arxiv.org/abs/2505.13036", "authors": ["Sai Koneru", "Maike Züfle", "Thai-Binh Nguyen", "Seymanur Akti", "Jan Niehues", "Alexander Waibel"], "title": "KIT's Offline Speech Translation and Instruction Following Submission for IWSLT 2025", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The scope of the International Workshop on Spoken Language Translation\n(IWSLT) has recently broadened beyond traditional Speech Translation (ST) to\nencompass a wider array of tasks, including Speech Question Answering and\nSummarization. This shift is partly driven by the growing capabilities of\nmodern systems, particularly with the success of Large Language Models (LLMs).\nIn this paper, we present the Karlsruhe Institute of Technology's submissions\nfor the Offline ST and Instruction Following (IF) tracks, where we leverage\nLLMs to enhance performance across all tasks. For the Offline ST track, we\npropose a pipeline that employs multiple automatic speech recognition systems,\nwhose outputs are fused using an LLM with document-level context. This is\nfollowed by a two-step translation process, incorporating additional refinement\nstep to improve translation quality. For the IF track, we develop an end-to-end\nmodel that integrates a speech encoder with an LLM to perform a wide range of\ninstruction-following tasks. We complement it with a final document-level\nrefinement stage to further enhance output quality by using contextual\ninformation."}
{"id": "2505.12908", "pdf": "https://arxiv.org/pdf/2505.12908", "abs": "https://arxiv.org/abs/2505.12908", "authors": ["Xiao Wang", "Yu Jin", "Lan Chen", "Bo Jiang", "Lin Zhu", "Yonghong Tian", "Jin Tang", "Bin Luo"], "title": "Dynamic Graph Induced Contour-aware Heat Conduction Network for Event-based Object Detection", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Event-based Vision Sensors (EVS) have demonstrated significant advantages\nover traditional RGB frame-based cameras in low-light conditions, high-speed\nmotion capture, and low latency. Consequently, object detection based on EVS\nhas attracted increasing attention from researchers. Current event stream\nobject detection algorithms are typically built upon Convolutional Neural\nNetworks (CNNs) or Transformers, which either capture limited local features\nusing convolutional filters or incur high computational costs due to the\nutilization of self-attention. Recently proposed vision heat conduction\nbackbone networks have shown a good balance between efficiency and accuracy;\nhowever, these models are not specifically designed for event stream data. They\nexhibit weak capability in modeling object contour information and fail to\nexploit the benefits of multi-scale features. To address these issues, this\npaper proposes a novel dynamic graph induced contour-aware heat conduction\nnetwork for event stream based object detection, termed CvHeat-DET. The\nproposed model effectively leverages the clear contour information inherent in\nevent streams to predict the thermal diffusivity coefficients within the heat\nconduction model, and integrates hierarchical structural graph features to\nenhance feature learning across multiple scales. Extensive experiments on three\nbenchmark datasets for event stream-based object detection fully validated the\neffectiveness of the proposed model. The source code of this paper will be\nreleased on https://github.com/Event-AHU/OpenEvDET."}
{"id": "2505.13053", "pdf": "https://arxiv.org/pdf/2505.13053", "abs": "https://arxiv.org/abs/2505.13053", "authors": ["Amelie S. Robrecht", "Christoph R. Kowalski", "Stefan Kopp"], "title": "SNAPE-PM: Building and Utilizing Dynamic Partner Models for Adaptive Explanation Generation", "categories": ["cs.CL", "cs.AI"], "comment": "currently under review at Frontiers in Communication", "summary": "Adapting to the addressee is crucial for successful explanations, yet poses\nsignificant challenges for dialogsystems. We adopt the approach of treating\nexplanation generation as a non-stationary decision process, where the optimal\nstrategy varies according to changing beliefs about the explainee and the\ninteraction context. In this paper we address the questions of (1) how to track\nthe interaction context and the relevant listener features in a formally\ndefined computational partner model, and (2) how to utilize this model in the\ndynamically adjusted, rational decision process that determines the currently\nbest explanation strategy. We propose a Bayesian inference-based approach to\ncontinuously update the partner model based on user feedback, and a\nnon-stationary Markov Decision Process to adjust decision-making based on the\npartner model values. We evaluate an implementation of this framework with five\nsimulated interlocutors, demonstrating its effectiveness in adapting to\ndifferent partners with constant and even changing feedback behavior. The\nresults show high adaptivity with distinct explanation strategies emerging for\ndifferent partners, highlighting the potential of our approach to improve\nexplainable AI systems and dialogsystems in general."}
{"id": "2505.12911", "pdf": "https://arxiv.org/pdf/2505.12911", "abs": "https://arxiv.org/abs/2505.12911", "authors": ["Simone Alberto Peirone", "Francesca Pistilli", "Giuseppe Averta"], "title": "HiERO: understanding the hierarchy of human behavior enhances reasoning on egocentric videos", "categories": ["cs.CV"], "comment": "Project page https://github.com/sapeirone/hiero", "summary": "Human activities are particularly complex and variable, and this makes\nchallenging for deep learning models to reason about them. However, we note\nthat such variability does have an underlying structure, composed of a\nhierarchy of patterns of related actions. We argue that such structure can\nemerge naturally from unscripted videos of human activities, and can be\nleveraged to better reason about their content. We present HiERO, a\nweakly-supervised method to enrich video segments features with the\ncorresponding hierarchical activity threads. By aligning video clips with their\nnarrated descriptions, HiERO infers contextual, semantic and temporal reasoning\nwith an hierarchical architecture. We prove the potential of our enriched\nfeatures with multiple video-text alignment benchmarks (EgoMCQ, EgoNLQ) with\nminimal additional training, and in zero-shot for procedure learning tasks\n(EgoProceL and Ego4D Goal-Step). Notably, HiERO achieves state-of-the-art\nperformance in all the benchmarks, and for procedure learning tasks it\noutperforms fully-supervised methods by a large margin (+12.5% F1 on EgoProceL)\nin zero shot. Our results prove the relevance of using knowledge of the\nhierarchy of human activities for multiple reasoning tasks in egocentric\nvision."}
{"id": "2505.13069", "pdf": "https://arxiv.org/pdf/2505.13069", "abs": "https://arxiv.org/abs/2505.13069", "authors": ["Ambre Marie", "Ilias Maoudj", "Guillaume Dardenne", "Gwenolé Quellec"], "title": "Suicide Risk Assessment Using Multimodal Speech Features: A Study on the SW1 Challenge Dataset", "categories": ["cs.CL", "cs.SD", "eess.AS", "I.2.7; I.5.1"], "comment": "Submitted to the SpeechWellness Challenge at Interspeech 2025; 5\n  pages, 2 figures, 2 tables", "summary": "The 1st SpeechWellness Challenge conveys the need for speech-based suicide\nrisk assessment in adolescents. This study investigates a multimodal approach\nfor this challenge, integrating automatic transcription with WhisperX,\nlinguistic embeddings from Chinese RoBERTa, and audio embeddings from WavLM.\nAdditionally, handcrafted acoustic features -- including MFCCs, spectral\ncontrast, and pitch-related statistics -- were incorporated. We explored three\nfusion strategies: early concatenation, modality-specific processing, and\nweighted attention with mixup regularization. Results show that weighted\nattention provided the best generalization, achieving 69% accuracy on the\ndevelopment set, though a performance gap between development and test sets\nhighlights generalization challenges. Our findings, strictly tied to the\nMINI-KID framework, emphasize the importance of refining embedding\nrepresentations and fusion mechanisms to enhance classification reliability."}
{"id": "2505.12912", "pdf": "https://arxiv.org/pdf/2505.12912", "abs": "https://arxiv.org/abs/2505.12912", "authors": ["Kazuki Adachi", "Shin'ya Yamaguchi", "Tomoki Hamagami"], "title": "Uniformity First: Uniformity-aware Test-time Adaptation of Vision-language Models against Image Corruption", "categories": ["cs.CV"], "comment": "Code is available at https://github.com/kzkadc/uninfo", "summary": "Pre-trained vision-language models such as contrastive language-image\npre-training (CLIP) have demonstrated a remarkable generalizability, which has\nenabled a wide range of applications represented by zero-shot classification.\nHowever, vision-language models still suffer when they face datasets with large\ngaps from training ones, i.e., distribution shifts. We found that CLIP is\nespecially vulnerable to sensor degradation, a type of realistic distribution\nshift caused by sensor conditions such as weather, light, or noise. Collecting\na new dataset from a test distribution for fine-tuning highly costs since\nsensor degradation occurs unexpectedly and has a range of variety. Thus, we\ninvestigate test-time adaptation (TTA) of zero-shot classification, which\nenables on-the-fly adaptation to the test distribution with unlabeled test\ndata. Existing TTA methods for CLIP mainly focus on modifying image and text\nembeddings or predictions to address distribution shifts. Although these\nmethods can adapt to domain shifts, such as fine-grained labels spaces or\ndifferent renditions in input images, they fail to adapt to distribution shifts\ncaused by sensor degradation. We found that this is because image embeddings\nare \"corrupted\" in terms of uniformity, a measure related to the amount of\ninformation. To make models robust to sensor degradation, we propose a novel\nmethod called uniformity-aware information-balanced TTA (UnInfo). To address\nthe corruption of image embeddings, we introduce uniformity-aware confidence\nmaximization, information-aware loss balancing, and knowledge distillation from\nthe exponential moving average (EMA) teacher. Through experiments, we\ndemonstrate that our UnInfo improves accuracy under sensor degradation by\nretaining information in terms of uniformity."}
{"id": "2505.13077", "pdf": "https://arxiv.org/pdf/2505.13077", "abs": "https://arxiv.org/abs/2505.13077", "authors": ["Xiang Fei", "Jinghui Lu", "Qi Sun", "Hao Feng", "Yanjie Wang", "Wei Shi", "An-Lan Wang", "Jingqun Tang", "Can Huang"], "title": "Advancing Sequential Numerical Prediction in Autoregressive Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Autoregressive models have become the de facto choice for sequence generation\ntasks, but standard approaches treat digits as independent tokens and apply\ncross-entropy loss, overlooking the coherent structure of numerical sequences.\nThis paper introduces Numerical Token Integrity Loss (NTIL) to address this\ngap. NTIL operates at two levels: (1) token-level, where it extends the Earth\nMover's Distance (EMD) to preserve ordinal relationships between numerical\nvalues, and (2) sequence-level, where it penalizes the overall discrepancy\nbetween the predicted and actual sequences. This dual approach improves\nnumerical prediction and integrates effectively with LLMs/MLLMs. Extensive\nexperiments show significant performance improvements with NTIL."}
{"id": "2505.12935", "pdf": "https://arxiv.org/pdf/2505.12935", "abs": "https://arxiv.org/abs/2505.12935", "authors": ["Di You", "Daniel Siromani", "Pier Luigi Dragotti"], "title": "LatentINDIGO: An INN-Guided Latent Diffusion Algorithm for Image Restoration", "categories": ["cs.CV"], "comment": "Submitted to IEEE Transactions on Image Processing (TIP)", "summary": "There is a growing interest in the use of latent diffusion models (LDMs) for\nimage restoration (IR) tasks due to their ability to model effectively the\ndistribution of natural images. While significant progress has been made, there\nare still key challenges that need to be addressed. First, many approaches\ndepend on a predefined degradation operator, making them ill-suited for complex\nor unknown degradations that deviate from standard analytical models. Second,\nmany methods struggle to provide a stable guidance in the latent space and\nfinally most methods convert latent representations back to the pixel domain\nfor guidance at every sampling iteration, which significantly increases\ncomputational and memory overhead. To overcome these limitations, we introduce\na wavelet-inspired invertible neural network (INN) that simulates degradations\nthrough a forward transform and reconstructs lost details via the inverse\ntransform. We further integrate this design into a latent diffusion pipeline\nthrough two proposed approaches: LatentINDIGO-PixelINN, which operates in the\npixel domain, and LatentINDIGO-LatentINN, which stays fully in the latent space\nto reduce complexity. Both approaches alternate between updating intermediate\nlatent variables under the guidance of our INN and refining the INN forward\nmodel to handle unknown degradations. In addition, a regularization step\npreserves the proximity of latent variables to the natural image manifold.\nExperiments demonstrate that our algorithm achieves state-of-the-art\nperformance on synthetic and real-world low-quality images, and can be readily\nadapted to arbitrary output sizes."}
{"id": "2505.13089", "pdf": "https://arxiv.org/pdf/2505.13089", "abs": "https://arxiv.org/abs/2505.13089", "authors": ["Sondre Wold", "Lucas Georges Gabriel Charpentier", "Étienne Simon"], "title": "Systematic Generalization in Language Models Scales with Information Entropy", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025: Findings", "summary": "Systematic generalization remains challenging for current language models,\nwhich are known to be both sensitive to semantically similar permutations of\nthe input and to struggle with known concepts presented in novel contexts.\nAlthough benchmarks exist for assessing compositional behavior, it is unclear\nhow to measure the difficulty of a systematic generalization problem. In this\nwork, we show how one aspect of systematic generalization can be described by\nthe entropy of the distribution of component parts in the training data. We\nformalize a framework for measuring entropy in a sequence-to-sequence task and\nfind that the performance of popular model architectures scales with the\nentropy. Our work connects systematic generalization to information efficiency,\nand our results indicate that success at high entropy can be achieved even\nwithout built-in priors, and that success at low entropy can serve as a target\nfor assessing progress towards robust systematic generalization."}
{"id": "2505.12966", "pdf": "https://arxiv.org/pdf/2505.12966", "abs": "https://arxiv.org/abs/2505.12966", "authors": ["Zihan Xiong", "Xiaohua Wu", "Lei Chen", "Fangqi Lou"], "title": "Multiscale Adaptive Conflict-Balancing Model For Multimedia Deepfake Detection", "categories": ["cs.CV", "cs.AI"], "comment": "9 pages,ICMR accepted", "summary": "Advances in computer vision and deep learning have blurred the line between\ndeepfakes and authentic media, undermining multimedia credibility through\naudio-visual forgery. Current multimodal detection methods remain limited by\nunbalanced learning between modalities. To tackle this issue, we propose an\nAudio-Visual Joint Learning Method (MACB-DF) to better mitigate modality\nconflicts and neglect by leveraging contrastive learning to assist in\nmulti-level and cross-modal fusion, thereby fully balancing and exploiting\ninformation from each modality. Additionally, we designed an\northogonalization-multimodal pareto module that preserves unimodal information\nwhile addressing gradient conflicts in audio-video encoders caused by differing\noptimization targets of the loss functions. Extensive experiments and ablation\nstudies conducted on mainstream deepfake datasets demonstrate consistent\nperformance gains of our model across key evaluation metrics, achieving an\naverage accuracy of 95.5% across multiple datasets. Notably, our method\nexhibits superior cross-dataset generalization capabilities, with absolute\nimprovements of 8.0% and 7.7% in ACC scores over the previous best-performing\napproach when trained on DFDC and tested on DefakeAVMiT and FakeAVCeleb\ndatasets."}
{"id": "2505.13090", "pdf": "https://arxiv.org/pdf/2505.13090", "abs": "https://arxiv.org/abs/2505.13090", "authors": ["David Stap", "Christof Monz"], "title": "The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation", "categories": ["cs.CL"], "comment": null, "summary": "Prior research diverges on language diversity in LLM fine-tuning: Some\nstudies report benefits while others find no advantages. Through controlled\nfine-tuning experiments across 132 translation directions, we systematically\nresolve these disparities. We find that expanding language diversity during\nfine-tuning improves translation quality for both unsupervised and --\nsurprisingly -- supervised pairs, despite less diverse models being fine-tuned\nexclusively on these supervised pairs. However, benefits plateau or decrease\nbeyond a certain diversity threshold. We show that increased language diversity\ncreates more language-agnostic representations. These representational\nadaptations help explain the improved performance in models fine-tuned with\ngreater diversity."}
{"id": "2505.12998", "pdf": "https://arxiv.org/pdf/2505.12998", "abs": "https://arxiv.org/abs/2505.12998", "authors": ["Vinkle Srivastav", "Juliette Puel", "Jonathan Vappou", "Elijah Van Houten", "Paolo Cabras", "Nicolas Padoy"], "title": "A Skull-Adaptive Framework for AI-Based 3D Transcranial Focused Ultrasound Simulation", "categories": ["cs.CV"], "comment": "The project page is available at\n  https://github.com/CAMMA-public/TFUScapes", "summary": "Transcranial focused ultrasound (tFUS) is an emerging modality for\nnon-invasive brain stimulation and therapeutic intervention, offering\nmillimeter-scale spatial precision and the ability to target deep brain\nstructures. However, the heterogeneous and anisotropic nature of the human\nskull introduces significant distortions to the propagating ultrasound\nwavefront, which require time-consuming patient-specific planning and\ncorrections using numerical solvers for accurate targeting. To enable\ndata-driven approaches in this domain, we introduce TFUScapes, the first\nlarge-scale, high-resolution dataset of tFUS simulations through anatomically\nrealistic human skulls derived from T1-weighted MRI images. We have developed a\nscalable simulation engine pipeline using the k-Wave pseudo-spectral solver,\nwhere each simulation returns a steady-state pressure field generated by a\nfocused ultrasound transducer placed at realistic scalp locations. In addition\nto the dataset, we present DeepTFUS, a deep learning model that estimates\nnormalized pressure fields directly from input 3D CT volumes and transducer\nposition. The model extends a U-Net backbone with transducer-aware\nconditioning, incorporating Fourier-encoded position embeddings and MLP layers\nto create global transducer embeddings. These embeddings are fused with U-Net\nencoder features via feature-wise modulation, dynamic convolutions, and\ncross-attention mechanisms. The model is trained using a combination of\nspatially weighted and gradient-sensitive loss functions, enabling it to\napproximate high-fidelity wavefields. The TFUScapes dataset is publicly\nreleased to accelerate research at the intersection of computational acoustics,\nneurotechnology, and deep learning. The project page is available at\nhttps://github.com/CAMMA-public/TFUScapes."}
{"id": "2505.13115", "pdf": "https://arxiv.org/pdf/2505.13115", "abs": "https://arxiv.org/abs/2505.13115", "authors": ["Debarpan Bhattacharya", "Apoorva Kulkarni", "Sriram Ganapathy"], "title": "Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": "Accepted in INTERSPEECH, 2025, Rotterdam, The Netherlands", "summary": "The popular success of text-based large language models (LLM) has streamlined\nthe attention of the multimodal community to combine other modalities like\nvision and audio along with text to achieve similar multimodal capabilities. In\nthis quest, large audio language models (LALMs) have to be evaluated on\nreasoning related tasks which are different from traditional classification or\ngeneration tasks. Towards this goal, we propose a novel dataset called temporal\nreasoning evaluation of audio (TREA).\n  We benchmark open-source LALMs and observe that they are consistently behind\nhuman capabilities on the tasks in the TREA dataset. While evaluating LALMs, we\nalso propose an uncertainty metric, which computes the invariance of the model\nto semantically identical perturbations of the input. Our analysis shows that\nthe accuracy and uncertainty metrics are not necessarily correlated and thus,\npoints to a need for wholesome evaluation of LALMs for high-stakes\napplications."}
{"id": "2505.13023", "pdf": "https://arxiv.org/pdf/2505.13023", "abs": "https://arxiv.org/abs/2505.13023", "authors": ["Yimao Guo", "Zuomin Qu", "Wei Lu", "Xiangyang Luo"], "title": "Anti-Inpainting: A Proactive Defense against Malicious Diffusion-based Inpainters under Unknown Conditions", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": null, "summary": "As diffusion-based malicious image manipulation becomes increasingly\nprevalent, multiple proactive defense methods are developed to safeguard images\nagainst unauthorized tampering. However, most proactive defense methods only\ncan safeguard images against manipulation under known conditions, and fail to\nprotect images from manipulations guided by tampering conditions crafted by\nmalicious users. To tackle this issue, we propose Anti-Inpainting, a proactive\ndefense method that achieves adequate protection under unknown conditions\nthrough a triple mechanism to address this challenge. Specifically, a\nmulti-level deep feature extractor is presented to obtain intricate features\nduring the diffusion denoising process to improve protective effectiveness. We\ndesign multi-scale semantic-preserving data augmentation to enhance the\ntransferability of adversarial perturbations across unknown conditions by\nmulti-scale transformations while preserving semantic integrity. In addition,\nwe propose a selection-based distribution deviation optimization strategy to\nimprove the protection of adversarial perturbation against manipulation under\ndiverse random seeds. Extensive experiments indicate the proactive defensive\nperformance of Anti-Inpainting against diffusion-based inpainters guided by\nunknown conditions in InpaintGuardBench and CelebA-HQ. At the same time, we\nalso demonstrate the proposed approach's robustness under various image\npurification methods and its transferability across different versions of\ndiffusion models."}
{"id": "2505.13136", "pdf": "https://arxiv.org/pdf/2505.13136", "abs": "https://arxiv.org/abs/2505.13136", "authors": ["Anton Ehrmanntraut", "Julia Wunderle", "Jan Pfister", "Fotis Jannidis", "Andreas Hotho"], "title": "ModernGBERT: German-only 1B Encoder Model Trained from Scratch", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "under review @ARR", "summary": "Despite the prominence of decoder-only language models, encoders remain\ncrucial for resource-constrained applications. We introduce ModernGBERT (134M,\n1B), a fully transparent family of German encoder models trained from scratch,\nincorporating architectural innovations from ModernBERT. To evaluate the\npractical trade-offs of training encoders from scratch, we also present\nLL\\\"aMmlein2Vec (120M, 1B, 7B), a family of encoders derived from German\ndecoder-only models via LLM2Vec. We benchmark all models on natural language\nunderstanding, text embedding, and long-context reasoning tasks, enabling a\ncontrolled comparison between dedicated encoders and converted decoders. Our\nresults show that ModernGBERT 1B outperforms prior state-of-the-art German\nencoders as well as encoders adapted via LLM2Vec, with regard to performance\nand parameter-efficiency. All models, training data, checkpoints and code are\npublicly available, advancing the German NLP ecosystem with transparent,\nhigh-performance encoder models."}
{"id": "2505.13039", "pdf": "https://arxiv.org/pdf/2505.13039", "abs": "https://arxiv.org/abs/2505.13039", "authors": ["Xiao Wu", "Xiaoqing Zhang", "Zunjie Xiao", "Lingxi Hu", "Risa Higashita", "Jiang Liu"], "title": "Expert-Like Reparameterization of Heterogeneous Pyramid Receptive Fields in Efficient CNNs for Fair Medical Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "Efficient convolutional neural network (CNN) architecture designs have\nattracted growing research interests. However, they usually apply single\nreceptive field (RF), small asymmetric RFs, or pyramid RFs to learn different\nfeature representations, still encountering two significant challenges in\nmedical image classification tasks: 1) They have limitations in capturing\ndiverse lesion characteristics efficiently, e.g., tiny, coordination, small and\nsalient, which have unique roles on results, especially imbalanced medical\nimage classification. 2) The predictions generated by those CNNs are often\nunfair/biased, bringing a high risk by employing them to real-world medical\ndiagnosis conditions. To tackle these issues, we develop a new concept,\nExpert-Like Reparameterization of Heterogeneous Pyramid Receptive Fields\n(ERoHPRF), to simultaneously boost medical image classification performance and\nfairness. This concept aims to mimic the multi-expert consultation mode by\napplying the well-designed heterogeneous pyramid RF bags to capture different\nlesion characteristics effectively via convolution operations with multiple\nheterogeneous kernel sizes. Additionally, ERoHPRF introduces an expert-like\nstructural reparameterization technique to merge its parameters with the\ntwo-stage strategy, ensuring competitive computation cost and inference speed\nthrough comparisons to a single RF. To manifest the effectiveness and\ngeneralization ability of ERoHPRF, we incorporate it into mainstream efficient\nCNN architectures. The extensive experiments show that our method maintains a\nbetter trade-off than state-of-the-art methods in terms of medical image\nclassification, fairness, and computation overhead. The codes of this paper\nwill be released soon."}
{"id": "2505.13141", "pdf": "https://arxiv.org/pdf/2505.13141", "abs": "https://arxiv.org/abs/2505.13141", "authors": ["Zheng Wei Lim", "Alham Fikri Aji", "Trevor Cohn"], "title": "Understanding Cross-Lingual Inconsistency in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are demonstrably capable of cross-lingual\ntransfer, but can produce inconsistent output when prompted with the same\nqueries written in different languages. To understand how language models are\nable to generalize knowledge from one language to the others, we apply the\nlogit lens to interpret the implicit steps taken by LLMs to solve multilingual\nmulti-choice reasoning questions. We find LLMs predict inconsistently and are\nless accurate because they rely on subspaces of individual languages, rather\nthan working in a shared semantic space. While larger models are more\nmultilingual, we show their hidden states are more likely to dissociate from\nthe shared representation compared to smaller models, but are nevertheless more\ncapable of retrieving knowledge embedded across different languages. Finally,\nwe demonstrate that knowledge sharing can be modulated by steering the models'\nlatent processing towards the shared semantic space. We find reinforcing\nutilization of the shared space improves the models' multilingual reasoning\nperformance, as a result of more knowledge transfer from, and better output\nconsistency with English."}
{"id": "2505.13043", "pdf": "https://arxiv.org/pdf/2505.13043", "abs": "https://arxiv.org/abs/2505.13043", "authors": ["Hao-Ran Yang", "Xiaohui Chen", "Chuan-Xian Ren"], "title": "A Generalized Label Shift Perspective for Cross-Domain Gaze Estimation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Aiming to generalize the well-trained gaze estimation model to new target\ndomains, Cross-domain Gaze Estimation (CDGE) is developed for real-world\napplication scenarios. Existing CDGE methods typically extract the\ndomain-invariant features to mitigate domain shift in feature space, which is\nproved insufficient by Generalized Label Shift (GLS) theory. In this paper, we\nintroduce a novel GLS perspective to CDGE and modelize the cross-domain problem\nby label and conditional shift problem. A GLS correction framework is presented\nand a feasible realization is proposed, in which a importance reweighting\nstrategy based on truncated Gaussian distribution is introduced to overcome the\ncontinuity challenges in label shift correction. To embed the reweighted source\ndistribution to conditional invariant learning, we further derive a\nprobability-aware estimation of conditional operator discrepancy. Extensive\nexperiments on standard CDGE tasks with different backbone models validate the\nsuperior generalization capability across domain and applicability on various\nmodels of proposed method."}
{"id": "2505.13147", "pdf": "https://arxiv.org/pdf/2505.13147", "abs": "https://arxiv.org/abs/2505.13147", "authors": ["Aswathy Velutharambath", "Roman Klinger", "Kai Sassenberg"], "title": "What if Deception Cannot be Detected? A Cross-Linguistic Study on the Limits of Deception Detection from Text", "categories": ["cs.CL"], "comment": null, "summary": "Can deception be detected solely from written text? Cues of deceptive\ncommunication are inherently subtle, even more so in text-only communication.\nYet, prior studies have reported considerable success in automatic deception\ndetection. We hypothesize that such findings are largely driven by artifacts\nintroduced during data collection and do not generalize beyond specific\ndatasets. We revisit this assumption by introducing a belief-based deception\nframework, which defines deception as a misalignment between an author's claims\nand true beliefs, irrespective of factual accuracy, allowing deception cues to\nbe studied in isolation. Based on this framework, we construct three corpora,\ncollectively referred to as DeFaBel, including a German-language corpus of\ndeceptive and non-deceptive arguments and a multilingual version in German and\nEnglish, each collected under varying conditions to account for belief change\nand enable cross-linguistic analysis. Using these corpora, we evaluate commonly\nreported linguistic cues of deception. Across all three DeFaBel variants, these\ncues show negligible, statistically insignificant correlations with deception\nlabels, contrary to prior work that treats such cues as reliable indicators. We\nfurther benchmark against other English deception datasets following similar\ndata collection protocols. While some show statistically significant\ncorrelations, effect sizes remain low and, critically, the set of predictive\ncues is inconsistent across datasets. We also evaluate deception detection\nusing feature-based models, pretrained language models, and instruction-tuned\nlarge language models. While some models perform well on established deception\ndatasets, they consistently perform near chance on DeFaBel. Our findings\nchallenge the assumption that deception can be reliably inferred from\nlinguistic cues and call for rethinking how deception is studied and modeled in\nNLP."}
{"id": "2505.13050", "pdf": "https://arxiv.org/pdf/2505.13050", "abs": "https://arxiv.org/abs/2505.13050", "authors": ["Beibei Lin", "Zifeng Yuan", "Tingting Chen"], "title": "RGB-to-Polarization Estimation: A New Task and Benchmark Study", "categories": ["cs.CV"], "comment": null, "summary": "Polarization images provide rich physical information that is fundamentally\nabsent from standard RGB images, benefiting a wide range of computer vision\napplications such as reflection separation and material classification.\nHowever, the acquisition of polarization images typically requires additional\noptical components, which increases both the cost and the complexity of the\napplications. To bridge this gap, we introduce a new task: RGB-to-polarization\nimage estimation, which aims to infer polarization information directly from\nRGB images. In this work, we establish the first comprehensive benchmark for\nthis task by leveraging existing polarization datasets and evaluating a diverse\nset of state-of-the-art deep learning models, including both\nrestoration-oriented and generative architectures. Through extensive\nquantitative and qualitative analysis, our benchmark not only establishes the\ncurrent performance ceiling of RGB-to-polarization estimation, but also\nsystematically reveals the respective strengths and limitations of different\nmodel families -- such as direct reconstruction versus generative synthesis,\nand task-specific training versus large-scale pre-training. In addition, we\nprovide some potential directions for future research on polarization\nestimation. This benchmark is intended to serve as a foundational resource to\nfacilitate the design and evaluation of future methods for polarization\nestimation from standard RGB inputs."}
{"id": "2505.13156", "pdf": "https://arxiv.org/pdf/2505.13156", "abs": "https://arxiv.org/abs/2505.13156", "authors": ["Zhi Liu", "Tao Yang", "Jing Wang", "Yexin Chen", "Zhan Gao", "Jiaxi Yang", "Kui Chen", "Bingji Lu", "Xiaochen Li", "Changyong Luo", "Yan Li", "Xiaohong Gu", "Peng Cao"], "title": "Tianyi: A Traditional Chinese Medicine all-rounder language model and its Real-World Clinical Practice", "categories": ["cs.CL", "cs.AI"], "comment": "23 pages, 4 figures, and 1 tables", "summary": "Natural medicines, particularly Traditional Chinese Medicine (TCM), are\ngaining global recognition for their therapeutic potential in addressing human\nsymptoms and diseases. TCM, with its systematic theories and extensive\npractical experience, provides abundant resources for healthcare. However, the\neffective application of TCM requires precise syndrome diagnosis, determination\nof treatment principles, and prescription formulation, which demand decades of\nclinical expertise. Despite advancements in TCM-based decision systems, machine\nlearning, and deep learning research, limitations in data and single-objective\nconstraints hinder their practical application. In recent years, large language\nmodels (LLMs) have demonstrated potential in complex tasks, but lack\nspecialization in TCM and face significant challenges, such as too big model\nscale to deploy and issues with hallucination. To address these challenges, we\nintroduce Tianyi with 7.6-billion-parameter LLM, a model scale proper and\nspecifically designed for TCM, pre-trained and fine-tuned on diverse TCM\ncorpora, including classical texts, expert treatises, clinical records, and\nknowledge graphs. Tianyi is designed to assimilate interconnected and\nsystematic TCM knowledge through a progressive learning manner. Additionally,\nwe establish TCMEval, a comprehensive evaluation benchmark, to assess LLMs in\nTCM examinations, clinical tasks, domain-specific question-answering, and\nreal-world trials. The extensive evaluations demonstrate the significant\npotential of Tianyi as an AI assistant in TCM clinical practice and research,\nbridging the gap between TCM knowledge and practical application."}
{"id": "2505.13061", "pdf": "https://arxiv.org/pdf/2505.13061", "abs": "https://arxiv.org/abs/2505.13061", "authors": ["CHengtang Yao", "Zhidan Liu", "Jiaxi Zeng", "Lidong Yu", "Yuwei Wu", "Yunde Jia"], "title": "3D Visual Illusion Depth Estimation", "categories": ["cs.CV"], "comment": "Project:\n  https://github.com/YaoChengTang/3D-Visual-Illusion-Depth-Estimation", "summary": "3D visual illusion is a perceptual phenomenon where a two-dimensional plane\nis manipulated to simulate three-dimensional spatial relationships, making a\nflat artwork or object look three-dimensional in the human visual system. In\nthis paper, we reveal that the machine visual system is also seriously fooled\nby 3D visual illusions, including monocular and binocular depth estimation. In\norder to explore and analyze the impact of 3D visual illusion on depth\nestimation, we collect a large dataset containing almost 3k scenes and 200k\nimages to train and evaluate SOTA monocular and binocular depth estimation\nmethods. We also propose a robust depth estimation framework that uses common\nsense from a vision-language model to adaptively select reliable depth from\nbinocular disparity and monocular depth. Experiments show that SOTA monocular,\nbinocular, and multi-view depth estimation approaches are all fooled by various\n3D visual illusions, while our method achieves SOTA performance."}
{"id": "2505.13157", "pdf": "https://arxiv.org/pdf/2505.13157", "abs": "https://arxiv.org/abs/2505.13157", "authors": ["Yassine El Boudouri", "Walter Nuninger", "Julian Alvarez", "Yvan Peter"], "title": "Role-Playing Evaluation for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate a notable capacity for adopting\npersonas and engaging in role-playing. However, evaluating this ability\npresents significant challenges, as human assessments are resource-intensive\nand automated evaluations can be biased. To address this, we introduce\nRole-Playing Eval (RPEval), a novel benchmark designed to assess LLM\nrole-playing capabilities across four key dimensions: emotional understanding,\ndecision-making, moral alignment, and in-character consistency. This article\ndetails the construction of RPEval and presents baseline evaluations. Our code\nand dataset are available at https://github.com/yelboudouri/RPEval"}
{"id": "2505.13088", "pdf": "https://arxiv.org/pdf/2505.13088", "abs": "https://arxiv.org/abs/2505.13088", "authors": ["Zhaoyi Wang", "Shengyu Huang", "Jemil Avers Butt", "Yuanzhou Cai", "Matej Varga", "Andreas Wieser"], "title": "Cross-modal feature fusion for robust point cloud registration with ambiguous geometry", "categories": ["cs.CV", "cs.LG"], "comment": "To appear in the ISPRS Journal of Photogrammetry and Remote Sensing.\n  19 pages, 14 figures", "summary": "Point cloud registration has seen significant advancements with the\napplication of deep learning techniques. However, existing approaches often\noverlook the potential of integrating radiometric information from RGB images.\nThis limitation reduces their effectiveness in aligning point clouds pairs,\nespecially in regions where geometric data alone is insufficient. When used\neffectively, radiometric information can enhance the registration process by\nproviding context that is missing from purely geometric data. In this paper, we\npropose CoFF, a novel Cross-modal Feature Fusion method that utilizes both\npoint cloud geometry and RGB images for pairwise point cloud registration.\nAssuming that the co-registration between point clouds and RGB images is\navailable, CoFF explicitly addresses the challenges where geometric information\nalone is unclear, such as in regions with symmetric similarity or planar\nstructures, through a two-stage fusion of 3D point cloud features and 2D image\nfeatures. It incorporates a cross-modal feature fusion module that assigns\npixel-wise image features to 3D input point clouds to enhance learned 3D point\nfeatures, and integrates patch-wise image features with superpoint features to\nimprove the quality of coarse matching. This is followed by a coarse-to-fine\nmatching module that accurately establishes correspondences using the fused\nfeatures. We extensively evaluate CoFF on four common datasets: 3DMatch,\n3DLoMatch, IndoorLRS, and the recently released ScanNet++ datasets. In\naddition, we assess CoFF on specific subset datasets containing geometrically\nambiguous cases. Our experimental results demonstrate that CoFF achieves\nstate-of-the-art registration performance across all benchmarks, including\nremarkable registration recalls of 95.9% and 81.6% on the widely-used 3DMatch\nand 3DLoMatch datasets, respectively...(Truncated to fit arXiv abstract length)"}
{"id": "2505.13171", "pdf": "https://arxiv.org/pdf/2505.13171", "abs": "https://arxiv.org/abs/2505.13171", "authors": ["Yixuan Xu", "Antoine Bosselut", "Imanol Schlag"], "title": "Positional Fragility in LLMs: How Offset Effects Reshape Our Understanding of Memorization Risks", "categories": ["cs.CL"], "comment": null, "summary": "Large language models are known to memorize parts of their training data,\nposing risk of copyright violations. To systematically examine this risk, we\npretrain language models (1B/3B/8B) from scratch on 83B tokens, mixing\nweb-scale data with public domain books used to simulate copyrighted content at\ncontrolled frequencies at lengths at least ten times longer than prior work. We\nthereby identified the offset effect, a phenomenon characterized by two key\nfindings: (1) verbatim memorization is most strongly triggered by short\nprefixes drawn from the beginning of the context window, with memorization\ndecreasing counterintuitively as prefix length increases; and (2) a sharp\ndecline in verbatim recall when prefix begins offset from the initial tokens of\nthe context window. We attribute this to positional fragility: models rely\ndisproportionately on the earliest tokens in their context window as retrieval\nanchors, making them sensitive to even slight shifts. We further observe that\nwhen the model fails to retrieve memorized content, it often produces\ndegenerated text. Leveraging these findings, we show that shifting sensitive\ndata deeper into the context window suppresses both extractable memorization\nand degeneration. Our results suggest that positional offset is a critical and\npreviously overlooked axis for evaluating memorization risks, since prior work\nimplicitly assumed uniformity by probing only from the beginning of training\nsequences."}
{"id": "2505.13091", "pdf": "https://arxiv.org/pdf/2505.13091", "abs": "https://arxiv.org/abs/2505.13091", "authors": ["Yuanbo Wang", "Zhaoxuan Zhang", "Jiajin Qiu", "Dilong Sun", "Zhengyu Meng", "Xiaopeng Wei", "Xin Yang"], "title": "Touch2Shape: Touch-Conditioned 3D Diffusion for Shape Exploration and Reconstruction", "categories": ["cs.CV"], "comment": "10 pages, 6 figures", "summary": "Diffusion models have made breakthroughs in 3D generation tasks. Current 3D\ndiffusion models focus on reconstructing target shape from images or a set of\npartial observations. While excelling in global context understanding, they\nstruggle to capture the local details of complex shapes and limited to the\nocclusion and lighting conditions. To overcome these limitations, we utilize\ntactile images to capture the local 3D information and propose a Touch2Shape\nmodel, which leverages a touch-conditioned diffusion model to explore and\nreconstruct the target shape from touch. For shape reconstruction, we have\ndeveloped a touch embedding module to condition the diffusion model in creating\na compact representation and a touch shape fusion module to refine the\nreconstructed shape. For shape exploration, we combine the diffusion model with\nreinforcement learning to train a policy. This involves using the generated\nlatent vector from the diffusion model to guide the touch exploration policy\ntraining through a novel reward design. Experiments validate the reconstruction\nquality thorough both qualitatively and quantitative analysis, and our touch\nexploration policy further boosts reconstruction performance."}
{"id": "2505.13173", "pdf": "https://arxiv.org/pdf/2505.13173", "abs": "https://arxiv.org/abs/2505.13173", "authors": ["V. S. D. S. Mahesh Akavarapu", "Hrishikesh Terdalkar", "Pramit Bhattacharyya", "Shubhangi Agarwal", "Vishakha Deulgaonkar", "Pralay Manna", "Chaitali Dangarikar", "Arnab Bhattacharya"], "title": "A Case Study of Cross-Lingual Zero-Shot Generalization for Classical Languages in LLMs", "categories": ["cs.CL", "I.2.7"], "comment": "Accepted to ACL 2025 Findings", "summary": "Large Language Models (LLMs) have demonstrated remarkable generalization\ncapabilities across diverse tasks and languages. In this study, we focus on\nnatural language understanding in three classical languages -- Sanskrit,\nAncient Greek and Latin -- to investigate the factors affecting cross-lingual\nzero-shot generalization. First, we explore named entity recognition and\nmachine translation into English. While LLMs perform equal to or better than\nfine-tuned baselines on out-of-domain data, smaller models often struggle,\nespecially with niche or abstract entity types. In addition, we concentrate on\nSanskrit by presenting a factoid question-answering (QA) dataset and show that\nincorporating context via retrieval-augmented generation approach significantly\nboosts performance. In contrast, we observe pronounced performance drops for\nsmaller LLMs across these QA tasks. These results suggest model scale as an\nimportant factor influencing cross-lingual generalization. Assuming that models\nused such as GPT-4o and Llama-3.1 are not instruction fine-tuned on classical\nlanguages, our findings provide insights into how LLMs may generalize on these\nlanguages and their consequent utility in classical studies."}
{"id": "2505.13099", "pdf": "https://arxiv.org/pdf/2505.13099", "abs": "https://arxiv.org/abs/2505.13099", "authors": ["Shinichi Mae", "Ryosuke Yamada", "Hirokatsu Kataoka"], "title": "Industry-focused Synthetic Segmentation Pre-training", "categories": ["cs.CV"], "comment": null, "summary": "Pre-training on real-image datasets has been widely proven effective for\nimproving instance segmentation. However, industrial applications face two key\nchallenges: (1) legal and ethical restrictions, such as ImageNet's prohibition\nof commercial use, and (2) limited transferability due to the domain gap\nbetween web images and industrial imagery. Even recent vision foundation\nmodels, including the segment anything model (SAM), show notable performance\ndegradation in industrial settings. These challenges raise critical questions:\nCan we build a vision foundation model for industrial applications without\nrelying on real images or manual annotations? And can such models outperform\neven fine-tuned SAM on industrial datasets? To address these questions, we\npropose the Instance Core Segmentation Dataset (InsCore), a synthetic\npre-training dataset based on formula-driven supervised learning (FDSL).\nInsCore generates fully annotated instance segmentation images that reflect key\ncharacteristics of industrial data, including complex occlusions, dense\nhierarchical masks, and diverse non-rigid shapes, distinct from typical web\nimagery. Unlike previous methods, InsCore requires neither real images nor\nhuman annotations. Experiments on five industrial datasets show that models\npre-trained with InsCore outperform those trained on COCO and ImageNet-21k, as\nwell as fine-tuned SAM, achieving an average improvement of 6.2 points in\ninstance segmentation performance. This result is achieved using only 100k\nsynthetic images, more than 100 times fewer than the 11 million images in SAM's\nSA-1B dataset, demonstrating the data efficiency of our approach. These\nfindings position InsCore as a practical and license-free vision foundation\nmodel for industrial applications."}
{"id": "2505.13176", "pdf": "https://arxiv.org/pdf/2505.13176", "abs": "https://arxiv.org/abs/2505.13176", "authors": ["Zihao Cheng", "Hongru Wang", "Zeming Liu", "Yuhang Guo", "Yuanfang Guo", "Yunhong Wang", "Haifeng Wang"], "title": "ToolSpectrum : Towards Personalized Tool Utilization for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025 Findings", "summary": "While integrating external tools into large language models (LLMs) enhances\ntheir ability to access real-time information and domain-specific services,\nexisting approaches focus narrowly on functional tool selection following user\ninstructions, overlooking the context-aware personalization in tool selection.\nThis oversight leads to suboptimal user satisfaction and inefficient tool\nutilization, particularly when overlapping toolsets require nuanced selection\nbased on contextual factors. To bridge this gap, we introduce ToolSpectrum, a\nbenchmark designed to evaluate LLMs' capabilities in personalized tool\nutilization. Specifically, we formalize two key dimensions of personalization,\nuser profile and environmental factors, and analyze their individual and\nsynergistic impacts on tool utilization. Through extensive experiments on\nToolSpectrum, we demonstrate that personalized tool utilization significantly\nimproves user experience across diverse scenarios. However, even\nstate-of-the-art LLMs exhibit the limited ability to reason jointly about user\nprofiles and environmental factors, often prioritizing one dimension at the\nexpense of the other. Our findings underscore the necessity of context-aware\npersonalization in tool-augmented LLMs and reveal critical limitations for\ncurrent models. Our data and code are available at\nhttps://github.com/Chengziha0/ToolSpectrum."}
{"id": "2505.13101", "pdf": "https://arxiv.org/pdf/2505.13101", "abs": "https://arxiv.org/abs/2505.13101", "authors": ["Shaowu Wu", "Liting Zeng", "Wei Lu", "Xiangyang Luo"], "title": "ARIW-Framework: Adaptive Robust Iterative Watermarking Framework", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 4 figures", "summary": "With the rapid rise of large models, copyright protection for generated image\ncontent has become a critical security challenge. Although deep learning\nwatermarking techniques offer an effective solution for digital image copyright\nprotection, they still face limitations in terms of visual quality, robustness\nand generalization. To address these issues, this paper proposes an adaptive\nrobust iterative watermarking framework (ARIW-Framework) that achieves\nhigh-quality watermarked images while maintaining exceptional robustness and\ngeneralization performance. Specifically, we introduce an iterative approach to\noptimize the encoder for generating robust residuals. The encoder incorporates\nnoise layers and a decoder to compute robustness weights for residuals under\nvarious noise attacks. By employing a parallel optimization strategy, the\nframework enhances robustness against multiple types of noise attacks.\nFurthermore, we leverage image gradients to determine the embedding strength at\neach pixel location, significantly improving the visual quality of the\nwatermarked images. Extensive experiments demonstrate that the proposed method\nachieves superior visual quality while exhibiting remarkable robustness and\ngeneralization against noise attacks."}
{"id": "2505.13181", "pdf": "https://arxiv.org/pdf/2505.13181", "abs": "https://arxiv.org/abs/2505.13181", "authors": ["Zhengrui Ma", "Yang Feng", "Chenze Shao", "Fandong Meng", "Jie Zhou", "Min Zhang"], "title": "Efficient Speech Language Modeling via Energy Distance in Continuous Latent Space", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Demos and code are available at https://github.com/ictnlp/SLED-TTS", "summary": "We introduce SLED, an alternative approach to speech language modeling by\nencoding speech waveforms into sequences of continuous latent representations\nand modeling them autoregressively using an energy distance objective. The\nenergy distance offers an analytical measure of the distributional gap by\ncontrasting simulated and target samples, enabling efficient training to\ncapture the underlying continuous autoregressive distribution. By bypassing\nreliance on residual vector quantization, SLED avoids discretization errors and\neliminates the need for the complicated hierarchical architectures common in\nexisting speech language models. It simplifies the overall modeling pipeline\nwhile preserving the richness of speech information and maintaining inference\nefficiency. Empirical results demonstrate that SLED achieves strong performance\nin both zero-shot and streaming speech synthesis, showing its potential for\nbroader applications in general-purpose speech language models."}
{"id": "2505.13123", "pdf": "https://arxiv.org/pdf/2505.13123", "abs": "https://arxiv.org/abs/2505.13123", "authors": ["Snehashis Majhi", "Giacomo D'Amicantonio", "Antitza Dantcheva", "Quan Kong", "Lorenzo Garattoni", "Gianpiero Francesca", "Egor Bondarev", "Francois Bremond"], "title": "Just Dance with $π$! A Poly-modal Inductor for Weakly-supervised Video Anomaly Detection", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Weakly-supervised methods for video anomaly detection (VAD) are\nconventionally based merely on RGB spatio-temporal features, which continues to\nlimit their reliability in real-world scenarios. This is due to the fact that\nRGB-features are not sufficiently distinctive in setting apart categories such\nas shoplifting from visually similar events. Therefore, towards robust complex\nreal-world VAD, it is essential to augment RGB spatio-temporal features by\nadditional modalities. Motivated by this, we introduce the Poly-modal Induced\nframework for VAD: \"PI-VAD\", a novel approach that augments RGB representations\nby five additional modalities. Specifically, the modalities include sensitivity\nto fine-grained motion (Pose), three dimensional scene and entity\nrepresentation (Depth), surrounding objects (Panoptic masks), global motion\n(optical flow), as well as language cues (VLM). Each modality represents an\naxis of a polygon, streamlined to add salient cues to RGB. PI-VAD includes two\nplug-in modules, namely Pseudo-modality Generation module and Cross Modal\nInduction module, which generate modality-specific prototypical representation\nand, thereby, induce multi-modal information into RGB cues. These modules\noperate by performing anomaly-aware auxiliary tasks and necessitate five\nmodality backbones -- only during training. Notably, PI-VAD achieves\nstate-of-the-art accuracy on three prominent VAD datasets encompassing\nreal-world scenarios, without requiring the computational overhead of five\nmodality backbones at inference."}
{"id": "2505.13204", "pdf": "https://arxiv.org/pdf/2505.13204", "abs": "https://arxiv.org/abs/2505.13204", "authors": ["Jikai Wang", "Zhenxu Tian", "Juntao Li", "Qingrong Xia", "Xinyu Duan", "Zhefeng Wang", "Baoxing Huai", "Min Zhang"], "title": "Alignment-Augmented Speculative Decoding with Alignment Sampling and Conditional Verification", "categories": ["cs.CL"], "comment": "Pre-print", "summary": "Recent works have revealed the great potential of speculative decoding in\naccelerating the autoregressive generation process of large language models.\nThe success of these methods relies on the alignment between draft candidates\nand the sampled outputs of the target model. Existing methods mainly achieve\ndraft-target alignment with training-based methods, e.g., EAGLE, Medusa,\ninvolving considerable training costs. In this paper, we present a\ntraining-free alignment-augmented speculative decoding algorithm. We propose\nalignment sampling, which leverages output distribution obtained in the\nprefilling phase to provide more aligned draft candidates. To further benefit\nfrom high-quality but non-aligned draft candidates, we also introduce a simple\nyet effective flexible verification strategy. Through an adaptive probability\nthreshold, our approach can improve generation accuracy while further improving\ninference efficiency. Experiments on 8 datasets (including question answering,\nsummarization and code completion tasks) show that our approach increases the\naverage generation score by 3.3 points for the LLaMA3 model. Our method\nachieves a mean acceptance length up to 2.39 and speed up generation by 2.23."}
{"id": "2505.13130", "pdf": "https://arxiv.org/pdf/2505.13130", "abs": "https://arxiv.org/abs/2505.13130", "authors": ["Muhammad Awais Amin", "Adama Ilboudo", "Abdul Samad bin Shahid", "Amjad Ali", "Waqas Haider Khan Bangyal"], "title": "Adaptive Image Restoration for Video Surveillance: A Real-Time Approach", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "One of the major challenges in the field of computer vision especially for\ndetection, segmentation, recognition, monitoring, and automated solutions, is\nthe quality of images. Image degradation, often caused by factors such as rain,\nfog, lighting, etc., has a negative impact on automated\ndecision-making.Furthermore, several image restoration solutions exist,\nincluding restoration models for single degradation and restoration models for\nmultiple degradations. However, these solutions are not suitable for real-time\nprocessing. In this study, the aim was to develop a real-time image restoration\nsolution for video surveillance. To achieve this, using transfer learning with\nResNet_50, we developed a model for automatically identifying the types of\ndegradation present in an image to reference the necessary treatment(s) for\nimage restoration. Our solution has the advantage of being flexible and\nscalable."}
{"id": "2505.13210", "pdf": "https://arxiv.org/pdf/2505.13210", "abs": "https://arxiv.org/abs/2505.13210", "authors": ["Xiaocong Du", "Haoyu Pei", "Haipeng Zhang"], "title": "Picturized and Recited with Dialects: A Multimodal Chinese Representation Framework for Sentiment Analysis of Classical Chinese Poetry", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Classical Chinese poetry is a vital and enduring part of Chinese literature,\nconveying profound emotional resonance. Existing studies analyze sentiment\nbased on textual meanings, overlooking the unique rhythmic and visual features\ninherent in poetry,especially since it is often recited and accompanied by\nChinese paintings. In this work, we propose a dialect-enhanced multimodal\nframework for classical Chinese poetry sentiment analysis. We extract\nsentence-level audio features from the poetry and incorporate audio from\nmultiple dialects,which may retain regional ancient Chinese phonetic features,\nenriching the phonetic representation. Additionally, we generate sentence-level\nvisual features, and the multimodal features are fused with textual features\nenhanced by LLM translation through multimodal contrastive representation\nlearning. Our framework outperforms state-of-the-art methods on two public\ndatasets, achieving at least 2.51% improvement in accuracy and 1.63% in macro\nF1. We open-source the code to facilitate research in this area and provide\ninsights for general multimodal Chinese representation."}
{"id": "2505.13137", "pdf": "https://arxiv.org/pdf/2505.13137", "abs": "https://arxiv.org/abs/2505.13137", "authors": ["Robert-Jan Bruintjes", "Jan van Gemert"], "title": "Learning to Adapt to Position Bias in Vision Transformer Classifiers", "categories": ["cs.CV"], "comment": null, "summary": "How discriminative position information is for image classification depends\non the data. On the one hand, the camera position is arbitrary and objects can\nappear anywhere in the image, arguing for translation invariance. At the same\ntime, position information is key for exploiting capture/center bias, and scene\nlayout, e.g.: the sky is up. We show that position bias, the level to which a\ndataset is more easily solved when positional information on input features is\nused, plays a crucial role in the performance of Vision Transformers image\nclassifiers. To investigate, we propose Position-SHAP, a direct measure of\nposition bias by extending SHAP to work with position embeddings. We show\nvarious levels of position bias in different datasets, and find that the\noptimal choice of position embedding depends on the position bias apparent in\nthe dataset. We therefore propose Auto-PE, a single-parameter position\nembedding extension, which allows the position embedding to modulate its norm,\nenabling the unlearning of position information. Auto-PE combines with existing\nPEs to match or improve accuracy on classification datasets."}
{"id": "2505.13220", "pdf": "https://arxiv.org/pdf/2505.13220", "abs": "https://arxiv.org/abs/2505.13220", "authors": ["Jie Ying", "Zihong Chen", "Zhefan Wang", "Wanli Jiang", "Chenyang Wang", "Zhonghang Yuan", "Haoyang Su", "Huanjun Kong", "Fan Yang", "Nanqing Dong"], "title": "SeedBench: A Multi-task Benchmark for Evaluating Large Language Models in Seed Science", "categories": ["cs.CL"], "comment": "Accepted by ACL 2025", "summary": "Seed science is essential for modern agriculture, directly influencing crop\nyields and global food security. However, challenges such as interdisciplinary\ncomplexity and high costs with limited returns hinder progress, leading to a\nshortage of experts and insufficient technological support. While large\nlanguage models (LLMs) have shown promise across various fields, their\napplication in seed science remains limited due to the scarcity of digital\nresources, complex gene-trait relationships, and the lack of standardized\nbenchmarks. To address this gap, we introduce SeedBench -- the first multi-task\nbenchmark specifically designed for seed science. Developed in collaboration\nwith domain experts, SeedBench focuses on seed breeding and simulates key\naspects of modern breeding processes. We conduct a comprehensive evaluation of\n26 leading LLMs, encompassing proprietary, open-source, and domain-specific\nfine-tuned models. Our findings not only highlight the substantial gaps between\nthe power of LLMs and the real-world seed science problems, but also make a\nfoundational step for research on LLMs for seed design."}
{"id": "2505.13140", "pdf": "https://arxiv.org/pdf/2505.13140", "abs": "https://arxiv.org/abs/2505.13140", "authors": ["Takahiro Maeda", "Jinkun Cao", "Norimichi Ukita", "Kris Kitani"], "title": "CacheFlow: Fast Human Motion Prediction by Cached Normalizing Flow", "categories": ["cs.CV"], "comment": null, "summary": "Many density estimation techniques for 3D human motion prediction require a\nsignificant amount of inference time, often exceeding the duration of the\npredicted time horizon. To address the need for faster density estimation for\n3D human motion prediction, we introduce a novel flow-based method for human\nmotion prediction called CacheFlow. Unlike previous conditional generative\nmodels that suffer from time efficiency, CacheFlow takes advantage of an\nunconditional flow-based generative model that transforms a Gaussian mixture\ninto the density of future motions. The results of the computation of the\nflow-based generative model can be precomputed and cached. Then, for\nconditional prediction, we seek a mapping from historical trajectories to\nsamples in the Gaussian mixture. This mapping can be done by a much more\nlightweight model, thus saving significant computation overhead compared to a\ntypical conditional flow model. In such a two-stage fashion and by caching\nresults from the slow flow model computation, we build our CacheFlow without\nloss of prediction accuracy and model expressiveness. This inference process is\ncompleted in approximately one millisecond, making it 4 times faster than\nprevious VAE methods and 30 times faster than previous diffusion-based methods\non standard benchmarks such as Human3.6M and AMASS datasets. Furthermore, our\nmethod demonstrates improved density estimation accuracy and comparable\nprediction accuracy to a SOTA method on Human3.6M. Our code and models will be\npublicly available."}
{"id": "2505.13244", "pdf": "https://arxiv.org/pdf/2505.13244", "abs": "https://arxiv.org/abs/2505.13244", "authors": ["Jieying Xue", "Phuong Minh Nguyen", "Minh Le Nguyen", "Xin Liu"], "title": "JNLP at SemEval-2025 Task 11: Cross-Lingual Multi-Label Emotion Detection Using Generative Models", "categories": ["cs.CL", "cs.LG"], "comment": "Published in The 19th International Workshop on Semantic Evaluation\n  (SemEval-2025)", "summary": "With the rapid advancement of global digitalization, users from different\ncountries increasingly rely on social media for information exchange. In this\ncontext, multilingual multi-label emotion detection has emerged as a critical\nresearch area. This study addresses SemEval-2025 Task 11: Bridging the Gap in\nText-Based Emotion Detection. Our paper focuses on two sub-tracks of this task:\n(1) Track A: Multi-label emotion detection, and (2) Track B: Emotion intensity.\nTo tackle multilingual challenges, we leverage pre-trained multilingual models\nand focus on two architectures: (1) a fine-tuned BERT-based classification\nmodel and (2) an instruction-tuned generative LLM. Additionally, we propose two\nmethods for handling multi-label classification: the base method, which maps an\ninput directly to all its corresponding emotion labels, and the pairwise\nmethod, which models the relationship between the input text and each emotion\ncategory individually. Experimental results demonstrate the strong\ngeneralization ability of our approach in multilingual emotion recognition. In\nTrack A, our method achieved Top 4 performance across 10 languages, ranking 1st\nin Hindi. In Track B, our approach also secured Top 5 performance in 7\nlanguages, highlighting its simplicity and effectiveness\\footnote{Our code is\navailable at https://github.com/yingjie7/mlingual_multilabel_emo_detection."}
{"id": "2505.13174", "pdf": "https://arxiv.org/pdf/2505.13174", "abs": "https://arxiv.org/abs/2505.13174", "authors": ["Alp Eren Sari", "Paolo Favaro"], "title": "FlowCut: Unsupervised Video Instance Segmentation via Temporal Mask Matching", "categories": ["cs.CV"], "comment": null, "summary": "We propose FlowCut, a simple and capable method for unsupervised video\ninstance segmentation consisting of a three-stage framework to construct a\nhigh-quality video dataset with pseudo labels. To our knowledge, our work is\nthe first attempt to curate a video dataset with pseudo-labels for unsupervised\nvideo instance segmentation. In the first stage, we generate pseudo-instance\nmasks by exploiting the affinities of features from both images and optical\nflows. In the second stage, we construct short video segments containing\nhigh-quality, consistent pseudo-instance masks by temporally matching them\nacross the frames. In the third stage, we use the YouTubeVIS-2021 video dataset\nto extract our training instance segmentation set, and then train a video\nsegmentation model. FlowCut achieves state-of-the-art performance on the\nYouTubeVIS-2019, YouTubeVIS-2021, DAVIS-2017, and DAVIS-2017 Motion benchmarks."}
{"id": "2505.13251", "pdf": "https://arxiv.org/pdf/2505.13251", "abs": "https://arxiv.org/abs/2505.13251", "authors": ["Sidney Wong"], "title": "Stronger Together: Unleashing the Social Impact of Hate Speech Research", "categories": ["cs.CL"], "comment": "Accepted Proceedings of the Linguistic Society of America 2025 Annual\n  Meeting", "summary": "The advent of the internet has been both a blessing and a curse for once\nmarginalised communities. When used well, the internet can be used to connect\nand establish communities crossing different intersections; however, it can\nalso be used as a tool to alienate people and communities as well as perpetuate\nhate, misinformation, and disinformation especially on social media platforms.\nWe propose steering hate speech research and researchers away from pre-existing\ncomputational solutions and consider social methods to inform social solutions\nto address this social problem. In a similar way linguistics research can\ninform language planning policy, linguists should apply what we know about\nlanguage and society to mitigate some of the emergent risks and dangers of\nanti-social behaviour in digital spaces. We argue linguists and NLP researchers\ncan play a principle role in unleashing the social impact potential of\nlinguistics research working alongside communities, advocates, activists, and\npolicymakers to enable equitable digital inclusion and to close the digital\ndivide."}
{"id": "2505.13191", "pdf": "https://arxiv.org/pdf/2505.13191", "abs": "https://arxiv.org/abs/2505.13191", "authors": ["Pengcheng Pan", "Yonekura Shogo", "Yasuo Kuniyoshi"], "title": "Emergence of Fixational and Saccadic Movements in a Multi-Level Recurrent Attention Model for Vision", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Inspired by foveal vision, hard attention models promise interpretability and\nparameter economy. However, existing models like the Recurrent Model of Visual\nAttention (RAM) and Deep Recurrent Attention Model (DRAM) failed to model the\nhierarchy of human vision system, that compromise on the visual exploration\ndynamics. As a result, they tend to produce attention that are either overly\nfixational or excessively saccadic, diverging from human eye movement behavior.\nIn this paper, we propose a Multi-Level Recurrent Attention Model (MRAM), a\nnovel hard attention framework that explicitly models the neural hierarchy of\nhuman visual processing. By decoupling the function of glimpse location\ngeneration and task execution in two recurrent layers, MRAM emergent a balanced\nbehavior between fixation and saccadic movement. Our results show that MRAM not\nonly achieves more human-like attention dynamics, but also consistently\noutperforms CNN, RAM and DRAM baselines on standard image classification\nbenchmarks."}
{"id": "2505.13252", "pdf": "https://arxiv.org/pdf/2505.13252", "abs": "https://arxiv.org/abs/2505.13252", "authors": ["Rikhil Amonkar", "Ronan Le Bras", "Li Zhang"], "title": "Natural Language Planning via Coding and Inference Scaling", "categories": ["cs.CL"], "comment": null, "summary": "Real-life textual planning tasks such as meeting scheduling have posed much\nchallenge to LLMs especially when the complexity is high. While previous work\nprimarily studied auto-regressive generation of plans with closed-source\nmodels, we systematically evaluate both closed- and open-source models,\nincluding those that scales output length with complexity during inference, in\ngenerating programs, which are executed to output the plan. We consider not\nonly standard Python code, but also the code to a constraint satisfaction\nproblem solver. Despite the algorithmic nature of the task, we show that\nprogramming often but not always outperforms planning. Our detailed error\nanalysis also indicates a lack of robustness and efficiency in the generated\ncode that hinders generalization."}
{"id": "2505.13201", "pdf": "https://arxiv.org/pdf/2505.13201", "abs": "https://arxiv.org/abs/2505.13201", "authors": ["Yuzhen Chen", "Hojun Son", "Arpan Kusari"], "title": "MatPredict: a dataset and benchmark for learning material properties of diverse indoor objects", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Determining material properties from camera images can expand the ability to\nidentify complex objects in indoor environments, which is valuable for consumer\nrobotics applications. To support this, we introduce MatPredict, a dataset that\ncombines the high-quality synthetic objects from Replica dataset with MatSynth\ndataset's material properties classes - to create objects with diverse material\nproperties. We select 3D meshes of specific foreground objects and render them\nwith different material properties. In total, we generate \\textbf{18} commonly\noccurring objects with \\textbf{14} different materials. We showcase how we\nprovide variability in terms of lighting and camera placement for these\nobjects. Next, we provide a benchmark for inferring material properties from\nvisual images using these perturbed models in the scene, discussing the\nspecific neural network models involved and their performance based on\ndifferent image comparison metrics. By accurately simulating light interactions\nwith different materials, we can enhance realism, which is crucial for training\nmodels effectively through large-scale simulations. This research aims to\nrevolutionize perception in consumer robotics. The dataset is provided\n\\href{https://huggingface.co/datasets/UMTRI/MatPredict}{here} and the code is\nprovided \\href{https://github.com/arpan-kusari/MatPredict}{here}."}
{"id": "2505.13254", "pdf": "https://arxiv.org/pdf/2505.13254", "abs": "https://arxiv.org/abs/2505.13254", "authors": ["Siran Liu", "Yang Ye", "Qianchao Zhu", "Zheng Cao", "Yongchao He"], "title": "HeteroSpec: Leveraging Contextual Heterogeneity for Efficient Speculative Decoding", "categories": ["cs.CL"], "comment": null, "summary": "Autoregressive decoding, the standard approach for Large Language Model (LLM)\ninference, remains a significant bottleneck due to its sequential nature. While\nspeculative decoding algorithms mitigate this inefficiency through parallel\nverification, they fail to exploit the inherent heterogeneity in linguistic\ncomplexity, a key factor leading to suboptimal resource allocation. We address\nthis by proposing HeteroSpec, a heterogeneity-adaptive speculative decoding\nframework that dynamically optimizes computational resource allocation based on\nlinguistic context complexity. HeteroSpec introduces two key mechanisms: (1) A\nnovel cumulative meta-path Top-$K$ entropy metric for efficiently identifying\npredictable contexts. (2) A dynamic resource allocation strategy based on\ndata-driven entropy partitioning, enabling adaptive speculative expansion and\npruning tailored to local context difficulty. Evaluated on five public\nbenchmarks and four models, HeteroSpec achieves an average speedup of\n4.26$\\times$. It consistently outperforms state-of-the-art EAGLE-3 across\nspeedup rates, average acceptance length, and verification cost. Notably,\nHeteroSpec requires no draft model retraining, incurs minimal overhead, and is\northogonal to other acceleration techniques. It demonstrates enhanced\nacceleration with stronger draft models, establishing a new paradigm for\ncontext-aware LLM inference acceleration."}
{"id": "2505.13211", "pdf": "https://arxiv.org/pdf/2505.13211", "abs": "https://arxiv.org/abs/2505.13211", "authors": ["Sand. ai", "Hansi Teng", "Hongyu Jia", "Lei Sun", "Lingzhi Li", "Maolin Li", "Mingqiu Tang", "Shuai Han", "Tianning Zhang", "W. Q. Zhang", "Weifeng Luo", "Xiaoyang Kang", "Yuchen Sun", "Yue Cao", "Yunpeng Huang", "Yutong Lin", "Yuxin Fang", "Zewei Tao", "Zheng Zhang", "Zhongshu Wang", "Zixun Liu", "Dai Shi", "Guoli Su", "Hanwen Sun", "Hong Pan", "Jie Wang", "Jiexin Sheng", "Min Cui", "Min Hu", "Ming Yan", "Shucheng Yin", "Siran Zhang", "Tingting Liu", "Xianping Yin", "Xiaoyu Yang", "Xin Song", "Xuan Hu", "Yankai Zhang", "Yuqiao Li"], "title": "MAGI-1: Autoregressive Video Generation at Scale", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present MAGI-1, a world model that generates videos by autoregressively\npredicting a sequence of video chunks, defined as fixed-length segments of\nconsecutive frames. Trained to denoise per-chunk noise that increases\nmonotonically over time, MAGI-1 enables causal temporal modeling and naturally\nsupports streaming generation. It achieves strong performance on image-to-video\n(I2V) tasks conditioned on text instructions, providing high temporal\nconsistency and scalability, which are made possible by several algorithmic\ninnovations and a dedicated infrastructure stack. MAGI-1 facilitates\ncontrollable generation via chunk-wise prompting and supports real-time,\nmemory-efficient deployment by maintaining constant peak inference cost,\nregardless of video length. The largest variant of MAGI-1 comprises 24 billion\nparameters and supports context lengths of up to 4 million tokens,\ndemonstrating the scalability and robustness of our approach. The code and\nmodels are available at https://github.com/SandAI-org/MAGI-1 and\nhttps://github.com/SandAI-org/MagiAttention. The product can be accessed at\nhttps://sand.ai."}
{"id": "2505.13257", "pdf": "https://arxiv.org/pdf/2505.13257", "abs": "https://arxiv.org/abs/2505.13257", "authors": ["Zilu Tang", "Afra Feyza Akyürek", "Ekin Akyürek", "Derry Wijaya"], "title": "WikiPersonas: What Can We Learn From Personalized Alignment to Famous People?", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "9 pages, preprint", "summary": "Preference alignment has become a standard pipeline in finetuning models to\nfollow \\emph{generic} human preferences. Majority of work seeks to optimize\nmodel to produce responses that would be preferable \\emph{on average},\nsimplifying the diverse and often \\emph{contradicting} space of human\npreferences. While research has increasingly focused on personalized alignment:\nadapting models to individual user preferences, there is a lack of personalized\npreference dataset which focus on nuanced individual-level preferences. To\naddress this, we introduce WikiPersona: the first fine-grained personalization\nusing well-documented, famous individuals. Our dataset challenges models to\nalign with these personas through an interpretable process: generating\nverifiable textual descriptions of a persona's background and preferences in\naddition to alignment. We systematically evaluate different personalization\napproaches and find that as few-shot prompting with preferences and fine-tuning\nfail to simultaneously ensure effectiveness and efficiency, using\n\\textit{inferred personal preferences} as prefixes enables effective\npersonalization, especially in topics where preferences clash while leading to\nmore equitable generalization across unseen personas."}
{"id": "2505.13212", "pdf": "https://arxiv.org/pdf/2505.13212", "abs": "https://arxiv.org/abs/2505.13212", "authors": ["Qingling Shu", "Sibao Chen", "Zhihui You", "Wei Lu", "Jin Tang", "Bin Luo"], "title": "RB-SCD: A New Benchmark for Semantic Change Detection of Roads and Bridges in Traffic Scenes", "categories": ["cs.CV"], "comment": null, "summary": "Accurate detection of changes in roads and bridges, such as construction,\nrenovation, and demolition, is essential for urban planning and traffic\nmanagement. However, existing methods often struggle to extract fine-grained\nsemantic change information due to the lack of high-quality annotated datasets\nin traffic scenarios. To address this, we introduce the Road and Bridge\nSemantic Change Detection (RB-SCD) dataset, a comprehensive benchmark\ncomprising 260 pairs of high-resolution remote sensing images from diverse\ncities and countries. RB-SCD captures 11 types of semantic changes across\nvaried road and bridge structures, enabling detailed structural and functional\nanalysis. Building on this dataset, we propose a novel framework, Multimodal\nFrequency-Driven Change Detector (MFDCD), which integrates multimodal features\nin the frequency domain. MFDCD includes a Dynamic Frequency Coupler (DFC) that\nfuses hierarchical visual features with wavelet-based frequency components, and\na Textual Frequency Filter (TFF) that transforms CLIP-derived textual features\ninto the frequency domain and applies graph-based filtering. Experimental\nresults on RB-SCD and three public benchmarks demonstrate the effectiveness of\nour approach."}
{"id": "2505.13258", "pdf": "https://arxiv.org/pdf/2505.13258", "abs": "https://arxiv.org/abs/2505.13258", "authors": ["Jingyi Ren", "Yekun Xu", "Xiaolong Wang", "Weitao Li", "Weizhi Ma", "Yang Liu"], "title": "Effective and Transparent RAG: Adaptive-Reward Reinforcement Learning for Decision Traceability", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has significantly improved the\nperformance of large language models (LLMs) on knowledge-intensive domains.\nHowever, although RAG achieved successes across distinct domains, there are\nstill some unsolved challenges: 1) Effectiveness. Existing research mainly\nfocuses on developing more powerful RAG retrievers, but how to enhance the\ngenerator's (LLM's) ability to utilize the retrieved information for reasoning\nand generation? 2) Transparency. Most RAG methods ignore which retrieved\ncontent actually contributes to the reasoning process, resulting in a lack of\ninterpretability and visibility. To address this, we propose ARENA\n(Adaptive-Rewarded Evidence Navigation Agent), a transparent RAG generator\nframework trained via reinforcement learning (RL) with our proposed rewards.\nBased on the structured generation and adaptive reward calculation, our\nRL-based training enables the model to identify key evidence, perform\nstructured reasoning, and generate answers with interpretable decision traces.\nApplied to Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct, abundant experiments\nwith various RAG baselines demonstrate that our model achieves 10-30%\nimprovements on all multi-hop QA datasets, which is comparable with the SOTA\nCommercially-developed LLMs (e.g., OpenAI-o1, DeepSeek-R1). Further analyses\nshow that ARENA has strong flexibility to be adopted on new datasets without\nextra training. Our models and codes are publicly released."}
{"id": "2505.13215", "pdf": "https://arxiv.org/pdf/2505.13215", "abs": "https://arxiv.org/abs/2505.13215", "authors": ["Seungjun Oh", "Younggeun Lee", "Hyejin Jeon", "Eunbyung Park"], "title": "Hybrid 3D-4D Gaussian Splatting for Fast Dynamic Scene Representation", "categories": ["cs.CV"], "comment": "https://ohsngjun.github.io/3D-4DGS/", "summary": "Recent advancements in dynamic 3D scene reconstruction have shown promising\nresults, enabling high-fidelity 3D novel view synthesis with improved temporal\nconsistency. Among these, 4D Gaussian Splatting (4DGS) has emerged as an\nappealing approach due to its ability to model high-fidelity spatial and\ntemporal variations. However, existing methods suffer from substantial\ncomputational and memory overhead due to the redundant allocation of 4D\nGaussians to static regions, which can also degrade image quality. In this\nwork, we introduce hybrid 3D-4D Gaussian Splatting (3D-4DGS), a novel framework\nthat adaptively represents static regions with 3D Gaussians while reserving 4D\nGaussians for dynamic elements. Our method begins with a fully 4D Gaussian\nrepresentation and iteratively converts temporally invariant Gaussians into 3D,\nsignificantly reducing the number of parameters and improving computational\nefficiency. Meanwhile, dynamic Gaussians retain their full 4D representation,\ncapturing complex motions with high fidelity. Our approach achieves\nsignificantly faster training times compared to baseline 4D Gaussian Splatting\nmethods while maintaining or improving the visual quality."}
{"id": "2505.13259", "pdf": "https://arxiv.org/pdf/2505.13259", "abs": "https://arxiv.org/abs/2505.13259", "authors": ["Tianshi Zheng", "Zheye Deng", "Hong Ting Tsang", "Weiqi Wang", "Jiaxin Bai", "Zihao Wang", "Yangqiu Song"], "title": "From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery", "categories": ["cs.CL"], "comment": "16 pages", "summary": "Large Language Models (LLMs) are catalyzing a paradigm shift in scientific\ndiscovery, evolving from task-specific automation tools into increasingly\nautonomous agents and fundamentally redefining research processes and human-AI\ncollaboration. This survey systematically charts this burgeoning field, placing\na central focus on the changing roles and escalating capabilities of LLMs in\nscience. Through the lens of the scientific method, we introduce a foundational\nthree-level taxonomy-Tool, Analyst, and Scientist-to delineate their escalating\nautonomy and evolving responsibilities within the research lifecycle. We\nfurther identify pivotal challenges and future research trajectories such as\nrobotic automation, self-improvement, and ethical governance. Overall, this\nsurvey provides a conceptual architecture and strategic foresight to navigate\nand shape the future of AI-driven scientific discovery, fostering both rapid\ninnovation and responsible advancement. Github Repository:\nhttps://github.com/HKUST-KnowComp/Awesome-LLM-Scientific-Discovery."}
{"id": "2505.13219", "pdf": "https://arxiv.org/pdf/2505.13219", "abs": "https://arxiv.org/abs/2505.13219", "authors": ["Jiafu Wu", "Yabiao Wang", "Jian Li", "Jinlong Peng", "Yun Cao", "Chengjie Wang", "Jiangning Zhang"], "title": "Swin DiT: Diffusion Transformer using Pseudo Shifted Windows", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion Transformers (DiTs) achieve remarkable performance within the\ndomain of image generation through the incorporation of the transformer\narchitecture. Conventionally, DiTs are constructed by stacking serial isotropic\nglobal information modeling transformers, which face significant computational\ncost when processing high-resolution images. We empirically analyze that latent\nspace image generation does not exhibit a strong dependence on global\ninformation as traditionally assumed. Most of the layers in the model\ndemonstrate redundancy in global computation. In addition, conventional\nattention mechanisms exhibit low-frequency inertia issues. To address these\nissues, we propose \\textbf{P}seudo \\textbf{S}hifted \\textbf{W}indow\n\\textbf{A}ttention (PSWA), which fundamentally mitigates global model\nredundancy. PSWA achieves intermediate global-local information interaction\nthrough window attention, while employing a high-frequency bridging branch to\nsimulate shifted window operations, supplementing appropriate global and\nhigh-frequency information. Furthermore, we propose the Progressive Coverage\nChannel Allocation(PCCA) strategy that captures high-order attention similarity\nwithout additional computational cost. Building upon all of them, we propose a\nseries of Pseudo \\textbf{S}hifted \\textbf{Win}dow DiTs (\\textbf{Swin DiT}),\naccompanied by extensive experiments demonstrating their superior performance.\nFor example, our proposed Swin-DiT-L achieves a 54%$\\uparrow$ FID improvement\nover DiT-XL/2 while requiring less computational.\nhttps://github.com/wujiafu007/Swin-DiT"}
{"id": "2505.13268", "pdf": "https://arxiv.org/pdf/2505.13268", "abs": "https://arxiv.org/abs/2505.13268", "authors": ["Livia Qian", "Carol Figueroa", "Gabriel Skantze"], "title": "Representation of perceived prosodic similarity of conversational feedback", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Interspeech 2025", "summary": "Vocal feedback (e.g., `mhm', `yeah', `okay') is an important component of\nspoken dialogue and is crucial to ensuring common ground in conversational\nsystems. The exact meaning of such feedback is conveyed through both lexical\nand prosodic form. In this work, we investigate the perceived prosodic\nsimilarity of vocal feedback with the same lexical form, and to what extent\nexisting speech representations reflect such similarities. A triadic comparison\ntask with recruited participants is used to measure perceived similarity of\nfeedback responses taken from two different datasets. We find that spectral and\nself-supervised speech representations encode prosody better than extracted\npitch features, especially in the case of feedback from the same speaker. We\nalso find that it is possible to further condense and align the representations\nto human perception through contrastive learning."}
{"id": "2505.13225", "pdf": "https://arxiv.org/pdf/2505.13225", "abs": "https://arxiv.org/abs/2505.13225", "authors": ["David Levin", "Gonen Singer"], "title": "Automatic Complementary Separation Pruning Toward Lightweight CNNs", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we present Automatic Complementary Separation Pruning (ACSP),\na novel and fully automated pruning method for convolutional neural networks.\nACSP integrates the strengths of both structured pruning and activation-based\npruning, enabling the efficient removal of entire components such as neurons\nand channels while leveraging activations to identify and retain the most\nrelevant components. Our approach is designed specifically for supervised\nlearning tasks, where we construct a graph space that encodes the separation\ncapabilities of each component with respect to all class pairs. By employing\ncomplementary selection principles and utilizing a clustering algorithm, ACSP\nensures that the selected components maintain diverse and complementary\nseparation capabilities, reducing redundancy and maintaining high network\nperformance. The method automatically determines the optimal subset of\ncomponents in each layer, utilizing a knee-finding algorithm to select the\nminimal subset that preserves performance without requiring user-defined\npruning volumes. Extensive experiments on multiple architectures, including\nVGG-16, ResNet-50, and MobileNet-V2, across datasets like CIFAR-10, CIFAR-100,\nand ImageNet-1K, demonstrate that ACSP achieves competitive accuracy compared\nto other methods while significantly reducing computational costs. This fully\nautomated approach not only enhances scalability but also makes ACSP especially\npractical for real-world deployment by eliminating the need for manually\ndefining the pruning volume."}
{"id": "2505.13271", "pdf": "https://arxiv.org/pdf/2505.13271", "abs": "https://arxiv.org/abs/2505.13271", "authors": ["Lei Sheng", "Shuai-Shuai Xu"], "title": "CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement Learning", "categories": ["cs.CL"], "comment": "11 pages, 5 figures", "summary": "Large language models (LLMs) have demonstrated strong capabilities in\ntranslating natural language questions about relational databases into SQL\nqueries. In particular, test-time scaling techniques such as Self-Consistency\nand Self-Correction can enhance SQL generation accuracy by increasing\ncomputational effort during inference. However, these methods have notable\nlimitations: Self-Consistency may select suboptimal outputs despite majority\nvotes, while Self-Correction typically addresses only syntactic errors. To\nleverage the strengths of both approaches, we propose CSC-SQL, a novel method\nthat integrates Self-Consistency and Self-Correction. CSC-SQL selects the two\nmost frequently occurring outputs from parallel sampling and feeds them into a\nmerge revision model for correction. Additionally, we employ the Group Relative\nPolicy Optimization (GRPO) algorithm to fine-tune both the SQL generation and\nrevision models via reinforcement learning, significantly enhancing output\nquality. Experimental results confirm the effectiveness and generalizability of\nCSC-SQL. On the BIRD development set, our 3B model achieves 65.28% execution\naccuracy, while the 7B model achieves 69.19%. The code will be open sourced at\nhttps://github.com/CycloneBoy/csc_sql."}
{"id": "2505.13233", "pdf": "https://arxiv.org/pdf/2505.13233", "abs": "https://arxiv.org/abs/2505.13233", "authors": ["Lincan Cai", "Jingxuan Kang", "Shuang Li", "Wenxuan Ma", "Binhui Xie", "Zhida Qin", "Jian Liang"], "title": "From Local Details to Global Context: Advancing Vision-Language Models with Attention-Based Selection", "categories": ["cs.CV"], "comment": null, "summary": "Pretrained vision-language models (VLMs), e.g., CLIP, demonstrate impressive\nzero-shot capabilities on downstream tasks. Prior research highlights the\ncrucial role of visual augmentation techniques, like random cropping, in\nalignment with fine-grained class descriptions generated by large language\nmodels (LLMs), significantly enhancing zero-shot performance by incorporating\nmulti-view information. However, the inherent randomness of these augmentations\ncan inevitably introduce background artifacts and cause models to overly focus\non local details, compromising global semantic understanding. To address these\nissues, we propose an \\textbf{A}ttention-\\textbf{B}ased \\textbf{S}election\n(\\textbf{ABS}) method from local details to global context, which applies\nattention-guided cropping in both raw images and feature space, supplement\nglobal semantic information through strategic feature selection. Additionally,\nwe introduce a soft matching technique to effectively filter LLM descriptions\nfor better alignment. \\textbf{ABS} achieves state-of-the-art performance on\nout-of-distribution generalization and zero-shot classification tasks. Notably,\n\\textbf{ABS} is training-free and even rivals few-shot and test-time adaptation\nmethods. Our code is available at\n\\href{https://github.com/BIT-DA/ABS}{\\textcolor{darkgreen}{https://github.com/BIT-DA/ABS}}."}
{"id": "2505.13282", "pdf": "https://arxiv.org/pdf/2505.13282", "abs": "https://arxiv.org/abs/2505.13282", "authors": ["Sahil Mishra", "Kumar Arjun", "Tanmoy Chakraborty"], "title": "$\\textit{Rank, Chunk and Expand}$: Lineage-Oriented Reasoning for Taxonomy Expansion", "categories": ["cs.CL"], "comment": null, "summary": "Taxonomies are hierarchical knowledge graphs crucial for recommendation\nsystems, and web applications. As data grows, expanding taxonomies is\nessential, but existing methods face key challenges: (1) discriminative models\nstruggle with representation limits and generalization, while (2) generative\nmethods either process all candidates at once, introducing noise and exceeding\ncontext limits, or discard relevant entities by selecting noisy candidates. We\npropose LORex ($\\textbf{L}$ineage-$\\textbf{O}$riented $\\textbf{Re}$asoning for\nTaxonomy E$\\textbf{x}$pansion), a plug-and-play framework that combines\ndiscriminative ranking and generative reasoning for efficient taxonomy\nexpansion. Unlike prior methods, LORex ranks and chunks candidate terms into\nbatches, filtering noise and iteratively refining selections by reasoning\ncandidates' hierarchy to ensure contextual efficiency. Extensive experiments\nacross four benchmarks and twelve baselines show that LORex improves accuracy\nby 12% and Wu & Palmer similarity by 5% over state-of-the-art methods."}
{"id": "2505.13235", "pdf": "https://arxiv.org/pdf/2505.13235", "abs": "https://arxiv.org/abs/2505.13235", "authors": ["Dang Hoai Nam", "Huynh Tong Dang Khoa", "Vo Nguyen Le Duy"], "title": "WriteViT: Handwritten Text Generation with Vision Transformer", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Humans can quickly generalize handwriting styles from a single example by\nintuitively separating content from style. Machines, however, struggle with\nthis task, especially in low-data settings, often missing subtle spatial and\nstylistic cues. Motivated by this gap, we introduce WriteViT, a one-shot\nhandwritten text synthesis framework that incorporates Vision Transformers\n(ViT), a family of models that have shown strong performance across various\ncomputer vision tasks. WriteViT integrates a ViT-based Writer Identifier for\nextracting style embeddings, a multi-scale generator built with Transformer\nencoder-decoder blocks enhanced by conditional positional encoding (CPE), and a\nlightweight ViT-based recognizer. While previous methods typically rely on CNNs\nor CRNNs, our design leverages transformers in key components to better capture\nboth fine-grained stroke details and higher-level style information. Although\nhandwritten text synthesis has been widely explored, its application to\nVietnamese -- a language rich in diacritics and complex typography -- remains\nlimited. Experiments on Vietnamese and English datasets demonstrate that\nWriteViT produces high-quality, style-consistent handwriting while maintaining\nstrong recognition performance in low-resource scenarios. These results\nhighlight the promise of transformer-based designs for multilingual handwriting\ngeneration and efficient style adaptation."}
{"id": "2505.13302", "pdf": "https://arxiv.org/pdf/2505.13302", "abs": "https://arxiv.org/abs/2505.13302", "authors": ["Alice Plebe", "Timothy Douglas", "Diana Riazi", "R. Maria del Rio-Chanona"], "title": "I'll believe it when I see it: Images increase misinformation sharing in Vision-Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models are increasingly integrated into news recommendation\nsystems, raising concerns about their role in spreading misinformation. In\nhumans, visual content is known to boost credibility and shareability of\ninformation, yet its effect on vision-language models (VLMs) remains unclear.\nWe present the first study examining how images influence VLMs' propensity to\nreshare news content, whether this effect varies across model families, and how\npersona conditioning and content attributes modulate this behavior. To support\nthis analysis, we introduce two methodological contributions: a\njailbreaking-inspired prompting strategy that elicits resharing decisions from\nVLMs while simulating users with antisocial traits and political alignments;\nand a multimodal dataset of fact-checked political news from PolitiFact, paired\nwith corresponding images and ground-truth veracity labels. Experiments across\nmodel families reveal that image presence increases resharing rates by 4.8% for\ntrue news and 15.0% for false news. Persona conditioning further modulates this\neffect: Dark Triad traits amplify resharing of false news, whereas\nRepublican-aligned profiles exhibit reduced veracity sensitivity. Of all the\ntested models, only Claude-3-Haiku demonstrates robustness to visual\nmisinformation. These findings highlight emerging risks in multimodal model\nbehavior and motivate the development of tailored evaluation frameworks and\nmitigation strategies for personalized AI systems. Code and dataset are\navailable at: https://github.com/3lis/misinfo_vlm"}
{"id": "2505.13250", "pdf": "https://arxiv.org/pdf/2505.13250", "abs": "https://arxiv.org/abs/2505.13250", "authors": ["Hashan K. Weerasooriya", "Prateek Chennuri", "Weijian Zhang", "Istvan Gyongy", "Stanley H. Chan"], "title": "Joint Depth and Reflectivity Estimation using Single-Photon LiDAR", "categories": ["cs.CV"], "comment": null, "summary": "Single-Photon Light Detection and Ranging (SP-LiDAR is emerging as a leading\ntechnology for long-range, high-precision 3D vision tasks. In SP-LiDAR,\ntimestamps encode two complementary pieces of information: pulse travel time\n(depth) and the number of photons reflected by the object (reflectivity).\nExisting SP-LiDAR reconstruction methods typically recover depth and\nreflectivity separately or sequentially use one modality to estimate the other.\nMoreover, the conventional 3D histogram construction is effective mainly for\nslow-moving or stationary scenes. In dynamic scenes, however, it is more\nefficient and effective to directly process the timestamps. In this paper, we\nintroduce an estimation method to simultaneously recover both depth and\nreflectivity in fast-moving scenes. We offer two contributions: (1) A\ntheoretical analysis demonstrating the mutual correlation between depth and\nreflectivity and the conditions under which joint estimation becomes\nbeneficial. (2) A novel reconstruction method, \"SPLiDER\", which exploits the\nshared information to enhance signal recovery. On both synthetic and real\nSP-LiDAR data, our method outperforms existing approaches, achieving superior\njoint reconstruction quality."}
{"id": "2505.13307", "pdf": "https://arxiv.org/pdf/2505.13307", "abs": "https://arxiv.org/abs/2505.13307", "authors": ["Qiguang Chen", "Libo Qin", "Jinhao Liu", "Yue Liao", "Jiaqi Wang", "Jingxuan Zhou", "Wanxiang Che"], "title": "RBF++: Quantifying and Optimizing Reasoning Boundaries across Measurable and Unmeasurable Capabilities for Chain-of-Thought Reasoning", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Manuscript", "summary": "Chain-of-Thought (CoT) reasoning has proven effective in enhancing large\nlanguage models (LLMs) on complex tasks, spurring research into its underlying\nmechanisms. However, two primary challenges remain for real-world applications:\n(1) the lack of quantitative metrics and actionable guidelines for evaluating\nand optimizing measurable boundaries of CoT capability, and (2) the absence of\nmethods to assess boundaries of unmeasurable CoT capability, such as multimodal\nperception. To address these gaps, we introduce the Reasoning Boundary\nFramework++ (RBF++). To tackle the first challenge, we define the reasoning\nboundary (RB) as the maximum limit of CoT performance. We also propose a\ncombination law for RBs, enabling quantitative analysis and offering actionable\nguidance across various CoT tasks. For the second challenge, particularly in\nmultimodal scenarios, we introduce a constant assumption, which replaces\nunmeasurable RBs with scenario-specific constants. Additionally, we propose the\nreasoning boundary division mechanism, which divides unmeasurable RBs into two\nsub-boundaries, facilitating the quantification and optimization of both\nunmeasurable domain knowledge and multimodal perception capabilities. Extensive\nexperiments involving 38 models across 13 tasks validate the feasibility of our\nframework in cross-modal settings. Additionally, we evaluate 10 CoT strategies,\noffer insights into optimization and decay from two complementary perspectives,\nand expand evaluation benchmarks for measuring RBs in LLM reasoning. We hope\nthis work advances the understanding of RBs and optimization strategies in\nLLMs. Code and data are available at\nhttps://github.com/LightChen233/reasoning-boundary."}
{"id": "2505.13261", "pdf": "https://arxiv.org/pdf/2505.13261", "abs": "https://arxiv.org/abs/2505.13261", "authors": ["Mingrui Chen", "Haogeng Liu", "Hao Liang", "Huaibo Huang", "Wentao Zhang", "Ran He"], "title": "Unlocking the Potential of Difficulty Prior in RL-based Multimodal Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "In this work, we investigate how explicitly modeling problem's difficulty\nprior information shapes the effectiveness of reinforcement learning based\nfine-tuning for multimodal reasoning. Our exploration mainly comprises of\nfollowing three perspective: First, through offline data curation, we analyze\nthe U-shaped difficulty distribution of two given datasets using the base model\nby multi-round sampling, and then filter out prompts that are either too simple\nor extremely difficult to provide meaningful gradients and perform subsequent\ntwo-stage training. Second, we implement an online advantage differentiation,\ncomputing group-wise empirical accuracy as a difficulty proxy to adaptively\nreweight advantages estimation, providing stronger learning signals for more\nchallenging problems. Finally, we introduce difficulty hints as explicit\nprompts for more complex samples in the second training stage, encouraging the\nmodel to calibrate its reasoning depth and perform reflective validation\nchecks. Our comprehensive approach demonstrates significant performances across\nvarious multi-modal mathematical reasoning benchmarks with only 2K+0.6K\ntwo-stage training data."}
{"id": "2505.13312", "pdf": "https://arxiv.org/pdf/2505.13312", "abs": "https://arxiv.org/abs/2505.13312", "authors": ["Zhijie Deng", "Chris Yuhao Liu", "Zirui Pang", "Xinlei He", "Lei Feng", "Qi Xuan", "Zhaowei Zhu", "Jiaheng Wei"], "title": "GUARD: Generation-time LLM Unlearning via Adaptive Restriction and Detection", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in\nmemorizing vast amounts of knowledge across diverse domains. However, the\nability to selectively forget specific knowledge is critical for ensuring the\nsafety and compliance of deployed models. Existing unlearning efforts typically\nfine-tune the model with resources such as forget data, retain data, and a\ncalibration model. These additional gradient steps blur the decision boundary\nbetween forget and retain knowledge, making unlearning often at the expense of\noverall performance. To avoid the negative impact of fine-tuning, it would be\nbetter to unlearn solely at inference time by safely guarding the model against\ngenerating responses related to the forget target, without destroying the\nfluency of text generation. In this work, we propose Generation-time Unlearning\nvia Adaptive Restriction and Detection (GUARD), a framework that enables\ndynamic unlearning during LLM generation. Specifically, we first employ a\nprompt classifier to detect unlearning targets and extract the corresponding\nforbidden token. We then dynamically penalize and filter candidate tokens\nduring generation using a combination of token matching and semantic matching,\neffectively preventing the model from leaking the forgotten content.\nExperimental results on copyright content unlearning tasks over the Harry\nPotter dataset and the MUSE benchmark, as well as entity unlearning tasks on\nthe TOFU dataset, demonstrate that GUARD achieves strong forget quality across\nvarious tasks while causing almost no degradation to the LLM's general\ncapabilities, striking an excellent trade-off between forgetting and utility."}
{"id": "2505.13266", "pdf": "https://arxiv.org/pdf/2505.13266", "abs": "https://arxiv.org/abs/2505.13266", "authors": ["Yehao Liu", "Xiaosu Xu", "Zijian Wang", "Yiqing Yao"], "title": "DB3D-L: Depth-aware BEV Feature Transformation for Accurate 3D Lane Detection", "categories": ["cs.CV"], "comment": null, "summary": "3D Lane detection plays an important role in autonomous driving. Recent\nadvances primarily build Birds-Eye-View (BEV) feature from front-view (FV)\nimages to perceive 3D information of Lane more effectively. However,\nconstructing accurate BEV information from FV image is limited due to the\nlacking of depth information, causing previous works often rely heavily on the\nassumption of a flat ground plane. Leveraging monocular depth estimation to\nassist in constructing BEV features is less constrained, but existing methods\nstruggle to effectively integrate the two tasks. To address the above issue, in\nthis paper, an accurate 3D lane detection method based on depth-aware BEV\nfeature transtormation is proposed. In detail, an effective feature extraction\nmodule is designed, in which a Depth Net is integrated to obtain the vital\ndepth information for 3D perception, thereby simplifying the complexity of view\ntransformation. Subquently a feature reduce module is proposed to reduce height\ndimension of FV features and depth features, thereby enables effective fusion\nof crucial FV features and depth features. Then a fusion module is designed to\nbuild BEV feature from prime FV feature and depth information. The proposed\nmethod performs comparably with state-of-the-art methods on both synthetic\nApollo, realistic OpenLane datasets."}
{"id": "2505.13328", "pdf": "https://arxiv.org/pdf/2505.13328", "abs": "https://arxiv.org/abs/2505.13328", "authors": ["Hongru Wang", "Wenyu Huang", "Yufei Wang", "Yuanhao Xi", "Jianqiao Lu", "Huan Zhang", "Nan Hu", "Zeming Liu", "Jeff Z. Pan", "Kam-Fai Wong"], "title": "Rethinking Stateful Tool Use in Multi-Turn Dialogues: Benchmarks and Challenges", "categories": ["cs.CL"], "comment": null, "summary": "Existing benchmarks that assess Language Models (LMs) as Language Agents\n(LAs) for tool use primarily focus on stateless, single-turn interactions or\npartial evaluations, such as tool selection in a single turn, overlooking the\ninherent stateful nature of interactions in multi-turn applications. To fulfill\nthis gap, we propose \\texttt{DialogTool}, a multi-turn dialogue dataset with\nstateful tool interactions considering the whole life cycle of tool use, across\nsix key tasks in three stages: 1) \\textit{tool creation}; 2) \\textit{tool\nutilization}: tool awareness, tool selection, tool execution; and 3)\n\\textit{role-consistent response}: response generation and role play.\nFurthermore, we build \\texttt{VirtualMobile} -- an embodied virtual mobile\nevaluation environment to simulate API calls and assess the robustness of the\ncreated APIs\\footnote{We will use tools and APIs alternatively, there are no\nsignificant differences between them in this paper.}. Taking advantage of these\nartifacts, we conduct comprehensive evaluation on 13 distinct open- and\nclosed-source LLMs and provide detailed analysis at each stage, revealing that\nthe existing state-of-the-art LLMs still cannot perform well to use tools over\nlong horizons."}
{"id": "2505.13279", "pdf": "https://arxiv.org/pdf/2505.13279", "abs": "https://arxiv.org/abs/2505.13279", "authors": ["Zhiqiang Yan", "Jianhao Jiao", "Zhengxue Wang", "Gim Hee Lee"], "title": "Event-Driven Dynamic Scene Depth Completion", "categories": ["cs.CV"], "comment": "9 pages", "summary": "Depth completion in dynamic scenes poses significant challenges due to rapid\nego-motion and object motion, which can severely degrade the quality of input\nmodalities such as RGB images and LiDAR measurements. Conventional RGB-D\nsensors often struggle to align precisely and capture reliable depth under such\nconditions. In contrast, event cameras with their high temporal resolution and\nsensitivity to motion at the pixel level provide complementary cues that are\n%particularly beneficial in dynamic environments.To this end, we propose\nEventDC, the first event-driven depth completion framework. It consists of two\nkey components: Event-Modulated Alignment (EMA) and Local Depth Filtering\n(LDF). Both modules adaptively learn the two fundamental components of\nconvolution operations: offsets and weights conditioned on motion-sensitive\nevent streams. In the encoder, EMA leverages events to modulate the sampling\npositions of RGB-D features to achieve pixel redistribution for improved\nalignment and fusion. In the decoder, LDF refines depth estimations around\nmoving objects by learning motion-aware masks from events. Additionally,\nEventDC incorporates two loss terms to further benefit global alignment and\nenhance local depth recovery. Moreover, we establish the first benchmark for\nevent-based depth completion comprising one real-world and two synthetic\ndatasets to facilitate future research. Extensive experiments on this benchmark\ndemonstrate the superiority of our EventDC."}
{"id": "2505.13338", "pdf": "https://arxiv.org/pdf/2505.13338", "abs": "https://arxiv.org/abs/2505.13338", "authors": ["Qiongqiong Wang", "Hardik B. Sailor", "Tianchi Liu", "Ai Ti Aw"], "title": "Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation", "categories": ["cs.CL", "cs.AI", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Current speech-LLMs exhibit limited capability in contextual reasoning\nalongside paralinguistic understanding, primarily due to the lack of\nQuestion-Answer (QA) datasets that cover both aspects. We propose a novel\nframework for dataset generation from in-the-wild speech data, that integrates\ncontextual reasoning with paralinguistic information. It consists of a pseudo\nparalinguistic label-based data condensation of in-the-wild speech and\nLLM-based Contextual Paralinguistic QA (CPQA) generation. The effectiveness is\nvalidated by a strong correlation in evaluations of the Qwen2-Audio-7B-Instruct\nmodel on a dataset created by our framework and human-generated CPQA dataset.\nThe results also reveal the speech-LLM's limitations in handling empathetic\nreasoning tasks, highlighting the need for such datasets and more robust\nmodels. The proposed framework is first of its kind and has potential in\ntraining more robust speech-LLMs with paralinguistic reasoning capabilities."}
{"id": "2505.13281", "pdf": "https://arxiv.org/pdf/2505.13281", "abs": "https://arxiv.org/abs/2505.13281", "authors": ["Zekun Wang", "Sashank Varma"], "title": "Computer Vision Models Show Human-Like Sensitivity to Geometric and Topological Concepts", "categories": ["cs.CV"], "comment": "10 pages, 4 figures, CosSci 2025", "summary": "With the rapid improvement of machine learning (ML) models, cognitive\nscientists are increasingly asking about their alignment with how humans think.\nHere, we ask this question for computer vision models and human sensitivity to\ngeometric and topological (GT) concepts. Under the core knowledge account,\nthese concepts are innate and supported by dedicated neural circuitry. In this\nwork, we investigate an alternative explanation, that GT concepts are learned\n``for free'' through everyday interaction with the environment. We do so using\ncomputer visions models, which are trained on large image datasets. We build on\nprior studies to investigate the overall performance and human alignment of\nthree classes of models -- convolutional neural networks (CNNs),\ntransformer-based models, and vision-language models -- on an odd-one-out task\ntesting 43 GT concepts spanning seven classes. Transformer-based models achieve\nthe highest overall accuracy, surpassing that of young children. They also show\nstrong alignment with children's performance, finding the same classes of\nconcepts easy vs. difficult. By contrast, vision-language models underperform\ntheir vision-only counterparts and deviate further from human profiles,\nindicating that na\\\"ive multimodality might compromise abstract geometric\nsensitivity. These findings support the use of computer vision models to\nevaluate the sufficiency of the learning account for explaining human\nsensitivity to GT concepts, while also suggesting that integrating linguistic\nand visual representations might have unpredicted deleterious consequences."}
{"id": "2505.13346", "pdf": "https://arxiv.org/pdf/2505.13346", "abs": "https://arxiv.org/abs/2505.13346", "authors": ["Austin Xu", "Yilun Zhou", "Xuan-Phi Nguyen", "Caiming Xiong", "Shafiq Joty"], "title": "J4R: Learning to Judge with Equivalent Initial State Group Relative Preference Optimization", "categories": ["cs.CL", "cs.AI"], "comment": "25 pages, 4 figures, 6 tables. To be updated with links for\n  code/benchmark", "summary": "To keep pace with the increasing pace of large language models (LLM)\ndevelopment, model output evaluation has transitioned away from time-consuming\nhuman evaluation to automatic evaluation, where LLMs themselves are tasked with\nassessing and critiquing other model outputs. LLM-as-judge models are a class\nof generative evaluators that excel in evaluating relatively simple domains,\nlike chat quality, but struggle in reasoning intensive domains where model\nresponses contain more substantive and challenging content. To remedy existing\njudge shortcomings, we explore training judges with reinforcement learning\n(RL). We make three key contributions: (1) We propose the Equivalent Initial\nState Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us\nto train our judge to be robust to positional biases that arise in more complex\nevaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that\nevaluates judges in diverse reasoning settings not covered by prior work. (3)\nWe train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that\noutperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or\nexceeding the performance of larger GRPO-trained judges on both JudgeBench and\nReasoningJudgeBench."}
{"id": "2505.13300", "pdf": "https://arxiv.org/pdf/2505.13300", "abs": "https://arxiv.org/abs/2505.13300", "authors": ["Zekai Li", "Xinhao Zhong", "Samir Khaki", "Zhiyuan Liang", "Yuhao Zhou", "Mingjia Shi", "Ziqiao Wang", "Xuanlei Zhao", "Wangbo Zhao", "Ziheng Qin", "Mengxuan Wu", "Pengfei Zhou", "Haonan Wang", "David Junhao Zhang", "Jia-Wei Liu", "Shaobo Wang", "Dai Liu", "Linfeng Zhang", "Guang Li", "Kun Wang", "Zheng Zhu", "Zhiheng Ma", "Joey Tianyi Zhou", "Jiancheng Lv", "Yaochu Jin", "Peihao Wang", "Kaipeng Zhang", "Lingjuan Lyu", "Yiran Huang", "Zeynep Akata", "Zhiwei Deng", "Xindi Wu", "George Cazenavette", "Yuzhang Shang", "Justin Cui", "Jindong Gu", "Qian Zheng", "Hao Ye", "Shuo Wang", "Xiaobo Wang", "Yan Yan", "Angela Yao", "Mike Zheng Shou", "Tianlong Chen", "Hakan Bilen", "Baharan Mirzasoleiman", "Manolis Kellis", "Konstantinos N. Plataniotis", "Zhangyang Wang", "Bo Zhao", "Yang You", "Kai Wang"], "title": "DD-Ranking: Rethinking the Evaluation of Dataset Distillation", "categories": ["cs.CV"], "comment": "20 pages, 4 figures", "summary": "In recent years, dataset distillation has provided a reliable solution for\ndata compression, where models trained on the resulting smaller synthetic\ndatasets achieve performance comparable to those trained on the original\ndatasets. To further improve the performance of synthetic datasets, various\ntraining pipelines and optimization objectives have been proposed, greatly\nadvancing the field of dataset distillation. Recent decoupled dataset\ndistillation methods introduce soft labels and stronger data augmentation\nduring the post-evaluation phase and scale dataset distillation up to larger\ndatasets (e.g., ImageNet-1K). However, this raises a question: Is accuracy\nstill a reliable metric to fairly evaluate dataset distillation methods? Our\nempirical findings suggest that the performance improvements of these methods\noften stem from additional techniques rather than the inherent quality of the\nimages themselves, with even randomly sampled images achieving superior\nresults. Such misaligned evaluation settings severely hinder the development of\nDD. Therefore, we propose DD-Ranking, a unified evaluation framework, along\nwith new general evaluation metrics to uncover the true performance\nimprovements achieved by different methods. By refocusing on the actual\ninformation enhancement of distilled datasets, DD-Ranking provides a more\ncomprehensive and fair evaluation standard for future research advancements."}
{"id": "2505.13348", "pdf": "https://arxiv.org/pdf/2505.13348", "abs": "https://arxiv.org/abs/2505.13348", "authors": ["Narek Maloyan", "Bislan Ashinov", "Dmitry Namiot"], "title": "Investigating the Vulnerability of LLM-as-a-Judge Architectures to Prompt-Injection Attacks", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly employed as evaluators\n(LLM-as-a-Judge) for assessing the quality of machine-generated text. This\nparadigm offers scalability and cost-effectiveness compared to human\nannotation. However, the reliability and security of such systems, particularly\ntheir robustness against adversarial manipulations, remain critical concerns.\nThis paper investigates the vulnerability of LLM-as-a-Judge architectures to\nprompt-injection attacks, where malicious inputs are designed to compromise the\njudge's decision-making process. We formalize two primary attack strategies:\nComparative Undermining Attack (CUA), which directly targets the final decision\noutput, and Justification Manipulation Attack (JMA), which aims to alter the\nmodel's generated reasoning. Using the Greedy Coordinate Gradient (GCG)\noptimization method, we craft adversarial suffixes appended to one of the\nresponses being compared. Experiments conducted on the MT-Bench Human Judgments\ndataset with open-source instruction-tuned LLMs (Qwen2.5-3B-Instruct and\nFalcon3-3B-Instruct) demonstrate significant susceptibility. The CUA achieves\nan Attack Success Rate (ASR) exceeding 30\\%, while JMA also shows notable\neffectiveness. These findings highlight substantial vulnerabilities in current\nLLM-as-a-Judge systems, underscoring the need for robust defense mechanisms and\nfurther research into adversarial evaluation and trustworthiness in LLM-based\nassessment frameworks."}
{"id": "2505.13306", "pdf": "https://arxiv.org/pdf/2505.13306", "abs": "https://arxiv.org/abs/2505.13306", "authors": ["Chengsong Sun", "Weiping Li", "Xiang Li", "Yuankun Liu", "Lianlei Shan"], "title": "GMM-Based Comprehensive Feature Extraction and Relative Distance Preservation For Few-Shot Cross-Modal Retrieval", "categories": ["cs.CV", "cs.IR"], "comment": null, "summary": "Few-shot cross-modal retrieval focuses on learning cross-modal\nrepresentations with limited training samples, enabling the model to handle\nunseen classes during inference. Unlike traditional cross-modal retrieval\ntasks, which assume that both training and testing data share the same class\ndistribution, few-shot retrieval involves data with sparse representations\nacross modalities. Existing methods often fail to adequately model the\nmulti-peak distribution of few-shot cross-modal data, resulting in two main\nbiases in the latent semantic space: intra-modal bias, where sparse samples\nfail to capture intra-class diversity, and inter-modal bias, where\nmisalignments between image and text distributions exacerbate the semantic gap.\nThese biases hinder retrieval accuracy. To address these issues, we propose a\nnovel method, GCRDP, for few-shot cross-modal retrieval. This approach\neffectively captures the complex multi-peak distribution of data using a\nGaussian Mixture Model (GMM) and incorporates a multi-positive sample\ncontrastive learning mechanism for comprehensive feature modeling.\nAdditionally, we introduce a new strategy for cross-modal semantic alignment,\nwhich constrains the relative distances between image and text feature\ndistributions, thereby improving the accuracy of cross-modal representations.\nWe validate our approach through extensive experiments on four benchmark\ndatasets, demonstrating superior performance over six state-of-the-art methods."}
{"id": "2505.13353", "pdf": "https://arxiv.org/pdf/2505.13353", "abs": "https://arxiv.org/abs/2505.13353", "authors": ["Adam Štorek", "Mukur Gupta", "Samira Hajizadeh", "Prashast Srivastava", "Suman Jana"], "title": "Sense and Sensitivity: Examining the Influence of Semantic Recall on Long Context Code Reasoning", "categories": ["cs.CL", "cs.LG", "cs.SE"], "comment": null, "summary": "Although modern Large Language Models (LLMs) support extremely large\ncontexts, their effectiveness in utilizing long context for code reasoning\nremains unclear. This paper investigates LLM reasoning ability over code\nsnippets within large repositories and how it relates to their recall ability.\nSpecifically, we differentiate between lexical code recall (verbatim retrieval)\nand semantic code recall (remembering what the code does). To measure semantic\nrecall, we propose SemTrace, a code reasoning technique where the impact of\nspecific statements on output is attributable and unpredictable. We also\npresent a method to quantify semantic recall sensitivity in existing\nbenchmarks. Our evaluation of state-of-the-art LLMs reveals a significant drop\nin code reasoning accuracy as a code snippet approaches the middle of the input\ncontext, particularly with techniques requiring high semantic recall like\nSemTrace. Moreover, we find that lexical recall varies by granularity, with\nmodels excelling at function retrieval but struggling with line-by-line recall.\nNotably, a disconnect exists between lexical and semantic recall, suggesting\ndifferent underlying mechanisms. Finally, our findings indicate that current\ncode reasoning benchmarks may exhibit low semantic recall sensitivity,\npotentially underestimating LLM challenges in leveraging in-context\ninformation."}
{"id": "2505.13309", "pdf": "https://arxiv.org/pdf/2505.13309", "abs": "https://arxiv.org/abs/2505.13309", "authors": ["Jad Mansour", "Sebastian Realpe", "Hayat Rajani", "Michele Grimaldi", "Rafael Garcia", "Nuno Gracias"], "title": "eStonefish-scenes: A synthetically generated dataset for underwater event-based optical flow prediction tasks", "categories": ["cs.CV", "I.2.5; I.2.6; I.2.9; I.2.10"], "comment": "Submitted to IJRR", "summary": "The combined use of event-based vision and Spiking Neural Networks (SNNs) is\nexpected to significantly impact robotics, particularly in tasks like visual\nodometry and obstacle avoidance. While existing real-world event-based datasets\nfor optical flow prediction, typically captured with Unmanned Aerial Vehicles\n(UAVs), offer valuable insights, they are limited in diversity, scalability,\nand are challenging to collect. Moreover, there is a notable lack of labelled\ndatasets for underwater applications, which hinders the integration of\nevent-based vision with Autonomous Underwater Vehicles (AUVs). To address this,\nsynthetic datasets could provide a scalable solution while bridging the gap\nbetween simulation and reality. In this work, we introduce eStonefish-scenes, a\nsynthetic event-based optical flow dataset based on the Stonefish simulator.\nAlong with the dataset, we present a data generation pipeline that enables the\ncreation of customizable underwater environments. This pipeline allows for\nsimulating dynamic scenarios, such as biologically inspired schools of fish\nexhibiting realistic motion patterns, including obstacle avoidance and reactive\nnavigation around corals. Additionally, we introduce a scene generator that can\nbuild realistic reef seabeds by randomly distributing coral across the terrain.\nTo streamline data accessibility, we present eWiz, a comprehensive library\ndesigned for processing event-based data, offering tools for data loading,\naugmentation, visualization, encoding, and training data generation, along with\nloss functions and performance metrics."}
{"id": "2505.13360", "pdf": "https://arxiv.org/pdf/2505.13360", "abs": "https://arxiv.org/abs/2505.13360", "authors": ["Chenyang Yang", "Yike Shi", "Qianou Ma", "Michael Xieyang Liu", "Christian Kästner", "Tongshuang Wu"], "title": "What Prompts Don't Say: Understanding and Managing Underspecification in LLM Prompts", "categories": ["cs.CL", "cs.SE"], "comment": null, "summary": "Building LLM-powered software requires developers to communicate their\nrequirements through natural language, but developer prompts are frequently\nunderspecified, failing to fully capture many user-important requirements. In\nthis paper, we present an in-depth analysis of prompt underspecification,\nshowing that while LLMs can often (41.1%) guess unspecified requirements by\ndefault, such behavior is less robust: Underspecified prompts are 2x more\nlikely to regress over model or prompt changes, sometimes with accuracy drops\nby more than 20%. We then demonstrate that simply adding more requirements to a\nprompt does not reliably improve performance, due to LLMs' limited\ninstruction-following capabilities and competing constraints, and standard\nprompt optimizers do not offer much help. To address this, we introduce novel\nrequirements-aware prompt optimization mechanisms that can improve performance\nby 4.8% on average over baselines that naively specify everything in the\nprompt. Beyond prompt optimization, we envision that effectively managing\nprompt underspecification requires a broader process, including proactive\nrequirements discovery, evaluation, and monitoring."}
{"id": "2505.13316", "pdf": "https://arxiv.org/pdf/2505.13316", "abs": "https://arxiv.org/abs/2505.13316", "authors": ["Gabriele Spadaro", "Alberto Presta", "Jhony H. Giraldo", "Marco Grangetto", "Wei Hu", "Giuseppe Valenzise", "Attilio Fiandrotti", "Enzo Tartaglione"], "title": "Denoising Diffusion Probabilistic Model for Point Cloud Compression at Low Bit-Rates", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "6 pages, 5 figures, accepted at ICME 2025", "summary": "Efficient compression of low-bit-rate point clouds is critical for\nbandwidth-constrained applications. However, existing techniques mainly focus\non high-fidelity reconstruction, requiring many bits for compression. This\npaper proposes a \"Denoising Diffusion Probabilistic Model\" (DDPM) architecture\nfor point cloud compression (DDPM-PCC) at low bit-rates. A PointNet encoder\nproduces the condition vector for the generation, which is then quantized via a\nlearnable vector quantizer. This configuration allows to achieve a low bitrates\nwhile preserving quality. Experiments on ShapeNet and ModelNet40 show improved\nrate-distortion at low rates compared to standardized and state-of-the-art\napproaches. We publicly released the code at\nhttps://github.com/EIDOSLAB/DDPM-PCC."}
{"id": "2505.13379", "pdf": "https://arxiv.org/pdf/2505.13379", "abs": "https://arxiv.org/abs/2505.13379", "authors": ["Gongfan Fang", "Xinyin Ma", "Xinchao Wang"], "title": "Thinkless: LLM Learns When to Think", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reasoning Language Models, capable of extended chain-of-thought reasoning,\nhave demonstrated remarkable performance on tasks requiring complex logical\ninference. However, applying elaborate reasoning for all queries often results\nin substantial computational inefficiencies, particularly when many problems\nadmit straightforward solutions. This motivates an open question: Can LLMs\nlearn when to think? To answer this, we propose Thinkless, a learnable\nframework that empowers an LLM to adaptively select between short-form and\nlong-form reasoning, based on both task complexity and the model's ability.\nThinkless is trained under a reinforcement learning paradigm and employs two\ncontrol tokens, <short> for concise responses and <think> for detailed\nreasoning. At the core of our method is a Decoupled Group Relative Policy\nOptimization (DeGRPO) algorithm, which decomposes the learning objective of\nhybrid reasoning into two components: (1) a control token loss that governs the\nselection of the reasoning mode, and (2) a response loss that improves the\naccuracy of the generated answers. This decoupled formulation enables\nfine-grained control over the contributions of each objective, stabilizing\ntraining and effectively preventing collapse observed in vanilla GRPO.\nEmpirically, on several benchmarks such as Minerva Algebra, MATH-500, and\nGSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% -\n90%, significantly improving the efficiency of Reasoning Language Models. The\ncode is available at https://github.com/VainF/Thinkless"}
{"id": "2505.13318", "pdf": "https://arxiv.org/pdf/2505.13318", "abs": "https://arxiv.org/abs/2505.13318", "authors": ["Paula Feldman", "Martin Sinnona", "Viviana Siless", "Claudio Delrieux", "Emmanuel Iarussi"], "title": "VesselGPT: Autoregressive Modeling of Vascular Geometry", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Anatomical trees are critical for clinical diagnosis and treatment planning,\nyet their complex and diverse geometry make accurate representation a\nsignificant challenge. Motivated by the latest advances in large language\nmodels, we introduce an autoregressive method for synthesizing anatomical\ntrees. Our approach first embeds vessel structures into a learned discrete\nvocabulary using a VQ-VAE architecture, then models their generation\nautoregressively with a GPT-2 model. This method effectively captures intricate\ngeometries and branching patterns, enabling realistic vascular tree synthesis.\nComprehensive qualitative and quantitative evaluations reveal that our\ntechnique achieves high-fidelity tree reconstruction with compact discrete\nrepresentations. Moreover, our B-spline representation of vessel cross-sections\npreserves critical morphological details that are often overlooked in previous'\nmethods parameterizations. To the best of our knowledge, this work is the first\nto generate blood vessels in an autoregressive manner. Code, data, and trained\nmodels will be made available."}
{"id": "2505.13388", "pdf": "https://arxiv.org/pdf/2505.13388", "abs": "https://arxiv.org/abs/2505.13388", "authors": ["David Anugraha", "Zilu Tang", "Lester James V. Miranda", "Hanyang Zhao", "Mohammad Rifqi Farhansyah", "Garry Kuwanto", "Derry Wijaya", "Genta Indra Winata"], "title": "R3: Robust Rubric-Agnostic Reward Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preprint", "summary": "Reward models are essential for aligning language model outputs with human\npreferences, yet existing approaches often lack both controllability and\ninterpretability. These models are typically optimized for narrow objectives,\nlimiting their generalizability to broader downstream tasks. Moreover, their\nscalar outputs are difficult to interpret without contextual reasoning. To\naddress these limitations, we introduce R3, a novel reward modeling framework\nthat is rubric-agnostic, generalizable across evaluation dimensions, and\nprovides interpretable, reasoned score assignments. R3 enables more transparent\nand flexible evaluation of language models, supporting robust alignment with\ndiverse human values and use cases. Our models, data, and code are available as\nopen source at https://github.com/rubricreward/r3"}
{"id": "2505.13327", "pdf": "https://arxiv.org/pdf/2505.13327", "abs": "https://arxiv.org/abs/2505.13327", "authors": ["Ajian Liu", "Haocheng Yuan", "Xiao Guo", "Hui Ma", "Wanyi Zhuang", "Changtao Miao", "Yan Hong", "Chuanbiao Song", "Jun Lan", "Qi Chu", "Tao Gong", "Yanyan Liang", "Weiqiang Wang", "Jun Wan", "Xiaoming Liu", "Zhen Lei"], "title": "Benchmarking Unified Face Attack Detection via Hierarchical Prompt Tuning", "categories": ["cs.CV"], "comment": null, "summary": "Presentation Attack Detection and Face Forgery Detection are designed to\nprotect face data from physical media-based Presentation Attacks and digital\nediting-based DeepFakes respectively. But separate training of these two models\nmakes them vulnerable to unknown attacks and burdens deployment environments.\nThe lack of a Unified Face Attack Detection model to handle both types of\nattacks is mainly due to two factors. First, there's a lack of adequate\nbenchmarks for models to explore. Existing UAD datasets have limited attack\ntypes and samples, restricting the model's ability to address advanced threats.\nTo address this, we propose UniAttackDataPlus (UniAttackData+), the most\nextensive and sophisticated collection of forgery techniques to date. It\nincludes 2,875 identities and their 54 kinds of falsified samples, totaling\n697,347 videos. Second, there's a lack of a reliable classification criterion.\nCurrent methods try to find an arbitrary criterion within the same semantic\nspace, which fails when encountering diverse attacks. So, we present a novel\nVisual-Language Model-based Hierarchical Prompt Tuning Framework (HiPTune) that\nadaptively explores multiple classification criteria from different semantic\nspaces. We build a Visual Prompt Tree to explore various classification rules\nhierarchically. Then, by adaptively pruning the prompts, the model can select\nthe most suitable prompts to guide the encoder to extract discriminative\nfeatures at different levels in a coarse-to-fine way. Finally, to help the\nmodel understand the classification criteria in visual space, we propose a\nDynamically Prompt Integration module to project the visual prompts to the text\nencoder for more accurate semantics. Experiments on 12 datasets have shown the\npotential to inspire further innovations in the UAD field."}
{"id": "2505.13403", "pdf": "https://arxiv.org/pdf/2505.13403", "abs": "https://arxiv.org/abs/2505.13403", "authors": ["Renjie Pi", "Felix Bai", "Qibin Chen", "Simon Wang", "Jiulong Shan", "Kieran Liu", "Meng Cao"], "title": "MR. Judge: Multimodal Reasoner as a Judge", "categories": ["cs.CL"], "comment": null, "summary": "The paradigm of using Large Language Models (LLMs) and Multimodal Large\nLanguage Models (MLLMs) as evaluative judges has emerged as an effective\napproach in RLHF and inference-time scaling. In this work, we propose\nMultimodal Reasoner as a Judge (MR. Judge), a paradigm for empowering\ngeneral-purpose MLLMs judges with strong reasoning capabilities. Instead of\ndirectly assigning scores for each response, we formulate the judgement process\nas a reasoning-inspired multiple-choice problem. Specifically, the judge model\nfirst conducts deliberate reasoning covering different aspects of the responses\nand eventually selects the best response from them. This reasoning process not\nonly improves the interpretibility of the judgement, but also greatly enhances\nthe performance of MLLM judges. To cope with the lack of questions with scored\nresponses, we propose the following strategy to achieve automatic annotation:\n1) Reverse Response Candidates Synthesis: starting from a supervised\nfine-tuning (SFT) dataset, we treat the original response as the best candidate\nand prompt the MLLM to generate plausible but flawed negative candidates. 2)\nText-based reasoning extraction: we carefully design a data synthesis pipeline\nfor distilling the reasoning capability from a text-based reasoning model,\nwhich is adopted to enable the MLLM judges to regain complex reasoning ability\nvia warm up supervised fine-tuning. Experiments demonstrate that our MR. Judge\nis effective across a wide range of tasks. Specifically, our MR. Judge-7B\nsurpasses GPT-4o by 9.9% on VL-RewardBench, and improves performance on MM-Vet\nduring inference-time scaling by up to 7.7%."}
{"id": "2505.13344", "pdf": "https://arxiv.org/pdf/2505.13344", "abs": "https://arxiv.org/abs/2505.13344", "authors": ["Ahmet Berke Gokmen", "Yigit Ekin", "Bahri Batuhan Bilecen", "Aysegul Dundar"], "title": "RoPECraft: Training-Free Motion Transfer with Trajectory-Guided RoPE Optimization on Diffusion Transformers", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "https://berkegokmen1.github.io/RoPECraft/", "summary": "We propose RoPECraft, a training-free video motion transfer method for\ndiffusion transformers that operates solely by modifying their rotary\npositional embeddings (RoPE). We first extract dense optical flow from a\nreference video, and utilize the resulting motion offsets to warp the\ncomplex-exponential tensors of RoPE, effectively encoding motion into the\ngeneration process. These embeddings are then further optimized during\ndenoising time steps via trajectory alignment between the predicted and target\nvelocities using a flow-matching objective. To keep the output faithful to the\ntext prompt and prevent duplicate generations, we incorporate a regularization\nterm based on the phase components of the reference video's Fourier transform,\nprojecting the phase angles onto a smooth manifold to suppress high-frequency\nartifacts. Experiments on benchmarks reveal that RoPECraft outperforms all\nrecently published methods, both qualitatively and quantitatively."}
{"id": "2505.13404", "pdf": "https://arxiv.org/pdf/2505.13404", "abs": "https://arxiv.org/abs/2505.13404", "authors": ["Nithin Rao Koluguri", "Monica Sekoyan", "George Zelenfroynd", "Sasha Meister", "Shuoyang Ding", "Sofia Kostandian", "He Huang", "Nikolay Karpov", "Jagadeesh Balam", "Vitaly Lavrukhin", "Yifan Peng", "Sara Papi", "Marco Gaido", "Alessio Brutti", "Boris Ginsburg"], "title": "Granary: Speech Recognition and Translation Dataset in 25 European Languages", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Multi-task and multilingual approaches benefit large models, yet speech\nprocessing for low-resource languages remains underexplored due to data\nscarcity. To address this, we present Granary, a large-scale collection of\nspeech datasets for recognition and translation across 25 European languages.\nThis is the first open-source effort at this scale for both transcription and\ntranslation. We enhance data quality using a pseudo-labeling pipeline with\nsegmentation, two-pass inference, hallucination filtering, and punctuation\nrestoration. We further generate translation pairs from pseudo-labeled\ntranscriptions using EuroLLM, followed by a data filtration pipeline. Designed\nfor efficiency, our pipeline processes vast amount of data within hours. We\nassess models trained on processed data by comparing their performance on\npreviously curated datasets for both high- and low-resource languages. Our\nfindings show that these models achieve similar performance using approx. 50%\nless data. Dataset will be made available at\nhttps://hf.co/datasets/nvidia/Granary"}
{"id": "2505.13389", "pdf": "https://arxiv.org/pdf/2505.13389", "abs": "https://arxiv.org/abs/2505.13389", "authors": ["Peiyuan Zhang", "Haofeng Huang", "Yongqi Chen", "Will Lin", "Zhengzhong Liu", "Ion Stoica", "Eric P. Xing", "Hao Zhang"], "title": "Faster Video Diffusion with Trainable Sparse Attention", "categories": ["cs.CV"], "comment": null, "summary": "Scaling video diffusion transformers (DiTs) is limited by their quadratic 3D\nattention, even though most of the attention mass concentrates on a small\nsubset of positions. We turn this observation into VSA, a trainable,\nhardware-efficient sparse attention that replaces full attention at \\emph{both}\ntraining and inference. In VSA, a lightweight coarse stage pools tokens into\ntiles and identifies high-weight \\emph{critical tokens}; a fine stage computes\ntoken-level attention only inside those tiles subjecting to block computing\nlayout to ensure hard efficiency. This leads to a single differentiable kernel\nthat trains end-to-end, requires no post-hoc profiling, and sustains 85\\% of\nFlashAttention3 MFU. We perform a large sweep of ablation studies and\nscaling-law experiments by pretraining DiTs from 60M to 1.4B parameters. VSA\nreaches a Pareto point that cuts training FLOPS by 2.53$\\times$ with no drop in\ndiffusion loss. Retrofitting the open-source Wan-2.1 model speeds up attention\ntime by 6$\\times$ and lowers end-to-end generation time from 31s to 18s with\ncomparable quality. These results establish trainable sparse attention as a\npractical alternative to full attention and a key enabler for further scaling\nof video diffusion models."}
{"id": "2505.13417", "pdf": "https://arxiv.org/pdf/2505.13417", "abs": "https://arxiv.org/abs/2505.13417", "authors": ["Jiajie Zhang", "Nianyi Lin", "Lei Hou", "Ling Feng", "Juanzi Li"], "title": "AdaptThink: Reasoning Models Can Learn When to Think", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recently, large reasoning models have achieved impressive performance on\nvarious tasks by employing human-like deep thinking. However, the lengthy\nthinking process substantially increases inference overhead, making efficiency\na critical bottleneck. In this work, we first demonstrate that NoThinking,\nwhich prompts the reasoning model to skip thinking and directly generate the\nfinal solution, is a better choice for relatively simple tasks in terms of both\nperformance and efficiency. Motivated by this, we propose AdaptThink, a novel\nRL algorithm to teach reasoning models to choose the optimal thinking mode\nadaptively based on problem difficulty. Specifically, AdaptThink features two\ncore components: (1) a constrained optimization objective that encourages the\nmodel to choose NoThinking while maintaining the overall performance; (2) an\nimportance sampling strategy that balances Thinking and NoThinking samples\nduring on-policy training, thereby enabling cold start and allowing the model\nto explore and exploit both thinking modes throughout the training process. Our\nexperiments indicate that AdaptThink significantly reduces the inference costs\nwhile further enhancing performance. Notably, on three math datasets,\nAdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B\nby 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive\nthinking-mode selection for optimizing the balance between reasoning quality\nand efficiency. Our codes and models are available at\nhttps://github.com/THU-KEG/AdaptThink."}
{"id": "2505.13419", "pdf": "https://arxiv.org/pdf/2505.13419", "abs": "https://arxiv.org/abs/2505.13419", "authors": ["Zhuozhao Hu", "Kaishen Yuan", "Xin Liu", "Zitong Yu", "Yuan Zong", "Jingang Shi", "Huanjing Yue", "Jingyu Yang"], "title": "FEALLM: Advancing Facial Emotion Analysis in Multimodal Large Language Models with Emotional Synergy and Reasoning", "categories": ["cs.CV"], "comment": "10 pages, 7 figures", "summary": "Facial Emotion Analysis (FEA) plays a crucial role in visual affective\ncomputing, aiming to infer a person's emotional state based on facial data.\nScientifically, facial expressions (FEs) result from the coordinated movement\nof facial muscles, which can be decomposed into specific action units (AUs)\nthat provide detailed emotional insights. However, traditional methods often\nstruggle with limited interpretability, constrained generalization and\nreasoning abilities. Recently, Multimodal Large Language Models (MLLMs) have\nshown exceptional performance in various visual tasks, while they still face\nsignificant challenges in FEA due to the lack of specialized datasets and their\ninability to capture the intricate relationships between FEs and AUs. To\naddress these issues, we introduce a novel FEA Instruction Dataset that\nprovides accurate and aligned FE and AU descriptions and establishes causal\nreasoning relationships between them, followed by constructing a new benchmark,\nFEABench. Moreover, we propose FEALLM, a novel MLLM architecture designed to\ncapture more detailed facial information, enhancing its capability in FEA\ntasks. Our model demonstrates strong performance on FEABench and impressive\ngeneralization capability through zero-shot evaluation on various datasets,\nincluding RAF-DB, AffectNet, BP4D, and DISFA, showcasing its robustness and\neffectiveness in FEA tasks. The dataset and code will be available at\nhttps://github.com/953206211/FEALLM."}
{"id": "2505.13418", "pdf": "https://arxiv.org/pdf/2505.13418", "abs": "https://arxiv.org/abs/2505.13418", "authors": ["Lotem Peled-Cohen", "Maya Zadok", "Nitay Calderon", "Hila Gonen", "Roi Reichart"], "title": "Dementia Through Different Eyes: Explainable Modeling of Human and LLM Perceptions for Early Awareness", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Cognitive decline often surfaces in language years before diagnosis. It is\nfrequently non-experts, such as those closest to the patient, who first sense a\nchange and raise concern. As LLMs become integrated into daily communication\nand used over prolonged periods, it may even be an LLM that notices something\nis off. But what exactly do they notice--and should be noticing--when making\nthat judgment? This paper investigates how dementia is perceived through\nlanguage by non-experts. We presented transcribed picture descriptions to\nnon-expert humans and LLMs, asking them to intuitively judge whether each text\nwas produced by someone healthy or with dementia. We introduce an explainable\nmethod that uses LLMs to extract high-level, expert-guided features\nrepresenting these picture descriptions, and use logistic regression to model\nhuman and LLM perceptions and compare with clinical diagnoses. Our analysis\nreveals that human perception of dementia is inconsistent and relies on a\nnarrow, and sometimes misleading, set of cues. LLMs, by contrast, draw on a\nricher, more nuanced feature set that aligns more closely with clinical\npatterns. Still, both groups show a tendency toward false negatives, frequently\noverlooking dementia cases. Through our interpretable framework and the\ninsights it provides, we hope to help non-experts better recognize the\nlinguistic signs that matter."}
{"id": "2505.13426", "pdf": "https://arxiv.org/pdf/2505.13426", "abs": "https://arxiv.org/abs/2505.13426", "authors": ["Liang Chen", "Hongcheng Gao", "Tianyu Liu", "Zhiqi Huang", "Flood Sung", "Xinyu Zhou", "Yuxin Wu", "Baobao Chang"], "title": "G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language Model via Reinforcement Learning", "categories": ["cs.CV"], "comment": "21 pages, 14 figures, code released at\n  https://github.com/chenllliang/G1", "summary": "Vision-Language Models (VLMs) excel in many direct multimodal tasks but\nstruggle to translate this prowess into effective decision-making within\ninteractive, visually rich environments like games. This ``knowing-doing'' gap\nsignificantly limits their potential as autonomous agents, as leading VLMs\noften performing badly in simple games. To address this, we introduce VLM-Gym,\na curated reinforcement learning (RL) environment featuring diverse visual\ngames with unified interfaces and adjustable, compositional difficulty,\nspecifically designed for scalable multi-game parallel training. Leveraging\nVLM-Gym, we train G0 models using pure RL-driven self-evolution, which\ndemonstrate emergent perception and reasoning patterns. To further mitigate\nchallenges arising from game diversity, we develop G1 models. G1 incorporates a\nperception-enhanced cold start prior to RL fine-tuning. Our resulting G1 models\nconsistently surpass their teacher across all games and outperform leading\nproprietary models like Claude-3.7-Sonnet-Thinking. Systematic analysis reveals\nan intriguing finding: perception and reasoning abilities mutually bootstrap\neach other throughout the RL training process. Source code including VLM-Gym\nand RL training are released at https://github.com/chenllliang/G1 to foster\nfuture research in advancing VLMs as capable interactive agents."}
{"id": "2505.13434", "pdf": "https://arxiv.org/pdf/2505.13434", "abs": "https://arxiv.org/abs/2505.13434", "authors": ["Mateusz Bystroński", "Mikołaj Hołysz", "Grzegorz Piotrowski", "Nitesh V. Chawla", "Tomasz Kajdanowicz"], "title": "SMOTExT: SMOTE meets Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Data scarcity and class imbalance are persistent challenges in training\nrobust NLP models, especially in specialized domains or low-resource settings.\nWe propose a novel technique, SMOTExT, that adapts the idea of Synthetic\nMinority Over-sampling (SMOTE) to textual data. Our method generates new\nsynthetic examples by interpolating between BERT-based embeddings of two\nexisting examples and then decoding the resulting latent point into text with\nxRAG architecture. By leveraging xRAG's cross-modal retrieval-generation\nframework, we can effectively turn interpolated vectors into coherent text.\nWhile this is preliminary work supported by qualitative outputs only, the\nmethod shows strong potential for knowledge distillation and data augmentation\nin few-shot settings. Notably, our approach also shows promise for\nprivacy-preserving machine learning: in early experiments, training models\nsolely on generated data achieved comparable performance to models trained on\nthe original dataset. This suggests a viable path toward safe and effective\nlearning under data protection constraints."}
{"id": "2505.13429", "pdf": "https://arxiv.org/pdf/2505.13429", "abs": "https://arxiv.org/abs/2505.13429", "authors": ["Cristobal Eyzaguirre", "Igor Vasiljevic", "Achal Dave", "Jiajun Wu", "Rares Andrei Ambrus", "Thomas Kollar", "Juan Carlos Niebles", "Pavel Tokmakov"], "title": "Understanding Complexity in VideoQA via Visual Program Generation", "categories": ["cs.CV"], "comment": null, "summary": "We propose a data-driven approach to analyzing query complexity in Video\nQuestion Answering (VideoQA). Previous efforts in benchmark design have relied\non human expertise to design challenging questions, yet we experimentally show\nthat humans struggle to predict which questions are difficult for machine\nlearning models. Our automatic approach leverages recent advances in code\ngeneration for visual question answering, using the complexity of generated\ncode as a proxy for question difficulty. We demonstrate that this measure\ncorrelates significantly better with model performance than human estimates. To\noperationalize this insight, we propose an algorithm for estimating question\ncomplexity from code. It identifies fine-grained primitives that correlate with\nthe hardest questions for any given set of models, making it easy to scale to\nnew approaches in the future. Finally, to further illustrate the utility of our\nmethod, we extend it to automatically generate complex questions, constructing\na new benchmark that is 1.9 times harder than the popular NExT-QA."}
{"id": "2505.13444", "pdf": "https://arxiv.org/pdf/2505.13444", "abs": "https://arxiv.org/abs/2505.13444", "authors": ["Liyan Tang", "Grace Kim", "Xinyu Zhao", "Thom Lake", "Wenxuan Ding", "Fangcong Yin", "Prasann Singhal", "Manya Wadhwa", "Zeyu Leo Liu", "Zayne Sprague", "Ramya Namuduri", "Bodun Hu", "Juan Diego Rodriguez", "Puyuan Peng", "Greg Durrett"], "title": "ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Chart understanding presents a unique challenge for large vision-language\nmodels (LVLMs), as it requires the integration of sophisticated textual and\nvisual reasoning capabilities. However, current LVLMs exhibit a notable\nimbalance between these skills, falling short on visual reasoning that is\ndifficult to perform in text. We conduct a case study using a synthetic dataset\nsolvable only through visual reasoning and show that model performance degrades\nsignificantly with increasing visual complexity, while human performance\nremains robust. We then introduce ChartMuseum, a new Chart Question Answering\n(QA) benchmark containing 1,162 expert-annotated questions spanning multiple\nreasoning types, curated from real-world charts across 184 sources,\nspecifically built to evaluate complex visual and textual reasoning. Unlike\nprior chart understanding benchmarks -- where frontier models perform similarly\nand near saturation -- our benchmark exposes a substantial gap between model\nand human performance, while effectively differentiating model capabilities:\nalthough humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro\nattains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct\nachieves only 38.5%. Moreover, on questions requiring primarily visual\nreasoning, all models experience a 35%-55% performance drop from\ntext-reasoning-heavy question performance. Lastly, our qualitative error\nanalysis reveals specific categories of visual reasoning that are challenging\nfor current LVLMs."}
{"id": "2505.13436", "pdf": "https://arxiv.org/pdf/2505.13436", "abs": "https://arxiv.org/abs/2505.13436", "authors": ["R. James Cotton"], "title": "KinTwin: Imitation Learning with Torque and Muscle Driven Biomechanical Models Enables Precise Replication of Able-Bodied and Impaired Movement from Markerless Motion Capture", "categories": ["cs.CV"], "comment": null, "summary": "Broader access to high-quality movement analysis could greatly benefit\nmovement science and rehabilitation, such as allowing more detailed\ncharacterization of movement impairments and responses to interventions, or\neven enabling early detection of new neurological conditions or fall risk.\nWhile emerging technologies are making it easier to capture kinematics with\nbiomechanical models, or how joint angles change over time, inferring the\nunderlying physics that give rise to these movements, including ground reaction\nforces, joint torques, or even muscle activations, is still challenging. Here\nwe explore whether imitation learning applied to a biomechanical model from a\nlarge dataset of movements from able-bodied and impaired individuals can learn\nto compute these inverse dynamics. Although imitation learning in human pose\nestimation has seen great interest in recent years, our work differences in\nseveral ways: we focus on using an accurate biomechanical model instead of\nmodels adopted for computer vision, we test it on a dataset that contains\nparticipants with impaired movements, we reported detailed tracking metrics\nrelevant for the clinical measurement of movement including joint angles and\nground contact events, and finally we apply imitation learning to a\nmuscle-driven neuromusculoskeletal model. We show that our imitation learning\npolicy, KinTwin, can accurately replicate the kinematics of a wide range of\nmovements, including those with assistive devices or therapist assistance, and\nthat it can infer clinically meaningful differences in joint torques and muscle\nactivations. Our work demonstrates the potential for using imitation learning\nto enable high-quality movement analysis in clinical practice."}
{"id": "2505.13448", "pdf": "https://arxiv.org/pdf/2505.13448", "abs": "https://arxiv.org/abs/2505.13448", "authors": ["Vinay Samuel", "Harshita Diddee", "Yiming Zhang", "Daphne Ippolito"], "title": "CIE: Controlling Language Model Text Generations Using Continuous Signals", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 3 figures", "summary": "Aligning language models with user intent is becoming increasingly relevant\nto enhance user experience. This calls for designing methods that can allow\nusers to control the properties of the language that LMs generate. For example,\ncontrolling the length of the generation, the complexity of the language that\ngets chosen, the sentiment, tone, etc. Most existing work attempts to integrate\nusers' control by conditioning LM generations on natural language prompts or\ndiscrete control signals, which are often brittle and hard to scale. In this\nwork, we are interested in \\textit{continuous} control signals, ones that exist\nalong a spectrum that can't easily be captured in a natural language prompt or\nvia existing techniques in conditional generation. Through a case study in\ncontrolling the precise response-length of generations produced by LMs, we\ndemonstrate how after fine-tuning, behaviors of language models can be\ncontrolled via continuous signals -- as vectors that are interpolated between a\n\"low\" and a \"high\" token embedding. Our method more reliably exerts\nresponse-length control than in-context learning methods or fine-tuning methods\nthat represent the control signal as a discrete signal. Our full open-sourced\ncode and datasets are available at https://github.com/vsamuel2003/CIE."}
{"id": "2505.13437", "pdf": "https://arxiv.org/pdf/2505.13437", "abs": "https://arxiv.org/abs/2505.13437", "authors": ["Dian Shao", "Mingfei Shi", "Shengda Xu", "Haodong Chen", "Yongle Huang", "Binglu Wang"], "title": "FinePhys: Fine-grained Human Action Generation by Explicitly Incorporating Physical Laws for Effective Skeletal Guidance", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR 2025", "summary": "Despite significant advances in video generation, synthesizing physically\nplausible human actions remains a persistent challenge, particularly in\nmodeling fine-grained semantics and complex temporal dynamics. For instance,\ngenerating gymnastics routines such as \"switch leap with 0.5 turn\" poses\nsubstantial difficulties for current methods, often yielding unsatisfactory\nresults. To bridge this gap, we propose FinePhys, a Fine-grained human action\ngeneration framework that incorporates Physics to obtain effective skeletal\nguidance. Specifically, FinePhys first estimates 2D poses in an online manner\nand then performs 2D-to-3D dimension lifting via in-context learning. To\nmitigate the instability and limited interpretability of purely data-driven 3D\nposes, we further introduce a physics-based motion re-estimation module\ngoverned by Euler-Lagrange equations, calculating joint accelerations via\nbidirectional temporal updating. The physically predicted 3D poses are then\nfused with data-driven ones, offering multi-scale 2D heatmap guidance for the\ndiffusion process. Evaluated on three fine-grained action subsets from FineGym\n(FX-JUMP, FX-TURN, and FX-SALTO), FinePhys significantly outperforms\ncompetitive baselines. Comprehensive qualitative results further demonstrate\nFinePhys's ability to generate more natural and plausible fine-grained human\nactions."}
{"id": "2505.11545", "pdf": "https://arxiv.org/pdf/2505.11545", "abs": "https://arxiv.org/abs/2505.11545", "authors": ["Xingyu Ji", "Parker Glenn", "Aditya G. Parameswaran", "Madelon Hulsebos"], "title": "TARGET: Benchmarking Table Retrieval for Generative Tasks", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.DB"], "comment": null, "summary": "The data landscape is rich with structured data, often of high value to\norganizations, driving important applications in data analysis and machine\nlearning. Recent progress in representation learning and generative models for\nsuch data has led to the development of natural language interfaces to\nstructured data, including those leveraging text-to-SQL. Contextualizing\ninteractions, either through conversational interfaces or agentic components,\nin structured data through retrieval-augmented generation can provide\nsubstantial benefits in the form of freshness, accuracy, and comprehensiveness\nof answers. The key question is: how do we retrieve the right table(s) for the\nanalytical query or task at hand? To this end, we introduce TARGET: a benchmark\nfor evaluating TAble Retrieval for GEnerative Tasks. With TARGET we analyze the\nretrieval performance of different retrievers in isolation, as well as their\nimpact on downstream tasks. We find that dense embedding-based retrievers far\noutperform a BM25 baseline which is less effective than it is for retrieval\nover unstructured text. We also surface the sensitivity of retrievers across\nvarious metadata (e.g., missing table titles), and demonstrate a stark\nvariation of retrieval performance across datasets and tasks. TARGET is\navailable at https://target-benchmark.github.io."}
{"id": "2505.13439", "pdf": "https://arxiv.org/pdf/2505.13439", "abs": "https://arxiv.org/abs/2505.13439", "authors": ["Huawei Lin", "Tong Geng", "Zhaozhuo Xu", "Weijie Zhao"], "title": "VTBench: Evaluating Visual Tokenizers for Autoregressive Image Generation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "24 pages, 13 figures, 3 tables", "summary": "Autoregressive (AR) models have recently shown strong performance in image\ngeneration, where a critical component is the visual tokenizer (VT) that maps\ncontinuous pixel inputs to discrete token sequences. The quality of the VT\nlargely defines the upper bound of AR model performance. However, current\ndiscrete VTs fall significantly behind continuous variational autoencoders\n(VAEs), leading to degraded image reconstructions and poor preservation of\ndetails and text. Existing benchmarks focus on end-to-end generation quality,\nwithout isolating VT performance. To address this gap, we introduce VTBench, a\ncomprehensive benchmark that systematically evaluates VTs across three core\ntasks: Image Reconstruction, Detail Preservation, and Text Preservation, and\ncovers a diverse range of evaluation scenarios. We systematically assess\nstate-of-the-art VTs using a set of metrics to evaluate the quality of\nreconstructed images. Our findings reveal that continuous VAEs produce superior\nvisual representations compared to discrete VTs, particularly in retaining\nspatial structure and semantic detail. In contrast, the degraded\nrepresentations produced by discrete VTs often lead to distorted\nreconstructions, loss of fine-grained textures, and failures in preserving text\nand object integrity. Furthermore, we conduct experiments on GPT-4o image\ngeneration and discuss its potential AR nature, offering new insights into the\nrole of visual tokenization. We release our benchmark and codebase publicly to\nsupport further research and call on the community to develop strong,\ngeneral-purpose open-source VTs."}
{"id": "2505.11572", "pdf": "https://arxiv.org/pdf/2505.11572", "abs": "https://arxiv.org/abs/2505.11572", "authors": ["Anand Rai", "Satyam Rahangdale", "Utkarsh Anand", "Animesh Mukherjee"], "title": "ASR-FAIRBENCH: Measuring and Benchmarking Equity Across Speech Recognition Systems", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Paper accepted at INTERSPEECH 2025", "summary": "Automatic Speech Recognition (ASR) systems have become ubiquitous in everyday\napplications, yet significant disparities in performance across diverse\ndemographic groups persist. In this work, we introduce the ASR-FAIRBENCH\nleaderboard which is designed to assess both the accuracy and equity of ASR\nmodels in real-time. Leveraging the Meta's Fair-Speech dataset, which captures\ndiverse demographic characteristics, we employ a mixed-effects Poisson\nregression model to derive an overall fairness score. This score is integrated\nwith traditional metrics like Word Error Rate (WER) to compute the Fairness\nAdjusted ASR Score (FAAS), providing a comprehensive evaluation framework. Our\napproach reveals significant performance disparities in SOTA ASR models across\ndemographic groups and offers a benchmark to drive the development of more\ninclusive ASR technologies."}
{"id": "2505.13440", "pdf": "https://arxiv.org/pdf/2505.13440", "abs": "https://arxiv.org/abs/2505.13440", "authors": ["Ruoyu Wang", "Yi Ma", "Shenghua Gao"], "title": "Recollection from Pensieve: Novel View Synthesis via Learning from Uncalibrated Videos", "categories": ["cs.CV"], "comment": "13 pages, 4 figures", "summary": "Currently almost all state-of-the-art novel view synthesis and reconstruction\nmodels rely on calibrated cameras or additional geometric priors for training.\nThese prerequisites significantly limit their applicability to massive\nuncalibrated data. To alleviate this requirement and unlock the potential for\nself-supervised training on large-scale uncalibrated videos, we propose a novel\ntwo-stage strategy to train a view synthesis model from only raw video frames\nor multi-view images, without providing camera parameters or other priors. In\nthe first stage, we learn to reconstruct the scene implicitly in a latent space\nwithout relying on any explicit 3D representation. Specifically, we predict\nper-frame latent camera and scene context features, and employ a view synthesis\nmodel as a proxy for explicit rendering. This pretraining stage substantially\nreduces the optimization complexity and encourages the network to learn the\nunderlying 3D consistency in a self-supervised manner. The learned latent\ncamera and implicit scene representation have a large gap compared with the\nreal 3D world. To reduce this gap, we introduce the second stage training by\nexplicitly predicting 3D Gaussian primitives. We additionally apply explicit\nGaussian Splatting rendering loss and depth projection loss to align the\nlearned latent representations with physically grounded 3D geometry. In this\nway, Stage 1 provides a strong initialization and Stage 2 enforces 3D\nconsistency - the two stages are complementary and mutually beneficial.\nExtensive experiments demonstrate the effectiveness of our approach, achieving\nhigh-quality novel view synthesis and accurate camera pose estimation, compared\nto methods that employ supervision with calibration, pose, or depth\ninformation. The code is available at https://github.com/Dwawayu/Pensieve."}
{"id": "2505.11595", "pdf": "https://arxiv.org/pdf/2505.11595", "abs": "https://arxiv.org/abs/2505.11595", "authors": ["Peter Chen", "Xiaopeng Li", "Ziniu Li", "Xi Chen", "Tianyi Lin"], "title": "Spectral Policy Optimization: Coloring your Incorrect Reasoning in GRPO", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "28 pages", "summary": "Reinforcement learning (RL) has demonstrated significant success in enhancing\nreasoning capabilities in large language models (LLMs). One of the most widely\nused RL methods is Group Relative Policy Optimization\n(GRPO)~\\cite{Shao-2024-Deepseekmath}, known for its memory efficiency and\nsuccess in training DeepSeek-R1~\\cite{Guo-2025-Deepseek}. However, GRPO stalls\nwhen all sampled responses in a group are incorrect -- referred to as an\n\\emph{all-negative-sample} group -- as it fails to update the policy, hindering\nlearning progress. The contributions of this paper are two-fold. First, we\npropose a simple yet effective framework that introduces response diversity\nwithin all-negative-sample groups in GRPO using AI feedback. We also provide a\ntheoretical analysis, via a stylized model, showing how this diversification\nimproves learning dynamics. Second, we empirically validate our approach,\nshowing the improved performance across various model sizes (7B, 14B, 32B) in\nboth offline and online learning settings with 10 benchmarks, including base\nand distilled variants. Our findings highlight that learning from\nall-negative-sample groups is not only feasible but beneficial, advancing\nrecent insights from \\citet{Xiong-2025-Minimalist}."}
{"id": "2505.11518", "pdf": "https://arxiv.org/pdf/2505.11518", "abs": "https://arxiv.org/abs/2505.11518", "authors": ["Merham Fouladvand", "Peuroly Batra"], "title": "Deep Unrolled Meta-Learning for Multi-Coil and Multi-Modality MRI with Adaptive Optimization", "categories": ["math.OC", "cs.CV"], "comment": null, "summary": "We propose a unified deep meta-learning framework for accelerated magnetic\nresonance imaging (MRI) that jointly addresses multi-coil reconstruction and\ncross-modality synthesis. Motivated by the limitations of conventional methods\nin handling undersampled data and missing modalities, our approach unrolls a\nprovably convergent optimization algorithm into a structured neural network\narchitecture. Each phase of the network mimics a step of an adaptive\nforward-backward scheme with extrapolation, enabling the model to incorporate\nboth data fidelity and nonconvex regularization in a principled manner. To\nenhance generalization across different acquisition settings, we integrate\nmeta-learning, which enables the model to rapidly adapt to unseen sampling\npatterns and modality combinations using task-specific meta-knowledge. The\nproposed method is evaluated on the open source datasets, showing significant\nimprovements in PSNR and SSIM over conventional supervised learning, especially\nunder aggressive undersampling and domain shifts. Our results demonstrate the\nsynergy of unrolled optimization, task-aware meta-learning, and modality\nfusion, offering a scalable and generalizable solution for real-world clinical\nMRI reconstruction."}
{"id": "2505.11611", "pdf": "https://arxiv.org/pdf/2505.11611", "abs": "https://arxiv.org/abs/2505.11611", "authors": ["Bofan Gong", "Shiyang Lai", "Dawn Song"], "title": "Probing the Vulnerability of Large Language Models to Polysemantic Interventions", "categories": ["cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "Polysemanticity -- where individual neurons encode multiple unrelated\nfeatures -- is a well-known characteristic of large neural networks and remains\na central challenge in the interpretability of language models. At the same\ntime, its implications for model safety are also poorly understood. Leveraging\nrecent advances in sparse autoencoders, we investigate the polysemantic\nstructure of two small models (Pythia-70M and GPT-2-Small) and evaluate their\nvulnerability to targeted, covert interventions at the prompt, feature, token,\nand neuron levels. Our analysis reveals a consistent polysemantic topology\nshared across both models. Strikingly, we demonstrate that this structure can\nbe exploited to mount effective interventions on two larger, black-box\ninstruction-tuned models (LLaMA3.1-8B-Instruct and Gemma-2-9B-Instruct). These\nfindings suggest not only the generalizability of the interventions but also\npoint to a stable and transferable polysemantic structure that could\npotentially persist across architectures and training regimes."}
{"id": "2505.11535", "pdf": "https://arxiv.org/pdf/2505.11535", "abs": "https://arxiv.org/abs/2505.11535", "authors": ["Yuhang Wang", "Hao Zhou"], "title": "Bridging Human Oversight and Black-box Driver Assistance: Vision-Language Models for Predictive Alerting in Lane Keeping Assist Systems", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": null, "summary": "Lane Keeping Assist systems, while increasingly prevalent, often suffer from\nunpredictable real-world failures, largely due to their opaque, black-box\nnature, which limits driver anticipation and trust. To bridge the gap between\nautomated assistance and effective human oversight, we present LKAlert, a novel\nsupervisory alert system that leverages VLM to forecast potential LKA risk 1-3\nseconds in advance. LKAlert processes dash-cam video and CAN data, integrating\nsurrogate lane segmentation features from a parallel interpretable model as\nautomated guiding attention. Unlike traditional binary classifiers, LKAlert\nissues both predictive alert and concise natural language explanation,\nenhancing driver situational awareness and trust. To support the development\nand evaluation of such systems, we introduce OpenLKA-Alert, the first benchmark\ndataset designed for predictive and explainable LKA failure warnings. It\ncontains synchronized multimodal inputs and human-authored justifications\nacross annotated temporal windows. We further contribute a generalizable\nmethodological framework for VLM-based black-box behavior prediction, combining\nsurrogate feature guidance with LoRA. This framework enables VLM to reason over\nstructured visual context without altering its vision backbone, making it\nbroadly applicable to other complex, opaque systems requiring interpretable\noversight. Empirical results correctly predicts upcoming LKA failures with\n69.8% accuracy and a 58.6\\% F1-score. The system also generates high-quality\ntextual explanations for drivers (71.7 ROUGE-L) and operates efficiently at\napproximately 2 Hz, confirming its suitability for real-time, in-vehicle use.\nOur findings establish LKAlert as a practical solution for enhancing the safety\nand usability of current ADAS and offer a scalable paradigm for applying VLMs\nto human-centered supervision of black-box automation."}
{"id": "2505.11614", "pdf": "https://arxiv.org/pdf/2505.11614", "abs": "https://arxiv.org/abs/2505.11614", "authors": ["Jian-Qiao Zhu", "Hanbo Xie", "Dilip Arumugam", "Robert C. Wilson", "Thomas L. Griffiths"], "title": "Using Reinforcement Learning to Train Large Language Models to Explain Human Decisions", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "A central goal of cognitive modeling is to develop models that not only\npredict human behavior but also provide insight into the underlying cognitive\nmechanisms. While neural network models trained on large-scale behavioral data\noften achieve strong predictive performance, they typically fall short in\noffering interpretable explanations of the cognitive processes they capture. In\nthis work, we explore the potential of pretrained large language models (LLMs)\nto serve as dual-purpose cognitive models--capable of both accurate prediction\nand interpretable explanation in natural language. Specifically, we employ\nreinforcement learning with outcome-based rewards to guide LLMs toward\ngenerating explicit reasoning traces for explaining human risky choices. Our\nfindings demonstrate that this approach produces high-quality explanations\nalongside strong quantitative predictions of human decisions."}
{"id": "2505.11538", "pdf": "https://arxiv.org/pdf/2505.11538", "abs": "https://arxiv.org/abs/2505.11538", "authors": ["Jiacheng Hou", "Zhenjie Song", "Ercan Engin Kuruoglu"], "title": "BrainNetMLP: An Efficient and Effective Baseline for Functional Brain Network Classification", "categories": ["q-bio.NC", "cs.CV"], "comment": "V1.0", "summary": "Recent studies have made great progress in functional brain network\nclassification by modeling the brain as a network of Regions of Interest (ROIs)\nand leveraging their connections to understand brain functionality and diagnose\nmental disorders. Various deep learning architectures, including Convolutional\nNeural Networks, Graph Neural Networks, and the recent Transformer, have been\ndeveloped. However, despite the increasing complexity of these models, the\nperformance gain has not been as salient. This raises a question: Does\nincreasing model complexity necessarily lead to higher classification accuracy?\nIn this paper, we revisit the simplest deep learning architecture, the\nMulti-Layer Perceptron (MLP), and propose a pure MLP-based method, named\nBrainNetMLP, for functional brain network classification, which capitalizes on\nthe advantages of MLP, including efficient computation and fewer parameters.\nMoreover, BrainNetMLP incorporates a dual-branch structure to jointly capture\nboth spatial connectivity and spectral information, enabling precise\nspatiotemporal feature fusion. We evaluate our proposed BrainNetMLP on two\npublic and popular brain network classification datasets, the Human Connectome\nProject (HCP) and the Autism Brain Imaging Data Exchange (ABIDE). Experimental\nresults demonstrate pure MLP-based methods can achieve state-of-the-art\nperformance, revealing the potential of MLP-based models as more efficient yet\neffective alternatives in functional brain network classification. The code\nwill be available at https://github.com/JayceonHo/BrainNetMLP."}
{"id": "2505.11717", "pdf": "https://arxiv.org/pdf/2505.11717", "abs": "https://arxiv.org/abs/2505.11717", "authors": ["Xilong Wang", "John Bloch", "Zedian Shao", "Yuepeng Hu", "Shuyan Zhou", "Neil Zhenqiang Gong"], "title": "EnvInjection: Environmental Prompt Injection Attack to Multi-modal Web Agents", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Multi-modal large language model (MLLM)-based web agents interact with\nwebpage environments by generating actions based on screenshots of the\nwebpages. Environmental prompt injection attacks manipulate the environment to\ninduce the web agent to perform a specific, attacker-chosen action--referred to\nas the target action. However, existing attacks suffer from limited\neffectiveness or stealthiness, or are impractical in real-world settings. In\nthis work, we propose EnvInjection, a new attack that addresses these\nlimitations. Our attack adds a perturbation to the raw pixel values of the\nrendered webpage, which can be implemented by modifying the webpage's source\ncode. After these perturbed pixels are mapped into a screenshot, the\nperturbation induces the web agent to perform the target action. We formulate\nthe task of finding the perturbation as an optimization problem. A key\nchallenge in solving this problem is that the mapping between raw pixel values\nand screenshot is non-differentiable, making it difficult to backpropagate\ngradients to the perturbation. To overcome this, we train a neural network to\napproximate the mapping and apply projected gradient descent to solve the\nreformulated optimization problem. Extensive evaluation on multiple webpage\ndatasets shows that EnvInjection is highly effective and significantly\noutperforms existing baselines."}
{"id": "2505.11576", "pdf": "https://arxiv.org/pdf/2505.11576", "abs": "https://arxiv.org/abs/2505.11576", "authors": ["Shuchen Wu", "Stephan Alaniz", "Shyamgopal Karthik", "Peter Dayan", "Eric Schulz", "Zeynep Akata"], "title": "Concept-Guided Interpretability via Neural Chunking", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "35 pages, 32 figures. arXiv admin note: text overlap with\n  arXiv:2502.01803", "summary": "Neural networks are often black boxes, reflecting the significant challenge\nof understanding their internal workings. We propose a different perspective\nthat challenges the prevailing view: rather than being inscrutable, neural\nnetworks exhibit patterns in their raw population activity that mirror\nregularities in the training data. We refer to this as the Reflection\nHypothesis and provide evidence for this phenomenon in both simple recurrent\nneural networks (RNNs) and complex large language models (LLMs). Building on\nthis insight, we propose to leverage cognitively-inspired methods of chunking\nto segment high-dimensional neural population dynamics into interpretable units\nthat reflect underlying concepts. We propose three methods to extract these\nemerging entities, complementing each other based on label availability and\ndimensionality. Discrete sequence chunking (DSC) creates a dictionary of\nentities; population averaging (PA) extracts recurring entities that correspond\nto known labels; and unsupervised chunk discovery (UCD) can be used when labels\nare absent. We demonstrate the effectiveness of these methods in extracting\nentities across varying model sizes, ranging from inducing compositionality in\nRNNs to uncovering recurring neural population states in large models with\ndiverse architectures, and illustrate their advantage over other methods.\nThroughout, we observe a robust correspondence between the extracted entities\nand concrete or abstract concepts. Artificially inducing the extracted entities\nin neural populations effectively alters the network's generation of associated\nconcepts. Our work points to a new direction for interpretability, one that\nharnesses both cognitive principles and the structure of naturalistic data to\nreveal the hidden computations of complex learning systems, gradually\ntransforming them from black boxes into systems we can begin to understand."}
{"id": "2505.11731", "pdf": "https://arxiv.org/pdf/2505.11731", "abs": "https://arxiv.org/abs/2505.11731", "authors": ["Harshil Vejendla", "Haizhou Shi", "Yibin Wang", "Tunyu Zhang", "Huan Zhang", "Hao Wang"], "title": "Efficient Uncertainty Estimation via Distillation of Bayesian Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Preprint; work in progress", "summary": "Recent advances in uncertainty estimation for Large Language Models (LLMs)\nduring downstream adaptation have addressed key challenges of reliability and\nsimplicity. However, existing Bayesian methods typically require multiple\nsampling iterations during inference, creating significant efficiency issues\nthat limit practical deployment. In this paper, we investigate the possibility\nof eliminating the need for test-time sampling for LLM uncertainty estimation.\nSpecifically, when given an off-the-shelf Bayesian LLM, we distill its aligned\nconfidence into a non-Bayesian student LLM by minimizing the divergence between\ntheir predictive distributions. Unlike typical calibration methods, our\ndistillation is carried out solely on the training dataset without the need of\nan additional validation dataset. This simple yet effective approach achieves\nN-times more efficient uncertainty estimation during testing, where N is the\nnumber of samples traditionally required by Bayesian LLMs. Our extensive\nexperiments demonstrate that uncertainty estimation capabilities on training\ndata can successfully generalize to unseen test data through our distillation\ntechnique, consistently producing results comparable to (or even better than)\nstate-of-the-art Bayesian LLMs."}
{"id": "2505.11594", "pdf": "https://arxiv.org/pdf/2505.11594", "abs": "https://arxiv.org/abs/2505.11594", "authors": ["Jintao Zhang", "Jia Wei", "Pengle Zhang", "Xiaoming Xu", "Haofeng Huang", "Haoxu Wang", "Kai Jiang", "Jun Zhu", "Jianfei Chen"], "title": "SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training", "categories": ["cs.LG", "cs.AI", "cs.AR", "cs.CV", "cs.PF"], "comment": null, "summary": "The efficiency of attention is important due to its quadratic time\ncomplexity. We enhance the efficiency of attention through two key\ncontributions: First, we leverage the new FP4 Tensor Cores in Blackwell GPUs to\naccelerate attention computation. Our implementation achieves 1038 TOPS on\nRTX5090, which is a 5x speedup over the fastest FlashAttention on RTX5090.\nExperiments show that our FP4 attention can accelerate inference of various\nmodels in a plug-and-play way. Second, we pioneer low-bit attention to training\ntasks. Existing low-bit attention works like FlashAttention3 and SageAttention\nfocus only on inference. However, the efficiency of training large models is\nalso important. To explore whether low-bit attention can be effectively applied\nto training tasks, we design an accurate and efficient 8-bit attention for both\nforward and backward propagation. Experiments indicate that 8-bit attention\nachieves lossless performance in fine-tuning tasks but exhibits slower\nconvergence in pretraining tasks. The code will be available at\nhttps://github.com/thu-ml/SageAttention."}
{"id": "2505.11737", "pdf": "https://arxiv.org/pdf/2505.11737", "abs": "https://arxiv.org/abs/2505.11737", "authors": ["Tunyu Zhang", "Haizhou Shi", "Yibin Wang", "Hengyi Wang", "Xiaoxiao He", "Zhuowei Li", "Haoxian Chen", "Ligong Han", "Kai Xu", "Huan Zhang", "Dimitris Metaxas", "Hao Wang"], "title": "Token-Level Uncertainty Estimation for Large Language Model Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Preprint; Work in progress", "summary": "While Large Language Models (LLMs) have demonstrated impressive capabilities,\ntheir output quality remains inconsistent across various application scenarios,\nmaking it difficult to identify trustworthy responses, especially in complex\ntasks requiring multi-step reasoning. In this paper, we propose a token-level\nuncertainty estimation framework to enable LLMs to self-assess and self-improve\ntheir generation quality in mathematical reasoning. Specifically, we introduce\nlow-rank random weight perturbation to LLM decoding, generating predictive\ndistributions that we use to estimate token-level uncertainties. We then\naggregate these uncertainties to reflect semantic uncertainty of the generated\nsequences. Experiments on mathematical reasoning datasets of varying difficulty\ndemonstrate that our token-level uncertainty metrics strongly correlate with\nanswer correctness and model robustness. Additionally, we explore using\nuncertainty to directly enhance the model's reasoning performance through\nmultiple generations and the particle filtering algorithm. Our approach\nconsistently outperforms existing uncertainty estimation methods, establishing\neffective uncertainty estimation as a valuable tool for both evaluating and\nimproving reasoning generation in LLMs."}
{"id": "2505.11645", "pdf": "https://arxiv.org/pdf/2505.11645", "abs": "https://arxiv.org/abs/2505.11645", "authors": ["Jinzhou Cao", "Xiangxu Wang", "Jiashi Chen", "Wei Tu", "Zhenhui Li", "Xindong Yang", "Tianhong Zhao", "Qingquan Li"], "title": "Urban Representation Learning for Fine-grained Economic Mapping: A Semi-supervised Graph-based Approach", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted for publication in International Society Journal of\n  Photogrammetry and Remote Sensing (ISPRS). 70 pages, 10 Figures, 15 Tables", "summary": "Fine-grained economic mapping through urban representation learning has\nemerged as a crucial tool for evidence-based economic decisions. While existing\nmethods primarily rely on supervised or unsupervised approaches, they often\noverlook semi-supervised learning in data-scarce scenarios and lack unified\nmulti-task frameworks for comprehensive sectoral economic analysis. To address\nthese gaps, we propose SemiGTX, an explainable semi-supervised graph learning\nframework for sectoral economic mapping. The framework is designed with\ndedicated fusion encoding modules for various geospatial data modalities,\nseamlessly integrating them into a cohesive graph structure. It introduces a\nsemi-information loss function that combines spatial self-supervision with\nlocally masked supervised regression, enabling more informative and effective\nregion representations. Through multi-task learning, SemiGTX concurrently maps\nGDP across primary, secondary, and tertiary sectors within a unified model.\nExtensive experiments conducted in the Pearl River Delta region of China\ndemonstrate the model's superior performance compared to existing methods,\nachieving R2 scores of 0.93, 0.96, and 0.94 for the primary, secondary and\ntertiary sectors, respectively. Cross-regional experiments in Beijing and\nChengdu further illustrate its generality. Systematic analysis reveals how\ndifferent data modalities influence model predictions, enhancing explainability\nwhile providing valuable insights for regional development planning. This\nrepresentation learning framework advances regional economic monitoring through\ndiverse urban data integration, providing a robust foundation for precise\neconomic forecasting."}
{"id": "2505.11756", "pdf": "https://arxiv.org/pdf/2505.11756", "abs": "https://arxiv.org/abs/2505.11756", "authors": ["David Chanin", "Tomáš Dulka", "Adrià Garriga-Alonso"], "title": "Feature Hedging: Correlated Features Break Narrow Sparse Autoencoders", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "It is assumed that sparse autoencoders (SAEs) decompose polysemantic\nactivations into interpretable linear directions, as long as the activations\nare composed of sparse linear combinations of underlying features. However, we\nfind that if an SAE is more narrow than the number of underlying \"true\nfeatures\" on which it is trained, and there is correlation between features,\nthe SAE will merge components of correlated features together, thus destroying\nmonosemanticity. In LLM SAEs, these two conditions are almost certainly true.\nThis phenomenon, which we call feature hedging, is caused by SAE reconstruction\nloss, and is more severe the narrower the SAE. In this work, we introduce the\nproblem of feature hedging and study it both theoretically in toy models and\nempirically in SAEs trained on LLMs. We suspect that feature hedging may be one\nof the core reasons that SAEs consistently underperform supervised baselines.\nFinally, we use our understanding of feature hedging to propose an improved\nvariant of matryoshka SAEs. Our work shows there remain fundamental issues with\nSAEs, but we are hopeful that that highlighting feature hedging will catalyze\nfuture advances that allow SAEs to achieve their full potential of interpreting\nLLMs at scale."}
{"id": "2505.11651", "pdf": "https://arxiv.org/pdf/2505.11651", "abs": "https://arxiv.org/abs/2505.11651", "authors": ["Radek Osmulsk", "Gabriel de Souza P. Moreira", "Ronay Ak", "Mengyao Xu", "Benedikt Schifferer", "Even Oldridge"], "title": "MIRACL-VISION: A Large, multilingual, visual document retrieval benchmark", "categories": ["cs.IR", "cs.CV"], "comment": null, "summary": "Document retrieval is an important task for search and Retrieval-Augmented\nGeneration (RAG) applications. Large Language Models (LLMs) have contributed to\nimproving the accuracy of text-based document retrieval. However, documents\nwith complex layout and visual elements like tables, charts and infographics\nare not perfectly represented in textual format. Recently, image-based document\nretrieval pipelines have become popular, which use visual large language models\n(VLMs) to retrieve relevant page images given a query. Current evaluation\nbenchmarks on visual document retrieval are limited, as they primarily focus\nonly English language, rely on synthetically generated questions and offer a\nsmall corpus size. Therefore, we introduce MIRACL-VISION, a multilingual visual\ndocument retrieval evaluation benchmark. MIRACL-VISION covers 18 languages, and\nis an extension of the MIRACL dataset, a popular benchmark to evaluate\ntext-based multilingual retrieval pipelines. MIRACL was built using a\nhuman-intensive annotation process to generate high-quality questions. In order\nto reduce MIRACL-VISION corpus size to make evaluation more compute friendly\nwhile keeping the datasets challenging, we have designed a method for\neliminating the \"easy\" negatives from the corpus. We conducted extensive\nexperiments comparing MIRACL-VISION with other benchmarks, using popular public\ntext and image models. We observe a gap in state-of-the-art VLM-based embedding\nmodels on multilingual capabilities, with up to 59.7% lower retrieval accuracy\nthan a text-based retrieval models. Even for the English language, the visual\nmodels retrieval accuracy is 12.1% lower compared to text-based models.\nMIRACL-VISION is a challenging, representative, multilingual evaluation\nbenchmark for visual retrieval pipelines and will help the community build\nrobust models for document retrieval."}
{"id": "2505.11770", "pdf": "https://arxiv.org/pdf/2505.11770", "abs": "https://arxiv.org/abs/2505.11770", "authors": ["Jing Huang", "Junyi Tao", "Thomas Icard", "Diyi Yang", "Christopher Potts"], "title": "Internal Causal Mechanisms Robustly Predict Language Model Out-of-Distribution Behaviors", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": "ICML 2025", "summary": "Interpretability research now offers a variety of techniques for identifying\nabstract internal mechanisms in neural networks. Can such techniques be used to\npredict how models will behave on out-of-distribution examples? In this work,\nwe provide a positive answer to this question. Through a diverse set of\nlanguage modeling tasks--including symbol manipulation, knowledge retrieval,\nand instruction following--we show that the most robust features for\ncorrectness prediction are those that play a distinctive causal role in the\nmodel's behavior. Specifically, we propose two methods that leverage causal\nmechanisms to predict the correctness of model outputs: counterfactual\nsimulation (checking whether key causal variables are realized) and value\nprobing (using the values of those variables to make predictions). Both achieve\nhigh AUC-ROC in distribution and outperform methods that rely on\ncausal-agnostic features in out-of-distribution settings, where predicting\nmodel behaviors is more crucial. Our work thus highlights a novel and\nsignificant application for internal causal analysis of language models."}
{"id": "2505.11717", "pdf": "https://arxiv.org/pdf/2505.11717", "abs": "https://arxiv.org/abs/2505.11717", "authors": ["Xilong Wang", "John Bloch", "Zedian Shao", "Yuepeng Hu", "Shuyan Zhou", "Neil Zhenqiang Gong"], "title": "EnvInjection: Environmental Prompt Injection Attack to Multi-modal Web Agents", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Multi-modal large language model (MLLM)-based web agents interact with\nwebpage environments by generating actions based on screenshots of the\nwebpages. Environmental prompt injection attacks manipulate the environment to\ninduce the web agent to perform a specific, attacker-chosen action--referred to\nas the target action. However, existing attacks suffer from limited\neffectiveness or stealthiness, or are impractical in real-world settings. In\nthis work, we propose EnvInjection, a new attack that addresses these\nlimitations. Our attack adds a perturbation to the raw pixel values of the\nrendered webpage, which can be implemented by modifying the webpage's source\ncode. After these perturbed pixels are mapped into a screenshot, the\nperturbation induces the web agent to perform the target action. We formulate\nthe task of finding the perturbation as an optimization problem. A key\nchallenge in solving this problem is that the mapping between raw pixel values\nand screenshot is non-differentiable, making it difficult to backpropagate\ngradients to the perturbation. To overcome this, we train a neural network to\napproximate the mapping and apply projected gradient descent to solve the\nreformulated optimization problem. Extensive evaluation on multiple webpage\ndatasets shows that EnvInjection is highly effective and significantly\noutperforms existing baselines."}
{"id": "2505.11812", "pdf": "https://arxiv.org/pdf/2505.11812", "abs": "https://arxiv.org/abs/2505.11812", "authors": ["Yang Tan", "Wenrui Gou", "Bozitao Zhong", "Liang Hong", "Huiqun Yu", "Bingxin Zhou"], "title": "VenusX: Unlocking Fine-Grained Functional Understanding of Proteins", "categories": ["cs.LG", "cs.CL", "q-bio.QM"], "comment": "29 pages, 3 figures, 17 tables", "summary": "Deep learning models have driven significant progress in predicting protein\nfunction and interactions at the protein level. While these advancements have\nbeen invaluable for many biological applications such as enzyme engineering and\nfunction annotation, a more detailed perspective is essential for understanding\nprotein functional mechanisms and evaluating the biological knowledge captured\nby models. To address this demand, we introduce VenusX, the first large-scale\nbenchmark for fine-grained functional annotation and function-based protein\npairing at the residue, fragment, and domain levels. VenusX comprises three\nmajor task categories across six types of annotations, including residue-level\nbinary classification, fragment-level multi-class classification, and pairwise\nfunctional similarity scoring for identifying critical active sites, binding\nsites, conserved sites, motifs, domains, and epitopes. The benchmark features\nover 878,000 samples curated from major open-source databases such as InterPro,\nBioLiP, and SAbDab. By providing mixed-family and cross-family splits at three\nsequence identity thresholds, our benchmark enables a comprehensive assessment\nof model performance on both in-distribution and out-of-distribution scenarios.\nFor baseline evaluation, we assess a diverse set of popular and open-source\nmodels, including pre-trained protein language models, sequence-structure\nhybrids, structure-based methods, and alignment-based techniques. Their\nperformance is reported across all benchmark datasets and evaluation settings\nusing multiple metrics, offering a thorough comparison and a strong foundation\nfor future research. Code and data are publicly available at\nhttps://github.com/ai4protein/VenusX."}
{"id": "2505.11797", "pdf": "https://arxiv.org/pdf/2505.11797", "abs": "https://arxiv.org/abs/2505.11797", "authors": ["Hancan Zhu", "Jinhao Chen", "Guanghua He"], "title": "MedVKAN: Efficient Feature Extraction with Mamba and KAN for Medical Image Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Medical image segmentation relies heavily on convolutional neural networks\n(CNNs) and Transformer-based models. However, CNNs are constrained by limited\nreceptive fields, while Transformers suffer from scalability challenges due to\ntheir quadratic computational complexity. To address these limitations, recent\nadvances have explored alternative architectures. The state-space model Mamba\noffers near-linear complexity while capturing long-range dependencies, and the\nKolmogorov-Arnold Network (KAN) enhances nonlinear expressiveness by replacing\nfixed activation functions with learnable ones. Building on these strengths, we\npropose MedVKAN, an efficient feature extraction model integrating Mamba and\nKAN. Specifically, we introduce the EFC-KAN module, which enhances KAN with\nconvolutional operations to improve local pixel interaction. We further design\nthe VKAN module, integrating Mamba with EFC-KAN as a replacement for\nTransformer modules, significantly improving feature extraction. Extensive\nexperiments on five public medical image segmentation datasets show that\nMedVKAN achieves state-of-the-art performance on four datasets and ranks second\non the remaining one. These results validate the potential of Mamba and KAN for\nmedical image segmentation while introducing an innovative and computationally\nefficient feature extraction framework. The code is available at:\nhttps://github.com/beginner-cjh/MedVKAN."}
{"id": "2505.11842", "pdf": "https://arxiv.org/pdf/2505.11842", "abs": "https://arxiv.org/abs/2505.11842", "authors": ["Xuannan Liu", "Zekun Li", "Zheqi He", "Peipei Li", "Shuhan Xia", "Xing Cui", "Huaibo Huang", "Xi Yang", "Ran He"], "title": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs", "categories": ["cs.CV", "cs.CL"], "comment": "Project page:\n  https://liuxuannan.github.io/Video-SafetyBench.github.io/", "summary": "The increasing deployment of Large Vision-Language Models (LVLMs) raises\nsafety concerns under potential malicious inputs. However, existing multimodal\nsafety evaluations primarily focus on model vulnerabilities exposed by static\nimage inputs, ignoring the temporal dynamics of video that may induce distinct\nsafety risks. To bridge this gap, we introduce Video-SafetyBench, the first\ncomprehensive benchmark designed to evaluate the safety of LVLMs under\nvideo-text attacks. It comprises 2,264 video-text pairs spanning 48\nfine-grained unsafe categories, each pairing a synthesized video with either a\nharmful query, which contains explicit malice, or a benign query, which appears\nharmless but triggers harmful behavior when interpreted alongside the video. To\ngenerate semantically accurate videos for safety evaluation, we design a\ncontrollable pipeline that decomposes video semantics into subject images (what\nis shown) and motion text (how it moves), which jointly guide the synthesis of\nquery-relevant videos. To effectively evaluate uncertain or borderline harmful\noutputs, we propose RJScore, a novel LLM-based metric that incorporates the\nconfidence of judge models and human-aligned decision threshold calibration.\nExtensive experiments show that benign-query video composition achieves average\nattack success rates of 67.2%, revealing consistent vulnerabilities to\nvideo-induced attacks. We believe Video-SafetyBench will catalyze future\nresearch into video-based safety evaluation and defense strategies."}
{"id": "2505.11832", "pdf": "https://arxiv.org/pdf/2505.11832", "abs": "https://arxiv.org/abs/2505.11832", "authors": ["Yuxiang Lai", "Jike Zhong", "Vanessa Su", "Xiaofeng Yang"], "title": "Patient-Specific Autoregressive Models for Organ Motion Prediction in Radiotherapy", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Radiotherapy often involves a prolonged treatment period. During this time,\npatients may experience organ motion due to breathing and other physiological\nfactors. Predicting and modeling this motion before treatment is crucial for\nensuring precise radiation delivery. However, existing pre-treatment organ\nmotion prediction methods primarily rely on deformation analysis using\nprincipal component analysis (PCA), which is highly dependent on registration\nquality and struggles to capture periodic temporal dynamics for motion\nmodeling.In this paper, we observe that organ motion prediction closely\nresembles an autoregressive process, a technique widely used in natural\nlanguage processing (NLP). Autoregressive models predict the next token based\non previous inputs, naturally aligning with our objective of predicting future\norgan motion phases. Building on this insight, we reformulate organ motion\nprediction as an autoregressive process to better capture patient-specific\nmotion patterns. Specifically, we acquire 4D CT scans for each patient before\ntreatment, with each sequence comprising multiple 3D CT phases. These phases\nare fed into the autoregressive model to predict future phases based on prior\nphase motion patterns. We evaluate our method on a real-world test set of 4D CT\nscans from 50 patients who underwent radiotherapy at our institution and a\npublic dataset containing 4D CT scans from 20 patients (some with multiple\nscans), totaling over 1,300 3D CT phases. The performance in predicting the\nmotion of the lung and heart surpasses existing benchmarks, demonstrating its\neffectiveness in capturing motion dynamics from CT images. These results\nhighlight the potential of our method to improve pre-treatment planning in\nradiotherapy, enabling more precise and adaptive radiation delivery."}
{"id": "2505.11861", "pdf": "https://arxiv.org/pdf/2505.11861", "abs": "https://arxiv.org/abs/2505.11861", "authors": ["Qi Zhou", "Jie Zhang", "Dongxia Wang", "Qiang Liu", "Tianlin Li", "Jin Song Dong", "Wenhai Wang", "Qing Guo"], "title": "Fair-PP: A Synthetic Dataset for Aligning LLM with Personalized Preferences of Social Equity", "categories": ["cs.AI", "cs.CL", "91C99", "I.2.7; J.4"], "comment": "under review", "summary": "Human preference plays a crucial role in the refinement of large language\nmodels (LLMs). However, collecting human preference feedback is costly and most\nexisting datasets neglect the correlation between personalization and\npreferences. To address this issue, we introduce Fair-PP, a synthetic dataset\nof personalized preferences targeting social equity, derived from real-world\nsocial survey data, which includes 28 social groups, 98 equity topics, and 5\npersonal preference dimensions. Leveraging GPT-4o-mini, we engage in\nrole-playing based on seven representative persona portrayals guided by\nexisting social survey data, yielding a total of 238,623 preference records.\nThrough Fair-PP, we also contribute (i) An automated framework for generating\npreference data, along with a more fine-grained dataset of personalized\npreferences; (ii) analysis of the positioning of the existing mainstream LLMs\nacross five major global regions within the personalized preference space; and\n(iii) a sample reweighting method for personalized preference alignment,\nenabling alignment with a target persona while maximizing the divergence from\nother personas. Empirical experiments show our method outperforms the\nbaselines."}
{"id": "2505.11865", "pdf": "https://arxiv.org/pdf/2505.11865", "abs": "https://arxiv.org/abs/2505.11865", "authors": ["Teli Ma", "Jia Zheng", "Zifan Wang", "Ziyao Gao", "Jiaming Zhou", "Junwei Liang"], "title": "GLOVER++: Unleashing the Potential of Affordance Learning from Human Behaviors for Robotic Manipulation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Learning manipulation skills from human demonstration videos offers a\npromising path toward generalizable and interpretable robotic\nintelligence-particularly through the lens of actionable affordances. However,\ntransferring such knowledge remains challenging due to: 1) a lack of\nlarge-scale datasets with precise affordance annotations, and 2) insufficient\nexploration of affordances in diverse manipulation contexts. To address these\ngaps, we introduce HOVA-500K, a large-scale, affordance-annotated dataset\ncomprising 500,000 images across 1,726 object categories and 675 actions. We\nalso release a standardized benchmarking suite for multi-modal affordance\nreasoning. Built upon HOVA-500K, we present GLOVER++, a global-to-local\naffordance training framework that effectively transfers actionable affordance\nknowledge from human demonstrations to downstream open-vocabulary reasoning\ntasks. GLOVER++ achieves state-of-the-art results on the HOVA-500K benchmark\nand demonstrates strong generalization across diverse downstream robotic\nmanipulation tasks. By explicitly modeling actionable affordances, GLOVER++\nfacilitates robust transfer across scenes, modalities, and tasks. We hope that\nHOVA-500K and the GLOVER++ framework will serve as valuable resources for\nbridging the gap between human demonstrations and robotic manipulation\ncapabilities."}
{"id": "2505.11875", "pdf": "https://arxiv.org/pdf/2505.11875", "abs": "https://arxiv.org/abs/2505.11875", "authors": ["Chi-Min Chan", "Chunpu Xu", "Jiaming Ji", "Zhen Ye", "Pengcheng Wen", "Chunyang Jiang", "Yaodong Yang", "Wei Xue", "Sirui Han", "Yike Guo"], "title": "J1: Exploring Simple Test-Time Scaling for LLM-as-a-Judge", "categories": ["cs.LG", "cs.CL"], "comment": "33 pages, 27 figures", "summary": "The current focus of AI research is shifting from emphasizing model training\ntowards enhancing evaluation quality, a transition that is crucial for driving\nfurther advancements in AI systems. Traditional evaluation methods typically\nrely on reward models assigning scalar preference scores to outputs. Although\neffective, such approaches lack interpretability, leaving users often uncertain\nabout why a reward model rates a particular response as high or low. The advent\nof LLM-as-a-Judge provides a more scalable and interpretable method of\nsupervision, offering insights into the decision-making process. Moreover, with\nthe emergence of large reasoning models, which consume more tokens for deeper\nthinking and answer refinement, scaling test-time computation in the\nLLM-as-a-Judge paradigm presents an avenue for further boosting performance and\nproviding more interpretability through reasoning traces. In this paper, we\nintroduce $\\textbf{J1-7B}$, which is first supervised fine-tuned on\nreflection-enhanced datasets collected via rejection-sampling and subsequently\ntrained using Reinforcement Learning (RL) with verifiable rewards. At inference\ntime, we apply Simple Test-Time Scaling (STTS) strategies for additional\nperformance improvement. Experimental results demonstrate that $\\textbf{J1-7B}$\nsurpasses the previous state-of-the-art LLM-as-a-Judge by $ \\textbf{4.8}$\\% and\nexhibits a $ \\textbf{5.1}$\\% stronger scaling trend under STTS. Additionally,\nwe present three key findings: (1) Existing LLM-as-a-Judge does not inherently\nexhibit such scaling trend. (2) Model simply fine-tuned on reflection-enhanced\ndatasets continues to demonstrate similarly weak scaling behavior. (3)\nSignificant scaling trend emerges primarily during the RL phase, suggesting\nthat effective STTS capability is acquired predominantly through RL training."}
{"id": "2505.11879", "pdf": "https://arxiv.org/pdf/2505.11879", "abs": "https://arxiv.org/abs/2505.11879", "authors": ["Reihaneh Yourdkhani", "Arash Tavoosian", "Navid Asadi Khomami", "Mehdi Tale Masouleh"], "title": "Experimental Study on Automatically Assembling Custom Catering Packages With a 3-DOF Delta Robot Using Deep Learning Methods", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "This paper introduces a pioneering experimental study on the automated\npacking of a catering package using a two-fingered gripper affixed to a\n3-degree-of-freedom Delta parallel robot. A distinctive contribution lies in\nthe application of a deep learning approach to tackle this challenge. A custom\ndataset, comprising 1,500 images, is meticulously curated for this endeavor,\nrepresenting a noteworthy initiative as the first dataset focusing on\nPersian-manufactured products. The study employs the YOLOV5 model for object\ndetection, followed by segmentation using the FastSAM model. Subsequently,\nrotation angle calculation is facilitated with segmentation masks, and a\nrotated rectangle encapsulating the object is generated. This rectangle forms\nthe basis for calculating two grasp points using a novel geometrical approach\ninvolving eigenvectors. An extensive experimental study validates the proposed\nmodel, where all pertinent information is seamlessly transmitted to the 3-DOF\nDelta parallel robot. The proposed algorithm ensures real-time detection,\ncalibration, and the fully autonomous packing process of a catering package,\nboasting an impressive over 80\\% success rate in automatic grasping. This study\nmarks a significant stride in advancing the capabilities of robotic systems for\npractical applications in packaging automation."}
{"id": "2505.11979", "pdf": "https://arxiv.org/pdf/2505.11979", "abs": "https://arxiv.org/abs/2505.11979", "authors": ["Tarik Houichime", "Younes El Amrani"], "title": "Introduction to Analytical Software Engineering Design Paradigm", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.MS", "cs.PL"], "comment": "The Conference's autorization to submit a preprint was granted", "summary": "As modern software systems expand in scale and complexity, the challenges\nassociated with their modeling and formulation grow increasingly intricate.\nTraditional approaches often fall short in effectively addressing these\ncomplexities, particularly in tasks such as design pattern detection for\nmaintenance and assessment, as well as code refactoring for optimization and\nlong-term sustainability. This growing inadequacy underscores the need for a\nparadigm shift in how such challenges are approached and resolved. This paper\npresents Analytical Software Engineering (ASE), a novel design paradigm aimed\nat balancing abstraction, tool accessibility, compatibility, and scalability.\nASE enables effective modeling and resolution of complex software engineering\nproblems. The paradigm is evaluated through two frameworks\nBehavioral-Structural Sequences (BSS) and Optimized Design Refactoring (ODR),\nboth developed in accordance with ASE principles. BSS offers a compact,\nlanguage-agnostic representation of codebases to facilitate precise design\npattern detection. ODR unifies artifact and solution representations to\noptimize code refactoring via heuristic algorithms while eliminating iterative\ncomputational overhead. By providing a structured approach to software design\nchallenges, ASE lays the groundwork for future research in encoding and\nanalyzing complex software metrics."}
{"id": "2505.11883", "pdf": "https://arxiv.org/pdf/2505.11883", "abs": "https://arxiv.org/abs/2505.11883", "authors": ["Zihuan Qiu", "Yi Xu", "Chiyuan He", "Fanman Meng", "Linfeng Xu", "Qingbo Wu", "Hongliang Li"], "title": "MINGLE: Mixtures of Null-Space Gated Low-Rank Experts for Test-Time Continual Model Merging", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Continual model merging integrates independently fine-tuned models\nsequentially without access to original training data, providing a scalable and\nefficient solution to continual learning. However, current methods still face\ncritical challenges, notably parameter interference among tasks and limited\nadaptability to evolving test distributions. The former causes catastrophic\nforgetting of integrated tasks, while the latter hinders effective adaptation\nto new tasks. To address these, we propose MINGLE, a novel framework for\ntest-time continual model merging, which leverages test-time adaptation using a\nsmall set of unlabeled test samples from the current task to dynamically guide\nthe merging process. MINGLE employs a mixture-of-experts architecture composed\nof parameter-efficient, low-rank experts, enabling efficient adaptation and\nimproving robustness to distribution shifts. To mitigate catastrophic\nforgetting, we propose Null-Space Constrained Gating, which restricts gating\nupdates to subspaces orthogonal to prior task representations. This suppresses\nactivations on old task inputs and preserves model behavior on past tasks. To\nfurther balance stability and adaptability, we design an Adaptive Relaxation\nStrategy, which dynamically adjusts the constraint strength based on\ninterference signals captured during test-time adaptation. Extensive\nexperiments on standard continual merging benchmarks demonstrate that MINGLE\nachieves robust generalization, reduces forgetting significantly, and\nconsistently surpasses previous state-of-the-art methods by 7-9\\% on average\nacross diverse task orders."}
{"id": "2505.12039", "pdf": "https://arxiv.org/pdf/2505.12039", "abs": "https://arxiv.org/abs/2505.12039", "authors": ["Renqi Chen", "Haoyang Su", "Shixiang Tang", "Zhenfei Yin", "Qi Wu", "Hui Li", "Ye Sun", "Nanqing Dong", "Wanli Ouyang", "Philip Torr"], "title": "AI-Driven Automation Can Become the Foundation of Next-Era Science of Science Research", "categories": ["cs.AI", "cs.CL", "physics.soc-ph"], "comment": null, "summary": "The Science of Science (SoS) explores the mechanisms underlying scientific\ndiscovery, and offers valuable insights for enhancing scientific efficiency and\nfostering innovation. Traditional approaches often rely on simplistic\nassumptions and basic statistical tools, such as linear regression and\nrule-based simulations, which struggle to capture the complexity and scale of\nmodern research ecosystems. The advent of artificial intelligence (AI) presents\na transformative opportunity for the next generation of SoS, enabling the\nautomation of large-scale pattern discovery and uncovering insights previously\nunattainable. This paper offers a forward-looking perspective on the\nintegration of Science of Science with AI for automated research pattern\ndiscovery and highlights key open challenges that could greatly benefit from\nAI. We outline the advantages of AI over traditional methods, discuss potential\nlimitations, and propose pathways to overcome them. Additionally, we present a\npreliminary multi-agent system as an illustrative example to simulate research\nsocieties, showcasing AI's ability to replicate real-world research patterns\nand accelerate progress in Science of Science research."}
{"id": "2505.11909", "pdf": "https://arxiv.org/pdf/2505.11909", "abs": "https://arxiv.org/abs/2505.11909", "authors": ["Pengfei Lyu", "Pak-Hei Yeung", "Xiaosheng Yu", "Jing Xia", "Jianning Chi", "Chengdong Wu", "Jagath C. Rajapakse"], "title": "Bridging the Inter-Domain Gap through Low-Level Features for Cross-Modal Medical Image Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": "11 pages, 2 figures", "summary": "This paper addresses the task of cross-modal medical image segmentation by\nexploring unsupervised domain adaptation (UDA) approaches. We propose a\nmodel-agnostic UDA framework, LowBridge, which builds on a simple observation\nthat cross-modal images share some similar low-level features (e.g., edges) as\nthey are depicting the same structures. Specifically, we first train a\ngenerative model to recover the source images from their edge features,\nfollowed by training a segmentation model on the generated source images,\nseparately. At test time, edge features from the target images are input to the\npretrained generative model to generate source-style target domain images,\nwhich are then segmented using the pretrained segmentation network. Despite its\nsimplicity, extensive experiments on various publicly available datasets\ndemonstrate that \\proposed achieves state-of-the-art performance, outperforming\neleven existing UDA approaches under different settings. Notably, further\nablation studies show that \\proposed is agnostic to different types of\ngenerative and segmentation models, suggesting its potential to be seamlessly\nplugged with the most advanced models to achieve even more outstanding results\nin the future. The code is available at https://github.com/JoshuaLPF/LowBridge."}
{"id": "2505.12058", "pdf": "https://arxiv.org/pdf/2505.12058", "abs": "https://arxiv.org/abs/2505.12058", "authors": ["Vincent Koc"], "title": "Tiny QA Benchmark++: Ultra-Lightweight, Synthetic Multilingual Dataset Generation & Smoke-Tests for Continuous LLM Evaluation", "categories": ["cs.AI", "cs.CL", "I.2.7; I.2.6; H.2.8"], "comment": "28 pages, 7 figures, 3 tables. Includes expanded appendix & full\n  score matrices. Dataset & code: HF Hub + GitHub + Pypi links in abstract.\n  Core data and code Apache-2.0; synthetic packs eval-only", "summary": "Tiny QA Benchmark++ (TQB++) presents an ultra-lightweight, multilingual\nsmoke-test suite designed to give large-language-model (LLM) pipelines a\nunit-test style safety net dataset that runs in seconds with minimal cost. Born\nout of the tight feedback-loop demands building the Comet Opik\nprompt-optimization SDK, where waiting on heavyweight benchmarks breaks\ndeveloper flow. TQB++ couples a 52-item English gold set (less than 20 kB) with\na tiny synthetic-data generator pypi package built on provider-agnostic\nLiteLLM. The generator lets practitioners mint their own tiny packs in any\nlanguage, domain, or difficulty, while ten ready-made packs already cover\nArabic, Chinese, French, German, Japanese, Korean, Portuguese, Russian,\nSpanish, and Turkish. Every dataset ships with Croissant metadata and\nplug-and-play files for OpenAI-Evals, LangChain, and standard CI tools, so\nteams can drop deterministic micro-benchmarks directly into pull-request gates,\nprompt-engineering loops, and production dashboards without touching GPU\nbudgets. A complete TQB++ run adds only a few seconds to pipeline latency yet\nreliably flags prompt-template errors, tokenizer drift, and fine-tuning\nside-effects long before full-scale suites like MMLU or BIG-Bench would finish\nconfiguring. The entire framework is released to accelerate continuous,\nresource-efficient quality assurance across the generative-AI ecosystem."}
{"id": "2505.11913", "pdf": "https://arxiv.org/pdf/2505.11913", "abs": "https://arxiv.org/abs/2505.11913", "authors": ["Sven Dummer", "Puru Vaish", "Christoph Brune"], "title": "Joint Manifold Learning and Optimal Transport for Dynamic Imaging", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Dynamic imaging is critical for understanding and visualizing dynamic\nbiological processes in medicine and cell biology. These applications often\nencounter the challenge of a limited amount of time series data and time\npoints, which hinders learning meaningful patterns. Regularization methods\nprovide valuable prior knowledge to address this challenge, enabling the\nextraction of relevant information despite the scarcity of time-series data and\ntime points. In particular, low-dimensionality assumptions on the image\nmanifold address sample scarcity, while time progression models, such as\noptimal transport (OT), provide priors on image development to mitigate the\nlack of time points. Existing approaches using low-dimensionality assumptions\ndisregard a temporal prior but leverage information from multiple time series.\nOT-prior methods, however, incorporate the temporal prior but regularize only\nindividual time series, ignoring information from other time series of the same\nimage modality. In this work, we investigate the effect of integrating a\nlow-dimensionality assumption of the underlying image manifold with an OT\nregularizer for time-evolving images. In particular, we propose a latent model\nrepresentation of the underlying image manifold and promote consistency between\nthis representation, the time series data, and the OT prior on the\ntime-evolving images. We discuss the advantages of enriching OT interpolations\nwith latent models and integrating OT priors into latent models."}
{"id": "2505.12065", "pdf": "https://arxiv.org/pdf/2505.12065", "abs": "https://arxiv.org/abs/2505.12065", "authors": ["Tiannuo Yang", "Zebin Yao", "Bowen Jin", "Lixiao Cui", "Yusen Li", "Gang Wang", "Xiaoguang Liu"], "title": "Demystifying and Enhancing the Efficiency of Large Language Model Based Search Agents", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "Large Language Model (LLM)-based search agents have shown remarkable\ncapabilities in solving complex tasks by dynamically decomposing problems and\naddressing them through interleaved reasoning and retrieval. However, this\ninterleaved paradigm introduces substantial efficiency bottlenecks. First, we\nobserve that both highly accurate and overly approximate retrieval methods\ndegrade system efficiency: exact search incurs significant retrieval overhead,\nwhile coarse retrieval requires additional reasoning steps during generation.\nSecond, we identify inefficiencies in system design, including improper\nscheduling and frequent retrieval stalls, which lead to cascading latency --\nwhere even minor delays in retrieval amplify end-to-end inference time. To\naddress these challenges, we introduce SearchAgent-X, a high-efficiency\ninference framework for LLM-based search agents. SearchAgent-X leverages\nhigh-recall approximate retrieval and incorporates two key techniques:\npriority-aware scheduling and non-stall retrieval. Extensive experiments\ndemonstrate that SearchAgent-X consistently outperforms state-of-the-art\nsystems such as vLLM and HNSW-based retrieval across diverse tasks, achieving\nup to 3.4$\\times$ higher throughput and 5$\\times$ lower latency, without\ncompromising generation quality. SearchAgent-X is available at\nhttps://github.com/tiannuo-yang/SearchAgent-X."}
{"id": "2505.11998", "pdf": "https://arxiv.org/pdf/2505.11998", "abs": "https://arxiv.org/abs/2505.11998", "authors": ["Prashant Shivaram Bhat", "Shakib Yazdani", "Elahe Arani", "Bahram Zonooz"], "title": "Parameter Efficient Continual Learning with Dynamic Low-Rank Adaptation", "categories": ["cs.LG", "cs.CV"], "comment": "27 pages, 5 figures", "summary": "Catastrophic forgetting has remained a critical challenge for deep neural\nnetworks in Continual Learning (CL) as it undermines consolidated knowledge\nwhen learning new tasks. Parameter efficient fine tuning CL techniques are\ngaining traction for their effectiveness in addressing catastrophic forgetting\nwith a lightweight training schedule while avoiding degradation of consolidated\nknowledge in pre-trained models. However, low rank adapters (LoRA) in these\napproaches are highly sensitive to rank selection which can lead to sub-optimal\nresource allocation and performance. To this end, we introduce PEARL, a\nrehearsal-free CL framework that entails dynamic rank allocation for LoRA\ncomponents during CL training. Specifically, PEARL leverages reference task\nweights and adaptively determines the rank of task-specific LoRA components\nbased on the current tasks' proximity to reference task weights in parameter\nspace. To demonstrate the versatility of PEARL, we evaluate it across three\nvision architectures (ResNet, Separable Convolutional Network and Vision\nTransformer) and a multitude of CL scenarios, and show that PEARL outperforms\nall considered baselines by a large margin."}
{"id": "2505.12135", "pdf": "https://arxiv.org/pdf/2505.12135", "abs": "https://arxiv.org/abs/2505.12135", "authors": ["Omar Choukrani", "Idriss Malek", "Daniil Orel", "Zhuohan Xie", "Zangir Iklassov", "Martin Takáč", "Salem Lahlou"], "title": "LLM-BABYBENCH: Understanding and Evaluating Grounded Planning and Reasoning in LLMs", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Assessing the capacity of Large Language Models (LLMs) to plan and reason\nwithin the constraints of interactive environments is crucial for developing\ncapable AI agents. We introduce $\\textbf{LLM-BabyBench}$, a new benchmark suite\ndesigned specifically for this purpose. Built upon a textual adaptation of the\nprocedurally generated BabyAI grid world, this suite evaluates LLMs on three\nfundamental aspects of grounded intelligence: (1) predicting the consequences\nof actions on the environment state ($\\textbf{Predict}$ task), (2) generating\nsequences of low-level actions to achieve specified objectives ($\\textbf{Plan}$\ntask), and (3) decomposing high-level instructions into coherent subgoal\nsequences ($\\textbf{Decompose}$ task). We detail the methodology for generating\nthe three corresponding datasets ($\\texttt{LLM-BabyBench-Predict}$,\n$\\texttt{-Plan}$, $\\texttt{-Decompose}$) by extracting structured information\nfrom an expert agent operating within the text-based environment. Furthermore,\nwe provide a standardized evaluation harness and metrics, including environment\ninteraction for validating generated plans, to facilitate reproducible\nassessment of diverse LLMs. Initial baseline results highlight the challenges\nposed by these grounded reasoning tasks. The benchmark suite, datasets, data\ngeneration code, and evaluation code are made publicly available\n($\\href{https://github.com/choukrani/llm-babybench}{\\text{GitHub}}$,\n$\\href{https://huggingface.co/datasets/salem-mbzuai/LLM-BabyBench}{\\text{HuggingFace}}$)."}
{"id": "2505.12051", "pdf": "https://arxiv.org/pdf/2505.12051", "abs": "https://arxiv.org/abs/2505.12051", "authors": ["Yinghui Zhang", "Tailin Chen", "Yuchen Zhang", "Zeyu Fu"], "title": "Enhanced Multimodal Hate Video Detection via Channel-wise and Modality-wise Fusion", "categories": ["cs.MM", "cs.AI", "cs.CV"], "comment": "ICDMW 2024, Github: https://github.com/EvelynZ10/cmfusion", "summary": "The rapid rise of video content on platforms such as TikTok and YouTube has\ntransformed information dissemination, but it has also facilitated the spread\nof harmful content, particularly hate videos. Despite significant efforts to\ncombat hate speech, detecting these videos remains challenging due to their\noften implicit nature. Current detection methods primarily rely on unimodal\napproaches, which inadequately capture the complementary features across\ndifferent modalities. While multimodal techniques offer a broader perspective,\nmany fail to effectively integrate temporal dynamics and modality-wise\ninteractions essential for identifying nuanced hate content. In this paper, we\npresent CMFusion, an enhanced multimodal hate video detection model utilizing a\nnovel Channel-wise and Modality-wise Fusion Mechanism. CMFusion first extracts\nfeatures from text, audio, and video modalities using pre-trained models and\nthen incorporates a temporal cross-attention mechanism to capture dependencies\nbetween video and audio streams. The learned features are then processed by\nchannel-wise and modality-wise fusion modules to obtain informative\nrepresentations of videos. Our extensive experiments on a real-world dataset\ndemonstrate that CMFusion significantly outperforms five widely used baselines\nin terms of accuracy, precision, recall, and F1 score. Comprehensive ablation\nstudies and parameter analyses further validate our design choices,\nhighlighting the model's effectiveness in detecting hate videos. The source\ncodes will be made publicly available at https://github.com/EvelynZ10/cmfusion."}
{"id": "2505.12185", "pdf": "https://arxiv.org/pdf/2505.12185", "abs": "https://arxiv.org/abs/2505.12185", "authors": ["Sen Fang", "Weiyuan Ding", "Bowen Xu"], "title": "EVALOOP: Assessing LLM Robustness in Programming from a Self-consistency Perspective", "categories": ["cs.SE", "cs.CL", "cs.LG"], "comment": "19 pages, 11 figures", "summary": "Assessing the programming capabilities of Large Language Models (LLMs) is\ncrucial for their effective use in software engineering. Current evaluations,\nhowever, predominantly measure the accuracy of generated code on static\nbenchmarks, neglecting the critical aspect of model robustness during\nprogramming tasks. While adversarial attacks offer insights on model\nrobustness, their effectiveness is limited and evaluation could be constrained.\nCurrent adversarial attack methods for robustness evaluation yield inconsistent\nresults, struggling to provide a unified evaluation across different LLMs. We\nintroduce EVALOOP, a novel assessment framework that evaluate the robustness\nfrom a self-consistency perspective, i.e., leveraging the natural duality\ninherent in popular software engineering tasks, e.g., code generation and code\nsummarization. EVALOOP initiates a self-contained feedback loop: an LLM\ngenerates output (e.g., code) from an input (e.g., natural language\nspecification), and then use the generated output as the input to produce a new\noutput (e.g., summarizes that code into a new specification). EVALOOP repeats\nthe process to assess the effectiveness of EVALOOP in each loop. This cyclical\nstrategy intrinsically evaluates robustness without rely on any external attack\nsetups, providing a unified metric to evaluate LLMs' robustness in programming.\nWe evaluate 16 prominent LLMs (e.g., GPT-4.1, O4-mini) on EVALOOP and found\nthat EVALOOP typically induces a 5.01%-19.31% absolute drop in pass@1\nperformance within ten loops. Intriguingly, robustness does not always align\nwith initial performance (i.e., one-time query); for instance, GPT-3.5-Turbo,\ndespite superior initial code generation compared to DeepSeek-V2, demonstrated\nlower robustness over repeated evaluation loop."}
{"id": "2505.12061", "pdf": "https://arxiv.org/pdf/2505.12061", "abs": "https://arxiv.org/abs/2505.12061", "authors": ["Samuel T. M. Ball"], "title": "Bayesian Deep Learning Approaches for Uncertainty-Aware Retinal OCT Image Segmentation for Multiple Sclerosis", "categories": ["eess.IV", "cs.CV", "68U10, 92C55", "I.2.10; I.4.6; J.3"], "comment": null, "summary": "Optical Coherence Tomography (OCT) provides valuable insights in\nophthalmology, cardiology, and neurology due to high-resolution,\ncross-sectional images of the retina. One critical task for ophthalmologists\nusing OCT is delineation of retinal layers within scans. This process is\ntime-consuming and prone to human bias, affecting the accuracy and reliability\nof diagnoses. Previous efforts to automate delineation using deep learning face\nchallenges in uptake from clinicians and statisticians due to the absence of\nuncertainty estimation, leading to \"confidently wrong\" models via\nhallucinations. In this study, we address these challenges by applying Bayesian\nconvolutional neural networks (BCNNs) to segment an openly available OCT\nimaging dataset containing 35 human retina OCTs split between healthy controls\nand patients with multiple sclerosis. Our findings demonstrate that Bayesian\nmodels can be used to provide uncertainty maps of the segmentation, which can\nfurther be used to identify highly uncertain samples that exhibit recording\nartefacts such as noise or miscalibration at inference time. Our method also\nallows for uncertainty-estimation for important secondary measurements such as\nlayer thicknesses, that are medically relevant for patients. We show that these\nfeatures come in addition to greater performance compared to similar work over\nall delineations; with an overall Dice score of 95.65%. Our work brings greater\nclinical applicability, statistical robustness, and performance to retinal OCT\nsegmentation."}
{"id": "2505.12189", "pdf": "https://arxiv.org/pdf/2505.12189", "abs": "https://arxiv.org/abs/2505.12189", "authors": ["Marco Valentino", "Geonhee Kim", "Dhairya Dalal", "Zhixue Zhao", "André Freitas"], "title": "Mitigating Content Effects on Reasoning in Language Models through Fine-Grained Activation Steering", "categories": ["cs.AI", "cs.CL"], "comment": "Work in progress", "summary": "Large language models (LLMs) frequently demonstrate reasoning limitations,\noften conflating content plausibility (i.e., material inference) with logical\nvalidity (i.e., formal inference). This can result in biased inferences, where\nplausible arguments are incorrectly deemed logically valid or vice versa.\nMitigating this limitation is critical, as it undermines the trustworthiness\nand generalizability of LLMs in applications that demand rigorous logical\nconsistency. This paper investigates the problem of mitigating content biases\non formal reasoning through activation steering. Specifically, we curate a\ncontrolled syllogistic reasoning dataset to disentangle formal validity from\ncontent plausibility. After localising the layers responsible for formal and\nmaterial inference, we investigate contrastive activation steering methods for\ntest-time interventions. An extensive empirical analysis on different LLMs\nreveals that contrastive steering consistently supports linear control over\ncontent biases. However, we observe that a static approach is insufficient for\nimproving all the tested models. We then leverage the possibility to control\ncontent effects by dynamically determining the value of the steering parameters\nvia fine-grained conditional methods. We found that conditional steering is\neffective on unresponsive models, achieving up to 15% absolute improvement in\nformal reasoning accuracy with a newly introduced kNN-based method (K-CAST).\nFinally, additional experiments reveal that steering for content effects is\nrobust to prompt variations, incurs minimal side effects on language modeling\ncapabilities, and can partially generalize to out-of-distribution reasoning\ntasks. Practically, this paper demonstrates that activation-level interventions\ncan offer a scalable strategy for enhancing the robustness of LLMs,\ncontributing towards more systematic and unbiased formal reasoning."}
{"id": "2505.12089", "pdf": "https://arxiv.org/pdf/2505.12089", "abs": "https://arxiv.org/abs/2505.12089", "authors": ["Sangmin Lee", "Eunpil Park", "Angel Canelo", "Hyunhee Park", "Youngjo Kim", "Hyung-Ju Chun", "Xin Jin", "Chongyi Li", "Chun-Le Guo", "Radu Timofte", "Qi Wu", "Tianheng Qiu", "Yuchun Dong", "Shenglin Ding", "Guanghua Pan", "Weiyu Zhou", "Tao Hu", "Yixu Feng", "Duwei Dai", "Yu Cao", "Peng Wu", "Wei Dong", "Yanning Zhang", "Qingsen Yan", "Simon J. Larsen", "Ruixuan Jiang", "Senyan Xu", "Xingbo Wang", "Xin Lu", "Marcos V. Conde", "Javier Abad-Hernandez", "Alvaro Garcıa-Lara", "Daniel Feijoo", "Alvaro Garcıa", "Zeyu Xiao", "Zhuoyuan Li"], "title": "NTIRE 2025 Challenge on Efficient Burst HDR and Restoration: Datasets, Methods, and Results", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "This paper reviews the NTIRE 2025 Efficient Burst HDR and Restoration\nChallenge, which aims to advance efficient multi-frame high dynamic range (HDR)\nand restoration techniques. The challenge is based on a novel RAW multi-frame\nfusion dataset, comprising nine noisy and misaligned RAW frames with various\nexposure levels per scene. Participants were tasked with developing solutions\ncapable of effectively fusing these frames while adhering to strict efficiency\nconstraints: fewer than 30 million model parameters and a computational budget\nunder 4.0 trillion FLOPs. A total of 217 participants registered, with six\nteams finally submitting valid solutions. The top-performing approach achieved\na PSNR of 43.22 dB, showcasing the potential of novel methods in this domain.\nThis paper provides a comprehensive overview of the challenge, compares the\nproposed solutions, and serves as a valuable reference for researchers and\npractitioners in efficient burst HDR and restoration."}
{"id": "2505.12225", "pdf": "https://arxiv.org/pdf/2505.12225", "abs": "https://arxiv.org/abs/2505.12225", "authors": ["Jizhou Guo", "Zhaomin Wu", "Philip S. Yu"], "title": "Reward Inside the Model: A Lightweight Hidden-State Reward Model for LLM's Best-of-N sampling", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "High-quality reward models are crucial for unlocking the reasoning potential\nof large language models (LLMs), with best-of-N voting demonstrating\nsignificant performance gains. However, current reward models, which typically\noperate on the textual output of LLMs, are computationally expensive and\nparameter-heavy, limiting their real-world applications. We introduce the\nEfficient Linear Hidden State Reward (ELHSR) model - a novel, highly\nparameter-efficient approach that leverages the rich information embedded in\nLLM hidden states to address these issues. ELHSR systematically outperform\nbaselines with less than 0.005% of the parameters of baselines, requiring only\na few samples for training. ELHSR also achieves orders-of-magnitude efficiency\nimprovement with significantly less time and fewer FLOPs per sample than\nbaseline reward models. Moreover, ELHSR exhibits robust performance even when\ntrained only on logits, extending its applicability to some closed-source LLMs.\nIn addition, ELHSR can also be combined with traditional reward models to\nachieve additional performance gains."}
{"id": "2505.12114", "pdf": "https://arxiv.org/pdf/2505.12114", "abs": "https://arxiv.org/abs/2505.12114", "authors": ["Dena F. Mujtaba", "Nihar R. Mahapatra"], "title": "Behind the Screens: Uncovering Bias in AI-Driven Video Interview Assessments Using Counterfactuals", "categories": ["cs.HC", "cs.CV", "eess.IV"], "comment": null, "summary": "AI-enhanced personality assessments are increasingly shaping hiring\ndecisions, using affective computing to predict traits from the Big Five\n(OCEAN) model. However, integrating AI into these assessments raises ethical\nconcerns, especially around bias amplification rooted in training data. These\nbiases can lead to discriminatory outcomes based on protected attributes like\ngender, ethnicity, and age. To address this, we introduce a\ncounterfactual-based framework to systematically evaluate and quantify bias in\nAI-driven personality assessments. Our approach employs generative adversarial\nnetworks (GANs) to generate counterfactual representations of job applicants by\naltering protected attributes, enabling fairness analysis without access to the\nunderlying model. Unlike traditional bias assessments that focus on unimodal or\nstatic data, our method supports multimodal evaluation-spanning visual, audio,\nand textual features. This comprehensive approach is particularly important in\nhigh-stakes applications like hiring, where third-party vendors often provide\nAI systems as black boxes. Applied to a state-of-the-art personality prediction\nmodel, our method reveals significant disparities across demographic groups. We\nalso validate our framework using a protected attribute classifier to confirm\nthe effectiveness of our counterfactual generation. This work provides a\nscalable tool for fairness auditing of commercial AI hiring platforms,\nespecially in black-box settings where training data and model internals are\ninaccessible. Our results highlight the importance of counterfactual approaches\nin improving ethical transparency in affective computing."}
{"id": "2505.12260", "pdf": "https://arxiv.org/pdf/2505.12260", "abs": "https://arxiv.org/abs/2505.12260", "authors": ["Guangyuan Ma", "Yongliang Ma", "Xuanrui Gou", "Zhenpeng Su", "Ming Zhou", "Songlin Hu"], "title": "LightRetriever: A LLM-based Hybrid Retrieval Architecture with 1000x Faster Query Inference", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs)-based hybrid retrieval uses LLMs to encode\nqueries and documents into low-dimensional dense or high-dimensional sparse\nvectors. It retrieves documents relevant to search queries based on vector\nsimilarities. Documents are pre-encoded offline, while queries arrive in\nreal-time, necessitating an efficient online query encoder. Although LLMs\nsignificantly enhance retrieval capabilities, serving deeply parameterized LLMs\nslows down query inference throughput and increases demands for online\ndeployment resources. In this paper, we propose LightRetriever, a novel\nLLM-based hybrid retriever with extremely lightweight query encoders. Our\nmethod retains a full-sized LLM for document encoding, but reduces the workload\nof query encoding to no more than an embedding lookup. Compared to serving a\nfull-sized LLM on an H800 GPU, our approach achieves over a 1000x speedup for\nquery inference with GPU acceleration, and even a 20x speedup without GPU.\nExperiments on large-scale retrieval benchmarks demonstrate that our method\ngeneralizes well across diverse retrieval tasks, retaining an average of 95%\nfull-sized performance."}
{"id": "2505.12120", "pdf": "https://arxiv.org/pdf/2505.12120", "abs": "https://arxiv.org/abs/2505.12120", "authors": ["Dmitry Nechaev", "Alexey Pchelnikov", "Ekaterina Ivanova"], "title": "HISTAI: An Open-Source, Large-Scale Whole Slide Image Dataset for Computational Pathology", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Recent advancements in Digital Pathology (DP), particularly through\nartificial intelligence and Foundation Models, have underscored the importance\nof large-scale, diverse, and richly annotated datasets. Despite their critical\nrole, publicly available Whole Slide Image (WSI) datasets often lack sufficient\nscale, tissue diversity, and comprehensive clinical metadata, limiting the\nrobustness and generalizability of AI models. In response, we introduce the\nHISTAI dataset, a large, multimodal, open-access WSI collection comprising over\n60,000 slides from various tissue types. Each case in the HISTAI dataset is\naccompanied by extensive clinical metadata, including diagnosis, demographic\ninformation, detailed pathological annotations, and standardized diagnostic\ncoding. The dataset aims to fill gaps identified in existing resources,\npromoting innovation, reproducibility, and the development of clinically\nrelevant computational pathology solutions. The dataset can be accessed at\nhttps://github.com/HistAI/HISTAI."}
{"id": "2505.12269", "pdf": "https://arxiv.org/pdf/2505.12269", "abs": "https://arxiv.org/abs/2505.12269", "authors": ["Kerry Xiao", "Amy Zang"], "title": "Vague Knowledge: Evidence from Analyst Reports", "categories": ["econ.GN", "cs.AI", "cs.CL", "math.LO", "q-fin.EC", "q-fin.GN", "03B48, 03B65, 03E02, 03E15, 03E72, 18E45, 28A05, 62F15, 68T01,\n  68T35, 68T50, 91G30,", "F.4; I.2.3; I.2.4; I.2.7; J.1; J.4; J.5"], "comment": null, "summary": "People in the real world often possess vague knowledge of future payoffs, for\nwhich quantification is not feasible or desirable. We argue that language, with\ndiffering ability to convey vague information, plays an important but less\nknown-role in subjective expectations. Empirically, we find that in their\nreports, analysts include useful information in linguistic expressions but not\nnumerical forecasts. Specifically, the textual tone of analyst reports has\npredictive power for forecast errors and subsequent revisions in numerical\nforecasts, and this relation becomes stronger when analyst's language is\nvaguer, when uncertainty is higher, and when analysts are busier. Overall, our\ntheory and evidence suggest that some useful information is vaguely known and\nonly communicated through language."}
{"id": "2505.12203", "pdf": "https://arxiv.org/pdf/2505.12203", "abs": "https://arxiv.org/abs/2505.12203", "authors": ["Zhiting Zheng", "Shuqi Wu", "Wen Ding"], "title": "CTLformer: A Hybrid Denoising Model Combining Convolutional Layers and Self-Attention for Enhanced CT Image Reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Low-dose CT (LDCT) images are often accompanied by significant noise, which\nnegatively impacts image quality and subsequent diagnostic accuracy. To address\nthe challenges of multi-scale feature fusion and diverse noise distribution\npatterns in LDCT denoising, this paper introduces an innovative model,\nCTLformer, which combines convolutional structures with transformer\narchitecture. Two key innovations are proposed: a multi-scale attention\nmechanism and a dynamic attention control mechanism. The multi-scale attention\nmechanism, implemented through the Token2Token mechanism and self-attention\ninteraction modules, effectively captures both fine details and global\nstructures at different scales, enhancing relevant features and suppressing\nnoise. The dynamic attention control mechanism adapts the attention\ndistribution based on the noise characteristics of the input image, focusing on\nhigh-noise regions while preserving details in low-noise areas, thereby\nenhancing robustness and improving denoising performance. Furthermore,\nCTLformer integrates convolutional layers for efficient feature extraction and\nuses overlapping inference to mitigate boundary artifacts, further\nstrengthening its denoising capability. Experimental results on the 2016\nNational Institutes of Health AAPM Mayo Clinic LDCT Challenge dataset\ndemonstrate that CTLformer significantly outperforms existing methods in both\ndenoising performance and model efficiency, greatly improving the quality of\nLDCT images. The proposed CTLformer not only provides an efficient solution for\nLDCT denoising but also shows broad potential in medical image analysis,\nespecially for clinical applications dealing with complex noise patterns."}
{"id": "2505.12284", "pdf": "https://arxiv.org/pdf/2505.12284", "abs": "https://arxiv.org/abs/2505.12284", "authors": ["Danlong Yuan", "Tian Xie", "Shaohan Huang", "Zhuocheng Gong", "Huishuai Zhang", "Chong Luo", "Furu Wei", "Dongyan Zhao"], "title": "Efficient RL Training for Reasoning Models via Length-Aware Optimization", "categories": ["cs.AI", "cs.CL"], "comment": "Under review", "summary": "Large reasoning models, such as OpenAI o1 or DeepSeek R1, have demonstrated\nremarkable performance on reasoning tasks but often incur a long reasoning path\nwith significant memory and time costs. Existing methods primarily aim to\nshorten reasoning paths by introducing additional training data and stages. In\nthis paper, we propose three critical reward designs integrated directly into\nthe reinforcement learning process of large reasoning models, which reduce the\nresponse length without extra training stages. Experiments on four settings\nshow that our method significantly decreases response length while maintaining\nor even improving performance. Specifically, in a logic reasoning setting, we\nachieve a 40% reduction in response length averaged by steps alongside a 14%\ngain in performance. For math problems, we reduce response length averaged by\nsteps by 33% while preserving performance."}
{"id": "2505.12233", "pdf": "https://arxiv.org/pdf/2505.12233", "abs": "https://arxiv.org/abs/2505.12233", "authors": ["Yeonkyung Lee", "Woojung Han", "Youngjun Jun", "Hyeonmin Kim", "Jungkyung Cho", "Seong Jae Hwang"], "title": "PRETI: Patient-Aware Retinal Foundation Model via Metadata-Guided Representation Learning", "categories": ["eess.IV", "cs.CV"], "comment": "MICCAI2025 early accept", "summary": "Retinal foundation models have significantly advanced retinal image analysis\nby leveraging self-supervised learning to reduce dependence on labeled data\nwhile achieving strong generalization. Many recent approaches enhance retinal\nimage understanding using report supervision, but obtaining clinical reports is\noften costly and challenging. In contrast, metadata (e.g., age, gender) is\nwidely available and serves as a valuable resource for analyzing disease\nprogression. To effectively incorporate patient-specific information, we\npropose PRETI, a retinal foundation model that integrates metadata-aware\nlearning with robust self-supervised representation learning. We introduce\nLearnable Metadata Embedding (LME), which dynamically refines metadata\nrepresentations. Additionally, we construct patient-level data pairs,\nassociating images from the same individual to improve robustness against\nnon-clinical variations. To further optimize retinal image representation, we\npropose Retina-Aware Adaptive Masking (RAAM), a strategy that selectively\napplies masking within the retinal region and dynamically adjusts the masking\nratio during training. PRETI captures both global structures and fine-grained\npathological details, resulting in superior diagnostic performance. Extensive\nexperiments demonstrate that PRETI achieves state-of-the-art results across\ndiverse diseases and biomarker predictions using in-house and public data,\nindicating the importance of metadata-guided foundation models in retinal\ndisease analysis. Our code and pretrained model are available at\nhttps://github.com/MICV-yonsei/PRETI"}
{"id": "2505.12301", "pdf": "https://arxiv.org/pdf/2505.12301", "abs": "https://arxiv.org/abs/2505.12301", "authors": ["Luyu Chen", "Zeyu Zhang", "Haoran Tan", "Quanyu Dai", "Hao Yang", "Zhenhua Dong", "Xu Chen"], "title": "Beyond Single-Point Judgment: Distribution Alignment for LLM-as-a-Judge", "categories": ["cs.AI", "cs.CL"], "comment": "19 pages, 3 tables, 3 figures", "summary": "LLMs have emerged as powerful evaluators in the LLM-as-a-Judge paradigm,\noffering significant efficiency and flexibility compared to human judgments.\nHowever, previous methods primarily rely on single-point evaluations,\noverlooking the inherent diversity and uncertainty in human evaluations. This\napproach leads to information loss and decreases the reliability of\nevaluations. To address this limitation, we propose a novel training framework\nthat explicitly aligns the LLM-generated judgment distribution with empirical\nhuman distributions. Specifically, we propose a distributional alignment\nobjective based on KL divergence, combined with an auxiliary cross-entropy\nregularization to stabilize the training process. Furthermore, considering that\nempirical distributions may derive from limited human annotations, we\nincorporate adversarial training to enhance model robustness against\ndistribution perturbations. Extensive experiments across various LLM backbones\nand evaluation tasks demonstrate that our framework significantly outperforms\nexisting closed-source LLMs and conventional single-point alignment methods,\nwith improved alignment quality, evaluation accuracy, and robustness."}
{"id": "2505.12261", "pdf": "https://arxiv.org/pdf/2505.12261", "abs": "https://arxiv.org/abs/2505.12261", "authors": ["Hanchen Wang", "Yixuan Wu", "Yinan Feng", "Peng Jin", "Shihang Feng", "Yiming Mao", "James Wiskin", "Baris Turkbey", "Peter A. Pinto", "Bradford J. Wood", "Songting Luo", "Yinpeng Chen", "Emad Boctor", "Youzuo Lin"], "title": "OpenPros: A Large-Scale Dataset for Limited View Prostate Ultrasound Computed Tomography", "categories": ["physics.med-ph", "cs.CV"], "comment": null, "summary": "Prostate cancer is one of the most common and lethal cancers among men,\nmaking its early detection critically important. Although ultrasound imaging\noffers greater accessibility and cost-effectiveness compared to MRI,\ntraditional transrectal ultrasound methods suffer from low sensitivity,\nespecially in detecting anteriorly located tumors. Ultrasound computed\ntomography provides quantitative tissue characterization, but its clinical\nimplementation faces significant challenges, particularly under anatomically\nconstrained limited-angle acquisition conditions specific to prostate imaging.\nTo address these unmet needs, we introduce OpenPros, the first large-scale\nbenchmark dataset explicitly developed for limited-view prostate USCT. Our\ndataset includes over 280,000 paired samples of realistic 2D speed-of-sound\n(SOS) phantoms and corresponding ultrasound full-waveform data, generated from\nanatomically accurate 3D digital prostate models derived from real clinical\nMRI/CT scans and ex vivo ultrasound measurements, annotated by medical experts.\nSimulations are conducted under clinically realistic configurations using\nadvanced finite-difference time-domain and Runge-Kutta acoustic wave solvers,\nboth provided as open-source components. Through comprehensive baseline\nexperiments, we demonstrate that state-of-the-art deep learning methods surpass\ntraditional physics-based approaches in both inference efficiency and\nreconstruction accuracy. Nevertheless, current deep learning models still fall\nshort of delivering clinically acceptable high-resolution images with\nsufficient accuracy. By publicly releasing OpenPros, we aim to encourage the\ndevelopment of advanced machine learning algorithms capable of bridging this\nperformance gap and producing clinically usable, high-resolution, and highly\naccurate prostate ultrasound images. The dataset is publicly accessible at\nhttps://open-pros.github.io/."}
{"id": "2505.12307", "pdf": "https://arxiv.org/pdf/2505.12307", "abs": "https://arxiv.org/abs/2505.12307", "authors": ["Maoyuan Ye", "Jing Zhang", "Juhua Liu", "Bo Du", "Dacheng Tao"], "title": "LogicOCR: Do Your Large Multimodal Models Excel at Logical Reasoning on Text-Rich Images?", "categories": ["cs.CV", "cs.CL"], "comment": "GitHub: \\url{https://github.com/MiliLab/LogicOCR}", "summary": "Recent advances in Large Multimodal Models (LMMs) have significantly improved\ntheir reasoning and Optical Character Recognition (OCR) capabilities. However,\ntheir performance on complex logical reasoning tasks involving text-rich images\nremains underexplored. To bridge this gap, we introduce LogicOCR, a benchmark\ncomprising 1,100 multiple-choice questions designed to evaluate LMMs' logical\nreasoning abilities on text-rich images, while minimizing reliance on\ndomain-specific knowledge (e.g., mathematics). We construct LogicOCR by\ncurating a text corpus from the Chinese National Civil Servant Examination and\ndevelop a scalable, automated pipeline to convert it into multimodal samples.\nFirst, we design prompt templates to steer GPT-Image-1 to generate images with\ndiverse backgrounds, interleaved text-illustration layouts, and varied fonts,\nensuring contextual relevance and visual realism. Then, the generated images\nare manually verified, with low-quality examples discarded. We evaluate a range\nof representative open-source and proprietary LMMs under both Chain-of-Thought\n(CoT) and direct-answer settings. Our multi-dimensional analysis reveals key\ninsights, such as the impact of test-time scaling, input modality differences,\nand sensitivity to visual-text orientation. Notably, LMMs still lag in\nmultimodal reasoning compared to text-only inputs, indicating that they have\nnot fully bridged visual reading with reasoning. We hope LogicOCR will serve as\na valuable resource for advancing multimodal reasoning research. The dataset is\navailable at https://github.com/MiliLab/LogicOCR."}
{"id": "2505.12278", "pdf": "https://arxiv.org/pdf/2505.12278", "abs": "https://arxiv.org/abs/2505.12278", "authors": ["Zhengyi Luo", "Chen Tessler", "Toru Lin", "Ye Yuan", "Tairan He", "Wenli Xiao", "Yunrong Guo", "Gal Chechik", "Kris Kitani", "Linxi Fan", "Yuke Zhu"], "title": "Emergent Active Perception and Dexterity of Simulated Humanoids from Visual Reinforcement Learning", "categories": ["cs.RO", "cs.CV"], "comment": "Project page: https://zhengyiluo.github.io/PDC", "summary": "Human behavior is fundamentally shaped by visual perception -- our ability to\ninteract with the world depends on actively gathering relevant information and\nadapting our movements accordingly. Behaviors like searching for objects,\nreaching, and hand-eye coordination naturally emerge from the structure of our\nsensory system. Inspired by these principles, we introduce Perceptive Dexterous\nControl (PDC), a framework for vision-driven dexterous whole-body control with\nsimulated humanoids. PDC operates solely on egocentric vision for task\nspecification, enabling object search, target placement, and skill selection\nthrough visual cues, without relying on privileged state information (e.g., 3D\nobject positions and geometries). This perception-as-interface paradigm enables\nlearning a single policy to perform multiple household tasks, including\nreaching, grasping, placing, and articulated object manipulation. We also show\nthat training from scratch with reinforcement learning can produce emergent\nbehaviors such as active search. These results demonstrate how vision-driven\ncontrol and complex tasks induce human-like behaviors and can serve as the key\ningredients in closing the perception-action loop for animation, robotics, and\nembodied AI."}
{"id": "2505.12312", "pdf": "https://arxiv.org/pdf/2505.12312", "abs": "https://arxiv.org/abs/2505.12312", "authors": ["Qi Feng", "Hidetoshi Shimodaira"], "title": "Visuospatial Cognitive Assistant", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "comment": "31 pages, 10 figures, 6 tables. The implementation and fine-tuned\n  model (ViCA-7B) are publicly available at https://huggingface.co/nkkbr/ViCA.\n  The ViCA-322K dataset can be found at\n  https://huggingface.co/datasets/nkkbr/ViCA-322K, and the ViCA-Thinking-2.68K\n  dataset is at https://huggingface.co/datasets/nkkbr/ViCA-thinking-2.68k", "summary": "Video-based spatial cognition is vital for robotics and embodied AI but\nchallenges current Vision-Language Models (VLMs). This paper makes two key\ncontributions. First, we introduce ViCA (Visuospatial Cognitive\nAssistant)-322K, a diverse dataset of 322,003 QA pairs from real-world indoor\nvideos (ARKitScenes, ScanNet, ScanNet++), offering supervision for 3D\nmetadata-grounded queries and video-based complex reasoning. Second, we develop\nViCA-7B, fine-tuned on ViCA-322K, which achieves new state-of-the-art on all\neight VSI-Bench tasks, outperforming existing models, including larger ones\n(e.g., +26.1 on Absolute Distance). For interpretability, we present\nViCA-Thinking-2.68K, a dataset with explicit reasoning chains, and fine-tune\nViCA-7B to create ViCA-7B-Thinking, a model that articulates its spatial\nreasoning. Our work highlights the importance of targeted data and suggests\npaths for improved temporal-spatial modeling. We release all resources to\nfoster research in robust visuospatial intelligence."}
{"id": "2505.12298", "pdf": "https://arxiv.org/pdf/2505.12298", "abs": "https://arxiv.org/abs/2505.12298", "authors": ["Amal Lahchim", "Lazar Davic"], "title": "Attention-Enhanced U-Net for Accurate Segmentation of COVID-19 Infected Lung Regions in CT Scans", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "14 pages, 9 figures, created using Google Colab and PyTorch. Compares\n  segmentation models for COVID-19 CT data", "summary": "In this study, we propose a robust methodology for automatic segmentation of\ninfected lung regions in COVID-19 CT scans using convolutional neural networks.\nThe approach is based on a modified U-Net architecture enhanced with attention\nmechanisms, data augmentation, and postprocessing techniques. It achieved a\nDice coefficient of 0.8658 and mean IoU of 0.8316, outperforming other methods.\nThe dataset was sourced from public repositories and augmented for diversity.\nResults demonstrate superior segmentation performance. Future work includes\nexpanding the dataset, exploring 3D segmentation, and preparing the model for\nclinical deployment."}
{"id": "2505.12363", "pdf": "https://arxiv.org/pdf/2505.12363", "abs": "https://arxiv.org/abs/2505.12363", "authors": ["Qi Feng", "Hidetoshi Shimodaira"], "title": "Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "comment": "26 pages, 19 figures, 4 tables. Code, models, and dataset are\n  available at our project page: https://github.com/nkkbr/ViCA", "summary": "While Multimodal Large Language Models (MLLMs) excel at general\nvision-language tasks, visuospatial cognition - reasoning about spatial\nlayouts, relations, and dynamics - remains a significant challenge. Existing\nmodels often lack the necessary architectural components and specialized\ntraining data for fine-grained spatial understanding. We introduce ViCA2\n(Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial\nreasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP\nfor semantics and Hiera for spatial structure, coupled with a token ratio\ncontrol mechanism for efficiency. We also developed ViCA-322K, a new\nlarge-scale dataset with over 322,000 spatially grounded question-answer pairs\nfor targeted instruction tuning. On the challenging VSI-Bench benchmark, our\nViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly\nsurpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and\nleading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the\neffectiveness of our approach in achieving strong visuospatial intelligence\nwith a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset\nto facilitate further research."}
{"id": "2505.12322", "pdf": "https://arxiv.org/pdf/2505.12322", "abs": "https://arxiv.org/abs/2505.12322", "authors": ["Ali Gholamzadeh", "Noor Sajid"], "title": "Model alignment using inter-modal bridges", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Foundation models have demonstrated remarkable performance across modalities\nsuch as language and vision. However, model reuse across distinct modalities\n(e.g., text and vision) remains limited due to the difficulty of aligning\ninternal representations. Existing methods require extensive paired training\ndata or are constrained to specific domains. We introduce a semi-supervised\napproach for model alignment via conditional flow matching. The conditional\nflow between latent spaces of different modalities (e.g., text-to-image or\nbiological-to-artificial neuronal activity) can be learned in two settings:\n($1$) solving a (balanced or unbalanced) optimal transport problem with an\ninter-space bridge cost, and ($2$) performing memory-efficient alignment using\nlabelled exemplars. Despite being constrained by the original models' capacity,\nour method--under both settings--matches downstream task performance of\nend-to-end trained models on object recognition and image generation tasks\nacross MNIST, ImageNet, and \\cite{majaj2015simple} datasets, particularly when\nlabelled training data is scarce ($<20\\%$). Our method provides a\ndata-efficient solution for inter-modal model alignment with minimal\nsupervision."}
{"id": "2505.12371", "pdf": "https://arxiv.org/pdf/2505.12371", "abs": "https://arxiv.org/abs/2505.12371", "authors": ["Yinghao Zhu", "Ziyi He", "Haoran Hu", "Xiaochen Zheng", "Xichen Zhang", "Zixiang Wang", "Junyi Gao", "Liantao Ma", "Lequan Yu"], "title": "MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional Methods for Diverse Medical Tasks", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has stimulated interest\nin multi-agent collaboration for addressing complex medical tasks. However, the\npractical advantages of multi-agent collaboration approaches remain\ninsufficiently understood. Existing evaluations often lack generalizability,\nfailing to cover diverse tasks reflective of real-world clinical practice, and\nfrequently omit rigorous comparisons against both single-LLM-based and\nestablished conventional methods. To address this critical gap, we introduce\nMedAgentBoard, a comprehensive benchmark for the systematic evaluation of\nmulti-agent collaboration, single-LLM, and conventional approaches.\nMedAgentBoard encompasses four diverse medical task categories: (1) medical\n(visual) question answering, (2) lay summary generation, (3) structured\nElectronic Health Record (EHR) predictive modeling, and (4) clinical workflow\nautomation, across text, medical images, and structured EHR data. Our extensive\nexperiments reveal a nuanced landscape: while multi-agent collaboration\ndemonstrates benefits in specific scenarios, such as enhancing task\ncompleteness in clinical workflow automation, it does not consistently\noutperform advanced single LLMs (e.g., in textual medical QA) or, critically,\nspecialized conventional methods that generally maintain better performance in\ntasks like medical VQA and EHR-based prediction. MedAgentBoard offers a vital\nresource and actionable insights, emphasizing the necessity of a task-specific,\nevidence-based approach to selecting and developing AI solutions in medicine.\nIt underscores that the inherent complexity and overhead of multi-agent\ncollaboration must be carefully weighed against tangible performance gains. All\ncode, datasets, detailed prompts, and experimental results are open-sourced at\nhttps://medagentboard.netlify.app/."}
{"id": "2505.12332", "pdf": "https://arxiv.org/pdf/2505.12332", "abs": "https://arxiv.org/abs/2505.12332", "authors": ["Qianyue Hu", "Junyan Wu", "Wei Lu", "Xiangyang Luo"], "title": "VoiceCloak: A Multi-Dimensional Defense Framework against Unauthorized Diffusion-based Voice Cloning", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.MM", "eess.AS"], "comment": null, "summary": "Diffusion Models (DMs) have achieved remarkable success in realistic voice\ncloning (VC), while they also increase the risk of malicious misuse. Existing\nproactive defenses designed for traditional VC models aim to disrupt the\nforgery process, but they have been proven incompatible with DMs due to the\nintricate generative mechanisms of diffusion. To bridge this gap, we introduce\nVoiceCloak, a multi-dimensional proactive defense framework with the goal of\nobfuscating speaker identity and degrading perceptual quality in potential\nunauthorized VC. To achieve these goals, we conduct a focused analysis to\nidentify specific vulnerabilities within DMs, allowing VoiceCloak to disrupt\nthe cloning process by introducing adversarial perturbations into the reference\naudio. Specifically, to obfuscate speaker identity, VoiceCloak first targets\nspeaker identity by distorting representation learning embeddings to maximize\nidentity variation, which is guided by auditory perception principles.\nAdditionally, VoiceCloak disrupts crucial conditional guidance processes,\nparticularly attention context, thereby preventing the alignment of vocal\ncharacteristics that are essential for achieving convincing cloning. Then, to\naddress the second objective, VoiceCloak introduces score magnitude\namplification to actively steer the reverse trajectory away from the generation\nof high-quality speech. Noise-guided semantic corruption is further employed to\ndisrupt structural speech semantics captured by DMs, degrading output quality.\nExtensive experiments highlight VoiceCloak's outstanding defense success rate\nagainst unauthorized diffusion-based voice cloning. Audio samples of VoiceCloak\nare available at https://voice-cloak.github.io/VoiceCloak/."}
{"id": "2505.12442", "pdf": "https://arxiv.org/pdf/2505.12442", "abs": "https://arxiv.org/abs/2505.12442", "authors": ["Liwen Wang", "Wenxuan Wang", "Shuai Wang", "Zongjie Li", "Zhenlan Ji", "Zongyi Lyu", "Daoyuan Wu", "Shing-Chi Cheung"], "title": "IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has led to the\nemergence of Multi-Agent Systems (MAS) to perform complex tasks through\ncollaboration. However, the intricate nature of MAS, including their\narchitecture and agent interactions, raises significant concerns regarding\nintellectual property (IP) protection. In this paper, we introduce MASLEAK, a\nnovel attack framework designed to extract sensitive information from MAS\napplications. MASLEAK targets a practical, black-box setting, where the\nadversary has no prior knowledge of the MAS architecture or agent\nconfigurations. The adversary can only interact with the MAS through its public\nAPI, submitting attack query $q$ and observing outputs from the final agent.\nInspired by how computer worms propagate and infect vulnerable network hosts,\nMASLEAK carefully crafts adversarial query $q$ to elicit, propagate, and retain\nresponses from each MAS agent that reveal a full set of proprietary components,\nincluding the number of agents, system topology, system prompts, task\ninstructions, and tool usages. We construct the first synthetic dataset of MAS\napplications with 810 applications and also evaluate MASLEAK against real-world\nMAS applications, including Coze and CrewAI. MASLEAK achieves high accuracy in\nextracting MAS IP, with an average attack success rate of 87% for system\nprompts and task instructions, and 92% for system architecture in most cases.\nWe conclude by discussing the implications of our findings and the potential\ndefenses."}
{"id": "2505.12337", "pdf": "https://arxiv.org/pdf/2505.12337", "abs": "https://arxiv.org/abs/2505.12337", "authors": ["Junlin Song", "Miguel Olivares-Mendez"], "title": "Structureless VIO", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Visual odometry (VO) is typically considered as a chicken-and-egg problem, as\nthe localization and mapping modules are tightly-coupled. The estimation of\nvisual map relies on accurate localization information. Meanwhile, localization\nrequires precise map points to provide motion constraints. This classical\ndesign principle is naturally inherited by visual-inertial odometry (VIO).\nEfficient localization solution that does not require a map has not been fully\ninvestigated. To this end, we propose a novel structureless VIO, where the\nvisual map is removed from the odometry framework. Experimental results\ndemonstrated that, compared to the structure-based VIO baseline, our\nstructureless VIO not only substantially improves computational efficiency but\nalso has advantages in accuracy."}
{"id": "2505.12457", "pdf": "https://arxiv.org/pdf/2505.12457", "abs": "https://arxiv.org/abs/2505.12457", "authors": ["Yang Zhao", "Kai Xiong", "Xiao Ding", "Li Du", "YangouOuyang", "Zhouhao Sun", "Jiannan Guan", "Wenbin Zhang", "Bin Liu", "Dong Hu", "Bing Qin", "Ting Liu"], "title": "UFO-RL: Uncertainty-Focused Optimization for Efficient Reinforcement Learning Data Selection", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Scaling RL for LLMs is computationally expensive, largely due to\nmulti-sampling for policy optimization and evaluation, making efficient data\nselection crucial. Inspired by the Zone of Proximal Development (ZPD) theory,\nwe hypothesize LLMs learn best from data within their potential comprehension\nzone. Addressing the limitation of conventional, computationally intensive\nmulti-sampling methods for data assessment, we introduce UFO-RL. This novel\nframework uses a computationally efficient single-pass uncertainty estimation\nto identify informative data instances, achieving up to 185x faster data\nevaluation. UFO-RL leverages this metric to select data within the estimated\nZPD for training. Experiments show that training with just 10% of data selected\nby UFO-RL yields performance comparable to or surpassing full-data training,\nreducing overall training time by up to 16x while enhancing stability and\ngeneralization. UFO-RL offers a practical and highly efficient strategy for\nscaling RL fine-tuning of LLMs by focusing learning on valuable data."}
{"id": "2505.12343", "pdf": "https://arxiv.org/pdf/2505.12343", "abs": "https://arxiv.org/abs/2505.12343", "authors": ["Kai Tang", "Jinhao You", "Xiuqi Ge", "Hanze Li", "Yichen Guo", "Xiande Huang"], "title": "Mitigating Hallucinations via Inter-Layer Consistency Aggregation in Large Vision-Language Models", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Despite the impressive capabilities of Large Vision-Language Models (LVLMs),\nthey remain susceptible to hallucinations-generating content that is\ninconsistent with the input image. Existing training-free hallucination\nmitigation methods often suffer from unstable performance and high sensitivity\nto hyperparameter settings, limiting their practicality and broader adoption.\nIn this paper, we propose a novel decoding mechanism, Decoding with Inter-layer\nConsistency via Layer Aggregation (DCLA), which requires no retraining,\nfine-tuning, or access to external knowledge bases. Specifically, our approach\nconstructs a dynamic semantic reference by aggregating representations from\nprevious layers, and corrects semantically deviated layers to enforce\ninter-layer consistency. The method allows DCLA to robustly mitigate\nhallucinations across multiple LVLMs. Experiments on hallucination benchmarks\nsuch as MME and POPE demonstrate that DCLA effectively reduces hallucinations\nwhile enhancing the reliability and performance of LVLMs."}
{"id": "2505.12565", "pdf": "https://arxiv.org/pdf/2505.12565", "abs": "https://arxiv.org/abs/2505.12565", "authors": ["Carl Edwards", "Chi Han", "Gawon Lee", "Thao Nguyen", "Bowen Jin", "Chetan Kumar Prasad", "Sara Szymkuć", "Bartosz A. Grzybowski", "Ying Diao", "Jiawei Han", "Ge Liu", "Hao Peng", "Martin D. Burke", "Heng Ji"], "title": "mCLM: A Function-Infused and Synthesis-Friendly Modular Chemical Language Model", "categories": ["cs.AI", "cs.CL", "cs.LG", "q-bio.QM"], "comment": null, "summary": "Despite their ability to understand chemical knowledge and accurately\ngenerate sequential representations, large language models (LLMs) remain\nlimited in their capacity to propose novel molecules with drug-like properties.\nIn addition, the molecules that LLMs propose can often be challenging to make\nin the lab. To more effectively enable the discovery of functional small\nmolecules, LLMs need to learn a molecular language. However, LLMs are currently\nlimited by encoding molecules from atoms. In this paper, we argue that just\nlike tokenizing texts into (sub-)word tokens instead of characters, molecules\nshould be decomposed and reassembled at the level of functional building\nblocks, i.e., parts of molecules that bring unique functions and serve as\neffective building blocks for real-world automated laboratory synthesis. This\nmotivates us to propose mCLM, a modular Chemical-Language Model tokenizing\nmolecules into building blocks and learning a bilingual language model of both\nnatural language descriptions of functions and molecule building blocks. By\nreasoning on such functional building blocks, mCLM guarantees to generate\nefficiently synthesizable molecules thanks to recent progress in block-based\nchemistry, while also improving the functions of molecules in a principled\nmanner. In experiments on 430 FDA-approved drugs, we find mCLM capable of\nsignificantly improving 5 out of 6 chemical functions critical to determining\ndrug potentials. More importantly, mCLM can reason on multiple functions and\nimprove the FDA-rejected drugs (``fallen angels'') over multiple iterations to\ngreatly improve their shortcomings."}
{"id": "2505.12359", "pdf": "https://arxiv.org/pdf/2505.12359", "abs": "https://arxiv.org/abs/2505.12359", "authors": ["Yichen Guo", "Hanze Li", "Zonghao Zhang", "Jinhao You", "Kai Tang", "Xiande Huang"], "title": "STAR: Stage-Wise Attention-Guided Token Reduction for Efficient Large Vision-Language Models Inference", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Although large vision-language models (LVLMs) leverage rich visual token\nrepresentations to achieve strong performance on multimodal tasks, these tokens\nalso introduce significant computational overhead during inference. Existing\ntraining-free token pruning methods typically adopt a single-stage strategy,\nfocusing either on visual self-attention or visual-textual cross-attention.\nHowever, such localized perspectives often overlook the broader information\nflow across the model, leading to substantial performance degradation,\nespecially under high pruning ratios. In this work, we propose STAR (Stage-wise\nAttention-guided token Reduction), a training-free, plug-and-play framework\nthat approaches token pruning from a global perspective. Instead of pruning at\na single point, STAR performs attention-guided reduction in two complementary\nstages: an early-stage pruning based on visual self-attention to remove\nredundant low-level features, and a later-stage pruning guided by cross-modal\nattention to discard task-irrelevant tokens. This holistic approach allows STAR\nto significantly reduce computational cost while better preserving\ntask-critical information. Extensive experiments across multiple LVLM\narchitectures and benchmarks show that STAR achieves strong acceleration while\nmaintaining comparable, and in some cases even improved performance."}
{"id": "2505.12629", "pdf": "https://arxiv.org/pdf/2505.12629", "abs": "https://arxiv.org/abs/2505.12629", "authors": ["Yuchang Sun", "Yanxi Chen", "Yaliang Li", "Bolin Ding"], "title": "Enhancing Latent Computation in Transformers with Latent Tokens", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Augmenting large language models (LLMs) with auxiliary tokens has emerged as\na promising strategy for enhancing model performance. In this work, we\nintroduce a lightweight method termed latent tokens; these are dummy tokens\nthat may be non-interpretable in natural language but steer the autoregressive\ndecoding process of a Transformer-based LLM via the attention mechanism. The\nproposed latent tokens can be seamlessly integrated with a pre-trained\nTransformer, trained in a parameter-efficient manner, and applied flexibly at\ninference time, while adding minimal complexity overhead to the existing\ninfrastructure of standard Transformers. We propose several hypotheses about\nthe underlying mechanisms of latent tokens and design synthetic tasks\naccordingly to verify them. Numerical results confirm that the proposed method\nnoticeably outperforms the baselines, particularly in the out-of-distribution\ngeneralization scenarios, highlighting its potential in improving the\nadaptability of LLMs."}
{"id": "2505.12373", "pdf": "https://arxiv.org/pdf/2505.12373", "abs": "https://arxiv.org/abs/2505.12373", "authors": ["Kapil Dev"], "title": "Modeling Aesthetic Preferences in 3D Shapes: A Large-Scale Paired Comparison Study Across Object Categories", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "11 pages, 8 figures, submitted to IEEE Transactions on Visualization\n  and Computer Graphics (TVCG)", "summary": "Human aesthetic preferences for 3D shapes are central to industrial design,\nvirtual reality, and consumer product development. However, most computational\nmodels of 3D aesthetics lack empirical grounding in large-scale human\njudgments, limiting their practical relevance. We present a large-scale study\nof human preferences. We collected 22,301 pairwise comparisons across five\nobject categories (chairs, tables, mugs, lamps, and dining chairs) via Amazon\nMechanical Turk. Building on a previously published\ndataset~\\cite{dev2020learning}, we introduce new non-linear modeling and\ncross-category analysis to uncover the geometric drivers of aesthetic\npreference. We apply the Bradley-Terry model to infer latent aesthetic scores\nand use Random Forests with SHAP analysis to identify and interpret the most\ninfluential geometric features (e.g., symmetry, curvature, compactness). Our\ncross-category analysis reveals both universal principles and domain-specific\ntrends in aesthetic preferences. We focus on human interpretable geometric\nfeatures to ensure model transparency and actionable design insights, rather\nthan relying on black-box deep learning approaches. Our findings bridge\ncomputational aesthetics and cognitive science, providing practical guidance\nfor designers and a publicly available dataset to support reproducibility. This\nwork advances the understanding of 3D shape aesthetics through a human-centric,\ndata-driven framework."}
{"id": "2505.12632", "pdf": "https://arxiv.org/pdf/2505.12632", "abs": "https://arxiv.org/abs/2505.12632", "authors": ["Yunseok Jang", "Yeda Song", "Sungryull Sohn", "Lajanugen Logeswaran", "Tiange Luo", "Dong-Ki Kim", "Kyunghoon Bae", "Honglak Lee"], "title": "Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "CVPR 2025", "summary": "Recent advancements in Large Language Models (LLMs) and Vision-Language\nModels (VLMs) have sparked significant interest in developing GUI visual\nagents. We introduce MONDAY (Mobile OS Navigation Task Dataset for Agents from\nYouTube), a large-scale dataset of 313K annotated frames from 20K instructional\nvideos capturing diverse real-world mobile OS navigation across multiple\nplatforms. Models that include MONDAY in their pre-training phases demonstrate\nrobust cross-platform generalization capabilities, consistently outperforming\nmodels trained on existing single OS datasets while achieving an average\nperformance gain of 18.11%p on an unseen mobile OS platform. To enable\ncontinuous dataset expansion as mobile platforms evolve, we present an\nautomated framework that leverages publicly available video content to create\ncomprehensive task datasets without manual annotation. Our framework comprises\nrobust OCR-based scene detection (95.04% F1score), near-perfect UI element\ndetection (99.87% hit ratio), and novel multi-step action identification to\nextract reliable action sequences across diverse interface configurations. We\ncontribute both the MONDAY dataset and our automated collection framework to\nfacilitate future research in mobile OS navigation."}
{"id": "2505.12418", "pdf": "https://arxiv.org/pdf/2505.12418", "abs": "https://arxiv.org/abs/2505.12418", "authors": ["Yuanpeng He", "Yali Bi", "Lijian Li", "Chi-Man Pun", "Wenpin Jiao", "Zhi Jin"], "title": "Mutual Evidential Deep Learning for Medical Image Segmentation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Existing semi-supervised medical segmentation co-learning frameworks have\nrealized that model performance can be diminished by the biases in model\nrecognition caused by low-quality pseudo-labels. Due to the averaging nature of\ntheir pseudo-label integration strategy, they fail to explore the reliability\nof pseudo-labels from different sources. In this paper, we propose a mutual\nevidential deep learning (MEDL) framework that offers a potentially viable\nsolution for pseudo-label generation in semi-supervised learning from two\nperspectives. First, we introduce networks with different architectures to\ngenerate complementary evidence for unlabeled samples and adopt an improved\nclass-aware evidential fusion to guide the confident synthesis of evidential\npredictions sourced from diverse architectural networks. Second, utilizing the\nuncertainty in the fused evidence, we design an asymptotic Fisher\ninformation-based evidential learning strategy. This strategy enables the model\nto initially focus on unlabeled samples with more reliable pseudo-labels,\ngradually shifting attention to samples with lower-quality pseudo-labels while\navoiding over-penalization of mislabeled classes in high data uncertainty\nsamples. Additionally, for labeled data, we continue to adopt an\nuncertainty-driven asymptotic learning strategy, gradually guiding the model to\nfocus on challenging voxels. Extensive experiments on five mainstream datasets\nhave demonstrated that MEDL achieves state-of-the-art performance."}
{"id": "2505.12680", "pdf": "https://arxiv.org/pdf/2505.12680", "abs": "https://arxiv.org/abs/2505.12680", "authors": ["Haoyu Zhao", "Yihan Geng", "Shange Tang", "Yong Lin", "Bohan Lyu", "Hongzhou Lin", "Chi Jin", "Sanjeev Arora"], "title": "Ineq-Comp: Benchmarking Human-Intuitive Compositional Reasoning in Automated Theorem Proving on Inequalities", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "27 pages", "summary": "LLM-based formal proof assistants (e.g., in Lean) hold great promise for\nautomating mathematical discovery. But beyond syntactic correctness, do these\nsystems truly understand mathematical structure as humans do? We investigate\nthis question through the lens of mathematical inequalities -- a fundamental\ntool across many domains. While modern provers can solve basic inequalities, we\nprobe their ability to handle human-intuitive compositionality. We introduce\nIneq-Comp, a benchmark built from elementary inequalities through systematic\ntransformations, including variable duplication, algebraic rewriting, and\nmulti-step composition. Although these problems remain easy for humans, we find\nthat most provers -- including Goedel, STP, and Kimina-7B -- struggle\nsignificantly. DeepSeek-Prover-V2-7B shows relative robustness -- possibly\nbecause it is trained to decompose the problems into sub-problems -- but still\nsuffers a 20\\% performance drop (pass@32). Strikingly, performance remains poor\nfor all models even when formal proofs of the constituent parts are provided in\ncontext, revealing that the source of weakness is indeed in compositional\nreasoning. Our results expose a persisting gap between the generalization\nbehavior of current AI provers and human mathematical intuition."}
{"id": "2505.12432", "pdf": "https://arxiv.org/pdf/2505.12432", "abs": "https://arxiv.org/abs/2505.12432", "authors": ["Zirun Guo", "Minjie Hong", "Tao Jin"], "title": "Observe-R1: Unlocking Reasoning Abilities of MLLMs with Dynamic Progressive Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Reinforcement Learning (RL) has shown promise in improving the reasoning\nabilities of Large Language Models (LLMs). However, the specific challenges of\nadapting RL to multimodal data and formats remain relatively unexplored. In\nthis work, we present Observe-R1, a novel framework aimed at enhancing the\nreasoning capabilities of multimodal large language models (MLLMs). We draw\ninspirations from human learning progression--from simple to complex and easy\nto difficult, and propose a gradual learning paradigm for MLLMs. To this end,\nwe construct the NeuraLadder dataset, which is organized and sampled according\nto the difficulty and complexity of data samples for RL training. To tackle\nmultimodal tasks, we introduce a multimodal format constraint that encourages\ncareful observation of images, resulting in enhanced visual abilities and\nclearer and more structured responses. Additionally, we implement a bonus\nreward system that favors concise, correct answers within a length constraint,\nalongside a dynamic weighting mechanism that prioritizes uncertain and\nmedium-difficulty problems, ensuring that more informative samples have a\ngreater impact on training. Our experiments with the Qwen2.5-VL-3B and\nQwen2.5-VL-7B models on 20k samples from the NeuraLadder dataset show that\nObserve-R1 outperforms a series of larger reasoning models on both reasoning\nand general benchmarks, achieving superior clarity and conciseness in reasoning\nchains. Ablation studies validate the effectiveness of our strategies,\nhighlighting the robustness and generalization of our approach. The dataset and\ncode will be released at https://github.com/zrguo/Observe-R1."}
{"id": "2505.12692", "pdf": "https://arxiv.org/pdf/2505.12692", "abs": "https://arxiv.org/abs/2505.12692", "authors": ["Ziwei Xu", "Udit Sanghi", "Mohan Kankanhalli"], "title": "Bullying the Machine: How Personas Increase LLM Vulnerability", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in interactions where\nthey are prompted to adopt personas. This paper investigates whether such\npersona conditioning affects model safety under bullying, an adversarial\nmanipulation that applies psychological pressures in order to force the victim\nto comply to the attacker. We introduce a simulation framework in which an\nattacker LLM engages a victim LLM using psychologically grounded bullying\ntactics, while the victim adopts personas aligned with the Big Five personality\ntraits. Experiments using multiple open-source LLMs and a wide range of\nadversarial goals reveal that certain persona configurations -- such as\nweakened agreeableness or conscientiousness -- significantly increase victim's\nsusceptibility to unsafe outputs. Bullying tactics involving emotional or\nsarcastic manipulation, such as gaslighting and ridicule, are particularly\neffective. These findings suggest that persona-driven interaction introduces a\nnovel vector for safety risks in LLMs and highlight the need for persona-aware\nsafety evaluation and alignment strategies."}
{"id": "2505.12477", "pdf": "https://arxiv.org/pdf/2505.12477", "abs": "https://arxiv.org/abs/2505.12477", "authors": ["Hugues Van Assel", "Mark Ibrahim", "Tommaso Biancalani", "Aviv Regev", "Randall Balestriero"], "title": "Joint Embedding vs Reconstruction: Provable Benefits of Latent Space Prediction for Self Supervised Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "33 pages, 9 figures", "summary": "Reconstruction and joint embedding have emerged as two leading paradigms in\nSelf Supervised Learning (SSL). Reconstruction methods focus on recovering the\noriginal sample from a different view in input space. On the other hand, joint\nembedding methods align the representations of different views in latent space.\nBoth approaches offer compelling advantages, yet practitioners lack clear\nguidelines for choosing between them. In this work, we unveil the core\nmechanisms that distinguish each paradigm. By leveraging closed form solutions\nfor both approaches, we precisely characterize how the view generation process,\ne.g. data augmentation, impacts the learned representations. We then\ndemonstrate that, unlike supervised learning, both SSL paradigms require a\nminimal alignment between augmentations and irrelevant features to achieve\nasymptotic optimality with increasing sample size. Our findings indicate that\nin scenarios where these irrelevant features have a large magnitude, joint\nembedding methods are preferable because they impose a strictly weaker\nalignment condition compared to reconstruction based methods. These results not\nonly clarify the trade offs between the two paradigms but also substantiate the\nempirical success of joint embedding approaches on real world challenging\ndatasets."}
{"id": "2505.12763", "pdf": "https://arxiv.org/pdf/2505.12763", "abs": "https://arxiv.org/abs/2505.12763", "authors": ["Sunghwan Kim", "Dongjin Kang", "Taeyoon Kwon", "Hyungjoo Chae", "Dongha Lee", "Jinyoung Yeo"], "title": "Rethinking Reward Model Evaluation Through the Lens of Reward Overoptimization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Reward models (RMs) play a crucial role in reinforcement learning from human\nfeedback (RLHF), aligning model behavior with human preferences. However,\nexisting benchmarks for reward models show a weak correlation with the\nperformance of optimized policies, suggesting that they fail to accurately\nassess the true capabilities of RMs. To bridge this gap, we explore several\nevaluation designs through the lens of reward overoptimization\\textemdash a\nphenomenon that captures both how well the reward model aligns with human\npreferences and the dynamics of the learning signal it provides to the policy.\nThe results highlight three key findings on how to construct a reliable\nbenchmark: (i) it is important to minimize differences between chosen and\nrejected responses beyond correctness, (ii) evaluating reward models requires\nmultiple comparisons across a wide range of chosen and rejected responses, and\n(iii) given that reward models encounter responses with diverse\nrepresentations, responses should be sourced from a variety of models. However,\nwe also observe that a extremely high correlation with degree of\noveroptimization leads to comparatively lower correlation with certain\ndownstream performance. Thus, when designing a benchmark, it is desirable to\nuse the degree of overoptimization as a useful tool, rather than the end goal."}
{"id": "2505.12512", "pdf": "https://arxiv.org/pdf/2505.12512", "abs": "https://arxiv.org/abs/2505.12512", "authors": ["Truman Hickok"], "title": "Scalable Strategies for Continual Learning with Replay", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Future deep learning models will be distinguished by systems that perpetually\nlearn through interaction, imagination, and cooperation, blurring the line\nbetween training and inference. This makes continual learning a critical\nchallenge, as methods that efficiently maximize bidirectional transfer across\nlearning trajectories will be essential. Replay is on track to play a\nfoundational role in continual learning, allowing models to directly reconcile\nnew information with past knowledge. In practice, however, replay is quite\nunscalable, doubling the cost of continual learning when applied naively.\nMoreover, the continual learning literature has not fully synchronized with the\nmulti-task fine-tuning literature, having not fully integrated highly scalable\ntechniques like model merging and low rank adaptation into a replay-enabled\ntoolset that can produce a unified model in the face of many sequential tasks.\nIn this paper, we begin by applying and analyzing low rank adaptation in a\ncontinual learning setting. Next, we introduce consolidation, a phasic approach\nto replay which leads to up to 55\\% less replay samples being needed for a\ngiven performance target. Then, we propose sequential merging, an offshoot of\ntask arithmetic which is tailored to the continual learning setting and is\nshown to work well in combination with replay. Finally, we demonstrate that the\ndeveloped strategies can operate synergistically, resulting in a highly\nscalable toolset that outperforms standalone variants."}
{"id": "2505.12842", "pdf": "https://arxiv.org/pdf/2505.12842", "abs": "https://arxiv.org/abs/2505.12842", "authors": ["Zheng Wu", "Pengzhou Cheng", "Zongru Wu", "Lingzhong Dong", "Zhuosheng Zhang"], "title": "GEM: Gaussian Embedding Modeling for Out-of-Distribution Detection in GUI Agents", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Graphical user interface (GUI) agents have recently emerged as an intriguing\nparadigm for human-computer interaction, capable of automatically executing\nuser instructions to operate intelligent terminal devices. However, when\nencountering out-of-distribution (OOD) instructions that violate environmental\nconstraints or exceed the current capabilities of agents, GUI agents may suffer\ntask breakdowns or even pose security threats. Therefore, effective OOD\ndetection for GUI agents is essential. Traditional OOD detection methods\nperform suboptimally in this domain due to the complex embedding space and\nevolving GUI environments. In this work, we observe that the in-distribution\ninput semantic space of GUI agents exhibits a clustering pattern with respect\nto the distance from the centroid. Based on the finding, we propose GEM, a\nnovel method based on fitting a Gaussian mixture model over input embedding\ndistances extracted from the GUI Agent that reflect its capability boundary.\nEvaluated on eight datasets spanning smartphones, computers, and web browsers,\nour method achieves an average accuracy improvement of 23.70\\% over the\nbest-performing baseline. Analysis verifies the generalization ability of our\nmethod through experiments on nine different backbones. The codes are available\nat https://github.com/Wuzheng02/GEM-OODforGUIagents."}
{"id": "2505.12552", "pdf": "https://arxiv.org/pdf/2505.12552", "abs": "https://arxiv.org/abs/2505.12552", "authors": ["Junliang Ye", "Lei Wang", "Md Zakir Hossain"], "title": "FreqSelect: Frequency-Aware fMRI-to-Image Reconstruction", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "Research report", "summary": "Reconstructing natural images from functional magnetic resonance imaging\n(fMRI) data remains a core challenge in natural decoding due to the mismatch\nbetween the richness of visual stimuli and the noisy, low resolution nature of\nfMRI signals. While recent two-stage models, combining deep variational\nautoencoders (VAEs) with diffusion models, have advanced this task, they treat\nall spatial-frequency components of the input equally. This uniform treatment\nforces the model to extract meaning features and suppress irrelevant noise\nsimultaneously, limiting its effectiveness. We introduce FreqSelect, a\nlightweight, adaptive module that selectively filters spatial-frequency bands\nbefore encoding. By dynamically emphasizing frequencies that are most\npredictive of brain activity and suppressing those that are uninformative,\nFreqSelect acts as a content-aware gate between image features and natural\ndata. It integrates seamlessly into standard very deep VAE-diffusion pipelines\nand requires no additional supervision. Evaluated on the Natural Scenes\ndataset, FreqSelect consistently improves reconstruction quality across both\nlow- and high-level metrics. Beyond performance gains, the learned\nfrequency-selection patterns offer interpretable insights into how different\nvisual frequencies are represented in the brain. Our method generalizes across\nsubjects and scenes, and holds promise for extension to other neuroimaging\nmodalities, offering a principled approach to enhancing both decoding accuracy\nand neuroscientific interpretability."}
{"id": "2505.12871", "pdf": "https://arxiv.org/pdf/2505.12871", "abs": "https://arxiv.org/abs/2505.12871", "authors": ["Zi Liang", "Haibo Hu", "Qingqing Ye", "Yaxin Xiao", "Ronghua Li"], "title": "Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "comment": "To appear at ICML 25", "summary": "Low rank adaptation (LoRA) has emerged as a prominent technique for\nfine-tuning large language models (LLMs) thanks to its superb efficiency gains\nover previous methods. While extensive studies have examined the performance\nand structural properties of LoRA, its behavior upon training-time attacks\nremain underexplored, posing significant security risks. In this paper, we\ntheoretically investigate the security implications of LoRA's low-rank\nstructure during fine-tuning, in the context of its robustness against data\npoisoning and backdoor attacks. We propose an analytical framework that models\nLoRA's training dynamics, employs the neural tangent kernel to simplify the\nanalysis of the training process, and applies information theory to establish\nconnections between LoRA's low rank structure and its vulnerability against\ntraining-time attacks. Our analysis indicates that LoRA exhibits better\nrobustness to backdoor attacks than full fine-tuning, while becomes more\nvulnerable to untargeted data poisoning due to its over-simplified information\ngeometry. Extensive experimental evaluations have corroborated our theoretical\nfindings."}
{"id": "2505.12581", "pdf": "https://arxiv.org/pdf/2505.12581", "abs": "https://arxiv.org/abs/2505.12581", "authors": ["Lucas M. Dorneles", "Luan Fonseca Garcia", "Joel Luís Carbonera"], "title": "An approach based on class activation maps for investigating the effects of data augmentation on neural networks for image classification", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Neural networks have become increasingly popular in the last few years as an\neffective tool for the task of image classification due to the impressive\nperformance they have achieved on this task. In image classification tasks, it\nis common to use data augmentation strategies to increase the robustness of\ntrained networks to changes in the input images and to avoid overfitting.\nAlthough data augmentation is a widely adopted technique, the literature lacks\na body of research analyzing the effects data augmentation methods have on the\npatterns learned by neural network models working on complex datasets. The\nprimary objective of this work is to propose a methodology and set of metrics\nthat may allow a quantitative approach to analyzing the effects of data\naugmentation in convolutional networks applied to image classification. An\nimportant tool used in the proposed approach lies in the concept of class\nactivation maps for said models, which allow us to identify and measure the\nimportance these models assign to each individual pixel in an image when\nexecuting the classification task. From these maps, we may then extract metrics\nover the similarities and differences between maps generated by these models\ntrained on a given dataset with different data augmentation strategies.\nExperiments made using this methodology suggest that the effects of these data\naugmentation techniques not only can be analyzed in this way but also allow us\nto identify different impact profiles over the trained models."}
{"id": "2505.12886", "pdf": "https://arxiv.org/pdf/2505.12886", "abs": "https://arxiv.org/abs/2505.12886", "authors": ["Zhongxiang Sun", "Qipeng Wang", "Haoyu Wang", "Xiao Zhang", "Jun Xu"], "title": "Detection and Mitigation of Hallucination in Large Reasoning Models: A Mechanistic Perspective", "categories": ["cs.AI", "cs.CL", "cs.CY"], "comment": "25 pages", "summary": "Large Reasoning Models (LRMs) have shown impressive capabilities in\nmulti-step reasoning tasks. However, alongside these successes, a more\ndeceptive form of model error has emerged--Reasoning Hallucination--where\nlogically coherent but factually incorrect reasoning traces lead to persuasive\nyet faulty conclusions. Unlike traditional hallucinations, these errors are\nembedded within structured reasoning, making them more difficult to detect and\npotentially more harmful. In this work, we investigate reasoning hallucinations\nfrom a mechanistic perspective. We propose the Reasoning Score, which\nquantifies the depth of reasoning by measuring the divergence between logits\nobtained from projecting late layers of LRMs to the vocabulary space,\neffectively distinguishing shallow pattern-matching from genuine deep\nreasoning. Using this score, we conduct an in-depth analysis on the ReTruthQA\ndataset and identify two key reasoning hallucination patterns: early-stage\nfluctuation in reasoning depth and incorrect backtracking to flawed prior\nsteps. These insights motivate our Reasoning Hallucination Detection (RHD)\nframework, which achieves state-of-the-art performance across multiple domains.\nTo mitigate reasoning hallucinations, we further introduce GRPO-R, an enhanced\nreinforcement learning algorithm that incorporates step-level deep reasoning\nrewards via potential-based shaping. Our theoretical analysis establishes\nstronger generalization guarantees, and experiments demonstrate improved\nreasoning quality and reduced hallucination rates."}
{"id": "2505.12642", "pdf": "https://arxiv.org/pdf/2505.12642", "abs": "https://arxiv.org/abs/2505.12642", "authors": ["Jung Hoon Lee", "Sujith Vijayan"], "title": "Two out of Three (ToT): using self-consistency to make robust predictions", "categories": ["cs.LG", "cs.CV"], "comment": "12 pages, 7 main figures, 1 supplementary table and 2 supplementary\n  figures", "summary": "Deep learning (DL) can automatically construct intelligent agents, deep\nneural networks (alternatively, DL models), that can outperform humans in\ncertain tasks. However, the operating principles of DL remain poorly\nunderstood, making its decisions incomprehensible. As a result, it poses a\ngreat risk to deploy DL in high-stakes domains in which mistakes or errors may\nlead to critical consequences. Here, we aim to develop an algorithm that can\nhelp DL models make more robust decisions by allowing them to abstain from\nanswering when they are uncertain. Our algorithm, named `Two out of Three\n(ToT)', is inspired by the sensitivity of the human brain to conflicting\ninformation. ToT creates two alternative predictions in addition to the\noriginal model prediction and uses the alternative predictions to decide\nwhether it should provide an answer or not."}
{"id": "2505.12891", "pdf": "https://arxiv.org/pdf/2505.12891", "abs": "https://arxiv.org/abs/2505.12891", "authors": ["Shaohang Wei", "Wei Li", "Feifan Song", "Wen Luo", "Tianyi Zhuang", "Haochen Tan", "Zhijiang Guo", "Houfeng Wang"], "title": "TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios", "categories": ["cs.AI", "cs.CL"], "comment": "First version. There are still some examples to be added into the\n  appendix", "summary": "Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehend\nthe real world. However, existing works neglect the real-world challenges for\ntemporal reasoning: (1) intensive temporal information, (2) fast-changing event\ndynamics, and (3) complex temporal dependencies in social interactions. To\nbridge this gap, we propose a multi-level benchmark TIME, designed for temporal\nreasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3\nlevels with 11 fine-grained sub-tasks. This benchmark encompasses 3\nsub-datasets reflecting different real-world challenges: TIME-Wiki, TIME-News,\nand TIME-Dial. We conduct extensive experiments on reasoning models and\nnon-reasoning models. And we conducted an in-depth analysis of temporal\nreasoning performance across diverse real-world scenarios and tasks, and\nsummarized the impact of test-time scaling on temporal reasoning capabilities.\nAdditionally, we release TIME-Lite, a human-annotated subset to foster future\nresearch and standardized evaluation in temporal reasoning. The code is\navailable at https://github.com/sylvain-wei/TIME , and the dataset is available\nat https://huggingface.co/datasets/SylvainWei/TIME ."}
{"id": "2505.12681", "pdf": "https://arxiv.org/pdf/2505.12681", "abs": "https://arxiv.org/abs/2505.12681", "authors": ["Hana Satou", "Alan Mitkiy"], "title": "On the Mechanisms of Adversarial Data Augmentation for Robust and Adaptive Transfer Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Transfer learning across domains with distribution shift remains a\nfundamental challenge in building robust and adaptable machine learning\nsystems. While adversarial perturbations are traditionally viewed as threats\nthat expose model vulnerabilities, recent studies suggest that they can also\nserve as constructive tools for data augmentation. In this work, we\nsystematically investigate the role of adversarial data augmentation (ADA) in\nenhancing both robustness and adaptivity in transfer learning settings. We\nanalyze how adversarial examples, when used strategically during training,\nimprove domain generalization by enriching decision boundaries and reducing\noverfitting to source-domain-specific features. We further propose a unified\nframework that integrates ADA with consistency regularization and\ndomain-invariant representation learning. Extensive experiments across multiple\nbenchmark datasets -- including VisDA, DomainNet, and Office-Home --\ndemonstrate that our method consistently improves target-domain performance\nunder both unsupervised and few-shot domain adaptation settings. Our results\nhighlight a constructive perspective of adversarial learning, transforming\nperturbation from a destructive attack into a regularizing force for\ncross-domain transferability."}
{"id": "2505.12900", "pdf": "https://arxiv.org/pdf/2505.12900", "abs": "https://arxiv.org/abs/2505.12900", "authors": ["Shuyang Hou", "Zhangxiao Shen", "Huayi Wu", "Jianyuan Liang", "Haoyue Jiao", "Yaxian Qing", "Xiaopu Zhang", "Xu Li", "Zhipeng Gui", "Xuefeng Guan", "Longgang Xiang"], "title": "AutoGEEval: A Multimodal and Automated Framework for Geospatial Code Generation on GEE with Large Language Models", "categories": ["cs.SE", "cs.AI", "cs.CG", "cs.CL", "cs.DB"], "comment": null, "summary": "Geospatial code generation is emerging as a key direction in the integration\nof artificial intelligence and geoscientific analysis. However, there remains a\nlack of standardized tools for automatic evaluation in this domain. To address\nthis gap, we propose AutoGEEval, the first multimodal, unit-level automated\nevaluation framework for geospatial code generation tasks on the Google Earth\nEngine (GEE) platform powered by large language models (LLMs). Built upon the\nGEE Python API, AutoGEEval establishes a benchmark suite (AutoGEEval-Bench)\ncomprising 1325 test cases that span 26 GEE data types. The framework\nintegrates both question generation and answer verification components to\nenable an end-to-end automated evaluation pipeline-from function invocation to\nexecution validation. AutoGEEval supports multidimensional quantitative\nanalysis of model outputs in terms of accuracy, resource consumption, execution\nefficiency, and error types. We evaluate 18 state-of-the-art LLMs-including\ngeneral-purpose, reasoning-augmented, code-centric, and geoscience-specialized\nmodels-revealing their performance characteristics and potential optimization\npathways in GEE code generation. This work provides a unified protocol and\nfoundational resource for the development and assessment of geospatial code\ngeneration models, advancing the frontier of automated natural language to\ndomain-specific code translation."}
{"id": "2505.12748", "pdf": "https://arxiv.org/pdf/2505.12748", "abs": "https://arxiv.org/abs/2505.12748", "authors": ["Hangyu Li", "Qin Zhao", "Haoran Xu", "Xinyu Jiang", "Qingwei Ben", "Feiyu Jia", "Haoyu Zhao", "Liang Xu", "Jia Zeng", "Hanqing Wang", "Bo Dai", "Junting Dong", "Jiangmiao Pang"], "title": "TeleOpBench: A Simulator-Centric Benchmark for Dual-Arm Dexterous Teleoperation", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "13 pages", "summary": "Teleoperation is a cornerstone of embodied-robot learning, and bimanual\ndexterous teleoperation in particular provides rich demonstrations that are\ndifficult to obtain with fully autonomous systems. While recent studies have\nproposed diverse hardware pipelines-ranging from inertial motion-capture gloves\nto exoskeletons and vision-based interfaces-there is still no unified benchmark\nthat enables fair, reproducible comparison of these systems. In this paper, we\nintroduce TeleOpBench, a simulator-centric benchmark tailored to bimanual\ndexterous teleoperation. TeleOpBench contains 30 high-fidelity task\nenvironments that span pick-and-place, tool use, and collaborative\nmanipulation, covering a broad spectrum of kinematic and force-interaction\ndifficulty. Within this benchmark we implement four representative\nteleoperation modalities-(i) MoCap, (ii) VR device, (iii) arm-hand\nexoskeletons, and (iv) monocular vision tracking-and evaluate them with a\ncommon protocol and metric suite. To validate that performance in simulation is\npredictive of real-world behavior, we conduct mirrored experiments on a\nphysical dual-arm platform equipped with two 6-DoF dexterous hands. Across 10\nheld-out tasks we observe a strong correlation between simulator and hardware\nperformance, confirming the external validity of TeleOpBench. TeleOpBench\nestablishes a common yardstick for teleoperation research and provides an\nextensible platform for future algorithmic and hardware innovation."}
{"id": "2505.12938", "pdf": "https://arxiv.org/pdf/2505.12938", "abs": "https://arxiv.org/abs/2505.12938", "authors": ["Uri Dalal", "Meirav Segal", "Zvika Ben-Haim", "Dan Lahav", "Omer Nevo"], "title": "Leveraging LLM Inconsistency to Boost Pass@k Performance", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) achieve impressive abilities in numerous\ndomains, but exhibit inconsistent performance in response to minor input\nchanges. Rather than view this as a drawback, in this paper we introduce a\nnovel method for leveraging models' inconsistency to boost Pass@k performance.\nSpecifically, we present a \"Variator\" agent that generates k variants of a\ngiven task and submits one candidate solution for each one. Our variant\ngeneration approach is applicable to a wide range of domains as it is task\nagnostic and compatible with free-form inputs. We demonstrate the efficacy of\nour agent theoretically using a probabilistic model of the inconsistency\neffect, and show empirically that it outperforms the baseline on the APPS\ndataset. Furthermore, we establish that inconsistency persists even in frontier\nreasoning models across coding and cybersecurity domains, suggesting our method\nis likely to remain relevant for future model generations."}
{"id": "2505.12751", "pdf": "https://arxiv.org/pdf/2505.12751", "abs": "https://arxiv.org/abs/2505.12751", "authors": ["Filippo Leveni"], "title": "Structure-based Anomaly Detection and Clustering", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "Doctoral dissertation at Politecnico di Milano", "summary": "Anomaly detection is a fundamental problem in domains such as healthcare,\nmanufacturing, and cybersecurity. This thesis proposes new unsupervised methods\nfor anomaly detection in both structured and streaming data settings. In the\nfirst part, we focus on structure-based anomaly detection, where normal data\nfollows low-dimensional manifolds while anomalies deviate from them. We\nintroduce Preference Isolation Forest (PIF), which embeds data into a\nhigh-dimensional preference space via manifold fitting, and isolates outliers\nusing two variants: Voronoi-iForest, based on geometric distances, and\nRuzHash-iForest, leveraging Locality Sensitive Hashing for scalability. We also\npropose Sliding-PIF, which captures local manifold information for streaming\nscenarios. Our methods outperform existing techniques on synthetic and real\ndatasets. We extend this to structure-based clustering with MultiLink, a novel\nmethod for recovering multiple geometric model families in noisy data.\nMultiLink merges clusters via a model-aware linkage strategy, enabling robust\nmulti-class structure recovery. It offers key advantages over existing\napproaches, such as speed, reduced sensitivity to thresholds, and improved\nrobustness to poor initial sampling. The second part of the thesis addresses\nonline anomaly detection in evolving data streams. We propose Online Isolation\nForest (Online-iForest), which uses adaptive, multi-resolution histograms and\ndynamically updates tree structures to track changes over time. It avoids\nretraining while achieving accuracy comparable to offline models, with superior\nefficiency for real-time applications. Finally, we tackle anomaly detection in\ncybersecurity via open-set recognition for malware classification. We enhance a\nGradient Boosting classifier with MaxLogit to detect unseen malware families, a\nmethod now integrated into Cleafy's production system."}
{"id": "2505.12992", "pdf": "https://arxiv.org/pdf/2505.12992", "abs": "https://arxiv.org/abs/2505.12992", "authors": ["Baohao Liao", "Hanze Dong", "Yuhui Xu", "Doyen Sahoo", "Christof Monz", "Junnan Li", "Caiming Xiong"], "title": "Fractured Chain-of-Thought Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Inference-time scaling techniques have significantly bolstered the reasoning\ncapabilities of large language models (LLMs) by harnessing additional\ncomputational effort at inference without retraining. Similarly,\nChain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy\nby generating rich intermediate reasoning trajectories, but these approaches\nincur substantial token costs that impede their deployment in latency-sensitive\nsettings. In this work, we first show that truncated CoT, which stops reasoning\nbefore completion and directly generates the final answer, often matches full\nCoT sampling while using dramatically fewer tokens. Building on this insight,\nwe introduce Fractured Sampling, a unified inference-time strategy that\ninterpolates between full CoT and solution-only sampling along three orthogonal\naxes: (1) the number of reasoning trajectories, (2) the number of final\nsolutions per trajectory, and (3) the depth at which reasoning traces are\ntruncated. Through extensive experiments on five diverse reasoning benchmarks\nand several model scales, we demonstrate that Fractured Sampling consistently\nachieves superior accuracy-cost trade-offs, yielding steep log-linear scaling\ngains in Pass@k versus token budget. Our analysis reveals how to allocate\ncomputation across these dimensions to maximize performance, paving the way for\nmore efficient and scalable LLM reasoning."}
{"id": "2505.12774", "pdf": "https://arxiv.org/pdf/2505.12774", "abs": "https://arxiv.org/abs/2505.12774", "authors": ["Zichen Geng", "Zeeshan Hayder", "Wei Liu", "Ajmal Mian"], "title": "UniHM: Universal Human Motion Generation with Object Interactions in Indoor Scenes", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": null, "summary": "Human motion synthesis in complex scenes presents a fundamental challenge,\nextending beyond conventional Text-to-Motion tasks by requiring the integration\nof diverse modalities such as static environments, movable objects, natural\nlanguage prompts, and spatial waypoints. Existing language-conditioned motion\nmodels often struggle with scene-aware motion generation due to limitations in\nmotion tokenization, which leads to information loss and fails to capture the\ncontinuous, context-dependent nature of 3D human movement. To address these\nissues, we propose UniHM, a unified motion language model that leverages\ndiffusion-based generation for synthesizing scene-aware human motion. UniHM is\nthe first framework to support both Text-to-Motion and Text-to-Human-Object\nInteraction (HOI) in complex 3D scenes. Our approach introduces three key\ncontributions: (1) a mixed-motion representation that fuses continuous 6DoF\nmotion with discrete local motion tokens to improve motion realism; (2) a novel\nLook-Up-Free Quantization VAE (LFQ-VAE) that surpasses traditional VQ-VAEs in\nboth reconstruction accuracy and generative performance; and (3) an enriched\nversion of the Lingo dataset augmented with HumanML3D annotations, providing\nstronger supervision for scene-specific motion learning. Experimental results\ndemonstrate that UniHM achieves comparative performance on the OMOMO benchmark\nfor text-to-HOI synthesis and yields competitive results on HumanML3D for\ngeneral text-conditioned motion generation."}
{"id": "2505.13028", "pdf": "https://arxiv.org/pdf/2505.13028", "abs": "https://arxiv.org/abs/2505.13028", "authors": ["Sayon Palit", "Daniel Woods"], "title": "Evaluatiing the efficacy of LLM Safety Solutions : The Palit Benchmark Dataset", "categories": ["cs.CR", "cs.AI", "cs.CL", "F.2.2, I.2.7; F.2.2, I.2.7; F.2.2, I.2.7"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly integrated into critical\nsystems in industries like healthcare and finance. Users can often submit\nqueries to LLM-enabled chatbots, some of which can enrich responses with\ninformation retrieved from internal databases storing sensitive data. This\ngives rise to a range of attacks in which a user submits a malicious query and\nthe LLM-system outputs a response that creates harm to the owner, such as\nleaking internal data or creating legal liability by harming a third-party.\nWhile security tools are being developed to counter these threats, there is\nlittle formal evaluation of their effectiveness and usability. This study\naddresses this gap by conducting a thorough comparative analysis of LLM\nsecurity tools. We identified 13 solutions (9 closed-source, 4 open-source),\nbut only 7 were evaluated due to a lack of participation by proprietary model\nowners.To evaluate, we built a benchmark dataset of malicious prompts, and\nevaluate these tools performance against a baseline LLM model\n(ChatGPT-3.5-Turbo). Our results show that the baseline model has too many\nfalse positives to be used for this task. Lakera Guard and ProtectAI LLM Guard\nemerged as the best overall tools showcasing the tradeoff between usability and\nperformance. The study concluded with recommendations for greater transparency\namong closed source providers, improved context-aware detections, enhanced\nopen-source engagement, increased user awareness, and the adoption of more\nrepresentative performance metrics."}
{"id": "2505.12782", "pdf": "https://arxiv.org/pdf/2505.12782", "abs": "https://arxiv.org/abs/2505.12782", "authors": ["Kai Zhang", "Xingyu Chen", "Xiaofeng Zhang"], "title": "AdaToken-3D: Dynamic Spatial Gating for Efficient 3D Large Multimodal-Models Reasoning", "categories": ["cs.GR", "cs.CV", "cs.IR", "cs.IT", "math.IT"], "comment": null, "summary": "Large Multimodal Models (LMMs) have become a pivotal research focus in deep\nlearning, demonstrating remarkable capabilities in 3D scene understanding.\nHowever, current 3D LMMs employing thousands of spatial tokens for multimodal\nreasoning suffer from critical inefficiencies: excessive computational overhead\nand redundant information flows. Unlike 2D VLMs processing single images, 3D\nLMMs exhibit inherent architectural redundancy due to the heterogeneous\nmechanisms between spatial tokens and visual tokens. To address this challenge,\nwe propose AdaToken-3D, an adaptive spatial token optimization framework that\ndynamically prunes redundant tokens through spatial contribution analysis. Our\nmethod automatically tailors pruning strategies to different 3D LMM\narchitectures by quantifying token-level information flows via attention\npattern mining. Extensive experiments on LLaVA-3D (a 7B parameter 3D-LMM)\ndemonstrate that AdaToken-3D achieves 21\\% faster inference speed and 63\\%\nFLOPs reduction while maintaining original task accuracy. Beyond efficiency\ngains, this work systematically investigates redundancy patterns in multimodal\nspatial information flows through quantitative token interaction analysis. Our\nfindings reveal that over 60\\% of spatial tokens contribute minimally ($<$5\\%)\nto the final predictions, establishing theoretical foundations for efficient 3D\nmultimodal learning."}
{"id": "2505.13032", "pdf": "https://arxiv.org/pdf/2505.13032", "abs": "https://arxiv.org/abs/2505.13032", "authors": ["Ziyang Ma", "Yinghao Ma", "Yanqiao Zhu", "Chen Yang", "Yi-Wen Chao", "Ruiyang Xu", "Wenxi Chen", "Yuanzhe Chen", "Zhuo Chen", "Jian Cong", "Kai Li", "Keliang Li", "Siyou Li", "Xinfeng Li", "Xiquan Li", "Zheng Lian", "Yuzhe Liang", "Minghao Liu", "Zhikang Niu", "Tianrui Wang", "Yuping Wang", "Yuxuan Wang", "Yihao Wu", "Guanrou Yang", "Jianwei Yu", "Ruibin Yuan", "Zhisheng Zheng", "Ziya Zhou", "Haina Zhu", "Wei Xue", "Emmanouil Benetos", "Kai Yu", "Eng-Siong Chng", "Xie Chen"], "title": "MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "comment": "Open-source at https://github.com/ddlBoJack/MMAR", "summary": "We introduce MMAR, a new benchmark designed to evaluate the deep reasoning\ncapabilities of Audio-Language Models (ALMs) across massive multi-disciplinary\ntasks. MMAR comprises 1,000 meticulously curated audio-question-answer\ntriplets, collected from real-world internet videos and refined through\niterative error corrections and quality checks to ensure high quality. Unlike\nexisting benchmarks that are limited to specific domains of sound, music, or\nspeech, MMAR extends them to a broad spectrum of real-world audio scenarios,\nincluding mixed-modality combinations of sound, music, and speech. Each\nquestion in MMAR is hierarchically categorized across four reasoning layers:\nSignal, Perception, Semantic, and Cultural, with additional sub-categories\nwithin each layer to reflect task diversity and complexity. To further foster\nresearch in this area, we annotate every question with a Chain-of-Thought (CoT)\nrationale to promote future advancements in audio reasoning. Each item in the\nbenchmark demands multi-step deep reasoning beyond surface-level understanding.\nMoreover, a part of the questions requires graduate-level perceptual and\ndomain-specific knowledge, elevating the benchmark's difficulty and depth. We\nevaluate MMAR using a broad set of models, including Large Audio-Language\nModels (LALMs), Large Audio Reasoning Models (LARMs), Omni Language Models\n(OLMs), Large Language Models (LLMs), and Large Reasoning Models (LRMs), with\naudio caption inputs. The performance of these models on MMAR highlights the\nbenchmark's challenging nature, and our analysis further reveals critical\nlimitations of understanding and reasoning capabilities among current models.\nWe hope MMAR will serve as a catalyst for future advances in this important but\nlittle-explored area."}
{"id": "2505.12835", "pdf": "https://arxiv.org/pdf/2505.12835", "abs": "https://arxiv.org/abs/2505.12835", "authors": ["Hengxing Cai", "Jinhan Dong", "Jingjun Tan", "Jingcheng Deng", "Sihang Li", "Zhifeng Gao", "Haidong Wang", "Zicheng Su", "Agachai Sumalee", "Renxin Zhong"], "title": "FlightGPT: Towards Generalizable and Interpretable UAV Vision-and-Language Navigation with Vision-Language Models", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Unmanned Aerial Vehicle (UAV) Vision-and-Language Navigation (VLN) is vital\nfor applications such as disaster response, logistics delivery, and urban\ninspection. However, existing methods often struggle with insufficient\nmultimodal fusion, weak generalization, and poor interpretability. To address\nthese challenges, we propose FlightGPT, a novel UAV VLN framework built upon\nVision-Language Models (VLMs) with powerful multimodal perception capabilities.\nWe design a two-stage training pipeline: first, Supervised Fine-Tuning (SFT)\nusing high-quality demonstrations to improve initialization and structured\nreasoning; then, Group Relative Policy Optimization (GRPO) algorithm, guided by\na composite reward that considers goal accuracy, reasoning quality, and format\ncompliance, to enhance generalization and adaptability. Furthermore, FlightGPT\nintroduces a Chain-of-Thought (CoT)-based reasoning mechanism to improve\ndecision interpretability. Extensive experiments on the city-scale dataset\nCityNav demonstrate that FlightGPT achieves state-of-the-art performance across\nall scenarios, with a 9.22\\% higher success rate than the strongest baseline in\nunseen environments. Our implementation is publicly available."}
{"id": "2505.13098", "pdf": "https://arxiv.org/pdf/2505.13098", "abs": "https://arxiv.org/abs/2505.13098", "authors": ["Lars-Peter Meyer", "Johannes Frey", "Desiree Heim", "Felix Brei", "Claus Stadler", "Kurt Junghanns", "Michael Martin"], "title": "LLM-KG-Bench 3.0: A Compass for SemanticTechnology Capabilities in the Ocean of LLMs", "categories": ["cs.AI", "cs.CL", "cs.DB"], "comment": "Peer reviewed publication at ESWC 2025 Resources Track", "summary": "Current Large Language Models (LLMs) can assist developing program code\nbeside many other things, but can they support working with Knowledge Graphs\n(KGs) as well? Which LLM is offering the best capabilities in the field of\nSemantic Web and Knowledge Graph Engineering (KGE)? Is this possible to\ndetermine without checking many answers manually? The LLM-KG-Bench framework in\nVersion 3.0 is designed to answer these questions. It consists of an extensible\nset of tasks for automated evaluation of LLM answers and covers different\naspects of working with semantic technologies. In this paper the LLM-KG-Bench\nframework is presented in Version 3 along with a dataset of prompts, answers\nand evaluations generated with it and several state-of-the-art LLMs.\nSignificant enhancements have been made to the framework since its initial\nrelease, including an updated task API that offers greater flexibility in\nhandling evaluation tasks, revised tasks, and extended support for various open\nmodels through the vllm library, among other improvements. A comprehensive\ndataset has been generated using more than 30 contemporary open and proprietary\nLLMs, enabling the creation of exemplary model cards that demonstrate the\nmodels' capabilities in working with RDF and SPARQL, as well as comparing their\nperformance on Turtle and JSON-LD RDF serialization tasks."}
{"id": "2505.12836", "pdf": "https://arxiv.org/pdf/2505.12836", "abs": "https://arxiv.org/abs/2505.12836", "authors": ["Muhamed Kuric", "Martin Zach", "Andreas Habring", "Michael Unser", "Thomas Pock"], "title": "The Gaussian Latent Machine: Efficient Prior and Posterior Sampling for Inverse Problems", "categories": ["eess.IV", "cs.CV", "cs.LG", "stat.ML", "65C40, 65C05, 68U10, 65C60"], "comment": null, "summary": "We consider the problem of sampling from a product-of-experts-type model that\nencompasses many standard prior and posterior distributions commonly found in\nBayesian imaging. We show that this model can be easily lifted into a novel\nlatent variable model, which we refer to as a Gaussian latent machine. This\nleads to a general sampling approach that unifies and generalizes many existing\nsampling algorithms in the literature. Most notably, it yields a highly\nefficient and effective two-block Gibbs sampling approach in the general case,\nwhile also specializing to direct sampling algorithms in particular cases.\nFinally, we present detailed numerical experiments that demonstrate the\nefficiency and effectiveness of our proposed sampling approach across a wide\nrange of prior and posterior sampling problems from Bayesian imaging."}
{"id": "2505.13109", "pdf": "https://arxiv.org/pdf/2505.13109", "abs": "https://arxiv.org/abs/2505.13109", "authors": ["Guangda Liu", "Chengwei Li", "Zhenyu Ning", "Jing Lin", "Yiwu Yao", "Danning Ke", "Minyi Guo", "Jieru Zhao"], "title": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods."}
{"id": "2505.12863", "pdf": "https://arxiv.org/pdf/2505.12863", "abs": "https://arxiv.org/abs/2505.12863", "authors": ["Jongmin Jung", "Dongmin Kim", "Sihun Lee", "Seola Cho", "Hyungjoon Soh", "Irmak Bukey", "Chris Donahue", "Dasaem Jeong"], "title": "Unified Cross-modal Translation of Score Images, Symbolic Music, and Performance Audio", "categories": ["cs.SD", "cs.AI", "cs.CV", "eess.AS"], "comment": "Submitted to IEEE Transactions on Audio, Speech and Language\n  Processing (TASLPRO)", "summary": "Music exists in various modalities, such as score images, symbolic scores,\nMIDI, and audio. Translations between each modality are established as core\ntasks of music information retrieval, such as automatic music transcription\n(audio-to-MIDI) and optical music recognition (score image to symbolic score).\nHowever, most past work on multimodal translation trains specialized models on\nindividual translation tasks. In this paper, we propose a unified approach,\nwhere we train a general-purpose model on many translation tasks\nsimultaneously. Two key factors make this unified approach viable: a new\nlarge-scale dataset and the tokenization of each modality. Firstly, we propose\na new dataset that consists of more than 1,300 hours of paired audio-score\nimage data collected from YouTube videos, which is an order of magnitude larger\nthan any existing music modal translation datasets. Secondly, our unified\ntokenization framework discretizes score images, audio, MIDI, and MusicXML into\na sequence of tokens, enabling a single encoder-decoder Transformer to tackle\nmultiple cross-modal translation as one coherent sequence-to-sequence task.\nExperimental results confirm that our unified multitask model improves upon\nsingle-task baselines in several key areas, notably reducing the symbol error\nrate for optical music recognition from 24.58% to a state-of-the-art 13.67%,\nwhile similarly substantial improvements are observed across the other\ntranslation tasks. Notably, our approach achieves the first successful\nscore-image-conditioned audio generation, marking a significant breakthrough in\ncross-modal music generation."}
{"id": "2505.13126", "pdf": "https://arxiv.org/pdf/2505.13126", "abs": "https://arxiv.org/abs/2505.13126", "authors": ["Liancheng Gong", "Wang Zhu", "Jesse Thomason", "Li Zhang"], "title": "Zero-Shot Iterative Formalization and Planning in Partially Observable Environments", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "In planning, using LLMs not to predict plans but to formalize an environment\ninto the Planning Domain Definition Language (PDDL) has been shown to greatly\nimprove performance and control. While most work focused on fully observable\nenvironments, we tackle the more realistic and challenging partially observable\nenvironments where existing methods are incapacitated by the lack of complete\ninformation. We propose PDDLego+, a framework to iteratively formalize, plan,\ngrow, and refine PDDL representations in a zero-shot manner, without needing\naccess to any existing trajectories. On two textual simulated environments, we\nshow that PDDLego+ not only achieves superior performance, but also shows\nrobustness against problem complexity. We also show that the domain knowledge\ncaptured after a successful trial is interpretable and benefits future tasks."}
{"id": "2505.12884", "pdf": "https://arxiv.org/pdf/2505.12884", "abs": "https://arxiv.org/abs/2505.12884", "authors": ["Yuanze Hu", "Zhaoxin Fan", "Xinyu Wang", "Gen Li", "Ye Qiu", "Zhichao Yang", "Wenjun Wu", "Kejian Wu", "Yifan Sun", "Xiaotie Deng", "Jin Dong"], "title": "TinyAlign: Boosting Lightweight Vision-Language Models by Mitigating Modal Alignment Bottlenecks", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Lightweight Vision-Language Models (VLMs) are indispensable for\nresource-constrained applications. The prevailing approach to aligning vision\nand language models involves freezing both the vision encoder and the language\nmodel while training small connector modules. However, this strategy heavily\ndepends on the intrinsic capabilities of the language model, which can be\nsuboptimal for lightweight models with limited representational capacity. In\nthis work, we investigate this alignment bottleneck through the lens of mutual\ninformation, demonstrating that the constrained capacity of the language model\ninherently limits the Effective Mutual Information (EMI) between multimodal\ninputs and outputs, thereby compromising alignment quality. To address this\nchallenge, we propose TinyAlign, a novel framework inspired by\nRetrieval-Augmented Generation, which strategically retrieves relevant context\nfrom a memory bank to enrich multimodal inputs and enhance their alignment.\nExtensive empirical evaluations reveal that TinyAlign significantly reduces\ntraining loss, accelerates convergence, and enhances task performance.\nRemarkably, it allows models to achieve baseline-level performance with only\n40\\% of the fine-tuning data, highlighting exceptional data efficiency. Our\nwork thus offers a practical pathway for developing more capable lightweight\nVLMs while introducing a fresh theoretical lens to better understand and\naddress alignment bottlenecks in constrained multimodal systems."}
{"id": "2505.13208", "pdf": "https://arxiv.org/pdf/2505.13208", "abs": "https://arxiv.org/abs/2505.13208", "authors": ["Colin Krawchuk", "Nikhil Khatri", "Neil John Ortega", "Dimitri Kartsaklis"], "title": "Efficient Generation of Parameterised Quantum Circuits from Large Texts", "categories": ["quant-ph", "cs.AI", "cs.CL"], "comment": null, "summary": "Quantum approaches to natural language processing (NLP) are redefining how\nlinguistic information is represented and processed. While traditional hybrid\nquantum-classical models rely heavily on classical neural networks, recent\nadvancements propose a novel framework, DisCoCirc, capable of directly encoding\nentire documents as parameterised quantum circuits (PQCs), besides enjoying\nsome additional interpretability and compositionality benefits. Following these\nideas, this paper introduces an efficient methodology for converting\nlarge-scale texts into quantum circuits using tree-like representations of\npregroup diagrams. Exploiting the compositional parallels between language and\nquantum mechanics, grounded in symmetric monoidal categories, our approach\nenables faithful and efficient encoding of syntactic and discourse\nrelationships in long and complex texts (up to 6410 words in our experiments)\nto quantum circuits. The developed system is provided to the community as part\nof the augmented open-source quantum NLP package lambeq Gen II."}
{"id": "2505.12887", "pdf": "https://arxiv.org/pdf/2505.12887", "abs": "https://arxiv.org/abs/2505.12887", "authors": ["Junzhi Ning", "Cheng Tang", "Kaijin Zhou", "Diping Song", "Lihao Liu", "Ming Hu", "Wei Li", "Yanzhou Su", "Tianbing Li", "Jiyao Liu", "Yejin", "Sheng Zhang", "Yuanfeng Ji", "Junjun He"], "title": "RetinaLogos: Fine-Grained Synthesis of High-Resolution Retinal Images Through Captions", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "The scarcity of high-quality, labelled retinal imaging data, which presents a\nsignificant challenge in the development of machine learning models for\nophthalmology, hinders progress in the field. To synthesise Colour Fundus\nPhotographs (CFPs), existing methods primarily relying on predefined disease\nlabels face significant limitations. However, current methods remain limited,\nthus failing to generate images for broader categories with diverse and\nfine-grained anatomical structures. To overcome these challenges, we first\nintroduce an innovative pipeline that creates a large-scale, synthetic\nCaption-CFP dataset comprising 1.4 million entries, called RetinaLogos-1400k.\nSpecifically, RetinaLogos-1400k uses large language models (LLMs) to describe\nretinal conditions and key structures, such as optic disc configuration,\nvascular distribution, nerve fibre layers, and pathological features.\nFurthermore, based on this dataset, we employ a novel three-step training\nframework, called RetinaLogos, which enables fine-grained semantic control over\nretinal images and accurately captures different stages of disease progression,\nsubtle anatomical variations, and specific lesion types. Extensive experiments\ndemonstrate state-of-the-art performance across multiple datasets, with 62.07%\nof text-driven synthetic images indistinguishable from real ones by\nophthalmologists. Moreover, the synthetic data improves accuracy by 10%-25% in\ndiabetic retinopathy grading and glaucoma detection, thereby providing a\nscalable solution to augment ophthalmic datasets."}
{"id": "2505.13227", "pdf": "https://arxiv.org/pdf/2505.13227", "abs": "https://arxiv.org/abs/2505.13227", "authors": ["Tianbao Xie", "Jiaqi Deng", "Xiaochuan Li", "Junlin Yang", "Haoyuan Wu", "Jixuan Chen", "Wenjing Hu", "Xinyuan Wang", "Yuhui Xu", "Zekun Wang", "Yiheng Xu", "Junli Wang", "Doyen Sahoo", "Tao Yu", "Caiming Xiong"], "title": "Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": "49 pages, 13 figures", "summary": "Graphical user interface (GUI) grounding, the ability to map natural language\ninstructions to specific actions on graphical user interfaces, remains a\ncritical bottleneck in computer use agent development. Current benchmarks\noversimplify grounding tasks as short referring expressions, failing to capture\nthe complexity of real-world interactions that require software commonsense,\nlayout understanding, and fine-grained manipulation capabilities. To address\nthese limitations, we introduce OSWorld-G, a comprehensive benchmark comprising\n564 finely annotated samples across diverse task types including text matching,\nelement recognition, layout understanding, and precise manipulation.\nAdditionally, we synthesize and release the largest computer use grounding\ndataset Jedi, which contains 4 million examples through multi-perspective\ndecoupling of tasks. Our multi-scale models trained on Jedi demonstrate its\neffectiveness by outperforming existing approaches on ScreenSpot-v2,\nScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved\ngrounding with Jedi directly enhances agentic capabilities of general\nfoundation models on complex computer tasks, improving from 5% to 27% on\nOSWorld. Through detailed ablation studies, we identify key factors\ncontributing to grounding performance and verify that combining specialized\ndata for different interface elements enables compositional generalization to\nnovel interfaces. All benchmark, data, checkpoints, and code are open-sourced\nand available at https://osworld-grounding.github.io."}
{"id": "2505.12944", "pdf": "https://arxiv.org/pdf/2505.12944", "abs": "https://arxiv.org/abs/2505.12944", "authors": ["Jan Hagnberger", "Daniel Musekamp", "Mathias Niepert"], "title": "CALM-PDE: Continuous and Adaptive Convolutions for Latent Space Modeling of Time-dependent PDEs", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.NE", "physics.comp-ph"], "comment": null, "summary": "Solving time-dependent Partial Differential Equations (PDEs) using a densely\ndiscretized spatial domain is a fundamental problem in various scientific and\nengineering disciplines, including modeling climate phenomena and fluid\ndynamics. However, performing these computations directly in the physical space\noften incurs significant computational costs. To address this issue, several\nneural surrogate models have been developed that operate in a compressed latent\nspace to solve the PDE. While these approaches reduce computational complexity,\nthey often use Transformer-based attention mechanisms to handle irregularly\nsampled domains, resulting in increased memory consumption. In contrast,\nconvolutional neural networks allow memory-efficient encoding and decoding but\nare limited to regular discretizations. Motivated by these considerations, we\npropose CALM-PDE, a model class that efficiently solves arbitrarily discretized\nPDEs in a compressed latent space. We introduce a novel continuous\nconvolution-based encoder-decoder architecture that uses an\nepsilon-neighborhood-constrained kernel and learns to apply the convolution\noperator to adaptive and optimized query points. We demonstrate the\neffectiveness of CALM-PDE on a diverse set of PDEs with both regularly and\nirregularly sampled spatial domains. CALM-PDE is competitive with or\noutperforms existing baseline methods while offering significant improvements\nin memory and inference time efficiency compared to Transformer-based methods."}
{"id": "2505.13237", "pdf": "https://arxiv.org/pdf/2505.13237", "abs": "https://arxiv.org/abs/2505.13237", "authors": ["Chih-Kai Yang", "Neo Ho", "Yen-Ting Piao", "Hung-yi Lee"], "title": "SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based on Speech and Audio Information", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted to Interspeech 2025", "summary": "Large audio-language models (LALMs) extend the large language models with\nmultimodal understanding in speech, audio, etc. While their performances on\nspeech and audio-processing tasks are extensively studied, their reasoning\nabilities remain underexplored. Particularly, their multi-hop reasoning, the\nability to recall and integrate multiple facts, lacks systematic evaluation.\nExisting benchmarks focus on general speech and audio-processing tasks,\nconversational abilities, and fairness but overlook this aspect. To bridge this\ngap, we introduce SAKURA, a benchmark assessing LALMs' multi-hop reasoning\nbased on speech and audio information. Results show that LALMs struggle to\nintegrate speech/audio representations for multi-hop reasoning, even when they\nextract the relevant information correctly, highlighting a fundamental\nchallenge in multimodal reasoning. Our findings expose a critical limitation in\nLALMs, offering insights and resources for future research."}
{"id": "2505.12963", "pdf": "https://arxiv.org/pdf/2505.12963", "abs": "https://arxiv.org/abs/2505.12963", "authors": ["Maksim I. Ivanov", "Olga E. Mendybaeva", "Yuri E. Karyakin", "Igor N. Glukhikh", "Aleksey V. Lebedev"], "title": "Segmentation of temporomandibular joint structures on mri images using neural networks for diagnosis of pathologies", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "10 pages, 10 figures", "summary": "This article explores the use of artificial intelligence for the diagnosis of\npathologies of the temporomandibular joint (TMJ), in particular, for the\nsegmentation of the articular disc on MRI images. The relevance of the work is\ndue to the high prevalence of TMJ pathologies, as well as the need to improve\nthe accuracy and speed of diagnosis in medical institutions. During the study,\nthe existing solutions (Diagnocat, MandSeg) were analyzed, which, as a result,\nare not suitable for studying the articular disc due to the orientation towards\nbone structures. To solve the problem, an original dataset was collected from\n94 images with the classes \"temporomandibular joint\" and \"jaw\". To increase the\namount of data, augmentation methods were used. After that, the models of\nU-Net, YOLOv8n, YOLOv11n and Roboflow neural networks were trained and\ncompared. The evaluation was carried out according to the Dice Score,\nPrecision, Sensitivity, Specificity, and Mean Average Precision metrics. The\nresults confirm the potential of using the Roboflow model for segmentation of\nthe temporomandibular joint. In the future, it is planned to develop an\nalgorithm for measuring the distance between the jaws and determining the\nposition of the articular disc, which will improve the diagnosis of TMJ\npathologies."}
{"id": "2505.13308", "pdf": "https://arxiv.org/pdf/2505.13308", "abs": "https://arxiv.org/abs/2505.13308", "authors": ["Hengli Li", "Chenxi Li", "Tong Wu", "Xuekai Zhu", "Yuxuan Wang", "Zhaoxin Yu", "Eric Hanchen Jiang", "Song-Chun Zhu", "Zixia Jia", "Ying Nian Wu", "Zilong Zheng"], "title": "Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reasoning ability, a core component of human intelligence, continues to pose\na significant challenge for Large Language Models (LLMs) in the pursuit of AGI.\nAlthough model performance has improved under the training scaling law,\nsignificant challenges remain, particularly with respect to training\nalgorithms, such as catastrophic forgetting, and the limited availability of\nnovel training data. As an alternative, test-time scaling enhances reasoning\nperformance by increasing test-time computation without parameter updating.\nUnlike prior methods in this paradigm focused on token space, we propose\nleveraging latent space for more effective reasoning and better adherence to\nthe test-time scaling law. We introduce LatentSeek, a novel framework that\nenhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA)\nwithin the model's latent space. Specifically, LatentSeek leverages policy\ngradient to iteratively update latent representations, guided by self-generated\nreward signals. LatentSeek is evaluated on a range of reasoning benchmarks,\nincluding GSM8K, MATH-500, and AIME2024, across multiple LLM architectures.\nResults show that LatentSeek consistently outperforms strong baselines, such as\nChain-of-Thought prompting and fine-tuning-based methods. Furthermore, our\nanalysis demonstrates that LatentSeek is highly efficient, typically converging\nwithin a few iterations for problems of average complexity, while also\nbenefiting from additional iterations, thereby highlighting the potential of\ntest-time scaling in the latent space. These findings position LatentSeek as a\nlightweight, scalable, and effective solution for enhancing the reasoning\ncapabilities of LLMs."}
{"id": "2505.12978", "pdf": "https://arxiv.org/pdf/2505.12978", "abs": "https://arxiv.org/abs/2505.12978", "authors": ["Yinzhe Wu", "Jiahao Huang", "Fanwen Wang", "Mengze Gao", "Congyu Liao", "Guang Yang", "Kawin Setsompop"], "title": "Enhancing Diffusion-Weighted Images (DWI) for Diffusion MRI: Is it Enough without Non-Diffusion-Weighted B=0 Reference?", "categories": ["eess.IV", "cs.CV"], "comment": "IEEE ISBI 2025", "summary": "Diffusion MRI (dMRI) is essential for studying brain microstructure, but\nhigh-resolution imaging remains challenging due to the inherent trade-offs\nbetween acquisition time and signal-to-noise ratio (SNR). Conventional methods\noften optimize only the diffusion-weighted images (DWIs) without considering\ntheir relationship with the non-diffusion-weighted (b=0) reference images.\nHowever, calculating diffusion metrics, such as the apparent diffusion\ncoefficient (ADC) and diffusion tensor with its derived metrics like fractional\nanisotropy (FA) and mean diffusivity (MD), relies on the ratio between each DWI\nand the b=0 image, which is crucial for clinical observation and diagnostics.\nIn this study, we demonstrate that solely enhancing DWIs using a conventional\npixel-wise mean squared error (MSE) loss is insufficient, as the error in ratio\nbetween generated DWIs and b=0 diverges. We propose a novel ratio loss, defined\nas the MSE loss between the predicted and ground-truth log of DWI/b=0 ratios.\nOur results show that incorporating the ratio loss significantly improves the\nconvergence of this ratio error, achieving lower ratio MSE and slightly\nenhancing the peak signal-to-noise ratio (PSNR) of generated DWIs. This leads\nto improved dMRI super-resolution and better preservation of b=0 ratio-based\nfeatures for the derivation of diffusion metrics."}
{"id": "2505.13380", "pdf": "https://arxiv.org/pdf/2505.13380", "abs": "https://arxiv.org/abs/2505.13380", "authors": ["Nam V. Nguyen", "Huy Nguyen", "Quang Pham", "Van Nguyen", "Savitha Ramasamy", "Nhat Ho"], "title": "CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via Competition", "categories": ["cs.AI", "cs.CL"], "comment": "52 pages. This work is an improved version of the previous study at\n  arXiv:2402.02526", "summary": "Sparse mixture of experts (SMoE) offers an appealing solution to scale up the\nmodel complexity beyond the mean of increasing the network's depth or width.\nHowever, we argue that effective SMoE training remains challenging because of\nthe suboptimal routing process where experts that perform computation do not\ndirectly contribute to the routing process. In this work, we propose\ncompetition, a novel mechanism to route tokens to experts with the highest\nneural response. Theoretically, we show that the competition mechanism enjoys a\nbetter sample efficiency than the traditional softmax routing. Furthermore, we\ndevelop CompeteSMoE, a simple yet effective algorithm to train large language\nmodels by deploying a router to learn the competition policy, thus enjoying\nstrong performances at a low training overhead. Our extensive empirical\nevaluations on both the visual instruction tuning and language pre-training\ntasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE\ncompared to state-of-the-art SMoE strategies. We have made the implementation\navailable at: https://github.com/Fsoft-AIC/CompeteSMoE. This work is an\nimproved version of the previous study at arXiv:2402.02526"}
{"id": "2505.12999", "pdf": "https://arxiv.org/pdf/2505.12999", "abs": "https://arxiv.org/abs/2505.12999", "authors": ["Lorena Garcia-Foncillas Macias", "Aaron Kujawa", "Aya Elshalakany", "Jonathan Shapey", "Tom Vercauteren"], "title": "A generalisable head MRI defacing pipeline: Evaluation on 2,566 meningioma scans", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Reliable MRI defacing techniques to safeguard patient privacy while\npreserving brain anatomy are critical for research collaboration. Existing\nmethods often struggle with incomplete defacing or degradation of brain tissue\nregions. We present a robust, generalisable defacing pipeline for\nhigh-resolution MRI that integrates atlas-based registration with brain\nmasking. Our method was evaluated on 2,566 heterogeneous clinical scans for\nmeningioma and achieved a 99.92 per cent success rate (2,564/2,566) upon visual\ninspection. Excellent anatomical preservation is demonstrated with a Dice\nsimilarity coefficient of 0.9975 plus or minus 0.0023 between brain masks\nautomatically extracted from the original and defaced volumes. Source code is\navailable at https://github.com/cai4cai/defacing_pipeline."}
{"id": "2505.13393", "pdf": "https://arxiv.org/pdf/2505.13393", "abs": "https://arxiv.org/abs/2505.13393", "authors": ["Christopher K. Frantz"], "title": "IG Parser: A Software Package for the Encoding of Institutional Statements using the Institutional Grammar", "categories": ["cs.MA", "cs.AI", "cs.CL", "68T30, 68T50", "E.2; H.1.0; I.7.2; I.6.5; K.4.1"], "comment": "24 pages", "summary": "This article provides an overview of IG Parser, a software that facilitates\nqualitative content analysis of formal (e.g., legal) rules or informal (e.g.,\nsocio-normative) norms, and strategies (such as conventions) -- referred to as\n\\emph{institutions} -- that govern social systems and operate configurally to\ndescribe \\emph{institutional systems}. To this end, the IG Parser employs a\ndistinctive syntax that ensures rigorous encoding of natural language, while\nautomating the transformation into various formats that support the downstream\nanalysis using diverse analytical techniques. The conceptual core of the IG\nParser is an associated syntax, IG Script, that operationalizes the conceptual\nfoundations of the Institutional Grammar, and more specifically Institutional\nGrammar 2.0, an analytical paradigm for institutional analysis. This article\npresents the IG Parser, including its conceptual foundations, syntactic\nspecification of IG Script, alongside architectural principles. This\nintroduction is augmented with selective illustrative examples that highlight\nthe use and benefit associated with the tool."}
{"id": "2505.13081", "pdf": "https://arxiv.org/pdf/2505.13081", "abs": "https://arxiv.org/abs/2505.13081", "authors": ["Xiaoyu Yang", "Jie Lu", "En Yu"], "title": "Walking the Tightrope: Disentangling Beneficial and Detrimental Drifts in Non-Stationary Custom-Tuning", "categories": ["cs.LG", "cs.CV"], "comment": "17 pages, 5figures", "summary": "This paper uncovers a critical yet overlooked phenomenon in multi-modal large\nlanguage models (MLLMs): detrimental concept drift within chain-of-thought\n(CoT) reasoning during non-stationary reinforcement fine-tuning (RFT), where\nreasoning token distributions evolve unpredictably, thereby introducing\nsignificant biases in final predictions. To address this, we are pioneers in\nestablishing the theoretical bridge between concept drift theory and RFT\nprocesses by formalizing CoT's autoregressive token streams as non-stationary\ndistributions undergoing arbitrary temporal shifts. Leveraging this framework,\nwe propose a novel counterfact-aware RFT that systematically decouples\nbeneficial distribution adaptation from harmful concept drift through concept\ngraph-empowered LLM experts generating counterfactual reasoning trajectories.\nOur solution, Counterfactual Preference Optimization (CPO), enables stable RFT\nin non-stationary environments, particularly within the medical domain, through\ncustom-tuning of counterfactual-aware preference alignment. Extensive\nexperiments demonstrate our superior performance of robustness, generalization\nand coordination within RFT. Besides, we also contributed a large-scale dataset\nCXR-CounterFact (CCF), comprising 320,416 meticulously curated counterfactual\nreasoning trajectories derived from MIMIC-CXR. Our code and data are public."}
{"id": "2505.13398", "pdf": "https://arxiv.org/pdf/2505.13398", "abs": "https://arxiv.org/abs/2505.13398", "authors": ["Matan Abudy", "Orr Well", "Emmanuel Chemla", "Roni Katzir", "Nur Lan"], "title": "A Minimum Description Length Approach to Regularization in Neural Networks", "categories": ["cs.LG", "cs.CL"], "comment": "9 pages", "summary": "State-of-the-art neural networks can be trained to become remarkable\nsolutions to many problems. But while these architectures can express symbolic,\nperfect solutions, trained models often arrive at approximations instead. We\nshow that the choice of regularization method plays a crucial role: when\ntrained on formal languages with standard regularization ($L_1$, $L_2$, or\nnone), expressive architectures not only fail to converge to correct solutions\nbut are actively pushed away from perfect initializations. In contrast,\napplying the Minimum Description Length (MDL) principle to balance model\ncomplexity with data fit provides a theoretically grounded regularization\nmethod. Using MDL, perfect solutions are selected over approximations,\nindependently of the optimization algorithm. We propose that unlike existing\nregularization techniques, MDL introduces the appropriate inductive bias to\neffectively counteract overfitting and promote generalization."}
{"id": "2505.13152", "pdf": "https://arxiv.org/pdf/2505.13152", "abs": "https://arxiv.org/abs/2505.13152", "authors": ["Jonas Brenig", "Radu Timofte"], "title": "Higher fidelity perceptual image and video compression with a latent conditioned residual denoising diffusion model", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted at AIM Workshop 2024 at ECCV 2024", "summary": "Denoising diffusion models achieved impressive results on several image\ngeneration tasks often outperforming GAN based models. Recently, the generative\ncapabilities of diffusion models have been employed for perceptual image\ncompression, such as in CDC. A major drawback of these diffusion-based methods\nis that, while producing impressive perceptual quality images they are dropping\nin fidelity/increasing the distortion to the original uncompressed images when\ncompared with other traditional or learned image compression schemes aiming for\nfidelity. In this paper, we propose a hybrid compression scheme optimized for\nperceptual quality, extending the approach of the CDC model with a decoder\nnetwork in order to reduce the impact on distortion metrics such as PSNR. After\nusing the decoder network to generate an initial image, optimized for\ndistortion, the latent conditioned diffusion model refines the reconstruction\nfor perceptual quality by predicting the residual. On standard benchmarks, we\nachieve up to +2dB PSNR fidelity improvements while maintaining comparable\nLPIPS and FID perceptual scores when compared with CDC. Additionally, the\napproach is easily extensible to video compression, where we achieve similar\nresults."}
{"id": "2505.13408", "pdf": "https://arxiv.org/pdf/2505.13408", "abs": "https://arxiv.org/abs/2505.13408", "authors": ["Jinhe Bi", "Danqi Yan", "Yifan Wang", "Wenke Huang", "Haokun Chen", "Guancheng Wan", "Mang Ye", "Xun Xiao", "Hinrich Schuetze", "Volker Tresp", "Yunpu Ma"], "title": "CoT-Kinetics: A Theoretical Modeling Assessing LRM Reasoning Process", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent Large Reasoning Models significantly improve the reasoning ability of\nLarge Language Models by learning to reason, exhibiting the promising\nperformance in solving complex tasks. LRMs solve tasks that require complex\nreasoning by explicitly generating reasoning trajectories together with\nanswers. Nevertheless, judging the quality of such an output answer is not easy\nbecause only considering the correctness of the answer is not enough and the\nsoundness of the reasoning trajectory part matters as well. Logically, if the\nsoundness of the reasoning part is poor, even if the answer is correct, the\nconfidence of the derived answer should be low. Existing methods did consider\njointly assessing the overall output answer by taking into account the\nreasoning part, however, their capability is still not satisfactory as the\ncausal relationship of the reasoning to the concluded answer cannot properly\nreflected. In this paper, inspired by classical mechanics, we present a novel\napproach towards establishing a CoT-Kinetics energy equation. Specifically, our\nCoT-Kinetics energy equation formulates the token state transformation process,\nwhich is regulated by LRM internal transformer layers, as like a particle\nkinetics dynamics governed in a mechanical field. Our CoT-Kinetics energy\nassigns a scalar score to evaluate specifically the soundness of the reasoning\nphase, telling how confident the derived answer could be given the evaluated\nreasoning. As such, the LRM's overall output quality can be accurately\nmeasured, rather than a coarse judgment (e.g., correct or incorrect) anymore."}
{"id": "2505.13227", "pdf": "https://arxiv.org/pdf/2505.13227", "abs": "https://arxiv.org/abs/2505.13227", "authors": ["Tianbao Xie", "Jiaqi Deng", "Xiaochuan Li", "Junlin Yang", "Haoyuan Wu", "Jixuan Chen", "Wenjing Hu", "Xinyuan Wang", "Yuhui Xu", "Zekun Wang", "Yiheng Xu", "Junli Wang", "Doyen Sahoo", "Tao Yu", "Caiming Xiong"], "title": "Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": "49 pages, 13 figures", "summary": "Graphical user interface (GUI) grounding, the ability to map natural language\ninstructions to specific actions on graphical user interfaces, remains a\ncritical bottleneck in computer use agent development. Current benchmarks\noversimplify grounding tasks as short referring expressions, failing to capture\nthe complexity of real-world interactions that require software commonsense,\nlayout understanding, and fine-grained manipulation capabilities. To address\nthese limitations, we introduce OSWorld-G, a comprehensive benchmark comprising\n564 finely annotated samples across diverse task types including text matching,\nelement recognition, layout understanding, and precise manipulation.\nAdditionally, we synthesize and release the largest computer use grounding\ndataset Jedi, which contains 4 million examples through multi-perspective\ndecoupling of tasks. Our multi-scale models trained on Jedi demonstrate its\neffectiveness by outperforming existing approaches on ScreenSpot-v2,\nScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved\ngrounding with Jedi directly enhances agentic capabilities of general\nfoundation models on complex computer tasks, improving from 5% to 27% on\nOSWorld. Through detailed ablation studies, we identify key factors\ncontributing to grounding performance and verify that combining specialized\ndata for different interface elements enables compositional generalization to\nnovel interfaces. All benchmark, data, checkpoints, and code are open-sourced\nand available at https://osworld-grounding.github.io."}
{"id": "2505.13430", "pdf": "https://arxiv.org/pdf/2505.13430", "abs": "https://arxiv.org/abs/2505.13430", "authors": ["Sifeng Shang", "Jiayi Zhou", "Chenyu Lin", "Minxian Li", "Kaiyang Zhou"], "title": "Fine-tuning Quantized Neural Networks with Zeroth-order Optimization", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "As the size of large language models grows exponentially, GPU memory has\nbecome a bottleneck for adapting these models to downstream tasks. In this\npaper, we aim to push the limits of memory-efficient training by minimizing\nmemory usage on model weights, gradients, and optimizer states, within a\nunified framework. Our idea is to eliminate both gradients and optimizer states\nusing zeroth-order optimization, which approximates gradients by perturbing\nweights during forward passes to identify gradient directions. To minimize\nmemory usage on weights, we employ model quantization, e.g., converting from\nbfloat16 to int4. However, directly applying zeroth-order optimization to\nquantized weights is infeasible due to the precision gap between discrete\nweights and continuous gradients, which would otherwise require de-quantization\nand re-quantization. To overcome this challenge, we propose Quantized\nZeroth-order Optimization (QZO), a novel approach that perturbs the continuous\nquantization scale for gradient estimation and uses a directional derivative\nclipping method to stabilize training. QZO is orthogonal to both scalar-based\nand codebook-based post-training quantization methods. Compared to\nfull-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by\nmore than 18$\\times$ for 4-bit LLMs, and enables fine-tuning Llama-2-13B and\nStable Diffusion 3.5 Large within a single 24GB GPU."}
{"id": "2505.13232", "pdf": "https://arxiv.org/pdf/2505.13232", "abs": "https://arxiv.org/abs/2505.13232", "authors": ["Younghyun Kim", "Jongheon Jeong", "Sangkyung Kwak", "Kyungmin Lee", "Juho Lee", "Jinwoo Shin"], "title": "StarFT: Robust Fine-tuning of Zero-shot Models via Spuriosity Alignment", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Learning robust representations from data often requires scale, which has led\nto the success of recent zero-shot models such as CLIP. However, the obtained\nrobustness can easily be deteriorated when these models are fine-tuned on other\ndownstream tasks (e.g., of smaller scales). Previous works often interpret this\nphenomenon in the context of domain shift, developing fine-tuning methods that\naim to preserve the original domain as much as possible. However, in a\ndifferent context, fine-tuned models with limited data are also prone to\nlearning features that are spurious to humans, such as background or texture.\nIn this paper, we propose StarFT (Spurious Textual Alignment Regularization), a\nnovel framework for fine-tuning zero-shot models to enhance robustness by\npreventing them from learning spuriosity. We introduce a regularization that\naligns the output distribution for spuriosity-injected labels with the original\nzero-shot model, ensuring that the model is not induced to extract irrelevant\nfeatures further from these descriptions.We leverage recent language models to\nget such spuriosity-injected labels by generating alternative textual\ndescriptions that highlight potentially confounding features.Extensive\nexperiments validate the robust generalization of StarFT and its emerging\nproperties: zero-shot group robustness and improved zero-shot classification.\nNotably, StarFT boosts both worst-group and average accuracy by 14.30% and\n3.02%, respectively, in the Waterbirds group shift scenario, where other robust\nfine-tuning baselines show even degraded performance."}
{"id": "2505.13438", "pdf": "https://arxiv.org/pdf/2505.13438", "abs": "https://arxiv.org/abs/2505.13438", "authors": ["Penghui Qi", "Zichen Liu", "Tianyu Pang", "Chao Du", "Wee Sun Lee", "Min Lin"], "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Scaling test-time compute is crucial for enhancing the reasoning capabilities\nof large language models (LLMs). Existing approaches typically employ\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\nof reasoning traces. However, such methods optimize only the final performance\nunder a large and fixed token budget, which hinders efficiency in both training\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\noptimize anytime reasoning performance, which aims to improve token efficiency\nand the flexibility of reasoning under varying token budget constraints. To\nachieve this, we truncate the complete thinking process to fit within sampled\ntoken budgets from a prior distribution, compelling the model to summarize the\noptimal answer for each truncated thinking for verification. This introduces\nverifiable dense rewards into the reasoning process, facilitating more\neffective credit assignment in RL optimization. We then optimize the thinking\nand summary policies in a decoupled manner to maximize the cumulative reward.\nAdditionally, we introduce a novel variance reduction technique, Budget\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\nof the learning process when reinforcing the thinking policy. Empirical results\nin mathematical reasoning tasks demonstrate that our method consistently\noutperforms GRPO across all thinking budgets under various prior distributions,\nenhancing both training and token efficiency."}
{"id": "2505.13289", "pdf": "https://arxiv.org/pdf/2505.13289", "abs": "https://arxiv.org/abs/2505.13289", "authors": ["Alonso Urbano", "David W. Romero", "Max Zimmer", "Sebastian Pokutta"], "title": "RECON: Robust symmetry discovery via Explicit Canonical Orientation Normalization", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Real-world data often exhibits unknown or approximate symmetries, yet\nexisting equivariant networks must commit to a fixed transformation group prior\nto training, e.g., continuous $SO(2)$ rotations. This mismatch degrades\nperformance when the actual data symmetries differ from those in the\ntransformation group. We introduce RECON, a framework to discover each input's\nintrinsic symmetry distribution from unlabeled data. RECON leverages class-pose\ndecompositions and applies a data-driven normalization to align arbitrary\nreference frames into a common natural pose, yielding directly comparable and\ninterpretable symmetry descriptors. We demonstrate effective symmetry discovery\non 2D image benchmarks and -- for the first time -- extend it to 3D\ntransformation groups, paving the way towards more flexible equivariant\nmodeling."}
{"id": "2505.13445", "pdf": "https://arxiv.org/pdf/2505.13445", "abs": "https://arxiv.org/abs/2505.13445", "authors": ["Xiaoyuan Liu", "Tian Liang", "Zhiwei He", "Jiahao Xu", "Wenxuan Wang", "Pinjia He", "Zhaopeng Tu", "Haitao Mi", "Dong Yu"], "title": "Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards", "categories": ["cs.AI", "cs.CL"], "comment": "code available at https://github.com/xyliu-cs/RISE", "summary": "Large Language Models (LLMs) show great promise in complex reasoning, with\nReinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement\nstrategy. However, a prevalent issue is ``superficial self-reflection'', where\nmodels fail to robustly verify their own outputs. We introduce RISE\n(Reinforcing Reasoning with Self-Verification), a novel online RL framework\ndesigned to tackle this. RISE explicitly and simultaneously trains an LLM to\nimprove both its problem-solving and self-verification abilities within a\nsingle, integrated RL process. The core mechanism involves leveraging\nverifiable rewards from an outcome verifier to provide on-the-fly feedback for\nboth solution generation and self-verification tasks. In each iteration, the\nmodel generates solutions, then critiques its own on-policy generated\nsolutions, with both trajectories contributing to the policy update. Extensive\nexperiments on diverse mathematical reasoning benchmarks show that RISE\nconsistently improves model's problem-solving accuracy while concurrently\nfostering strong self-verification skills. Our analyses highlight the\nadvantages of online verification and the benefits of increased verification\ncompute. Additionally, RISE models exhibit more frequent and accurate\nself-verification behaviors during reasoning. These advantages reinforce RISE\nas a flexible and effective path towards developing more robust and self-aware\nreasoners."}
{"id": "2505.13307", "pdf": "https://arxiv.org/pdf/2505.13307", "abs": "https://arxiv.org/abs/2505.13307", "authors": ["Qiguang Chen", "Libo Qin", "Jinhao Liu", "Yue Liao", "Jiaqi Wang", "Jingxuan Zhou", "Wanxiang Che"], "title": "RBF++: Quantifying and Optimizing Reasoning Boundaries across Measurable and Unmeasurable Capabilities for Chain-of-Thought Reasoning", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Manuscript", "summary": "Chain-of-Thought (CoT) reasoning has proven effective in enhancing large\nlanguage models (LLMs) on complex tasks, spurring research into its underlying\nmechanisms. However, two primary challenges remain for real-world applications:\n(1) the lack of quantitative metrics and actionable guidelines for evaluating\nand optimizing measurable boundaries of CoT capability, and (2) the absence of\nmethods to assess boundaries of unmeasurable CoT capability, such as multimodal\nperception. To address these gaps, we introduce the Reasoning Boundary\nFramework++ (RBF++). To tackle the first challenge, we define the reasoning\nboundary (RB) as the maximum limit of CoT performance. We also propose a\ncombination law for RBs, enabling quantitative analysis and offering actionable\nguidance across various CoT tasks. For the second challenge, particularly in\nmultimodal scenarios, we introduce a constant assumption, which replaces\nunmeasurable RBs with scenario-specific constants. Additionally, we propose the\nreasoning boundary division mechanism, which divides unmeasurable RBs into two\nsub-boundaries, facilitating the quantification and optimization of both\nunmeasurable domain knowledge and multimodal perception capabilities. Extensive\nexperiments involving 38 models across 13 tasks validate the feasibility of our\nframework in cross-modal settings. Additionally, we evaluate 10 CoT strategies,\noffer insights into optimization and decay from two complementary perspectives,\nand expand evaluation benchmarks for measuring RBs in LLM reasoning. We hope\nthis work advances the understanding of RBs and optimization strategies in\nLLMs. Code and data are available at\nhttps://github.com/LightChen233/reasoning-boundary."}
{"id": "2505.13391", "pdf": "https://arxiv.org/pdf/2505.13391", "abs": "https://arxiv.org/abs/2505.13391", "authors": ["Mikołaj Małkiński", "Jacek Mańdziuk"], "title": "Advancing Generalization Across a Variety of Abstract Visual Reasoning Tasks", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": "Accepted to the 34th International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)", "summary": "The abstract visual reasoning (AVR) domain presents a diverse suite of\nanalogy-based tasks devoted to studying model generalization. Recent years have\nbrought dynamic progress in the field, particularly in i.i.d. scenarios, in\nwhich models are trained and evaluated on the same data distributions.\nNevertheless, o.o.d. setups that assess model generalization to new test\ndistributions remain challenging even for the most recent models. To advance\ngeneralization in AVR tasks, we present the Pathways of Normalized Group\nConvolution model (PoNG), a novel neural architecture that features group\nconvolution, normalization, and a parallel design. We consider a wide set of\nAVR benchmarks, including Raven's Progressive Matrices and visual analogy\nproblems with both synthetic and real-world images. The experiments demonstrate\nstrong generalization capabilities of the proposed model, which in several\nsettings outperforms the existing literature methods."}
{"id": "2505.13414", "pdf": "https://arxiv.org/pdf/2505.13414", "abs": "https://arxiv.org/abs/2505.13414", "authors": ["Yaqian Chen", "Hanxue Gu", "Haoyu Dong", "Qihang Li", "Yuwen Chen", "Nicholas Konz", "Lin Li", "Maciej A. Mazurowski"], "title": "GuidedMorph: Two-Stage Deformable Registration for Breast MRI", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Accurately registering breast MR images from different time points enables\nthe alignment of anatomical structures and tracking of tumor progression,\nsupporting more effective breast cancer detection, diagnosis, and treatment\nplanning. However, the complexity of dense tissue and its highly non-rigid\nnature pose challenges for conventional registration methods, which primarily\nfocus on aligning general structures while overlooking intricate internal\ndetails. To address this, we propose \\textbf{GuidedMorph}, a novel two-stage\nregistration framework designed to better align dense tissue. In addition to a\nsingle-scale network for global structure alignment, we introduce a framework\nthat utilizes dense tissue information to track breast movement. The learned\ntransformation fields are fused by introducing the Dual Spatial Transformer\nNetwork (DSTN), improving overall alignment accuracy. A novel warping method\nbased on the Euclidean distance transform (EDT) is also proposed to accurately\nwarp the registered dense tissue and breast masks, preserving fine structural\ndetails during deformation. The framework supports paradigms that require\nexternal segmentation models and with image data only. It also operates\neffectively with the VoxelMorph and TransMorph backbones, offering a versatile\nsolution for breast registration. We validate our method on ISPY2 and internal\ndataset, demonstrating superior performance in dense tissue, overall breast\nalignment, and breast structural similarity index measure (SSIM), with notable\nimprovements by over 13.01% in dense tissue Dice, 3.13% in breast Dice, and\n1.21% in breast SSIM compared to the best learning-based baseline."}
{"id": "2505.13427", "pdf": "https://arxiv.org/pdf/2505.13427", "abs": "https://arxiv.org/abs/2505.13427", "authors": ["Lingxiao Du", "Fanqing Meng", "Zongkai Liu", "Zhixiang Zhou", "Ping Luo", "Qiaosheng Zhang", "Wenqi Shao"], "title": "MM-PRM: Enhancing Multimodal Mathematical Reasoning with Scalable Step-Level Supervision", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "While Multimodal Large Language Models (MLLMs) have achieved impressive\nprogress in vision-language understanding, they still struggle with complex\nmulti-step reasoning, often producing logically inconsistent or partially\ncorrect solutions. A key limitation lies in the lack of fine-grained\nsupervision over intermediate reasoning steps. To address this, we propose\nMM-PRM, a process reward model trained within a fully automated, scalable\nframework. We first build MM-Policy, a strong multimodal model trained on\ndiverse mathematical reasoning data. Then, we construct MM-K12, a curated\ndataset of 10,000 multimodal math problems with verifiable answers, which\nserves as seed data. Leveraging a Monte Carlo Tree Search (MCTS)-based\npipeline, we generate over 700k step-level annotations without human labeling.\nThe resulting PRM is used to score candidate reasoning paths in the Best-of-N\ninference setup and achieves significant improvements across both in-domain\n(MM-K12 test set) and out-of-domain (OlympiadBench, MathVista, etc.)\nbenchmarks. Further analysis confirms the effectiveness of soft labels, smaller\nlearning rates, and path diversity in optimizing PRM performance. MM-PRM\ndemonstrates that process supervision is a powerful tool for enhancing the\nlogical robustness of multimodal reasoning systems. We release all our codes\nand data at https://github.com/ModalMinds/MM-PRM."}
{"id": "2505.13430", "pdf": "https://arxiv.org/pdf/2505.13430", "abs": "https://arxiv.org/abs/2505.13430", "authors": ["Sifeng Shang", "Jiayi Zhou", "Chenyu Lin", "Minxian Li", "Kaiyang Zhou"], "title": "Fine-tuning Quantized Neural Networks with Zeroth-order Optimization", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "As the size of large language models grows exponentially, GPU memory has\nbecome a bottleneck for adapting these models to downstream tasks. In this\npaper, we aim to push the limits of memory-efficient training by minimizing\nmemory usage on model weights, gradients, and optimizer states, within a\nunified framework. Our idea is to eliminate both gradients and optimizer states\nusing zeroth-order optimization, which approximates gradients by perturbing\nweights during forward passes to identify gradient directions. To minimize\nmemory usage on weights, we employ model quantization, e.g., converting from\nbfloat16 to int4. However, directly applying zeroth-order optimization to\nquantized weights is infeasible due to the precision gap between discrete\nweights and continuous gradients, which would otherwise require de-quantization\nand re-quantization. To overcome this challenge, we propose Quantized\nZeroth-order Optimization (QZO), a novel approach that perturbs the continuous\nquantization scale for gradient estimation and uses a directional derivative\nclipping method to stabilize training. QZO is orthogonal to both scalar-based\nand codebook-based post-training quantization methods. Compared to\nfull-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by\nmore than 18$\\times$ for 4-bit LLMs, and enables fine-tuning Llama-2-13B and\nStable Diffusion 3.5 Large within a single 24GB GPU."}
{"id": "2505.13444", "pdf": "https://arxiv.org/pdf/2505.13444", "abs": "https://arxiv.org/abs/2505.13444", "authors": ["Liyan Tang", "Grace Kim", "Xinyu Zhao", "Thom Lake", "Wenxuan Ding", "Fangcong Yin", "Prasann Singhal", "Manya Wadhwa", "Zeyu Leo Liu", "Zayne Sprague", "Ramya Namuduri", "Bodun Hu", "Juan Diego Rodriguez", "Puyuan Peng", "Greg Durrett"], "title": "ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Chart understanding presents a unique challenge for large vision-language\nmodels (LVLMs), as it requires the integration of sophisticated textual and\nvisual reasoning capabilities. However, current LVLMs exhibit a notable\nimbalance between these skills, falling short on visual reasoning that is\ndifficult to perform in text. We conduct a case study using a synthetic dataset\nsolvable only through visual reasoning and show that model performance degrades\nsignificantly with increasing visual complexity, while human performance\nremains robust. We then introduce ChartMuseum, a new Chart Question Answering\n(QA) benchmark containing 1,162 expert-annotated questions spanning multiple\nreasoning types, curated from real-world charts across 184 sources,\nspecifically built to evaluate complex visual and textual reasoning. Unlike\nprior chart understanding benchmarks -- where frontier models perform similarly\nand near saturation -- our benchmark exposes a substantial gap between model\nand human performance, while effectively differentiating model capabilities:\nalthough humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro\nattains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct\nachieves only 38.5%. Moreover, on questions requiring primarily visual\nreasoning, all models experience a 35%-55% performance drop from\ntext-reasoning-heavy question performance. Lastly, our qualitative error\nanalysis reveals specific categories of visual reasoning that are challenging\nfor current LVLMs."}
{"id": "2505.13447", "pdf": "https://arxiv.org/pdf/2505.13447", "abs": "https://arxiv.org/abs/2505.13447", "authors": ["Zhengyang Geng", "Mingyang Deng", "Xingjian Bai", "J. Zico Kolter", "Kaiming He"], "title": "Mean Flows for One-step Generative Modeling", "categories": ["cs.LG", "cs.CV"], "comment": "Tech report", "summary": "We propose a principled and effective framework for one-step generative\nmodeling. We introduce the notion of average velocity to characterize flow\nfields, in contrast to instantaneous velocity modeled by Flow Matching methods.\nA well-defined identity between average and instantaneous velocities is derived\nand used to guide neural network training. Our method, termed the MeanFlow\nmodel, is self-contained and requires no pre-training, distillation, or\ncurriculum learning. MeanFlow demonstrates strong empirical performance: it\nachieves an FID of 3.43 with a single function evaluation (1-NFE) on ImageNet\n256x256 trained from scratch, significantly outperforming previous\nstate-of-the-art one-step diffusion/flow models. Our study substantially\nnarrows the gap between one-step diffusion/flow models and their multi-step\npredecessors, and we hope it will motivate future research to revisit the\nfoundations of these powerful models."}
