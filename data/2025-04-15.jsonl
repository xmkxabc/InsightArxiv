{"id": "2504.08901", "pdf": "https://arxiv.org/pdf/2504.08901", "abs": "https://arxiv.org/abs/2504.08901", "authors": ["Asterios Reppas", "Grigorios-Aris Cheimariotis", "Panos K. Papadopoulos", "Panagiotis Frasiolas", "Dimitrios Zarpalas"], "title": "HAL-NeRF: High Accuracy Localization Leveraging Neural Radiance Fields", "categories": ["cs.CV"], "comment": "8 pages, 4 figures", "summary": "Precise camera localization is a critical task in XR applications and\nrobotics. Using only the camera captures as input to a system is an inexpensive\noption that enables localization in large indoor and outdoor environments, but\nit presents challenges in achieving high accuracy. Specifically, camera\nrelocalization methods, such as Absolute Pose Regression (APR), can localize\ncameras with a median translation error of more than $0.5m$ in outdoor scenes.\nThis paper presents HAL-NeRF, a high-accuracy localization method that combines\na CNN pose regressor with a refinement module based on a Monte Carlo particle\nfilter. The Nerfacto model, an implementation of Neural Radiance Fields\n(NeRFs), is used to augment the data for training the pose regressor and to\nmeasure photometric loss in the particle filter refinement module. HAL-NeRF\nleverages Nerfacto's ability to synthesize high-quality novel views,\nsignificantly improving the performance of the localization pipeline. HAL-NeRF\nachieves state-of-the-art results that are conventionally measured as the\naverage of the median per scene errors. The translation error was $0.025m$ and\nthe rotation error was $0.59$ degrees and 0.04m and 0.58 degrees on the\n7-Scenes dataset and Cambridge Landmarks datasets respectively, with the\ntrade-off of increased computational time. This work highlights the potential\nof combining APR with NeRF-based refinement techniques to advance monocular\ncamera relocalization accuracy."}
{"id": "2504.08902", "pdf": "https://arxiv.org/pdf/2504.08902", "abs": "https://arxiv.org/abs/2504.08902", "authors": ["Pascal Chang", "Sergio Sancho", "Jingwei Tang", "Markus Gross", "Vinicius C. Azevedo"], "title": "LookingGlass: Generative Anamorphoses via Laplacian Pyramid Warping", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at CVPR 2025 (Oral)", "summary": "Anamorphosis refers to a category of images that are intentionally distorted,\nmaking them unrecognizable when viewed directly. Their true form only reveals\nitself when seen from a specific viewpoint, which can be through some\ncatadioptric device like a mirror or a lens. While the construction of these\nmathematical devices can be traced back to as early as the 17th century, they\nare only interpretable when viewed from a specific vantage point and tend to\nlose meaning when seen normally. In this paper, we revisit these famous optical\nillusions with a generative twist. With the help of latent rectified flow\nmodels, we propose a method to create anamorphic images that still retain a\nvalid interpretation when viewed directly. To this end, we introduce Laplacian\nPyramid Warping, a frequency-aware image warping technique key to generating\nhigh-quality visuals. Our work extends Visual Anagrams (arXiv:2311.17919) to\nlatent space models and to a wider range of spatial transforms, enabling the\ncreation of novel generative perceptual illusions."}
{"id": "2504.08906", "pdf": "https://arxiv.org/pdf/2504.08906", "abs": "https://arxiv.org/abs/2504.08906", "authors": ["Jiahuan Long", "Zhengqin Xu", "Tingsong Jiang", "Wen Yao", "Shuai Jia", "Chao Ma", "Xiaoqian Chen"], "title": "Robust SAM: On the Adversarial Robustness of Vision Foundation Models", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by AAAI2025", "summary": "The Segment Anything Model (SAM) is a widely used vision foundation model\nwith diverse applications, including image segmentation, detection, and\ntracking. Given SAM's wide applications, understanding its robustness against\nadversarial attacks is crucial for real-world deployment. However, research on\nSAM's robustness is still in its early stages. Existing attacks often overlook\nthe role of prompts in evaluating SAM's robustness, and there has been\ninsufficient exploration of defense methods to balance the robustness and\naccuracy. To address these gaps, this paper proposes an adversarial robustness\nframework designed to evaluate and enhance the robustness of SAM. Specifically,\nwe introduce a cross-prompt attack method to enhance the attack transferability\nacross different prompt types. Besides attacking, we propose a few-parameter\nadaptation strategy to defend SAM against various adversarial attacks. To\nbalance robustness and accuracy, we use the singular value decomposition (SVD)\nto constrain the space of trainable parameters, where only singular values are\nadaptable. Experiments demonstrate that our cross-prompt attack method\noutperforms previous approaches in terms of attack success rate on both SAM and\nSAM 2. By adapting only 512 parameters, we achieve at least a 15\\% improvement\nin mean intersection over union (mIoU) against various adversarial attacks.\nCompared to previous defense methods, our approach enhances the robustness of\nSAM while maximally maintaining its original performance."}
{"id": "2504.08915", "pdf": "https://arxiv.org/pdf/2504.08915", "abs": "https://arxiv.org/abs/2504.08915", "authors": ["Jiahuan Long", "Tingsong Jiang", "Wen Yao", "Yizhe Xiong", "Zhengqin Xu", "Shuai Jia", "Chao Ma"], "title": "Parameter-Free Fine-tuning via Redundancy Elimination for Vision Foundation Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision foundation models (VFMs) are large pre-trained models that form the\nbackbone of various vision tasks. Fine-tuning VFMs can further unlock their\npotential for downstream tasks or scenarios. However, VFMs often contain\nsignificant feature redundancy, which may limit their adaptability to new\ntasks. In this paper, we investigate the redundancies in the segment anything\nmodel (SAM) and then propose a parameter-free fine-tuning method to address\nthis issue. Unlike traditional fine-tuning methods that adjust parameters, our\nmethod emphasizes selecting, reusing, and enhancing pre-trained features,\noffering a new perspective on model fine-tuning. Specifically, we introduce a\nchannel selection algorithm based on the model's output difference to identify\nredundant and effective channels. By selectively replacing the redundant\nchannels with more effective ones, we filter out less useful features and reuse\nthe more relevant features to downstream tasks, thereby enhancing the\ntask-specific feature representation. Experiments on both out-of-domain and\nin-domain datasets demonstrate the efficiency and effectiveness of our method.\nNotably, our approach can seamlessly integrate with existing fine-tuning\nstrategies (e.g., LoRA, Adapter), further boosting the performance of already\nfine-tuned models. Moreover, since our channel selection involves only model\ninference, our method significantly reduces computational and GPU memory\noverhead."}
{"id": "2504.08775", "pdf": "https://arxiv.org/pdf/2504.08775", "abs": "https://arxiv.org/abs/2504.08775", "authors": ["Christopher Wolfram", "Aaron Schein"], "title": "Layers at Similar Depths Generate Similar Activations Across LLM Architectures", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "How do the latent spaces used by independently-trained LLMs relate to one\nanother? We study the nearest neighbor relationships induced by activations at\ndifferent layers of 24 open-weight LLMs, and find that they 1) tend to vary\nfrom layer to layer within a model, and 2) are approximately shared between\ncorresponding layers of different models. Claim 2 shows that these nearest\nneighbor relationships are not arbitrary, as they are shared across models, but\nClaim 1 shows that they are not \"obvious\" either, as there is no single set of\nnearest neighbor relationships that is universally shared. Together, these\nsuggest that LLMs generate a progression of activation geometries from layer to\nlayer, but that this entire progression is largely shared between models,\nstretched and squeezed to fit into different architectures."}
{"id": "2504.08959", "pdf": "https://arxiv.org/pdf/2504.08959", "abs": "https://arxiv.org/abs/2504.08959", "authors": ["Yilin Wang", "Chuan Guo", "Yuxuan Mu", "Muhammad Gohar Javed", "Xinxin Zuo", "Juwei Lu", "Hai Jiang", "Li Cheng"], "title": "MotionDreamer: One-to-Many Motion Synthesis with Localized Generative Masked Transformer", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "ICLR 2025 acceptance", "summary": "Generative masked transformers have demonstrated remarkable success across\nvarious content generation tasks, primarily due to their ability to effectively\nmodel large-scale dataset distributions with high consistency. However, in the\nanimation domain, large datasets are not always available. Applying generative\nmasked modeling to generate diverse instances from a single MoCap reference may\nlead to overfitting, a challenge that remains unexplored. In this work, we\npresent MotionDreamer, a localized masked modeling paradigm designed to learn\ninternal motion patterns from a given motion with arbitrary topology and\nduration. By embedding the given motion into quantized tokens with a novel\ndistribution regularization method, MotionDreamer constructs a robust and\ninformative codebook for local motion patterns. Moreover, a sliding window\nlocal attention is introduced in our masked transformer, enabling the\ngeneration of natural yet diverse animations that closely resemble the\nreference motion patterns. As demonstrated through comprehensive experiments,\nMotionDreamer outperforms the state-of-the-art methods that are typically GAN\nor Diffusion-based in both faithfulness and diversity. Thanks to the\nconsistency and robustness of the quantization-based approach, MotionDreamer\ncan also effectively perform downstream tasks such as temporal motion editing,\n\\textcolor{update}{crowd animation}, and beat-aligned dance generation, all\nusing a single reference motion. Visit our project page:\nhttps://motiondreamer.github.io/"}
{"id": "2504.08776", "pdf": "https://arxiv.org/pdf/2504.08776", "abs": "https://arxiv.org/abs/2504.08776", "authors": ["Gautam Kishore Shahi", "Oshani Seneviratne", "Marc Spaniol"], "title": "SemCAFE: When Named Entities make the Difference Assessing Web Source Reliability through Entity-level Analytics", "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "With the shift from traditional to digital media, the online landscape now\nhosts not only reliable news articles but also a significant amount of\nunreliable content. Digital media has faster reachability by significantly\ninfluencing public opinion and advancing political agendas. While newspaper\nreaders may be familiar with their preferred outlets political leanings or\ncredibility, determining unreliable news articles is much more challenging. The\ncredibility of many online sources is often opaque, with AI generated content\nbeing easily disseminated at minimal cost. Unreliable news articles,\nparticularly those that followed the Russian invasion of Ukraine in 2022,\nclosely mimic the topics and writing styles of credible sources, making them\ndifficult to distinguish. To address this, we introduce SemCAFE, a system\ndesigned to detect news reliability by incorporating entity relatedness into\nits assessment. SemCAFE employs standard Natural Language Processing\ntechniques, such as boilerplate removal and tokenization, alongside entity\nlevel semantic analysis using the YAGO knowledge base. By creating a semantic\nfingerprint for each news article, SemCAFE could assess the credibility of\n46,020 reliable and 3,407 unreliable articles on the 2022 Russian invasion of\nUkraine. Our approach improved the macro F1 score by 12% over state of the art\nmethods. The sample data and code are available on GitHub"}
{"id": "2504.08966", "pdf": "https://arxiv.org/pdf/2504.08966", "abs": "https://arxiv.org/abs/2504.08966", "authors": ["Mohamed Dhouib", "Davide Buscaldi", "Sonia Vanier", "Aymen Shabou"], "title": "PACT: Pruning and Clustering-Based Token Reduction for Faster Visual Language Models", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Visual Language Models require substantial computational resources for\ninference due to the additional input tokens needed to represent visual\ninformation. However, these visual tokens often contain redundant and\nunimportant information, resulting in an unnecessarily high number of tokens.\nTo address this, we introduce PACT, a method that reduces inference time and\nmemory usage by pruning irrelevant tokens and merging visually redundant ones\nat an early layer of the language model. Our approach uses a novel importance\nmetric to identify unimportant tokens without relying on attention scores,\nmaking it compatible with FlashAttention. We also propose a novel clustering\nalgorithm, called Distance Bounded Density Peak Clustering, which efficiently\nclusters visual tokens while constraining the distances between elements within\na cluster by a predefined threshold. We demonstrate the effectiveness of PACT\nthrough extensive experiments."}
{"id": "2504.08778", "pdf": "https://arxiv.org/pdf/2504.08778", "abs": "https://arxiv.org/abs/2504.08778", "authors": ["Bo Xiong", "Steffen Staab"], "title": "From Tokens to Lattices: Emergent Lattice Structures in Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "ICLR 2025", "summary": "Pretrained masked language models (MLMs) have demonstrated an impressive\ncapability to comprehend and encode conceptual knowledge, revealing a lattice\nstructure among concepts. This raises a critical question: how does this\nconceptualization emerge from MLM pretraining? In this paper, we explore this\nproblem from the perspective of Formal Concept Analysis (FCA), a mathematical\nframework that derives concept lattices from the observations of\nobject-attribute relationships. We show that the MLM's objective implicitly\nlearns a \\emph{formal context} that describes objects, attributes, and their\ndependencies, which enables the reconstruction of a concept lattice through\nFCA. We propose a novel framework for concept lattice construction from\npretrained MLMs and investigate the origin of the inductive biases of MLMs in\nlattice structure learning. Our framework differs from previous work because it\ndoes not rely on human-defined concepts and allows for discovering \"latent\"\nconcepts that extend beyond human definitions. We create three datasets for\nevaluation, and the empirical results verify our hypothesis."}
{"id": "2504.08982", "pdf": "https://arxiv.org/pdf/2504.08982", "abs": "https://arxiv.org/abs/2504.08982", "authors": ["Kyle Stein", "Andrew Arash Mahyari", "Guillermo Francia III", "Eman El-Sheikh"], "title": "Adaptive Additive Parameter Updates of Vision Transformers for Few-Shot Continual Learning", "categories": ["cs.CV"], "comment": null, "summary": "Integrating new class information without losing previously acquired\nknowledge remains a central challenge in artificial intelligence, often\nreferred to as catastrophic forgetting. Few-shot class incremental learning\n(FSCIL) addresses this by first training a model on a robust dataset of base\nclasses and then incrementally adapting it in successive sessions using only a\nfew labeled examples per novel class. However, this approach is prone to\noverfitting on the limited new data, which can compromise overall performance\nand exacerbate forgetting. In this work, we propose a simple yet effective\nnovel FSCIL framework that leverages a frozen Vision Transformer (ViT) backbone\naugmented with parameter-efficient additive updates. Our approach freezes the\npre-trained ViT parameters and selectively injects trainable weights into the\nself-attention modules via an additive update mechanism. This design updates\nonly a small subset of parameters to accommodate new classes without\nsacrificing the representations learned during the base session. By fine-tuning\na limited number of parameters, our method preserves the generalizable features\nin the frozen ViT while reducing the risk of overfitting. Furthermore, as most\nparameters remain fixed, the model avoids overwriting previously learned\nknowledge when small novel data batches are introduced. Extensive experiments\non benchmark datasets demonstrate that our approach yields state-of-the-art\nperformance compared to baseline FSCIL methods."}
{"id": "2504.08779", "pdf": "https://arxiv.org/pdf/2504.08779", "abs": "https://arxiv.org/abs/2504.08779", "authors": ["Ruoxin Xiong", "Yanyu Wang", "Suat Gunhan", "Yimin Zhu", "Charles Berryman"], "title": "Can AI Master Construction Management (CM)? Benchmarking State-of-the-Art Large Language Models on CM Certification Exams", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The growing complexity of construction management (CM) projects, coupled with\nchallenges such as strict regulatory requirements and labor shortages, requires\nspecialized analytical tools that streamline project workflow and enhance\nperformance. Although large language models (LLMs) have demonstrated\nexceptional performance in general reasoning tasks, their effectiveness in\ntackling CM-specific challenges, such as precise quantitative analysis and\nregulatory interpretation, remains inadequately explored. To bridge this gap,\nthis study introduces CMExamSet, a comprehensive benchmarking dataset\ncomprising 689 authentic multiple-choice questions sourced from four nationally\naccredited CM certification exams. Our zero-shot evaluation assesses overall\naccuracy, subject areas (e.g., construction safety), reasoning complexity\n(single-step and multi-step), and question formats (text-only,\nfigure-referenced, and table-referenced). The results indicate that GPT-4o and\nClaude 3.7 surpass typical human pass thresholds (70%), with average accuracies\nof 82% and 83%, respectively. Additionally, both models performed better on\nsingle-step tasks, with accuracies of 85.7% (GPT-4o) and 86.7% (Claude 3.7).\nMulti-step tasks were more challenging, reducing performance to 76.5% and\n77.6%, respectively. Furthermore, both LLMs show significant limitations on\nfigure-referenced questions, with accuracies dropping to approximately 40%. Our\nerror pattern analysis further reveals that conceptual misunderstandings are\nthe most common (44.4% and 47.9%), underscoring the need for enhanced\ndomain-specific reasoning models. These findings underscore the potential of\nLLMs as valuable supplementary analytical tools in CM, while highlighting the\nneed for domain-specific refinements and sustained human oversight in complex\ndecision making."}
{"id": "2504.09033", "pdf": "https://arxiv.org/pdf/2504.09033", "abs": "https://arxiv.org/abs/2504.09033", "authors": ["Snigdha Agarwal", "Neelam Sinha"], "title": "Chest X-ray Classification using Deep Convolution Models on Low-resolution images with Uncertain Labels", "categories": ["cs.CV", "cs.AI"], "comment": "5 pages, 5 figures", "summary": "Deep Convolutional Neural Networks have consistently proven to achieve\nstate-of-the-art results on a lot of imaging tasks over the past years'\nmajority of which comprise of high-quality data. However, it is important to\nwork on low-resolution images since it could be a cheaper alternative for\nremote healthcare access where the primary need of automated pathology\nidentification models occurs. Medical diagnosis using low-resolution images is\nchallenging since critical details may not be easily identifiable. In this\npaper, we report classification results by experimenting on different input\nimage sizes of Chest X-rays to deep CNN models and discuss the feasibility of\nclassification on varying image sizes. We also leverage the noisy labels in the\ndataset by proposing a Randomized Flipping of labels techniques. We use an\nensemble of multi-label classification models on frontal and lateral studies.\nOur models are trained on 5 out of the 14 chest pathologies of the publicly\navailable CheXpert dataset. We incorporate techniques such as augmentation,\nregularization for model improvement and use class activation maps to visualize\nthe neural network's decision making. Comparison with classification results on\ndata from 200 subjects, obtained on the corresponding high-resolution images,\nreported in the original CheXpert paper, has been presented. For pathologies\nCardiomegaly, Consolidation and Edema, we obtain 3% higher accuracy with our\nmodel architecture."}
{"id": "2504.08781", "pdf": "https://arxiv.org/pdf/2504.08781", "abs": "https://arxiv.org/abs/2504.08781", "authors": ["Xu-Xiang Zhong", "Chao Yi", "Han-Jia Ye"], "title": "Efficient Evaluation of Large Language Models via Collaborative Filtering", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "With the development of Large Language Models (LLMs), numerous benchmarks\nhave been proposed to measure and compare the capabilities of different LLMs.\nHowever, evaluating LLMs is costly due to the large number of test instances\nand their slow inference speed. In this paper, we aim to explore how to\nefficiently estimate a model's real performance on a given benchmark based on\nits evaluation results on a small number of instances sampled from the\nbenchmark. Inspired by Collaborative Filtering (CF) in Recommendation Systems\n(RS), we treat LLMs as users and test instances as items and propose a\ntwo-stage method. In the first stage, we treat instance selection as\nrecommending products to users to choose instances that can easily distinguish\nmodel performance. In the second stage, we see performance prediction as rating\nprediction problem in RS to predict the target LLM's behavior on unselected\ninstances. Experiments on multiple LLMs and datasets imply that our method can\naccurately estimate the target model's performance while largely reducing its\ninference overhead."}
{"id": "2504.09039", "pdf": "https://arxiv.org/pdf/2504.09039", "abs": "https://arxiv.org/abs/2504.09039", "authors": ["Gen Li", "Yang Xiao", "Jie Ji", "Kaiyuan Deng", "Bo Hui", "Linke Guo", "Xiaolong Ma"], "title": "Sculpting Memory: Multi-Concept Forgetting in Diffusion Models via Dynamic Mask and Concept-Aware Optimization", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Text-to-image (T2I) diffusion models have achieved remarkable success in\ngenerating high-quality images from textual prompts. However, their ability to\nstore vast amounts of knowledge raises concerns in scenarios where selective\nforgetting is necessary, such as removing copyrighted content, reducing biases,\nor eliminating harmful concepts. While existing unlearning methods can remove\ncertain concepts, they struggle with multi-concept forgetting due to\ninstability, residual knowledge persistence, and generation quality\ndegradation. To address these challenges, we propose \\textbf{Dynamic Mask\ncoupled with Concept-Aware Loss}, a novel unlearning framework designed for\nmulti-concept forgetting in diffusion models. Our \\textbf{Dynamic Mask}\nmechanism adaptively updates gradient masks based on current optimization\nstates, allowing selective weight modifications that prevent interference with\nunrelated knowledge. Additionally, our \\textbf{Concept-Aware Loss} explicitly\nguides the unlearning process by enforcing semantic consistency through\nsuperclass alignment, while a regularization loss based on knowledge\ndistillation ensures that previously unlearned concepts remain forgotten during\nsequential unlearning. We conduct extensive experiments to evaluate our\napproach. Results demonstrate that our method outperforms existing unlearning\ntechniques in forgetting effectiveness, output fidelity, and semantic\ncoherence, particularly in multi-concept scenarios. Our work provides a\nprincipled and flexible framework for stable and high-fidelity unlearning in\ngenerative models. The code will be released publicly."}
{"id": "2504.08792", "pdf": "https://arxiv.org/pdf/2504.08792", "abs": "https://arxiv.org/abs/2504.08792", "authors": ["Toqeer Ehsan", "Thamar Solorio"], "title": "Enhancing NER Performance in Low-Resource Pakistani Languages using Cross-Lingual Data Augmentation", "categories": ["cs.CL", "cs.IR"], "comment": "Accepted to W-NUT 2025 @ NAACL", "summary": "Named Entity Recognition (NER), a fundamental task in Natural Language\nProcessing (NLP), has shown significant advancements for high-resource\nlanguages. However, due to a lack of annotated datasets and limited\nrepresentation in Pre-trained Language Models (PLMs), it remains understudied\nand challenging for low-resource languages. To address these challenges, we\npropose a data augmentation technique that generates culturally plausible\nsentences and experiments on four low-resource Pakistani languages; Urdu,\nShahmukhi, Sindhi, and Pashto. By fine-tuning multilingual masked Large\nLanguage Models (LLMs), our approach demonstrates significant improvements in\nNER performance for Shahmukhi and Pashto. We further explore the capability of\ngenerative LLMs for NER and data augmentation using few-shot learning."}
{"id": "2504.09048", "pdf": "https://arxiv.org/pdf/2504.09048", "abs": "https://arxiv.org/abs/2504.09048", "authors": ["Yongchang Wu", "Zipeng Qi", "Zhenwei Shi", "Zhengxia Zou"], "title": "BlockGaussian: Efficient Large-Scale Scene NovelView Synthesis via Adaptive Block-Based Gaussian Splatting", "categories": ["cs.CV"], "comment": "https://github.com/SunshineWYC/BlockGaussian", "summary": "The recent advancements in 3D Gaussian Splatting (3DGS) have demonstrated\nremarkable potential in novel view synthesis tasks. The divide-and-conquer\nparadigm has enabled large-scale scene reconstruction, but significant\nchallenges remain in scene partitioning, optimization, and merging processes.\nThis paper introduces BlockGaussian, a novel framework incorporating a\ncontent-aware scene partition strategy and visibility-aware block optimization\nto achieve efficient and high-quality large-scale scene reconstruction.\nSpecifically, our approach considers the content-complexity variation across\ndifferent regions and balances computational load during scene partitioning,\nenabling efficient scene reconstruction. To tackle the supervision mismatch\nissue during independent block optimization, we introduce auxiliary points\nduring individual block optimization to align the ground-truth supervision,\nwhich enhances the reconstruction quality. Furthermore, we propose a\npseudo-view geometry constraint that effectively mitigates rendering\ndegradation caused by airspace floaters during block merging. Extensive\nexperiments on large-scale scenes demonstrate that our approach achieves\nstate-of-the-art performance in both reconstruction efficiency and rendering\nquality, with a 5x speedup in optimization and an average PSNR improvement of\n1.21 dB on multiple benchmarks. Notably, BlockGaussian significantly reduces\ncomputational requirements, enabling large-scale scene reconstruction on a\nsingle 24GB VRAM device. The project page is available at\nhttps://github.com/SunshineWYC/BlockGaussian"}
{"id": "2504.08798", "pdf": "https://arxiv.org/pdf/2504.08798", "abs": "https://arxiv.org/abs/2504.08798", "authors": ["Xiaomei Zhang", "Zhaoxi Zhang", "Yanjun Zhang", "Xufei Zheng", "Leo Yu Zhang", "Shengshan Hu", "Shirui Pan"], "title": "Exploring Gradient-Guided Masked Language Model to Detect Textual Adversarial Attacks", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Textual adversarial examples pose serious threats to the reliability of\nnatural language processing systems. Recent studies suggest that adversarial\nexamples tend to deviate from the underlying manifold of normal texts, whereas\npre-trained masked language models can approximate the manifold of normal data.\nThese findings inspire the exploration of masked language models for detecting\ntextual adversarial attacks. We first introduce Masked Language Model-based\nDetection (MLMD), leveraging the mask and unmask operations of the masked\nlanguage modeling (MLM) objective to induce the difference in manifold changes\nbetween normal and adversarial texts. Although MLMD achieves competitive\ndetection performance, its exhaustive one-by-one masking strategy introduces\nsignificant computational overhead. Our posterior analysis reveals that a\nsignificant number of non-keywords in the input are not important for detection\nbut consume resources. Building on this, we introduce Gradient-guided MLMD\n(GradMLMD), which leverages gradient information to identify and skip\nnon-keywords during detection, significantly reducing resource consumption\nwithout compromising detection performance."}
{"id": "2504.09062", "pdf": "https://arxiv.org/pdf/2504.09062", "abs": "https://arxiv.org/abs/2504.09062", "authors": ["Zhijie Shen", "Chunyu Lin", "Shujuan Huang", "Lang Nie", "Kang Liao", "Yao Zhao"], "title": "You Need a Transition Plane: Bridging Continuous Panoramic 3D Reconstruction with Perspective Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Recently, reconstructing scenes from a single panoramic image using advanced\n3D Gaussian Splatting (3DGS) techniques has attracted growing interest.\nPanoramic images offer a 360$\\times$ 180 field of view (FoV), capturing the\nentire scene in a single shot. However, panoramic images introduce severe\ndistortion, making it challenging to render 3D Gaussians into 2D distorted\nequirectangular space directly. Converting equirectangular images to cubemap\nprojections partially alleviates this problem but introduces new challenges,\nsuch as projection distortion and discontinuities across cube-face boundaries.\nTo address these limitations, we present a novel framework, named TPGS, to\nbridge continuous panoramic 3D scene reconstruction with perspective Gaussian\nsplatting. Firstly, we introduce a Transition Plane between adjacent cube faces\nto enable smoother transitions in splatting directions and mitigate\noptimization ambiguity in the boundary region. Moreover, an intra-to-inter face\noptimization strategy is proposed to enhance local details and restore visual\nconsistency across cube-face boundaries. Specifically, we optimize 3D Gaussians\nwithin individual cube faces and then fine-tune them in the stitched panoramic\nspace. Additionally, we introduce a spherical sampling technique to eliminate\nvisible stitching seams. Extensive experiments on indoor and outdoor,\negocentric, and roaming benchmark datasets demonstrate that our approach\noutperforms existing state-of-the-art methods. Code and models will be\navailable at https://github.com/zhijieshen-bjtu/TPGS."}
{"id": "2504.08808", "pdf": "https://arxiv.org/pdf/2504.08808", "abs": "https://arxiv.org/abs/2504.08808", "authors": ["Zhengke Sun", "Hangwei Qian", "Ivor Tsang"], "title": "Exploring the Effectiveness and Interpretability of Texts in LLM-based Time Series Models", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have been applied to time series forecasting\ntasks, leveraging pre-trained language models as the backbone and incorporating\ntextual data to purportedly enhance the comprehensive capabilities of LLMs for\ntime series. However, are these texts really helpful for interpretation? This\nstudy seeks to investigate the actual efficacy and interpretability of such\ntextual incorporations. Through a series of empirical experiments on textual\nprompts and textual prototypes, our findings reveal that the misalignment\nbetween two modalities exists, and the textual information does not\nsignificantly improve time series forecasting performance in many cases.\nFurthermore, visualization analysis indicates that the textual representations\nlearned by existing frameworks lack sufficient interpretability when applied to\ntime series data. We further propose a novel metric named Semantic Matching\nIndex (SMI) to better evaluate the matching degree between time series and\ntexts during our post hoc interpretability investigation. Our analysis reveals\nthe misalignment and limited interpretability of texts in current time-series\nLLMs, and we hope this study can raise awareness of the interpretability of\ntexts for time series. The code is available at\nhttps://github.com/zachysun/TS-Lang-Exp."}
{"id": "2504.09066", "pdf": "https://arxiv.org/pdf/2504.09066", "abs": "https://arxiv.org/abs/2504.09066", "authors": ["Yifan Yang", "Lei Zou", "Bing Zhou", "Daoyang Li", "Binbin Lin", "Joynal Abedin", "Mingzheng Yang"], "title": "Hyperlocal disaster damage assessment using bi-temporal street-view imagery and pre-trained vision models", "categories": ["cs.CV"], "comment": "27 pages,9 figures", "summary": "Street-view images offer unique advantages for disaster damage estimation as\nthey capture impacts from a visual perspective and provide detailed,\non-the-ground insights. Despite several investigations attempting to analyze\nstreet-view images for damage estimation, they mainly focus on post-disaster\nimages. The potential of time-series street-view images remains underexplored.\nPre-disaster images provide valuable benchmarks for accurate damage estimations\nat building and street levels. These images could aid annotators in objectively\nlabeling post-disaster impacts, improving the reliability of labeled data sets\nfor model training, and potentially enhancing the model performance in damage\nevaluation. The goal of this study is to estimate hyperlocal, on-the-ground\ndisaster damages using bi-temporal street-view images and advanced pre-trained\nvision models. Street-view images before and after 2024 Hurricane Milton in\nHorseshoe Beach, Florida, were collected for experiments. The objectives are:\n(1) to assess the performance gains of incorporating pre-disaster street-view\nimages as a no-damage category in fine-tuning pre-trained models, including\nSwin Transformer and ConvNeXt, for damage level classification; (2) to design\nand evaluate a dual-channel algorithm that reads pair-wise pre- and\npost-disaster street-view images for hyperlocal damage assessment. The results\nindicate that incorporating pre-disaster street-view images and employing a\ndual-channel processing framework can significantly enhance damage assessment\naccuracy. The accuracy improves from 66.14% with the Swin Transformer baseline\nto 77.11% with the dual-channel Feature-Fusion ConvNeXt model. This research\nenables rapid, operational damage assessments at hyperlocal spatial\nresolutions, providing valuable insights to support effective decision-making\nin disaster management and resilience planning."}
{"id": "2504.08820", "pdf": "https://arxiv.org/pdf/2504.08820", "abs": "https://arxiv.org/abs/2504.08820", "authors": ["Jing Yao", "Xiaoyuan Yi", "Jindong Wang", "Zhicheng Dou", "Xing Xie"], "title": "CAReDiO: Cultural Alignment of LLM via Representativeness and Distinctiveness Guided Data Optimization", "categories": ["cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) more deeply integrate into human life across\nvarious regions, aligning them with pluralistic cultures is crucial for\nimproving user experience and mitigating cultural conflicts. Existing\napproaches develop culturally aligned LLMs primarily through fine-tuning with\nmassive carefully curated culture-specific corpora. Nevertheless, inspired by\nculture theories, we identify two key challenges faced by these datasets: (1)\nRepresentativeness: These corpora fail to fully capture the target culture's\ncore characteristics with redundancy, causing computation waste; (2)\nDistinctiveness: They struggle to distinguish the unique nuances of a given\nculture from shared patterns across other relevant ones, hindering precise\ncultural modeling. To handle these challenges, we introduce CAReDiO, a novel\ncultural data construction framework. Specifically, CAReDiO utilizes powerful\nLLMs to automatically generate cultural conversation data, where both the\nqueries and responses are further optimized by maximizing representativeness\nand distinctiveness. Using CAReDiO, we construct a small yet effective dataset,\ncovering five cultures, and compare it with several recent cultural corpora.\nExtensive experiments demonstrate that our method generates more effective data\nand enables cultural alignment with as few as 100 training samples, enhancing\nboth performance and efficiency."}
{"id": "2504.09069", "pdf": "https://arxiv.org/pdf/2504.09069", "abs": "https://arxiv.org/abs/2504.09069", "authors": ["Shuning Sun", "Yu Zhang", "Chen Wu", "Dianjie Lu", "Dianjie Lu", "Guijuan Zhan", "Yang Weng", "Zhuoran Zheng"], "title": "UniFlowRestore: A General Video Restoration Framework via Flow Matching and Prompt Guidance", "categories": ["cs.CV"], "comment": null, "summary": "Video imaging is often affected by complex degradations such as blur, noise,\nand compression artifacts. Traditional restoration methods follow a\n\"single-task single-model\" paradigm, resulting in poor generalization and high\ncomputational cost, limiting their applicability in real-world scenarios with\ndiverse degradation types. We propose UniFlowRestore, a general video\nrestoration framework that models restoration as a time-continuous evolution\nunder a prompt-guided and physics-informed vector field. A physics-aware\nbackbone PhysicsUNet encodes degradation priors as potential energy, while\nPromptGenerator produces task-relevant prompts as momentum. These components\ndefine a Hamiltonian system whose vector field integrates inertial dynamics,\ndecaying physical gradients, and prompt-based guidance. The system is optimized\nvia a fixed-step ODE solver to achieve efficient and unified restoration across\ntasks. Experiments show that UniFlowRestore delivers stateof-the-art\nperformance with strong generalization and efficiency. Quantitative results\ndemonstrate that UniFlowRestore achieves state-of-the-art performance,\nattaining the highest PSNR (33.89 dB) and SSIM (0.97) on the video denoising\ntask, while maintaining top or second-best scores across all evaluated tasks."}
{"id": "2504.08838", "pdf": "https://arxiv.org/pdf/2504.08838", "abs": "https://arxiv.org/abs/2504.08838", "authors": ["Mike Lasby", "Nish Sinnadurai", "Valavan Manohararajah", "Sean Lie", "Vithursan Thangarasa"], "title": "SD$^2$: Self-Distilled Sparse Drafters", "categories": ["cs.CL", "cs.AI", "I.2.0; I.2.7"], "comment": "21 pages", "summary": "Speculative decoding is a powerful technique for reducing the latency of\nLarge Language Models (LLMs), offering a fault-tolerant framework that enables\nthe use of highly compressed draft models. In this work, we introduce\nSelf-Distilled Sparse Drafters (SD$^2$), a novel methodology that leverages\nself-data distillation and fine-grained weight sparsity to produce highly\nefficient and well-aligned draft models. SD$^2$ systematically enhances draft\ntoken acceptance rates while significantly reducing Multiply-Accumulate\noperations (MACs), even in the Universal Assisted Generation (UAG) setting,\nwhere draft and target models originate from different model families. On a\nLlama-3.1-70B target model, SD$^2$ provides a $\\times$1.59 higher Mean Accepted\nLength (MAL) compared to layer-pruned draft models and reduces MACs by over\n43.87% with a 8.36% reduction in MAL compared to a dense draft models. Our\nresults highlight the potential of sparsity-aware fine-tuning and compression\nstrategies to improve LLM inference efficiency while maintaining alignment with\ntarget models."}
{"id": "2504.09076", "pdf": "https://arxiv.org/pdf/2504.09076", "abs": "https://arxiv.org/abs/2504.09076", "authors": ["Mk Bashar", "Ocean Monjur", "Samia Islam", "Mohammad Galib Shams", "Niamul Quader"], "title": "Exploring Synergistic Ensemble Learning: Uniting CNNs, MLP-Mixers, and Vision Transformers to Enhance Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, Convolutional Neural Networks (CNNs), MLP-mixers, and Vision\nTransformers have risen to prominence as leading neural architectures in image\nclassification. Prior research has underscored the distinct advantages of each\narchitecture, and there is growing evidence that combining modules from\ndifferent architectures can boost performance. In this study, we build upon and\nimprove previous work exploring the complementarity between different\narchitectures. Instead of heuristically merging modules from various\narchitectures through trial and error, we preserve the integrity of each\narchitecture and combine them using ensemble techniques. By maintaining the\ndistinctiveness of each architecture, we aim to explore their inherent\ncomplementarity more deeply and with implicit isolation. This approach provides\na more systematic understanding of their individual strengths.\n  In addition to uncovering insights into architectural complementarity, we\nshowcase the effectiveness of even basic ensemble methods that combine models\nfrom diverse architectures. These methods outperform ensembles comprised of\nsimilar architectures. Our straightforward ensemble framework serves as a\nfoundational strategy for blending complementary architectures, offering a\nsolid starting point for further investigations into the unique strengths and\nsynergies among different architectures and their ensembles in image\nclassification. A direct outcome of this work is the creation of an ensemble of\nclassification networks that surpasses the accuracy of the previous\nstate-of-the-art single classification network on ImageNet, setting a new\nbenchmark, all while requiring less overall latency."}
{"id": "2504.08905", "pdf": "https://arxiv.org/pdf/2504.08905", "abs": "https://arxiv.org/abs/2504.08905", "authors": ["Yunfan Zhang", "Kathleen McKeown", "Smaranda Muresan"], "title": "Forecasting Communication Derailments Through Conversation Generation", "categories": ["cs.CL"], "comment": null, "summary": "Forecasting communication derailment can be useful in real-world settings\nsuch as online content moderation, conflict resolution, and business\nnegotiations. However, despite language models' success at identifying\noffensive speech present in conversations, they struggle to forecast future\ncommunication derailments. In contrast to prior work that predicts conversation\noutcomes solely based on the past conversation history, our approach samples\nmultiple future conversation trajectories conditioned on existing conversation\nhistory using a fine-tuned LLM. It predicts the communication outcome based on\nthe consensus of these trajectories. We also experimented with leveraging\nsocio-linguistic attributes, which reflect turn-level conversation dynamics, as\nguidance when generating future conversations. Our method of future\nconversation trajectories surpasses state-of-the-art results on English\ncommunication derailment prediction benchmarks and demonstrates significant\naccuracy gains in ablation studies."}
{"id": "2504.09077", "pdf": "https://arxiv.org/pdf/2504.09077", "abs": "https://arxiv.org/abs/2504.09077", "authors": ["Bingyu Nan", "Feng Liu", "Xuezhong Qian", "Wei Song"], "title": "A Visual Self-attention Mechanism Facial Expression Recognition Network beyond Convnext", "categories": ["cs.CV"], "comment": null, "summary": "Facial expression recognition is an important research direction in the field\nof artificial intelligence. Although new breakthroughs have been made in recent\nyears, the uneven distribution of datasets and the similarity between different\ncategories of facial expressions, as well as the differences within the same\ncategory among different subjects, remain challenges. This paper proposes a\nvisual facial expression signal feature processing network based on truncated\nConvNeXt approach(Conv-cut), to improve the accuracy of FER under challenging\nconditions. The network uses a truncated ConvNeXt-Base as the feature\nextractor, and then we designed a Detail Extraction Block to extract detailed\nfeatures, and introduced a Self-Attention mechanism to enable the network to\nlearn the extracted features more effectively. To evaluate the proposed\nConv-cut approach, we conducted experiments on the RAF-DB and FERPlus datasets,\nand the results show that our model has achieved state-of-the-art performance.\nOur code could be accessed at Github."}
{"id": "2504.08958", "pdf": "https://arxiv.org/pdf/2504.08958", "abs": "https://arxiv.org/abs/2504.08958", "authors": ["Mehmet Arif Demirtaş", "Claire Zheng", "Max Fowler", "Kathryn Cunningham"], "title": "Generating Planning Feedback for Open-Ended Programming Exercises with LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted as full paper at AIED 2025", "summary": "To complete an open-ended programming exercise, students need to both plan a\nhigh-level solution and implement it using the appropriate syntax. However,\nthese problems are often autograded on the correctness of the final submission\nthrough test cases, and students cannot get feedback on their planning process.\nLarge language models (LLM) may be able to generate this feedback by detecting\nthe overall code structure even for submissions with syntax errors. To this\nend, we propose an approach that detects which high-level goals and patterns\n(i.e. programming plans) exist in a student program with LLMs. We show that\nboth the full GPT-4o model and a small variant (GPT-4o-mini) can detect these\nplans with remarkable accuracy, outperforming baselines inspired by\nconventional approaches to code analysis. We further show that the smaller,\ncost-effective variant (GPT-4o-mini) achieves results on par with\nstate-of-the-art (GPT-4o) after fine-tuning, creating promising implications\nfor smaller models for real-time grading. These smaller models can be\nincorporated into autograders for open-ended code-writing exercises to provide\nfeedback for students' implicit planning skills, even when their program is\nsyntactically incorrect. Furthermore, LLMs may be useful in providing feedback\nfor problems in other domains where students start with a set of high-level\nsolution steps and iteratively compute the output, such as math and physics\nproblems."}
{"id": "2504.09083", "pdf": "https://arxiv.org/pdf/2504.09083", "abs": "https://arxiv.org/abs/2504.09083", "authors": ["Muhammad Adil", "Gaang Lee", "Vicente A. Gonzalez", "Qipei Mei"], "title": "Using Vision Language Models for Safety Hazard Identification in Construction", "categories": ["cs.CV"], "comment": null, "summary": "Safety hazard identification and prevention are the key elements of proactive\nsafety management. Previous research has extensively explored the applications\nof computer vision to automatically identify hazards from image clips collected\nfrom construction sites. However, these methods struggle to identify\ncontext-specific hazards, as they focus on detecting predefined individual\nentities without understanding their spatial relationships and interactions.\nFurthermore, their limited adaptability to varying construction site guidelines\nand conditions hinders their generalization across different projects. These\nlimitations reduce their ability to assess hazards in complex construction\nenvironments and adaptability to unseen risks, leading to potential safety\ngaps. To address these challenges, we proposed and experimentally validated a\nVision Language Model (VLM)-based framework for the identification of\nconstruction hazards. The framework incorporates a prompt engineering module\nthat structures safety guidelines into contextual queries, allowing VLM to\nprocess visual information and generate hazard assessments aligned with the\nregulation guide. Within this framework, we evaluated state-of-the-art VLMs,\nincluding GPT-4o, Gemini, Llama 3.2, and InternVL2, using a custom dataset of\n1100 construction site images. Experimental results show that GPT-4o and Gemini\n1.5 Pro outperformed alternatives and displayed promising BERTScore of 0.906\nand 0.888 respectively, highlighting their ability to identify both general and\ncontext-specific hazards. However, processing times remain a significant\nchallenge, impacting real-time feasibility. These findings offer insights into\nthe practical deployment of VLMs for construction site hazard detection,\nthereby contributing to the enhancement of proactive safety management."}
{"id": "2504.08961", "pdf": "https://arxiv.org/pdf/2504.08961", "abs": "https://arxiv.org/abs/2504.08961", "authors": ["Kseniia Petukhova", "Ekaterina Kochmar"], "title": "A Fully Automated Pipeline for Conversational Discourse Annotation: Tree Scheme Generation and Labeling with Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have shown promise in\nautomating discourse annotation for conversations. While manually designing\ntree annotation schemes significantly improves annotation quality for humans\nand models, their creation remains time-consuming and requires expert\nknowledge. We propose a fully automated pipeline that uses LLMs to construct\nsuch schemes and perform annotation. We evaluate our approach on speech\nfunctions (SFs) and the Switchboard-DAMSL (SWBD-DAMSL) taxonomies. Our\nexperiments compare various design choices, and we show that frequency-guided\ndecision trees, paired with an advanced LLM for annotation, can outperform\npreviously manually designed trees and even match or surpass human annotators\nwhile significantly reducing the time required for annotation. We release all\ncode and resultant schemes and annotations to facilitate future research on\ndiscourse annotation."}
{"id": "2504.09086", "pdf": "https://arxiv.org/pdf/2504.09086", "abs": "https://arxiv.org/abs/2504.09086", "authors": ["Yunfei Long", "Abhinav Kumar", "Xiaoming Liu", "Daniel Morris"], "title": "RICCARDO: Radar Hit Prediction and Convolution for Camera-Radar 3D Object Detection", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Radar hits reflect from points on both the boundary and internal to object\noutlines. This results in a complex distribution of radar hits that depends on\nfactors including object category, size, and orientation. Current radar-camera\nfusion methods implicitly account for this with a black-box neural network. In\nthis paper, we explicitly utilize a radar hit distribution model to assist\nfusion. First, we build a model to predict radar hit distributions conditioned\non object properties obtained from a monocular detector. Second, we use the\npredicted distribution as a kernel to match actual measured radar points in the\nneighborhood of the monocular detections, generating matching scores at nearby\npositions. Finally, a fusion stage combines context with the kernel detector to\nrefine the matching scores. Our method achieves the state-of-the-art\nradar-camera detection performance on nuScenes. Our source code is available at\nhttps://github.com/longyunf/riccardo."}
{"id": "2504.09049", "pdf": "https://arxiv.org/pdf/2504.09049", "abs": "https://arxiv.org/abs/2504.09049", "authors": ["Adrianna Romanowski", "Pedro H. V. Valois", "Kazuhiro Fukui"], "title": "From Punchlines to Predictions: A Metric to Assess LLM Performance in Identifying Humor in Stand-Up Comedy", "categories": ["cs.CL"], "comment": "Accepted to CMCL2025 @ NAACL", "summary": "Comedy serves as a profound reflection of the times we live in and is a\nstaple element of human interactions. In light of the widespread adoption of\nLarge Language Models (LLMs), the intersection of humor and AI has become no\nlaughing matter. Advancements in the naturalness of human-computer interaction\ncorrelates with improvements in AI systems' abilities to understand humor. In\nthis study, we assess the ability of models in accurately identifying humorous\nquotes from a stand-up comedy transcript. Stand-up comedy's unique comedic\nnarratives make it an ideal dataset to improve the overall naturalness of\ncomedic understanding. We propose a novel humor detection metric designed to\nevaluate LLMs amongst various prompts on their capability to extract humorous\npunchlines. The metric has a modular structure that offers three different\nscoring methods - fuzzy string matching, sentence embedding, and subspace\nsimilarity - to provide an overarching assessment of a model's performance. The\nmodel's results are compared against those of human evaluators on the same\ntask. Our metric reveals that regardless of prompt engineering, leading models,\nChatGPT, Claude, and DeepSeek, achieve scores of at most 51% in humor\ndetection. Notably, this performance surpasses that of humans who achieve a\nscore of 41%. The analysis of human evaluators and LLMs reveals variability in\nagreement, highlighting the subjectivity inherent in humor and the complexities\ninvolved in extracting humorous quotes from live performance transcripts. Code\navailable at https://github.com/swaggirl9000/humor."}
{"id": "2504.09097", "pdf": "https://arxiv.org/pdf/2504.09097", "abs": "https://arxiv.org/abs/2504.09097", "authors": ["Jeongwan On", "Kyeonghwan Gwak", "Gunyoung Kang", "Junuk Cha", "Soohyun Hwang", "Hyein Hwang", "Seungryul Baek"], "title": "BIGS: Bimanual Category-agnostic Interaction Reconstruction from Monocular Videos via 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Reconstructing 3Ds of hand-object interaction (HOI) is a fundamental problem\nthat can find numerous applications. Despite recent advances, there is no\ncomprehensive pipeline yet for bimanual class-agnostic interaction\nreconstruction from a monocular RGB video, where two hands and an unknown\nobject are interacting with each other. Previous works tackled the limited\nhand-object interaction case, where object templates are pre-known or only one\nhand is involved in the interaction. The bimanual interaction reconstruction\nexhibits severe occlusions introduced by complex interactions between two hands\nand an object. To solve this, we first introduce BIGS (Bimanual Interaction 3D\nGaussian Splatting), a method that reconstructs 3D Gaussians of hands and an\nunknown object from a monocular video. To robustly obtain object Gaussians\navoiding severe occlusions, we leverage prior knowledge of pre-trained\ndiffusion model with score distillation sampling (SDS) loss, to reconstruct\nunseen object parts. For hand Gaussians, we exploit the 3D priors of hand model\n(i.e., MANO) and share a single Gaussian for two hands to effectively\naccumulate hand 3D information, given limited views. To further consider the 3D\nalignment between hands and objects, we include the interacting-subjects\noptimization step during Gaussian optimization. Our method achieves the\nstate-of-the-art accuracy on two challenging datasets, in terms of 3D hand pose\nestimation (MPJPE), 3D object reconstruction (CDh, CDo, F10), and rendering\nquality (PSNR, SSIM, LPIPS), respectively."}
{"id": "2504.09071", "pdf": "https://arxiv.org/pdf/2504.09071", "abs": "https://arxiv.org/abs/2504.09071", "authors": ["Matt Grenander", "Siddharth Varia", "Paula Czarnowska", "Yogarshi Vyas", "Kishaloy Halder", "Bonan Min"], "title": "Exploration of Plan-Guided Summarization for Narrative Texts: the Case of Small Language Models", "categories": ["cs.CL"], "comment": "Accepted to the 7th Workshop on Narrative Understanding (WNU),\n  co-located with NAACL 2025", "summary": "Plan-guided summarization attempts to reduce hallucinations in small language\nmodels (SLMs) by grounding generated summaries to the source text, typically by\ntargeting fine-grained details such as dates or named entities. In this work,\nwe investigate whether plan-based approaches in SLMs improve summarization in\nlong document, narrative tasks. Narrative texts' length and complexity often\nmean they are difficult to summarize faithfully. We analyze existing\nplan-guided solutions targeting fine-grained details, and also propose our own\nhigher-level, narrative-based plan formulation. Our results show that neither\napproach significantly improves on a baseline without planning in either\nsummary quality or faithfulness. Human evaluation reveals that while\nplan-guided approaches are often well grounded to their plan, plans are equally\nlikely to contain hallucinations compared to summaries. As a result, the\nplan-guided summaries are just as unfaithful as those from models without\nplanning. Our work serves as a cautionary tale to plan-guided approaches to\nsummarization, especially for long, complex domains such as narrative texts."}
{"id": "2504.09106", "pdf": "https://arxiv.org/pdf/2504.09106", "abs": "https://arxiv.org/abs/2504.09106", "authors": ["Yonghao Huang", "Leiting Chen", "Chuan Zhou"], "title": "Multi-modal and Multi-view Fundus Image Fusion for Retinopathy Diagnosis via Multi-scale Cross-attention and Shifted Window Self-attention", "categories": ["cs.CV"], "comment": null, "summary": "The joint interpretation of multi-modal and multi-view fundus images is\ncritical for retinopathy prevention, as different views can show the complete\n3D eyeball field and different modalities can provide complementary lesion\nareas. Compared with single images, the sequence relationships in multi-modal\nand multi-view fundus images contain long-range dependencies in lesion\nfeatures. By modeling the long-range dependencies in these sequences, lesion\nareas can be more comprehensively mined, and modality-specific lesions can be\ndetected. To learn the long-range dependency relationship and fuse\ncomplementary multi-scale lesion features between different fundus modalities,\nwe design a multi-modal fundus image fusion method based on multi-scale\ncross-attention, which solves the static receptive field problem in previous\nmulti-modal medical fusion methods based on attention. To capture multi-view\nrelative positional relationships between different views and fuse\ncomprehensive lesion features between different views, we design a multi-view\nfundus image fusion method based on shifted window self-attention, which also\nsolves the computational complexity of the multi-view fundus fusion method\nbased on self-attention is quadratic to the size and number of multi-view\nfundus images. Finally, we design a multi-task retinopathy diagnosis framework\nto help ophthalmologists reduce workload and improve diagnostic accuracy by\ncombining the proposed two fusion methods. The experimental results of\nretinopathy classification and report generation tasks indicate our method's\npotential to improve the efficiency and reliability of retinopathy diagnosis in\nclinical practice, achieving a classification accuracy of 82.53\\% and a report\ngeneration BlEU-1 of 0.543."}
{"id": "2504.09073", "pdf": "https://arxiv.org/pdf/2504.09073", "abs": "https://arxiv.org/abs/2504.09073", "authors": ["Akanksha Mehndiratta", "Krishna Asawa"], "title": "A Multi-view Discourse Framework for Integrating Semantic and Syntactic Features in Dialog Agents", "categories": ["cs.CL"], "comment": null, "summary": "Multiturn dialogue models aim to generate human-like responses by leveraging\nconversational context, consisting of utterances from previous exchanges.\nExisting methods often neglect the interactions between these utterances or\ntreat all of them as equally significant. This paper introduces a\ndiscourse-aware framework for response selection in retrieval-based dialogue\nsystems. The proposed model first encodes each utterance and response with\ncontextual, positional, and syntactic features using Multi-view Canonical\nCorrelation Analysis (MCCA). It then learns discourse tokens that capture\nrelationships between an utterance and its surrounding turns in a shared\nsubspace via Canonical Correlation Analysis (CCA). This two-step approach\neffectively integrates semantic and syntactic features to build discourse-level\nunderstanding. Experiments on the Ubuntu Dialogue Corpus demonstrate that our\nmodel achieves significant improvements in automatic evaluation metrics,\nhighlighting its effectiveness in response selection."}
{"id": "2504.09109", "pdf": "https://arxiv.org/pdf/2504.09109", "abs": "https://arxiv.org/abs/2504.09109", "authors": ["Ganxi Xu", "Jinyi Long", "Hanrui Wu", "Jia Zhang"], "title": "Probability Distribution Alignment and Low-Rank Weight Decomposition for Source-Free Domain Adaptive Brain Decoding", "categories": ["cs.CV"], "comment": null, "summary": "Brain decoding currently faces significant challenges in individual\ndifferences, modality alignment, and high-dimensional embeddings. To address\nindividual differences, researchers often use source subject data, which leads\nto issues such as privacy leakage and heavy data storage burdens. In modality\nalignment, current works focus on aligning the softmax probability distribution\nbut neglect the alignment of marginal probability distributions, resulting in\nmodality misalignment. Additionally, images and text are aligned separately\nwith fMRI without considering the complex interplay between images and text,\nleading to poor image reconstruction. Finally, the enormous dimensionality of\nCLIP embeddings causes significant computational costs. Although the\ndimensionality of CLIP embeddings can be reduced by ignoring the number of\npatches obtained from images and the number of tokens acquired from text, this\ncomes at the cost of a significant drop in model performance, creating a\ndilemma. To overcome these limitations, we propose a source-free domain\nadaptation-based brain decoding framework"}
{"id": "2504.09094", "pdf": "https://arxiv.org/pdf/2504.09094", "abs": "https://arxiv.org/abs/2504.09094", "authors": ["Akanksha Mehndiratta", "Krishna Asawa"], "title": "Enhancing Dialogue Systems with Discourse-Level Understanding Using Deep Canonical Correlation Analysis", "categories": ["cs.CL"], "comment": null, "summary": "The evolution of conversational agents has been driven by the need for more\ncontextually aware systems that can effectively manage dialogue over extended\ninteractions. To address the limitations of existing models in capturing and\nutilizing long-term conversational history, we propose a novel framework that\nintegrates Deep Canonical Correlation Analysis (DCCA) for discourse-level\nunderstanding. This framework learns discourse tokens to capture relationships\nbetween utterances and their surrounding context, enabling a better\nunderstanding of long-term dependencies. Experiments on the Ubuntu Dialogue\nCorpus demonstrate significant enhancement in response selection, based on the\nimproved automatic evaluation metric scores. The results highlight the\npotential of DCCA in improving dialogue systems by allowing them to filter out\nirrelevant context and retain critical discourse information for more accurate\nresponse retrieval."}
{"id": "2504.09129", "pdf": "https://arxiv.org/pdf/2504.09129", "abs": "https://arxiv.org/abs/2504.09129", "authors": ["Jizong Peng", "Tze Ho Elden Tse", "Kai Xu", "Wenchao Gao", "Angela Yao"], "title": "A Constrained Optimization Approach for Gaussian Splatting from Coarsely-posed Images and Noisy Lidar Point Clouds", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) is a powerful reconstruction technique, but it\nneeds to be initialized from accurate camera poses and high-fidelity point\nclouds. Typically, the initialization is taken from Structure-from-Motion (SfM)\nalgorithms; however, SfM is time-consuming and restricts the application of\n3DGS in real-world scenarios and large-scale scene reconstruction. We introduce\na constrained optimization method for simultaneous camera pose estimation and\n3D reconstruction that does not require SfM support. Core to our approach is\ndecomposing a camera pose into a sequence of camera-to-(device-)center and\n(device-)center-to-world optimizations. To facilitate, we propose two\noptimization constraints conditioned to the sensitivity of each parameter group\nand restricts each parameter's search space. In addition, as we learn the scene\ngeometry directly from the noisy point clouds, we propose geometric constraints\nto improve the reconstruction quality. Experiments demonstrate that the\nproposed method significantly outperforms the existing (multi-modal) 3DGS\nbaseline and methods supplemented by COLMAP on both our collected dataset and\ntwo public benchmarks."}
{"id": "2504.09118", "pdf": "https://arxiv.org/pdf/2504.09118", "abs": "https://arxiv.org/abs/2504.09118", "authors": ["Yifei He", "Måns I. Andersson", "Stefano Markidis"], "title": "Optimizing FDTD Solvers for Electromagnetics: A Compiler-Guided Approach with High-Level Tensor Abstractions", "categories": ["cs.CL"], "comment": null, "summary": "The Finite Difference Time Domain (FDTD) method is a widely used numerical\ntechnique for solving Maxwell's equations, particularly in computational\nelectromagnetics and photonics. It enables accurate modeling of wave\npropagation in complex media and structures but comes with significant\ncomputational challenges. Traditional FDTD implementations rely on handwritten,\nplatform-specific code that optimizes certain kernels while underperforming in\nothers. The lack of portability increases development overhead and creates\nperformance bottlenecks, limiting scalability across modern hardware\narchitectures. To address these challenges, we introduce an end-to-end\ndomain-specific compiler based on the MLIR/LLVM infrastructure for FDTD\nsimulations. Our approach generates efficient and portable code optimized for\ndiverse hardware platforms.We implement the three-dimensional FDTD kernel as\noperations on a 3D tensor abstraction with explicit computational semantics.\nHigh-level optimizations such as loop tiling, fusion, and vectorization are\nautomatically applied by the compiler. We evaluate our customized code\ngeneration pipeline on Intel, AMD, and ARM platforms, achieving up to\n$10\\times$ speedup over baseline Python implementation using NumPy."}
{"id": "2504.09149", "pdf": "https://arxiv.org/pdf/2504.09149", "abs": "https://arxiv.org/abs/2504.09149", "authors": ["Changhao Li", "Yu Xin", "Xiaowei Zhou", "Ariel Shamir", "Hao Zhang", "Ligang Liu", "Ruizhen Hu"], "title": "MASH: Masked Anchored SpHerical Distances for 3D Shape Representation and Generation", "categories": ["cs.CV", "cs.CG"], "comment": "11 pages, 11 figures, SIGGRAPH 2025 Accept - Conference", "summary": "We introduce Masked Anchored SpHerical Distances (MASH), a novel multi-view\nand parametrized representation of 3D shapes. Inspired by multi-view geometry\nand motivated by the importance of perceptual shape understanding for learning\n3D shapes, MASH represents a 3D shape as a collection of observable local\nsurface patches, each defined by a spherical distance function emanating from\nan anchor point. We further leverage the compactness of spherical harmonics to\nencode the MASH functions, combined with a generalized view cone with a\nparameterized base that masks the spatial extent of the spherical function to\nattain locality. We develop a differentiable optimization algorithm capable of\nconverting any point cloud into a MASH representation accurately approximating\nground-truth surfaces with arbitrary geometry and topology. Extensive\nexperiments demonstrate that MASH is versatile for multiple applications\nincluding surface reconstruction, shape generation, completion, and blending,\nachieving superior performance thanks to its unique representation encompassing\nboth implicit and explicit features."}
{"id": "2504.09130", "pdf": "https://arxiv.org/pdf/2504.09130", "abs": "https://arxiv.org/abs/2504.09130", "authors": ["Yikun Wang", "Siyin Wang", "Qinyuan Cheng", "Zhaoye Fei", "Liang Ding", "Qipeng Guo", "Dacheng Tao", "Xipeng Qiu"], "title": "VisuoThink: Empowering LVLM Reasoning with Multimodal Tree Search", "categories": ["cs.CL"], "comment": "12 pages", "summary": "Recent advancements in Large Vision-Language Models have showcased remarkable\ncapabilities. However, they often falter when confronted with complex reasoning\ntasks that humans typically address through visual aids and deliberate,\nstep-by-step thinking. While existing methods have explored text-based slow\nthinking or rudimentary visual assistance, they fall short of capturing the\nintricate, interleaved nature of human visual-verbal reasoning processes. To\novercome these limitations and inspired by the mechanisms of slow thinking in\nhuman cognition, we introduce VisuoThink, a novel framework that seamlessly\nintegrates visuospatial and linguistic domains. VisuoThink facilitates\nmultimodal slow thinking by enabling progressive visual-textual reasoning and\nincorporates test-time scaling through look-ahead tree search. Extensive\nexperiments demonstrate that VisuoThink significantly enhances reasoning\ncapabilities via inference-time scaling, even without fine-tuning, achieving\nstate-of-the-art performance in tasks involving geometry and spatial reasoning."}
{"id": "2504.09155", "pdf": "https://arxiv.org/pdf/2504.09155", "abs": "https://arxiv.org/abs/2504.09155", "authors": ["Zhanzhou Feng", "Shiliang Zhang"], "title": "Evolved Hierarchical Masking for Self-Supervised Learning", "categories": ["cs.CV"], "comment": null, "summary": "Existing Masked Image Modeling methods apply fixed mask patterns to guide the\nself-supervised training. As those mask patterns resort to different criteria\nto depict image contents, sticking to a fixed pattern leads to a limited vision\ncues modeling capability.This paper introduces an evolved hierarchical masking\nmethod to pursue general visual cues modeling in self-supervised learning. The\nproposed method leverages the vision model being trained to parse the input\nvisual cues into a hierarchy structure, which is hence adopted to generate\nmasks accordingly. The accuracy of hierarchy is on par with the capability of\nthe model being trained, leading to evolved mask patterns at different training\nstages. Initially, generated masks focus on low-level visual cues to grasp\nbasic textures, then gradually evolve to depict higher-level cues to reinforce\nthe learning of more complicated object semantics and contexts. Our method does\nnot require extra pre-trained models or annotations and ensures training\nefficiency by evolving the training difficulty. We conduct extensive\nexperiments on seven downstream tasks including partial-duplicate image\nretrieval relying on low-level details, as well as image classification and\nsemantic segmentation that require semantic parsing capability. Experimental\nresults demonstrate that it substantially boosts performance across these\ntasks. For instance, it surpasses the recent MAE by 1.1\\% in imageNet-1K\nclassification and 1.4\\% in ADE20K segmentation with the same training epochs.\nWe also align the proposed method with the current research focus on LLMs. The\nproposed approach bridges the gap with large-scale pre-training on semantic\ndemanding tasks and enhances intricate detail perception in tasks requiring\nlow-level feature recognition."}
{"id": "2504.09135", "pdf": "https://arxiv.org/pdf/2504.09135", "abs": "https://arxiv.org/abs/2504.09135", "authors": ["Haotian Ye", "Himanshu Jain", "Chong You", "Ananda Theertha Suresh", "Haowei Lin", "James Zou", "Felix Yu"], "title": "Efficient and Asymptotically Unbiased Constrained Decoding for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "In real-world applications of large language models, outputs are often\nrequired to be confined: selecting items from predefined product or document\nsets, generating phrases that comply with safety standards, or conforming to\nspecialized formatting styles. To control the generation, constrained decoding\nhas been widely adopted. However, existing prefix-tree-based constrained\ndecoding is inefficient under GPU-based model inference paradigms, and it\nintroduces unintended biases into the output distribution. This paper\nintroduces Dynamic Importance Sampling for Constrained Decoding (DISC) with\nGPU-based Parallel Prefix-Verification (PPV), a novel algorithm that leverages\ndynamic importance sampling to achieve theoretically guaranteed asymptotic\nunbiasedness and overcomes the inefficiency of prefix-tree. Extensive\nexperiments demonstrate the superiority of our method over existing methods in\nboth efficiency and output quality. These results highlight the potential of\nour methods to improve constrained generation in applications where adherence\nto specific constraints is essential."}
{"id": "2504.09156", "pdf": "https://arxiv.org/pdf/2504.09156", "abs": "https://arxiv.org/abs/2504.09156", "authors": ["Shengyu Gong", "Yueyang Li", "Zijian Kang", "Weiming Zeng", "Hongjie Yan", "Wai Ting Siok", "Nizhuan Wang"], "title": "LEREL: Lipschitz Continuity-Constrained Emotion Recognition Ensemble Learning For Electroencephalography", "categories": ["cs.CV"], "comment": null, "summary": "Accurate and efficient perception of emotional states in oneself and others\nis crucial, as emotion-related disorders are associated with severe\npsychosocial impairments. While electroencephalography (EEG) offers a powerful\ntool for emotion detection, current EEG-based emotion recognition (EER) methods\nface key limitations: insufficient model stability, limited accuracy in\nprocessing high-dimensional nonlinear EEG signals, and poor robustness against\nintra-subject variability and signal noise. To address these challenges, we\npropose LEREL (Lipschitz continuity-constrained Emotion Recognition Ensemble\nLearning), a novel framework that significantly enhances both the accuracy and\nrobustness of emotion recognition performance. The LEREL framework employs\nLipschitz continuity constraints to enhance model stability and generalization\nin EEG emotion recognition, reducing signal variability and noise\nsusceptibility while maintaining strong performance on small-sample datasets.\nThe ensemble learning strategy reduces single-model bias and variance through\nmulti-classifier decision fusion, further optimizing overall performance.\nExperimental results on three public benchmark datasets (EAV, FACED and SEED)\ndemonstrate LEREL's effectiveness, achieving average recognition accuracies of\n76.43%, 83.00% and 89.22%, respectively."}
{"id": "2504.09164", "pdf": "https://arxiv.org/pdf/2504.09164", "abs": "https://arxiv.org/abs/2504.09164", "authors": ["Michael Farrell"], "title": "Can postgraduate translation students identify machine-generated text?", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, accepted for MT Summit 2025, Geneva, Switzerland, 23-27\n  June 2025", "summary": "Given the growing use of generative artificial intelligence as a tool for\ncreating multilingual content and bypassing both machine and traditional\ntranslation methods, this study explores the ability of linguistically trained\nindividuals to discern machine-generated output from human-written text (HT).\nAfter brief training sessions on the textual anomalies typically found in\nsynthetic text (ST), twenty-three postgraduate translation students analysed\nexcerpts of Italian prose and assigned likelihood scores to indicate whether\nthey believed they were human-written or AI-generated (ChatGPT-4o). The results\nshow that, on average, the students struggled to distinguish between HT and ST,\nwith only two participants achieving notable accuracy. Closer analysis revealed\nthat the students often identified the same textual anomalies in both HT and\nST, although features such as low burstiness and self-contradiction were more\nfrequently associated with ST. These findings suggest the need for improvements\nin the preparatory training. Moreover, the study raises questions about the\nnecessity of editing synthetic text to make it sound more human-like and\nrecommends further research to determine whether AI-generated text is already\nsufficiently natural-sounding not to require further refinement."}
{"id": "2504.09160", "pdf": "https://arxiv.org/pdf/2504.09160", "abs": "https://arxiv.org/abs/2504.09160", "authors": ["Qingyuan Wang", "Rui Song", "Jiaojiao Li", "Kerui Cheng", "David Ferstl", "Yinlin Hu"], "title": "SCFlow2: Plug-and-Play Object Pose Refiner with Shape-Constraint Scene Flow", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "We introduce SCFlow2, a plug-and-play refinement framework for 6D object pose\nestimation. Most recent 6D object pose methods rely on refinement to get\naccurate results. However, most existing refinement methods either suffer from\nnoises in establishing correspondences, or rely on retraining for novel\nobjects. SCFlow2 is based on the SCFlow model designed for refinement with\nshape constraint, but formulates the additional depth as a regularization in\nthe iteration via 3D scene flow for RGBD frames. The key design of SCFlow2 is\nan introduction of geometry constraints into the training of recurrent matching\nnetwork, by combining the rigid-motion embeddings in 3D scene flow and 3D shape\nprior of the target. We train SCFlow2 on a combination of dataset Objaverse,\nGSO and ShapeNet, and evaluate on BOP datasets with novel objects. After using\nour method as a post-processing, most state-of-the-art methods produce\nsignificantly better results, without any retraining or fine-tuning. The source\ncode is available at https://scflow2.github.io."}
{"id": "2504.09170", "pdf": "https://arxiv.org/pdf/2504.09170", "abs": "https://arxiv.org/abs/2504.09170", "authors": ["Rabindra Lamsal", "Maria Rodriguez Read", "Shanika Karunasekera"], "title": "Langformers: Unified NLP Pipelines for Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Transformer-based language models have revolutionized the field of natural\nlanguage processing (NLP). However, using these models often involves\nnavigating multiple frameworks and tools, as well as writing repetitive\nboilerplate code. This complexity can discourage non-programmers and beginners,\nand even slow down prototyping for experienced developers. To address these\nchallenges, we introduce Langformers, an open-source Python library designed to\nstreamline NLP pipelines through a unified, factory-based interface for large\nlanguage model (LLM) and masked language model (MLM) tasks. Langformers\nintegrates conversational AI, MLM pretraining, text classification, sentence\nembedding/reranking, data labelling, semantic search, and knowledge\ndistillation into a cohesive API, supporting popular platforms such as Hugging\nFace and Ollama. Key innovations include: (1) task-specific factories that\nabstract training, inference, and deployment complexities; (2) built-in memory\nand streaming for conversational agents; and (3) lightweight, modular design\nthat prioritizes ease of use. Documentation: https://langformers.com"}
{"id": "2504.09195", "pdf": "https://arxiv.org/pdf/2504.09195", "abs": "https://arxiv.org/abs/2504.09195", "authors": ["Tzoulio Chamiti", "Leandro Di Bella", "Adrian Munteanu", "Nikos Deligiannis"], "title": "ReferGPT: Towards Zero-Shot Referring Multi-Object Tracking", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted CVPR 2025 Workshop on Distillation of Foundation Models for\n  Autonomous Driving", "summary": "Tracking multiple objects based on textual queries is a challenging task that\nrequires linking language understanding with object association across frames.\nPrevious works typically train the whole process end-to-end or integrate an\nadditional referring text module into a multi-object tracker, but they both\nrequire supervised training and potentially struggle with generalization to\nopen-set queries. In this work, we introduce ReferGPT, a novel zero-shot\nreferring multi-object tracking framework. We provide a multi-modal large\nlanguage model (MLLM) with spatial knowledge enabling it to generate 3D-aware\ncaptions. This enhances its descriptive capabilities and supports a more\nflexible referring vocabulary without training. We also propose a robust\nquery-matching strategy, leveraging CLIP-based semantic encoding and fuzzy\nmatching to associate MLLM generated captions with user queries. Extensive\nexperiments on Refer-KITTI, Refer-KITTIv2 and Refer-KITTI+ demonstrate that\nReferGPT achieves competitive performance against trained methods, showcasing\nits robustness and zero-shot capabilities in autonomous driving. The codes are\navailable on https://github.com/Tzoulio/ReferGPT"}
{"id": "2504.09184", "pdf": "https://arxiv.org/pdf/2504.09184", "abs": "https://arxiv.org/abs/2504.09184", "authors": ["Lennart Finke", "Thomas Dooms", "Mat Allen", "Juan Diego Rodriguez", "Noa Nabeshima", "Dan Braun"], "title": "Parameterized Synthetic Text Generation with SimpleStories", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present SimpleStories, a large synthetic story dataset in simple language,\nconsisting of 2 million stories each in English and Japanese. Our method\nemploys parametrization of prompts with features at multiple levels of\nabstraction, allowing for systematic control over story characteristics to\nensure broad syntactic and semantic diversity. Building on and addressing\nlimitations in the TinyStories dataset, our approach demonstrates that\nsimplicity and variety can be achieved simultaneously in synthetic text\ngeneration at scale."}
{"id": "2504.09196", "pdf": "https://arxiv.org/pdf/2504.09196", "abs": "https://arxiv.org/abs/2504.09196", "authors": ["Feng Lv", "Chunlong Xia", "Shuo Wang", "Huo Cao"], "title": "RT-DATR:Real-time Unsupervised Domain Adaptive Detection Transformer with Adversarial Feature Learning", "categories": ["cs.CV"], "comment": null, "summary": "Despite domain-adaptive object detectors based on CNN and transformers have\nmade significant progress in cross-domain detection tasks, it is regrettable\nthat domain adaptation for real-time transformer-based detectors has not yet\nbeen explored. Directly applying existing domain adaptation algorithms has\nproven to be suboptimal. In this paper, we propose RT-DATR, a simple and\nefficient real-time domain adaptive detection transformer. Building on RT-DETR\nas our base detector, we first introduce a local object-level feature alignment\nmodule to significantly enhance the feature representation of domain invariance\nduring object transfer. Additionally, we introduce a scene semantic feature\nalignment module designed to boost cross-domain detection performance by\naligning scene semantic features. Finally, we introduced a domain query and\ndecoupled it from the object query to further align the instance feature\ndistribution within the decoder layer, reduce the domain gap, and maintain\ndiscriminative ability. Experimental results on various benchmarks demonstrate\nthat our method outperforms current state-of-the-art approaches. Our code will\nbe released soon."}
{"id": "2504.09191", "pdf": "https://arxiv.org/pdf/2504.09191", "abs": "https://arxiv.org/abs/2504.09191", "authors": ["Weilong Dong", "Peiguang Li", "Yu Tian", "Xinyi Zeng", "Fengdi Li", "Sirui Wang"], "title": "Feature-Aware Malicious Output Detection and Mitigation", "categories": ["cs.CL"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has brought significant\nbenefits to various domains while introducing substantial risks. Despite being\nfine-tuned through reinforcement learning, LLMs lack the capability to discern\nmalicious content, limiting their defense against jailbreak. To address these\nsafety concerns, we propose a feature-aware method for harmful response\nrejection (FMM), which detects the presence of malicious features within the\nmodel's feature space and adaptively adjusts the model's rejection mechanism.\nBy employing a simple discriminator, we detect potential malicious traits\nduring the decoding phase. Upon detecting features indicative of toxic tokens,\nFMM regenerates the current token. By employing activation patching, an\nadditional rejection vector is incorporated during the subsequent token\ngeneration, steering the model towards a refusal response. Experimental results\ndemonstrate the effectiveness of our approach across multiple language models\nand diverse attack techniques, while crucially maintaining the models' standard\ngeneration capabilities."}
{"id": "2504.09202", "pdf": "https://arxiv.org/pdf/2504.09202", "abs": "https://arxiv.org/abs/2504.09202", "authors": ["Tung Luu", "Nam Le", "Duc Le", "Bac Le"], "title": "From Visual Explanations to Counterfactual Explanations with Latent Diffusion", "categories": ["cs.CV"], "comment": "2025 IEEE/CVF Winter Conference on Applications of Computer Vision\n  (WACV)", "summary": "Visual counterfactual explanations are ideal hypothetical images that change\nthe decision-making of the classifier with high confidence toward the desired\nclass while remaining visually plausible and close to the initial image. In\nthis paper, we propose a new approach to tackle two key challenges in recent\nprominent works: i) determining which specific counterfactual features are\ncrucial for distinguishing the \"concept\" of the target class from the original\nclass, and ii) supplying valuable explanations for the non-robust classifier\nwithout relying on the support of an adversarially robust model. Our method\nidentifies the essential region for modification through algorithms that\nprovide visual explanations, and then our framework generates realistic\ncounterfactual explanations by combining adversarial attacks based on pruning\nthe adversarial gradient of the target classifier and the latent diffusion\nmodel. The proposed method outperforms previous state-of-the-art results on\nvarious evaluation criteria on ImageNet and CelebA-HQ datasets. In general, our\nmethod can be applied to arbitrary classifiers, highlight the strong\nassociation between visual and counterfactual explanations, make semantically\nmeaningful changes from the target classifier, and provide observers with\nsubtle counterfactual images."}
{"id": "2504.09305", "pdf": "https://arxiv.org/pdf/2504.09305", "abs": "https://arxiv.org/abs/2504.09305", "authors": ["Owen Patterson", "Chee Ng"], "title": "Enhancing Contrastive Demonstration Selection with Semantic Diversity for Robust In-Context Machine Translation", "categories": ["cs.CL"], "comment": null, "summary": "In-Context Learning (ICL) empowers large language models to perform tasks by\nconditioning on a few input-output examples. However, the performance of ICL is\nhighly sensitive to the selection of these demonstrations. While existing\nmethods focus on similarity or contrastive selection, they often overlook the\nimportance of diversity among the chosen examples. In this paper, we propose\nDiverseConE (Diversity-Enhanced Contrastive Example Selection), a novel\napproach for demonstration selection in in-context learning for machine\ntranslation. Our method builds upon contrastive selection by incorporating a\ndiversity enhancement step based on embedding space dissimilarity. We conduct\nextensive experiments on the Llama2-7b model across four language pairs\n(English-Chinese, Chinese-English, Russian-German, German-Russian) in 1-shot\nand 3-shot settings, using COMET20 and COMET22 for evaluation. Our results\ndemonstrate that DiverseConE consistently outperforms strong baseline methods,\nincluding random selection, BM25, TopK, and a state-of-the-art contrastive\nselection method. Further analysis, including diversity metrics and human\nevaluation, validates the effectiveness of our approach and highlights the\nbenefits of considering demonstration diversity for improved translation\nquality."}
{"id": "2504.09203", "pdf": "https://arxiv.org/pdf/2504.09203", "abs": "https://arxiv.org/abs/2504.09203", "authors": ["Saikat Dutta", "Akhil Vasim", "Siddhant Gole", "Hamid Rezatofighi", "Biplab Banerjee"], "title": "AerOSeg: Harnessing SAM for Open-Vocabulary Segmentation in Remote Sensing Images", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at EarthVision workshop, CVPR 2025", "summary": "Image segmentation beyond predefined categories is a key challenge in remote\nsensing, where novel and unseen classes often emerge during inference.\nOpen-vocabulary image Segmentation addresses these generalization issues in\ntraditional supervised segmentation models while reducing reliance on extensive\nper-pixel annotations, which are both expensive and labor-intensive to obtain.\nMost Open-Vocabulary Segmentation (OVS) methods are designed for natural images\nbut struggle with remote sensing data due to scale variations, orientation\nchanges, and complex scene compositions. This necessitates the development of\nOVS approaches specifically tailored for remote sensing. In this context, we\npropose AerOSeg, a novel OVS approach for remote sensing data. First, we\ncompute robust image-text correlation features using multiple rotated versions\nof the input image and domain-specific prompts. These features are then refined\nthrough spatial and class refinement blocks. Inspired by the success of the\nSegment Anything Model (SAM) in diverse domains, we leverage SAM features to\nguide the spatial refinement of correlation features. Additionally, we\nintroduce a semantic back-projection module and loss to ensure the seamless\npropagation of SAM's semantic information throughout the segmentation pipeline.\nFinally, we enhance the refined correlation features using a multi-scale\nattention-aware decoder to produce the final segmentation map. We validate our\nSAM-guided Open-Vocabulary Remote Sensing Segmentation model on three benchmark\nremote sensing datasets: iSAID, DLRSD, and OpenEarthMap. Our model outperforms\nstate-of-the-art open-vocabulary segmentation methods, achieving an average\nimprovement of 2.54 h-mIoU."}
{"id": "2504.09309", "pdf": "https://arxiv.org/pdf/2504.09309", "abs": "https://arxiv.org/abs/2504.09309", "authors": ["Emily Johnson", "Xavier Holt", "Noah Wilson"], "title": "Improving the Accuracy and Efficiency of Legal Document Tagging with Large Language Models and Instruction Prompts", "categories": ["cs.CL"], "comment": null, "summary": "Legal multi-label classification is a critical task for organizing and\naccessing the vast amount of legal documentation. Despite its importance, it\nfaces challenges such as the complexity of legal language, intricate label\ndependencies, and significant label imbalance. In this paper, we propose\nLegal-LLM, a novel approach that leverages the instruction-following\ncapabilities of Large Language Models (LLMs) through fine-tuning. We reframe\nthe multi-label classification task as a structured generation problem,\ninstructing the LLM to directly output the relevant legal categories for a\ngiven document. We evaluate our method on two benchmark datasets, POSTURE50K\nand EURLEX57K, using micro-F1 and macro-F1 scores. Our experimental results\ndemonstrate that Legal-LLM outperforms a range of strong baseline models,\nincluding traditional methods and other Transformer-based approaches.\nFurthermore, ablation studies and human evaluations validate the effectiveness\nof our approach, particularly in handling label imbalance and generating\nrelevant and accurate legal labels."}
{"id": "2504.09215", "pdf": "https://arxiv.org/pdf/2504.09215", "abs": "https://arxiv.org/abs/2504.09215", "authors": ["Zhicheng Zhang", "Hao Tang", "Jinhui Tang"], "title": "Multi-scale Activation, Refinement, and Aggregation: Exploring Diverse Cues for Fine-Grained Bird Recognition", "categories": ["cs.CV", "cs.MM"], "comment": "Accepted by AAAI2025", "summary": "Given the critical role of birds in ecosystems, Fine-Grained Bird Recognition\n(FGBR) has gained increasing attention, particularly in distinguishing birds\nwithin similar subcategories. Although Vision Transformer (ViT)-based methods\noften outperform Convolutional Neural Network (CNN)-based methods in FGBR,\nrecent studies reveal that the limited receptive field of plain ViT model\nhinders representational richness and makes them vulnerable to scale variance.\nThus, enhancing the multi-scale capabilities of existing ViT-based models to\novercome this bottleneck in FGBR is a worthwhile pursuit. In this paper, we\npropose a novel framework for FGBR, namely Multi-scale Diverse Cues Modeling\n(MDCM), which explores diverse cues at different scales across various stages\nof a multi-scale Vision Transformer (MS-ViT) in an\n\"Activation-Selection-Aggregation\" paradigm. Specifically, we first propose a\nmulti-scale cue activation module to ensure the discriminative cues learned at\ndifferent stage are mutually different. Subsequently, a multi-scale token\nselection mechanism is proposed to remove redundant noise and highlight\ndiscriminative, scale-specific cues at each stage. Finally, the selected tokens\nfrom each stage are independently utilized for bird recognition, and the\nrecognition results from multiple stages are adaptively fused through a\nmulti-scale dynamic aggregation mechanism for final model decisions. Both\nqualitative and quantitative results demonstrate the effectiveness of our\nproposed MDCM, which outperforms CNN- and ViT-based models on several\nwidely-used FGBR benchmarks."}
{"id": "2504.09373", "pdf": "https://arxiv.org/pdf/2504.09373", "abs": "https://arxiv.org/abs/2504.09373", "authors": ["Ramya Namuduri", "Yating Wu", "Anshun Asher Zheng", "Manya Wadhwa", "Greg Durrett", "Junyi Jessy Li"], "title": "QUDsim: Quantifying Discourse Similarities in LLM-Generated Text", "categories": ["cs.CL"], "comment": null, "summary": "As large language models become increasingly capable at various writing\ntasks, their weakness at generating unique and creative content becomes a major\nliability. Although LLMs have the ability to generate text covering diverse\ntopics, there is an overall sense of repetitiveness across texts that we aim to\nformalize and quantify via a similarity metric. The familiarity between\ndocuments arises from the persistence of underlying discourse structures.\nHowever, existing similarity metrics dependent on lexical overlap and syntactic\npatterns largely capture $\\textit{content}$ overlap, thus making them\nunsuitable for detecting $\\textit{structural}$ similarities. We introduce an\nabstraction based on linguistic theories in Questions Under Discussion (QUD)\nand question semantics to help quantify differences in discourse progression.\nWe then use this framework to build $\\textbf{QUDsim}$, a similarity metric that\ncan detect discursive parallels between documents. Using QUDsim, we find that\nLLMs often reuse discourse structures (more so than humans) across samples,\neven when content differs. Furthermore, LLMs are not only repetitive and\nstructurally uniform, but are also divergent from human authors in the types of\nstructures they use."}
{"id": "2504.09223", "pdf": "https://arxiv.org/pdf/2504.09223", "abs": "https://arxiv.org/abs/2504.09223", "authors": ["Wenjin Ke", "Zhe Li", "Dong Li", "Lu Tian", "Emad Barsoum"], "title": "DL-QAT: Weight-Decomposed Low-Rank Quantization-Aware Training for Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Improving the efficiency of inference in Large Language Models (LLMs) is a\ncritical area of research. Post-training Quantization (PTQ) is a popular\ntechnique, but it often faces challenges at low-bit levels, particularly in\ndownstream tasks. Quantization-aware Training (QAT) can alleviate this problem,\nbut it requires significantly more computational resources. To tackle this, we\nintroduced Weight-Decomposed Low-Rank Quantization-Aware Training (DL-QAT),\nwhich merges the advantages of QAT while training only less than 1% of the\ntotal parameters. Specifically, we introduce a group-specific quantization\nmagnitude to adjust the overall scale of each quantization group. Within each\nquantization group, we use LoRA matrices to update the weight size and\ndirection in the quantization space. We validated the effectiveness of our\nmethod on the LLaMA and LLaMA2 model families. The results show significant\nimprovements over our baseline method across different quantization\ngranularities. For instance, for LLaMA-7B, our approach outperforms the\nprevious state-of-the-art method by 4.2% in MMLU on 3-bit LLaMA-7B model.\nAdditionally, our quantization results on pre-trained models also surpass\nprevious QAT methods, demonstrating the superior performance and efficiency of\nour approach."}
{"id": "2504.09378", "pdf": "https://arxiv.org/pdf/2504.09378", "abs": "https://arxiv.org/abs/2504.09378", "authors": ["Kartik Ravisankar", "Hyojung Han", "Marine Carpuat"], "title": "Can you map it to English? The Role of Cross-Lingual Alignment in Multilingual Performance of LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) pre-trained predominantly on English text\nexhibit surprising multilingual capabilities, yet the mechanisms driving\ncross-lingual generalization remain poorly understood. This work investigates\nhow the alignment of representations for text written in different languages\ncorrelates with LLM performance on natural language understanding tasks and\ntranslation tasks, both at the language and the instance level. For this\npurpose, we introduce cross-lingual alignment metrics such as the\nDiscriminative Alignment Index (DALI) to quantify the alignment at an instance\nlevel for discriminative tasks. Through experiments on three natural language\nunderstanding tasks (Belebele, XStoryCloze, XCOPA), and machine translation, we\nfind that while cross-lingual alignment metrics strongly correlate with task\naccuracy at the language level, the sample-level alignment often fails to\ndistinguish correct from incorrect predictions, exposing alignment as a\nnecessary but insufficient condition for success."}
{"id": "2504.09228", "pdf": "https://arxiv.org/pdf/2504.09228", "abs": "https://arxiv.org/abs/2504.09228", "authors": ["You Wu", "Xucheng Wang", "Xiangyang Yang", "Mengyuan Liu", "Dan Zeng", "Hengzhou Ye", "Shuiwang Li"], "title": "Learning Occlusion-Robust Vision Transformers for Real-Time UAV Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Single-stream architectures using Vision Transformer (ViT) backbones show\ngreat potential for real-time UAV tracking recently. However, frequent\nocclusions from obstacles like buildings and trees expose a major drawback:\nthese models often lack strategies to handle occlusions effectively. New\nmethods are needed to enhance the occlusion resilience of single-stream ViT\nmodels in aerial tracking. In this work, we propose to learn Occlusion-Robust\nRepresentations (ORR) based on ViTs for UAV tracking by enforcing an invariance\nof the feature representation of a target with respect to random masking\noperations modeled by a spatial Cox process. Hopefully, this random masking\napproximately simulates target occlusions, thereby enabling us to learn ViTs\nthat are robust to target occlusion for UAV tracking. This framework is termed\nORTrack. Additionally, to facilitate real-time applications, we propose an\nAdaptive Feature-Based Knowledge Distillation (AFKD) method to create a more\ncompact tracker, which adaptively mimics the behavior of the teacher model\nORTrack according to the task's difficulty. This student model, dubbed\nORTrack-D, retains much of ORTrack's performance while offering higher\nefficiency. Extensive experiments on multiple benchmarks validate the\neffectiveness of our method, demonstrating its state-of-the-art performance.\nCodes is available at https://github.com/wuyou3474/ORTrack."}
{"id": "2504.09387", "pdf": "https://arxiv.org/pdf/2504.09387", "abs": "https://arxiv.org/abs/2504.09387", "authors": ["Sriram Padmanabhan", "Kanishka Misra", "Kyle Mahowald", "Eunsol Choi"], "title": "On Language Models' Sensitivity to Suspicious Coincidences", "categories": ["cs.CL"], "comment": null, "summary": "Humans are sensitive to suspicious coincidences when generalizing inductively\nover data, as they make assumptions as to how the data was sampled. This\nresults in smaller, more specific hypotheses being favored over more general\nones. For instance, when provided the set {Austin, Dallas, Houston}, one is\nmore likely to think that this is sampled from \"Texas Cities\" over \"US Cities\"\neven though both are compatible. Suspicious coincidence is strongly connected\nto pragmatic reasoning, and can serve as a testbed to analyze systems on their\nsensitivity towards the communicative goals of the task (i.e., figuring out the\ntrue category underlying the data). In this paper, we analyze whether\nsuspicious coincidence effects are reflected in language models' (LMs)\nbehavior. We do so in the context of two domains: 1) the number game, where\nhumans made judgments of whether a number (e.g., 4) fits a list of given\nnumbers (e.g., 16, 32, 2); and 2) by extending the number game setup to\nprominent cities. For both domains, the data is compatible with multiple\nhypotheses and we study which hypothesis is most consistent with the models'\nbehavior. On analyzing five models, we do not find strong evidence for\nsuspicious coincidences in LMs' zero-shot behavior. However, when provided\naccess to the hypotheses space via chain-of-thought or explicit prompting, LMs\nstart to show an effect resembling suspicious coincidences, sometimes even\nshowing effects consistent with humans. Our study suggests that inductive\nreasoning behavior in LMs can be enhanced with explicit access to the\nhypothesis landscape."}
{"id": "2504.09249", "pdf": "https://arxiv.org/pdf/2504.09249", "abs": "https://arxiv.org/abs/2504.09249", "authors": ["Aniket Pal", "Sanket Biswas", "Alloy Das", "Ayush Lodh", "Priyanka Banerjee", "Soumitri Chattopadhyay", "Dimosthenis Karatzas", "Josep Llados", "C. V. Jawahar"], "title": "NoTeS-Bank: Benchmarking Neural Transcription and Search for Scientific Notes Understanding", "categories": ["cs.CV", "cs.IR", "cs.LG", "cs.MM"], "comment": null, "summary": "Understanding and reasoning over academic handwritten notes remains a\nchallenge in document AI, particularly for mathematical equations, diagrams,\nand scientific notations. Existing visual question answering (VQA) benchmarks\nfocus on printed or structured handwritten text, limiting generalization to\nreal-world note-taking. To address this, we introduce NoTeS-Bank, an evaluation\nbenchmark for Neural Transcription and Search in note-based question answering.\nNoTeS-Bank comprises complex notes across multiple domains, requiring models to\nprocess unstructured and multimodal content. The benchmark defines two tasks:\n(1) Evidence-Based VQA, where models retrieve localized answers with\nbounding-box evidence, and (2) Open-Domain VQA, where models classify the\ndomain before retrieving relevant documents and answers. Unlike classical\nDocument VQA datasets relying on optical character recognition (OCR) and\nstructured data, NoTeS-BANK demands vision-language fusion, retrieval, and\nmultimodal reasoning. We benchmark state-of-the-art Vision-Language Models\n(VLMs) and retrieval frameworks, exposing structured transcription and\nreasoning limitations. NoTeS-Bank provides a rigorous evaluation with NDCG@5,\nMRR, Recall@K, IoU, and ANLS, establishing a new standard for visual document\nunderstanding and reasoning."}
{"id": "2504.09389", "pdf": "https://arxiv.org/pdf/2504.09389", "abs": "https://arxiv.org/abs/2504.09389", "authors": ["Vishakh Padmakumar", "Chen Yueh-Han", "Jane Pan", "Valerie Chen", "He He"], "title": "Beyond Memorization: Mapping the Originality-Quality Frontier of Language Models", "categories": ["cs.CL"], "comment": null, "summary": "As large language models (LLMs) are increasingly used for ideation and\nscientific discovery, it is important to evaluate their ability to generate\nnovel output. Prior work evaluates novelty as the originality with respect to\ntraining data, but original outputs can be low quality. In contrast, non-expert\njudges may favor high-quality but memorized outputs, limiting the reliability\nof human preference as a metric. We propose a new novelty metric for LLM\ngenerations that balances originality and quality -- the harmonic mean of the\nfraction of \\ngrams unseen during training and a task-specific quality score.\nWe evaluate the novelty of generations from two families of open-data models\n(OLMo and Pythia) on three creative tasks: story completion, poetry writing,\nand creative tool use. We find that LLM generated text is less novel than human\nwritten text. To elicit more novel outputs, we experiment with various\ninference-time methods, which reveals a trade-off between originality and\nquality. While these methods can boost novelty, they do so by increasing\noriginality at the expense of quality. In contrast, increasing model size or\napplying post-training reliably shifts the Pareto frontier, highlighting that\nstarting with a stronger base model is a more effective way to improve novelty."}
{"id": "2504.09255", "pdf": "https://arxiv.org/pdf/2504.09255", "abs": "https://arxiv.org/abs/2504.09255", "authors": ["Sijing Wu", "Yunhao Li", "Ziwen Xu", "Yixuan Gao", "Huiyu Duan", "Wei Sun", "Guangtao Zhai"], "title": "FVQ: A Large-Scale Dataset and A LMM-based Method for Face Video Quality Assessment", "categories": ["cs.CV"], "comment": null, "summary": "Face video quality assessment (FVQA) deserves to be explored in addition to\ngeneral video quality assessment (VQA), as face videos are the primary content\non social media platforms and human visual system (HVS) is particularly\nsensitive to human faces. However, FVQA is rarely explored due to the lack of\nlarge-scale FVQA datasets. To fill this gap, we present the first large-scale\nin-the-wild FVQA dataset, FVQ-20K, which contains 20,000 in-the-wild face\nvideos together with corresponding mean opinion score (MOS) annotations. Along\nwith the FVQ-20K dataset, we further propose a specialized FVQA method named\nFVQ-Rater to achieve human-like rating and scoring for face video, which is the\nfirst attempt to explore the potential of large multimodal models (LMMs) for\nthe FVQA task. Concretely, we elaborately extract multi-dimensional features\nincluding spatial features, temporal features, and face-specific features\n(i.e., portrait features and face embeddings) to provide comprehensive visual\ninformation, and take advantage of the LoRA-based instruction tuning technique\nto achieve quality-specific fine-tuning, which shows superior performance on\nboth FVQ-20K and CFVQA datasets. Extensive experiments and comprehensive\nanalysis demonstrate the significant potential of the FVQ-20K dataset and\nFVQ-Rater method in promoting the development of FVQA."}
{"id": "2504.09394", "pdf": "https://arxiv.org/pdf/2504.09394", "abs": "https://arxiv.org/abs/2504.09394", "authors": ["Joseph Liu", "Yoonsoo Nam", "Xinyue Cui", "Swabha Swayamdipta"], "title": "Evaluation Under Imperfect Benchmarks and Ratings: A Case Study in Text Simplification", "categories": ["cs.CL"], "comment": "Submitted to COLM 2025. 9 pages, 6 figures", "summary": "Despite the successes of language models, their evaluation remains a daunting\nchallenge for new and existing tasks. We consider the task of text\nsimplification, commonly used to improve information accessibility, where\nevaluation faces two major challenges. First, the data in existing benchmarks\nmight not reflect the capabilities of current language models on the task,\noften containing disfluent, incoherent, or simplistic examples. Second,\nexisting human ratings associated with the benchmarks often contain a high\ndegree of disagreement, resulting in inconsistent ratings; nevertheless,\nexisting metrics still have to show higher correlations with these imperfect\nratings. As a result, evaluation for the task is not reliable and does not\nreflect expected trends (e.g., more powerful models being assigned higher\nscores). We address these challenges for the task of text simplification\nthrough three contributions. First, we introduce SynthSimpliEval, a synthetic\nbenchmark for text simplification featuring simplified sentences generated by\nmodels of varying sizes. Through a pilot study, we show that human ratings on\nour benchmark exhibit high inter-annotator agreement and reflect the expected\ntrend: larger models produce higher-quality simplifications. Second, we show\nthat auto-evaluation with a panel of LLM judges (LLMs-as-a-jury) often suffices\nto obtain consistent ratings for the evaluation of text simplification. Third,\nwe demonstrate that existing learnable metrics for text simplification benefit\nfrom training on our LLMs-as-a-jury-rated synthetic data, closing the gap with\npure LLMs-as-a-jury for evaluation. Overall, through our case study on text\nsimplification, we show that a reliable evaluation requires higher quality test\ndata, which could be obtained through synthetic data and LLMs-as-a-jury\nratings."}
{"id": "2504.09258", "pdf": "https://arxiv.org/pdf/2504.09258", "abs": "https://arxiv.org/abs/2504.09258", "authors": ["Jianyu Wu", "Hao Yang", "Xinhua Zeng", "Guibing He", "Zhiyu Chen", "Zihui Li", "Xiaochuan Zhang", "Yangyang Ma", "Run Fang", "Yang Liu"], "title": "PathVLM-R1: A Reinforcement Learning-Driven Reasoning Model for Pathology Visual-Language Tasks", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "The diagnosis of pathological images is often limited by expert availability\nand regional disparities, highlighting the importance of automated diagnosis\nusing Vision-Language Models (VLMs). Traditional multimodal models typically\nemphasize outcomes over the reasoning process, compromising the reliability of\nclinical decisions. To address the weak reasoning abilities and lack of\nsupervised processes in pathological VLMs, we have innovatively proposed\nPathVLM-R1, a visual language model designed specifically for pathological\nimages. We have based our model on Qwen2.5-VL-7B-Instruct and enhanced its\nperformance for pathological tasks through meticulously designed post-training\nstrategies. Firstly, we conduct supervised fine-tuning guided by pathological\ndata to imbue the model with foundational pathological knowledge, forming a new\npathological base model. Subsequently, we introduce Group Relative Policy\nOptimization (GRPO) and propose a dual reward-driven reinforcement learning\noptimization, ensuring strict constraint on logical supervision of the\nreasoning process and accuracy of results via cross-modal process reward and\noutcome accuracy reward. In the pathological image question-answering tasks,\nthe testing results of PathVLM-R1 demonstrate a 14% improvement in accuracy\ncompared to baseline methods, and it demonstrated superior performance compared\nto the Qwen2.5-VL-32B version despite having a significantly smaller parameter\nsize. Furthermore, in out-domain data evaluation involving four medical imaging\nmodalities: Computed Tomography (CT), dermoscopy, fundus photography, and\nOptical Coherence Tomography (OCT) images: PathVLM-R1's transfer performance\nimproved by an average of 17.3% compared to traditional SFT methods. These\nresults clearly indicate that PathVLM-R1 not only enhances accuracy but also\npossesses broad applicability and expansion potential."}
{"id": "2504.09398", "pdf": "https://arxiv.org/pdf/2504.09398", "abs": "https://arxiv.org/abs/2504.09398", "authors": ["Gaurav Kumar", "Murali Mohana Krishna Dandu"], "title": "Composable NLP Workflows for BERT-based Ranking and QA System", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 3 figures, 6 tables", "summary": "There has been a lot of progress towards building NLP models that scale to\nmultiple tasks. However, real-world systems contain multiple components and it\nis tedious to handle cross-task interaction with varying levels of text\ngranularity. In this work, we built an end-to-end Ranking and\nQuestion-Answering (QA) system using Forte, a toolkit that makes composable NLP\npipelines. We utilized state-of-the-art deep learning models such as BERT,\nRoBERTa in our pipeline, evaluated the performance on MS-MARCO and Covid-19\ndatasets using metrics such as BLUE, MRR, F1 and compared the results of\nranking and QA systems with their corresponding benchmark results. The modular\nnature of our pipeline and low latency of reranker makes it easy to build\ncomplex NLP applications easily."}
{"id": "2504.09261", "pdf": "https://arxiv.org/pdf/2504.09261", "abs": "https://arxiv.org/abs/2504.09261", "authors": ["Ziran Qin", "Youru Lv", "Mingbao Lin", "Zeren Zhang", "Danping Zou", "Weiyao Lin"], "title": "Head-Aware KV Cache Compression for Efficient Visual Autoregressive Modeling", "categories": ["cs.CV"], "comment": null, "summary": "Visual Autoregressive (VAR) models have emerged as a powerful approach for\nmulti-modal content creation, offering high efficiency and quality across\ndiverse multimedia applications. However, they face significant memory\nbottlenecks due to extensive KV cache accumulation during inference. Existing\nKV cache compression techniques for large language models are suboptimal for\nVAR models due to, as we identify in this paper, two distinct categories of\nattention heads in VAR models: Structural Heads, which preserve spatial\ncoherence through diagonal attention patterns, and Contextual Heads, which\nmaintain semantic consistency through vertical attention patterns. These\ndifferences render single-strategy KV compression techniques ineffective for\nVAR models. To address this, we propose HACK, a training-free Head-Aware\nCompression method for KV cache. HACK allocates asymmetric cache budgets and\nemploys pattern-specific compression strategies tailored to the essential\ncharacteristics of each head category. Experiments on Infinity-2B, Infinity-8B,\nand VAR-d30 demonstrate its effectiveness in text-to-image and\nclass-conditional generation tasks. HACK can hack down up to 50\\% and 70\\% of\ncache with minimal performance degradation for VAR-d30 and Infinity-8B,\nrespectively. Even with 70\\% and 90\\% KV cache compression in VAR-d30 and\nInfinity-8B, HACK still maintains high-quality generation while reducing memory\nusage by 44.2\\% and 58.9\\%, respectively."}
{"id": "2504.09402", "pdf": "https://arxiv.org/pdf/2504.09402", "abs": "https://arxiv.org/abs/2504.09402", "authors": ["Feijiang Han", "Licheng Guo", "Hengtao Cui", "Zhiyuan Lyu"], "title": "Question Tokens Deserve More Attention: Enhancing Large Language Models without Training through Step-by-Step Reading and Question Attention Recalibration", "categories": ["cs.CL", "cs.AI"], "comment": "CIS 5300", "summary": "Large Language Models (LLMs) often struggle with tasks that require a deep\nunderstanding of complex questions, especially when faced with long-range\ndependencies or multi-step reasoning. This work investigates the limitations of\ncurrent LLMs in question comprehension and identifies three insights: (1)\nrepeating question tokens improves comprehension by increasing attention to\nquestion regions, (2) increased backward dependencies negatively affect\nperformance due to unidirectional attentional constraints, and (3)\nrecalibrating attentional mechanisms to prioritize question-relevant regions\nimproves performance.\n  Based on these findings, we first propose a family of prompt-based strategies\n- Step-by-Step Reading (SSR), SSR+, and SSR++ - that guide LLMs to\nincrementally process question tokens and align their reasoning with the input\nstructure. These methods significantly improve performance, with SSR++\nachieving state-of-the-art results on several benchmarks: 96.66% on GSM8K,\n94.61% on ASDiv, and 76.28% on AQuA. Second, we introduce a training-free\nattention recalibration mechanism that dynamically adjusts attention\ndistributions during inference to emphasize question-relevant regions. This\nmethod improves the accuracy of LLaMA 3.1-8B on AQuA by 5.17% without changing\nmodel parameters or input prompts.\n  Taken together, our results highlight the importance of structured prompt\ndesign and attention optimization in improving LLM comprehension, providing\nlightweight yet effective tools for improving performance in various NLP tasks."}
{"id": "2504.09282", "pdf": "https://arxiv.org/pdf/2504.09282", "abs": "https://arxiv.org/abs/2504.09282", "authors": ["Zheyuan Zhang", "Monica Dou", "Linkai Peng", "Hongyi Pan", "Ulas Bagci", "Boqing Gong"], "title": "VideoAds for Fast-Paced Video Understanding: Where Opensource Foundation Models Beat GPT-4o & Gemini-1.5 Pro", "categories": ["cs.CV"], "comment": null, "summary": "Advertisement videos serve as a rich and valuable source of purpose-driven\ninformation, encompassing high-quality visual, textual, and contextual cues\ndesigned to engage viewers. They are often more complex than general videos of\nsimilar duration due to their structured narratives and rapid scene\ntransitions, posing significant challenges to multi-modal large language models\n(MLLMs). In this work, we introduce VideoAds, the first dataset tailored for\nbenchmarking the performance of MLLMs on advertisement videos. VideoAds\ncomprises well-curated advertisement videos with complex temporal structures,\naccompanied by \\textbf{manually} annotated diverse questions across three core\ntasks: visual finding, video summary, and visual reasoning. We propose a\nquantitative measure to compare VideoAds against existing benchmarks in terms\nof video complexity. Through extensive experiments, we find that\nQwen2.5-VL-72B, an opensource MLLM, achieves 73.35\\% accuracy on VideoAds,\noutperforming GPT-4o (66.82\\%) and Gemini-1.5 Pro (69.66\\%); the two\nproprietary models especially fall behind the opensource model in video\nsummarization and reasoning, but perform the best in visual finding. Notably,\nhuman experts easily achieve a remarkable accuracy of 94.27\\%. These results\nunderscore the necessity of advancing MLLMs' temporal modeling capabilities and\nhighlight VideoAds as a potentially pivotal benchmark for future research in\nunderstanding video that requires high FPS sampling. The dataset and evaluation\ncode will be publicly available at https://videoadsbenchmark.netlify.app."}
{"id": "2504.09407", "pdf": "https://arxiv.org/pdf/2504.09407", "abs": "https://arxiv.org/abs/2504.09407", "authors": ["Yuxuan Lu", "Bingsheng Yao", "Hansu Gu", "Jing Huang", "Jessie Wang", "Yang Li", "Jiri Gesi", "Qi He", "Toby Jia-Jun Li", "Dakuo Wang"], "title": "UXAgent: A System for Simulating Usability Testing of Web Design with LLM Agents", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Usability testing is a fundamental research method that user experience (UX)\nresearchers use to evaluate and iterate a web design, but\\textbf{ how to\nevaluate and iterate the usability testing study design } itself? Recent\nadvances in Large Language Model-simulated Agent (\\textbf{LLM Agent}) research\ninspired us to design \\textbf{UXAgent} to support UX researchers in evaluating\nand reiterating their usability testing study design before they conduct the\nreal human-subject study. Our system features a Persona Generator module, an\nLLM Agent module, and a Universal Browser Connector module to automatically\ngenerate thousands of simulated users to interactively test the target website.\nThe system also provides an Agent Interview Interface and a Video Replay\nInterface so that the UX researchers can easily review and analyze the\ngenerated qualitative and quantitative log data. Through a heuristic\nevaluation, five UX researcher participants praised the innovation of our\nsystem but also expressed concerns about the future of LLM Agent usage in UX\nstudies."}
{"id": "2504.09291", "pdf": "https://arxiv.org/pdf/2504.09291", "abs": "https://arxiv.org/abs/2504.09291", "authors": ["Jiaying Qian", "Ziheng Jia", "Zicheng Zhang", "Zeyu Zhang", "Guangtao Zhai", "Xiongkuo Min"], "title": "Towards Explainable Partial-AIGC Image Quality Assessment", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "The rapid advancement of AI-driven visual generation technologies has\ncatalyzed significant breakthroughs in image manipulation, particularly in\nachieving photorealistic localized editing effects on natural scene images\n(NSIs). Despite extensive research on image quality assessment (IQA) for\nAI-generated images (AGIs), most studies focus on fully AI-generated outputs\n(e.g., text-to-image generation), leaving the quality assessment of\npartial-AIGC images (PAIs)-images with localized AI-driven edits an almost\nunprecedented field. Motivated by this gap, we construct the first large-scale\nPAI dataset towards explainable partial-AIGC image quality assessment (EPAIQA),\nthe EPAIQA-15K, which includes 15K images with localized AI manipulation in\ndifferent regions and over 300K multi-dimensional human ratings. Based on this,\nwe leverage large multi-modal models (LMMs) and propose a three-stage model\ntraining paradigm. This paradigm progressively trains the LMM for editing\nregion grounding, quantitative quality scoring, and quality explanation.\nFinally, we develop the EPAIQA series models, which possess explainable quality\nfeedback capabilities. Our work represents a pioneering effort in the\nperceptual IQA field for comprehensive PAI quality assessment."}
{"id": "2504.09420", "pdf": "https://arxiv.org/pdf/2504.09420", "abs": "https://arxiv.org/abs/2504.09420", "authors": ["Yutao Mou", "Yuxiao Luo", "Shikun Zhang", "Wei Ye"], "title": "SaRO: Enhancing LLM Safety through Reasoning-based Alignment", "categories": ["cs.CL"], "comment": null, "summary": "Current safety alignment techniques for large language models (LLMs) face two\nkey challenges: (1) under-generalization, which leaves models vulnerable to\nnovel jailbreak attacks, and (2) over-alignment, which leads to the excessive\nrefusal of benign instructions. Our preliminary investigation reveals semantic\noverlap between jailbreak/harmful queries and normal prompts in embedding\nspace, suggesting that more effective safety alignment requires a deeper\nsemantic understanding. This motivates us to incorporate safety-policy-driven\nreasoning into the alignment process. To this end, we propose the\nSafety-oriented Reasoning Optimization Framework (SaRO), which consists of two\nstages: (1) Reasoning-style Warmup (RW) that enables LLMs to internalize\nlong-chain reasoning through supervised fine-tuning, and (2) Safety-oriented\nReasoning Process Optimization (SRPO) that promotes safety reflection via\ndirect preference optimization (DPO). Extensive experiments demonstrate the\nsuperiority of SaRO over traditional alignment methods."}
{"id": "2504.09297", "pdf": "https://arxiv.org/pdf/2504.09297", "abs": "https://arxiv.org/abs/2504.09297", "authors": ["Huu-Phong Phan-Nguyen", "Anh Dao", "Tien-Huy Nguyen", "Tuan Quang", "Huu-Loc Tran", "Tinh-Anh Nguyen-Nhu", "Huy-Thach Pham", "Quan Nguyen", "Hoang M. Le", "Quang-Vinh Dinh"], "title": "Cycle Training with Semi-Supervised Domain Adaptation: Bridging Accuracy and Efficiency for Real-Time Mobile Scene Detection", "categories": ["cs.CV"], "comment": null, "summary": "Nowadays, smartphones are ubiquitous, and almost everyone owns one. At the\nsame time, the rapid development of AI has spurred extensive research on\napplying deep learning techniques to image classification. However, due to the\nlimited resources available on mobile devices, significant challenges remain in\nbalancing accuracy with computational efficiency. In this paper, we propose a\nnovel training framework called Cycle Training, which adopts a three-stage\ntraining process that alternates between exploration and stabilization phases\nto optimize model performance. Additionally, we incorporate Semi-Supervised\nDomain Adaptation (SSDA) to leverage the power of large models and unlabeled\ndata, thereby effectively expanding the training dataset. Comprehensive\nexperiments on the CamSSD dataset for mobile scene detection demonstrate that\nour framework not only significantly improves classification accuracy but also\nensures real-time inference efficiency. Specifically, our method achieves a\n94.00% in Top-1 accuracy and a 99.17% in Top-3 accuracy and runs inference in\njust 1.61ms using CPU, demonstrating its suitability for real-world mobile\ndeployment."}
{"id": "2504.09421", "pdf": "https://arxiv.org/pdf/2504.09421", "abs": "https://arxiv.org/abs/2504.09421", "authors": ["Wuyang Lan", "Wenzheng Wang", "Changwei Ji", "Guoxing Yang", "Yongbo Zhang", "Xiaohong Liu", "Song Wu", "Guangyu Wang"], "title": "ClinicalGPT-R1: Pushing reasoning capability of generalist disease diagnosis with large language model", "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, 6 figures", "summary": "Recent advances in reasoning with large language models (LLMs)has shown\nremarkable reasoning capabilities in domains such as mathematics and coding,\nyet their application to clinical diagnosis remains underexplored. Here, we\nintroduce ClinicalGPT-R1, a reasoning enhanced generalist large language model\nfor disease diagnosis. Trained on a dataset of 20,000 real-world clinical\nrecords, ClinicalGPT-R1 leverages diverse training strategies to enhance\ndiagnostic reasoning. To benchmark performance, we curated MedBench-Hard, a\nchallenging dataset spanning seven major medical specialties and representative\ndiseases. Experimental results demonstrate that ClinicalGPT-R1 outperforms\nGPT-4o in Chinese diagnostic tasks and achieves comparable performance to GPT-4\nin English settings. This comparative study effectively validates the superior\nperformance of ClinicalGPT-R1 in disease diagnosis tasks. Resources are\navailable at https://github.com/medfound/medfound."}
{"id": "2504.09298", "pdf": "https://arxiv.org/pdf/2504.09298", "abs": "https://arxiv.org/abs/2504.09298", "authors": ["Tinh-Anh Nguyen-Nhu", "Huu-Loc Tran", "Nguyen-Khang Le", "Minh-Nhat Nguyen", "Tien-Huy Nguyen", "Hoang-Long Nguyen-Huu", "Huu-Phong Phan-Nguyen", "Huy-Thach Pham", "Quan Nguyen", "Hoang M. Le", "Quang-Vinh Dinh"], "title": "A Lightweight Moment Retrieval System with Global Re-Ranking and Robust Adaptive Bidirectional Temporal Search", "categories": ["cs.CV"], "comment": null, "summary": "The exponential growth of digital video content has posed critical challenges\nin moment-level video retrieval, where existing methodologies struggle to\nefficiently localize specific segments within an expansive video corpus.\nCurrent retrieval systems are constrained by computational inefficiencies,\ntemporal context limitations, and the intrinsic complexity of navigating video\ncontent. In this paper, we address these limitations through a novel\nInteractive Video Corpus Moment Retrieval framework that integrates a\nSuperGlobal Reranking mechanism and Adaptive Bidirectional Temporal Search\n(ABTS), strategically optimizing query similarity, temporal stability, and\ncomputational resources. By preprocessing a large corpus of videos using a\nkeyframe extraction model and deduplication technique through image hashing,\nour approach provides a scalable solution that significantly reduces storage\nrequirements while maintaining high localization precision across diverse video\nrepositories."}
{"id": "2504.09482", "pdf": "https://arxiv.org/pdf/2504.09482", "abs": "https://arxiv.org/abs/2504.09482", "authors": ["Sharanya Dasgupta", "Sujoy Nath", "Arkaprabha Basu", "Pourya Shamsolmoali", "Swagatam Das"], "title": "HalluShift: Measuring Distribution Shifts towards Hallucination Detection in LLMs", "categories": ["cs.CL", "cs.AI", "cs.ET"], "comment": null, "summary": "Large Language Models (LLMs) have recently garnered widespread attention due\nto their adeptness at generating innovative responses to the given prompts\nacross a multitude of domains. However, LLMs often suffer from the inherent\nlimitation of hallucinations and generate incorrect information while\nmaintaining well-structured and coherent responses. In this work, we\nhypothesize that hallucinations stem from the internal dynamics of LLMs. Our\nobservations indicate that, during passage generation, LLMs tend to deviate\nfrom factual accuracy in subtle parts of responses, eventually shifting toward\nmisinformation. This phenomenon bears a resemblance to human cognition, where\nindividuals may hallucinate while maintaining logical coherence, embedding\nuncertainty within minor segments of their speech. To investigate this further,\nwe introduce an innovative approach, HalluShift, designed to analyze the\ndistribution shifts in the internal state space and token probabilities of the\nLLM-generated responses. Our method attains superior performance compared to\nexisting baselines across various benchmark datasets. Our codebase is available\nat https://github.com/sharanya-dasgupta001/hallushift."}
{"id": "2504.09322", "pdf": "https://arxiv.org/pdf/2504.09322", "abs": "https://arxiv.org/abs/2504.09322", "authors": ["Tyler Spears", "Shen Zhu", "Yinzhu Jin", "Aman Shrivastava", "P. Thomas Fletcher"], "title": "MedIL: Implicit Latent Spaces for Generating Heterogeneous Medical Images at Arbitrary Resolutions", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "In this work, we introduce MedIL, a first-of-its-kind autoencoder built for\nencoding medical images with heterogeneous sizes and resolutions for image\ngeneration. Medical images are often large and heterogeneous, where fine\ndetails are of vital clinical importance. Image properties change drastically\nwhen considering acquisition equipment, patient demographics, and pathology,\nmaking realistic medical image generation challenging. Recent work in latent\ndiffusion models (LDMs) has shown success in generating images resampled to a\nfixed-size. However, this is a narrow subset of the resolutions native to image\nacquisition, and resampling discards fine anatomical details. MedIL utilizes\nimplicit neural representations to treat images as continuous signals, where\nencoding and decoding can be performed at arbitrary resolutions without prior\nresampling. We quantitatively and qualitatively show how MedIL compresses and\npreserves clinically-relevant features over large multi-site, multi-resolution\ndatasets of both T1w brain MRIs and lung CTs. We further demonstrate how MedIL\ncan influence the quality of images generated with a diffusion model, and\ndiscuss how MedIL can enhance generative models to resemble raw clinical\nacquisitions."}
{"id": "2504.09488", "pdf": "https://arxiv.org/pdf/2504.09488", "abs": "https://arxiv.org/abs/2504.09488", "authors": ["Jiashu Yang", "Ningning Wang", "Yian Zhao", "Chaoran Feng", "Junjia Du", "Hao Pang", "Zhirui Fang", "Xuxin Cheng"], "title": "Kongzi: A Historical Large Language Model with Fact Enhancement", "categories": ["cs.CL"], "comment": "22 pages, 12 figures", "summary": "The capabilities of the latest large language models (LLMs) have been\nextended from pure natural language understanding to complex reasoning tasks.\nHowever, current reasoning models often exhibit factual inaccuracies in longer\nreasoning chains, which poses challenges for historical reasoning and limits\nthe potential of LLMs in complex, knowledge-intensive tasks. Historical studies\nrequire not only the accurate presentation of factual information but also the\nability to establish cross-temporal correlations and derive coherent\nconclusions from fragmentary and often ambiguous sources. To address these\nchallenges, we propose Kongzi, a large language model specifically designed for\nhistorical analysis. Through the integration of curated, high-quality\nhistorical data and a novel fact-reinforcement learning strategy, Kongzi\ndemonstrates strong factual alignment and sophisticated reasoning depth.\nExtensive experiments on tasks such as historical question answering and\nnarrative generation demonstrate that Kongzi outperforms existing models in\nboth factual accuracy and reasoning depth. By effectively addressing the unique\nchallenges inherent in historical texts, Kongzi sets a new standard for the\ndevelopment of accurate and reliable LLMs in professional domains."}
{"id": "2504.09326", "pdf": "https://arxiv.org/pdf/2504.09326", "abs": "https://arxiv.org/abs/2504.09326", "authors": ["Huai-Qian Khor", "Yante Li", "Xingxun Jiang", "Guoying Zhao"], "title": "Infused Suppression Of Magnification Artefacts For Micro-AU Detection", "categories": ["cs.CV"], "comment": null, "summary": "Facial micro-expressions are spontaneous, brief and subtle facial motions\nthat unveil the underlying, suppressed emotions. Detecting Action Units (AUs)\nin micro-expressions is crucial because it yields a finer representation of\nfacial motions than categorical emotions, effectively resolving the ambiguity\namong different expressions. One of the difficulties in micro-expression\nanalysis is that facial motions are subtle and brief, thereby increasing the\ndifficulty in correlating facial motion features to AU occurrence. To bridge\nthe subtlety issue, flow-related features and motion magnification are a few\ncommon approaches as they can yield descriptive motion changes and increased\nmotion amplitude respectively. While motion magnification can amplify the\nmotion changes, it also accounts for illumination changes and projection errors\nduring the amplification process, thereby creating motion artefacts that\nconfuse the model to learn inauthentic magnified motion features. The problem\nis further aggravated in the context of a more complicated task where more AU\nclasses are analyzed in cross-database settings. To address this issue, we\npropose InfuseNet, a layer-wise unitary feature infusion framework that\nleverages motion context to constrain the Action Unit (AU) learning within an\ninformative facial movement region, thereby alleviating the influence of\nmagnification artefacts. On top of that, we propose leveraging magnified latent\nfeatures instead of reconstructing magnified samples to limit the distortion\nand artefacts caused by the projection inaccuracy in the motion reconstruction\nprocess. Via alleviating the magnification artefacts, InfuseNet has surpassed\nthe state-of-the-art results in the CD6ME protocol. Further quantitative\nstudies have also demonstrated the efficacy of motion artefacts alleviation."}
{"id": "2504.09504", "pdf": "https://arxiv.org/pdf/2504.09504", "abs": "https://arxiv.org/abs/2504.09504", "authors": ["Wei Tao", "Xiaoyang Qu", "Kai Lu", "Jiguang Wan", "Guokuan Li", "Jianzong Wang"], "title": "MADLLM: Multivariate Anomaly Detection via Pre-trained LLMs", "categories": ["cs.CL"], "comment": "Accepted by IEEE International Conference on Multimedia & Expo 2025\n  (ICME 2025)", "summary": "When applying pre-trained large language models (LLMs) to address anomaly\ndetection tasks, the multivariate time series (MTS) modality of anomaly\ndetection does not align with the text modality of LLMs. Existing methods\nsimply transform the MTS data into multiple univariate time series sequences,\nwhich can cause many problems. This paper introduces MADLLM, a novel\nmultivariate anomaly detection method via pre-trained LLMs. We design a new\ntriple encoding technique to align the MTS modality with the text modality of\nLLMs. Specifically, this technique integrates the traditional patch embedding\nmethod with two novel embedding approaches: Skip Embedding, which alters the\norder of patch processing in traditional methods to help LLMs retain knowledge\nof previous features, and Feature Embedding, which leverages contrastive\nlearning to allow the model to better understand the correlations between\ndifferent features. Experimental results demonstrate that our method\noutperforms state-of-the-art methods in various public anomaly detection\ndatasets."}
{"id": "2504.09328", "pdf": "https://arxiv.org/pdf/2504.09328", "abs": "https://arxiv.org/abs/2504.09328", "authors": ["Sonia Laguna", "Alberto Garcia-Garcia", "Marie-Julie Rakotosaona", "Stylianos Moschoglou", "Leonhard Helminger", "Sergio Orts-Escolano"], "title": "Text To 3D Object Generation For Scalable Room Assembly", "categories": ["cs.CV", "cs.LG"], "comment": "Published at the ICLR 2025 Workshop on Synthetic Data", "summary": "Modern machine learning models for scene understanding, such as depth\nestimation and object tracking, rely on large, high-quality datasets that mimic\nreal-world deployment scenarios. To address data scarcity, we propose an\nend-to-end system for synthetic data generation for scalable, high-quality, and\ncustomizable 3D indoor scenes. By integrating and adapting text-to-image and\nmulti-view diffusion models with Neural Radiance Field-based meshing, this\nsystem generates highfidelity 3D object assets from text prompts and\nincorporates them into pre-defined floor plans using a rendering tool. By\nintroducing novel loss functions and training strategies into existing methods,\nthe system supports on-demand scene generation, aiming to alleviate the\nscarcity of current available data, generally manually crafted by artists. This\nsystem advances the role of synthetic data in addressing machine learning\ntraining limitations, enabling more robust and generalizable models for\nreal-world applications."}
{"id": "2504.09522", "pdf": "https://arxiv.org/pdf/2504.09522", "abs": "https://arxiv.org/abs/2504.09522", "authors": ["Chen Sun", "Renat Aksitov", "Andrey Zhmoginov", "Nolan Andrew Miller", "Max Vladymyrov", "Ulrich Rueckert", "Been Kim", "Mark Sandler"], "title": "How new data permeates LLM knowledge and how to dilute it", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models learn and continually learn through the accumulation of\ngradient-based updates, but how individual pieces of new information affect\nexisting knowledge, leading to both beneficial generalization and problematic\nhallucination, remains poorly understood. We demonstrate that when learning new\ninformation, LLMs exhibit a \"priming\" effect: learning a new fact can cause the\nmodel to inappropriately apply that knowledge in unrelated contexts. To\nsystematically study this phenomenon, we introduce \"Outlandish,\" a carefully\ncurated dataset of 1320 diverse text samples designed to probe how new\nknowledge permeates through an LLM's existing knowledge base. Using this\ndataset, we show that the degree of priming after learning new information can\nbe predicted by measuring the token probability of key words before learning.\nThis relationship holds robustly across different model architectures (PALM-2,\nGemma, Llama), sizes, and training stages. Finally, we develop two novel\ntechniques to modulate how new knowledge affects existing model behavior: (1) a\n``stepping-stone'' text augmentation strategy and (2) an ``ignore-k'' update\npruning method. These approaches reduce undesirable priming effects by 50-95\\%\nwhile preserving the model's ability to learn new information. Our findings\nprovide both empirical insights into how LLMs learn and practical tools for\nimproving the specificity of knowledge insertion in language models. Further\nmaterials: https://sunchipsster1.github.io/projects/outlandish/"}
{"id": "2504.09354", "pdf": "https://arxiv.org/pdf/2504.09354", "abs": "https://arxiv.org/abs/2504.09354", "authors": ["Duy-Cat Can", "Quang-Huy Tang", "Huong Ha", "Binh T. Nguyen", "Oliver Y. Chén"], "title": "REMEMBER: Retrieval-based Explainable Multimodal Evidence-guided Modeling for Brain Evaluation and Reasoning in Zero- and Few-shot Neurodegenerative Diagnosis", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "q-bio.QM"], "comment": null, "summary": "Timely and accurate diagnosis of neurodegenerative disorders, such as\nAlzheimer's disease, is central to disease management. Existing deep learning\nmodels require large-scale annotated datasets and often function as \"black\nboxes\". Additionally, datasets in clinical practice are frequently small or\nunlabeled, restricting the full potential of deep learning methods. Here, we\nintroduce REMEMBER -- Retrieval-based Explainable Multimodal Evidence-guided\nModeling for Brain Evaluation and Reasoning -- a new machine learning framework\nthat facilitates zero- and few-shot Alzheimer's diagnosis using brain MRI scans\nthrough a reference-based reasoning process. Specifically, REMEMBER first\ntrains a contrastively aligned vision-text model using expert-annotated\nreference data and extends pseudo-text modalities that encode abnormality\ntypes, diagnosis labels, and composite clinical descriptions. Then, at\ninference time, REMEMBER retrieves similar, human-validated cases from a\ncurated dataset and integrates their contextual information through a dedicated\nevidence encoding module and attention-based inference head. Such an\nevidence-guided design enables REMEMBER to imitate real-world clinical\ndecision-making process by grounding predictions in retrieved imaging and\ntextual context. Specifically, REMEMBER outputs diagnostic predictions\nalongside an interpretable report, including reference images and explanations\naligned with clinical workflows. Experimental results demonstrate that REMEMBER\nachieves robust zero- and few-shot performance and offers a powerful and\nexplainable framework to neuroimaging-based diagnosis in the real world,\nespecially under limited data."}
{"id": "2504.09566", "pdf": "https://arxiv.org/pdf/2504.09566", "abs": "https://arxiv.org/abs/2504.09566", "authors": ["Chenghao Li", "Chaoning Zhang", "Yi Lu", "Jiaquan Zhang", "Qigan Sun", "Xudong Wang", "Jiwei Wei", "Guoqing Wang", "Yang Yang", "Heng Tao Shen"], "title": "Syzygy of Thoughts: Improving LLM CoT with the Minimal Free Resolution", "categories": ["cs.CL"], "comment": null, "summary": "Chain-of-Thought (CoT) prompting enhances the reasoning of large language\nmodels (LLMs) by decomposing problems into sequential steps, mimicking human\nlogic and reducing errors. However, complex tasks with vast solution spaces and\nvague constraints often exceed the capacity of a single reasoning chain.\nInspired by Minimal Free Resolution (MFR) in commutative algebra and algebraic\ngeometry, we propose Syzygy of Thoughts (SoT)-a novel framework that extends\nCoT by introducing auxiliary, interrelated reasoning paths. SoT captures deeper\nlogical dependencies, enabling more robust and structured problem-solving. MFR\ndecomposes a module into a sequence of free modules with minimal rank,\nproviding a structured analytical approach to complex systems. This method\nintroduces the concepts of \"Module\", \"Betti numbers\",\"Freeness\", \"Mapping\",\n\"Exactness\" and \"Minimality\", enabling the systematic decomposition of the\noriginal complex problem into logically complete minimal subproblems while\npreserving key problem features and reducing reasoning length. We tested SoT\nacross diverse datasets (e.g., GSM8K, MATH) and models (e.g., GPT-4o-mini,\nQwen2.5), achieving inference accuracy that matches or surpasses mainstream\nCoTs standards. Additionally, by aligning the sampling process with algebraic\nconstraints, our approach enhances the scalability of inference time in LLMs,\nensuring both transparent reasoning and high performance. Our code will be\npublicly available at https://github.com/dlMARiA/Syzygy-of-thoughts."}
{"id": "2504.09361", "pdf": "https://arxiv.org/pdf/2504.09361", "abs": "https://arxiv.org/abs/2504.09361", "authors": ["Jiahuan Long", "Tingsong Jiang", "Wen Yao", "Shuai Jia", "Weijia Zhang", "Weien Zhou", "Chao Ma", "Xiaoqian Chen"], "title": "PapMOT: Exploring Adversarial Patch Attack against Multiple Object Tracking", "categories": ["cs.CV"], "comment": "Accepted by ECCV 2024", "summary": "Tracking multiple objects in a continuous video stream is crucial for many\ncomputer vision tasks. It involves detecting and associating objects with their\nrespective identities across successive frames. Despite significant progress\nmade in multiple object tracking (MOT), recent studies have revealed the\nvulnerability of existing MOT methods to adversarial attacks. Nevertheless, all\nof these attacks belong to digital attacks that inject pixel-level noise into\ninput images, and are therefore ineffective in physical scenarios. To fill this\ngap, we propose PapMOT, which can generate physical adversarial patches against\nMOT for both digital and physical scenarios. Besides attacking the detection\nmechanism, PapMOT also optimizes a printable patch that can be detected as new\ntargets to mislead the identity association process. Moreover, we introduce a\npatch enhancement strategy to further degrade the temporal consistency of\ntracking results across video frames, resulting in more aggressive attacks. We\nfurther develop new evaluation metrics to assess the robustness of MOT against\nsuch attacks. Extensive evaluations on multiple datasets demonstrate that our\nPapMOT can successfully attack various architectures of MOT trackers in digital\nscenarios. We also validate the effectiveness of PapMOT for physical attacks by\ndeploying printed adversarial patches in the real world."}
{"id": "2504.09570", "pdf": "https://arxiv.org/pdf/2504.09570", "abs": "https://arxiv.org/abs/2504.09570", "authors": ["Biao Fu", "Minpeng Liao", "Kai Fan", "Chengxi Li", "Liang Zhang", "Yidong Chen", "Xiaodong Shi"], "title": "LLMs Can Achieve High-quality Simultaneous Machine Translation as Efficiently as Offline", "categories": ["cs.CL"], "comment": null, "summary": "When the complete source sentence is provided, Large Language Models (LLMs)\nperform excellently in offline machine translation even with a simple prompt\n\"Translate the following sentence from [src lang] into [tgt lang]:\". However,\nin many real scenarios, the source tokens arrive in a streaming manner and\nsimultaneous machine translation (SiMT) is required, then the efficiency and\nperformance of decoder-only LLMs are significantly limited by their\nauto-regressive nature. To enable LLMs to achieve high-quality SiMT as\nefficiently as offline translation, we propose a novel paradigm that includes\nconstructing supervised fine-tuning (SFT) data for SiMT, along with new\ntraining and inference strategies. To replicate the token input/output stream\nin SiMT, the source and target tokens are rearranged into an interleaved\nsequence, separated by special tokens according to varying latency\nrequirements. This enables powerful LLMs to learn read and write operations\nadaptively, based on varying latency prompts, while still maintaining efficient\nauto-regressive decoding. Experimental results show that, even with limited SFT\ndata, our approach achieves state-of-the-art performance across various SiMT\nbenchmarks, and preserves the original abilities of offline translation.\nMoreover, our approach generalizes well to document-level SiMT setting without\nrequiring specific fine-tuning, even beyond the offline translation model."}
{"id": "2504.09377", "pdf": "https://arxiv.org/pdf/2504.09377", "abs": "https://arxiv.org/abs/2504.09377", "authors": ["Jiawei Wu", "Zhifei Yang", "Zhe Wang", "Zhi Jin"], "title": "Beyond Degradation Conditions: All-in-One Image Restoration via HOG Transformers", "categories": ["cs.CV"], "comment": null, "summary": "All-in-one image restoration, which aims to address diverse degradations\nwithin a unified framework, is critical for practical applications. However,\nexisting methods rely on predicting and integrating degradation conditions,\nwhich can misactivate degradation-specific features in complex scenarios,\nlimiting their restoration performance. To address this issue, we propose a\nnovel all-in-one image restoration framework guided by Histograms of Oriented\nGradients (HOG), named HOGformer. By leveraging the degradation-discriminative\ncapability of HOG descriptors, HOGformer employs a dynamic self-attention\nmechanism that adaptively attends to long-range spatial dependencies based on\ndegradation-aware HOG cues. To enhance the degradation sensitivity of attention\ninputs, we design a HOG-guided local dynamic-range convolution module that\ncaptures long-range degradation similarities while maintaining awareness of\nglobal structural information. Furthermore, we propose a dynamic interaction\nfeed-forward module, efficiently increasing the model capacity to adapt to\ndifferent degradations through channel-spatial interactions. Extensive\nexperiments across diverse benchmarks, including adverse weather and natural\ndegradations, demonstrate that HOGformer achieves state-of-the-art performance\nand generalizes effectively to complex real-world degradations. Code is\navailable at https://github.com/Fire-friend/HOGformer."}
{"id": "2504.09586", "pdf": "https://arxiv.org/pdf/2504.09586", "abs": "https://arxiv.org/abs/2504.09586", "authors": ["Zuoli Tang", "Junjie Ou", "Kaiqin Hu", "Chunwei Wu", "Zhaoxin Huan", "Chilin Fu", "Xiaolu Zhang", "Jun Zhou", "Chenliang Li"], "title": "Short-Path Prompting in LLMs: Analyzing Reasoning Instability and Solutions for Robust Performance", "categories": ["cs.CL"], "comment": "Under review", "summary": "Recent years have witnessed significant progress in large language models'\n(LLMs) reasoning, which is largely due to the chain-of-thought (CoT)\napproaches, allowing models to generate intermediate reasoning steps before\nreaching the final answer. Building on these advances, state-of-the-art LLMs\nare instruction-tuned to provide long and detailed CoT pathways when responding\nto reasoning-related questions. However, human beings are naturally cognitive\nmisers and will prompt language models to give rather short responses, thus\nraising a significant conflict with CoT reasoning. In this paper, we delve into\nhow LLMs' reasoning performance changes when users provide short-path prompts.\nThe results and analysis reveal that language models can reason effectively and\nrobustly without explicit CoT prompts, while under short-path prompting, LLMs'\nreasoning ability drops significantly and becomes unstable, even on\ngrade-school problems. To address this issue, we propose two approaches: an\ninstruction-guided approach and a fine-tuning approach, both designed to\neffectively manage the conflict. Experimental results show that both methods\nachieve high accuracy, providing insights into the trade-off between\ninstruction adherence and reasoning accuracy in current models."}
{"id": "2504.09379", "pdf": "https://arxiv.org/pdf/2504.09379", "abs": "https://arxiv.org/abs/2504.09379", "authors": ["Lei Sun", "Yuhan Bao", "Jiajun Zhai", "Jingyun Liang", "Yulun Zhang", "Kaiwei Wang", "Danda Pani Paudel", "Luc Van Gool"], "title": "Low-Light Image Enhancement using Event-Based Illumination Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Low-light image enhancement (LLIE) aims to improve the visibility of images\ncaptured in poorly lit environments. Prevalent event-based solutions primarily\nutilize events triggered by motion, i.e., ''motion events'' to strengthen only\nthe edge texture, while leaving the high dynamic range and excellent low-light\nresponsiveness of event cameras largely unexplored. This paper instead opens a\nnew avenue from the perspective of estimating the illumination using\n''temporal-mapping'' events, i.e., by converting the timestamps of events\ntriggered by a transmittance modulation into brightness values. The resulting\nfine-grained illumination cues facilitate a more effective decomposition and\nenhancement of the reflectance component in low-light images through the\nproposed Illumination-aided Reflectance Enhancement module. Furthermore, the\ndegradation model of temporal-mapping events under low-light condition is\ninvestigated for realistic training data synthesizing. To address the lack of\ndatasets under this regime, we construct a beam-splitter setup and collect\nEvLowLight dataset that includes images, temporal-mapping events, and motion\nevents. Extensive experiments across 5 synthetic datasets and our real-world\nEvLowLight dataset substantiate that the devised pipeline, dubbed RetinEV,\nexcels in producing well-illuminated, high dynamic range images, outperforming\nprevious state-of-the-art event-based methods by up to 6.62 dB, while\nmaintaining an efficient inference speed of 35.6 frame-per-second on a 640X480\nimage."}
{"id": "2504.09620", "pdf": "https://arxiv.org/pdf/2504.09620", "abs": "https://arxiv.org/abs/2504.09620", "authors": ["Yuta Matsui", "Ryosuke Yamaki", "Ryo Ueda", "Seitaro Shinagawa", "Tadahiro Taniguchi"], "title": "Metropolis-Hastings Captioning Game: Knowledge Fusion of Vision Language Models via Decentralized Bayesian Inference", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.MA"], "comment": null, "summary": "We propose the Metropolis-Hastings Captioning Game (MHCG), a method to fuse\nknowledge of multiple vision-language models (VLMs) by learning from each\nother. Although existing methods that combine multiple models suffer from\ninference costs and architectural constraints, MHCG avoids these problems by\nperforming decentralized Bayesian inference through a process resembling a\nlanguage game. The knowledge fusion process establishes communication between\ntwo VLM agents alternately captioning images and learning from each other. We\nconduct two image-captioning experiments with two VLMs, each pre-trained on a\ndifferent dataset. The first experiment demonstrates that MHCG achieves\nconsistent improvement in reference-free evaluation metrics. The second\nexperiment investigates how MHCG contributes to sharing VLMs' category-level\nvocabulary by observing the occurrence of the vocabulary in the generated\ncaptions."}
{"id": "2504.09384", "pdf": "https://arxiv.org/pdf/2504.09384", "abs": "https://arxiv.org/abs/2504.09384", "authors": ["Shengzhe Chen", "Zhaoxuan Dong", "Jun Liu"], "title": "Contour Flow Constraint: Preserving Global Shape Similarity for Deep Learning based Image Segmentation", "categories": ["cs.CV"], "comment": "Submitted to IEEE Transactions on Image Processin on Dec-14-2023.\n  Revised on Oct-16-2024", "summary": "For effective image segmentation, it is crucial to employ constraints\ninformed by prior knowledge about the characteristics of the areas to be\nsegmented to yield favorable segmentation outcomes. However, the existing\nmethods have primarily focused on priors of specific properties or shapes,\nlacking consideration of the general global shape similarity from a Contour\nFlow (CF) perspective. Furthermore, naturally integrating this contour flow\nprior image segmentation model into the activation functions of deep\nconvolutional networks through mathematical methods is currently unexplored. In\nthis paper, we establish a concept of global shape similarity based on the\npremise that two shapes exhibit comparable contours. Furthermore, we\nmathematically derive a contour flow constraint that ensures the preservation\nof global shape similarity. We propose two implementations to integrate the\nconstraint with deep neural networks. Firstly, the constraint is converted to a\nshape loss, which can be seamlessly incorporated into the training phase for\nany learning-based segmentation framework. Secondly, we add the constraint into\na variational segmentation model and derive its iterative schemes for solution.\nThe scheme is then unrolled to get the architecture of the proposed CFSSnet.\nValidation experiments on diverse datasets are conducted on classic benchmark\ndeep network segmentation models. The results indicate a great improvement in\nsegmentation accuracy and shape similarity for the proposed shape loss,\nshowcasing the general adaptability of the proposed loss term regardless of\nspecific network architectures. CFSSnet shows robustness in segmenting\nnoise-contaminated images, and inherent capability to preserve global shape\nsimilarity."}
{"id": "2504.09639", "pdf": "https://arxiv.org/pdf/2504.09639", "abs": "https://arxiv.org/abs/2504.09639", "authors": ["Haotian Wang", "Han Zhao", "Shuaiting Chen", "Xiaoyu Tian", "Sitong Zhao", "Yunjie Ji", "Yiping Peng", "Xiangang Li"], "title": "Leveraging Reasoning Model Answers to Enhance Non-Reasoning Model Capability", "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in large language models (LLMs), such as DeepSeek-R1 and\nOpenAI-o1, have demonstrated the significant effectiveness of test-time\nscaling, achieving substantial performance gains across various benchmarks.\nThese advanced models utilize deliberate \"thinking\" steps to systematically\nenhance answer quality. In this paper, we propose leveraging these high-quality\noutputs generated by reasoning-intensive models to improve less computationally\ndemanding, non-reasoning models. We explore and compare methodologies for\nutilizing the answers produced by reasoning models to train and improve\nnon-reasoning models. Through straightforward Supervised Fine-Tuning (SFT)\nexperiments on established benchmarks, we demonstrate consistent improvements\nacross various benchmarks, underscoring the potential of this approach for\nadvancing the ability of models to answer questions directly."}
{"id": "2504.09393", "pdf": "https://arxiv.org/pdf/2504.09393", "abs": "https://arxiv.org/abs/2504.09393", "authors": ["Nooshin Bahador"], "title": "Vision Transformers Exhibit Human-Like Biases: Evidence of Orientation and Color Selectivity, Categorical Perception, and Phase Transitions", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "This study explored whether Vision Transformers (ViTs) developed orientation\nand color biases similar to those observed in the human brain. Using synthetic\ndatasets with controlled variations in noise levels, angles, lengths, widths,\nand colors, we analyzed the behavior of ViTs fine-tuned with LoRA. Our findings\nrevealed four key insights: First, ViTs exhibited an oblique effect showing the\nlowest angle prediction errors at 180 deg (horizontal) across all conditions.\nSecond, angle prediction errors varied by color. Errors were highest for bluish\nhues and lowest for yellowish ones. Additionally, clustering analysis of angle\nprediction errors showed that ViTs grouped colors in a way that aligned with\nhuman perceptual categories. In addition to orientation and color biases, we\nobserved phase transition phenomena. While two phase transitions occurred\nconsistently across all conditions, the training loss curves exhibited delayed\ntransitions when color was incorporated as an additional data attribute.\nFinally, we observed that attention heads in certain layers inherently develop\nspecialized capabilities, functioning as task-agnostic feature extractors\nregardless of the downstream task. These observations suggest that biases and\nproperties arise primarily from pre-training on the original dataset which\nshapes the model's foundational representations and the inherent architectural\nconstraints of the vision transformer, rather than being solely determined by\ndownstream data statistics."}
{"id": "2504.09643", "pdf": "https://arxiv.org/pdf/2504.09643", "abs": "https://arxiv.org/abs/2504.09643", "authors": ["Nikita Sorokin", "Ivan Sedykh", "Valentin Malykh"], "title": "Iterative Self-Training for Code Generation via Reinforced Re-Ranking", "categories": ["cs.CL", "cs.IR", "cs.SE"], "comment": "Published at ECIR 2025", "summary": "Generating high-quality code that solves complex programming tasks is\nchallenging, especially with current decoder-based models that produce highly\nstochastic outputs. In code generation, even minor errors can easily break the\nentire solution. Leveraging multiple sampled solutions can significantly\nimprove the overall output quality.\n  One effective way to enhance code generation is by pairing a code generation\nmodel with a reranker model, which selects the best solution from the generated\nsamples. We propose a novel iterative self-training approach for self-training\nreranker models using Proximal Policy Optimization (PPO), aimed at improving\nboth reranking accuracy and the overall code generation process. Unlike\ntraditional PPO approaches, where the focus is on optimizing a generative model\nwith a reward model, our approach emphasizes the development of a robust\nreward/reranking model. This model improves the quality of generated code\nthrough reranking and addresses problems and errors that the reward model might\noverlook during PPO alignment with the reranker. Our method iteratively refines\nthe training dataset by re-evaluating outputs, identifying high-scoring\nnegative examples, and incorporating them into the training loop, that boosting\nmodel performance.\n  Our evaluation on the MultiPL-E dataset demonstrates that our 13.4B parameter\nmodel outperforms a 33B model in code generation quality while being three\ntimes faster. Moreover, it achieves performance comparable to GPT-4 and\nsurpasses it in one programming language."}
{"id": "2504.09424", "pdf": "https://arxiv.org/pdf/2504.09424", "abs": "https://arxiv.org/abs/2504.09424", "authors": ["Luis Vieira"], "title": "Comparing Performance of Preprocessing Techniques for Traffic Sign Recognition Using a HOG-SVM", "categories": ["cs.CV"], "comment": "working paper (preprint)", "summary": "This study compares the performance of various preprocessing techniques for\nTraffic Sign Recognition (TSR) using Histogram of Oriented Gradients (HOG) and\nSupport Vector Machine (SVM) on the German Traffic Sign Recognition Benchmark\n(GTSRB) dataset. Techniques such as CLAHE, HUE, and YUV were evaluated for\ntheir impact on classification accuracy. Results indicate that YUV in\nparticular significantly enhance the performance of the HOG-SVM classifier\n(improving accuracy from 89.65% to 91.25%), providing insights into\nimprovements for preprocessing pipeline of TSR applications."}
{"id": "2504.09645", "pdf": "https://arxiv.org/pdf/2504.09645", "abs": "https://arxiv.org/abs/2504.09645", "authors": ["Aung Kyaw Htet", "Mark Dras"], "title": "Myanmar XNLI: Building a Dataset and Exploring Low-resource Approaches to Natural Language Inference with Myanmar", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite dramatic recent progress in NLP, it is still a major challenge to\napply Large Language Models (LLM) to low-resource languages. This is made\nvisible in benchmarks such as Cross-Lingual Natural Language Inference (XNLI),\na key task that demonstrates cross-lingual capabilities of NLP systems across a\nset of 15 languages. In this paper, we extend the XNLI task for one additional\nlow-resource language, Myanmar, as a proxy challenge for broader low-resource\nlanguages, and make three core contributions. First, we build a dataset called\nMyanmar XNLI (myXNLI) using community crowd-sourced methods, as an extension to\nthe existing XNLI corpus. This involves a two-stage process of community-based\nconstruction followed by expert verification; through an analysis, we\ndemonstrate and quantify the value of the expert verification stage in the\ncontext of community-based construction for low-resource languages. We make the\nmyXNLI dataset available to the community for future research. Second, we carry\nout evaluations of recent multilingual language models on the myXNLI benchmark,\nas well as explore data-augmentation methods to improve model performance. Our\ndata-augmentation methods improve model accuracy by up to 2 percentage points\nfor Myanmar, while uplifting other languages at the same time. Third, we\ninvestigate how well these data-augmentation methods generalise to other\nlow-resource languages in the XNLI dataset."}
{"id": "2504.09426", "pdf": "https://arxiv.org/pdf/2504.09426", "abs": "https://arxiv.org/abs/2504.09426", "authors": ["Shengao Wang", "Arjun Chandra", "Aoming Liu", "Venkatesh Saligrama", "Boqing Gong"], "title": "BabyVLM: Data-Efficient Pretraining of VLMs Inspired by Infant Learning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Human infants rapidly develop visual reasoning skills from minimal input,\nsuggesting that developmentally inspired pretraining could significantly\nenhance the efficiency of vision-language models (VLMs). Although recent\nefforts have leveraged infant-inspired datasets like SAYCam, existing\nevaluation benchmarks remain misaligned--they are either too simplistic,\nnarrowly scoped, or tailored for large-scale pretrained models. Additionally,\ntraining exclusively on infant data overlooks the broader, diverse input from\nwhich infants naturally learn. To address these limitations, we propose\nBabyVLM, a novel framework comprising comprehensive in-domain evaluation\nbenchmarks and a synthetic training dataset created via child-directed\ntransformations of existing datasets. We demonstrate that VLMs trained with our\nsynthetic dataset achieve superior performance on BabyVLM tasks compared to\nmodels trained solely on SAYCam or general-purpose data of the SAYCam size.\nBabyVLM thus provides a robust, developmentally aligned evaluation tool and\nillustrates how compact models trained on carefully curated data can generalize\neffectively, opening pathways toward data-efficient vision-language learning\nparadigms."}
{"id": "2504.09665", "pdf": "https://arxiv.org/pdf/2504.09665", "abs": "https://arxiv.org/abs/2504.09665", "authors": ["Liqiang Wen", "Guanming Xiong", "Tong Mo", "Bing Li", "Weiping Li", "Wen Zhao"], "title": "CLEAR-KGQA: Clarification-Enhanced Ambiguity Resolution for Knowledge Graph Question Answering", "categories": ["cs.CL"], "comment": "This work has been accepted by the IJCNN 2025 main track", "summary": "This study addresses the challenge of ambiguity in knowledge graph question\nanswering (KGQA). While recent KGQA systems have made significant progress,\nparticularly with the integration of large language models (LLMs), they\ntypically assume user queries are unambiguous, which is an assumption that\nrarely holds in real-world applications. To address these limitations, we\npropose a novel framework that dynamically handles both entity ambiguity (e.g.,\ndistinguishing between entities with similar names) and intent ambiguity (e.g.,\nclarifying different interpretations of user queries) through interactive\nclarification. Our approach employs a Bayesian inference mechanism to quantify\nquery ambiguity and guide LLMs in determining when and how to request\nclarification from users within a multi-turn dialogue framework. We further\ndevelop a two-agent interaction framework where an LLM-based user simulator\nenables iterative refinement of logical forms through simulated user feedback.\nExperimental results on the WebQSP and CWQ dataset demonstrate that our method\nsignificantly improves performance by effectively resolving semantic\nambiguities. Additionally, we contribute a refined dataset of disambiguated\nqueries, derived from interaction histories, to facilitate future research in\nthis direction."}
{"id": "2504.09441", "pdf": "https://arxiv.org/pdf/2504.09441", "abs": "https://arxiv.org/abs/2504.09441", "authors": ["Jiahua Xu", "Dawei Zhou", "Lei Hu", "Zaiyi Liu", "Nannan Wang", "Xinbo Gao"], "title": "Structure-Accurate Medical Image Translation based on Dynamic Frequency Balance and Knowledge Guidance", "categories": ["cs.CV", "eess.IV"], "comment": "Medical image translation, Diffusion model, 16 pages", "summary": "Multimodal medical images play a crucial role in the precise and\ncomprehensive clinical diagnosis. Diffusion model is a powerful strategy to\nsynthesize the required medical images. However, existing approaches still\nsuffer from the problem of anatomical structure distortion due to the\noverfitting of high-frequency information and the weakening of low-frequency\ninformation. Thus, we propose a novel method based on dynamic frequency balance\nand knowledge guidance. Specifically, we first extract the low-frequency and\nhigh-frequency components by decomposing the critical features of the model\nusing wavelet transform. Then, a dynamic frequency balance module is designed\nto adaptively adjust frequency for enhancing global low-frequency features and\neffective high-frequency details as well as suppressing high-frequency noise.\nTo further overcome the challenges posed by the large differences between\ndifferent medical modalities, we construct a knowledge-guided mechanism that\nfuses the prior clinical knowledge from a visual language model with visual\nfeatures, to facilitate the generation of accurate anatomical structures.\nExperimental evaluations on multiple datasets show the proposed method achieves\nsignificant improvements in qualitative and quantitative assessments, verifying\nits effectiveness and superiority."}
{"id": "2504.09687", "pdf": "https://arxiv.org/pdf/2504.09687", "abs": "https://arxiv.org/abs/2504.09687", "authors": ["Salman Faroz"], "title": "Domain-Adaptive Continued Pre-Training of Small Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Continued pre-training of small language models offers a promising path for\ndomain adaptation with limited computational resources. I've investigated this\napproach within educational domains, evaluating it as a resource-efficient\nalternative to training models from scratch. Using a 125M parameter model, I\ndemonstrate significant performance improvements through incremental training\non 400 million tokens, followed by further training to reach 1 billion tokens.\nMy approach includes comprehensive data preprocessing, memory-optimized\ntraining configurations, and benchmark-based evaluation. Results show notable\ngains in knowledge-intensive tasks (MMLU +8.1%) and contextual understanding\n(HellaSwag +7.6%), while revealing educational domain specialization\ntrade-offs. I analyze token efficiency, catastrophic forgetting mitigation\nstrategies, and scaling patterns. My findings suggest that thoughtful\npreprocessing and training methodologies enable meaningful improvements in\nlanguage model capabilities even with constrained computational resources,\nopening pathways for domain-specific adaptation of smaller language models."}
{"id": "2504.09446", "pdf": "https://arxiv.org/pdf/2504.09446", "abs": "https://arxiv.org/abs/2504.09446", "authors": ["Lincoln Linlin Xu", "Yimin Zhu", "Zack Dewis", "Zhengsen Xu", "Motasem Alkayid", "Mabel Heffring", "Saeid Taleghanidoozdoozan"], "title": "Sparse Deformable Mamba for Hyperspectral Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "Although the recent Mamba models significantly improve hyperspectral image\n(HSI) classification, one critical challenge is caused by the difficulty to\nbuild the Mamba sequence efficiently and effectively. This paper presents a\nSparse Deformable Mamba (SDMamba) approach for enhanced HSI classification,\nwith the following contributions. First, to enhance Mamba sequence, an\nefficient Sparse Deformable Sequencing (SDS) approach is designed to adaptively\nlearn the \"optimal\" sequence, leading to sparse and deformable Mamba sequence\nwith increased detail preservation and decreased computations. Second, to boost\nspatial-spectral feature learning, based on SDS, a Sparse Deformable Spatial\nMamba Module (SDSpaM) and a Sparse Deformable Spectral Mamba Module (SDSpeM)\nare designed for tailored modeling of the spatial information spectral\ninformation. Last, to improve the fusion of SDSpaM and SDSpeM, an attention\nbased feature fusion approach is designed to integrate the outputs of the\nSDSpaM and SDSpeM. The proposed method is tested on several benchmark datasets\nwith many state-of-the-art approaches, demonstrating that the proposed approach\ncan achieve higher accuracy, faster speed, and better detail small-class\npreservation capability."}
{"id": "2504.09696", "pdf": "https://arxiv.org/pdf/2504.09696", "abs": "https://arxiv.org/abs/2504.09696", "authors": ["Jixiao Zhang", "Chunsheng Zuo"], "title": "GRPO-LEAD: A Difficulty-Aware Reinforcement Learning Approach for Concise Mathematical Reasoning in Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in R1-like reasoning models leveraging Group Relative Policy\nOptimization (GRPO) have significantly improved the performance of language\nmodels on mathematical reasoning tasks. However, current GRPO implementations\nencounter critical challenges, including reward sparsity due to binary accuracy\nmetrics, limited incentives for conciseness, and insufficient focus on complex\nreasoning tasks. To address these issues, we propose GRPO-LEAD, a suite of\nnovel enhancements tailored for mathematical reasoning. Specifically, GRPO-LEAD\nintroduces (1) a length-dependent accuracy reward to encourage concise and\nprecise solutions, (2) an explicit penalty mechanism for incorrect answers to\nsharpen decision boundaries, and (3) a difficulty-aware advantage reweighting\nstrategy that amplifies learning signals for challenging problems. Furthermore,\nwe systematically examine the impact of model scale and supervised fine-tuning\n(SFT) strategies, demonstrating that larger-scale base models and carefully\ncurated datasets significantly enhance reinforcement learning effectiveness.\nExtensive empirical evaluations and ablation studies confirm that GRPO-LEAD\nsubstantially mitigates previous shortcomings, resulting in language models\nthat produce more concise, accurate, and robust reasoning across diverse\nmathematical tasks."}
{"id": "2504.09448", "pdf": "https://arxiv.org/pdf/2504.09448", "abs": "https://arxiv.org/abs/2504.09448", "authors": ["Lin Zhu", "Yifeng Yang", "Zichao Nie", "Yuan Gao", "Jiarui Li", "Qinying Gu", "Xinbing Wang", "Chenghu Zhou", "Nanyang Ye"], "title": "InfoBound: A Provable Information-Bounds Inspired Framework for Both OoD Generalization and OoD Detection", "categories": ["cs.CV"], "comment": "Under Review", "summary": "In real-world scenarios, distribution shifts give rise to the importance of\ntwo problems: out-of-distribution (OoD) generalization, which focuses on\nmodels' generalization ability against covariate shifts (i.e., the changes of\nenvironments), and OoD detection, which aims to be aware of semantic shifts\n(i.e., test-time unseen classes). Real-world testing environments often involve\na combination of both covariate and semantic shifts. While numerous methods\nhave been proposed to address these critical issues, only a few works tackled\nthem simultaneously. Moreover, prior works often improve one problem but\nsacrifice the other. To overcome these limitations, we delve into boosting OoD\ndetection and OoD generalization from the perspective of information theory,\nwhich can be easily applied to existing models and different tasks. Building\nupon the theoretical bounds for mutual information and conditional entropy, we\nprovide a unified approach, composed of Mutual Information Minimization\n(MI-Min) and Conditional Entropy Maximizing (CE-Max). Extensive experiments and\ncomprehensive evaluations on multi-label image classification and object\ndetection have demonstrated the superiority of our method. It successfully\nmitigates trade-offs between the two challenges compared to competitive\nbaselines."}
{"id": "2504.09714", "pdf": "https://arxiv.org/pdf/2504.09714", "abs": "https://arxiv.org/abs/2504.09714", "authors": ["Ayşe Aysu Cengiz", "Ahmet Kaan Sever", "Elif Ecem Ümütlü", "Naime Şeyma Erdem", "Burak Aytan", "Büşra Tufan", "Abdullah Topraksoy", "Esra Darıcı", "Cagri Toraman"], "title": "Evaluating the Quality of Benchmark Datasets for Low-Resource Languages: A Case Study on Turkish", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The reliance on translated or adapted datasets from English or multilingual\nresources introduces challenges regarding linguistic and cultural suitability.\nThis study addresses the need for robust and culturally appropriate benchmarks\nby evaluating the quality of 17 commonly used Turkish benchmark datasets. Using\na comprehensive framework that assesses six criteria, both human and LLM-judge\nannotators provide detailed evaluations to identify dataset strengths and\nshortcomings.\n  Our results reveal that 70% of the benchmark datasets fail to meet our\nheuristic quality standards. The correctness of the usage of technical terms is\nthe strongest criterion, but 85% of the criteria are not satisfied in the\nexamined datasets. Although LLM judges demonstrate potential, they are less\neffective than human annotators, particularly in understanding cultural common\nsense knowledge and interpreting fluent, unambiguous text. GPT-4o has stronger\nlabeling capabilities for grammatical and technical tasks, while Llama3.3-70B\nexcels at correctness and cultural knowledge evaluation. Our findings emphasize\nthe urgent need for more rigorous quality control in creating and adapting\ndatasets for low-resource languages."}
{"id": "2504.09451", "pdf": "https://arxiv.org/pdf/2504.09451", "abs": "https://arxiv.org/abs/2504.09451", "authors": ["Tianyi Wang", "Harry Cheng", "Ming-Hui Liu", "Mohan Kankanhalli"], "title": "FractalForensics: Proactive Deepfake Detection and Localization via Fractal Watermarks", "categories": ["cs.CV"], "comment": null, "summary": "Proactive Deepfake detection via robust watermarks has been raised ever since\npassive Deepfake detectors encountered challenges in identifying high-quality\nsynthetic images. However, while demonstrating reasonable detection\nperformance, they lack localization functionality and explainability in\ndetection results. Additionally, the unstable robustness of watermarks can\nsignificantly affect the detection performance accordingly. In this study, we\npropose novel fractal watermarks for proactive Deepfake detection and\nlocalization, namely FractalForensics. Benefiting from the characteristics of\nfractals, we devise a parameter-driven watermark generation pipeline that\nderives fractal-based watermarks and conducts one-way encryption regarding the\nparameters selected. Subsequently, we propose a semi-fragile watermarking\nframework for watermark embedding and recovery, trained to be robust against\nbenign image processing operations and fragile when facing Deepfake\nmanipulations in a black-box setting. Meanwhile, we introduce an entry-to-patch\nstrategy that implicitly embeds the watermark matrix entries into image patches\nat corresponding positions, achieving localization of Deepfake manipulations.\nExtensive experiments demonstrate satisfactory robustness and fragility of our\napproach against common image processing operations and Deepfake manipulations,\noutperforming state-of-the-art semi-fragile watermarking algorithms and passive\ndetectors for Deepfake detection. Furthermore, by highlighting the areas\nmanipulated, our method provides explainability for the proactive Deepfake\ndetection results."}
{"id": "2504.09753", "pdf": "https://arxiv.org/pdf/2504.09753", "abs": "https://arxiv.org/abs/2504.09753", "authors": ["Ram Mohan Rao Kadiyala", "Siddartha Pullakhandam", "Siddhant Gupta", "Drishti Sharma", "Jebish Purbey", "Kanwal Mehreen", "Muhammad Arham", "Hamza Farooq"], "title": "Improving Multilingual Capabilities with Cultural and Local Knowledge in Large Language Models While Enhancing Native Performance", "categories": ["cs.CL", "cs.AI"], "comment": "ARR Feb 2025 submission", "summary": "Large Language Models (LLMs) have shown remarkable capabilities, but their\ndevelopment has primarily focused on English and other high-resource languages,\nleaving many languages underserved. We present our latest Hindi-English\nbi-lingual LLM \\textbf{Mantra-14B} with ~3\\% average improvement in benchmark\nscores over both languages, outperforming models twice its size. Using a\ncurated dataset composed of English and Hindi instruction data of 485K samples,\nwe instruction tuned models such as Qwen-2.5-14B-Instruct and Phi-4 to improve\nperformance over both English and Hindi. Our experiments encompassing seven\ndifferent LLMs of varying parameter sizes and over 140 training attempts with\nvarying English-Hindi training data ratios demonstrated that it is possible to\nsignificantly improve multilingual performance without compromising native\nperformance. Further, our approach avoids resource-intensive techniques like\nvocabulary expansion or architectural modifications, thus keeping the model\nsize small. Our results indicate that modest fine-tuning with culturally and\nlocally informed data can bridge performance gaps without incurring significant\ncomputational overhead. We release our training code, datasets, and models\nunder mit and apache licenses to aid further research towards under-represented\nand low-resource languages."}
{"id": "2504.09454", "pdf": "https://arxiv.org/pdf/2504.09454", "abs": "https://arxiv.org/abs/2504.09454", "authors": ["Weinan Jia", "Mengqi Huang", "Nan Chen", "Lei Zhang", "Zhendong Mao"], "title": "D$^2$iT: Dynamic Diffusion Transformer for Accurate Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models are widely recognized for their ability to generate\nhigh-fidelity images. Despite the excellent performance and scalability of the\nDiffusion Transformer (DiT) architecture, it applies fixed compression across\ndifferent image regions during the diffusion process, disregarding the\nnaturally varying information densities present in these regions. However,\nlarge compression leads to limited local realism, while small compression\nincreases computational complexity and compromises global consistency,\nultimately impacting the quality of generated images. To address these\nlimitations, we propose dynamically compressing different image regions by\nrecognizing the importance of different regions, and introduce a novel\ntwo-stage framework designed to enhance the effectiveness and efficiency of\nimage generation: (1) Dynamic VAE (DVAE) at first stage employs a hierarchical\nencoder to encode different image regions at different downsampling rates,\ntailored to their specific information densities, thereby providing more\naccurate and natural latent codes for the diffusion process. (2) Dynamic\nDiffusion Transformer (D$^2$iT) at second stage generates images by predicting\nmulti-grained noise, consisting of coarse-grained (less latent code in smooth\nregions) and fine-grained (more latent codes in detailed regions), through an\nnovel combination of the Dynamic Grain Transformer and the Dynamic Content\nTransformer. The strategy of combining rough prediction of noise with detailed\nregions correction achieves a unification of global consistency and local\nrealism. Comprehensive experiments on various generation tasks validate the\neffectiveness of our approach. Code will be released at\nhttps://github.com/jiawn-creator/Dynamic-DiT."}
{"id": "2504.09763", "pdf": "https://arxiv.org/pdf/2504.09763", "abs": "https://arxiv.org/abs/2504.09763", "authors": ["Zaid Khan", "Elias Stengel-Eskin", "Archiki Prasad", "Jaemin Cho", "Mohit Bansal"], "title": "Executable Functional Abstractions: Inferring Generative Programs for Advanced Math Problems", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project Page: https://zaidkhan.me/EFAGen/", "summary": "Scientists often infer abstract procedures from specific instances of\nproblems and use the abstractions to generate new, related instances. For\nexample, programs encoding the formal rules and properties of a system have\nbeen useful in fields ranging from RL (procedural environments) to physics\n(simulation engines). These programs can be seen as functions which execute to\ndifferent outputs based on their parameterizations (e.g., gridworld\nconfiguration or initial physical conditions). We introduce the term EFA\n(Executable Functional Abstraction) to denote such programs for math problems.\nEFA-like constructs have been shown to be useful for math reasoning as problem\ngenerators for stress-testing models. However, prior work has been limited to\nabstractions for grade-school math (whose simple rules are easy to encode in\nprograms), while generating EFAs for advanced math has thus far required human\nengineering. We explore the automatic construction of EFAs for advanced math\nproblems. We operationalize the task of automatically constructing EFAs as a\nprogram synthesis task, and develop EFAGen, which conditions an LLM on a seed\nmath problem and its step-by-step solution to generate candidate EFA programs\nthat are faithful to the generalized problem and solution class underlying the\nseed problem. Furthermore, we formalize properties any valid EFA must possess\nin terms of executable unit tests, and show how the tests can be used as\nverifiable rewards to train LLMs to become better writers of EFAs. We\ndemonstrate that EFAs constructed by EFAGen behave rationally by remaining\nfaithful to seed problems, produce learnable problem variations, and that\nEFAGen can infer EFAs across multiple diverse sources of competition-level math\nproblems. Finally, we show downstream uses of model-written EFAs e.g. finding\nproblem variations that are harder or easier for a learner to solve, as well as\ndata generation."}
{"id": "2504.09455", "pdf": "https://arxiv.org/pdf/2504.09455", "abs": "https://arxiv.org/abs/2504.09455", "authors": ["Hussain Md. Safwan", "Mahbub Islam Mahim", "Fawwaz Mohammed Amin"], "title": "Enhancing Wide-Angle Image Using Narrow-Angle View of the Same Scene", "categories": ["cs.CV", "eess.IV", "F.2.2; I.2.7"], "comment": null, "summary": "A common dilemma while photographing a scene is whether to capture it in\nwider angle, allowing more of the scene to be covered but in lesser details or\nto click in narrow angle that captures better details but leaves out portions\nof the scene. We propose a novel method in this paper that infuses wider shots\nwith finer quality details that is usually associated with an image captured by\nthe primary lens by capturing the same scene using both narrow and wide field\nof view (FoV) lenses. We do so by training a GAN-based model to learn to\nextract the visual quality parameters from a narrow angle shot and to transfer\nthese to the corresponding wide-angle image of the scene. We have mentioned in\ndetails the proposed technique to isolate the visual essence of an image and to\ntransfer it into another image. We have also elaborately discussed our\nimplementation details and have presented the results of evaluation over\nseveral benchmark datasets and comparisons with contemporary advancements in\nthe field."}
{"id": "2504.09781", "pdf": "https://arxiv.org/pdf/2504.09781", "abs": "https://arxiv.org/abs/2504.09781", "authors": ["Jingtian Wu", "Claire Cardie"], "title": "Reasoning Court: Combining Reasoning, Action, and Judgment for Multi-Hop Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While large language models (LLMs) have demonstrated strong capabilities in\ntasks like question answering and fact verification, they continue to suffer\nfrom hallucinations and reasoning errors, especially in multi-hop tasks that\nrequire integration of multiple information sources. Current methods address\nthese issues through retrieval-based techniques (grounding reasoning in\nexternal evidence), reasoning-based approaches (enhancing coherence via\nimproved prompting), or hybrid strategies combining both elements. One\nprominent hybrid method, ReAct, has outperformed purely retrieval-based or\nreasoning-based approaches; however, it lacks internal verification of\nintermediate reasoning steps, allowing potential errors to propagate through\ncomplex reasoning tasks. In this paper, we introduce Reasoning Court (RC), a\nnovel framework that extends iterative reasoning-and-retrieval methods, such as\nReAct, with a dedicated LLM judge. Unlike ReAct, RC employs this judge to\nindependently evaluate multiple candidate answers and their associated\nreasoning generated by separate LLM agents. The judge is asked to select the\nanswer that it considers the most factually grounded and logically coherent\nbased on the presented reasoning and evidence, or synthesizes a new answer\nusing available evidence and its pre-trained knowledge if all candidates are\ninadequate, flawed, or invalid. Evaluations on multi-hop benchmarks (HotpotQA,\nMuSiQue) and fact-verification (FEVER) demonstrate that RC consistently\noutperforms state-of-the-art few-shot prompting methods without task-specific\nfine-tuning."}
{"id": "2504.09472", "pdf": "https://arxiv.org/pdf/2504.09472", "abs": "https://arxiv.org/abs/2504.09472", "authors": ["Pooja Guhan", "Divya Kothandaraman", "Tsung-Wei Huang", "Guan-Ming Su", "Dinesh Manocha"], "title": "CamMimic: Zero-Shot Image To Camera Motion Personalized Video Generation Using Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "We introduce CamMimic, an innovative algorithm tailored for dynamic video\nediting needs. It is designed to seamlessly transfer the camera motion observed\nin a given reference video onto any scene of the user's choice in a zero-shot\nmanner without requiring any additional data. Our algorithm achieves this using\na two-phase strategy by leveraging a text-to-video diffusion model. In the\nfirst phase, we develop a multi-concept learning method using a combination of\nLoRA layers and an orthogonality loss to capture and understand the underlying\nspatial-temporal characteristics of the reference video as well as the spatial\nfeatures of the user's desired scene. The second phase proposes a unique\nhomography-based refinement strategy to enhance the temporal and spatial\nalignment of the generated video. We demonstrate the efficacy of our method\nthrough experiments conducted on a dataset containing combinations of diverse\nscenes and reference videos containing a variety of camera motions. In the\nabsence of an established metric for assessing camera motion transfer between\nunrelated scenes, we propose CameraScore, a novel metric that utilizes\nhomography representations to measure camera motion similarity between the\nreference and generated videos. Extensive quantitative and qualitative\nevaluations demonstrate that our approach generates high-quality,\nmotion-enhanced videos. Additionally, a user study reveals that 70.31% of\nparticipants preferred our method for scene preservation, while 90.45% favored\nit for motion transfer. We hope this work lays the foundation for future\nadvancements in camera motion transfer across different scenes."}
{"id": "2504.09795", "pdf": "https://arxiv.org/pdf/2504.09795", "abs": "https://arxiv.org/abs/2504.09795", "authors": ["Ryota Tanaka", "Taichi Iki", "Taku Hasegawa", "Kyosuke Nishida", "Kuniko Saito", "Jun Suzuki"], "title": "VDocRAG: Retrieval-Augmented Generation over Visually-Rich Documents", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR"], "comment": "Accepted by CVPR 2025; project page: https://vdocrag.github.io", "summary": "We aim to develop a retrieval-augmented generation (RAG) framework that\nanswers questions over a corpus of visually-rich documents presented in mixed\nmodalities (e.g., charts, tables) and diverse formats (e.g., PDF, PPTX). In\nthis paper, we introduce a new RAG framework, VDocRAG, which can directly\nunderstand varied documents and modalities in a unified image format to prevent\nmissing information that occurs by parsing documents to obtain text. To improve\nthe performance, we propose novel self-supervised pre-training tasks that adapt\nlarge vision-language models for retrieval by compressing visual information\ninto dense token representations while aligning them with textual content in\ndocuments. Furthermore, we introduce OpenDocVQA, the first unified collection\nof open-domain document visual question answering datasets, encompassing\ndiverse document types and formats. OpenDocVQA provides a comprehensive\nresource for training and evaluating retrieval and question answering models on\nvisually-rich documents in an open-domain setting. Experiments show that\nVDocRAG substantially outperforms conventional text-based RAG and has strong\ngeneralization capability, highlighting the potential of an effective RAG\nparadigm for real-world documents."}
{"id": "2504.09480", "pdf": "https://arxiv.org/pdf/2504.09480", "abs": "https://arxiv.org/abs/2504.09480", "authors": ["Yongchao Feng", "Yajie Liu", "Shuai Yang", "Wenrui Cai", "Jinqing Zhang", "Qiqi Zhan", "Ziyue Huang", "Hongxi Yan", "Qiao Wan", "Chenguang Liu", "Junzhe Wang", "Jiahui Lv", "Ziqi Liu", "Tengyuan Shi", "Qingjie Liu", "Yunhong Wang"], "title": "Vision-Language Model for Object Detection and Segmentation: A Review and Evaluation", "categories": ["cs.CV", "cs.AI"], "comment": "A Review and Evaluation about Vision-Language Model for Object\n  Detection and Segmentation", "summary": "Vision-Language Model (VLM) have gained widespread adoption in\nOpen-Vocabulary (OV) object detection and segmentation tasks. Despite they have\nshown promise on OV-related tasks, their effectiveness in conventional vision\ntasks has thus far been unevaluated. In this work, we present the systematic\nreview of VLM-based detection and segmentation, view VLM as the foundational\nmodel and conduct comprehensive evaluations across multiple downstream tasks\nfor the first time: 1) The evaluation spans eight detection scenarios\n(closed-set detection, domain adaptation, crowded objects, etc.) and eight\nsegmentation scenarios (few-shot, open-world, small object, etc.), revealing\ndistinct performance advantages and limitations of various VLM architectures\nacross tasks. 2) As for detection tasks, we evaluate VLMs under three\nfinetuning granularities: \\textit{zero prediction}, \\textit{visual\nfine-tuning}, and \\textit{text prompt}, and further analyze how different\nfinetuning strategies impact performance under varied task. 3) Based on\nempirical findings, we provide in-depth analysis of the correlations between\ntask characteristics, model architectures, and training methodologies, offering\ninsights for future VLM design. 4) We believe that this work shall be valuable\nto the pattern recognition experts working in the fields of computer vision,\nmultimodal learning, and vision foundation models by introducing them to the\nproblem, and familiarizing them with the current status of the progress while\nproviding promising directions for future research. A project associated with\nthis review and evaluation has been created at\nhttps://github.com/better-chao/perceptual_abilities_evaluation."}
{"id": "2504.09802", "pdf": "https://arxiv.org/pdf/2504.09802", "abs": "https://arxiv.org/abs/2504.09802", "authors": ["Wenrui Cai", "Chengyu Wang", "Junbing Yan", "Jun Huang", "Xiangzhong Fang"], "title": "Training Small Reasoning LLMs with Cognitive Preference Alignment", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The reasoning capabilities of large language models (LLMs), such as OpenAI's\no1 and DeepSeek-R1, have seen substantial advancements through deep thinking.\nHowever, these enhancements come with significant resource demands,\nunderscoring the need to explore strategies to train effective reasoning LLMs\nwith far fewer parameters. A critical challenge is that smaller models have\ndifferent capacities and cognitive trajectories than their larger counterparts.\nHence, direct distillation of chain-of-thought (CoT) results from large LLMs to\nsmaller ones can be sometimes ineffective and requires a huge amount of\nannotated data. In this paper, we introduce a novel framework called\nCritique-Rethink-Verify (CRV), designed for training smaller yet powerful\nreasoning LLMs. Our CRV framework consists of multiple LLM agents, each\nspecializing in unique abilities: (i) critiquing the CoTs according to the\ncognitive capabilities of smaller models, (ii) rethinking and refining these\nCoTs based on the critiques, and (iii) verifying the correctness of the refined\nresults. We further propose the cognitive preference optimization (CogPO)\nalgorithm to enhance the reasoning abilities of smaller models by aligning\nthoughts of these models with their cognitive capacities. Comprehensive\nevaluations on challenging reasoning benchmarks demonstrate the efficacy of CRV\nand CogPO, which outperforms other training methods by a large margin."}
{"id": "2504.09491", "pdf": "https://arxiv.org/pdf/2504.09491", "abs": "https://arxiv.org/abs/2504.09491", "authors": ["Yexing Xu", "Longguang Wang", "Minglin Chen", "Sheng Ao", "Li Li", "Yulan Guo"], "title": "DropoutGS: Dropping Out Gaussians for Better Sparse-view Rendering", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Although 3D Gaussian Splatting (3DGS) has demonstrated promising results in\nnovel view synthesis, its performance degrades dramatically with sparse inputs\nand generates undesirable artifacts. As the number of training views decreases,\nthe novel view synthesis task degrades to a highly under-determined problem\nsuch that existing methods suffer from the notorious overfitting issue.\nInterestingly, we observe that models with fewer Gaussian primitives exhibit\nless overfitting under sparse inputs. Inspired by this observation, we propose\na Random Dropout Regularization (RDR) to exploit the advantages of\nlow-complexity models to alleviate overfitting. In addition, to remedy the lack\nof high-frequency details for these models, an Edge-guided Splitting Strategy\n(ESS) is developed. With these two techniques, our method (termed DropoutGS)\nprovides a simple yet effective plug-in approach to improve the generalization\nperformance of existing 3DGS methods. Extensive experiments show that our\nDropoutGS produces state-of-the-art performance under sparse views on benchmark\ndatasets including Blender, LLFF, and DTU. The project page is at:\nhttps://xuyx55.github.io/DropoutGS/."}
{"id": "2504.09818", "pdf": "https://arxiv.org/pdf/2504.09818", "abs": "https://arxiv.org/abs/2504.09818", "authors": ["Rong Yao", "Hailin Hu", "Yifei Fu", "Hanting Chen", "Wenyi Fang", "Fanyi Du", "Kai Han", "Yunhe Wang"], "title": "Transferable text data distillation by trajectory matching", "categories": ["cs.CL"], "comment": null, "summary": "In the realm of large language model (LLM), as the size of large models\nincreases, it also brings higher training costs. There is a urgent need to\nminimize the data size in LLM training. Compared with data selection method,\nthe data distillation method aims to synthesize a small number of data samples\nto achieve the training effect of the full data set and has better flexibility.\nDespite its successes in computer vision, the discreteness of text data has\nhitherto stymied its exploration in natural language processing (NLP). In this\nwork, we proposed a method that involves learning pseudo prompt data based on\ntrajectory matching and finding its nearest neighbor ID to achieve\ncross-architecture transfer. During the distillation process, we introduce a\nregularization loss to improve the robustness of our distilled data. To our\nbest knowledge, this is the first data distillation work suitable for text\ngeneration tasks such as instruction tuning. Evaluations on two benchmarks,\nincluding ARC-Easy and MMLU instruction tuning datasets, established the\nsuperiority of our distillation approach over the SOTA data selection method\nLESS. Furthermore, our method demonstrates a good transferability over LLM\nstructures (i.e., OPT to Llama)."}
{"id": "2504.09498", "pdf": "https://arxiv.org/pdf/2504.09498", "abs": "https://arxiv.org/abs/2504.09498", "authors": ["Yue Yang", "Christoph Leuze", "Brian Hargreaves", "Bruce Daniel", "Fred Baik"], "title": "EasyREG: Easy Depth-Based Markerless Registration and Tracking using Augmented Reality Device for Surgical Guidance", "categories": ["cs.CV"], "comment": null, "summary": "The use of Augmented Reality (AR) devices for surgical guidance has gained\nincreasing traction in the medical field. Traditional registration methods\noften rely on external fiducial markers to achieve high accuracy and real-time\nperformance. However, these markers introduce cumbersome calibration procedures\nand can be challenging to deploy in clinical settings. While commercial\nsolutions have attempted real-time markerless tracking using the native RGB\ncameras of AR devices, their accuracy remains questionable for medical\nguidance, primarily due to occlusions and significant outliers between the live\nsensor data and the preoperative target anatomy point cloud derived from MRI or\nCT scans. In this work, we present a markerless framework that relies only on\nthe depth sensor of AR devices and consists of two modules: a registration\nmodule for high-precision, outlier-robust target anatomy localization, and a\ntracking module for real-time pose estimation. The registration module\nintegrates depth sensor error correction, a human-in-the-loop region filtering\ntechnique, and a robust global alignment with curvature-aware feature sampling,\nfollowed by local ICP refinement, for markerless alignment of preoperative\nmodels with patient anatomy. The tracking module employs a fast and robust\nregistration algorithm that uses the initial pose from the registration module\nto estimate the target pose in real-time. We comprehensively evaluated the\nperformance of both modules through simulation and real-world measurements. The\nresults indicate that our markerless system achieves superior performance for\nregistration and comparable performance for tracking to industrial solutions.\nThe two-module design makes our system a one-stop solution for surgical\nprocedures where the target anatomy moves or stays static during surgery."}
{"id": "2504.09824", "pdf": "https://arxiv.org/pdf/2504.09824", "abs": "https://arxiv.org/abs/2504.09824", "authors": ["Keyan Xu", "Dingzirui Wang", "Xuanliang Zhang", "Qingfu Zhu", "Wanxiang Che"], "title": "Abacus-SQL: A Text-to-SQL System Empowering Cross-Domain and Open-Domain Database Retrieval", "categories": ["cs.CL"], "comment": "11 pages, 3figures", "summary": "The existing text-to-SQL systems have made significant progress in SQL query\ngeneration, but they still face numerous challenges. Existing systems often\nlack retrieval capabilities for open-domain databases, requiring users to\nmanually filter relevant databases. Additionally, their cross-domain\ntransferability is limited, making it challenging to accommodate diverse query\nrequirements. To address these issues, we propose Abacus-SQL. Abacus-SQL\nutilizes database retrieval technology to accurately locate the required\ndatabases in an open-domain database environment. It also enhances the system\ncross-domain transfer ability through data augmentation methods. Moreover,\nAbacus-SQL employs Pre-SQL and Self-debug methods, thereby enhancing the\naccuracy of SQL queries. Experimental results demonstrate that Abacus-SQL\nperforms excellently in multi-turn text-to-SQL tasks, effectively validating\nthe approach's effectiveness. Abacus-SQL is publicly accessible at\nhttps://huozi.8wss.com/abacus-sql/."}
{"id": "2504.09502", "pdf": "https://arxiv.org/pdf/2504.09502", "abs": "https://arxiv.org/abs/2504.09502", "authors": ["Pengfei Wang", "Hao Zheng", "Zhigang Hu", "Aikun Xu", "Meiguang Zheng", "Liu Yang"], "title": "PCM-SAR: Physics-Driven Contrastive Mutual Learning for SAR Classification", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Existing SAR image classification methods based on Contrastive Learning often\nrely on sample generation strategies designed for optical images, failing to\ncapture the distinct semantic and physical characteristics of SAR data. To\naddress this, we propose Physics-Driven Contrastive Mutual Learning for SAR\nClassification (PCM-SAR), which incorporates domain-specific physical insights\nto improve sample generation and feature extraction. PCM-SAR utilizes the\ngray-level co-occurrence matrix (GLCM) to simulate realistic noise patterns and\napplies semantic detection for unsupervised local sampling, ensuring generated\nsamples accurately reflect SAR imaging properties. Additionally, a multi-level\nfeature fusion mechanism based on mutual learning enables collaborative\nrefinement of feature representations. Notably, PCM-SAR significantly enhances\nsmaller models by refining SAR feature representations, compensating for their\nlimited capacity. Experimental results show that PCM-SAR consistently\noutperforms SOTA methods across diverse datasets and SAR classification tasks."}
{"id": "2504.09866", "pdf": "https://arxiv.org/pdf/2504.09866", "abs": "https://arxiv.org/abs/2504.09866", "authors": ["Ziyu Zhuang"], "title": "PASS-FC: Progressive and Adaptive Search Scheme for Fact Checking of Comprehensive Claims", "categories": ["cs.CL"], "comment": null, "summary": "Automated fact-checking faces challenges in handling complex real-world\nclaims. We present PASS-FC, a novel framework that addresses these issues\nthrough claim augmentation, adaptive question generation, and iterative\nverification. PASS-FC enhances atomic claims with temporal and entity context,\nemploys advanced search techniques, and utilizes a reflection mechanism. We\nevaluate PASS-FC on six diverse datasets, demonstrating superior performance\nacross general knowledge, scientific, real-world, and multilingual\nfact-checking tasks. Our framework often surpasses stronger baseline models.\nHyperparameter analysis reveals optimal settings for evidence quantity and\nreflection label triggers, while ablation studies highlight the importance of\nclaim augmentation and language-specific adaptations. PASS-FC's performance\nunderscores its effectiveness in improving fact-checking accuracy and\nadaptability across various domains. We will open-source our code and\nexperimental results to facilitate further research in this area."}
{"id": "2504.09506", "pdf": "https://arxiv.org/pdf/2504.09506", "abs": "https://arxiv.org/abs/2504.09506", "authors": ["Yanze Jiang", "Yanfeng Gu", "Xian Li"], "title": "Pillar-Voxel Fusion Network for 3D Object Detection in Airborne Hyperspectral Point Clouds", "categories": ["cs.CV"], "comment": null, "summary": "Hyperspectral point clouds (HPCs) can simultaneously characterize 3D spatial\nand spectral information of ground objects, offering excellent 3D perception\nand target recognition capabilities. Current approaches for generating HPCs\noften involve fusion techniques with hyperspectral images and LiDAR point\nclouds, which inevitably lead to geometric-spectral distortions due to fusion\nerrors and obstacle occlusions. These adverse effects limit their performance\nin downstream fine-grained tasks across multiple scenarios, particularly in\nairborne applications. To address these issues, we propose PiV-AHPC, a 3D\nobject detection network for airborne HPCs. To the best of our knowledge, this\nis the first attempt at this HPCs task. Specifically, we first develop a\npillar-voxel dual-branch encoder, where the former captures spectral and\nvertical structural features from HPCs to overcome spectral distortion, while\nthe latter emphasizes extracting accurate 3D spatial features from point\nclouds. A multi-level feature fusion mechanism is devised to enhance\ninformation interaction between the two branches, achieving neighborhood\nfeature alignment and channel-adaptive selection, thereby organically\nintegrating heterogeneous features and mitigating geometric distortion.\nExtensive experiments on two airborne HPCs datasets demonstrate that PiV-AHPC\npossesses state-of-the-art detection performance and high generalization\ncapability."}
{"id": "2504.09886", "pdf": "https://arxiv.org/pdf/2504.09886", "abs": "https://arxiv.org/abs/2504.09886", "authors": ["Michael Kamerath", "Aniello De Santo"], "title": "Investigating Syntactic Biases in Multilingual Transformers with RC Attachment Ambiguities in Italian and English", "categories": ["cs.CL"], "comment": null, "summary": "This paper leverages past sentence processing studies to investigate whether\nmonolingual and multilingual LLMs show human-like preferences when presented\nwith examples of relative clause attachment ambiguities in Italian and English.\nFurthermore, we test whether these preferences can be modulated by lexical\nfactors (the type of verb/noun in the matrix clause) which have been shown to\nbe tied to subtle constraints on syntactic and semantic relations. Our results\noverall showcase how LLM behavior varies interestingly across models, but also\ngeneral failings of these models in correctly capturing human-like preferences.\nIn light of these results, we argue that RC attachment is the ideal benchmark\nfor cross-linguistic investigations of LLMs' linguistic knowledge and biases."}
{"id": "2504.09507", "pdf": "https://arxiv.org/pdf/2504.09507", "abs": "https://arxiv.org/abs/2504.09507", "authors": ["Mengjiao Wang", "Junpei Zhang", "Xu Liu", "Yuting Yang", "Mengru Ma"], "title": "FVOS for MOSE Track of 4th PVUW Challenge: 3rd Place Solution", "categories": ["cs.CV"], "comment": "5 pages, 3 figures", "summary": "Video Object Segmentation (VOS) is one of the most fundamental and\nchallenging tasks in computer vision and has a wide range of applications. Most\nexisting methods rely on spatiotemporal memory networks to extract frame-level\nfeatures and have achieved promising results on commonly used datasets.\nHowever, these methods often struggle in more complex real-world scenarios.\nThis paper addresses this issue, aiming to achieve accurate segmentation of\nvideo objects in challenging scenes. We propose fine-tuning VOS (FVOS),\noptimizing existing methods for specific datasets through tailored training.\nAdditionally, we introduce a morphological post-processing strategy to address\nthe issue of excessively large gaps between adjacent objects in single-model\npredictions. Finally, we apply a voting-based fusion method on multi-scale\nsegmentation results to generate the final output. Our approach achieves J&F\nscores of 76.81% and 83.92% during the validation and testing stages,\nrespectively, securing third place overall in the MOSE Track of the 4th PVUW\nchallenge 2025."}
{"id": "2504.09895", "pdf": "https://arxiv.org/pdf/2504.09895", "abs": "https://arxiv.org/abs/2504.09895", "authors": ["Shuai Zhao", "Linchao Zhu", "Yi Yang"], "title": "Learning from Reference Answers: Versatile Language Model Alignment without Binary Human Preference Data", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "work in progress", "summary": "Large language models~(LLMs) are expected to be helpful, harmless, and\nhonest. In various alignment scenarios, such as general human preference,\nsafety, and confidence alignment, binary preference data collection and reward\nmodeling are resource-intensive but necessary for human preference\ntransferring. In this work, we explore using the similarity between sampled\ngenerations and high-quality reference answers as an alternative reward\nfunction for LLM alignment. Using similarity as a reward circumvents training\nreward models, and collecting a single reference answer potentially costs less\ntime than constructing binary preference pairs when multiple candidates are\navailable. Specifically, we develop \\textit{RefAlign}, a versatile\nREINFORCE-style alignment algorithm, which is free of reference and reward\nmodels. Instead, RefAlign utilizes BERTScore between sampled generations and\nhigh-quality reference answers as the surrogate reward. Beyond general human\npreference optimization, RefAlign can be readily extended to diverse scenarios,\nsuch as safety and confidence alignment, by incorporating the similarity reward\nwith task-related objectives. In various scenarios, {RefAlign} demonstrates\ncomparable performance to previous alignment methods while offering high\nefficiency."}
{"id": "2504.09513", "pdf": "https://arxiv.org/pdf/2504.09513", "abs": "https://arxiv.org/abs/2504.09513", "authors": ["Puyu Han", "Jiaju Kang", "Yuhang Pan", "Erting Pan", "Zeyu Zhang", "Qunchao Jin", "Juntao Jiang", "Zhichen Liu", "Luqi Gong"], "title": "DiffuMural: Restoring Dunhuang Murals with Multi-scale Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "Large-scale pre-trained diffusion models have produced excellent results in\nthe field of conditional image generation. However, restoration of ancient\nmurals, as an important downstream task in this field, poses significant\nchallenges to diffusion model-based restoration methods due to its large\ndefective area and scarce training samples. Conditional restoration tasks are\nmore concerned with whether the restored part meets the aesthetic standards of\nmural restoration in terms of overall style and seam detail, and such metrics\nfor evaluating heuristic image complements are lacking in current research. We\ntherefore propose DiffuMural, a combined Multi-scale convergence and\nCollaborative Diffusion mechanism with ControlNet and cyclic consistency loss\nto optimise the matching between the generated images and the conditional\ncontrol. DiffuMural demonstrates outstanding capabilities in mural restoration,\nleveraging training data from 23 large-scale Dunhuang murals that exhibit\nconsistent visual aesthetics. The model excels in restoring intricate details,\nachieving a coherent overall appearance, and addressing the unique challenges\nposed by incomplete murals lacking factual grounding. Our evaluation framework\nincorporates four key metrics to quantitatively assess incomplete murals:\nfactual accuracy, textural detail, contextual semantics, and holistic visual\ncoherence. Furthermore, we integrate humanistic value assessments to ensure the\nrestored murals retain their cultural and artistic significance. Extensive\nexperiments validate that our method outperforms state-of-the-art (SOTA)\napproaches in both qualitative and quantitative metrics."}
{"id": "2504.09896", "pdf": "https://arxiv.org/pdf/2504.09896", "abs": "https://arxiv.org/abs/2504.09896", "authors": ["Aish Albladi", "Md Kaosar Uddin", "Minarul Islam", "Cheryl Seals"], "title": "TWSSenti: A Novel Hybrid Framework for Topic-Wise Sentiment Analysis on Social Media Using Transformer Models", "categories": ["cs.CL"], "comment": "41 pages, 12 figures, includes algorithm and comparative tables", "summary": "Sentiment analysis is a crucial task in natural language processing (NLP)\nthat enables the extraction of meaningful insights from textual data,\nparticularly from dynamic platforms like Twitter and IMDB. This study explores\na hybrid framework combining transformer-based models, specifically BERT,\nGPT-2, RoBERTa, XLNet, and DistilBERT, to improve sentiment classification\naccuracy and robustness. The framework addresses challenges such as noisy data,\ncontextual ambiguity, and generalization across diverse datasets by leveraging\nthe unique strengths of these models. BERT captures bidirectional context,\nGPT-2 enhances generative capabilities, RoBERTa optimizes contextual\nunderstanding with larger corpora and dynamic masking, XLNet models dependency\nthrough permutation-based learning, and DistilBERT offers efficiency with\nreduced computational overhead while maintaining high accuracy. We demonstrate\ntext cleaning, tokenization, and feature extraction using Term Frequency\nInverse Document Frequency (TF-IDF) and Bag of Words (BoW), ensure high-quality\ninput data for the models. The hybrid approach was evaluated on benchmark\ndatasets Sentiment140 and IMDB, achieving superior accuracy rates of 94\\% and\n95\\%, respectively, outperforming standalone models. The results validate the\neffectiveness of combining multiple transformer models in ensemble-like setups\nto address the limitations of individual architectures. This research\nhighlights its applicability to real-world tasks such as social media\nmonitoring, customer sentiment analysis, and public opinion tracking which\noffers a pathway for future advancements in hybrid NLP frameworks."}
{"id": "2504.09514", "pdf": "https://arxiv.org/pdf/2504.09514", "abs": "https://arxiv.org/abs/2504.09514", "authors": ["Aisha L. Shuaibu", "Kieran A. Gibb", "Peter A. Wijeratne", "Ivor J. A. Simpson"], "title": "Capturing Longitudinal Changes in Brain Morphology Using Temporally Parameterized Neural Displacement Fields", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted for publication at Medical Imaging with Deep Learning (MIDL)", "summary": "Longitudinal image registration enables studying temporal changes in brain\nmorphology which is useful in applications where monitoring the growth or\natrophy of specific structures is important. However this task is challenging\ndue to; noise/artifacts in the data and quantifying small anatomical changes\nbetween sequential scans. We propose a novel longitudinal registration method\nthat models structural changes using temporally parameterized neural\ndisplacement fields. Specifically, we implement an implicit neural\nrepresentation (INR) using a multi-layer perceptron that serves as a continuous\ncoordinate-based approximation of the deformation field at any time point. In\neffect, for any N scans of a particular subject, our model takes as input a 3D\nspatial coordinate location x, y, z and a corresponding temporal representation\nt and learns to describe the continuous morphology of structures for both\nobserved and unobserved points in time. Furthermore, we leverage the analytic\nderivatives of the INR to derive a new regularization function that enforces\nmonotonic rate of change in the trajectory of the voxels, which is shown to\nprovide more biologically plausible patterns. We demonstrate the effectiveness\nof our method on 4D brain MR registration."}
{"id": "2504.09903", "pdf": "https://arxiv.org/pdf/2504.09903", "abs": "https://arxiv.org/abs/2504.09903", "authors": ["Bo-Wei Chen", "An-Zi Yen", "Chung-Chi Chen"], "title": "Refining Financial Consumer Complaints through Multi-Scale Model Interaction", "categories": ["cs.CL"], "comment": null, "summary": "Legal writing demands clarity, formality, and domain-specific\nprecision-qualities often lacking in documents authored by individuals without\nlegal training. To bridge this gap, this paper explores the task of legal text\nrefinement that transforms informal, conversational inputs into persuasive\nlegal arguments. We introduce FinDR, a Chinese dataset of financial dispute\nrecords, annotated with official judgments on claim reasonableness. Our\nproposed method, Multi-Scale Model Interaction (MSMI), leverages a lightweight\nclassifier to evaluate outputs and guide iterative refinement by Large Language\nModels (LLMs). Experimental results demonstrate that MSMI significantly\noutperforms single-pass prompting strategies. Additionally, we validate the\ngeneralizability of MSMI on several short-text benchmarks, showing improved\nadversarial robustness. Our findings reveal the potential of multi-model\ncollaboration for enhancing legal document generation and broader text\nrefinement tasks."}
{"id": "2504.09518", "pdf": "https://arxiv.org/pdf/2504.09518", "abs": "https://arxiv.org/abs/2504.09518", "authors": ["Ting Huang", "Zeyu Zhang", "Yemin Wang", "Hao Tang"], "title": "3D CoCa: Contrastive Learners are 3D Captioners", "categories": ["cs.CV"], "comment": null, "summary": "3D captioning, which aims to describe the content of 3D scenes in natural\nlanguage, remains highly challenging due to the inherent sparsity of point\nclouds and weak cross-modal alignment in existing methods. To address these\nchallenges, we propose 3D CoCa, a novel unified framework that seamlessly\ncombines contrastive vision-language learning with 3D caption generation in a\nsingle architecture. Our approach leverages a frozen CLIP vision-language\nbackbone to provide rich semantic priors, a spatially-aware 3D scene encoder to\ncapture geometric context, and a multi-modal decoder to generate descriptive\ncaptions. Unlike prior two-stage methods that rely on explicit object\nproposals, 3D CoCa jointly optimizes contrastive and captioning objectives in a\nshared feature space, eliminating the need for external detectors or\nhandcrafted proposals. This joint training paradigm yields stronger spatial\nreasoning and richer semantic grounding by aligning 3D and textual\nrepresentations. Extensive experiments on the ScanRefer and Nr3D benchmarks\ndemonstrate that 3D CoCa significantly outperforms current state-of-the-arts by\n10.2% and 5.76% in CIDEr at 0.5IoU, respectively. Code will be available at\nhttps://github.com/AIGeeksGroup/3DCoCa."}
{"id": "2504.09909", "pdf": "https://arxiv.org/pdf/2504.09909", "abs": "https://arxiv.org/abs/2504.09909", "authors": ["Farha Nausheen", "Khandakar Ahmed", "M Imad Khan"], "title": "Quantum Natural Language Processing: A Comprehensive Review of Models, Methods, and Applications", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In recent developments, deep learning methodologies applied to Natural\nLanguage Processing (NLP) have revealed a paradox: They improve performance but\ndemand considerable data and resources for their training. Alternatively,\nquantum computing exploits the principles of quantum mechanics to overcome the\ncomputational limitations of current methodologies, thereby establishing an\nemerging field known as quantum natural language processing (QNLP). This domain\nholds the potential to attain a quantum advantage in the processing of\nlinguistic structures, surpassing classical models in both efficiency and\naccuracy. In this paper, it is proposed to categorise QNLP models based on\nquantum computing principles, architecture, and computational approaches. This\npaper attempts to provide a survey on how quantum meets language by mapping\nstate-of-the-art in this area, embracing quantum encoding techniques for\nclassical data, QNLP models for prevalent NLP tasks, and quantum optimisation\ntechniques for hyper parameter tuning. The landscape of quantum computing\napproaches applied to various NLP tasks is summarised by showcasing the\nspecific QNLP methods used, and the popularity of these methods is indicated by\ntheir count. From the findings, it is observed that QNLP approaches are still\nlimited to small data sets, with only a few models explored extensively, and\nthere is increasing interest in the application of quantum computing to natural\nlanguage processing tasks."}
{"id": "2504.09528", "pdf": "https://arxiv.org/pdf/2504.09528", "abs": "https://arxiv.org/abs/2504.09528", "authors": ["Xing Zi", "Tengjun Ni", "Xianjing Fan", "Xian Tao", "Jun Li", "Ali Braytee", "Mukesh Prasad"], "title": "AeroLite: Tag-Guided Lightweight Generation of Aerial Image Captions", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Accurate and automated captioning of aerial imagery is crucial for\napplications like environmental monitoring, urban planning, and disaster\nmanagement. However, this task remains challenging due to complex spatial\nsemantics and domain variability. To address these issues, we introduce\n\\textbf{AeroLite}, a lightweight, tag-guided captioning framework designed to\nequip small-scale language models (1--3B parameters) with robust and\ninterpretable captioning capabilities specifically for remote sensing images.\n\\textbf{AeroLite} leverages GPT-4o to generate a large-scale, semantically rich\npseudo-caption dataset by integrating multiple remote sensing benchmarks,\nincluding DLRSD, iSAID, LoveDA, WHU, and RSSCN7. To explicitly capture key\nsemantic elements such as orientation and land-use types, AeroLite employs\nnatural language processing techniques to extract relevant semantic tags. These\ntags are then learned by a dedicated multi-label CLIP encoder, ensuring precise\nsemantic predictions. To effectively fuse visual and semantic information, we\npropose a novel bridging multilayer perceptron (MLP) architecture, aligning\nsemantic tags with visual embeddings while maintaining minimal computational\noverhead. AeroLite's flexible design also enables seamless integration with\nvarious pretrained large language models. We adopt a two-stage LoRA-based\ntraining approach: the initial stage leverages our pseudo-caption dataset to\ncapture broad remote sensing semantics, followed by fine-tuning on smaller,\ncurated datasets like UCM and Sydney Captions to refine domain-specific\nalignment. Experimental evaluations demonstrate that AeroLite surpasses\nsignificantly larger models (e.g., 13B parameters) in standard captioning\nmetrics, including BLEU and METEOR, while maintaining substantially lower\ncomputational costs."}
{"id": "2504.09910", "pdf": "https://arxiv.org/pdf/2504.09910", "abs": "https://arxiv.org/abs/2504.09910", "authors": ["Yujing Wang", "Hainan Zhang", "Liang Pang", "Yongxin Tong", "Binghui Guo", "Hongwei Zheng", "Zhiming Zheng"], "title": "Learning to Erase Private Knowledge from Multi-Documents for Retrieval-Augmented Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) is a promising technique for applying\nLLMs to proprietary domains. However, retrieved documents may contain sensitive\nknowledge, posing risks of privacy leakage in generative results. Thus,\neffectively erasing private information from retrieved documents is a key\nchallenge for RAG. Unlike traditional text anonymization, RAG should consider:\n(1) the inherent multi-document reasoning may face de-anonymization attacks;\n(2) private knowledge varies by scenarios, so users should be allowed to\ncustomize which information to erase; (3) preserving sufficient publicly\navailable knowledge for generation tasks. This paper introduces the privacy\nerasure task for RAG and proposes Eraser4RAG, a private knowledge eraser which\neffectively removes user-defined private knowledge from documents while\npreserving sufficient public knowledge for generation. Specifically, we first\nconstruct a global knowledge graph to identify potential knowledge across\ndocuments, aiming to defend against de-anonymization attacks. Then we randomly\nsplit it into private and public sub-graphs, and fine-tune Flan-T5 to rewrite\nthe retrieved documents excluding private triples. Finally, PPO algorithm\noptimizes the rewriting model to minimize private triples and maximize public\ntriples retention. Experiments on four QA datasets demonstrate that Eraser4RAG\nachieves superior erase performance than GPT-4o."}
{"id": "2504.09530", "pdf": "https://arxiv.org/pdf/2504.09530", "abs": "https://arxiv.org/abs/2504.09530", "authors": ["Shuchao Duan", "Amirhossein Dadashzadeh", "Alan Whone", "Majid Mirmehdi"], "title": "Trajectory-guided Motion Perception for Facial Expression Quality Assessment in Neurological Disorders", "categories": ["cs.CV"], "comment": "Accepted to IEEE FG 2025 (preprint)", "summary": "Automated facial expression quality assessment (FEQA) in neurological\ndisorders is critical for enhancing diagnostic accuracy and improving patient\ncare, yet effectively capturing the subtle motions and nuances of facial muscle\nmovements remains a challenge. We propose to analyse facial landmark\ntrajectories, a compact yet informative representation, that encodes these\nsubtle motions from a high-level structural perspective. Hence, we introduce\nTrajectory-guided Motion Perception Transformer (TraMP-Former), a novel FEQA\nframework that fuses landmark trajectory features for fine-grained motion\ncapture with visual semantic cues from RGB frames, ultimately regressing the\ncombined features into a quality score. Extensive experiments demonstrate that\nTraMP-Former achieves new state-of-the-art performance on benchmark datasets\nwith neurological disorders, including PFED5 (up by 6.51%) and an augmented\nToronto NeuroFace (up by 7.62%). Our ablation studies further validate the\nefficiency and effectiveness of landmark trajectories in FEQA. Our code is\navailable at https://github.com/shuchaoduan/TraMP-Former."}
{"id": "2504.09923", "pdf": "https://arxiv.org/pdf/2504.09923", "abs": "https://arxiv.org/abs/2504.09923", "authors": ["Yujin Kim", "Euiin Yi", "Minu Kim", "Se-Young Yun", "Taehyeon Kim"], "title": "Guiding Reasoning in Small Language Models with LLM Assistance", "categories": ["cs.CL"], "comment": "20 pages, 10 figures, 11 tables", "summary": "The limited reasoning capabilities of small language models (SLMs) cast doubt\non their suitability for tasks demanding deep, multi-step logical deduction.\nThis paper introduces a framework called Small Reasons, Large Hints (SMART),\nwhich selectively augments SLM reasoning with targeted guidance from large\nlanguage models (LLMs). Inspired by the concept of cognitive scaffolding, SMART\nemploys a score-based evaluation to identify uncertain reasoning steps and\ninjects corrective LLM-generated reasoning only when necessary. By framing\nstructured reasoning as an optimal policy search, our approach steers the\nreasoning trajectory toward correct solutions without exhaustive sampling. Our\nexperiments on mathematical reasoning datasets demonstrate that targeted\nexternal scaffolding significantly improves performance, paving the way for\ncollaborative use of both SLM and LLM to tackle complex reasoning tasks that\nare currently unsolvable by SLMs alone."}
{"id": "2504.09535", "pdf": "https://arxiv.org/pdf/2504.09535", "abs": "https://arxiv.org/abs/2504.09535", "authors": ["Yuting Zhao", "Yuheng Ji", "Xiaoshuai Hao", "Shuxiao Li"], "title": "FastRSR: Efficient and Accurate Road Surface Reconstruction from Bird's Eye View", "categories": ["cs.CV"], "comment": null, "summary": "Road Surface Reconstruction (RSR) is crucial for autonomous driving, enabling\nthe understanding of road surface conditions. Recently, RSR from the Bird's Eye\nView (BEV) has gained attention for its potential to enhance performance.\nHowever, existing methods for transforming perspective views to BEV face\nchallenges such as information loss and representation sparsity. Moreover,\nstereo matching in BEV is limited by the need to balance accuracy with\ninference speed. To address these challenges, we propose two efficient and\naccurate BEV-based RSR models: FastRSR-mono and FastRSR-stereo. Specifically,\nwe first introduce Depth-Aware Projection (DAP), an efficient view\ntransformation strategy designed to mitigate information loss and sparsity by\nquerying depth and image features to aggregate BEV data within specific road\nsurface regions using a pre-computed look-up table. To optimize accuracy and\nspeed in stereo matching, we design the Spatial Attention Enhancement (SAE) and\nConfidence Attention Generation (CAG) modules. SAE adaptively highlights\nimportant regions, while CAG focuses on high-confidence predictions and filters\nout irrelevant information. FastRSR achieves state-of-the-art performance,\nexceeding monocular competitors by over 6.0% in elevation absolute error and\nproviding at least a 3.0x speedup by stereo methods on the RSRD dataset. The\nsource code will be released."}
{"id": "2504.09958", "pdf": "https://arxiv.org/pdf/2504.09958", "abs": "https://arxiv.org/abs/2504.09958", "authors": ["Fuqiang Niu", "Yi Yang", "Xianghua Fu", "Genan Dai", "Bowen Zhang"], "title": "C-MTCSD: A Chinese Multi-Turn Conversational Stance Detection Dataset", "categories": ["cs.CL"], "comment": "WWW2025", "summary": "Stance detection has become an essential tool for analyzing public\ndiscussions on social media. Current methods face significant challenges,\nparticularly in Chinese language processing and multi-turn conversational\nanalysis. To address these limitations, we introduce C-MTCSD, the largest\nChinese multi-turn conversational stance detection dataset, comprising 24,264\ncarefully annotated instances from Sina Weibo, which is 4.2 times larger than\nthe only prior Chinese conversational stance detection dataset. Our\ncomprehensive evaluation using both traditional approaches and large language\nmodels reveals the complexity of C-MTCSD: even state-of-the-art models achieve\nonly 64.07% F1 score in the challenging zero-shot setting, while performance\nconsistently degrades with increasing conversation depth. Traditional models\nparticularly struggle with implicit stance detection, achieving below 50% F1\nscore. This work establishes a challenging new benchmark for Chinese stance\ndetection research, highlighting significant opportunities for future\nimprovements."}
{"id": "2504.09540", "pdf": "https://arxiv.org/pdf/2504.09540", "abs": "https://arxiv.org/abs/2504.09540", "authors": ["Hao Wang", "Xiaobao Wei", "Xiaoan Zhang", "Jianing Li", "Chengyu Bai", "Ying Li", "Ming Lu", "Wenzhao Zheng", "Shanghang Zhang"], "title": "EmbodiedOcc++: Boosting Embodied 3D Occupancy Prediction with Plane Regularization and Uncertainty Sampler", "categories": ["cs.CV"], "comment": null, "summary": "Online 3D occupancy prediction provides a comprehensive spatial understanding\nof embodied environments. While the innovative EmbodiedOcc framework utilizes\n3D semantic Gaussians for progressive indoor occupancy prediction, it overlooks\nthe geometric characteristics of indoor environments, which are primarily\ncharacterized by planar structures. This paper introduces EmbodiedOcc++,\nenhancing the original framework with two key innovations: a Geometry-guided\nRefinement Module (GRM) that constrains Gaussian updates through plane\nregularization, along with a Semantic-aware Uncertainty Sampler (SUS) that\nenables more effective updates in overlapping regions between consecutive\nframes. GRM regularizes the position update to align with surface normals. It\ndetermines the adaptive regularization weight using curvature-based and\ndepth-based constraints, allowing semantic Gaussians to align accurately with\nplanar surfaces while adapting in complex regions. To effectively improve\ngeometric consistency from different views, SUS adaptively selects proper\nGaussians to update. Comprehensive experiments on the EmbodiedOcc-ScanNet\nbenchmark demonstrate that EmbodiedOcc++ achieves state-of-the-art performance\nacross different settings. Our method demonstrates improved edge accuracy and\nretains more geometric details while ensuring computational efficiency, which\nis essential for online embodied perception. The code will be released at:\nhttps://github.com/PKUHaoWang/EmbodiedOcc2."}
{"id": "2504.09980", "pdf": "https://arxiv.org/pdf/2504.09980", "abs": "https://arxiv.org/abs/2504.09980", "authors": ["Anneliese Kelterer", "Barbara Schuppler"], "title": "Turn-taking annotation for quantitative and qualitative analyses of conversation", "categories": ["cs.CL", "cs.DB", "cs.HC", "eess.AS"], "comment": "41 pages", "summary": "This paper has two goals. First, we present the turn-taking annotation layers\ncreated for 95 minutes of conversational speech of the Graz Corpus of Read and\nSpontaneous Speech (GRASS), available to the scientific community. Second, we\ndescribe the annotation system and the annotation process in more detail, so\nother researchers may use it for their own conversational data. The annotation\nsystem was developed with an interdisciplinary application in mind. It should\nbe based on sequential criteria according to Conversation Analysis, suitable\nfor subsequent phonetic analysis, thus time-aligned annotations were made\nPraat, and it should be suitable for automatic classification, which required\nthe continuous annotation of speech and a label inventory that is not too large\nand results in a high inter-rater agreement. Turn-taking was annotated on two\nlayers, Inter-Pausal Units (IPU) and points of potential completion (PCOMP;\nsimilar to transition relevance places). We provide a detailed description of\nthe annotation process and of segmentation and labelling criteria. A detailed\nanalysis of inter-rater agreement and common confusions shows that agreement\nfor IPU annotation is near-perfect, that agreement for PCOMP annotations is\nsubstantial, and that disagreements often are either partial or can be\nexplained by a different analysis of a sequence which also has merit. The\nannotation system can be applied to a variety of conversational data for\nlinguistic studies and technological applications, and we hope that the\nannotations, as well as the annotation system will contribute to a stronger\ncross-fertilization between these disciplines."}
{"id": "2504.09549", "pdf": "https://arxiv.org/pdf/2504.09549", "abs": "https://arxiv.org/abs/2504.09549", "authors": ["Xiang Hu", "Pingping Zhang", "Yuhao Wang", "Bin Yan", "Huchuan Lu"], "title": "SD-ReID: View-aware Stable Diffusion for Aerial-Ground Person Re-Identification", "categories": ["cs.CV"], "comment": null, "summary": "Aerial-Ground Person Re-IDentification (AG-ReID) aims to retrieve specific\npersons across cameras with different viewpoints. Previous works focus on\ndesigning discriminative ReID models to maintain identity consistency despite\ndrastic changes in camera viewpoints. The core idea behind these methods is\nquite natural, but designing a view-robust network is a very challenging task.\nMoreover, they overlook the contribution of view-specific features in enhancing\nthe model's capability to represent persons. To address these issues, we\npropose a novel two-stage feature learning framework named SD-ReID for AG-ReID,\nwhich takes advantage of the powerful understanding capacity of generative\nmodels, e.g., Stable Diffusion (SD), to generate view-specific features between\ndifferent viewpoints. In the first stage, we train a simple ViT-based model to\nextract coarse-grained representations and controllable conditions. Then, in\nthe second stage, we fine-tune the SD model to learn complementary\nrepresentations guided by the controllable conditions. Furthermore, we propose\nthe View-Refine Decoder (VRD) to obtain additional controllable conditions to\ngenerate missing cross-view features. Finally, we use the coarse-grained\nrepresentations and all-view features generated by SD to retrieve target\npersons. Extensive experiments on the AG-ReID benchmarks demonstrate the\neffectiveness of our proposed SD-ReID. The source code will be available upon\nacceptance."}
{"id": "2504.10020", "pdf": "https://arxiv.org/pdf/2504.10020", "abs": "https://arxiv.org/abs/2504.10020", "authors": ["Hao Yin", "Gunagzong Si", "Zilei Wang"], "title": "The Mirage of Performance Gains: Why Contrastive Decoding Fails to Address Multimodal Hallucination", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Contrastive decoding strategies are widely used to reduce hallucinations in\nmultimodal large language models (MLLMs). These methods work by constructing\ncontrastive samples to induce hallucinations and then suppressing them in the\noutput distribution. However, this paper demonstrates that such approaches fail\nto effectively mitigate the hallucination problem. The performance improvements\nobserved on POPE Benchmark are largely driven by two misleading factors: (1)\ncrude, unidirectional adjustments to the model's output distribution and (2)\nthe adaptive plausibility constraint, which reduces the sampling strategy to\ngreedy search. To further illustrate these issues, we introduce a series of\nspurious improvement methods and evaluate their performance against contrastive\ndecoding techniques. Experimental results reveal that the observed performance\ngains in contrastive decoding are entirely unrelated to its intended goal of\nmitigating hallucinations. Our findings challenge common assumptions about the\neffectiveness of contrastive decoding strategies and pave the way for\ndeveloping genuinely effective solutions to hallucinations in MLLMs."}
{"id": "2504.09555", "pdf": "https://arxiv.org/pdf/2504.09555", "abs": "https://arxiv.org/abs/2504.09555", "authors": ["Jinhao Li", "Zijian Chen", "Runze Dong", "Tingzhu Chen", "Changbo Wang", "Guangtao Zhai"], "title": "Mitigating Long-tail Distribution in Oracle Bone Inscriptions: Dataset, Model, and Benchmark", "categories": ["cs.CV"], "comment": null, "summary": "The oracle bone inscription (OBI) recognition plays a significant role in\nunderstanding the history and culture of ancient China. However, the existing\nOBI datasets suffer from a long-tail distribution problem, leading to biased\nperformance of OBI recognition models across majority and minority classes.\nWith recent advancements in generative models, OBI synthesis-based data\naugmentation has become a promising avenue to expand the sample size of\nminority classes. Unfortunately, current OBI datasets lack large-scale\nstructure-aligned image pairs for generative model training. To address these\nproblems, we first present the Oracle-P15K, a structure-aligned OBI dataset for\nOBI generation and denoising, consisting of 14,542 images infused with domain\nknowledge from OBI experts. Second, we propose a diffusion model-based pseudo\nOBI generator, called OBIDiff, to achieve realistic and controllable OBI\ngeneration. Given a clean glyph image and a target rubbing-style image, it can\neffectively transfer the noise style of the original rubbing to the glyph\nimage. Extensive experiments on OBI downstream tasks and user preference\nstudies show the effectiveness of the proposed Oracle-P15K dataset and\ndemonstrate that OBIDiff can accurately preserve inherent glyph structures\nwhile transferring authentic rubbing styles effectively."}
{"id": "2504.10036", "pdf": "https://arxiv.org/pdf/2504.10036", "abs": "https://arxiv.org/abs/2504.10036", "authors": ["Zhengxuan Zhang", "Zhuowen Liang", "Yin Wu", "Teng Lin", "Yuyu Luo", "Nan Tang"], "title": "DataMosaic: Explainable and Verifiable Multi-Modal Data Analytics through Extract-Reason-Verify", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are transforming data analytics, but their\nwidespread adoption is hindered by two critical limitations: they are not\nexplainable (opaque reasoning processes) and not verifiable (prone to\nhallucinations and unchecked errors). While retrieval-augmented generation\n(RAG) improves accuracy by grounding LLMs in external data, it fails to address\nthe core challenges of trustworthy analytics - especially when processing\nnoisy, inconsistent, or multi-modal data (for example, text, tables, images).\nWe propose DataMosaic, a framework designed to make LLM-powered analytics both\nexplainable and verifiable. By dynamically extracting task-specific structures\n(for example, tables, graphs, trees) from raw data, DataMosaic provides\ntransparent, step-by-step reasoning traces and enables validation of\nintermediate results. Built on a multi-agent framework, DataMosaic orchestrates\nself-adaptive agents that align with downstream task requirements, enhancing\nconsistency, completeness, and privacy. Through this approach, DataMosaic not\nonly tackles the limitations of current LLM-powered analytics systems but also\nlays the groundwork for a new paradigm of grounded, accurate, and explainable\nmulti-modal data analytics."}
{"id": "2504.09588", "pdf": "https://arxiv.org/pdf/2504.09588", "abs": "https://arxiv.org/abs/2504.09588", "authors": ["Zhicong Wu", "Hongbin Xu", "Gang Xu", "Ping Nie", "Zhixin Yan", "Jinkai Zheng", "Liangqiong Qu", "Ming Li", "Liqiang Nie"], "title": "TextSplat: Text-Guided Semantic Fusion for Generalizable Gaussian Splatting", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in Generalizable Gaussian Splatting have enabled robust\n3D reconstruction from sparse input views by utilizing feed-forward Gaussian\nSplatting models, achieving superior cross-scene generalization. However, while\nmany methods focus on geometric consistency, they often neglect the potential\nof text-driven guidance to enhance semantic understanding, which is crucial for\naccurately reconstructing fine-grained details in complex scenes. To address\nthis limitation, we propose TextSplat--the first text-driven Generalizable\nGaussian Splatting framework. By employing a text-guided fusion of diverse\nsemantic cues, our framework learns robust cross-modal feature representations\nthat improve the alignment of geometric and semantic information, producing\nhigh-fidelity 3D reconstructions. Specifically, our framework employs three\nparallel modules to obtain complementary representations: the Diffusion Prior\nDepth Estimator for accurate depth information, the Semantic Aware Segmentation\nNetwork for detailed semantic information, and the Multi-View Interaction\nNetwork for refined cross-view features. Then, in the Text-Guided Semantic\nFusion Module, these representations are integrated via the text-guided and\nattention-based feature aggregation mechanism, resulting in enhanced 3D\nGaussian parameters enriched with detailed semantic cues. Experimental results\non various benchmark datasets demonstrate improved performance compared to\nexisting methods across multiple evaluation metrics, validating the\neffectiveness of our framework. The code will be publicly available."}
{"id": "2504.10063", "pdf": "https://arxiv.org/pdf/2504.10063", "abs": "https://arxiv.org/abs/2504.10063", "authors": ["Alexandra Bazarova", "Aleksandr Yugay", "Andrey Shulga", "Alina Ermilova", "Andrei Volodichev", "Konstantin Polev", "Julia Belikova", "Rauf Parchiev", "Dmitry Simakov", "Maxim Savchenko", "Andrey Savchenko", "Serguei Barannikov", "Alexey Zaytsev"], "title": "Hallucination Detection in LLMs via Topological Divergence on Attention Graphs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Hallucination, i.e., generating factually incorrect content, remains a\ncritical challenge for large language models (LLMs). We introduce TOHA, a\nTOpology-based HAllucination detector in the RAG setting, which leverages a\ntopological divergence metric to quantify the structural properties of graphs\ninduced by attention matrices. Examining the topological divergence between\nprompt and response subgraphs reveals consistent patterns: higher divergence\nvalues in specific attention heads correlate with hallucinated outputs,\nindependent of the dataset. Extensive experiments, including evaluation on\nquestion answering and data-to-text tasks, show that our approach achieves\nstate-of-the-art or competitive results on several benchmarks, two of which\nwere annotated by us and are being publicly released to facilitate further\nresearch. Beyond its strong in-domain performance, TOHA maintains remarkable\ndomain transferability across multiple open-source LLMs. Our findings suggest\nthat analyzing the topological structure of attention matrices can serve as an\nefficient and robust indicator of factual reliability in LLMs."}
{"id": "2504.09598", "pdf": "https://arxiv.org/pdf/2504.09598", "abs": "https://arxiv.org/abs/2504.09598", "authors": ["Yining Zhao", "Ali Braytee", "Mukesh Prasad"], "title": "DualPrompt-MedCap: A Dual-Prompt Enhanced Approach for Medical Image Captioning", "categories": ["cs.CV", "I.2.10; I.4.8; J.3"], "comment": "11 pages, 4 figures, 2 tables", "summary": "Medical image captioning via vision-language models has shown promising\npotential for clinical diagnosis assistance. However, generating contextually\nrelevant descriptions with accurate modality recognition remains challenging.\nWe present DualPrompt-MedCap, a novel dual-prompt enhancement framework that\naugments Large Vision-Language Models (LVLMs) through two specialized\ncomponents: (1) a modality-aware prompt derived from a semi-supervised\nclassification model pretrained on medical question-answer pairs, and (2) a\nquestion-guided prompt leveraging biomedical language model embeddings. To\naddress the lack of captioning ground truth, we also propose an evaluation\nframework that jointly considers spatial-semantic relevance and medical\nnarrative quality. Experiments on multiple medical datasets demonstrate that\nDualPrompt-MedCap outperforms the baseline BLIP-3 by achieving a 22%\nimprovement in modality recognition accuracy while generating more\ncomprehensive and question-aligned descriptions. Our method enables the\ngeneration of clinically accurate reports that can serve as medical experts'\nprior knowledge and automatic annotations for downstream vision-language tasks."}
{"id": "2504.10065", "pdf": "https://arxiv.org/pdf/2504.10065", "abs": "https://arxiv.org/abs/2504.10065", "authors": ["Zeng Ren", "Xinyi Guan", "Martin Rohrmeier"], "title": "A Computational Cognitive Model for Processing Repetitions of Hierarchical Relations", "categories": ["cs.CL"], "comment": null, "summary": "Patterns are fundamental to human cognition, enabling the recognition of\nstructure and regularity across diverse domains. In this work, we focus on\nstructural repeats, patterns that arise from the repetition of hierarchical\nrelations within sequential data, and develop a candidate computational model\nof how humans detect and understand such structural repeats. Based on a\nweighted deduction system, our model infers the minimal generative process of a\ngiven sequence in the form of a Template program, a formalism that enriches the\ncontext-free grammar with repetition combinators. Such representation\nefficiently encodes the repetition of sub-computations in a recursive manner.\nAs a proof of concept, we demonstrate the expressiveness of our model on short\nsequences from music and action planning. The proposed model offers broader\ninsights into the mental representations and cognitive mechanisms underlying\nhuman pattern recognition."}
{"id": "2504.09601", "pdf": "https://arxiv.org/pdf/2504.09601", "abs": "https://arxiv.org/abs/2504.09601", "authors": ["Jia Wei", "Xiaoqi Zhao", "Jonghye Woo", "Jinsong Ouyang", "Georges El Fakhri", "Qingyu Chen", "Xiaofeng Liu"], "title": "Mixture-of-Shape-Experts (MoSE): End-to-End Shape Dictionary Framework to Prompt SAM for Generalizable Medical Segmentation", "categories": ["cs.CV", "cs.LG", "cs.MM", "eess.IV", "physics.med-ph"], "comment": "Accepted to CVPR 2025 workshop", "summary": "Single domain generalization (SDG) has recently attracted growing attention\nin medical image segmentation. One promising strategy for SDG is to leverage\nconsistent semantic shape priors across different imaging protocols, scanner\nvendors, and clinical sites. However, existing dictionary learning methods that\nencode shape priors often suffer from limited representational power with a\nsmall set of offline computed shape elements, or overfitting when the\ndictionary size grows. Moreover, they are not readily compatible with large\nfoundation models such as the Segment Anything Model (SAM). In this paper, we\npropose a novel Mixture-of-Shape-Experts (MoSE) framework that seamlessly\nintegrates the idea of mixture-of-experts (MoE) training into dictionary\nlearning to efficiently capture diverse and robust shape priors. Our method\nconceptualizes each dictionary atom as a shape expert, which specializes in\nencoding distinct semantic shape information. A gating network dynamically\nfuses these shape experts into a robust shape map, with sparse activation\nguided by SAM encoding to prevent overfitting. We further provide this shape\nmap as a prompt to SAM, utilizing the powerful generalization capability of SAM\nthrough bidirectional integration. All modules, including the shape dictionary,\nare trained in an end-to-end manner. Extensive experiments on multiple public\ndatasets demonstrate its effectiveness."}
{"id": "2504.10077", "pdf": "https://arxiv.org/pdf/2504.10077", "abs": "https://arxiv.org/abs/2504.10077", "authors": ["Abhinav Joshi", "Areeb Ahmad", "Divyaksh Shukla", "Ashutosh Modi"], "title": "Towards Quantifying Commonsense Reasoning with Mechanistic Insights", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at NAACL 2025; 28 pages (9 pages + 7 pages references + 12\n  pages appendix)", "summary": "Commonsense reasoning deals with the implicit knowledge that is well\nunderstood by humans and typically acquired via interactions with the world. In\nrecent times, commonsense reasoning and understanding of various LLMs have been\nevaluated using text-based tasks. In this work, we argue that a proxy of this\nunderstanding can be maintained as a graphical structure that can further help\nto perform a rigorous evaluation of commonsense reasoning abilities about\nvarious real-world activities. We create an annotation scheme for capturing\nthis implicit knowledge in the form of a graphical structure for 37 daily human\nactivities. We find that the created resource can be used to frame an enormous\nnumber of commonsense queries (~ 10^{17}), facilitating rigorous evaluation of\ncommonsense reasoning in LLMs. Moreover, recently, the remarkable performance\nof LLMs has raised questions about whether these models are truly capable of\nreasoning in the wild and, in general, how reasoning occurs inside these\nmodels. In this resource paper, we bridge this gap by proposing design\nmechanisms that facilitate research in a similar direction. Our findings\nsuggest that the reasoning components are localized in LLMs that play a\nprominent role in decision-making when prompted with a commonsense query."}
{"id": "2504.09606", "pdf": "https://arxiv.org/pdf/2504.09606", "abs": "https://arxiv.org/abs/2504.09606", "authors": ["Lexington Whalen", "Zhenbang Du", "Haoran You", "Chaojian Li", "Sixu Li", "Yingyan", "Lin"], "title": "Early-Bird Diffusion: Investigating and Leveraging Timestep-Aware Early-Bird Tickets in Diffusion Models for Efficient Training", "categories": ["cs.CV"], "comment": "10 pages, 5 figures. Accepted to the IEEE/CVF Conference on Computer\n  Vision and Pattern Recognition 2025", "summary": "Training diffusion models (DMs) requires substantial computational resources\ndue to multiple forward and backward passes across numerous timesteps,\nmotivating research into efficient training techniques. In this paper, we\npropose EB-Diff-Train, a new efficient DM training approach that is orthogonal\nto other methods of accelerating DM training, by investigating and leveraging\nEarly-Bird (EB) tickets -- sparse subnetworks that manifest early in the\ntraining process and maintain high generation quality.\n  We first investigate the existence of traditional EB tickets in DMs, enabling\ncompetitive generation quality without fully training a dense model.\n  Then, we delve into the concept of diffusion-dedicated EB tickets, drawing on\ninsights from varying importance of different timestep regions. These tickets\nadapt their sparsity levels according to the importance of corresponding\ntimestep regions, allowing for aggressive sparsity during non-critical regions\nwhile conserving computational resources for crucial timestep regions.\n  Building on this, we develop an efficient DM training technique that derives\ntimestep-aware EB tickets, trains them in parallel, and combines them during\ninference for image generation. Extensive experiments validate the existence of\nboth traditional and timestep-aware EB tickets, as well as the effectiveness of\nour proposed EB-Diff-Train method. This approach can significantly reduce\ntraining time both spatially and temporally -- achieving 2.9$\\times$ to\n5.8$\\times$ speedups over training unpruned dense models, and up to\n10.3$\\times$ faster training compared to standard train-prune-finetune\npipelines -- without compromising generative quality.\n  Our code is available at https://github.com/GATECH-EIC/Early-Bird-Diffusion."}
{"id": "2504.10157", "pdf": "https://arxiv.org/pdf/2504.10157", "abs": "https://arxiv.org/abs/2504.10157", "authors": ["Xinnong Zhang", "Jiayu Lin", "Xinyi Mou", "Shiyue Yang", "Xiawei Liu", "Libo Sun", "Hanjia Lyu", "Yihang Yang", "Weihong Qi", "Yue Chen", "Guanying Li", "Ling Yan", "Yao Hu", "Siming Chen", "Yu Wang", "Jingxuan Huang", "Jiebo Luo", "Shiping Tang", "Libo Wu", "Baohua Zhou", "Zhongyu Wei"], "title": "SocioVerse: A World Model for Social Simulation Powered by LLM Agents and A Pool of 10 Million Real-World Users", "categories": ["cs.CL", "cs.CY"], "comment": "work in progress", "summary": "Social simulation is transforming traditional social science research by\nmodeling human behavior through interactions between virtual individuals and\ntheir environments. With recent advances in large language models (LLMs), this\napproach has shown growing potential in capturing individual differences and\npredicting group behaviors. However, existing methods face alignment challenges\nrelated to the environment, target users, interaction mechanisms, and\nbehavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven\nworld model for social simulation. Our framework features four powerful\nalignment components and a user pool of 10 million real individuals. To\nvalidate its effectiveness, we conducted large-scale simulation experiments\nacross three distinct domains: politics, news, and economics. Results\ndemonstrate that SocioVerse can reflect large-scale population dynamics while\nensuring diversity, credibility, and representativeness through standardized\nprocedures and minimal manual adjustments."}
{"id": "2504.09608", "pdf": "https://arxiv.org/pdf/2504.09608", "abs": "https://arxiv.org/abs/2504.09608", "authors": ["Xingke Song", "Xiaoying Yang", "Chenglin Yao", "Jianfeng Ren", "Ruibin Bai", "Xin Chen", "Xudong Jiang"], "title": "ERL-MPP: Evolutionary Reinforcement Learning with Multi-head Puzzle Perception for Solving Large-scale Jigsaw Puzzles of Eroded Gaps", "categories": ["cs.CV"], "comment": "9 pages, 5 figures", "summary": "Solving jigsaw puzzles has been extensively studied. While most existing\nmodels focus on solving either small-scale puzzles or puzzles with no gap\nbetween fragments, solving large-scale puzzles with gaps presents distinctive\nchallenges in both image understanding and combinatorial optimization. To\ntackle these challenges, we propose a framework of Evolutionary Reinforcement\nLearning with Multi-head Puzzle Perception (ERL-MPP) to derive a better set of\nswapping actions for solving the puzzles. Specifically, to tackle the\nchallenges of perceiving the puzzle with gaps, a Multi-head Puzzle Perception\nNetwork (MPPN) with a shared encoder is designed, where multiple puzzlet heads\ncomprehensively perceive the local assembly status, and a discriminator head\nprovides a global assessment of the puzzle. To explore the large swapping\naction space efficiently, an Evolutionary Reinforcement Learning (EvoRL) agent\nis designed, where an actor recommends a set of suitable swapping actions from\na large action space based on the perceived puzzle status, a critic updates the\nactor using the estimated rewards and the puzzle status, and an evaluator\ncoupled with evolutionary strategies evolves the actions aligning with the\nhistorical assembly experience. The proposed ERL-MPP is comprehensively\nevaluated on the JPLEG-5 dataset with large gaps and the MIT dataset with\nlarge-scale puzzles. It significantly outperforms all state-of-the-art models\non both datasets."}
{"id": "2504.10160", "pdf": "https://arxiv.org/pdf/2504.10160", "abs": "https://arxiv.org/abs/2504.10160", "authors": ["Zhaopeng Feng", "Shaosheng Cao", "Jiahan Ren", "Jiayuan Su", "Ruizhe Chen", "Yan Zhang", "Zhe Xu", "Yao Hu", "Jian Wu", "Zuozhu Liu"], "title": "MT-R1-Zero: Advancing LLM-based Machine Translation via R1-Zero-like Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Work in progress. Our code is available at\n  https://github.com/fzp0424/MT-R1-Zero", "summary": "Large-scale reinforcement learning (RL) methods have proven highly effective\nin enhancing the reasoning abilities of large language models (LLMs),\nparticularly for tasks with verifiable solutions such as mathematics and\ncoding. However, applying this idea to machine translation (MT), where outputs\nare flexibly formatted and difficult to automatically evaluate with explicit\nrules, remains underexplored. In this work, we introduce MT-R1-Zero, the first\nopen-source adaptation of the R1-Zero RL framework for MT without supervised\nfine-tuning or cold-start. We propose a rule-metric mixed reward mechanism to\nguide LLMs towards improved translation quality via emergent reasoning. On the\nWMT 24 English-Chinese benchmark, our MT-R1-Zero-3B-Mix achieves competitive\nperformance, surpassing TowerInstruct-7B-v0.2 by an average of 1.26 points.\nMeanwhile, our MT-R1-Zero-7B-Mix attains a high average score of 62.25 across\nall metrics, placing it on par with advanced proprietary models such as GPT-4o\nand Claude-3.5-Sonnet, while the MT-R1-Zero-7B-Sem variant achieves\nstate-of-the-art scores on semantic metrics. Moreover, our work exhibits strong\ngeneralization capabilities on out-of-distribution MT tasks, robustly\nsupporting multilingual and low-resource settings. Extensive analysis of model\nbehavior across different initializations and reward metrics offers pioneering\ninsight into the critical role of reward design, LLM adaptability, training\ndynamics, and emergent reasoning patterns within the R1-Zero paradigm for MT.\nOur code is available at https://github.com/fzp0424/MT-R1-Zero."}
{"id": "2504.09621", "pdf": "https://arxiv.org/pdf/2504.09621", "abs": "https://arxiv.org/abs/2504.09621", "authors": ["Jiuchen Chen", "Xinyu Yan", "Qizhi Xu", "Kaiqi Li"], "title": "Tokenize Image Patches: Global Context Fusion for Effective Haze Removal in Large Images", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Global contextual information and local detail features are essential for\nhaze removal tasks. Deep learning models perform well on small, low-resolution\nimages, but they encounter difficulties with large, high-resolution ones due to\nGPU memory limitations. As a compromise, they often resort to image slicing or\ndownsampling. The former diminishes global information, while the latter\ndiscards high-frequency details. To address these challenges, we propose\nDehazeXL, a haze removal method that effectively balances global context and\nlocal feature extraction, enabling end-to-end modeling of large images on\nmainstream GPU hardware. Additionally, to evaluate the efficiency of global\ncontext utilization in haze removal performance, we design a visual attribution\nmethod tailored to the characteristics of haze removal tasks. Finally,\nrecognizing the lack of benchmark datasets for haze removal in large images, we\nhave developed an ultra-high-resolution haze removal dataset (8KDehaze) to\nsupport model training and testing. It includes 10000 pairs of clear and hazy\nremote sensing images, each sized at 8192 $\\times$ 8192 pixels. Extensive\nexperiments demonstrate that DehazeXL can infer images up to 10240 $\\times$\n10240 pixels with only 21 GB of memory, achieving state-of-the-art results\namong all evaluated methods. The source code and experimental dataset are\navailable at https://github.com/CastleChen339/DehazeXL."}
{"id": "2504.10167", "pdf": "https://arxiv.org/pdf/2504.10167", "abs": "https://arxiv.org/abs/2504.10167", "authors": ["Xu Zhang", "Zhifei Liu", "Jiahao Wang", "Huixuan Zhang", "Fan Xu", "Junzhe Zhang", "Xiaojun Wan"], "title": "C-FAITH: A Chinese Fine-Grained Benchmark for Automated Hallucination Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite the rapid advancement of large language models, they remain highly\nsusceptible to generating hallucinations, which significantly hinders their\nwidespread application. Hallucination research requires dynamic and\nfine-grained evaluation. However, most existing hallucination benchmarks\n(especially in Chinese language) rely on human annotations, making automatical\nand cost-effective hallucination evaluation challenging. To address this, we\nintroduce HaluAgent, an agentic framework that automatically constructs\nfine-grained QA dataset based on some knowledge documents. Our experiments\ndemonstrate that the manually designed rules and prompt optimization can\nimprove the quality of generated data. Using HaluAgent, we construct C-FAITH, a\nChinese QA hallucination benchmark created from 1,399 knowledge documents\nobtained from web scraping, totaling 60,702 entries. We comprehensively\nevaluate 16 mainstream LLMs with our proposed C-FAITH, providing detailed\nexperimental results and analysis."}
{"id": "2504.09623", "pdf": "https://arxiv.org/pdf/2504.09623", "abs": "https://arxiv.org/abs/2504.09623", "authors": ["Atharv Mahesh Mane", "Dulanga Weerakoon", "Vigneshwaran Subbaraju", "Sougata Sen", "Sanjay E. Sarma", "Archan Misra"], "title": "Ges3ViG: Incorporating Pointing Gestures into Language-Based 3D Visual Grounding for Embodied Reference Understanding", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "Accepted to the IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2025", "summary": "3-Dimensional Embodied Reference Understanding (3D-ERU) combines a language\ndescription and an accompanying pointing gesture to identify the most relevant\ntarget object in a 3D scene. Although prior work has explored pure\nlanguage-based 3D grounding, there has been limited exploration of 3D-ERU,\nwhich also incorporates human pointing gestures. To address this gap, we\nintroduce a data augmentation framework-Imputer, and use it to curate a new\nbenchmark dataset-ImputeRefer for 3D-ERU, by incorporating human pointing\ngestures into existing 3D scene datasets that only contain language\ninstructions. We also propose Ges3ViG, a novel model for 3D-ERU that achieves\n~30% improvement in accuracy as compared to other 3D-ERU models and ~9%\ncompared to other purely language-based 3D grounding models. Our code and\ndataset are available at https://github.com/AtharvMane/Ges3ViG."}
{"id": "2504.10168", "pdf": "https://arxiv.org/pdf/2504.10168", "abs": "https://arxiv.org/abs/2504.10168", "authors": ["Mohamed A. Abdallah", "Samhaa R. El-Beltagy"], "title": "HalluSearch at SemEval-2025 Task 3: A Search-Enhanced RAG Pipeline for Hallucination Detection", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this paper, we present HalluSearch, a multilingual pipeline designed to\ndetect fabricated text spans in Large Language Model (LLM) outputs. Developed\nas part of Mu-SHROOM, the Multilingual Shared-task on Hallucinations and\nRelated Observable Overgeneration Mistakes, HalluSearch couples\nretrieval-augmented verification with fine-grained factual splitting to\nidentify and localize hallucinations in fourteen different languages. Empirical\nevaluations show that HalluSearch performs competitively, placing fourth in\nboth English (within the top ten percent) and Czech. While the system's\nretrieval-based strategy generally proves robust, it faces challenges in\nlanguages with limited online coverage, underscoring the need for further\nresearch to ensure consistent hallucination detection across diverse linguistic\ncontexts."}
{"id": "2504.09641", "pdf": "https://arxiv.org/pdf/2504.09641", "abs": "https://arxiv.org/abs/2504.09641", "authors": ["Xingjian Zhang", "Siwei Wen", "Wenjun Wu", "Lei Huang"], "title": "TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Recently, improving the reasoning ability of large multimodal models (LMMs)\nthrough reinforcement learning has made great progress. However, most existing\nworks are based on highly reasoning-intensive datasets such as mathematics and\ncode, and researchers generally choose large-scale models as the foundation. We\nargue that exploring small-scale models' reasoning capabilities remains\nvaluable for researchers with limited computational resources. Moreover,\nenabling models to explain their reasoning processes on general\nquestion-answering datasets is equally meaningful. Therefore, we present the\nsmall-scale video reasoning model TinyLLaVA-Video-R1. Based on TinyLLaVA-Video,\na traceably trained video understanding model with no more than 4B parameters,\nit not only demonstrates significantly improved reasoning and thinking\ncapabilities after using reinforcement learning on general Video-QA datasets,\nbut also exhibits the emergent characteristic of \"aha moments\". Furthermore, we\nshare a series of experimental findings, aiming to provide practical insights\nfor future exploration of video reasoning (thinking) abilities in small-scale\nmodels. It is available at https://github.com/ZhangXJ199/TinyLLaVA-Video-R1."}
{"id": "2504.10185", "pdf": "https://arxiv.org/pdf/2504.10185", "abs": "https://arxiv.org/abs/2504.10185", "authors": ["Soumyadeep Pal", "Changsheng Wang", "James Diffenderfer", "Bhavya Kailkhura", "Sijia Liu"], "title": "LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in Current Benchmarks", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language model unlearning has become a critical challenge in ensuring\nsafety and controlled model behavior by removing undesired data-model\ninfluences from the pretrained model while preserving general utility.\nSignificant recent efforts have been dedicated to developing LLM unlearning\nbenchmarks such as WMDP (Weapons of Mass Destruction Proxy) and MUSE (Machine\nUnlearning Six-way Evaluation), facilitating standardized unlearning\nperformance assessment and method comparison. Despite their usefulness, we\nuncover for the first time a novel coreset effect within these benchmarks.\nSpecifically, we find that LLM unlearning achieved with the original (full)\nforget set can be effectively maintained using a significantly smaller subset\n(functioning as a \"coreset\"), e.g., as little as 5% of the forget set, even\nwhen selected at random. This suggests that LLM unlearning in these benchmarks\ncan be performed surprisingly easily, even in an extremely low-data regime. We\ndemonstrate that this coreset effect remains strong, regardless of the LLM\nunlearning method used, such as NPO (Negative Preference Optimization) and RMU\n(Representation Misdirection Unlearning), the popular ones in these benchmarks.\nThe surprisingly strong coreset effect is also robust across various data\nselection methods, ranging from random selection to more sophisticated\nheuristic approaches. We explain the coreset effect in LLM unlearning through a\nkeyword-based perspective, showing that keywords extracted from the forget set\nalone contribute significantly to unlearning effectiveness and indicating that\ncurrent unlearning is driven by a compact set of high-impact tokens rather than\nthe entire dataset. We further justify the faithfulness of coreset-unlearned\nmodels along additional dimensions, such as mode connectivity and robustness to\njailbreaking attacks. Codes are available at\nhttps://github.com/OPTML-Group/MU-Coreset."}
{"id": "2504.09644", "pdf": "https://arxiv.org/pdf/2504.09644", "abs": "https://arxiv.org/abs/2504.09644", "authors": ["Kaiyu Li", "Zepeng Xin", "Li Pang", "Chao Pang", "Yupeng Deng", "Jing Yao", "Guisong Xia", "Deyu Meng", "Zhi Wang", "Xiangyong Cao"], "title": "SegEarth-R1: Geospatial Pixel Reasoning via Large Language Model", "categories": ["cs.CV"], "comment": null, "summary": "Remote sensing has become critical for understanding environmental dynamics,\nurban planning, and disaster management. However, traditional remote sensing\nworkflows often rely on explicit segmentation or detection methods, which\nstruggle to handle complex, implicit queries that require reasoning over\nspatial context, domain knowledge, and implicit user intent. Motivated by this,\nwe introduce a new task, \\ie, geospatial pixel reasoning, which allows implicit\nquerying and reasoning and generates the mask of the target region. To advance\nthis task, we construct and release the first large-scale benchmark dataset\ncalled EarthReason, which comprises 5,434 manually annotated image masks with\nover 30,000 implicit question-answer pairs. Moreover, we propose SegEarth-R1, a\nsimple yet effective language-guided segmentation baseline that integrates a\nhierarchical visual encoder, a large language model (LLM) for instruction\nparsing, and a tailored mask generator for spatial correlation. The design of\nSegEarth-R1 incorporates domain-specific adaptations, including aggressive\nvisual token compression to handle ultra-high-resolution remote sensing images,\na description projection module to fuse language and multi-scale features, and\na streamlined mask prediction pipeline that directly queries description\nembeddings. Extensive experiments demonstrate that SegEarth-R1 achieves\nstate-of-the-art performance on both reasoning and referring segmentation\ntasks, significantly outperforming traditional and LLM-based segmentation\nmethods. Our data and code will be released at\nhttps://github.com/earth-insights/SegEarth-R1."}
{"id": "2504.10187", "pdf": "https://arxiv.org/pdf/2504.10187", "abs": "https://arxiv.org/abs/2504.10187", "authors": ["Jiaan Wang", "Fandong Meng", "Jie Zhou"], "title": "Deep Reasoning Translation via Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, deep reasoning LLMs (e.g., OpenAI o1/o3 and DeepSeek-R1) have shown\npromising performance in various complex tasks. Free translation is an\nimportant and interesting task in the multilingual world, which requires going\nbeyond word-for-word translation and taking cultural differences into account.\nThis task is still under-explored in deep reasoning LLMs. In this paper, we\nintroduce DeepTrans, a deep reasoning translation model that learns free\ntranslation via reinforcement learning. Specifically, we carefully build a\nreward model with pre-defined scoring criteria on both the translation results\nand the thought process. Given the source sentences, the reward model teaches\nthe deep translation model how to think and free-translate them during\nreinforcement learning. In this way, training DeepTrans does not need any\nlabeled translations, avoiding the human-intensive annotation or\nresource-intensive data synthesis. Experimental results show the effectiveness\nof DeepTrans. Using Qwen2.5-7B as the backbone, DeepTrans improves performance\nby 16.3% in literature translation, and outperforms strong deep reasoning\nbaselines as well as baselines that are fine-tuned with synthesized data.\nMoreover, we summarize the failures and interesting findings during our RL\nexploration. We hope this work could inspire other researchers in free\ntranslation."}
{"id": "2504.09656", "pdf": "https://arxiv.org/pdf/2504.09656", "abs": "https://arxiv.org/abs/2504.09656", "authors": ["Xingrui Wang", "Jiang Liu", "Ze Wang", "Xiaodong Yu", "Jialian Wu", "Ximeng Sun", "Yusheng Su", "Alan Yuille", "Zicheng Liu", "Emad Barsoum"], "title": "KeyVID: Keyframe-Aware Video Diffusion for Audio-Synchronized Visual Animation", "categories": ["cs.CV"], "comment": null, "summary": "Generating video from various conditions, such as text, image, and audio,\nenables both spatial and temporal control, leading to high-quality generation\nresults. Videos with dramatic motions often require a higher frame rate to\nensure smooth motion. Currently, most audio-to-visual animation models use\nuniformly sampled frames from video clips. However, these uniformly sampled\nframes fail to capture significant key moments in dramatic motions at low frame\nrates and require significantly more memory when increasing the number of\nframes directly. In this paper, we propose KeyVID, a keyframe-aware\naudio-to-visual animation framework that significantly improves the generation\nquality for key moments in audio signals while maintaining computation\nefficiency. Given an image and an audio input, we first localize keyframe time\nsteps from the audio. Then, we use a keyframe generator to generate the\ncorresponding visual keyframes. Finally, we generate all intermediate frames\nusing the motion interpolator. Through extensive experiments, we demonstrate\nthat KeyVID significantly improves audio-video synchronization and video\nquality across multiple datasets, particularly for highly dynamic motions. The\ncode is released in https://github.com/XingruiWang/KeyVID."}
{"id": "2504.10191", "pdf": "https://arxiv.org/pdf/2504.10191", "abs": "https://arxiv.org/abs/2504.10191", "authors": ["Veniamin Veselovsky", "Berke Argin", "Benedikt Stroebl", "Chris Wendler", "Robert West", "James Evans", "Thomas L. Griffiths", "Arvind Narayanan"], "title": "Localized Cultural Knowledge is Conserved and Controllable in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Just as humans display language patterns influenced by their native tongue\nwhen speaking new languages, LLMs often default to English-centric responses\neven when generating in other languages. Nevertheless, we observe that local\ncultural information persists within the models and can be readily activated\nfor cultural customization. We first demonstrate that explicitly providing\ncultural context in prompts significantly improves the models' ability to\ngenerate culturally localized responses. We term the disparity in model\nperformance with versus without explicit cultural context the explicit-implicit\nlocalization gap, indicating that while cultural knowledge exists within LLMs,\nit may not naturally surface in multilingual interactions if cultural context\nis not explicitly provided. Despite the explicit prompting benefit, however,\nthe answers reduce in diversity and tend toward stereotypes. Second, we\nidentify an explicit cultural customization vector, conserved across all\nnon-English languages we explore, which enables LLMs to be steered from the\nsynthetic English cultural world-model toward each non-English cultural world.\nSteered responses retain the diversity of implicit prompting and reduce\nstereotypes to dramatically improve the potential for customization. We discuss\nthe implications of explicit cultural customization for understanding the\nconservation of alternative cultural world models within LLMs, and their\ncontrollable utility for translation, cultural customization, and the\npossibility of making the explicit implicit through soft control for expanded\nLLM function and appeal."}
{"id": "2504.09666", "pdf": "https://arxiv.org/pdf/2504.09666", "abs": "https://arxiv.org/abs/2504.09666", "authors": ["Yao Yuan", "Pan Gao", "Qun Dai", "Jie Qin", "Wei Xiang"], "title": "Uncertainty Guided Refinement for Fine-Grained Salient Object Detection", "categories": ["cs.CV"], "comment": "IEEE Transactions on Image Processing 2025", "summary": "Recently, salient object detection (SOD) methods have achieved impressive\nperformance. However, salient regions predicted by existing methods usually\ncontain unsaturated regions and shadows, which limits the model for reliable\nfine-grained predictions. To address this, we introduce the uncertainty\nguidance learning approach to SOD, intended to enhance the model's perception\nof uncertain regions. Specifically, we design a novel Uncertainty Guided\nRefinement Attention Network (UGRAN), which incorporates three important\ncomponents, i.e., the Multilevel Interaction Attention (MIA) module, the Scale\nSpatial-Consistent Attention (SSCA) module, and the Uncertainty Refinement\nAttention (URA) module. Unlike conventional methods dedicated to enhancing\nfeatures, the proposed MIA facilitates the interaction and perception of\nmultilevel features, leveraging the complementary characteristics among\nmultilevel features. Then, through the proposed SSCA, the salient information\nacross diverse scales within the aggregated features can be integrated more\ncomprehensively and integrally. In the subsequent steps, we utilize the\nuncertainty map generated from the saliency prediction map to enhance the\nmodel's perception capability of uncertain regions, generating a\nhighly-saturated fine-grained saliency prediction map. Additionally, we devise\nan adaptive dynamic partition (ADP) mechanism to minimize the computational\noverhead of the URA module and improve the utilization of uncertainty guidance.\nExperiments on seven benchmark datasets demonstrate the superiority of the\nproposed UGRAN over the state-of-the-art methodologies. Codes will be released\nat https://github.com/I2-Multimedia-Lab/UGRAN."}
{"id": "2504.10198", "pdf": "https://arxiv.org/pdf/2504.10198", "abs": "https://arxiv.org/abs/2504.10198", "authors": ["Hanghui Guo", "Jia Zhu", "Shimin Di", "Weijie Shi", "Zhangze Chen", "Jiajie Xu"], "title": "DioR: Adaptive Cognitive Detection and Contextual Retrieval Optimization for Dynamic Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": "24 pages, 9 figures", "summary": "Dynamic Retrieval-augmented Generation (RAG) has shown great success in\nmitigating hallucinations in large language models (LLMs) during generation.\nHowever, existing dynamic RAG methods face significant limitations in two key\naspects: 1) Lack of an effective mechanism to control retrieval triggers, and\n2) Lack of effective scrutiny of retrieval content. To address these\nlimitations, we propose an innovative dynamic RAG method, DioR (Adaptive\nCognitive Detection and Contextual Retrieval Optimization), which consists of\ntwo main components: adaptive cognitive detection and contextual retrieval\noptimization, specifically designed to determine when retrieval is needed and\nwhat to retrieve for LLMs is useful. Experimental results demonstrate that DioR\nachieves superior performance on all tasks, demonstrating the effectiveness of\nour work."}
{"id": "2504.09671", "pdf": "https://arxiv.org/pdf/2504.09671", "abs": "https://arxiv.org/abs/2504.09671", "authors": ["Pranav Manu", "Astitva Srivastava", "Amit Raj", "Varun Jampani", "Avinash Sharma", "P. J. Narayanan"], "title": "LightHeadEd: Relightable & Editable Head Avatars from a Smartphone", "categories": ["cs.CV"], "comment": null, "summary": "Creating photorealistic, animatable, and relightable 3D head avatars\ntraditionally requires expensive Lightstage with multiple calibrated cameras,\nmaking it inaccessible for widespread adoption. To bridge this gap, we present\na novel, cost-effective approach for creating high-quality relightable head\navatars using only a smartphone equipped with polaroid filters. Our approach\ninvolves simultaneously capturing cross-polarized and parallel-polarized video\nstreams in a dark room with a single point-light source, separating the skin's\ndiffuse and specular components during dynamic facial performances. We\nintroduce a hybrid representation that embeds 2D Gaussians in the UV space of a\nparametric head model, facilitating efficient real-time rendering while\npreserving high-fidelity geometric details. Our learning-based neural\nanalysis-by-synthesis pipeline decouples pose and expression-dependent\ngeometrical offsets from appearance, decomposing the surface into albedo,\nnormal, and specular UV texture maps, along with the environment maps. We\ncollect a unique dataset of various subjects performing diverse facial\nexpressions and head movements."}
{"id": "2504.10227", "pdf": "https://arxiv.org/pdf/2504.10227", "abs": "https://arxiv.org/abs/2504.10227", "authors": ["Tianjie Ju", "Zhenyu Shao", "Bowen Wang", "Yujia Chen", "Zhuosheng Zhang", "Hao Fei", "Mong-Li Lee", "Wynne Hsu", "Sufeng Duan", "Gongshen Liu"], "title": "Probing then Editing Response Personality of Large Language Models", "categories": ["cs.CL"], "comment": "Working in Progress", "summary": "Large Language Models (LLMs) have demonstrated promising capabilities to\ngenerate responses that exhibit consistent personality traits. Despite the\nmajor attempts to analyze personality expression through output-based\nevaluations, little is known about how such traits are internally encoded\nwithin LLM parameters. In this paper, we introduce a layer-wise probing\nframework to systematically investigate the layer-wise capability of LLMs in\nencoding personality for responding. We conduct probing experiments on 11\nopen-source LLMs over the PersonalityEdit benchmark and find that LLMs\npredominantly encode personality for responding in their middle and upper\nlayers, with instruction-tuned models demonstrating a slightly clearer\nseparation of personality traits. Furthermore, by interpreting the trained\nprobing hyperplane as a layer-wise boundary for each personality category, we\npropose a layer-wise perturbation method to edit the personality expressed by\nLLMs during inference. Our results show that even when the prompt explicitly\nspecifies a particular personality, our method can still successfully alter the\nresponse personality of LLMs. Interestingly, the difficulty of converting\nbetween certain personality traits varies substantially, which aligns with the\nrepresentational distances in our probing experiments. Finally, we conduct a\ncomprehensive MMLU benchmark evaluation and time overhead analysis,\ndemonstrating that our proposed personality editing method incurs only minimal\ndegradation in general capabilities while maintaining low training costs and\nacceptable inference latency. Our code is publicly available at\nhttps://github.com/universe-sky/probing-then-editing-personality."}
{"id": "2504.09694", "pdf": "https://arxiv.org/pdf/2504.09694", "abs": "https://arxiv.org/abs/2504.09694", "authors": ["Jiachen Liu", "Yuan Xue", "Haomiao Ni", "Rui Yu", "Zihan Zhou", "Sharon X. Huang"], "title": "Computer-Aided Layout Generation for Building Design: A Review", "categories": ["cs.CV"], "comment": "CVMJ 2025", "summary": "Generating realistic building layouts for automatic building design has been\nstudied in both the computer vision and architecture domains. Traditional\napproaches from the architecture domain, which are based on optimization\ntechniques or heuristic design guidelines, can synthesize desirable layouts,\nbut usually require post-processing and involve human interaction in the design\npipeline, making them costly and timeconsuming. The advent of deep generative\nmodels has significantly improved the fidelity and diversity of the generated\narchitecture layouts, reducing the workload by designers and making the process\nmuch more efficient. In this paper, we conduct a comprehensive review of three\nmajor research topics of architecture layout design and generation: floorplan\nlayout generation, scene layout synthesis, and generation of some other formats\nof building layouts. For each topic, we present an overview of the leading\nparadigms, categorized either by research domains (architecture or machine\nlearning) or by user input conditions or constraints. We then introduce the\ncommonly-adopted benchmark datasets that are used to verify the effectiveness\nof the methods, as well as the corresponding evaluation metrics. Finally, we\nidentify the well-solved problems and limitations of existing approaches, then\npropose new perspectives as promising directions for future research in this\nimportant research area. A project associated with this survey to maintain the\nresources is available at awesome-building-layout-generation."}
{"id": "2504.10284", "pdf": "https://arxiv.org/pdf/2504.10284", "abs": "https://arxiv.org/abs/2504.10284", "authors": ["Weiqi Wang", "Jiefu Ou", "Yangqiu Song", "Benjamin Van Durme", "Daniel Khashabi"], "title": "Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the Evaluation Protocol", "categories": ["cs.CL"], "comment": null, "summary": "Literature review tables are essential for summarizing and comparing\ncollections of scientific papers. We explore the task of generating tables that\nbest fulfill a user's informational needs given a collection of scientific\npapers. Building on recent work (Newman et al., 2024), we extend prior\napproaches to address real-world complexities through a combination of\nLLM-based methods and human annotations. Our contributions focus on three key\nchallenges encountered in real-world use: (i) User prompts are often\nunder-specified; (ii) Retrieved candidate papers frequently contain irrelevant\ncontent; and (iii) Task evaluation should move beyond shallow text similarity\ntechniques and instead assess the utility of inferred tables for\ninformation-seeking tasks (e.g., comparing papers). To support reproducible\nevaluation, we introduce ARXIV2TABLE, a more realistic and challenging\nbenchmark for this task, along with a novel approach to improve literature\nreview table generation in real-world scenarios. Our extensive experiments on\nthis benchmark show that both open-weight and proprietary LLMs struggle with\nthe task, highlighting its difficulty and the need for further advancements.\nOur dataset and code are available at https://github.com/JHU-CLSP/arXiv2Table."}
{"id": "2504.09700", "pdf": "https://arxiv.org/pdf/2504.09700", "abs": "https://arxiv.org/abs/2504.09700", "authors": ["Zijian Wu", "Shuojue Yang", "Yueming Jin", "Septimiu E Salcudean"], "title": "ToolTipNet: A Segmentation-Driven Deep Learning Baseline for Surgical Instrument Tip Detection", "categories": ["cs.CV"], "comment": null, "summary": "In robot-assisted laparoscopic radical prostatectomy (RALP), the location of\nthe instrument tip is important to register the ultrasound frame with the\nlaparoscopic camera frame. A long-standing limitation is that the instrument\ntip position obtained from the da Vinci API is inaccurate and requires hand-eye\ncalibration. Thus, directly computing the position of the tool tip in the\ncamera frame using the vision-based method becomes an attractive solution.\nBesides, surgical instrument tip detection is the key component of other tasks,\nlike surgical skill assessment and surgery automation. However, this task is\nchallenging due to the small size of the tool tip and the articulation of the\nsurgical instrument. Surgical instrument segmentation becomes relatively easy\ndue to the emergence of the Segmentation Foundation Model, i.e., Segment\nAnything. Based on this advancement, we explore the deep learning-based\nsurgical instrument tip detection approach that takes the part-level instrument\nsegmentation mask as input. Comparison experiments with a hand-crafted\nimage-processing approach demonstrate the superiority of the proposed method on\nsimulated and real datasets."}
{"id": "2504.10335", "pdf": "https://arxiv.org/pdf/2504.10335", "abs": "https://arxiv.org/abs/2504.10335", "authors": ["Maharaj Brahma", "N J Karthika", "Atul Singh", "Devaraj Adiga", "Smruti Bhate", "Ganesh Ramakrishnan", "Rohit Saluja", "Maunendra Sankar Desarkar"], "title": "MorphTok: Morphologically Grounded Tokenization for Indian Languages", "categories": ["cs.CL"], "comment": null, "summary": "Tokenization is a crucial step in NLP, especially with the rise of large\nlanguage models (LLMs), impacting downstream performance, computational cost,\nand efficiency. Existing LLMs rely on the classical Byte-pair Encoding (BPE)\nalgorithm for subword tokenization that greedily merges frequent character\nbigrams. This often leads to segmentation that does not align with\nlinguistically meaningful units. To address this, we propose morphology-aware\nsegmentation as a pre-tokenization step prior to applying BPE. To facilitate\nmorphology-aware segmentation, we create a novel dataset for Hindi and Marathi,\nincorporating sandhi splitting to enhance the subword tokenization. Experiments\non downstream tasks show that morphologically grounded tokenization improves\nperformance for machine translation and language modeling. Additionally, to\nhandle the ambiguity in the Unicode characters for diacritics, particularly\ndependent vowels in syllable-based writing systems, we introduce Constrained\nBPE (CBPE), an extension to the traditional BPE algorithm that incorporates\nscript-specific constraints. Specifically, CBPE handles dependent vowels. Our\nresults show that CBPE achieves a 1.68\\% reduction in fertility scores while\nmaintaining comparable or improved downstream performance in machine\ntranslation, offering a computationally efficient alternative to standard BPE.\nMoreover, to evaluate segmentation across different tokenization algorithms, we\nintroduce a new human evaluation metric, \\textit{EvalTok}, enabling more\nhuman-grounded assessment."}
{"id": "2504.09724", "pdf": "https://arxiv.org/pdf/2504.09724", "abs": "https://arxiv.org/abs/2504.09724", "authors": ["Gaurav Shinde", "Anuradha Ravi", "Emon Dey", "Shadman Sakib", "Milind Rampure", "Nirmalya Roy"], "title": "A Survey on Efficient Vision-Language Models", "categories": ["cs.CV"], "comment": "35 pages, 16 figures", "summary": "Vision-language models (VLMs) integrate visual and textual information,\nenabling a wide range of applications such as image captioning and visual\nquestion answering, making them crucial for modern AI systems. However, their\nhigh computational demands pose challenges for real-time applications. This has\nled to a growing focus on developing efficient vision language models. In this\nsurvey, we review key techniques for optimizing VLMs on edge and\nresource-constrained devices. We also explore compact VLM architectures,\nframeworks and provide detailed insights into the performance-memory trade-offs\nof efficient VLMs. Furthermore, we establish a GitHub repository at\nhttps://github.com/MPSCUMBC/Efficient-Vision-Language-Models-A-Survey to\ncompile all surveyed papers, which we will actively update. Our objective is to\nfoster deeper research in this area."}
{"id": "2504.10340", "pdf": "https://arxiv.org/pdf/2504.10340", "abs": "https://arxiv.org/abs/2504.10340", "authors": ["Shahriar Noroozizadeh", "Sayantan Kumar", "Jeremy C. Weiss"], "title": "Forecasting from Clinical Textual Time Series: Adaptations of the Encoder and Decoder Language Model Families", "categories": ["cs.CL", "cs.AI"], "comment": "Machine Learning for Healthcare (MLHC 2025)", "summary": "Clinical case reports encode rich, temporal patient trajectories that are\noften underexploited by traditional machine learning methods relying on\nstructured data. In this work, we introduce the forecasting problem from\ntextual time series, where timestamped clinical findings--extracted via an\nLLM-assisted annotation pipeline--serve as the primary input for prediction. We\nsystematically evaluate a diverse suite of models, including fine-tuned\ndecoder-based large language models and encoder-based transformers, on tasks of\nevent occurrence prediction, temporal ordering, and survival analysis. Our\nexperiments reveal that encoder-based models consistently achieve higher F1\nscores and superior temporal concordance for short- and long-horizon event\nforecasting, while fine-tuned masking approaches enhance ranking performance.\nIn contrast, instruction-tuned decoder models demonstrate a relative advantage\nin survival analysis, especially in early prognosis settings. Our sensitivity\nanalyses further demonstrate the importance of time ordering, which requires\nclinical time series construction, as compared to text ordering, the format of\nthe text inputs that LLMs are classically trained on. This highlights the\nadditional benefit that can be ascertained from time-ordered corpora, with\nimplications for temporal tasks in the era of widespread LLM use."}
{"id": "2504.09738", "pdf": "https://arxiv.org/pdf/2504.09738", "abs": "https://arxiv.org/abs/2504.09738", "authors": ["Vasilii Korolkov", "Andrey Yanchenko"], "title": "Automatic Detection of Intro and Credits in Video using CLIP and Multihead Attention", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "68T07", "I.2.10; I.4.8; I.5.1"], "comment": "22 pages, 11 figures, submitted as a preprint. ArXiv preprint only,\n  not submitted to a journal yet", "summary": "Detecting transitions between intro/credits and main content in videos is a\ncrucial task for content segmentation, indexing, and recommendation systems.\nManual annotation of such transitions is labor-intensive and error-prone, while\nheuristic-based methods often fail to generalize across diverse video styles.\nIn this work, we introduce a deep learning-based approach that formulates the\nproblem as a sequence-to-sequence classification task, where each second of a\nvideo is labeled as either \"intro\" or \"film.\" Our method extracts frames at a\nfixed rate of 1 FPS, encodes them using CLIP (Contrastive Language-Image\nPretraining), and processes the resulting feature representations with a\nmultihead attention model incorporating learned positional encoding. The system\nachieves an F1-score of 91.0%, Precision of 89.0%, and Recall of 97.0% on the\ntest set, and is optimized for real-time inference, achieving 11.5 FPS on CPU\nand 107 FPS on high-end GPUs. This approach has practical applications in\nautomated content indexing, highlight detection, and video summarization.\nFuture work will explore multimodal learning, incorporating audio features and\nsubtitles to further enhance detection accuracy."}
{"id": "2504.10342", "pdf": "https://arxiv.org/pdf/2504.10342", "abs": "https://arxiv.org/abs/2504.10342", "authors": ["Yueqi Song", "Tianyue Ou", "Yibo Kong", "Zecheng Li", "Graham Neubig", "Xiang Yue"], "title": "VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain Knowledge", "categories": ["cs.CL"], "comment": "56 pages, 43 figures", "summary": "Current multimodal benchmarks often conflate reasoning with domain-specific\nknowledge, making it difficult to isolate and evaluate general reasoning\nabilities in non-expert settings. To address this, we introduce VisualPuzzles,\na benchmark that targets visual reasoning while deliberately minimizing\nreliance on specialized knowledge. VisualPuzzles consists of diverse questions\nspanning five categories: algorithmic, analogical, deductive, inductive, and\nspatial reasoning. One major source of our questions is manually translated\nlogical reasoning questions from the Chinese Civil Service Examination.\nExperiments show that VisualPuzzles requires significantly less intensive\ndomain-specific knowledge and more complex reasoning compared to benchmarks\nlike MMMU, enabling us to better evaluate genuine multimodal reasoning.\nEvaluations show that state-of-the-art multimodal large language models\nconsistently lag behind human performance on VisualPuzzles, and that strong\nperformance on knowledge-intensive benchmarks does not necessarily translate to\nsuccess on reasoning-focused, knowledge-light tasks. Additionally, reasoning\nenhancements such as scaling up inference compute (with \"thinking\" modes) yield\ninconsistent gains across models and task types, and we observe no clear\ncorrelation between model size and performance. We also found that models\nexhibit different reasoning and answering patterns on VisualPuzzles compared to\nbenchmarks with heavier emphasis on knowledge. VisualPuzzles offers a clearer\nlens through which to evaluate reasoning capabilities beyond factual recall and\ndomain knowledge."}
{"id": "2504.09764", "pdf": "https://arxiv.org/pdf/2504.09764", "abs": "https://arxiv.org/abs/2504.09764", "authors": ["Yuyang Ji", "Haohan Wang"], "title": "Socratic Chart: Cooperating Multiple Agents for Robust SVG Chart Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have shown remarkable versatility\nbut face challenges in demonstrating true visual understanding, particularly in\nchart reasoning tasks. Existing benchmarks like ChartQA reveal significant\nreliance on text-based shortcuts and probabilistic pattern-matching rather than\ngenuine visual reasoning. To rigorously evaluate visual reasoning, we introduce\na more challenging test scenario by removing textual labels and introducing\nchart perturbations in the ChartQA dataset. Under these conditions, models like\nGPT-4o and Gemini-2.0 Pro experience up to a 30% performance drop, underscoring\ntheir limitations. To address these challenges, we propose Socratic Chart, a\nnew framework that transforms chart images into Scalable Vector Graphics (SVG)\nrepresentations, enabling MLLMs to integrate textual and visual modalities for\nenhanced chart understanding. Socratic Chart employs a multi-agent pipeline\nwith specialized agent-generators to extract primitive chart attributes (e.g.,\nbar heights, line coordinates) and an agent-critic to validate results,\nensuring high-fidelity symbolic representations. Our framework surpasses\nstate-of-the-art models in accurately capturing chart primitives and improving\nreasoning performance, establishing a robust pathway for advancing MLLM visual\nunderstanding."}
{"id": "2504.10356", "pdf": "https://arxiv.org/pdf/2504.10356", "abs": "https://arxiv.org/abs/2504.10356", "authors": ["Dieuwke Hupkes", "Nikolay Bogoychev"], "title": "MultiLoKo: a multilingual local knowledge benchmark for LLMs spanning 31 languages", "categories": ["cs.CL"], "comment": null, "summary": "We present MultiLoKo, a new benchmark for evaluating multilinguality in LLMs\ncovering 31 languages. MultiLoKo consists of three partitions: a main partition\nconsisting of 500 questions per language, separately sourced to be locally\nrelevant to the specific language, and two translated partitions, containing\nhuman-authored translations from 30 non-English languages to English and vice\nversa. For comparison, we also release corresponding machine-authored\ntranslations. The data is equally distributed over two splits: a dev split and\na blind, out-of-distribution test split. MultiLoKo can be used to study a\nvariety of questions regarding the multilinguality of LLMs as well as\nmeta-questions about multilingual benchmark creation. We compute MultiLoKo\nscores for 11 base and chat models marketed to be multilingual and study their\naverage performance, their performance parity across languages, how much their\nability to answer questions depends on the question language, and which\nlanguages are most difficult. None of the models we studied performs well on\nMultiLoKo, as indicated by low average scores as well as large differences\nbetween the best and worst scoring languages. Furthermore, we find a\nsubstantial effect of the question language, indicating sub-optimal knowledge\ntransfer between languages. Lastly, we find that using local vs\nEnglish-translated data can result in differences more than 20 points for the\nbest performing models, drastically change the estimated difficulty of some\nlanguages. For using machines instead of human translations, we find a weaker\neffect on ordering of language difficulty, a larger difference in model\nrankings, and a substantial drop in estimated performance for all models."}
{"id": "2504.09766", "pdf": "https://arxiv.org/pdf/2504.09766", "abs": "https://arxiv.org/abs/2504.09766", "authors": ["Diego Marcondes"], "title": "On the representation of stack operators by mathematical morphology", "categories": ["cs.CV"], "comment": null, "summary": "This paper introduces the class of grey-scale image stack operators as those\nthat (a) map binary-images into binary-images and (b) commute in average with\ncross-sectioning. We show that stack operators are 1-Lipchitz extensions of set\noperators which can be represented by applying a characteristic set operator to\nthe cross-sections of the image and summing. In particular, they are a\ngeneralisation of stack filters, for which the characteristic set operators are\nincreasing. Our main result is that stack operators inherit lattice properties\nof the characteristic set operators. We focus on the case of\ntranslation-invariant and locally defined stack operators and show the main\nresult by deducing the characteristic function, kernel, and basis\nrepresentation of stack operators. The results of this paper have implications\non the design of image operators, since imply that to solve some grey-scale\nimage processing problems it is enough to design an operator for performing the\ndesired transformation on binary images, and then considering its extension\ngiven by a stack operator. We leave many topics for future research regarding\nthe machine learning of stack operators and the characterisation of the image\nprocessing problems that can be solved by them."}
{"id": "2504.10359", "pdf": "https://arxiv.org/pdf/2504.10359", "abs": "https://arxiv.org/abs/2504.10359", "authors": ["Aryan Shrivastava", "Paula Akemi Aoyagui"], "title": "DICE: A Framework for Dimensional and Contextual Evaluation of Language Models", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Language models (LMs) are increasingly being integrated into a wide range of\napplications, yet the modern evaluation paradigm does not sufficiently reflect\nhow they are actually being used. Current evaluations rely on benchmarks that\noften lack direct applicability to the real-world contexts in which LMs are\nbeing deployed. To address this gap, we propose Dimensional and Contextual\nEvaluation (DICE), an approach that evaluates LMs on granular,\ncontext-dependent dimensions. In this position paper, we begin by examining the\ninsufficiency of existing LM benchmarks, highlighting their limited\napplicability to real-world use cases. Next, we propose a set of granular\nevaluation parameters that capture dimensions of LM behavior that are more\nmeaningful to stakeholders across a variety of application domains.\nSpecifically, we introduce the concept of context-agnostic parameters - such as\nrobustness, coherence, and epistemic honesty - and context-specific parameters\nthat must be tailored to the specific contextual constraints and demands of\nstakeholders choosing to deploy LMs into a particular setting. We then discuss\npotential approaches to operationalize this evaluation framework, finishing\nwith the opportunities and challenges DICE presents to the LM evaluation\nlandscape. Ultimately, this work serves as a practical and approachable\nstarting point for context-specific and stakeholder-relevant evaluation of LMs."}
{"id": "2504.09789", "pdf": "https://arxiv.org/pdf/2504.09789", "abs": "https://arxiv.org/abs/2504.09789", "authors": ["Chao Liu", "Arash Vahdat"], "title": "EquiVDM: Equivariant Video Diffusion Models with Temporally Consistent Noise", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Temporally consistent video-to-video generation is essential for applications\nof video diffusion models in areas such as sim-to-real, style-transfer, video\nupsampling, etc. In this paper, we propose a video diffusion framework that\nleverages temporally consistent noise to generate coherent video frames without\nspecialized modules or additional constraints. We show that the standard\ntraining objective of diffusion models, when applied with temporally consistent\nnoise, encourages the model to be equivariant to spatial transformations in\ninput video and noise. This enables our model to better follow motion patterns\nfrom the input video, producing aligned motion and high-fidelity frames.\nFurthermore, we extend our approach to 3D-consistent video generation by\nattaching noise as textures on 3D meshes, ensuring 3D consistency in\nsim-to-real applications. Experimental results demonstrate that our method\nsurpasses state-of-the-art baselines in motion alignment, 3D consistency, and\nvideo quality while requiring only a few sampling steps in practice."}
{"id": "2504.10368", "pdf": "https://arxiv.org/pdf/2504.10368", "abs": "https://arxiv.org/abs/2504.10368", "authors": ["Wenyuan Zhang", "Shuaiyi Nie", "Xinghua Zhang", "Zefeng Zhang", "Tingwen Liu"], "title": "S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability of Large Reasoning Models", "categories": ["cs.CL", "cs.AI"], "comment": "Work in Progress", "summary": "We introduce S1-Bench, a novel benchmark designed to evaluate Large Reasoning\nModels' (LRMs) performance on simple tasks that favor intuitive system 1\nthinking rather than deliberative system 2 reasoning. While LRMs have achieved\nsignificant breakthroughs in complex reasoning tasks through explicit chains of\nthought, their reliance on deep analytical thinking may limit their system 1\nthinking capabilities. Moreover, a lack of benchmark currently exists to\nevaluate LRMs' performance in tasks that require such capabilities. To fill\nthis gap, S1-Bench presents a set of simple, diverse, and naturally clear\nquestions across multiple domains and languages, specifically designed to\nassess LRMs' performance in such tasks. Our comprehensive evaluation of 22 LRMs\nreveals significant lower efficiency tendencies, with outputs averaging 15.5\ntimes longer than those of traditional small LLMs. Additionally, LRMs often\nidentify correct answers early but continue unnecessary deliberation, with some\nmodels even producing numerous errors. These findings highlight the rigid\nreasoning patterns of current LRMs and underscore the substantial development\nneeded to achieve balanced dual-system thinking capabilities that can adapt\nappropriately to task complexity."}
{"id": "2504.09797", "pdf": "https://arxiv.org/pdf/2504.09797", "abs": "https://arxiv.org/abs/2504.09797", "authors": ["Dinh Dai Quan Tran", "Hoang-Thien Nguyen. Thanh-Huy Nguyen", "Gia-Van To", "Tien-Huy Nguyen", "Quan Nguyen"], "title": "IGL-DT: Iterative Global-Local Feature Learning with Dual-Teacher Semantic Segmentation Framework under Limited Annotation Scheme", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 5 figures", "summary": "Semi-Supervised Semantic Segmentation (SSSS) aims to improve segmentation\naccuracy by leveraging a small set of labeled images alongside a larger pool of\nunlabeled data. Recent advances primarily focus on pseudo-labeling, consistency\nregularization, and co-training strategies. However, existing methods struggle\nto balance global semantic representation with fine-grained local feature\nextraction. To address this challenge, we propose a novel tri-branch\nsemi-supervised segmentation framework incorporating a dual-teacher strategy,\nnamed IGL-DT. Our approach employs SwinUnet for high-level semantic guidance\nthrough Global Context Learning and ResUnet for detailed feature refinement via\nLocal Regional Learning. Additionally, a Discrepancy Learning mechanism\nmitigates over-reliance on a single teacher, promoting adaptive feature\nlearning. Extensive experiments on benchmark datasets demonstrate that our\nmethod outperforms state-of-the-art approaches, achieving superior segmentation\nperformance across various data regimes."}
{"id": "2504.10391", "pdf": "https://arxiv.org/pdf/2504.10391", "abs": "https://arxiv.org/abs/2504.10391", "authors": ["Varun Vasudevan", "Faezeh Akhavizadegan", "Abhinav Prakash", "Yokila Arora", "Jason Cho", "Tanya Mendiratta", "Sushant Kumar", "Kannan Achan"], "title": "LLM-driven Constrained Copy Generation through Iterative Refinement", "categories": ["cs.CL"], "comment": "10 pages, 2 figures, 7 Tables", "summary": "Crafting a marketing message (copy), or copywriting is a challenging\ngeneration task, as the copy must adhere to various constraints. Copy creation\nis inherently iterative for humans, starting with an initial draft followed by\nsuccessive refinements. However, manual copy creation is time-consuming and\nexpensive, resulting in only a few copies for each use case. This limitation\nrestricts our ability to personalize content to customers. Contrary to the\nmanual approach, LLMs can generate copies quickly, but the generated content\ndoes not consistently meet all the constraints on the first attempt (similar to\nhumans). While recent studies have shown promise in improving constrained\ngeneration through iterative refinement, they have primarily addressed tasks\nwith only a few simple constraints. Consequently, the effectiveness of\niterative refinement for tasks such as copy generation, which involves many\nintricate constraints, remains unclear. To address this gap, we propose an\nLLM-based end-to-end framework for scalable copy generation using iterative\nrefinement. To the best of our knowledge, this is the first study to address\nmultiple challenging constraints simultaneously in copy generation. Examples of\nthese constraints include length, topics, keywords, preferred lexical ordering,\nand tone of voice. We demonstrate the performance of our framework by creating\ncopies for e-commerce banners for three different use cases of varying\ncomplexity. Our results show that iterative refinement increases the copy\nsuccess rate by $16.25-35.91$% across use cases. Furthermore, the copies\ngenerated using our approach outperformed manually created content in multiple\npilot studies using a multi-armed bandit framework. The winning copy improved\nthe click-through rate by $38.5-45.21$%."}
{"id": "2504.09814", "pdf": "https://arxiv.org/pdf/2504.09814", "abs": "https://arxiv.org/abs/2504.09814", "authors": ["Beomseok Kang", "Niluthpol Chowdhury Mithun", "Abhinav Rajvanshi", "Han-Pang Chiu", "Supun Samarasekera"], "title": "DUDA: Distilled Unsupervised Domain Adaptation for Lightweight Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Unsupervised Domain Adaptation (UDA) is essential for enabling semantic\nsegmentation in new domains without requiring costly pixel-wise annotations.\nState-of-the-art (SOTA) UDA methods primarily use self-training with\narchitecturally identical teacher and student networks, relying on Exponential\nMoving Average (EMA) updates. However, these approaches face substantial\nperformance degradation with lightweight models due to inherent architectural\ninflexibility leading to low-quality pseudo-labels. To address this, we propose\nDistilled Unsupervised Domain Adaptation (DUDA), a novel framework that\ncombines EMA-based self-training with knowledge distillation (KD). Our method\nemploys an auxiliary student network to bridge the architectural gap between\nheavyweight and lightweight models for EMA-based updates, resulting in improved\npseudo-label quality. DUDA employs a strategic fusion of UDA and KD,\nincorporating innovative elements such as gradual distillation from large to\nsmall networks, inconsistency loss prioritizing poorly adapted classes, and\nlearning with multiple teachers. Extensive experiments across four UDA\nbenchmarks demonstrate DUDA's superiority in achieving SOTA performance with\nlightweight models, often surpassing the performance of heavyweight models from\nother approaches."}
{"id": "2504.10405", "pdf": "https://arxiv.org/pdf/2504.10405", "abs": "https://arxiv.org/abs/2504.10405", "authors": ["Diogo Sousa", "Guilherme Barbosa", "Catarina Rocha", "Dulce Oliveira"], "title": "Performance of Large Language Models in Supporting Medical Diagnosis and Treatment", "categories": ["cs.CL", "cs.AI", "cs.ET", "cs.HC", "I.2.7; J.3"], "comment": "21 pages, 6 figures, 4 tables. Acknowledgements: The authors\n  acknowledge the support of the AITriage4SU Project (2024.07400.IACDC/2024),\n  funded by the FCT (Foundation for Science and Technology), Portugal", "summary": "The integration of Large Language Models (LLMs) into healthcare holds\nsignificant potential to enhance diagnostic accuracy and support medical\ntreatment planning. These AI-driven systems can analyze vast datasets,\nassisting clinicians in identifying diseases, recommending treatments, and\npredicting patient outcomes. This study evaluates the performance of a range of\ncontemporary LLMs, including both open-source and closed-source models, on the\n2024 Portuguese National Exam for medical specialty access (PNA), a\nstandardized medical knowledge assessment. Our results highlight considerable\nvariation in accuracy and cost-effectiveness, with several models demonstrating\nperformance exceeding human benchmarks for medical students on this specific\ntask. We identify leading models based on a combined score of accuracy and\ncost, discuss the implications of reasoning methodologies like\nChain-of-Thought, and underscore the potential for LLMs to function as valuable\ncomplementary tools aiding medical professionals in complex clinical\ndecision-making."}
{"id": "2504.09819", "pdf": "https://arxiv.org/pdf/2504.09819", "abs": "https://arxiv.org/abs/2504.09819", "authors": ["Chenyang Zhao", "Jia Wan", "Antoni B. Chan"], "title": "Density-based Object Detection in Crowded Scenes", "categories": ["cs.CV"], "comment": null, "summary": "Compared with the generic scenes, crowded scenes contain highly-overlapped\ninstances, which result in: 1) more ambiguous anchors during training of object\ndetectors, and 2) more predictions are likely to be mistakenly suppressed in\npost-processing during inference. To address these problems, we propose two new\nstrategies, density-guided anchors (DGA) and density-guided NMS (DG-NMS), which\nuses object density maps to jointly compute optimal anchor assignments and\nreweighing, as well as an adaptive NMS. Concretely, based on an unbalanced\noptimal transport (UOT) problem, the density owned by each ground-truth object\nis transported to each anchor position at a minimal transport cost. And density\non anchors comprises an instance-specific density distribution, from which DGA\ndecodes the optimal anchor assignment and re-weighting strategy. Meanwhile,\nDG-NMS utilizes the predicted density map to adaptively adjust the NMS\nthreshold to reduce mistaken suppressions. In the UOT, a novel overlap-aware\ntransport cost is specifically designed for ambiguous anchors caused by\noverlapped neighboring objects. Extensive experiments on the challenging\nCrowdHuman dataset with Citypersons dataset demonstrate that our proposed\ndensity-guided detector is effective and robust to crowdedness. The code and\npre-trained models will be made available later."}
{"id": "2504.10415", "pdf": "https://arxiv.org/pdf/2504.10415", "abs": "https://arxiv.org/abs/2504.10415", "authors": ["Parshin Shojaee", "Ngoc-Hieu Nguyen", "Kazem Meidani", "Amir Barati Farimani", "Khoa D Doan", "Chandan K Reddy"], "title": "LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project page:\n  https://github.com/deep-symbolic-mathematics/llm-srbench , Benchmark page:\n  https://huggingface.co/datasets/nnheui/llm-srbench", "summary": "Scientific equation discovery is a fundamental task in the history of\nscientific progress, enabling the derivation of laws governing natural\nphenomena. Recently, Large Language Models (LLMs) have gained interest for this\ntask due to their potential to leverage embedded scientific knowledge for\nhypothesis generation. However, evaluating the true discovery capabilities of\nthese methods remains challenging, as existing benchmarks often rely on common\nequations that are susceptible to memorization by LLMs, leading to inflated\nperformance metrics that do not reflect discovery. In this paper, we introduce\nLLM-SRBench, a comprehensive benchmark with 239 challenging problems across\nfour scientific domains specifically designed to evaluate LLM-based scientific\nequation discovery methods while preventing trivial memorization. Our benchmark\ncomprises two main categories: LSR-Transform, which transforms common physical\nmodels into less common mathematical representations to test reasoning beyond\nmemorized forms, and LSR-Synth, which introduces synthetic, discovery-driven\nproblems requiring data-driven reasoning. Through extensive evaluation of\nseveral state-of-the-art methods, using both open and closed LLMs, we find that\nthe best-performing system so far achieves only 31.5% symbolic accuracy. These\nfindings highlight the challenges of scientific equation discovery, positioning\nLLM-SRBench as a valuable resource for future research."}
{"id": "2504.09828", "pdf": "https://arxiv.org/pdf/2504.09828", "abs": "https://arxiv.org/abs/2504.09828", "authors": ["Hezhao Liu", "Yang Lu", "Mengke Li", "Yiqun Zhang", "Shreyank N Gowda", "Chen Gong", "Hanzi Wang"], "title": "FATE: A Prompt-Tuning-Based Semi-Supervised Learning Framework for Extremely Limited Labeled Data", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Semi-supervised learning (SSL) has achieved significant progress by\nleveraging both labeled data and unlabeled data. Existing SSL methods overlook\na common real-world scenario when labeled data is extremely scarce, potentially\nas limited as a single labeled sample in the dataset. General SSL approaches\nstruggle to train effectively from scratch under such constraints, while\nmethods utilizing pre-trained models often fail to find an optimal balance\nbetween leveraging limited labeled data and abundant unlabeled data. To address\nthis challenge, we propose Firstly Adapt, Then catEgorize (FATE), a novel SSL\nframework tailored for scenarios with extremely limited labeled data. At its\ncore, the two-stage prompt tuning paradigm FATE exploits unlabeled data to\ncompensate for scarce supervision signals, then transfers to downstream tasks.\nConcretely, FATE first adapts a pre-trained model to the feature distribution\nof downstream data using volumes of unlabeled samples in an unsupervised\nmanner. It then applies an SSL method specifically designed for pre-trained\nmodels to complete the final classification task. FATE is designed to be\ncompatible with both vision and vision-language pre-trained models. Extensive\nexperiments demonstrate that FATE effectively mitigates challenges arising from\nthe scarcity of labeled samples in SSL, achieving an average performance\nimprovement of 33.74% across seven benchmarks compared to state-of-the-art SSL\nmethods. Code is available at\nhttps://anonymous.4open.science/r/Semi-supervised-learning-BA72."}
{"id": "2504.10418", "pdf": "https://arxiv.org/pdf/2504.10418", "abs": "https://arxiv.org/abs/2504.10418", "authors": ["Jing Chen", "Zhihua Wei", "Wei Zhang", "Yingying Hu", "Qiong Zhang"], "title": "CliniChat: A Multi-Source Knowledge-Driven Framework for Clinical Interview Dialogue Reconstruction and Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) hold great promise for assisting clinical\ninterviews due to their fluent interactive capabilities and extensive medical\nknowledge. However, the lack of high-quality interview dialogue data and widely\naccepted evaluation methods has significantly impeded this process. So we\npropose CliniChat, a framework that integrates multi-source knowledge to enable\nLLMs to simulate real-world clinical interviews. It consists of two modules:\nClini-Recon and Clini-Eval, each responsible for reconstructing and evaluating\ninterview dialogues, respectively. By incorporating three sources of knowledge,\nClini-Recon transforms clinical notes into systematic, professional, and\nempathetic interview dialogues. Clini-Eval combines a comprehensive evaluation\nmetric system with a two-phase automatic evaluation approach, enabling LLMs to\nassess interview performance like experts. We contribute MedQA-Dialog, a\nhigh-quality synthetic interview dialogue dataset, and CliniChatGLM, a model\nspecialized for clinical interviews. Experimental results demonstrate that\nCliniChatGLM's interview capabilities undergo a comprehensive upgrade,\nparticularly in history-taking, achieving state-of-the-art performance."}
{"id": "2504.09843", "pdf": "https://arxiv.org/pdf/2504.09843", "abs": "https://arxiv.org/abs/2504.09843", "authors": ["Lu Yue", "Dongliang Zhou", "Liang Xie", "Erwei Yin", "Feitian Zhang"], "title": "ST-Booster: An Iterative SpatioTemporal Perception Booster for Vision-and-Language Navigation in Continuous Environments", "categories": ["cs.CV", "cs.RO"], "comment": "11 pages, 7 figures", "summary": "Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires\nagents to navigate unknown, continuous spaces based on natural language\ninstructions. Compared to discrete settings, VLN-CE poses two core perception\nchallenges. First, the absence of predefined observation points leads to\nheterogeneous visual memories and weakened global spatial correlations. Second,\ncumulative reconstruction errors in three-dimensional scenes introduce\nstructural noise, impairing local feature perception. To address these\nchallenges, this paper proposes ST-Booster, an iterative spatiotemporal booster\nthat enhances navigation performance through multi-granularity perception and\ninstruction-aware reasoning. ST-Booster consists of three key modules --\nHierarchical SpatioTemporal Encoding (HSTE), Multi-Granularity Aligned Fusion\n(MGAF), and ValueGuided Waypoint Generation (VGWG). HSTE encodes long-term\nglobal memory using topological graphs and captures shortterm local details via\ngrid maps. MGAF aligns these dualmap representations with instructions through\ngeometry-aware knowledge fusion. The resulting representations are iteratively\nrefined through pretraining tasks. During reasoning, VGWG generates Guided\nAttention Heatmaps (GAHs) to explicitly model environment-instruction relevance\nand optimize waypoint selection. Extensive comparative experiments and\nperformance analyses are conducted, demonstrating that ST-Booster outperforms\nexisting state-of-the-art methods, particularly in complex, disturbance-prone\nenvironments."}
{"id": "2504.10419", "pdf": "https://arxiv.org/pdf/2504.10419", "abs": "https://arxiv.org/abs/2504.10419", "authors": ["Michał Turski", "Mateusz Chiliński", "Łukasz Borchmann"], "title": "Unchecked and Overlooked: Addressing the Checkbox Blind Spot in Large Language Models with CheckboxQA", "categories": ["cs.CL"], "comment": null, "summary": "Checkboxes are critical in real-world document processing where the presence\nor absence of ticks directly informs data extraction and decision-making\nprocesses. Yet, despite the strong performance of Large Vision and Language\nModels across a wide range of tasks, they struggle with interpreting checkable\ncontent. This challenge becomes particularly pressing in industries where a\nsingle overlooked checkbox may lead to costly regulatory or contractual\noversights. To address this gap, we introduce the CheckboxQA dataset, a\ntargeted resource designed to evaluate and improve model performance on\ncheckbox-related tasks. It reveals the limitations of current models and serves\nas a valuable tool for advancing document comprehension systems, with\nsignificant implications for applications in sectors such as legal tech and\nfinance.\n  The dataset is publicly available at:\nhttps://github.com/Snowflake-Labs/CheckboxQA"}
{"id": "2504.09852", "pdf": "https://arxiv.org/pdf/2504.09852", "abs": "https://arxiv.org/abs/2504.09852", "authors": ["Boris Kriuk", "Simranjit Kaur Gill", "Shoaib Aslam", "Amir Fakhrutdinov"], "title": "GFT: Gradient Focal Transformer", "categories": ["cs.CV"], "comment": "11 pages, 3 tables, 5 figures", "summary": "Fine-Grained Image Classification (FGIC) remains a complex task in computer\nvision, as it requires models to distinguish between categories with subtle\nlocalized visual differences. Well-studied CNN-based models, while strong in\nlocal feature extraction, often fail to capture the global context required for\nfine-grained recognition, while more recent ViT-backboned models address FGIC\nwith attention-driven mechanisms but lack the ability to adaptively focus on\ntruly discriminative regions. TransFG and other ViT-based extensions introduced\npart-aware token selection to enhance attention localization, yet they still\nstruggle with computational efficiency, attention region selection flexibility,\nand detail-focus narrative in complex environments. This paper introduces GFT\n(Gradient Focal Transformer), a new ViT-derived framework created for FGIC\ntasks. GFT integrates the Gradient Attention Learning Alignment (GALA)\nmechanism to dynamically prioritize class-discriminative features by analyzing\nattention gradient flow. Coupled with a Progressive Patch Selection (PPS)\nstrategy, the model progressively filters out less informative regions,\nreducing computational overhead while enhancing sensitivity to fine details.\nGFT achieves SOTA accuracy on FGVC Aircraft, Food-101, and COCO datasets with\n93M parameters, outperforming ViT-based advanced FGIC models in efficiency. By\nbridging global context and localized detail extraction, GFT sets a new\nbenchmark in fine-grained recognition, offering interpretable solutions for\nreal-world deployment scenarios."}
{"id": "2504.10421", "pdf": "https://arxiv.org/pdf/2504.10421", "abs": "https://arxiv.org/abs/2504.10421", "authors": ["Xinhao Yi", "Jake Lever", "Kevin Bryson", "Zaiqiao Meng"], "title": "Can We Edit LLMs for Long-Tail Biomedical Knowledge?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Knowledge editing has emerged as an effective approach for updating large\nlanguage models (LLMs) by modifying their internal knowledge. However, their\napplication to the biomedical domain faces unique challenges due to the\nlong-tailed distribution of biomedical knowledge, where rare and infrequent\ninformation is prevalent. In this paper, we conduct the first comprehensive\nstudy to investigate the effectiveness of knowledge editing methods for editing\nlong-tail biomedical knowledge. Our results indicate that, while existing\nediting methods can enhance LLMs' performance on long-tail biomedical\nknowledge, their performance on long-tail knowledge remains inferior to that on\nhigh-frequency popular knowledge, even after editing. Our further analysis\nreveals that long-tail biomedical knowledge contains a significant amount of\none-to-many knowledge, where one subject and relation link to multiple objects.\nThis high prevalence of one-to-many knowledge limits the effectiveness of\nknowledge editing in improving LLMs' understanding of long-tail biomedical\nknowledge, highlighting the need for tailored strategies to bridge this\nperformance gap."}
{"id": "2504.09876", "pdf": "https://arxiv.org/pdf/2504.09876", "abs": "https://arxiv.org/abs/2504.09876", "authors": ["Tran Quoc Khanh Le", "Nguyen Lan Vi Vu", "Ha-Hieu Pham", "Xuan-Loc Huynh", "Tien-Huy Nguyen", "Minh Huu Nhat Le", "Quan Nguyen", "Hien D. Nguyen"], "title": "HDC: Hierarchical Distillation for Multi-level Noisy Consistency in Semi-Supervised Fetal Ultrasound Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Transvaginal ultrasound is a critical imaging modality for evaluating\ncervical anatomy and detecting physiological changes. However, accurate\nsegmentation of cervical structures remains challenging due to low contrast,\nshadow artifacts, and fuzzy boundaries. While convolutional neural networks\n(CNNs) have shown promising results in medical image segmentation, their\nperformance is often limited by the need for large-scale annotated datasets -\nan impractical requirement in clinical ultrasound imaging. Semi-supervised\nlearning (SSL) offers a compelling solution by leveraging unlabeled data, but\nexisting teacher-student frameworks often suffer from confirmation bias and\nhigh computational costs. We propose HDC, a novel semi-supervised segmentation\nframework that integrates Hierarchical Distillation and Consistency learning\nwithin a multi-level noise mean-teacher framework. Unlike conventional\napproaches that rely solely on pseudo-labeling, we introduce a hierarchical\ndistillation mechanism that guides feature-level learning via two novel\nobjectives: (1) Correlation Guidance Loss to align feature representations\nbetween the teacher and main student branch, and (2) Mutual Information Loss to\nstabilize representations between the main and noisy student branches. Our\nframework reduces model complexity while improving generalization. Extensive\nexperiments on two fetal ultrasound datasets, FUGC and PSFH, demonstrate that\nour method achieves competitive performance with significantly lower\ncomputational overhead than existing multi-teacher models."}
{"id": "2504.10430", "pdf": "https://arxiv.org/pdf/2504.10430", "abs": "https://arxiv.org/abs/2504.10430", "authors": ["Minqian Liu", "Zhiyang Xu", "Xinyi Zhang", "Heajun An", "Sarvech Qadir", "Qi Zhang", "Pamela J. Wisniewski", "Jin-Hee Cho", "Sang Won Lee", "Ruoxi Jia", "Lifu Huang"], "title": "LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "20 pages, 7 figures, 4 tables", "summary": "Recent advancements in Large Language Models (LLMs) have enabled them to\napproach human-level persuasion capabilities. However, such potential also\nraises concerns about the safety risks of LLM-driven persuasion, particularly\ntheir potential for unethical influence through manipulation, deception,\nexploitation of vulnerabilities, and many other harmful tactics. In this work,\nwe present a systematic investigation of LLM persuasion safety through two\ncritical aspects: (1) whether LLMs appropriately reject unethical persuasion\ntasks and avoid unethical strategies during execution, including cases where\nthe initial persuasion goal appears ethically neutral, and (2) how influencing\nfactors like personality traits and external pressures affect their behavior.\nTo this end, we introduce PersuSafety, the first comprehensive framework for\nthe assessment of persuasion safety which consists of three stages, i.e.,\npersuasion scene creation, persuasive conversation simulation, and persuasion\nsafety assessment. PersuSafety covers 6 diverse unethical persuasion topics and\n15 common unethical strategies. Through extensive experiments across 8 widely\nused LLMs, we observe significant safety concerns in most LLMs, including\nfailing to identify harmful persuasion tasks and leveraging various unethical\npersuasion strategies. Our study calls for more attention to improve safety\nalignment in progressive and goal-driven conversations such as persuasion."}
{"id": "2504.09878", "pdf": "https://arxiv.org/pdf/2504.09878", "abs": "https://arxiv.org/abs/2504.09878", "authors": ["Yunpeng Tan", "Junlin Hao", "Jiangkai Wu", "Liming Liu", "Qingyang Li", "Xinggong Zhang"], "title": "MCBlock: Boosting Neural Radiance Field Training Speed by MCTS-based Dynamic-Resolution Ray Sampling", "categories": ["cs.CV"], "comment": null, "summary": "Neural Radiance Field (NeRF) is widely known for high-fidelity novel view\nsynthesis. However, even the state-of-the-art NeRF model, Gaussian Splatting,\nrequires minutes for training, far from the real-time performance required by\nmultimedia scenarios like telemedicine. One of the obstacles is its inefficient\nsampling, which is only partially addressed by existing works. Existing\npoint-sampling algorithms uniformly sample simple-texture regions (easy to fit)\nand complex-texture regions (hard to fit), while existing ray-sampling\nalgorithms sample these regions all in the finest granularity (i.e. the pixel\nlevel), both wasting GPU training resources. Actually, regions with different\ntexture intensities require different sampling granularities. To this end, we\npropose a novel dynamic-resolution ray-sampling algorithm, MCBlock, which\nemploys Monte Carlo Tree Search (MCTS) to partition each training image into\npixel blocks with different sizes for active block-wise training. Specifically,\nthe trees are initialized according to the texture of training images to boost\nthe initialization speed, and an expansion/pruning module dynamically optimizes\nthe block partition. MCBlock is implemented in Nerfstudio, an open-source\ntoolset, and achieves a training acceleration of up to 2.33x, surpassing other\nray-sampling algorithms. We believe MCBlock can apply to any cone-tracing NeRF\nmodel and contribute to the multimedia community."}
{"id": "2504.10481", "pdf": "https://arxiv.org/pdf/2504.10481", "abs": "https://arxiv.org/abs/2504.10481", "authors": ["Ding Chen", "Qingchen Yu", "Pengyuan Wang", "Wentao Zhang", "Bo Tang", "Feiyu Xiong", "Xinchi Li", "Minchuan Yang", "Zhiyu Li"], "title": "xVerify: Efficient Answer Verifier for Reasoning Model Evaluations", "categories": ["cs.CL"], "comment": "32 pages", "summary": "With the release of the o1 model by OpenAI, reasoning models adopting slow\nthinking strategies have gradually emerged. As the responses generated by such\nmodels often include complex reasoning, intermediate steps, and\nself-reflection, existing evaluation methods are often inadequate. They\nstruggle to determine whether the LLM output is truly equivalent to the\nreference answer, and also have difficulty identifying and extracting the final\nanswer from long, complex responses. To address this issue, we propose xVerify,\nan efficient answer verifier for reasoning model evaluations. xVerify\ndemonstrates strong capability in equivalence judgment, enabling it to\neffectively determine whether the answers produced by reasoning models are\nequivalent to reference answers across various types of objective questions. To\ntrain and evaluate xVerify, we construct the VAR dataset by collecting\nquestion-answer pairs generated by multiple LLMs across various datasets,\nleveraging multiple reasoning models and challenging evaluation sets designed\nspecifically for reasoning model assessment. A multi-round annotation process\nis employed to ensure label accuracy. Based on the VAR dataset, we train\nmultiple xVerify models of different scales. In evaluation experiments\nconducted on both the test set and generalization set, all xVerify models\nachieve overall F1 scores and accuracy exceeding 95\\%. Notably, the smallest\nvariant, xVerify-0.5B-I, outperforms all evaluation methods except GPT-4o,\nwhile xVerify-3B-Ib surpasses GPT-4o in overall performance. These results\nvalidate the effectiveness and generalizability of xVerify."}
{"id": "2504.09881", "pdf": "https://arxiv.org/pdf/2504.09881", "abs": "https://arxiv.org/abs/2504.09881", "authors": ["Changwei Wang", "Shunpeng Chen", "Yukun Song", "Rongtao Xu", "Zherui Zhang", "Jiguang Zhang", "Haoran Yang", "Yu Zhang", "Kexue Fu", "Shide Du", "Zhiwei Xu", "Longxiang Gao", "Li Guo", "Shibiao Xu"], "title": "Focus on Local: Finding Reliable Discriminative Regions for Visual Place Recognition", "categories": ["cs.CV"], "comment": "Accepted by AAAI 2025", "summary": "Visual Place Recognition (VPR) is aimed at predicting the location of a query\nimage by referencing a database of geotagged images. For VPR task, often fewer\ndiscriminative local regions in an image produce important effects while\nmundane background regions do not contribute or even cause perceptual aliasing\nbecause of easy overlap. However, existing methods lack precisely modeling and\nfull exploitation of these discriminative regions. In this paper, we propose\nthe Focus on Local (FoL) approach to stimulate the performance of image\nretrieval and re-ranking in VPR simultaneously by mining and exploiting\nreliable discriminative local regions in images and introducing\npseudo-correlation supervision. First, we design two losses,\nExtraction-Aggregation Spatial Alignment Loss (SAL) and Foreground-Background\nContrast Enhancement Loss (CEL), to explicitly model reliable discriminative\nlocal regions and use them to guide the generation of global representations\nand efficient re-ranking. Second, we introduce a weakly-supervised local\nfeature training strategy based on pseudo-correspondences obtained from\naggregating global features to alleviate the lack of local correspondences\nground truth for the VPR task. Third, we suggest an efficient re-ranking\npipeline that is efficiently and precisely based on discriminative region\nguidance. Finally, experimental results show that our FoL achieves the\nstate-of-the-art on multiple VPR benchmarks in both image retrieval and\nre-ranking stages and also significantly outperforms existing two-stage VPR\nmethods in terms of computational efficiency. Code and models are available at\nhttps://github.com/chenshunpeng/FoL"}
{"id": "2504.08016", "pdf": "https://arxiv.org/pdf/2504.08016", "abs": "https://arxiv.org/abs/2504.08016", "authors": ["Soo Yong Lee", "Hyunjin Hwang", "Taekwan Kim", "Yuyeong Kim", "Kyuri Park", "Jaemin Yoo", "Denny Borsboom", "Kijung Shin"], "title": "Emergence of psychopathological computations in large language models", "categories": ["q-bio.NC", "cs.AI", "cs.CL"], "comment": "pre-print", "summary": "Can large language models (LLMs) implement computations of psychopathology?\nAn effective approach to the question hinges on addressing two factors. First,\nfor conceptual validity, we require a general and computational account of\npsychopathology that is applicable to computational entities without biological\nembodiment or subjective experience. Second, mechanisms underlying LLM\nbehaviors need to be studied for better methodological validity. Thus, we\nestablish a computational-theoretical framework to provide an account of\npsychopathology applicable to LLMs. To ground the theory for empirical\nanalysis, we also propose a novel mechanistic interpretability method alongside\na tailored empirical analytic framework. Based on the frameworks, we conduct\nexperiments demonstrating three key claims: first, that distinct dysfunctional\nand problematic representational states are implemented in LLMs; second, that\ntheir activations can spread and self-sustain to trap LLMs; and third, that\ndynamic, cyclic structural causal models encoded in the LLMs underpin these\npatterns. In concert, the empirical results corroborate our hypothesis that\nnetwork-theoretic computations of psychopathology have already emerged in LLMs.\nThis suggests that certain LLM behaviors mirroring psychopathology may not be a\nsuperficial mimicry but a feature of their internal processing. Thus, our work\nalludes to the possibility of AI systems with psychopathological behaviors in\nthe near future."}
{"id": "2504.09887", "pdf": "https://arxiv.org/pdf/2504.09887", "abs": "https://arxiv.org/abs/2504.09887", "authors": ["Yiwen Wang", "Ying Liang", "Yuxuan Zhang", "Xinning Chai", "Zhengxue Cheng", "Yingsheng Qin", "Yucai Yang", "Rong Xie", "Li Song"], "title": "Enhanced Semantic Extraction and Guidance for UGC Image Super Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Due to the disparity between real-world degradations in user-generated\ncontent(UGC) images and synthetic degradations, traditional super-resolution\nmethods struggle to generalize effectively, necessitating a more robust\napproach to model real-world distortions. In this paper, we propose a novel\napproach to UGC image super-resolution by integrating semantic guidance into a\ndiffusion framework. Our method addresses the inconsistency between\ndegradations in wild and synthetic datasets by separately simulating the\ndegradation processes on the LSDIR dataset and combining them with the official\npaired training set. Furthermore, we enhance degradation removal and detail\ngeneration by incorporating a pretrained semantic extraction model (SAM2) and\nfine-tuning key hyperparameters for improved perceptual fidelity. Extensive\nexperiments demonstrate the superiority of our approach against\nstate-of-the-art methods. Additionally, the proposed model won second place in\nthe CVPR NTIRE 2025 Short-form UGC Image Super-Resolution Challenge, further\nvalidating its effectiveness. The code is available at\nhttps://github.c10pom/Moonsofang/NTIRE-2025-SRlab."}
{"id": "2504.08744", "pdf": "https://arxiv.org/pdf/2504.08744", "abs": "https://arxiv.org/abs/2504.08744", "authors": ["Esmail Gumaan"], "title": "ExpertRAG: Efficient RAG with Mixture of Experts -- Optimizing Context Retrieval for Adaptive LLM Responses", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "30 pages, 4 figures", "summary": "ExpertRAG is a novel theoretical framework that integrates Mixture-of-Experts\n(MoE) architectures with Retrieval Augmented Generation (RAG) to advance the\nefficiency and accuracy of knowledge-intensive language modeling. We propose a\ndynamic retrieval gating mechanism coupled with expert routing, enabling the\nmodel to selectively consult an external knowledge store or rely on specialized\ninternal experts based on the query's needs. The paper lays out the theoretical\nfoundations of ExpertRAG, including a probabilistic formulation that treats\nretrieval and expert selection as latent decisions, and mathematical\njustifications for its efficiency in both computation and knowledge\nutilization. We derive formulae to quantify the expected computational cost\nsavings from selective retrieval and the capacity gains from sparse expert\nutilization. A comparative analysis positions ExpertRAG against standard RAG\n(with always-on retrieval) and pure MoE models (e.g., Switch Transformer,\nMixtral) to highlight its unique balance between parametric knowledge and\nnon-parametric retrieval. We also outline an experimental validation strategy,\nproposing benchmarks and evaluation protocols to test ExpertRAG's performance\non factual recall, generalization, and inference efficiency. The proposed\nframework, although presented theoretically, is supported by insights from\nprior work in RAG and MoE, and is poised to provide more factual, efficient,\nand adaptive generation by leveraging the best of both paradigms. In summary,\nExpertRAG contributes a new perspective on scaling and augmenting language\nmodels, backed by a thorough analysis and a roadmap for empirical validation."}
{"id": "2504.09897", "pdf": "https://arxiv.org/pdf/2504.09897", "abs": "https://arxiv.org/abs/2504.09897", "authors": ["Jaewoo Lee", "Keyang Xuan", "Chanakya Ekbote", "Sandeep Polisetty", "Yi R.", "Fung", "Paul Pu Liang"], "title": "TAMP: Token-Adaptive Layerwise Pruning in Multimodal Large Language Models", "categories": ["cs.CV"], "comment": "Preprint", "summary": "Multimodal Large Language Models (MLLMs) have shown remarkable versatility in\nunderstanding diverse multimodal data and tasks. However, these capabilities\ncome with an increased model scale. While post-training pruning reduces model\nsize in unimodal models, its application to MLLMs often yields limited success.\nOur analysis discovers that conventional methods fail to account for the unique\ntoken attributes across layers and modalities inherent to MLLMs. Inspired by\nthis observation, we propose TAMP, a simple yet effective pruning framework\ntailored for MLLMs, featuring two key components: (1) Diversity-Aware Sparsity,\nwhich adjusts sparsity ratio per layer based on diversities among multimodal\noutput tokens, preserving more parameters in high-diversity layers; and (2)\nAdaptive Multimodal Input Activation, which identifies representative\nmultimodal input tokens using attention scores to guide unstructured weight\npruning. We validate our method on two state-of-the-art MLLMs: LLaVA-NeXT,\ndesigned for vision-language tasks, and VideoLLaMA2, capable of processing\naudio, visual, and language modalities. Empirical experiments across various\nmultimodal evaluation benchmarks demonstrate that each component of our\napproach substantially outperforms existing pruning techniques."}
{"id": "2504.08745", "pdf": "https://arxiv.org/pdf/2504.08745", "abs": "https://arxiv.org/abs/2504.08745", "authors": ["Mert Yazan", "Suzan Verberne", "Frederik Situmeang"], "title": "Improving RAG for Personalization with Author Features and Contrastive Examples", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Personalization with retrieval-augmented generation (RAG) often fails to\ncapture fine-grained features of authors, making it hard to identify their\nunique traits. To enrich the RAG context, we propose providing Large Language\nModels (LLMs) with author-specific features, such as average sentiment polarity\nand frequently used words, in addition to past samples from the author's\nprofile. We introduce a new feature called Contrastive Examples: documents from\nother authors are retrieved to help LLM identify what makes an author's style\nunique in comparison to others. Our experiments show that adding a couple of\nsentences about the named entities, dependency patterns, and words a person\nuses frequently significantly improves personalized text generation. Combining\nfeatures with contrastive examples boosts the performance further, achieving a\nrelative 15% improvement over baseline RAG while outperforming the benchmarks.\nOur results show the value of fine-grained features for better personalization,\nwhile opening a new research dimension for including contrastive examples as a\ncomplement with RAG. We release our code publicly."}
{"id": "2504.09899", "pdf": "https://arxiv.org/pdf/2504.09899", "abs": "https://arxiv.org/abs/2504.09899", "authors": ["Ziwang Xu", "Lanqing Guo", "Satoshi Tsutsui", "Shuyan Zhang", "Alex C. Kot", "Bihan Wen"], "title": "Digital Staining with Knowledge Distillation: A Unified Framework for Unpaired and Paired-But-Misaligned Data", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted to IEEE Transactions on Medical Imaging", "summary": "Staining is essential in cell imaging and medical diagnostics but poses\nsignificant challenges, including high cost, time consumption, labor intensity,\nand irreversible tissue alterations. Recent advances in deep learning have\nenabled digital staining through supervised model training. However, collecting\nlarge-scale, perfectly aligned pairs of stained and unstained images remains\ndifficult. In this work, we propose a novel unsupervised deep learning\nframework for digital cell staining that reduces the need for extensive paired\ndata using knowledge distillation. We explore two training schemes: (1)\nunpaired and (2) paired-but-misaligned settings. For the unpaired case, we\nintroduce a two-stage pipeline, comprising light enhancement followed by\ncolorization, as a teacher model. Subsequently, we obtain a student staining\ngenerator through knowledge distillation with hybrid non-reference losses. To\nleverage the pixel-wise information between adjacent sections, we further\nextend to the paired-but-misaligned setting, adding the Learning to Align\nmodule to utilize pixel-level information. Experiment results on our dataset\ndemonstrate that our proposed unsupervised deep staining method can generate\nstained images with more accurate positions and shapes of the cell targets in\nboth settings. Compared with competing methods, our method achieves improved\nresults both qualitatively and quantitatively (e.g., NIQE and PSNR).We applied\nour digital staining method to the White Blood Cell (WBC) dataset,\ninvestigating its potential for medical applications."}
{"id": "2504.08748", "pdf": "https://arxiv.org/pdf/2504.08748", "abs": "https://arxiv.org/abs/2504.08748", "authors": ["Lang Mei", "Siyu Mo", "Zhihan Yang", "Chong Chen"], "title": "A Survey of Multimodal Retrieval-Augmented Generation", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.ET", "cs.LG"], "comment": null, "summary": "Multimodal Retrieval-Augmented Generation (MRAG) enhances large language\nmodels (LLMs) by integrating multimodal data (text, images, videos) into\nretrieval and generation processes, overcoming the limitations of text-only\nRetrieval-Augmented Generation (RAG). While RAG improves response accuracy by\nincorporating external textual knowledge, MRAG extends this framework to\ninclude multimodal retrieval and generation, leveraging contextual information\nfrom diverse data types. This approach reduces hallucinations and enhances\nquestion-answering systems by grounding responses in factual, multimodal\nknowledge. Recent studies show MRAG outperforms traditional RAG, especially in\nscenarios requiring both visual and textual understanding. This survey reviews\nMRAG's essential components, datasets, evaluation methods, and limitations,\nproviding insights into its construction and improvement. It also identifies\nchallenges and future research directions, highlighting MRAG's potential to\nrevolutionize multimodal information retrieval and generation. By offering a\ncomprehensive perspective, this work encourages further exploration into this\npromising paradigm."}
{"id": "2504.09900", "pdf": "https://arxiv.org/pdf/2504.09900", "abs": "https://arxiv.org/abs/2504.09900", "authors": ["Muhammad Fasih Tariq", "Muhammad Azeem Javed"], "title": "Small Object Detection with YOLO: A Performance Analysis Across Model Versions and Hardware", "categories": ["cs.CV"], "comment": null, "summary": "This paper provides an extensive evaluation of YOLO object detection models\n(v5, v8, v9, v10, v11) by com- paring their performance across various hardware\nplatforms and optimization libraries. Our study investigates inference speed\nand detection accuracy on Intel and AMD CPUs using popular libraries such as\nONNX and OpenVINO, as well as on GPUs through TensorRT and other GPU-optimized\nframeworks. Furthermore, we analyze the sensitivity of these YOLO models to\nobject size within the image, examining performance when detecting objects that\noccupy 1%, 2.5%, and 5% of the total area of the image. By identifying the\ntrade-offs in efficiency, accuracy, and object size adaptability, this paper\noffers insights for optimal model selection based on specific hardware\nconstraints and detection requirements, aiding practitioners in deploying YOLO\nmodels effectively for real-world applications."}
{"id": "2504.08753", "pdf": "https://arxiv.org/pdf/2504.08753", "abs": "https://arxiv.org/abs/2504.08753", "authors": ["Jyothi", "T. Satyanarayana Murthy"], "title": "Domain Specific Question to SQL Conversion with Embedded Data Balancing Technique", "categories": ["cs.IR", "cs.CL", "cs.DB"], "comment": null, "summary": "The rise of deep learning in natural language processing has fostered the\ncreation of text to structured query language models composed of an encoder and\na decoder. Researchers have experimented with various intermediate processing\nlike schema linking, table type aware, value extract. To generate accurate SQL\nresults for the user question. However error analysis performed on the failed\ncases on these systems shows, 29 percentage of the errors would be because the\nsystem was unable to understand the values expressed by the user in their\nquestion. This challenge affects the generation of accurate SQL queries,\nespecially when dealing with domain-specific terms and specific value\nconditions, where traditional methods struggle to maintain consistency and\nprecision. To overcome these obstacles, proposed two intermediations like\nimplementing data balancing technique and over sampling domain-specific queries\nwhich would refine the model architecture to enhance value recognition and fine\ntuning the model for domain-specific questions. This proposed solution achieved\n10.98 percentage improvement in accuracy of the model performance compared to\nthe state of the art model tested on WikiSQL dataset. to convert the user\nquestion accurately to SQL queries. Applying oversampling technique on the\ndomain-specific questions shown a significant improvement as compared with\ntraditional approaches."}
{"id": "2504.09904", "pdf": "https://arxiv.org/pdf/2504.09904", "abs": "https://arxiv.org/abs/2504.09904", "authors": ["Mert Asim Karaoglu", "Wenbo Ji", "Ahmed Abbas", "Nassir Navab", "Benjamin Busam", "Alexander Ladikos"], "title": "LiteTracker: Leveraging Temporal Causality for Accurate Low-latency Tissue Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Tissue tracking plays a critical role in various surgical navigation and\nextended reality (XR) applications. While current methods trained on large\nsynthetic datasets achieve high tracking accuracy and generalize well to\nendoscopic scenes, their runtime performances fail to meet the low-latency\nrequirements necessary for real-time surgical applications. To address this\nlimitation, we propose LiteTracker, a low-latency method for tissue tracking in\nendoscopic video streams. LiteTracker builds on a state-of-the-art long-term\npoint tracking method, and introduces a set of training-free runtime\noptimizations. These optimizations enable online, frame-by-frame tracking by\nleveraging a temporal memory buffer for efficient feature reuse and utilizing\nprior motion for accurate track initialization. LiteTracker demonstrates\nsignificant runtime improvements being around 7x faster than its predecessor\nand 2x than the state-of-the-art. Beyond its primary focus on efficiency,\nLiteTracker delivers high-accuracy tracking and occlusion prediction,\nperforming competitively on both the STIR and SuPer datasets. We believe\nLiteTracker is an important step toward low-latency tissue tracking for\nreal-time surgical applications in the operating room."}
{"id": "2504.08763", "pdf": "https://arxiv.org/pdf/2504.08763", "abs": "https://arxiv.org/abs/2504.08763", "authors": ["Shiraj Pokharel", "Georg P. Roßrucker", "Mario M. Kubek"], "title": "WebMap -- Large Language Model-assisted Semantic Link Induction in the Web", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "68T50, 68T07", "I.2.6; I.2.7; H.3.3"], "comment": "11 pages, 3 figures, accepted at the 2024 24th International\n  Conference on Innovations for Community Services (I4CS), June 12 - 14,\n  Maastricht, The Netherlands, 2024", "summary": "Carrying out research tasks is only inadequately supported, if not hindered,\nby current web search engines. This paper therefore proposes functional\nextensions of WebMap, a semantically induced overlay linking structure on the\nweb to inherently facilitate research activities. These add-ons support the\ndynamic determination and regrouping of document clusters, the creation of a\nsemantic signpost in the web, and the interactive tracing of topics back to\ntheir origins."}
{"id": "2504.09914", "pdf": "https://arxiv.org/pdf/2504.09914", "abs": "https://arxiv.org/abs/2504.09914", "authors": ["Maria Tzelepi", "Vasileios Mezaris"], "title": "Improving Multimodal Hateful Meme Detection Exploiting LMM-Generated Knowledge", "categories": ["cs.CV"], "comment": "Accepted for publication, Multimodal Learning and Applications\n  Workshop (MULA 2025) @ IEEE/CVF CVPR 2025, Nashville, TN, USA, June 2025.\n  This is the authors' \"accepted version\"", "summary": "Memes have become a dominant form of communication in social media in recent\nyears. Memes are typically humorous and harmless, however there are also memes\nthat promote hate speech, being in this way harmful to individuals and groups\nbased on their identity. Therefore, detecting hateful content in memes has\nemerged as a task of critical importance. The need for understanding the\ncomplex interactions of images and their embedded text renders the hateful meme\ndetection a challenging multimodal task. In this paper we propose to address\nthe aforementioned task leveraging knowledge encoded in powerful Large\nMultimodal Models (LMM). Specifically, we propose to exploit LMMs in a two-fold\nmanner. First, by extracting knowledge oriented to the hateful meme detection\ntask in order to build strong meme representations. Specifically, generic\nsemantic descriptions and emotions that the images along with their embedded\ntexts elicit are extracted, which are then used to train a simple\nclassification head for hateful meme detection. Second, by developing a novel\nhard mining approach introducing directly LMM-encoded knowledge to the training\nprocess, providing further improvements. We perform extensive experiments on\ntwo datasets that validate the effectiveness of the proposed method, achieving\nstate-of-the-art performance. Our code and trained models are publicly\navailable at: https://github.com/IDT-ITI/LMM-CLIP-meme."}
{"id": "2504.08764", "pdf": "https://arxiv.org/pdf/2504.08764", "abs": "https://arxiv.org/abs/2504.08764", "authors": ["Chris Brogly", "Saif Rjaibi", "Charlotte Liang", "Erica Lam", "Edward Wang", "Adam Levitan", "Sarah Paleczny", "Michael Cusimano"], "title": "Evaluation of the phi-3-mini SLM for identification of texts related to medicine, health, and sports injuries", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Small Language Models (SLMs) have potential to be used for automatically\nlabelling and identifying aspects of text data for medicine/health-related\npurposes from documents and the web. As their resource requirements are\nsignificantly lower than Large Language Models (LLMs), these can be deployed\npotentially on more types of devices. SLMs often are benchmarked on\nhealth/medicine-related tasks, such as MedQA, although performance on these can\nvary especially depending on the size of the model in terms of number of\nparameters. Furthermore, these test results may not necessarily reflect\nreal-world performance regarding the automatic labelling or identification of\ntexts in documents and the web. As a result, we compared topic-relatedness\nscores from Microsofts phi-3-mini-4k-instruct SLM to the topic-relatedness\nscores from 7 human evaluators on 1144 samples of medical/health-related texts\nand 1117 samples of sports injury-related texts. These texts were from a larger\ndataset of about 9 million news headlines, each of which were processed and\nassigned scores by phi-3-mini-4k-instruct. Our sample was selected (filtered)\nbased on 1 (low filtering) or more (high filtering) Boolean conditions on the\nphi-3 SLM scores. We found low-moderate significant correlations between the\nscores from the SLM and human evaluators for sports injury texts with low\nfiltering (\\r{ho} = 0.3413, p < 0.001) and medicine/health texts with high\nfiltering (\\r{ho} = 0.3854, p < 0.001), and low significant correlation for\nmedicine/health texts with low filtering (\\r{ho} = 0.2255, p < 0.001). There\nwas negligible, insignificant correlation for sports injury-related texts with\nhigh filtering (\\r{ho} = 0.0318, p = 0.4466)."}
{"id": "2504.09925", "pdf": "https://arxiv.org/pdf/2504.09925", "abs": "https://arxiv.org/abs/2504.09925", "authors": ["Zheng Liu", "Mengjie Liu", "Jingzhou Chen", "Jingwei Xu", "Bin Cui", "Conghui He", "Wentao Zhang"], "title": "FUSION: Fully Integration of Vision-Language Representations for Deep Cross-Modal Understanding", "categories": ["cs.CV"], "comment": null, "summary": "We introduce FUSION, a family of multimodal large language models (MLLMs)\nwith a fully vision-language alignment and integration paradigm. Unlike\nexisting methods that primarily rely on late-stage modality interaction during\nLLM decoding, our approach achieves deep, dynamic integration throughout the\nentire processing pipeline. To this end, we propose Text-Guided Unified Vision\nEncoding, incorporating textual information in vision encoding to achieve\npixel-level integration. We further design Context-Aware Recursive Alignment\nDecoding that recursively aggregates visual features conditioned on textual\ncontext during decoding, enabling fine-grained, question-level semantic\nintegration. To guide feature mapping and mitigate modality discrepancies, we\ndevelop Dual-Supervised Semantic Mapping Loss. Additionally, we construct a\nSynthesized Language-Driven Question-Answer (QA) dataset through a new data\nsynthesis method, prioritizing high-quality QA pairs to optimize text-guided\nfeature integration. Building on these foundations, we train FUSION at two\nscales-3B, 8B-and demonstrate that our full-modality integration approach\nsignificantly outperforms existing methods with only 630 vision tokens.\nNotably, FUSION 3B surpasses Cambrian-1 8B and Florence-VL 8B on most\nbenchmarks. FUSION 3B continues to outperform Cambrian-1 8B even when limited\nto 300 vision tokens. Our ablation studies show that FUSION outperforms\nLLaVA-NeXT on over half of the benchmarks under same configuration without\ndynamic resolution, highlighting the effectiveness of our approach. We release\nour code, model weights, and dataset. https://github.com/starriver030515/FUSION"}
{"id": "2504.08777", "pdf": "https://arxiv.org/pdf/2504.08777", "abs": "https://arxiv.org/abs/2504.08777", "authors": ["Teo Susnjak", "Cole Palffy", "Tatiana Zimina", "Nazgul Altynbekova", "Kunal Garg", "Leona Gilbert"], "title": "The Lyme Disease Controversy: An AI-Driven Discourse Analysis of a Quarter Century of Academic Debate and Divides", "categories": ["cs.CY", "cs.CL"], "comment": null, "summary": "The scientific discourse surrounding Chronic Lyme Disease (CLD) and\nPost-Treatment Lyme Disease Syndrome (PTLDS) has evolved over the past\ntwenty-five years into a complex and polarised debate, shaped by shifting\nresearch priorities, institutional influences, and competing explanatory\nmodels. This study presents the first large-scale, systematic examination of\nthis discourse using an innovative hybrid AI-driven methodology, combining\nlarge language models with structured human validation to analyse thousands of\nscholarly abstracts spanning 25 years. By integrating Large Language Models\n(LLMs) with expert oversight, we developed a quantitative framework for\ntracking epistemic shifts in contested medical fields, with applications to\nother content analysis domains. Our analysis revealed a progressive transition\nfrom infection-based models of Lyme disease to immune-mediated explanations for\npersistent symptoms. This study offers new empirical insights into the\nstructural and epistemic forces shaping Lyme disease research, providing a\nscalable and replicable methodology for analysing discourse, while underscoring\nthe value of AI-assisted methodologies in social science and medical research."}
{"id": "2504.09948", "pdf": "https://arxiv.org/pdf/2504.09948", "abs": "https://arxiv.org/abs/2504.09948", "authors": ["Huijie Liu", "Bingcan Wang", "Jie Hu", "Xiaoming Wei", "Guoliang Kang"], "title": "Omni-Dish: Photorealistic and Faithful Image Generation and Editing for Arbitrary Chinese Dishes", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "10 pages, 10 figures, 3 tables", "summary": "Dish images play a crucial role in the digital era, with the demand for\nculturally distinctive dish images continuously increasing due to the\ndigitization of the food industry and e-commerce. In general cases, existing\ntext-to-image generation models excel in producing high-quality images;\nhowever, they struggle to capture diverse characteristics and faithful details\nof specific domains, particularly Chinese dishes. To address this limitation,\nwe propose Omni-Dish, the first text-to-image generation model specifically\ntailored for Chinese dishes. We develop a comprehensive dish curation pipeline,\nbuilding the largest dish dataset to date. Additionally, we introduce a\nrecaption strategy and employ a coarse-to-fine training scheme to help the\nmodel better learn fine-grained culinary nuances. During inference, we enhance\nthe user's textual input using a pre-constructed high-quality caption library\nand a large language model, enabling more photorealistic and faithful image\ngeneration. Furthermore, to extend our model's capability for dish editing\ntasks, we propose Concept-Enhanced P2P. Based on this approach, we build a dish\nediting dataset and train a specialized editing model. Extensive experiments\ndemonstrate the superiority of our methods."}
{"id": "2504.08801", "pdf": "https://arxiv.org/pdf/2504.08801", "abs": "https://arxiv.org/abs/2504.08801", "authors": ["Andrew Kiruluta", "Priscilla Burity", "Samantha Williams"], "title": "Learnable Multi-Scale Wavelet Transformer: A Novel Alternative to Self-Attention", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Transformer architectures, underpinned by the self-attention mechanism, have\nachieved state-of-the-art results across numerous natural language processing\n(NLP) tasks by effectively modeling long-range dependencies. However, the\ncomputational complexity of self-attention, scaling quadratically with input\nsequence length, presents significant challenges for processing very long\nsequences or operating under resource constraints. This paper introduces the\nLearnable Multi-Scale Wavelet Transformer (LMWT), a novel architecture that\nreplaces the standard dot-product self-attention with a learnable multi-scale\nHaar wavelet transform module. Leveraging the intrinsic multi-resolution\nproperties of wavelets, the LMWT efficiently captures both local details and\nglobal context. Crucially, the parameters of the wavelet transform, including\nscale-specific coefficients, are learned end-to-end during training, allowing\nthe model to adapt its decomposition strategy to the data and task. We present\nthe detailed mathematical formulation of the learnable Haar wavelet module and\nits integration into the transformer framework, supplemented by an\narchitectural diagram. We conduct a comprehensive experimental evaluation on a\nstandard machine translation benchmark (WMT16 En-De), comparing the LMWT\nagainst a baseline self-attention transformer using metrics like BLEU score,\nperplexity, and token accuracy. Furthermore, we analyze the computational\ncomplexity, highlighting the linear scaling of our approach, discuss its\nnovelty in the context of related work, and explore the interpretability\noffered by visualizing the learned Haar coefficients. Our results indicate that\nthe LMWT achieves competitive performance while offering substantial\ncomputational advantages, positioning it as a promising and novel alternative\nfor efficient sequence modeling."}
{"id": "2504.09953", "pdf": "https://arxiv.org/pdf/2504.09953", "abs": "https://arxiv.org/abs/2504.09953", "authors": ["Katja Ludwig", "Yuliia Oksymets", "Robin Schön", "Daniel Kienzle", "Rainer Lienhart"], "title": "Efficient 2D to Full 3D Human Pose Uplifting including Joint Rotations", "categories": ["cs.CV"], "comment": "accepted at CVSports@CVPR'25", "summary": "In sports analytics, accurately capturing both the 3D locations and rotations\nof body joints is essential for understanding an athlete's biomechanics. While\nHuman Mesh Recovery (HMR) models can estimate joint rotations, they often\nexhibit lower accuracy in joint localization compared to 3D Human Pose\nEstimation (HPE) models. Recent work addressed this limitation by combining a\n3D HPE model with inverse kinematics (IK) to estimate both joint locations and\nrotations. However, IK is computationally expensive. To overcome this, we\npropose a novel 2D-to-3D uplifting model that directly estimates 3D human\nposes, including joint rotations, in a single forward pass. We investigate\nmultiple rotation representations, loss functions, and training strategies -\nboth with and without access to ground truth rotations. Our models achieve\nstate-of-the-art accuracy in rotation estimation, are 150 times faster than the\nIK-based approach, and surpass HMR models in joint localization precision."}
{"id": "2504.08804", "pdf": "https://arxiv.org/pdf/2504.08804", "abs": "https://arxiv.org/abs/2504.08804", "authors": ["Pooya Razavi", "Sonya J. Powers"], "title": "Estimating Item Difficulty Using Large Language Models and Tree-Based Machine Learning Algorithms", "categories": ["cs.CY", "cs.CL", "cs.LG"], "comment": null, "summary": "Estimating item difficulty through field-testing is often resource-intensive\nand time-consuming. As such, there is strong motivation to develop methods that\ncan predict item difficulty at scale using only the item content. Large\nLanguage Models (LLMs) represent a new frontier for this goal. The present\nresearch examines the feasibility of using an LLM to predict item difficulty\nfor K-5 mathematics and reading assessment items (N = 5170). Two estimation\napproaches were implemented: (a) a direct estimation method that prompted the\nLLM to assign a single difficulty rating to each item, and (b) a feature-based\nstrategy where the LLM extracted multiple cognitive and linguistic features,\nwhich were then used in ensemble tree-based models (random forests and gradient\nboosting) to predict difficulty. Overall, direct LLM estimates showed moderate\nto strong correlations with true item difficulties. However, their accuracy\nvaried by grade level, often performing worse for early grades. In contrast,\nthe feature-based method yielded stronger predictive accuracy, with\ncorrelations as high as r = 0.87 and lower error estimates compared to both\ndirect LLM predictions and baseline regressors. These findings highlight the\npromise of LLMs in streamlining item development and reducing reliance on\nextensive field testing and underscore the importance of structured feature\nextraction. We provide a seven-step workflow for testing professionals who\nwould want to implement a similar item difficulty estimation approach with\ntheir item pool."}
{"id": "2504.09956", "pdf": "https://arxiv.org/pdf/2504.09956", "abs": "https://arxiv.org/abs/2504.09956", "authors": ["Katarzyna Filus", "Michał Romaszewski", "Mateusz Żarski"], "title": "Semantic Depth Matters: Explaining Errors of Deep Vision Networks through Perceived Class Similarities", "categories": ["cs.CV", "cs.LG", "68T01", "I.2.6"], "comment": null, "summary": "Understanding deep neural network (DNN) behavior requires more than\nevaluating classification accuracy alone; analyzing errors and their\npredictability is equally crucial. Current evaluation methodologies lack\ntransparency, particularly in explaining the underlying causes of network\nmisclassifications. To address this, we introduce a novel framework that\ninvestigates the relationship between the semantic hierarchy depth perceived by\na network and its real-data misclassification patterns. Central to our\nframework is the Similarity Depth (SD) metric, which quantifies the semantic\nhierarchy depth perceived by a network along with a method of evaluation of how\nclosely the network's errors align with its internally perceived similarity\nstructure. We also propose a graph-based visualization of model semantic\nrelationships and misperceptions. A key advantage of our approach is that\nleveraging class templates -- representations derived from classifier layer\nweights -- is applicable to already trained networks without requiring\nadditional data or experiments. Our approach reveals that deep vision networks\nencode specific semantic hierarchies and that high semantic depth improves the\ncompliance between perceived class similarities and actual errors."}
{"id": "2504.08812", "pdf": "https://arxiv.org/pdf/2504.08812", "abs": "https://arxiv.org/abs/2504.08812", "authors": ["David O. Johnston", "Arkajyoti Chakraborty", "Nora Belrose"], "title": "Mechanistic Anomaly Detection for \"Quirky\" Language Models", "categories": ["cs.LG", "cs.CL"], "comment": "ICLR Building Trust Workshop 2025", "summary": "As LLMs grow in capability, the task of supervising LLMs becomes more\nchallenging. Supervision failures can occur if LLMs are sensitive to factors\nthat supervisors are unaware of. We investigate Mechanistic Anomaly Detection\n(MAD) as a technique to augment supervision of capable models; we use internal\nmodel features to identify anomalous training signals so they can be\ninvestigated or discarded. We train detectors to flag points from the test\nenvironment that differ substantially from the training environment, and\nexperiment with a large variety of detector features and scoring rules to\ndetect anomalies in a set of ``quirky'' language models. We find that detectors\ncan achieve high discrimination on some tasks, but no detector is effective\nacross all models and tasks. MAD techniques may be effective in low-stakes\napplications, but advances in both detection and evaluation are likely needed\nif they are to be used in high stakes settings."}
{"id": "2504.09960", "pdf": "https://arxiv.org/pdf/2504.09960", "abs": "https://arxiv.org/abs/2504.09960", "authors": ["Hoang M. Truong", "Vinh-Thuan Ly", "Huy G. Tran", "Thuan-Phat Nguyen", "Tram T. Doan"], "title": "Dual-Path Enhancements in Event-Based Eye Tracking: Augmented Robustness and Adaptive Temporal Modeling", "categories": ["cs.CV"], "comment": "Camera-ready version for CVPRW 2025. Accepted for presentation at the\n  IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops\n  (CVPRW 2025)", "summary": "Event-based eye tracking has become a pivotal technology for augmented\nreality and human-computer interaction. Yet, existing methods struggle with\nreal-world challenges such as abrupt eye movements and environmental noise.\nBuilding on the efficiency of the Lightweight Spatiotemporal Network-a causal\narchitecture optimized for edge devices-we introduce two key advancements.\nFirst, a robust data augmentation pipeline incorporating temporal shift,\nspatial flip, and event deletion improves model resilience, reducing Euclidean\ndistance error by 12% (1.61 vs. 1.70 baseline) on challenging samples. Second,\nwe propose KnightPupil, a hybrid architecture combining an EfficientNet-B3\nbackbone for spatial feature extraction, a bidirectional GRU for contextual\ntemporal modeling, and a Linear Time-Varying State-Space Module to adapt to\nsparse inputs and noise dynamically. Evaluated on the 3ET+ benchmark, our\nframework achieved 1.61 Euclidean distance on the private test set of the\nEvent-based Eye Tracking Challenge at CVPR 2025, demonstrating its\neffectiveness for practical deployment in AR/VR systems while providing a\nfoundation for future innovations in neuromorphic vision."}
{"id": "2504.08846", "pdf": "https://arxiv.org/pdf/2504.08846", "abs": "https://arxiv.org/abs/2504.08846", "authors": ["Mostafa Faghih Shojaei", "Rahul Gulati", "Benjamin A. Jasperson", "Shangshang Wang", "Simone Cimolato", "Dangli Cao", "Willie Neiswanger", "Krishna Garikipati"], "title": "AI-University: An LLM-based platform for instructional alignment to scientific classrooms", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "comment": "10 pages, 3 figures", "summary": "We introduce AI University (AI-U), a flexible framework for AI-driven course\ncontent delivery that adapts to instructors' teaching styles. At its core, AI-U\nfine-tunes a large language model (LLM) with retrieval-augmented generation\n(RAG) to generate instructor-aligned responses from lecture videos, notes, and\ntextbooks. Using a graduate-level finite-element-method (FEM) course as a case\nstudy, we present a scalable pipeline to systematically construct training\ndata, fine-tune an open-source LLM with Low-Rank Adaptation (LoRA), and\noptimize its responses through RAG-based synthesis. Our evaluation - combining\ncosine similarity, LLM-based assessment, and expert review - demonstrates\nstrong alignment with course materials. We also have developed a prototype web\napplication, available at https://my-ai-university.com, that enhances\ntraceability by linking AI-generated responses to specific sections of the\nrelevant course material and time-stamped instances of the open-access video\nlectures. Our expert model is found to have greater cosine similarity with a\nreference on 86% of test cases. An LLM judge also found our expert model to\noutperform the base Llama 3.2 model approximately four times out of five. AI-U\noffers a scalable approach to AI-assisted education, paving the way for broader\nadoption in higher education. Here, our framework has been presented in the\nsetting of a class on FEM - a subject that is central to training PhD and\nMaster students in engineering science. However, this setting is a particular\ninstance of a broader context: fine-tuning LLMs to research content in science."}
{"id": "2504.09966", "pdf": "https://arxiv.org/pdf/2504.09966", "abs": "https://arxiv.org/abs/2504.09966", "authors": ["Dongliang Luo", "Hanshen Zhu", "Ziyang Zhang", "Dingkang Liang", "Xudong Xie", "Yuliang Liu", "Xiang Bai"], "title": "SemiETS: Integrating Spatial and Content Consistencies for Semi-Supervised End-to-end Text Spotting", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025. Code will be available at\n  \\url{https://github.com/DrLuo/SemiETS}", "summary": "Most previous scene text spotting methods rely on high-quality manual\nannotations to achieve promising performance. To reduce their expensive costs,\nwe study semi-supervised text spotting (SSTS) to exploit useful information\nfrom unlabeled images. However, directly applying existing semi-supervised\nmethods of general scenes to SSTS will face new challenges: 1) inconsistent\npseudo labels between detection and recognition tasks, and 2) sub-optimal\nsupervisions caused by inconsistency between teacher/student. Thus, we propose\na new Semi-supervised framework for End-to-end Text Spotting, namely SemiETS\nthat leverages the complementarity of text detection and recognition.\nSpecifically, it gradually generates reliable hierarchical pseudo labels for\neach task, thereby reducing noisy labels. Meanwhile, it extracts important\ninformation in locations and transcriptions from bidirectional flows to improve\nconsistency. Extensive experiments on three datasets under various settings\ndemonstrate the effectiveness of SemiETS on arbitrary-shaped text. For example,\nit outperforms previous state-of-the-art SSL methods by a large margin on\nend-to-end spotting (+8.7%, +5.6%, and +2.6% H-mean under 0.5%, 1%, and 2%\nlabeled data settings on Total-Text, respectively). More importantly, it still\nimproves upon a strongly supervised text spotter trained with plenty of labeled\ndata by 2.0%. Compelling domain adaptation ability shows practical potential.\nMoreover, our method demonstrates consistent improvement on different text\nspotters."}
{"id": "2504.08907", "pdf": "https://arxiv.org/pdf/2504.08907", "abs": "https://arxiv.org/abs/2504.08907", "authors": ["Ayushi Mishra", "Yang Bai", "Priyadarshan Narayanasamy", "Nakul Garg", "Nirupam Roy"], "title": "Spatial Audio Processing with Large Language Model on Wearable Devices", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "Integrating spatial context into large language models (LLMs) has the\npotential to revolutionize human-computer interaction, particularly in wearable\ndevices. In this work, we present a novel system architecture that incorporates\nspatial speech understanding into LLMs, enabling contextually aware and\nadaptive applications for wearable technologies. Our approach leverages\nmicrostructure-based spatial sensing to extract precise Direction of Arrival\n(DoA) information using a monaural microphone. To address the lack of existing\ndataset for microstructure-assisted speech recordings, we synthetically create\na dataset called OmniTalk by using the LibriSpeech dataset. This spatial\ninformation is fused with linguistic embeddings from OpenAI's Whisper model,\nallowing each modality to learn complementary contextual representations. The\nfused embeddings are aligned with the input space of LLaMA-3.2 3B model and\nfine-tuned with lightweight adaptation technique LoRA to optimize for on-device\nprocessing. SING supports spatially-aware automatic speech recognition (ASR),\nachieving a mean error of $25.72^\\circ$-a substantial improvement compared to\nthe 88.52$^\\circ$ median error in existing work-with a word error rate (WER) of\n5.3. SING also supports soundscaping, for example, inference how many people\nwere talking and their directions, with up to 5 people and a median DoA error\nof 16$^\\circ$. Our system demonstrates superior performance in spatial speech\nunderstanding while addressing the challenges of power efficiency, privacy, and\nhardware constraints, paving the way for advanced applications in augmented\nreality, accessibility, and immersive experiences."}
{"id": "2504.09967", "pdf": "https://arxiv.org/pdf/2504.09967", "abs": "https://arxiv.org/abs/2504.09967", "authors": ["Xun Zhu", "Fanbin Mo", "Zheng Zhang", "Jiaxi Wang", "Yiming Shi", "Ming Wu", "Chuang Zhang", "Miao Li", "Ji Wu"], "title": "Enhancing Multi-task Learning Capability of Medical Generalist Foundation Model via Image-centric Multi-annotation Data", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The emergence of medical generalist foundation models has revolutionized\nconventional task-specific model development paradigms, aiming to better handle\nmultiple tasks through joint training on large-scale medical datasets. However,\nrecent advances prioritize simple data scaling or architectural component\nenhancement, while neglecting to re-examine multi-task learning from a\ndata-centric perspective. Critically, simply aggregating existing data\nresources leads to decentralized image-task alignment, which fails to cultivate\ncomprehensive image understanding or align with clinical needs for\nmulti-dimensional image interpretation. In this paper, we introduce the\nimage-centric multi-annotation X-ray dataset (IMAX), the first attempt to\nenhance the multi-task learning capabilities of medical multi-modal large\nlanguage models (MLLMs) from the data construction level. To be specific, IMAX\nis featured from the following attributes: 1) High-quality data curation. A\ncomprehensive collection of more than 354K entries applicable to seven\ndifferent medical tasks. 2) Image-centric dense annotation. Each X-ray image is\nassociated with an average of 4.10 tasks and 7.46 training entries, ensuring\nmulti-task representation richness per image. Compared to the general\ndecentralized multi-annotation X-ray dataset (DMAX), IMAX consistently\ndemonstrates significant multi-task average performance gains ranging from\n3.20% to 21.05% across seven open-source state-of-the-art medical MLLMs.\nMoreover, we investigate differences in statistical patterns exhibited by IMAX\nand DMAX training processes, exploring potential correlations between\noptimization dynamics and multi-task performance. Finally, leveraging the core\nconcept of IMAX data construction, we propose an optimized DMAX-based training\nstrategy to alleviate the dilemma of obtaining high-quality IMAX data in\npractical scenarios."}
{"id": "2504.08942", "pdf": "https://arxiv.org/pdf/2504.08942", "abs": "https://arxiv.org/abs/2504.08942", "authors": ["Xing Han Lù", "Amirhossein Kazemnejad", "Nicholas Meade", "Arkil Patel", "Dongchan Shin", "Alejandra Zambrano", "Karolina Stańczak", "Peter Shaw", "Christopher J. Pal", "Siva Reddy"], "title": "AgentRewardBench: Evaluating Automatic Evaluations of Web Agent Trajectories", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Web agents enable users to perform tasks on web browsers through natural\nlanguage interaction. Evaluating web agents trajectories is an important\nproblem, since it helps us determine whether the agent successfully completed\nthe tasks. Rule-based methods are widely used for this purpose, but they are\nchallenging to extend to new tasks and may not always recognize successful\ntrajectories. We may achieve higher accuracy through human evaluation, but the\nprocess would be substantially slower and more expensive. Automatic evaluations\nwith LLMs may avoid the challenges of designing new rules and manually\nannotating trajectories, enabling faster and cost-effective evaluation.\nHowever, it is unclear how effective they are at evaluating web agents. To this\nend, we propose AgentRewardBench, the first benchmark to assess the\neffectiveness of LLM judges for evaluating web agents. AgentRewardBench\ncontains 1302 trajectories across 5 benchmarks and 4 LLMs. Each trajectory in\nAgentRewardBench is reviewed by an expert, who answers questions pertaining to\nthe success, side effects, and repetitiveness of the agent. Using our\nbenchmark, we evaluate 12 LLM judges and find that no single LLM excels across\nall benchmarks. We also find that the rule-based evaluation used by common\nbenchmarks tends to underreport the success rate of web agents, highlighting a\nkey weakness of rule-based evaluation and the need to develop more flexible\nautomatic evaluations. We release the benchmark at:\nhttps://agent-reward-bench.github.io"}
{"id": "2504.09973", "pdf": "https://arxiv.org/pdf/2504.09973", "abs": "https://arxiv.org/abs/2504.09973", "authors": ["Gang Wu", "Junjun Jiang", "Kui Jiang", "Xianming Liu", "Liqiang Nie"], "title": "Beyond Degradation Redundancy: Contrastive Prompt Learning for All-in-One Image Restoration", "categories": ["cs.CV"], "comment": "Project page: https://github.com/Aitical/CPLIR", "summary": "All-in-one image restoration, addressing diverse degradation types with a\nunified model, presents significant challenges in designing task-specific\nprompts that effectively guide restoration across multiple degradation\nscenarios. While adaptive prompt learning enables end-to-end optimization, it\noften yields overlapping or redundant task representations. Conversely,\nexplicit prompts derived from pretrained classifiers enhance discriminability\nbut may discard critical visual information for reconstruction. To address\nthese limitations, we introduce Contrastive Prompt Learning (CPL), a novel\nframework that fundamentally enhances prompt-task alignment through two\ncomplementary innovations: a \\emph{Sparse Prompt Module (SPM)} that efficiently\ncaptures degradation-specific features while minimizing redundancy, and a\n\\emph{Contrastive Prompt Regularization (CPR)} that explicitly strengthens task\nboundaries by incorporating negative prompt samples across different\ndegradation types. Unlike previous approaches that focus primarily on\ndegradation classification, CPL optimizes the critical interaction between\nprompts and the restoration model itself. Extensive experiments across five\ncomprehensive benchmarks demonstrate that CPL consistently enhances\nstate-of-the-art all-in-one restoration models, achieving significant\nimprovements in both standard multi-task scenarios and challenging composite\ndegradation settings. Our framework establishes new state-of-the-art\nperformance while maintaining parameter efficiency, offering a principled\nsolution for unified image restoration."}
{"id": "2504.08949", "pdf": "https://arxiv.org/pdf/2504.08949", "abs": "https://arxiv.org/abs/2504.08949", "authors": ["Haokai Ma", "Yunshan Ma", "Ruobing Xie", "Lei Meng", "Jialie Shen", "Xingwu Sun", "Zhanhui Kang", "Tat-Seng Chua"], "title": "Large Language Model Empowered Recommendation Meets All-domain Continual Pre-Training", "categories": ["cs.IR", "cs.CL"], "comment": "In submission", "summary": "Recent research efforts have investigated how to integrate Large Language\nModels (LLMs) into recommendation, capitalizing on their semantic comprehension\nand open-world knowledge for user behavior understanding. These approaches\npredominantly employ supervised fine-tuning on single-domain user interactions\nto adapt LLMs for specific recommendation tasks. However, they typically\nencounter dual challenges: the mismatch between general language\nrepresentations and domain-specific preference patterns, as well as the limited\nadaptability to multi-domain recommendation scenarios. To bridge these gaps, we\nintroduce CPRec -- an All-domain Continual Pre-Training framework for\nRecommendation -- designed to holistically align LLMs with universal user\nbehaviors through the continual pre-training paradigm. Specifically, we first\ndesign a unified prompt template and organize users' multi-domain behaviors\ninto domain-specific behavioral sequences and all-domain mixed behavioral\nsequences that emulate real-world user decision logic. To optimize behavioral\nknowledge infusion, we devise a Warmup-Stable-Annealing learning rate schedule\ntailored for the continual pre-training paradigm in recommendation to\nprogressively enhance the LLM's capability in knowledge adaptation from\nopen-world knowledge to universal recommendation tasks. To evaluate the\neffectiveness of our CPRec, we implement it on a large-scale dataset covering\nseven domains and conduct extensive experiments on five real-world datasets\nfrom two distinct platforms. Experimental results confirm that our continual\npre-training paradigm significantly mitigates the semantic-behavioral\ndiscrepancy and achieves state-of-the-art performance in all recommendation\nscenarios. The source code will be released upon acceptance."}
{"id": "2504.09979", "pdf": "https://arxiv.org/pdf/2504.09979", "abs": "https://arxiv.org/abs/2504.09979", "authors": ["Teppei Suzuki", "Keisuke Ozawa"], "title": "Resampling Benchmark for Efficient Comprehensive Evaluation of Large Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "We propose an efficient evaluation protocol for large vision-language models\n(VLMs). Given their broad knowledge and reasoning capabilities, multiple\nbenchmarks are needed for comprehensive assessment, making evaluation\ncomputationally expensive. To improve efficiency, we construct a subset that\nyields results comparable to full benchmark evaluations. Our benchmark\nclassification experiments reveal that no single benchmark fully covers all\nchallenges. We then introduce a subset construction method using farthest point\nsampling (FPS). Our experiments show that FPS-based benchmarks maintain a\nstrong correlation (> 0.96) with full evaluations while using only ~1\\% of the\ndata. Additionally, applying FPS to an existing benchmark improves correlation\nwith overall evaluation results, suggesting its potential to reduce unintended\ndataset biases."}
{"id": "2504.09037", "pdf": "https://arxiv.org/pdf/2504.09037", "abs": "https://arxiv.org/abs/2504.09037", "authors": ["Zixuan Ke", "Fangkai Jiao", "Yifei Ming", "Xuan-Phi Nguyen", "Austin Xu", "Do Xuan Long", "Minzhi Li", "Chengwei Qin", "Peifeng Wang", "Silvio Savarese", "Caiming Xiong", "Shafiq Joty"], "title": "A Survey of Frontiers in LLM Reasoning: Inference Scaling, Learning to Reason, and Agentic Systems", "categories": ["cs.AI", "cs.CL"], "comment": "72 pages, 6 figures", "summary": "Reasoning is a fundamental cognitive process that enables logical inference,\nproblem-solving, and decision-making. With the rapid advancement of large\nlanguage models (LLMs), reasoning has emerged as a key capability that\ndistinguishes advanced AI systems from conventional models that empower\nchatbots. In this survey, we categorize existing methods along two orthogonal\ndimensions: (1) Regimes, which define the stage at which reasoning is achieved\n(either at inference time or through dedicated training); and (2)\nArchitectures, which determine the components involved in the reasoning\nprocess, distinguishing between standalone LLMs and agentic compound systems\nthat incorporate external tools, and multi-agent collaborations. Within each\ndimension, we analyze two key perspectives: (1) Input level, which focuses on\ntechniques that construct high-quality prompts that the LLM condition on; and\n(2) Output level, which methods that refine multiple sampled candidates to\nenhance reasoning quality. This categorization provides a systematic\nunderstanding of the evolving landscape of LLM reasoning, highlighting emerging\ntrends such as the shift from inference-scaling to learning-to-reason (e.g.,\nDeepSeek-R1), and the transition to agentic workflows (e.g., OpenAI Deep\nResearch, Manus Agent). Additionally, we cover a broad spectrum of learning\nalgorithms, from supervised fine-tuning to reinforcement learning such as PPO\nand GRPO, and the training of reasoners and verifiers. We also examine key\ndesigns of agentic workflows, from established patterns like\ngenerator-evaluator and LLM debate to recent innovations. ..."}
{"id": "2504.09990", "pdf": "https://arxiv.org/pdf/2504.09990", "abs": "https://arxiv.org/abs/2504.09990", "authors": ["LeiLei Ma", "Shuo Xu", "MingKun Xie", "Lei Wang", "Dengdi Sun", "Haifeng Zhao"], "title": "Correlative and Discriminative Label Grouping for Multi-Label Visual Prompt Tuning", "categories": ["cs.CV"], "comment": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n  2025", "summary": "Modeling label correlations has always played a pivotal role in multi-label\nimage classification (MLC), attracting significant attention from researchers.\nHowever, recent studies have overemphasized co-occurrence relationships among\nlabels, which can lead to overfitting risk on this overemphasis, resulting in\nsuboptimal models. To tackle this problem, we advocate for balancing\ncorrelative and discriminative relationships among labels to mitigate the risk\nof overfitting and enhance model performance. To this end, we propose the\nMulti-Label Visual Prompt Tuning framework, a novel and parameter-efficient\nmethod that groups classes into multiple class subsets according to label\nco-occurrence and mutual exclusivity relationships, and then models them\nrespectively to balance the two relationships. In this work, since each group\ncontains multiple classes, multiple prompt tokens are adopted within Vision\nTransformer (ViT) to capture the correlation or discriminative label\nrelationship within each group, and effectively learn correlation or\ndiscriminative representations for class subsets. On the other hand, each group\ncontains multiple group-aware visual representations that may correspond to\nmultiple classes, and the mixture of experts (MoE) model can cleverly assign\nthem from the group-aware to the label-aware, adaptively obtaining label-aware\nrepresentation, which is more conducive to classification. Experiments on\nmultiple benchmark datasets show that our proposed approach achieves\ncompetitive results and outperforms SOTA methods on multiple pre-trained\nmodels."}
{"id": "2504.09058", "pdf": "https://arxiv.org/pdf/2504.09058", "abs": "https://arxiv.org/abs/2504.09058", "authors": ["Chengyuan Liu", "Shihang Wang", "Lizhi Qing", "Kaisong Song", "Junjie Cao", "Jun Lin", "Ji Zhang", "Ang Li", "Kun Kuang", "Fei Wu"], "title": "Towards Stepwise Domain Knowledge-Driven Reasoning Optimization and Reflection Improvement", "categories": ["cs.AI", "cs.CL"], "comment": "Under review", "summary": "Recently, stepwise supervision on Chain of Thoughts (CoTs) presents an\nenhancement on the logical reasoning tasks such as coding and math, with the\nhelp of Monte Carlo Tree Search (MCTS). However, its contribution to tasks\nrequiring domain-specific expertise and knowledge remains unexplored. Motivated\nby the interest, we identify several potential challenges of vanilla MCTS\nwithin this context, and propose the framework of Stepwise Domain\nKnowledge-Driven Reasoning Optimization, employing the MCTS algorithm to\ndevelop step-level supervision for problems that require essential\ncomprehension, reasoning, and specialized knowledge. Additionally, we also\nintroduce the Preference Optimization towards Reflection Paths, which\niteratively learns self-reflection on the reasoning thoughts from better\nperspectives. We have conducted extensive experiments to evaluate the advantage\nof the methodologies. Empirical results demonstrate the effectiveness on\nvarious legal-domain problems. We also report a diverse set of valuable\nfindings, hoping to encourage the enthusiasm to the research of domain-specific\nLLMs and MCTS."}
{"id": "2504.09998", "pdf": "https://arxiv.org/pdf/2504.09998", "abs": "https://arxiv.org/abs/2504.09998", "authors": ["Alejandro Luque-Cerpa", "Elizabeth Polgreen", "Ajitha Rajan", "Hazem Torfah"], "title": "Metric-Guided Synthesis of Class Activation Mapping", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Class activation mapping (CAM) is a widely adopted class of saliency methods\nused to explain the behavior of convolutional neural networks (CNNs). These\nmethods generate heatmaps that highlight the parts of the input most relevant\nto the CNN output. Various CAM methods have been proposed, each distinguished\nby the expressions used to derive heatmaps. In general, users look for heatmaps\nwith specific properties that reflect different aspects of CNN functionality.\nThese may include similarity to ground truth, robustness, equivariance, and\nmore. Although existing CAM methods implicitly encode some of these properties\nin their expressions, they do not allow for variability in heatmap generation\nfollowing the user's intent or domain knowledge. In this paper, we address this\nlimitation by introducing SyCAM, a metric-based approach for synthesizing CAM\nexpressions. Given a predefined evaluation metric for saliency maps, SyCAM\nautomatically generates CAM expressions optimized for that metric. We\nspecifically explore a syntax-guided synthesis instantiation of SyCAM, where\nCAM expressions are derived based on predefined syntactic constraints and the\ngiven metric. Using several established evaluation metrics, we demonstrate the\nefficacy and flexibility of our approach in generating targeted heatmaps. We\ncompare SyCAM with other well-known CAM methods on three prominent models:\nResNet50, VGG16, and VGG19."}
{"id": "2504.09081", "pdf": "https://arxiv.org/pdf/2504.09081", "abs": "https://arxiv.org/abs/2504.09081", "authors": ["Prabhat Pandey", "Rupak Vignesh Swaminathan", "K V Vijay Girish", "Arunasish Sen", "Jian Xie", "Grant P. Strimel", "Andreas Schwarz"], "title": "SIFT-50M: A Large-Scale Multilingual Dataset for Speech Instruction Fine-Tuning", "categories": ["eess.AS", "cs.AI", "cs.CL"], "comment": null, "summary": "We introduce SIFT (Speech Instruction Fine-Tuning), a 50M-example dataset\ndesigned for instruction fine-tuning and pre-training of speech-text large\nlanguage models (LLMs). SIFT-50M is built from publicly available speech\ncorpora, which collectively contain 14K hours of speech, and leverages LLMs\nalong with off-the-shelf expert models. The dataset spans five languages,\nencompassing a diverse range of speech understanding as well as controllable\nspeech generation instructions. Using SIFT-50M, we train SIFT-LLM, which\noutperforms existing speech-text LLMs on instruction-following benchmarks while\nachieving competitive performance on foundational speech tasks. To support\nfurther research, we also introduce EvalSIFT, a benchmark dataset specifically\ndesigned to evaluate the instruction-following capabilities of speech-text\nLLMs."}
{"id": "2504.10001", "pdf": "https://arxiv.org/pdf/2504.10001", "abs": "https://arxiv.org/abs/2504.10001", "authors": ["Junlin Hao", "Peiheng Wang", "Haoyang Wang", "Xinggong Zhang", "Zongming Guo"], "title": "GaussVideoDreamer: 3D Scene Generation with Video Diffusion and Inconsistency-Aware Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Single-image 3D scene reconstruction presents significant challenges due to\nits inherently ill-posed nature and limited input constraints. Recent advances\nhave explored two promising directions: multiview generative models that train\non 3D consistent datasets but struggle with out-of-distribution generalization,\nand 3D scene inpainting and completion frameworks that suffer from cross-view\ninconsistency and suboptimal error handling, as they depend exclusively on\ndepth data or 3D smoothness, which ultimately degrades output quality and\ncomputational performance. Building upon these approaches, we present\nGaussVideoDreamer, which advances generative multimedia approaches by bridging\nthe gap between image, video, and 3D generation, integrating their strengths\nthrough two key innovations: (1) A progressive video inpainting strategy that\nharnesses temporal coherence for improved multiview consistency and faster\nconvergence. (2) A 3D Gaussian Splatting consistency mask to guide the video\ndiffusion with 3D consistent multiview evidence. Our pipeline combines three\ncore components: a geometry-aware initialization protocol, Inconsistency-Aware\nGaussian Splatting, and a progressive video inpainting strategy. Experimental\nresults demonstrate that our approach achieves 32% higher LLaVA-IQA scores and\nat least 2x speedup compared to existing methods while maintaining robust\nperformance across diverse scenes."}
{"id": "2504.09100", "pdf": "https://arxiv.org/pdf/2504.09100", "abs": "https://arxiv.org/abs/2504.09100", "authors": ["Chengyu Wang", "Taolin Zhang", "Richang Hong", "Jun Huang"], "title": "A Short Survey on Small Reasoning Models: Training, Inference, Applications and Research Directions", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recently, the reasoning capabilities of large reasoning models (LRMs), such\nas DeepSeek-R1, have seen significant advancements through the slow thinking\nprocess. Despite these achievements, the substantial computational demands of\nLRMs present considerable challenges. In contrast, small reasoning models\n(SRMs), often distilled from larger ones, offer greater efficiency and can\nexhibit distinct capabilities and cognitive trajectories compared to LRMs. This\nwork surveys around 170 recently published papers on SRMs for tackling various\ncomplex reasoning tasks. We review the current landscape of SRMs and analyze\ndiverse training and inference techniques related to SRMs. Furthermore, we\nprovide a comprehensive review of SRMs for domain-specific applications and\ndiscuss possible future research directions. This survey serves as an essential\nreference for researchers to leverage or develop SRMs for advanced reasoning\nfunctionalities with high efficiency."}
{"id": "2504.10004", "pdf": "https://arxiv.org/pdf/2504.10004", "abs": "https://arxiv.org/abs/2504.10004", "authors": ["Matías Piqueras", "Alexandra Segerberg", "Matteo Magnani", "Måns Magnusson", "Nataša Sladoje"], "title": "An Image is Worth $K$ Topics: A Visual Structural Topic Model with Pretrained Image Embeddings", "categories": ["cs.CV", "cs.CY", "stat.AP", "stat.ME"], "comment": null, "summary": "Political scientists are increasingly interested in analyzing visual content\nat scale. However, the existing computational toolbox is still in need of\nmethods and models attuned to the specific challenges and goals of social and\npolitical inquiry. In this article, we introduce a visual Structural Topic\nModel (vSTM) that combines pretrained image embeddings with a structural topic\nmodel. This has important advantages compared to existing approaches. First,\npretrained embeddings allow the model to capture the semantic complexity of\nimages relevant to political contexts. Second, the structural topic model\nprovides the ability to analyze how topics and covariates are related, while\nmaintaining a nuanced representation of images as a mixture of multiple topics.\nIn our empirical application, we show that the vSTM is able to identify topics\nthat are interpretable, coherent, and substantively relevant to the study of\nonline political communication."}
{"id": "2504.09257", "pdf": "https://arxiv.org/pdf/2504.09257", "abs": "https://arxiv.org/abs/2504.09257", "authors": ["Sohom Ghosh", "Arnab Maji", "Sudip Kumar Naskar"], "title": "MiMIC: Multi-Modal Indian Earnings Calls Dataset to Predict Stock Prices", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Code and Dataset:\n  https://huggingface.co/datasets/sohomghosh/MiMIC_Multi-Modal_Indian_Earnings_Calls_Dataset", "summary": "Predicting stock market prices following corporate earnings calls remains a\nsignificant challenge for investors and researchers alike, requiring innovative\napproaches that can process diverse information sources. This study\ninvestigates the impact of corporate earnings calls on stock prices by\nintroducing a multi-modal predictive model. We leverage textual data from\nearnings call transcripts, along with images and tables from accompanying\npresentations, to forecast stock price movements on the trading day immediately\nfollowing these calls. To facilitate this research, we developed the MiMIC\n(Multi-Modal Indian Earnings Calls) dataset, encompassing companies\nrepresenting the Nifty 50, Nifty MidCap 50, and Nifty Small 50 indices. The\ndataset includes earnings call transcripts, presentations, fundamentals,\ntechnical indicators, and subsequent stock prices. We present a multimodal\nanalytical framework that integrates quantitative variables with predictive\nsignals derived from textual and visual modalities, thereby enabling a holistic\napproach to feature representation and analysis. This multi-modal approach\ndemonstrates the potential for integrating diverse information sources to\nenhance financial forecasting accuracy. To promote further research in\ncomputational economics, we have made the MiMIC dataset publicly available\nunder the CC-NC-SA-4.0 licence. Our work contributes to the growing body of\nliterature on market reactions to corporate communications and highlights the\nefficacy of multi-modal machine learning techniques in financial analysis."}
{"id": "2504.10012", "pdf": "https://arxiv.org/pdf/2504.10012", "abs": "https://arxiv.org/abs/2504.10012", "authors": ["Yufei Deng", "Yuanjian Wang", "Rong Xiao", "Chenwei Tang", "Jizhe Zhou", "Jiahao Fan", "Deng Xiong", "Jiancheng Lv", "Huajin Tang"], "title": "EBAD-Gaussian: Event-driven Bundle Adjusted Deblur Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "While 3D Gaussian Splatting (3D-GS) achieves photorealistic novel view\nsynthesis, its performance degrades with motion blur. In scenarios with rapid\nmotion or low-light conditions, existing RGB-based deblurring methods struggle\nto model camera pose and radiance changes during exposure, reducing\nreconstruction accuracy. Event cameras, capturing continuous brightness changes\nduring exposure, can effectively assist in modeling motion blur and improving\nreconstruction quality. Therefore, we propose Event-driven Bundle Adjusted\nDeblur Gaussian Splatting (EBAD-Gaussian), which reconstructs sharp 3D\nGaussians from event streams and severely blurred images. This method jointly\nlearns the parameters of these Gaussians while recovering camera motion\ntrajectories during exposure time. Specifically, we first construct a blur loss\nfunction by synthesizing multiple latent sharp images during the exposure time,\nminimizing the difference between real and synthesized blurred images. Then we\nuse event stream to supervise the light intensity changes between latent sharp\nimages at any time within the exposure period, supplementing the light\nintensity dynamic changes lost in RGB images. Furthermore, we optimize the\nlatent sharp images at intermediate exposure times based on the event-based\ndouble integral (EDI) prior, applying consistency constraints to enhance the\ndetails and texture information of the reconstructed images. Extensive\nexperiments on synthetic and real-world datasets show that EBAD-Gaussian can\nachieve high-quality 3D scene reconstruction under the condition of blurred\nimages and event stream inputs."}
{"id": "2504.09265", "pdf": "https://arxiv.org/pdf/2504.09265", "abs": "https://arxiv.org/abs/2504.09265", "authors": ["Lei Kang", "Jia Li", "Mi Tian", "Hua Huang"], "title": "Mixture of Group Experts for Learning Invariant Representations", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Sparsely activated Mixture-of-Experts (MoE) models effectively increase the\nnumber of parameters while maintaining consistent computational costs per\ntoken. However, vanilla MoE models often suffer from limited diversity and\nspecialization among experts, constraining their performance and scalability,\nespecially as the number of experts increases. In this paper, we present a\nnovel perspective on vanilla MoE with top-$k$ routing inspired by sparse\nrepresentation. This allows us to bridge established theoretical insights from\nsparse representation into MoE models. Building on this foundation, we propose\na group sparse regularization approach for the input of top-$k$ routing, termed\nMixture of Group Experts (MoGE). MoGE indirectly regularizes experts by\nimposing structural constraints on the routing inputs, while preserving the\noriginal MoE architecture. Furthermore, we organize the routing input into a 2D\ntopographic map, spatially grouping neighboring elements. This structure\nenables MoGE to capture representations invariant to minor transformations,\nthereby significantly enhancing expert diversity and specialization.\nComprehensive evaluations across various Transformer models for image\nclassification and language modeling tasks demonstrate that MoGE substantially\noutperforms its MoE counterpart, with minimal additional memory and computation\noverhead. Our approach provides a simple yet effective solution to scale the\nnumber of experts and reduce redundancy among them. The source code is included\nin the supplementary material and will be publicly released."}
{"id": "2504.10018", "pdf": "https://arxiv.org/pdf/2504.10018", "abs": "https://arxiv.org/abs/2504.10018", "authors": ["Xiao Wang", "Haiyang Wang", "Shiao Wang", "Qiang Chen", "Jiandong Jin", "Haoyu Song", "Bo Jiang", "Chenglong Li"], "title": "RGB-Event based Pedestrian Attribute Recognition: A Benchmark Dataset and An Asymmetric RWKV Fusion Framework", "categories": ["cs.CV", "cs.AI"], "comment": "The First Benchmark Dataset for RGB-Event Multimodal Pedestrian\n  Attribute Recognition Task", "summary": "Existing pedestrian attribute recognition methods are generally developed\nbased on RGB frame cameras. However, these approaches are constrained by the\nlimitations of RGB cameras, such as sensitivity to lighting conditions and\nmotion blur, which hinder their performance. Furthermore, current attribute\nrecognition primarily focuses on analyzing pedestrians' external appearance and\nclothing, lacking an exploration of emotional dimensions. In this paper, we\nrevisit these issues and propose a novel multi-modal RGB-Event attribute\nrecognition task by drawing inspiration from the advantages of event cameras in\nlow-light, high-speed, and low-power consumption. Specifically, we introduce\nthe first large-scale multi-modal pedestrian attribute recognition dataset,\ntermed EventPAR, comprising 100K paired RGB-Event samples that cover 50\nattributes related to both appearance and six human emotions, diverse scenes,\nand various seasons. By retraining and evaluating mainstream PAR models on this\ndataset, we establish a comprehensive benchmark and provide a solid foundation\nfor future research in terms of data and algorithmic baselines. In addition, we\npropose a novel RWKV-based multi-modal pedestrian attribute recognition\nframework, featuring an RWKV visual encoder and an asymmetric RWKV fusion\nmodule. Extensive experiments are conducted on our proposed dataset as well as\ntwo simulated datasets (MARS-Attribute and DukeMTMC-VID-Attribute), achieving\nstate-of-the-art results. The source code and dataset will be released on\nhttps://github.com/Event-AHU/OpenPAR"}
{"id": "2504.09271", "pdf": "https://arxiv.org/pdf/2504.09271", "abs": "https://arxiv.org/abs/2504.09271", "authors": ["Koustuv Saha", "Yoshee Jain", "Munmun De Choudhury"], "title": "Linguistic Comparison of AI- and Human-Written Responses to Online Mental Health Queries", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.SI"], "comment": null, "summary": "The ubiquity and widespread use of digital and online technologies have\ntransformed mental health support, with online mental health communities\n(OMHCs) providing safe spaces for peer support. More recently, generative AI\nand large language models (LLMs) have introduced new possibilities for\nscalable, around-the-clock mental health assistance that could potentially\naugment and supplement the capabilities of OMHCs. Although genAI shows promise\nin delivering immediate and personalized responses, their effectiveness in\nreplicating the nuanced, experience-based support of human peers remains an\nopen question. In this study, we harnessed 24,114 posts and 138,758 online\ncommunity (OC) responses from 55 OMHCs on Reddit. We prompted several\nstate-of-the-art LLMs (GPT-4-Turbo, Llama-3, and Mistral-7B) with these posts,\nand compared their (AI) responses to human-written (OC) responses based on a\nvariety of linguistic measures across psycholinguistics and lexico-semantics.\nOur findings revealed that AI responses are more verbose, readable, and\nanalytically structured, but lack linguistic diversity and personal narratives\ninherent in human-human interactions. Through a qualitative examination, we\nfound validation as well as complementary insights into the nature of AI\nresponses, such as its neutrality of stance and the absence of seeking\nback-and-forth clarifications. We discuss the ethical and practical\nimplications of integrating generative AI into OMHCs, advocating for frameworks\nthat balance AI's scalability and timeliness with the irreplaceable\nauthenticity, social interactiveness, and expertise of human connections that\nform the ethos of online support communities."}
{"id": "2504.10021", "pdf": "https://arxiv.org/pdf/2504.10021", "abs": "https://arxiv.org/abs/2504.10021", "authors": ["Nikolai Röhrich", "Alwin Hoffmann", "Richard Nordsieck", "Emilio Zarbali", "Alireza Javanmardi"], "title": "Masked Autoencoder Self Pre-Training for Defect Detection in Microelectronics", "categories": ["cs.CV"], "comment": "16 pages, 5 figures", "summary": "Whereas in general computer vision, transformer-based architectures have\nquickly become the gold standard, microelectronics defect detection still\nheavily relies on convolutional neural networks (CNNs). We hypothesize that\nthis is due to the fact that a) transformers have an increased need for data\nand b) labelled image generation procedures for microelectronics are costly,\nand labelled data is therefore sparse. Whereas in other domains, pre-training\non large natural image datasets can mitigate this problem, in microelectronics\ntransfer learning is hindered due to the dissimilarity of domain data and\nnatural images. Therefore, we evaluate self pre-training, where models are\npre-trained on the target dataset, rather than another dataset. We propose a\nvision transformer (ViT) pre-training framework for defect detection in\nmicroelectronics based on masked autoencoders (MAE). In MAE, a large share of\nimage patches is masked and reconstructed by the model during pre-training. We\nperform pre-training and defect detection using a dataset of less than 10.000\nscanning acoustic microscopy (SAM) images labelled using transient thermal\nanalysis (TTA). Our experimental results show that our approach leads to\nsubstantial performance gains compared to a) supervised ViT, b) ViT pre-trained\non natural image datasets, and c) state-of-the-art CNN-based defect detection\nmodels used in the literature. Additionally, interpretability analysis reveals\nthat our self pre-trained models, in comparison to ViT baselines, correctly\nfocus on defect-relevant features such as cracks in the solder material. This\ndemonstrates that our approach yields fault-specific feature representations,\nmaking our self pre-trained models viable for real-world defect detection in\nmicroelectronics."}
{"id": "2504.09354", "pdf": "https://arxiv.org/pdf/2504.09354", "abs": "https://arxiv.org/abs/2504.09354", "authors": ["Duy-Cat Can", "Quang-Huy Tang", "Huong Ha", "Binh T. Nguyen", "Oliver Y. Chén"], "title": "REMEMBER: Retrieval-based Explainable Multimodal Evidence-guided Modeling for Brain Evaluation and Reasoning in Zero- and Few-shot Neurodegenerative Diagnosis", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "q-bio.QM"], "comment": null, "summary": "Timely and accurate diagnosis of neurodegenerative disorders, such as\nAlzheimer's disease, is central to disease management. Existing deep learning\nmodels require large-scale annotated datasets and often function as \"black\nboxes\". Additionally, datasets in clinical practice are frequently small or\nunlabeled, restricting the full potential of deep learning methods. Here, we\nintroduce REMEMBER -- Retrieval-based Explainable Multimodal Evidence-guided\nModeling for Brain Evaluation and Reasoning -- a new machine learning framework\nthat facilitates zero- and few-shot Alzheimer's diagnosis using brain MRI scans\nthrough a reference-based reasoning process. Specifically, REMEMBER first\ntrains a contrastively aligned vision-text model using expert-annotated\nreference data and extends pseudo-text modalities that encode abnormality\ntypes, diagnosis labels, and composite clinical descriptions. Then, at\ninference time, REMEMBER retrieves similar, human-validated cases from a\ncurated dataset and integrates their contextual information through a dedicated\nevidence encoding module and attention-based inference head. Such an\nevidence-guided design enables REMEMBER to imitate real-world clinical\ndecision-making process by grounding predictions in retrieved imaging and\ntextual context. Specifically, REMEMBER outputs diagnostic predictions\nalongside an interpretable report, including reference images and explanations\naligned with clinical workflows. Experimental results demonstrate that REMEMBER\nachieves robust zero- and few-shot performance and offers a powerful and\nexplainable framework to neuroimaging-based diagnosis in the real world,\nespecially under limited data."}
{"id": "2504.10024", "pdf": "https://arxiv.org/pdf/2504.10024", "abs": "https://arxiv.org/abs/2504.10024", "authors": ["Mengkun She", "Felix Seegräber", "David Nakath", "Patricia Schöntag", "Kevin Köser"], "title": "Relative Illumination Fields: Learning Medium and Light Independent Underwater Scenes", "categories": ["cs.CV"], "comment": "10 pages, 6 figures. First two authors contributed equally to this\n  work", "summary": "We address the challenge of constructing a consistent and photorealistic\nNeural Radiance Field in inhomogeneously illuminated, scattering environments\nwith unknown, co-moving light sources. While most existing works on underwater\nscene representation focus on a static homogeneous illumination, limited\nattention has been paid to scenarios such as when a robot explores water deeper\nthan a few tens of meters, where sunlight becomes insufficient. To address\nthis, we propose a novel illumination field locally attached to the camera,\nenabling the capture of uneven lighting effects within the viewing frustum. We\ncombine this with a volumetric medium representation to an overall method that\neffectively handles interaction between dynamic illumination field and static\nscattering medium. Evaluation results demonstrate the effectiveness and\nflexibility of our approach."}
{"id": "2504.09426", "pdf": "https://arxiv.org/pdf/2504.09426", "abs": "https://arxiv.org/abs/2504.09426", "authors": ["Shengao Wang", "Arjun Chandra", "Aoming Liu", "Venkatesh Saligrama", "Boqing Gong"], "title": "BabyVLM: Data-Efficient Pretraining of VLMs Inspired by Infant Learning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Human infants rapidly develop visual reasoning skills from minimal input,\nsuggesting that developmentally inspired pretraining could significantly\nenhance the efficiency of vision-language models (VLMs). Although recent\nefforts have leveraged infant-inspired datasets like SAYCam, existing\nevaluation benchmarks remain misaligned--they are either too simplistic,\nnarrowly scoped, or tailored for large-scale pretrained models. Additionally,\ntraining exclusively on infant data overlooks the broader, diverse input from\nwhich infants naturally learn. To address these limitations, we propose\nBabyVLM, a novel framework comprising comprehensive in-domain evaluation\nbenchmarks and a synthetic training dataset created via child-directed\ntransformations of existing datasets. We demonstrate that VLMs trained with our\nsynthetic dataset achieve superior performance on BabyVLM tasks compared to\nmodels trained solely on SAYCam or general-purpose data of the SAYCam size.\nBabyVLM thus provides a robust, developmentally aligned evaluation tool and\nillustrates how compact models trained on carefully curated data can generalize\neffectively, opening pathways toward data-efficient vision-language learning\nparadigms."}
{"id": "2504.10035", "pdf": "https://arxiv.org/pdf/2504.10035", "abs": "https://arxiv.org/abs/2504.10035", "authors": ["Thomas Gossard", "Andreas Ziegler", "Andreas Zell"], "title": "TT3D: Table Tennis 3D Reconstruction", "categories": ["cs.CV"], "comment": "Accepted to CVSport 2025", "summary": "Sports analysis requires processing large amounts of data, which is\ntime-consuming and costly. Advancements in neural networks have significantly\nalleviated this burden, enabling highly accurate ball tracking in sports\nbroadcasts. However, relying solely on 2D ball tracking is limiting, as it\ndepends on the camera's viewpoint and falls short of supporting comprehensive\ngame analysis. To address this limitation, we propose a novel approach for\nreconstructing precise 3D ball trajectories from online table tennis match\nrecordings. Our method leverages the underlying physics of the ball's motion to\nidentify the bounce state that minimizes the reprojection error of the ball's\nflying trajectory, hence ensuring an accurate and reliable 3D reconstruction. A\nkey advantage of our approach is its ability to infer ball spin without relying\non human pose estimation or racket tracking, which are often unreliable or\nunavailable in broadcast footage. We developed an automated camera calibration\nmethod capable of reliably tracking camera movements. Additionally, we adapted\nan existing 3D pose estimation model, which lacks depth motion capture, to\naccurately track player movements. Together, these contributions enable the\nfull 3D reconstruction of a table tennis rally."}
{"id": "2504.09466", "pdf": "https://arxiv.org/pdf/2504.09466", "abs": "https://arxiv.org/abs/2504.09466", "authors": ["Weixiang Zhao", "Jiahe Guo", "Yulin Hu", "Yang Deng", "An Zhang", "Xingyu Sui", "Xinyang Han", "Yanyan Zhao", "Bing Qin", "Tat-Seng Chua", "Ting Liu"], "title": "AdaSteer: Your Aligned LLM is Inherently an Adaptive Jailbreak Defender", "categories": ["cs.CR", "cs.CL"], "comment": "17 pages, 6 figures, 9 tables", "summary": "Despite extensive efforts in safety alignment, large language models (LLMs)\nremain vulnerable to jailbreak attacks. Activation steering offers a\ntraining-free defense method but relies on fixed steering coefficients,\nresulting in suboptimal protection and increased false rejections of benign\ninputs. To address this, we propose AdaSteer, an adaptive activation steering\nmethod that dynamically adjusts model behavior based on input characteristics.\nWe identify two key properties: Rejection Law (R-Law), which shows that\nstronger steering is needed for jailbreak inputs opposing the rejection\ndirection, and Harmfulness Law (H-Law), which differentiates adversarial and\nbenign inputs. AdaSteer steers input representations along both the Rejection\nDirection (RD) and Harmfulness Direction (HD), with adaptive coefficients\nlearned via logistic regression, ensuring robust jailbreak defense while\npreserving benign input handling. Experiments on LLaMA-3.1, Gemma-2, and\nQwen2.5 show that AdaSteer outperforms baseline methods across multiple\njailbreak attacks with minimal impact on utility. Our results highlight the\npotential of interpretable model internals for real-time, flexible safety\nenforcement in LLMs."}
{"id": "2504.10039", "pdf": "https://arxiv.org/pdf/2504.10039", "abs": "https://arxiv.org/abs/2504.10039", "authors": ["Sergey Kuznetsov", "Sanduni Pinnawala", "Peter A. Wijeratne", "Ivor J. A. Simpson"], "title": "Investigating the Role of Bilateral Symmetry for Inpainting Brain MRI", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Inpainting has recently emerged as a valuable and interesting technology to\nemploy in the analysis of medical imaging data, in particular brain MRI. A wide\nvariety of methodologies for inpainting MRI have been proposed and demonstrated\non tasks including anomaly detection. In this work we investigate the\nstatistical relationship between inpainted brain structures and the amount of\nsubject-specific conditioning information, i.e. the other areas of the image\nthat are masked. In particular, we analyse the distribution of inpainting\nresults when masking additional regions of the image, specifically the\ncontra-lateral structure. This allows us to elucidate where in the brain the\nmodel is drawing information from, and in particular, what is the importance of\nhemispherical symmetry? Our experiments interrogate a diffusion inpainting\nmodel through analysing the inpainting of subcortical brain structures based on\nintensity and estimated area change. We demonstrate that some structures show a\nstrong influence of symmetry in the conditioning of the inpainting process."}
{"id": "2504.09479", "pdf": "https://arxiv.org/pdf/2504.09479", "abs": "https://arxiv.org/abs/2504.09479", "authors": ["Zhiqing Cui", "Jiahao Yuan", "Hanqing Wang", "Yanshu Li", "Chenxu Du", "Zhenglong Ding"], "title": "Draw with Thought: Unleashing Multimodal Reasoning for Scientific Diagram Generation", "categories": ["cs.AI", "cs.CL"], "comment": "26 pages, 14 figures", "summary": "Scientific diagrams are vital tools for communicating structured knowledge\nacross disciplines. However, they are often published as static raster images,\nlosing symbolic semantics and limiting reuse. While Multimodal Large Language\nModels (MLLMs) offer a pathway to bridging vision and structure, existing\nmethods lack semantic control and structural interpretability, especially on\ncomplex diagrams. We propose Draw with Thought (DwT), a training-free framework\nthat guides MLLMs to reconstruct diagrams into editable mxGraph XML code\nthrough cognitively-grounded Chain-of-Thought reasoning. DwT enables\ninterpretable and controllable outputs without model fine-tuning by dividing\nthe task into two stages: Coarse-to-Fine Planning, which handles perceptual\nstructuring and semantic specification, and Structure-Aware Code Generation,\nenhanced by format-guided refinement. To support evaluation, we release\nPlot2XML, a benchmark of 247 real-world scientific diagrams with gold-standard\nXML annotations. Extensive experiments across eight MLLMs show that our\napproach yields high-fidelity, semantically aligned, and structurally valid\nreconstructions, with human evaluations confirming strong alignment in both\naccuracy and visual aesthetics, offering a scalable solution for converting\nstatic visuals into executable representations and advancing machine\nunderstanding of scientific graphics."}
{"id": "2504.10044", "pdf": "https://arxiv.org/pdf/2504.10044", "abs": "https://arxiv.org/abs/2504.10044", "authors": ["Bingwen Zhu", "Yudong Jiang", "Baohan Xu", "Siqian Yang", "Mingyu Yin", "Yidi Wu", "Huyang Sun", "Zuxuan Wu"], "title": "Aligning Anime Video Generation with Human Feedback", "categories": ["cs.CV"], "comment": "10 pages, 5 figures, 7 tables", "summary": "Anime video generation faces significant challenges due to the scarcity of\nanime data and unusual motion patterns, leading to issues such as motion\ndistortion and flickering artifacts, which result in misalignment with human\npreferences. Existing reward models, designed primarily for real-world videos,\nfail to capture the unique appearance and consistency requirements of anime. In\nthis work, we propose a pipeline to enhance anime video generation by\nleveraging human feedback for better alignment. Specifically, we construct the\nfirst multi-dimensional reward dataset for anime videos, comprising 30k\nhuman-annotated samples that incorporating human preferences for both visual\nappearance and visual consistency. Based on this, we develop AnimeReward, a\npowerful reward model that employs specialized vision-language models for\ndifferent evaluation dimensions to guide preference alignment. Furthermore, we\nintroduce Gap-Aware Preference Optimization (GAPO), a novel training method\nthat explicitly incorporates preference gaps into the optimization process,\nenhancing alignment performance and efficiency. Extensive experiment results\nshow that AnimeReward outperforms existing reward models, and the inclusion of\nGAPO leads to superior alignment in both quantitative benchmarks and human\nevaluations, demonstrating the effectiveness of our pipeline in enhancing anime\nvideo quality. Our dataset and code will be publicly available."}
{"id": "2504.09582", "pdf": "https://arxiv.org/pdf/2504.09582", "abs": "https://arxiv.org/abs/2504.09582", "authors": ["Christos Theodoropoulos", "Andrei Catalin Coman", "James Henderson", "Marie-Francine Moens"], "title": "Reduction of Supervision for Biomedical Knowledge Discovery", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Published as part of the PhD dissertation: Theodoropoulos, Christos,\n  Marie-Francine Moens, and Matthew Blaschko. \"Deep Learning Models for the\n  Extraction of Knowledge from Text.\" (2025)", "summary": "Knowledge discovery is hindered by the increasing volume of publications and\nthe scarcity of extensive annotated data. To tackle the challenge of\ninformation overload, it is essential to employ automated methods for knowledge\nextraction and processing. Finding the right balance between the level of\nsupervision and the effectiveness of models poses a significant challenge.\nWhile supervised techniques generally result in better performance, they have\nthe major drawback of demanding labeled data. This requirement is\nlabor-intensive and time-consuming and hinders scalability when exploring new\ndomains. In this context, our study addresses the challenge of identifying\nsemantic relationships between biomedical entities (e.g., diseases, proteins)\nin unstructured text while minimizing dependency on supervision. We introduce a\nsuite of unsupervised algorithms based on dependency trees and attention\nmechanisms and employ a range of pointwise binary classification methods.\nTransitioning from weakly supervised to fully unsupervised settings, we assess\nthe methods' ability to learn from data with noisy labels. The evaluation on\nbiomedical benchmark datasets explores the effectiveness of the methods. Our\napproach tackles a central issue in knowledge discovery: balancing performance\nwith minimal supervision. By gradually decreasing supervision, we assess the\nrobustness of pointwise binary classification techniques in handling noisy\nlabels, revealing their capability to shift from weakly supervised to entirely\nunsupervised scenarios. Comprehensive benchmarking offers insights into the\neffectiveness of these techniques, suggesting an encouraging direction toward\nadaptable knowledge discovery systems, representing progress in creating\ndata-efficient methodologies for extracting useful insights when annotated data\nis limited."}
{"id": "2504.10048", "pdf": "https://arxiv.org/pdf/2504.10048", "abs": "https://arxiv.org/abs/2504.10048", "authors": ["Chengyi Du", "Keyan Jin"], "title": "Multi-Object Grounding via Hierarchical Contrastive Siamese Transformers", "categories": ["cs.CV"], "comment": null, "summary": "Multi-object grounding in 3D scenes involves localizing multiple objects\nbased on natural language input. While previous work has primarily focused on\nsingle-object grounding, real-world scenarios often demand the localization of\nseveral objects. To tackle this challenge, we propose Hierarchical Contrastive\nSiamese Transformers (H-COST), which employs a Hierarchical Processing strategy\nto progressively refine object localization, enhancing the understanding of\ncomplex language instructions. Additionally, we introduce a Contrastive Siamese\nTransformer framework, where two networks with the identical structure are\nused: one auxiliary network processes robust object relations from ground-truth\nlabels to guide and enhance the second network, the reference network, which\noperates on segmented point-cloud data. This contrastive mechanism strengthens\nthe model' s semantic understanding and significantly enhances its ability to\nprocess complex point-cloud data. Our approach outperforms previous\nstate-of-the-art methods by 9.5% on challenging multi-object grounding\nbenchmarks."}
{"id": "2504.09602", "pdf": "https://arxiv.org/pdf/2504.09602", "abs": "https://arxiv.org/abs/2504.09602", "authors": ["Zhehao Dong", "Zhen Lu", "Yue Yang"], "title": "Fine-tuning an Large Language Model for Automating Computational Fluid Dynamics Simulations", "categories": ["physics.flu-dyn", "cs.AI", "cs.CL"], "comment": null, "summary": "Configuring computational fluid dynamics (CFD) simulations typically demands\nextensive domain expertise, limiting broader access. Although large language\nmodels (LLMs) have advanced scientific computing, their use in automating CFD\nworkflows is underdeveloped. We introduce a novel approach centered on\ndomain-specific LLM adaptation. By fine-tuning Qwen2.5-7B-Instruct on NL2FOAM,\nour custom dataset of 28716 natural language-to-OpenFOAM configuration pairs\nwith chain-of-thought (CoT) annotations, we enable direct translation from\nnatural language descriptions to executable CFD setups. A multi-agent framework\norchestrates the process, autonomously verifying inputs, generating\nconfigurations, running simulations, and correcting errors. Evaluation on a\nbenchmark of 21 diverse flow cases demonstrates state-of-the-art performance,\nachieving 88.7% solution accuracy and 82.6% first-attempt success rate. This\nsignificantly outperforms larger general-purpose models like\nQwen2.5-72B-Instruct, DeepSeek-R1, and Llama3.3-70B-Instruct, while also\nrequiring fewer correction iterations and maintaining high computational\nefficiency. The results highlight the critical role of domain-specific\nadaptation in deploying LLM assistants for complex engineering workflows."}
{"id": "2504.10049", "pdf": "https://arxiv.org/pdf/2504.10049", "abs": "https://arxiv.org/abs/2504.10049", "authors": ["Théo Gigant", "Camille Guinaudeau", "Frédéric Dufaux"], "title": "Summarization of Multimodal Presentations with Vision-Language Models: Study of the Effect of Modalities and Structure", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Vision-Language Models (VLMs) can process visual and textual information in\nmultiple formats: texts, images, interleaved texts and images, or even\nhour-long videos. In this work, we conduct fine-grained quantitative and\nqualitative analyses of automatic summarization of multimodal presentations\nusing VLMs with various representations as input. From these experiments, we\nsuggest cost-effective strategies for generating summaries from text-heavy\nmultimodal documents under different input-length budgets using VLMs. We show\nthat slides extracted from the video stream can be beneficially used as input\nagainst the raw video, and that a structured representation from interleaved\nslides and transcript provides the best performance. Finally, we reflect and\ncomment on the nature of cross-modal interactions in multimodal presentations\nand share suggestions to improve the capabilities of VLMs to understand\ndocuments of this nature."}
{"id": "2504.09689", "pdf": "https://arxiv.org/pdf/2504.09689", "abs": "https://arxiv.org/abs/2504.09689", "authors": ["Jiahao Qiu", "Yinghui He", "Xinzhe Juan", "Yiming Wang", "Yuhan Liu", "Zixin Yao", "Yue Wu", "Xun Jiang", "Ling Yang", "Mengdi Wang"], "title": "EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "cs.LG"], "comment": "18 pages, 8 figures", "summary": "The rise of LLM-driven AI characters raises safety concerns, particularly for\nvulnerable human users with psychological disorders. To address these risks, we\npropose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate\nmental health hazards in human-AI interactions. EmoAgent comprises two\ncomponents: EmoEval simulates virtual users, including those portraying\nmentally vulnerable individuals, to assess mental health changes before and\nafter interactions with AI characters. It uses clinically proven psychological\nand psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks\ninduced by LLM. EmoGuard serves as an intermediary, monitoring users' mental\nstatus, predicting potential harm, and providing corrective feedback to\nmitigate risks. Experiments conducted in popular character-based chatbots show\nthat emotionally engaging dialogues can lead to psychological deterioration in\nvulnerable users, with mental state deterioration in more than 34.4% of the\nsimulations. EmoGuard significantly reduces these deterioration rates,\nunderscoring its role in ensuring safer AI-human interactions. Our code is\navailable at: https://github.com/1akaman/EmoAgent"}
{"id": "2504.10068", "pdf": "https://arxiv.org/pdf/2504.10068", "abs": "https://arxiv.org/abs/2504.10068", "authors": ["Yang Shi", "Jiaheng Liu", "Yushuo Guan", "Zhenhua Wu", "Yuanxing Zhang", "Zihao Wang", "Weihong Lin", "Jingyun Hua", "Zekun Wang", "Xinlong Chen", "Bohan Zeng", "Wentao Zhang", "Fuzheng Zhang", "Wenjing Yang", "Di Zhang"], "title": "Mavors: Multi-granularity Video Representation for Multimodal Large Language Model", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "22 pages", "summary": "Long-context video understanding in multimodal large language models (MLLMs)\nfaces a critical challenge: balancing computational efficiency with the\nretention of fine-grained spatio-temporal patterns. Existing approaches (e.g.,\nsparse sampling, dense sampling with low resolution, and token compression)\nsuffer from significant information loss in temporal dynamics, spatial details,\nor subtle interactions, particularly in videos with complex motion or varying\nresolutions. To address this, we propose $\\mathbf{Mavors}$, a novel framework\nthat introduces $\\mathbf{M}$ulti-gr$\\mathbf{a}$nularity\n$\\mathbf{v}$ide$\\mathbf{o}$ $\\mathbf{r}$epre$\\mathbf{s}$entation for holistic\nlong-video modeling. Specifically, Mavors directly encodes raw video content\ninto latent representations through two core components: 1) an Intra-chunk\nVision Encoder (IVE) that preserves high-resolution spatial features via 3D\nconvolutions and Vision Transformers, and 2) an Inter-chunk Feature Aggregator\n(IFA) that establishes temporal coherence across chunks using transformer-based\ndependency modeling with chunk-level rotary position encodings. Moreover, the\nframework unifies image and video understanding by treating images as\nsingle-frame videos via sub-image decomposition. Experiments across diverse\nbenchmarks demonstrate Mavors' superiority in maintaining both spatial fidelity\nand temporal continuity, significantly outperforming existing methods in tasks\nrequiring fine-grained spatio-temporal reasoning."}
{"id": "2504.09710", "pdf": "https://arxiv.org/pdf/2504.09710", "abs": "https://arxiv.org/abs/2504.09710", "authors": ["Zhenting Wang", "Guofeng Cui", "Kun Wan", "Wentian Zhao"], "title": "DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM Post-training", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Recent advances in reinforcement learning (RL)-based post-training have led\nto notable improvements in large language models (LLMs), particularly in\nenhancing their reasoning capabilities to handle complex tasks. However, most\nexisting methods treat the training data as a unified whole, overlooking the\nfact that modern LLM training often involves a mixture of data from diverse\ndistributions-varying in both source and difficulty. This heterogeneity\nintroduces a key challenge: how to adaptively schedule training across\ndistributions to optimize learning efficiency. In this paper, we present a\nprincipled curriculum learning framework grounded in the notion of\ndistribution-level learnability. Our core insight is that the magnitude of\npolicy advantages reflects how much a model can still benefit from further\ntraining on a given distribution. Based on this, we propose a\ndistribution-level curriculum learning framework for RL-based LLM\npost-training, which leverages the Upper Confidence Bound (UCB) principle to\ndynamically adjust sampling probabilities for different distrubutions. This\napproach prioritizes distributions with either high average advantage\n(exploitation) or low sample count (exploration), yielding an adaptive and\ntheoretically grounded training schedule. We instantiate our curriculum\nlearning framework with GRPO as the underlying RL algorithm and demonstrate its\neffectiveness on logic reasoning datasets with multiple difficulties and\nsources. Our experiments show that our framework significantly improves\nconvergence speed and final performance, highlighting the value of\ndistribution-aware curriculum strategies in LLM post-training. Code:\nhttps://github.com/ZhentingWang/DUMP."}
{"id": "2504.10070", "pdf": "https://arxiv.org/pdf/2504.10070", "abs": "https://arxiv.org/abs/2504.10070", "authors": ["Kiana Hoshanfar", "Alireza Hosseini", "Ahmad Kalhor", "Babak Nadjar Araabi"], "title": "DTFSal: Audio-Visual Dynamic Token Fusion for Video Saliency Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Audio-visual saliency prediction aims to mimic human visual attention by\nidentifying salient regions in videos through the integration of both visual\nand auditory information. Although visual-only approaches have significantly\nadvanced, effectively incorporating auditory cues remains challenging due to\ncomplex spatio-temporal interactions and high computational demands. To address\nthese challenges, we propose Dynamic Token Fusion Saliency (DFTSal), a novel\naudio-visual saliency prediction framework designed to balance accuracy with\ncomputational efficiency. Our approach features a multi-scale visual encoder\nequipped with two novel modules: the Learnable Token Enhancement Block (LTEB),\nwhich adaptively weights tokens to emphasize crucial saliency cues, and the\nDynamic Learnable Token Fusion Block (DLTFB), which employs a shifting\noperation to reorganize and merge features, effectively capturing long-range\ndependencies and detailed spatial information. In parallel, an audio branch\nprocesses raw audio signals to extract meaningful auditory features. Both\nvisual and audio features are integrated using our Adaptive Multimodal Fusion\nBlock (AMFB), which employs local, global, and adaptive fusion streams for\nprecise cross-modal fusion. The resulting fused features are processed by a\nhierarchical multi-decoder structure, producing accurate saliency maps.\nExtensive evaluations on six audio-visual benchmarks demonstrate that DFTSal\nachieves SOTA performance while maintaining computational efficiency."}
{"id": "2504.09723", "pdf": "https://arxiv.org/pdf/2504.09723", "abs": "https://arxiv.org/abs/2504.09723", "authors": ["Dakuo Wang", "Ting-Yao Hsu", "Yuxuan Lu", "Limeng Cui", "Yaochen Xie", "William Headean", "Bingsheng Yao", "Akash Veeragouni", "Jiapeng Liu", "Sreyashi Nag", "Jessie Wang"], "title": "AgentA/B: Automated and Scalable Web A/BTesting with Interactive LLM Agents", "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "A/B testing experiment is a widely adopted method for evaluating UI/UX design\ndecisions in modern web applications. Yet, traditional A/B testing remains\nconstrained by its dependence on the large-scale and live traffic of human\nparticipants, and the long time of waiting for the testing result. Through\nformative interviews with six experienced industry practitioners, we identified\ncritical bottlenecks in current A/B testing workflows. In response, we present\nAgentA/B, a novel system that leverages Large Language Model-based autonomous\nagents (LLM Agents) to automatically simulate user interaction behaviors with\nreal webpages. AgentA/B enables scalable deployment of LLM agents with diverse\npersonas, each capable of navigating the dynamic webpage and interactively\nexecuting multi-step interactions like search, clicking, filtering, and\npurchasing. In a demonstrative controlled experiment, we employ AgentA/B to\nsimulate a between-subject A/B testing with 1,000 LLM agents Amazon.com, and\ncompare agent behaviors with real human shopping behaviors at a scale. Our\nfindings suggest AgentA/B can emulate human-like behavior patterns."}
{"id": "2504.10079", "pdf": "https://arxiv.org/pdf/2504.10079", "abs": "https://arxiv.org/abs/2504.10079", "authors": ["Hongyu Qu", "Ling Xing", "Rui Yan", "Yazhou Yao", "Guo-Sen Xie", "Xiangbo Shu"], "title": "Hierarchical Relation-augmented Representation Generalization for Few-shot Action Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Few-shot action recognition (FSAR) aims to recognize novel action categories\nwith few exemplars. Existing methods typically learn frame-level\nrepresentations independently for each video by designing various inter-frame\ntemporal modeling strategies. However, they neglect explicit relation modeling\nbetween videos and tasks, thus failing to capture shared temporal patterns\nacross videos and reuse temporal knowledge from historical tasks. In light of\nthis, we propose HR2G-shot, a Hierarchical Relation-augmented Representation\nGeneralization framework for FSAR, which unifies three types of relation\nmodeling (inter-frame, inter-video, and inter-task) to learn task-specific\ntemporal patterns from a holistic view. In addition to conducting inter-frame\ntemporal interactions, we further devise two components to respectively explore\ninter-video and inter-task relationships: i) Inter-video Semantic Correlation\n(ISC) performs cross-video frame-level interactions in a fine-grained manner,\nthereby capturing task-specific query features and learning intra- and\ninter-class temporal correlations among support features; ii) Inter-task\nKnowledge Transfer (IKT) retrieves and aggregates relevant temporal knowledge\nfrom the bank, which stores diverse temporal patterns from historical tasks.\nExtensive experiments on five benchmarks show that HR2G-shot outperforms\ncurrent top-leading FSAR methods."}
{"id": "2504.09737", "pdf": "https://arxiv.org/pdf/2504.09737", "abs": "https://arxiv.org/abs/2504.09737", "authors": ["Nitya Thakkar", "Mert Yuksekgonul", "Jake Silberg", "Animesh Garg", "Nanyun Peng", "Fei Sha", "Rose Yu", "Carl Vondrick", "James Zou"], "title": "Can LLM feedback enhance review quality? A randomized study of 20K reviews at ICLR 2025", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "comment": "30 pages, 7 figures", "summary": "Peer review at AI conferences is stressed by rapidly rising submission\nvolumes, leading to deteriorating review quality and increased author\ndissatisfaction. To address these issues, we developed Review Feedback Agent, a\nsystem leveraging multiple large language models (LLMs) to improve review\nclarity and actionability by providing automated feedback on vague comments,\ncontent misunderstandings, and unprofessional remarks to reviewers. Implemented\nat ICLR 2025 as a large randomized control study, our system provided optional\nfeedback to more than 20,000 randomly selected reviews. To ensure high-quality\nfeedback for reviewers at this scale, we also developed a suite of automated\nreliability tests powered by LLMs that acted as guardrails to ensure feedback\nquality, with feedback only being sent to reviewers if it passed all the tests.\nThe results show that 27% of reviewers who received feedback updated their\nreviews, and over 12,000 feedback suggestions from the agent were incorporated\nby those reviewers. This suggests that many reviewers found the AI-generated\nfeedback sufficiently helpful to merit updating their reviews. Incorporating AI\nfeedback led to significantly longer reviews (an average increase of 80 words\namong those who updated after receiving feedback) and more informative reviews,\nas evaluated by blinded researchers. Moreover, reviewers who were selected to\nreceive AI feedback were also more engaged during paper rebuttals, as seen in\nlonger author-reviewer discussions. This work demonstrates that carefully\ndesigned LLM-generated review feedback can enhance peer review quality by\nmaking reviews more specific and actionable while increasing engagement between\nreviewers and authors. The Review Feedback Agent is publicly available at\nhttps://github.com/zou-group/review_feedback_agent."}
{"id": "2504.10080", "pdf": "https://arxiv.org/pdf/2504.10080", "abs": "https://arxiv.org/abs/2504.10080", "authors": ["Yucheng Lu", "Shunxin Wang", "Dovile Juodelyte", "Veronika Cheplygina"], "title": "Learning to Harmonize Cross-vendor X-ray Images by Non-linear Image Dynamics Correction", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "In this paper, we explore how conventional image enhancement can improve\nmodel robustness in medical image analysis. By applying commonly used\nnormalization methods to images from various vendors and studying their\ninfluence on model generalization in transfer learning, we show that the\nnonlinear characteristics of domain-specific image dynamics cannot be addressed\nby simple linear transforms. To tackle this issue, we reformulate the image\nharmonization task as an exposure correction problem and propose a method\ntermed Global Deep Curve Estimation (GDCE) to reduce domain-specific exposure\nmismatch. GDCE performs enhancement via a pre-defined polynomial function and\nis trained with the help of a ``domain discriminator'', aiming to improve model\ntransparency in downstream tasks compared to existing black-box methods."}
{"id": "2504.09816", "pdf": "https://arxiv.org/pdf/2504.09816", "abs": "https://arxiv.org/abs/2504.09816", "authors": ["Quentin Fitte-Rey", "Matyas Amrouche", "Romain Deveaud"], "title": "Augmented Relevance Datasets with Fine-Tuned Small LLMs", "categories": ["cs.IR", "cs.CL", "H.3.3; I.2.7"], "comment": "10 pages, 3 figures, and 6 tables. Accepted and presented to LLM4EVAL\n  at WSDM '25", "summary": "Building high-quality datasets and labeling query-document relevance are\nessential yet resource-intensive tasks, requiring detailed guidelines and\nsubstantial effort from human annotators. This paper explores the use of small,\nfine-tuned large language models (LLMs) to automate relevance assessment, with\na focus on improving ranking models' performance by augmenting their training\ndataset. We fine-tuned small LLMs to enhance relevance assessments, thereby\nimproving dataset creation quality for downstream ranking model training. Our\nexperiments demonstrate that these fine-tuned small LLMs not only outperform\ncertain closed source models on our dataset but also lead to substantial\nimprovements in ranking model performance. These results highlight the\npotential of leveraging small LLMs for efficient and scalable dataset\naugmentation, providing a practical solution for search engine optimization."}
{"id": "2504.10084", "pdf": "https://arxiv.org/pdf/2504.10084", "abs": "https://arxiv.org/abs/2504.10084", "authors": ["Yating Liu", "Yaowei Li", "Xiangyuan Lan", "Wenming Yang", "Zimo Liu", "Qingmin Liao"], "title": "UP-Person: Unified Parameter-Efficient Transfer Learning for Text-based Person Retrieval", "categories": ["cs.CV"], "comment": "16 pages, 7 figures, first submited to IEEE TCSVT on 2024 May. Under\n  review", "summary": "Text-based Person Retrieval (TPR) as a multi-modal task, which aims to\nretrieve the target person from a pool of candidate images given a text\ndescription, has recently garnered considerable attention due to the progress\nof contrastive visual-language pre-trained model. Prior works leverage\npre-trained CLIP to extract person visual and textual features and fully\nfine-tune the entire network, which have shown notable performance improvements\ncompared to uni-modal pre-training models. However, full-tuning a large model\nis prone to overfitting and hinders the generalization ability. In this paper,\nwe propose a novel Unified Parameter-Efficient Transfer Learning (PETL) method\nfor Text-based Person Retrieval (UP-Person) to thoroughly transfer the\nmulti-modal knowledge from CLIP. Specifically, UP-Person simultaneously\nintegrates three lightweight PETL components including Prefix, LoRA and\nAdapter, where Prefix and LoRA are devised together to mine local information\nwith task-specific information prompts, and Adapter is designed to adjust\nglobal feature representations. Additionally, two vanilla submodules are\noptimized to adapt to the unified architecture of TPR. For one thing, S-Prefix\nis proposed to boost attention of prefix and enhance the gradient propagation\nof prefix tokens, which improves the flexibility and performance of the vanilla\nprefix. For another thing, L-Adapter is designed in parallel with layer\nnormalization to adjust the overall distribution, which can resolve conflicts\ncaused by overlap and interaction among multiple submodules. Extensive\nexperimental results demonstrate that our UP-Person achieves state-of-the-art\nresults across various person retrieval datasets, including CUHK-PEDES,\nICFG-PEDES and RSTPReid while merely fine-tuning 4.7\\% parameters. Code is\navailable at https://github.com/Liu-Yating/UP-Person."}
{"id": "2504.09848", "pdf": "https://arxiv.org/pdf/2504.09848", "abs": "https://arxiv.org/abs/2504.09848", "authors": ["Jie Feng", "Jinwei Zeng", "Qingyue Long", "Hongyi Chen", "Jie Zhao", "Yanxin Xi", "Zhilun Zhou", "Yuan Yuan", "Shengyuan Wang", "Qingbin Zeng", "Songwei Li", "Yunke Zhang", "Yuming Lin", "Tong Li", "Jingtao Ding", "Chen Gao", "Fengli Xu", "Yong Li"], "title": "A Survey of Large Language Model-Powered Spatial Intelligence Across Scales: Advances in Embodied Agents, Smart Cities, and Earth Science", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Over the past year, the development of large language models (LLMs) has\nbrought spatial intelligence into focus, with much attention on vision-based\nembodied intelligence. However, spatial intelligence spans a broader range of\ndisciplines and scales, from navigation and urban planning to remote sensing\nand earth science. What are the differences and connections between spatial\nintelligence across these fields? In this paper, we first review human spatial\ncognition and its implications for spatial intelligence in LLMs. We then\nexamine spatial memory, knowledge representations, and abstract reasoning in\nLLMs, highlighting their roles and connections. Finally, we analyze spatial\nintelligence across scales -- from embodied to urban and global levels --\nfollowing a framework that progresses from spatial memory and understanding to\nspatial reasoning and intelligence. Through this survey, we aim to provide\ninsights into interdisciplinary spatial intelligence research and inspire\nfuture studies."}
{"id": "2504.10090", "pdf": "https://arxiv.org/pdf/2504.10090", "abs": "https://arxiv.org/abs/2504.10090", "authors": ["I-Sheng Fang", "Jun-Cheng Chen"], "title": "CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) and multimodal large language models (MLLMs)\nhave significantly advanced artificial intelligence. However, visual reasoning,\nreasoning involving both visual and textual inputs, remains underexplored.\nRecent advancements, including the reasoning models like OpenAI o1 and Gemini\n2.0 Flash Thinking, which incorporate image inputs, have opened this\ncapability. In this ongoing work, we focus specifically on photography-related\ntasks because a photo is a visual snapshot of the physical world where the\nunderlying physics (i.e., illumination, blur extent, etc.) interplay with the\ncamera parameters. Successfully reasoning from the visual information of a\nphoto to identify these numerical camera settings requires the MLLMs to have a\ndeeper understanding of the underlying physics for precise visual\ncomprehension, representing a challenging and intelligent capability essential\nfor practical applications like photography assistant agents. We aim to\nevaluate MLLMs on their ability to distinguish visual differences related to\nnumerical camera settings, extending a methodology previously proposed for\nvision-language models (VLMs). Our preliminary results demonstrate the\nimportance of visual reasoning in photography-related tasks. Moreover, these\nresults show that no single MLLM consistently dominates across all evaluation\ntasks, demonstrating ongoing challenges and opportunities in developing MLLMs\nwith better visual reasoning."}
{"id": "2504.09858", "pdf": "https://arxiv.org/pdf/2504.09858", "abs": "https://arxiv.org/abs/2504.09858", "authors": ["Wenjie Ma", "Jingxuan He", "Charlie Snell", "Tyler Griggs", "Sewon Min", "Matei Zaharia"], "title": "Reasoning Models Can Be Effective Without Thinking", "categories": ["cs.AI", "cs.CL"], "comment": "33 pages, 7 main figures, 2 tables", "summary": "Recent LLMs have significantly improved reasoning capabilities, primarily by\nincluding an explicit, lengthy Thinking process as part of generation. In this\npaper, we question whether this explicit thinking is necessary. Using the\nstate-of-the-art DeepSeek-R1-Distill-Qwen, we find that bypassing the thinking\nprocess via simple prompting, denoted as NoThinking, can be surprisingly\neffective. When controlling for the number of tokens, NoThinking outperforms\nThinking across a diverse set of seven challenging reasoning\ndatasets--including mathematical problem solving, formal theorem proving, and\ncoding--especially in low-budget settings, e.g., 51.3 vs. 28.9 on ACM 23 with\n700 tokens. Notably, the performance of NoThinking becomes more competitive\nwith pass@k as k increases. Building on this observation, we demonstrate that a\nparallel scaling approach that uses NoThinking to generate N outputs\nindependently and aggregates them is highly effective. For aggregation, we use\ntask-specific verifiers when available, or we apply simple best-of-N strategies\nsuch as confidence-based selection. Our method outperforms a range of baselines\nwith similar latency using Thinking, and is comparable to Thinking with\nsignificantly longer latency (up to 9x). Together, our research encourages a\nreconsideration of the necessity of lengthy thinking processes, while also\nestablishing a competitive reference for achieving strong reasoning performance\nin low-budget settings or at low latency using parallel scaling."}
{"id": "2504.10105", "pdf": "https://arxiv.org/pdf/2504.10105", "abs": "https://arxiv.org/abs/2504.10105", "authors": ["Zexin Ji", "Beiji Zou", "Xiaoyan Kui", "Sebastien Thureau", "Su Ruan"], "title": "Global and Local Mamba Network for Multi-Modality Medical Image Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Convolutional neural networks and Transformer have made significant\nprogresses in multi-modality medical image super-resolution. However, these\nmethods either have a fixed receptive field for local learning or significant\ncomputational burdens for global learning, limiting the super-resolution\nperformance. To solve this problem, State Space Models, notably Mamba, is\nintroduced to efficiently model long-range dependencies in images with linear\ncomputational complexity. Relying on the Mamba and the fact that low-resolution\nimages rely on global information to compensate for missing details, while\nhigh-resolution reference images need to provide more local details for\naccurate super-resolution, we propose a global and local Mamba network\n(GLMamba) for multi-modality medical image super-resolution. To be specific,\nour GLMamba is a two-branch network equipped with a global Mamba branch and a\nlocal Mamba branch. The global Mamba branch captures long-range relationships\nin low-resolution inputs, and the local Mamba branch focuses more on\nshort-range details in high-resolution reference images. We also use the deform\nblock to adaptively extract features of both branches to enhance the\nrepresentation ability. A modulator is designed to further enhance deformable\nfeatures in both global and local Mamba blocks. To fully integrate the\nreference image for low-resolution image super-resolution, we further develop a\nmulti-modality feature fusion block to adaptively fuse features by considering\nsimilarities, differences, and complementary aspects between modalities. In\naddition, a contrastive edge loss (CELoss) is developed for sufficient\nenhancement of edge textures and contrast in medical images."}
{"id": "2504.09936", "pdf": "https://arxiv.org/pdf/2504.09936", "abs": "https://arxiv.org/abs/2504.09936", "authors": ["Yuxuan Tian", "Zihan Wang", "Yebo Peng", "Aomufei Yuan", "Zhiming Wang", "Bairen Yi", "Xin Liu", "Yong Cui", "Tong Yang"], "title": "KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "18 pages, 8 figures", "summary": "Efficient inference of large language models (LLMs) is hindered by an\never-growing key-value (KV) cache, making KV cache compression a critical\nresearch direction. Traditional methods selectively evict less important KV\ncache entries based on attention scores or position heuristics, which leads to\ninformation loss and hallucinations. Recently, merging-based strategies have\nbeen explored to retain more information by merging KV pairs that would be\ndiscarded; however, these existing approaches inevitably introduce\ninconsistencies in attention distributions before and after merging, causing\noutput perturbation and degraded generation quality. To overcome this\nchallenge, we propose KeepKV, a novel adaptive KV cache merging method designed\nto eliminate output perturbation while preserving performance under strict\nmemory constraints. KeepKV introduces the Electoral Votes mechanism that\nrecords merging history and adaptively adjusts attention scores. Moreover, it\nfurther leverages a novel Zero Inference-Perturbation Merging methods, keeping\nattention consistency and compensating for attention loss resulting from cache\nmerging. KeepKV successfully retains essential context information within a\nsignificantly compressed cache. Extensive experiments on various benchmarks and\nLLM architectures demonstrate that KeepKV substantially reduces memory usage,\nenhances inference throughput by more than 2x and keeps superior generation\nquality even with 10% KV cache budgets."}
{"id": "2504.10106", "pdf": "https://arxiv.org/pdf/2504.10106", "abs": "https://arxiv.org/abs/2504.10106", "authors": ["Marc Gutiérrez-Pérez", "Antonio Agudo"], "title": "SoccerNet-v3D: Leveraging Sports Broadcast Replays for 3D Scene Understanding", "categories": ["cs.CV", "cs.AI", "I.2; I.4; I.5"], "comment": null, "summary": "Sports video analysis is a key domain in computer vision, enabling detailed\nspatial understanding through multi-view correspondences. In this work, we\nintroduce SoccerNet-v3D and ISSIA-3D, two enhanced and scalable datasets\ndesigned for 3D scene understanding in soccer broadcast analysis. These\ndatasets extend SoccerNet-v3 and ISSIA by incorporating field-line-based camera\ncalibration and multi-view synchronization, enabling 3D object localization\nthrough triangulation. We propose a monocular 3D ball localization task built\nupon the triangulation of ground-truth 2D ball annotations, along with several\ncalibration and reprojection metrics to assess annotation quality on demand.\nAdditionally, we present a single-image 3D ball localization method as a\nbaseline, leveraging camera calibration and ball size priors to estimate the\nball's position from a monocular viewpoint. To further refine 2D annotations,\nwe introduce a bounding box optimization technique that ensures alignment with\nthe 3D scene representation. Our proposed datasets establish new benchmarks for\n3D soccer scene understanding, enhancing both spatial and temporal analysis in\nsports analytics. Finally, we provide code to facilitate access to our\nannotations and the generation pipelines for the datasets."}
{"id": "2504.09946", "pdf": "https://arxiv.org/pdf/2504.09946", "abs": "https://arxiv.org/abs/2504.09946", "authors": ["Qian Wang", "Zhanzhi Lou", "Zhenheng Tang", "Nuo Chen", "Xuandong Zhao", "Wenxuan Zhang", "Dawn Song", "Bingsheng He"], "title": "Assessing Judging Bias in Large Reasoning Models: An Empirical Study", "categories": ["cs.CY", "cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) like DeepSeek-R1 and OpenAI-o1 have\ndemonstrated remarkable reasoning capabilities, raising important questions\nabout their biases in LLM-as-a-judge settings. We present a comprehensive\nbenchmark comparing judging biases between LLMs and LRMs across both subjective\npreference-alignment datasets and objective fact-based datasets. Through\ninvestigation of bandwagon, authority, position, and distraction biases, we\nuncover four key findings: (1) despite their advanced reasoning capabilities,\nLRMs remain susceptible to the above biases; (2) LRMs demonstrate better\nrobustness than LLMs specifically on fact-related datasets; (3) LRMs exhibit\nnotable position bias, preferring options in later positions; and (4) we\nidentify a novel \"superficial reflection bias\" where phrases mimicking\nreasoning (e.g., \"wait, let me think...\") significantly influence model\njudgments. To address these biases, we design and evaluate three mitigation\nstrategies: specialized system prompts that reduce judging biases by up to 19\\%\nin preference alignment datasets and 14\\% in fact-related datasets, in-context\nlearning that provides up to 27\\% improvement on preference tasks but shows\ninconsistent results on factual tasks, and a self-reflection mechanism that\nreduces biases by up to 10\\% in preference datasets and 16\\% in fact-related\ndatasets, with self-reflection proving particularly effective for LRMs. Our\nwork provides crucial insights for developing more reliable LLM-as-a-Judge\nframeworks, especially as LRMs become increasingly deployed as automated\njudges."}
{"id": "2504.10117", "pdf": "https://arxiv.org/pdf/2504.10117", "abs": "https://arxiv.org/abs/2504.10117", "authors": ["Peizheng Li", "Shuxiao Ding", "You Zhou", "Qingwen Zhang", "Onat Inak", "Larissa Triess", "Niklas Hanselmann", "Marius Cordts", "Andreas Zell"], "title": "AGO: Adaptive Grounding for Open World 3D Occupancy Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Open-world 3D semantic occupancy prediction aims to generate a voxelized 3D\nrepresentation from sensor inputs while recognizing both known and unknown\nobjects. Transferring open-vocabulary knowledge from vision-language models\n(VLMs) offers a promising direction but remains challenging. However, methods\nbased on VLM-derived 2D pseudo-labels with traditional supervision are limited\nby a predefined label space and lack general prediction capabilities. Direct\nalignment with pretrained image embeddings, on the other hand, fails to achieve\nreliable performance due to often inconsistent image and text representations\nin VLMs. To address these challenges, we propose AGO, a novel 3D occupancy\nprediction framework with adaptive grounding to handle diverse open-world\nscenarios. AGO first encodes surrounding images and class prompts into 3D and\ntext embeddings, respectively, leveraging similarity-based grounding training\nwith 3D pseudo-labels. Additionally, a modality adapter maps 3D embeddings into\na space aligned with VLM-derived image embeddings, reducing modality gaps.\nExperiments on Occ3D-nuScenes show that AGO improves unknown object prediction\nin zero-shot and few-shot transfer while achieving state-of-the-art\nclosed-world self-supervised performance, surpassing prior methods by 4.09\nmIoU."}
{"id": "2504.10000", "pdf": "https://arxiv.org/pdf/2504.10000", "abs": "https://arxiv.org/abs/2504.10000", "authors": ["Yanbo Wang", "Jiyang Guan", "Jian Liang", "Ran He"], "title": "Do We Really Need Curated Malicious Data for Safety Alignment in Multi-modal Large Language Models?", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "Accepted to CVPR 2025, codes in process", "summary": "Multi-modal large language models (MLLMs) have made significant progress, yet\ntheir safety alignment remains limited. Typically, current open-source MLLMs\nrely on the alignment inherited from their language module to avoid harmful\ngenerations. However, the lack of safety measures specifically designed for\nmulti-modal inputs creates an alignment gap, leaving MLLMs vulnerable to\nvision-domain attacks such as typographic manipulation. Current methods utilize\na carefully designed safety dataset to enhance model defense capability, while\nthe specific knowledge or patterns acquired from the high-quality dataset\nremain unclear. Through comparison experiments, we find that the alignment gap\nprimarily arises from data distribution biases, while image content, response\nquality, or the contrastive behavior of the dataset makes little contribution\nto boosting multi-modal safety. To further investigate this and identify the\nkey factors in improving MLLM safety, we propose finetuning MLLMs on a small\nset of benign instruct-following data with responses replaced by simple, clear\nrejection sentences. Experiments show that, without the need for\nlabor-intensive collection of high-quality malicious data, model safety can\nstill be significantly improved, as long as a specific fraction of rejection\ndata exists in the finetuning set, indicating the security alignment is not\nlost but rather obscured during multi-modal pretraining or instruction\nfinetuning. Simply correcting the underlying data bias could narrow the safety\ngap in the vision domain."}
{"id": "2504.10123", "pdf": "https://arxiv.org/pdf/2504.10123", "abs": "https://arxiv.org/abs/2504.10123", "authors": ["Tzu-Yun Tseng", "Hongyu Lyu", "Josephine Li", "Julie Stephany Berrio", "Mao Shan", "Stewart Worrall"], "title": "M2S-RoAD: Multi-Modal Semantic Segmentation for Road Damage Using Camera and LiDAR Data", "categories": ["cs.CV"], "comment": null, "summary": "Road damage can create safety and comfort challenges for both human drivers\nand autonomous vehicles (AVs). This damage is particularly prevalent in rural\nareas due to less frequent surveying and maintenance of roads. Automated\ndetection of pavement deterioration can be used as an input to AVs and driver\nassistance systems to improve road safety. Current research in this field has\npredominantly focused on urban environments driven largely by public datasets,\nwhile rural areas have received significantly less attention. This paper\nintroduces M2S-RoAD, a dataset for the semantic segmentation of different\nclasses of road damage. M2S-RoAD was collected in various towns across New\nSouth Wales, Australia, and labelled for semantic segmentation to identify nine\ndistinct types of road damage. This dataset will be released upon the\nacceptance of the paper."}
{"id": "2504.10049", "pdf": "https://arxiv.org/pdf/2504.10049", "abs": "https://arxiv.org/abs/2504.10049", "authors": ["Théo Gigant", "Camille Guinaudeau", "Frédéric Dufaux"], "title": "Summarization of Multimodal Presentations with Vision-Language Models: Study of the Effect of Modalities and Structure", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Vision-Language Models (VLMs) can process visual and textual information in\nmultiple formats: texts, images, interleaved texts and images, or even\nhour-long videos. In this work, we conduct fine-grained quantitative and\nqualitative analyses of automatic summarization of multimodal presentations\nusing VLMs with various representations as input. From these experiments, we\nsuggest cost-effective strategies for generating summaries from text-heavy\nmultimodal documents under different input-length budgets using VLMs. We show\nthat slides extracted from the video stream can be beneficially used as input\nagainst the raw video, and that a structured representation from interleaved\nslides and transcript provides the best performance. Finally, we reflect and\ncomment on the nature of cross-modal interactions in multimodal presentations\nand share suggestions to improve the capabilities of VLMs to understand\ndocuments of this nature."}
{"id": "2504.10148", "pdf": "https://arxiv.org/pdf/2504.10148", "abs": "https://arxiv.org/abs/2504.10148", "authors": ["Chunyang Zhang", "Zhenhong Sun", "Zhicheng Zhang", "Junyan Wang", "Yu Zhang", "Dong Gong", "Huadong Mo", "Daoyi Dong"], "title": "Hierarchical and Step-Layer-Wise Tuning of Attention Specialty for Multi-Instance Synthesis in Diffusion Transformers", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image (T2I) generation models often struggle with multi-instance\nsynthesis (MIS), where they must accurately depict multiple distinct instances\nin a single image based on complex prompts detailing individual features.\nTraditional MIS control methods for UNet architectures like SD v1.5/SDXL fail\nto adapt to DiT-based models like FLUX and SD v3.5, which rely on integrated\nattention between image and text tokens rather than text-image cross-attention.\nTo enhance MIS in DiT, we first analyze the mixed attention mechanism in DiT.\nOur token-wise and layer-wise analysis of attention maps reveals a hierarchical\nresponse structure: instance tokens dominate early layers, background tokens in\nmiddle layers, and attribute tokens in later layers. Building on this\nobservation, we propose a training-free approach for enhancing MIS in DiT-based\nmodels with hierarchical and step-layer-wise attention specialty tuning (AST).\nAST amplifies key regions while suppressing irrelevant areas in distinct\nattention maps across layers and steps, guided by the hierarchical structure.\nThis optimizes multimodal interactions by hierarchically decoupling the complex\nprompts with instance-based sketches. We evaluate our approach using upgraded\nsketch-based layouts for the T2I-CompBench and customized complex scenes. Both\nquantitative and qualitative results confirm our method enhances complex layout\ngeneration, ensuring precise instance placement and attribute representation in\nMIS."}
{"id": "2504.10055", "pdf": "https://arxiv.org/pdf/2504.10055", "abs": "https://arxiv.org/abs/2504.10055", "authors": ["Theodor Wulff", "Rahul Singh Maharjan", "Xinyun Chi", "Angelo Cangelosi"], "title": "Joint Action Language Modelling for Transparent Policy Execution", "categories": ["cs.RO", "cs.CL"], "comment": null, "summary": "An agent's intention often remains hidden behind the black-box nature of\nembodied policies. Communication using natural language statements that\ndescribe the next action can provide transparency towards the agent's behavior.\nWe aim to insert transparent behavior directly into the learning process, by\ntransforming the problem of policy learning into a language generation problem\nand combining it with traditional autoregressive modelling. The resulting model\nproduces transparent natural language statements followed by tokens\nrepresenting the specific actions to solve long-horizon tasks in the\nLanguage-Table environment. Following previous work, the model is able to learn\nto produce a policy represented by special discretized tokens in an\nautoregressive manner. We place special emphasis on investigating the\nrelationship between predicting actions and producing high-quality language for\na transparent agent. We find that in many cases both the quality of the action\ntrajectory and the transparent statement increase when they are generated\nsimultaneously."}
{"id": "2504.10158", "pdf": "https://arxiv.org/pdf/2504.10158", "abs": "https://arxiv.org/abs/2504.10158", "authors": ["Jiansheng Li", "Xingxuan Zhang", "Hao Zou", "Yige Guo", "Renzhe Xu", "Yilong Liu", "Chuzhao Zhu", "Yue He", "Peng Cui"], "title": "COUNTS: Benchmarking Object Detectors and Multimodal Large Language Models under Distribution Shifts", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Current object detectors often suffer significant perfor-mance degradation in\nreal-world applications when encountering distributional shifts. Consequently,\nthe out-of-distribution (OOD) generalization capability of object detectors has\ngarnered increasing attention from researchers. Despite this growing interest,\nthere remains a lack of a large-scale, comprehensive dataset and evaluation\nbenchmark with fine-grained annotations tailored to assess the OOD\ngeneralization on more intricate tasks like object detection and grounding. To\naddress this gap, we introduce COUNTS, a large-scale OOD dataset with\nobject-level annotations. COUNTS encompasses 14 natural distributional shifts,\nover 222K samples, and more than 1,196K labeled bounding boxes. Leveraging\nCOUNTS, we introduce two novel benchmarks: O(OD)2 and OODG. O(OD)2 is designed\nto comprehensively evaluate the OOD generalization capabilities of object\ndetectors by utilizing controlled distribution shifts between training and\ntesting data. OODG, on the other hand, aims to assess the OOD generalization of\ngrounding abilities in multimodal large language models (MLLMs). Our findings\nreveal that, while large models and extensive pre-training data substantially\nen hance performance in in-distribution (IID) scenarios, significant\nlimitations and opportunities for improvement persist in OOD contexts for both\nobject detectors and MLLMs. In visual grounding tasks, even the advanced GPT-4o\nand Gemini-1.5 only achieve 56.7% and 28.0% accuracy, respectively. We hope\nCOUNTS facilitates advancements in the development and assessment of robust\nobject detectors and MLLMs capable of maintaining high performance under\ndistributional shifts."}
{"id": "2504.10068", "pdf": "https://arxiv.org/pdf/2504.10068", "abs": "https://arxiv.org/abs/2504.10068", "authors": ["Yang Shi", "Jiaheng Liu", "Yushuo Guan", "Zhenhua Wu", "Yuanxing Zhang", "Zihao Wang", "Weihong Lin", "Jingyun Hua", "Zekun Wang", "Xinlong Chen", "Bohan Zeng", "Wentao Zhang", "Fuzheng Zhang", "Wenjing Yang", "Di Zhang"], "title": "Mavors: Multi-granularity Video Representation for Multimodal Large Language Model", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "22 pages", "summary": "Long-context video understanding in multimodal large language models (MLLMs)\nfaces a critical challenge: balancing computational efficiency with the\nretention of fine-grained spatio-temporal patterns. Existing approaches (e.g.,\nsparse sampling, dense sampling with low resolution, and token compression)\nsuffer from significant information loss in temporal dynamics, spatial details,\nor subtle interactions, particularly in videos with complex motion or varying\nresolutions. To address this, we propose $\\mathbf{Mavors}$, a novel framework\nthat introduces $\\mathbf{M}$ulti-gr$\\mathbf{a}$nularity\n$\\mathbf{v}$ide$\\mathbf{o}$ $\\mathbf{r}$epre$\\mathbf{s}$entation for holistic\nlong-video modeling. Specifically, Mavors directly encodes raw video content\ninto latent representations through two core components: 1) an Intra-chunk\nVision Encoder (IVE) that preserves high-resolution spatial features via 3D\nconvolutions and Vision Transformers, and 2) an Inter-chunk Feature Aggregator\n(IFA) that establishes temporal coherence across chunks using transformer-based\ndependency modeling with chunk-level rotary position encodings. Moreover, the\nframework unifies image and video understanding by treating images as\nsingle-frame videos via sub-image decomposition. Experiments across diverse\nbenchmarks demonstrate Mavors' superiority in maintaining both spatial fidelity\nand temporal continuity, significantly outperforming existing methods in tasks\nrequiring fine-grained spatio-temporal reasoning."}
{"id": "2504.10165", "pdf": "https://arxiv.org/pdf/2504.10165", "abs": "https://arxiv.org/abs/2504.10165", "authors": ["Nguyen Ngoc Dat", "Tom Richardson", "Matthew Watson", "Kilian Meier", "Jenna Kline", "Sid Reid", "Guy Maalouf", "Duncan Hine", "Majid Mirmehdi", "Tilo Burghardt"], "title": "WildLive: Near Real-time Visual Wildlife Tracking onboard UAVs", "categories": ["cs.CV", "cs.AI"], "comment": "Submitted in CV4Animals 2025", "summary": "Live tracking of wildlife via high-resolution video processing directly\nonboard drones is widely unexplored and most existing solutions rely on\nstreaming video to ground stations to support navigation. Yet, both autonomous\nanimal-reactive flight control beyond visual line of sight and/or\nmission-specific individual and behaviour recognition tasks rely to some degree\non this capability. In response, we introduce WildLive -- a near real-time\nanimal detection and tracking framework for high-resolution imagery running\ndirectly onboard uncrewed aerial vehicles (UAVs). The system performs\nmulti-animal detection and tracking at 17fps+ for HD and 7fps+ on 4K video\nstreams suitable for operation during higher altitude flights to minimise\nanimal disturbance. Our system is optimised for Jetson Orin AGX onboard\nhardware. It integrates the efficiency of sparse optical flow tracking and\nmission-specific sampling with device-optimised and proven YOLO-driven object\ndetection and segmentation techniques. Essentially, computational resource is\nfocused onto spatio-temporal regions of high uncertainty to significantly\nimprove UAV processing speeds without domain-specific loss of accuracy.\nAlongside, we introduce our WildLive dataset, which comprises 200k+ annotated\nanimal instances across 19k+ frames from 4K UAV videos collected at the Ol\nPejeta Conservancy in Kenya. All frames contain ground truth bounding boxes,\nsegmentation masks, as well as individual tracklets and tracking point\ntrajectories. We compare our system against current object tracking approaches\nincluding OC-SORT, ByteTrack, and SORT. Our multi-animal tracking experiments\nwith onboard hardware confirm that near real-time high-resolution wildlife\ntracking is possible on UAVs whilst maintaining high accuracy levels as needed\nfor future navigational and mission-specific animal-centric operational\nautonomy."}
{"id": "2504.10081", "pdf": "https://arxiv.org/pdf/2504.10081", "abs": "https://arxiv.org/abs/2504.10081", "authors": ["Yichi Zhang", "Zihao Zeng", "Dongbai Li", "Yao Huang", "Zhijie Deng", "Yinpeng Dong"], "title": "RealSafe-R1: Safety-Aligned DeepSeek-R1 without Compromising Reasoning Capability", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs), such as OpenAI o1 and DeepSeek-R1, have been\nrapidly progressing and achieving breakthrough performance on complex reasoning\ntasks such as mathematics and coding. However, the open-source R1 models have\nraised safety concerns in wide applications, such as the tendency to comply\nwith malicious queries, which greatly impacts the utility of these powerful\nmodels in their applications. In this paper, we introduce RealSafe-R1 as\nsafety-aligned versions of DeepSeek-R1 distilled models. To train these models,\nwe construct a dataset of 15k safety-aware reasoning trajectories generated by\nDeepSeek-R1, under explicit instructions for expected refusal behavior. Both\nquantitative experiments and qualitative case studies demonstrate the models'\nimprovements, which are shown in their safety guardrails against both harmful\nqueries and jailbreak attacks. Importantly, unlike prior safety alignment\nefforts that often compromise reasoning performance, our method preserves the\nmodels' reasoning capabilities by maintaining the training data within the\noriginal distribution of generation. Model weights of RealSafe-R1 are\nopen-source at https://huggingface.co/RealSafe."}
{"id": "2504.10174", "pdf": "https://arxiv.org/pdf/2504.10174", "abs": "https://arxiv.org/abs/2504.10174", "authors": ["Yiding Lu", "Mouxing Yang", "Dezhong Peng", "Peng Hu", "Yijie Lin", "Xi Peng"], "title": "LLaVA-ReID: Selective Multi-image Questioner for Interactive Person Re-Identification", "categories": ["cs.CV"], "comment": null, "summary": "Traditional text-based person ReID assumes that person descriptions from\nwitnesses are complete and provided at once. However, in real-world scenarios,\nsuch descriptions are often partial or vague. To address this limitation, we\nintroduce a new task called interactive person re-identification (Inter-ReID).\nInter-ReID is a dialogue-based retrieval task that iteratively refines initial\ndescriptions through ongoing interactions with the witnesses. To facilitate the\nstudy of this new task, we construct a dialogue dataset that incorporates\nmultiple types of questions by decomposing fine-grained attributes of\nindividuals. We further propose LLaVA-ReID, a question model that generates\ntargeted questions based on visual and textual contexts to elicit additional\ndetails about the target person. Leveraging a looking-forward strategy, we\nprioritize the most informative questions as supervision during training.\nExperimental results on both Inter-ReID and text-based ReID benchmarks\ndemonstrate that LLaVA-ReID significantly outperforms baselines."}
{"id": "2504.10090", "pdf": "https://arxiv.org/pdf/2504.10090", "abs": "https://arxiv.org/abs/2504.10090", "authors": ["I-Sheng Fang", "Jun-Cheng Chen"], "title": "CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) and multimodal large language models (MLLMs)\nhave significantly advanced artificial intelligence. However, visual reasoning,\nreasoning involving both visual and textual inputs, remains underexplored.\nRecent advancements, including the reasoning models like OpenAI o1 and Gemini\n2.0 Flash Thinking, which incorporate image inputs, have opened this\ncapability. In this ongoing work, we focus specifically on photography-related\ntasks because a photo is a visual snapshot of the physical world where the\nunderlying physics (i.e., illumination, blur extent, etc.) interplay with the\ncamera parameters. Successfully reasoning from the visual information of a\nphoto to identify these numerical camera settings requires the MLLMs to have a\ndeeper understanding of the underlying physics for precise visual\ncomprehension, representing a challenging and intelligent capability essential\nfor practical applications like photography assistant agents. We aim to\nevaluate MLLMs on their ability to distinguish visual differences related to\nnumerical camera settings, extending a methodology previously proposed for\nvision-language models (VLMs). Our preliminary results demonstrate the\nimportance of visual reasoning in photography-related tasks. Moreover, these\nresults show that no single MLLM consistently dominates across all evaluation\ntasks, demonstrating ongoing challenges and opportunities in developing MLLMs\nwith better visual reasoning."}
{"id": "2504.10190", "pdf": "https://arxiv.org/pdf/2504.10190", "abs": "https://arxiv.org/abs/2504.10190", "authors": ["Kaushik Bhargav Sivangi", "Idris Zakariyya", "Paul Henderson", "Fani Deligianni"], "title": "Differentially Private 2D Human Pose Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Human pose estimation (HPE) has become essential in numerous applications\nincluding healthcare, activity recognition, and human-computer interaction.\nHowever, the privacy implications of processing sensitive visual data present\nsignificant deployment barriers in critical domains. While traditional\nanonymization techniques offer limited protection and often compromise data\nutility for broader motion analysis, Differential Privacy (DP) provides formal\nprivacy guarantees but typically degrades model performance when applied\nnaively. In this work, we present the first differentially private 2D human\npose estimation (2D-HPE) by applying Differentially Private Stochastic Gradient\nDescent (DP-SGD) to this task. To effectively balance privacy with performance,\nwe adopt Projected DP-SGD (PDP-SGD), which projects the noisy gradients to a\nlow-dimensional subspace. Additionally, we adapt TinyViT, a compact and\nefficient vision transformer for coordinate classification in HPE, providing a\nlightweight yet powerful backbone that enhances privacy-preserving deployment\nfeasibility on resource-limited devices. Our approach is particularly valuable\nfor multimedia interpretation tasks, enabling privacy-safe analysis and\nunderstanding of human motion across diverse visual media while preserving the\nsemantic meaning required for downstream applications. Comprehensive\nexperiments on the MPII Human Pose Dataset demonstrate significant performance\nenhancement with PDP-SGD achieving 78.48% PCKh@0.5 at a strict privacy budget\n($\\epsilon=0.2$), compared to 63.85% for standard DP-SGD. This work lays\nfoundation for privacy-preserving human pose estimation in real-world,\nsensitive applications."}
{"id": "2504.10127", "pdf": "https://arxiv.org/pdf/2504.10127", "abs": "https://arxiv.org/abs/2504.10127", "authors": ["Junlei Zhang", "Zichen Ding", "Chang Ma", "Zijie Chen", "Qiushi Sun", "Zhenzhong Lan", "Junxian He"], "title": "Breaking the Data Barrier -- Building GUI Agents Through Task Generalization", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "24 pages, 11 figures", "summary": "Graphical User Interface (GUI) agents offer cross-platform solutions for\nautomating complex digital tasks, with significant potential to transform\nproductivity workflows. However, their performance is often constrained by the\nscarcity of high-quality trajectory data. To address this limitation, we\npropose training Vision Language Models (VLMs) on data-rich,\nreasoning-intensive tasks during a dedicated mid-training stage, and then\nexamine how incorporating these tasks facilitates generalization to GUI\nplanning scenarios. Specifically, we explore a range of tasks with readily\navailable instruction-tuning data, including GUI perception, multimodal\nreasoning, and textual reasoning. Through extensive experiments across 11\nmid-training tasks, we demonstrate that: (1) Task generalization proves highly\neffective, yielding substantial improvements across most settings. For\ninstance, multimodal mathematical reasoning enhances performance on\nAndroidWorld by an absolute 6.3%. Remarkably, text-only mathematical data\nsignificantly boosts GUI web agent performance, achieving a 5.6% improvement on\nWebArena and 5.4% improvement on AndroidWorld, underscoring notable cross-modal\ngeneralization from text-based to visual domains; (2) Contrary to prior\nassumptions, GUI perception data - previously considered closely aligned with\nGUI agent tasks and widely utilized for training - has a comparatively limited\nimpact on final performance; (3) Building on these insights, we identify the\nmost effective mid-training tasks and curate optimized mixture datasets,\nresulting in absolute performance gains of 8.0% on WebArena and 12.2% on\nAndroidWorld. Our work provides valuable insights into cross-domain knowledge\ntransfer for GUI agents and offers a practical approach to addressing data\nscarcity challenges in this emerging field. The code, data and models will be\navailable at https://github.com/hkust-nlp/GUIMid."}
{"id": "2504.10201", "pdf": "https://arxiv.org/pdf/2504.10201", "abs": "https://arxiv.org/abs/2504.10201", "authors": ["Raphael Achddou", "Yann Gousseau", "Saïd Ladjal", "Sabine Süsstrunk"], "title": "VibrantLeaves: A principled parametric image generator for training deep restoration models", "categories": ["cs.CV"], "comment": null, "summary": "Even though Deep Neural Networks are extremely powerful for image restoration\ntasks, they have several limitations. They are poorly understood and suffer\nfrom strong biases inherited from the training sets. One way to address these\nshortcomings is to have a better control over the training sets, in particular\nby using synthetic sets. In this paper, we propose a synthetic image generator\nrelying on a few simple principles. In particular, we focus on geometric\nmodeling, textures, and a simple modeling of image acquisition. These\nproperties, integrated in a classical Dead Leaves model, enable the creation of\nefficient training sets. Standard image denoising and super-resolution networks\ncan be trained on such datasets, reaching performance almost on par with\ntraining on natural image datasets. As a first step towards explainability, we\nprovide a careful analysis of the considered principles, identifying which\nimage properties are necessary to obtain good performances. Besides, such\ntraining also yields better robustness to various geometric and radiometric\nperturbations of the test sets."}
{"id": "2504.10179", "pdf": "https://arxiv.org/pdf/2504.10179", "abs": "https://arxiv.org/abs/2504.10179", "authors": ["Anwesha Mohanty", "Venkatesh Balavadhani Parthasarathy", "Arsalan Shahid"], "title": "The Future of MLLM Prompting is Adaptive: A Comprehensive Experimental Evaluation of Prompt Engineering Methods for Robust Multimodal Performance", "categories": ["cs.AI", "cs.CL", "cs.ET"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) are set to transform how machines\nprocess and generate human-like responses by integrating diverse modalities\nsuch as text, images, and code. Yet, effectively harnessing their capabilities\nhinges on optimal prompt engineering. We present a comprehensive experimental\nevaluation of seven prompt engineering methods applied to 13 open-source MLLMs\nover 24 tasks spanning Reasoning and Compositionality, Multimodal Understanding\nand Alignment, Complex Code Generation and Execution, and Knowledge Retrieval\nand Integration. Our approach stratifies models by parameter count into Small\n(<4B), Medium (4B-10B), and Large (>10B) categories and compares prompting\ntechniques including Zero-Shot, One-Shot, Few-Shot, Chain-of-Thought,\nAnalogical, Generated Knowledge, and Tree-of-Thought. While Large MLLMs excel\nin structured tasks such as code generation, achieving accuracies up to 96.88%\nunder Few-Shot prompting, all models struggle with complex reasoning and\nabstract understanding, often yielding accuracies below 60% and high\nhallucination rates. Structured reasoning prompts frequently increased\nhallucination up to 75% in small models and led to longer response times (over\n20 seconds in Large MLLMs), while simpler prompting methods provided more\nconcise and efficient outputs. No single prompting method uniformly optimises\nall task types. Instead, adaptive strategies combining example-based guidance\nwith selective structured reasoning are essential to enhance robustness,\nefficiency, and factual accuracy. Our findings offer practical recommendations\nfor prompt engineering and support more reliable deployment of MLLMs across\napplications including AI-assisted coding, knowledge retrieval, and multimodal\ncontent understanding."}
{"id": "2504.10214", "pdf": "https://arxiv.org/pdf/2504.10214", "abs": "https://arxiv.org/abs/2504.10214", "authors": ["Songze Li", "Qixing Xu", "Tonghua Su", "Xu-Yao Zhang", "Zhongjie Wang"], "title": "Balancing Stability and Plasticity in Pretrained Detector: A Dual-Path Framework for Incremental Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "The balance between stability and plasticity remains a fundamental challenge\nin pretrained model-based incremental object detection (PTMIOD). While existing\nPTMIOD methods demonstrate strong performance on in-domain tasks aligned with\npretraining data, their plasticity to cross-domain scenarios remains\nunderexplored. Through systematic component-wise analysis of pretrained\ndetectors, we reveal a fundamental discrepancy: the localization modules\ndemonstrate inherent cross-domain stability-preserving precise bounding box\nestimation across distribution shifts-while the classification components\nrequire enhanced plasticity to mitigate discriminability degradation in\ncross-domain scenarios. Motivated by these findings, we propose a dual-path\nframework built upon pretrained DETR-based detectors which decouples\nlocalization stability and classification plasticity: the localization path\nmaintains stability to preserve pretrained localization knowledge, while the\nclassification path facilitates plasticity via parameter-efficient fine-tuning\nand resists forgetting with pseudo-feature replay. Extensive evaluations on\nboth in-domain (MS COCO and PASCAL VOC) and cross-domain (TT100K) benchmarks\nshow state-of-the-art performance, demonstrating our method's ability to\neffectively balance stability and plasticity in PTMIOD, achieving robust\ncross-domain adaptation and strong retention of anti-forgetting capabilities."}
{"id": "2504.10250", "pdf": "https://arxiv.org/pdf/2504.10250", "abs": "https://arxiv.org/abs/2504.10250", "authors": ["Eugene Yang", "Nicola Tonellotto", "Dawn Lawrie", "Sean MacAvaney", "James Mayfield", "Douglas W. Oard", "Scott Miller"], "title": "MURR: Model Updating with Regularized Replay for Searching a Document Stream", "categories": ["cs.IR", "cs.CL"], "comment": "Published at ECIR 2025. 16 pages, 4 figures", "summary": "The Internet produces a continuous stream of new documents and user-generated\nqueries. These naturally change over time based on events in the world and the\nevolution of language. Neural retrieval models that were trained once on a\nfixed set of query-document pairs will quickly start misrepresenting\nnewly-created content and queries, leading to less effective retrieval.\nTraditional statistical sparse retrieval can update collection statistics to\nreflect these changes in the use of language in documents and queries. In\ncontrast, continued fine-tuning of the language model underlying neural\nretrieval approaches such as DPR and ColBERT creates incompatibility with\npreviously-encoded documents. Re-encoding and re-indexing all\npreviously-processed documents can be costly. In this work, we explore updating\na neural dual encoder retrieval model without reprocessing past documents in\nthe stream. We propose MURR, a model updating strategy with regularized replay,\nto ensure the model can still faithfully search existing documents without\nreprocessing, while continuing to update the model for the latest topics. In\nour simulated streaming environments, we show that fine-tuning models using\nMURR leads to more effective and more consistent retrieval results than other\nstrategies as the stream of documents and queries progresses."}
{"id": "2504.10242", "pdf": "https://arxiv.org/pdf/2504.10242", "abs": "https://arxiv.org/abs/2504.10242", "authors": ["Tianyu Xin", "Jin-Liang Xiao", "Zeyu Xia", "Shan Yin", "Liang-Jian Deng"], "title": "CAT: A Conditional Adaptation Tailor for Efficient and Effective Instance-Specific Pansharpening on Real-World Data", "categories": ["cs.CV"], "comment": null, "summary": "Pansharpening is a crucial remote sensing technique that fuses low-resolution\nmultispectral (LRMS) images with high-resolution panchromatic (PAN) images to\ngenerate high-resolution multispectral (HRMS) imagery. Although deep learning\ntechniques have significantly advanced pansharpening, many existing methods\nsuffer from limited cross-sensor generalization and high computational\noverhead, restricting their real-time applications. To address these\nchallenges, we propose an efficient framework that quickly adapts to a specific\ninput instance, completing both training and inference in a short time. Our\nframework splits the input image into multiple patches, selects a subset for\nunsupervised CAT training, and then performs inference on all patches,\nstitching them into the final output. The CAT module, integrated between the\nfeature extraction and channel transformation stages of a pre-trained network,\ntailors the fused features and fixes the parameters for efficient inference,\ngenerating improved results. Our approach offers two key advantages: (1)\n$\\textit{Improved Generalization Ability}$: by mitigating cross-sensor\ndegradation, our model--although pre-trained on a specific dataset--achieves\nsuperior performance on datasets captured by other sensors; (2)\n$\\textit{Enhanced Computational Efficiency}$: the CAT-enhanced network can\nswiftly adapt to the test sample using the single LRMS-PAN pair input, without\nrequiring extensive large-scale data retraining. Experiments on the real-world\ndata from WorldView-3 and WorldView-2 datasets demonstrate that our method\nachieves state-of-the-art performance on cross-sensor real-world data, while\nachieving both training and inference of $512\\times512$ image within\n$\\textit{0.4 seconds}$ and $4000\\times4000$ image within $\\textit{3 seconds}$\nat the fastest setting on a commonly used RTX 3090 GPU."}
{"id": "2504.10277", "pdf": "https://arxiv.org/pdf/2504.10277", "abs": "https://arxiv.org/abs/2504.10277", "authors": ["Pierre Le Jeune", "Jiaen Liu", "Luca Rossi", "Matteo Dora"], "title": "RealHarm: A Collection of Real-World Language Model Application Failures", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "Language model deployments in consumer-facing applications introduce numerous\nrisks. While existing research on harms and hazards of such applications\nfollows top-down approaches derived from regulatory frameworks and theoretical\nanalyses, empirical evidence of real-world failure modes remains underexplored.\nIn this work, we introduce RealHarm, a dataset of annotated problematic\ninteractions with AI agents built from a systematic review of publicly reported\nincidents. Analyzing harms, causes, and hazards specifically from the\ndeployer's perspective, we find that reputational damage constitutes the\npredominant organizational harm, while misinformation emerges as the most\ncommon hazard category. We empirically evaluate state-of-the-art guardrails and\ncontent moderation systems to probe whether such systems would have prevented\nthe incidents, revealing a significant gap in the protection of AI\napplications."}
{"id": "2504.10254", "pdf": "https://arxiv.org/pdf/2504.10254", "abs": "https://arxiv.org/abs/2504.10254", "authors": ["Xuqiang Cao", "Linnan Zhao", "Jiaxuan Zhao", "Fang Liu", "Puhua Chen", "Wenping Ma"], "title": "MASSeg : 2nd Technical Report for 4th PVUW MOSE Track", "categories": ["cs.CV", "cs.AI"], "comment": "5 pages,4 figures,Technical report on Complex Video Object\n  Segmentation", "summary": "Complex video object segmentation continues to face significant challenges in\nsmall object recognition, occlusion handling, and dynamic scene modeling. This\nreport presents our solution, which ranked second in the MOSE track of CVPR\n2025 PVUW Challenge. Based on an existing segmentation framework, we propose an\nimproved model named MASSeg for complex video object segmentation, and\nconstruct an enhanced dataset, MOSE+, which includes typical scenarios with\nocclusions, cluttered backgrounds, and small target instances. During training,\nwe incorporate a combination of inter-frame consistent and inconsistent data\naugmentation strategies to improve robustness and generalization. During\ninference, we design a mask output scaling strategy to better adapt to varying\nobject sizes and occlusion levels. As a result, MASSeg achieves a J score of\n0.8250, F score of 0.9007, and a J&F score of 0.8628 on the MOSE test set."}
{"id": "2504.10352", "pdf": "https://arxiv.org/pdf/2504.10352", "abs": "https://arxiv.org/abs/2504.10352", "authors": ["Yifan Yang", "Shujie Liu", "Jinyu Li", "Yuxuan Hu", "Haibin Wu", "Hui Wang", "Jianwei Yu", "Lingwei Meng", "Haiyang Sun", "Yanqing Liu", "Yan Lu", "Kai Yu", "Xie Chen"], "title": "Pseudo-Autoregressive Neural Codec Language Models for Efficient Zero-Shot Text-to-Speech Synthesis", "categories": ["eess.AS", "cs.CL"], "comment": "Submitted to ACM MM 2025", "summary": "Recent zero-shot text-to-speech (TTS) systems face a common dilemma:\nautoregressive (AR) models suffer from slow generation and lack duration\ncontrollability, while non-autoregressive (NAR) models lack temporal modeling\nand typically require complex designs. In this paper, we introduce a novel\npseudo-autoregressive (PAR) codec language modeling approach that unifies AR\nand NAR modeling. Combining explicit temporal modeling from AR with parallel\ngeneration from NAR, PAR generates dynamic-length spans at fixed time steps.\nBuilding on PAR, we propose PALLE, a two-stage TTS system that leverages PAR\nfor initial generation followed by NAR refinement. In the first stage, PAR\nprogressively generates speech tokens along the time dimension, with each step\npredicting all positions in parallel but only retaining the left-most span. In\nthe second stage, low-confidence tokens are iteratively refined in parallel,\nleveraging the global contextual information. Experiments demonstrate that\nPALLE, trained on LibriTTS, outperforms state-of-the-art systems trained on\nlarge-scale data, including F5-TTS, E2-TTS, and MaskGCT, on the LibriSpeech\ntest-clean set in terms of speech quality, speaker similarity, and\nintelligibility, while achieving up to ten times faster inference speed. Audio\nsamples are available at https://anonymous-palle.github.io."}
{"id": "2504.10258", "pdf": "https://arxiv.org/pdf/2504.10258", "abs": "https://arxiv.org/abs/2504.10258", "authors": ["Shuai Liu", "Youmeng Li", "Jizeng Wei"], "title": "XY-Cut++: Advanced Layout Ordering via Hierarchical Mask Mechanism on a Novel Benchmark", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Document Reading Order Recovery is a fundamental task in document image\nunderstanding, playing a pivotal role in enhancing Retrieval-Augmented\nGeneration (RAG) and serving as a critical preprocessing step for large\nlanguage models (LLMs). Existing methods often struggle with complex\nlayouts(e.g., multi-column newspapers), high-overhead interactions between\ncross-modal elements (visual regions and textual semantics), and a lack of\nrobust evaluation benchmarks. We introduce XY-Cut++, an advanced layout\nordering method that integrates pre-mask processing, multi-granularity\nsegmentation, and cross-modal matching to address these challenges. Our method\nsignificantly enhances layout ordering accuracy compared to traditional XY-Cut\ntechniques. Specifically, XY-Cut++ achieves state-of-the-art performance (98.8\nBLEU overall) while maintaining simplicity and efficiency. It outperforms\nexisting baselines by up to 24\\% and demonstrates consistent accuracy across\nsimple and complex layouts on the newly introduced DocBench-100 dataset. This\nadvancement establishes a reliable foundation for document structure recovery,\nsetting a new standard for layout ordering tasks and facilitating more\neffective RAG and LLM preprocessing."}
{"id": "2504.10384", "pdf": "https://arxiv.org/pdf/2504.10384", "abs": "https://arxiv.org/abs/2504.10384", "authors": ["Alana Marie Dee", "Sajjad Moazeni"], "title": "A 10.8mW Mixed-Signal Simulated Bifurcation Ising Solver using SRAM Compute-In-Memory with 0.6us Time-to-Solution", "categories": ["eess.SY", "cs.CL", "cs.SY"], "comment": null, "summary": "Combinatorial optimization problems are funda- mental for various fields\nranging from finance to wireless net- works. This work presents a simulated\nbifurcation (SB) Ising solver in CMOS for NP-hard optimization problems. Analog\ndomain computing led to a superior implementation of this algorithm as inherent\nand injected noise is required in SB Ising solvers. The architecture novelties\ninclude the use of SRAM compute-in-memory (CIM) to accelerate bifurcation as\nwell as the generation and injection of optimal decaying noise in the analog\ndomain. We propose a novel 10-T SRAM cell capable of performing ternary\nmultiplication. When measured with 60- node, 50% density, random, binary MAXCUT\ngraphs, this all- to-all connected Ising solver reliably achieves above 93% of\nthe ground state solution in 0.6us with 10.8mW average power in TSMC 180nm\nCMOS. Our chip achieves an order of magnitude improvement in time-to-solution\nand power compared to previously proposed Ising solvers in CMOS and other\nplatforms."}
{"id": "2504.10267", "pdf": "https://arxiv.org/pdf/2504.10267", "abs": "https://arxiv.org/abs/2504.10267", "authors": ["Mengdi Wang", "Efe Bozkir", "Enkelejda Kasneci"], "title": "Trade-offs in Privacy-Preserving Eye Tracking through Iris Obfuscation: A Benchmarking Study", "categories": ["cs.CV"], "comment": "The 25th International Conference on Digital Signal Processing (DSP\n  2025)", "summary": "Recent developments in hardware, computer graphics, and AI may soon enable\nAR/VR head-mounted displays (HMDs) to become everyday devices like smartphones\nand tablets. Eye trackers within HMDs provide a special opportunity for such\nsetups as it is possible to facilitate gaze-based research and interaction.\nHowever, estimating users' gaze information often requires raw eye images and\nvideos that contain iris textures, which are considered a gold standard\nbiometric for user authentication, and this raises privacy concerns. Previous\nresearch in the eye-tracking community focused on obfuscating iris textures\nwhile keeping utility tasks such as gaze estimation accurate. Despite these\nattempts, there is no comprehensive benchmark that evaluates state-of-the-art\napproaches. Considering all, in this paper, we benchmark blurring, noising,\ndownsampling, rubber sheet model, and iris style transfer to obfuscate user\nidentity, and compare their impact on image quality, privacy, utility, and risk\nof imposter attack on two datasets. We use eye segmentation and gaze estimation\nas utility tasks, and reduction in iris recognition accuracy as a measure of\nprivacy protection, and false acceptance rate to estimate risk of attack. Our\nexperiments show that canonical image processing methods like blurring and\nnoising cause a marginal impact on deep learning-based tasks. While\ndownsampling, rubber sheet model, and iris style transfer are effective in\nhiding user identifiers, iris style transfer, with higher computation cost,\noutperforms others in both utility tasks, and is more resilient against spoof\nattacks. Our analyses indicate that there is no universal optimal approach to\nbalance privacy, utility, and computation burden. Therefore, we recommend\npractitioners consider the strengths and weaknesses of each approach, and\npossible combinations of those to reach an optimal privacy-utility trade-off."}
{"id": "2504.10443", "pdf": "https://arxiv.org/pdf/2504.10443", "abs": "https://arxiv.org/abs/2504.10443", "authors": ["Haoran Hao", "Jiaming Han", "Yiyuan Zhang", "Xiangyu Yue"], "title": "Multimodal Long Video Modeling Based on Temporal Dynamic Context", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have led to significant\nbreakthroughs in video understanding. However, existing models still struggle\nwith long video processing due to the context length constraint of LLMs and the\nvast amount of information within the video. Although some recent methods are\ndesigned for long video understanding, they often lose crucial information\nduring token compression and struggle with additional modality like audio. In\nthis work, we propose a dynamic long video encoding method utilizing the\ntemporal relationship between frames, named Temporal Dynamic Context (TDC).\nFirstly, we segment the video into semantically consistent scenes based on\ninter-frame similarities, then encode each frame into tokens using visual-audio\nencoders. Secondly, we propose a novel temporal context compressor to reduce\nthe number of tokens within each segment. Specifically, we employ a query-based\nTransformer to aggregate video, audio, and instruction text tokens into a\nlimited set of temporal context tokens. Finally, we feed the static frame\ntokens and the temporal context tokens into the LLM for video understanding.\nFurthermore, to handle extremely long videos, we propose a training-free\nchain-of-thought strategy that progressively extracts answers from multiple\nvideo segments. These intermediate answers serve as part of the reasoning\nprocess and contribute to the final answer. We conduct extensive experiments on\ngeneral video understanding and audio-video understanding benchmarks, where our\nmethod demonstrates strong performance. The code and models are available at\nhttps://github.com/Hoar012/TDC-Video."}
{"id": "2504.10275", "pdf": "https://arxiv.org/pdf/2504.10275", "abs": "https://arxiv.org/abs/2504.10275", "authors": ["Harsh Yadav", "Maximilian Schaefer", "Kun Zhao", "Tobias Meisen"], "title": "LMFormer: Lane based Motion Prediction Transformer", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted: Autonomous Driving Workshop, CVPR 2025", "summary": "Motion prediction plays an important role in autonomous driving. This study\npresents LMFormer, a lane-aware transformer network for trajectory prediction\ntasks. In contrast to previous studies, our work provides a simple mechanism to\ndynamically prioritize the lanes and shows that such a mechanism introduces\nexplainability into the learning behavior of the network. Additionally,\nLMFormer uses the lane connection information at intersections, lane merges,\nand lane splits, in order to learn long-range dependency in lane structure.\nMoreover, we also address the issue of refining the predicted trajectories and\npropose an efficient method for iterative refinement through stacked\ntransformer layers. For benchmarking, we evaluate LMFormer on the nuScenes\ndataset and demonstrate that it achieves SOTA performance across multiple\nmetrics. Furthermore, the Deep Scenario dataset is used to not only illustrate\ncross-dataset network performance but also the unification capabilities of\nLMFormer to train on multiple datasets and achieve better performance."}
{"id": "2504.10445", "pdf": "https://arxiv.org/pdf/2504.10445", "abs": "https://arxiv.org/abs/2504.10445", "authors": ["Suyu Ye", "Haojun Shi", "Darren Shih", "Hyokun Yun", "Tanya Roosta", "Tianmin Shu"], "title": "RealWebAssist: A Benchmark for Long-Horizon Web Assistance with Real-World Users", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "Project Website: https://scai.cs.jhu.edu/projects/RealWebAssist/\n  Code: https://github.com/SCAI-JHU/RealWebAssist", "summary": "To achieve successful assistance with long-horizon web-based tasks, AI agents\nmust be able to sequentially follow real-world user instructions over a long\nperiod. Unlike existing web-based agent benchmarks, sequential instruction\nfollowing in the real world poses significant challenges beyond performing a\nsingle, clearly defined task. For instance, real-world human instructions can\nbe ambiguous, require different levels of AI assistance, and may evolve over\ntime, reflecting changes in the user's mental state. To address this gap, we\nintroduce RealWebAssist, a novel benchmark designed to evaluate sequential\ninstruction-following in realistic scenarios involving long-horizon\ninteractions with the web, visual GUI grounding, and understanding ambiguous\nreal-world user instructions. RealWebAssist includes a dataset of sequential\ninstructions collected from real-world human users. Each user instructs a\nweb-based assistant to perform a series of tasks on multiple websites. A\nsuccessful agent must reason about the true intent behind each instruction,\nkeep track of the mental state of the user, understand user-specific routines,\nand ground the intended tasks to actions on the correct GUI elements. Our\nexperimental results show that state-of-the-art models struggle to understand\nand ground user instructions, posing critical challenges in following\nreal-world user instructions for long-horizon web assistance."}
{"id": "2504.10278", "pdf": "https://arxiv.org/pdf/2504.10278", "abs": "https://arxiv.org/abs/2504.10278", "authors": ["Jinyue Zhang", "Xiangrong Zhang", "Zhongjian Huang", "Tianyang Zhang", "Yifei Jiang", "Licheng Jiao"], "title": "DiffMOD: Progressive Diffusion Point Denoising for Moving Object Detection in Remote Sensing", "categories": ["cs.CV", "68T10", "I.4.8"], "comment": "9 pages, 7 figures", "summary": "Moving object detection (MOD) in remote sensing is significantly challenged\nby low resolution, extremely small object sizes, and complex noise\ninterference. Current deep learning-based MOD methods rely on probability\ndensity estimation, which restricts flexible information interaction between\nobjects and across temporal frames. To flexibly capture high-order inter-object\nand temporal relationships, we propose a point-based MOD in remote sensing.\nInspired by diffusion models, the network optimization is formulated as a\nprogressive denoising process that iteratively recovers moving object centers\nfrom sparse noisy points. Specifically, we sample scattered features from the\nbackbone outputs as atomic units for subsequent processing, while global\nfeature embeddings are aggregated to compensate for the limited coverage of\nsparse point features. By modeling spatial relative positions and semantic\naffinities, Spatial Relation Aggregation Attention is designed to enable\nhigh-order interactions among point-level features for enhanced object\nrepresentation. To enhance temporal consistency, the Temporal Propagation and\nGlobal Fusion module is designed, which leverages an implicit memory reasoning\nmechanism for robust cross-frame feature integration. To align with the\nprogressive denoising process, we propose a progressive MinK optimal transport\nassignment strategy that establishes specialized learning objectives at each\ndenoising level. Additionally, we introduce a missing loss function to\ncounteract the clustering tendency of denoised points around salient objects.\nExperiments on the RsData remote sensing MOD dataset show that our MOD method\nbased on scattered point denoising can more effectively explore potential\nrelationships between sparse moving objects and improve the detection\ncapability and temporal consistency."}
{"id": "2504.10458", "pdf": "https://arxiv.org/pdf/2504.10458", "abs": "https://arxiv.org/abs/2504.10458", "authors": ["Xiaobo Xia", "Run Luo"], "title": "GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents", "categories": ["cs.CV", "cs.CL", "cs.HC"], "comment": null, "summary": "Existing efforts in building Graphical User Interface (GUI) agents largely\nrely on the training paradigm of supervised fine-tuning on Large\nVision-Language Models (LVLMs). However, this approach not only demands\nextensive amounts of training data but also struggles to effectively understand\nGUI screenshots and generalize to unseen interfaces. The issue significantly\nlimits its application in real-world scenarios, especially for high-level\ntasks. Inspired by Reinforcement Fine-Tuning (RFT) in large reasoning models\n(e.g., DeepSeek-R1), which efficiently enhances the problem-solving\ncapabilities of large language models in real-world settings, we propose \\name,\nthe first reinforcement learning framework designed to enhance the GUI\ncapabilities of LVLMs in high-level real-world task scenarios, through unified\naction space rule modeling. By leveraging a small amount of carefully curated\nhigh-quality data across multiple platforms (including Windows, Linux, MacOS,\nAndroid, and Web) and employing policy optimization algorithms such as Group\nRelative Policy Optimization (GRPO) to update the model, \\name achieves\nsuperior performance using only 0.02\\% of the data (3K vs. 13M) compared to\nprevious state-of-the-art methods like OS-Atlas across eight benchmarks\nspanning three different platforms (mobile, desktop, and web). These results\ndemonstrate the immense potential of reinforcement learning based on unified\naction space rule modeling in improving the execution capabilities of LVLMs for\nreal-world GUI agent tasks."}
{"id": "2504.10288", "pdf": "https://arxiv.org/pdf/2504.10288", "abs": "https://arxiv.org/abs/2504.10288", "authors": ["Mathieu Manni", "Dmitry Karpov", "K. Joost Batenburg", "Sharon Shwartz", "Nicola Viganò"], "title": "Noise2Ghost: Self-supervised deep convolutional reconstruction for ghost imaging", "categories": ["cs.CV", "cs.LG", "physics.data-an"], "comment": null, "summary": "We present a new self-supervised deep-learning-based Ghost Imaging (GI)\nreconstruction method, which provides unparalleled reconstruction performance\nfor noisy acquisitions among unsupervised methods. We present the supporting\nmathematical framework and results from theoretical and real data use cases.\nSelf-supervision removes the need for clean reference data while offering\nstrong noise reduction. This provides the necessary tools for addressing\nsignal-to-noise ratio concerns for GI acquisitions in emerging and cutting-edge\nlow-light GI scenarios. Notable examples include micro- and nano-scale x-ray\nemission imaging, e.g., x-ray fluorescence imaging of dose-sensitive samples.\nTheir applications include in-vivo and in-operando case studies for biological\nsamples and batteries."}
{"id": "2504.10471", "pdf": "https://arxiv.org/pdf/2504.10471", "abs": "https://arxiv.org/abs/2504.10471", "authors": ["Chenghao Xiao", "Isaac Chung", "Imene Kerboua", "Jamie Stirling", "Xin Zhang", "Márton Kardos", "Roman Solomatin", "Noura Al Moubayed", "Kenneth Enevoldsen", "Niklas Muennighoff"], "title": "MIEB: Massive Image Embedding Benchmark", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Image representations are often evaluated through disjointed, task-specific\nprotocols, leading to a fragmented understanding of model capabilities. For\ninstance, it is unclear whether an image embedding model adept at clustering\nimages is equally good at retrieving relevant images given a piece of text. We\nintroduce the Massive Image Embedding Benchmark (MIEB) to evaluate the\nperformance of image and image-text embedding models across the broadest\nspectrum to date. MIEB spans 38 languages across 130 individual tasks, which we\ngroup into 8 high-level categories. We benchmark 50 models across our\nbenchmark, finding that no single method dominates across all task categories.\nWe reveal hidden capabilities in advanced vision models such as their accurate\nvisual representation of texts, and their yet limited capabilities in\ninterleaved encodings and matching images and texts in the presence of\nconfounders. We also show that the performance of vision encoders on MIEB\ncorrelates highly with their performance when used in multimodal large language\nmodels. Our code, dataset, and leaderboard are publicly available at\nhttps://github.com/embeddings-benchmark/mteb."}
{"id": "2504.10316", "pdf": "https://arxiv.org/pdf/2504.10316", "abs": "https://arxiv.org/abs/2504.10316", "authors": ["Huiqi Wu", "Jianbo Mei", "Yingjie Huang", "Yining Xu", "Jingjiao You", "Yilong Liu", "Li Yao"], "title": "ESCT3D: Efficient and Selectively Controllable Text-Driven 3D Content Generation with Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, significant advancements have been made in text-driven 3D\ncontent generation. However, several challenges remain. In practical\napplications, users often provide extremely simple text inputs while expecting\nhigh-quality 3D content. Generating optimal results from such minimal text is a\ndifficult task due to the strong dependency of text-to-3D models on the quality\nof input prompts. Moreover, the generation process exhibits high variability,\nmaking it difficult to control. Consequently, multiple iterations are typically\nrequired to produce content that meets user expectations, reducing generation\nefficiency. To address this issue, we propose GPT-4V for self-optimization,\nwhich significantly enhances the efficiency of generating satisfactory content\nin a single attempt. Furthermore, the controllability of text-to-3D generation\nmethods has not been fully explored. Our approach enables users to not only\nprovide textual descriptions but also specify additional conditions, such as\nstyle, edges, scribbles, poses, or combinations of multiple conditions,\nallowing for more precise control over the generated 3D content. Additionally,\nduring training, we effectively integrate multi-view information, including\nmulti-view depth, masks, features, and images, to address the common Janus\nproblem in 3D content generation. Extensive experiments demonstrate that our\nmethod achieves robust generalization, facilitating the efficient and\ncontrollable generation of high-quality 3D content."}
{"id": "2504.10317", "pdf": "https://arxiv.org/pdf/2504.10317", "abs": "https://arxiv.org/abs/2504.10317", "authors": ["Yuxin Wen", "Jim Wu", "Ajay Jain", "Tom Goldstein", "Ashwinee Panda"], "title": "Analysis of Attention in Video Diffusion Transformers", "categories": ["cs.CV"], "comment": null, "summary": "We conduct an in-depth analysis of attention in video diffusion transformers\n(VDiTs) and report a number of novel findings. We identify three key properties\nof attention in VDiTs: Structure, Sparsity, and Sinks. Structure: We observe\nthat attention patterns across different VDiTs exhibit similar structure across\ndifferent prompts, and that we can make use of the similarity of attention\npatterns to unlock video editing via self-attention map transfer. Sparse: We\nstudy attention sparsity in VDiTs, finding that proposed sparsity methods do\nnot work for all VDiTs, because some layers that are seemingly sparse cannot be\nsparsified. Sinks: We make the first study of attention sinks in VDiTs,\ncomparing and contrasting them to attention sinks in language models. We\npropose a number of future directions that can make use of our insights to\nimprove the efficiency-quality Pareto frontier for VDiTs."}
{"id": "2504.10320", "pdf": "https://arxiv.org/pdf/2504.10320", "abs": "https://arxiv.org/abs/2504.10320", "authors": ["Zongcan Ding", "Haodong Zhang", "Peng Wu", "Guansong Pang", "Zhiwei Yang", "Peng Wang", "Yanning Zhang"], "title": "SlowFastVAD: Video Anomaly Detection via Integrating Simple Detector and RAG-Enhanced Vision-Language Model", "categories": ["cs.CV"], "comment": null, "summary": "Video anomaly detection (VAD) aims to identify unexpected events in videos\nand has wide applications in safety-critical domains. While semi-supervised\nmethods trained on only normal samples have gained traction, they often suffer\nfrom high false alarm rates and poor interpretability. Recently,\nvision-language models (VLMs) have demonstrated strong multimodal reasoning\ncapabilities, offering new opportunities for explainable anomaly detection.\nHowever, their high computational cost and lack of domain adaptation hinder\nreal-time deployment and reliability. Inspired by dual complementary pathways\nin human visual perception, we propose SlowFastVAD, a hybrid framework that\nintegrates a fast anomaly detector with a slow anomaly detector (namely a\nretrieval augmented generation (RAG) enhanced VLM), to address these\nlimitations. Specifically, the fast detector first provides coarse anomaly\nconfidence scores, and only a small subset of ambiguous segments, rather than\nthe entire video, is further analyzed by the slower yet more interpretable VLM\nfor elaborate detection and reasoning. Furthermore, to adapt VLMs to\ndomain-specific VAD scenarios, we construct a knowledge base including normal\npatterns based on few normal samples and abnormal patterns inferred by VLMs.\nDuring inference, relevant patterns are retrieved and used to augment prompts\nfor anomaly reasoning. Finally, we smoothly fuse the anomaly confidence of fast\nand slow detectors to enhance robustness of anomaly detection. Extensive\nexperiments on four benchmarks demonstrate that SlowFastVAD effectively\ncombines the strengths of both fast and slow detectors, and achieves remarkable\ndetection accuracy and interpretability with significantly reduced\ncomputational overhead, making it well-suited for real-world VAD applications\nwith high reliability requirements."}
{"id": "2504.10329", "pdf": "https://arxiv.org/pdf/2504.10329", "abs": "https://arxiv.org/abs/2504.10329", "authors": ["Xingyu Lu", "Yuhang Hu", "YiFan Zhang", "Kaiyu Jiang", "Changyi Liu", "Tianke Zhang", "Jinpeng Wang", "Bin Wen", "Chun Yuan", "Fan Yang", "Tingting Gao", "Di Zhang"], "title": "InstructEngine: Instruction-driven Text-to-Image Alignment", "categories": ["cs.CV"], "comment": "8 pages, 7 figures", "summary": "Reinforcement Learning from Human/AI Feedback (RLHF/RLAIF) has been\nextensively utilized for preference alignment of text-to-image models. Existing\nmethods face certain limitations in terms of both data and algorithm. For\ntraining data, most approaches rely on manual annotated preference data, either\nby directly fine-tuning the generators or by training reward models to provide\ntraining signals. However, the high annotation cost makes them difficult to\nscale up, the reward model consumes extra computation and cannot guarantee\naccuracy. From an algorithmic perspective, most methods neglect the value of\ntext and only take the image feedback as a comparative signal, which is\ninefficient and sparse. To alleviate these drawbacks, we propose the\nInstructEngine framework. Regarding annotation cost, we first construct a\ntaxonomy for text-to-image generation, then develop an automated data\nconstruction pipeline based on it. Leveraging advanced large multimodal models\nand human-defined rules, we generate 25K text-image preference pairs. Finally,\nwe introduce cross-validation alignment method, which refines data efficiency\nby organizing semantically analogous samples into mutually comparable pairs.\nEvaluations on DrawBench demonstrate that InstructEngine improves SD v1.5 and\nSDXL's performance by 10.53% and 5.30%, outperforming state-of-the-art\nbaselines, with ablation study confirming the benefits of InstructEngine's all\ncomponents. A win rate of over 50% in human reviews also proves that\nInstructEngine better aligns with human preferences."}
{"id": "2504.10331", "pdf": "https://arxiv.org/pdf/2504.10331", "abs": "https://arxiv.org/abs/2504.10331", "authors": ["Hao Sun", "Fenggen Yu", "Huiyao Xu", "Tao Zhang", "Changqing Zou"], "title": "LL-Gaussian: Low-Light Scene Reconstruction and Enhancement via Gaussian Splatting for Novel View Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Novel view synthesis (NVS) in low-light scenes remains a significant\nchallenge due to degraded inputs characterized by severe noise, low dynamic\nrange (LDR) and unreliable initialization. While recent NeRF-based approaches\nhave shown promising results, most suffer from high computational costs, and\nsome rely on carefully captured or pre-processed data--such as RAW sensor\ninputs or multi-exposure sequences--which severely limits their practicality.\nIn contrast, 3D Gaussian Splatting (3DGS) enables real-time rendering with\ncompetitive visual fidelity; however, existing 3DGS-based methods struggle with\nlow-light sRGB inputs, resulting in unstable Gaussian initialization and\nineffective noise suppression. To address these challenges, we propose\nLL-Gaussian, a novel framework for 3D reconstruction and enhancement from\nlow-light sRGB images, enabling pseudo normal-light novel view synthesis. Our\nmethod introduces three key innovations: 1) an end-to-end Low-Light Gaussian\nInitialization Module (LLGIM) that leverages dense priors from learning-based\nMVS approach to generate high-quality initial point clouds; 2) a dual-branch\nGaussian decomposition model that disentangles intrinsic scene properties\n(reflectance and illumination) from transient interference, enabling stable and\ninterpretable optimization; 3) an unsupervised optimization strategy guided by\nboth physical constrains and diffusion prior to jointly steer decomposition and\nenhancement. Additionally, we contribute a challenging dataset collected in\nextreme low-light environments and demonstrate the effectiveness of\nLL-Gaussian. Compared to state-of-the-art NeRF-based methods, LL-Gaussian\nachieves up to 2,000 times faster inference and reduces training time to just\n2%, while delivering superior reconstruction and rendering quality."}
{"id": "2504.10350", "pdf": "https://arxiv.org/pdf/2504.10350", "abs": "https://arxiv.org/abs/2504.10350", "authors": ["Filipa Lino", "Carlos Santiago", "Manuel Marques"], "title": "Benchmarking 3D Human Pose Estimation Models Under Occlusions", "categories": ["cs.CV"], "comment": null, "summary": "This paper addresses critical challenges in 3D Human Pose Estimation (HPE) by\nanalyzing the robustness and sensitivity of existing models to occlusions,\ncamera position, and action variability. Using a novel synthetic dataset,\nBlendMimic3D, which includes diverse scenarios with multi-camera setups and\nseveral occlusion types, we conduct specific tests on several state-of-the-art\nmodels. Our study focuses on the discrepancy in keypoint formats between common\ndatasets such as Human3.6M, and 2D datasets such as COCO, commonly used for 2D\ndetection models and frequently input of 3D HPE models. Our work explores the\nimpact of occlusions on model performance and the generality of models trained\nexclusively under standard conditions. The findings suggest significant\nsensitivity to occlusions and camera settings, revealing a need for models that\nbetter adapt to real-world variability and occlusion scenarios. This research\ncontributed to ongoing efforts to improve the fidelity and applicability of 3D\nHPE systems in complex environments."}
{"id": "2504.10351", "pdf": "https://arxiv.org/pdf/2504.10351", "abs": "https://arxiv.org/abs/2504.10351", "authors": ["Kaiwen Zheng", "Xuri Ge", "Junchen Fu", "Jun Peng", "Joemon M. Jose"], "title": "Multimodal Representation Learning Techniques for Comprehensive Facial State Analysis", "categories": ["cs.CV"], "comment": "Accepted by ICME2025", "summary": "Multimodal foundation models have significantly improved feature\nrepresentation by integrating information from multiple modalities, making them\nhighly suitable for a broader set of applications. However, the exploration of\nmultimodal facial representation for understanding perception has been limited.\nUnderstanding and analyzing facial states, such as Action Units (AUs) and\nemotions, require a comprehensive and robust framework that bridges visual and\nlinguistic modalities. In this paper, we present a comprehensive pipeline for\nmultimodal facial state analysis. First, we compile a new Multimodal Face\nDataset (MFA) by generating detailed multilevel language descriptions of face,\nincorporating Action Unit (AU) and emotion descriptions, by leveraging GPT-4o.\nSecond, we introduce a novel Multilevel Multimodal Face Foundation model (MF^2)\ntailored for Action Unit (AU) and emotion recognition. Our model incorporates\ncomprehensive visual feature modeling at both local and global levels of face\nimage, enhancing its ability to represent detailed facial appearances. This\ndesign aligns visual representations with structured AU and emotion\ndescriptions, ensuring effective cross-modal integration. Third, we develop a\nDecoupled Fine-Tuning Network (DFN) that efficiently adapts MF^2 across various\ntasks and datasets. This approach not only reduces computational overhead but\nalso broadens the applicability of the foundation model to diverse scenarios.\nExperimentation show superior performance for AU and emotion detection tasks."}
{"id": "2504.10353", "pdf": "https://arxiv.org/pdf/2504.10353", "abs": "https://arxiv.org/abs/2504.10353", "authors": ["Jeremiah Giordani"], "title": "Patch and Shuffle: A Preprocessing Technique for Texture Classification in Autonomous Cementitious Fabrication", "categories": ["cs.CV"], "comment": "Originally completed as a final project for CEE 374 at Princeton\n  University", "summary": "Autonomous fabrication systems are transforming construction and\nmanufacturing, yet they remain vulnerable to print errors. Texture\nclassification is a key component of computer vision systems that enable\nreal-time monitoring and adjustment during cementitious fabrication.\nTraditional classification methods often rely on global image features, which\ncan bias the model toward semantic content rather than low-level textures. In\nthis paper, we introduce a novel preprocessing technique called \"patch and\nshuffle,\" which segments input images into smaller patches, shuffles them, and\nreconstructs a jumbled image before classification. This transformation removes\nsemantic context, forcing the classifier to rely on local texture features.\n  We evaluate this approach on a dataset of extruded cement images, using a\nResNet-18-based architecture. Our experiments compare the patch and shuffle\nmethod to a standard pipeline, holding all other factors constant. Results show\na significant improvement in accuracy: the patch and shuffle model achieved\n90.64% test accuracy versus 72.46% for the baseline. These findings suggest\nthat disrupting global structure enhances performance in texture-based\nclassification tasks.\n  This method has implications for broader vision tasks where low-level\nfeatures matter more than high-level semantics. The technique may improve\nclassification in applications ranging from fabrication monitoring to medical\nimaging."}
{"id": "2504.10358", "pdf": "https://arxiv.org/pdf/2504.10358", "abs": "https://arxiv.org/abs/2504.10358", "authors": ["Rui Chen", "Lei Sun", "Jing Tang", "Geng Li", "Xiangxiang Chu"], "title": "FingER: Content Aware Fine-grained Evaluation with Reasoning for AI-Generated Videos", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 4 figures", "summary": "Recent advances in video generation have posed great challenges in the\nassessment of AI-generated content, particularly with the emergence of\nincreasingly sophisticated models. The various inconsistencies and defects\nobserved in such videos are inherently complex, making overall scoring\nnotoriously difficult. In this paper, we emphasize the critical importance of\nintegrating fine-grained reasoning into video evaluation, and we propose\n$\\textbf{F}$ing$\\textbf{ER}$, a novel entity-level reasoning evaluation\nframework that first automatically generates $\\textbf{F}$ine-grained\n$\\textbf{E}$ntity-level questions, and then answers those questions by a\n$\\textbf{R}$easoning model with scores, which can be subsequently weighted\nsummed to an overall score for different applications. Specifically, we\nleverage LLMs to derive entity-level questions across five distinct\nperspectives, which (i) often focus on some specific entities of the content,\nthereby making answering or scoring much easier by MLLMs, and (ii) are more\ninterpretable. Then we construct a FingER dataset, consisting of approximately\n3.3k videos and corresponding 60k fine-grained QA annotations, each with\ndetailed reasons. Based on that, we further investigate various training\nprotocols to best incentivize the reasoning capability of MLLMs for correct\nanswer prediction. Extensive experiments demonstrate that a reasoning model\ntrained using Group Relative Policy Optimization (GRPO) with a cold-start\nstrategy achieves the best performance. Notably, our model surpasses existing\nmethods by a relative margin of $11.8\\%$ on GenAI-Bench and $5.5\\%$ on\nMonetBench with only 3.3k training videos, which is at most one-tenth of the\ntraining samples utilized by other methods. Our code and dataset will be\nreleased soon."}
{"id": "2504.10375", "pdf": "https://arxiv.org/pdf/2504.10375", "abs": "https://arxiv.org/abs/2504.10375", "authors": ["Maud Biquard", "Marie Chabert", "Florence Genin", "Christophe Latry", "Thomas Oberlin"], "title": "PG-DPIR: An efficient plug-and-play method for high-count Poisson-Gaussian inverse problems", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Poisson-Gaussian noise describes the noise of various imaging systems thus\nthe need of efficient algorithms for Poisson-Gaussian image restoration. Deep\nlearning methods offer state-of-the-art performance but often require\nsensor-specific training when used in a supervised setting. A promising\nalternative is given by plug-and-play (PnP) methods, which consist in learning\nonly a regularization through a denoiser, allowing to restore images from\nseveral sources with the same network. This paper introduces PG-DPIR, an\nefficient PnP method for high-count Poisson-Gaussian inverse problems, adapted\nfrom DPIR. While DPIR is designed for white Gaussian noise, a naive adaptation\nto Poisson-Gaussian noise leads to prohibitively slow algorithms due to the\nabsence of a closed-form proximal operator. To address this, we adapt DPIR for\nthe specificities of Poisson-Gaussian noise and propose in particular an\nefficient initialization of the gradient descent required for the proximal step\nthat accelerates convergence by several orders of magnitude. Experiments are\nconducted on satellite image restoration and super-resolution problems.\nHigh-resolution realistic Pleiades images are simulated for the experiments,\nwhich demonstrate that PG-DPIR achieves state-of-the-art performance with\nimproved efficiency, which seems promising for on-ground satellite processing\nchains."}
{"id": "2504.10395", "pdf": "https://arxiv.org/pdf/2504.10395", "abs": "https://arxiv.org/abs/2504.10395", "authors": ["Ragini Bal Mahesh", "Ronny Hänsch"], "title": "Better Coherence, Better Height: Fusing Physical Models and Deep Learning for Forest Height Estimation from Interferometric SAR Data", "categories": ["cs.CV"], "comment": null, "summary": "Estimating forest height from Synthetic Aperture Radar (SAR) images often\nrelies on traditional physical models, which, while interpretable and\ndata-efficient, can struggle with generalization. In contrast, Deep Learning\n(DL) approaches lack physical insight. To address this, we propose CoHNet - an\nend-to-end framework that combines the best of both worlds: DL optimized with\nphysics-informed constraints. We leverage a pre-trained neural surrogate model\nto enforce physical plausibility through a unique training loss. Our\nexperiments show that this approach not only improves forest height estimation\naccuracy but also produces meaningful features that enhance the reliability of\npredictions."}
{"id": "2504.10400", "pdf": "https://arxiv.org/pdf/2504.10400", "abs": "https://arxiv.org/abs/2504.10400", "authors": ["Pietro Bonazzi", "Christian Vogt", "Michael Jost", "Lyes Khacef", "Federico Paredes-Vallés", "Michele Magno"], "title": "Towards Low-Latency Event-based Obstacle Avoidance on a FPGA-Drone", "categories": ["cs.CV"], "comment": null, "summary": "This work quantitatively evaluates the performance of event-based vision\nsystems (EVS) against conventional RGB-based models for action prediction in\ncollision avoidance on an FPGA accelerator. Our experiments demonstrate that\nthe EVS model achieves a significantly higher effective frame rate (1 kHz) and\nlower temporal (-20 ms) and spatial prediction errors (-20 mm) compared to the\nRGB-based model, particularly when tested on out-of-distribution data. The EVS\nmodel also exhibits superior robustness in selecting optimal evasion maneuvers.\nIn particular, in distinguishing between movement and stationary states, it\nachieves a 59 percentage point advantage in precision (78% vs. 19%) and a\nsubstantially higher F1 score (0.73 vs. 0.06), highlighting the susceptibility\nof the RGB model to overfitting. Further analysis in different combinations of\nspatial classes confirms the consistent performance of the EVS model in both\ntest data sets. Finally, we evaluated the system end-to-end and achieved a\nlatency of approximately 2.14 ms, with event aggregation (1 ms) and inference\non the processing unit (0.94 ms) accounting for the largest components. These\nresults underscore the advantages of event-based vision for real-time collision\navoidance and demonstrate its potential for deployment in resource-constrained\nenvironments."}
{"id": "2504.10409", "pdf": "https://arxiv.org/pdf/2504.10409", "abs": "https://arxiv.org/abs/2504.10409", "authors": ["Mingchuan Ma", "Yuhao Zhou", "Jindi Lv", "Yuxin Tian", "Dan Si", "Shujian Li", "Qing Ye", "Jiancheng Lv"], "title": "GPS: Distilling Compact Memories via Grid-based Patch Sampling for Efficient Online Class-Incremental Learning", "categories": ["cs.CV"], "comment": "10 pages, 10 figures", "summary": "Online class-incremental learning aims to enable models to continuously adapt\nto new classes with limited access to past data, while mitigating catastrophic\nforgetting. Replay-based methods address this by maintaining a small memory\nbuffer of previous samples, achieving competitive performance. For effective\nreplay under constrained storage, recent approaches leverage distilled data to\nenhance the informativeness of memory. However, such approaches often involve\nsignificant computational overhead due to the use of bi-level optimization.\nMotivated by these limitations, we introduce Grid-based Patch Sampling (GPS), a\nlightweight and effective strategy for distilling informative memory samples\nwithout relying on a trainable model. GPS generates informative samples by\nsampling a subset of pixels from the original image, yielding compact\nlow-resolution representations that preserve both semantic content and\nstructural information. During replay, these representations are reassembled to\nsupport training and evaluation. Experiments on extensive benchmarks\ndemonstrate that GRS can be seamlessly integrated into existing replay\nframeworks, leading to 3%-4% improvements in average end accuracy under\nmemory-constrained settings, with limited computational overhead."}
{"id": "2504.10414", "pdf": "https://arxiv.org/pdf/2504.10414", "abs": "https://arxiv.org/abs/2504.10414", "authors": ["Jiaxin Lu", "Chun-Hao Paul Huang", "Uttaran Bhattacharya", "Qixing Huang", "Yi Zhou"], "title": "HUMOTO: A 4D Dataset of Mocap Human Object Interactions", "categories": ["cs.CV"], "comment": "19 pages, 15 figures", "summary": "We present Human Motions with Objects (HUMOTO), a high-fidelity dataset of\nhuman-object interactions for motion generation, computer vision, and robotics\napplications. Featuring 736 sequences (7,875 seconds at 30 fps), HUMOTO\ncaptures interactions with 63 precisely modeled objects and 72 articulated\nparts. Our innovations include a scene-driven LLM scripting pipeline creating\ncomplete, purposeful tasks with natural progression, and a mocap-and-camera\nrecording setup to effectively handle occlusions. Spanning diverse activities\nfrom cooking to outdoor picnics, HUMOTO preserves both physical accuracy and\nlogical task flow. Professional artists rigorously clean and verify each\nsequence, minimizing foot sliding and object penetrations. We also provide\nbenchmarks compared to other datasets. HUMOTO's comprehensive full-body motion\nand simultaneous multi-object interactions address key data-capturing\nchallenges and provide opportunities to advance realistic human-object\ninteraction modeling across research domains with practical applications in\nanimation, robotics, and embodied AI systems. Project:\nhttps://jiaxin-lu.github.io/humoto/ ."}
{"id": "2504.10433", "pdf": "https://arxiv.org/pdf/2504.10433", "abs": "https://arxiv.org/abs/2504.10433", "authors": ["Jian Liu", "Wei Sun", "Hui Yang", "Jin Zheng", "Zichen Geng", "Hossein Rahmani", "Ajmal Mian"], "title": "MonoDiff9D: Monocular Category-Level 9D Object Pose Estimation via Diffusion Model", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted by ICRA'25", "summary": "Object pose estimation is a core means for robots to understand and interact\nwith their environment. For this task, monocular category-level methods are\nattractive as they require only a single RGB camera. However, current methods\nrely on shape priors or CAD models of the intra-class known objects. We propose\na diffusion-based monocular category-level 9D object pose generation method,\nMonoDiff9D. Our motivation is to leverage the probabilistic nature of diffusion\nmodels to alleviate the need for shape priors, CAD models, or depth sensors for\nintra-class unknown object pose estimation. We first estimate coarse depth via\nDINOv2 from the monocular image in a zero-shot manner and convert it into a\npoint cloud. We then fuse the global features of the point cloud with the input\nimage and use the fused features along with the encoded time step to condition\nMonoDiff9D. Finally, we design a transformer-based denoiser to recover the\nobject pose from Gaussian noise. Extensive experiments on two popular benchmark\ndatasets show that MonoDiff9D achieves state-of-the-art monocular\ncategory-level 9D object pose estimation accuracy without the need for shape\npriors or CAD models at any stage. Our code will be made public at\nhttps://github.com/CNJianLiu/MonoDiff9D."}
{"id": "2504.10434", "pdf": "https://arxiv.org/pdf/2504.10434", "abs": "https://arxiv.org/abs/2504.10434", "authors": ["Taihang Hu", "Linxuan Li", "Kai Wang", "Yaxing Wang", "Jian Yang", "Ming-Ming Cheng"], "title": "Anchor Token Matching: Implicit Structure Locking for Training-free AR Image Editing", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image generation has seen groundbreaking advancements with diffusion\nmodels, enabling high-fidelity synthesis and precise image editing through\ncross-attention manipulation. Recently, autoregressive (AR) models have\nre-emerged as powerful alternatives, leveraging next-token generation to match\ndiffusion models. However, existing editing techniques designed for diffusion\nmodels fail to translate directly to AR models due to fundamental differences\nin structural control. Specifically, AR models suffer from spatial poverty of\nattention maps and sequential accumulation of structural errors during image\nediting, which disrupt object layouts and global consistency. In this work, we\nintroduce Implicit Structure Locking (ISLock), the first training-free editing\nstrategy for AR visual models. Rather than relying on explicit attention\nmanipulation or fine-tuning, ISLock preserves structural blueprints by\ndynamically aligning self-attention patterns with reference images through the\nAnchor Token Matching (ATM) protocol. By implicitly enforcing structural\nconsistency in latent space, our method ISLock enables structure-aware editing\nwhile maintaining generative autonomy. Extensive experiments demonstrate that\nISLock achieves high-quality, structure-consistent edits without additional\ntraining and is superior or comparable to conventional editing techniques. Our\nfindings pioneer the way for efficient and flexible AR-based image editing,\nfurther bridging the performance gap between diffusion and autoregressive\ngenerative models. The code will be publicly available at\nhttps://github.com/hutaiHang/ATM"}
{"id": "2504.10443", "pdf": "https://arxiv.org/pdf/2504.10443", "abs": "https://arxiv.org/abs/2504.10443", "authors": ["Haoran Hao", "Jiaming Han", "Yiyuan Zhang", "Xiangyu Yue"], "title": "Multimodal Long Video Modeling Based on Temporal Dynamic Context", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have led to significant\nbreakthroughs in video understanding. However, existing models still struggle\nwith long video processing due to the context length constraint of LLMs and the\nvast amount of information within the video. Although some recent methods are\ndesigned for long video understanding, they often lose crucial information\nduring token compression and struggle with additional modality like audio. In\nthis work, we propose a dynamic long video encoding method utilizing the\ntemporal relationship between frames, named Temporal Dynamic Context (TDC).\nFirstly, we segment the video into semantically consistent scenes based on\ninter-frame similarities, then encode each frame into tokens using visual-audio\nencoders. Secondly, we propose a novel temporal context compressor to reduce\nthe number of tokens within each segment. Specifically, we employ a query-based\nTransformer to aggregate video, audio, and instruction text tokens into a\nlimited set of temporal context tokens. Finally, we feed the static frame\ntokens and the temporal context tokens into the LLM for video understanding.\nFurthermore, to handle extremely long videos, we propose a training-free\nchain-of-thought strategy that progressively extracts answers from multiple\nvideo segments. These intermediate answers serve as part of the reasoning\nprocess and contribute to the final answer. We conduct extensive experiments on\ngeneral video understanding and audio-video understanding benchmarks, where our\nmethod demonstrates strong performance. The code and models are available at\nhttps://github.com/Hoar012/TDC-Video."}
{"id": "2504.10452", "pdf": "https://arxiv.org/pdf/2504.10452", "abs": "https://arxiv.org/abs/2504.10452", "authors": ["Ramin Mousa", "Hadis Taherinia", "Khabiba Abdiyeva", "Amir Ali Bengari", "Mohammadmahdi Vahediahmar"], "title": "Integrating Vision and Location with Transformers: A Multimodal Deep Learning Framework for Medical Wound Analysis", "categories": ["cs.CV"], "comment": null, "summary": "Effective recognition of acute and difficult-to-heal wounds is a necessary\nstep in wound diagnosis. An efficient classification model can help wound\nspecialists classify wound types with less financial and time costs and also\nhelp in deciding on the optimal treatment method. Traditional machine learning\nmodels suffer from feature selection and are usually cumbersome models for\naccurate recognition. Recently, deep learning (DL) has emerged as a powerful\ntool in wound diagnosis. Although DL seems promising for wound type\nrecognition, there is still a large scope for improving the efficiency and\naccuracy of the model. In this study, a DL-based multimodal classifier was\ndeveloped using wound images and their corresponding locations to classify them\ninto multiple classes, including diabetic, pressure, surgical, and venous\nulcers. A body map was also created to provide location data, which can help\nwound specialists label wound locations more effectively. The model uses a\nVision Transformer to extract hierarchical features from input images, a\nDiscrete Wavelet Transform (DWT) layer to capture low and high frequency\ncomponents, and a Transformer to extract spatial features. The number of\nneurons and weight vector optimization were performed using three swarm-based\noptimization techniques (Monster Gorilla Toner (MGTO), Improved Gray Wolf\nOptimization (IGWO), and Fox Optimization Algorithm). The evaluation results\nshow that weight vector optimization using optimization algorithms can increase\ndiagnostic accuracy and make it a very effective approach for wound detection.\nIn the classification using the original body map, the proposed model was able\nto achieve an accuracy of 0.8123 using image data and an accuracy of 0.8007\nusing a combination of image data and wound location. Also, the accuracy of the\nmodel in combination with the optimization models varied from 0.7801 to 0.8342."}
{"id": "2504.10458", "pdf": "https://arxiv.org/pdf/2504.10458", "abs": "https://arxiv.org/abs/2504.10458", "authors": ["Xiaobo Xia", "Run Luo"], "title": "GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents", "categories": ["cs.CV", "cs.CL", "cs.HC"], "comment": null, "summary": "Existing efforts in building Graphical User Interface (GUI) agents largely\nrely on the training paradigm of supervised fine-tuning on Large\nVision-Language Models (LVLMs). However, this approach not only demands\nextensive amounts of training data but also struggles to effectively understand\nGUI screenshots and generalize to unseen interfaces. The issue significantly\nlimits its application in real-world scenarios, especially for high-level\ntasks. Inspired by Reinforcement Fine-Tuning (RFT) in large reasoning models\n(e.g., DeepSeek-R1), which efficiently enhances the problem-solving\ncapabilities of large language models in real-world settings, we propose \\name,\nthe first reinforcement learning framework designed to enhance the GUI\ncapabilities of LVLMs in high-level real-world task scenarios, through unified\naction space rule modeling. By leveraging a small amount of carefully curated\nhigh-quality data across multiple platforms (including Windows, Linux, MacOS,\nAndroid, and Web) and employing policy optimization algorithms such as Group\nRelative Policy Optimization (GRPO) to update the model, \\name achieves\nsuperior performance using only 0.02\\% of the data (3K vs. 13M) compared to\nprevious state-of-the-art methods like OS-Atlas across eight benchmarks\nspanning three different platforms (mobile, desktop, and web). These results\ndemonstrate the immense potential of reinforcement learning based on unified\naction space rule modeling in improving the execution capabilities of LVLMs for\nreal-world GUI agent tasks."}
{"id": "2504.10462", "pdf": "https://arxiv.org/pdf/2504.10462", "abs": "https://arxiv.org/abs/2504.10462", "authors": ["Weixian Lei", "Jiacong Wang", "Haochen Wang", "Xiangtai Li", "Jun Hao Liew", "Jiashi Feng", "Zilong Huang"], "title": "The Scalability of Simplicity: Empirical Analysis of Vision-Language Learning with a Single Transformer", "categories": ["cs.CV"], "comment": null, "summary": "This paper introduces SAIL, a single transformer unified multimodal large\nlanguage model (MLLM) that integrates raw pixel encoding and language decoding\nwithin a singular architecture. Unlike existing modular MLLMs, which rely on a\npre-trained vision transformer (ViT), SAIL eliminates the need for a separate\nvision encoder, presenting a more minimalist architecture design. Instead of\nintroducing novel architectural components, SAIL adapts mix-attention\nmechanisms and multimodal positional encodings to better align with the\ndistinct characteristics of visual and textual modalities. We systematically\ncompare SAIL's properties-including scalability, cross-modal information flow\npatterns, and visual representation capabilities-with those of modular MLLMs.\nBy scaling both training data and model size, SAIL achieves performance\ncomparable to modular MLLMs. Notably, the removal of pretrained ViT components\nenhances SAIL's scalability and results in significantly different cross-modal\ninformation flow patterns. Moreover, SAIL demonstrates strong visual\nrepresentation capabilities, achieving results on par with ViT-22B in vision\ntasks such as semantic segmentation. Code and models are available at\nhttps://github.com/bytedance/SAIL."}
{"id": "2504.10465", "pdf": "https://arxiv.org/pdf/2504.10465", "abs": "https://arxiv.org/abs/2504.10465", "authors": ["Tao Zhang", "Xiangtai Li", "Zilong Huang", "Yanwei Li", "Weixian Lei", "Xueqing Deng", "Shihao Chen", "Shunping Ji", "Jiashi Feng"], "title": "Pixel-SAIL: Single Transformer For Pixel-Grounded Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) achieve remarkable performance for\nfine-grained pixel-level understanding tasks. However, all the works rely\nheavily on extra components, such as vision encoder (CLIP), segmentation\nexperts, leading to high system complexity and limiting model scaling. In this\nwork, our goal is to explore a highly simplified MLLM without introducing extra\ncomponents. Our work is motivated by the recent works on Single trAnsformer as\na unified vIsion-Language Model (SAIL) design, where these works jointly learn\nvision tokens and text tokens in transformers. We present Pixel-SAIL, a single\ntransformer for pixel-wise MLLM tasks. In particular, we present three\ntechnical improvements on the plain baseline. First, we design a learnable\nupsampling module to refine visual token features. Secondly, we propose a novel\nvisual prompt injection strategy to enable the single transformer to understand\nvisual prompt inputs and benefit from the early fusion of visual prompt\nembeddings and vision tokens. Thirdly, we introduce a vision expert\ndistillation strategy to efficiently enhance the single transformer's\nfine-grained feature extraction capability. In addition, we have collected a\ncomprehensive pixel understanding benchmark (PerBench), using a manual check.\nIt includes three tasks: detailed object description, visual prompt-based\nquestion answering, and visual-text referring segmentation. Extensive\nexperiments on four referring segmentation benchmarks, one visual prompt\nbenchmark, and our PerBench show that our Pixel-SAIL achieves comparable or\neven better results with a much simpler pipeline. Code and model will be\nreleased at https://github.com/magic-research/Sa2VA."}
{"id": "2504.10466", "pdf": "https://arxiv.org/pdf/2504.10466", "abs": "https://arxiv.org/abs/2504.10466", "authors": ["Xiaoyan Cong", "Jiayi Shen", "Zekun Li", "Rao Fu", "Tao Lu", "Srinath Sridhar"], "title": "Art3D: Training-Free 3D Generation from Flat-Colored Illustration", "categories": ["cs.CV"], "comment": "Technical Report. Course Project of Brown CSCI 1430 Computer Vision.\n  Project Page: https://joy-jy11.github.io/", "summary": "Large-scale pre-trained image-to-3D generative models have exhibited\nremarkable capabilities in diverse shape generations. However, most of them\nstruggle to synthesize plausible 3D assets when the reference image is\nflat-colored like hand drawings due to the lack of 3D illusion, which are often\nthe most user-friendly input modalities in art content creation. To this end,\nwe propose Art3D, a training-free method that can lift flat-colored 2D designs\ninto 3D. By leveraging structural and semantic features with pre- trained 2D\nimage generation models and a VLM-based realism evaluation, Art3D successfully\nenhances the three-dimensional illusion in reference images, thus simplifying\nthe process of generating 3D from 2D, and proves adaptable to a wide range of\npainting styles. To benchmark the generalization performance of existing\nimage-to-3D models on flat-colored images without 3D feeling, we collect a new\ndataset, Flat-2D, with over 100 samples. Experimental results demonstrate the\nperformance and robustness of Art3D, exhibiting superior generalizable capacity\nand promising practical applicability. Our source code and dataset will be\npublicly available on our project page: https://joy-jy11.github.io/ ."}
{"id": "2504.10471", "pdf": "https://arxiv.org/pdf/2504.10471", "abs": "https://arxiv.org/abs/2504.10471", "authors": ["Chenghao Xiao", "Isaac Chung", "Imene Kerboua", "Jamie Stirling", "Xin Zhang", "Márton Kardos", "Roman Solomatin", "Noura Al Moubayed", "Kenneth Enevoldsen", "Niklas Muennighoff"], "title": "MIEB: Massive Image Embedding Benchmark", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Image representations are often evaluated through disjointed, task-specific\nprotocols, leading to a fragmented understanding of model capabilities. For\ninstance, it is unclear whether an image embedding model adept at clustering\nimages is equally good at retrieving relevant images given a piece of text. We\nintroduce the Massive Image Embedding Benchmark (MIEB) to evaluate the\nperformance of image and image-text embedding models across the broadest\nspectrum to date. MIEB spans 38 languages across 130 individual tasks, which we\ngroup into 8 high-level categories. We benchmark 50 models across our\nbenchmark, finding that no single method dominates across all task categories.\nWe reveal hidden capabilities in advanced vision models such as their accurate\nvisual representation of texts, and their yet limited capabilities in\ninterleaved encodings and matching images and texts in the presence of\nconfounders. We also show that the performance of vision encoders on MIEB\ncorrelates highly with their performance when used in multimodal large language\nmodels. Our code, dataset, and leaderboard are publicly available at\nhttps://github.com/embeddings-benchmark/mteb."}
{"id": "2504.10479", "pdf": "https://arxiv.org/pdf/2504.10479", "abs": "https://arxiv.org/abs/2504.10479", "authors": ["Jinguo Zhu", "Weiyun Wang", "Zhe Chen", "Zhaoyang Liu", "Shenglong Ye", "Lixin Gu", "Yuchen Duan", "Hao Tian", "Weijie Su", "Jie Shao", "Zhangwei Gao", "Erfei Cui", "Yue Cao", "Yangzhou Liu", "Weiye Xu", "Hao Li", "Jiahao Wang", "Han Lv", "Dengnian Chen", "Songze Li", "Yinan He", "Tan Jiang", "Jiapeng Luo", "Yi Wang", "Conghui He", "Botian Shi", "Xingcheng Zhang", "Wenqi Shao", "Junjun He", "Yingtong Xiong", "Wenwen Qu", "Peng Sun", "Penglong Jiao", "Lijun Wu", "Kaipeng Zhang", "Huipeng Deng", "Jiaye Ge", "Kai Chen", "Limin Wang", "Min Dou", "Lewei Lu", "Xizhou Zhu", "Tong Lu", "Dahua Lin", "Yu Qiao", "Jifeng Dai", "Wenhai Wang"], "title": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "categories": ["cs.CV"], "comment": "Technical Report", "summary": "We introduce InternVL3, a significant advancement in the InternVL series\nfeaturing a native multimodal pre-training paradigm. Rather than adapting a\ntext-only large language model (LLM) into a multimodal large language model\n(MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and\nlinguistic capabilities from both diverse multimodal data and pure-text corpora\nduring a single pre-training stage. This unified training paradigm effectively\naddresses the complexities and alignment challenges commonly encountered in\nconventional post-hoc training pipelines for MLLMs. To further improve\nperformance and scalability, InternVL3 incorporates variable visual position\nencoding (V2PE) to support extended multimodal contexts, employs advanced\npost-training techniques such as supervised fine-tuning (SFT) and mixed\npreference optimization (MPO), and adopts test-time scaling strategies\nalongside an optimized training infrastructure. Extensive empirical evaluations\ndemonstrate that InternVL3 delivers superior performance across a wide range of\nmulti-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the\nMMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its\ncapabilities remain highly competitive with leading proprietary models,\nincluding ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also\nmaintaining strong pure-language proficiency. In pursuit of open-science\nprinciples, we will publicly release both the training data and model weights\nto foster further research and development in next-generation MLLMs."}
{"id": "2504.10483", "pdf": "https://arxiv.org/pdf/2504.10483", "abs": "https://arxiv.org/abs/2504.10483", "authors": ["Xingjian Leng", "Jaskirat Singh", "Yunzhong Hou", "Zhenchang Xing", "Saining Xie", "Liang Zheng"], "title": "REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion Transformers", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "In this paper we tackle a fundamental question: \"Can we train latent\ndiffusion models together with the variational auto-encoder (VAE) tokenizer in\nan end-to-end manner?\" Traditional deep-learning wisdom dictates that\nend-to-end training is often preferable when possible. However, for latent\ndiffusion transformers, it is observed that end-to-end training both VAE and\ndiffusion-model using standard diffusion-loss is ineffective, even causing a\ndegradation in final performance. We show that while diffusion loss is\nineffective, end-to-end training can be unlocked through the\nrepresentation-alignment (REPA) loss -- allowing both VAE and diffusion model\nto be jointly tuned during the training process. Despite its simplicity, the\nproposed training recipe (REPA-E) shows remarkable performance; speeding up\ndiffusion model training by over 17x and 45x over REPA and vanilla training\nrecipes, respectively. Interestingly, we observe that end-to-end tuning with\nREPA-E also improves the VAE itself; leading to improved latent space structure\nand downstream generation performance. In terms of final performance, our\napproach sets a new state-of-the-art; achieving FID of 1.26 and 1.83 with and\nwithout classifier-free guidance on ImageNet 256 x 256. Code is available at\nhttps://end2end-diffusion.github.io."}
{"id": "2504.10485", "pdf": "https://arxiv.org/pdf/2504.10485", "abs": "https://arxiv.org/abs/2504.10485", "authors": ["Yunsong Zhou", "Naisheng Ye", "William Ljungbergh", "Tianyu Li", "Jiazhi Yang", "Zetong Yang", "Hongzi Zhu", "Christoffer Petersson", "Hongyang Li"], "title": "Decoupled Diffusion Sparks Adaptive Scene Generation", "categories": ["cs.CV"], "comment": null, "summary": "Controllable scene generation could reduce the cost of diverse data\ncollection substantially for autonomous driving. Prior works formulate the\ntraffic layout generation as predictive progress, either by denoising entire\nsequences at once or by iteratively predicting the next frame. However, full\nsequence denoising hinders online reaction, while the latter's short-sighted\nnext-frame prediction lacks precise goal-state guidance. Further, the learned\nmodel struggles to generate complex or challenging scenarios due to a large\nnumber of safe and ordinal driving behaviors from open datasets. To overcome\nthese, we introduce Nexus, a decoupled scene generation framework that improves\nreactivity and goal conditioning by simulating both ordinal and challenging\nscenarios from fine-grained tokens with independent noise states. At the core\nof the decoupled pipeline is the integration of a partial noise-masking\ntraining strategy and a noise-aware schedule that ensures timely environmental\nupdates throughout the denoising process. To complement challenging scenario\ngeneration, we collect a dataset consisting of complex corner cases. It covers\n540 hours of simulated data, including high-risk interactions such as cut-in,\nsudden braking, and collision. Nexus achieves superior generation realism while\npreserving reactivity and goal orientation, with a 40% reduction in\ndisplacement error. We further demonstrate that Nexus improves closed-loop\nplanning by 20% through data augmentation and showcase its capability in\nsafety-critical data generation."}
{"id": "2504.10486", "pdf": "https://arxiv.org/pdf/2504.10486", "abs": "https://arxiv.org/abs/2504.10486", "authors": ["Zeren Jiang", "Shaofei Wang", "Siyu Tang"], "title": "DNF-Avatar: Distilling Neural Fields for Real-time Animatable Avatar Relighting", "categories": ["cs.CV"], "comment": "16 pages, 8 figures, Project pages:\n  https://jzr99.github.io/DNF-Avatar/", "summary": "Creating relightable and animatable human avatars from monocular videos is a\nrising research topic with a range of applications, e.g. virtual reality,\nsports, and video games. Previous works utilize neural fields together with\nphysically based rendering (PBR), to estimate geometry and disentangle\nappearance properties of human avatars. However, one drawback of these methods\nis the slow rendering speed due to the expensive Monte Carlo ray tracing. To\ntackle this problem, we proposed to distill the knowledge from implicit neural\nfields (teacher) to explicit 2D Gaussian splatting (student) representation to\ntake advantage of the fast rasterization property of Gaussian splatting. To\navoid ray-tracing, we employ the split-sum approximation for PBR appearance. We\nalso propose novel part-wise ambient occlusion probes for shadow computation.\nShadow prediction is achieved by querying these probes only once per pixel,\nwhich paves the way for real-time relighting of avatars. These techniques\ncombined give high-quality relighting results with realistic shadow effects.\nOur experiments demonstrate that the proposed student model achieves comparable\nor even better relighting results with our teacher model while being 370 times\nfaster at inference time, achieving a 67 FPS rendering speed."}
{"id": "2504.10487", "pdf": "https://arxiv.org/pdf/2504.10487", "abs": "https://arxiv.org/abs/2504.10487", "authors": ["Yasser Benigmim", "Mohammad Fahes", "Tuan-Hung Vu", "Andrei Bursuc", "Raoul de Charette"], "title": "FLOSS: Free Lunch in Open-vocabulary Semantic Segmentation", "categories": ["cs.CV", "cs.LG"], "comment": "Project Page: https://yasserben.github.io/FLOSS/", "summary": "Recent Open-Vocabulary Semantic Segmentation (OVSS) models extend the CLIP\nmodel to segmentation while maintaining the use of multiple templates (e.g., a\nphoto of <class>, a sketch of a <class>, etc.) for constructing class-wise\naveraged text embeddings, acting as a classifier. In this paper, we challenge\nthis status quo and investigate the impact of templates for OVSS. Empirically,\nwe observe that for each class, there exist single-template classifiers\nsignificantly outperforming the conventional averaged classifier. We refer to\nthem as class-experts. Given access to unlabeled images and without any\ntraining involved, we estimate these experts by leveraging the class-wise\nprediction entropy of single-template classifiers, selecting as class-wise\nexperts those which yield the lowest entropy. All experts, each specializing in\na specific class, collaborate in a newly proposed fusion method to generate\nmore accurate OVSS predictions. Our plug-and-play method, coined FLOSS, is\northogonal and complementary to existing OVSS methods, offering a ''free\nlunch'' to systematically improve OVSS without labels and additional training.\nExtensive experiments demonstrate that FLOSS consistently boosts\nstate-of-the-art methods on various OVSS benchmarks. Moreover, the selected\nexpert templates can generalize well from one dataset to others sharing the\nsame semantic categories, yet exhibiting distribution shifts. Additionally, we\nobtain satisfactory improvements under a low-data regime, where only a few\nunlabeled images are available. Our code is available at\nhttps://github.com/yasserben/FLOSS ."}
{"id": "2504.08823", "pdf": "https://arxiv.org/pdf/2504.08823", "abs": "https://arxiv.org/abs/2504.08823", "authors": ["Xiaobing Yu", "Jin Yang", "Xiao Wu", "Peijie Qiu", "Xiaofeng Liu"], "title": "FM-LoRA: Factorized Low-Rank Meta-Prompting for Continual Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "8 Pages, 4 figures", "summary": "How to adapt a pre-trained model continuously for sequential tasks with\ndifferent prediction class labels and domains and finally learn a generalizable\nmodel across diverse tasks is a long-lasting challenge. Continual learning (CL)\nhas emerged as a promising approach to leverage pre-trained models (e.g.,\nTransformers) for sequential tasks. While many existing CL methods\nincrementally store additional learned structures, such as Low-Rank Adaptation\n(LoRA) adapters or prompts and sometimes even preserve features from previous\nsamples to maintain performance. This leads to unsustainable parameter growth\nand escalating storage costs as the number of tasks increases. Moreover,\ncurrent approaches often lack task similarity awareness, which further hinders\nthe models ability to effectively adapt to new tasks without interfering with\npreviously acquired knowledge. To address these challenges, we propose FM-LoRA,\na novel and efficient low-rank adaptation method that integrates both a dynamic\nrank selector (DRS) and dynamic meta-prompting (DMP). This framework allocates\nmodel capacity more effectively across tasks by leveraging a shared low-rank\nsubspace critical for preserving knowledge, thereby avoiding continual\nparameter expansion. Extensive experiments on various CL benchmarks, including\nImageNet-R, CIFAR100, and CUB200 for class-incremental learning (CIL), and\nDomainNet for domain-incremental learning (DIL), with Transformers backbone\ndemonstrate that FM-LoRA effectively mitigates catastrophic forgetting while\ndelivering robust performance across a diverse range of tasks and domains."}
{"id": "2504.08824", "pdf": "https://arxiv.org/pdf/2504.08824", "abs": "https://arxiv.org/abs/2504.08824", "authors": ["Natalia Sikora", "Robert L. Manschke", "Alethea M. Tang", "Peter Dunstan", "Dean A. Harris", "Su Yang"], "title": "ColonScopeX: Leveraging Explainable Expert Systems with Multimodal Data for Improved Early Diagnosis of Colorectal Cancer", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.HC"], "comment": "Published to AAAI-25 Bridge Program", "summary": "Colorectal cancer (CRC) ranks as the second leading cause of cancer-related\ndeaths and the third most prevalent malignant tumour worldwide. Early detection\nof CRC remains problematic due to its non-specific and often embarrassing\nsymptoms, which patients frequently overlook or hesitate to report to\nclinicians. Crucially, the stage at which CRC is diagnosed significantly\nimpacts survivability, with a survival rate of 80-95\\% for Stage I and a stark\ndecline to 10\\% for Stage IV. Unfortunately, in the UK, only 14.4\\% of cases\nare diagnosed at the earliest stage (Stage I).\n  In this study, we propose ColonScopeX, a machine learning framework utilizing\nexplainable AI (XAI) methodologies to enhance the early detection of CRC and\npre-cancerous lesions. Our approach employs a multimodal model that integrates\nsignals from blood sample measurements, processed using the Savitzky-Golay\nalgorithm for fingerprint smoothing, alongside comprehensive patient metadata,\nincluding medication history, comorbidities, age, weight, and BMI. By\nleveraging XAI techniques, we aim to render the model's decision-making process\ntransparent and interpretable, thereby fostering greater trust and\nunderstanding in its predictions. The proposed framework could be utilised as a\ntriage tool or a screening tool of the general population.\n  This research highlights the potential of combining diverse patient data\nsources and explainable machine learning to tackle critical challenges in\nmedical diagnostics."}
{"id": "2504.08909", "pdf": "https://arxiv.org/pdf/2504.08909", "abs": "https://arxiv.org/abs/2504.08909", "authors": ["Islam Mansour", "Georg Fischer", "Ronny Haensch", "Irena Hajnsek"], "title": "Hybrid AI-Physical Modeling for Penetration Bias Correction in X-band InSAR DEMs: A Greenland Case Study", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": "8 pages", "summary": "Digital elevation models derived from Interferometric Synthetic Aperture\nRadar (InSAR) data over glacial and snow-covered regions often exhibit\nsystematic elevation errors, commonly termed \"penetration bias.\" We leverage\nexisting physics-based models and propose an integrated correction framework\nthat combines parametric physical modeling with machine learning. We evaluate\nthe approach across three distinct training scenarios - each defined by a\ndifferent set of acquisition parameters - to assess overall performance and the\nmodel's ability to generalize. Our experiments on Greenland's ice sheet using\nTanDEM-X data show that the proposed hybrid model corrections significantly\nreduce the mean and standard deviation of DEM errors compared to a purely\nphysical modeling baseline. The hybrid framework also achieves significantly\nimproved generalization than a pure ML approach when trained on data with\nlimited diversity in acquisition parameters."}
{"id": "2504.08937", "pdf": "https://arxiv.org/pdf/2504.08937", "abs": "https://arxiv.org/abs/2504.08937", "authors": ["Minjie Deng", "Yan Wei", "Hao Zhai", "An Wu", "Yuncan Ouyang", "Qianyao Peng"], "title": "Rethinking Few-Shot Fusion: Granular Ball Priors Enable General-Purpose Deep Image Fusion", "categories": ["cs.GR", "cs.CV", "cs.LG", "eess.IV", "stat.ML"], "comment": null, "summary": "In image fusion tasks, due to the lack of real fused images as priors, most\ndeep learning-based fusion methods obtain global weight features from original\nimages in large-scale data pairs to generate images that approximate real fused\nimages. However, unlike previous studies, this paper utilizes Granular Ball\nadaptation to extract features in the brightness space as priors for deep\nnetworks, enabling the fusion network to converge quickly and complete the\nfusion task. This leads to few-shot training for a general image fusion\nnetwork, and based on this, we propose the GBFF fusion method. According to the\ninformation expression division of pixel pairs in the original fused image, we\nclassify pixel pairs with significant performance as the positive domain and\nnon-significant pixel pairs as the boundary domain. We perform split inference\nin the brightness space using Granular Ball adaptation to compute weights for\npixels that express information to varying degrees, generating approximate\nsupervision images that provide priors for the neural network in the structural\nbrightness space. Additionally, the extracted global saliency features also\nadaptively provide priors for setting the loss function weights of each image\nin the network, guiding the network to converge quickly at both global and\npixel levels alongside the supervised images, thereby enhancing the\nexpressiveness of the fused images. Each modality only used 10 pairs of images\nas the training set, completing the fusion task with a limited number of\niterations. Experiments validate the effectiveness of the algorithm and theory,\nand qualitative and quantitative comparisons with SOTA methods show that this\napproach is highly competitive in terms of fusion time and image\nexpressiveness."}
{"id": "2504.08974", "pdf": "https://arxiv.org/pdf/2504.08974", "abs": "https://arxiv.org/abs/2504.08974", "authors": ["Pouya Pezeshkpour", "Moin Aminnaseri", "Estevam Hruschka"], "title": "Mixed Signals: Decoding VLMs' Reasoning and Underlying Bias in Vision-Language Conflict", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) have demonstrated impressive performance by\neffectively integrating visual and textual information to solve complex tasks.\nHowever, it is not clear how these models reason over the visual and textual\ndata together, nor how the flow of information between modalities is\nstructured. In this paper, we examine how VLMs reason by analyzing their biases\nwhen confronted with scenarios that present conflicting image and text cues, a\ncommon occurrence in real-world applications. To uncover the extent and nature\nof these biases, we build upon existing benchmarks to create five datasets\ncontaining mismatched image-text pairs, covering topics in mathematics,\nscience, and visual descriptions. Our analysis shows that VLMs favor text in\nsimpler queries but shift toward images as query complexity increases. This\nbias correlates with model scale, with the difference between the percentage of\nimage- and text-preferred responses ranging from +56.8% (image favored) to\n-74.4% (text favored), depending on the task and model. In addition, we explore\nthree mitigation strategies: simple prompt modifications, modifications that\nexplicitly instruct models on how to handle conflicting information (akin to\nchain-of-thought prompting), and a task decomposition strategy that analyzes\neach modality separately before combining their results. Our findings indicate\nthat the effectiveness of these strategies in identifying and mitigating bias\nvaries significantly and is closely linked to the model's overall performance\non the task and the specific modality in question."}
{"id": "2504.09088", "pdf": "https://arxiv.org/pdf/2504.09088", "abs": "https://arxiv.org/abs/2504.09088", "authors": ["Yonghao Huang", "Leiting Chen", "Chuan Zhou"], "title": "Multi-Modal Brain Tumor Segmentation via 3D Multi-Scale Self-attention and Cross-attention", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Due to the success of CNN-based and Transformer-based models in various\ncomputer vision tasks, recent works study the applicability of CNN-Transformer\nhybrid architecture models in 3D multi-modality medical segmentation tasks.\nIntroducing Transformer brings long-range dependent information modeling\nability in 3D medical images to hybrid models via the self-attention mechanism.\nHowever, these models usually employ fixed receptive fields of 3D volumetric\nfeatures within each self-attention layer, ignoring the multi-scale volumetric\nlesion features. To address this issue, we propose a CNN-Transformer hybrid 3D\nmedical image segmentation model, named TMA-TransBTS, based on an\nencoder-decoder structure. TMA-TransBTS realizes simultaneous extraction of\nmulti-scale 3D features and modeling of long-distance dependencies by\nmulti-scale division and aggregation of 3D tokens in a self-attention layer.\nFurthermore, TMA-TransBTS proposes a 3D multi-scale cross-attention module to\nestablish a link between the encoder and the decoder for extracting rich volume\nrepresentations by exploiting the mutual attention mechanism of cross-attention\nand multi-scale aggregation of 3D tokens. Extensive experimental results on\nthree public 3D medical segmentation datasets show that TMA-TransBTS achieves\nhigher averaged segmentation results than previous state-of-the-art CNN-based\n3D methods and CNN-Transform hybrid 3D methods for the segmentation of 3D\nmulti-modality brain tumors."}
{"id": "2504.09182", "pdf": "https://arxiv.org/pdf/2504.09182", "abs": "https://arxiv.org/abs/2504.09182", "authors": ["Zeyu Yang", "Zhilin Chen", "Yipeng Sun", "Anika Strittmatter", "Anish Raj", "Ahmad Allababidi", "Johann S. Rink", "Frank G. Zöllner"], "title": "seg2med: a segmentation-based medical image generation framework using denoising diffusion probabilistic models", "categories": ["eess.IV", "cs.CV"], "comment": "17 pages, 10 figures", "summary": "In this study, we present seg2med, an advanced medical image synthesis\nframework that uses Denoising Diffusion Probabilistic Models (DDPM) to generate\nhigh-quality synthetic medical images conditioned on anatomical masks from\nTotalSegmentator. The framework synthesizes CT and MR images from segmentation\nmasks derived from real patient data and XCAT digital phantoms, achieving a\nStructural Similarity Index Measure (SSIM) of 0.94 +/- 0.02 for CT and 0.89 +/-\n0.04 for MR images compared to ground-truth images of real patients. It also\nachieves a Feature Similarity Index Measure (FSIM) of 0.78 +/- 0.04 for CT\nimages from XCAT. The generative quality is further supported by a Fr\\'echet\nInception Distance (FID) of 3.62 for CT image generation.\n  Additionally, seg2med can generate paired CT and MR images with consistent\nanatomical structures and convert images between CT and MR modalities,\nachieving SSIM values of 0.91 +/- 0.03 for MR-to-CT and 0.77 +/- 0.04 for\nCT-to-MR conversion. Despite the limitations of incomplete anatomical details\nin segmentation masks, the framework shows strong performance in cross-modality\nsynthesis and multimodal imaging.\n  seg2med also demonstrates high anatomical fidelity in CT synthesis, achieving\na mean Dice coefficient greater than 0.90 for 11 abdominal organs and greater\nthan 0.80 for 34 organs out of 59 in 58 test cases. The highest Dice of 0.96\n+/- 0.01 was recorded for the right scapula. Leveraging the TotalSegmentator\ntoolkit, seg2med enables segmentation mask generation across diverse datasets,\nsupporting applications in clinical imaging, data augmentation, multimodal\nsynthesis, and diagnostic algorithm development."}
{"id": "2504.09209", "pdf": "https://arxiv.org/pdf/2504.09209", "abs": "https://arxiv.org/abs/2504.09209", "authors": ["Xiangyue Zhang", "Jianfang Li", "Jiaxu Zhang", "Jianqiang Ren", "Liefeng Bo", "Zhigang Tu"], "title": "EchoMask: Speech-Queried Attention-based Mask Modeling for Holistic Co-Speech Motion Generation", "categories": ["cs.GR", "cs.CV", "cs.SD"], "comment": "12 pages, 12 figures", "summary": "Masked modeling framework has shown promise in co-speech motion generation.\nHowever, it struggles to identify semantically significant frames for effective\nmotion masking. In this work, we propose a speech-queried attention-based mask\nmodeling framework for co-speech motion generation. Our key insight is to\nleverage motion-aligned speech features to guide the masked motion modeling\nprocess, selectively masking rhythm-related and semantically expressive motion\nframes. Specifically, we first propose a motion-audio alignment module (MAM) to\nconstruct a latent motion-audio joint space. In this space, both low-level and\nhigh-level speech features are projected, enabling motion-aligned speech\nrepresentation using learnable speech queries. Then, a speech-queried attention\nmechanism (SQA) is introduced to compute frame-level attention scores through\ninteractions between motion keys and speech queries, guiding selective masking\ntoward motion frames with high attention scores. Finally, the motion-aligned\nspeech features are also injected into the generation network to facilitate\nco-speech motion generation. Qualitative and quantitative evaluations confirm\nthat our method outperforms existing state-of-the-art approaches, successfully\nproducing high-quality co-speech motion."}
{"id": "2504.09265", "pdf": "https://arxiv.org/pdf/2504.09265", "abs": "https://arxiv.org/abs/2504.09265", "authors": ["Lei Kang", "Jia Li", "Mi Tian", "Hua Huang"], "title": "Mixture of Group Experts for Learning Invariant Representations", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Sparsely activated Mixture-of-Experts (MoE) models effectively increase the\nnumber of parameters while maintaining consistent computational costs per\ntoken. However, vanilla MoE models often suffer from limited diversity and\nspecialization among experts, constraining their performance and scalability,\nespecially as the number of experts increases. In this paper, we present a\nnovel perspective on vanilla MoE with top-$k$ routing inspired by sparse\nrepresentation. This allows us to bridge established theoretical insights from\nsparse representation into MoE models. Building on this foundation, we propose\na group sparse regularization approach for the input of top-$k$ routing, termed\nMixture of Group Experts (MoGE). MoGE indirectly regularizes experts by\nimposing structural constraints on the routing inputs, while preserving the\noriginal MoE architecture. Furthermore, we organize the routing input into a 2D\ntopographic map, spatially grouping neighboring elements. This structure\nenables MoGE to capture representations invariant to minor transformations,\nthereby significantly enhancing expert diversity and specialization.\nComprehensive evaluations across various Transformer models for image\nclassification and language modeling tasks demonstrate that MoGE substantially\noutperforms its MoE counterpart, with minimal additional memory and computation\noverhead. Our approach provides a simple yet effective solution to scale the\nnumber of experts and reduce redundancy among them. The source code is included\nin the supplementary material and will be publicly released."}
{"id": "2504.09341", "pdf": "https://arxiv.org/pdf/2504.09341", "abs": "https://arxiv.org/abs/2504.09341", "authors": ["Hsuan Wei Liao", "Christopher Klugmann", "Daniel Kondermann", "Rafid Mahmood"], "title": "Minority Reports: Balancing Cost and Quality in Ground Truth Data Annotation", "categories": ["cs.LG", "cs.CV", "cs.HC"], "comment": "39 pages", "summary": "High-quality data annotation is an essential but laborious and costly aspect\nof developing machine learning-based software. We explore the inherent tradeoff\nbetween annotation accuracy and cost by detecting and removing minority reports\n-- instances where annotators provide incorrect responses -- that indicate\nunnecessary redundancy in task assignments. We propose an approach to prune\npotentially redundant annotation task assignments before they are executed by\nestimating the likelihood of an annotator disagreeing with the majority vote\nfor a given task. Our approach is informed by an empirical analysis over\ncomputer vision datasets annotated by a professional data annotation platform,\nwhich reveals that the likelihood of a minority report event is dependent\nprimarily on image ambiguity, worker variability, and worker fatigue.\nSimulations over these datasets show that we can reduce the number of\nannotations required by over 60% with a small compromise in label quality,\nsaving approximately 6.6 days-equivalent of labor. Our approach provides\nannotation service platforms with a method to balance cost and dataset quality.\nMachine learning practitioners can tailor annotation accuracy levels according\nto specific application needs, thereby optimizing budget allocation while\nmaintaining the data quality necessary for critical settings like autonomous\ndriving technology."}
{"id": "2504.09352", "pdf": "https://arxiv.org/pdf/2504.09352", "abs": "https://arxiv.org/abs/2504.09352", "authors": ["Iason Chaimalas", "Arnas Vyšniauskas", "Gabriel Brostow"], "title": "Explorer: Robust Collection of Interactable GUI Elements", "categories": ["cs.HC", "cs.AI", "cs.CV"], "comment": "19 pages, 17 figures", "summary": "Automation of existing Graphical User Interfaces (GUIs) is important but hard\nto achieve. Upstream of making the GUI user-accessible or somehow scriptable,\neven the data-collection to understand the original interface poses significant\nchallenges. For example, large quantities of general UI data seem helpful for\ntraining general machine learning (ML) models, but accessibility for each\nperson can hinge on the ML's precision on a specific app. We therefore take the\nperspective that a given user needs confidence, that the relevant UI elements\nare being detected correctly throughout one app or digital environment. We\nmostly assume that the target application is known in advance, so that data\ncollection and ML-training can be personalized for the test-time target domain.\nThe proposed Explorer system focuses on detecting on-screen buttons and\ntext-entry fields, i.e. interactables, where the training process has access to\na live version of the application. The live application can run on almost any\npopular platform except iOS phones, and the collection is especially\nstreamlined for Android phones or for desktop Chrome browsers. Explorer also\nenables the recording of interactive user sessions, and subsequent mapping of\nhow such sessions overlap and sometimes loop back to similar states. We show\nhow having such a map enables a kind of path planning through the GUI, letting\na user issue audio commands to get to their destination. Critically, we are\nreleasing our code for Explorer openly at https://github.com/varnelis/Explorer."}
{"id": "2504.09430", "pdf": "https://arxiv.org/pdf/2504.09430", "abs": "https://arxiv.org/abs/2504.09430", "authors": ["Ruiwen Ding", "Lin Li", "Rajath Soans", "Tosha Shah", "Radha Krishnan", "Marc Alexander Sze", "Sasha Lukyanov", "Yash Deshpande", "Antong Chen"], "title": "Predicting ulcer in H&E images of inflammatory bowel disease using domain-knowledge-driven graph neural network", "categories": ["eess.IV", "cs.CV"], "comment": "Work accepted at ISBI 2025", "summary": "Inflammatory bowel disease (IBD) involves chronic inflammation of the\ndigestive tract, with treatment options often burdened by adverse effects.\nIdentifying biomarkers for personalized treatment is crucial. While immune\ncells play a key role in IBD, accurately identifying ulcer regions in whole\nslide images (WSIs) is essential for characterizing these cells and exploring\npotential therapeutics. Multiple instance learning (MIL) approaches have\nadvanced WSI analysis but they lack spatial context awareness. In this work, we\npropose a weakly-supervised model called DomainGCN that employs a graph\nconvolution neural network (GCN) and incorporates domain-specific knowledge of\nulcer features, specifically, the presence of epithelium, lymphocytes, and\ndebris for WSI-level ulcer prediction in IBD. We demonstrate that DomainGCN\noutperforms various state-of-the-art (SOTA) MIL methods and show the added\nvalue of domain knowledge."}
{"id": "2504.09456", "pdf": "https://arxiv.org/pdf/2504.09456", "abs": "https://arxiv.org/abs/2504.09456", "authors": ["Pengkun Jiao", "Bin Zhu", "Jingjing Chen", "Chong-Wah Ngo", "Yu-Gang Jiang"], "title": "Don't Deceive Me: Mitigating Gaslighting through Attention Reallocation in LMMs", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Large Multimodal Models (LMMs) have demonstrated remarkable capabilities\nacross a wide range of tasks. However, their vulnerability to user\ngaslighting-the deliberate use of misleading or contradictory inputs-raises\ncritical concerns about their reliability in real-world applications. In this\npaper, we address the novel and challenging issue of mitigating the negative\nimpact of negation-based gaslighting on LMMs, where deceptive user statements\nlead to significant drops in model accuracy. Specifically, we introduce\nGasEraser, a training-free approach that reallocates attention weights from\nmisleading textual tokens to semantically salient visual regions. By\nsuppressing the influence of \"attention sink\" tokens and enhancing focus on\nvisually grounded cues, GasEraser significantly improves LMM robustness without\nrequiring retraining or additional supervision. Extensive experimental results\ndemonstrate that GasEraser is effective across several leading open-source LMMs\non the GaslightingBench. Notably, for LLaVA-v1.5-7B, GasEraser reduces the\nmisguidance rate by 48.2%, demonstrating its potential for more trustworthy\nLMMs."}
{"id": "2504.09516", "pdf": "https://arxiv.org/pdf/2504.09516", "abs": "https://arxiv.org/abs/2504.09516", "authors": ["Yasar Abbas Ur Rehman", "Kin Wai Lau", "Yuyang Xie", "Ma Lan", "JiaJun Shen"], "title": "FSSUAVL: A Discriminative Framework using Vision Models for Federated Self-Supervised Audio and Image Understanding", "categories": ["cs.SD", "cs.CV", "eess.AS"], "comment": "8 pages", "summary": "Recent studies have demonstrated that vision models can effectively learn\nmultimodal audio-image representations when paired. However, the challenge of\nenabling deep models to learn representations from unpaired modalities remains\nunresolved. This issue is especially pertinent in scenarios like Federated\nLearning (FL), where data is often decentralized, heterogeneous, and lacks a\nreliable guarantee of paired data. Previous attempts tackled this issue through\nthe use of auxiliary pretrained encoders or generative models on local clients,\nwhich invariably raise computational cost with increasing number modalities.\nUnlike these approaches, in this paper, we aim to address the task of unpaired\naudio and image recognition using \\texttt{FSSUAVL}, a single deep model\npretrained in FL with self-supervised contrastive learning (SSL). Instead of\naligning the audio and image modalities, \\texttt{FSSUAVL} jointly discriminates\nthem by projecting them into a common embedding space using contrastive SSL.\nThis extends the utility of \\texttt{FSSUAVL} to paired and unpaired audio and\nimage recognition tasks. Our experiments with CNN and ViT demonstrate that\n\\texttt{FSSUAVL} significantly improves performance across various image- and\naudio-based downstream tasks compared to using separate deep models for each\nmodality. Additionally, \\texttt{FSSUAVL}'s capacity to learn multimodal feature\nrepresentations allows for integrating auxiliary information, if available, to\nenhance recognition accuracy."}
{"id": "2504.09544", "pdf": "https://arxiv.org/pdf/2504.09544", "abs": "https://arxiv.org/abs/2504.09544", "authors": ["Yemin Yu", "Neil Tenenholtz", "Lester Mackey", "Ying Wei", "David Alvarez-Melis", "Ava P. Amini", "Alex X. Lu"], "title": "Causal integration of chemical structures improves representations of microscopy images for morphological profiling", "categories": ["cs.LG", "cs.CE", "cs.CV"], "comment": "24 pages", "summary": "Recent advances in self-supervised deep learning have improved our ability to\nquantify cellular morphological changes in high-throughput microscopy screens,\na process known as morphological profiling. However, most current methods only\nlearn from images, despite many screens being inherently multimodal, as they\ninvolve both a chemical or genetic perturbation as well as an image-based\nreadout. We hypothesized that incorporating chemical compound structure during\nself-supervised pre-training could improve learned representations of images in\nhigh-throughput microscopy screens. We introduce a representation learning\nframework, MICON (Molecular-Image Contrastive Learning), that models chemical\ncompounds as treatments that induce counterfactual transformations of cell\nphenotypes. MICON significantly outperforms classical hand-crafted features\nsuch as CellProfiler and existing deep-learning-based representation learning\nmethods in challenging evaluation settings where models must identify\nreproducible effects of drugs across independent replicates and data-generating\ncenters. We demonstrate that incorporating chemical compound information into\nthe learning process provides consistent improvements in our evaluation setting\nand that modeling compounds specifically as treatments in a causal framework\noutperforms approaches that directly align images and compounds in a single\nrepresentation space. Our findings point to a new direction for representation\nlearning in morphological profiling, suggesting that methods should explicitly\naccount for the multimodal nature of microscopy screening data."}
{"id": "2504.09620", "pdf": "https://arxiv.org/pdf/2504.09620", "abs": "https://arxiv.org/abs/2504.09620", "authors": ["Yuta Matsui", "Ryosuke Yamaki", "Ryo Ueda", "Seitaro Shinagawa", "Tadahiro Taniguchi"], "title": "Metropolis-Hastings Captioning Game: Knowledge Fusion of Vision Language Models via Decentralized Bayesian Inference", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.MA"], "comment": null, "summary": "We propose the Metropolis-Hastings Captioning Game (MHCG), a method to fuse\nknowledge of multiple vision-language models (VLMs) by learning from each\nother. Although existing methods that combine multiple models suffer from\ninference costs and architectural constraints, MHCG avoids these problems by\nperforming decentralized Bayesian inference through a process resembling a\nlanguage game. The knowledge fusion process establishes communication between\ntwo VLM agents alternately captioning images and learning from each other. We\nconduct two image-captioning experiments with two VLMs, each pre-trained on a\ndifferent dataset. The first experiment demonstrates that MHCG achieves\nconsistent improvement in reference-free evaluation metrics. The second\nexperiment investigates how MHCG contributes to sharing VLMs' category-level\nvocabulary by observing the occurrence of the vocabulary in the generated\ncaptions."}
{"id": "2504.09648", "pdf": "https://arxiv.org/pdf/2504.09648", "abs": "https://arxiv.org/abs/2504.09648", "authors": ["Guixian Chen", "Jianhao Ma", "Salar Fattahi"], "title": "RANSAC Revisited: An Improved Algorithm for Robust Subspace Recovery under Adversarial and Noisy Corruptions", "categories": ["cs.LG", "cs.CV", "stat.CO"], "comment": null, "summary": "In this paper, we study the problem of robust subspace recovery (RSR) in the\npresence of both strong adversarial corruptions and Gaussian noise.\nSpecifically, given a limited number of noisy samples -- some of which are\ntampered by an adaptive and strong adversary -- we aim to recover a\nlow-dimensional subspace that approximately contains a significant fraction of\nthe uncorrupted samples, up to an error that scales with the Gaussian noise.\nExisting approaches to this problem often suffer from high computational costs\nor rely on restrictive distributional assumptions, limiting their applicability\nin truly adversarial settings. To address these challenges, we revisit the\nclassical random sample consensus (RANSAC) algorithm, which offers strong\nrobustness to adversarial outliers, but sacrifices efficiency and robustness\nagainst Gaussian noise and model misspecification in the process. We propose a\ntwo-stage algorithm, RANSAC+, that precisely pinpoints and remedies the failure\nmodes of standard RANSAC. Our method is provably robust to both Gaussian and\nadversarial corruptions, achieves near-optimal sample complexity without\nrequiring prior knowledge of the subspace dimension, and is more efficient than\nexisting RANSAC-type methods."}
{"id": "2504.09655", "pdf": "https://arxiv.org/pdf/2504.09655", "abs": "https://arxiv.org/abs/2504.09655", "authors": ["Justin Namuk Kim", "Yiqiao Liu", "Rajath Soans", "Keith Persson", "Sarah Halek", "Michal Tomaszewski", "Jianda Yuan", "Gregory Goldmacher", "Antong Chen"], "title": "OmniMamba4D: Spatio-temporal Mamba for longitudinal CT lesion segmentation", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted at IEEE International Symposium on Biomedical Imaging (ISBI)\n  2025", "summary": "Accurate segmentation of longitudinal CT scans is important for monitoring\ntumor progression and evaluating treatment responses. However, existing 3D\nsegmentation models solely focus on spatial information. To address this gap,\nwe propose OmniMamba4D, a novel segmentation model designed for 4D medical\nimages (3D images over time). OmniMamba4D utilizes a spatio-temporal\ntetra-orientated Mamba block to effectively capture both spatial and temporal\nfeatures. Unlike traditional 3D models, which analyze single-time points,\nOmniMamba4D processes 4D CT data, providing comprehensive spatio-temporal\ninformation on lesion progression. Evaluated on an internal dataset comprising\nof 3,252 CT scans, OmniMamba4D achieves a competitive Dice score of 0.682,\ncomparable to state-of-the-arts (SOTA) models, while maintaining computational\nefficiency and better detecting disappeared lesions. This work demonstrates a\nnew framework to leverage spatio-temporal information for longitudinal CT\nlesion segmentation."}
{"id": "2504.09712", "pdf": "https://arxiv.org/pdf/2504.09712", "abs": "https://arxiv.org/abs/2504.09712", "authors": ["Julius Broomfield", "Tom Gibbs", "Ethan Kosak-Hine", "George Ingebretsen", "Tia Nasir", "Jason Zhang", "Reihaneh Iranmanesh", "Sara Pieri", "Reihaneh Rabbany", "Kellin Pelrine"], "title": "The Structural Safety Generalization Problem", "categories": ["cs.CR", "cs.AI", "cs.CV"], "comment": null, "summary": "LLM jailbreaks are a widespread safety challenge. Given this problem has not\nyet been tractable, we suggest targeting a key failure mechanism: the failure\nof safety to generalize across semantically equivalent inputs. We further focus\nthe target by requiring desirable tractability properties of attacks to study:\nexplainability, transferability between models, and transferability between\ngoals. We perform red-teaming within this framework by uncovering new\nvulnerabilities to multi-turn, multi-image, and translation-based attacks.\nThese attacks are semantically equivalent by our design to their single-turn,\nsingle-image, or untranslated counterparts, enabling systematic comparisons; we\nshow that the different structures yield different safety outcomes. We then\ndemonstrate the potential for this framework to enable new defenses by\nproposing a Structure Rewriting Guardrail, which converts an input to a\nstructure more conducive to safety assessment. This guardrail significantly\nimproves refusal of harmful inputs, without over-refusing benign ones. Thus, by\nframing this intermediate challenge - more tractable than universal defenses\nbut essential for long-term safety - we highlight a critical milestone for AI\nsafety research."}
{"id": "2504.09795", "pdf": "https://arxiv.org/pdf/2504.09795", "abs": "https://arxiv.org/abs/2504.09795", "authors": ["Ryota Tanaka", "Taichi Iki", "Taku Hasegawa", "Kyosuke Nishida", "Kuniko Saito", "Jun Suzuki"], "title": "VDocRAG: Retrieval-Augmented Generation over Visually-Rich Documents", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR"], "comment": "Accepted by CVPR 2025; project page: https://vdocrag.github.io", "summary": "We aim to develop a retrieval-augmented generation (RAG) framework that\nanswers questions over a corpus of visually-rich documents presented in mixed\nmodalities (e.g., charts, tables) and diverse formats (e.g., PDF, PPTX). In\nthis paper, we introduce a new RAG framework, VDocRAG, which can directly\nunderstand varied documents and modalities in a unified image format to prevent\nmissing information that occurs by parsing documents to obtain text. To improve\nthe performance, we propose novel self-supervised pre-training tasks that adapt\nlarge vision-language models for retrieval by compressing visual information\ninto dense token representations while aligning them with textual content in\ndocuments. Furthermore, we introduce OpenDocVQA, the first unified collection\nof open-domain document visual question answering datasets, encompassing\ndiverse document types and formats. OpenDocVQA provides a comprehensive\nresource for training and evaluating retrieval and question answering models on\nvisually-rich documents in an open-domain setting. Experiments show that\nVDocRAG substantially outperforms conventional text-based RAG and has strong\ngeneralization capability, highlighting the potential of an effective RAG\nparadigm for real-world documents."}
{"id": "2504.09885", "pdf": "https://arxiv.org/pdf/2504.09885", "abs": "https://arxiv.org/abs/2504.09885", "authors": ["Zihao Liu", "Mingwen Ou", "Zunnan Xu", "Jiaqi Huang", "Haonan Han", "Ronghui Li", "Xiu Li"], "title": "Separate to Collaborate: Dual-Stream Diffusion Model for Coordinated Piano Hand Motion Synthesis", "categories": ["cs.SD", "cs.CV", "eess.AS"], "comment": "12 pages, 4 figures", "summary": "Automating the synthesis of coordinated bimanual piano performances poses\nsignificant challenges, particularly in capturing the intricate choreography\nbetween the hands while preserving their distinct kinematic signatures. In this\npaper, we propose a dual-stream neural framework designed to generate\nsynchronized hand gestures for piano playing from audio input, addressing the\ncritical challenge of modeling both hand independence and coordination. Our\nframework introduces two key innovations: (i) a decoupled diffusion-based\ngeneration framework that independently models each hand's motion via\ndual-noise initialization, sampling distinct latent noise for each while\nleveraging a shared positional condition, and (ii) a Hand-Coordinated\nAsymmetric Attention (HCAA) mechanism suppresses symmetric (common-mode) noise\nto highlight asymmetric hand-specific features, while adaptively enhancing\ninter-hand coordination during denoising. The system operates hierarchically:\nit first predicts 3D hand positions from audio features and then generates\njoint angles through position-aware diffusion models, where parallel denoising\nstreams interact via HCAA. Comprehensive evaluations demonstrate that our\nframework outperforms existing state-of-the-art methods across multiple\nmetrics."}
{"id": "2504.09949", "pdf": "https://arxiv.org/pdf/2504.09949", "abs": "https://arxiv.org/abs/2504.09949", "authors": ["Heming Xu", "Xiaohui Liu", "Zhilu Zhang", "Hongzhi Zhang", "Xiaohe Wu", "Wangmeng Zuo"], "title": "Pseudo-Label Guided Real-World Image De-weathering: A Learning Framework with Imperfect Supervision", "categories": ["cs.GR", "cs.CV"], "comment": "15 pages, 16 figures", "summary": "Real-world image de-weathering aims at removingvarious undesirable\nweather-related artifacts, e.g., rain, snow,and fog. To this end, acquiring\nideal training pairs is crucial.Existing real-world datasets are typically\nconstructed paired databy extracting clean and degraded images from live\nstreamsof landscape scene on the Internet. Despite the use of strictfiltering\nmechanisms during collection, training pairs inevitablyencounter inconsistency\nin terms of lighting, object position, scenedetails, etc, making de-weathering\nmodels possibly suffer fromdeformation artifacts under non-ideal supervision.\nIn this work,we propose a unified solution for real-world image\nde-weatheringwith non-ideal supervision, i.e., a pseudo-label guided\nlearningframework, to address various inconsistencies within the realworld\npaired dataset. Generally, it consists of a de-weatheringmodel (De-W) and a\nConsistent Label Constructor (CLC), bywhich restoration result can be\nadaptively supervised by originalground-truth image to recover sharp textures\nwhile maintainingconsistency with the degraded inputs in non-weather\ncontentthrough the supervision of pseudo-labels. Particularly, a Crossframe\nSimilarity Aggregation (CSA) module is deployed withinCLC to enhance the\nquality of pseudo-labels by exploring thepotential complementary information of\nmulti-frames throughgraph model. Moreover, we introduce an Information\nAllocationStrategy (IAS) to integrate the original ground-truth imagesand\npseudo-labels, thereby facilitating the joint supervision forthe training of\nde-weathering model. Extensive experimentsdemonstrate that our method exhibits\nsignificant advantageswhen trained on imperfectly aligned de-weathering\ndatasets incomparison with other approaches."}
{"id": "2504.09975", "pdf": "https://arxiv.org/pdf/2504.09975", "abs": "https://arxiv.org/abs/2504.09975", "authors": ["Si-Tong Wei", "Rui-Huan Wang", "Chuan-Zhi Zhou", "Baoquan Chen", "Peng-Shuai Wang"], "title": "OctGPT: Octree-based Multiscale Autoregressive Models for 3D Shape Generation", "categories": ["cs.GR", "cs.CV"], "comment": "SIGGRAPH 2025", "summary": "Autoregressive models have achieved remarkable success across various\ndomains, yet their performance in 3D shape generation lags significantly behind\nthat of diffusion models. In this paper, we introduce OctGPT, a novel\nmultiscale autoregressive model for 3D shape generation that dramatically\nimproves the efficiency and performance of prior 3D autoregressive approaches,\nwhile rivaling or surpassing state-of-the-art diffusion models. Our method\nemploys a serialized octree representation to efficiently capture the\nhierarchical and spatial structures of 3D shapes. Coarse geometry is encoded\nvia octree structures, while fine-grained details are represented by binary\ntokens generated using a vector quantized variational autoencoder (VQVAE),\ntransforming 3D shapes into compact \\emph{multiscale binary sequences} suitable\nfor autoregressive prediction. To address the computational challenges of\nhandling long sequences, we incorporate octree-based transformers enhanced with\n3D rotary positional encodings, scale-specific embeddings, and token-parallel\ngeneration schemes. These innovations reduce training time by 13 folds and\ngeneration time by 69 folds, enabling the efficient training of high-resolution\n3D shapes, e.g.,$1024^3$, on just four NVIDIA 4090 GPUs only within days.\nOctGPT showcases exceptional versatility across various tasks, including text-,\nsketch-, and image-conditioned generation, as well as scene-level synthesis\ninvolving multiple objects. Extensive experiments demonstrate that OctGPT\naccelerates convergence and improves generation quality over prior\nautoregressive methods, offering a new paradigm for high-quality, scalable 3D\ncontent creation."}
{"id": "2504.10000", "pdf": "https://arxiv.org/pdf/2504.10000", "abs": "https://arxiv.org/abs/2504.10000", "authors": ["Yanbo Wang", "Jiyang Guan", "Jian Liang", "Ran He"], "title": "Do We Really Need Curated Malicious Data for Safety Alignment in Multi-modal Large Language Models?", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "Accepted to CVPR 2025, codes in process", "summary": "Multi-modal large language models (MLLMs) have made significant progress, yet\ntheir safety alignment remains limited. Typically, current open-source MLLMs\nrely on the alignment inherited from their language module to avoid harmful\ngenerations. However, the lack of safety measures specifically designed for\nmulti-modal inputs creates an alignment gap, leaving MLLMs vulnerable to\nvision-domain attacks such as typographic manipulation. Current methods utilize\na carefully designed safety dataset to enhance model defense capability, while\nthe specific knowledge or patterns acquired from the high-quality dataset\nremain unclear. Through comparison experiments, we find that the alignment gap\nprimarily arises from data distribution biases, while image content, response\nquality, or the contrastive behavior of the dataset makes little contribution\nto boosting multi-modal safety. To further investigate this and identify the\nkey factors in improving MLLM safety, we propose finetuning MLLMs on a small\nset of benign instruct-following data with responses replaced by simple, clear\nrejection sentences. Experiments show that, without the need for\nlabor-intensive collection of high-quality malicious data, model safety can\nstill be significantly improved, as long as a specific fraction of rejection\ndata exists in the finetuning set, indicating the security alignment is not\nlost but rather obscured during multi-modal pretraining or instruction\nfinetuning. Simply correcting the underlying data bias could narrow the safety\ngap in the vision domain."}
{"id": "2504.10003", "pdf": "https://arxiv.org/pdf/2504.10003", "abs": "https://arxiv.org/abs/2504.10003", "authors": ["Yiming Zeng", "Hao Ren", "Shuhang Wang", "Junlong Huang", "Hui Cheng"], "title": "NaviDiffusor: Cost-Guided Diffusion Model for Visual Navigation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Visual navigation, a fundamental challenge in mobile robotics, demands\nversatile policies to handle diverse environments. Classical methods leverage\ngeometric solutions to minimize specific costs, offering adaptability to new\nscenarios but are prone to system errors due to their multi-modular design and\nreliance on hand-crafted rules. Learning-based methods, while achieving high\nplanning success rates, face difficulties in generalizing to unseen\nenvironments beyond the training data and often require extensive training. To\naddress these limitations, we propose a hybrid approach that combines the\nstrengths of learning-based methods and classical approaches for RGB-only\nvisual navigation. Our method first trains a conditional diffusion model on\ndiverse path-RGB observation pairs. During inference, it integrates the\ngradients of differentiable scene-specific and task-level costs, guiding the\ndiffusion model to generate valid paths that meet the constraints. This\napproach alleviates the need for retraining, offering a plug-and-play solution.\nExtensive experiments in both indoor and outdoor settings, across simulated and\nreal-world scenarios, demonstrate zero-shot transfer capability of our\napproach, achieving higher success rates and fewer collisions compared to\nbaseline methods. Code will be released at\nhttps://github.com/SYSU-RoboticsLab/NaviD."}
{"id": "2504.10007", "pdf": "https://arxiv.org/pdf/2504.10007", "abs": "https://arxiv.org/abs/2504.10007", "authors": ["Jiani Ni", "He Zhao", "Jintong Gao", "Dandan Guo", "Hongyuan Zha"], "title": "Balancing Two Classifiers via A Simplex ETF Structure for Model Calibration", "categories": ["cs.LG", "cs.CV"], "comment": "CVPR2025", "summary": "In recent years, deep neural networks (DNNs) have demonstrated\nstate-of-the-art performance across various domains. However, despite their\nsuccess, they often face calibration issues, particularly in safety-critical\napplications such as autonomous driving and healthcare, where unreliable\npredictions can have serious consequences. Recent research has started to\nimprove model calibration from the view of the classifier. However, the\nexploration of designing the classifier to solve the model calibration problem\nis insufficient. Let alone most of the existing methods ignore the calibration\nerrors arising from underconfidence. In this work, we propose a novel method by\nbalancing learnable and ETF classifiers to solve the overconfidence or\nunderconfidence problem for model Calibration named BalCAL. By introducing a\nconfidence-tunable module and a dynamic adjustment method, we ensure better\nalignment between model confidence and its true accuracy. Extensive\nexperimental validation shows that ours significantly improves model\ncalibration performance while maintaining high predictive accuracy,\noutperforming existing techniques. This provides a novel solution to the\ncalibration challenges commonly encountered in deep learning."}
{"id": "2504.10014", "pdf": "https://arxiv.org/pdf/2504.10014", "abs": "https://arxiv.org/abs/2504.10014", "authors": ["Hang Yin", "Yan-Ming Zhang", "Jian Xu", "Jian-Long Chang", "Yin Li", "Cheng-Lin Liu"], "title": "Air Quality Prediction with A Meteorology-Guided Modality-Decoupled Spatio-Temporal Network", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Air quality prediction plays a crucial role in public health and\nenvironmental protection. Accurate air quality prediction is a complex\nmultivariate spatiotemporal problem, that involves interactions across temporal\npatterns, pollutant correlations, spatial station dependencies, and\nparticularly meteorological influences that govern pollutant dispersion and\nchemical transformations. Existing works underestimate the critical role of\natmospheric conditions in air quality prediction and neglect comprehensive\nmeteorological data utilization, thereby impairing the modeling of dynamic\ninterdependencies between air quality and meteorological data. To overcome\nthis, we propose MDSTNet, an encoder-decoder framework that explicitly models\nair quality observations and atmospheric conditions as distinct modalities,\nintegrating multi-pressure-level meteorological data and weather forecasts to\ncapture atmosphere-pollution dependencies for prediction. Meantime, we\nconstruct ChinaAirNet, the first nationwide dataset combining air quality\nrecords with multi-pressure-level meteorological observations. Experimental\nresults on ChinaAirNet demonstrate MDSTNet's superiority, substantially\nreducing 48-hour prediction errors by 17.54\\% compared to the state-of-the-art\nmodel. The source code and dataset will be available on github."}
{"id": "2504.10020", "pdf": "https://arxiv.org/pdf/2504.10020", "abs": "https://arxiv.org/abs/2504.10020", "authors": ["Hao Yin", "Gunagzong Si", "Zilei Wang"], "title": "The Mirage of Performance Gains: Why Contrastive Decoding Fails to Address Multimodal Hallucination", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Contrastive decoding strategies are widely used to reduce hallucinations in\nmultimodal large language models (MLLMs). These methods work by constructing\ncontrastive samples to induce hallucinations and then suppressing them in the\noutput distribution. However, this paper demonstrates that such approaches fail\nto effectively mitigate the hallucination problem. The performance improvements\nobserved on POPE Benchmark are largely driven by two misleading factors: (1)\ncrude, unidirectional adjustments to the model's output distribution and (2)\nthe adaptive plausibility constraint, which reduces the sampling strategy to\ngreedy search. To further illustrate these issues, we introduce a series of\nspurious improvement methods and evaluate their performance against contrastive\ndecoding techniques. Experimental results reveal that the observed performance\ngains in contrastive decoding are entirely unrelated to its intended goal of\nmitigating hallucinations. Our findings challenge common assumptions about the\neffectiveness of contrastive decoding strategies and pave the way for\ndeveloping genuinely effective solutions to hallucinations in MLLMs."}
{"id": "2504.10025", "pdf": "https://arxiv.org/pdf/2504.10025", "abs": "https://arxiv.org/abs/2504.10025", "authors": ["Uyen Phan", "Ozer Can Devecioglu", "Serkan Kiranyaz", "Moncef Gabbouj"], "title": "Progressive Transfer Learning for Multi-Pass Fundus Image Restoration", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "13 pages, 12 figures including appendix", "summary": "Diabetic retinopathy is a leading cause of vision impairment, making its\nearly diagnosis through fundus imaging critical for effective treatment\nplanning. However, the presence of poor quality fundus images caused by factors\nsuch as inadequate illumination, noise, blurring and other motion artifacts\nyields a significant challenge for accurate DR screening. In this study, we\npropose progressive transfer learning for multi pass restoration to iteratively\nenhance the quality of degraded fundus images, ensuring more reliable DR\nscreening. Unlike previous methods that often focus on a single pass\nrestoration, multi pass restoration via PTL can achieve a superior blind\nrestoration performance that can even improve most of the good quality fundus\nimages in the dataset. Initially, a Cycle GAN model is trained to restore low\nquality images, followed by PTL induced restoration passes over the latest\nrestored outputs to improve overall quality in each pass. The proposed method\ncan learn blind restoration without requiring any paired data while surpassing\nits limitations by leveraging progressive learning and fine tuning strategies\nto minimize distortions and preserve critical retinal features. To evaluate\nPTL's effectiveness on multi pass restoration, we conducted experiments on\nDeepDRiD, a large scale fundus imaging dataset specifically curated for\ndiabetic retinopathy detection. Our result demonstrates state of the art\nperformance, showcasing PTL's potential as a superior approach to iterative\nimage quality restoration."}
{"id": "2504.10041", "pdf": "https://arxiv.org/pdf/2504.10041", "abs": "https://arxiv.org/abs/2504.10041", "authors": ["Hao Ren", "Yiming Zeng", "Zetong Bi", "Zhaoliang Wan", "Junlong Huang", "Hui Cheng"], "title": "Prior Does Matter: Visual Navigation via Denoising Diffusion Bridge Models", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Recent advancements in diffusion-based imitation learning, which show\nimpressive performance in modeling multimodal distributions and training\nstability, have led to substantial progress in various robot learning tasks. In\nvisual navigation, previous diffusion-based policies typically generate action\nsequences by initiating from denoising Gaussian noise. However, the target\naction distribution often diverges significantly from Gaussian noise, leading\nto redundant denoising steps and increased learning complexity. Additionally,\nthe sparsity of effective action distributions makes it challenging for the\npolicy to generate accurate actions without guidance. To address these issues,\nwe propose a novel, unified visual navigation framework leveraging the\ndenoising diffusion bridge models named NaviBridger. This approach enables\naction generation by initiating from any informative prior actions, enhancing\nguidance and efficiency in the denoising process. We explore how diffusion\nbridges can enhance imitation learning in visual navigation tasks and further\nexamine three source policies for generating prior actions. Extensive\nexperiments in both simulated and real-world indoor and outdoor scenarios\ndemonstrate that NaviBridger accelerates policy inference and outperforms the\nbaselines in generating target action sequences. Code is available at\nhttps://github.com/hren20/NaiviBridger."}
{"id": "2504.10127", "pdf": "https://arxiv.org/pdf/2504.10127", "abs": "https://arxiv.org/abs/2504.10127", "authors": ["Junlei Zhang", "Zichen Ding", "Chang Ma", "Zijie Chen", "Qiushi Sun", "Zhenzhong Lan", "Junxian He"], "title": "Breaking the Data Barrier -- Building GUI Agents Through Task Generalization", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "24 pages, 11 figures", "summary": "Graphical User Interface (GUI) agents offer cross-platform solutions for\nautomating complex digital tasks, with significant potential to transform\nproductivity workflows. However, their performance is often constrained by the\nscarcity of high-quality trajectory data. To address this limitation, we\npropose training Vision Language Models (VLMs) on data-rich,\nreasoning-intensive tasks during a dedicated mid-training stage, and then\nexamine how incorporating these tasks facilitates generalization to GUI\nplanning scenarios. Specifically, we explore a range of tasks with readily\navailable instruction-tuning data, including GUI perception, multimodal\nreasoning, and textual reasoning. Through extensive experiments across 11\nmid-training tasks, we demonstrate that: (1) Task generalization proves highly\neffective, yielding substantial improvements across most settings. For\ninstance, multimodal mathematical reasoning enhances performance on\nAndroidWorld by an absolute 6.3%. Remarkably, text-only mathematical data\nsignificantly boosts GUI web agent performance, achieving a 5.6% improvement on\nWebArena and 5.4% improvement on AndroidWorld, underscoring notable cross-modal\ngeneralization from text-based to visual domains; (2) Contrary to prior\nassumptions, GUI perception data - previously considered closely aligned with\nGUI agent tasks and widely utilized for training - has a comparatively limited\nimpact on final performance; (3) Building on these insights, we identify the\nmost effective mid-training tasks and curate optimized mixture datasets,\nresulting in absolute performance gains of 8.0% on WebArena and 12.2% on\nAndroidWorld. Our work provides valuable insights into cross-domain knowledge\ntransfer for GUI agents and offers a practical approach to addressing data\nscarcity challenges in this emerging field. The code, data and models will be\navailable at https://github.com/hkust-nlp/GUIMid."}
{"id": "2504.10143", "pdf": "https://arxiv.org/pdf/2504.10143", "abs": "https://arxiv.org/abs/2504.10143", "authors": ["Yichao Cai", "Yuhang Liu", "Erdun Gao", "Tianjiao Jiang", "Zhen Zhang", "Anton van den Hengel", "Javen Qinfeng Shi"], "title": "Negate or Embrace: On How Misalignment Shapes Multimodal Representation Learning", "categories": ["cs.LG", "cs.CV"], "comment": "38 pages", "summary": "Multimodal representation learning, exemplified by multimodal contrastive\nlearning (MMCL) using image-text pairs, aims to learn powerful representations\nby aligning cues across modalities. This approach relies on the core assumption\nthat the exemplar image-text pairs constitute two representations of an\nidentical concept. However, recent research has revealed that real-world\ndatasets often exhibit misalignment. There are two distinct viewpoints on how\nto address this issue: one suggests mitigating the misalignment, and the other\nleveraging it. We seek here to reconcile these seemingly opposing perspectives,\nand to provide a practical guide for practitioners. Using latent variable\nmodels we thus formalize misalignment by introducing two specific mechanisms:\nselection bias, where some semantic variables are missing, and perturbation\nbias, where semantic variables are distorted -- both affecting latent variables\nshared across modalities. Our theoretical analysis demonstrates that, under\nmild assumptions, the representations learned by MMCL capture exactly the\ninformation related to the subset of the semantic variables invariant to\nselection and perturbation biases. This provides a unified perspective for\nunderstanding misalignment. Based on this, we further offer actionable insights\ninto how misalignment should inform the design of real-world ML systems. We\nvalidate our theoretical findings through extensive empirical studies on both\nsynthetic data and real image-text datasets, shedding light on the nuanced\nimpact of misalignment on multimodal representation learning."}
{"id": "2504.10244", "pdf": "https://arxiv.org/pdf/2504.10244", "abs": "https://arxiv.org/abs/2504.10244", "authors": ["Ziyao Shang", "Misha Kaandorp", "Kelly Payette", "Marina Fernandez Garcia", "Roxane Licandro", "Georg Langs", "Jordina Aviles Verdera", "Jana Hutter", "Bjoern Menze", "Gregor Kasprian", "Meritxell Bach Cuadra", "Andras Jakab"], "title": "Towards contrast- and pathology-agnostic clinical fetal brain MRI segmentation using SynthSeg", "categories": ["eess.IV", "cs.CV"], "comment": "21 pages, 16 figures", "summary": "Magnetic resonance imaging (MRI) has played a crucial role in fetal\nneurodevelopmental research. Structural annotations of MR images are an\nimportant step for quantitative analysis of the developing human brain, with\nDeep learning providing an automated alternative for this otherwise tedious\nmanual process. However, segmentation performances of Convolutional Neural\nNetworks often suffer from domain shift, where the network fails when applied\nto subjects that deviate from the distribution with which it is trained on. In\nthis work, we aim to train networks capable of automatically segmenting fetal\nbrain MRIs with a wide range of domain shifts pertaining to differences in\nsubject physiology and acquisition environments, in particular shape-based\ndifferences commonly observed in pathological cases. We introduce a novel\ndata-driven train-time sampling strategy that seeks to fully exploit the\ndiversity of a given training dataset to enhance the domain generalizability of\nthe trained networks. We adapted our sampler, together with other existing data\naugmentation techniques, to the SynthSeg framework, a generator that utilizes\ndomain randomization to generate diverse training data, and ran thorough\nexperimentations and ablation studies on a wide range of training/testing data\nto test the validity of the approaches. Our networks achieved notable\nimprovements in the segmentation quality on testing subjects with intense\nanatomical abnormalities (p < 1e-4), though at the cost of a slighter decrease\nin performance in cases with fewer abnormalities. Our work also lays the\nfoundation for future works on creating and adapting data-driven sampling\nstrategies for other training pipelines."}
{"id": "2504.10281", "pdf": "https://arxiv.org/pdf/2504.10281", "abs": "https://arxiv.org/abs/2504.10281", "authors": ["Jingyun Yang", "Ruoyan Avery Yin", "Chi Jiang", "Yuepeng Hu", "Xiaokai Zhu", "Xingjian Hu", "Sutharsika Kumar", "Xiao Wang", "Xiaohua Zhai", "Keran Rong", "Yunyue Zhu", "Tianyi Zhang", "Zongyou Yin", "Jing Kong", "Neil Zhenqiang Gong", "Zhichu Ren", "Haozhe Wang"], "title": "Zero-shot Autonomous Microscopy for Scalable and Intelligent Characterization of 2D Materials", "categories": ["cond-mat.mtrl-sci", "cond-mat.mes-hall", "cs.AI", "cs.CV", "cs.LG"], "comment": "13 pages, 4 figures", "summary": "Characterization of atomic-scale materials traditionally requires human\nexperts with months to years of specialized training. Even for trained human\noperators, accurate and reliable characterization remains challenging when\nexamining newly discovered materials such as two-dimensional (2D) structures.\nThis bottleneck drives demand for fully autonomous experimentation systems\ncapable of comprehending research objectives without requiring large training\ndatasets. In this work, we present ATOMIC (Autonomous Technology for Optical\nMicroscopy & Intelligent Characterization), an end-to-end framework that\nintegrates foundation models to enable fully autonomous, zero-shot\ncharacterization of 2D materials. Our system integrates the vision foundation\nmodel (i.e., Segment Anything Model), large language models (i.e., ChatGPT),\nunsupervised clustering, and topological analysis to automate microscope\ncontrol, sample scanning, image segmentation, and intelligent analysis through\nprompt engineering, eliminating the need for additional training. When\nanalyzing typical MoS2 samples, our approach achieves 99.7% segmentation\naccuracy for single layer identification, which is equivalent to that of human\nexperts. In addition, the integrated model is able to detect grain boundary\nslits that are challenging to identify with human eyes. Furthermore, the system\nretains robust accuracy despite variable conditions including defocus, color\ntemperature fluctuations, and exposure variations. It is applicable to a broad\nspectrum of common 2D materials-including graphene, MoS2, WSe2, SnSe-regardless\nof whether they were fabricated via chemical vapor deposition or mechanical\nexfoliation. This work represents the implementation of foundation models to\nachieve autonomous analysis, establishing a scalable and data-efficient\ncharacterization paradigm that fundamentally transforms the approach to\nnanoscale materials research."}
{"id": "2504.10445", "pdf": "https://arxiv.org/pdf/2504.10445", "abs": "https://arxiv.org/abs/2504.10445", "authors": ["Suyu Ye", "Haojun Shi", "Darren Shih", "Hyokun Yun", "Tanya Roosta", "Tianmin Shu"], "title": "RealWebAssist: A Benchmark for Long-Horizon Web Assistance with Real-World Users", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "Project Website: https://scai.cs.jhu.edu/projects/RealWebAssist/\n  Code: https://github.com/SCAI-JHU/RealWebAssist", "summary": "To achieve successful assistance with long-horizon web-based tasks, AI agents\nmust be able to sequentially follow real-world user instructions over a long\nperiod. Unlike existing web-based agent benchmarks, sequential instruction\nfollowing in the real world poses significant challenges beyond performing a\nsingle, clearly defined task. For instance, real-world human instructions can\nbe ambiguous, require different levels of AI assistance, and may evolve over\ntime, reflecting changes in the user's mental state. To address this gap, we\nintroduce RealWebAssist, a novel benchmark designed to evaluate sequential\ninstruction-following in realistic scenarios involving long-horizon\ninteractions with the web, visual GUI grounding, and understanding ambiguous\nreal-world user instructions. RealWebAssist includes a dataset of sequential\ninstructions collected from real-world human users. Each user instructs a\nweb-based assistant to perform a series of tasks on multiple websites. A\nsuccessful agent must reason about the true intent behind each instruction,\nkeep track of the mental state of the user, understand user-specific routines,\nand ground the intended tasks to actions on the correct GUI elements. Our\nexperimental results show that state-of-the-art models struggle to understand\nand ground user instructions, posing critical challenges in following\nreal-world user instructions for long-horizon web assistance."}
{"id": "2504.08901", "pdf": "https://arxiv.org/pdf/2504.08901", "abs": "https://arxiv.org/abs/2504.08901", "authors": ["Asterios Reppas", "Grigorios-Aris Cheimariotis", "Panos K. Papadopoulos", "Panagiotis Frasiolas", "Dimitrios Zarpalas"], "title": "HAL-NeRF: High Accuracy Localization Leveraging Neural Radiance Fields", "categories": ["cs.CV"], "comment": "8 pages, 4 figures", "summary": "Precise camera localization is a critical task in XR applications and\nrobotics. Using only the camera captures as input to a system is an inexpensive\noption that enables localization in large indoor and outdoor environments, but\nit presents challenges in achieving high accuracy. Specifically, camera\nrelocalization methods, such as Absolute Pose Regression (APR), can localize\ncameras with a median translation error of more than $0.5m$ in outdoor scenes.\nThis paper presents HAL-NeRF, a high-accuracy localization method that combines\na CNN pose regressor with a refinement module based on a Monte Carlo particle\nfilter. The Nerfacto model, an implementation of Neural Radiance Fields\n(NeRFs), is used to augment the data for training the pose regressor and to\nmeasure photometric loss in the particle filter refinement module. HAL-NeRF\nleverages Nerfacto's ability to synthesize high-quality novel views,\nsignificantly improving the performance of the localization pipeline. HAL-NeRF\nachieves state-of-the-art results that are conventionally measured as the\naverage of the median per scene errors. The translation error was $0.025m$ and\nthe rotation error was $0.59$ degrees and 0.04m and 0.58 degrees on the\n7-Scenes dataset and Cambridge Landmarks datasets respectively, with the\ntrade-off of increased computational time. This work highlights the potential\nof combining APR with NeRF-based refinement techniques to advance monocular\ncamera relocalization accuracy."}
{"id": "2504.08902", "pdf": "https://arxiv.org/pdf/2504.08902", "abs": "https://arxiv.org/abs/2504.08902", "authors": ["Pascal Chang", "Sergio Sancho", "Jingwei Tang", "Markus Gross", "Vinicius C. Azevedo"], "title": "LookingGlass: Generative Anamorphoses via Laplacian Pyramid Warping", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at CVPR 2025 (Oral)", "summary": "Anamorphosis refers to a category of images that are intentionally distorted,\nmaking them unrecognizable when viewed directly. Their true form only reveals\nitself when seen from a specific viewpoint, which can be through some\ncatadioptric device like a mirror or a lens. While the construction of these\nmathematical devices can be traced back to as early as the 17th century, they\nare only interpretable when viewed from a specific vantage point and tend to\nlose meaning when seen normally. In this paper, we revisit these famous optical\nillusions with a generative twist. With the help of latent rectified flow\nmodels, we propose a method to create anamorphic images that still retain a\nvalid interpretation when viewed directly. To this end, we introduce Laplacian\nPyramid Warping, a frequency-aware image warping technique key to generating\nhigh-quality visuals. Our work extends Visual Anagrams (arXiv:2311.17919) to\nlatent space models and to a wider range of spatial transforms, enabling the\ncreation of novel generative perceptual illusions."}
{"id": "2504.08906", "pdf": "https://arxiv.org/pdf/2504.08906", "abs": "https://arxiv.org/abs/2504.08906", "authors": ["Jiahuan Long", "Zhengqin Xu", "Tingsong Jiang", "Wen Yao", "Shuai Jia", "Chao Ma", "Xiaoqian Chen"], "title": "Robust SAM: On the Adversarial Robustness of Vision Foundation Models", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by AAAI2025", "summary": "The Segment Anything Model (SAM) is a widely used vision foundation model\nwith diverse applications, including image segmentation, detection, and\ntracking. Given SAM's wide applications, understanding its robustness against\nadversarial attacks is crucial for real-world deployment. However, research on\nSAM's robustness is still in its early stages. Existing attacks often overlook\nthe role of prompts in evaluating SAM's robustness, and there has been\ninsufficient exploration of defense methods to balance the robustness and\naccuracy. To address these gaps, this paper proposes an adversarial robustness\nframework designed to evaluate and enhance the robustness of SAM. Specifically,\nwe introduce a cross-prompt attack method to enhance the attack transferability\nacross different prompt types. Besides attacking, we propose a few-parameter\nadaptation strategy to defend SAM against various adversarial attacks. To\nbalance robustness and accuracy, we use the singular value decomposition (SVD)\nto constrain the space of trainable parameters, where only singular values are\nadaptable. Experiments demonstrate that our cross-prompt attack method\noutperforms previous approaches in terms of attack success rate on both SAM and\nSAM 2. By adapting only 512 parameters, we achieve at least a 15\\% improvement\nin mean intersection over union (mIoU) against various adversarial attacks.\nCompared to previous defense methods, our approach enhances the robustness of\nSAM while maximally maintaining its original performance."}
{"id": "2504.08915", "pdf": "https://arxiv.org/pdf/2504.08915", "abs": "https://arxiv.org/abs/2504.08915", "authors": ["Jiahuan Long", "Tingsong Jiang", "Wen Yao", "Yizhe Xiong", "Zhengqin Xu", "Shuai Jia", "Chao Ma"], "title": "Parameter-Free Fine-tuning via Redundancy Elimination for Vision Foundation Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision foundation models (VFMs) are large pre-trained models that form the\nbackbone of various vision tasks. Fine-tuning VFMs can further unlock their\npotential for downstream tasks or scenarios. However, VFMs often contain\nsignificant feature redundancy, which may limit their adaptability to new\ntasks. In this paper, we investigate the redundancies in the segment anything\nmodel (SAM) and then propose a parameter-free fine-tuning method to address\nthis issue. Unlike traditional fine-tuning methods that adjust parameters, our\nmethod emphasizes selecting, reusing, and enhancing pre-trained features,\noffering a new perspective on model fine-tuning. Specifically, we introduce a\nchannel selection algorithm based on the model's output difference to identify\nredundant and effective channels. By selectively replacing the redundant\nchannels with more effective ones, we filter out less useful features and reuse\nthe more relevant features to downstream tasks, thereby enhancing the\ntask-specific feature representation. Experiments on both out-of-domain and\nin-domain datasets demonstrate the efficiency and effectiveness of our method.\nNotably, our approach can seamlessly integrate with existing fine-tuning\nstrategies (e.g., LoRA, Adapter), further boosting the performance of already\nfine-tuned models. Moreover, since our channel selection involves only model\ninference, our method significantly reduces computational and GPU memory\noverhead."}
{"id": "2504.08775", "pdf": "https://arxiv.org/pdf/2504.08775", "abs": "https://arxiv.org/abs/2504.08775", "authors": ["Christopher Wolfram", "Aaron Schein"], "title": "Layers at Similar Depths Generate Similar Activations Across LLM Architectures", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "How do the latent spaces used by independently-trained LLMs relate to one\nanother? We study the nearest neighbor relationships induced by activations at\ndifferent layers of 24 open-weight LLMs, and find that they 1) tend to vary\nfrom layer to layer within a model, and 2) are approximately shared between\ncorresponding layers of different models. Claim 2 shows that these nearest\nneighbor relationships are not arbitrary, as they are shared across models, but\nClaim 1 shows that they are not \"obvious\" either, as there is no single set of\nnearest neighbor relationships that is universally shared. Together, these\nsuggest that LLMs generate a progression of activation geometries from layer to\nlayer, but that this entire progression is largely shared between models,\nstretched and squeezed to fit into different architectures."}
{"id": "2504.08959", "pdf": "https://arxiv.org/pdf/2504.08959", "abs": "https://arxiv.org/abs/2504.08959", "authors": ["Yilin Wang", "Chuan Guo", "Yuxuan Mu", "Muhammad Gohar Javed", "Xinxin Zuo", "Juwei Lu", "Hai Jiang", "Li Cheng"], "title": "MotionDreamer: One-to-Many Motion Synthesis with Localized Generative Masked Transformer", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "ICLR 2025 acceptance", "summary": "Generative masked transformers have demonstrated remarkable success across\nvarious content generation tasks, primarily due to their ability to effectively\nmodel large-scale dataset distributions with high consistency. However, in the\nanimation domain, large datasets are not always available. Applying generative\nmasked modeling to generate diverse instances from a single MoCap reference may\nlead to overfitting, a challenge that remains unexplored. In this work, we\npresent MotionDreamer, a localized masked modeling paradigm designed to learn\ninternal motion patterns from a given motion with arbitrary topology and\nduration. By embedding the given motion into quantized tokens with a novel\ndistribution regularization method, MotionDreamer constructs a robust and\ninformative codebook for local motion patterns. Moreover, a sliding window\nlocal attention is introduced in our masked transformer, enabling the\ngeneration of natural yet diverse animations that closely resemble the\nreference motion patterns. As demonstrated through comprehensive experiments,\nMotionDreamer outperforms the state-of-the-art methods that are typically GAN\nor Diffusion-based in both faithfulness and diversity. Thanks to the\nconsistency and robustness of the quantization-based approach, MotionDreamer\ncan also effectively perform downstream tasks such as temporal motion editing,\n\\textcolor{update}{crowd animation}, and beat-aligned dance generation, all\nusing a single reference motion. Visit our project page:\nhttps://motiondreamer.github.io/"}
{"id": "2504.08776", "pdf": "https://arxiv.org/pdf/2504.08776", "abs": "https://arxiv.org/abs/2504.08776", "authors": ["Gautam Kishore Shahi", "Oshani Seneviratne", "Marc Spaniol"], "title": "SemCAFE: When Named Entities make the Difference Assessing Web Source Reliability through Entity-level Analytics", "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "With the shift from traditional to digital media, the online landscape now\nhosts not only reliable news articles but also a significant amount of\nunreliable content. Digital media has faster reachability by significantly\ninfluencing public opinion and advancing political agendas. While newspaper\nreaders may be familiar with their preferred outlets political leanings or\ncredibility, determining unreliable news articles is much more challenging. The\ncredibility of many online sources is often opaque, with AI generated content\nbeing easily disseminated at minimal cost. Unreliable news articles,\nparticularly those that followed the Russian invasion of Ukraine in 2022,\nclosely mimic the topics and writing styles of credible sources, making them\ndifficult to distinguish. To address this, we introduce SemCAFE, a system\ndesigned to detect news reliability by incorporating entity relatedness into\nits assessment. SemCAFE employs standard Natural Language Processing\ntechniques, such as boilerplate removal and tokenization, alongside entity\nlevel semantic analysis using the YAGO knowledge base. By creating a semantic\nfingerprint for each news article, SemCAFE could assess the credibility of\n46,020 reliable and 3,407 unreliable articles on the 2022 Russian invasion of\nUkraine. Our approach improved the macro F1 score by 12% over state of the art\nmethods. The sample data and code are available on GitHub"}
{"id": "2504.08966", "pdf": "https://arxiv.org/pdf/2504.08966", "abs": "https://arxiv.org/abs/2504.08966", "authors": ["Mohamed Dhouib", "Davide Buscaldi", "Sonia Vanier", "Aymen Shabou"], "title": "PACT: Pruning and Clustering-Based Token Reduction for Faster Visual Language Models", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Visual Language Models require substantial computational resources for\ninference due to the additional input tokens needed to represent visual\ninformation. However, these visual tokens often contain redundant and\nunimportant information, resulting in an unnecessarily high number of tokens.\nTo address this, we introduce PACT, a method that reduces inference time and\nmemory usage by pruning irrelevant tokens and merging visually redundant ones\nat an early layer of the language model. Our approach uses a novel importance\nmetric to identify unimportant tokens without relying on attention scores,\nmaking it compatible with FlashAttention. We also propose a novel clustering\nalgorithm, called Distance Bounded Density Peak Clustering, which efficiently\nclusters visual tokens while constraining the distances between elements within\na cluster by a predefined threshold. We demonstrate the effectiveness of PACT\nthrough extensive experiments."}
{"id": "2504.08778", "pdf": "https://arxiv.org/pdf/2504.08778", "abs": "https://arxiv.org/abs/2504.08778", "authors": ["Bo Xiong", "Steffen Staab"], "title": "From Tokens to Lattices: Emergent Lattice Structures in Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "ICLR 2025", "summary": "Pretrained masked language models (MLMs) have demonstrated an impressive\ncapability to comprehend and encode conceptual knowledge, revealing a lattice\nstructure among concepts. This raises a critical question: how does this\nconceptualization emerge from MLM pretraining? In this paper, we explore this\nproblem from the perspective of Formal Concept Analysis (FCA), a mathematical\nframework that derives concept lattices from the observations of\nobject-attribute relationships. We show that the MLM's objective implicitly\nlearns a \\emph{formal context} that describes objects, attributes, and their\ndependencies, which enables the reconstruction of a concept lattice through\nFCA. We propose a novel framework for concept lattice construction from\npretrained MLMs and investigate the origin of the inductive biases of MLMs in\nlattice structure learning. Our framework differs from previous work because it\ndoes not rely on human-defined concepts and allows for discovering \"latent\"\nconcepts that extend beyond human definitions. We create three datasets for\nevaluation, and the empirical results verify our hypothesis."}
{"id": "2504.08982", "pdf": "https://arxiv.org/pdf/2504.08982", "abs": "https://arxiv.org/abs/2504.08982", "authors": ["Kyle Stein", "Andrew Arash Mahyari", "Guillermo Francia III", "Eman El-Sheikh"], "title": "Adaptive Additive Parameter Updates of Vision Transformers for Few-Shot Continual Learning", "categories": ["cs.CV"], "comment": null, "summary": "Integrating new class information without losing previously acquired\nknowledge remains a central challenge in artificial intelligence, often\nreferred to as catastrophic forgetting. Few-shot class incremental learning\n(FSCIL) addresses this by first training a model on a robust dataset of base\nclasses and then incrementally adapting it in successive sessions using only a\nfew labeled examples per novel class. However, this approach is prone to\noverfitting on the limited new data, which can compromise overall performance\nand exacerbate forgetting. In this work, we propose a simple yet effective\nnovel FSCIL framework that leverages a frozen Vision Transformer (ViT) backbone\naugmented with parameter-efficient additive updates. Our approach freezes the\npre-trained ViT parameters and selectively injects trainable weights into the\nself-attention modules via an additive update mechanism. This design updates\nonly a small subset of parameters to accommodate new classes without\nsacrificing the representations learned during the base session. By fine-tuning\na limited number of parameters, our method preserves the generalizable features\nin the frozen ViT while reducing the risk of overfitting. Furthermore, as most\nparameters remain fixed, the model avoids overwriting previously learned\nknowledge when small novel data batches are introduced. Extensive experiments\non benchmark datasets demonstrate that our approach yields state-of-the-art\nperformance compared to baseline FSCIL methods."}
{"id": "2504.08779", "pdf": "https://arxiv.org/pdf/2504.08779", "abs": "https://arxiv.org/abs/2504.08779", "authors": ["Ruoxin Xiong", "Yanyu Wang", "Suat Gunhan", "Yimin Zhu", "Charles Berryman"], "title": "Can AI Master Construction Management (CM)? Benchmarking State-of-the-Art Large Language Models on CM Certification Exams", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The growing complexity of construction management (CM) projects, coupled with\nchallenges such as strict regulatory requirements and labor shortages, requires\nspecialized analytical tools that streamline project workflow and enhance\nperformance. Although large language models (LLMs) have demonstrated\nexceptional performance in general reasoning tasks, their effectiveness in\ntackling CM-specific challenges, such as precise quantitative analysis and\nregulatory interpretation, remains inadequately explored. To bridge this gap,\nthis study introduces CMExamSet, a comprehensive benchmarking dataset\ncomprising 689 authentic multiple-choice questions sourced from four nationally\naccredited CM certification exams. Our zero-shot evaluation assesses overall\naccuracy, subject areas (e.g., construction safety), reasoning complexity\n(single-step and multi-step), and question formats (text-only,\nfigure-referenced, and table-referenced). The results indicate that GPT-4o and\nClaude 3.7 surpass typical human pass thresholds (70%), with average accuracies\nof 82% and 83%, respectively. Additionally, both models performed better on\nsingle-step tasks, with accuracies of 85.7% (GPT-4o) and 86.7% (Claude 3.7).\nMulti-step tasks were more challenging, reducing performance to 76.5% and\n77.6%, respectively. Furthermore, both LLMs show significant limitations on\nfigure-referenced questions, with accuracies dropping to approximately 40%. Our\nerror pattern analysis further reveals that conceptual misunderstandings are\nthe most common (44.4% and 47.9%), underscoring the need for enhanced\ndomain-specific reasoning models. These findings underscore the potential of\nLLMs as valuable supplementary analytical tools in CM, while highlighting the\nneed for domain-specific refinements and sustained human oversight in complex\ndecision making."}
{"id": "2504.09033", "pdf": "https://arxiv.org/pdf/2504.09033", "abs": "https://arxiv.org/abs/2504.09033", "authors": ["Snigdha Agarwal", "Neelam Sinha"], "title": "Chest X-ray Classification using Deep Convolution Models on Low-resolution images with Uncertain Labels", "categories": ["cs.CV", "cs.AI"], "comment": "5 pages, 5 figures", "summary": "Deep Convolutional Neural Networks have consistently proven to achieve\nstate-of-the-art results on a lot of imaging tasks over the past years'\nmajority of which comprise of high-quality data. However, it is important to\nwork on low-resolution images since it could be a cheaper alternative for\nremote healthcare access where the primary need of automated pathology\nidentification models occurs. Medical diagnosis using low-resolution images is\nchallenging since critical details may not be easily identifiable. In this\npaper, we report classification results by experimenting on different input\nimage sizes of Chest X-rays to deep CNN models and discuss the feasibility of\nclassification on varying image sizes. We also leverage the noisy labels in the\ndataset by proposing a Randomized Flipping of labels techniques. We use an\nensemble of multi-label classification models on frontal and lateral studies.\nOur models are trained on 5 out of the 14 chest pathologies of the publicly\navailable CheXpert dataset. We incorporate techniques such as augmentation,\nregularization for model improvement and use class activation maps to visualize\nthe neural network's decision making. Comparison with classification results on\ndata from 200 subjects, obtained on the corresponding high-resolution images,\nreported in the original CheXpert paper, has been presented. For pathologies\nCardiomegaly, Consolidation and Edema, we obtain 3% higher accuracy with our\nmodel architecture."}
{"id": "2504.08781", "pdf": "https://arxiv.org/pdf/2504.08781", "abs": "https://arxiv.org/abs/2504.08781", "authors": ["Xu-Xiang Zhong", "Chao Yi", "Han-Jia Ye"], "title": "Efficient Evaluation of Large Language Models via Collaborative Filtering", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "With the development of Large Language Models (LLMs), numerous benchmarks\nhave been proposed to measure and compare the capabilities of different LLMs.\nHowever, evaluating LLMs is costly due to the large number of test instances\nand their slow inference speed. In this paper, we aim to explore how to\nefficiently estimate a model's real performance on a given benchmark based on\nits evaluation results on a small number of instances sampled from the\nbenchmark. Inspired by Collaborative Filtering (CF) in Recommendation Systems\n(RS), we treat LLMs as users and test instances as items and propose a\ntwo-stage method. In the first stage, we treat instance selection as\nrecommending products to users to choose instances that can easily distinguish\nmodel performance. In the second stage, we see performance prediction as rating\nprediction problem in RS to predict the target LLM's behavior on unselected\ninstances. Experiments on multiple LLMs and datasets imply that our method can\naccurately estimate the target model's performance while largely reducing its\ninference overhead."}
{"id": "2504.09039", "pdf": "https://arxiv.org/pdf/2504.09039", "abs": "https://arxiv.org/abs/2504.09039", "authors": ["Gen Li", "Yang Xiao", "Jie Ji", "Kaiyuan Deng", "Bo Hui", "Linke Guo", "Xiaolong Ma"], "title": "Sculpting Memory: Multi-Concept Forgetting in Diffusion Models via Dynamic Mask and Concept-Aware Optimization", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Text-to-image (T2I) diffusion models have achieved remarkable success in\ngenerating high-quality images from textual prompts. However, their ability to\nstore vast amounts of knowledge raises concerns in scenarios where selective\nforgetting is necessary, such as removing copyrighted content, reducing biases,\nor eliminating harmful concepts. While existing unlearning methods can remove\ncertain concepts, they struggle with multi-concept forgetting due to\ninstability, residual knowledge persistence, and generation quality\ndegradation. To address these challenges, we propose \\textbf{Dynamic Mask\ncoupled with Concept-Aware Loss}, a novel unlearning framework designed for\nmulti-concept forgetting in diffusion models. Our \\textbf{Dynamic Mask}\nmechanism adaptively updates gradient masks based on current optimization\nstates, allowing selective weight modifications that prevent interference with\nunrelated knowledge. Additionally, our \\textbf{Concept-Aware Loss} explicitly\nguides the unlearning process by enforcing semantic consistency through\nsuperclass alignment, while a regularization loss based on knowledge\ndistillation ensures that previously unlearned concepts remain forgotten during\nsequential unlearning. We conduct extensive experiments to evaluate our\napproach. Results demonstrate that our method outperforms existing unlearning\ntechniques in forgetting effectiveness, output fidelity, and semantic\ncoherence, particularly in multi-concept scenarios. Our work provides a\nprincipled and flexible framework for stable and high-fidelity unlearning in\ngenerative models. The code will be released publicly."}
{"id": "2504.08792", "pdf": "https://arxiv.org/pdf/2504.08792", "abs": "https://arxiv.org/abs/2504.08792", "authors": ["Toqeer Ehsan", "Thamar Solorio"], "title": "Enhancing NER Performance in Low-Resource Pakistani Languages using Cross-Lingual Data Augmentation", "categories": ["cs.CL", "cs.IR"], "comment": "Accepted to W-NUT 2025 @ NAACL", "summary": "Named Entity Recognition (NER), a fundamental task in Natural Language\nProcessing (NLP), has shown significant advancements for high-resource\nlanguages. However, due to a lack of annotated datasets and limited\nrepresentation in Pre-trained Language Models (PLMs), it remains understudied\nand challenging for low-resource languages. To address these challenges, we\npropose a data augmentation technique that generates culturally plausible\nsentences and experiments on four low-resource Pakistani languages; Urdu,\nShahmukhi, Sindhi, and Pashto. By fine-tuning multilingual masked Large\nLanguage Models (LLMs), our approach demonstrates significant improvements in\nNER performance for Shahmukhi and Pashto. We further explore the capability of\ngenerative LLMs for NER and data augmentation using few-shot learning."}
{"id": "2504.09048", "pdf": "https://arxiv.org/pdf/2504.09048", "abs": "https://arxiv.org/abs/2504.09048", "authors": ["Yongchang Wu", "Zipeng Qi", "Zhenwei Shi", "Zhengxia Zou"], "title": "BlockGaussian: Efficient Large-Scale Scene NovelView Synthesis via Adaptive Block-Based Gaussian Splatting", "categories": ["cs.CV"], "comment": "https://github.com/SunshineWYC/BlockGaussian", "summary": "The recent advancements in 3D Gaussian Splatting (3DGS) have demonstrated\nremarkable potential in novel view synthesis tasks. The divide-and-conquer\nparadigm has enabled large-scale scene reconstruction, but significant\nchallenges remain in scene partitioning, optimization, and merging processes.\nThis paper introduces BlockGaussian, a novel framework incorporating a\ncontent-aware scene partition strategy and visibility-aware block optimization\nto achieve efficient and high-quality large-scale scene reconstruction.\nSpecifically, our approach considers the content-complexity variation across\ndifferent regions and balances computational load during scene partitioning,\nenabling efficient scene reconstruction. To tackle the supervision mismatch\nissue during independent block optimization, we introduce auxiliary points\nduring individual block optimization to align the ground-truth supervision,\nwhich enhances the reconstruction quality. Furthermore, we propose a\npseudo-view geometry constraint that effectively mitigates rendering\ndegradation caused by airspace floaters during block merging. Extensive\nexperiments on large-scale scenes demonstrate that our approach achieves\nstate-of-the-art performance in both reconstruction efficiency and rendering\nquality, with a 5x speedup in optimization and an average PSNR improvement of\n1.21 dB on multiple benchmarks. Notably, BlockGaussian significantly reduces\ncomputational requirements, enabling large-scale scene reconstruction on a\nsingle 24GB VRAM device. The project page is available at\nhttps://github.com/SunshineWYC/BlockGaussian"}
{"id": "2504.08798", "pdf": "https://arxiv.org/pdf/2504.08798", "abs": "https://arxiv.org/abs/2504.08798", "authors": ["Xiaomei Zhang", "Zhaoxi Zhang", "Yanjun Zhang", "Xufei Zheng", "Leo Yu Zhang", "Shengshan Hu", "Shirui Pan"], "title": "Exploring Gradient-Guided Masked Language Model to Detect Textual Adversarial Attacks", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Textual adversarial examples pose serious threats to the reliability of\nnatural language processing systems. Recent studies suggest that adversarial\nexamples tend to deviate from the underlying manifold of normal texts, whereas\npre-trained masked language models can approximate the manifold of normal data.\nThese findings inspire the exploration of masked language models for detecting\ntextual adversarial attacks. We first introduce Masked Language Model-based\nDetection (MLMD), leveraging the mask and unmask operations of the masked\nlanguage modeling (MLM) objective to induce the difference in manifold changes\nbetween normal and adversarial texts. Although MLMD achieves competitive\ndetection performance, its exhaustive one-by-one masking strategy introduces\nsignificant computational overhead. Our posterior analysis reveals that a\nsignificant number of non-keywords in the input are not important for detection\nbut consume resources. Building on this, we introduce Gradient-guided MLMD\n(GradMLMD), which leverages gradient information to identify and skip\nnon-keywords during detection, significantly reducing resource consumption\nwithout compromising detection performance."}
{"id": "2504.09062", "pdf": "https://arxiv.org/pdf/2504.09062", "abs": "https://arxiv.org/abs/2504.09062", "authors": ["Zhijie Shen", "Chunyu Lin", "Shujuan Huang", "Lang Nie", "Kang Liao", "Yao Zhao"], "title": "You Need a Transition Plane: Bridging Continuous Panoramic 3D Reconstruction with Perspective Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Recently, reconstructing scenes from a single panoramic image using advanced\n3D Gaussian Splatting (3DGS) techniques has attracted growing interest.\nPanoramic images offer a 360$\\times$ 180 field of view (FoV), capturing the\nentire scene in a single shot. However, panoramic images introduce severe\ndistortion, making it challenging to render 3D Gaussians into 2D distorted\nequirectangular space directly. Converting equirectangular images to cubemap\nprojections partially alleviates this problem but introduces new challenges,\nsuch as projection distortion and discontinuities across cube-face boundaries.\nTo address these limitations, we present a novel framework, named TPGS, to\nbridge continuous panoramic 3D scene reconstruction with perspective Gaussian\nsplatting. Firstly, we introduce a Transition Plane between adjacent cube faces\nto enable smoother transitions in splatting directions and mitigate\noptimization ambiguity in the boundary region. Moreover, an intra-to-inter face\noptimization strategy is proposed to enhance local details and restore visual\nconsistency across cube-face boundaries. Specifically, we optimize 3D Gaussians\nwithin individual cube faces and then fine-tune them in the stitched panoramic\nspace. Additionally, we introduce a spherical sampling technique to eliminate\nvisible stitching seams. Extensive experiments on indoor and outdoor,\negocentric, and roaming benchmark datasets demonstrate that our approach\noutperforms existing state-of-the-art methods. Code and models will be\navailable at https://github.com/zhijieshen-bjtu/TPGS."}
{"id": "2504.08808", "pdf": "https://arxiv.org/pdf/2504.08808", "abs": "https://arxiv.org/abs/2504.08808", "authors": ["Zhengke Sun", "Hangwei Qian", "Ivor Tsang"], "title": "Exploring the Effectiveness and Interpretability of Texts in LLM-based Time Series Models", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have been applied to time series forecasting\ntasks, leveraging pre-trained language models as the backbone and incorporating\ntextual data to purportedly enhance the comprehensive capabilities of LLMs for\ntime series. However, are these texts really helpful for interpretation? This\nstudy seeks to investigate the actual efficacy and interpretability of such\ntextual incorporations. Through a series of empirical experiments on textual\nprompts and textual prototypes, our findings reveal that the misalignment\nbetween two modalities exists, and the textual information does not\nsignificantly improve time series forecasting performance in many cases.\nFurthermore, visualization analysis indicates that the textual representations\nlearned by existing frameworks lack sufficient interpretability when applied to\ntime series data. We further propose a novel metric named Semantic Matching\nIndex (SMI) to better evaluate the matching degree between time series and\ntexts during our post hoc interpretability investigation. Our analysis reveals\nthe misalignment and limited interpretability of texts in current time-series\nLLMs, and we hope this study can raise awareness of the interpretability of\ntexts for time series. The code is available at\nhttps://github.com/zachysun/TS-Lang-Exp."}
{"id": "2504.09066", "pdf": "https://arxiv.org/pdf/2504.09066", "abs": "https://arxiv.org/abs/2504.09066", "authors": ["Yifan Yang", "Lei Zou", "Bing Zhou", "Daoyang Li", "Binbin Lin", "Joynal Abedin", "Mingzheng Yang"], "title": "Hyperlocal disaster damage assessment using bi-temporal street-view imagery and pre-trained vision models", "categories": ["cs.CV"], "comment": "27 pages,9 figures", "summary": "Street-view images offer unique advantages for disaster damage estimation as\nthey capture impacts from a visual perspective and provide detailed,\non-the-ground insights. Despite several investigations attempting to analyze\nstreet-view images for damage estimation, they mainly focus on post-disaster\nimages. The potential of time-series street-view images remains underexplored.\nPre-disaster images provide valuable benchmarks for accurate damage estimations\nat building and street levels. These images could aid annotators in objectively\nlabeling post-disaster impacts, improving the reliability of labeled data sets\nfor model training, and potentially enhancing the model performance in damage\nevaluation. The goal of this study is to estimate hyperlocal, on-the-ground\ndisaster damages using bi-temporal street-view images and advanced pre-trained\nvision models. Street-view images before and after 2024 Hurricane Milton in\nHorseshoe Beach, Florida, were collected for experiments. The objectives are:\n(1) to assess the performance gains of incorporating pre-disaster street-view\nimages as a no-damage category in fine-tuning pre-trained models, including\nSwin Transformer and ConvNeXt, for damage level classification; (2) to design\nand evaluate a dual-channel algorithm that reads pair-wise pre- and\npost-disaster street-view images for hyperlocal damage assessment. The results\nindicate that incorporating pre-disaster street-view images and employing a\ndual-channel processing framework can significantly enhance damage assessment\naccuracy. The accuracy improves from 66.14% with the Swin Transformer baseline\nto 77.11% with the dual-channel Feature-Fusion ConvNeXt model. This research\nenables rapid, operational damage assessments at hyperlocal spatial\nresolutions, providing valuable insights to support effective decision-making\nin disaster management and resilience planning."}
{"id": "2504.08820", "pdf": "https://arxiv.org/pdf/2504.08820", "abs": "https://arxiv.org/abs/2504.08820", "authors": ["Jing Yao", "Xiaoyuan Yi", "Jindong Wang", "Zhicheng Dou", "Xing Xie"], "title": "CAReDiO: Cultural Alignment of LLM via Representativeness and Distinctiveness Guided Data Optimization", "categories": ["cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) more deeply integrate into human life across\nvarious regions, aligning them with pluralistic cultures is crucial for\nimproving user experience and mitigating cultural conflicts. Existing\napproaches develop culturally aligned LLMs primarily through fine-tuning with\nmassive carefully curated culture-specific corpora. Nevertheless, inspired by\nculture theories, we identify two key challenges faced by these datasets: (1)\nRepresentativeness: These corpora fail to fully capture the target culture's\ncore characteristics with redundancy, causing computation waste; (2)\nDistinctiveness: They struggle to distinguish the unique nuances of a given\nculture from shared patterns across other relevant ones, hindering precise\ncultural modeling. To handle these challenges, we introduce CAReDiO, a novel\ncultural data construction framework. Specifically, CAReDiO utilizes powerful\nLLMs to automatically generate cultural conversation data, where both the\nqueries and responses are further optimized by maximizing representativeness\nand distinctiveness. Using CAReDiO, we construct a small yet effective dataset,\ncovering five cultures, and compare it with several recent cultural corpora.\nExtensive experiments demonstrate that our method generates more effective data\nand enables cultural alignment with as few as 100 training samples, enhancing\nboth performance and efficiency."}
{"id": "2504.09069", "pdf": "https://arxiv.org/pdf/2504.09069", "abs": "https://arxiv.org/abs/2504.09069", "authors": ["Shuning Sun", "Yu Zhang", "Chen Wu", "Dianjie Lu", "Dianjie Lu", "Guijuan Zhan", "Yang Weng", "Zhuoran Zheng"], "title": "UniFlowRestore: A General Video Restoration Framework via Flow Matching and Prompt Guidance", "categories": ["cs.CV"], "comment": null, "summary": "Video imaging is often affected by complex degradations such as blur, noise,\nand compression artifacts. Traditional restoration methods follow a\n\"single-task single-model\" paradigm, resulting in poor generalization and high\ncomputational cost, limiting their applicability in real-world scenarios with\ndiverse degradation types. We propose UniFlowRestore, a general video\nrestoration framework that models restoration as a time-continuous evolution\nunder a prompt-guided and physics-informed vector field. A physics-aware\nbackbone PhysicsUNet encodes degradation priors as potential energy, while\nPromptGenerator produces task-relevant prompts as momentum. These components\ndefine a Hamiltonian system whose vector field integrates inertial dynamics,\ndecaying physical gradients, and prompt-based guidance. The system is optimized\nvia a fixed-step ODE solver to achieve efficient and unified restoration across\ntasks. Experiments show that UniFlowRestore delivers stateof-the-art\nperformance with strong generalization and efficiency. Quantitative results\ndemonstrate that UniFlowRestore achieves state-of-the-art performance,\nattaining the highest PSNR (33.89 dB) and SSIM (0.97) on the video denoising\ntask, while maintaining top or second-best scores across all evaluated tasks."}
{"id": "2504.08838", "pdf": "https://arxiv.org/pdf/2504.08838", "abs": "https://arxiv.org/abs/2504.08838", "authors": ["Mike Lasby", "Nish Sinnadurai", "Valavan Manohararajah", "Sean Lie", "Vithursan Thangarasa"], "title": "SD$^2$: Self-Distilled Sparse Drafters", "categories": ["cs.CL", "cs.AI", "I.2.0; I.2.7"], "comment": "21 pages", "summary": "Speculative decoding is a powerful technique for reducing the latency of\nLarge Language Models (LLMs), offering a fault-tolerant framework that enables\nthe use of highly compressed draft models. In this work, we introduce\nSelf-Distilled Sparse Drafters (SD$^2$), a novel methodology that leverages\nself-data distillation and fine-grained weight sparsity to produce highly\nefficient and well-aligned draft models. SD$^2$ systematically enhances draft\ntoken acceptance rates while significantly reducing Multiply-Accumulate\noperations (MACs), even in the Universal Assisted Generation (UAG) setting,\nwhere draft and target models originate from different model families. On a\nLlama-3.1-70B target model, SD$^2$ provides a $\\times$1.59 higher Mean Accepted\nLength (MAL) compared to layer-pruned draft models and reduces MACs by over\n43.87% with a 8.36% reduction in MAL compared to a dense draft models. Our\nresults highlight the potential of sparsity-aware fine-tuning and compression\nstrategies to improve LLM inference efficiency while maintaining alignment with\ntarget models."}
{"id": "2504.09076", "pdf": "https://arxiv.org/pdf/2504.09076", "abs": "https://arxiv.org/abs/2504.09076", "authors": ["Mk Bashar", "Ocean Monjur", "Samia Islam", "Mohammad Galib Shams", "Niamul Quader"], "title": "Exploring Synergistic Ensemble Learning: Uniting CNNs, MLP-Mixers, and Vision Transformers to Enhance Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, Convolutional Neural Networks (CNNs), MLP-mixers, and Vision\nTransformers have risen to prominence as leading neural architectures in image\nclassification. Prior research has underscored the distinct advantages of each\narchitecture, and there is growing evidence that combining modules from\ndifferent architectures can boost performance. In this study, we build upon and\nimprove previous work exploring the complementarity between different\narchitectures. Instead of heuristically merging modules from various\narchitectures through trial and error, we preserve the integrity of each\narchitecture and combine them using ensemble techniques. By maintaining the\ndistinctiveness of each architecture, we aim to explore their inherent\ncomplementarity more deeply and with implicit isolation. This approach provides\na more systematic understanding of their individual strengths.\n  In addition to uncovering insights into architectural complementarity, we\nshowcase the effectiveness of even basic ensemble methods that combine models\nfrom diverse architectures. These methods outperform ensembles comprised of\nsimilar architectures. Our straightforward ensemble framework serves as a\nfoundational strategy for blending complementary architectures, offering a\nsolid starting point for further investigations into the unique strengths and\nsynergies among different architectures and their ensembles in image\nclassification. A direct outcome of this work is the creation of an ensemble of\nclassification networks that surpasses the accuracy of the previous\nstate-of-the-art single classification network on ImageNet, setting a new\nbenchmark, all while requiring less overall latency."}
{"id": "2504.08905", "pdf": "https://arxiv.org/pdf/2504.08905", "abs": "https://arxiv.org/abs/2504.08905", "authors": ["Yunfan Zhang", "Kathleen McKeown", "Smaranda Muresan"], "title": "Forecasting Communication Derailments Through Conversation Generation", "categories": ["cs.CL"], "comment": null, "summary": "Forecasting communication derailment can be useful in real-world settings\nsuch as online content moderation, conflict resolution, and business\nnegotiations. However, despite language models' success at identifying\noffensive speech present in conversations, they struggle to forecast future\ncommunication derailments. In contrast to prior work that predicts conversation\noutcomes solely based on the past conversation history, our approach samples\nmultiple future conversation trajectories conditioned on existing conversation\nhistory using a fine-tuned LLM. It predicts the communication outcome based on\nthe consensus of these trajectories. We also experimented with leveraging\nsocio-linguistic attributes, which reflect turn-level conversation dynamics, as\nguidance when generating future conversations. Our method of future\nconversation trajectories surpasses state-of-the-art results on English\ncommunication derailment prediction benchmarks and demonstrates significant\naccuracy gains in ablation studies."}
{"id": "2504.09077", "pdf": "https://arxiv.org/pdf/2504.09077", "abs": "https://arxiv.org/abs/2504.09077", "authors": ["Bingyu Nan", "Feng Liu", "Xuezhong Qian", "Wei Song"], "title": "A Visual Self-attention Mechanism Facial Expression Recognition Network beyond Convnext", "categories": ["cs.CV"], "comment": null, "summary": "Facial expression recognition is an important research direction in the field\nof artificial intelligence. Although new breakthroughs have been made in recent\nyears, the uneven distribution of datasets and the similarity between different\ncategories of facial expressions, as well as the differences within the same\ncategory among different subjects, remain challenges. This paper proposes a\nvisual facial expression signal feature processing network based on truncated\nConvNeXt approach(Conv-cut), to improve the accuracy of FER under challenging\nconditions. The network uses a truncated ConvNeXt-Base as the feature\nextractor, and then we designed a Detail Extraction Block to extract detailed\nfeatures, and introduced a Self-Attention mechanism to enable the network to\nlearn the extracted features more effectively. To evaluate the proposed\nConv-cut approach, we conducted experiments on the RAF-DB and FERPlus datasets,\nand the results show that our model has achieved state-of-the-art performance.\nOur code could be accessed at Github."}
{"id": "2504.08958", "pdf": "https://arxiv.org/pdf/2504.08958", "abs": "https://arxiv.org/abs/2504.08958", "authors": ["Mehmet Arif Demirtaş", "Claire Zheng", "Max Fowler", "Kathryn Cunningham"], "title": "Generating Planning Feedback for Open-Ended Programming Exercises with LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted as full paper at AIED 2025", "summary": "To complete an open-ended programming exercise, students need to both plan a\nhigh-level solution and implement it using the appropriate syntax. However,\nthese problems are often autograded on the correctness of the final submission\nthrough test cases, and students cannot get feedback on their planning process.\nLarge language models (LLM) may be able to generate this feedback by detecting\nthe overall code structure even for submissions with syntax errors. To this\nend, we propose an approach that detects which high-level goals and patterns\n(i.e. programming plans) exist in a student program with LLMs. We show that\nboth the full GPT-4o model and a small variant (GPT-4o-mini) can detect these\nplans with remarkable accuracy, outperforming baselines inspired by\nconventional approaches to code analysis. We further show that the smaller,\ncost-effective variant (GPT-4o-mini) achieves results on par with\nstate-of-the-art (GPT-4o) after fine-tuning, creating promising implications\nfor smaller models for real-time grading. These smaller models can be\nincorporated into autograders for open-ended code-writing exercises to provide\nfeedback for students' implicit planning skills, even when their program is\nsyntactically incorrect. Furthermore, LLMs may be useful in providing feedback\nfor problems in other domains where students start with a set of high-level\nsolution steps and iteratively compute the output, such as math and physics\nproblems."}
{"id": "2504.09083", "pdf": "https://arxiv.org/pdf/2504.09083", "abs": "https://arxiv.org/abs/2504.09083", "authors": ["Muhammad Adil", "Gaang Lee", "Vicente A. Gonzalez", "Qipei Mei"], "title": "Using Vision Language Models for Safety Hazard Identification in Construction", "categories": ["cs.CV"], "comment": null, "summary": "Safety hazard identification and prevention are the key elements of proactive\nsafety management. Previous research has extensively explored the applications\nof computer vision to automatically identify hazards from image clips collected\nfrom construction sites. However, these methods struggle to identify\ncontext-specific hazards, as they focus on detecting predefined individual\nentities without understanding their spatial relationships and interactions.\nFurthermore, their limited adaptability to varying construction site guidelines\nand conditions hinders their generalization across different projects. These\nlimitations reduce their ability to assess hazards in complex construction\nenvironments and adaptability to unseen risks, leading to potential safety\ngaps. To address these challenges, we proposed and experimentally validated a\nVision Language Model (VLM)-based framework for the identification of\nconstruction hazards. The framework incorporates a prompt engineering module\nthat structures safety guidelines into contextual queries, allowing VLM to\nprocess visual information and generate hazard assessments aligned with the\nregulation guide. Within this framework, we evaluated state-of-the-art VLMs,\nincluding GPT-4o, Gemini, Llama 3.2, and InternVL2, using a custom dataset of\n1100 construction site images. Experimental results show that GPT-4o and Gemini\n1.5 Pro outperformed alternatives and displayed promising BERTScore of 0.906\nand 0.888 respectively, highlighting their ability to identify both general and\ncontext-specific hazards. However, processing times remain a significant\nchallenge, impacting real-time feasibility. These findings offer insights into\nthe practical deployment of VLMs for construction site hazard detection,\nthereby contributing to the enhancement of proactive safety management."}
{"id": "2504.08961", "pdf": "https://arxiv.org/pdf/2504.08961", "abs": "https://arxiv.org/abs/2504.08961", "authors": ["Kseniia Petukhova", "Ekaterina Kochmar"], "title": "A Fully Automated Pipeline for Conversational Discourse Annotation: Tree Scheme Generation and Labeling with Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have shown promise in\nautomating discourse annotation for conversations. While manually designing\ntree annotation schemes significantly improves annotation quality for humans\nand models, their creation remains time-consuming and requires expert\nknowledge. We propose a fully automated pipeline that uses LLMs to construct\nsuch schemes and perform annotation. We evaluate our approach on speech\nfunctions (SFs) and the Switchboard-DAMSL (SWBD-DAMSL) taxonomies. Our\nexperiments compare various design choices, and we show that frequency-guided\ndecision trees, paired with an advanced LLM for annotation, can outperform\npreviously manually designed trees and even match or surpass human annotators\nwhile significantly reducing the time required for annotation. We release all\ncode and resultant schemes and annotations to facilitate future research on\ndiscourse annotation."}
{"id": "2504.09086", "pdf": "https://arxiv.org/pdf/2504.09086", "abs": "https://arxiv.org/abs/2504.09086", "authors": ["Yunfei Long", "Abhinav Kumar", "Xiaoming Liu", "Daniel Morris"], "title": "RICCARDO: Radar Hit Prediction and Convolution for Camera-Radar 3D Object Detection", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Radar hits reflect from points on both the boundary and internal to object\noutlines. This results in a complex distribution of radar hits that depends on\nfactors including object category, size, and orientation. Current radar-camera\nfusion methods implicitly account for this with a black-box neural network. In\nthis paper, we explicitly utilize a radar hit distribution model to assist\nfusion. First, we build a model to predict radar hit distributions conditioned\non object properties obtained from a monocular detector. Second, we use the\npredicted distribution as a kernel to match actual measured radar points in the\nneighborhood of the monocular detections, generating matching scores at nearby\npositions. Finally, a fusion stage combines context with the kernel detector to\nrefine the matching scores. Our method achieves the state-of-the-art\nradar-camera detection performance on nuScenes. Our source code is available at\nhttps://github.com/longyunf/riccardo."}
{"id": "2504.09049", "pdf": "https://arxiv.org/pdf/2504.09049", "abs": "https://arxiv.org/abs/2504.09049", "authors": ["Adrianna Romanowski", "Pedro H. V. Valois", "Kazuhiro Fukui"], "title": "From Punchlines to Predictions: A Metric to Assess LLM Performance in Identifying Humor in Stand-Up Comedy", "categories": ["cs.CL"], "comment": "Accepted to CMCL2025 @ NAACL", "summary": "Comedy serves as a profound reflection of the times we live in and is a\nstaple element of human interactions. In light of the widespread adoption of\nLarge Language Models (LLMs), the intersection of humor and AI has become no\nlaughing matter. Advancements in the naturalness of human-computer interaction\ncorrelates with improvements in AI systems' abilities to understand humor. In\nthis study, we assess the ability of models in accurately identifying humorous\nquotes from a stand-up comedy transcript. Stand-up comedy's unique comedic\nnarratives make it an ideal dataset to improve the overall naturalness of\ncomedic understanding. We propose a novel humor detection metric designed to\nevaluate LLMs amongst various prompts on their capability to extract humorous\npunchlines. The metric has a modular structure that offers three different\nscoring methods - fuzzy string matching, sentence embedding, and subspace\nsimilarity - to provide an overarching assessment of a model's performance. The\nmodel's results are compared against those of human evaluators on the same\ntask. Our metric reveals that regardless of prompt engineering, leading models,\nChatGPT, Claude, and DeepSeek, achieve scores of at most 51% in humor\ndetection. Notably, this performance surpasses that of humans who achieve a\nscore of 41%. The analysis of human evaluators and LLMs reveals variability in\nagreement, highlighting the subjectivity inherent in humor and the complexities\ninvolved in extracting humorous quotes from live performance transcripts. Code\navailable at https://github.com/swaggirl9000/humor."}
{"id": "2504.09097", "pdf": "https://arxiv.org/pdf/2504.09097", "abs": "https://arxiv.org/abs/2504.09097", "authors": ["Jeongwan On", "Kyeonghwan Gwak", "Gunyoung Kang", "Junuk Cha", "Soohyun Hwang", "Hyein Hwang", "Seungryul Baek"], "title": "BIGS: Bimanual Category-agnostic Interaction Reconstruction from Monocular Videos via 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Reconstructing 3Ds of hand-object interaction (HOI) is a fundamental problem\nthat can find numerous applications. Despite recent advances, there is no\ncomprehensive pipeline yet for bimanual class-agnostic interaction\nreconstruction from a monocular RGB video, where two hands and an unknown\nobject are interacting with each other. Previous works tackled the limited\nhand-object interaction case, where object templates are pre-known or only one\nhand is involved in the interaction. The bimanual interaction reconstruction\nexhibits severe occlusions introduced by complex interactions between two hands\nand an object. To solve this, we first introduce BIGS (Bimanual Interaction 3D\nGaussian Splatting), a method that reconstructs 3D Gaussians of hands and an\nunknown object from a monocular video. To robustly obtain object Gaussians\navoiding severe occlusions, we leverage prior knowledge of pre-trained\ndiffusion model with score distillation sampling (SDS) loss, to reconstruct\nunseen object parts. For hand Gaussians, we exploit the 3D priors of hand model\n(i.e., MANO) and share a single Gaussian for two hands to effectively\naccumulate hand 3D information, given limited views. To further consider the 3D\nalignment between hands and objects, we include the interacting-subjects\noptimization step during Gaussian optimization. Our method achieves the\nstate-of-the-art accuracy on two challenging datasets, in terms of 3D hand pose\nestimation (MPJPE), 3D object reconstruction (CDh, CDo, F10), and rendering\nquality (PSNR, SSIM, LPIPS), respectively."}
{"id": "2504.09071", "pdf": "https://arxiv.org/pdf/2504.09071", "abs": "https://arxiv.org/abs/2504.09071", "authors": ["Matt Grenander", "Siddharth Varia", "Paula Czarnowska", "Yogarshi Vyas", "Kishaloy Halder", "Bonan Min"], "title": "Exploration of Plan-Guided Summarization for Narrative Texts: the Case of Small Language Models", "categories": ["cs.CL"], "comment": "Accepted to the 7th Workshop on Narrative Understanding (WNU),\n  co-located with NAACL 2025", "summary": "Plan-guided summarization attempts to reduce hallucinations in small language\nmodels (SLMs) by grounding generated summaries to the source text, typically by\ntargeting fine-grained details such as dates or named entities. In this work,\nwe investigate whether plan-based approaches in SLMs improve summarization in\nlong document, narrative tasks. Narrative texts' length and complexity often\nmean they are difficult to summarize faithfully. We analyze existing\nplan-guided solutions targeting fine-grained details, and also propose our own\nhigher-level, narrative-based plan formulation. Our results show that neither\napproach significantly improves on a baseline without planning in either\nsummary quality or faithfulness. Human evaluation reveals that while\nplan-guided approaches are often well grounded to their plan, plans are equally\nlikely to contain hallucinations compared to summaries. As a result, the\nplan-guided summaries are just as unfaithful as those from models without\nplanning. Our work serves as a cautionary tale to plan-guided approaches to\nsummarization, especially for long, complex domains such as narrative texts."}
{"id": "2504.09106", "pdf": "https://arxiv.org/pdf/2504.09106", "abs": "https://arxiv.org/abs/2504.09106", "authors": ["Yonghao Huang", "Leiting Chen", "Chuan Zhou"], "title": "Multi-modal and Multi-view Fundus Image Fusion for Retinopathy Diagnosis via Multi-scale Cross-attention and Shifted Window Self-attention", "categories": ["cs.CV"], "comment": null, "summary": "The joint interpretation of multi-modal and multi-view fundus images is\ncritical for retinopathy prevention, as different views can show the complete\n3D eyeball field and different modalities can provide complementary lesion\nareas. Compared with single images, the sequence relationships in multi-modal\nand multi-view fundus images contain long-range dependencies in lesion\nfeatures. By modeling the long-range dependencies in these sequences, lesion\nareas can be more comprehensively mined, and modality-specific lesions can be\ndetected. To learn the long-range dependency relationship and fuse\ncomplementary multi-scale lesion features between different fundus modalities,\nwe design a multi-modal fundus image fusion method based on multi-scale\ncross-attention, which solves the static receptive field problem in previous\nmulti-modal medical fusion methods based on attention. To capture multi-view\nrelative positional relationships between different views and fuse\ncomprehensive lesion features between different views, we design a multi-view\nfundus image fusion method based on shifted window self-attention, which also\nsolves the computational complexity of the multi-view fundus fusion method\nbased on self-attention is quadratic to the size and number of multi-view\nfundus images. Finally, we design a multi-task retinopathy diagnosis framework\nto help ophthalmologists reduce workload and improve diagnostic accuracy by\ncombining the proposed two fusion methods. The experimental results of\nretinopathy classification and report generation tasks indicate our method's\npotential to improve the efficiency and reliability of retinopathy diagnosis in\nclinical practice, achieving a classification accuracy of 82.53\\% and a report\ngeneration BlEU-1 of 0.543."}
{"id": "2504.09073", "pdf": "https://arxiv.org/pdf/2504.09073", "abs": "https://arxiv.org/abs/2504.09073", "authors": ["Akanksha Mehndiratta", "Krishna Asawa"], "title": "A Multi-view Discourse Framework for Integrating Semantic and Syntactic Features in Dialog Agents", "categories": ["cs.CL"], "comment": null, "summary": "Multiturn dialogue models aim to generate human-like responses by leveraging\nconversational context, consisting of utterances from previous exchanges.\nExisting methods often neglect the interactions between these utterances or\ntreat all of them as equally significant. This paper introduces a\ndiscourse-aware framework for response selection in retrieval-based dialogue\nsystems. The proposed model first encodes each utterance and response with\ncontextual, positional, and syntactic features using Multi-view Canonical\nCorrelation Analysis (MCCA). It then learns discourse tokens that capture\nrelationships between an utterance and its surrounding turns in a shared\nsubspace via Canonical Correlation Analysis (CCA). This two-step approach\neffectively integrates semantic and syntactic features to build discourse-level\nunderstanding. Experiments on the Ubuntu Dialogue Corpus demonstrate that our\nmodel achieves significant improvements in automatic evaluation metrics,\nhighlighting its effectiveness in response selection."}
{"id": "2504.09109", "pdf": "https://arxiv.org/pdf/2504.09109", "abs": "https://arxiv.org/abs/2504.09109", "authors": ["Ganxi Xu", "Jinyi Long", "Hanrui Wu", "Jia Zhang"], "title": "Probability Distribution Alignment and Low-Rank Weight Decomposition for Source-Free Domain Adaptive Brain Decoding", "categories": ["cs.CV"], "comment": null, "summary": "Brain decoding currently faces significant challenges in individual\ndifferences, modality alignment, and high-dimensional embeddings. To address\nindividual differences, researchers often use source subject data, which leads\nto issues such as privacy leakage and heavy data storage burdens. In modality\nalignment, current works focus on aligning the softmax probability distribution\nbut neglect the alignment of marginal probability distributions, resulting in\nmodality misalignment. Additionally, images and text are aligned separately\nwith fMRI without considering the complex interplay between images and text,\nleading to poor image reconstruction. Finally, the enormous dimensionality of\nCLIP embeddings causes significant computational costs. Although the\ndimensionality of CLIP embeddings can be reduced by ignoring the number of\npatches obtained from images and the number of tokens acquired from text, this\ncomes at the cost of a significant drop in model performance, creating a\ndilemma. To overcome these limitations, we propose a source-free domain\nadaptation-based brain decoding framework"}
{"id": "2504.09094", "pdf": "https://arxiv.org/pdf/2504.09094", "abs": "https://arxiv.org/abs/2504.09094", "authors": ["Akanksha Mehndiratta", "Krishna Asawa"], "title": "Enhancing Dialogue Systems with Discourse-Level Understanding Using Deep Canonical Correlation Analysis", "categories": ["cs.CL"], "comment": null, "summary": "The evolution of conversational agents has been driven by the need for more\ncontextually aware systems that can effectively manage dialogue over extended\ninteractions. To address the limitations of existing models in capturing and\nutilizing long-term conversational history, we propose a novel framework that\nintegrates Deep Canonical Correlation Analysis (DCCA) for discourse-level\nunderstanding. This framework learns discourse tokens to capture relationships\nbetween utterances and their surrounding context, enabling a better\nunderstanding of long-term dependencies. Experiments on the Ubuntu Dialogue\nCorpus demonstrate significant enhancement in response selection, based on the\nimproved automatic evaluation metric scores. The results highlight the\npotential of DCCA in improving dialogue systems by allowing them to filter out\nirrelevant context and retain critical discourse information for more accurate\nresponse retrieval."}
{"id": "2504.09129", "pdf": "https://arxiv.org/pdf/2504.09129", "abs": "https://arxiv.org/abs/2504.09129", "authors": ["Jizong Peng", "Tze Ho Elden Tse", "Kai Xu", "Wenchao Gao", "Angela Yao"], "title": "A Constrained Optimization Approach for Gaussian Splatting from Coarsely-posed Images and Noisy Lidar Point Clouds", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) is a powerful reconstruction technique, but it\nneeds to be initialized from accurate camera poses and high-fidelity point\nclouds. Typically, the initialization is taken from Structure-from-Motion (SfM)\nalgorithms; however, SfM is time-consuming and restricts the application of\n3DGS in real-world scenarios and large-scale scene reconstruction. We introduce\na constrained optimization method for simultaneous camera pose estimation and\n3D reconstruction that does not require SfM support. Core to our approach is\ndecomposing a camera pose into a sequence of camera-to-(device-)center and\n(device-)center-to-world optimizations. To facilitate, we propose two\noptimization constraints conditioned to the sensitivity of each parameter group\nand restricts each parameter's search space. In addition, as we learn the scene\ngeometry directly from the noisy point clouds, we propose geometric constraints\nto improve the reconstruction quality. Experiments demonstrate that the\nproposed method significantly outperforms the existing (multi-modal) 3DGS\nbaseline and methods supplemented by COLMAP on both our collected dataset and\ntwo public benchmarks."}
{"id": "2504.09118", "pdf": "https://arxiv.org/pdf/2504.09118", "abs": "https://arxiv.org/abs/2504.09118", "authors": ["Yifei He", "Måns I. Andersson", "Stefano Markidis"], "title": "Optimizing FDTD Solvers for Electromagnetics: A Compiler-Guided Approach with High-Level Tensor Abstractions", "categories": ["cs.CL"], "comment": null, "summary": "The Finite Difference Time Domain (FDTD) method is a widely used numerical\ntechnique for solving Maxwell's equations, particularly in computational\nelectromagnetics and photonics. It enables accurate modeling of wave\npropagation in complex media and structures but comes with significant\ncomputational challenges. Traditional FDTD implementations rely on handwritten,\nplatform-specific code that optimizes certain kernels while underperforming in\nothers. The lack of portability increases development overhead and creates\nperformance bottlenecks, limiting scalability across modern hardware\narchitectures. To address these challenges, we introduce an end-to-end\ndomain-specific compiler based on the MLIR/LLVM infrastructure for FDTD\nsimulations. Our approach generates efficient and portable code optimized for\ndiverse hardware platforms.We implement the three-dimensional FDTD kernel as\noperations on a 3D tensor abstraction with explicit computational semantics.\nHigh-level optimizations such as loop tiling, fusion, and vectorization are\nautomatically applied by the compiler. We evaluate our customized code\ngeneration pipeline on Intel, AMD, and ARM platforms, achieving up to\n$10\\times$ speedup over baseline Python implementation using NumPy."}
{"id": "2504.09149", "pdf": "https://arxiv.org/pdf/2504.09149", "abs": "https://arxiv.org/abs/2504.09149", "authors": ["Changhao Li", "Yu Xin", "Xiaowei Zhou", "Ariel Shamir", "Hao Zhang", "Ligang Liu", "Ruizhen Hu"], "title": "MASH: Masked Anchored SpHerical Distances for 3D Shape Representation and Generation", "categories": ["cs.CV", "cs.CG"], "comment": "11 pages, 11 figures, SIGGRAPH 2025 Accept - Conference", "summary": "We introduce Masked Anchored SpHerical Distances (MASH), a novel multi-view\nand parametrized representation of 3D shapes. Inspired by multi-view geometry\nand motivated by the importance of perceptual shape understanding for learning\n3D shapes, MASH represents a 3D shape as a collection of observable local\nsurface patches, each defined by a spherical distance function emanating from\nan anchor point. We further leverage the compactness of spherical harmonics to\nencode the MASH functions, combined with a generalized view cone with a\nparameterized base that masks the spatial extent of the spherical function to\nattain locality. We develop a differentiable optimization algorithm capable of\nconverting any point cloud into a MASH representation accurately approximating\nground-truth surfaces with arbitrary geometry and topology. Extensive\nexperiments demonstrate that MASH is versatile for multiple applications\nincluding surface reconstruction, shape generation, completion, and blending,\nachieving superior performance thanks to its unique representation encompassing\nboth implicit and explicit features."}
{"id": "2504.09130", "pdf": "https://arxiv.org/pdf/2504.09130", "abs": "https://arxiv.org/abs/2504.09130", "authors": ["Yikun Wang", "Siyin Wang", "Qinyuan Cheng", "Zhaoye Fei", "Liang Ding", "Qipeng Guo", "Dacheng Tao", "Xipeng Qiu"], "title": "VisuoThink: Empowering LVLM Reasoning with Multimodal Tree Search", "categories": ["cs.CL"], "comment": "12 pages", "summary": "Recent advancements in Large Vision-Language Models have showcased remarkable\ncapabilities. However, they often falter when confronted with complex reasoning\ntasks that humans typically address through visual aids and deliberate,\nstep-by-step thinking. While existing methods have explored text-based slow\nthinking or rudimentary visual assistance, they fall short of capturing the\nintricate, interleaved nature of human visual-verbal reasoning processes. To\novercome these limitations and inspired by the mechanisms of slow thinking in\nhuman cognition, we introduce VisuoThink, a novel framework that seamlessly\nintegrates visuospatial and linguistic domains. VisuoThink facilitates\nmultimodal slow thinking by enabling progressive visual-textual reasoning and\nincorporates test-time scaling through look-ahead tree search. Extensive\nexperiments demonstrate that VisuoThink significantly enhances reasoning\ncapabilities via inference-time scaling, even without fine-tuning, achieving\nstate-of-the-art performance in tasks involving geometry and spatial reasoning."}
{"id": "2504.09155", "pdf": "https://arxiv.org/pdf/2504.09155", "abs": "https://arxiv.org/abs/2504.09155", "authors": ["Zhanzhou Feng", "Shiliang Zhang"], "title": "Evolved Hierarchical Masking for Self-Supervised Learning", "categories": ["cs.CV"], "comment": null, "summary": "Existing Masked Image Modeling methods apply fixed mask patterns to guide the\nself-supervised training. As those mask patterns resort to different criteria\nto depict image contents, sticking to a fixed pattern leads to a limited vision\ncues modeling capability.This paper introduces an evolved hierarchical masking\nmethod to pursue general visual cues modeling in self-supervised learning. The\nproposed method leverages the vision model being trained to parse the input\nvisual cues into a hierarchy structure, which is hence adopted to generate\nmasks accordingly. The accuracy of hierarchy is on par with the capability of\nthe model being trained, leading to evolved mask patterns at different training\nstages. Initially, generated masks focus on low-level visual cues to grasp\nbasic textures, then gradually evolve to depict higher-level cues to reinforce\nthe learning of more complicated object semantics and contexts. Our method does\nnot require extra pre-trained models or annotations and ensures training\nefficiency by evolving the training difficulty. We conduct extensive\nexperiments on seven downstream tasks including partial-duplicate image\nretrieval relying on low-level details, as well as image classification and\nsemantic segmentation that require semantic parsing capability. Experimental\nresults demonstrate that it substantially boosts performance across these\ntasks. For instance, it surpasses the recent MAE by 1.1\\% in imageNet-1K\nclassification and 1.4\\% in ADE20K segmentation with the same training epochs.\nWe also align the proposed method with the current research focus on LLMs. The\nproposed approach bridges the gap with large-scale pre-training on semantic\ndemanding tasks and enhances intricate detail perception in tasks requiring\nlow-level feature recognition."}
{"id": "2504.09135", "pdf": "https://arxiv.org/pdf/2504.09135", "abs": "https://arxiv.org/abs/2504.09135", "authors": ["Haotian Ye", "Himanshu Jain", "Chong You", "Ananda Theertha Suresh", "Haowei Lin", "James Zou", "Felix Yu"], "title": "Efficient and Asymptotically Unbiased Constrained Decoding for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "In real-world applications of large language models, outputs are often\nrequired to be confined: selecting items from predefined product or document\nsets, generating phrases that comply with safety standards, or conforming to\nspecialized formatting styles. To control the generation, constrained decoding\nhas been widely adopted. However, existing prefix-tree-based constrained\ndecoding is inefficient under GPU-based model inference paradigms, and it\nintroduces unintended biases into the output distribution. This paper\nintroduces Dynamic Importance Sampling for Constrained Decoding (DISC) with\nGPU-based Parallel Prefix-Verification (PPV), a novel algorithm that leverages\ndynamic importance sampling to achieve theoretically guaranteed asymptotic\nunbiasedness and overcomes the inefficiency of prefix-tree. Extensive\nexperiments demonstrate the superiority of our method over existing methods in\nboth efficiency and output quality. These results highlight the potential of\nour methods to improve constrained generation in applications where adherence\nto specific constraints is essential."}
{"id": "2504.09156", "pdf": "https://arxiv.org/pdf/2504.09156", "abs": "https://arxiv.org/abs/2504.09156", "authors": ["Shengyu Gong", "Yueyang Li", "Zijian Kang", "Weiming Zeng", "Hongjie Yan", "Wai Ting Siok", "Nizhuan Wang"], "title": "LEREL: Lipschitz Continuity-Constrained Emotion Recognition Ensemble Learning For Electroencephalography", "categories": ["cs.CV"], "comment": null, "summary": "Accurate and efficient perception of emotional states in oneself and others\nis crucial, as emotion-related disorders are associated with severe\npsychosocial impairments. While electroencephalography (EEG) offers a powerful\ntool for emotion detection, current EEG-based emotion recognition (EER) methods\nface key limitations: insufficient model stability, limited accuracy in\nprocessing high-dimensional nonlinear EEG signals, and poor robustness against\nintra-subject variability and signal noise. To address these challenges, we\npropose LEREL (Lipschitz continuity-constrained Emotion Recognition Ensemble\nLearning), a novel framework that significantly enhances both the accuracy and\nrobustness of emotion recognition performance. The LEREL framework employs\nLipschitz continuity constraints to enhance model stability and generalization\nin EEG emotion recognition, reducing signal variability and noise\nsusceptibility while maintaining strong performance on small-sample datasets.\nThe ensemble learning strategy reduces single-model bias and variance through\nmulti-classifier decision fusion, further optimizing overall performance.\nExperimental results on three public benchmark datasets (EAV, FACED and SEED)\ndemonstrate LEREL's effectiveness, achieving average recognition accuracies of\n76.43%, 83.00% and 89.22%, respectively."}
{"id": "2504.09164", "pdf": "https://arxiv.org/pdf/2504.09164", "abs": "https://arxiv.org/abs/2504.09164", "authors": ["Michael Farrell"], "title": "Can postgraduate translation students identify machine-generated text?", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, accepted for MT Summit 2025, Geneva, Switzerland, 23-27\n  June 2025", "summary": "Given the growing use of generative artificial intelligence as a tool for\ncreating multilingual content and bypassing both machine and traditional\ntranslation methods, this study explores the ability of linguistically trained\nindividuals to discern machine-generated output from human-written text (HT).\nAfter brief training sessions on the textual anomalies typically found in\nsynthetic text (ST), twenty-three postgraduate translation students analysed\nexcerpts of Italian prose and assigned likelihood scores to indicate whether\nthey believed they were human-written or AI-generated (ChatGPT-4o). The results\nshow that, on average, the students struggled to distinguish between HT and ST,\nwith only two participants achieving notable accuracy. Closer analysis revealed\nthat the students often identified the same textual anomalies in both HT and\nST, although features such as low burstiness and self-contradiction were more\nfrequently associated with ST. These findings suggest the need for improvements\nin the preparatory training. Moreover, the study raises questions about the\nnecessity of editing synthetic text to make it sound more human-like and\nrecommends further research to determine whether AI-generated text is already\nsufficiently natural-sounding not to require further refinement."}
{"id": "2504.09160", "pdf": "https://arxiv.org/pdf/2504.09160", "abs": "https://arxiv.org/abs/2504.09160", "authors": ["Qingyuan Wang", "Rui Song", "Jiaojiao Li", "Kerui Cheng", "David Ferstl", "Yinlin Hu"], "title": "SCFlow2: Plug-and-Play Object Pose Refiner with Shape-Constraint Scene Flow", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "We introduce SCFlow2, a plug-and-play refinement framework for 6D object pose\nestimation. Most recent 6D object pose methods rely on refinement to get\naccurate results. However, most existing refinement methods either suffer from\nnoises in establishing correspondences, or rely on retraining for novel\nobjects. SCFlow2 is based on the SCFlow model designed for refinement with\nshape constraint, but formulates the additional depth as a regularization in\nthe iteration via 3D scene flow for RGBD frames. The key design of SCFlow2 is\nan introduction of geometry constraints into the training of recurrent matching\nnetwork, by combining the rigid-motion embeddings in 3D scene flow and 3D shape\nprior of the target. We train SCFlow2 on a combination of dataset Objaverse,\nGSO and ShapeNet, and evaluate on BOP datasets with novel objects. After using\nour method as a post-processing, most state-of-the-art methods produce\nsignificantly better results, without any retraining or fine-tuning. The source\ncode is available at https://scflow2.github.io."}
{"id": "2504.09170", "pdf": "https://arxiv.org/pdf/2504.09170", "abs": "https://arxiv.org/abs/2504.09170", "authors": ["Rabindra Lamsal", "Maria Rodriguez Read", "Shanika Karunasekera"], "title": "Langformers: Unified NLP Pipelines for Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Transformer-based language models have revolutionized the field of natural\nlanguage processing (NLP). However, using these models often involves\nnavigating multiple frameworks and tools, as well as writing repetitive\nboilerplate code. This complexity can discourage non-programmers and beginners,\nand even slow down prototyping for experienced developers. To address these\nchallenges, we introduce Langformers, an open-source Python library designed to\nstreamline NLP pipelines through a unified, factory-based interface for large\nlanguage model (LLM) and masked language model (MLM) tasks. Langformers\nintegrates conversational AI, MLM pretraining, text classification, sentence\nembedding/reranking, data labelling, semantic search, and knowledge\ndistillation into a cohesive API, supporting popular platforms such as Hugging\nFace and Ollama. Key innovations include: (1) task-specific factories that\nabstract training, inference, and deployment complexities; (2) built-in memory\nand streaming for conversational agents; and (3) lightweight, modular design\nthat prioritizes ease of use. Documentation: https://langformers.com"}
{"id": "2504.09195", "pdf": "https://arxiv.org/pdf/2504.09195", "abs": "https://arxiv.org/abs/2504.09195", "authors": ["Tzoulio Chamiti", "Leandro Di Bella", "Adrian Munteanu", "Nikos Deligiannis"], "title": "ReferGPT: Towards Zero-Shot Referring Multi-Object Tracking", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted CVPR 2025 Workshop on Distillation of Foundation Models for\n  Autonomous Driving", "summary": "Tracking multiple objects based on textual queries is a challenging task that\nrequires linking language understanding with object association across frames.\nPrevious works typically train the whole process end-to-end or integrate an\nadditional referring text module into a multi-object tracker, but they both\nrequire supervised training and potentially struggle with generalization to\nopen-set queries. In this work, we introduce ReferGPT, a novel zero-shot\nreferring multi-object tracking framework. We provide a multi-modal large\nlanguage model (MLLM) with spatial knowledge enabling it to generate 3D-aware\ncaptions. This enhances its descriptive capabilities and supports a more\nflexible referring vocabulary without training. We also propose a robust\nquery-matching strategy, leveraging CLIP-based semantic encoding and fuzzy\nmatching to associate MLLM generated captions with user queries. Extensive\nexperiments on Refer-KITTI, Refer-KITTIv2 and Refer-KITTI+ demonstrate that\nReferGPT achieves competitive performance against trained methods, showcasing\nits robustness and zero-shot capabilities in autonomous driving. The codes are\navailable on https://github.com/Tzoulio/ReferGPT"}
{"id": "2504.09184", "pdf": "https://arxiv.org/pdf/2504.09184", "abs": "https://arxiv.org/abs/2504.09184", "authors": ["Lennart Finke", "Thomas Dooms", "Mat Allen", "Juan Diego Rodriguez", "Noa Nabeshima", "Dan Braun"], "title": "Parameterized Synthetic Text Generation with SimpleStories", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present SimpleStories, a large synthetic story dataset in simple language,\nconsisting of 2 million stories each in English and Japanese. Our method\nemploys parametrization of prompts with features at multiple levels of\nabstraction, allowing for systematic control over story characteristics to\nensure broad syntactic and semantic diversity. Building on and addressing\nlimitations in the TinyStories dataset, our approach demonstrates that\nsimplicity and variety can be achieved simultaneously in synthetic text\ngeneration at scale."}
{"id": "2504.09196", "pdf": "https://arxiv.org/pdf/2504.09196", "abs": "https://arxiv.org/abs/2504.09196", "authors": ["Feng Lv", "Chunlong Xia", "Shuo Wang", "Huo Cao"], "title": "RT-DATR:Real-time Unsupervised Domain Adaptive Detection Transformer with Adversarial Feature Learning", "categories": ["cs.CV"], "comment": null, "summary": "Despite domain-adaptive object detectors based on CNN and transformers have\nmade significant progress in cross-domain detection tasks, it is regrettable\nthat domain adaptation for real-time transformer-based detectors has not yet\nbeen explored. Directly applying existing domain adaptation algorithms has\nproven to be suboptimal. In this paper, we propose RT-DATR, a simple and\nefficient real-time domain adaptive detection transformer. Building on RT-DETR\nas our base detector, we first introduce a local object-level feature alignment\nmodule to significantly enhance the feature representation of domain invariance\nduring object transfer. Additionally, we introduce a scene semantic feature\nalignment module designed to boost cross-domain detection performance by\naligning scene semantic features. Finally, we introduced a domain query and\ndecoupled it from the object query to further align the instance feature\ndistribution within the decoder layer, reduce the domain gap, and maintain\ndiscriminative ability. Experimental results on various benchmarks demonstrate\nthat our method outperforms current state-of-the-art approaches. Our code will\nbe released soon."}
{"id": "2504.09191", "pdf": "https://arxiv.org/pdf/2504.09191", "abs": "https://arxiv.org/abs/2504.09191", "authors": ["Weilong Dong", "Peiguang Li", "Yu Tian", "Xinyi Zeng", "Fengdi Li", "Sirui Wang"], "title": "Feature-Aware Malicious Output Detection and Mitigation", "categories": ["cs.CL"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has brought significant\nbenefits to various domains while introducing substantial risks. Despite being\nfine-tuned through reinforcement learning, LLMs lack the capability to discern\nmalicious content, limiting their defense against jailbreak. To address these\nsafety concerns, we propose a feature-aware method for harmful response\nrejection (FMM), which detects the presence of malicious features within the\nmodel's feature space and adaptively adjusts the model's rejection mechanism.\nBy employing a simple discriminator, we detect potential malicious traits\nduring the decoding phase. Upon detecting features indicative of toxic tokens,\nFMM regenerates the current token. By employing activation patching, an\nadditional rejection vector is incorporated during the subsequent token\ngeneration, steering the model towards a refusal response. Experimental results\ndemonstrate the effectiveness of our approach across multiple language models\nand diverse attack techniques, while crucially maintaining the models' standard\ngeneration capabilities."}
{"id": "2504.09202", "pdf": "https://arxiv.org/pdf/2504.09202", "abs": "https://arxiv.org/abs/2504.09202", "authors": ["Tung Luu", "Nam Le", "Duc Le", "Bac Le"], "title": "From Visual Explanations to Counterfactual Explanations with Latent Diffusion", "categories": ["cs.CV"], "comment": "2025 IEEE/CVF Winter Conference on Applications of Computer Vision\n  (WACV)", "summary": "Visual counterfactual explanations are ideal hypothetical images that change\nthe decision-making of the classifier with high confidence toward the desired\nclass while remaining visually plausible and close to the initial image. In\nthis paper, we propose a new approach to tackle two key challenges in recent\nprominent works: i) determining which specific counterfactual features are\ncrucial for distinguishing the \"concept\" of the target class from the original\nclass, and ii) supplying valuable explanations for the non-robust classifier\nwithout relying on the support of an adversarially robust model. Our method\nidentifies the essential region for modification through algorithms that\nprovide visual explanations, and then our framework generates realistic\ncounterfactual explanations by combining adversarial attacks based on pruning\nthe adversarial gradient of the target classifier and the latent diffusion\nmodel. The proposed method outperforms previous state-of-the-art results on\nvarious evaluation criteria on ImageNet and CelebA-HQ datasets. In general, our\nmethod can be applied to arbitrary classifiers, highlight the strong\nassociation between visual and counterfactual explanations, make semantically\nmeaningful changes from the target classifier, and provide observers with\nsubtle counterfactual images."}
{"id": "2504.09305", "pdf": "https://arxiv.org/pdf/2504.09305", "abs": "https://arxiv.org/abs/2504.09305", "authors": ["Owen Patterson", "Chee Ng"], "title": "Enhancing Contrastive Demonstration Selection with Semantic Diversity for Robust In-Context Machine Translation", "categories": ["cs.CL"], "comment": null, "summary": "In-Context Learning (ICL) empowers large language models to perform tasks by\nconditioning on a few input-output examples. However, the performance of ICL is\nhighly sensitive to the selection of these demonstrations. While existing\nmethods focus on similarity or contrastive selection, they often overlook the\nimportance of diversity among the chosen examples. In this paper, we propose\nDiverseConE (Diversity-Enhanced Contrastive Example Selection), a novel\napproach for demonstration selection in in-context learning for machine\ntranslation. Our method builds upon contrastive selection by incorporating a\ndiversity enhancement step based on embedding space dissimilarity. We conduct\nextensive experiments on the Llama2-7b model across four language pairs\n(English-Chinese, Chinese-English, Russian-German, German-Russian) in 1-shot\nand 3-shot settings, using COMET20 and COMET22 for evaluation. Our results\ndemonstrate that DiverseConE consistently outperforms strong baseline methods,\nincluding random selection, BM25, TopK, and a state-of-the-art contrastive\nselection method. Further analysis, including diversity metrics and human\nevaluation, validates the effectiveness of our approach and highlights the\nbenefits of considering demonstration diversity for improved translation\nquality."}
{"id": "2504.09203", "pdf": "https://arxiv.org/pdf/2504.09203", "abs": "https://arxiv.org/abs/2504.09203", "authors": ["Saikat Dutta", "Akhil Vasim", "Siddhant Gole", "Hamid Rezatofighi", "Biplab Banerjee"], "title": "AerOSeg: Harnessing SAM for Open-Vocabulary Segmentation in Remote Sensing Images", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at EarthVision workshop, CVPR 2025", "summary": "Image segmentation beyond predefined categories is a key challenge in remote\nsensing, where novel and unseen classes often emerge during inference.\nOpen-vocabulary image Segmentation addresses these generalization issues in\ntraditional supervised segmentation models while reducing reliance on extensive\nper-pixel annotations, which are both expensive and labor-intensive to obtain.\nMost Open-Vocabulary Segmentation (OVS) methods are designed for natural images\nbut struggle with remote sensing data due to scale variations, orientation\nchanges, and complex scene compositions. This necessitates the development of\nOVS approaches specifically tailored for remote sensing. In this context, we\npropose AerOSeg, a novel OVS approach for remote sensing data. First, we\ncompute robust image-text correlation features using multiple rotated versions\nof the input image and domain-specific prompts. These features are then refined\nthrough spatial and class refinement blocks. Inspired by the success of the\nSegment Anything Model (SAM) in diverse domains, we leverage SAM features to\nguide the spatial refinement of correlation features. Additionally, we\nintroduce a semantic back-projection module and loss to ensure the seamless\npropagation of SAM's semantic information throughout the segmentation pipeline.\nFinally, we enhance the refined correlation features using a multi-scale\nattention-aware decoder to produce the final segmentation map. We validate our\nSAM-guided Open-Vocabulary Remote Sensing Segmentation model on three benchmark\nremote sensing datasets: iSAID, DLRSD, and OpenEarthMap. Our model outperforms\nstate-of-the-art open-vocabulary segmentation methods, achieving an average\nimprovement of 2.54 h-mIoU."}
{"id": "2504.09309", "pdf": "https://arxiv.org/pdf/2504.09309", "abs": "https://arxiv.org/abs/2504.09309", "authors": ["Emily Johnson", "Xavier Holt", "Noah Wilson"], "title": "Improving the Accuracy and Efficiency of Legal Document Tagging with Large Language Models and Instruction Prompts", "categories": ["cs.CL"], "comment": null, "summary": "Legal multi-label classification is a critical task for organizing and\naccessing the vast amount of legal documentation. Despite its importance, it\nfaces challenges such as the complexity of legal language, intricate label\ndependencies, and significant label imbalance. In this paper, we propose\nLegal-LLM, a novel approach that leverages the instruction-following\ncapabilities of Large Language Models (LLMs) through fine-tuning. We reframe\nthe multi-label classification task as a structured generation problem,\ninstructing the LLM to directly output the relevant legal categories for a\ngiven document. We evaluate our method on two benchmark datasets, POSTURE50K\nand EURLEX57K, using micro-F1 and macro-F1 scores. Our experimental results\ndemonstrate that Legal-LLM outperforms a range of strong baseline models,\nincluding traditional methods and other Transformer-based approaches.\nFurthermore, ablation studies and human evaluations validate the effectiveness\nof our approach, particularly in handling label imbalance and generating\nrelevant and accurate legal labels."}
{"id": "2504.09215", "pdf": "https://arxiv.org/pdf/2504.09215", "abs": "https://arxiv.org/abs/2504.09215", "authors": ["Zhicheng Zhang", "Hao Tang", "Jinhui Tang"], "title": "Multi-scale Activation, Refinement, and Aggregation: Exploring Diverse Cues for Fine-Grained Bird Recognition", "categories": ["cs.CV", "cs.MM"], "comment": "Accepted by AAAI2025", "summary": "Given the critical role of birds in ecosystems, Fine-Grained Bird Recognition\n(FGBR) has gained increasing attention, particularly in distinguishing birds\nwithin similar subcategories. Although Vision Transformer (ViT)-based methods\noften outperform Convolutional Neural Network (CNN)-based methods in FGBR,\nrecent studies reveal that the limited receptive field of plain ViT model\nhinders representational richness and makes them vulnerable to scale variance.\nThus, enhancing the multi-scale capabilities of existing ViT-based models to\novercome this bottleneck in FGBR is a worthwhile pursuit. In this paper, we\npropose a novel framework for FGBR, namely Multi-scale Diverse Cues Modeling\n(MDCM), which explores diverse cues at different scales across various stages\nof a multi-scale Vision Transformer (MS-ViT) in an\n\"Activation-Selection-Aggregation\" paradigm. Specifically, we first propose a\nmulti-scale cue activation module to ensure the discriminative cues learned at\ndifferent stage are mutually different. Subsequently, a multi-scale token\nselection mechanism is proposed to remove redundant noise and highlight\ndiscriminative, scale-specific cues at each stage. Finally, the selected tokens\nfrom each stage are independently utilized for bird recognition, and the\nrecognition results from multiple stages are adaptively fused through a\nmulti-scale dynamic aggregation mechanism for final model decisions. Both\nqualitative and quantitative results demonstrate the effectiveness of our\nproposed MDCM, which outperforms CNN- and ViT-based models on several\nwidely-used FGBR benchmarks."}
{"id": "2504.09373", "pdf": "https://arxiv.org/pdf/2504.09373", "abs": "https://arxiv.org/abs/2504.09373", "authors": ["Ramya Namuduri", "Yating Wu", "Anshun Asher Zheng", "Manya Wadhwa", "Greg Durrett", "Junyi Jessy Li"], "title": "QUDsim: Quantifying Discourse Similarities in LLM-Generated Text", "categories": ["cs.CL"], "comment": null, "summary": "As large language models become increasingly capable at various writing\ntasks, their weakness at generating unique and creative content becomes a major\nliability. Although LLMs have the ability to generate text covering diverse\ntopics, there is an overall sense of repetitiveness across texts that we aim to\nformalize and quantify via a similarity metric. The familiarity between\ndocuments arises from the persistence of underlying discourse structures.\nHowever, existing similarity metrics dependent on lexical overlap and syntactic\npatterns largely capture $\\textit{content}$ overlap, thus making them\nunsuitable for detecting $\\textit{structural}$ similarities. We introduce an\nabstraction based on linguistic theories in Questions Under Discussion (QUD)\nand question semantics to help quantify differences in discourse progression.\nWe then use this framework to build $\\textbf{QUDsim}$, a similarity metric that\ncan detect discursive parallels between documents. Using QUDsim, we find that\nLLMs often reuse discourse structures (more so than humans) across samples,\neven when content differs. Furthermore, LLMs are not only repetitive and\nstructurally uniform, but are also divergent from human authors in the types of\nstructures they use."}
{"id": "2504.09223", "pdf": "https://arxiv.org/pdf/2504.09223", "abs": "https://arxiv.org/abs/2504.09223", "authors": ["Wenjin Ke", "Zhe Li", "Dong Li", "Lu Tian", "Emad Barsoum"], "title": "DL-QAT: Weight-Decomposed Low-Rank Quantization-Aware Training for Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Improving the efficiency of inference in Large Language Models (LLMs) is a\ncritical area of research. Post-training Quantization (PTQ) is a popular\ntechnique, but it often faces challenges at low-bit levels, particularly in\ndownstream tasks. Quantization-aware Training (QAT) can alleviate this problem,\nbut it requires significantly more computational resources. To tackle this, we\nintroduced Weight-Decomposed Low-Rank Quantization-Aware Training (DL-QAT),\nwhich merges the advantages of QAT while training only less than 1% of the\ntotal parameters. Specifically, we introduce a group-specific quantization\nmagnitude to adjust the overall scale of each quantization group. Within each\nquantization group, we use LoRA matrices to update the weight size and\ndirection in the quantization space. We validated the effectiveness of our\nmethod on the LLaMA and LLaMA2 model families. The results show significant\nimprovements over our baseline method across different quantization\ngranularities. For instance, for LLaMA-7B, our approach outperforms the\nprevious state-of-the-art method by 4.2% in MMLU on 3-bit LLaMA-7B model.\nAdditionally, our quantization results on pre-trained models also surpass\nprevious QAT methods, demonstrating the superior performance and efficiency of\nour approach."}
{"id": "2504.09378", "pdf": "https://arxiv.org/pdf/2504.09378", "abs": "https://arxiv.org/abs/2504.09378", "authors": ["Kartik Ravisankar", "Hyojung Han", "Marine Carpuat"], "title": "Can you map it to English? The Role of Cross-Lingual Alignment in Multilingual Performance of LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) pre-trained predominantly on English text\nexhibit surprising multilingual capabilities, yet the mechanisms driving\ncross-lingual generalization remain poorly understood. This work investigates\nhow the alignment of representations for text written in different languages\ncorrelates with LLM performance on natural language understanding tasks and\ntranslation tasks, both at the language and the instance level. For this\npurpose, we introduce cross-lingual alignment metrics such as the\nDiscriminative Alignment Index (DALI) to quantify the alignment at an instance\nlevel for discriminative tasks. Through experiments on three natural language\nunderstanding tasks (Belebele, XStoryCloze, XCOPA), and machine translation, we\nfind that while cross-lingual alignment metrics strongly correlate with task\naccuracy at the language level, the sample-level alignment often fails to\ndistinguish correct from incorrect predictions, exposing alignment as a\nnecessary but insufficient condition for success."}
{"id": "2504.09228", "pdf": "https://arxiv.org/pdf/2504.09228", "abs": "https://arxiv.org/abs/2504.09228", "authors": ["You Wu", "Xucheng Wang", "Xiangyang Yang", "Mengyuan Liu", "Dan Zeng", "Hengzhou Ye", "Shuiwang Li"], "title": "Learning Occlusion-Robust Vision Transformers for Real-Time UAV Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Single-stream architectures using Vision Transformer (ViT) backbones show\ngreat potential for real-time UAV tracking recently. However, frequent\nocclusions from obstacles like buildings and trees expose a major drawback:\nthese models often lack strategies to handle occlusions effectively. New\nmethods are needed to enhance the occlusion resilience of single-stream ViT\nmodels in aerial tracking. In this work, we propose to learn Occlusion-Robust\nRepresentations (ORR) based on ViTs for UAV tracking by enforcing an invariance\nof the feature representation of a target with respect to random masking\noperations modeled by a spatial Cox process. Hopefully, this random masking\napproximately simulates target occlusions, thereby enabling us to learn ViTs\nthat are robust to target occlusion for UAV tracking. This framework is termed\nORTrack. Additionally, to facilitate real-time applications, we propose an\nAdaptive Feature-Based Knowledge Distillation (AFKD) method to create a more\ncompact tracker, which adaptively mimics the behavior of the teacher model\nORTrack according to the task's difficulty. This student model, dubbed\nORTrack-D, retains much of ORTrack's performance while offering higher\nefficiency. Extensive experiments on multiple benchmarks validate the\neffectiveness of our method, demonstrating its state-of-the-art performance.\nCodes is available at https://github.com/wuyou3474/ORTrack."}
{"id": "2504.09387", "pdf": "https://arxiv.org/pdf/2504.09387", "abs": "https://arxiv.org/abs/2504.09387", "authors": ["Sriram Padmanabhan", "Kanishka Misra", "Kyle Mahowald", "Eunsol Choi"], "title": "On Language Models' Sensitivity to Suspicious Coincidences", "categories": ["cs.CL"], "comment": null, "summary": "Humans are sensitive to suspicious coincidences when generalizing inductively\nover data, as they make assumptions as to how the data was sampled. This\nresults in smaller, more specific hypotheses being favored over more general\nones. For instance, when provided the set {Austin, Dallas, Houston}, one is\nmore likely to think that this is sampled from \"Texas Cities\" over \"US Cities\"\neven though both are compatible. Suspicious coincidence is strongly connected\nto pragmatic reasoning, and can serve as a testbed to analyze systems on their\nsensitivity towards the communicative goals of the task (i.e., figuring out the\ntrue category underlying the data). In this paper, we analyze whether\nsuspicious coincidence effects are reflected in language models' (LMs)\nbehavior. We do so in the context of two domains: 1) the number game, where\nhumans made judgments of whether a number (e.g., 4) fits a list of given\nnumbers (e.g., 16, 32, 2); and 2) by extending the number game setup to\nprominent cities. For both domains, the data is compatible with multiple\nhypotheses and we study which hypothesis is most consistent with the models'\nbehavior. On analyzing five models, we do not find strong evidence for\nsuspicious coincidences in LMs' zero-shot behavior. However, when provided\naccess to the hypotheses space via chain-of-thought or explicit prompting, LMs\nstart to show an effect resembling suspicious coincidences, sometimes even\nshowing effects consistent with humans. Our study suggests that inductive\nreasoning behavior in LMs can be enhanced with explicit access to the\nhypothesis landscape."}
{"id": "2504.09249", "pdf": "https://arxiv.org/pdf/2504.09249", "abs": "https://arxiv.org/abs/2504.09249", "authors": ["Aniket Pal", "Sanket Biswas", "Alloy Das", "Ayush Lodh", "Priyanka Banerjee", "Soumitri Chattopadhyay", "Dimosthenis Karatzas", "Josep Llados", "C. V. Jawahar"], "title": "NoTeS-Bank: Benchmarking Neural Transcription and Search for Scientific Notes Understanding", "categories": ["cs.CV", "cs.IR", "cs.LG", "cs.MM"], "comment": null, "summary": "Understanding and reasoning over academic handwritten notes remains a\nchallenge in document AI, particularly for mathematical equations, diagrams,\nand scientific notations. Existing visual question answering (VQA) benchmarks\nfocus on printed or structured handwritten text, limiting generalization to\nreal-world note-taking. To address this, we introduce NoTeS-Bank, an evaluation\nbenchmark for Neural Transcription and Search in note-based question answering.\nNoTeS-Bank comprises complex notes across multiple domains, requiring models to\nprocess unstructured and multimodal content. The benchmark defines two tasks:\n(1) Evidence-Based VQA, where models retrieve localized answers with\nbounding-box evidence, and (2) Open-Domain VQA, where models classify the\ndomain before retrieving relevant documents and answers. Unlike classical\nDocument VQA datasets relying on optical character recognition (OCR) and\nstructured data, NoTeS-BANK demands vision-language fusion, retrieval, and\nmultimodal reasoning. We benchmark state-of-the-art Vision-Language Models\n(VLMs) and retrieval frameworks, exposing structured transcription and\nreasoning limitations. NoTeS-Bank provides a rigorous evaluation with NDCG@5,\nMRR, Recall@K, IoU, and ANLS, establishing a new standard for visual document\nunderstanding and reasoning."}
{"id": "2504.09389", "pdf": "https://arxiv.org/pdf/2504.09389", "abs": "https://arxiv.org/abs/2504.09389", "authors": ["Vishakh Padmakumar", "Chen Yueh-Han", "Jane Pan", "Valerie Chen", "He He"], "title": "Beyond Memorization: Mapping the Originality-Quality Frontier of Language Models", "categories": ["cs.CL"], "comment": null, "summary": "As large language models (LLMs) are increasingly used for ideation and\nscientific discovery, it is important to evaluate their ability to generate\nnovel output. Prior work evaluates novelty as the originality with respect to\ntraining data, but original outputs can be low quality. In contrast, non-expert\njudges may favor high-quality but memorized outputs, limiting the reliability\nof human preference as a metric. We propose a new novelty metric for LLM\ngenerations that balances originality and quality -- the harmonic mean of the\nfraction of \\ngrams unseen during training and a task-specific quality score.\nWe evaluate the novelty of generations from two families of open-data models\n(OLMo and Pythia) on three creative tasks: story completion, poetry writing,\nand creative tool use. We find that LLM generated text is less novel than human\nwritten text. To elicit more novel outputs, we experiment with various\ninference-time methods, which reveals a trade-off between originality and\nquality. While these methods can boost novelty, they do so by increasing\noriginality at the expense of quality. In contrast, increasing model size or\napplying post-training reliably shifts the Pareto frontier, highlighting that\nstarting with a stronger base model is a more effective way to improve novelty."}
{"id": "2504.09255", "pdf": "https://arxiv.org/pdf/2504.09255", "abs": "https://arxiv.org/abs/2504.09255", "authors": ["Sijing Wu", "Yunhao Li", "Ziwen Xu", "Yixuan Gao", "Huiyu Duan", "Wei Sun", "Guangtao Zhai"], "title": "FVQ: A Large-Scale Dataset and A LMM-based Method for Face Video Quality Assessment", "categories": ["cs.CV"], "comment": null, "summary": "Face video quality assessment (FVQA) deserves to be explored in addition to\ngeneral video quality assessment (VQA), as face videos are the primary content\non social media platforms and human visual system (HVS) is particularly\nsensitive to human faces. However, FVQA is rarely explored due to the lack of\nlarge-scale FVQA datasets. To fill this gap, we present the first large-scale\nin-the-wild FVQA dataset, FVQ-20K, which contains 20,000 in-the-wild face\nvideos together with corresponding mean opinion score (MOS) annotations. Along\nwith the FVQ-20K dataset, we further propose a specialized FVQA method named\nFVQ-Rater to achieve human-like rating and scoring for face video, which is the\nfirst attempt to explore the potential of large multimodal models (LMMs) for\nthe FVQA task. Concretely, we elaborately extract multi-dimensional features\nincluding spatial features, temporal features, and face-specific features\n(i.e., portrait features and face embeddings) to provide comprehensive visual\ninformation, and take advantage of the LoRA-based instruction tuning technique\nto achieve quality-specific fine-tuning, which shows superior performance on\nboth FVQ-20K and CFVQA datasets. Extensive experiments and comprehensive\nanalysis demonstrate the significant potential of the FVQ-20K dataset and\nFVQ-Rater method in promoting the development of FVQA."}
{"id": "2504.09394", "pdf": "https://arxiv.org/pdf/2504.09394", "abs": "https://arxiv.org/abs/2504.09394", "authors": ["Joseph Liu", "Yoonsoo Nam", "Xinyue Cui", "Swabha Swayamdipta"], "title": "Evaluation Under Imperfect Benchmarks and Ratings: A Case Study in Text Simplification", "categories": ["cs.CL"], "comment": "Submitted to COLM 2025. 9 pages, 6 figures", "summary": "Despite the successes of language models, their evaluation remains a daunting\nchallenge for new and existing tasks. We consider the task of text\nsimplification, commonly used to improve information accessibility, where\nevaluation faces two major challenges. First, the data in existing benchmarks\nmight not reflect the capabilities of current language models on the task,\noften containing disfluent, incoherent, or simplistic examples. Second,\nexisting human ratings associated with the benchmarks often contain a high\ndegree of disagreement, resulting in inconsistent ratings; nevertheless,\nexisting metrics still have to show higher correlations with these imperfect\nratings. As a result, evaluation for the task is not reliable and does not\nreflect expected trends (e.g., more powerful models being assigned higher\nscores). We address these challenges for the task of text simplification\nthrough three contributions. First, we introduce SynthSimpliEval, a synthetic\nbenchmark for text simplification featuring simplified sentences generated by\nmodels of varying sizes. Through a pilot study, we show that human ratings on\nour benchmark exhibit high inter-annotator agreement and reflect the expected\ntrend: larger models produce higher-quality simplifications. Second, we show\nthat auto-evaluation with a panel of LLM judges (LLMs-as-a-jury) often suffices\nto obtain consistent ratings for the evaluation of text simplification. Third,\nwe demonstrate that existing learnable metrics for text simplification benefit\nfrom training on our LLMs-as-a-jury-rated synthetic data, closing the gap with\npure LLMs-as-a-jury for evaluation. Overall, through our case study on text\nsimplification, we show that a reliable evaluation requires higher quality test\ndata, which could be obtained through synthetic data and LLMs-as-a-jury\nratings."}
{"id": "2504.09258", "pdf": "https://arxiv.org/pdf/2504.09258", "abs": "https://arxiv.org/abs/2504.09258", "authors": ["Jianyu Wu", "Hao Yang", "Xinhua Zeng", "Guibing He", "Zhiyu Chen", "Zihui Li", "Xiaochuan Zhang", "Yangyang Ma", "Run Fang", "Yang Liu"], "title": "PathVLM-R1: A Reinforcement Learning-Driven Reasoning Model for Pathology Visual-Language Tasks", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "The diagnosis of pathological images is often limited by expert availability\nand regional disparities, highlighting the importance of automated diagnosis\nusing Vision-Language Models (VLMs). Traditional multimodal models typically\nemphasize outcomes over the reasoning process, compromising the reliability of\nclinical decisions. To address the weak reasoning abilities and lack of\nsupervised processes in pathological VLMs, we have innovatively proposed\nPathVLM-R1, a visual language model designed specifically for pathological\nimages. We have based our model on Qwen2.5-VL-7B-Instruct and enhanced its\nperformance for pathological tasks through meticulously designed post-training\nstrategies. Firstly, we conduct supervised fine-tuning guided by pathological\ndata to imbue the model with foundational pathological knowledge, forming a new\npathological base model. Subsequently, we introduce Group Relative Policy\nOptimization (GRPO) and propose a dual reward-driven reinforcement learning\noptimization, ensuring strict constraint on logical supervision of the\nreasoning process and accuracy of results via cross-modal process reward and\noutcome accuracy reward. In the pathological image question-answering tasks,\nthe testing results of PathVLM-R1 demonstrate a 14% improvement in accuracy\ncompared to baseline methods, and it demonstrated superior performance compared\nto the Qwen2.5-VL-32B version despite having a significantly smaller parameter\nsize. Furthermore, in out-domain data evaluation involving four medical imaging\nmodalities: Computed Tomography (CT), dermoscopy, fundus photography, and\nOptical Coherence Tomography (OCT) images: PathVLM-R1's transfer performance\nimproved by an average of 17.3% compared to traditional SFT methods. These\nresults clearly indicate that PathVLM-R1 not only enhances accuracy but also\npossesses broad applicability and expansion potential."}
{"id": "2504.09398", "pdf": "https://arxiv.org/pdf/2504.09398", "abs": "https://arxiv.org/abs/2504.09398", "authors": ["Gaurav Kumar", "Murali Mohana Krishna Dandu"], "title": "Composable NLP Workflows for BERT-based Ranking and QA System", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 3 figures, 6 tables", "summary": "There has been a lot of progress towards building NLP models that scale to\nmultiple tasks. However, real-world systems contain multiple components and it\nis tedious to handle cross-task interaction with varying levels of text\ngranularity. In this work, we built an end-to-end Ranking and\nQuestion-Answering (QA) system using Forte, a toolkit that makes composable NLP\npipelines. We utilized state-of-the-art deep learning models such as BERT,\nRoBERTa in our pipeline, evaluated the performance on MS-MARCO and Covid-19\ndatasets using metrics such as BLUE, MRR, F1 and compared the results of\nranking and QA systems with their corresponding benchmark results. The modular\nnature of our pipeline and low latency of reranker makes it easy to build\ncomplex NLP applications easily."}
{"id": "2504.09261", "pdf": "https://arxiv.org/pdf/2504.09261", "abs": "https://arxiv.org/abs/2504.09261", "authors": ["Ziran Qin", "Youru Lv", "Mingbao Lin", "Zeren Zhang", "Danping Zou", "Weiyao Lin"], "title": "Head-Aware KV Cache Compression for Efficient Visual Autoregressive Modeling", "categories": ["cs.CV"], "comment": null, "summary": "Visual Autoregressive (VAR) models have emerged as a powerful approach for\nmulti-modal content creation, offering high efficiency and quality across\ndiverse multimedia applications. However, they face significant memory\nbottlenecks due to extensive KV cache accumulation during inference. Existing\nKV cache compression techniques for large language models are suboptimal for\nVAR models due to, as we identify in this paper, two distinct categories of\nattention heads in VAR models: Structural Heads, which preserve spatial\ncoherence through diagonal attention patterns, and Contextual Heads, which\nmaintain semantic consistency through vertical attention patterns. These\ndifferences render single-strategy KV compression techniques ineffective for\nVAR models. To address this, we propose HACK, a training-free Head-Aware\nCompression method for KV cache. HACK allocates asymmetric cache budgets and\nemploys pattern-specific compression strategies tailored to the essential\ncharacteristics of each head category. Experiments on Infinity-2B, Infinity-8B,\nand VAR-d30 demonstrate its effectiveness in text-to-image and\nclass-conditional generation tasks. HACK can hack down up to 50\\% and 70\\% of\ncache with minimal performance degradation for VAR-d30 and Infinity-8B,\nrespectively. Even with 70\\% and 90\\% KV cache compression in VAR-d30 and\nInfinity-8B, HACK still maintains high-quality generation while reducing memory\nusage by 44.2\\% and 58.9\\%, respectively."}
{"id": "2504.09402", "pdf": "https://arxiv.org/pdf/2504.09402", "abs": "https://arxiv.org/abs/2504.09402", "authors": ["Feijiang Han", "Licheng Guo", "Hengtao Cui", "Zhiyuan Lyu"], "title": "Question Tokens Deserve More Attention: Enhancing Large Language Models without Training through Step-by-Step Reading and Question Attention Recalibration", "categories": ["cs.CL", "cs.AI"], "comment": "CIS 5300", "summary": "Large Language Models (LLMs) often struggle with tasks that require a deep\nunderstanding of complex questions, especially when faced with long-range\ndependencies or multi-step reasoning. This work investigates the limitations of\ncurrent LLMs in question comprehension and identifies three insights: (1)\nrepeating question tokens improves comprehension by increasing attention to\nquestion regions, (2) increased backward dependencies negatively affect\nperformance due to unidirectional attentional constraints, and (3)\nrecalibrating attentional mechanisms to prioritize question-relevant regions\nimproves performance.\n  Based on these findings, we first propose a family of prompt-based strategies\n- Step-by-Step Reading (SSR), SSR+, and SSR++ - that guide LLMs to\nincrementally process question tokens and align their reasoning with the input\nstructure. These methods significantly improve performance, with SSR++\nachieving state-of-the-art results on several benchmarks: 96.66% on GSM8K,\n94.61% on ASDiv, and 76.28% on AQuA. Second, we introduce a training-free\nattention recalibration mechanism that dynamically adjusts attention\ndistributions during inference to emphasize question-relevant regions. This\nmethod improves the accuracy of LLaMA 3.1-8B on AQuA by 5.17% without changing\nmodel parameters or input prompts.\n  Taken together, our results highlight the importance of structured prompt\ndesign and attention optimization in improving LLM comprehension, providing\nlightweight yet effective tools for improving performance in various NLP tasks."}
{"id": "2504.09282", "pdf": "https://arxiv.org/pdf/2504.09282", "abs": "https://arxiv.org/abs/2504.09282", "authors": ["Zheyuan Zhang", "Monica Dou", "Linkai Peng", "Hongyi Pan", "Ulas Bagci", "Boqing Gong"], "title": "VideoAds for Fast-Paced Video Understanding: Where Opensource Foundation Models Beat GPT-4o & Gemini-1.5 Pro", "categories": ["cs.CV"], "comment": null, "summary": "Advertisement videos serve as a rich and valuable source of purpose-driven\ninformation, encompassing high-quality visual, textual, and contextual cues\ndesigned to engage viewers. They are often more complex than general videos of\nsimilar duration due to their structured narratives and rapid scene\ntransitions, posing significant challenges to multi-modal large language models\n(MLLMs). In this work, we introduce VideoAds, the first dataset tailored for\nbenchmarking the performance of MLLMs on advertisement videos. VideoAds\ncomprises well-curated advertisement videos with complex temporal structures,\naccompanied by \\textbf{manually} annotated diverse questions across three core\ntasks: visual finding, video summary, and visual reasoning. We propose a\nquantitative measure to compare VideoAds against existing benchmarks in terms\nof video complexity. Through extensive experiments, we find that\nQwen2.5-VL-72B, an opensource MLLM, achieves 73.35\\% accuracy on VideoAds,\noutperforming GPT-4o (66.82\\%) and Gemini-1.5 Pro (69.66\\%); the two\nproprietary models especially fall behind the opensource model in video\nsummarization and reasoning, but perform the best in visual finding. Notably,\nhuman experts easily achieve a remarkable accuracy of 94.27\\%. These results\nunderscore the necessity of advancing MLLMs' temporal modeling capabilities and\nhighlight VideoAds as a potentially pivotal benchmark for future research in\nunderstanding video that requires high FPS sampling. The dataset and evaluation\ncode will be publicly available at https://videoadsbenchmark.netlify.app."}
{"id": "2504.09407", "pdf": "https://arxiv.org/pdf/2504.09407", "abs": "https://arxiv.org/abs/2504.09407", "authors": ["Yuxuan Lu", "Bingsheng Yao", "Hansu Gu", "Jing Huang", "Jessie Wang", "Yang Li", "Jiri Gesi", "Qi He", "Toby Jia-Jun Li", "Dakuo Wang"], "title": "UXAgent: A System for Simulating Usability Testing of Web Design with LLM Agents", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Usability testing is a fundamental research method that user experience (UX)\nresearchers use to evaluate and iterate a web design, but\\textbf{ how to\nevaluate and iterate the usability testing study design } itself? Recent\nadvances in Large Language Model-simulated Agent (\\textbf{LLM Agent}) research\ninspired us to design \\textbf{UXAgent} to support UX researchers in evaluating\nand reiterating their usability testing study design before they conduct the\nreal human-subject study. Our system features a Persona Generator module, an\nLLM Agent module, and a Universal Browser Connector module to automatically\ngenerate thousands of simulated users to interactively test the target website.\nThe system also provides an Agent Interview Interface and a Video Replay\nInterface so that the UX researchers can easily review and analyze the\ngenerated qualitative and quantitative log data. Through a heuristic\nevaluation, five UX researcher participants praised the innovation of our\nsystem but also expressed concerns about the future of LLM Agent usage in UX\nstudies."}
{"id": "2504.09291", "pdf": "https://arxiv.org/pdf/2504.09291", "abs": "https://arxiv.org/abs/2504.09291", "authors": ["Jiaying Qian", "Ziheng Jia", "Zicheng Zhang", "Zeyu Zhang", "Guangtao Zhai", "Xiongkuo Min"], "title": "Towards Explainable Partial-AIGC Image Quality Assessment", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "The rapid advancement of AI-driven visual generation technologies has\ncatalyzed significant breakthroughs in image manipulation, particularly in\nachieving photorealistic localized editing effects on natural scene images\n(NSIs). Despite extensive research on image quality assessment (IQA) for\nAI-generated images (AGIs), most studies focus on fully AI-generated outputs\n(e.g., text-to-image generation), leaving the quality assessment of\npartial-AIGC images (PAIs)-images with localized AI-driven edits an almost\nunprecedented field. Motivated by this gap, we construct the first large-scale\nPAI dataset towards explainable partial-AIGC image quality assessment (EPAIQA),\nthe EPAIQA-15K, which includes 15K images with localized AI manipulation in\ndifferent regions and over 300K multi-dimensional human ratings. Based on this,\nwe leverage large multi-modal models (LMMs) and propose a three-stage model\ntraining paradigm. This paradigm progressively trains the LMM for editing\nregion grounding, quantitative quality scoring, and quality explanation.\nFinally, we develop the EPAIQA series models, which possess explainable quality\nfeedback capabilities. Our work represents a pioneering effort in the\nperceptual IQA field for comprehensive PAI quality assessment."}
{"id": "2504.09420", "pdf": "https://arxiv.org/pdf/2504.09420", "abs": "https://arxiv.org/abs/2504.09420", "authors": ["Yutao Mou", "Yuxiao Luo", "Shikun Zhang", "Wei Ye"], "title": "SaRO: Enhancing LLM Safety through Reasoning-based Alignment", "categories": ["cs.CL"], "comment": null, "summary": "Current safety alignment techniques for large language models (LLMs) face two\nkey challenges: (1) under-generalization, which leaves models vulnerable to\nnovel jailbreak attacks, and (2) over-alignment, which leads to the excessive\nrefusal of benign instructions. Our preliminary investigation reveals semantic\noverlap between jailbreak/harmful queries and normal prompts in embedding\nspace, suggesting that more effective safety alignment requires a deeper\nsemantic understanding. This motivates us to incorporate safety-policy-driven\nreasoning into the alignment process. To this end, we propose the\nSafety-oriented Reasoning Optimization Framework (SaRO), which consists of two\nstages: (1) Reasoning-style Warmup (RW) that enables LLMs to internalize\nlong-chain reasoning through supervised fine-tuning, and (2) Safety-oriented\nReasoning Process Optimization (SRPO) that promotes safety reflection via\ndirect preference optimization (DPO). Extensive experiments demonstrate the\nsuperiority of SaRO over traditional alignment methods."}
{"id": "2504.09297", "pdf": "https://arxiv.org/pdf/2504.09297", "abs": "https://arxiv.org/abs/2504.09297", "authors": ["Huu-Phong Phan-Nguyen", "Anh Dao", "Tien-Huy Nguyen", "Tuan Quang", "Huu-Loc Tran", "Tinh-Anh Nguyen-Nhu", "Huy-Thach Pham", "Quan Nguyen", "Hoang M. Le", "Quang-Vinh Dinh"], "title": "Cycle Training with Semi-Supervised Domain Adaptation: Bridging Accuracy and Efficiency for Real-Time Mobile Scene Detection", "categories": ["cs.CV"], "comment": null, "summary": "Nowadays, smartphones are ubiquitous, and almost everyone owns one. At the\nsame time, the rapid development of AI has spurred extensive research on\napplying deep learning techniques to image classification. However, due to the\nlimited resources available on mobile devices, significant challenges remain in\nbalancing accuracy with computational efficiency. In this paper, we propose a\nnovel training framework called Cycle Training, which adopts a three-stage\ntraining process that alternates between exploration and stabilization phases\nto optimize model performance. Additionally, we incorporate Semi-Supervised\nDomain Adaptation (SSDA) to leverage the power of large models and unlabeled\ndata, thereby effectively expanding the training dataset. Comprehensive\nexperiments on the CamSSD dataset for mobile scene detection demonstrate that\nour framework not only significantly improves classification accuracy but also\nensures real-time inference efficiency. Specifically, our method achieves a\n94.00% in Top-1 accuracy and a 99.17% in Top-3 accuracy and runs inference in\njust 1.61ms using CPU, demonstrating its suitability for real-world mobile\ndeployment."}
{"id": "2504.09421", "pdf": "https://arxiv.org/pdf/2504.09421", "abs": "https://arxiv.org/abs/2504.09421", "authors": ["Wuyang Lan", "Wenzheng Wang", "Changwei Ji", "Guoxing Yang", "Yongbo Zhang", "Xiaohong Liu", "Song Wu", "Guangyu Wang"], "title": "ClinicalGPT-R1: Pushing reasoning capability of generalist disease diagnosis with large language model", "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, 6 figures", "summary": "Recent advances in reasoning with large language models (LLMs)has shown\nremarkable reasoning capabilities in domains such as mathematics and coding,\nyet their application to clinical diagnosis remains underexplored. Here, we\nintroduce ClinicalGPT-R1, a reasoning enhanced generalist large language model\nfor disease diagnosis. Trained on a dataset of 20,000 real-world clinical\nrecords, ClinicalGPT-R1 leverages diverse training strategies to enhance\ndiagnostic reasoning. To benchmark performance, we curated MedBench-Hard, a\nchallenging dataset spanning seven major medical specialties and representative\ndiseases. Experimental results demonstrate that ClinicalGPT-R1 outperforms\nGPT-4o in Chinese diagnostic tasks and achieves comparable performance to GPT-4\nin English settings. This comparative study effectively validates the superior\nperformance of ClinicalGPT-R1 in disease diagnosis tasks. Resources are\navailable at https://github.com/medfound/medfound."}
{"id": "2504.09298", "pdf": "https://arxiv.org/pdf/2504.09298", "abs": "https://arxiv.org/abs/2504.09298", "authors": ["Tinh-Anh Nguyen-Nhu", "Huu-Loc Tran", "Nguyen-Khang Le", "Minh-Nhat Nguyen", "Tien-Huy Nguyen", "Hoang-Long Nguyen-Huu", "Huu-Phong Phan-Nguyen", "Huy-Thach Pham", "Quan Nguyen", "Hoang M. Le", "Quang-Vinh Dinh"], "title": "A Lightweight Moment Retrieval System with Global Re-Ranking and Robust Adaptive Bidirectional Temporal Search", "categories": ["cs.CV"], "comment": null, "summary": "The exponential growth of digital video content has posed critical challenges\nin moment-level video retrieval, where existing methodologies struggle to\nefficiently localize specific segments within an expansive video corpus.\nCurrent retrieval systems are constrained by computational inefficiencies,\ntemporal context limitations, and the intrinsic complexity of navigating video\ncontent. In this paper, we address these limitations through a novel\nInteractive Video Corpus Moment Retrieval framework that integrates a\nSuperGlobal Reranking mechanism and Adaptive Bidirectional Temporal Search\n(ABTS), strategically optimizing query similarity, temporal stability, and\ncomputational resources. By preprocessing a large corpus of videos using a\nkeyframe extraction model and deduplication technique through image hashing,\nour approach provides a scalable solution that significantly reduces storage\nrequirements while maintaining high localization precision across diverse video\nrepositories."}
{"id": "2504.09482", "pdf": "https://arxiv.org/pdf/2504.09482", "abs": "https://arxiv.org/abs/2504.09482", "authors": ["Sharanya Dasgupta", "Sujoy Nath", "Arkaprabha Basu", "Pourya Shamsolmoali", "Swagatam Das"], "title": "HalluShift: Measuring Distribution Shifts towards Hallucination Detection in LLMs", "categories": ["cs.CL", "cs.AI", "cs.ET"], "comment": null, "summary": "Large Language Models (LLMs) have recently garnered widespread attention due\nto their adeptness at generating innovative responses to the given prompts\nacross a multitude of domains. However, LLMs often suffer from the inherent\nlimitation of hallucinations and generate incorrect information while\nmaintaining well-structured and coherent responses. In this work, we\nhypothesize that hallucinations stem from the internal dynamics of LLMs. Our\nobservations indicate that, during passage generation, LLMs tend to deviate\nfrom factual accuracy in subtle parts of responses, eventually shifting toward\nmisinformation. This phenomenon bears a resemblance to human cognition, where\nindividuals may hallucinate while maintaining logical coherence, embedding\nuncertainty within minor segments of their speech. To investigate this further,\nwe introduce an innovative approach, HalluShift, designed to analyze the\ndistribution shifts in the internal state space and token probabilities of the\nLLM-generated responses. Our method attains superior performance compared to\nexisting baselines across various benchmark datasets. Our codebase is available\nat https://github.com/sharanya-dasgupta001/hallushift."}
{"id": "2504.09322", "pdf": "https://arxiv.org/pdf/2504.09322", "abs": "https://arxiv.org/abs/2504.09322", "authors": ["Tyler Spears", "Shen Zhu", "Yinzhu Jin", "Aman Shrivastava", "P. Thomas Fletcher"], "title": "MedIL: Implicit Latent Spaces for Generating Heterogeneous Medical Images at Arbitrary Resolutions", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "In this work, we introduce MedIL, a first-of-its-kind autoencoder built for\nencoding medical images with heterogeneous sizes and resolutions for image\ngeneration. Medical images are often large and heterogeneous, where fine\ndetails are of vital clinical importance. Image properties change drastically\nwhen considering acquisition equipment, patient demographics, and pathology,\nmaking realistic medical image generation challenging. Recent work in latent\ndiffusion models (LDMs) has shown success in generating images resampled to a\nfixed-size. However, this is a narrow subset of the resolutions native to image\nacquisition, and resampling discards fine anatomical details. MedIL utilizes\nimplicit neural representations to treat images as continuous signals, where\nencoding and decoding can be performed at arbitrary resolutions without prior\nresampling. We quantitatively and qualitatively show how MedIL compresses and\npreserves clinically-relevant features over large multi-site, multi-resolution\ndatasets of both T1w brain MRIs and lung CTs. We further demonstrate how MedIL\ncan influence the quality of images generated with a diffusion model, and\ndiscuss how MedIL can enhance generative models to resemble raw clinical\nacquisitions."}
{"id": "2504.09488", "pdf": "https://arxiv.org/pdf/2504.09488", "abs": "https://arxiv.org/abs/2504.09488", "authors": ["Jiashu Yang", "Ningning Wang", "Yian Zhao", "Chaoran Feng", "Junjia Du", "Hao Pang", "Zhirui Fang", "Xuxin Cheng"], "title": "Kongzi: A Historical Large Language Model with Fact Enhancement", "categories": ["cs.CL"], "comment": "22 pages, 12 figures", "summary": "The capabilities of the latest large language models (LLMs) have been\nextended from pure natural language understanding to complex reasoning tasks.\nHowever, current reasoning models often exhibit factual inaccuracies in longer\nreasoning chains, which poses challenges for historical reasoning and limits\nthe potential of LLMs in complex, knowledge-intensive tasks. Historical studies\nrequire not only the accurate presentation of factual information but also the\nability to establish cross-temporal correlations and derive coherent\nconclusions from fragmentary and often ambiguous sources. To address these\nchallenges, we propose Kongzi, a large language model specifically designed for\nhistorical analysis. Through the integration of curated, high-quality\nhistorical data and a novel fact-reinforcement learning strategy, Kongzi\ndemonstrates strong factual alignment and sophisticated reasoning depth.\nExtensive experiments on tasks such as historical question answering and\nnarrative generation demonstrate that Kongzi outperforms existing models in\nboth factual accuracy and reasoning depth. By effectively addressing the unique\nchallenges inherent in historical texts, Kongzi sets a new standard for the\ndevelopment of accurate and reliable LLMs in professional domains."}
{"id": "2504.09326", "pdf": "https://arxiv.org/pdf/2504.09326", "abs": "https://arxiv.org/abs/2504.09326", "authors": ["Huai-Qian Khor", "Yante Li", "Xingxun Jiang", "Guoying Zhao"], "title": "Infused Suppression Of Magnification Artefacts For Micro-AU Detection", "categories": ["cs.CV"], "comment": null, "summary": "Facial micro-expressions are spontaneous, brief and subtle facial motions\nthat unveil the underlying, suppressed emotions. Detecting Action Units (AUs)\nin micro-expressions is crucial because it yields a finer representation of\nfacial motions than categorical emotions, effectively resolving the ambiguity\namong different expressions. One of the difficulties in micro-expression\nanalysis is that facial motions are subtle and brief, thereby increasing the\ndifficulty in correlating facial motion features to AU occurrence. To bridge\nthe subtlety issue, flow-related features and motion magnification are a few\ncommon approaches as they can yield descriptive motion changes and increased\nmotion amplitude respectively. While motion magnification can amplify the\nmotion changes, it also accounts for illumination changes and projection errors\nduring the amplification process, thereby creating motion artefacts that\nconfuse the model to learn inauthentic magnified motion features. The problem\nis further aggravated in the context of a more complicated task where more AU\nclasses are analyzed in cross-database settings. To address this issue, we\npropose InfuseNet, a layer-wise unitary feature infusion framework that\nleverages motion context to constrain the Action Unit (AU) learning within an\ninformative facial movement region, thereby alleviating the influence of\nmagnification artefacts. On top of that, we propose leveraging magnified latent\nfeatures instead of reconstructing magnified samples to limit the distortion\nand artefacts caused by the projection inaccuracy in the motion reconstruction\nprocess. Via alleviating the magnification artefacts, InfuseNet has surpassed\nthe state-of-the-art results in the CD6ME protocol. Further quantitative\nstudies have also demonstrated the efficacy of motion artefacts alleviation."}
{"id": "2504.09504", "pdf": "https://arxiv.org/pdf/2504.09504", "abs": "https://arxiv.org/abs/2504.09504", "authors": ["Wei Tao", "Xiaoyang Qu", "Kai Lu", "Jiguang Wan", "Guokuan Li", "Jianzong Wang"], "title": "MADLLM: Multivariate Anomaly Detection via Pre-trained LLMs", "categories": ["cs.CL"], "comment": "Accepted by IEEE International Conference on Multimedia & Expo 2025\n  (ICME 2025)", "summary": "When applying pre-trained large language models (LLMs) to address anomaly\ndetection tasks, the multivariate time series (MTS) modality of anomaly\ndetection does not align with the text modality of LLMs. Existing methods\nsimply transform the MTS data into multiple univariate time series sequences,\nwhich can cause many problems. This paper introduces MADLLM, a novel\nmultivariate anomaly detection method via pre-trained LLMs. We design a new\ntriple encoding technique to align the MTS modality with the text modality of\nLLMs. Specifically, this technique integrates the traditional patch embedding\nmethod with two novel embedding approaches: Skip Embedding, which alters the\norder of patch processing in traditional methods to help LLMs retain knowledge\nof previous features, and Feature Embedding, which leverages contrastive\nlearning to allow the model to better understand the correlations between\ndifferent features. Experimental results demonstrate that our method\noutperforms state-of-the-art methods in various public anomaly detection\ndatasets."}
{"id": "2504.09328", "pdf": "https://arxiv.org/pdf/2504.09328", "abs": "https://arxiv.org/abs/2504.09328", "authors": ["Sonia Laguna", "Alberto Garcia-Garcia", "Marie-Julie Rakotosaona", "Stylianos Moschoglou", "Leonhard Helminger", "Sergio Orts-Escolano"], "title": "Text To 3D Object Generation For Scalable Room Assembly", "categories": ["cs.CV", "cs.LG"], "comment": "Published at the ICLR 2025 Workshop on Synthetic Data", "summary": "Modern machine learning models for scene understanding, such as depth\nestimation and object tracking, rely on large, high-quality datasets that mimic\nreal-world deployment scenarios. To address data scarcity, we propose an\nend-to-end system for synthetic data generation for scalable, high-quality, and\ncustomizable 3D indoor scenes. By integrating and adapting text-to-image and\nmulti-view diffusion models with Neural Radiance Field-based meshing, this\nsystem generates highfidelity 3D object assets from text prompts and\nincorporates them into pre-defined floor plans using a rendering tool. By\nintroducing novel loss functions and training strategies into existing methods,\nthe system supports on-demand scene generation, aiming to alleviate the\nscarcity of current available data, generally manually crafted by artists. This\nsystem advances the role of synthetic data in addressing machine learning\ntraining limitations, enabling more robust and generalizable models for\nreal-world applications."}
{"id": "2504.09522", "pdf": "https://arxiv.org/pdf/2504.09522", "abs": "https://arxiv.org/abs/2504.09522", "authors": ["Chen Sun", "Renat Aksitov", "Andrey Zhmoginov", "Nolan Andrew Miller", "Max Vladymyrov", "Ulrich Rueckert", "Been Kim", "Mark Sandler"], "title": "How new data permeates LLM knowledge and how to dilute it", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models learn and continually learn through the accumulation of\ngradient-based updates, but how individual pieces of new information affect\nexisting knowledge, leading to both beneficial generalization and problematic\nhallucination, remains poorly understood. We demonstrate that when learning new\ninformation, LLMs exhibit a \"priming\" effect: learning a new fact can cause the\nmodel to inappropriately apply that knowledge in unrelated contexts. To\nsystematically study this phenomenon, we introduce \"Outlandish,\" a carefully\ncurated dataset of 1320 diverse text samples designed to probe how new\nknowledge permeates through an LLM's existing knowledge base. Using this\ndataset, we show that the degree of priming after learning new information can\nbe predicted by measuring the token probability of key words before learning.\nThis relationship holds robustly across different model architectures (PALM-2,\nGemma, Llama), sizes, and training stages. Finally, we develop two novel\ntechniques to modulate how new knowledge affects existing model behavior: (1) a\n``stepping-stone'' text augmentation strategy and (2) an ``ignore-k'' update\npruning method. These approaches reduce undesirable priming effects by 50-95\\%\nwhile preserving the model's ability to learn new information. Our findings\nprovide both empirical insights into how LLMs learn and practical tools for\nimproving the specificity of knowledge insertion in language models. Further\nmaterials: https://sunchipsster1.github.io/projects/outlandish/"}
{"id": "2504.09354", "pdf": "https://arxiv.org/pdf/2504.09354", "abs": "https://arxiv.org/abs/2504.09354", "authors": ["Duy-Cat Can", "Quang-Huy Tang", "Huong Ha", "Binh T. Nguyen", "Oliver Y. Chén"], "title": "REMEMBER: Retrieval-based Explainable Multimodal Evidence-guided Modeling for Brain Evaluation and Reasoning in Zero- and Few-shot Neurodegenerative Diagnosis", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "q-bio.QM"], "comment": null, "summary": "Timely and accurate diagnosis of neurodegenerative disorders, such as\nAlzheimer's disease, is central to disease management. Existing deep learning\nmodels require large-scale annotated datasets and often function as \"black\nboxes\". Additionally, datasets in clinical practice are frequently small or\nunlabeled, restricting the full potential of deep learning methods. Here, we\nintroduce REMEMBER -- Retrieval-based Explainable Multimodal Evidence-guided\nModeling for Brain Evaluation and Reasoning -- a new machine learning framework\nthat facilitates zero- and few-shot Alzheimer's diagnosis using brain MRI scans\nthrough a reference-based reasoning process. Specifically, REMEMBER first\ntrains a contrastively aligned vision-text model using expert-annotated\nreference data and extends pseudo-text modalities that encode abnormality\ntypes, diagnosis labels, and composite clinical descriptions. Then, at\ninference time, REMEMBER retrieves similar, human-validated cases from a\ncurated dataset and integrates their contextual information through a dedicated\nevidence encoding module and attention-based inference head. Such an\nevidence-guided design enables REMEMBER to imitate real-world clinical\ndecision-making process by grounding predictions in retrieved imaging and\ntextual context. Specifically, REMEMBER outputs diagnostic predictions\nalongside an interpretable report, including reference images and explanations\naligned with clinical workflows. Experimental results demonstrate that REMEMBER\nachieves robust zero- and few-shot performance and offers a powerful and\nexplainable framework to neuroimaging-based diagnosis in the real world,\nespecially under limited data."}
{"id": "2504.09566", "pdf": "https://arxiv.org/pdf/2504.09566", "abs": "https://arxiv.org/abs/2504.09566", "authors": ["Chenghao Li", "Chaoning Zhang", "Yi Lu", "Jiaquan Zhang", "Qigan Sun", "Xudong Wang", "Jiwei Wei", "Guoqing Wang", "Yang Yang", "Heng Tao Shen"], "title": "Syzygy of Thoughts: Improving LLM CoT with the Minimal Free Resolution", "categories": ["cs.CL"], "comment": null, "summary": "Chain-of-Thought (CoT) prompting enhances the reasoning of large language\nmodels (LLMs) by decomposing problems into sequential steps, mimicking human\nlogic and reducing errors. However, complex tasks with vast solution spaces and\nvague constraints often exceed the capacity of a single reasoning chain.\nInspired by Minimal Free Resolution (MFR) in commutative algebra and algebraic\ngeometry, we propose Syzygy of Thoughts (SoT)-a novel framework that extends\nCoT by introducing auxiliary, interrelated reasoning paths. SoT captures deeper\nlogical dependencies, enabling more robust and structured problem-solving. MFR\ndecomposes a module into a sequence of free modules with minimal rank,\nproviding a structured analytical approach to complex systems. This method\nintroduces the concepts of \"Module\", \"Betti numbers\",\"Freeness\", \"Mapping\",\n\"Exactness\" and \"Minimality\", enabling the systematic decomposition of the\noriginal complex problem into logically complete minimal subproblems while\npreserving key problem features and reducing reasoning length. We tested SoT\nacross diverse datasets (e.g., GSM8K, MATH) and models (e.g., GPT-4o-mini,\nQwen2.5), achieving inference accuracy that matches or surpasses mainstream\nCoTs standards. Additionally, by aligning the sampling process with algebraic\nconstraints, our approach enhances the scalability of inference time in LLMs,\nensuring both transparent reasoning and high performance. Our code will be\npublicly available at https://github.com/dlMARiA/Syzygy-of-thoughts."}
{"id": "2504.09361", "pdf": "https://arxiv.org/pdf/2504.09361", "abs": "https://arxiv.org/abs/2504.09361", "authors": ["Jiahuan Long", "Tingsong Jiang", "Wen Yao", "Shuai Jia", "Weijia Zhang", "Weien Zhou", "Chao Ma", "Xiaoqian Chen"], "title": "PapMOT: Exploring Adversarial Patch Attack against Multiple Object Tracking", "categories": ["cs.CV"], "comment": "Accepted by ECCV 2024", "summary": "Tracking multiple objects in a continuous video stream is crucial for many\ncomputer vision tasks. It involves detecting and associating objects with their\nrespective identities across successive frames. Despite significant progress\nmade in multiple object tracking (MOT), recent studies have revealed the\nvulnerability of existing MOT methods to adversarial attacks. Nevertheless, all\nof these attacks belong to digital attacks that inject pixel-level noise into\ninput images, and are therefore ineffective in physical scenarios. To fill this\ngap, we propose PapMOT, which can generate physical adversarial patches against\nMOT for both digital and physical scenarios. Besides attacking the detection\nmechanism, PapMOT also optimizes a printable patch that can be detected as new\ntargets to mislead the identity association process. Moreover, we introduce a\npatch enhancement strategy to further degrade the temporal consistency of\ntracking results across video frames, resulting in more aggressive attacks. We\nfurther develop new evaluation metrics to assess the robustness of MOT against\nsuch attacks. Extensive evaluations on multiple datasets demonstrate that our\nPapMOT can successfully attack various architectures of MOT trackers in digital\nscenarios. We also validate the effectiveness of PapMOT for physical attacks by\ndeploying printed adversarial patches in the real world."}
{"id": "2504.09570", "pdf": "https://arxiv.org/pdf/2504.09570", "abs": "https://arxiv.org/abs/2504.09570", "authors": ["Biao Fu", "Minpeng Liao", "Kai Fan", "Chengxi Li", "Liang Zhang", "Yidong Chen", "Xiaodong Shi"], "title": "LLMs Can Achieve High-quality Simultaneous Machine Translation as Efficiently as Offline", "categories": ["cs.CL"], "comment": null, "summary": "When the complete source sentence is provided, Large Language Models (LLMs)\nperform excellently in offline machine translation even with a simple prompt\n\"Translate the following sentence from [src lang] into [tgt lang]:\". However,\nin many real scenarios, the source tokens arrive in a streaming manner and\nsimultaneous machine translation (SiMT) is required, then the efficiency and\nperformance of decoder-only LLMs are significantly limited by their\nauto-regressive nature. To enable LLMs to achieve high-quality SiMT as\nefficiently as offline translation, we propose a novel paradigm that includes\nconstructing supervised fine-tuning (SFT) data for SiMT, along with new\ntraining and inference strategies. To replicate the token input/output stream\nin SiMT, the source and target tokens are rearranged into an interleaved\nsequence, separated by special tokens according to varying latency\nrequirements. This enables powerful LLMs to learn read and write operations\nadaptively, based on varying latency prompts, while still maintaining efficient\nauto-regressive decoding. Experimental results show that, even with limited SFT\ndata, our approach achieves state-of-the-art performance across various SiMT\nbenchmarks, and preserves the original abilities of offline translation.\nMoreover, our approach generalizes well to document-level SiMT setting without\nrequiring specific fine-tuning, even beyond the offline translation model."}
{"id": "2504.09377", "pdf": "https://arxiv.org/pdf/2504.09377", "abs": "https://arxiv.org/abs/2504.09377", "authors": ["Jiawei Wu", "Zhifei Yang", "Zhe Wang", "Zhi Jin"], "title": "Beyond Degradation Conditions: All-in-One Image Restoration via HOG Transformers", "categories": ["cs.CV"], "comment": null, "summary": "All-in-one image restoration, which aims to address diverse degradations\nwithin a unified framework, is critical for practical applications. However,\nexisting methods rely on predicting and integrating degradation conditions,\nwhich can misactivate degradation-specific features in complex scenarios,\nlimiting their restoration performance. To address this issue, we propose a\nnovel all-in-one image restoration framework guided by Histograms of Oriented\nGradients (HOG), named HOGformer. By leveraging the degradation-discriminative\ncapability of HOG descriptors, HOGformer employs a dynamic self-attention\nmechanism that adaptively attends to long-range spatial dependencies based on\ndegradation-aware HOG cues. To enhance the degradation sensitivity of attention\ninputs, we design a HOG-guided local dynamic-range convolution module that\ncaptures long-range degradation similarities while maintaining awareness of\nglobal structural information. Furthermore, we propose a dynamic interaction\nfeed-forward module, efficiently increasing the model capacity to adapt to\ndifferent degradations through channel-spatial interactions. Extensive\nexperiments across diverse benchmarks, including adverse weather and natural\ndegradations, demonstrate that HOGformer achieves state-of-the-art performance\nand generalizes effectively to complex real-world degradations. Code is\navailable at https://github.com/Fire-friend/HOGformer."}
{"id": "2504.09586", "pdf": "https://arxiv.org/pdf/2504.09586", "abs": "https://arxiv.org/abs/2504.09586", "authors": ["Zuoli Tang", "Junjie Ou", "Kaiqin Hu", "Chunwei Wu", "Zhaoxin Huan", "Chilin Fu", "Xiaolu Zhang", "Jun Zhou", "Chenliang Li"], "title": "Short-Path Prompting in LLMs: Analyzing Reasoning Instability and Solutions for Robust Performance", "categories": ["cs.CL"], "comment": "Under review", "summary": "Recent years have witnessed significant progress in large language models'\n(LLMs) reasoning, which is largely due to the chain-of-thought (CoT)\napproaches, allowing models to generate intermediate reasoning steps before\nreaching the final answer. Building on these advances, state-of-the-art LLMs\nare instruction-tuned to provide long and detailed CoT pathways when responding\nto reasoning-related questions. However, human beings are naturally cognitive\nmisers and will prompt language models to give rather short responses, thus\nraising a significant conflict with CoT reasoning. In this paper, we delve into\nhow LLMs' reasoning performance changes when users provide short-path prompts.\nThe results and analysis reveal that language models can reason effectively and\nrobustly without explicit CoT prompts, while under short-path prompting, LLMs'\nreasoning ability drops significantly and becomes unstable, even on\ngrade-school problems. To address this issue, we propose two approaches: an\ninstruction-guided approach and a fine-tuning approach, both designed to\neffectively manage the conflict. Experimental results show that both methods\nachieve high accuracy, providing insights into the trade-off between\ninstruction adherence and reasoning accuracy in current models."}
{"id": "2504.09379", "pdf": "https://arxiv.org/pdf/2504.09379", "abs": "https://arxiv.org/abs/2504.09379", "authors": ["Lei Sun", "Yuhan Bao", "Jiajun Zhai", "Jingyun Liang", "Yulun Zhang", "Kaiwei Wang", "Danda Pani Paudel", "Luc Van Gool"], "title": "Low-Light Image Enhancement using Event-Based Illumination Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Low-light image enhancement (LLIE) aims to improve the visibility of images\ncaptured in poorly lit environments. Prevalent event-based solutions primarily\nutilize events triggered by motion, i.e., ''motion events'' to strengthen only\nthe edge texture, while leaving the high dynamic range and excellent low-light\nresponsiveness of event cameras largely unexplored. This paper instead opens a\nnew avenue from the perspective of estimating the illumination using\n''temporal-mapping'' events, i.e., by converting the timestamps of events\ntriggered by a transmittance modulation into brightness values. The resulting\nfine-grained illumination cues facilitate a more effective decomposition and\nenhancement of the reflectance component in low-light images through the\nproposed Illumination-aided Reflectance Enhancement module. Furthermore, the\ndegradation model of temporal-mapping events under low-light condition is\ninvestigated for realistic training data synthesizing. To address the lack of\ndatasets under this regime, we construct a beam-splitter setup and collect\nEvLowLight dataset that includes images, temporal-mapping events, and motion\nevents. Extensive experiments across 5 synthetic datasets and our real-world\nEvLowLight dataset substantiate that the devised pipeline, dubbed RetinEV,\nexcels in producing well-illuminated, high dynamic range images, outperforming\nprevious state-of-the-art event-based methods by up to 6.62 dB, while\nmaintaining an efficient inference speed of 35.6 frame-per-second on a 640X480\nimage."}
{"id": "2504.09620", "pdf": "https://arxiv.org/pdf/2504.09620", "abs": "https://arxiv.org/abs/2504.09620", "authors": ["Yuta Matsui", "Ryosuke Yamaki", "Ryo Ueda", "Seitaro Shinagawa", "Tadahiro Taniguchi"], "title": "Metropolis-Hastings Captioning Game: Knowledge Fusion of Vision Language Models via Decentralized Bayesian Inference", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.MA"], "comment": null, "summary": "We propose the Metropolis-Hastings Captioning Game (MHCG), a method to fuse\nknowledge of multiple vision-language models (VLMs) by learning from each\nother. Although existing methods that combine multiple models suffer from\ninference costs and architectural constraints, MHCG avoids these problems by\nperforming decentralized Bayesian inference through a process resembling a\nlanguage game. The knowledge fusion process establishes communication between\ntwo VLM agents alternately captioning images and learning from each other. We\nconduct two image-captioning experiments with two VLMs, each pre-trained on a\ndifferent dataset. The first experiment demonstrates that MHCG achieves\nconsistent improvement in reference-free evaluation metrics. The second\nexperiment investigates how MHCG contributes to sharing VLMs' category-level\nvocabulary by observing the occurrence of the vocabulary in the generated\ncaptions."}
{"id": "2504.09384", "pdf": "https://arxiv.org/pdf/2504.09384", "abs": "https://arxiv.org/abs/2504.09384", "authors": ["Shengzhe Chen", "Zhaoxuan Dong", "Jun Liu"], "title": "Contour Flow Constraint: Preserving Global Shape Similarity for Deep Learning based Image Segmentation", "categories": ["cs.CV"], "comment": "Submitted to IEEE Transactions on Image Processin on Dec-14-2023.\n  Revised on Oct-16-2024", "summary": "For effective image segmentation, it is crucial to employ constraints\ninformed by prior knowledge about the characteristics of the areas to be\nsegmented to yield favorable segmentation outcomes. However, the existing\nmethods have primarily focused on priors of specific properties or shapes,\nlacking consideration of the general global shape similarity from a Contour\nFlow (CF) perspective. Furthermore, naturally integrating this contour flow\nprior image segmentation model into the activation functions of deep\nconvolutional networks through mathematical methods is currently unexplored. In\nthis paper, we establish a concept of global shape similarity based on the\npremise that two shapes exhibit comparable contours. Furthermore, we\nmathematically derive a contour flow constraint that ensures the preservation\nof global shape similarity. We propose two implementations to integrate the\nconstraint with deep neural networks. Firstly, the constraint is converted to a\nshape loss, which can be seamlessly incorporated into the training phase for\nany learning-based segmentation framework. Secondly, we add the constraint into\na variational segmentation model and derive its iterative schemes for solution.\nThe scheme is then unrolled to get the architecture of the proposed CFSSnet.\nValidation experiments on diverse datasets are conducted on classic benchmark\ndeep network segmentation models. The results indicate a great improvement in\nsegmentation accuracy and shape similarity for the proposed shape loss,\nshowcasing the general adaptability of the proposed loss term regardless of\nspecific network architectures. CFSSnet shows robustness in segmenting\nnoise-contaminated images, and inherent capability to preserve global shape\nsimilarity."}
{"id": "2504.09639", "pdf": "https://arxiv.org/pdf/2504.09639", "abs": "https://arxiv.org/abs/2504.09639", "authors": ["Haotian Wang", "Han Zhao", "Shuaiting Chen", "Xiaoyu Tian", "Sitong Zhao", "Yunjie Ji", "Yiping Peng", "Xiangang Li"], "title": "Leveraging Reasoning Model Answers to Enhance Non-Reasoning Model Capability", "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in large language models (LLMs), such as DeepSeek-R1 and\nOpenAI-o1, have demonstrated the significant effectiveness of test-time\nscaling, achieving substantial performance gains across various benchmarks.\nThese advanced models utilize deliberate \"thinking\" steps to systematically\nenhance answer quality. In this paper, we propose leveraging these high-quality\noutputs generated by reasoning-intensive models to improve less computationally\ndemanding, non-reasoning models. We explore and compare methodologies for\nutilizing the answers produced by reasoning models to train and improve\nnon-reasoning models. Through straightforward Supervised Fine-Tuning (SFT)\nexperiments on established benchmarks, we demonstrate consistent improvements\nacross various benchmarks, underscoring the potential of this approach for\nadvancing the ability of models to answer questions directly."}
{"id": "2504.09393", "pdf": "https://arxiv.org/pdf/2504.09393", "abs": "https://arxiv.org/abs/2504.09393", "authors": ["Nooshin Bahador"], "title": "Vision Transformers Exhibit Human-Like Biases: Evidence of Orientation and Color Selectivity, Categorical Perception, and Phase Transitions", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "This study explored whether Vision Transformers (ViTs) developed orientation\nand color biases similar to those observed in the human brain. Using synthetic\ndatasets with controlled variations in noise levels, angles, lengths, widths,\nand colors, we analyzed the behavior of ViTs fine-tuned with LoRA. Our findings\nrevealed four key insights: First, ViTs exhibited an oblique effect showing the\nlowest angle prediction errors at 180 deg (horizontal) across all conditions.\nSecond, angle prediction errors varied by color. Errors were highest for bluish\nhues and lowest for yellowish ones. Additionally, clustering analysis of angle\nprediction errors showed that ViTs grouped colors in a way that aligned with\nhuman perceptual categories. In addition to orientation and color biases, we\nobserved phase transition phenomena. While two phase transitions occurred\nconsistently across all conditions, the training loss curves exhibited delayed\ntransitions when color was incorporated as an additional data attribute.\nFinally, we observed that attention heads in certain layers inherently develop\nspecialized capabilities, functioning as task-agnostic feature extractors\nregardless of the downstream task. These observations suggest that biases and\nproperties arise primarily from pre-training on the original dataset which\nshapes the model's foundational representations and the inherent architectural\nconstraints of the vision transformer, rather than being solely determined by\ndownstream data statistics."}
{"id": "2504.09643", "pdf": "https://arxiv.org/pdf/2504.09643", "abs": "https://arxiv.org/abs/2504.09643", "authors": ["Nikita Sorokin", "Ivan Sedykh", "Valentin Malykh"], "title": "Iterative Self-Training for Code Generation via Reinforced Re-Ranking", "categories": ["cs.CL", "cs.IR", "cs.SE"], "comment": "Published at ECIR 2025", "summary": "Generating high-quality code that solves complex programming tasks is\nchallenging, especially with current decoder-based models that produce highly\nstochastic outputs. In code generation, even minor errors can easily break the\nentire solution. Leveraging multiple sampled solutions can significantly\nimprove the overall output quality.\n  One effective way to enhance code generation is by pairing a code generation\nmodel with a reranker model, which selects the best solution from the generated\nsamples. We propose a novel iterative self-training approach for self-training\nreranker models using Proximal Policy Optimization (PPO), aimed at improving\nboth reranking accuracy and the overall code generation process. Unlike\ntraditional PPO approaches, where the focus is on optimizing a generative model\nwith a reward model, our approach emphasizes the development of a robust\nreward/reranking model. This model improves the quality of generated code\nthrough reranking and addresses problems and errors that the reward model might\noverlook during PPO alignment with the reranker. Our method iteratively refines\nthe training dataset by re-evaluating outputs, identifying high-scoring\nnegative examples, and incorporating them into the training loop, that boosting\nmodel performance.\n  Our evaluation on the MultiPL-E dataset demonstrates that our 13.4B parameter\nmodel outperforms a 33B model in code generation quality while being three\ntimes faster. Moreover, it achieves performance comparable to GPT-4 and\nsurpasses it in one programming language."}
{"id": "2504.09424", "pdf": "https://arxiv.org/pdf/2504.09424", "abs": "https://arxiv.org/abs/2504.09424", "authors": ["Luis Vieira"], "title": "Comparing Performance of Preprocessing Techniques for Traffic Sign Recognition Using a HOG-SVM", "categories": ["cs.CV"], "comment": "working paper (preprint)", "summary": "This study compares the performance of various preprocessing techniques for\nTraffic Sign Recognition (TSR) using Histogram of Oriented Gradients (HOG) and\nSupport Vector Machine (SVM) on the German Traffic Sign Recognition Benchmark\n(GTSRB) dataset. Techniques such as CLAHE, HUE, and YUV were evaluated for\ntheir impact on classification accuracy. Results indicate that YUV in\nparticular significantly enhance the performance of the HOG-SVM classifier\n(improving accuracy from 89.65% to 91.25%), providing insights into\nimprovements for preprocessing pipeline of TSR applications."}
{"id": "2504.09645", "pdf": "https://arxiv.org/pdf/2504.09645", "abs": "https://arxiv.org/abs/2504.09645", "authors": ["Aung Kyaw Htet", "Mark Dras"], "title": "Myanmar XNLI: Building a Dataset and Exploring Low-resource Approaches to Natural Language Inference with Myanmar", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite dramatic recent progress in NLP, it is still a major challenge to\napply Large Language Models (LLM) to low-resource languages. This is made\nvisible in benchmarks such as Cross-Lingual Natural Language Inference (XNLI),\na key task that demonstrates cross-lingual capabilities of NLP systems across a\nset of 15 languages. In this paper, we extend the XNLI task for one additional\nlow-resource language, Myanmar, as a proxy challenge for broader low-resource\nlanguages, and make three core contributions. First, we build a dataset called\nMyanmar XNLI (myXNLI) using community crowd-sourced methods, as an extension to\nthe existing XNLI corpus. This involves a two-stage process of community-based\nconstruction followed by expert verification; through an analysis, we\ndemonstrate and quantify the value of the expert verification stage in the\ncontext of community-based construction for low-resource languages. We make the\nmyXNLI dataset available to the community for future research. Second, we carry\nout evaluations of recent multilingual language models on the myXNLI benchmark,\nas well as explore data-augmentation methods to improve model performance. Our\ndata-augmentation methods improve model accuracy by up to 2 percentage points\nfor Myanmar, while uplifting other languages at the same time. Third, we\ninvestigate how well these data-augmentation methods generalise to other\nlow-resource languages in the XNLI dataset."}
{"id": "2504.09426", "pdf": "https://arxiv.org/pdf/2504.09426", "abs": "https://arxiv.org/abs/2504.09426", "authors": ["Shengao Wang", "Arjun Chandra", "Aoming Liu", "Venkatesh Saligrama", "Boqing Gong"], "title": "BabyVLM: Data-Efficient Pretraining of VLMs Inspired by Infant Learning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Human infants rapidly develop visual reasoning skills from minimal input,\nsuggesting that developmentally inspired pretraining could significantly\nenhance the efficiency of vision-language models (VLMs). Although recent\nefforts have leveraged infant-inspired datasets like SAYCam, existing\nevaluation benchmarks remain misaligned--they are either too simplistic,\nnarrowly scoped, or tailored for large-scale pretrained models. Additionally,\ntraining exclusively on infant data overlooks the broader, diverse input from\nwhich infants naturally learn. To address these limitations, we propose\nBabyVLM, a novel framework comprising comprehensive in-domain evaluation\nbenchmarks and a synthetic training dataset created via child-directed\ntransformations of existing datasets. We demonstrate that VLMs trained with our\nsynthetic dataset achieve superior performance on BabyVLM tasks compared to\nmodels trained solely on SAYCam or general-purpose data of the SAYCam size.\nBabyVLM thus provides a robust, developmentally aligned evaluation tool and\nillustrates how compact models trained on carefully curated data can generalize\neffectively, opening pathways toward data-efficient vision-language learning\nparadigms."}
{"id": "2504.09665", "pdf": "https://arxiv.org/pdf/2504.09665", "abs": "https://arxiv.org/abs/2504.09665", "authors": ["Liqiang Wen", "Guanming Xiong", "Tong Mo", "Bing Li", "Weiping Li", "Wen Zhao"], "title": "CLEAR-KGQA: Clarification-Enhanced Ambiguity Resolution for Knowledge Graph Question Answering", "categories": ["cs.CL"], "comment": "This work has been accepted by the IJCNN 2025 main track", "summary": "This study addresses the challenge of ambiguity in knowledge graph question\nanswering (KGQA). While recent KGQA systems have made significant progress,\nparticularly with the integration of large language models (LLMs), they\ntypically assume user queries are unambiguous, which is an assumption that\nrarely holds in real-world applications. To address these limitations, we\npropose a novel framework that dynamically handles both entity ambiguity (e.g.,\ndistinguishing between entities with similar names) and intent ambiguity (e.g.,\nclarifying different interpretations of user queries) through interactive\nclarification. Our approach employs a Bayesian inference mechanism to quantify\nquery ambiguity and guide LLMs in determining when and how to request\nclarification from users within a multi-turn dialogue framework. We further\ndevelop a two-agent interaction framework where an LLM-based user simulator\nenables iterative refinement of logical forms through simulated user feedback.\nExperimental results on the WebQSP and CWQ dataset demonstrate that our method\nsignificantly improves performance by effectively resolving semantic\nambiguities. Additionally, we contribute a refined dataset of disambiguated\nqueries, derived from interaction histories, to facilitate future research in\nthis direction."}
{"id": "2504.09441", "pdf": "https://arxiv.org/pdf/2504.09441", "abs": "https://arxiv.org/abs/2504.09441", "authors": ["Jiahua Xu", "Dawei Zhou", "Lei Hu", "Zaiyi Liu", "Nannan Wang", "Xinbo Gao"], "title": "Structure-Accurate Medical Image Translation based on Dynamic Frequency Balance and Knowledge Guidance", "categories": ["cs.CV", "eess.IV"], "comment": "Medical image translation, Diffusion model, 16 pages", "summary": "Multimodal medical images play a crucial role in the precise and\ncomprehensive clinical diagnosis. Diffusion model is a powerful strategy to\nsynthesize the required medical images. However, existing approaches still\nsuffer from the problem of anatomical structure distortion due to the\noverfitting of high-frequency information and the weakening of low-frequency\ninformation. Thus, we propose a novel method based on dynamic frequency balance\nand knowledge guidance. Specifically, we first extract the low-frequency and\nhigh-frequency components by decomposing the critical features of the model\nusing wavelet transform. Then, a dynamic frequency balance module is designed\nto adaptively adjust frequency for enhancing global low-frequency features and\neffective high-frequency details as well as suppressing high-frequency noise.\nTo further overcome the challenges posed by the large differences between\ndifferent medical modalities, we construct a knowledge-guided mechanism that\nfuses the prior clinical knowledge from a visual language model with visual\nfeatures, to facilitate the generation of accurate anatomical structures.\nExperimental evaluations on multiple datasets show the proposed method achieves\nsignificant improvements in qualitative and quantitative assessments, verifying\nits effectiveness and superiority."}
{"id": "2504.09687", "pdf": "https://arxiv.org/pdf/2504.09687", "abs": "https://arxiv.org/abs/2504.09687", "authors": ["Salman Faroz"], "title": "Domain-Adaptive Continued Pre-Training of Small Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Continued pre-training of small language models offers a promising path for\ndomain adaptation with limited computational resources. I've investigated this\napproach within educational domains, evaluating it as a resource-efficient\nalternative to training models from scratch. Using a 125M parameter model, I\ndemonstrate significant performance improvements through incremental training\non 400 million tokens, followed by further training to reach 1 billion tokens.\nMy approach includes comprehensive data preprocessing, memory-optimized\ntraining configurations, and benchmark-based evaluation. Results show notable\ngains in knowledge-intensive tasks (MMLU +8.1%) and contextual understanding\n(HellaSwag +7.6%), while revealing educational domain specialization\ntrade-offs. I analyze token efficiency, catastrophic forgetting mitigation\nstrategies, and scaling patterns. My findings suggest that thoughtful\npreprocessing and training methodologies enable meaningful improvements in\nlanguage model capabilities even with constrained computational resources,\nopening pathways for domain-specific adaptation of smaller language models."}
{"id": "2504.09446", "pdf": "https://arxiv.org/pdf/2504.09446", "abs": "https://arxiv.org/abs/2504.09446", "authors": ["Lincoln Linlin Xu", "Yimin Zhu", "Zack Dewis", "Zhengsen Xu", "Motasem Alkayid", "Mabel Heffring", "Saeid Taleghanidoozdoozan"], "title": "Sparse Deformable Mamba for Hyperspectral Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "Although the recent Mamba models significantly improve hyperspectral image\n(HSI) classification, one critical challenge is caused by the difficulty to\nbuild the Mamba sequence efficiently and effectively. This paper presents a\nSparse Deformable Mamba (SDMamba) approach for enhanced HSI classification,\nwith the following contributions. First, to enhance Mamba sequence, an\nefficient Sparse Deformable Sequencing (SDS) approach is designed to adaptively\nlearn the \"optimal\" sequence, leading to sparse and deformable Mamba sequence\nwith increased detail preservation and decreased computations. Second, to boost\nspatial-spectral feature learning, based on SDS, a Sparse Deformable Spatial\nMamba Module (SDSpaM) and a Sparse Deformable Spectral Mamba Module (SDSpeM)\nare designed for tailored modeling of the spatial information spectral\ninformation. Last, to improve the fusion of SDSpaM and SDSpeM, an attention\nbased feature fusion approach is designed to integrate the outputs of the\nSDSpaM and SDSpeM. The proposed method is tested on several benchmark datasets\nwith many state-of-the-art approaches, demonstrating that the proposed approach\ncan achieve higher accuracy, faster speed, and better detail small-class\npreservation capability."}
{"id": "2504.09696", "pdf": "https://arxiv.org/pdf/2504.09696", "abs": "https://arxiv.org/abs/2504.09696", "authors": ["Jixiao Zhang", "Chunsheng Zuo"], "title": "GRPO-LEAD: A Difficulty-Aware Reinforcement Learning Approach for Concise Mathematical Reasoning in Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in R1-like reasoning models leveraging Group Relative Policy\nOptimization (GRPO) have significantly improved the performance of language\nmodels on mathematical reasoning tasks. However, current GRPO implementations\nencounter critical challenges, including reward sparsity due to binary accuracy\nmetrics, limited incentives for conciseness, and insufficient focus on complex\nreasoning tasks. To address these issues, we propose GRPO-LEAD, a suite of\nnovel enhancements tailored for mathematical reasoning. Specifically, GRPO-LEAD\nintroduces (1) a length-dependent accuracy reward to encourage concise and\nprecise solutions, (2) an explicit penalty mechanism for incorrect answers to\nsharpen decision boundaries, and (3) a difficulty-aware advantage reweighting\nstrategy that amplifies learning signals for challenging problems. Furthermore,\nwe systematically examine the impact of model scale and supervised fine-tuning\n(SFT) strategies, demonstrating that larger-scale base models and carefully\ncurated datasets significantly enhance reinforcement learning effectiveness.\nExtensive empirical evaluations and ablation studies confirm that GRPO-LEAD\nsubstantially mitigates previous shortcomings, resulting in language models\nthat produce more concise, accurate, and robust reasoning across diverse\nmathematical tasks."}
{"id": "2504.09448", "pdf": "https://arxiv.org/pdf/2504.09448", "abs": "https://arxiv.org/abs/2504.09448", "authors": ["Lin Zhu", "Yifeng Yang", "Zichao Nie", "Yuan Gao", "Jiarui Li", "Qinying Gu", "Xinbing Wang", "Chenghu Zhou", "Nanyang Ye"], "title": "InfoBound: A Provable Information-Bounds Inspired Framework for Both OoD Generalization and OoD Detection", "categories": ["cs.CV", "cs.LG"], "comment": "Under Review", "summary": "In real-world scenarios, distribution shifts give rise to the importance of\ntwo problems: out-of-distribution (OoD) generalization, which focuses on\nmodels' generalization ability against covariate shifts (i.e., the changes of\nenvironments), and OoD detection, which aims to be aware of semantic shifts\n(i.e., test-time unseen classes). Real-world testing environments often involve\na combination of both covariate and semantic shifts. While numerous methods\nhave been proposed to address these critical issues, only a few works tackled\nthem simultaneously. Moreover, prior works often improve one problem but\nsacrifice the other. To overcome these limitations, we delve into boosting OoD\ndetection and OoD generalization from the perspective of information theory,\nwhich can be easily applied to existing models and different tasks. Building\nupon the theoretical bounds for mutual information and conditional entropy, we\nprovide a unified approach, composed of Mutual Information Minimization\n(MI-Min) and Conditional Entropy Maximizing (CE-Max). Extensive experiments and\ncomprehensive evaluations on multi-label image classification and object\ndetection have demonstrated the superiority of our method. It successfully\nmitigates trade-offs between the two challenges compared to competitive\nbaselines."}
{"id": "2504.09714", "pdf": "https://arxiv.org/pdf/2504.09714", "abs": "https://arxiv.org/abs/2504.09714", "authors": ["Ayşe Aysu Cengiz", "Ahmet Kaan Sever", "Elif Ecem Ümütlü", "Naime Şeyma Erdem", "Burak Aytan", "Büşra Tufan", "Abdullah Topraksoy", "Esra Darıcı", "Cagri Toraman"], "title": "Evaluating the Quality of Benchmark Datasets for Low-Resource Languages: A Case Study on Turkish", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The reliance on translated or adapted datasets from English or multilingual\nresources introduces challenges regarding linguistic and cultural suitability.\nThis study addresses the need for robust and culturally appropriate benchmarks\nby evaluating the quality of 17 commonly used Turkish benchmark datasets. Using\na comprehensive framework that assesses six criteria, both human and LLM-judge\nannotators provide detailed evaluations to identify dataset strengths and\nshortcomings.\n  Our results reveal that 70% of the benchmark datasets fail to meet our\nheuristic quality standards. The correctness of the usage of technical terms is\nthe strongest criterion, but 85% of the criteria are not satisfied in the\nexamined datasets. Although LLM judges demonstrate potential, they are less\neffective than human annotators, particularly in understanding cultural common\nsense knowledge and interpreting fluent, unambiguous text. GPT-4o has stronger\nlabeling capabilities for grammatical and technical tasks, while Llama3.3-70B\nexcels at correctness and cultural knowledge evaluation. Our findings emphasize\nthe urgent need for more rigorous quality control in creating and adapting\ndatasets for low-resource languages."}
{"id": "2504.09451", "pdf": "https://arxiv.org/pdf/2504.09451", "abs": "https://arxiv.org/abs/2504.09451", "authors": ["Tianyi Wang", "Harry Cheng", "Ming-Hui Liu", "Mohan Kankanhalli"], "title": "FractalForensics: Proactive Deepfake Detection and Localization via Fractal Watermarks", "categories": ["cs.CV"], "comment": null, "summary": "Proactive Deepfake detection via robust watermarks has been raised ever since\npassive Deepfake detectors encountered challenges in identifying high-quality\nsynthetic images. However, while demonstrating reasonable detection\nperformance, they lack localization functionality and explainability in\ndetection results. Additionally, the unstable robustness of watermarks can\nsignificantly affect the detection performance accordingly. In this study, we\npropose novel fractal watermarks for proactive Deepfake detection and\nlocalization, namely FractalForensics. Benefiting from the characteristics of\nfractals, we devise a parameter-driven watermark generation pipeline that\nderives fractal-based watermarks and conducts one-way encryption regarding the\nparameters selected. Subsequently, we propose a semi-fragile watermarking\nframework for watermark embedding and recovery, trained to be robust against\nbenign image processing operations and fragile when facing Deepfake\nmanipulations in a black-box setting. Meanwhile, we introduce an entry-to-patch\nstrategy that implicitly embeds the watermark matrix entries into image patches\nat corresponding positions, achieving localization of Deepfake manipulations.\nExtensive experiments demonstrate satisfactory robustness and fragility of our\napproach against common image processing operations and Deepfake manipulations,\noutperforming state-of-the-art semi-fragile watermarking algorithms and passive\ndetectors for Deepfake detection. Furthermore, by highlighting the areas\nmanipulated, our method provides explainability for the proactive Deepfake\ndetection results."}
{"id": "2504.09753", "pdf": "https://arxiv.org/pdf/2504.09753", "abs": "https://arxiv.org/abs/2504.09753", "authors": ["Ram Mohan Rao Kadiyala", "Siddartha Pullakhandam", "Siddhant Gupta", "Drishti Sharma", "Jebish Purbey", "Kanwal Mehreen", "Muhammad Arham", "Hamza Farooq"], "title": "Improving Multilingual Capabilities with Cultural and Local Knowledge in Large Language Models While Enhancing Native Performance", "categories": ["cs.CL", "cs.AI"], "comment": "ARR Feb 2025 submission", "summary": "Large Language Models (LLMs) have shown remarkable capabilities, but their\ndevelopment has primarily focused on English and other high-resource languages,\nleaving many languages underserved. We present our latest Hindi-English\nbi-lingual LLM \\textbf{Mantra-14B} with ~3\\% average improvement in benchmark\nscores over both languages, outperforming models twice its size. Using a\ncurated dataset composed of English and Hindi instruction data of 485K samples,\nwe instruction tuned models such as Qwen-2.5-14B-Instruct and Phi-4 to improve\nperformance over both English and Hindi. Our experiments encompassing seven\ndifferent LLMs of varying parameter sizes and over 140 training attempts with\nvarying English-Hindi training data ratios demonstrated that it is possible to\nsignificantly improve multilingual performance without compromising native\nperformance. Further, our approach avoids resource-intensive techniques like\nvocabulary expansion or architectural modifications, thus keeping the model\nsize small. Our results indicate that modest fine-tuning with culturally and\nlocally informed data can bridge performance gaps without incurring significant\ncomputational overhead. We release our training code, datasets, and models\nunder mit and apache licenses to aid further research towards under-represented\nand low-resource languages."}
{"id": "2504.09454", "pdf": "https://arxiv.org/pdf/2504.09454", "abs": "https://arxiv.org/abs/2504.09454", "authors": ["Weinan Jia", "Mengqi Huang", "Nan Chen", "Lei Zhang", "Zhendong Mao"], "title": "D$^2$iT: Dynamic Diffusion Transformer for Accurate Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models are widely recognized for their ability to generate\nhigh-fidelity images. Despite the excellent performance and scalability of the\nDiffusion Transformer (DiT) architecture, it applies fixed compression across\ndifferent image regions during the diffusion process, disregarding the\nnaturally varying information densities present in these regions. However,\nlarge compression leads to limited local realism, while small compression\nincreases computational complexity and compromises global consistency,\nultimately impacting the quality of generated images. To address these\nlimitations, we propose dynamically compressing different image regions by\nrecognizing the importance of different regions, and introduce a novel\ntwo-stage framework designed to enhance the effectiveness and efficiency of\nimage generation: (1) Dynamic VAE (DVAE) at first stage employs a hierarchical\nencoder to encode different image regions at different downsampling rates,\ntailored to their specific information densities, thereby providing more\naccurate and natural latent codes for the diffusion process. (2) Dynamic\nDiffusion Transformer (D$^2$iT) at second stage generates images by predicting\nmulti-grained noise, consisting of coarse-grained (less latent code in smooth\nregions) and fine-grained (more latent codes in detailed regions), through an\nnovel combination of the Dynamic Grain Transformer and the Dynamic Content\nTransformer. The strategy of combining rough prediction of noise with detailed\nregions correction achieves a unification of global consistency and local\nrealism. Comprehensive experiments on various generation tasks validate the\neffectiveness of our approach. Code will be released at\nhttps://github.com/jiawn-creator/Dynamic-DiT."}
{"id": "2504.09763", "pdf": "https://arxiv.org/pdf/2504.09763", "abs": "https://arxiv.org/abs/2504.09763", "authors": ["Zaid Khan", "Elias Stengel-Eskin", "Archiki Prasad", "Jaemin Cho", "Mohit Bansal"], "title": "Executable Functional Abstractions: Inferring Generative Programs for Advanced Math Problems", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project Page: https://zaidkhan.me/EFAGen/", "summary": "Scientists often infer abstract procedures from specific instances of\nproblems and use the abstractions to generate new, related instances. For\nexample, programs encoding the formal rules and properties of a system have\nbeen useful in fields ranging from RL (procedural environments) to physics\n(simulation engines). These programs can be seen as functions which execute to\ndifferent outputs based on their parameterizations (e.g., gridworld\nconfiguration or initial physical conditions). We introduce the term EFA\n(Executable Functional Abstraction) to denote such programs for math problems.\nEFA-like constructs have been shown to be useful for math reasoning as problem\ngenerators for stress-testing models. However, prior work has been limited to\nabstractions for grade-school math (whose simple rules are easy to encode in\nprograms), while generating EFAs for advanced math has thus far required human\nengineering. We explore the automatic construction of EFAs for advanced math\nproblems. We operationalize the task of automatically constructing EFAs as a\nprogram synthesis task, and develop EFAGen, which conditions an LLM on a seed\nmath problem and its step-by-step solution to generate candidate EFA programs\nthat are faithful to the generalized problem and solution class underlying the\nseed problem. Furthermore, we formalize properties any valid EFA must possess\nin terms of executable unit tests, and show how the tests can be used as\nverifiable rewards to train LLMs to become better writers of EFAs. We\ndemonstrate that EFAs constructed by EFAGen behave rationally by remaining\nfaithful to seed problems, produce learnable problem variations, and that\nEFAGen can infer EFAs across multiple diverse sources of competition-level math\nproblems. Finally, we show downstream uses of model-written EFAs e.g. finding\nproblem variations that are harder or easier for a learner to solve, as well as\ndata generation."}
{"id": "2504.09455", "pdf": "https://arxiv.org/pdf/2504.09455", "abs": "https://arxiv.org/abs/2504.09455", "authors": ["Hussain Md. Safwan", "Mahbub Islam Mahim", "Fawwaz Mohammed Amin"], "title": "Enhancing Wide-Angle Image Using Narrow-Angle View of the Same Scene", "categories": ["cs.CV", "eess.IV", "F.2.2; I.2.7"], "comment": null, "summary": "A common dilemma while photographing a scene is whether to capture it in\nwider angle, allowing more of the scene to be covered but in lesser details or\nto click in narrow angle that captures better details but leaves out portions\nof the scene. We propose a novel method in this paper that infuses wider shots\nwith finer quality details that is usually associated with an image captured by\nthe primary lens by capturing the same scene using both narrow and wide field\nof view (FoV) lenses. We do so by training a GAN-based model to learn to\nextract the visual quality parameters from a narrow angle shot and to transfer\nthese to the corresponding wide-angle image of the scene. We have mentioned in\ndetails the proposed technique to isolate the visual essence of an image and to\ntransfer it into another image. We have also elaborately discussed our\nimplementation details and have presented the results of evaluation over\nseveral benchmark datasets and comparisons with contemporary advancements in\nthe field."}
{"id": "2504.09781", "pdf": "https://arxiv.org/pdf/2504.09781", "abs": "https://arxiv.org/abs/2504.09781", "authors": ["Jingtian Wu", "Claire Cardie"], "title": "Reasoning Court: Combining Reasoning, Action, and Judgment for Multi-Hop Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While large language models (LLMs) have demonstrated strong capabilities in\ntasks like question answering and fact verification, they continue to suffer\nfrom hallucinations and reasoning errors, especially in multi-hop tasks that\nrequire integration of multiple information sources. Current methods address\nthese issues through retrieval-based techniques (grounding reasoning in\nexternal evidence), reasoning-based approaches (enhancing coherence via\nimproved prompting), or hybrid strategies combining both elements. One\nprominent hybrid method, ReAct, has outperformed purely retrieval-based or\nreasoning-based approaches; however, it lacks internal verification of\nintermediate reasoning steps, allowing potential errors to propagate through\ncomplex reasoning tasks. In this paper, we introduce Reasoning Court (RC), a\nnovel framework that extends iterative reasoning-and-retrieval methods, such as\nReAct, with a dedicated LLM judge. Unlike ReAct, RC employs this judge to\nindependently evaluate multiple candidate answers and their associated\nreasoning generated by separate LLM agents. The judge is asked to select the\nanswer that it considers the most factually grounded and logically coherent\nbased on the presented reasoning and evidence, or synthesizes a new answer\nusing available evidence and its pre-trained knowledge if all candidates are\ninadequate, flawed, or invalid. Evaluations on multi-hop benchmarks (HotpotQA,\nMuSiQue) and fact-verification (FEVER) demonstrate that RC consistently\noutperforms state-of-the-art few-shot prompting methods without task-specific\nfine-tuning."}
{"id": "2504.09472", "pdf": "https://arxiv.org/pdf/2504.09472", "abs": "https://arxiv.org/abs/2504.09472", "authors": ["Pooja Guhan", "Divya Kothandaraman", "Tsung-Wei Huang", "Guan-Ming Su", "Dinesh Manocha"], "title": "CamMimic: Zero-Shot Image To Camera Motion Personalized Video Generation Using Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "We introduce CamMimic, an innovative algorithm tailored for dynamic video\nediting needs. It is designed to seamlessly transfer the camera motion observed\nin a given reference video onto any scene of the user's choice in a zero-shot\nmanner without requiring any additional data. Our algorithm achieves this using\na two-phase strategy by leveraging a text-to-video diffusion model. In the\nfirst phase, we develop a multi-concept learning method using a combination of\nLoRA layers and an orthogonality loss to capture and understand the underlying\nspatial-temporal characteristics of the reference video as well as the spatial\nfeatures of the user's desired scene. The second phase proposes a unique\nhomography-based refinement strategy to enhance the temporal and spatial\nalignment of the generated video. We demonstrate the efficacy of our method\nthrough experiments conducted on a dataset containing combinations of diverse\nscenes and reference videos containing a variety of camera motions. In the\nabsence of an established metric for assessing camera motion transfer between\nunrelated scenes, we propose CameraScore, a novel metric that utilizes\nhomography representations to measure camera motion similarity between the\nreference and generated videos. Extensive quantitative and qualitative\nevaluations demonstrate that our approach generates high-quality,\nmotion-enhanced videos. Additionally, a user study reveals that 70.31% of\nparticipants preferred our method for scene preservation, while 90.45% favored\nit for motion transfer. We hope this work lays the foundation for future\nadvancements in camera motion transfer across different scenes."}
{"id": "2504.09795", "pdf": "https://arxiv.org/pdf/2504.09795", "abs": "https://arxiv.org/abs/2504.09795", "authors": ["Ryota Tanaka", "Taichi Iki", "Taku Hasegawa", "Kyosuke Nishida", "Kuniko Saito", "Jun Suzuki"], "title": "VDocRAG: Retrieval-Augmented Generation over Visually-Rich Documents", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR"], "comment": "Accepted by CVPR 2025; project page: https://vdocrag.github.io", "summary": "We aim to develop a retrieval-augmented generation (RAG) framework that\nanswers questions over a corpus of visually-rich documents presented in mixed\nmodalities (e.g., charts, tables) and diverse formats (e.g., PDF, PPTX). In\nthis paper, we introduce a new RAG framework, VDocRAG, which can directly\nunderstand varied documents and modalities in a unified image format to prevent\nmissing information that occurs by parsing documents to obtain text. To improve\nthe performance, we propose novel self-supervised pre-training tasks that adapt\nlarge vision-language models for retrieval by compressing visual information\ninto dense token representations while aligning them with textual content in\ndocuments. Furthermore, we introduce OpenDocVQA, the first unified collection\nof open-domain document visual question answering datasets, encompassing\ndiverse document types and formats. OpenDocVQA provides a comprehensive\nresource for training and evaluating retrieval and question answering models on\nvisually-rich documents in an open-domain setting. Experiments show that\nVDocRAG substantially outperforms conventional text-based RAG and has strong\ngeneralization capability, highlighting the potential of an effective RAG\nparadigm for real-world documents."}
{"id": "2504.09480", "pdf": "https://arxiv.org/pdf/2504.09480", "abs": "https://arxiv.org/abs/2504.09480", "authors": ["Yongchao Feng", "Yajie Liu", "Shuai Yang", "Wenrui Cai", "Jinqing Zhang", "Qiqi Zhan", "Ziyue Huang", "Hongxi Yan", "Qiao Wan", "Chenguang Liu", "Junzhe Wang", "Jiahui Lv", "Ziqi Liu", "Tengyuan Shi", "Qingjie Liu", "Yunhong Wang"], "title": "Vision-Language Model for Object Detection and Segmentation: A Review and Evaluation", "categories": ["cs.CV", "cs.AI"], "comment": "A Review and Evaluation about Vision-Language Model for Object\n  Detection and Segmentation", "summary": "Vision-Language Model (VLM) have gained widespread adoption in\nOpen-Vocabulary (OV) object detection and segmentation tasks. Despite they have\nshown promise on OV-related tasks, their effectiveness in conventional vision\ntasks has thus far been unevaluated. In this work, we present the systematic\nreview of VLM-based detection and segmentation, view VLM as the foundational\nmodel and conduct comprehensive evaluations across multiple downstream tasks\nfor the first time: 1) The evaluation spans eight detection scenarios\n(closed-set detection, domain adaptation, crowded objects, etc.) and eight\nsegmentation scenarios (few-shot, open-world, small object, etc.), revealing\ndistinct performance advantages and limitations of various VLM architectures\nacross tasks. 2) As for detection tasks, we evaluate VLMs under three\nfinetuning granularities: \\textit{zero prediction}, \\textit{visual\nfine-tuning}, and \\textit{text prompt}, and further analyze how different\nfinetuning strategies impact performance under varied task. 3) Based on\nempirical findings, we provide in-depth analysis of the correlations between\ntask characteristics, model architectures, and training methodologies, offering\ninsights for future VLM design. 4) We believe that this work shall be valuable\nto the pattern recognition experts working in the fields of computer vision,\nmultimodal learning, and vision foundation models by introducing them to the\nproblem, and familiarizing them with the current status of the progress while\nproviding promising directions for future research. A project associated with\nthis review and evaluation has been created at\nhttps://github.com/better-chao/perceptual_abilities_evaluation."}
{"id": "2504.09802", "pdf": "https://arxiv.org/pdf/2504.09802", "abs": "https://arxiv.org/abs/2504.09802", "authors": ["Wenrui Cai", "Chengyu Wang", "Junbing Yan", "Jun Huang", "Xiangzhong Fang"], "title": "Training Small Reasoning LLMs with Cognitive Preference Alignment", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The reasoning capabilities of large language models (LLMs), such as OpenAI's\no1 and DeepSeek-R1, have seen substantial advancements through deep thinking.\nHowever, these enhancements come with significant resource demands,\nunderscoring the need to explore strategies to train effective reasoning LLMs\nwith far fewer parameters. A critical challenge is that smaller models have\ndifferent capacities and cognitive trajectories than their larger counterparts.\nHence, direct distillation of chain-of-thought (CoT) results from large LLMs to\nsmaller ones can be sometimes ineffective and requires a huge amount of\nannotated data. In this paper, we introduce a novel framework called\nCritique-Rethink-Verify (CRV), designed for training smaller yet powerful\nreasoning LLMs. Our CRV framework consists of multiple LLM agents, each\nspecializing in unique abilities: (i) critiquing the CoTs according to the\ncognitive capabilities of smaller models, (ii) rethinking and refining these\nCoTs based on the critiques, and (iii) verifying the correctness of the refined\nresults. We further propose the cognitive preference optimization (CogPO)\nalgorithm to enhance the reasoning abilities of smaller models by aligning\nthoughts of these models with their cognitive capacities. Comprehensive\nevaluations on challenging reasoning benchmarks demonstrate the efficacy of CRV\nand CogPO, which outperforms other training methods by a large margin."}
{"id": "2504.09491", "pdf": "https://arxiv.org/pdf/2504.09491", "abs": "https://arxiv.org/abs/2504.09491", "authors": ["Yexing Xu", "Longguang Wang", "Minglin Chen", "Sheng Ao", "Li Li", "Yulan Guo"], "title": "DropoutGS: Dropping Out Gaussians for Better Sparse-view Rendering", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Although 3D Gaussian Splatting (3DGS) has demonstrated promising results in\nnovel view synthesis, its performance degrades dramatically with sparse inputs\nand generates undesirable artifacts. As the number of training views decreases,\nthe novel view synthesis task degrades to a highly under-determined problem\nsuch that existing methods suffer from the notorious overfitting issue.\nInterestingly, we observe that models with fewer Gaussian primitives exhibit\nless overfitting under sparse inputs. Inspired by this observation, we propose\na Random Dropout Regularization (RDR) to exploit the advantages of\nlow-complexity models to alleviate overfitting. In addition, to remedy the lack\nof high-frequency details for these models, an Edge-guided Splitting Strategy\n(ESS) is developed. With these two techniques, our method (termed DropoutGS)\nprovides a simple yet effective plug-in approach to improve the generalization\nperformance of existing 3DGS methods. Extensive experiments show that our\nDropoutGS produces state-of-the-art performance under sparse views on benchmark\ndatasets including Blender, LLFF, and DTU. The project page is at:\nhttps://xuyx55.github.io/DropoutGS/."}
{"id": "2504.09818", "pdf": "https://arxiv.org/pdf/2504.09818", "abs": "https://arxiv.org/abs/2504.09818", "authors": ["Rong Yao", "Hailin Hu", "Yifei Fu", "Hanting Chen", "Wenyi Fang", "Fanyi Du", "Kai Han", "Yunhe Wang"], "title": "Transferable text data distillation by trajectory matching", "categories": ["cs.CL"], "comment": null, "summary": "In the realm of large language model (LLM), as the size of large models\nincreases, it also brings higher training costs. There is a urgent need to\nminimize the data size in LLM training. Compared with data selection method,\nthe data distillation method aims to synthesize a small number of data samples\nto achieve the training effect of the full data set and has better flexibility.\nDespite its successes in computer vision, the discreteness of text data has\nhitherto stymied its exploration in natural language processing (NLP). In this\nwork, we proposed a method that involves learning pseudo prompt data based on\ntrajectory matching and finding its nearest neighbor ID to achieve\ncross-architecture transfer. During the distillation process, we introduce a\nregularization loss to improve the robustness of our distilled data. To our\nbest knowledge, this is the first data distillation work suitable for text\ngeneration tasks such as instruction tuning. Evaluations on two benchmarks,\nincluding ARC-Easy and MMLU instruction tuning datasets, established the\nsuperiority of our distillation approach over the SOTA data selection method\nLESS. Furthermore, our method demonstrates a good transferability over LLM\nstructures (i.e., OPT to Llama)."}
{"id": "2504.09498", "pdf": "https://arxiv.org/pdf/2504.09498", "abs": "https://arxiv.org/abs/2504.09498", "authors": ["Yue Yang", "Christoph Leuze", "Brian Hargreaves", "Bruce Daniel", "Fred Baik"], "title": "EasyREG: Easy Depth-Based Markerless Registration and Tracking using Augmented Reality Device for Surgical Guidance", "categories": ["cs.CV"], "comment": null, "summary": "The use of Augmented Reality (AR) devices for surgical guidance has gained\nincreasing traction in the medical field. Traditional registration methods\noften rely on external fiducial markers to achieve high accuracy and real-time\nperformance. However, these markers introduce cumbersome calibration procedures\nand can be challenging to deploy in clinical settings. While commercial\nsolutions have attempted real-time markerless tracking using the native RGB\ncameras of AR devices, their accuracy remains questionable for medical\nguidance, primarily due to occlusions and significant outliers between the live\nsensor data and the preoperative target anatomy point cloud derived from MRI or\nCT scans. In this work, we present a markerless framework that relies only on\nthe depth sensor of AR devices and consists of two modules: a registration\nmodule for high-precision, outlier-robust target anatomy localization, and a\ntracking module for real-time pose estimation. The registration module\nintegrates depth sensor error correction, a human-in-the-loop region filtering\ntechnique, and a robust global alignment with curvature-aware feature sampling,\nfollowed by local ICP refinement, for markerless alignment of preoperative\nmodels with patient anatomy. The tracking module employs a fast and robust\nregistration algorithm that uses the initial pose from the registration module\nto estimate the target pose in real-time. We comprehensively evaluated the\nperformance of both modules through simulation and real-world measurements. The\nresults indicate that our markerless system achieves superior performance for\nregistration and comparable performance for tracking to industrial solutions.\nThe two-module design makes our system a one-stop solution for surgical\nprocedures where the target anatomy moves or stays static during surgery."}
{"id": "2504.09824", "pdf": "https://arxiv.org/pdf/2504.09824", "abs": "https://arxiv.org/abs/2504.09824", "authors": ["Keyan Xu", "Dingzirui Wang", "Xuanliang Zhang", "Qingfu Zhu", "Wanxiang Che"], "title": "Abacus-SQL: A Text-to-SQL System Empowering Cross-Domain and Open-Domain Database Retrieval", "categories": ["cs.CL"], "comment": "11 pages, 3figures", "summary": "The existing text-to-SQL systems have made significant progress in SQL query\ngeneration, but they still face numerous challenges. Existing systems often\nlack retrieval capabilities for open-domain databases, requiring users to\nmanually filter relevant databases. Additionally, their cross-domain\ntransferability is limited, making it challenging to accommodate diverse query\nrequirements. To address these issues, we propose Abacus-SQL. Abacus-SQL\nutilizes database retrieval technology to accurately locate the required\ndatabases in an open-domain database environment. It also enhances the system\ncross-domain transfer ability through data augmentation methods. Moreover,\nAbacus-SQL employs Pre-SQL and Self-debug methods, thereby enhancing the\naccuracy of SQL queries. Experimental results demonstrate that Abacus-SQL\nperforms excellently in multi-turn text-to-SQL tasks, effectively validating\nthe approach's effectiveness. Abacus-SQL is publicly accessible at\nhttps://huozi.8wss.com/abacus-sql/."}
{"id": "2504.09502", "pdf": "https://arxiv.org/pdf/2504.09502", "abs": "https://arxiv.org/abs/2504.09502", "authors": ["Pengfei Wang", "Hao Zheng", "Zhigang Hu", "Aikun Xu", "Meiguang Zheng", "Liu Yang"], "title": "PCM-SAR: Physics-Driven Contrastive Mutual Learning for SAR Classification", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Existing SAR image classification methods based on Contrastive Learning often\nrely on sample generation strategies designed for optical images, failing to\ncapture the distinct semantic and physical characteristics of SAR data. To\naddress this, we propose Physics-Driven Contrastive Mutual Learning for SAR\nClassification (PCM-SAR), which incorporates domain-specific physical insights\nto improve sample generation and feature extraction. PCM-SAR utilizes the\ngray-level co-occurrence matrix (GLCM) to simulate realistic noise patterns and\napplies semantic detection for unsupervised local sampling, ensuring generated\nsamples accurately reflect SAR imaging properties. Additionally, a multi-level\nfeature fusion mechanism based on mutual learning enables collaborative\nrefinement of feature representations. Notably, PCM-SAR significantly enhances\nsmaller models by refining SAR feature representations, compensating for their\nlimited capacity. Experimental results show that PCM-SAR consistently\noutperforms SOTA methods across diverse datasets and SAR classification tasks."}
{"id": "2504.09866", "pdf": "https://arxiv.org/pdf/2504.09866", "abs": "https://arxiv.org/abs/2504.09866", "authors": ["Ziyu Zhuang"], "title": "PASS-FC: Progressive and Adaptive Search Scheme for Fact Checking of Comprehensive Claims", "categories": ["cs.CL"], "comment": null, "summary": "Automated fact-checking faces challenges in handling complex real-world\nclaims. We present PASS-FC, a novel framework that addresses these issues\nthrough claim augmentation, adaptive question generation, and iterative\nverification. PASS-FC enhances atomic claims with temporal and entity context,\nemploys advanced search techniques, and utilizes a reflection mechanism. We\nevaluate PASS-FC on six diverse datasets, demonstrating superior performance\nacross general knowledge, scientific, real-world, and multilingual\nfact-checking tasks. Our framework often surpasses stronger baseline models.\nHyperparameter analysis reveals optimal settings for evidence quantity and\nreflection label triggers, while ablation studies highlight the importance of\nclaim augmentation and language-specific adaptations. PASS-FC's performance\nunderscores its effectiveness in improving fact-checking accuracy and\nadaptability across various domains. We will open-source our code and\nexperimental results to facilitate further research in this area."}
{"id": "2504.09506", "pdf": "https://arxiv.org/pdf/2504.09506", "abs": "https://arxiv.org/abs/2504.09506", "authors": ["Yanze Jiang", "Yanfeng Gu", "Xian Li"], "title": "Pillar-Voxel Fusion Network for 3D Object Detection in Airborne Hyperspectral Point Clouds", "categories": ["cs.CV"], "comment": null, "summary": "Hyperspectral point clouds (HPCs) can simultaneously characterize 3D spatial\nand spectral information of ground objects, offering excellent 3D perception\nand target recognition capabilities. Current approaches for generating HPCs\noften involve fusion techniques with hyperspectral images and LiDAR point\nclouds, which inevitably lead to geometric-spectral distortions due to fusion\nerrors and obstacle occlusions. These adverse effects limit their performance\nin downstream fine-grained tasks across multiple scenarios, particularly in\nairborne applications. To address these issues, we propose PiV-AHPC, a 3D\nobject detection network for airborne HPCs. To the best of our knowledge, this\nis the first attempt at this HPCs task. Specifically, we first develop a\npillar-voxel dual-branch encoder, where the former captures spectral and\nvertical structural features from HPCs to overcome spectral distortion, while\nthe latter emphasizes extracting accurate 3D spatial features from point\nclouds. A multi-level feature fusion mechanism is devised to enhance\ninformation interaction between the two branches, achieving neighborhood\nfeature alignment and channel-adaptive selection, thereby organically\nintegrating heterogeneous features and mitigating geometric distortion.\nExtensive experiments on two airborne HPCs datasets demonstrate that PiV-AHPC\npossesses state-of-the-art detection performance and high generalization\ncapability."}
{"id": "2504.09886", "pdf": "https://arxiv.org/pdf/2504.09886", "abs": "https://arxiv.org/abs/2504.09886", "authors": ["Michael Kamerath", "Aniello De Santo"], "title": "Investigating Syntactic Biases in Multilingual Transformers with RC Attachment Ambiguities in Italian and English", "categories": ["cs.CL"], "comment": null, "summary": "This paper leverages past sentence processing studies to investigate whether\nmonolingual and multilingual LLMs show human-like preferences when presented\nwith examples of relative clause attachment ambiguities in Italian and English.\nFurthermore, we test whether these preferences can be modulated by lexical\nfactors (the type of verb/noun in the matrix clause) which have been shown to\nbe tied to subtle constraints on syntactic and semantic relations. Our results\noverall showcase how LLM behavior varies interestingly across models, but also\ngeneral failings of these models in correctly capturing human-like preferences.\nIn light of these results, we argue that RC attachment is the ideal benchmark\nfor cross-linguistic investigations of LLMs' linguistic knowledge and biases."}
{"id": "2504.09507", "pdf": "https://arxiv.org/pdf/2504.09507", "abs": "https://arxiv.org/abs/2504.09507", "authors": ["Mengjiao Wang", "Junpei Zhang", "Xu Liu", "Yuting Yang", "Mengru Ma"], "title": "FVOS for MOSE Track of 4th PVUW Challenge: 3rd Place Solution", "categories": ["cs.CV"], "comment": "5 pages, 3 figures", "summary": "Video Object Segmentation (VOS) is one of the most fundamental and\nchallenging tasks in computer vision and has a wide range of applications. Most\nexisting methods rely on spatiotemporal memory networks to extract frame-level\nfeatures and have achieved promising results on commonly used datasets.\nHowever, these methods often struggle in more complex real-world scenarios.\nThis paper addresses this issue, aiming to achieve accurate segmentation of\nvideo objects in challenging scenes. We propose fine-tuning VOS (FVOS),\noptimizing existing methods for specific datasets through tailored training.\nAdditionally, we introduce a morphological post-processing strategy to address\nthe issue of excessively large gaps between adjacent objects in single-model\npredictions. Finally, we apply a voting-based fusion method on multi-scale\nsegmentation results to generate the final output. Our approach achieves J&F\nscores of 76.81% and 83.92% during the validation and testing stages,\nrespectively, securing third place overall in the MOSE Track of the 4th PVUW\nchallenge 2025."}
{"id": "2504.09895", "pdf": "https://arxiv.org/pdf/2504.09895", "abs": "https://arxiv.org/abs/2504.09895", "authors": ["Shuai Zhao", "Linchao Zhu", "Yi Yang"], "title": "Learning from Reference Answers: Versatile Language Model Alignment without Binary Human Preference Data", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "work in progress", "summary": "Large language models~(LLMs) are expected to be helpful, harmless, and\nhonest. In various alignment scenarios, such as general human preference,\nsafety, and confidence alignment, binary preference data collection and reward\nmodeling are resource-intensive but necessary for human preference\ntransferring. In this work, we explore using the similarity between sampled\ngenerations and high-quality reference answers as an alternative reward\nfunction for LLM alignment. Using similarity as a reward circumvents training\nreward models, and collecting a single reference answer potentially costs less\ntime than constructing binary preference pairs when multiple candidates are\navailable. Specifically, we develop \\textit{RefAlign}, a versatile\nREINFORCE-style alignment algorithm, which is free of reference and reward\nmodels. Instead, RefAlign utilizes BERTScore between sampled generations and\nhigh-quality reference answers as the surrogate reward. Beyond general human\npreference optimization, RefAlign can be readily extended to diverse scenarios,\nsuch as safety and confidence alignment, by incorporating the similarity reward\nwith task-related objectives. In various scenarios, {RefAlign} demonstrates\ncomparable performance to previous alignment methods while offering high\nefficiency."}
{"id": "2504.09513", "pdf": "https://arxiv.org/pdf/2504.09513", "abs": "https://arxiv.org/abs/2504.09513", "authors": ["Puyu Han", "Jiaju Kang", "Yuhang Pan", "Erting Pan", "Zeyu Zhang", "Qunchao Jin", "Juntao Jiang", "Zhichen Liu", "Luqi Gong"], "title": "DiffuMural: Restoring Dunhuang Murals with Multi-scale Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "Large-scale pre-trained diffusion models have produced excellent results in\nthe field of conditional image generation. However, restoration of ancient\nmurals, as an important downstream task in this field, poses significant\nchallenges to diffusion model-based restoration methods due to its large\ndefective area and scarce training samples. Conditional restoration tasks are\nmore concerned with whether the restored part meets the aesthetic standards of\nmural restoration in terms of overall style and seam detail, and such metrics\nfor evaluating heuristic image complements are lacking in current research. We\ntherefore propose DiffuMural, a combined Multi-scale convergence and\nCollaborative Diffusion mechanism with ControlNet and cyclic consistency loss\nto optimise the matching between the generated images and the conditional\ncontrol. DiffuMural demonstrates outstanding capabilities in mural restoration,\nleveraging training data from 23 large-scale Dunhuang murals that exhibit\nconsistent visual aesthetics. The model excels in restoring intricate details,\nachieving a coherent overall appearance, and addressing the unique challenges\nposed by incomplete murals lacking factual grounding. Our evaluation framework\nincorporates four key metrics to quantitatively assess incomplete murals:\nfactual accuracy, textural detail, contextual semantics, and holistic visual\ncoherence. Furthermore, we integrate humanistic value assessments to ensure the\nrestored murals retain their cultural and artistic significance. Extensive\nexperiments validate that our method outperforms state-of-the-art (SOTA)\napproaches in both qualitative and quantitative metrics."}
{"id": "2504.09896", "pdf": "https://arxiv.org/pdf/2504.09896", "abs": "https://arxiv.org/abs/2504.09896", "authors": ["Aish Albladi", "Md Kaosar Uddin", "Minarul Islam", "Cheryl Seals"], "title": "TWSSenti: A Novel Hybrid Framework for Topic-Wise Sentiment Analysis on Social Media Using Transformer Models", "categories": ["cs.CL"], "comment": "41 pages, 12 figures, includes algorithm and comparative tables", "summary": "Sentiment analysis is a crucial task in natural language processing (NLP)\nthat enables the extraction of meaningful insights from textual data,\nparticularly from dynamic platforms like Twitter and IMDB. This study explores\na hybrid framework combining transformer-based models, specifically BERT,\nGPT-2, RoBERTa, XLNet, and DistilBERT, to improve sentiment classification\naccuracy and robustness. The framework addresses challenges such as noisy data,\ncontextual ambiguity, and generalization across diverse datasets by leveraging\nthe unique strengths of these models. BERT captures bidirectional context,\nGPT-2 enhances generative capabilities, RoBERTa optimizes contextual\nunderstanding with larger corpora and dynamic masking, XLNet models dependency\nthrough permutation-based learning, and DistilBERT offers efficiency with\nreduced computational overhead while maintaining high accuracy. We demonstrate\ntext cleaning, tokenization, and feature extraction using Term Frequency\nInverse Document Frequency (TF-IDF) and Bag of Words (BoW), ensure high-quality\ninput data for the models. The hybrid approach was evaluated on benchmark\ndatasets Sentiment140 and IMDB, achieving superior accuracy rates of 94\\% and\n95\\%, respectively, outperforming standalone models. The results validate the\neffectiveness of combining multiple transformer models in ensemble-like setups\nto address the limitations of individual architectures. This research\nhighlights its applicability to real-world tasks such as social media\nmonitoring, customer sentiment analysis, and public opinion tracking which\noffers a pathway for future advancements in hybrid NLP frameworks."}
{"id": "2504.09514", "pdf": "https://arxiv.org/pdf/2504.09514", "abs": "https://arxiv.org/abs/2504.09514", "authors": ["Aisha L. Shuaibu", "Kieran A. Gibb", "Peter A. Wijeratne", "Ivor J. A. Simpson"], "title": "Capturing Longitudinal Changes in Brain Morphology Using Temporally Parameterized Neural Displacement Fields", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted for publication at Medical Imaging with Deep Learning (MIDL)", "summary": "Longitudinal image registration enables studying temporal changes in brain\nmorphology which is useful in applications where monitoring the growth or\natrophy of specific structures is important. However this task is challenging\ndue to; noise/artifacts in the data and quantifying small anatomical changes\nbetween sequential scans. We propose a novel longitudinal registration method\nthat models structural changes using temporally parameterized neural\ndisplacement fields. Specifically, we implement an implicit neural\nrepresentation (INR) using a multi-layer perceptron that serves as a continuous\ncoordinate-based approximation of the deformation field at any time point. In\neffect, for any N scans of a particular subject, our model takes as input a 3D\nspatial coordinate location x, y, z and a corresponding temporal representation\nt and learns to describe the continuous morphology of structures for both\nobserved and unobserved points in time. Furthermore, we leverage the analytic\nderivatives of the INR to derive a new regularization function that enforces\nmonotonic rate of change in the trajectory of the voxels, which is shown to\nprovide more biologically plausible patterns. We demonstrate the effectiveness\nof our method on 4D brain MR registration."}
{"id": "2504.09903", "pdf": "https://arxiv.org/pdf/2504.09903", "abs": "https://arxiv.org/abs/2504.09903", "authors": ["Bo-Wei Chen", "An-Zi Yen", "Chung-Chi Chen"], "title": "Refining Financial Consumer Complaints through Multi-Scale Model Interaction", "categories": ["cs.CL"], "comment": null, "summary": "Legal writing demands clarity, formality, and domain-specific\nprecision-qualities often lacking in documents authored by individuals without\nlegal training. To bridge this gap, this paper explores the task of legal text\nrefinement that transforms informal, conversational inputs into persuasive\nlegal arguments. We introduce FinDR, a Chinese dataset of financial dispute\nrecords, annotated with official judgments on claim reasonableness. Our\nproposed method, Multi-Scale Model Interaction (MSMI), leverages a lightweight\nclassifier to evaluate outputs and guide iterative refinement by Large Language\nModels (LLMs). Experimental results demonstrate that MSMI significantly\noutperforms single-pass prompting strategies. Additionally, we validate the\ngeneralizability of MSMI on several short-text benchmarks, showing improved\nadversarial robustness. Our findings reveal the potential of multi-model\ncollaboration for enhancing legal document generation and broader text\nrefinement tasks."}
{"id": "2504.09518", "pdf": "https://arxiv.org/pdf/2504.09518", "abs": "https://arxiv.org/abs/2504.09518", "authors": ["Ting Huang", "Zeyu Zhang", "Yemin Wang", "Hao Tang"], "title": "3D CoCa: Contrastive Learners are 3D Captioners", "categories": ["cs.CV"], "comment": null, "summary": "3D captioning, which aims to describe the content of 3D scenes in natural\nlanguage, remains highly challenging due to the inherent sparsity of point\nclouds and weak cross-modal alignment in existing methods. To address these\nchallenges, we propose 3D CoCa, a novel unified framework that seamlessly\ncombines contrastive vision-language learning with 3D caption generation in a\nsingle architecture. Our approach leverages a frozen CLIP vision-language\nbackbone to provide rich semantic priors, a spatially-aware 3D scene encoder to\ncapture geometric context, and a multi-modal decoder to generate descriptive\ncaptions. Unlike prior two-stage methods that rely on explicit object\nproposals, 3D CoCa jointly optimizes contrastive and captioning objectives in a\nshared feature space, eliminating the need for external detectors or\nhandcrafted proposals. This joint training paradigm yields stronger spatial\nreasoning and richer semantic grounding by aligning 3D and textual\nrepresentations. Extensive experiments on the ScanRefer and Nr3D benchmarks\ndemonstrate that 3D CoCa significantly outperforms current state-of-the-arts by\n10.2% and 5.76% in CIDEr at 0.5IoU, respectively. Code will be available at\nhttps://github.com/AIGeeksGroup/3DCoCa."}
{"id": "2504.09909", "pdf": "https://arxiv.org/pdf/2504.09909", "abs": "https://arxiv.org/abs/2504.09909", "authors": ["Farha Nausheen", "Khandakar Ahmed", "M Imad Khan"], "title": "Quantum Natural Language Processing: A Comprehensive Review of Models, Methods, and Applications", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In recent developments, deep learning methodologies applied to Natural\nLanguage Processing (NLP) have revealed a paradox: They improve performance but\ndemand considerable data and resources for their training. Alternatively,\nquantum computing exploits the principles of quantum mechanics to overcome the\ncomputational limitations of current methodologies, thereby establishing an\nemerging field known as quantum natural language processing (QNLP). This domain\nholds the potential to attain a quantum advantage in the processing of\nlinguistic structures, surpassing classical models in both efficiency and\naccuracy. In this paper, it is proposed to categorise QNLP models based on\nquantum computing principles, architecture, and computational approaches. This\npaper attempts to provide a survey on how quantum meets language by mapping\nstate-of-the-art in this area, embracing quantum encoding techniques for\nclassical data, QNLP models for prevalent NLP tasks, and quantum optimisation\ntechniques for hyper parameter tuning. The landscape of quantum computing\napproaches applied to various NLP tasks is summarised by showcasing the\nspecific QNLP methods used, and the popularity of these methods is indicated by\ntheir count. From the findings, it is observed that QNLP approaches are still\nlimited to small data sets, with only a few models explored extensively, and\nthere is increasing interest in the application of quantum computing to natural\nlanguage processing tasks."}
{"id": "2504.09528", "pdf": "https://arxiv.org/pdf/2504.09528", "abs": "https://arxiv.org/abs/2504.09528", "authors": ["Xing Zi", "Tengjun Ni", "Xianjing Fan", "Xian Tao", "Jun Li", "Ali Braytee", "Mukesh Prasad"], "title": "AeroLite: Tag-Guided Lightweight Generation of Aerial Image Captions", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Accurate and automated captioning of aerial imagery is crucial for\napplications like environmental monitoring, urban planning, and disaster\nmanagement. However, this task remains challenging due to complex spatial\nsemantics and domain variability. To address these issues, we introduce\n\\textbf{AeroLite}, a lightweight, tag-guided captioning framework designed to\nequip small-scale language models (1--3B parameters) with robust and\ninterpretable captioning capabilities specifically for remote sensing images.\n\\textbf{AeroLite} leverages GPT-4o to generate a large-scale, semantically rich\npseudo-caption dataset by integrating multiple remote sensing benchmarks,\nincluding DLRSD, iSAID, LoveDA, WHU, and RSSCN7. To explicitly capture key\nsemantic elements such as orientation and land-use types, AeroLite employs\nnatural language processing techniques to extract relevant semantic tags. These\ntags are then learned by a dedicated multi-label CLIP encoder, ensuring precise\nsemantic predictions. To effectively fuse visual and semantic information, we\npropose a novel bridging multilayer perceptron (MLP) architecture, aligning\nsemantic tags with visual embeddings while maintaining minimal computational\noverhead. AeroLite's flexible design also enables seamless integration with\nvarious pretrained large language models. We adopt a two-stage LoRA-based\ntraining approach: the initial stage leverages our pseudo-caption dataset to\ncapture broad remote sensing semantics, followed by fine-tuning on smaller,\ncurated datasets like UCM and Sydney Captions to refine domain-specific\nalignment. Experimental evaluations demonstrate that AeroLite surpasses\nsignificantly larger models (e.g., 13B parameters) in standard captioning\nmetrics, including BLEU and METEOR, while maintaining substantially lower\ncomputational costs."}
{"id": "2504.09910", "pdf": "https://arxiv.org/pdf/2504.09910", "abs": "https://arxiv.org/abs/2504.09910", "authors": ["Yujing Wang", "Hainan Zhang", "Liang Pang", "Yongxin Tong", "Binghui Guo", "Hongwei Zheng", "Zhiming Zheng"], "title": "Learning to Erase Private Knowledge from Multi-Documents for Retrieval-Augmented Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) is a promising technique for applying\nLLMs to proprietary domains. However, retrieved documents may contain sensitive\nknowledge, posing risks of privacy leakage in generative results. Thus,\neffectively erasing private information from retrieved documents is a key\nchallenge for RAG. Unlike traditional text anonymization, RAG should consider:\n(1) the inherent multi-document reasoning may face de-anonymization attacks;\n(2) private knowledge varies by scenarios, so users should be allowed to\ncustomize which information to erase; (3) preserving sufficient publicly\navailable knowledge for generation tasks. This paper introduces the privacy\nerasure task for RAG and proposes Eraser4RAG, a private knowledge eraser which\neffectively removes user-defined private knowledge from documents while\npreserving sufficient public knowledge for generation. Specifically, we first\nconstruct a global knowledge graph to identify potential knowledge across\ndocuments, aiming to defend against de-anonymization attacks. Then we randomly\nsplit it into private and public sub-graphs, and fine-tune Flan-T5 to rewrite\nthe retrieved documents excluding private triples. Finally, PPO algorithm\noptimizes the rewriting model to minimize private triples and maximize public\ntriples retention. Experiments on four QA datasets demonstrate that Eraser4RAG\nachieves superior erase performance than GPT-4o."}
{"id": "2504.09530", "pdf": "https://arxiv.org/pdf/2504.09530", "abs": "https://arxiv.org/abs/2504.09530", "authors": ["Shuchao Duan", "Amirhossein Dadashzadeh", "Alan Whone", "Majid Mirmehdi"], "title": "Trajectory-guided Motion Perception for Facial Expression Quality Assessment in Neurological Disorders", "categories": ["cs.CV"], "comment": "Accepted to IEEE FG 2025 (preprint)", "summary": "Automated facial expression quality assessment (FEQA) in neurological\ndisorders is critical for enhancing diagnostic accuracy and improving patient\ncare, yet effectively capturing the subtle motions and nuances of facial muscle\nmovements remains a challenge. We propose to analyse facial landmark\ntrajectories, a compact yet informative representation, that encodes these\nsubtle motions from a high-level structural perspective. Hence, we introduce\nTrajectory-guided Motion Perception Transformer (TraMP-Former), a novel FEQA\nframework that fuses landmark trajectory features for fine-grained motion\ncapture with visual semantic cues from RGB frames, ultimately regressing the\ncombined features into a quality score. Extensive experiments demonstrate that\nTraMP-Former achieves new state-of-the-art performance on benchmark datasets\nwith neurological disorders, including PFED5 (up by 6.51%) and an augmented\nToronto NeuroFace (up by 7.62%). Our ablation studies further validate the\nefficiency and effectiveness of landmark trajectories in FEQA. Our code is\navailable at https://github.com/shuchaoduan/TraMP-Former."}
{"id": "2504.09923", "pdf": "https://arxiv.org/pdf/2504.09923", "abs": "https://arxiv.org/abs/2504.09923", "authors": ["Yujin Kim", "Euiin Yi", "Minu Kim", "Se-Young Yun", "Taehyeon Kim"], "title": "Guiding Reasoning in Small Language Models with LLM Assistance", "categories": ["cs.CL"], "comment": "20 pages, 10 figures, 11 tables", "summary": "The limited reasoning capabilities of small language models (SLMs) cast doubt\non their suitability for tasks demanding deep, multi-step logical deduction.\nThis paper introduces a framework called Small Reasons, Large Hints (SMART),\nwhich selectively augments SLM reasoning with targeted guidance from large\nlanguage models (LLMs). Inspired by the concept of cognitive scaffolding, SMART\nemploys a score-based evaluation to identify uncertain reasoning steps and\ninjects corrective LLM-generated reasoning only when necessary. By framing\nstructured reasoning as an optimal policy search, our approach steers the\nreasoning trajectory toward correct solutions without exhaustive sampling. Our\nexperiments on mathematical reasoning datasets demonstrate that targeted\nexternal scaffolding significantly improves performance, paving the way for\ncollaborative use of both SLM and LLM to tackle complex reasoning tasks that\nare currently unsolvable by SLMs alone."}
{"id": "2504.09535", "pdf": "https://arxiv.org/pdf/2504.09535", "abs": "https://arxiv.org/abs/2504.09535", "authors": ["Yuting Zhao", "Yuheng Ji", "Xiaoshuai Hao", "Shuxiao Li"], "title": "FastRSR: Efficient and Accurate Road Surface Reconstruction from Bird's Eye View", "categories": ["cs.CV"], "comment": null, "summary": "Road Surface Reconstruction (RSR) is crucial for autonomous driving, enabling\nthe understanding of road surface conditions. Recently, RSR from the Bird's Eye\nView (BEV) has gained attention for its potential to enhance performance.\nHowever, existing methods for transforming perspective views to BEV face\nchallenges such as information loss and representation sparsity. Moreover,\nstereo matching in BEV is limited by the need to balance accuracy with\ninference speed. To address these challenges, we propose two efficient and\naccurate BEV-based RSR models: FastRSR-mono and FastRSR-stereo. Specifically,\nwe first introduce Depth-Aware Projection (DAP), an efficient view\ntransformation strategy designed to mitigate information loss and sparsity by\nquerying depth and image features to aggregate BEV data within specific road\nsurface regions using a pre-computed look-up table. To optimize accuracy and\nspeed in stereo matching, we design the Spatial Attention Enhancement (SAE) and\nConfidence Attention Generation (CAG) modules. SAE adaptively highlights\nimportant regions, while CAG focuses on high-confidence predictions and filters\nout irrelevant information. FastRSR achieves state-of-the-art performance,\nexceeding monocular competitors by over 6.0% in elevation absolute error and\nproviding at least a 3.0x speedup by stereo methods on the RSRD dataset. The\nsource code will be released."}
{"id": "2504.09958", "pdf": "https://arxiv.org/pdf/2504.09958", "abs": "https://arxiv.org/abs/2504.09958", "authors": ["Fuqiang Niu", "Yi Yang", "Xianghua Fu", "Genan Dai", "Bowen Zhang"], "title": "C-MTCSD: A Chinese Multi-Turn Conversational Stance Detection Dataset", "categories": ["cs.CL"], "comment": "WWW2025", "summary": "Stance detection has become an essential tool for analyzing public\ndiscussions on social media. Current methods face significant challenges,\nparticularly in Chinese language processing and multi-turn conversational\nanalysis. To address these limitations, we introduce C-MTCSD, the largest\nChinese multi-turn conversational stance detection dataset, comprising 24,264\ncarefully annotated instances from Sina Weibo, which is 4.2 times larger than\nthe only prior Chinese conversational stance detection dataset. Our\ncomprehensive evaluation using both traditional approaches and large language\nmodels reveals the complexity of C-MTCSD: even state-of-the-art models achieve\nonly 64.07% F1 score in the challenging zero-shot setting, while performance\nconsistently degrades with increasing conversation depth. Traditional models\nparticularly struggle with implicit stance detection, achieving below 50% F1\nscore. This work establishes a challenging new benchmark for Chinese stance\ndetection research, highlighting significant opportunities for future\nimprovements."}
{"id": "2504.09540", "pdf": "https://arxiv.org/pdf/2504.09540", "abs": "https://arxiv.org/abs/2504.09540", "authors": ["Hao Wang", "Xiaobao Wei", "Xiaoan Zhang", "Jianing Li", "Chengyu Bai", "Ying Li", "Ming Lu", "Wenzhao Zheng", "Shanghang Zhang"], "title": "EmbodiedOcc++: Boosting Embodied 3D Occupancy Prediction with Plane Regularization and Uncertainty Sampler", "categories": ["cs.CV"], "comment": null, "summary": "Online 3D occupancy prediction provides a comprehensive spatial understanding\nof embodied environments. While the innovative EmbodiedOcc framework utilizes\n3D semantic Gaussians for progressive indoor occupancy prediction, it overlooks\nthe geometric characteristics of indoor environments, which are primarily\ncharacterized by planar structures. This paper introduces EmbodiedOcc++,\nenhancing the original framework with two key innovations: a Geometry-guided\nRefinement Module (GRM) that constrains Gaussian updates through plane\nregularization, along with a Semantic-aware Uncertainty Sampler (SUS) that\nenables more effective updates in overlapping regions between consecutive\nframes. GRM regularizes the position update to align with surface normals. It\ndetermines the adaptive regularization weight using curvature-based and\ndepth-based constraints, allowing semantic Gaussians to align accurately with\nplanar surfaces while adapting in complex regions. To effectively improve\ngeometric consistency from different views, SUS adaptively selects proper\nGaussians to update. Comprehensive experiments on the EmbodiedOcc-ScanNet\nbenchmark demonstrate that EmbodiedOcc++ achieves state-of-the-art performance\nacross different settings. Our method demonstrates improved edge accuracy and\nretains more geometric details while ensuring computational efficiency, which\nis essential for online embodied perception. The code will be released at:\nhttps://github.com/PKUHaoWang/EmbodiedOcc2."}
{"id": "2504.09980", "pdf": "https://arxiv.org/pdf/2504.09980", "abs": "https://arxiv.org/abs/2504.09980", "authors": ["Anneliese Kelterer", "Barbara Schuppler"], "title": "Turn-taking annotation for quantitative and qualitative analyses of conversation", "categories": ["cs.CL", "cs.DB", "cs.HC", "eess.AS"], "comment": "41 pages", "summary": "This paper has two goals. First, we present the turn-taking annotation layers\ncreated for 95 minutes of conversational speech of the Graz Corpus of Read and\nSpontaneous Speech (GRASS), available to the scientific community. Second, we\ndescribe the annotation system and the annotation process in more detail, so\nother researchers may use it for their own conversational data. The annotation\nsystem was developed with an interdisciplinary application in mind. It should\nbe based on sequential criteria according to Conversation Analysis, suitable\nfor subsequent phonetic analysis, thus time-aligned annotations were made\nPraat, and it should be suitable for automatic classification, which required\nthe continuous annotation of speech and a label inventory that is not too large\nand results in a high inter-rater agreement. Turn-taking was annotated on two\nlayers, Inter-Pausal Units (IPU) and points of potential completion (PCOMP;\nsimilar to transition relevance places). We provide a detailed description of\nthe annotation process and of segmentation and labelling criteria. A detailed\nanalysis of inter-rater agreement and common confusions shows that agreement\nfor IPU annotation is near-perfect, that agreement for PCOMP annotations is\nsubstantial, and that disagreements often are either partial or can be\nexplained by a different analysis of a sequence which also has merit. The\nannotation system can be applied to a variety of conversational data for\nlinguistic studies and technological applications, and we hope that the\nannotations, as well as the annotation system will contribute to a stronger\ncross-fertilization between these disciplines."}
{"id": "2504.09549", "pdf": "https://arxiv.org/pdf/2504.09549", "abs": "https://arxiv.org/abs/2504.09549", "authors": ["Xiang Hu", "Pingping Zhang", "Yuhao Wang", "Bin Yan", "Huchuan Lu"], "title": "SD-ReID: View-aware Stable Diffusion for Aerial-Ground Person Re-Identification", "categories": ["cs.CV"], "comment": null, "summary": "Aerial-Ground Person Re-IDentification (AG-ReID) aims to retrieve specific\npersons across cameras with different viewpoints. Previous works focus on\ndesigning discriminative ReID models to maintain identity consistency despite\ndrastic changes in camera viewpoints. The core idea behind these methods is\nquite natural, but designing a view-robust network is a very challenging task.\nMoreover, they overlook the contribution of view-specific features in enhancing\nthe model's capability to represent persons. To address these issues, we\npropose a novel two-stage feature learning framework named SD-ReID for AG-ReID,\nwhich takes advantage of the powerful understanding capacity of generative\nmodels, e.g., Stable Diffusion (SD), to generate view-specific features between\ndifferent viewpoints. In the first stage, we train a simple ViT-based model to\nextract coarse-grained representations and controllable conditions. Then, in\nthe second stage, we fine-tune the SD model to learn complementary\nrepresentations guided by the controllable conditions. Furthermore, we propose\nthe View-Refine Decoder (VRD) to obtain additional controllable conditions to\ngenerate missing cross-view features. Finally, we use the coarse-grained\nrepresentations and all-view features generated by SD to retrieve target\npersons. Extensive experiments on the AG-ReID benchmarks demonstrate the\neffectiveness of our proposed SD-ReID. The source code will be available upon\nacceptance."}
{"id": "2504.10020", "pdf": "https://arxiv.org/pdf/2504.10020", "abs": "https://arxiv.org/abs/2504.10020", "authors": ["Hao Yin", "Gunagzong Si", "Zilei Wang"], "title": "The Mirage of Performance Gains: Why Contrastive Decoding Fails to Address Multimodal Hallucination", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Contrastive decoding strategies are widely used to reduce hallucinations in\nmultimodal large language models (MLLMs). These methods work by constructing\ncontrastive samples to induce hallucinations and then suppressing them in the\noutput distribution. However, this paper demonstrates that such approaches fail\nto effectively mitigate the hallucination problem. The performance improvements\nobserved on POPE Benchmark are largely driven by two misleading factors: (1)\ncrude, unidirectional adjustments to the model's output distribution and (2)\nthe adaptive plausibility constraint, which reduces the sampling strategy to\ngreedy search. To further illustrate these issues, we introduce a series of\nspurious improvement methods and evaluate their performance against contrastive\ndecoding techniques. Experimental results reveal that the observed performance\ngains in contrastive decoding are entirely unrelated to its intended goal of\nmitigating hallucinations. Our findings challenge common assumptions about the\neffectiveness of contrastive decoding strategies and pave the way for\ndeveloping genuinely effective solutions to hallucinations in MLLMs."}
{"id": "2504.09555", "pdf": "https://arxiv.org/pdf/2504.09555", "abs": "https://arxiv.org/abs/2504.09555", "authors": ["Jinhao Li", "Zijian Chen", "Runze Dong", "Tingzhu Chen", "Changbo Wang", "Guangtao Zhai"], "title": "Mitigating Long-tail Distribution in Oracle Bone Inscriptions: Dataset, Model, and Benchmark", "categories": ["cs.CV"], "comment": null, "summary": "The oracle bone inscription (OBI) recognition plays a significant role in\nunderstanding the history and culture of ancient China. However, the existing\nOBI datasets suffer from a long-tail distribution problem, leading to biased\nperformance of OBI recognition models across majority and minority classes.\nWith recent advancements in generative models, OBI synthesis-based data\naugmentation has become a promising avenue to expand the sample size of\nminority classes. Unfortunately, current OBI datasets lack large-scale\nstructure-aligned image pairs for generative model training. To address these\nproblems, we first present the Oracle-P15K, a structure-aligned OBI dataset for\nOBI generation and denoising, consisting of 14,542 images infused with domain\nknowledge from OBI experts. Second, we propose a diffusion model-based pseudo\nOBI generator, called OBIDiff, to achieve realistic and controllable OBI\ngeneration. Given a clean glyph image and a target rubbing-style image, it can\neffectively transfer the noise style of the original rubbing to the glyph\nimage. Extensive experiments on OBI downstream tasks and user preference\nstudies show the effectiveness of the proposed Oracle-P15K dataset and\ndemonstrate that OBIDiff can accurately preserve inherent glyph structures\nwhile transferring authentic rubbing styles effectively."}
{"id": "2504.10036", "pdf": "https://arxiv.org/pdf/2504.10036", "abs": "https://arxiv.org/abs/2504.10036", "authors": ["Zhengxuan Zhang", "Zhuowen Liang", "Yin Wu", "Teng Lin", "Yuyu Luo", "Nan Tang"], "title": "DataMosaic: Explainable and Verifiable Multi-Modal Data Analytics through Extract-Reason-Verify", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are transforming data analytics, but their\nwidespread adoption is hindered by two critical limitations: they are not\nexplainable (opaque reasoning processes) and not verifiable (prone to\nhallucinations and unchecked errors). While retrieval-augmented generation\n(RAG) improves accuracy by grounding LLMs in external data, it fails to address\nthe core challenges of trustworthy analytics - especially when processing\nnoisy, inconsistent, or multi-modal data (for example, text, tables, images).\nWe propose DataMosaic, a framework designed to make LLM-powered analytics both\nexplainable and verifiable. By dynamically extracting task-specific structures\n(for example, tables, graphs, trees) from raw data, DataMosaic provides\ntransparent, step-by-step reasoning traces and enables validation of\nintermediate results. Built on a multi-agent framework, DataMosaic orchestrates\nself-adaptive agents that align with downstream task requirements, enhancing\nconsistency, completeness, and privacy. Through this approach, DataMosaic not\nonly tackles the limitations of current LLM-powered analytics systems but also\nlays the groundwork for a new paradigm of grounded, accurate, and explainable\nmulti-modal data analytics."}
{"id": "2504.09588", "pdf": "https://arxiv.org/pdf/2504.09588", "abs": "https://arxiv.org/abs/2504.09588", "authors": ["Zhicong Wu", "Hongbin Xu", "Gang Xu", "Ping Nie", "Zhixin Yan", "Jinkai Zheng", "Liangqiong Qu", "Ming Li", "Liqiang Nie"], "title": "TextSplat: Text-Guided Semantic Fusion for Generalizable Gaussian Splatting", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in Generalizable Gaussian Splatting have enabled robust\n3D reconstruction from sparse input views by utilizing feed-forward Gaussian\nSplatting models, achieving superior cross-scene generalization. However, while\nmany methods focus on geometric consistency, they often neglect the potential\nof text-driven guidance to enhance semantic understanding, which is crucial for\naccurately reconstructing fine-grained details in complex scenes. To address\nthis limitation, we propose TextSplat--the first text-driven Generalizable\nGaussian Splatting framework. By employing a text-guided fusion of diverse\nsemantic cues, our framework learns robust cross-modal feature representations\nthat improve the alignment of geometric and semantic information, producing\nhigh-fidelity 3D reconstructions. Specifically, our framework employs three\nparallel modules to obtain complementary representations: the Diffusion Prior\nDepth Estimator for accurate depth information, the Semantic Aware Segmentation\nNetwork for detailed semantic information, and the Multi-View Interaction\nNetwork for refined cross-view features. Then, in the Text-Guided Semantic\nFusion Module, these representations are integrated via the text-guided and\nattention-based feature aggregation mechanism, resulting in enhanced 3D\nGaussian parameters enriched with detailed semantic cues. Experimental results\non various benchmark datasets demonstrate improved performance compared to\nexisting methods across multiple evaluation metrics, validating the\neffectiveness of our framework. The code will be publicly available."}
{"id": "2504.10063", "pdf": "https://arxiv.org/pdf/2504.10063", "abs": "https://arxiv.org/abs/2504.10063", "authors": ["Alexandra Bazarova", "Aleksandr Yugay", "Andrey Shulga", "Alina Ermilova", "Andrei Volodichev", "Konstantin Polev", "Julia Belikova", "Rauf Parchiev", "Dmitry Simakov", "Maxim Savchenko", "Andrey Savchenko", "Serguei Barannikov", "Alexey Zaytsev"], "title": "Hallucination Detection in LLMs via Topological Divergence on Attention Graphs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Hallucination, i.e., generating factually incorrect content, remains a\ncritical challenge for large language models (LLMs). We introduce TOHA, a\nTOpology-based HAllucination detector in the RAG setting, which leverages a\ntopological divergence metric to quantify the structural properties of graphs\ninduced by attention matrices. Examining the topological divergence between\nprompt and response subgraphs reveals consistent patterns: higher divergence\nvalues in specific attention heads correlate with hallucinated outputs,\nindependent of the dataset. Extensive experiments, including evaluation on\nquestion answering and data-to-text tasks, show that our approach achieves\nstate-of-the-art or competitive results on several benchmarks, two of which\nwere annotated by us and are being publicly released to facilitate further\nresearch. Beyond its strong in-domain performance, TOHA maintains remarkable\ndomain transferability across multiple open-source LLMs. Our findings suggest\nthat analyzing the topological structure of attention matrices can serve as an\nefficient and robust indicator of factual reliability in LLMs."}
{"id": "2504.09598", "pdf": "https://arxiv.org/pdf/2504.09598", "abs": "https://arxiv.org/abs/2504.09598", "authors": ["Yining Zhao", "Ali Braytee", "Mukesh Prasad"], "title": "DualPrompt-MedCap: A Dual-Prompt Enhanced Approach for Medical Image Captioning", "categories": ["cs.CV", "I.2.10; I.4.8; J.3"], "comment": "11 pages, 4 figures, 2 tables", "summary": "Medical image captioning via vision-language models has shown promising\npotential for clinical diagnosis assistance. However, generating contextually\nrelevant descriptions with accurate modality recognition remains challenging.\nWe present DualPrompt-MedCap, a novel dual-prompt enhancement framework that\naugments Large Vision-Language Models (LVLMs) through two specialized\ncomponents: (1) a modality-aware prompt derived from a semi-supervised\nclassification model pretrained on medical question-answer pairs, and (2) a\nquestion-guided prompt leveraging biomedical language model embeddings. To\naddress the lack of captioning ground truth, we also propose an evaluation\nframework that jointly considers spatial-semantic relevance and medical\nnarrative quality. Experiments on multiple medical datasets demonstrate that\nDualPrompt-MedCap outperforms the baseline BLIP-3 by achieving a 22%\nimprovement in modality recognition accuracy while generating more\ncomprehensive and question-aligned descriptions. Our method enables the\ngeneration of clinically accurate reports that can serve as medical experts'\nprior knowledge and automatic annotations for downstream vision-language tasks."}
{"id": "2504.10065", "pdf": "https://arxiv.org/pdf/2504.10065", "abs": "https://arxiv.org/abs/2504.10065", "authors": ["Zeng Ren", "Xinyi Guan", "Martin Rohrmeier"], "title": "A Computational Cognitive Model for Processing Repetitions of Hierarchical Relations", "categories": ["cs.CL"], "comment": null, "summary": "Patterns are fundamental to human cognition, enabling the recognition of\nstructure and regularity across diverse domains. In this work, we focus on\nstructural repeats, patterns that arise from the repetition of hierarchical\nrelations within sequential data, and develop a candidate computational model\nof how humans detect and understand such structural repeats. Based on a\nweighted deduction system, our model infers the minimal generative process of a\ngiven sequence in the form of a Template program, a formalism that enriches the\ncontext-free grammar with repetition combinators. Such representation\nefficiently encodes the repetition of sub-computations in a recursive manner.\nAs a proof of concept, we demonstrate the expressiveness of our model on short\nsequences from music and action planning. The proposed model offers broader\ninsights into the mental representations and cognitive mechanisms underlying\nhuman pattern recognition."}
{"id": "2504.09601", "pdf": "https://arxiv.org/pdf/2504.09601", "abs": "https://arxiv.org/abs/2504.09601", "authors": ["Jia Wei", "Xiaoqi Zhao", "Jonghye Woo", "Jinsong Ouyang", "Georges El Fakhri", "Qingyu Chen", "Xiaofeng Liu"], "title": "Mixture-of-Shape-Experts (MoSE): End-to-End Shape Dictionary Framework to Prompt SAM for Generalizable Medical Segmentation", "categories": ["cs.CV", "cs.LG", "cs.MM", "eess.IV", "physics.med-ph"], "comment": "Accepted to CVPR 2025 workshop", "summary": "Single domain generalization (SDG) has recently attracted growing attention\nin medical image segmentation. One promising strategy for SDG is to leverage\nconsistent semantic shape priors across different imaging protocols, scanner\nvendors, and clinical sites. However, existing dictionary learning methods that\nencode shape priors often suffer from limited representational power with a\nsmall set of offline computed shape elements, or overfitting when the\ndictionary size grows. Moreover, they are not readily compatible with large\nfoundation models such as the Segment Anything Model (SAM). In this paper, we\npropose a novel Mixture-of-Shape-Experts (MoSE) framework that seamlessly\nintegrates the idea of mixture-of-experts (MoE) training into dictionary\nlearning to efficiently capture diverse and robust shape priors. Our method\nconceptualizes each dictionary atom as a shape expert, which specializes in\nencoding distinct semantic shape information. A gating network dynamically\nfuses these shape experts into a robust shape map, with sparse activation\nguided by SAM encoding to prevent overfitting. We further provide this shape\nmap as a prompt to SAM, utilizing the powerful generalization capability of SAM\nthrough bidirectional integration. All modules, including the shape dictionary,\nare trained in an end-to-end manner. Extensive experiments on multiple public\ndatasets demonstrate its effectiveness."}
{"id": "2504.10077", "pdf": "https://arxiv.org/pdf/2504.10077", "abs": "https://arxiv.org/abs/2504.10077", "authors": ["Abhinav Joshi", "Areeb Ahmad", "Divyaksh Shukla", "Ashutosh Modi"], "title": "Towards Quantifying Commonsense Reasoning with Mechanistic Insights", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at NAACL 2025; 28 pages (9 pages + 7 pages references + 12\n  pages appendix)", "summary": "Commonsense reasoning deals with the implicit knowledge that is well\nunderstood by humans and typically acquired via interactions with the world. In\nrecent times, commonsense reasoning and understanding of various LLMs have been\nevaluated using text-based tasks. In this work, we argue that a proxy of this\nunderstanding can be maintained as a graphical structure that can further help\nto perform a rigorous evaluation of commonsense reasoning abilities about\nvarious real-world activities. We create an annotation scheme for capturing\nthis implicit knowledge in the form of a graphical structure for 37 daily human\nactivities. We find that the created resource can be used to frame an enormous\nnumber of commonsense queries (~ 10^{17}), facilitating rigorous evaluation of\ncommonsense reasoning in LLMs. Moreover, recently, the remarkable performance\nof LLMs has raised questions about whether these models are truly capable of\nreasoning in the wild and, in general, how reasoning occurs inside these\nmodels. In this resource paper, we bridge this gap by proposing design\nmechanisms that facilitate research in a similar direction. Our findings\nsuggest that the reasoning components are localized in LLMs that play a\nprominent role in decision-making when prompted with a commonsense query."}
{"id": "2504.09606", "pdf": "https://arxiv.org/pdf/2504.09606", "abs": "https://arxiv.org/abs/2504.09606", "authors": ["Lexington Whalen", "Zhenbang Du", "Haoran You", "Chaojian Li", "Sixu Li", "Yingyan", "Lin"], "title": "Early-Bird Diffusion: Investigating and Leveraging Timestep-Aware Early-Bird Tickets in Diffusion Models for Efficient Training", "categories": ["cs.CV"], "comment": "10 pages, 5 figures. Accepted to the IEEE/CVF Conference on Computer\n  Vision and Pattern Recognition 2025", "summary": "Training diffusion models (DMs) requires substantial computational resources\ndue to multiple forward and backward passes across numerous timesteps,\nmotivating research into efficient training techniques. In this paper, we\npropose EB-Diff-Train, a new efficient DM training approach that is orthogonal\nto other methods of accelerating DM training, by investigating and leveraging\nEarly-Bird (EB) tickets -- sparse subnetworks that manifest early in the\ntraining process and maintain high generation quality.\n  We first investigate the existence of traditional EB tickets in DMs, enabling\ncompetitive generation quality without fully training a dense model.\n  Then, we delve into the concept of diffusion-dedicated EB tickets, drawing on\ninsights from varying importance of different timestep regions. These tickets\nadapt their sparsity levels according to the importance of corresponding\ntimestep regions, allowing for aggressive sparsity during non-critical regions\nwhile conserving computational resources for crucial timestep regions.\n  Building on this, we develop an efficient DM training technique that derives\ntimestep-aware EB tickets, trains them in parallel, and combines them during\ninference for image generation. Extensive experiments validate the existence of\nboth traditional and timestep-aware EB tickets, as well as the effectiveness of\nour proposed EB-Diff-Train method. This approach can significantly reduce\ntraining time both spatially and temporally -- achieving 2.9$\\times$ to\n5.8$\\times$ speedups over training unpruned dense models, and up to\n10.3$\\times$ faster training compared to standard train-prune-finetune\npipelines -- without compromising generative quality.\n  Our code is available at https://github.com/GATECH-EIC/Early-Bird-Diffusion."}
{"id": "2504.10157", "pdf": "https://arxiv.org/pdf/2504.10157", "abs": "https://arxiv.org/abs/2504.10157", "authors": ["Xinnong Zhang", "Jiayu Lin", "Xinyi Mou", "Shiyue Yang", "Xiawei Liu", "Libo Sun", "Hanjia Lyu", "Yihang Yang", "Weihong Qi", "Yue Chen", "Guanying Li", "Ling Yan", "Yao Hu", "Siming Chen", "Yu Wang", "Jingxuan Huang", "Jiebo Luo", "Shiping Tang", "Libo Wu", "Baohua Zhou", "Zhongyu Wei"], "title": "SocioVerse: A World Model for Social Simulation Powered by LLM Agents and A Pool of 10 Million Real-World Users", "categories": ["cs.CL", "cs.CY"], "comment": "work in progress", "summary": "Social simulation is transforming traditional social science research by\nmodeling human behavior through interactions between virtual individuals and\ntheir environments. With recent advances in large language models (LLMs), this\napproach has shown growing potential in capturing individual differences and\npredicting group behaviors. However, existing methods face alignment challenges\nrelated to the environment, target users, interaction mechanisms, and\nbehavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven\nworld model for social simulation. Our framework features four powerful\nalignment components and a user pool of 10 million real individuals. To\nvalidate its effectiveness, we conducted large-scale simulation experiments\nacross three distinct domains: politics, news, and economics. Results\ndemonstrate that SocioVerse can reflect large-scale population dynamics while\nensuring diversity, credibility, and representativeness through standardized\nprocedures and minimal manual adjustments."}
{"id": "2504.09608", "pdf": "https://arxiv.org/pdf/2504.09608", "abs": "https://arxiv.org/abs/2504.09608", "authors": ["Xingke Song", "Xiaoying Yang", "Chenglin Yao", "Jianfeng Ren", "Ruibin Bai", "Xin Chen", "Xudong Jiang"], "title": "ERL-MPP: Evolutionary Reinforcement Learning with Multi-head Puzzle Perception for Solving Large-scale Jigsaw Puzzles of Eroded Gaps", "categories": ["cs.CV"], "comment": "9 pages, 5 figures", "summary": "Solving jigsaw puzzles has been extensively studied. While most existing\nmodels focus on solving either small-scale puzzles or puzzles with no gap\nbetween fragments, solving large-scale puzzles with gaps presents distinctive\nchallenges in both image understanding and combinatorial optimization. To\ntackle these challenges, we propose a framework of Evolutionary Reinforcement\nLearning with Multi-head Puzzle Perception (ERL-MPP) to derive a better set of\nswapping actions for solving the puzzles. Specifically, to tackle the\nchallenges of perceiving the puzzle with gaps, a Multi-head Puzzle Perception\nNetwork (MPPN) with a shared encoder is designed, where multiple puzzlet heads\ncomprehensively perceive the local assembly status, and a discriminator head\nprovides a global assessment of the puzzle. To explore the large swapping\naction space efficiently, an Evolutionary Reinforcement Learning (EvoRL) agent\nis designed, where an actor recommends a set of suitable swapping actions from\na large action space based on the perceived puzzle status, a critic updates the\nactor using the estimated rewards and the puzzle status, and an evaluator\ncoupled with evolutionary strategies evolves the actions aligning with the\nhistorical assembly experience. The proposed ERL-MPP is comprehensively\nevaluated on the JPLEG-5 dataset with large gaps and the MIT dataset with\nlarge-scale puzzles. It significantly outperforms all state-of-the-art models\non both datasets."}
{"id": "2504.10160", "pdf": "https://arxiv.org/pdf/2504.10160", "abs": "https://arxiv.org/abs/2504.10160", "authors": ["Zhaopeng Feng", "Shaosheng Cao", "Jiahan Ren", "Jiayuan Su", "Ruizhe Chen", "Yan Zhang", "Zhe Xu", "Yao Hu", "Jian Wu", "Zuozhu Liu"], "title": "MT-R1-Zero: Advancing LLM-based Machine Translation via R1-Zero-like Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Work in progress. Our code is available at\n  https://github.com/fzp0424/MT-R1-Zero", "summary": "Large-scale reinforcement learning (RL) methods have proven highly effective\nin enhancing the reasoning abilities of large language models (LLMs),\nparticularly for tasks with verifiable solutions such as mathematics and\ncoding. However, applying this idea to machine translation (MT), where outputs\nare flexibly formatted and difficult to automatically evaluate with explicit\nrules, remains underexplored. In this work, we introduce MT-R1-Zero, the first\nopen-source adaptation of the R1-Zero RL framework for MT without supervised\nfine-tuning or cold-start. We propose a rule-metric mixed reward mechanism to\nguide LLMs towards improved translation quality via emergent reasoning. On the\nWMT 24 English-Chinese benchmark, our MT-R1-Zero-3B-Mix achieves competitive\nperformance, surpassing TowerInstruct-7B-v0.2 by an average of 1.26 points.\nMeanwhile, our MT-R1-Zero-7B-Mix attains a high average score of 62.25 across\nall metrics, placing it on par with advanced proprietary models such as GPT-4o\nand Claude-3.5-Sonnet, while the MT-R1-Zero-7B-Sem variant achieves\nstate-of-the-art scores on semantic metrics. Moreover, our work exhibits strong\ngeneralization capabilities on out-of-distribution MT tasks, robustly\nsupporting multilingual and low-resource settings. Extensive analysis of model\nbehavior across different initializations and reward metrics offers pioneering\ninsight into the critical role of reward design, LLM adaptability, training\ndynamics, and emergent reasoning patterns within the R1-Zero paradigm for MT.\nOur code is available at https://github.com/fzp0424/MT-R1-Zero."}
{"id": "2504.09621", "pdf": "https://arxiv.org/pdf/2504.09621", "abs": "https://arxiv.org/abs/2504.09621", "authors": ["Jiuchen Chen", "Xinyu Yan", "Qizhi Xu", "Kaiqi Li"], "title": "Tokenize Image Patches: Global Context Fusion for Effective Haze Removal in Large Images", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Global contextual information and local detail features are essential for\nhaze removal tasks. Deep learning models perform well on small, low-resolution\nimages, but they encounter difficulties with large, high-resolution ones due to\nGPU memory limitations. As a compromise, they often resort to image slicing or\ndownsampling. The former diminishes global information, while the latter\ndiscards high-frequency details. To address these challenges, we propose\nDehazeXL, a haze removal method that effectively balances global context and\nlocal feature extraction, enabling end-to-end modeling of large images on\nmainstream GPU hardware. Additionally, to evaluate the efficiency of global\ncontext utilization in haze removal performance, we design a visual attribution\nmethod tailored to the characteristics of haze removal tasks. Finally,\nrecognizing the lack of benchmark datasets for haze removal in large images, we\nhave developed an ultra-high-resolution haze removal dataset (8KDehaze) to\nsupport model training and testing. It includes 10000 pairs of clear and hazy\nremote sensing images, each sized at 8192 $\\times$ 8192 pixels. Extensive\nexperiments demonstrate that DehazeXL can infer images up to 10240 $\\times$\n10240 pixels with only 21 GB of memory, achieving state-of-the-art results\namong all evaluated methods. The source code and experimental dataset are\navailable at https://github.com/CastleChen339/DehazeXL."}
{"id": "2504.10167", "pdf": "https://arxiv.org/pdf/2504.10167", "abs": "https://arxiv.org/abs/2504.10167", "authors": ["Xu Zhang", "Zhifei Liu", "Jiahao Wang", "Huixuan Zhang", "Fan Xu", "Junzhe Zhang", "Xiaojun Wan"], "title": "C-FAITH: A Chinese Fine-Grained Benchmark for Automated Hallucination Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite the rapid advancement of large language models, they remain highly\nsusceptible to generating hallucinations, which significantly hinders their\nwidespread application. Hallucination research requires dynamic and\nfine-grained evaluation. However, most existing hallucination benchmarks\n(especially in Chinese language) rely on human annotations, making automatical\nand cost-effective hallucination evaluation challenging. To address this, we\nintroduce HaluAgent, an agentic framework that automatically constructs\nfine-grained QA dataset based on some knowledge documents. Our experiments\ndemonstrate that the manually designed rules and prompt optimization can\nimprove the quality of generated data. Using HaluAgent, we construct C-FAITH, a\nChinese QA hallucination benchmark created from 1,399 knowledge documents\nobtained from web scraping, totaling 60,702 entries. We comprehensively\nevaluate 16 mainstream LLMs with our proposed C-FAITH, providing detailed\nexperimental results and analysis."}
{"id": "2504.09623", "pdf": "https://arxiv.org/pdf/2504.09623", "abs": "https://arxiv.org/abs/2504.09623", "authors": ["Atharv Mahesh Mane", "Dulanga Weerakoon", "Vigneshwaran Subbaraju", "Sougata Sen", "Sanjay E. Sarma", "Archan Misra"], "title": "Ges3ViG: Incorporating Pointing Gestures into Language-Based 3D Visual Grounding for Embodied Reference Understanding", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "Accepted to the IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2025", "summary": "3-Dimensional Embodied Reference Understanding (3D-ERU) combines a language\ndescription and an accompanying pointing gesture to identify the most relevant\ntarget object in a 3D scene. Although prior work has explored pure\nlanguage-based 3D grounding, there has been limited exploration of 3D-ERU,\nwhich also incorporates human pointing gestures. To address this gap, we\nintroduce a data augmentation framework-Imputer, and use it to curate a new\nbenchmark dataset-ImputeRefer for 3D-ERU, by incorporating human pointing\ngestures into existing 3D scene datasets that only contain language\ninstructions. We also propose Ges3ViG, a novel model for 3D-ERU that achieves\n~30% improvement in accuracy as compared to other 3D-ERU models and ~9%\ncompared to other purely language-based 3D grounding models. Our code and\ndataset are available at https://github.com/AtharvMane/Ges3ViG."}
{"id": "2504.10168", "pdf": "https://arxiv.org/pdf/2504.10168", "abs": "https://arxiv.org/abs/2504.10168", "authors": ["Mohamed A. Abdallah", "Samhaa R. El-Beltagy"], "title": "HalluSearch at SemEval-2025 Task 3: A Search-Enhanced RAG Pipeline for Hallucination Detection", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this paper, we present HalluSearch, a multilingual pipeline designed to\ndetect fabricated text spans in Large Language Model (LLM) outputs. Developed\nas part of Mu-SHROOM, the Multilingual Shared-task on Hallucinations and\nRelated Observable Overgeneration Mistakes, HalluSearch couples\nretrieval-augmented verification with fine-grained factual splitting to\nidentify and localize hallucinations in fourteen different languages. Empirical\nevaluations show that HalluSearch performs competitively, placing fourth in\nboth English (within the top ten percent) and Czech. While the system's\nretrieval-based strategy generally proves robust, it faces challenges in\nlanguages with limited online coverage, underscoring the need for further\nresearch to ensure consistent hallucination detection across diverse linguistic\ncontexts."}
{"id": "2504.09641", "pdf": "https://arxiv.org/pdf/2504.09641", "abs": "https://arxiv.org/abs/2504.09641", "authors": ["Xingjian Zhang", "Siwei Wen", "Wenjun Wu", "Lei Huang"], "title": "TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Recently, improving the reasoning ability of large multimodal models (LMMs)\nthrough reinforcement learning has made great progress. However, most existing\nworks are based on highly reasoning-intensive datasets such as mathematics and\ncode, and researchers generally choose large-scale models as the foundation. We\nargue that exploring small-scale models' reasoning capabilities remains\nvaluable for researchers with limited computational resources. Moreover,\nenabling models to explain their reasoning processes on general\nquestion-answering datasets is equally meaningful. Therefore, we present the\nsmall-scale video reasoning model TinyLLaVA-Video-R1. Based on TinyLLaVA-Video,\na traceably trained video understanding model with no more than 4B parameters,\nit not only demonstrates significantly improved reasoning and thinking\ncapabilities after using reinforcement learning on general Video-QA datasets,\nbut also exhibits the emergent characteristic of \"aha moments\". Furthermore, we\nshare a series of experimental findings, aiming to provide practical insights\nfor future exploration of video reasoning (thinking) abilities in small-scale\nmodels. It is available at https://github.com/ZhangXJ199/TinyLLaVA-Video-R1."}
{"id": "2504.10185", "pdf": "https://arxiv.org/pdf/2504.10185", "abs": "https://arxiv.org/abs/2504.10185", "authors": ["Soumyadeep Pal", "Changsheng Wang", "James Diffenderfer", "Bhavya Kailkhura", "Sijia Liu"], "title": "LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in Current Benchmarks", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language model unlearning has become a critical challenge in ensuring\nsafety and controlled model behavior by removing undesired data-model\ninfluences from the pretrained model while preserving general utility.\nSignificant recent efforts have been dedicated to developing LLM unlearning\nbenchmarks such as WMDP (Weapons of Mass Destruction Proxy) and MUSE (Machine\nUnlearning Six-way Evaluation), facilitating standardized unlearning\nperformance assessment and method comparison. Despite their usefulness, we\nuncover for the first time a novel coreset effect within these benchmarks.\nSpecifically, we find that LLM unlearning achieved with the original (full)\nforget set can be effectively maintained using a significantly smaller subset\n(functioning as a \"coreset\"), e.g., as little as 5% of the forget set, even\nwhen selected at random. This suggests that LLM unlearning in these benchmarks\ncan be performed surprisingly easily, even in an extremely low-data regime. We\ndemonstrate that this coreset effect remains strong, regardless of the LLM\nunlearning method used, such as NPO (Negative Preference Optimization) and RMU\n(Representation Misdirection Unlearning), the popular ones in these benchmarks.\nThe surprisingly strong coreset effect is also robust across various data\nselection methods, ranging from random selection to more sophisticated\nheuristic approaches. We explain the coreset effect in LLM unlearning through a\nkeyword-based perspective, showing that keywords extracted from the forget set\nalone contribute significantly to unlearning effectiveness and indicating that\ncurrent unlearning is driven by a compact set of high-impact tokens rather than\nthe entire dataset. We further justify the faithfulness of coreset-unlearned\nmodels along additional dimensions, such as mode connectivity and robustness to\njailbreaking attacks. Codes are available at\nhttps://github.com/OPTML-Group/MU-Coreset."}
{"id": "2504.09644", "pdf": "https://arxiv.org/pdf/2504.09644", "abs": "https://arxiv.org/abs/2504.09644", "authors": ["Kaiyu Li", "Zepeng Xin", "Li Pang", "Chao Pang", "Yupeng Deng", "Jing Yao", "Guisong Xia", "Deyu Meng", "Zhi Wang", "Xiangyong Cao"], "title": "SegEarth-R1: Geospatial Pixel Reasoning via Large Language Model", "categories": ["cs.CV"], "comment": null, "summary": "Remote sensing has become critical for understanding environmental dynamics,\nurban planning, and disaster management. However, traditional remote sensing\nworkflows often rely on explicit segmentation or detection methods, which\nstruggle to handle complex, implicit queries that require reasoning over\nspatial context, domain knowledge, and implicit user intent. Motivated by this,\nwe introduce a new task, \\ie, geospatial pixel reasoning, which allows implicit\nquerying and reasoning and generates the mask of the target region. To advance\nthis task, we construct and release the first large-scale benchmark dataset\ncalled EarthReason, which comprises 5,434 manually annotated image masks with\nover 30,000 implicit question-answer pairs. Moreover, we propose SegEarth-R1, a\nsimple yet effective language-guided segmentation baseline that integrates a\nhierarchical visual encoder, a large language model (LLM) for instruction\nparsing, and a tailored mask generator for spatial correlation. The design of\nSegEarth-R1 incorporates domain-specific adaptations, including aggressive\nvisual token compression to handle ultra-high-resolution remote sensing images,\na description projection module to fuse language and multi-scale features, and\na streamlined mask prediction pipeline that directly queries description\nembeddings. Extensive experiments demonstrate that SegEarth-R1 achieves\nstate-of-the-art performance on both reasoning and referring segmentation\ntasks, significantly outperforming traditional and LLM-based segmentation\nmethods. Our data and code will be released at\nhttps://github.com/earth-insights/SegEarth-R1."}
{"id": "2504.10187", "pdf": "https://arxiv.org/pdf/2504.10187", "abs": "https://arxiv.org/abs/2504.10187", "authors": ["Jiaan Wang", "Fandong Meng", "Jie Zhou"], "title": "Deep Reasoning Translation via Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, deep reasoning LLMs (e.g., OpenAI o1/o3 and DeepSeek-R1) have shown\npromising performance in various complex tasks. Free translation is an\nimportant and interesting task in the multilingual world, which requires going\nbeyond word-for-word translation and taking cultural differences into account.\nThis task is still under-explored in deep reasoning LLMs. In this paper, we\nintroduce DeepTrans, a deep reasoning translation model that learns free\ntranslation via reinforcement learning. Specifically, we carefully build a\nreward model with pre-defined scoring criteria on both the translation results\nand the thought process. Given the source sentences, the reward model teaches\nthe deep translation model how to think and free-translate them during\nreinforcement learning. In this way, training DeepTrans does not need any\nlabeled translations, avoiding the human-intensive annotation or\nresource-intensive data synthesis. Experimental results show the effectiveness\nof DeepTrans. Using Qwen2.5-7B as the backbone, DeepTrans improves performance\nby 16.3% in literature translation, and outperforms strong deep reasoning\nbaselines as well as baselines that are fine-tuned with synthesized data.\nMoreover, we summarize the failures and interesting findings during our RL\nexploration. We hope this work could inspire other researchers in free\ntranslation."}
{"id": "2504.09656", "pdf": "https://arxiv.org/pdf/2504.09656", "abs": "https://arxiv.org/abs/2504.09656", "authors": ["Xingrui Wang", "Jiang Liu", "Ze Wang", "Xiaodong Yu", "Jialian Wu", "Ximeng Sun", "Yusheng Su", "Alan Yuille", "Zicheng Liu", "Emad Barsoum"], "title": "KeyVID: Keyframe-Aware Video Diffusion for Audio-Synchronized Visual Animation", "categories": ["cs.CV"], "comment": null, "summary": "Generating video from various conditions, such as text, image, and audio,\nenables both spatial and temporal control, leading to high-quality generation\nresults. Videos with dramatic motions often require a higher frame rate to\nensure smooth motion. Currently, most audio-to-visual animation models use\nuniformly sampled frames from video clips. However, these uniformly sampled\nframes fail to capture significant key moments in dramatic motions at low frame\nrates and require significantly more memory when increasing the number of\nframes directly. In this paper, we propose KeyVID, a keyframe-aware\naudio-to-visual animation framework that significantly improves the generation\nquality for key moments in audio signals while maintaining computation\nefficiency. Given an image and an audio input, we first localize keyframe time\nsteps from the audio. Then, we use a keyframe generator to generate the\ncorresponding visual keyframes. Finally, we generate all intermediate frames\nusing the motion interpolator. Through extensive experiments, we demonstrate\nthat KeyVID significantly improves audio-video synchronization and video\nquality across multiple datasets, particularly for highly dynamic motions. The\ncode is released in https://github.com/XingruiWang/KeyVID."}
{"id": "2504.10191", "pdf": "https://arxiv.org/pdf/2504.10191", "abs": "https://arxiv.org/abs/2504.10191", "authors": ["Veniamin Veselovsky", "Berke Argin", "Benedikt Stroebl", "Chris Wendler", "Robert West", "James Evans", "Thomas L. Griffiths", "Arvind Narayanan"], "title": "Localized Cultural Knowledge is Conserved and Controllable in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Just as humans display language patterns influenced by their native tongue\nwhen speaking new languages, LLMs often default to English-centric responses\neven when generating in other languages. Nevertheless, we observe that local\ncultural information persists within the models and can be readily activated\nfor cultural customization. We first demonstrate that explicitly providing\ncultural context in prompts significantly improves the models' ability to\ngenerate culturally localized responses. We term the disparity in model\nperformance with versus without explicit cultural context the explicit-implicit\nlocalization gap, indicating that while cultural knowledge exists within LLMs,\nit may not naturally surface in multilingual interactions if cultural context\nis not explicitly provided. Despite the explicit prompting benefit, however,\nthe answers reduce in diversity and tend toward stereotypes. Second, we\nidentify an explicit cultural customization vector, conserved across all\nnon-English languages we explore, which enables LLMs to be steered from the\nsynthetic English cultural world-model toward each non-English cultural world.\nSteered responses retain the diversity of implicit prompting and reduce\nstereotypes to dramatically improve the potential for customization. We discuss\nthe implications of explicit cultural customization for understanding the\nconservation of alternative cultural world models within LLMs, and their\ncontrollable utility for translation, cultural customization, and the\npossibility of making the explicit implicit through soft control for expanded\nLLM function and appeal."}
{"id": "2504.09666", "pdf": "https://arxiv.org/pdf/2504.09666", "abs": "https://arxiv.org/abs/2504.09666", "authors": ["Yao Yuan", "Pan Gao", "Qun Dai", "Jie Qin", "Wei Xiang"], "title": "Uncertainty Guided Refinement for Fine-Grained Salient Object Detection", "categories": ["cs.CV"], "comment": "IEEE Transactions on Image Processing 2025", "summary": "Recently, salient object detection (SOD) methods have achieved impressive\nperformance. However, salient regions predicted by existing methods usually\ncontain unsaturated regions and shadows, which limits the model for reliable\nfine-grained predictions. To address this, we introduce the uncertainty\nguidance learning approach to SOD, intended to enhance the model's perception\nof uncertain regions. Specifically, we design a novel Uncertainty Guided\nRefinement Attention Network (UGRAN), which incorporates three important\ncomponents, i.e., the Multilevel Interaction Attention (MIA) module, the Scale\nSpatial-Consistent Attention (SSCA) module, and the Uncertainty Refinement\nAttention (URA) module. Unlike conventional methods dedicated to enhancing\nfeatures, the proposed MIA facilitates the interaction and perception of\nmultilevel features, leveraging the complementary characteristics among\nmultilevel features. Then, through the proposed SSCA, the salient information\nacross diverse scales within the aggregated features can be integrated more\ncomprehensively and integrally. In the subsequent steps, we utilize the\nuncertainty map generated from the saliency prediction map to enhance the\nmodel's perception capability of uncertain regions, generating a\nhighly-saturated fine-grained saliency prediction map. Additionally, we devise\nan adaptive dynamic partition (ADP) mechanism to minimize the computational\noverhead of the URA module and improve the utilization of uncertainty guidance.\nExperiments on seven benchmark datasets demonstrate the superiority of the\nproposed UGRAN over the state-of-the-art methodologies. Codes will be released\nat https://github.com/I2-Multimedia-Lab/UGRAN."}
{"id": "2504.10198", "pdf": "https://arxiv.org/pdf/2504.10198", "abs": "https://arxiv.org/abs/2504.10198", "authors": ["Hanghui Guo", "Jia Zhu", "Shimin Di", "Weijie Shi", "Zhangze Chen", "Jiajie Xu"], "title": "DioR: Adaptive Cognitive Detection and Contextual Retrieval Optimization for Dynamic Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": "24 pages, 9 figures", "summary": "Dynamic Retrieval-augmented Generation (RAG) has shown great success in\nmitigating hallucinations in large language models (LLMs) during generation.\nHowever, existing dynamic RAG methods face significant limitations in two key\naspects: 1) Lack of an effective mechanism to control retrieval triggers, and\n2) Lack of effective scrutiny of retrieval content. To address these\nlimitations, we propose an innovative dynamic RAG method, DioR (Adaptive\nCognitive Detection and Contextual Retrieval Optimization), which consists of\ntwo main components: adaptive cognitive detection and contextual retrieval\noptimization, specifically designed to determine when retrieval is needed and\nwhat to retrieve for LLMs is useful. Experimental results demonstrate that DioR\nachieves superior performance on all tasks, demonstrating the effectiveness of\nour work."}
{"id": "2504.09671", "pdf": "https://arxiv.org/pdf/2504.09671", "abs": "https://arxiv.org/abs/2504.09671", "authors": ["Pranav Manu", "Astitva Srivastava", "Amit Raj", "Varun Jampani", "Avinash Sharma", "P. J. Narayanan"], "title": "LightHeadEd: Relightable & Editable Head Avatars from a Smartphone", "categories": ["cs.CV"], "comment": null, "summary": "Creating photorealistic, animatable, and relightable 3D head avatars\ntraditionally requires expensive Lightstage with multiple calibrated cameras,\nmaking it inaccessible for widespread adoption. To bridge this gap, we present\na novel, cost-effective approach for creating high-quality relightable head\navatars using only a smartphone equipped with polaroid filters. Our approach\ninvolves simultaneously capturing cross-polarized and parallel-polarized video\nstreams in a dark room with a single point-light source, separating the skin's\ndiffuse and specular components during dynamic facial performances. We\nintroduce a hybrid representation that embeds 2D Gaussians in the UV space of a\nparametric head model, facilitating efficient real-time rendering while\npreserving high-fidelity geometric details. Our learning-based neural\nanalysis-by-synthesis pipeline decouples pose and expression-dependent\ngeometrical offsets from appearance, decomposing the surface into albedo,\nnormal, and specular UV texture maps, along with the environment maps. We\ncollect a unique dataset of various subjects performing diverse facial\nexpressions and head movements."}
{"id": "2504.10227", "pdf": "https://arxiv.org/pdf/2504.10227", "abs": "https://arxiv.org/abs/2504.10227", "authors": ["Tianjie Ju", "Zhenyu Shao", "Bowen Wang", "Yujia Chen", "Zhuosheng Zhang", "Hao Fei", "Mong-Li Lee", "Wynne Hsu", "Sufeng Duan", "Gongshen Liu"], "title": "Probing then Editing Response Personality of Large Language Models", "categories": ["cs.CL"], "comment": "Working in Progress", "summary": "Large Language Models (LLMs) have demonstrated promising capabilities to\ngenerate responses that exhibit consistent personality traits. Despite the\nmajor attempts to analyze personality expression through output-based\nevaluations, little is known about how such traits are internally encoded\nwithin LLM parameters. In this paper, we introduce a layer-wise probing\nframework to systematically investigate the layer-wise capability of LLMs in\nencoding personality for responding. We conduct probing experiments on 11\nopen-source LLMs over the PersonalityEdit benchmark and find that LLMs\npredominantly encode personality for responding in their middle and upper\nlayers, with instruction-tuned models demonstrating a slightly clearer\nseparation of personality traits. Furthermore, by interpreting the trained\nprobing hyperplane as a layer-wise boundary for each personality category, we\npropose a layer-wise perturbation method to edit the personality expressed by\nLLMs during inference. Our results show that even when the prompt explicitly\nspecifies a particular personality, our method can still successfully alter the\nresponse personality of LLMs. Interestingly, the difficulty of converting\nbetween certain personality traits varies substantially, which aligns with the\nrepresentational distances in our probing experiments. Finally, we conduct a\ncomprehensive MMLU benchmark evaluation and time overhead analysis,\ndemonstrating that our proposed personality editing method incurs only minimal\ndegradation in general capabilities while maintaining low training costs and\nacceptable inference latency. Our code is publicly available at\nhttps://github.com/universe-sky/probing-then-editing-personality."}
{"id": "2504.09694", "pdf": "https://arxiv.org/pdf/2504.09694", "abs": "https://arxiv.org/abs/2504.09694", "authors": ["Jiachen Liu", "Yuan Xue", "Haomiao Ni", "Rui Yu", "Zihan Zhou", "Sharon X. Huang"], "title": "Computer-Aided Layout Generation for Building Design: A Review", "categories": ["cs.CV"], "comment": "CVMJ 2025", "summary": "Generating realistic building layouts for automatic building design has been\nstudied in both the computer vision and architecture domains. Traditional\napproaches from the architecture domain, which are based on optimization\ntechniques or heuristic design guidelines, can synthesize desirable layouts,\nbut usually require post-processing and involve human interaction in the design\npipeline, making them costly and timeconsuming. The advent of deep generative\nmodels has significantly improved the fidelity and diversity of the generated\narchitecture layouts, reducing the workload by designers and making the process\nmuch more efficient. In this paper, we conduct a comprehensive review of three\nmajor research topics of architecture layout design and generation: floorplan\nlayout generation, scene layout synthesis, and generation of some other formats\nof building layouts. For each topic, we present an overview of the leading\nparadigms, categorized either by research domains (architecture or machine\nlearning) or by user input conditions or constraints. We then introduce the\ncommonly-adopted benchmark datasets that are used to verify the effectiveness\nof the methods, as well as the corresponding evaluation metrics. Finally, we\nidentify the well-solved problems and limitations of existing approaches, then\npropose new perspectives as promising directions for future research in this\nimportant research area. A project associated with this survey to maintain the\nresources is available at awesome-building-layout-generation."}
{"id": "2504.10284", "pdf": "https://arxiv.org/pdf/2504.10284", "abs": "https://arxiv.org/abs/2504.10284", "authors": ["Weiqi Wang", "Jiefu Ou", "Yangqiu Song", "Benjamin Van Durme", "Daniel Khashabi"], "title": "Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the Evaluation Protocol", "categories": ["cs.CL"], "comment": null, "summary": "Literature review tables are essential for summarizing and comparing\ncollections of scientific papers. We explore the task of generating tables that\nbest fulfill a user's informational needs given a collection of scientific\npapers. Building on recent work (Newman et al., 2024), we extend prior\napproaches to address real-world complexities through a combination of\nLLM-based methods and human annotations. Our contributions focus on three key\nchallenges encountered in real-world use: (i) User prompts are often\nunder-specified; (ii) Retrieved candidate papers frequently contain irrelevant\ncontent; and (iii) Task evaluation should move beyond shallow text similarity\ntechniques and instead assess the utility of inferred tables for\ninformation-seeking tasks (e.g., comparing papers). To support reproducible\nevaluation, we introduce ARXIV2TABLE, a more realistic and challenging\nbenchmark for this task, along with a novel approach to improve literature\nreview table generation in real-world scenarios. Our extensive experiments on\nthis benchmark show that both open-weight and proprietary LLMs struggle with\nthe task, highlighting its difficulty and the need for further advancements.\nOur dataset and code are available at https://github.com/JHU-CLSP/arXiv2Table."}
{"id": "2504.09700", "pdf": "https://arxiv.org/pdf/2504.09700", "abs": "https://arxiv.org/abs/2504.09700", "authors": ["Zijian Wu", "Shuojue Yang", "Yueming Jin", "Septimiu E Salcudean"], "title": "ToolTipNet: A Segmentation-Driven Deep Learning Baseline for Surgical Instrument Tip Detection", "categories": ["cs.CV"], "comment": null, "summary": "In robot-assisted laparoscopic radical prostatectomy (RALP), the location of\nthe instrument tip is important to register the ultrasound frame with the\nlaparoscopic camera frame. A long-standing limitation is that the instrument\ntip position obtained from the da Vinci API is inaccurate and requires hand-eye\ncalibration. Thus, directly computing the position of the tool tip in the\ncamera frame using the vision-based method becomes an attractive solution.\nBesides, surgical instrument tip detection is the key component of other tasks,\nlike surgical skill assessment and surgery automation. However, this task is\nchallenging due to the small size of the tool tip and the articulation of the\nsurgical instrument. Surgical instrument segmentation becomes relatively easy\ndue to the emergence of the Segmentation Foundation Model, i.e., Segment\nAnything. Based on this advancement, we explore the deep learning-based\nsurgical instrument tip detection approach that takes the part-level instrument\nsegmentation mask as input. Comparison experiments with a hand-crafted\nimage-processing approach demonstrate the superiority of the proposed method on\nsimulated and real datasets."}
{"id": "2504.10335", "pdf": "https://arxiv.org/pdf/2504.10335", "abs": "https://arxiv.org/abs/2504.10335", "authors": ["Maharaj Brahma", "N J Karthika", "Atul Singh", "Devaraj Adiga", "Smruti Bhate", "Ganesh Ramakrishnan", "Rohit Saluja", "Maunendra Sankar Desarkar"], "title": "MorphTok: Morphologically Grounded Tokenization for Indian Languages", "categories": ["cs.CL"], "comment": null, "summary": "Tokenization is a crucial step in NLP, especially with the rise of large\nlanguage models (LLMs), impacting downstream performance, computational cost,\nand efficiency. Existing LLMs rely on the classical Byte-pair Encoding (BPE)\nalgorithm for subword tokenization that greedily merges frequent character\nbigrams. This often leads to segmentation that does not align with\nlinguistically meaningful units. To address this, we propose morphology-aware\nsegmentation as a pre-tokenization step prior to applying BPE. To facilitate\nmorphology-aware segmentation, we create a novel dataset for Hindi and Marathi,\nincorporating sandhi splitting to enhance the subword tokenization. Experiments\non downstream tasks show that morphologically grounded tokenization improves\nperformance for machine translation and language modeling. Additionally, to\nhandle the ambiguity in the Unicode characters for diacritics, particularly\ndependent vowels in syllable-based writing systems, we introduce Constrained\nBPE (CBPE), an extension to the traditional BPE algorithm that incorporates\nscript-specific constraints. Specifically, CBPE handles dependent vowels. Our\nresults show that CBPE achieves a 1.68\\% reduction in fertility scores while\nmaintaining comparable or improved downstream performance in machine\ntranslation, offering a computationally efficient alternative to standard BPE.\nMoreover, to evaluate segmentation across different tokenization algorithms, we\nintroduce a new human evaluation metric, \\textit{EvalTok}, enabling more\nhuman-grounded assessment."}
{"id": "2504.09724", "pdf": "https://arxiv.org/pdf/2504.09724", "abs": "https://arxiv.org/abs/2504.09724", "authors": ["Gaurav Shinde", "Anuradha Ravi", "Emon Dey", "Shadman Sakib", "Milind Rampure", "Nirmalya Roy"], "title": "A Survey on Efficient Vision-Language Models", "categories": ["cs.CV"], "comment": "35 pages, 16 figures", "summary": "Vision-language models (VLMs) integrate visual and textual information,\nenabling a wide range of applications such as image captioning and visual\nquestion answering, making them crucial for modern AI systems. However, their\nhigh computational demands pose challenges for real-time applications. This has\nled to a growing focus on developing efficient vision language models. In this\nsurvey, we review key techniques for optimizing VLMs on edge and\nresource-constrained devices. We also explore compact VLM architectures,\nframeworks and provide detailed insights into the performance-memory trade-offs\nof efficient VLMs. Furthermore, we establish a GitHub repository at\nhttps://github.com/MPSCUMBC/Efficient-Vision-Language-Models-A-Survey to\ncompile all surveyed papers, which we will actively update. Our objective is to\nfoster deeper research in this area."}
{"id": "2504.10340", "pdf": "https://arxiv.org/pdf/2504.10340", "abs": "https://arxiv.org/abs/2504.10340", "authors": ["Shahriar Noroozizadeh", "Sayantan Kumar", "Jeremy C. Weiss"], "title": "Forecasting from Clinical Textual Time Series: Adaptations of the Encoder and Decoder Language Model Families", "categories": ["cs.CL", "cs.AI"], "comment": "Machine Learning for Healthcare (MLHC 2025)", "summary": "Clinical case reports encode rich, temporal patient trajectories that are\noften underexploited by traditional machine learning methods relying on\nstructured data. In this work, we introduce the forecasting problem from\ntextual time series, where timestamped clinical findings--extracted via an\nLLM-assisted annotation pipeline--serve as the primary input for prediction. We\nsystematically evaluate a diverse suite of models, including fine-tuned\ndecoder-based large language models and encoder-based transformers, on tasks of\nevent occurrence prediction, temporal ordering, and survival analysis. Our\nexperiments reveal that encoder-based models consistently achieve higher F1\nscores and superior temporal concordance for short- and long-horizon event\nforecasting, while fine-tuned masking approaches enhance ranking performance.\nIn contrast, instruction-tuned decoder models demonstrate a relative advantage\nin survival analysis, especially in early prognosis settings. Our sensitivity\nanalyses further demonstrate the importance of time ordering, which requires\nclinical time series construction, as compared to text ordering, the format of\nthe text inputs that LLMs are classically trained on. This highlights the\nadditional benefit that can be ascertained from time-ordered corpora, with\nimplications for temporal tasks in the era of widespread LLM use."}
{"id": "2504.09738", "pdf": "https://arxiv.org/pdf/2504.09738", "abs": "https://arxiv.org/abs/2504.09738", "authors": ["Vasilii Korolkov", "Andrey Yanchenko"], "title": "Automatic Detection of Intro and Credits in Video using CLIP and Multihead Attention", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "68T07", "I.2.10; I.4.8; I.5.1"], "comment": "22 pages, 11 figures, submitted as a preprint. ArXiv preprint only,\n  not submitted to a journal yet", "summary": "Detecting transitions between intro/credits and main content in videos is a\ncrucial task for content segmentation, indexing, and recommendation systems.\nManual annotation of such transitions is labor-intensive and error-prone, while\nheuristic-based methods often fail to generalize across diverse video styles.\nIn this work, we introduce a deep learning-based approach that formulates the\nproblem as a sequence-to-sequence classification task, where each second of a\nvideo is labeled as either \"intro\" or \"film.\" Our method extracts frames at a\nfixed rate of 1 FPS, encodes them using CLIP (Contrastive Language-Image\nPretraining), and processes the resulting feature representations with a\nmultihead attention model incorporating learned positional encoding. The system\nachieves an F1-score of 91.0%, Precision of 89.0%, and Recall of 97.0% on the\ntest set, and is optimized for real-time inference, achieving 11.5 FPS on CPU\nand 107 FPS on high-end GPUs. This approach has practical applications in\nautomated content indexing, highlight detection, and video summarization.\nFuture work will explore multimodal learning, incorporating audio features and\nsubtitles to further enhance detection accuracy."}
{"id": "2504.10342", "pdf": "https://arxiv.org/pdf/2504.10342", "abs": "https://arxiv.org/abs/2504.10342", "authors": ["Yueqi Song", "Tianyue Ou", "Yibo Kong", "Zecheng Li", "Graham Neubig", "Xiang Yue"], "title": "VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain Knowledge", "categories": ["cs.CL"], "comment": "56 pages, 43 figures", "summary": "Current multimodal benchmarks often conflate reasoning with domain-specific\nknowledge, making it difficult to isolate and evaluate general reasoning\nabilities in non-expert settings. To address this, we introduce VisualPuzzles,\na benchmark that targets visual reasoning while deliberately minimizing\nreliance on specialized knowledge. VisualPuzzles consists of diverse questions\nspanning five categories: algorithmic, analogical, deductive, inductive, and\nspatial reasoning. One major source of our questions is manually translated\nlogical reasoning questions from the Chinese Civil Service Examination.\nExperiments show that VisualPuzzles requires significantly less intensive\ndomain-specific knowledge and more complex reasoning compared to benchmarks\nlike MMMU, enabling us to better evaluate genuine multimodal reasoning.\nEvaluations show that state-of-the-art multimodal large language models\nconsistently lag behind human performance on VisualPuzzles, and that strong\nperformance on knowledge-intensive benchmarks does not necessarily translate to\nsuccess on reasoning-focused, knowledge-light tasks. Additionally, reasoning\nenhancements such as scaling up inference compute (with \"thinking\" modes) yield\ninconsistent gains across models and task types, and we observe no clear\ncorrelation between model size and performance. We also found that models\nexhibit different reasoning and answering patterns on VisualPuzzles compared to\nbenchmarks with heavier emphasis on knowledge. VisualPuzzles offers a clearer\nlens through which to evaluate reasoning capabilities beyond factual recall and\ndomain knowledge."}
{"id": "2504.09764", "pdf": "https://arxiv.org/pdf/2504.09764", "abs": "https://arxiv.org/abs/2504.09764", "authors": ["Yuyang Ji", "Haohan Wang"], "title": "Socratic Chart: Cooperating Multiple Agents for Robust SVG Chart Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have shown remarkable versatility\nbut face challenges in demonstrating true visual understanding, particularly in\nchart reasoning tasks. Existing benchmarks like ChartQA reveal significant\nreliance on text-based shortcuts and probabilistic pattern-matching rather than\ngenuine visual reasoning. To rigorously evaluate visual reasoning, we introduce\na more challenging test scenario by removing textual labels and introducing\nchart perturbations in the ChartQA dataset. Under these conditions, models like\nGPT-4o and Gemini-2.0 Pro experience up to a 30% performance drop, underscoring\ntheir limitations. To address these challenges, we propose Socratic Chart, a\nnew framework that transforms chart images into Scalable Vector Graphics (SVG)\nrepresentations, enabling MLLMs to integrate textual and visual modalities for\nenhanced chart understanding. Socratic Chart employs a multi-agent pipeline\nwith specialized agent-generators to extract primitive chart attributes (e.g.,\nbar heights, line coordinates) and an agent-critic to validate results,\nensuring high-fidelity symbolic representations. Our framework surpasses\nstate-of-the-art models in accurately capturing chart primitives and improving\nreasoning performance, establishing a robust pathway for advancing MLLM visual\nunderstanding."}
{"id": "2504.10356", "pdf": "https://arxiv.org/pdf/2504.10356", "abs": "https://arxiv.org/abs/2504.10356", "authors": ["Dieuwke Hupkes", "Nikolay Bogoychev"], "title": "MultiLoKo: a multilingual local knowledge benchmark for LLMs spanning 31 languages", "categories": ["cs.CL"], "comment": null, "summary": "We present MultiLoKo, a new benchmark for evaluating multilinguality in LLMs\ncovering 31 languages. MultiLoKo consists of three partitions: a main partition\nconsisting of 500 questions per language, separately sourced to be locally\nrelevant to the specific language, and two translated partitions, containing\nhuman-authored translations from 30 non-English languages to English and vice\nversa. For comparison, we also release corresponding machine-authored\ntranslations. The data is equally distributed over two splits: a dev split and\na blind, out-of-distribution test split. MultiLoKo can be used to study a\nvariety of questions regarding the multilinguality of LLMs as well as\nmeta-questions about multilingual benchmark creation. We compute MultiLoKo\nscores for 11 base and chat models marketed to be multilingual and study their\naverage performance, their performance parity across languages, how much their\nability to answer questions depends on the question language, and which\nlanguages are most difficult. None of the models we studied performs well on\nMultiLoKo, as indicated by low average scores as well as large differences\nbetween the best and worst scoring languages. Furthermore, we find a\nsubstantial effect of the question language, indicating sub-optimal knowledge\ntransfer between languages. Lastly, we find that using local vs\nEnglish-translated data can result in differences more than 20 points for the\nbest performing models, drastically change the estimated difficulty of some\nlanguages. For using machines instead of human translations, we find a weaker\neffect on ordering of language difficulty, a larger difference in model\nrankings, and a substantial drop in estimated performance for all models."}
{"id": "2504.09766", "pdf": "https://arxiv.org/pdf/2504.09766", "abs": "https://arxiv.org/abs/2504.09766", "authors": ["Diego Marcondes"], "title": "On the representation of stack operators by mathematical morphology", "categories": ["cs.CV"], "comment": null, "summary": "This paper introduces the class of grey-scale image stack operators as those\nthat (a) map binary-images into binary-images and (b) commute in average with\ncross-sectioning. We show that stack operators are 1-Lipchitz extensions of set\noperators which can be represented by applying a characteristic set operator to\nthe cross-sections of the image and summing. In particular, they are a\ngeneralisation of stack filters, for which the characteristic set operators are\nincreasing. Our main result is that stack operators inherit lattice properties\nof the characteristic set operators. We focus on the case of\ntranslation-invariant and locally defined stack operators and show the main\nresult by deducing the characteristic function, kernel, and basis\nrepresentation of stack operators. The results of this paper have implications\non the design of image operators, since imply that to solve some grey-scale\nimage processing problems it is enough to design an operator for performing the\ndesired transformation on binary images, and then considering its extension\ngiven by a stack operator. We leave many topics for future research regarding\nthe machine learning of stack operators and the characterisation of the image\nprocessing problems that can be solved by them."}
{"id": "2504.10359", "pdf": "https://arxiv.org/pdf/2504.10359", "abs": "https://arxiv.org/abs/2504.10359", "authors": ["Aryan Shrivastava", "Paula Akemi Aoyagui"], "title": "DICE: A Framework for Dimensional and Contextual Evaluation of Language Models", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Language models (LMs) are increasingly being integrated into a wide range of\napplications, yet the modern evaluation paradigm does not sufficiently reflect\nhow they are actually being used. Current evaluations rely on benchmarks that\noften lack direct applicability to the real-world contexts in which LMs are\nbeing deployed. To address this gap, we propose Dimensional and Contextual\nEvaluation (DICE), an approach that evaluates LMs on granular,\ncontext-dependent dimensions. In this position paper, we begin by examining the\ninsufficiency of existing LM benchmarks, highlighting their limited\napplicability to real-world use cases. Next, we propose a set of granular\nevaluation parameters that capture dimensions of LM behavior that are more\nmeaningful to stakeholders across a variety of application domains.\nSpecifically, we introduce the concept of context-agnostic parameters - such as\nrobustness, coherence, and epistemic honesty - and context-specific parameters\nthat must be tailored to the specific contextual constraints and demands of\nstakeholders choosing to deploy LMs into a particular setting. We then discuss\npotential approaches to operationalize this evaluation framework, finishing\nwith the opportunities and challenges DICE presents to the LM evaluation\nlandscape. Ultimately, this work serves as a practical and approachable\nstarting point for context-specific and stakeholder-relevant evaluation of LMs."}
{"id": "2504.09789", "pdf": "https://arxiv.org/pdf/2504.09789", "abs": "https://arxiv.org/abs/2504.09789", "authors": ["Chao Liu", "Arash Vahdat"], "title": "EquiVDM: Equivariant Video Diffusion Models with Temporally Consistent Noise", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Temporally consistent video-to-video generation is essential for applications\nof video diffusion models in areas such as sim-to-real, style-transfer, video\nupsampling, etc. In this paper, we propose a video diffusion framework that\nleverages temporally consistent noise to generate coherent video frames without\nspecialized modules or additional constraints. We show that the standard\ntraining objective of diffusion models, when applied with temporally consistent\nnoise, encourages the model to be equivariant to spatial transformations in\ninput video and noise. This enables our model to better follow motion patterns\nfrom the input video, producing aligned motion and high-fidelity frames.\nFurthermore, we extend our approach to 3D-consistent video generation by\nattaching noise as textures on 3D meshes, ensuring 3D consistency in\nsim-to-real applications. Experimental results demonstrate that our method\nsurpasses state-of-the-art baselines in motion alignment, 3D consistency, and\nvideo quality while requiring only a few sampling steps in practice."}
{"id": "2504.10368", "pdf": "https://arxiv.org/pdf/2504.10368", "abs": "https://arxiv.org/abs/2504.10368", "authors": ["Wenyuan Zhang", "Shuaiyi Nie", "Xinghua Zhang", "Zefeng Zhang", "Tingwen Liu"], "title": "S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability of Large Reasoning Models", "categories": ["cs.CL", "cs.AI"], "comment": "Work in Progress", "summary": "We introduce S1-Bench, a novel benchmark designed to evaluate Large Reasoning\nModels' (LRMs) performance on simple tasks that favor intuitive system 1\nthinking rather than deliberative system 2 reasoning. While LRMs have achieved\nsignificant breakthroughs in complex reasoning tasks through explicit chains of\nthought, their reliance on deep analytical thinking may limit their system 1\nthinking capabilities. Moreover, a lack of benchmark currently exists to\nevaluate LRMs' performance in tasks that require such capabilities. To fill\nthis gap, S1-Bench presents a set of simple, diverse, and naturally clear\nquestions across multiple domains and languages, specifically designed to\nassess LRMs' performance in such tasks. Our comprehensive evaluation of 22 LRMs\nreveals significant lower efficiency tendencies, with outputs averaging 15.5\ntimes longer than those of traditional small LLMs. Additionally, LRMs often\nidentify correct answers early but continue unnecessary deliberation, with some\nmodels even producing numerous errors. These findings highlight the rigid\nreasoning patterns of current LRMs and underscore the substantial development\nneeded to achieve balanced dual-system thinking capabilities that can adapt\nappropriately to task complexity."}
{"id": "2504.09797", "pdf": "https://arxiv.org/pdf/2504.09797", "abs": "https://arxiv.org/abs/2504.09797", "authors": ["Dinh Dai Quan Tran", "Hoang-Thien Nguyen. Thanh-Huy Nguyen", "Gia-Van To", "Tien-Huy Nguyen", "Quan Nguyen"], "title": "IGL-DT: Iterative Global-Local Feature Learning with Dual-Teacher Semantic Segmentation Framework under Limited Annotation Scheme", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 5 figures", "summary": "Semi-Supervised Semantic Segmentation (SSSS) aims to improve segmentation\naccuracy by leveraging a small set of labeled images alongside a larger pool of\nunlabeled data. Recent advances primarily focus on pseudo-labeling, consistency\nregularization, and co-training strategies. However, existing methods struggle\nto balance global semantic representation with fine-grained local feature\nextraction. To address this challenge, we propose a novel tri-branch\nsemi-supervised segmentation framework incorporating a dual-teacher strategy,\nnamed IGL-DT. Our approach employs SwinUnet for high-level semantic guidance\nthrough Global Context Learning and ResUnet for detailed feature refinement via\nLocal Regional Learning. Additionally, a Discrepancy Learning mechanism\nmitigates over-reliance on a single teacher, promoting adaptive feature\nlearning. Extensive experiments on benchmark datasets demonstrate that our\nmethod outperforms state-of-the-art approaches, achieving superior segmentation\nperformance across various data regimes."}
{"id": "2504.10391", "pdf": "https://arxiv.org/pdf/2504.10391", "abs": "https://arxiv.org/abs/2504.10391", "authors": ["Varun Vasudevan", "Faezeh Akhavizadegan", "Abhinav Prakash", "Yokila Arora", "Jason Cho", "Tanya Mendiratta", "Sushant Kumar", "Kannan Achan"], "title": "LLM-driven Constrained Copy Generation through Iterative Refinement", "categories": ["cs.CL"], "comment": "10 pages, 2 figures, 7 Tables", "summary": "Crafting a marketing message (copy), or copywriting is a challenging\ngeneration task, as the copy must adhere to various constraints. Copy creation\nis inherently iterative for humans, starting with an initial draft followed by\nsuccessive refinements. However, manual copy creation is time-consuming and\nexpensive, resulting in only a few copies for each use case. This limitation\nrestricts our ability to personalize content to customers. Contrary to the\nmanual approach, LLMs can generate copies quickly, but the generated content\ndoes not consistently meet all the constraints on the first attempt (similar to\nhumans). While recent studies have shown promise in improving constrained\ngeneration through iterative refinement, they have primarily addressed tasks\nwith only a few simple constraints. Consequently, the effectiveness of\niterative refinement for tasks such as copy generation, which involves many\nintricate constraints, remains unclear. To address this gap, we propose an\nLLM-based end-to-end framework for scalable copy generation using iterative\nrefinement. To the best of our knowledge, this is the first study to address\nmultiple challenging constraints simultaneously in copy generation. Examples of\nthese constraints include length, topics, keywords, preferred lexical ordering,\nand tone of voice. We demonstrate the performance of our framework by creating\ncopies for e-commerce banners for three different use cases of varying\ncomplexity. Our results show that iterative refinement increases the copy\nsuccess rate by $16.25-35.91$% across use cases. Furthermore, the copies\ngenerated using our approach outperformed manually created content in multiple\npilot studies using a multi-armed bandit framework. The winning copy improved\nthe click-through rate by $38.5-45.21$%."}
{"id": "2504.09814", "pdf": "https://arxiv.org/pdf/2504.09814", "abs": "https://arxiv.org/abs/2504.09814", "authors": ["Beomseok Kang", "Niluthpol Chowdhury Mithun", "Abhinav Rajvanshi", "Han-Pang Chiu", "Supun Samarasekera"], "title": "DUDA: Distilled Unsupervised Domain Adaptation for Lightweight Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Unsupervised Domain Adaptation (UDA) is essential for enabling semantic\nsegmentation in new domains without requiring costly pixel-wise annotations.\nState-of-the-art (SOTA) UDA methods primarily use self-training with\narchitecturally identical teacher and student networks, relying on Exponential\nMoving Average (EMA) updates. However, these approaches face substantial\nperformance degradation with lightweight models due to inherent architectural\ninflexibility leading to low-quality pseudo-labels. To address this, we propose\nDistilled Unsupervised Domain Adaptation (DUDA), a novel framework that\ncombines EMA-based self-training with knowledge distillation (KD). Our method\nemploys an auxiliary student network to bridge the architectural gap between\nheavyweight and lightweight models for EMA-based updates, resulting in improved\npseudo-label quality. DUDA employs a strategic fusion of UDA and KD,\nincorporating innovative elements such as gradual distillation from large to\nsmall networks, inconsistency loss prioritizing poorly adapted classes, and\nlearning with multiple teachers. Extensive experiments across four UDA\nbenchmarks demonstrate DUDA's superiority in achieving SOTA performance with\nlightweight models, often surpassing the performance of heavyweight models from\nother approaches."}
{"id": "2504.10405", "pdf": "https://arxiv.org/pdf/2504.10405", "abs": "https://arxiv.org/abs/2504.10405", "authors": ["Diogo Sousa", "Guilherme Barbosa", "Catarina Rocha", "Dulce Oliveira"], "title": "Performance of Large Language Models in Supporting Medical Diagnosis and Treatment", "categories": ["cs.CL", "cs.AI", "cs.ET", "cs.HC", "I.2.7; J.3"], "comment": "21 pages, 6 figures, 4 tables. Acknowledgements: The authors\n  acknowledge the support of the AITriage4SU Project (2024.07400.IACDC/2024),\n  funded by the FCT (Foundation for Science and Technology), Portugal", "summary": "The integration of Large Language Models (LLMs) into healthcare holds\nsignificant potential to enhance diagnostic accuracy and support medical\ntreatment planning. These AI-driven systems can analyze vast datasets,\nassisting clinicians in identifying diseases, recommending treatments, and\npredicting patient outcomes. This study evaluates the performance of a range of\ncontemporary LLMs, including both open-source and closed-source models, on the\n2024 Portuguese National Exam for medical specialty access (PNA), a\nstandardized medical knowledge assessment. Our results highlight considerable\nvariation in accuracy and cost-effectiveness, with several models demonstrating\nperformance exceeding human benchmarks for medical students on this specific\ntask. We identify leading models based on a combined score of accuracy and\ncost, discuss the implications of reasoning methodologies like\nChain-of-Thought, and underscore the potential for LLMs to function as valuable\ncomplementary tools aiding medical professionals in complex clinical\ndecision-making."}
{"id": "2504.09819", "pdf": "https://arxiv.org/pdf/2504.09819", "abs": "https://arxiv.org/abs/2504.09819", "authors": ["Chenyang Zhao", "Jia Wan", "Antoni B. Chan"], "title": "Density-based Object Detection in Crowded Scenes", "categories": ["cs.CV"], "comment": null, "summary": "Compared with the generic scenes, crowded scenes contain highly-overlapped\ninstances, which result in: 1) more ambiguous anchors during training of object\ndetectors, and 2) more predictions are likely to be mistakenly suppressed in\npost-processing during inference. To address these problems, we propose two new\nstrategies, density-guided anchors (DGA) and density-guided NMS (DG-NMS), which\nuses object density maps to jointly compute optimal anchor assignments and\nreweighing, as well as an adaptive NMS. Concretely, based on an unbalanced\noptimal transport (UOT) problem, the density owned by each ground-truth object\nis transported to each anchor position at a minimal transport cost. And density\non anchors comprises an instance-specific density distribution, from which DGA\ndecodes the optimal anchor assignment and re-weighting strategy. Meanwhile,\nDG-NMS utilizes the predicted density map to adaptively adjust the NMS\nthreshold to reduce mistaken suppressions. In the UOT, a novel overlap-aware\ntransport cost is specifically designed for ambiguous anchors caused by\noverlapped neighboring objects. Extensive experiments on the challenging\nCrowdHuman dataset with Citypersons dataset demonstrate that our proposed\ndensity-guided detector is effective and robust to crowdedness. The code and\npre-trained models will be made available later."}
{"id": "2504.10415", "pdf": "https://arxiv.org/pdf/2504.10415", "abs": "https://arxiv.org/abs/2504.10415", "authors": ["Parshin Shojaee", "Ngoc-Hieu Nguyen", "Kazem Meidani", "Amir Barati Farimani", "Khoa D Doan", "Chandan K Reddy"], "title": "LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project page:\n  https://github.com/deep-symbolic-mathematics/llm-srbench , Benchmark page:\n  https://huggingface.co/datasets/nnheui/llm-srbench", "summary": "Scientific equation discovery is a fundamental task in the history of\nscientific progress, enabling the derivation of laws governing natural\nphenomena. Recently, Large Language Models (LLMs) have gained interest for this\ntask due to their potential to leverage embedded scientific knowledge for\nhypothesis generation. However, evaluating the true discovery capabilities of\nthese methods remains challenging, as existing benchmarks often rely on common\nequations that are susceptible to memorization by LLMs, leading to inflated\nperformance metrics that do not reflect discovery. In this paper, we introduce\nLLM-SRBench, a comprehensive benchmark with 239 challenging problems across\nfour scientific domains specifically designed to evaluate LLM-based scientific\nequation discovery methods while preventing trivial memorization. Our benchmark\ncomprises two main categories: LSR-Transform, which transforms common physical\nmodels into less common mathematical representations to test reasoning beyond\nmemorized forms, and LSR-Synth, which introduces synthetic, discovery-driven\nproblems requiring data-driven reasoning. Through extensive evaluation of\nseveral state-of-the-art methods, using both open and closed LLMs, we find that\nthe best-performing system so far achieves only 31.5% symbolic accuracy. These\nfindings highlight the challenges of scientific equation discovery, positioning\nLLM-SRBench as a valuable resource for future research."}
{"id": "2504.09828", "pdf": "https://arxiv.org/pdf/2504.09828", "abs": "https://arxiv.org/abs/2504.09828", "authors": ["Hezhao Liu", "Yang Lu", "Mengke Li", "Yiqun Zhang", "Shreyank N Gowda", "Chen Gong", "Hanzi Wang"], "title": "FATE: A Prompt-Tuning-Based Semi-Supervised Learning Framework for Extremely Limited Labeled Data", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Semi-supervised learning (SSL) has achieved significant progress by\nleveraging both labeled data and unlabeled data. Existing SSL methods overlook\na common real-world scenario when labeled data is extremely scarce, potentially\nas limited as a single labeled sample in the dataset. General SSL approaches\nstruggle to train effectively from scratch under such constraints, while\nmethods utilizing pre-trained models often fail to find an optimal balance\nbetween leveraging limited labeled data and abundant unlabeled data. To address\nthis challenge, we propose Firstly Adapt, Then catEgorize (FATE), a novel SSL\nframework tailored for scenarios with extremely limited labeled data. At its\ncore, the two-stage prompt tuning paradigm FATE exploits unlabeled data to\ncompensate for scarce supervision signals, then transfers to downstream tasks.\nConcretely, FATE first adapts a pre-trained model to the feature distribution\nof downstream data using volumes of unlabeled samples in an unsupervised\nmanner. It then applies an SSL method specifically designed for pre-trained\nmodels to complete the final classification task. FATE is designed to be\ncompatible with both vision and vision-language pre-trained models. Extensive\nexperiments demonstrate that FATE effectively mitigates challenges arising from\nthe scarcity of labeled samples in SSL, achieving an average performance\nimprovement of 33.74% across seven benchmarks compared to state-of-the-art SSL\nmethods. Code is available at\nhttps://anonymous.4open.science/r/Semi-supervised-learning-BA72."}
{"id": "2504.10418", "pdf": "https://arxiv.org/pdf/2504.10418", "abs": "https://arxiv.org/abs/2504.10418", "authors": ["Jing Chen", "Zhihua Wei", "Wei Zhang", "Yingying Hu", "Qiong Zhang"], "title": "CliniChat: A Multi-Source Knowledge-Driven Framework for Clinical Interview Dialogue Reconstruction and Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) hold great promise for assisting clinical\ninterviews due to their fluent interactive capabilities and extensive medical\nknowledge. However, the lack of high-quality interview dialogue data and widely\naccepted evaluation methods has significantly impeded this process. So we\npropose CliniChat, a framework that integrates multi-source knowledge to enable\nLLMs to simulate real-world clinical interviews. It consists of two modules:\nClini-Recon and Clini-Eval, each responsible for reconstructing and evaluating\ninterview dialogues, respectively. By incorporating three sources of knowledge,\nClini-Recon transforms clinical notes into systematic, professional, and\nempathetic interview dialogues. Clini-Eval combines a comprehensive evaluation\nmetric system with a two-phase automatic evaluation approach, enabling LLMs to\nassess interview performance like experts. We contribute MedQA-Dialog, a\nhigh-quality synthetic interview dialogue dataset, and CliniChatGLM, a model\nspecialized for clinical interviews. Experimental results demonstrate that\nCliniChatGLM's interview capabilities undergo a comprehensive upgrade,\nparticularly in history-taking, achieving state-of-the-art performance."}
{"id": "2504.09843", "pdf": "https://arxiv.org/pdf/2504.09843", "abs": "https://arxiv.org/abs/2504.09843", "authors": ["Lu Yue", "Dongliang Zhou", "Liang Xie", "Erwei Yin", "Feitian Zhang"], "title": "ST-Booster: An Iterative SpatioTemporal Perception Booster for Vision-and-Language Navigation in Continuous Environments", "categories": ["cs.CV", "cs.RO"], "comment": "11 pages, 7 figures", "summary": "Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires\nagents to navigate unknown, continuous spaces based on natural language\ninstructions. Compared to discrete settings, VLN-CE poses two core perception\nchallenges. First, the absence of predefined observation points leads to\nheterogeneous visual memories and weakened global spatial correlations. Second,\ncumulative reconstruction errors in three-dimensional scenes introduce\nstructural noise, impairing local feature perception. To address these\nchallenges, this paper proposes ST-Booster, an iterative spatiotemporal booster\nthat enhances navigation performance through multi-granularity perception and\ninstruction-aware reasoning. ST-Booster consists of three key modules --\nHierarchical SpatioTemporal Encoding (HSTE), Multi-Granularity Aligned Fusion\n(MGAF), and ValueGuided Waypoint Generation (VGWG). HSTE encodes long-term\nglobal memory using topological graphs and captures shortterm local details via\ngrid maps. MGAF aligns these dualmap representations with instructions through\ngeometry-aware knowledge fusion. The resulting representations are iteratively\nrefined through pretraining tasks. During reasoning, VGWG generates Guided\nAttention Heatmaps (GAHs) to explicitly model environment-instruction relevance\nand optimize waypoint selection. Extensive comparative experiments and\nperformance analyses are conducted, demonstrating that ST-Booster outperforms\nexisting state-of-the-art methods, particularly in complex, disturbance-prone\nenvironments."}
{"id": "2504.10419", "pdf": "https://arxiv.org/pdf/2504.10419", "abs": "https://arxiv.org/abs/2504.10419", "authors": ["Michał Turski", "Mateusz Chiliński", "Łukasz Borchmann"], "title": "Unchecked and Overlooked: Addressing the Checkbox Blind Spot in Large Language Models with CheckboxQA", "categories": ["cs.CL"], "comment": null, "summary": "Checkboxes are critical in real-world document processing where the presence\nor absence of ticks directly informs data extraction and decision-making\nprocesses. Yet, despite the strong performance of Large Vision and Language\nModels across a wide range of tasks, they struggle with interpreting checkable\ncontent. This challenge becomes particularly pressing in industries where a\nsingle overlooked checkbox may lead to costly regulatory or contractual\noversights. To address this gap, we introduce the CheckboxQA dataset, a\ntargeted resource designed to evaluate and improve model performance on\ncheckbox-related tasks. It reveals the limitations of current models and serves\nas a valuable tool for advancing document comprehension systems, with\nsignificant implications for applications in sectors such as legal tech and\nfinance.\n  The dataset is publicly available at:\nhttps://github.com/Snowflake-Labs/CheckboxQA"}
{"id": "2504.09852", "pdf": "https://arxiv.org/pdf/2504.09852", "abs": "https://arxiv.org/abs/2504.09852", "authors": ["Boris Kriuk", "Simranjit Kaur Gill", "Shoaib Aslam", "Amir Fakhrutdinov"], "title": "GFT: Gradient Focal Transformer", "categories": ["cs.CV"], "comment": "11 pages, 3 tables, 5 figures", "summary": "Fine-Grained Image Classification (FGIC) remains a complex task in computer\nvision, as it requires models to distinguish between categories with subtle\nlocalized visual differences. Well-studied CNN-based models, while strong in\nlocal feature extraction, often fail to capture the global context required for\nfine-grained recognition, while more recent ViT-backboned models address FGIC\nwith attention-driven mechanisms but lack the ability to adaptively focus on\ntruly discriminative regions. TransFG and other ViT-based extensions introduced\npart-aware token selection to enhance attention localization, yet they still\nstruggle with computational efficiency, attention region selection flexibility,\nand detail-focus narrative in complex environments. This paper introduces GFT\n(Gradient Focal Transformer), a new ViT-derived framework created for FGIC\ntasks. GFT integrates the Gradient Attention Learning Alignment (GALA)\nmechanism to dynamically prioritize class-discriminative features by analyzing\nattention gradient flow. Coupled with a Progressive Patch Selection (PPS)\nstrategy, the model progressively filters out less informative regions,\nreducing computational overhead while enhancing sensitivity to fine details.\nGFT achieves SOTA accuracy on FGVC Aircraft, Food-101, and COCO datasets with\n93M parameters, outperforming ViT-based advanced FGIC models in efficiency. By\nbridging global context and localized detail extraction, GFT sets a new\nbenchmark in fine-grained recognition, offering interpretable solutions for\nreal-world deployment scenarios."}
{"id": "2504.10421", "pdf": "https://arxiv.org/pdf/2504.10421", "abs": "https://arxiv.org/abs/2504.10421", "authors": ["Xinhao Yi", "Jake Lever", "Kevin Bryson", "Zaiqiao Meng"], "title": "Can We Edit LLMs for Long-Tail Biomedical Knowledge?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Knowledge editing has emerged as an effective approach for updating large\nlanguage models (LLMs) by modifying their internal knowledge. However, their\napplication to the biomedical domain faces unique challenges due to the\nlong-tailed distribution of biomedical knowledge, where rare and infrequent\ninformation is prevalent. In this paper, we conduct the first comprehensive\nstudy to investigate the effectiveness of knowledge editing methods for editing\nlong-tail biomedical knowledge. Our results indicate that, while existing\nediting methods can enhance LLMs' performance on long-tail biomedical\nknowledge, their performance on long-tail knowledge remains inferior to that on\nhigh-frequency popular knowledge, even after editing. Our further analysis\nreveals that long-tail biomedical knowledge contains a significant amount of\none-to-many knowledge, where one subject and relation link to multiple objects.\nThis high prevalence of one-to-many knowledge limits the effectiveness of\nknowledge editing in improving LLMs' understanding of long-tail biomedical\nknowledge, highlighting the need for tailored strategies to bridge this\nperformance gap."}
{"id": "2504.09876", "pdf": "https://arxiv.org/pdf/2504.09876", "abs": "https://arxiv.org/abs/2504.09876", "authors": ["Tran Quoc Khanh Le", "Nguyen Lan Vi Vu", "Ha-Hieu Pham", "Xuan-Loc Huynh", "Tien-Huy Nguyen", "Minh Huu Nhat Le", "Quan Nguyen", "Hien D. Nguyen"], "title": "HDC: Hierarchical Distillation for Multi-level Noisy Consistency in Semi-Supervised Fetal Ultrasound Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Transvaginal ultrasound is a critical imaging modality for evaluating\ncervical anatomy and detecting physiological changes. However, accurate\nsegmentation of cervical structures remains challenging due to low contrast,\nshadow artifacts, and fuzzy boundaries. While convolutional neural networks\n(CNNs) have shown promising results in medical image segmentation, their\nperformance is often limited by the need for large-scale annotated datasets -\nan impractical requirement in clinical ultrasound imaging. Semi-supervised\nlearning (SSL) offers a compelling solution by leveraging unlabeled data, but\nexisting teacher-student frameworks often suffer from confirmation bias and\nhigh computational costs. We propose HDC, a novel semi-supervised segmentation\nframework that integrates Hierarchical Distillation and Consistency learning\nwithin a multi-level noise mean-teacher framework. Unlike conventional\napproaches that rely solely on pseudo-labeling, we introduce a hierarchical\ndistillation mechanism that guides feature-level learning via two novel\nobjectives: (1) Correlation Guidance Loss to align feature representations\nbetween the teacher and main student branch, and (2) Mutual Information Loss to\nstabilize representations between the main and noisy student branches. Our\nframework reduces model complexity while improving generalization. Extensive\nexperiments on two fetal ultrasound datasets, FUGC and PSFH, demonstrate that\nour method achieves competitive performance with significantly lower\ncomputational overhead than existing multi-teacher models."}
{"id": "2504.10430", "pdf": "https://arxiv.org/pdf/2504.10430", "abs": "https://arxiv.org/abs/2504.10430", "authors": ["Minqian Liu", "Zhiyang Xu", "Xinyi Zhang", "Heajun An", "Sarvech Qadir", "Qi Zhang", "Pamela J. Wisniewski", "Jin-Hee Cho", "Sang Won Lee", "Ruoxi Jia", "Lifu Huang"], "title": "LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "20 pages, 7 figures, 4 tables", "summary": "Recent advancements in Large Language Models (LLMs) have enabled them to\napproach human-level persuasion capabilities. However, such potential also\nraises concerns about the safety risks of LLM-driven persuasion, particularly\ntheir potential for unethical influence through manipulation, deception,\nexploitation of vulnerabilities, and many other harmful tactics. In this work,\nwe present a systematic investigation of LLM persuasion safety through two\ncritical aspects: (1) whether LLMs appropriately reject unethical persuasion\ntasks and avoid unethical strategies during execution, including cases where\nthe initial persuasion goal appears ethically neutral, and (2) how influencing\nfactors like personality traits and external pressures affect their behavior.\nTo this end, we introduce PersuSafety, the first comprehensive framework for\nthe assessment of persuasion safety which consists of three stages, i.e.,\npersuasion scene creation, persuasive conversation simulation, and persuasion\nsafety assessment. PersuSafety covers 6 diverse unethical persuasion topics and\n15 common unethical strategies. Through extensive experiments across 8 widely\nused LLMs, we observe significant safety concerns in most LLMs, including\nfailing to identify harmful persuasion tasks and leveraging various unethical\npersuasion strategies. Our study calls for more attention to improve safety\nalignment in progressive and goal-driven conversations such as persuasion."}
{"id": "2504.09878", "pdf": "https://arxiv.org/pdf/2504.09878", "abs": "https://arxiv.org/abs/2504.09878", "authors": ["Yunpeng Tan", "Junlin Hao", "Jiangkai Wu", "Liming Liu", "Qingyang Li", "Xinggong Zhang"], "title": "MCBlock: Boosting Neural Radiance Field Training Speed by MCTS-based Dynamic-Resolution Ray Sampling", "categories": ["cs.CV"], "comment": null, "summary": "Neural Radiance Field (NeRF) is widely known for high-fidelity novel view\nsynthesis. However, even the state-of-the-art NeRF model, Gaussian Splatting,\nrequires minutes for training, far from the real-time performance required by\nmultimedia scenarios like telemedicine. One of the obstacles is its inefficient\nsampling, which is only partially addressed by existing works. Existing\npoint-sampling algorithms uniformly sample simple-texture regions (easy to fit)\nand complex-texture regions (hard to fit), while existing ray-sampling\nalgorithms sample these regions all in the finest granularity (i.e. the pixel\nlevel), both wasting GPU training resources. Actually, regions with different\ntexture intensities require different sampling granularities. To this end, we\npropose a novel dynamic-resolution ray-sampling algorithm, MCBlock, which\nemploys Monte Carlo Tree Search (MCTS) to partition each training image into\npixel blocks with different sizes for active block-wise training. Specifically,\nthe trees are initialized according to the texture of training images to boost\nthe initialization speed, and an expansion/pruning module dynamically optimizes\nthe block partition. MCBlock is implemented in Nerfstudio, an open-source\ntoolset, and achieves a training acceleration of up to 2.33x, surpassing other\nray-sampling algorithms. We believe MCBlock can apply to any cone-tracing NeRF\nmodel and contribute to the multimedia community."}
{"id": "2504.10481", "pdf": "https://arxiv.org/pdf/2504.10481", "abs": "https://arxiv.org/abs/2504.10481", "authors": ["Ding Chen", "Qingchen Yu", "Pengyuan Wang", "Wentao Zhang", "Bo Tang", "Feiyu Xiong", "Xinchi Li", "Minchuan Yang", "Zhiyu Li"], "title": "xVerify: Efficient Answer Verifier for Reasoning Model Evaluations", "categories": ["cs.CL"], "comment": "32 pages", "summary": "With the release of the o1 model by OpenAI, reasoning models adopting slow\nthinking strategies have gradually emerged. As the responses generated by such\nmodels often include complex reasoning, intermediate steps, and\nself-reflection, existing evaluation methods are often inadequate. They\nstruggle to determine whether the LLM output is truly equivalent to the\nreference answer, and also have difficulty identifying and extracting the final\nanswer from long, complex responses. To address this issue, we propose xVerify,\nan efficient answer verifier for reasoning model evaluations. xVerify\ndemonstrates strong capability in equivalence judgment, enabling it to\neffectively determine whether the answers produced by reasoning models are\nequivalent to reference answers across various types of objective questions. To\ntrain and evaluate xVerify, we construct the VAR dataset by collecting\nquestion-answer pairs generated by multiple LLMs across various datasets,\nleveraging multiple reasoning models and challenging evaluation sets designed\nspecifically for reasoning model assessment. A multi-round annotation process\nis employed to ensure label accuracy. Based on the VAR dataset, we train\nmultiple xVerify models of different scales. In evaluation experiments\nconducted on both the test set and generalization set, all xVerify models\nachieve overall F1 scores and accuracy exceeding 95\\%. Notably, the smallest\nvariant, xVerify-0.5B-I, outperforms all evaluation methods except GPT-4o,\nwhile xVerify-3B-Ib surpasses GPT-4o in overall performance. These results\nvalidate the effectiveness and generalizability of xVerify."}
{"id": "2504.09881", "pdf": "https://arxiv.org/pdf/2504.09881", "abs": "https://arxiv.org/abs/2504.09881", "authors": ["Changwei Wang", "Shunpeng Chen", "Yukun Song", "Rongtao Xu", "Zherui Zhang", "Jiguang Zhang", "Haoran Yang", "Yu Zhang", "Kexue Fu", "Shide Du", "Zhiwei Xu", "Longxiang Gao", "Li Guo", "Shibiao Xu"], "title": "Focus on Local: Finding Reliable Discriminative Regions for Visual Place Recognition", "categories": ["cs.CV"], "comment": "Accepted by AAAI 2025", "summary": "Visual Place Recognition (VPR) is aimed at predicting the location of a query\nimage by referencing a database of geotagged images. For VPR task, often fewer\ndiscriminative local regions in an image produce important effects while\nmundane background regions do not contribute or even cause perceptual aliasing\nbecause of easy overlap. However, existing methods lack precisely modeling and\nfull exploitation of these discriminative regions. In this paper, we propose\nthe Focus on Local (FoL) approach to stimulate the performance of image\nretrieval and re-ranking in VPR simultaneously by mining and exploiting\nreliable discriminative local regions in images and introducing\npseudo-correlation supervision. First, we design two losses,\nExtraction-Aggregation Spatial Alignment Loss (SAL) and Foreground-Background\nContrast Enhancement Loss (CEL), to explicitly model reliable discriminative\nlocal regions and use them to guide the generation of global representations\nand efficient re-ranking. Second, we introduce a weakly-supervised local\nfeature training strategy based on pseudo-correspondences obtained from\naggregating global features to alleviate the lack of local correspondences\nground truth for the VPR task. Third, we suggest an efficient re-ranking\npipeline that is efficiently and precisely based on discriminative region\nguidance. Finally, experimental results show that our FoL achieves the\nstate-of-the-art on multiple VPR benchmarks in both image retrieval and\nre-ranking stages and also significantly outperforms existing two-stage VPR\nmethods in terms of computational efficiency. Code and models are available at\nhttps://github.com/chenshunpeng/FoL"}
{"id": "2504.08016", "pdf": "https://arxiv.org/pdf/2504.08016", "abs": "https://arxiv.org/abs/2504.08016", "authors": ["Soo Yong Lee", "Hyunjin Hwang", "Taekwan Kim", "Yuyeong Kim", "Kyuri Park", "Jaemin Yoo", "Denny Borsboom", "Kijung Shin"], "title": "Emergence of psychopathological computations in large language models", "categories": ["q-bio.NC", "cs.AI", "cs.CL"], "comment": "pre-print", "summary": "Can large language models (LLMs) implement computations of psychopathology?\nAn effective approach to the question hinges on addressing two factors. First,\nfor conceptual validity, we require a general and computational account of\npsychopathology that is applicable to computational entities without biological\nembodiment or subjective experience. Second, mechanisms underlying LLM\nbehaviors need to be studied for better methodological validity. Thus, we\nestablish a computational-theoretical framework to provide an account of\npsychopathology applicable to LLMs. To ground the theory for empirical\nanalysis, we also propose a novel mechanistic interpretability method alongside\na tailored empirical analytic framework. Based on the frameworks, we conduct\nexperiments demonstrating three key claims: first, that distinct dysfunctional\nand problematic representational states are implemented in LLMs; second, that\ntheir activations can spread and self-sustain to trap LLMs; and third, that\ndynamic, cyclic structural causal models encoded in the LLMs underpin these\npatterns. In concert, the empirical results corroborate our hypothesis that\nnetwork-theoretic computations of psychopathology have already emerged in LLMs.\nThis suggests that certain LLM behaviors mirroring psychopathology may not be a\nsuperficial mimicry but a feature of their internal processing. Thus, our work\nalludes to the possibility of AI systems with psychopathological behaviors in\nthe near future."}
{"id": "2504.09887", "pdf": "https://arxiv.org/pdf/2504.09887", "abs": "https://arxiv.org/abs/2504.09887", "authors": ["Yiwen Wang", "Ying Liang", "Yuxuan Zhang", "Xinning Chai", "Zhengxue Cheng", "Yingsheng Qin", "Yucai Yang", "Rong Xie", "Li Song"], "title": "Enhanced Semantic Extraction and Guidance for UGC Image Super Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Due to the disparity between real-world degradations in user-generated\ncontent(UGC) images and synthetic degradations, traditional super-resolution\nmethods struggle to generalize effectively, necessitating a more robust\napproach to model real-world distortions. In this paper, we propose a novel\napproach to UGC image super-resolution by integrating semantic guidance into a\ndiffusion framework. Our method addresses the inconsistency between\ndegradations in wild and synthetic datasets by separately simulating the\ndegradation processes on the LSDIR dataset and combining them with the official\npaired training set. Furthermore, we enhance degradation removal and detail\ngeneration by incorporating a pretrained semantic extraction model (SAM2) and\nfine-tuning key hyperparameters for improved perceptual fidelity. Extensive\nexperiments demonstrate the superiority of our approach against\nstate-of-the-art methods. Additionally, the proposed model won second place in\nthe CVPR NTIRE 2025 Short-form UGC Image Super-Resolution Challenge, further\nvalidating its effectiveness. The code is available at\nhttps://github.c10pom/Moonsofang/NTIRE-2025-SRlab."}
{"id": "2504.08744", "pdf": "https://arxiv.org/pdf/2504.08744", "abs": "https://arxiv.org/abs/2504.08744", "authors": ["Esmail Gumaan"], "title": "ExpertRAG: Efficient RAG with Mixture of Experts -- Optimizing Context Retrieval for Adaptive LLM Responses", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "30 pages, 4 figures", "summary": "ExpertRAG is a novel theoretical framework that integrates Mixture-of-Experts\n(MoE) architectures with Retrieval Augmented Generation (RAG) to advance the\nefficiency and accuracy of knowledge-intensive language modeling. We propose a\ndynamic retrieval gating mechanism coupled with expert routing, enabling the\nmodel to selectively consult an external knowledge store or rely on specialized\ninternal experts based on the query's needs. The paper lays out the theoretical\nfoundations of ExpertRAG, including a probabilistic formulation that treats\nretrieval and expert selection as latent decisions, and mathematical\njustifications for its efficiency in both computation and knowledge\nutilization. We derive formulae to quantify the expected computational cost\nsavings from selective retrieval and the capacity gains from sparse expert\nutilization. A comparative analysis positions ExpertRAG against standard RAG\n(with always-on retrieval) and pure MoE models (e.g., Switch Transformer,\nMixtral) to highlight its unique balance between parametric knowledge and\nnon-parametric retrieval. We also outline an experimental validation strategy,\nproposing benchmarks and evaluation protocols to test ExpertRAG's performance\non factual recall, generalization, and inference efficiency. The proposed\nframework, although presented theoretically, is supported by insights from\nprior work in RAG and MoE, and is poised to provide more factual, efficient,\nand adaptive generation by leveraging the best of both paradigms. In summary,\nExpertRAG contributes a new perspective on scaling and augmenting language\nmodels, backed by a thorough analysis and a roadmap for empirical validation."}
{"id": "2504.09897", "pdf": "https://arxiv.org/pdf/2504.09897", "abs": "https://arxiv.org/abs/2504.09897", "authors": ["Jaewoo Lee", "Keyang Xuan", "Chanakya Ekbote", "Sandeep Polisetty", "Yi R.", "Fung", "Paul Pu Liang"], "title": "TAMP: Token-Adaptive Layerwise Pruning in Multimodal Large Language Models", "categories": ["cs.CV"], "comment": "Preprint", "summary": "Multimodal Large Language Models (MLLMs) have shown remarkable versatility in\nunderstanding diverse multimodal data and tasks. However, these capabilities\ncome with an increased model scale. While post-training pruning reduces model\nsize in unimodal models, its application to MLLMs often yields limited success.\nOur analysis discovers that conventional methods fail to account for the unique\ntoken attributes across layers and modalities inherent to MLLMs. Inspired by\nthis observation, we propose TAMP, a simple yet effective pruning framework\ntailored for MLLMs, featuring two key components: (1) Diversity-Aware Sparsity,\nwhich adjusts sparsity ratio per layer based on diversities among multimodal\noutput tokens, preserving more parameters in high-diversity layers; and (2)\nAdaptive Multimodal Input Activation, which identifies representative\nmultimodal input tokens using attention scores to guide unstructured weight\npruning. We validate our method on two state-of-the-art MLLMs: LLaVA-NeXT,\ndesigned for vision-language tasks, and VideoLLaMA2, capable of processing\naudio, visual, and language modalities. Empirical experiments across various\nmultimodal evaluation benchmarks demonstrate that each component of our\napproach substantially outperforms existing pruning techniques."}
{"id": "2504.08745", "pdf": "https://arxiv.org/pdf/2504.08745", "abs": "https://arxiv.org/abs/2504.08745", "authors": ["Mert Yazan", "Suzan Verberne", "Frederik Situmeang"], "title": "Improving RAG for Personalization with Author Features and Contrastive Examples", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Personalization with retrieval-augmented generation (RAG) often fails to\ncapture fine-grained features of authors, making it hard to identify their\nunique traits. To enrich the RAG context, we propose providing Large Language\nModels (LLMs) with author-specific features, such as average sentiment polarity\nand frequently used words, in addition to past samples from the author's\nprofile. We introduce a new feature called Contrastive Examples: documents from\nother authors are retrieved to help LLM identify what makes an author's style\nunique in comparison to others. Our experiments show that adding a couple of\nsentences about the named entities, dependency patterns, and words a person\nuses frequently significantly improves personalized text generation. Combining\nfeatures with contrastive examples boosts the performance further, achieving a\nrelative 15% improvement over baseline RAG while outperforming the benchmarks.\nOur results show the value of fine-grained features for better personalization,\nwhile opening a new research dimension for including contrastive examples as a\ncomplement with RAG. We release our code publicly."}
{"id": "2504.09899", "pdf": "https://arxiv.org/pdf/2504.09899", "abs": "https://arxiv.org/abs/2504.09899", "authors": ["Ziwang Xu", "Lanqing Guo", "Satoshi Tsutsui", "Shuyan Zhang", "Alex C. Kot", "Bihan Wen"], "title": "Digital Staining with Knowledge Distillation: A Unified Framework for Unpaired and Paired-But-Misaligned Data", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted to IEEE Transactions on Medical Imaging", "summary": "Staining is essential in cell imaging and medical diagnostics but poses\nsignificant challenges, including high cost, time consumption, labor intensity,\nand irreversible tissue alterations. Recent advances in deep learning have\nenabled digital staining through supervised model training. However, collecting\nlarge-scale, perfectly aligned pairs of stained and unstained images remains\ndifficult. In this work, we propose a novel unsupervised deep learning\nframework for digital cell staining that reduces the need for extensive paired\ndata using knowledge distillation. We explore two training schemes: (1)\nunpaired and (2) paired-but-misaligned settings. For the unpaired case, we\nintroduce a two-stage pipeline, comprising light enhancement followed by\ncolorization, as a teacher model. Subsequently, we obtain a student staining\ngenerator through knowledge distillation with hybrid non-reference losses. To\nleverage the pixel-wise information between adjacent sections, we further\nextend to the paired-but-misaligned setting, adding the Learning to Align\nmodule to utilize pixel-level information. Experiment results on our dataset\ndemonstrate that our proposed unsupervised deep staining method can generate\nstained images with more accurate positions and shapes of the cell targets in\nboth settings. Compared with competing methods, our method achieves improved\nresults both qualitatively and quantitatively (e.g., NIQE and PSNR).We applied\nour digital staining method to the White Blood Cell (WBC) dataset,\ninvestigating its potential for medical applications."}
{"id": "2504.08748", "pdf": "https://arxiv.org/pdf/2504.08748", "abs": "https://arxiv.org/abs/2504.08748", "authors": ["Lang Mei", "Siyu Mo", "Zhihan Yang", "Chong Chen"], "title": "A Survey of Multimodal Retrieval-Augmented Generation", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.ET", "cs.LG"], "comment": null, "summary": "Multimodal Retrieval-Augmented Generation (MRAG) enhances large language\nmodels (LLMs) by integrating multimodal data (text, images, videos) into\nretrieval and generation processes, overcoming the limitations of text-only\nRetrieval-Augmented Generation (RAG). While RAG improves response accuracy by\nincorporating external textual knowledge, MRAG extends this framework to\ninclude multimodal retrieval and generation, leveraging contextual information\nfrom diverse data types. This approach reduces hallucinations and enhances\nquestion-answering systems by grounding responses in factual, multimodal\nknowledge. Recent studies show MRAG outperforms traditional RAG, especially in\nscenarios requiring both visual and textual understanding. This survey reviews\nMRAG's essential components, datasets, evaluation methods, and limitations,\nproviding insights into its construction and improvement. It also identifies\nchallenges and future research directions, highlighting MRAG's potential to\nrevolutionize multimodal information retrieval and generation. By offering a\ncomprehensive perspective, this work encourages further exploration into this\npromising paradigm."}
{"id": "2504.09900", "pdf": "https://arxiv.org/pdf/2504.09900", "abs": "https://arxiv.org/abs/2504.09900", "authors": ["Muhammad Fasih Tariq", "Muhammad Azeem Javed"], "title": "Small Object Detection with YOLO: A Performance Analysis Across Model Versions and Hardware", "categories": ["cs.CV"], "comment": null, "summary": "This paper provides an extensive evaluation of YOLO object detection models\n(v5, v8, v9, v10, v11) by com- paring their performance across various hardware\nplatforms and optimization libraries. Our study investigates inference speed\nand detection accuracy on Intel and AMD CPUs using popular libraries such as\nONNX and OpenVINO, as well as on GPUs through TensorRT and other GPU-optimized\nframeworks. Furthermore, we analyze the sensitivity of these YOLO models to\nobject size within the image, examining performance when detecting objects that\noccupy 1%, 2.5%, and 5% of the total area of the image. By identifying the\ntrade-offs in efficiency, accuracy, and object size adaptability, this paper\noffers insights for optimal model selection based on specific hardware\nconstraints and detection requirements, aiding practitioners in deploying YOLO\nmodels effectively for real-world applications."}
{"id": "2504.08753", "pdf": "https://arxiv.org/pdf/2504.08753", "abs": "https://arxiv.org/abs/2504.08753", "authors": ["Jyothi", "T. Satyanarayana Murthy"], "title": "Domain Specific Question to SQL Conversion with Embedded Data Balancing Technique", "categories": ["cs.IR", "cs.CL", "cs.DB"], "comment": null, "summary": "The rise of deep learning in natural language processing has fostered the\ncreation of text to structured query language models composed of an encoder and\na decoder. Researchers have experimented with various intermediate processing\nlike schema linking, table type aware, value extract. To generate accurate SQL\nresults for the user question. However error analysis performed on the failed\ncases on these systems shows, 29 percentage of the errors would be because the\nsystem was unable to understand the values expressed by the user in their\nquestion. This challenge affects the generation of accurate SQL queries,\nespecially when dealing with domain-specific terms and specific value\nconditions, where traditional methods struggle to maintain consistency and\nprecision. To overcome these obstacles, proposed two intermediations like\nimplementing data balancing technique and over sampling domain-specific queries\nwhich would refine the model architecture to enhance value recognition and fine\ntuning the model for domain-specific questions. This proposed solution achieved\n10.98 percentage improvement in accuracy of the model performance compared to\nthe state of the art model tested on WikiSQL dataset. to convert the user\nquestion accurately to SQL queries. Applying oversampling technique on the\ndomain-specific questions shown a significant improvement as compared with\ntraditional approaches."}
{"id": "2504.09904", "pdf": "https://arxiv.org/pdf/2504.09904", "abs": "https://arxiv.org/abs/2504.09904", "authors": ["Mert Asim Karaoglu", "Wenbo Ji", "Ahmed Abbas", "Nassir Navab", "Benjamin Busam", "Alexander Ladikos"], "title": "LiteTracker: Leveraging Temporal Causality for Accurate Low-latency Tissue Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Tissue tracking plays a critical role in various surgical navigation and\nextended reality (XR) applications. While current methods trained on large\nsynthetic datasets achieve high tracking accuracy and generalize well to\nendoscopic scenes, their runtime performances fail to meet the low-latency\nrequirements necessary for real-time surgical applications. To address this\nlimitation, we propose LiteTracker, a low-latency method for tissue tracking in\nendoscopic video streams. LiteTracker builds on a state-of-the-art long-term\npoint tracking method, and introduces a set of training-free runtime\noptimizations. These optimizations enable online, frame-by-frame tracking by\nleveraging a temporal memory buffer for efficient feature reuse and utilizing\nprior motion for accurate track initialization. LiteTracker demonstrates\nsignificant runtime improvements being around 7x faster than its predecessor\nand 2x than the state-of-the-art. Beyond its primary focus on efficiency,\nLiteTracker delivers high-accuracy tracking and occlusion prediction,\nperforming competitively on both the STIR and SuPer datasets. We believe\nLiteTracker is an important step toward low-latency tissue tracking for\nreal-time surgical applications in the operating room."}
{"id": "2504.08763", "pdf": "https://arxiv.org/pdf/2504.08763", "abs": "https://arxiv.org/abs/2504.08763", "authors": ["Shiraj Pokharel", "Georg P. Roßrucker", "Mario M. Kubek"], "title": "WebMap -- Large Language Model-assisted Semantic Link Induction in the Web", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "68T50, 68T07", "I.2.6; I.2.7; H.3.3"], "comment": "11 pages, 3 figures, accepted at the 2024 24th International\n  Conference on Innovations for Community Services (I4CS), June 12 - 14,\n  Maastricht, The Netherlands, 2024", "summary": "Carrying out research tasks is only inadequately supported, if not hindered,\nby current web search engines. This paper therefore proposes functional\nextensions of WebMap, a semantically induced overlay linking structure on the\nweb to inherently facilitate research activities. These add-ons support the\ndynamic determination and regrouping of document clusters, the creation of a\nsemantic signpost in the web, and the interactive tracing of topics back to\ntheir origins."}
{"id": "2504.09914", "pdf": "https://arxiv.org/pdf/2504.09914", "abs": "https://arxiv.org/abs/2504.09914", "authors": ["Maria Tzelepi", "Vasileios Mezaris"], "title": "Improving Multimodal Hateful Meme Detection Exploiting LMM-Generated Knowledge", "categories": ["cs.CV"], "comment": "Accepted for publication, Multimodal Learning and Applications\n  Workshop (MULA 2025) @ IEEE/CVF CVPR 2025, Nashville, TN, USA, June 2025.\n  This is the authors' \"accepted version\"", "summary": "Memes have become a dominant form of communication in social media in recent\nyears. Memes are typically humorous and harmless, however there are also memes\nthat promote hate speech, being in this way harmful to individuals and groups\nbased on their identity. Therefore, detecting hateful content in memes has\nemerged as a task of critical importance. The need for understanding the\ncomplex interactions of images and their embedded text renders the hateful meme\ndetection a challenging multimodal task. In this paper we propose to address\nthe aforementioned task leveraging knowledge encoded in powerful Large\nMultimodal Models (LMM). Specifically, we propose to exploit LMMs in a two-fold\nmanner. First, by extracting knowledge oriented to the hateful meme detection\ntask in order to build strong meme representations. Specifically, generic\nsemantic descriptions and emotions that the images along with their embedded\ntexts elicit are extracted, which are then used to train a simple\nclassification head for hateful meme detection. Second, by developing a novel\nhard mining approach introducing directly LMM-encoded knowledge to the training\nprocess, providing further improvements. We perform extensive experiments on\ntwo datasets that validate the effectiveness of the proposed method, achieving\nstate-of-the-art performance. Our code and trained models are publicly\navailable at: https://github.com/IDT-ITI/LMM-CLIP-meme."}
{"id": "2504.08764", "pdf": "https://arxiv.org/pdf/2504.08764", "abs": "https://arxiv.org/abs/2504.08764", "authors": ["Chris Brogly", "Saif Rjaibi", "Charlotte Liang", "Erica Lam", "Edward Wang", "Adam Levitan", "Sarah Paleczny", "Michael Cusimano"], "title": "Evaluation of the phi-3-mini SLM for identification of texts related to medicine, health, and sports injuries", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Small Language Models (SLMs) have potential to be used for automatically\nlabelling and identifying aspects of text data for medicine/health-related\npurposes from documents and the web. As their resource requirements are\nsignificantly lower than Large Language Models (LLMs), these can be deployed\npotentially on more types of devices. SLMs often are benchmarked on\nhealth/medicine-related tasks, such as MedQA, although performance on these can\nvary especially depending on the size of the model in terms of number of\nparameters. Furthermore, these test results may not necessarily reflect\nreal-world performance regarding the automatic labelling or identification of\ntexts in documents and the web. As a result, we compared topic-relatedness\nscores from Microsofts phi-3-mini-4k-instruct SLM to the topic-relatedness\nscores from 7 human evaluators on 1144 samples of medical/health-related texts\nand 1117 samples of sports injury-related texts. These texts were from a larger\ndataset of about 9 million news headlines, each of which were processed and\nassigned scores by phi-3-mini-4k-instruct. Our sample was selected (filtered)\nbased on 1 (low filtering) or more (high filtering) Boolean conditions on the\nphi-3 SLM scores. We found low-moderate significant correlations between the\nscores from the SLM and human evaluators for sports injury texts with low\nfiltering (\\r{ho} = 0.3413, p < 0.001) and medicine/health texts with high\nfiltering (\\r{ho} = 0.3854, p < 0.001), and low significant correlation for\nmedicine/health texts with low filtering (\\r{ho} = 0.2255, p < 0.001). There\nwas negligible, insignificant correlation for sports injury-related texts with\nhigh filtering (\\r{ho} = 0.0318, p = 0.4466)."}
{"id": "2504.09925", "pdf": "https://arxiv.org/pdf/2504.09925", "abs": "https://arxiv.org/abs/2504.09925", "authors": ["Zheng Liu", "Mengjie Liu", "Jingzhou Chen", "Jingwei Xu", "Bin Cui", "Conghui He", "Wentao Zhang"], "title": "FUSION: Fully Integration of Vision-Language Representations for Deep Cross-Modal Understanding", "categories": ["cs.CV"], "comment": null, "summary": "We introduce FUSION, a family of multimodal large language models (MLLMs)\nwith a fully vision-language alignment and integration paradigm. Unlike\nexisting methods that primarily rely on late-stage modality interaction during\nLLM decoding, our approach achieves deep, dynamic integration throughout the\nentire processing pipeline. To this end, we propose Text-Guided Unified Vision\nEncoding, incorporating textual information in vision encoding to achieve\npixel-level integration. We further design Context-Aware Recursive Alignment\nDecoding that recursively aggregates visual features conditioned on textual\ncontext during decoding, enabling fine-grained, question-level semantic\nintegration. To guide feature mapping and mitigate modality discrepancies, we\ndevelop Dual-Supervised Semantic Mapping Loss. Additionally, we construct a\nSynthesized Language-Driven Question-Answer (QA) dataset through a new data\nsynthesis method, prioritizing high-quality QA pairs to optimize text-guided\nfeature integration. Building on these foundations, we train FUSION at two\nscales-3B, 8B-and demonstrate that our full-modality integration approach\nsignificantly outperforms existing methods with only 630 vision tokens.\nNotably, FUSION 3B surpasses Cambrian-1 8B and Florence-VL 8B on most\nbenchmarks. FUSION 3B continues to outperform Cambrian-1 8B even when limited\nto 300 vision tokens. Our ablation studies show that FUSION outperforms\nLLaVA-NeXT on over half of the benchmarks under same configuration without\ndynamic resolution, highlighting the effectiveness of our approach. We release\nour code, model weights, and dataset. https://github.com/starriver030515/FUSION"}
{"id": "2504.08777", "pdf": "https://arxiv.org/pdf/2504.08777", "abs": "https://arxiv.org/abs/2504.08777", "authors": ["Teo Susnjak", "Cole Palffy", "Tatiana Zimina", "Nazgul Altynbekova", "Kunal Garg", "Leona Gilbert"], "title": "The Lyme Disease Controversy: An AI-Driven Discourse Analysis of a Quarter Century of Academic Debate and Divides", "categories": ["cs.CY", "cs.CL"], "comment": null, "summary": "The scientific discourse surrounding Chronic Lyme Disease (CLD) and\nPost-Treatment Lyme Disease Syndrome (PTLDS) has evolved over the past\ntwenty-five years into a complex and polarised debate, shaped by shifting\nresearch priorities, institutional influences, and competing explanatory\nmodels. This study presents the first large-scale, systematic examination of\nthis discourse using an innovative hybrid AI-driven methodology, combining\nlarge language models with structured human validation to analyse thousands of\nscholarly abstracts spanning 25 years. By integrating Large Language Models\n(LLMs) with expert oversight, we developed a quantitative framework for\ntracking epistemic shifts in contested medical fields, with applications to\nother content analysis domains. Our analysis revealed a progressive transition\nfrom infection-based models of Lyme disease to immune-mediated explanations for\npersistent symptoms. This study offers new empirical insights into the\nstructural and epistemic forces shaping Lyme disease research, providing a\nscalable and replicable methodology for analysing discourse, while underscoring\nthe value of AI-assisted methodologies in social science and medical research."}
{"id": "2504.09948", "pdf": "https://arxiv.org/pdf/2504.09948", "abs": "https://arxiv.org/abs/2504.09948", "authors": ["Huijie Liu", "Bingcan Wang", "Jie Hu", "Xiaoming Wei", "Guoliang Kang"], "title": "Omni-Dish: Photorealistic and Faithful Image Generation and Editing for Arbitrary Chinese Dishes", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "10 pages, 10 figures, 3 tables", "summary": "Dish images play a crucial role in the digital era, with the demand for\nculturally distinctive dish images continuously increasing due to the\ndigitization of the food industry and e-commerce. In general cases, existing\ntext-to-image generation models excel in producing high-quality images;\nhowever, they struggle to capture diverse characteristics and faithful details\nof specific domains, particularly Chinese dishes. To address this limitation,\nwe propose Omni-Dish, the first text-to-image generation model specifically\ntailored for Chinese dishes. We develop a comprehensive dish curation pipeline,\nbuilding the largest dish dataset to date. Additionally, we introduce a\nrecaption strategy and employ a coarse-to-fine training scheme to help the\nmodel better learn fine-grained culinary nuances. During inference, we enhance\nthe user's textual input using a pre-constructed high-quality caption library\nand a large language model, enabling more photorealistic and faithful image\ngeneration. Furthermore, to extend our model's capability for dish editing\ntasks, we propose Concept-Enhanced P2P. Based on this approach, we build a dish\nediting dataset and train a specialized editing model. Extensive experiments\ndemonstrate the superiority of our methods."}
{"id": "2504.08801", "pdf": "https://arxiv.org/pdf/2504.08801", "abs": "https://arxiv.org/abs/2504.08801", "authors": ["Andrew Kiruluta", "Priscilla Burity", "Samantha Williams"], "title": "Learnable Multi-Scale Wavelet Transformer: A Novel Alternative to Self-Attention", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Transformer architectures, underpinned by the self-attention mechanism, have\nachieved state-of-the-art results across numerous natural language processing\n(NLP) tasks by effectively modeling long-range dependencies. However, the\ncomputational complexity of self-attention, scaling quadratically with input\nsequence length, presents significant challenges for processing very long\nsequences or operating under resource constraints. This paper introduces the\nLearnable Multi-Scale Wavelet Transformer (LMWT), a novel architecture that\nreplaces the standard dot-product self-attention with a learnable multi-scale\nHaar wavelet transform module. Leveraging the intrinsic multi-resolution\nproperties of wavelets, the LMWT efficiently captures both local details and\nglobal context. Crucially, the parameters of the wavelet transform, including\nscale-specific coefficients, are learned end-to-end during training, allowing\nthe model to adapt its decomposition strategy to the data and task. We present\nthe detailed mathematical formulation of the learnable Haar wavelet module and\nits integration into the transformer framework, supplemented by an\narchitectural diagram. We conduct a comprehensive experimental evaluation on a\nstandard machine translation benchmark (WMT16 En-De), comparing the LMWT\nagainst a baseline self-attention transformer using metrics like BLEU score,\nperplexity, and token accuracy. Furthermore, we analyze the computational\ncomplexity, highlighting the linear scaling of our approach, discuss its\nnovelty in the context of related work, and explore the interpretability\noffered by visualizing the learned Haar coefficients. Our results indicate that\nthe LMWT achieves competitive performance while offering substantial\ncomputational advantages, positioning it as a promising and novel alternative\nfor efficient sequence modeling."}
{"id": "2504.09953", "pdf": "https://arxiv.org/pdf/2504.09953", "abs": "https://arxiv.org/abs/2504.09953", "authors": ["Katja Ludwig", "Yuliia Oksymets", "Robin Schön", "Daniel Kienzle", "Rainer Lienhart"], "title": "Efficient 2D to Full 3D Human Pose Uplifting including Joint Rotations", "categories": ["cs.CV"], "comment": "accepted at CVSports@CVPR'25", "summary": "In sports analytics, accurately capturing both the 3D locations and rotations\nof body joints is essential for understanding an athlete's biomechanics. While\nHuman Mesh Recovery (HMR) models can estimate joint rotations, they often\nexhibit lower accuracy in joint localization compared to 3D Human Pose\nEstimation (HPE) models. Recent work addressed this limitation by combining a\n3D HPE model with inverse kinematics (IK) to estimate both joint locations and\nrotations. However, IK is computationally expensive. To overcome this, we\npropose a novel 2D-to-3D uplifting model that directly estimates 3D human\nposes, including joint rotations, in a single forward pass. We investigate\nmultiple rotation representations, loss functions, and training strategies -\nboth with and without access to ground truth rotations. Our models achieve\nstate-of-the-art accuracy in rotation estimation, are 150 times faster than the\nIK-based approach, and surpass HMR models in joint localization precision."}
{"id": "2504.08804", "pdf": "https://arxiv.org/pdf/2504.08804", "abs": "https://arxiv.org/abs/2504.08804", "authors": ["Pooya Razavi", "Sonya J. Powers"], "title": "Estimating Item Difficulty Using Large Language Models and Tree-Based Machine Learning Algorithms", "categories": ["cs.CY", "cs.CL", "cs.LG"], "comment": null, "summary": "Estimating item difficulty through field-testing is often resource-intensive\nand time-consuming. As such, there is strong motivation to develop methods that\ncan predict item difficulty at scale using only the item content. Large\nLanguage Models (LLMs) represent a new frontier for this goal. The present\nresearch examines the feasibility of using an LLM to predict item difficulty\nfor K-5 mathematics and reading assessment items (N = 5170). Two estimation\napproaches were implemented: (a) a direct estimation method that prompted the\nLLM to assign a single difficulty rating to each item, and (b) a feature-based\nstrategy where the LLM extracted multiple cognitive and linguistic features,\nwhich were then used in ensemble tree-based models (random forests and gradient\nboosting) to predict difficulty. Overall, direct LLM estimates showed moderate\nto strong correlations with true item difficulties. However, their accuracy\nvaried by grade level, often performing worse for early grades. In contrast,\nthe feature-based method yielded stronger predictive accuracy, with\ncorrelations as high as r = 0.87 and lower error estimates compared to both\ndirect LLM predictions and baseline regressors. These findings highlight the\npromise of LLMs in streamlining item development and reducing reliance on\nextensive field testing and underscore the importance of structured feature\nextraction. We provide a seven-step workflow for testing professionals who\nwould want to implement a similar item difficulty estimation approach with\ntheir item pool."}
{"id": "2504.09956", "pdf": "https://arxiv.org/pdf/2504.09956", "abs": "https://arxiv.org/abs/2504.09956", "authors": ["Katarzyna Filus", "Michał Romaszewski", "Mateusz Żarski"], "title": "Semantic Depth Matters: Explaining Errors of Deep Vision Networks through Perceived Class Similarities", "categories": ["cs.CV", "cs.LG", "68T01", "I.2.6"], "comment": null, "summary": "Understanding deep neural network (DNN) behavior requires more than\nevaluating classification accuracy alone; analyzing errors and their\npredictability is equally crucial. Current evaluation methodologies lack\ntransparency, particularly in explaining the underlying causes of network\nmisclassifications. To address this, we introduce a novel framework that\ninvestigates the relationship between the semantic hierarchy depth perceived by\na network and its real-data misclassification patterns. Central to our\nframework is the Similarity Depth (SD) metric, which quantifies the semantic\nhierarchy depth perceived by a network along with a method of evaluation of how\nclosely the network's errors align with its internally perceived similarity\nstructure. We also propose a graph-based visualization of model semantic\nrelationships and misperceptions. A key advantage of our approach is that\nleveraging class templates -- representations derived from classifier layer\nweights -- is applicable to already trained networks without requiring\nadditional data or experiments. Our approach reveals that deep vision networks\nencode specific semantic hierarchies and that high semantic depth improves the\ncompliance between perceived class similarities and actual errors."}
{"id": "2504.08812", "pdf": "https://arxiv.org/pdf/2504.08812", "abs": "https://arxiv.org/abs/2504.08812", "authors": ["David O. Johnston", "Arkajyoti Chakraborty", "Nora Belrose"], "title": "Mechanistic Anomaly Detection for \"Quirky\" Language Models", "categories": ["cs.LG", "cs.CL"], "comment": "ICLR Building Trust Workshop 2025", "summary": "As LLMs grow in capability, the task of supervising LLMs becomes more\nchallenging. Supervision failures can occur if LLMs are sensitive to factors\nthat supervisors are unaware of. We investigate Mechanistic Anomaly Detection\n(MAD) as a technique to augment supervision of capable models; we use internal\nmodel features to identify anomalous training signals so they can be\ninvestigated or discarded. We train detectors to flag points from the test\nenvironment that differ substantially from the training environment, and\nexperiment with a large variety of detector features and scoring rules to\ndetect anomalies in a set of ``quirky'' language models. We find that detectors\ncan achieve high discrimination on some tasks, but no detector is effective\nacross all models and tasks. MAD techniques may be effective in low-stakes\napplications, but advances in both detection and evaluation are likely needed\nif they are to be used in high stakes settings."}
{"id": "2504.09960", "pdf": "https://arxiv.org/pdf/2504.09960", "abs": "https://arxiv.org/abs/2504.09960", "authors": ["Hoang M. Truong", "Vinh-Thuan Ly", "Huy G. Tran", "Thuan-Phat Nguyen", "Tram T. Doan"], "title": "Dual-Path Enhancements in Event-Based Eye Tracking: Augmented Robustness and Adaptive Temporal Modeling", "categories": ["cs.CV"], "comment": "Camera-ready version for CVPRW 2025. Accepted for presentation at the\n  IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops\n  (CVPRW 2025)", "summary": "Event-based eye tracking has become a pivotal technology for augmented\nreality and human-computer interaction. Yet, existing methods struggle with\nreal-world challenges such as abrupt eye movements and environmental noise.\nBuilding on the efficiency of the Lightweight Spatiotemporal Network-a causal\narchitecture optimized for edge devices-we introduce two key advancements.\nFirst, a robust data augmentation pipeline incorporating temporal shift,\nspatial flip, and event deletion improves model resilience, reducing Euclidean\ndistance error by 12% (1.61 vs. 1.70 baseline) on challenging samples. Second,\nwe propose KnightPupil, a hybrid architecture combining an EfficientNet-B3\nbackbone for spatial feature extraction, a bidirectional GRU for contextual\ntemporal modeling, and a Linear Time-Varying State-Space Module to adapt to\nsparse inputs and noise dynamically. Evaluated on the 3ET+ benchmark, our\nframework achieved 1.61 Euclidean distance on the private test set of the\nEvent-based Eye Tracking Challenge at CVPR 2025, demonstrating its\neffectiveness for practical deployment in AR/VR systems while providing a\nfoundation for future innovations in neuromorphic vision."}
{"id": "2504.08846", "pdf": "https://arxiv.org/pdf/2504.08846", "abs": "https://arxiv.org/abs/2504.08846", "authors": ["Mostafa Faghih Shojaei", "Rahul Gulati", "Benjamin A. Jasperson", "Shangshang Wang", "Simone Cimolato", "Dangli Cao", "Willie Neiswanger", "Krishna Garikipati"], "title": "AI-University: An LLM-based platform for instructional alignment to scientific classrooms", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "comment": "10 pages, 3 figures", "summary": "We introduce AI University (AI-U), a flexible framework for AI-driven course\ncontent delivery that adapts to instructors' teaching styles. At its core, AI-U\nfine-tunes a large language model (LLM) with retrieval-augmented generation\n(RAG) to generate instructor-aligned responses from lecture videos, notes, and\ntextbooks. Using a graduate-level finite-element-method (FEM) course as a case\nstudy, we present a scalable pipeline to systematically construct training\ndata, fine-tune an open-source LLM with Low-Rank Adaptation (LoRA), and\noptimize its responses through RAG-based synthesis. Our evaluation - combining\ncosine similarity, LLM-based assessment, and expert review - demonstrates\nstrong alignment with course materials. We also have developed a prototype web\napplication, available at https://my-ai-university.com, that enhances\ntraceability by linking AI-generated responses to specific sections of the\nrelevant course material and time-stamped instances of the open-access video\nlectures. Our expert model is found to have greater cosine similarity with a\nreference on 86% of test cases. An LLM judge also found our expert model to\noutperform the base Llama 3.2 model approximately four times out of five. AI-U\noffers a scalable approach to AI-assisted education, paving the way for broader\nadoption in higher education. Here, our framework has been presented in the\nsetting of a class on FEM - a subject that is central to training PhD and\nMaster students in engineering science. However, this setting is a particular\ninstance of a broader context: fine-tuning LLMs to research content in science."}
{"id": "2504.09966", "pdf": "https://arxiv.org/pdf/2504.09966", "abs": "https://arxiv.org/abs/2504.09966", "authors": ["Dongliang Luo", "Hanshen Zhu", "Ziyang Zhang", "Dingkang Liang", "Xudong Xie", "Yuliang Liu", "Xiang Bai"], "title": "SemiETS: Integrating Spatial and Content Consistencies for Semi-Supervised End-to-end Text Spotting", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025. Code will be available at\n  \\url{https://github.com/DrLuo/SemiETS}", "summary": "Most previous scene text spotting methods rely on high-quality manual\nannotations to achieve promising performance. To reduce their expensive costs,\nwe study semi-supervised text spotting (SSTS) to exploit useful information\nfrom unlabeled images. However, directly applying existing semi-supervised\nmethods of general scenes to SSTS will face new challenges: 1) inconsistent\npseudo labels between detection and recognition tasks, and 2) sub-optimal\nsupervisions caused by inconsistency between teacher/student. Thus, we propose\na new Semi-supervised framework for End-to-end Text Spotting, namely SemiETS\nthat leverages the complementarity of text detection and recognition.\nSpecifically, it gradually generates reliable hierarchical pseudo labels for\neach task, thereby reducing noisy labels. Meanwhile, it extracts important\ninformation in locations and transcriptions from bidirectional flows to improve\nconsistency. Extensive experiments on three datasets under various settings\ndemonstrate the effectiveness of SemiETS on arbitrary-shaped text. For example,\nit outperforms previous state-of-the-art SSL methods by a large margin on\nend-to-end spotting (+8.7%, +5.6%, and +2.6% H-mean under 0.5%, 1%, and 2%\nlabeled data settings on Total-Text, respectively). More importantly, it still\nimproves upon a strongly supervised text spotter trained with plenty of labeled\ndata by 2.0%. Compelling domain adaptation ability shows practical potential.\nMoreover, our method demonstrates consistent improvement on different text\nspotters."}
{"id": "2504.08907", "pdf": "https://arxiv.org/pdf/2504.08907", "abs": "https://arxiv.org/abs/2504.08907", "authors": ["Ayushi Mishra", "Yang Bai", "Priyadarshan Narayanasamy", "Nakul Garg", "Nirupam Roy"], "title": "Spatial Audio Processing with Large Language Model on Wearable Devices", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "Integrating spatial context into large language models (LLMs) has the\npotential to revolutionize human-computer interaction, particularly in wearable\ndevices. In this work, we present a novel system architecture that incorporates\nspatial speech understanding into LLMs, enabling contextually aware and\nadaptive applications for wearable technologies. Our approach leverages\nmicrostructure-based spatial sensing to extract precise Direction of Arrival\n(DoA) information using a monaural microphone. To address the lack of existing\ndataset for microstructure-assisted speech recordings, we synthetically create\na dataset called OmniTalk by using the LibriSpeech dataset. This spatial\ninformation is fused with linguistic embeddings from OpenAI's Whisper model,\nallowing each modality to learn complementary contextual representations. The\nfused embeddings are aligned with the input space of LLaMA-3.2 3B model and\nfine-tuned with lightweight adaptation technique LoRA to optimize for on-device\nprocessing. SING supports spatially-aware automatic speech recognition (ASR),\nachieving a mean error of $25.72^\\circ$-a substantial improvement compared to\nthe 88.52$^\\circ$ median error in existing work-with a word error rate (WER) of\n5.3. SING also supports soundscaping, for example, inference how many people\nwere talking and their directions, with up to 5 people and a median DoA error\nof 16$^\\circ$. Our system demonstrates superior performance in spatial speech\nunderstanding while addressing the challenges of power efficiency, privacy, and\nhardware constraints, paving the way for advanced applications in augmented\nreality, accessibility, and immersive experiences."}
{"id": "2504.09967", "pdf": "https://arxiv.org/pdf/2504.09967", "abs": "https://arxiv.org/abs/2504.09967", "authors": ["Xun Zhu", "Fanbin Mo", "Zheng Zhang", "Jiaxi Wang", "Yiming Shi", "Ming Wu", "Chuang Zhang", "Miao Li", "Ji Wu"], "title": "Enhancing Multi-task Learning Capability of Medical Generalist Foundation Model via Image-centric Multi-annotation Data", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The emergence of medical generalist foundation models has revolutionized\nconventional task-specific model development paradigms, aiming to better handle\nmultiple tasks through joint training on large-scale medical datasets. However,\nrecent advances prioritize simple data scaling or architectural component\nenhancement, while neglecting to re-examine multi-task learning from a\ndata-centric perspective. Critically, simply aggregating existing data\nresources leads to decentralized image-task alignment, which fails to cultivate\ncomprehensive image understanding or align with clinical needs for\nmulti-dimensional image interpretation. In this paper, we introduce the\nimage-centric multi-annotation X-ray dataset (IMAX), the first attempt to\nenhance the multi-task learning capabilities of medical multi-modal large\nlanguage models (MLLMs) from the data construction level. To be specific, IMAX\nis featured from the following attributes: 1) High-quality data curation. A\ncomprehensive collection of more than 354K entries applicable to seven\ndifferent medical tasks. 2) Image-centric dense annotation. Each X-ray image is\nassociated with an average of 4.10 tasks and 7.46 training entries, ensuring\nmulti-task representation richness per image. Compared to the general\ndecentralized multi-annotation X-ray dataset (DMAX), IMAX consistently\ndemonstrates significant multi-task average performance gains ranging from\n3.20% to 21.05% across seven open-source state-of-the-art medical MLLMs.\nMoreover, we investigate differences in statistical patterns exhibited by IMAX\nand DMAX training processes, exploring potential correlations between\noptimization dynamics and multi-task performance. Finally, leveraging the core\nconcept of IMAX data construction, we propose an optimized DMAX-based training\nstrategy to alleviate the dilemma of obtaining high-quality IMAX data in\npractical scenarios."}
{"id": "2504.08942", "pdf": "https://arxiv.org/pdf/2504.08942", "abs": "https://arxiv.org/abs/2504.08942", "authors": ["Xing Han Lù", "Amirhossein Kazemnejad", "Nicholas Meade", "Arkil Patel", "Dongchan Shin", "Alejandra Zambrano", "Karolina Stańczak", "Peter Shaw", "Christopher J. Pal", "Siva Reddy"], "title": "AgentRewardBench: Evaluating Automatic Evaluations of Web Agent Trajectories", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Web agents enable users to perform tasks on web browsers through natural\nlanguage interaction. Evaluating web agents trajectories is an important\nproblem, since it helps us determine whether the agent successfully completed\nthe tasks. Rule-based methods are widely used for this purpose, but they are\nchallenging to extend to new tasks and may not always recognize successful\ntrajectories. We may achieve higher accuracy through human evaluation, but the\nprocess would be substantially slower and more expensive. Automatic evaluations\nwith LLMs may avoid the challenges of designing new rules and manually\nannotating trajectories, enabling faster and cost-effective evaluation.\nHowever, it is unclear how effective they are at evaluating web agents. To this\nend, we propose AgentRewardBench, the first benchmark to assess the\neffectiveness of LLM judges for evaluating web agents. AgentRewardBench\ncontains 1302 trajectories across 5 benchmarks and 4 LLMs. Each trajectory in\nAgentRewardBench is reviewed by an expert, who answers questions pertaining to\nthe success, side effects, and repetitiveness of the agent. Using our\nbenchmark, we evaluate 12 LLM judges and find that no single LLM excels across\nall benchmarks. We also find that the rule-based evaluation used by common\nbenchmarks tends to underreport the success rate of web agents, highlighting a\nkey weakness of rule-based evaluation and the need to develop more flexible\nautomatic evaluations. We release the benchmark at:\nhttps://agent-reward-bench.github.io"}
{"id": "2504.09973", "pdf": "https://arxiv.org/pdf/2504.09973", "abs": "https://arxiv.org/abs/2504.09973", "authors": ["Gang Wu", "Junjun Jiang", "Kui Jiang", "Xianming Liu", "Liqiang Nie"], "title": "Beyond Degradation Redundancy: Contrastive Prompt Learning for All-in-One Image Restoration", "categories": ["cs.CV"], "comment": "Project page: https://github.com/Aitical/CPLIR", "summary": "All-in-one image restoration, addressing diverse degradation types with a\nunified model, presents significant challenges in designing task-specific\nprompts that effectively guide restoration across multiple degradation\nscenarios. While adaptive prompt learning enables end-to-end optimization, it\noften yields overlapping or redundant task representations. Conversely,\nexplicit prompts derived from pretrained classifiers enhance discriminability\nbut may discard critical visual information for reconstruction. To address\nthese limitations, we introduce Contrastive Prompt Learning (CPL), a novel\nframework that fundamentally enhances prompt-task alignment through two\ncomplementary innovations: a \\emph{Sparse Prompt Module (SPM)} that efficiently\ncaptures degradation-specific features while minimizing redundancy, and a\n\\emph{Contrastive Prompt Regularization (CPR)} that explicitly strengthens task\nboundaries by incorporating negative prompt samples across different\ndegradation types. Unlike previous approaches that focus primarily on\ndegradation classification, CPL optimizes the critical interaction between\nprompts and the restoration model itself. Extensive experiments across five\ncomprehensive benchmarks demonstrate that CPL consistently enhances\nstate-of-the-art all-in-one restoration models, achieving significant\nimprovements in both standard multi-task scenarios and challenging composite\ndegradation settings. Our framework establishes new state-of-the-art\nperformance while maintaining parameter efficiency, offering a principled\nsolution for unified image restoration."}
{"id": "2504.08949", "pdf": "https://arxiv.org/pdf/2504.08949", "abs": "https://arxiv.org/abs/2504.08949", "authors": ["Haokai Ma", "Yunshan Ma", "Ruobing Xie", "Lei Meng", "Jialie Shen", "Xingwu Sun", "Zhanhui Kang", "Tat-Seng Chua"], "title": "Large Language Model Empowered Recommendation Meets All-domain Continual Pre-Training", "categories": ["cs.IR", "cs.CL"], "comment": "In submission", "summary": "Recent research efforts have investigated how to integrate Large Language\nModels (LLMs) into recommendation, capitalizing on their semantic comprehension\nand open-world knowledge for user behavior understanding. These approaches\npredominantly employ supervised fine-tuning on single-domain user interactions\nto adapt LLMs for specific recommendation tasks. However, they typically\nencounter dual challenges: the mismatch between general language\nrepresentations and domain-specific preference patterns, as well as the limited\nadaptability to multi-domain recommendation scenarios. To bridge these gaps, we\nintroduce CPRec -- an All-domain Continual Pre-Training framework for\nRecommendation -- designed to holistically align LLMs with universal user\nbehaviors through the continual pre-training paradigm. Specifically, we first\ndesign a unified prompt template and organize users' multi-domain behaviors\ninto domain-specific behavioral sequences and all-domain mixed behavioral\nsequences that emulate real-world user decision logic. To optimize behavioral\nknowledge infusion, we devise a Warmup-Stable-Annealing learning rate schedule\ntailored for the continual pre-training paradigm in recommendation to\nprogressively enhance the LLM's capability in knowledge adaptation from\nopen-world knowledge to universal recommendation tasks. To evaluate the\neffectiveness of our CPRec, we implement it on a large-scale dataset covering\nseven domains and conduct extensive experiments on five real-world datasets\nfrom two distinct platforms. Experimental results confirm that our continual\npre-training paradigm significantly mitigates the semantic-behavioral\ndiscrepancy and achieves state-of-the-art performance in all recommendation\nscenarios. The source code will be released upon acceptance."}
{"id": "2504.09979", "pdf": "https://arxiv.org/pdf/2504.09979", "abs": "https://arxiv.org/abs/2504.09979", "authors": ["Teppei Suzuki", "Keisuke Ozawa"], "title": "Resampling Benchmark for Efficient Comprehensive Evaluation of Large Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "We propose an efficient evaluation protocol for large vision-language models\n(VLMs). Given their broad knowledge and reasoning capabilities, multiple\nbenchmarks are needed for comprehensive assessment, making evaluation\ncomputationally expensive. To improve efficiency, we construct a subset that\nyields results comparable to full benchmark evaluations. Our benchmark\nclassification experiments reveal that no single benchmark fully covers all\nchallenges. We then introduce a subset construction method using farthest point\nsampling (FPS). Our experiments show that FPS-based benchmarks maintain a\nstrong correlation (> 0.96) with full evaluations while using only ~1\\% of the\ndata. Additionally, applying FPS to an existing benchmark improves correlation\nwith overall evaluation results, suggesting its potential to reduce unintended\ndataset biases."}
{"id": "2504.09037", "pdf": "https://arxiv.org/pdf/2504.09037", "abs": "https://arxiv.org/abs/2504.09037", "authors": ["Zixuan Ke", "Fangkai Jiao", "Yifei Ming", "Xuan-Phi Nguyen", "Austin Xu", "Do Xuan Long", "Minzhi Li", "Chengwei Qin", "Peifeng Wang", "Silvio Savarese", "Caiming Xiong", "Shafiq Joty"], "title": "A Survey of Frontiers in LLM Reasoning: Inference Scaling, Learning to Reason, and Agentic Systems", "categories": ["cs.AI", "cs.CL"], "comment": "72 pages, 6 figures", "summary": "Reasoning is a fundamental cognitive process that enables logical inference,\nproblem-solving, and decision-making. With the rapid advancement of large\nlanguage models (LLMs), reasoning has emerged as a key capability that\ndistinguishes advanced AI systems from conventional models that empower\nchatbots. In this survey, we categorize existing methods along two orthogonal\ndimensions: (1) Regimes, which define the stage at which reasoning is achieved\n(either at inference time or through dedicated training); and (2)\nArchitectures, which determine the components involved in the reasoning\nprocess, distinguishing between standalone LLMs and agentic compound systems\nthat incorporate external tools, and multi-agent collaborations. Within each\ndimension, we analyze two key perspectives: (1) Input level, which focuses on\ntechniques that construct high-quality prompts that the LLM condition on; and\n(2) Output level, which methods that refine multiple sampled candidates to\nenhance reasoning quality. This categorization provides a systematic\nunderstanding of the evolving landscape of LLM reasoning, highlighting emerging\ntrends such as the shift from inference-scaling to learning-to-reason (e.g.,\nDeepSeek-R1), and the transition to agentic workflows (e.g., OpenAI Deep\nResearch, Manus Agent). Additionally, we cover a broad spectrum of learning\nalgorithms, from supervised fine-tuning to reinforcement learning such as PPO\nand GRPO, and the training of reasoners and verifiers. We also examine key\ndesigns of agentic workflows, from established patterns like\ngenerator-evaluator and LLM debate to recent innovations. ..."}
{"id": "2504.09990", "pdf": "https://arxiv.org/pdf/2504.09990", "abs": "https://arxiv.org/abs/2504.09990", "authors": ["LeiLei Ma", "Shuo Xu", "MingKun Xie", "Lei Wang", "Dengdi Sun", "Haifeng Zhao"], "title": "Correlative and Discriminative Label Grouping for Multi-Label Visual Prompt Tuning", "categories": ["cs.CV"], "comment": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n  2025", "summary": "Modeling label correlations has always played a pivotal role in multi-label\nimage classification (MLC), attracting significant attention from researchers.\nHowever, recent studies have overemphasized co-occurrence relationships among\nlabels, which can lead to overfitting risk on this overemphasis, resulting in\nsuboptimal models. To tackle this problem, we advocate for balancing\ncorrelative and discriminative relationships among labels to mitigate the risk\nof overfitting and enhance model performance. To this end, we propose the\nMulti-Label Visual Prompt Tuning framework, a novel and parameter-efficient\nmethod that groups classes into multiple class subsets according to label\nco-occurrence and mutual exclusivity relationships, and then models them\nrespectively to balance the two relationships. In this work, since each group\ncontains multiple classes, multiple prompt tokens are adopted within Vision\nTransformer (ViT) to capture the correlation or discriminative label\nrelationship within each group, and effectively learn correlation or\ndiscriminative representations for class subsets. On the other hand, each group\ncontains multiple group-aware visual representations that may correspond to\nmultiple classes, and the mixture of experts (MoE) model can cleverly assign\nthem from the group-aware to the label-aware, adaptively obtaining label-aware\nrepresentation, which is more conducive to classification. Experiments on\nmultiple benchmark datasets show that our proposed approach achieves\ncompetitive results and outperforms SOTA methods on multiple pre-trained\nmodels."}
{"id": "2504.09058", "pdf": "https://arxiv.org/pdf/2504.09058", "abs": "https://arxiv.org/abs/2504.09058", "authors": ["Chengyuan Liu", "Shihang Wang", "Lizhi Qing", "Kaisong Song", "Junjie Cao", "Jun Lin", "Ji Zhang", "Ang Li", "Kun Kuang", "Fei Wu"], "title": "Towards Stepwise Domain Knowledge-Driven Reasoning Optimization and Reflection Improvement", "categories": ["cs.AI", "cs.CL"], "comment": "Under review", "summary": "Recently, stepwise supervision on Chain of Thoughts (CoTs) presents an\nenhancement on the logical reasoning tasks such as coding and math, with the\nhelp of Monte Carlo Tree Search (MCTS). However, its contribution to tasks\nrequiring domain-specific expertise and knowledge remains unexplored. Motivated\nby the interest, we identify several potential challenges of vanilla MCTS\nwithin this context, and propose the framework of Stepwise Domain\nKnowledge-Driven Reasoning Optimization, employing the MCTS algorithm to\ndevelop step-level supervision for problems that require essential\ncomprehension, reasoning, and specialized knowledge. Additionally, we also\nintroduce the Preference Optimization towards Reflection Paths, which\niteratively learns self-reflection on the reasoning thoughts from better\nperspectives. We have conducted extensive experiments to evaluate the advantage\nof the methodologies. Empirical results demonstrate the effectiveness on\nvarious legal-domain problems. We also report a diverse set of valuable\nfindings, hoping to encourage the enthusiasm to the research of domain-specific\nLLMs and MCTS."}
{"id": "2504.09998", "pdf": "https://arxiv.org/pdf/2504.09998", "abs": "https://arxiv.org/abs/2504.09998", "authors": ["Alejandro Luque-Cerpa", "Elizabeth Polgreen", "Ajitha Rajan", "Hazem Torfah"], "title": "Metric-Guided Synthesis of Class Activation Mapping", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Class activation mapping (CAM) is a widely adopted class of saliency methods\nused to explain the behavior of convolutional neural networks (CNNs). These\nmethods generate heatmaps that highlight the parts of the input most relevant\nto the CNN output. Various CAM methods have been proposed, each distinguished\nby the expressions used to derive heatmaps. In general, users look for heatmaps\nwith specific properties that reflect different aspects of CNN functionality.\nThese may include similarity to ground truth, robustness, equivariance, and\nmore. Although existing CAM methods implicitly encode some of these properties\nin their expressions, they do not allow for variability in heatmap generation\nfollowing the user's intent or domain knowledge. In this paper, we address this\nlimitation by introducing SyCAM, a metric-based approach for synthesizing CAM\nexpressions. Given a predefined evaluation metric for saliency maps, SyCAM\nautomatically generates CAM expressions optimized for that metric. We\nspecifically explore a syntax-guided synthesis instantiation of SyCAM, where\nCAM expressions are derived based on predefined syntactic constraints and the\ngiven metric. Using several established evaluation metrics, we demonstrate the\nefficacy and flexibility of our approach in generating targeted heatmaps. We\ncompare SyCAM with other well-known CAM methods on three prominent models:\nResNet50, VGG16, and VGG19."}
{"id": "2504.09081", "pdf": "https://arxiv.org/pdf/2504.09081", "abs": "https://arxiv.org/abs/2504.09081", "authors": ["Prabhat Pandey", "Rupak Vignesh Swaminathan", "K V Vijay Girish", "Arunasish Sen", "Jian Xie", "Grant P. Strimel", "Andreas Schwarz"], "title": "SIFT-50M: A Large-Scale Multilingual Dataset for Speech Instruction Fine-Tuning", "categories": ["eess.AS", "cs.AI", "cs.CL"], "comment": null, "summary": "We introduce SIFT (Speech Instruction Fine-Tuning), a 50M-example dataset\ndesigned for instruction fine-tuning and pre-training of speech-text large\nlanguage models (LLMs). SIFT-50M is built from publicly available speech\ncorpora, which collectively contain 14K hours of speech, and leverages LLMs\nalong with off-the-shelf expert models. The dataset spans five languages,\nencompassing a diverse range of speech understanding as well as controllable\nspeech generation instructions. Using SIFT-50M, we train SIFT-LLM, which\noutperforms existing speech-text LLMs on instruction-following benchmarks while\nachieving competitive performance on foundational speech tasks. To support\nfurther research, we also introduce EvalSIFT, a benchmark dataset specifically\ndesigned to evaluate the instruction-following capabilities of speech-text\nLLMs."}
{"id": "2504.10001", "pdf": "https://arxiv.org/pdf/2504.10001", "abs": "https://arxiv.org/abs/2504.10001", "authors": ["Junlin Hao", "Peiheng Wang", "Haoyang Wang", "Xinggong Zhang", "Zongming Guo"], "title": "GaussVideoDreamer: 3D Scene Generation with Video Diffusion and Inconsistency-Aware Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Single-image 3D scene reconstruction presents significant challenges due to\nits inherently ill-posed nature and limited input constraints. Recent advances\nhave explored two promising directions: multiview generative models that train\non 3D consistent datasets but struggle with out-of-distribution generalization,\nand 3D scene inpainting and completion frameworks that suffer from cross-view\ninconsistency and suboptimal error handling, as they depend exclusively on\ndepth data or 3D smoothness, which ultimately degrades output quality and\ncomputational performance. Building upon these approaches, we present\nGaussVideoDreamer, which advances generative multimedia approaches by bridging\nthe gap between image, video, and 3D generation, integrating their strengths\nthrough two key innovations: (1) A progressive video inpainting strategy that\nharnesses temporal coherence for improved multiview consistency and faster\nconvergence. (2) A 3D Gaussian Splatting consistency mask to guide the video\ndiffusion with 3D consistent multiview evidence. Our pipeline combines three\ncore components: a geometry-aware initialization protocol, Inconsistency-Aware\nGaussian Splatting, and a progressive video inpainting strategy. Experimental\nresults demonstrate that our approach achieves 32% higher LLaVA-IQA scores and\nat least 2x speedup compared to existing methods while maintaining robust\nperformance across diverse scenes."}
{"id": "2504.09100", "pdf": "https://arxiv.org/pdf/2504.09100", "abs": "https://arxiv.org/abs/2504.09100", "authors": ["Chengyu Wang", "Taolin Zhang", "Richang Hong", "Jun Huang"], "title": "A Short Survey on Small Reasoning Models: Training, Inference, Applications and Research Directions", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recently, the reasoning capabilities of large reasoning models (LRMs), such\nas DeepSeek-R1, have seen significant advancements through the slow thinking\nprocess. Despite these achievements, the substantial computational demands of\nLRMs present considerable challenges. In contrast, small reasoning models\n(SRMs), often distilled from larger ones, offer greater efficiency and can\nexhibit distinct capabilities and cognitive trajectories compared to LRMs. This\nwork surveys around 170 recently published papers on SRMs for tackling various\ncomplex reasoning tasks. We review the current landscape of SRMs and analyze\ndiverse training and inference techniques related to SRMs. Furthermore, we\nprovide a comprehensive review of SRMs for domain-specific applications and\ndiscuss possible future research directions. This survey serves as an essential\nreference for researchers to leverage or develop SRMs for advanced reasoning\nfunctionalities with high efficiency."}
{"id": "2504.10004", "pdf": "https://arxiv.org/pdf/2504.10004", "abs": "https://arxiv.org/abs/2504.10004", "authors": ["Matías Piqueras", "Alexandra Segerberg", "Matteo Magnani", "Måns Magnusson", "Nataša Sladoje"], "title": "An Image is Worth $K$ Topics: A Visual Structural Topic Model with Pretrained Image Embeddings", "categories": ["cs.CV", "cs.CY", "stat.AP", "stat.ME"], "comment": null, "summary": "Political scientists are increasingly interested in analyzing visual content\nat scale. However, the existing computational toolbox is still in need of\nmethods and models attuned to the specific challenges and goals of social and\npolitical inquiry. In this article, we introduce a visual Structural Topic\nModel (vSTM) that combines pretrained image embeddings with a structural topic\nmodel. This has important advantages compared to existing approaches. First,\npretrained embeddings allow the model to capture the semantic complexity of\nimages relevant to political contexts. Second, the structural topic model\nprovides the ability to analyze how topics and covariates are related, while\nmaintaining a nuanced representation of images as a mixture of multiple topics.\nIn our empirical application, we show that the vSTM is able to identify topics\nthat are interpretable, coherent, and substantively relevant to the study of\nonline political communication."}
{"id": "2504.09257", "pdf": "https://arxiv.org/pdf/2504.09257", "abs": "https://arxiv.org/abs/2504.09257", "authors": ["Sohom Ghosh", "Arnab Maji", "Sudip Kumar Naskar"], "title": "MiMIC: Multi-Modal Indian Earnings Calls Dataset to Predict Stock Prices", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Code and Dataset:\n  https://huggingface.co/datasets/sohomghosh/MiMIC_Multi-Modal_Indian_Earnings_Calls_Dataset", "summary": "Predicting stock market prices following corporate earnings calls remains a\nsignificant challenge for investors and researchers alike, requiring innovative\napproaches that can process diverse information sources. This study\ninvestigates the impact of corporate earnings calls on stock prices by\nintroducing a multi-modal predictive model. We leverage textual data from\nearnings call transcripts, along with images and tables from accompanying\npresentations, to forecast stock price movements on the trading day immediately\nfollowing these calls. To facilitate this research, we developed the MiMIC\n(Multi-Modal Indian Earnings Calls) dataset, encompassing companies\nrepresenting the Nifty 50, Nifty MidCap 50, and Nifty Small 50 indices. The\ndataset includes earnings call transcripts, presentations, fundamentals,\ntechnical indicators, and subsequent stock prices. We present a multimodal\nanalytical framework that integrates quantitative variables with predictive\nsignals derived from textual and visual modalities, thereby enabling a holistic\napproach to feature representation and analysis. This multi-modal approach\ndemonstrates the potential for integrating diverse information sources to\nenhance financial forecasting accuracy. To promote further research in\ncomputational economics, we have made the MiMIC dataset publicly available\nunder the CC-NC-SA-4.0 licence. Our work contributes to the growing body of\nliterature on market reactions to corporate communications and highlights the\nefficacy of multi-modal machine learning techniques in financial analysis."}
{"id": "2504.10012", "pdf": "https://arxiv.org/pdf/2504.10012", "abs": "https://arxiv.org/abs/2504.10012", "authors": ["Yufei Deng", "Yuanjian Wang", "Rong Xiao", "Chenwei Tang", "Jizhe Zhou", "Jiahao Fan", "Deng Xiong", "Jiancheng Lv", "Huajin Tang"], "title": "EBAD-Gaussian: Event-driven Bundle Adjusted Deblur Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "While 3D Gaussian Splatting (3D-GS) achieves photorealistic novel view\nsynthesis, its performance degrades with motion blur. In scenarios with rapid\nmotion or low-light conditions, existing RGB-based deblurring methods struggle\nto model camera pose and radiance changes during exposure, reducing\nreconstruction accuracy. Event cameras, capturing continuous brightness changes\nduring exposure, can effectively assist in modeling motion blur and improving\nreconstruction quality. Therefore, we propose Event-driven Bundle Adjusted\nDeblur Gaussian Splatting (EBAD-Gaussian), which reconstructs sharp 3D\nGaussians from event streams and severely blurred images. This method jointly\nlearns the parameters of these Gaussians while recovering camera motion\ntrajectories during exposure time. Specifically, we first construct a blur loss\nfunction by synthesizing multiple latent sharp images during the exposure time,\nminimizing the difference between real and synthesized blurred images. Then we\nuse event stream to supervise the light intensity changes between latent sharp\nimages at any time within the exposure period, supplementing the light\nintensity dynamic changes lost in RGB images. Furthermore, we optimize the\nlatent sharp images at intermediate exposure times based on the event-based\ndouble integral (EDI) prior, applying consistency constraints to enhance the\ndetails and texture information of the reconstructed images. Extensive\nexperiments on synthetic and real-world datasets show that EBAD-Gaussian can\nachieve high-quality 3D scene reconstruction under the condition of blurred\nimages and event stream inputs."}
{"id": "2504.09265", "pdf": "https://arxiv.org/pdf/2504.09265", "abs": "https://arxiv.org/abs/2504.09265", "authors": ["Lei Kang", "Jia Li", "Mi Tian", "Hua Huang"], "title": "Mixture of Group Experts for Learning Invariant Representations", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Sparsely activated Mixture-of-Experts (MoE) models effectively increase the\nnumber of parameters while maintaining consistent computational costs per\ntoken. However, vanilla MoE models often suffer from limited diversity and\nspecialization among experts, constraining their performance and scalability,\nespecially as the number of experts increases. In this paper, we present a\nnovel perspective on vanilla MoE with top-$k$ routing inspired by sparse\nrepresentation. This allows us to bridge established theoretical insights from\nsparse representation into MoE models. Building on this foundation, we propose\na group sparse regularization approach for the input of top-$k$ routing, termed\nMixture of Group Experts (MoGE). MoGE indirectly regularizes experts by\nimposing structural constraints on the routing inputs, while preserving the\noriginal MoE architecture. Furthermore, we organize the routing input into a 2D\ntopographic map, spatially grouping neighboring elements. This structure\nenables MoGE to capture representations invariant to minor transformations,\nthereby significantly enhancing expert diversity and specialization.\nComprehensive evaluations across various Transformer models for image\nclassification and language modeling tasks demonstrate that MoGE substantially\noutperforms its MoE counterpart, with minimal additional memory and computation\noverhead. Our approach provides a simple yet effective solution to scale the\nnumber of experts and reduce redundancy among them. The source code is included\nin the supplementary material and will be publicly released."}
{"id": "2504.10018", "pdf": "https://arxiv.org/pdf/2504.10018", "abs": "https://arxiv.org/abs/2504.10018", "authors": ["Xiao Wang", "Haiyang Wang", "Shiao Wang", "Qiang Chen", "Jiandong Jin", "Haoyu Song", "Bo Jiang", "Chenglong Li"], "title": "RGB-Event based Pedestrian Attribute Recognition: A Benchmark Dataset and An Asymmetric RWKV Fusion Framework", "categories": ["cs.CV", "cs.AI"], "comment": "The First Benchmark Dataset for RGB-Event Multimodal Pedestrian\n  Attribute Recognition Task", "summary": "Existing pedestrian attribute recognition methods are generally developed\nbased on RGB frame cameras. However, these approaches are constrained by the\nlimitations of RGB cameras, such as sensitivity to lighting conditions and\nmotion blur, which hinder their performance. Furthermore, current attribute\nrecognition primarily focuses on analyzing pedestrians' external appearance and\nclothing, lacking an exploration of emotional dimensions. In this paper, we\nrevisit these issues and propose a novel multi-modal RGB-Event attribute\nrecognition task by drawing inspiration from the advantages of event cameras in\nlow-light, high-speed, and low-power consumption. Specifically, we introduce\nthe first large-scale multi-modal pedestrian attribute recognition dataset,\ntermed EventPAR, comprising 100K paired RGB-Event samples that cover 50\nattributes related to both appearance and six human emotions, diverse scenes,\nand various seasons. By retraining and evaluating mainstream PAR models on this\ndataset, we establish a comprehensive benchmark and provide a solid foundation\nfor future research in terms of data and algorithmic baselines. In addition, we\npropose a novel RWKV-based multi-modal pedestrian attribute recognition\nframework, featuring an RWKV visual encoder and an asymmetric RWKV fusion\nmodule. Extensive experiments are conducted on our proposed dataset as well as\ntwo simulated datasets (MARS-Attribute and DukeMTMC-VID-Attribute), achieving\nstate-of-the-art results. The source code and dataset will be released on\nhttps://github.com/Event-AHU/OpenPAR"}
{"id": "2504.09271", "pdf": "https://arxiv.org/pdf/2504.09271", "abs": "https://arxiv.org/abs/2504.09271", "authors": ["Koustuv Saha", "Yoshee Jain", "Munmun De Choudhury"], "title": "Linguistic Comparison of AI- and Human-Written Responses to Online Mental Health Queries", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.SI"], "comment": null, "summary": "The ubiquity and widespread use of digital and online technologies have\ntransformed mental health support, with online mental health communities\n(OMHCs) providing safe spaces for peer support. More recently, generative AI\nand large language models (LLMs) have introduced new possibilities for\nscalable, around-the-clock mental health assistance that could potentially\naugment and supplement the capabilities of OMHCs. Although genAI shows promise\nin delivering immediate and personalized responses, their effectiveness in\nreplicating the nuanced, experience-based support of human peers remains an\nopen question. In this study, we harnessed 24,114 posts and 138,758 online\ncommunity (OC) responses from 55 OMHCs on Reddit. We prompted several\nstate-of-the-art LLMs (GPT-4-Turbo, Llama-3, and Mistral-7B) with these posts,\nand compared their (AI) responses to human-written (OC) responses based on a\nvariety of linguistic measures across psycholinguistics and lexico-semantics.\nOur findings revealed that AI responses are more verbose, readable, and\nanalytically structured, but lack linguistic diversity and personal narratives\ninherent in human-human interactions. Through a qualitative examination, we\nfound validation as well as complementary insights into the nature of AI\nresponses, such as its neutrality of stance and the absence of seeking\nback-and-forth clarifications. We discuss the ethical and practical\nimplications of integrating generative AI into OMHCs, advocating for frameworks\nthat balance AI's scalability and timeliness with the irreplaceable\nauthenticity, social interactiveness, and expertise of human connections that\nform the ethos of online support communities."}
{"id": "2504.10021", "pdf": "https://arxiv.org/pdf/2504.10021", "abs": "https://arxiv.org/abs/2504.10021", "authors": ["Nikolai Röhrich", "Alwin Hoffmann", "Richard Nordsieck", "Emilio Zarbali", "Alireza Javanmardi"], "title": "Masked Autoencoder Self Pre-Training for Defect Detection in Microelectronics", "categories": ["cs.CV"], "comment": "16 pages, 5 figures", "summary": "Whereas in general computer vision, transformer-based architectures have\nquickly become the gold standard, microelectronics defect detection still\nheavily relies on convolutional neural networks (CNNs). We hypothesize that\nthis is due to the fact that a) transformers have an increased need for data\nand b) labelled image generation procedures for microelectronics are costly,\nand labelled data is therefore sparse. Whereas in other domains, pre-training\non large natural image datasets can mitigate this problem, in microelectronics\ntransfer learning is hindered due to the dissimilarity of domain data and\nnatural images. Therefore, we evaluate self pre-training, where models are\npre-trained on the target dataset, rather than another dataset. We propose a\nvision transformer (ViT) pre-training framework for defect detection in\nmicroelectronics based on masked autoencoders (MAE). In MAE, a large share of\nimage patches is masked and reconstructed by the model during pre-training. We\nperform pre-training and defect detection using a dataset of less than 10.000\nscanning acoustic microscopy (SAM) images labelled using transient thermal\nanalysis (TTA). Our experimental results show that our approach leads to\nsubstantial performance gains compared to a) supervised ViT, b) ViT pre-trained\non natural image datasets, and c) state-of-the-art CNN-based defect detection\nmodels used in the literature. Additionally, interpretability analysis reveals\nthat our self pre-trained models, in comparison to ViT baselines, correctly\nfocus on defect-relevant features such as cracks in the solder material. This\ndemonstrates that our approach yields fault-specific feature representations,\nmaking our self pre-trained models viable for real-world defect detection in\nmicroelectronics."}
{"id": "2504.09354", "pdf": "https://arxiv.org/pdf/2504.09354", "abs": "https://arxiv.org/abs/2504.09354", "authors": ["Duy-Cat Can", "Quang-Huy Tang", "Huong Ha", "Binh T. Nguyen", "Oliver Y. Chén"], "title": "REMEMBER: Retrieval-based Explainable Multimodal Evidence-guided Modeling for Brain Evaluation and Reasoning in Zero- and Few-shot Neurodegenerative Diagnosis", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "q-bio.QM"], "comment": null, "summary": "Timely and accurate diagnosis of neurodegenerative disorders, such as\nAlzheimer's disease, is central to disease management. Existing deep learning\nmodels require large-scale annotated datasets and often function as \"black\nboxes\". Additionally, datasets in clinical practice are frequently small or\nunlabeled, restricting the full potential of deep learning methods. Here, we\nintroduce REMEMBER -- Retrieval-based Explainable Multimodal Evidence-guided\nModeling for Brain Evaluation and Reasoning -- a new machine learning framework\nthat facilitates zero- and few-shot Alzheimer's diagnosis using brain MRI scans\nthrough a reference-based reasoning process. Specifically, REMEMBER first\ntrains a contrastively aligned vision-text model using expert-annotated\nreference data and extends pseudo-text modalities that encode abnormality\ntypes, diagnosis labels, and composite clinical descriptions. Then, at\ninference time, REMEMBER retrieves similar, human-validated cases from a\ncurated dataset and integrates their contextual information through a dedicated\nevidence encoding module and attention-based inference head. Such an\nevidence-guided design enables REMEMBER to imitate real-world clinical\ndecision-making process by grounding predictions in retrieved imaging and\ntextual context. Specifically, REMEMBER outputs diagnostic predictions\nalongside an interpretable report, including reference images and explanations\naligned with clinical workflows. Experimental results demonstrate that REMEMBER\nachieves robust zero- and few-shot performance and offers a powerful and\nexplainable framework to neuroimaging-based diagnosis in the real world,\nespecially under limited data."}
{"id": "2504.10024", "pdf": "https://arxiv.org/pdf/2504.10024", "abs": "https://arxiv.org/abs/2504.10024", "authors": ["Mengkun She", "Felix Seegräber", "David Nakath", "Patricia Schöntag", "Kevin Köser"], "title": "Relative Illumination Fields: Learning Medium and Light Independent Underwater Scenes", "categories": ["cs.CV"], "comment": "10 pages, 6 figures. First two authors contributed equally to this\n  work", "summary": "We address the challenge of constructing a consistent and photorealistic\nNeural Radiance Field in inhomogeneously illuminated, scattering environments\nwith unknown, co-moving light sources. While most existing works on underwater\nscene representation focus on a static homogeneous illumination, limited\nattention has been paid to scenarios such as when a robot explores water deeper\nthan a few tens of meters, where sunlight becomes insufficient. To address\nthis, we propose a novel illumination field locally attached to the camera,\nenabling the capture of uneven lighting effects within the viewing frustum. We\ncombine this with a volumetric medium representation to an overall method that\neffectively handles interaction between dynamic illumination field and static\nscattering medium. Evaluation results demonstrate the effectiveness and\nflexibility of our approach."}
{"id": "2504.09426", "pdf": "https://arxiv.org/pdf/2504.09426", "abs": "https://arxiv.org/abs/2504.09426", "authors": ["Shengao Wang", "Arjun Chandra", "Aoming Liu", "Venkatesh Saligrama", "Boqing Gong"], "title": "BabyVLM: Data-Efficient Pretraining of VLMs Inspired by Infant Learning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Human infants rapidly develop visual reasoning skills from minimal input,\nsuggesting that developmentally inspired pretraining could significantly\nenhance the efficiency of vision-language models (VLMs). Although recent\nefforts have leveraged infant-inspired datasets like SAYCam, existing\nevaluation benchmarks remain misaligned--they are either too simplistic,\nnarrowly scoped, or tailored for large-scale pretrained models. Additionally,\ntraining exclusively on infant data overlooks the broader, diverse input from\nwhich infants naturally learn. To address these limitations, we propose\nBabyVLM, a novel framework comprising comprehensive in-domain evaluation\nbenchmarks and a synthetic training dataset created via child-directed\ntransformations of existing datasets. We demonstrate that VLMs trained with our\nsynthetic dataset achieve superior performance on BabyVLM tasks compared to\nmodels trained solely on SAYCam or general-purpose data of the SAYCam size.\nBabyVLM thus provides a robust, developmentally aligned evaluation tool and\nillustrates how compact models trained on carefully curated data can generalize\neffectively, opening pathways toward data-efficient vision-language learning\nparadigms."}
{"id": "2504.10035", "pdf": "https://arxiv.org/pdf/2504.10035", "abs": "https://arxiv.org/abs/2504.10035", "authors": ["Thomas Gossard", "Andreas Ziegler", "Andreas Zell"], "title": "TT3D: Table Tennis 3D Reconstruction", "categories": ["cs.CV"], "comment": "Accepted to CVSport 2025", "summary": "Sports analysis requires processing large amounts of data, which is\ntime-consuming and costly. Advancements in neural networks have significantly\nalleviated this burden, enabling highly accurate ball tracking in sports\nbroadcasts. However, relying solely on 2D ball tracking is limiting, as it\ndepends on the camera's viewpoint and falls short of supporting comprehensive\ngame analysis. To address this limitation, we propose a novel approach for\nreconstructing precise 3D ball trajectories from online table tennis match\nrecordings. Our method leverages the underlying physics of the ball's motion to\nidentify the bounce state that minimizes the reprojection error of the ball's\nflying trajectory, hence ensuring an accurate and reliable 3D reconstruction. A\nkey advantage of our approach is its ability to infer ball spin without relying\non human pose estimation or racket tracking, which are often unreliable or\nunavailable in broadcast footage. We developed an automated camera calibration\nmethod capable of reliably tracking camera movements. Additionally, we adapted\nan existing 3D pose estimation model, which lacks depth motion capture, to\naccurately track player movements. Together, these contributions enable the\nfull 3D reconstruction of a table tennis rally."}
{"id": "2504.09466", "pdf": "https://arxiv.org/pdf/2504.09466", "abs": "https://arxiv.org/abs/2504.09466", "authors": ["Weixiang Zhao", "Jiahe Guo", "Yulin Hu", "Yang Deng", "An Zhang", "Xingyu Sui", "Xinyang Han", "Yanyan Zhao", "Bing Qin", "Tat-Seng Chua", "Ting Liu"], "title": "AdaSteer: Your Aligned LLM is Inherently an Adaptive Jailbreak Defender", "categories": ["cs.CR", "cs.CL"], "comment": "17 pages, 6 figures, 9 tables", "summary": "Despite extensive efforts in safety alignment, large language models (LLMs)\nremain vulnerable to jailbreak attacks. Activation steering offers a\ntraining-free defense method but relies on fixed steering coefficients,\nresulting in suboptimal protection and increased false rejections of benign\ninputs. To address this, we propose AdaSteer, an adaptive activation steering\nmethod that dynamically adjusts model behavior based on input characteristics.\nWe identify two key properties: Rejection Law (R-Law), which shows that\nstronger steering is needed for jailbreak inputs opposing the rejection\ndirection, and Harmfulness Law (H-Law), which differentiates adversarial and\nbenign inputs. AdaSteer steers input representations along both the Rejection\nDirection (RD) and Harmfulness Direction (HD), with adaptive coefficients\nlearned via logistic regression, ensuring robust jailbreak defense while\npreserving benign input handling. Experiments on LLaMA-3.1, Gemma-2, and\nQwen2.5 show that AdaSteer outperforms baseline methods across multiple\njailbreak attacks with minimal impact on utility. Our results highlight the\npotential of interpretable model internals for real-time, flexible safety\nenforcement in LLMs."}
{"id": "2504.10039", "pdf": "https://arxiv.org/pdf/2504.10039", "abs": "https://arxiv.org/abs/2504.10039", "authors": ["Sergey Kuznetsov", "Sanduni Pinnawala", "Peter A. Wijeratne", "Ivor J. A. Simpson"], "title": "Investigating the Role of Bilateral Symmetry for Inpainting Brain MRI", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Inpainting has recently emerged as a valuable and interesting technology to\nemploy in the analysis of medical imaging data, in particular brain MRI. A wide\nvariety of methodologies for inpainting MRI have been proposed and demonstrated\non tasks including anomaly detection. In this work we investigate the\nstatistical relationship between inpainted brain structures and the amount of\nsubject-specific conditioning information, i.e. the other areas of the image\nthat are masked. In particular, we analyse the distribution of inpainting\nresults when masking additional regions of the image, specifically the\ncontra-lateral structure. This allows us to elucidate where in the brain the\nmodel is drawing information from, and in particular, what is the importance of\nhemispherical symmetry? Our experiments interrogate a diffusion inpainting\nmodel through analysing the inpainting of subcortical brain structures based on\nintensity and estimated area change. We demonstrate that some structures show a\nstrong influence of symmetry in the conditioning of the inpainting process."}
{"id": "2504.09479", "pdf": "https://arxiv.org/pdf/2504.09479", "abs": "https://arxiv.org/abs/2504.09479", "authors": ["Zhiqing Cui", "Jiahao Yuan", "Hanqing Wang", "Yanshu Li", "Chenxu Du", "Zhenglong Ding"], "title": "Draw with Thought: Unleashing Multimodal Reasoning for Scientific Diagram Generation", "categories": ["cs.AI", "cs.CL"], "comment": "26 pages, 14 figures", "summary": "Scientific diagrams are vital tools for communicating structured knowledge\nacross disciplines. However, they are often published as static raster images,\nlosing symbolic semantics and limiting reuse. While Multimodal Large Language\nModels (MLLMs) offer a pathway to bridging vision and structure, existing\nmethods lack semantic control and structural interpretability, especially on\ncomplex diagrams. We propose Draw with Thought (DwT), a training-free framework\nthat guides MLLMs to reconstruct diagrams into editable mxGraph XML code\nthrough cognitively-grounded Chain-of-Thought reasoning. DwT enables\ninterpretable and controllable outputs without model fine-tuning by dividing\nthe task into two stages: Coarse-to-Fine Planning, which handles perceptual\nstructuring and semantic specification, and Structure-Aware Code Generation,\nenhanced by format-guided refinement. To support evaluation, we release\nPlot2XML, a benchmark of 247 real-world scientific diagrams with gold-standard\nXML annotations. Extensive experiments across eight MLLMs show that our\napproach yields high-fidelity, semantically aligned, and structurally valid\nreconstructions, with human evaluations confirming strong alignment in both\naccuracy and visual aesthetics, offering a scalable solution for converting\nstatic visuals into executable representations and advancing machine\nunderstanding of scientific graphics."}
{"id": "2504.10044", "pdf": "https://arxiv.org/pdf/2504.10044", "abs": "https://arxiv.org/abs/2504.10044", "authors": ["Bingwen Zhu", "Yudong Jiang", "Baohan Xu", "Siqian Yang", "Mingyu Yin", "Yidi Wu", "Huyang Sun", "Zuxuan Wu"], "title": "Aligning Anime Video Generation with Human Feedback", "categories": ["cs.CV"], "comment": "10 pages, 5 figures, 7 tables", "summary": "Anime video generation faces significant challenges due to the scarcity of\nanime data and unusual motion patterns, leading to issues such as motion\ndistortion and flickering artifacts, which result in misalignment with human\npreferences. Existing reward models, designed primarily for real-world videos,\nfail to capture the unique appearance and consistency requirements of anime. In\nthis work, we propose a pipeline to enhance anime video generation by\nleveraging human feedback for better alignment. Specifically, we construct the\nfirst multi-dimensional reward dataset for anime videos, comprising 30k\nhuman-annotated samples that incorporating human preferences for both visual\nappearance and visual consistency. Based on this, we develop AnimeReward, a\npowerful reward model that employs specialized vision-language models for\ndifferent evaluation dimensions to guide preference alignment. Furthermore, we\nintroduce Gap-Aware Preference Optimization (GAPO), a novel training method\nthat explicitly incorporates preference gaps into the optimization process,\nenhancing alignment performance and efficiency. Extensive experiment results\nshow that AnimeReward outperforms existing reward models, and the inclusion of\nGAPO leads to superior alignment in both quantitative benchmarks and human\nevaluations, demonstrating the effectiveness of our pipeline in enhancing anime\nvideo quality. Our dataset and code will be publicly available."}
{"id": "2504.09582", "pdf": "https://arxiv.org/pdf/2504.09582", "abs": "https://arxiv.org/abs/2504.09582", "authors": ["Christos Theodoropoulos", "Andrei Catalin Coman", "James Henderson", "Marie-Francine Moens"], "title": "Reduction of Supervision for Biomedical Knowledge Discovery", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Published as part of the PhD dissertation: Theodoropoulos, Christos,\n  Marie-Francine Moens, and Matthew Blaschko. \"Deep Learning Models for the\n  Extraction of Knowledge from Text.\" (2025)", "summary": "Knowledge discovery is hindered by the increasing volume of publications and\nthe scarcity of extensive annotated data. To tackle the challenge of\ninformation overload, it is essential to employ automated methods for knowledge\nextraction and processing. Finding the right balance between the level of\nsupervision and the effectiveness of models poses a significant challenge.\nWhile supervised techniques generally result in better performance, they have\nthe major drawback of demanding labeled data. This requirement is\nlabor-intensive and time-consuming and hinders scalability when exploring new\ndomains. In this context, our study addresses the challenge of identifying\nsemantic relationships between biomedical entities (e.g., diseases, proteins)\nin unstructured text while minimizing dependency on supervision. We introduce a\nsuite of unsupervised algorithms based on dependency trees and attention\nmechanisms and employ a range of pointwise binary classification methods.\nTransitioning from weakly supervised to fully unsupervised settings, we assess\nthe methods' ability to learn from data with noisy labels. The evaluation on\nbiomedical benchmark datasets explores the effectiveness of the methods. Our\napproach tackles a central issue in knowledge discovery: balancing performance\nwith minimal supervision. By gradually decreasing supervision, we assess the\nrobustness of pointwise binary classification techniques in handling noisy\nlabels, revealing their capability to shift from weakly supervised to entirely\nunsupervised scenarios. Comprehensive benchmarking offers insights into the\neffectiveness of these techniques, suggesting an encouraging direction toward\nadaptable knowledge discovery systems, representing progress in creating\ndata-efficient methodologies for extracting useful insights when annotated data\nis limited."}
{"id": "2504.10048", "pdf": "https://arxiv.org/pdf/2504.10048", "abs": "https://arxiv.org/abs/2504.10048", "authors": ["Chengyi Du", "Keyan Jin"], "title": "Multi-Object Grounding via Hierarchical Contrastive Siamese Transformers", "categories": ["cs.CV"], "comment": null, "summary": "Multi-object grounding in 3D scenes involves localizing multiple objects\nbased on natural language input. While previous work has primarily focused on\nsingle-object grounding, real-world scenarios often demand the localization of\nseveral objects. To tackle this challenge, we propose Hierarchical Contrastive\nSiamese Transformers (H-COST), which employs a Hierarchical Processing strategy\nto progressively refine object localization, enhancing the understanding of\ncomplex language instructions. Additionally, we introduce a Contrastive Siamese\nTransformer framework, where two networks with the identical structure are\nused: one auxiliary network processes robust object relations from ground-truth\nlabels to guide and enhance the second network, the reference network, which\noperates on segmented point-cloud data. This contrastive mechanism strengthens\nthe model' s semantic understanding and significantly enhances its ability to\nprocess complex point-cloud data. Our approach outperforms previous\nstate-of-the-art methods by 9.5% on challenging multi-object grounding\nbenchmarks."}
{"id": "2504.09602", "pdf": "https://arxiv.org/pdf/2504.09602", "abs": "https://arxiv.org/abs/2504.09602", "authors": ["Zhehao Dong", "Zhen Lu", "Yue Yang"], "title": "Fine-tuning an Large Language Model for Automating Computational Fluid Dynamics Simulations", "categories": ["physics.flu-dyn", "cs.AI", "cs.CL"], "comment": null, "summary": "Configuring computational fluid dynamics (CFD) simulations typically demands\nextensive domain expertise, limiting broader access. Although large language\nmodels (LLMs) have advanced scientific computing, their use in automating CFD\nworkflows is underdeveloped. We introduce a novel approach centered on\ndomain-specific LLM adaptation. By fine-tuning Qwen2.5-7B-Instruct on NL2FOAM,\nour custom dataset of 28716 natural language-to-OpenFOAM configuration pairs\nwith chain-of-thought (CoT) annotations, we enable direct translation from\nnatural language descriptions to executable CFD setups. A multi-agent framework\norchestrates the process, autonomously verifying inputs, generating\nconfigurations, running simulations, and correcting errors. Evaluation on a\nbenchmark of 21 diverse flow cases demonstrates state-of-the-art performance,\nachieving 88.7% solution accuracy and 82.6% first-attempt success rate. This\nsignificantly outperforms larger general-purpose models like\nQwen2.5-72B-Instruct, DeepSeek-R1, and Llama3.3-70B-Instruct, while also\nrequiring fewer correction iterations and maintaining high computational\nefficiency. The results highlight the critical role of domain-specific\nadaptation in deploying LLM assistants for complex engineering workflows."}
{"id": "2504.10049", "pdf": "https://arxiv.org/pdf/2504.10049", "abs": "https://arxiv.org/abs/2504.10049", "authors": ["Théo Gigant", "Camille Guinaudeau", "Frédéric Dufaux"], "title": "Summarization of Multimodal Presentations with Vision-Language Models: Study of the Effect of Modalities and Structure", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Vision-Language Models (VLMs) can process visual and textual information in\nmultiple formats: texts, images, interleaved texts and images, or even\nhour-long videos. In this work, we conduct fine-grained quantitative and\nqualitative analyses of automatic summarization of multimodal presentations\nusing VLMs with various representations as input. From these experiments, we\nsuggest cost-effective strategies for generating summaries from text-heavy\nmultimodal documents under different input-length budgets using VLMs. We show\nthat slides extracted from the video stream can be beneficially used as input\nagainst the raw video, and that a structured representation from interleaved\nslides and transcript provides the best performance. Finally, we reflect and\ncomment on the nature of cross-modal interactions in multimodal presentations\nand share suggestions to improve the capabilities of VLMs to understand\ndocuments of this nature."}
{"id": "2504.09689", "pdf": "https://arxiv.org/pdf/2504.09689", "abs": "https://arxiv.org/abs/2504.09689", "authors": ["Jiahao Qiu", "Yinghui He", "Xinzhe Juan", "Yiming Wang", "Yuhan Liu", "Zixin Yao", "Yue Wu", "Xun Jiang", "Ling Yang", "Mengdi Wang"], "title": "EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "cs.LG"], "comment": "18 pages, 8 figures", "summary": "The rise of LLM-driven AI characters raises safety concerns, particularly for\nvulnerable human users with psychological disorders. To address these risks, we\npropose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate\nmental health hazards in human-AI interactions. EmoAgent comprises two\ncomponents: EmoEval simulates virtual users, including those portraying\nmentally vulnerable individuals, to assess mental health changes before and\nafter interactions with AI characters. It uses clinically proven psychological\nand psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks\ninduced by LLM. EmoGuard serves as an intermediary, monitoring users' mental\nstatus, predicting potential harm, and providing corrective feedback to\nmitigate risks. Experiments conducted in popular character-based chatbots show\nthat emotionally engaging dialogues can lead to psychological deterioration in\nvulnerable users, with mental state deterioration in more than 34.4% of the\nsimulations. EmoGuard significantly reduces these deterioration rates,\nunderscoring its role in ensuring safer AI-human interactions. Our code is\navailable at: https://github.com/1akaman/EmoAgent"}
{"id": "2504.10068", "pdf": "https://arxiv.org/pdf/2504.10068", "abs": "https://arxiv.org/abs/2504.10068", "authors": ["Yang Shi", "Jiaheng Liu", "Yushuo Guan", "Zhenhua Wu", "Yuanxing Zhang", "Zihao Wang", "Weihong Lin", "Jingyun Hua", "Zekun Wang", "Xinlong Chen", "Bohan Zeng", "Wentao Zhang", "Fuzheng Zhang", "Wenjing Yang", "Di Zhang"], "title": "Mavors: Multi-granularity Video Representation for Multimodal Large Language Model", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "22 pages", "summary": "Long-context video understanding in multimodal large language models (MLLMs)\nfaces a critical challenge: balancing computational efficiency with the\nretention of fine-grained spatio-temporal patterns. Existing approaches (e.g.,\nsparse sampling, dense sampling with low resolution, and token compression)\nsuffer from significant information loss in temporal dynamics, spatial details,\nor subtle interactions, particularly in videos with complex motion or varying\nresolutions. To address this, we propose $\\mathbf{Mavors}$, a novel framework\nthat introduces $\\mathbf{M}$ulti-gr$\\mathbf{a}$nularity\n$\\mathbf{v}$ide$\\mathbf{o}$ $\\mathbf{r}$epre$\\mathbf{s}$entation for holistic\nlong-video modeling. Specifically, Mavors directly encodes raw video content\ninto latent representations through two core components: 1) an Intra-chunk\nVision Encoder (IVE) that preserves high-resolution spatial features via 3D\nconvolutions and Vision Transformers, and 2) an Inter-chunk Feature Aggregator\n(IFA) that establishes temporal coherence across chunks using transformer-based\ndependency modeling with chunk-level rotary position encodings. Moreover, the\nframework unifies image and video understanding by treating images as\nsingle-frame videos via sub-image decomposition. Experiments across diverse\nbenchmarks demonstrate Mavors' superiority in maintaining both spatial fidelity\nand temporal continuity, significantly outperforming existing methods in tasks\nrequiring fine-grained spatio-temporal reasoning."}
{"id": "2504.09710", "pdf": "https://arxiv.org/pdf/2504.09710", "abs": "https://arxiv.org/abs/2504.09710", "authors": ["Zhenting Wang", "Guofeng Cui", "Kun Wan", "Wentian Zhao"], "title": "DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM Post-training", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Recent advances in reinforcement learning (RL)-based post-training have led\nto notable improvements in large language models (LLMs), particularly in\nenhancing their reasoning capabilities to handle complex tasks. However, most\nexisting methods treat the training data as a unified whole, overlooking the\nfact that modern LLM training often involves a mixture of data from diverse\ndistributions-varying in both source and difficulty. This heterogeneity\nintroduces a key challenge: how to adaptively schedule training across\ndistributions to optimize learning efficiency. In this paper, we present a\nprincipled curriculum learning framework grounded in the notion of\ndistribution-level learnability. Our core insight is that the magnitude of\npolicy advantages reflects how much a model can still benefit from further\ntraining on a given distribution. Based on this, we propose a\ndistribution-level curriculum learning framework for RL-based LLM\npost-training, which leverages the Upper Confidence Bound (UCB) principle to\ndynamically adjust sampling probabilities for different distrubutions. This\napproach prioritizes distributions with either high average advantage\n(exploitation) or low sample count (exploration), yielding an adaptive and\ntheoretically grounded training schedule. We instantiate our curriculum\nlearning framework with GRPO as the underlying RL algorithm and demonstrate its\neffectiveness on logic reasoning datasets with multiple difficulties and\nsources. Our experiments show that our framework significantly improves\nconvergence speed and final performance, highlighting the value of\ndistribution-aware curriculum strategies in LLM post-training. Code:\nhttps://github.com/ZhentingWang/DUMP."}
{"id": "2504.10070", "pdf": "https://arxiv.org/pdf/2504.10070", "abs": "https://arxiv.org/abs/2504.10070", "authors": ["Kiana Hoshanfar", "Alireza Hosseini", "Ahmad Kalhor", "Babak Nadjar Araabi"], "title": "DTFSal: Audio-Visual Dynamic Token Fusion for Video Saliency Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Audio-visual saliency prediction aims to mimic human visual attention by\nidentifying salient regions in videos through the integration of both visual\nand auditory information. Although visual-only approaches have significantly\nadvanced, effectively incorporating auditory cues remains challenging due to\ncomplex spatio-temporal interactions and high computational demands. To address\nthese challenges, we propose Dynamic Token Fusion Saliency (DFTSal), a novel\naudio-visual saliency prediction framework designed to balance accuracy with\ncomputational efficiency. Our approach features a multi-scale visual encoder\nequipped with two novel modules: the Learnable Token Enhancement Block (LTEB),\nwhich adaptively weights tokens to emphasize crucial saliency cues, and the\nDynamic Learnable Token Fusion Block (DLTFB), which employs a shifting\noperation to reorganize and merge features, effectively capturing long-range\ndependencies and detailed spatial information. In parallel, an audio branch\nprocesses raw audio signals to extract meaningful auditory features. Both\nvisual and audio features are integrated using our Adaptive Multimodal Fusion\nBlock (AMFB), which employs local, global, and adaptive fusion streams for\nprecise cross-modal fusion. The resulting fused features are processed by a\nhierarchical multi-decoder structure, producing accurate saliency maps.\nExtensive evaluations on six audio-visual benchmarks demonstrate that DFTSal\nachieves SOTA performance while maintaining computational efficiency."}
{"id": "2504.09723", "pdf": "https://arxiv.org/pdf/2504.09723", "abs": "https://arxiv.org/abs/2504.09723", "authors": ["Dakuo Wang", "Ting-Yao Hsu", "Yuxuan Lu", "Limeng Cui", "Yaochen Xie", "William Headean", "Bingsheng Yao", "Akash Veeragouni", "Jiapeng Liu", "Sreyashi Nag", "Jessie Wang"], "title": "AgentA/B: Automated and Scalable Web A/BTesting with Interactive LLM Agents", "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "A/B testing experiment is a widely adopted method for evaluating UI/UX design\ndecisions in modern web applications. Yet, traditional A/B testing remains\nconstrained by its dependence on the large-scale and live traffic of human\nparticipants, and the long time of waiting for the testing result. Through\nformative interviews with six experienced industry practitioners, we identified\ncritical bottlenecks in current A/B testing workflows. In response, we present\nAgentA/B, a novel system that leverages Large Language Model-based autonomous\nagents (LLM Agents) to automatically simulate user interaction behaviors with\nreal webpages. AgentA/B enables scalable deployment of LLM agents with diverse\npersonas, each capable of navigating the dynamic webpage and interactively\nexecuting multi-step interactions like search, clicking, filtering, and\npurchasing. In a demonstrative controlled experiment, we employ AgentA/B to\nsimulate a between-subject A/B testing with 1,000 LLM agents Amazon.com, and\ncompare agent behaviors with real human shopping behaviors at a scale. Our\nfindings suggest AgentA/B can emulate human-like behavior patterns."}
{"id": "2504.10079", "pdf": "https://arxiv.org/pdf/2504.10079", "abs": "https://arxiv.org/abs/2504.10079", "authors": ["Hongyu Qu", "Ling Xing", "Rui Yan", "Yazhou Yao", "Guo-Sen Xie", "Xiangbo Shu"], "title": "Hierarchical Relation-augmented Representation Generalization for Few-shot Action Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Few-shot action recognition (FSAR) aims to recognize novel action categories\nwith few exemplars. Existing methods typically learn frame-level\nrepresentations independently for each video by designing various inter-frame\ntemporal modeling strategies. However, they neglect explicit relation modeling\nbetween videos and tasks, thus failing to capture shared temporal patterns\nacross videos and reuse temporal knowledge from historical tasks. In light of\nthis, we propose HR2G-shot, a Hierarchical Relation-augmented Representation\nGeneralization framework for FSAR, which unifies three types of relation\nmodeling (inter-frame, inter-video, and inter-task) to learn task-specific\ntemporal patterns from a holistic view. In addition to conducting inter-frame\ntemporal interactions, we further devise two components to respectively explore\ninter-video and inter-task relationships: i) Inter-video Semantic Correlation\n(ISC) performs cross-video frame-level interactions in a fine-grained manner,\nthereby capturing task-specific query features and learning intra- and\ninter-class temporal correlations among support features; ii) Inter-task\nKnowledge Transfer (IKT) retrieves and aggregates relevant temporal knowledge\nfrom the bank, which stores diverse temporal patterns from historical tasks.\nExtensive experiments on five benchmarks show that HR2G-shot outperforms\ncurrent top-leading FSAR methods."}
{"id": "2504.09737", "pdf": "https://arxiv.org/pdf/2504.09737", "abs": "https://arxiv.org/abs/2504.09737", "authors": ["Nitya Thakkar", "Mert Yuksekgonul", "Jake Silberg", "Animesh Garg", "Nanyun Peng", "Fei Sha", "Rose Yu", "Carl Vondrick", "James Zou"], "title": "Can LLM feedback enhance review quality? A randomized study of 20K reviews at ICLR 2025", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "comment": "30 pages, 7 figures", "summary": "Peer review at AI conferences is stressed by rapidly rising submission\nvolumes, leading to deteriorating review quality and increased author\ndissatisfaction. To address these issues, we developed Review Feedback Agent, a\nsystem leveraging multiple large language models (LLMs) to improve review\nclarity and actionability by providing automated feedback on vague comments,\ncontent misunderstandings, and unprofessional remarks to reviewers. Implemented\nat ICLR 2025 as a large randomized control study, our system provided optional\nfeedback to more than 20,000 randomly selected reviews. To ensure high-quality\nfeedback for reviewers at this scale, we also developed a suite of automated\nreliability tests powered by LLMs that acted as guardrails to ensure feedback\nquality, with feedback only being sent to reviewers if it passed all the tests.\nThe results show that 27% of reviewers who received feedback updated their\nreviews, and over 12,000 feedback suggestions from the agent were incorporated\nby those reviewers. This suggests that many reviewers found the AI-generated\nfeedback sufficiently helpful to merit updating their reviews. Incorporating AI\nfeedback led to significantly longer reviews (an average increase of 80 words\namong those who updated after receiving feedback) and more informative reviews,\nas evaluated by blinded researchers. Moreover, reviewers who were selected to\nreceive AI feedback were also more engaged during paper rebuttals, as seen in\nlonger author-reviewer discussions. This work demonstrates that carefully\ndesigned LLM-generated review feedback can enhance peer review quality by\nmaking reviews more specific and actionable while increasing engagement between\nreviewers and authors. The Review Feedback Agent is publicly available at\nhttps://github.com/zou-group/review_feedback_agent."}
{"id": "2504.10080", "pdf": "https://arxiv.org/pdf/2504.10080", "abs": "https://arxiv.org/abs/2504.10080", "authors": ["Yucheng Lu", "Shunxin Wang", "Dovile Juodelyte", "Veronika Cheplygina"], "title": "Learning to Harmonize Cross-vendor X-ray Images by Non-linear Image Dynamics Correction", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "In this paper, we explore how conventional image enhancement can improve\nmodel robustness in medical image analysis. By applying commonly used\nnormalization methods to images from various vendors and studying their\ninfluence on model generalization in transfer learning, we show that the\nnonlinear characteristics of domain-specific image dynamics cannot be addressed\nby simple linear transforms. To tackle this issue, we reformulate the image\nharmonization task as an exposure correction problem and propose a method\ntermed Global Deep Curve Estimation (GDCE) to reduce domain-specific exposure\nmismatch. GDCE performs enhancement via a pre-defined polynomial function and\nis trained with the help of a ``domain discriminator'', aiming to improve model\ntransparency in downstream tasks compared to existing black-box methods."}
{"id": "2504.09816", "pdf": "https://arxiv.org/pdf/2504.09816", "abs": "https://arxiv.org/abs/2504.09816", "authors": ["Quentin Fitte-Rey", "Matyas Amrouche", "Romain Deveaud"], "title": "Augmented Relevance Datasets with Fine-Tuned Small LLMs", "categories": ["cs.IR", "cs.CL", "H.3.3; I.2.7"], "comment": "10 pages, 3 figures, and 6 tables. Accepted and presented to LLM4EVAL\n  at WSDM '25", "summary": "Building high-quality datasets and labeling query-document relevance are\nessential yet resource-intensive tasks, requiring detailed guidelines and\nsubstantial effort from human annotators. This paper explores the use of small,\nfine-tuned large language models (LLMs) to automate relevance assessment, with\na focus on improving ranking models' performance by augmenting their training\ndataset. We fine-tuned small LLMs to enhance relevance assessments, thereby\nimproving dataset creation quality for downstream ranking model training. Our\nexperiments demonstrate that these fine-tuned small LLMs not only outperform\ncertain closed source models on our dataset but also lead to substantial\nimprovements in ranking model performance. These results highlight the\npotential of leveraging small LLMs for efficient and scalable dataset\naugmentation, providing a practical solution for search engine optimization."}
{"id": "2504.10084", "pdf": "https://arxiv.org/pdf/2504.10084", "abs": "https://arxiv.org/abs/2504.10084", "authors": ["Yating Liu", "Yaowei Li", "Xiangyuan Lan", "Wenming Yang", "Zimo Liu", "Qingmin Liao"], "title": "UP-Person: Unified Parameter-Efficient Transfer Learning for Text-based Person Retrieval", "categories": ["cs.CV"], "comment": "16 pages, 7 figures, first submited to IEEE TCSVT on 2024 May. Under\n  review", "summary": "Text-based Person Retrieval (TPR) as a multi-modal task, which aims to\nretrieve the target person from a pool of candidate images given a text\ndescription, has recently garnered considerable attention due to the progress\nof contrastive visual-language pre-trained model. Prior works leverage\npre-trained CLIP to extract person visual and textual features and fully\nfine-tune the entire network, which have shown notable performance improvements\ncompared to uni-modal pre-training models. However, full-tuning a large model\nis prone to overfitting and hinders the generalization ability. In this paper,\nwe propose a novel Unified Parameter-Efficient Transfer Learning (PETL) method\nfor Text-based Person Retrieval (UP-Person) to thoroughly transfer the\nmulti-modal knowledge from CLIP. Specifically, UP-Person simultaneously\nintegrates three lightweight PETL components including Prefix, LoRA and\nAdapter, where Prefix and LoRA are devised together to mine local information\nwith task-specific information prompts, and Adapter is designed to adjust\nglobal feature representations. Additionally, two vanilla submodules are\noptimized to adapt to the unified architecture of TPR. For one thing, S-Prefix\nis proposed to boost attention of prefix and enhance the gradient propagation\nof prefix tokens, which improves the flexibility and performance of the vanilla\nprefix. For another thing, L-Adapter is designed in parallel with layer\nnormalization to adjust the overall distribution, which can resolve conflicts\ncaused by overlap and interaction among multiple submodules. Extensive\nexperimental results demonstrate that our UP-Person achieves state-of-the-art\nresults across various person retrieval datasets, including CUHK-PEDES,\nICFG-PEDES and RSTPReid while merely fine-tuning 4.7\\% parameters. Code is\navailable at https://github.com/Liu-Yating/UP-Person."}
{"id": "2504.09848", "pdf": "https://arxiv.org/pdf/2504.09848", "abs": "https://arxiv.org/abs/2504.09848", "authors": ["Jie Feng", "Jinwei Zeng", "Qingyue Long", "Hongyi Chen", "Jie Zhao", "Yanxin Xi", "Zhilun Zhou", "Yuan Yuan", "Shengyuan Wang", "Qingbin Zeng", "Songwei Li", "Yunke Zhang", "Yuming Lin", "Tong Li", "Jingtao Ding", "Chen Gao", "Fengli Xu", "Yong Li"], "title": "A Survey of Large Language Model-Powered Spatial Intelligence Across Scales: Advances in Embodied Agents, Smart Cities, and Earth Science", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Over the past year, the development of large language models (LLMs) has\nbrought spatial intelligence into focus, with much attention on vision-based\nembodied intelligence. However, spatial intelligence spans a broader range of\ndisciplines and scales, from navigation and urban planning to remote sensing\nand earth science. What are the differences and connections between spatial\nintelligence across these fields? In this paper, we first review human spatial\ncognition and its implications for spatial intelligence in LLMs. We then\nexamine spatial memory, knowledge representations, and abstract reasoning in\nLLMs, highlighting their roles and connections. Finally, we analyze spatial\nintelligence across scales -- from embodied to urban and global levels --\nfollowing a framework that progresses from spatial memory and understanding to\nspatial reasoning and intelligence. Through this survey, we aim to provide\ninsights into interdisciplinary spatial intelligence research and inspire\nfuture studies."}
{"id": "2504.10090", "pdf": "https://arxiv.org/pdf/2504.10090", "abs": "https://arxiv.org/abs/2504.10090", "authors": ["I-Sheng Fang", "Jun-Cheng Chen"], "title": "CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) and multimodal large language models (MLLMs)\nhave significantly advanced artificial intelligence. However, visual reasoning,\nreasoning involving both visual and textual inputs, remains underexplored.\nRecent advancements, including the reasoning models like OpenAI o1 and Gemini\n2.0 Flash Thinking, which incorporate image inputs, have opened this\ncapability. In this ongoing work, we focus specifically on photography-related\ntasks because a photo is a visual snapshot of the physical world where the\nunderlying physics (i.e., illumination, blur extent, etc.) interplay with the\ncamera parameters. Successfully reasoning from the visual information of a\nphoto to identify these numerical camera settings requires the MLLMs to have a\ndeeper understanding of the underlying physics for precise visual\ncomprehension, representing a challenging and intelligent capability essential\nfor practical applications like photography assistant agents. We aim to\nevaluate MLLMs on their ability to distinguish visual differences related to\nnumerical camera settings, extending a methodology previously proposed for\nvision-language models (VLMs). Our preliminary results demonstrate the\nimportance of visual reasoning in photography-related tasks. Moreover, these\nresults show that no single MLLM consistently dominates across all evaluation\ntasks, demonstrating ongoing challenges and opportunities in developing MLLMs\nwith better visual reasoning."}
{"id": "2504.09858", "pdf": "https://arxiv.org/pdf/2504.09858", "abs": "https://arxiv.org/abs/2504.09858", "authors": ["Wenjie Ma", "Jingxuan He", "Charlie Snell", "Tyler Griggs", "Sewon Min", "Matei Zaharia"], "title": "Reasoning Models Can Be Effective Without Thinking", "categories": ["cs.AI", "cs.CL"], "comment": "33 pages, 7 main figures, 2 tables", "summary": "Recent LLMs have significantly improved reasoning capabilities, primarily by\nincluding an explicit, lengthy Thinking process as part of generation. In this\npaper, we question whether this explicit thinking is necessary. Using the\nstate-of-the-art DeepSeek-R1-Distill-Qwen, we find that bypassing the thinking\nprocess via simple prompting, denoted as NoThinking, can be surprisingly\neffective. When controlling for the number of tokens, NoThinking outperforms\nThinking across a diverse set of seven challenging reasoning\ndatasets--including mathematical problem solving, formal theorem proving, and\ncoding--especially in low-budget settings, e.g., 51.3 vs. 28.9 on ACM 23 with\n700 tokens. Notably, the performance of NoThinking becomes more competitive\nwith pass@k as k increases. Building on this observation, we demonstrate that a\nparallel scaling approach that uses NoThinking to generate N outputs\nindependently and aggregates them is highly effective. For aggregation, we use\ntask-specific verifiers when available, or we apply simple best-of-N strategies\nsuch as confidence-based selection. Our method outperforms a range of baselines\nwith similar latency using Thinking, and is comparable to Thinking with\nsignificantly longer latency (up to 9x). Together, our research encourages a\nreconsideration of the necessity of lengthy thinking processes, while also\nestablishing a competitive reference for achieving strong reasoning performance\nin low-budget settings or at low latency using parallel scaling."}
{"id": "2504.10105", "pdf": "https://arxiv.org/pdf/2504.10105", "abs": "https://arxiv.org/abs/2504.10105", "authors": ["Zexin Ji", "Beiji Zou", "Xiaoyan Kui", "Sebastien Thureau", "Su Ruan"], "title": "Global and Local Mamba Network for Multi-Modality Medical Image Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Convolutional neural networks and Transformer have made significant\nprogresses in multi-modality medical image super-resolution. However, these\nmethods either have a fixed receptive field for local learning or significant\ncomputational burdens for global learning, limiting the super-resolution\nperformance. To solve this problem, State Space Models, notably Mamba, is\nintroduced to efficiently model long-range dependencies in images with linear\ncomputational complexity. Relying on the Mamba and the fact that low-resolution\nimages rely on global information to compensate for missing details, while\nhigh-resolution reference images need to provide more local details for\naccurate super-resolution, we propose a global and local Mamba network\n(GLMamba) for multi-modality medical image super-resolution. To be specific,\nour GLMamba is a two-branch network equipped with a global Mamba branch and a\nlocal Mamba branch. The global Mamba branch captures long-range relationships\nin low-resolution inputs, and the local Mamba branch focuses more on\nshort-range details in high-resolution reference images. We also use the deform\nblock to adaptively extract features of both branches to enhance the\nrepresentation ability. A modulator is designed to further enhance deformable\nfeatures in both global and local Mamba blocks. To fully integrate the\nreference image for low-resolution image super-resolution, we further develop a\nmulti-modality feature fusion block to adaptively fuse features by considering\nsimilarities, differences, and complementary aspects between modalities. In\naddition, a contrastive edge loss (CELoss) is developed for sufficient\nenhancement of edge textures and contrast in medical images."}
{"id": "2504.09936", "pdf": "https://arxiv.org/pdf/2504.09936", "abs": "https://arxiv.org/abs/2504.09936", "authors": ["Yuxuan Tian", "Zihan Wang", "Yebo Peng", "Aomufei Yuan", "Zhiming Wang", "Bairen Yi", "Xin Liu", "Yong Cui", "Tong Yang"], "title": "KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "18 pages, 8 figures", "summary": "Efficient inference of large language models (LLMs) is hindered by an\never-growing key-value (KV) cache, making KV cache compression a critical\nresearch direction. Traditional methods selectively evict less important KV\ncache entries based on attention scores or position heuristics, which leads to\ninformation loss and hallucinations. Recently, merging-based strategies have\nbeen explored to retain more information by merging KV pairs that would be\ndiscarded; however, these existing approaches inevitably introduce\ninconsistencies in attention distributions before and after merging, causing\noutput perturbation and degraded generation quality. To overcome this\nchallenge, we propose KeepKV, a novel adaptive KV cache merging method designed\nto eliminate output perturbation while preserving performance under strict\nmemory constraints. KeepKV introduces the Electoral Votes mechanism that\nrecords merging history and adaptively adjusts attention scores. Moreover, it\nfurther leverages a novel Zero Inference-Perturbation Merging methods, keeping\nattention consistency and compensating for attention loss resulting from cache\nmerging. KeepKV successfully retains essential context information within a\nsignificantly compressed cache. Extensive experiments on various benchmarks and\nLLM architectures demonstrate that KeepKV substantially reduces memory usage,\nenhances inference throughput by more than 2x and keeps superior generation\nquality even with 10% KV cache budgets."}
{"id": "2504.10106", "pdf": "https://arxiv.org/pdf/2504.10106", "abs": "https://arxiv.org/abs/2504.10106", "authors": ["Marc Gutiérrez-Pérez", "Antonio Agudo"], "title": "SoccerNet-v3D: Leveraging Sports Broadcast Replays for 3D Scene Understanding", "categories": ["cs.CV", "cs.AI", "I.2; I.4; I.5"], "comment": null, "summary": "Sports video analysis is a key domain in computer vision, enabling detailed\nspatial understanding through multi-view correspondences. In this work, we\nintroduce SoccerNet-v3D and ISSIA-3D, two enhanced and scalable datasets\ndesigned for 3D scene understanding in soccer broadcast analysis. These\ndatasets extend SoccerNet-v3 and ISSIA by incorporating field-line-based camera\ncalibration and multi-view synchronization, enabling 3D object localization\nthrough triangulation. We propose a monocular 3D ball localization task built\nupon the triangulation of ground-truth 2D ball annotations, along with several\ncalibration and reprojection metrics to assess annotation quality on demand.\nAdditionally, we present a single-image 3D ball localization method as a\nbaseline, leveraging camera calibration and ball size priors to estimate the\nball's position from a monocular viewpoint. To further refine 2D annotations,\nwe introduce a bounding box optimization technique that ensures alignment with\nthe 3D scene representation. Our proposed datasets establish new benchmarks for\n3D soccer scene understanding, enhancing both spatial and temporal analysis in\nsports analytics. Finally, we provide code to facilitate access to our\nannotations and the generation pipelines for the datasets."}
{"id": "2504.09946", "pdf": "https://arxiv.org/pdf/2504.09946", "abs": "https://arxiv.org/abs/2504.09946", "authors": ["Qian Wang", "Zhanzhi Lou", "Zhenheng Tang", "Nuo Chen", "Xuandong Zhao", "Wenxuan Zhang", "Dawn Song", "Bingsheng He"], "title": "Assessing Judging Bias in Large Reasoning Models: An Empirical Study", "categories": ["cs.CY", "cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) like DeepSeek-R1 and OpenAI-o1 have\ndemonstrated remarkable reasoning capabilities, raising important questions\nabout their biases in LLM-as-a-judge settings. We present a comprehensive\nbenchmark comparing judging biases between LLMs and LRMs across both subjective\npreference-alignment datasets and objective fact-based datasets. Through\ninvestigation of bandwagon, authority, position, and distraction biases, we\nuncover four key findings: (1) despite their advanced reasoning capabilities,\nLRMs remain susceptible to the above biases; (2) LRMs demonstrate better\nrobustness than LLMs specifically on fact-related datasets; (3) LRMs exhibit\nnotable position bias, preferring options in later positions; and (4) we\nidentify a novel \"superficial reflection bias\" where phrases mimicking\nreasoning (e.g., \"wait, let me think...\") significantly influence model\njudgments. To address these biases, we design and evaluate three mitigation\nstrategies: specialized system prompts that reduce judging biases by up to 19\\%\nin preference alignment datasets and 14\\% in fact-related datasets, in-context\nlearning that provides up to 27\\% improvement on preference tasks but shows\ninconsistent results on factual tasks, and a self-reflection mechanism that\nreduces biases by up to 10\\% in preference datasets and 16\\% in fact-related\ndatasets, with self-reflection proving particularly effective for LRMs. Our\nwork provides crucial insights for developing more reliable LLM-as-a-Judge\nframeworks, especially as LRMs become increasingly deployed as automated\njudges."}
{"id": "2504.10117", "pdf": "https://arxiv.org/pdf/2504.10117", "abs": "https://arxiv.org/abs/2504.10117", "authors": ["Peizheng Li", "Shuxiao Ding", "You Zhou", "Qingwen Zhang", "Onat Inak", "Larissa Triess", "Niklas Hanselmann", "Marius Cordts", "Andreas Zell"], "title": "AGO: Adaptive Grounding for Open World 3D Occupancy Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Open-world 3D semantic occupancy prediction aims to generate a voxelized 3D\nrepresentation from sensor inputs while recognizing both known and unknown\nobjects. Transferring open-vocabulary knowledge from vision-language models\n(VLMs) offers a promising direction but remains challenging. However, methods\nbased on VLM-derived 2D pseudo-labels with traditional supervision are limited\nby a predefined label space and lack general prediction capabilities. Direct\nalignment with pretrained image embeddings, on the other hand, fails to achieve\nreliable performance due to often inconsistent image and text representations\nin VLMs. To address these challenges, we propose AGO, a novel 3D occupancy\nprediction framework with adaptive grounding to handle diverse open-world\nscenarios. AGO first encodes surrounding images and class prompts into 3D and\ntext embeddings, respectively, leveraging similarity-based grounding training\nwith 3D pseudo-labels. Additionally, a modality adapter maps 3D embeddings into\na space aligned with VLM-derived image embeddings, reducing modality gaps.\nExperiments on Occ3D-nuScenes show that AGO improves unknown object prediction\nin zero-shot and few-shot transfer while achieving state-of-the-art\nclosed-world self-supervised performance, surpassing prior methods by 4.09\nmIoU."}
{"id": "2504.10000", "pdf": "https://arxiv.org/pdf/2504.10000", "abs": "https://arxiv.org/abs/2504.10000", "authors": ["Yanbo Wang", "Jiyang Guan", "Jian Liang", "Ran He"], "title": "Do We Really Need Curated Malicious Data for Safety Alignment in Multi-modal Large Language Models?", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "Accepted to CVPR 2025, codes in process", "summary": "Multi-modal large language models (MLLMs) have made significant progress, yet\ntheir safety alignment remains limited. Typically, current open-source MLLMs\nrely on the alignment inherited from their language module to avoid harmful\ngenerations. However, the lack of safety measures specifically designed for\nmulti-modal inputs creates an alignment gap, leaving MLLMs vulnerable to\nvision-domain attacks such as typographic manipulation. Current methods utilize\na carefully designed safety dataset to enhance model defense capability, while\nthe specific knowledge or patterns acquired from the high-quality dataset\nremain unclear. Through comparison experiments, we find that the alignment gap\nprimarily arises from data distribution biases, while image content, response\nquality, or the contrastive behavior of the dataset makes little contribution\nto boosting multi-modal safety. To further investigate this and identify the\nkey factors in improving MLLM safety, we propose finetuning MLLMs on a small\nset of benign instruct-following data with responses replaced by simple, clear\nrejection sentences. Experiments show that, without the need for\nlabor-intensive collection of high-quality malicious data, model safety can\nstill be significantly improved, as long as a specific fraction of rejection\ndata exists in the finetuning set, indicating the security alignment is not\nlost but rather obscured during multi-modal pretraining or instruction\nfinetuning. Simply correcting the underlying data bias could narrow the safety\ngap in the vision domain."}
{"id": "2504.10123", "pdf": "https://arxiv.org/pdf/2504.10123", "abs": "https://arxiv.org/abs/2504.10123", "authors": ["Tzu-Yun Tseng", "Hongyu Lyu", "Josephine Li", "Julie Stephany Berrio", "Mao Shan", "Stewart Worrall"], "title": "M2S-RoAD: Multi-Modal Semantic Segmentation for Road Damage Using Camera and LiDAR Data", "categories": ["cs.CV"], "comment": null, "summary": "Road damage can create safety and comfort challenges for both human drivers\nand autonomous vehicles (AVs). This damage is particularly prevalent in rural\nareas due to less frequent surveying and maintenance of roads. Automated\ndetection of pavement deterioration can be used as an input to AVs and driver\nassistance systems to improve road safety. Current research in this field has\npredominantly focused on urban environments driven largely by public datasets,\nwhile rural areas have received significantly less attention. This paper\nintroduces M2S-RoAD, a dataset for the semantic segmentation of different\nclasses of road damage. M2S-RoAD was collected in various towns across New\nSouth Wales, Australia, and labelled for semantic segmentation to identify nine\ndistinct types of road damage. This dataset will be released upon the\nacceptance of the paper."}
{"id": "2504.10049", "pdf": "https://arxiv.org/pdf/2504.10049", "abs": "https://arxiv.org/abs/2504.10049", "authors": ["Théo Gigant", "Camille Guinaudeau", "Frédéric Dufaux"], "title": "Summarization of Multimodal Presentations with Vision-Language Models: Study of the Effect of Modalities and Structure", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Vision-Language Models (VLMs) can process visual and textual information in\nmultiple formats: texts, images, interleaved texts and images, or even\nhour-long videos. In this work, we conduct fine-grained quantitative and\nqualitative analyses of automatic summarization of multimodal presentations\nusing VLMs with various representations as input. From these experiments, we\nsuggest cost-effective strategies for generating summaries from text-heavy\nmultimodal documents under different input-length budgets using VLMs. We show\nthat slides extracted from the video stream can be beneficially used as input\nagainst the raw video, and that a structured representation from interleaved\nslides and transcript provides the best performance. Finally, we reflect and\ncomment on the nature of cross-modal interactions in multimodal presentations\nand share suggestions to improve the capabilities of VLMs to understand\ndocuments of this nature."}
{"id": "2504.10148", "pdf": "https://arxiv.org/pdf/2504.10148", "abs": "https://arxiv.org/abs/2504.10148", "authors": ["Chunyang Zhang", "Zhenhong Sun", "Zhicheng Zhang", "Junyan Wang", "Yu Zhang", "Dong Gong", "Huadong Mo", "Daoyi Dong"], "title": "Hierarchical and Step-Layer-Wise Tuning of Attention Specialty for Multi-Instance Synthesis in Diffusion Transformers", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image (T2I) generation models often struggle with multi-instance\nsynthesis (MIS), where they must accurately depict multiple distinct instances\nin a single image based on complex prompts detailing individual features.\nTraditional MIS control methods for UNet architectures like SD v1.5/SDXL fail\nto adapt to DiT-based models like FLUX and SD v3.5, which rely on integrated\nattention between image and text tokens rather than text-image cross-attention.\nTo enhance MIS in DiT, we first analyze the mixed attention mechanism in DiT.\nOur token-wise and layer-wise analysis of attention maps reveals a hierarchical\nresponse structure: instance tokens dominate early layers, background tokens in\nmiddle layers, and attribute tokens in later layers. Building on this\nobservation, we propose a training-free approach for enhancing MIS in DiT-based\nmodels with hierarchical and step-layer-wise attention specialty tuning (AST).\nAST amplifies key regions while suppressing irrelevant areas in distinct\nattention maps across layers and steps, guided by the hierarchical structure.\nThis optimizes multimodal interactions by hierarchically decoupling the complex\nprompts with instance-based sketches. We evaluate our approach using upgraded\nsketch-based layouts for the T2I-CompBench and customized complex scenes. Both\nquantitative and qualitative results confirm our method enhances complex layout\ngeneration, ensuring precise instance placement and attribute representation in\nMIS."}
{"id": "2504.10055", "pdf": "https://arxiv.org/pdf/2504.10055", "abs": "https://arxiv.org/abs/2504.10055", "authors": ["Theodor Wulff", "Rahul Singh Maharjan", "Xinyun Chi", "Angelo Cangelosi"], "title": "Joint Action Language Modelling for Transparent Policy Execution", "categories": ["cs.RO", "cs.CL"], "comment": null, "summary": "An agent's intention often remains hidden behind the black-box nature of\nembodied policies. Communication using natural language statements that\ndescribe the next action can provide transparency towards the agent's behavior.\nWe aim to insert transparent behavior directly into the learning process, by\ntransforming the problem of policy learning into a language generation problem\nand combining it with traditional autoregressive modelling. The resulting model\nproduces transparent natural language statements followed by tokens\nrepresenting the specific actions to solve long-horizon tasks in the\nLanguage-Table environment. Following previous work, the model is able to learn\nto produce a policy represented by special discretized tokens in an\nautoregressive manner. We place special emphasis on investigating the\nrelationship between predicting actions and producing high-quality language for\na transparent agent. We find that in many cases both the quality of the action\ntrajectory and the transparent statement increase when they are generated\nsimultaneously."}
{"id": "2504.10158", "pdf": "https://arxiv.org/pdf/2504.10158", "abs": "https://arxiv.org/abs/2504.10158", "authors": ["Jiansheng Li", "Xingxuan Zhang", "Hao Zou", "Yige Guo", "Renzhe Xu", "Yilong Liu", "Chuzhao Zhu", "Yue He", "Peng Cui"], "title": "COUNTS: Benchmarking Object Detectors and Multimodal Large Language Models under Distribution Shifts", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Current object detectors often suffer significant perfor-mance degradation in\nreal-world applications when encountering distributional shifts. Consequently,\nthe out-of-distribution (OOD) generalization capability of object detectors has\ngarnered increasing attention from researchers. Despite this growing interest,\nthere remains a lack of a large-scale, comprehensive dataset and evaluation\nbenchmark with fine-grained annotations tailored to assess the OOD\ngeneralization on more intricate tasks like object detection and grounding. To\naddress this gap, we introduce COUNTS, a large-scale OOD dataset with\nobject-level annotations. COUNTS encompasses 14 natural distributional shifts,\nover 222K samples, and more than 1,196K labeled bounding boxes. Leveraging\nCOUNTS, we introduce two novel benchmarks: O(OD)2 and OODG. O(OD)2 is designed\nto comprehensively evaluate the OOD generalization capabilities of object\ndetectors by utilizing controlled distribution shifts between training and\ntesting data. OODG, on the other hand, aims to assess the OOD generalization of\ngrounding abilities in multimodal large language models (MLLMs). Our findings\nreveal that, while large models and extensive pre-training data substantially\nen hance performance in in-distribution (IID) scenarios, significant\nlimitations and opportunities for improvement persist in OOD contexts for both\nobject detectors and MLLMs. In visual grounding tasks, even the advanced GPT-4o\nand Gemini-1.5 only achieve 56.7% and 28.0% accuracy, respectively. We hope\nCOUNTS facilitates advancements in the development and assessment of robust\nobject detectors and MLLMs capable of maintaining high performance under\ndistributional shifts."}
{"id": "2504.10068", "pdf": "https://arxiv.org/pdf/2504.10068", "abs": "https://arxiv.org/abs/2504.10068", "authors": ["Yang Shi", "Jiaheng Liu", "Yushuo Guan", "Zhenhua Wu", "Yuanxing Zhang", "Zihao Wang", "Weihong Lin", "Jingyun Hua", "Zekun Wang", "Xinlong Chen", "Bohan Zeng", "Wentao Zhang", "Fuzheng Zhang", "Wenjing Yang", "Di Zhang"], "title": "Mavors: Multi-granularity Video Representation for Multimodal Large Language Model", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "22 pages", "summary": "Long-context video understanding in multimodal large language models (MLLMs)\nfaces a critical challenge: balancing computational efficiency with the\nretention of fine-grained spatio-temporal patterns. Existing approaches (e.g.,\nsparse sampling, dense sampling with low resolution, and token compression)\nsuffer from significant information loss in temporal dynamics, spatial details,\nor subtle interactions, particularly in videos with complex motion or varying\nresolutions. To address this, we propose $\\mathbf{Mavors}$, a novel framework\nthat introduces $\\mathbf{M}$ulti-gr$\\mathbf{a}$nularity\n$\\mathbf{v}$ide$\\mathbf{o}$ $\\mathbf{r}$epre$\\mathbf{s}$entation for holistic\nlong-video modeling. Specifically, Mavors directly encodes raw video content\ninto latent representations through two core components: 1) an Intra-chunk\nVision Encoder (IVE) that preserves high-resolution spatial features via 3D\nconvolutions and Vision Transformers, and 2) an Inter-chunk Feature Aggregator\n(IFA) that establishes temporal coherence across chunks using transformer-based\ndependency modeling with chunk-level rotary position encodings. Moreover, the\nframework unifies image and video understanding by treating images as\nsingle-frame videos via sub-image decomposition. Experiments across diverse\nbenchmarks demonstrate Mavors' superiority in maintaining both spatial fidelity\nand temporal continuity, significantly outperforming existing methods in tasks\nrequiring fine-grained spatio-temporal reasoning."}
{"id": "2504.10165", "pdf": "https://arxiv.org/pdf/2504.10165", "abs": "https://arxiv.org/abs/2504.10165", "authors": ["Nguyen Ngoc Dat", "Tom Richardson", "Matthew Watson", "Kilian Meier", "Jenna Kline", "Sid Reid", "Guy Maalouf", "Duncan Hine", "Majid Mirmehdi", "Tilo Burghardt"], "title": "WildLive: Near Real-time Visual Wildlife Tracking onboard UAVs", "categories": ["cs.CV", "cs.AI"], "comment": "Submitted in CV4Animals 2025", "summary": "Live tracking of wildlife via high-resolution video processing directly\nonboard drones is widely unexplored and most existing solutions rely on\nstreaming video to ground stations to support navigation. Yet, both autonomous\nanimal-reactive flight control beyond visual line of sight and/or\nmission-specific individual and behaviour recognition tasks rely to some degree\non this capability. In response, we introduce WildLive -- a near real-time\nanimal detection and tracking framework for high-resolution imagery running\ndirectly onboard uncrewed aerial vehicles (UAVs). The system performs\nmulti-animal detection and tracking at 17fps+ for HD and 7fps+ on 4K video\nstreams suitable for operation during higher altitude flights to minimise\nanimal disturbance. Our system is optimised for Jetson Orin AGX onboard\nhardware. It integrates the efficiency of sparse optical flow tracking and\nmission-specific sampling with device-optimised and proven YOLO-driven object\ndetection and segmentation techniques. Essentially, computational resource is\nfocused onto spatio-temporal regions of high uncertainty to significantly\nimprove UAV processing speeds without domain-specific loss of accuracy.\nAlongside, we introduce our WildLive dataset, which comprises 200k+ annotated\nanimal instances across 19k+ frames from 4K UAV videos collected at the Ol\nPejeta Conservancy in Kenya. All frames contain ground truth bounding boxes,\nsegmentation masks, as well as individual tracklets and tracking point\ntrajectories. We compare our system against current object tracking approaches\nincluding OC-SORT, ByteTrack, and SORT. Our multi-animal tracking experiments\nwith onboard hardware confirm that near real-time high-resolution wildlife\ntracking is possible on UAVs whilst maintaining high accuracy levels as needed\nfor future navigational and mission-specific animal-centric operational\nautonomy."}
{"id": "2504.10081", "pdf": "https://arxiv.org/pdf/2504.10081", "abs": "https://arxiv.org/abs/2504.10081", "authors": ["Yichi Zhang", "Zihao Zeng", "Dongbai Li", "Yao Huang", "Zhijie Deng", "Yinpeng Dong"], "title": "RealSafe-R1: Safety-Aligned DeepSeek-R1 without Compromising Reasoning Capability", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs), such as OpenAI o1 and DeepSeek-R1, have been\nrapidly progressing and achieving breakthrough performance on complex reasoning\ntasks such as mathematics and coding. However, the open-source R1 models have\nraised safety concerns in wide applications, such as the tendency to comply\nwith malicious queries, which greatly impacts the utility of these powerful\nmodels in their applications. In this paper, we introduce RealSafe-R1 as\nsafety-aligned versions of DeepSeek-R1 distilled models. To train these models,\nwe construct a dataset of 15k safety-aware reasoning trajectories generated by\nDeepSeek-R1, under explicit instructions for expected refusal behavior. Both\nquantitative experiments and qualitative case studies demonstrate the models'\nimprovements, which are shown in their safety guardrails against both harmful\nqueries and jailbreak attacks. Importantly, unlike prior safety alignment\nefforts that often compromise reasoning performance, our method preserves the\nmodels' reasoning capabilities by maintaining the training data within the\noriginal distribution of generation. Model weights of RealSafe-R1 are\nopen-source at https://huggingface.co/RealSafe."}
{"id": "2504.10174", "pdf": "https://arxiv.org/pdf/2504.10174", "abs": "https://arxiv.org/abs/2504.10174", "authors": ["Yiding Lu", "Mouxing Yang", "Dezhong Peng", "Peng Hu", "Yijie Lin", "Xi Peng"], "title": "LLaVA-ReID: Selective Multi-image Questioner for Interactive Person Re-Identification", "categories": ["cs.CV"], "comment": null, "summary": "Traditional text-based person ReID assumes that person descriptions from\nwitnesses are complete and provided at once. However, in real-world scenarios,\nsuch descriptions are often partial or vague. To address this limitation, we\nintroduce a new task called interactive person re-identification (Inter-ReID).\nInter-ReID is a dialogue-based retrieval task that iteratively refines initial\ndescriptions through ongoing interactions with the witnesses. To facilitate the\nstudy of this new task, we construct a dialogue dataset that incorporates\nmultiple types of questions by decomposing fine-grained attributes of\nindividuals. We further propose LLaVA-ReID, a question model that generates\ntargeted questions based on visual and textual contexts to elicit additional\ndetails about the target person. Leveraging a looking-forward strategy, we\nprioritize the most informative questions as supervision during training.\nExperimental results on both Inter-ReID and text-based ReID benchmarks\ndemonstrate that LLaVA-ReID significantly outperforms baselines."}
{"id": "2504.10090", "pdf": "https://arxiv.org/pdf/2504.10090", "abs": "https://arxiv.org/abs/2504.10090", "authors": ["I-Sheng Fang", "Jun-Cheng Chen"], "title": "CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) and multimodal large language models (MLLMs)\nhave significantly advanced artificial intelligence. However, visual reasoning,\nreasoning involving both visual and textual inputs, remains underexplored.\nRecent advancements, including the reasoning models like OpenAI o1 and Gemini\n2.0 Flash Thinking, which incorporate image inputs, have opened this\ncapability. In this ongoing work, we focus specifically on photography-related\ntasks because a photo is a visual snapshot of the physical world where the\nunderlying physics (i.e., illumination, blur extent, etc.) interplay with the\ncamera parameters. Successfully reasoning from the visual information of a\nphoto to identify these numerical camera settings requires the MLLMs to have a\ndeeper understanding of the underlying physics for precise visual\ncomprehension, representing a challenging and intelligent capability essential\nfor practical applications like photography assistant agents. We aim to\nevaluate MLLMs on their ability to distinguish visual differences related to\nnumerical camera settings, extending a methodology previously proposed for\nvision-language models (VLMs). Our preliminary results demonstrate the\nimportance of visual reasoning in photography-related tasks. Moreover, these\nresults show that no single MLLM consistently dominates across all evaluation\ntasks, demonstrating ongoing challenges and opportunities in developing MLLMs\nwith better visual reasoning."}
{"id": "2504.10190", "pdf": "https://arxiv.org/pdf/2504.10190", "abs": "https://arxiv.org/abs/2504.10190", "authors": ["Kaushik Bhargav Sivangi", "Idris Zakariyya", "Paul Henderson", "Fani Deligianni"], "title": "Differentially Private 2D Human Pose Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Human pose estimation (HPE) has become essential in numerous applications\nincluding healthcare, activity recognition, and human-computer interaction.\nHowever, the privacy implications of processing sensitive visual data present\nsignificant deployment barriers in critical domains. While traditional\nanonymization techniques offer limited protection and often compromise data\nutility for broader motion analysis, Differential Privacy (DP) provides formal\nprivacy guarantees but typically degrades model performance when applied\nnaively. In this work, we present the first differentially private 2D human\npose estimation (2D-HPE) by applying Differentially Private Stochastic Gradient\nDescent (DP-SGD) to this task. To effectively balance privacy with performance,\nwe adopt Projected DP-SGD (PDP-SGD), which projects the noisy gradients to a\nlow-dimensional subspace. Additionally, we adapt TinyViT, a compact and\nefficient vision transformer for coordinate classification in HPE, providing a\nlightweight yet powerful backbone that enhances privacy-preserving deployment\nfeasibility on resource-limited devices. Our approach is particularly valuable\nfor multimedia interpretation tasks, enabling privacy-safe analysis and\nunderstanding of human motion across diverse visual media while preserving the\nsemantic meaning required for downstream applications. Comprehensive\nexperiments on the MPII Human Pose Dataset demonstrate significant performance\nenhancement with PDP-SGD achieving 78.48% PCKh@0.5 at a strict privacy budget\n($\\epsilon=0.2$), compared to 63.85% for standard DP-SGD. This work lays\nfoundation for privacy-preserving human pose estimation in real-world,\nsensitive applications."}
{"id": "2504.10127", "pdf": "https://arxiv.org/pdf/2504.10127", "abs": "https://arxiv.org/abs/2504.10127", "authors": ["Junlei Zhang", "Zichen Ding", "Chang Ma", "Zijie Chen", "Qiushi Sun", "Zhenzhong Lan", "Junxian He"], "title": "Breaking the Data Barrier -- Building GUI Agents Through Task Generalization", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "24 pages, 11 figures", "summary": "Graphical User Interface (GUI) agents offer cross-platform solutions for\nautomating complex digital tasks, with significant potential to transform\nproductivity workflows. However, their performance is often constrained by the\nscarcity of high-quality trajectory data. To address this limitation, we\npropose training Vision Language Models (VLMs) on data-rich,\nreasoning-intensive tasks during a dedicated mid-training stage, and then\nexamine how incorporating these tasks facilitates generalization to GUI\nplanning scenarios. Specifically, we explore a range of tasks with readily\navailable instruction-tuning data, including GUI perception, multimodal\nreasoning, and textual reasoning. Through extensive experiments across 11\nmid-training tasks, we demonstrate that: (1) Task generalization proves highly\neffective, yielding substantial improvements across most settings. For\ninstance, multimodal mathematical reasoning enhances performance on\nAndroidWorld by an absolute 6.3%. Remarkably, text-only mathematical data\nsignificantly boosts GUI web agent performance, achieving a 5.6% improvement on\nWebArena and 5.4% improvement on AndroidWorld, underscoring notable cross-modal\ngeneralization from text-based to visual domains; (2) Contrary to prior\nassumptions, GUI perception data - previously considered closely aligned with\nGUI agent tasks and widely utilized for training - has a comparatively limited\nimpact on final performance; (3) Building on these insights, we identify the\nmost effective mid-training tasks and curate optimized mixture datasets,\nresulting in absolute performance gains of 8.0% on WebArena and 12.2% on\nAndroidWorld. Our work provides valuable insights into cross-domain knowledge\ntransfer for GUI agents and offers a practical approach to addressing data\nscarcity challenges in this emerging field. The code, data and models will be\navailable at https://github.com/hkust-nlp/GUIMid."}
{"id": "2504.10201", "pdf": "https://arxiv.org/pdf/2504.10201", "abs": "https://arxiv.org/abs/2504.10201", "authors": ["Raphael Achddou", "Yann Gousseau", "Saïd Ladjal", "Sabine Süsstrunk"], "title": "VibrantLeaves: A principled parametric image generator for training deep restoration models", "categories": ["cs.CV"], "comment": null, "summary": "Even though Deep Neural Networks are extremely powerful for image restoration\ntasks, they have several limitations. They are poorly understood and suffer\nfrom strong biases inherited from the training sets. One way to address these\nshortcomings is to have a better control over the training sets, in particular\nby using synthetic sets. In this paper, we propose a synthetic image generator\nrelying on a few simple principles. In particular, we focus on geometric\nmodeling, textures, and a simple modeling of image acquisition. These\nproperties, integrated in a classical Dead Leaves model, enable the creation of\nefficient training sets. Standard image denoising and super-resolution networks\ncan be trained on such datasets, reaching performance almost on par with\ntraining on natural image datasets. As a first step towards explainability, we\nprovide a careful analysis of the considered principles, identifying which\nimage properties are necessary to obtain good performances. Besides, such\ntraining also yields better robustness to various geometric and radiometric\nperturbations of the test sets."}
{"id": "2504.10179", "pdf": "https://arxiv.org/pdf/2504.10179", "abs": "https://arxiv.org/abs/2504.10179", "authors": ["Anwesha Mohanty", "Venkatesh Balavadhani Parthasarathy", "Arsalan Shahid"], "title": "The Future of MLLM Prompting is Adaptive: A Comprehensive Experimental Evaluation of Prompt Engineering Methods for Robust Multimodal Performance", "categories": ["cs.AI", "cs.CL", "cs.ET"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) are set to transform how machines\nprocess and generate human-like responses by integrating diverse modalities\nsuch as text, images, and code. Yet, effectively harnessing their capabilities\nhinges on optimal prompt engineering. We present a comprehensive experimental\nevaluation of seven prompt engineering methods applied to 13 open-source MLLMs\nover 24 tasks spanning Reasoning and Compositionality, Multimodal Understanding\nand Alignment, Complex Code Generation and Execution, and Knowledge Retrieval\nand Integration. Our approach stratifies models by parameter count into Small\n(<4B), Medium (4B-10B), and Large (>10B) categories and compares prompting\ntechniques including Zero-Shot, One-Shot, Few-Shot, Chain-of-Thought,\nAnalogical, Generated Knowledge, and Tree-of-Thought. While Large MLLMs excel\nin structured tasks such as code generation, achieving accuracies up to 96.88%\nunder Few-Shot prompting, all models struggle with complex reasoning and\nabstract understanding, often yielding accuracies below 60% and high\nhallucination rates. Structured reasoning prompts frequently increased\nhallucination up to 75% in small models and led to longer response times (over\n20 seconds in Large MLLMs), while simpler prompting methods provided more\nconcise and efficient outputs. No single prompting method uniformly optimises\nall task types. Instead, adaptive strategies combining example-based guidance\nwith selective structured reasoning are essential to enhance robustness,\nefficiency, and factual accuracy. Our findings offer practical recommendations\nfor prompt engineering and support more reliable deployment of MLLMs across\napplications including AI-assisted coding, knowledge retrieval, and multimodal\ncontent understanding."}
{"id": "2504.10214", "pdf": "https://arxiv.org/pdf/2504.10214", "abs": "https://arxiv.org/abs/2504.10214", "authors": ["Songze Li", "Qixing Xu", "Tonghua Su", "Xu-Yao Zhang", "Zhongjie Wang"], "title": "Balancing Stability and Plasticity in Pretrained Detector: A Dual-Path Framework for Incremental Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "The balance between stability and plasticity remains a fundamental challenge\nin pretrained model-based incremental object detection (PTMIOD). While existing\nPTMIOD methods demonstrate strong performance on in-domain tasks aligned with\npretraining data, their plasticity to cross-domain scenarios remains\nunderexplored. Through systematic component-wise analysis of pretrained\ndetectors, we reveal a fundamental discrepancy: the localization modules\ndemonstrate inherent cross-domain stability-preserving precise bounding box\nestimation across distribution shifts-while the classification components\nrequire enhanced plasticity to mitigate discriminability degradation in\ncross-domain scenarios. Motivated by these findings, we propose a dual-path\nframework built upon pretrained DETR-based detectors which decouples\nlocalization stability and classification plasticity: the localization path\nmaintains stability to preserve pretrained localization knowledge, while the\nclassification path facilitates plasticity via parameter-efficient fine-tuning\nand resists forgetting with pseudo-feature replay. Extensive evaluations on\nboth in-domain (MS COCO and PASCAL VOC) and cross-domain (TT100K) benchmarks\nshow state-of-the-art performance, demonstrating our method's ability to\neffectively balance stability and plasticity in PTMIOD, achieving robust\ncross-domain adaptation and strong retention of anti-forgetting capabilities."}
{"id": "2504.10250", "pdf": "https://arxiv.org/pdf/2504.10250", "abs": "https://arxiv.org/abs/2504.10250", "authors": ["Eugene Yang", "Nicola Tonellotto", "Dawn Lawrie", "Sean MacAvaney", "James Mayfield", "Douglas W. Oard", "Scott Miller"], "title": "MURR: Model Updating with Regularized Replay for Searching a Document Stream", "categories": ["cs.IR", "cs.CL"], "comment": "Published at ECIR 2025. 16 pages, 4 figures", "summary": "The Internet produces a continuous stream of new documents and user-generated\nqueries. These naturally change over time based on events in the world and the\nevolution of language. Neural retrieval models that were trained once on a\nfixed set of query-document pairs will quickly start misrepresenting\nnewly-created content and queries, leading to less effective retrieval.\nTraditional statistical sparse retrieval can update collection statistics to\nreflect these changes in the use of language in documents and queries. In\ncontrast, continued fine-tuning of the language model underlying neural\nretrieval approaches such as DPR and ColBERT creates incompatibility with\npreviously-encoded documents. Re-encoding and re-indexing all\npreviously-processed documents can be costly. In this work, we explore updating\na neural dual encoder retrieval model without reprocessing past documents in\nthe stream. We propose MURR, a model updating strategy with regularized replay,\nto ensure the model can still faithfully search existing documents without\nreprocessing, while continuing to update the model for the latest topics. In\nour simulated streaming environments, we show that fine-tuning models using\nMURR leads to more effective and more consistent retrieval results than other\nstrategies as the stream of documents and queries progresses."}
{"id": "2504.10242", "pdf": "https://arxiv.org/pdf/2504.10242", "abs": "https://arxiv.org/abs/2504.10242", "authors": ["Tianyu Xin", "Jin-Liang Xiao", "Zeyu Xia", "Shan Yin", "Liang-Jian Deng"], "title": "CAT: A Conditional Adaptation Tailor for Efficient and Effective Instance-Specific Pansharpening on Real-World Data", "categories": ["cs.CV"], "comment": null, "summary": "Pansharpening is a crucial remote sensing technique that fuses low-resolution\nmultispectral (LRMS) images with high-resolution panchromatic (PAN) images to\ngenerate high-resolution multispectral (HRMS) imagery. Although deep learning\ntechniques have significantly advanced pansharpening, many existing methods\nsuffer from limited cross-sensor generalization and high computational\noverhead, restricting their real-time applications. To address these\nchallenges, we propose an efficient framework that quickly adapts to a specific\ninput instance, completing both training and inference in a short time. Our\nframework splits the input image into multiple patches, selects a subset for\nunsupervised CAT training, and then performs inference on all patches,\nstitching them into the final output. The CAT module, integrated between the\nfeature extraction and channel transformation stages of a pre-trained network,\ntailors the fused features and fixes the parameters for efficient inference,\ngenerating improved results. Our approach offers two key advantages: (1)\n$\\textit{Improved Generalization Ability}$: by mitigating cross-sensor\ndegradation, our model--although pre-trained on a specific dataset--achieves\nsuperior performance on datasets captured by other sensors; (2)\n$\\textit{Enhanced Computational Efficiency}$: the CAT-enhanced network can\nswiftly adapt to the test sample using the single LRMS-PAN pair input, without\nrequiring extensive large-scale data retraining. Experiments on the real-world\ndata from WorldView-3 and WorldView-2 datasets demonstrate that our method\nachieves state-of-the-art performance on cross-sensor real-world data, while\nachieving both training and inference of $512\\times512$ image within\n$\\textit{0.4 seconds}$ and $4000\\times4000$ image within $\\textit{3 seconds}$\nat the fastest setting on a commonly used RTX 3090 GPU."}
{"id": "2504.10277", "pdf": "https://arxiv.org/pdf/2504.10277", "abs": "https://arxiv.org/abs/2504.10277", "authors": ["Pierre Le Jeune", "Jiaen Liu", "Luca Rossi", "Matteo Dora"], "title": "RealHarm: A Collection of Real-World Language Model Application Failures", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "Language model deployments in consumer-facing applications introduce numerous\nrisks. While existing research on harms and hazards of such applications\nfollows top-down approaches derived from regulatory frameworks and theoretical\nanalyses, empirical evidence of real-world failure modes remains underexplored.\nIn this work, we introduce RealHarm, a dataset of annotated problematic\ninteractions with AI agents built from a systematic review of publicly reported\nincidents. Analyzing harms, causes, and hazards specifically from the\ndeployer's perspective, we find that reputational damage constitutes the\npredominant organizational harm, while misinformation emerges as the most\ncommon hazard category. We empirically evaluate state-of-the-art guardrails and\ncontent moderation systems to probe whether such systems would have prevented\nthe incidents, revealing a significant gap in the protection of AI\napplications."}
{"id": "2504.10254", "pdf": "https://arxiv.org/pdf/2504.10254", "abs": "https://arxiv.org/abs/2504.10254", "authors": ["Xuqiang Cao", "Linnan Zhao", "Jiaxuan Zhao", "Fang Liu", "Puhua Chen", "Wenping Ma"], "title": "MASSeg : 2nd Technical Report for 4th PVUW MOSE Track", "categories": ["cs.CV", "cs.AI"], "comment": "5 pages,4 figures,Technical report on Complex Video Object\n  Segmentation", "summary": "Complex video object segmentation continues to face significant challenges in\nsmall object recognition, occlusion handling, and dynamic scene modeling. This\nreport presents our solution, which ranked second in the MOSE track of CVPR\n2025 PVUW Challenge. Based on an existing segmentation framework, we propose an\nimproved model named MASSeg for complex video object segmentation, and\nconstruct an enhanced dataset, MOSE+, which includes typical scenarios with\nocclusions, cluttered backgrounds, and small target instances. During training,\nwe incorporate a combination of inter-frame consistent and inconsistent data\naugmentation strategies to improve robustness and generalization. During\ninference, we design a mask output scaling strategy to better adapt to varying\nobject sizes and occlusion levels. As a result, MASSeg achieves a J score of\n0.8250, F score of 0.9007, and a J&F score of 0.8628 on the MOSE test set."}
{"id": "2504.10352", "pdf": "https://arxiv.org/pdf/2504.10352", "abs": "https://arxiv.org/abs/2504.10352", "authors": ["Yifan Yang", "Shujie Liu", "Jinyu Li", "Yuxuan Hu", "Haibin Wu", "Hui Wang", "Jianwei Yu", "Lingwei Meng", "Haiyang Sun", "Yanqing Liu", "Yan Lu", "Kai Yu", "Xie Chen"], "title": "Pseudo-Autoregressive Neural Codec Language Models for Efficient Zero-Shot Text-to-Speech Synthesis", "categories": ["eess.AS", "cs.CL"], "comment": "Submitted to ACM MM 2025", "summary": "Recent zero-shot text-to-speech (TTS) systems face a common dilemma:\nautoregressive (AR) models suffer from slow generation and lack duration\ncontrollability, while non-autoregressive (NAR) models lack temporal modeling\nand typically require complex designs. In this paper, we introduce a novel\npseudo-autoregressive (PAR) codec language modeling approach that unifies AR\nand NAR modeling. Combining explicit temporal modeling from AR with parallel\ngeneration from NAR, PAR generates dynamic-length spans at fixed time steps.\nBuilding on PAR, we propose PALLE, a two-stage TTS system that leverages PAR\nfor initial generation followed by NAR refinement. In the first stage, PAR\nprogressively generates speech tokens along the time dimension, with each step\npredicting all positions in parallel but only retaining the left-most span. In\nthe second stage, low-confidence tokens are iteratively refined in parallel,\nleveraging the global contextual information. Experiments demonstrate that\nPALLE, trained on LibriTTS, outperforms state-of-the-art systems trained on\nlarge-scale data, including F5-TTS, E2-TTS, and MaskGCT, on the LibriSpeech\ntest-clean set in terms of speech quality, speaker similarity, and\nintelligibility, while achieving up to ten times faster inference speed. Audio\nsamples are available at https://anonymous-palle.github.io."}
{"id": "2504.10258", "pdf": "https://arxiv.org/pdf/2504.10258", "abs": "https://arxiv.org/abs/2504.10258", "authors": ["Shuai Liu", "Youmeng Li", "Jizeng Wei"], "title": "XY-Cut++: Advanced Layout Ordering via Hierarchical Mask Mechanism on a Novel Benchmark", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Document Reading Order Recovery is a fundamental task in document image\nunderstanding, playing a pivotal role in enhancing Retrieval-Augmented\nGeneration (RAG) and serving as a critical preprocessing step for large\nlanguage models (LLMs). Existing methods often struggle with complex\nlayouts(e.g., multi-column newspapers), high-overhead interactions between\ncross-modal elements (visual regions and textual semantics), and a lack of\nrobust evaluation benchmarks. We introduce XY-Cut++, an advanced layout\nordering method that integrates pre-mask processing, multi-granularity\nsegmentation, and cross-modal matching to address these challenges. Our method\nsignificantly enhances layout ordering accuracy compared to traditional XY-Cut\ntechniques. Specifically, XY-Cut++ achieves state-of-the-art performance (98.8\nBLEU overall) while maintaining simplicity and efficiency. It outperforms\nexisting baselines by up to 24\\% and demonstrates consistent accuracy across\nsimple and complex layouts on the newly introduced DocBench-100 dataset. This\nadvancement establishes a reliable foundation for document structure recovery,\nsetting a new standard for layout ordering tasks and facilitating more\neffective RAG and LLM preprocessing."}
{"id": "2504.10384", "pdf": "https://arxiv.org/pdf/2504.10384", "abs": "https://arxiv.org/abs/2504.10384", "authors": ["Alana Marie Dee", "Sajjad Moazeni"], "title": "A 10.8mW Mixed-Signal Simulated Bifurcation Ising Solver using SRAM Compute-In-Memory with 0.6us Time-to-Solution", "categories": ["eess.SY", "cs.CL", "cs.SY"], "comment": null, "summary": "Combinatorial optimization problems are funda- mental for various fields\nranging from finance to wireless net- works. This work presents a simulated\nbifurcation (SB) Ising solver in CMOS for NP-hard optimization problems. Analog\ndomain computing led to a superior implementation of this algorithm as inherent\nand injected noise is required in SB Ising solvers. The architecture novelties\ninclude the use of SRAM compute-in-memory (CIM) to accelerate bifurcation as\nwell as the generation and injection of optimal decaying noise in the analog\ndomain. We propose a novel 10-T SRAM cell capable of performing ternary\nmultiplication. When measured with 60- node, 50% density, random, binary MAXCUT\ngraphs, this all- to-all connected Ising solver reliably achieves above 93% of\nthe ground state solution in 0.6us with 10.8mW average power in TSMC 180nm\nCMOS. Our chip achieves an order of magnitude improvement in time-to-solution\nand power compared to previously proposed Ising solvers in CMOS and other\nplatforms."}
{"id": "2504.10267", "pdf": "https://arxiv.org/pdf/2504.10267", "abs": "https://arxiv.org/abs/2504.10267", "authors": ["Mengdi Wang", "Efe Bozkir", "Enkelejda Kasneci"], "title": "Trade-offs in Privacy-Preserving Eye Tracking through Iris Obfuscation: A Benchmarking Study", "categories": ["cs.CV"], "comment": "The 25th International Conference on Digital Signal Processing (DSP\n  2025)", "summary": "Recent developments in hardware, computer graphics, and AI may soon enable\nAR/VR head-mounted displays (HMDs) to become everyday devices like smartphones\nand tablets. Eye trackers within HMDs provide a special opportunity for such\nsetups as it is possible to facilitate gaze-based research and interaction.\nHowever, estimating users' gaze information often requires raw eye images and\nvideos that contain iris textures, which are considered a gold standard\nbiometric for user authentication, and this raises privacy concerns. Previous\nresearch in the eye-tracking community focused on obfuscating iris textures\nwhile keeping utility tasks such as gaze estimation accurate. Despite these\nattempts, there is no comprehensive benchmark that evaluates state-of-the-art\napproaches. Considering all, in this paper, we benchmark blurring, noising,\ndownsampling, rubber sheet model, and iris style transfer to obfuscate user\nidentity, and compare their impact on image quality, privacy, utility, and risk\nof imposter attack on two datasets. We use eye segmentation and gaze estimation\nas utility tasks, and reduction in iris recognition accuracy as a measure of\nprivacy protection, and false acceptance rate to estimate risk of attack. Our\nexperiments show that canonical image processing methods like blurring and\nnoising cause a marginal impact on deep learning-based tasks. While\ndownsampling, rubber sheet model, and iris style transfer are effective in\nhiding user identifiers, iris style transfer, with higher computation cost,\noutperforms others in both utility tasks, and is more resilient against spoof\nattacks. Our analyses indicate that there is no universal optimal approach to\nbalance privacy, utility, and computation burden. Therefore, we recommend\npractitioners consider the strengths and weaknesses of each approach, and\npossible combinations of those to reach an optimal privacy-utility trade-off."}
{"id": "2504.10443", "pdf": "https://arxiv.org/pdf/2504.10443", "abs": "https://arxiv.org/abs/2504.10443", "authors": ["Haoran Hao", "Jiaming Han", "Yiyuan Zhang", "Xiangyu Yue"], "title": "Multimodal Long Video Modeling Based on Temporal Dynamic Context", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have led to significant\nbreakthroughs in video understanding. However, existing models still struggle\nwith long video processing due to the context length constraint of LLMs and the\nvast amount of information within the video. Although some recent methods are\ndesigned for long video understanding, they often lose crucial information\nduring token compression and struggle with additional modality like audio. In\nthis work, we propose a dynamic long video encoding method utilizing the\ntemporal relationship between frames, named Temporal Dynamic Context (TDC).\nFirstly, we segment the video into semantically consistent scenes based on\ninter-frame similarities, then encode each frame into tokens using visual-audio\nencoders. Secondly, we propose a novel temporal context compressor to reduce\nthe number of tokens within each segment. Specifically, we employ a query-based\nTransformer to aggregate video, audio, and instruction text tokens into a\nlimited set of temporal context tokens. Finally, we feed the static frame\ntokens and the temporal context tokens into the LLM for video understanding.\nFurthermore, to handle extremely long videos, we propose a training-free\nchain-of-thought strategy that progressively extracts answers from multiple\nvideo segments. These intermediate answers serve as part of the reasoning\nprocess and contribute to the final answer. We conduct extensive experiments on\ngeneral video understanding and audio-video understanding benchmarks, where our\nmethod demonstrates strong performance. The code and models are available at\nhttps://github.com/Hoar012/TDC-Video."}
{"id": "2504.10275", "pdf": "https://arxiv.org/pdf/2504.10275", "abs": "https://arxiv.org/abs/2504.10275", "authors": ["Harsh Yadav", "Maximilian Schaefer", "Kun Zhao", "Tobias Meisen"], "title": "LMFormer: Lane based Motion Prediction Transformer", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted: Autonomous Driving Workshop, CVPR 2025", "summary": "Motion prediction plays an important role in autonomous driving. This study\npresents LMFormer, a lane-aware transformer network for trajectory prediction\ntasks. In contrast to previous studies, our work provides a simple mechanism to\ndynamically prioritize the lanes and shows that such a mechanism introduces\nexplainability into the learning behavior of the network. Additionally,\nLMFormer uses the lane connection information at intersections, lane merges,\nand lane splits, in order to learn long-range dependency in lane structure.\nMoreover, we also address the issue of refining the predicted trajectories and\npropose an efficient method for iterative refinement through stacked\ntransformer layers. For benchmarking, we evaluate LMFormer on the nuScenes\ndataset and demonstrate that it achieves SOTA performance across multiple\nmetrics. Furthermore, the Deep Scenario dataset is used to not only illustrate\ncross-dataset network performance but also the unification capabilities of\nLMFormer to train on multiple datasets and achieve better performance."}
{"id": "2504.10445", "pdf": "https://arxiv.org/pdf/2504.10445", "abs": "https://arxiv.org/abs/2504.10445", "authors": ["Suyu Ye", "Haojun Shi", "Darren Shih", "Hyokun Yun", "Tanya Roosta", "Tianmin Shu"], "title": "RealWebAssist: A Benchmark for Long-Horizon Web Assistance with Real-World Users", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "Project Website: https://scai.cs.jhu.edu/projects/RealWebAssist/\n  Code: https://github.com/SCAI-JHU/RealWebAssist", "summary": "To achieve successful assistance with long-horizon web-based tasks, AI agents\nmust be able to sequentially follow real-world user instructions over a long\nperiod. Unlike existing web-based agent benchmarks, sequential instruction\nfollowing in the real world poses significant challenges beyond performing a\nsingle, clearly defined task. For instance, real-world human instructions can\nbe ambiguous, require different levels of AI assistance, and may evolve over\ntime, reflecting changes in the user's mental state. To address this gap, we\nintroduce RealWebAssist, a novel benchmark designed to evaluate sequential\ninstruction-following in realistic scenarios involving long-horizon\ninteractions with the web, visual GUI grounding, and understanding ambiguous\nreal-world user instructions. RealWebAssist includes a dataset of sequential\ninstructions collected from real-world human users. Each user instructs a\nweb-based assistant to perform a series of tasks on multiple websites. A\nsuccessful agent must reason about the true intent behind each instruction,\nkeep track of the mental state of the user, understand user-specific routines,\nand ground the intended tasks to actions on the correct GUI elements. Our\nexperimental results show that state-of-the-art models struggle to understand\nand ground user instructions, posing critical challenges in following\nreal-world user instructions for long-horizon web assistance."}
{"id": "2504.10278", "pdf": "https://arxiv.org/pdf/2504.10278", "abs": "https://arxiv.org/abs/2504.10278", "authors": ["Jinyue Zhang", "Xiangrong Zhang", "Zhongjian Huang", "Tianyang Zhang", "Yifei Jiang", "Licheng Jiao"], "title": "DiffMOD: Progressive Diffusion Point Denoising for Moving Object Detection in Remote Sensing", "categories": ["cs.CV", "68T10", "I.4.8"], "comment": "9 pages, 7 figures", "summary": "Moving object detection (MOD) in remote sensing is significantly challenged\nby low resolution, extremely small object sizes, and complex noise\ninterference. Current deep learning-based MOD methods rely on probability\ndensity estimation, which restricts flexible information interaction between\nobjects and across temporal frames. To flexibly capture high-order inter-object\nand temporal relationships, we propose a point-based MOD in remote sensing.\nInspired by diffusion models, the network optimization is formulated as a\nprogressive denoising process that iteratively recovers moving object centers\nfrom sparse noisy points. Specifically, we sample scattered features from the\nbackbone outputs as atomic units for subsequent processing, while global\nfeature embeddings are aggregated to compensate for the limited coverage of\nsparse point features. By modeling spatial relative positions and semantic\naffinities, Spatial Relation Aggregation Attention is designed to enable\nhigh-order interactions among point-level features for enhanced object\nrepresentation. To enhance temporal consistency, the Temporal Propagation and\nGlobal Fusion module is designed, which leverages an implicit memory reasoning\nmechanism for robust cross-frame feature integration. To align with the\nprogressive denoising process, we propose a progressive MinK optimal transport\nassignment strategy that establishes specialized learning objectives at each\ndenoising level. Additionally, we introduce a missing loss function to\ncounteract the clustering tendency of denoised points around salient objects.\nExperiments on the RsData remote sensing MOD dataset show that our MOD method\nbased on scattered point denoising can more effectively explore potential\nrelationships between sparse moving objects and improve the detection\ncapability and temporal consistency."}
{"id": "2504.10458", "pdf": "https://arxiv.org/pdf/2504.10458", "abs": "https://arxiv.org/abs/2504.10458", "authors": ["Xiaobo Xia", "Run Luo"], "title": "GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents", "categories": ["cs.CV", "cs.CL", "cs.HC"], "comment": null, "summary": "Existing efforts in building Graphical User Interface (GUI) agents largely\nrely on the training paradigm of supervised fine-tuning on Large\nVision-Language Models (LVLMs). However, this approach not only demands\nextensive amounts of training data but also struggles to effectively understand\nGUI screenshots and generalize to unseen interfaces. The issue significantly\nlimits its application in real-world scenarios, especially for high-level\ntasks. Inspired by Reinforcement Fine-Tuning (RFT) in large reasoning models\n(e.g., DeepSeek-R1), which efficiently enhances the problem-solving\ncapabilities of large language models in real-world settings, we propose \\name,\nthe first reinforcement learning framework designed to enhance the GUI\ncapabilities of LVLMs in high-level real-world task scenarios, through unified\naction space rule modeling. By leveraging a small amount of carefully curated\nhigh-quality data across multiple platforms (including Windows, Linux, MacOS,\nAndroid, and Web) and employing policy optimization algorithms such as Group\nRelative Policy Optimization (GRPO) to update the model, \\name achieves\nsuperior performance using only 0.02\\% of the data (3K vs. 13M) compared to\nprevious state-of-the-art methods like OS-Atlas across eight benchmarks\nspanning three different platforms (mobile, desktop, and web). These results\ndemonstrate the immense potential of reinforcement learning based on unified\naction space rule modeling in improving the execution capabilities of LVLMs for\nreal-world GUI agent tasks."}
{"id": "2504.10288", "pdf": "https://arxiv.org/pdf/2504.10288", "abs": "https://arxiv.org/abs/2504.10288", "authors": ["Mathieu Manni", "Dmitry Karpov", "K. Joost Batenburg", "Sharon Shwartz", "Nicola Viganò"], "title": "Noise2Ghost: Self-supervised deep convolutional reconstruction for ghost imaging", "categories": ["cs.CV", "cs.LG", "physics.data-an"], "comment": null, "summary": "We present a new self-supervised deep-learning-based Ghost Imaging (GI)\nreconstruction method, which provides unparalleled reconstruction performance\nfor noisy acquisitions among unsupervised methods. We present the supporting\nmathematical framework and results from theoretical and real data use cases.\nSelf-supervision removes the need for clean reference data while offering\nstrong noise reduction. This provides the necessary tools for addressing\nsignal-to-noise ratio concerns for GI acquisitions in emerging and cutting-edge\nlow-light GI scenarios. Notable examples include micro- and nano-scale x-ray\nemission imaging, e.g., x-ray fluorescence imaging of dose-sensitive samples.\nTheir applications include in-vivo and in-operando case studies for biological\nsamples and batteries."}
{"id": "2504.10471", "pdf": "https://arxiv.org/pdf/2504.10471", "abs": "https://arxiv.org/abs/2504.10471", "authors": ["Chenghao Xiao", "Isaac Chung", "Imene Kerboua", "Jamie Stirling", "Xin Zhang", "Márton Kardos", "Roman Solomatin", "Noura Al Moubayed", "Kenneth Enevoldsen", "Niklas Muennighoff"], "title": "MIEB: Massive Image Embedding Benchmark", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Image representations are often evaluated through disjointed, task-specific\nprotocols, leading to a fragmented understanding of model capabilities. For\ninstance, it is unclear whether an image embedding model adept at clustering\nimages is equally good at retrieving relevant images given a piece of text. We\nintroduce the Massive Image Embedding Benchmark (MIEB) to evaluate the\nperformance of image and image-text embedding models across the broadest\nspectrum to date. MIEB spans 38 languages across 130 individual tasks, which we\ngroup into 8 high-level categories. We benchmark 50 models across our\nbenchmark, finding that no single method dominates across all task categories.\nWe reveal hidden capabilities in advanced vision models such as their accurate\nvisual representation of texts, and their yet limited capabilities in\ninterleaved encodings and matching images and texts in the presence of\nconfounders. We also show that the performance of vision encoders on MIEB\ncorrelates highly with their performance when used in multimodal large language\nmodels. Our code, dataset, and leaderboard are publicly available at\nhttps://github.com/embeddings-benchmark/mteb."}
{"id": "2504.10316", "pdf": "https://arxiv.org/pdf/2504.10316", "abs": "https://arxiv.org/abs/2504.10316", "authors": ["Huiqi Wu", "Jianbo Mei", "Yingjie Huang", "Yining Xu", "Jingjiao You", "Yilong Liu", "Li Yao"], "title": "ESCT3D: Efficient and Selectively Controllable Text-Driven 3D Content Generation with Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, significant advancements have been made in text-driven 3D\ncontent generation. However, several challenges remain. In practical\napplications, users often provide extremely simple text inputs while expecting\nhigh-quality 3D content. Generating optimal results from such minimal text is a\ndifficult task due to the strong dependency of text-to-3D models on the quality\nof input prompts. Moreover, the generation process exhibits high variability,\nmaking it difficult to control. Consequently, multiple iterations are typically\nrequired to produce content that meets user expectations, reducing generation\nefficiency. To address this issue, we propose GPT-4V for self-optimization,\nwhich significantly enhances the efficiency of generating satisfactory content\nin a single attempt. Furthermore, the controllability of text-to-3D generation\nmethods has not been fully explored. Our approach enables users to not only\nprovide textual descriptions but also specify additional conditions, such as\nstyle, edges, scribbles, poses, or combinations of multiple conditions,\nallowing for more precise control over the generated 3D content. Additionally,\nduring training, we effectively integrate multi-view information, including\nmulti-view depth, masks, features, and images, to address the common Janus\nproblem in 3D content generation. Extensive experiments demonstrate that our\nmethod achieves robust generalization, facilitating the efficient and\ncontrollable generation of high-quality 3D content."}
{"id": "2504.10317", "pdf": "https://arxiv.org/pdf/2504.10317", "abs": "https://arxiv.org/abs/2504.10317", "authors": ["Yuxin Wen", "Jim Wu", "Ajay Jain", "Tom Goldstein", "Ashwinee Panda"], "title": "Analysis of Attention in Video Diffusion Transformers", "categories": ["cs.CV"], "comment": null, "summary": "We conduct an in-depth analysis of attention in video diffusion transformers\n(VDiTs) and report a number of novel findings. We identify three key properties\nof attention in VDiTs: Structure, Sparsity, and Sinks. Structure: We observe\nthat attention patterns across different VDiTs exhibit similar structure across\ndifferent prompts, and that we can make use of the similarity of attention\npatterns to unlock video editing via self-attention map transfer. Sparse: We\nstudy attention sparsity in VDiTs, finding that proposed sparsity methods do\nnot work for all VDiTs, because some layers that are seemingly sparse cannot be\nsparsified. Sinks: We make the first study of attention sinks in VDiTs,\ncomparing and contrasting them to attention sinks in language models. We\npropose a number of future directions that can make use of our insights to\nimprove the efficiency-quality Pareto frontier for VDiTs."}
{"id": "2504.10320", "pdf": "https://arxiv.org/pdf/2504.10320", "abs": "https://arxiv.org/abs/2504.10320", "authors": ["Zongcan Ding", "Haodong Zhang", "Peng Wu", "Guansong Pang", "Zhiwei Yang", "Peng Wang", "Yanning Zhang"], "title": "SlowFastVAD: Video Anomaly Detection via Integrating Simple Detector and RAG-Enhanced Vision-Language Model", "categories": ["cs.CV"], "comment": null, "summary": "Video anomaly detection (VAD) aims to identify unexpected events in videos\nand has wide applications in safety-critical domains. While semi-supervised\nmethods trained on only normal samples have gained traction, they often suffer\nfrom high false alarm rates and poor interpretability. Recently,\nvision-language models (VLMs) have demonstrated strong multimodal reasoning\ncapabilities, offering new opportunities for explainable anomaly detection.\nHowever, their high computational cost and lack of domain adaptation hinder\nreal-time deployment and reliability. Inspired by dual complementary pathways\nin human visual perception, we propose SlowFastVAD, a hybrid framework that\nintegrates a fast anomaly detector with a slow anomaly detector (namely a\nretrieval augmented generation (RAG) enhanced VLM), to address these\nlimitations. Specifically, the fast detector first provides coarse anomaly\nconfidence scores, and only a small subset of ambiguous segments, rather than\nthe entire video, is further analyzed by the slower yet more interpretable VLM\nfor elaborate detection and reasoning. Furthermore, to adapt VLMs to\ndomain-specific VAD scenarios, we construct a knowledge base including normal\npatterns based on few normal samples and abnormal patterns inferred by VLMs.\nDuring inference, relevant patterns are retrieved and used to augment prompts\nfor anomaly reasoning. Finally, we smoothly fuse the anomaly confidence of fast\nand slow detectors to enhance robustness of anomaly detection. Extensive\nexperiments on four benchmarks demonstrate that SlowFastVAD effectively\ncombines the strengths of both fast and slow detectors, and achieves remarkable\ndetection accuracy and interpretability with significantly reduced\ncomputational overhead, making it well-suited for real-world VAD applications\nwith high reliability requirements."}
{"id": "2504.10329", "pdf": "https://arxiv.org/pdf/2504.10329", "abs": "https://arxiv.org/abs/2504.10329", "authors": ["Xingyu Lu", "Yuhang Hu", "YiFan Zhang", "Kaiyu Jiang", "Changyi Liu", "Tianke Zhang", "Jinpeng Wang", "Bin Wen", "Chun Yuan", "Fan Yang", "Tingting Gao", "Di Zhang"], "title": "InstructEngine: Instruction-driven Text-to-Image Alignment", "categories": ["cs.CV"], "comment": "8 pages, 7 figures", "summary": "Reinforcement Learning from Human/AI Feedback (RLHF/RLAIF) has been\nextensively utilized for preference alignment of text-to-image models. Existing\nmethods face certain limitations in terms of both data and algorithm. For\ntraining data, most approaches rely on manual annotated preference data, either\nby directly fine-tuning the generators or by training reward models to provide\ntraining signals. However, the high annotation cost makes them difficult to\nscale up, the reward model consumes extra computation and cannot guarantee\naccuracy. From an algorithmic perspective, most methods neglect the value of\ntext and only take the image feedback as a comparative signal, which is\ninefficient and sparse. To alleviate these drawbacks, we propose the\nInstructEngine framework. Regarding annotation cost, we first construct a\ntaxonomy for text-to-image generation, then develop an automated data\nconstruction pipeline based on it. Leveraging advanced large multimodal models\nand human-defined rules, we generate 25K text-image preference pairs. Finally,\nwe introduce cross-validation alignment method, which refines data efficiency\nby organizing semantically analogous samples into mutually comparable pairs.\nEvaluations on DrawBench demonstrate that InstructEngine improves SD v1.5 and\nSDXL's performance by 10.53% and 5.30%, outperforming state-of-the-art\nbaselines, with ablation study confirming the benefits of InstructEngine's all\ncomponents. A win rate of over 50% in human reviews also proves that\nInstructEngine better aligns with human preferences."}
{"id": "2504.10331", "pdf": "https://arxiv.org/pdf/2504.10331", "abs": "https://arxiv.org/abs/2504.10331", "authors": ["Hao Sun", "Fenggen Yu", "Huiyao Xu", "Tao Zhang", "Changqing Zou"], "title": "LL-Gaussian: Low-Light Scene Reconstruction and Enhancement via Gaussian Splatting for Novel View Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Novel view synthesis (NVS) in low-light scenes remains a significant\nchallenge due to degraded inputs characterized by severe noise, low dynamic\nrange (LDR) and unreliable initialization. While recent NeRF-based approaches\nhave shown promising results, most suffer from high computational costs, and\nsome rely on carefully captured or pre-processed data--such as RAW sensor\ninputs or multi-exposure sequences--which severely limits their practicality.\nIn contrast, 3D Gaussian Splatting (3DGS) enables real-time rendering with\ncompetitive visual fidelity; however, existing 3DGS-based methods struggle with\nlow-light sRGB inputs, resulting in unstable Gaussian initialization and\nineffective noise suppression. To address these challenges, we propose\nLL-Gaussian, a novel framework for 3D reconstruction and enhancement from\nlow-light sRGB images, enabling pseudo normal-light novel view synthesis. Our\nmethod introduces three key innovations: 1) an end-to-end Low-Light Gaussian\nInitialization Module (LLGIM) that leverages dense priors from learning-based\nMVS approach to generate high-quality initial point clouds; 2) a dual-branch\nGaussian decomposition model that disentangles intrinsic scene properties\n(reflectance and illumination) from transient interference, enabling stable and\ninterpretable optimization; 3) an unsupervised optimization strategy guided by\nboth physical constrains and diffusion prior to jointly steer decomposition and\nenhancement. Additionally, we contribute a challenging dataset collected in\nextreme low-light environments and demonstrate the effectiveness of\nLL-Gaussian. Compared to state-of-the-art NeRF-based methods, LL-Gaussian\nachieves up to 2,000 times faster inference and reduces training time to just\n2%, while delivering superior reconstruction and rendering quality."}
{"id": "2504.10350", "pdf": "https://arxiv.org/pdf/2504.10350", "abs": "https://arxiv.org/abs/2504.10350", "authors": ["Filipa Lino", "Carlos Santiago", "Manuel Marques"], "title": "Benchmarking 3D Human Pose Estimation Models Under Occlusions", "categories": ["cs.CV"], "comment": null, "summary": "This paper addresses critical challenges in 3D Human Pose Estimation (HPE) by\nanalyzing the robustness and sensitivity of existing models to occlusions,\ncamera position, and action variability. Using a novel synthetic dataset,\nBlendMimic3D, which includes diverse scenarios with multi-camera setups and\nseveral occlusion types, we conduct specific tests on several state-of-the-art\nmodels. Our study focuses on the discrepancy in keypoint formats between common\ndatasets such as Human3.6M, and 2D datasets such as COCO, commonly used for 2D\ndetection models and frequently input of 3D HPE models. Our work explores the\nimpact of occlusions on model performance and the generality of models trained\nexclusively under standard conditions. The findings suggest significant\nsensitivity to occlusions and camera settings, revealing a need for models that\nbetter adapt to real-world variability and occlusion scenarios. This research\ncontributed to ongoing efforts to improve the fidelity and applicability of 3D\nHPE systems in complex environments."}
{"id": "2504.10351", "pdf": "https://arxiv.org/pdf/2504.10351", "abs": "https://arxiv.org/abs/2504.10351", "authors": ["Kaiwen Zheng", "Xuri Ge", "Junchen Fu", "Jun Peng", "Joemon M. Jose"], "title": "Multimodal Representation Learning Techniques for Comprehensive Facial State Analysis", "categories": ["cs.CV"], "comment": "Accepted by ICME2025", "summary": "Multimodal foundation models have significantly improved feature\nrepresentation by integrating information from multiple modalities, making them\nhighly suitable for a broader set of applications. However, the exploration of\nmultimodal facial representation for understanding perception has been limited.\nUnderstanding and analyzing facial states, such as Action Units (AUs) and\nemotions, require a comprehensive and robust framework that bridges visual and\nlinguistic modalities. In this paper, we present a comprehensive pipeline for\nmultimodal facial state analysis. First, we compile a new Multimodal Face\nDataset (MFA) by generating detailed multilevel language descriptions of face,\nincorporating Action Unit (AU) and emotion descriptions, by leveraging GPT-4o.\nSecond, we introduce a novel Multilevel Multimodal Face Foundation model (MF^2)\ntailored for Action Unit (AU) and emotion recognition. Our model incorporates\ncomprehensive visual feature modeling at both local and global levels of face\nimage, enhancing its ability to represent detailed facial appearances. This\ndesign aligns visual representations with structured AU and emotion\ndescriptions, ensuring effective cross-modal integration. Third, we develop a\nDecoupled Fine-Tuning Network (DFN) that efficiently adapts MF^2 across various\ntasks and datasets. This approach not only reduces computational overhead but\nalso broadens the applicability of the foundation model to diverse scenarios.\nExperimentation show superior performance for AU and emotion detection tasks."}
{"id": "2504.10353", "pdf": "https://arxiv.org/pdf/2504.10353", "abs": "https://arxiv.org/abs/2504.10353", "authors": ["Jeremiah Giordani"], "title": "Patch and Shuffle: A Preprocessing Technique for Texture Classification in Autonomous Cementitious Fabrication", "categories": ["cs.CV"], "comment": "Originally completed as a final project for CEE 374 at Princeton\n  University", "summary": "Autonomous fabrication systems are transforming construction and\nmanufacturing, yet they remain vulnerable to print errors. Texture\nclassification is a key component of computer vision systems that enable\nreal-time monitoring and adjustment during cementitious fabrication.\nTraditional classification methods often rely on global image features, which\ncan bias the model toward semantic content rather than low-level textures. In\nthis paper, we introduce a novel preprocessing technique called \"patch and\nshuffle,\" which segments input images into smaller patches, shuffles them, and\nreconstructs a jumbled image before classification. This transformation removes\nsemantic context, forcing the classifier to rely on local texture features.\n  We evaluate this approach on a dataset of extruded cement images, using a\nResNet-18-based architecture. Our experiments compare the patch and shuffle\nmethod to a standard pipeline, holding all other factors constant. Results show\na significant improvement in accuracy: the patch and shuffle model achieved\n90.64% test accuracy versus 72.46% for the baseline. These findings suggest\nthat disrupting global structure enhances performance in texture-based\nclassification tasks.\n  This method has implications for broader vision tasks where low-level\nfeatures matter more than high-level semantics. The technique may improve\nclassification in applications ranging from fabrication monitoring to medical\nimaging."}
{"id": "2504.10358", "pdf": "https://arxiv.org/pdf/2504.10358", "abs": "https://arxiv.org/abs/2504.10358", "authors": ["Rui Chen", "Lei Sun", "Jing Tang", "Geng Li", "Xiangxiang Chu"], "title": "FingER: Content Aware Fine-grained Evaluation with Reasoning for AI-Generated Videos", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 4 figures", "summary": "Recent advances in video generation have posed great challenges in the\nassessment of AI-generated content, particularly with the emergence of\nincreasingly sophisticated models. The various inconsistencies and defects\nobserved in such videos are inherently complex, making overall scoring\nnotoriously difficult. In this paper, we emphasize the critical importance of\nintegrating fine-grained reasoning into video evaluation, and we propose\n$\\textbf{F}$ing$\\textbf{ER}$, a novel entity-level reasoning evaluation\nframework that first automatically generates $\\textbf{F}$ine-grained\n$\\textbf{E}$ntity-level questions, and then answers those questions by a\n$\\textbf{R}$easoning model with scores, which can be subsequently weighted\nsummed to an overall score for different applications. Specifically, we\nleverage LLMs to derive entity-level questions across five distinct\nperspectives, which (i) often focus on some specific entities of the content,\nthereby making answering or scoring much easier by MLLMs, and (ii) are more\ninterpretable. Then we construct a FingER dataset, consisting of approximately\n3.3k videos and corresponding 60k fine-grained QA annotations, each with\ndetailed reasons. Based on that, we further investigate various training\nprotocols to best incentivize the reasoning capability of MLLMs for correct\nanswer prediction. Extensive experiments demonstrate that a reasoning model\ntrained using Group Relative Policy Optimization (GRPO) with a cold-start\nstrategy achieves the best performance. Notably, our model surpasses existing\nmethods by a relative margin of $11.8\\%$ on GenAI-Bench and $5.5\\%$ on\nMonetBench with only 3.3k training videos, which is at most one-tenth of the\ntraining samples utilized by other methods. Our code and dataset will be\nreleased soon."}
{"id": "2504.10375", "pdf": "https://arxiv.org/pdf/2504.10375", "abs": "https://arxiv.org/abs/2504.10375", "authors": ["Maud Biquard", "Marie Chabert", "Florence Genin", "Christophe Latry", "Thomas Oberlin"], "title": "PG-DPIR: An efficient plug-and-play method for high-count Poisson-Gaussian inverse problems", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Poisson-Gaussian noise describes the noise of various imaging systems thus\nthe need of efficient algorithms for Poisson-Gaussian image restoration. Deep\nlearning methods offer state-of-the-art performance but often require\nsensor-specific training when used in a supervised setting. A promising\nalternative is given by plug-and-play (PnP) methods, which consist in learning\nonly a regularization through a denoiser, allowing to restore images from\nseveral sources with the same network. This paper introduces PG-DPIR, an\nefficient PnP method for high-count Poisson-Gaussian inverse problems, adapted\nfrom DPIR. While DPIR is designed for white Gaussian noise, a naive adaptation\nto Poisson-Gaussian noise leads to prohibitively slow algorithms due to the\nabsence of a closed-form proximal operator. To address this, we adapt DPIR for\nthe specificities of Poisson-Gaussian noise and propose in particular an\nefficient initialization of the gradient descent required for the proximal step\nthat accelerates convergence by several orders of magnitude. Experiments are\nconducted on satellite image restoration and super-resolution problems.\nHigh-resolution realistic Pleiades images are simulated for the experiments,\nwhich demonstrate that PG-DPIR achieves state-of-the-art performance with\nimproved efficiency, which seems promising for on-ground satellite processing\nchains."}
{"id": "2504.10395", "pdf": "https://arxiv.org/pdf/2504.10395", "abs": "https://arxiv.org/abs/2504.10395", "authors": ["Ragini Bal Mahesh", "Ronny Hänsch"], "title": "Better Coherence, Better Height: Fusing Physical Models and Deep Learning for Forest Height Estimation from Interferometric SAR Data", "categories": ["cs.CV"], "comment": null, "summary": "Estimating forest height from Synthetic Aperture Radar (SAR) images often\nrelies on traditional physical models, which, while interpretable and\ndata-efficient, can struggle with generalization. In contrast, Deep Learning\n(DL) approaches lack physical insight. To address this, we propose CoHNet - an\nend-to-end framework that combines the best of both worlds: DL optimized with\nphysics-informed constraints. We leverage a pre-trained neural surrogate model\nto enforce physical plausibility through a unique training loss. Our\nexperiments show that this approach not only improves forest height estimation\naccuracy but also produces meaningful features that enhance the reliability of\npredictions."}
{"id": "2504.10400", "pdf": "https://arxiv.org/pdf/2504.10400", "abs": "https://arxiv.org/abs/2504.10400", "authors": ["Pietro Bonazzi", "Christian Vogt", "Michael Jost", "Lyes Khacef", "Federico Paredes-Vallés", "Michele Magno"], "title": "Towards Low-Latency Event-based Obstacle Avoidance on a FPGA-Drone", "categories": ["cs.CV"], "comment": null, "summary": "This work quantitatively evaluates the performance of event-based vision\nsystems (EVS) against conventional RGB-based models for action prediction in\ncollision avoidance on an FPGA accelerator. Our experiments demonstrate that\nthe EVS model achieves a significantly higher effective frame rate (1 kHz) and\nlower temporal (-20 ms) and spatial prediction errors (-20 mm) compared to the\nRGB-based model, particularly when tested on out-of-distribution data. The EVS\nmodel also exhibits superior robustness in selecting optimal evasion maneuvers.\nIn particular, in distinguishing between movement and stationary states, it\nachieves a 59 percentage point advantage in precision (78% vs. 19%) and a\nsubstantially higher F1 score (0.73 vs. 0.06), highlighting the susceptibility\nof the RGB model to overfitting. Further analysis in different combinations of\nspatial classes confirms the consistent performance of the EVS model in both\ntest data sets. Finally, we evaluated the system end-to-end and achieved a\nlatency of approximately 2.14 ms, with event aggregation (1 ms) and inference\non the processing unit (0.94 ms) accounting for the largest components. These\nresults underscore the advantages of event-based vision for real-time collision\navoidance and demonstrate its potential for deployment in resource-constrained\nenvironments."}
{"id": "2504.10409", "pdf": "https://arxiv.org/pdf/2504.10409", "abs": "https://arxiv.org/abs/2504.10409", "authors": ["Mingchuan Ma", "Yuhao Zhou", "Jindi Lv", "Yuxin Tian", "Dan Si", "Shujian Li", "Qing Ye", "Jiancheng Lv"], "title": "GPS: Distilling Compact Memories via Grid-based Patch Sampling for Efficient Online Class-Incremental Learning", "categories": ["cs.CV"], "comment": "10 pages, 10 figures", "summary": "Online class-incremental learning aims to enable models to continuously adapt\nto new classes with limited access to past data, while mitigating catastrophic\nforgetting. Replay-based methods address this by maintaining a small memory\nbuffer of previous samples, achieving competitive performance. For effective\nreplay under constrained storage, recent approaches leverage distilled data to\nenhance the informativeness of memory. However, such approaches often involve\nsignificant computational overhead due to the use of bi-level optimization.\nMotivated by these limitations, we introduce Grid-based Patch Sampling (GPS), a\nlightweight and effective strategy for distilling informative memory samples\nwithout relying on a trainable model. GPS generates informative samples by\nsampling a subset of pixels from the original image, yielding compact\nlow-resolution representations that preserve both semantic content and\nstructural information. During replay, these representations are reassembled to\nsupport training and evaluation. Experiments on extensive benchmarks\ndemonstrate that GRS can be seamlessly integrated into existing replay\nframeworks, leading to 3%-4% improvements in average end accuracy under\nmemory-constrained settings, with limited computational overhead."}
{"id": "2504.10414", "pdf": "https://arxiv.org/pdf/2504.10414", "abs": "https://arxiv.org/abs/2504.10414", "authors": ["Jiaxin Lu", "Chun-Hao Paul Huang", "Uttaran Bhattacharya", "Qixing Huang", "Yi Zhou"], "title": "HUMOTO: A 4D Dataset of Mocap Human Object Interactions", "categories": ["cs.CV"], "comment": "19 pages, 15 figures", "summary": "We present Human Motions with Objects (HUMOTO), a high-fidelity dataset of\nhuman-object interactions for motion generation, computer vision, and robotics\napplications. Featuring 736 sequences (7,875 seconds at 30 fps), HUMOTO\ncaptures interactions with 63 precisely modeled objects and 72 articulated\nparts. Our innovations include a scene-driven LLM scripting pipeline creating\ncomplete, purposeful tasks with natural progression, and a mocap-and-camera\nrecording setup to effectively handle occlusions. Spanning diverse activities\nfrom cooking to outdoor picnics, HUMOTO preserves both physical accuracy and\nlogical task flow. Professional artists rigorously clean and verify each\nsequence, minimizing foot sliding and object penetrations. We also provide\nbenchmarks compared to other datasets. HUMOTO's comprehensive full-body motion\nand simultaneous multi-object interactions address key data-capturing\nchallenges and provide opportunities to advance realistic human-object\ninteraction modeling across research domains with practical applications in\nanimation, robotics, and embodied AI systems. Project:\nhttps://jiaxin-lu.github.io/humoto/ ."}
{"id": "2504.10433", "pdf": "https://arxiv.org/pdf/2504.10433", "abs": "https://arxiv.org/abs/2504.10433", "authors": ["Jian Liu", "Wei Sun", "Hui Yang", "Jin Zheng", "Zichen Geng", "Hossein Rahmani", "Ajmal Mian"], "title": "MonoDiff9D: Monocular Category-Level 9D Object Pose Estimation via Diffusion Model", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted by ICRA'25", "summary": "Object pose estimation is a core means for robots to understand and interact\nwith their environment. For this task, monocular category-level methods are\nattractive as they require only a single RGB camera. However, current methods\nrely on shape priors or CAD models of the intra-class known objects. We propose\na diffusion-based monocular category-level 9D object pose generation method,\nMonoDiff9D. Our motivation is to leverage the probabilistic nature of diffusion\nmodels to alleviate the need for shape priors, CAD models, or depth sensors for\nintra-class unknown object pose estimation. We first estimate coarse depth via\nDINOv2 from the monocular image in a zero-shot manner and convert it into a\npoint cloud. We then fuse the global features of the point cloud with the input\nimage and use the fused features along with the encoded time step to condition\nMonoDiff9D. Finally, we design a transformer-based denoiser to recover the\nobject pose from Gaussian noise. Extensive experiments on two popular benchmark\ndatasets show that MonoDiff9D achieves state-of-the-art monocular\ncategory-level 9D object pose estimation accuracy without the need for shape\npriors or CAD models at any stage. Our code will be made public at\nhttps://github.com/CNJianLiu/MonoDiff9D."}
{"id": "2504.10434", "pdf": "https://arxiv.org/pdf/2504.10434", "abs": "https://arxiv.org/abs/2504.10434", "authors": ["Taihang Hu", "Linxuan Li", "Kai Wang", "Yaxing Wang", "Jian Yang", "Ming-Ming Cheng"], "title": "Anchor Token Matching: Implicit Structure Locking for Training-free AR Image Editing", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image generation has seen groundbreaking advancements with diffusion\nmodels, enabling high-fidelity synthesis and precise image editing through\ncross-attention manipulation. Recently, autoregressive (AR) models have\nre-emerged as powerful alternatives, leveraging next-token generation to match\ndiffusion models. However, existing editing techniques designed for diffusion\nmodels fail to translate directly to AR models due to fundamental differences\nin structural control. Specifically, AR models suffer from spatial poverty of\nattention maps and sequential accumulation of structural errors during image\nediting, which disrupt object layouts and global consistency. In this work, we\nintroduce Implicit Structure Locking (ISLock), the first training-free editing\nstrategy for AR visual models. Rather than relying on explicit attention\nmanipulation or fine-tuning, ISLock preserves structural blueprints by\ndynamically aligning self-attention patterns with reference images through the\nAnchor Token Matching (ATM) protocol. By implicitly enforcing structural\nconsistency in latent space, our method ISLock enables structure-aware editing\nwhile maintaining generative autonomy. Extensive experiments demonstrate that\nISLock achieves high-quality, structure-consistent edits without additional\ntraining and is superior or comparable to conventional editing techniques. Our\nfindings pioneer the way for efficient and flexible AR-based image editing,\nfurther bridging the performance gap between diffusion and autoregressive\ngenerative models. The code will be publicly available at\nhttps://github.com/hutaiHang/ATM"}
{"id": "2504.10443", "pdf": "https://arxiv.org/pdf/2504.10443", "abs": "https://arxiv.org/abs/2504.10443", "authors": ["Haoran Hao", "Jiaming Han", "Yiyuan Zhang", "Xiangyu Yue"], "title": "Multimodal Long Video Modeling Based on Temporal Dynamic Context", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have led to significant\nbreakthroughs in video understanding. However, existing models still struggle\nwith long video processing due to the context length constraint of LLMs and the\nvast amount of information within the video. Although some recent methods are\ndesigned for long video understanding, they often lose crucial information\nduring token compression and struggle with additional modality like audio. In\nthis work, we propose a dynamic long video encoding method utilizing the\ntemporal relationship between frames, named Temporal Dynamic Context (TDC).\nFirstly, we segment the video into semantically consistent scenes based on\ninter-frame similarities, then encode each frame into tokens using visual-audio\nencoders. Secondly, we propose a novel temporal context compressor to reduce\nthe number of tokens within each segment. Specifically, we employ a query-based\nTransformer to aggregate video, audio, and instruction text tokens into a\nlimited set of temporal context tokens. Finally, we feed the static frame\ntokens and the temporal context tokens into the LLM for video understanding.\nFurthermore, to handle extremely long videos, we propose a training-free\nchain-of-thought strategy that progressively extracts answers from multiple\nvideo segments. These intermediate answers serve as part of the reasoning\nprocess and contribute to the final answer. We conduct extensive experiments on\ngeneral video understanding and audio-video understanding benchmarks, where our\nmethod demonstrates strong performance. The code and models are available at\nhttps://github.com/Hoar012/TDC-Video."}
{"id": "2504.10452", "pdf": "https://arxiv.org/pdf/2504.10452", "abs": "https://arxiv.org/abs/2504.10452", "authors": ["Ramin Mousa", "Hadis Taherinia", "Khabiba Abdiyeva", "Amir Ali Bengari", "Mohammadmahdi Vahediahmar"], "title": "Integrating Vision and Location with Transformers: A Multimodal Deep Learning Framework for Medical Wound Analysis", "categories": ["cs.CV"], "comment": null, "summary": "Effective recognition of acute and difficult-to-heal wounds is a necessary\nstep in wound diagnosis. An efficient classification model can help wound\nspecialists classify wound types with less financial and time costs and also\nhelp in deciding on the optimal treatment method. Traditional machine learning\nmodels suffer from feature selection and are usually cumbersome models for\naccurate recognition. Recently, deep learning (DL) has emerged as a powerful\ntool in wound diagnosis. Although DL seems promising for wound type\nrecognition, there is still a large scope for improving the efficiency and\naccuracy of the model. In this study, a DL-based multimodal classifier was\ndeveloped using wound images and their corresponding locations to classify them\ninto multiple classes, including diabetic, pressure, surgical, and venous\nulcers. A body map was also created to provide location data, which can help\nwound specialists label wound locations more effectively. The model uses a\nVision Transformer to extract hierarchical features from input images, a\nDiscrete Wavelet Transform (DWT) layer to capture low and high frequency\ncomponents, and a Transformer to extract spatial features. The number of\nneurons and weight vector optimization were performed using three swarm-based\noptimization techniques (Monster Gorilla Toner (MGTO), Improved Gray Wolf\nOptimization (IGWO), and Fox Optimization Algorithm). The evaluation results\nshow that weight vector optimization using optimization algorithms can increase\ndiagnostic accuracy and make it a very effective approach for wound detection.\nIn the classification using the original body map, the proposed model was able\nto achieve an accuracy of 0.8123 using image data and an accuracy of 0.8007\nusing a combination of image data and wound location. Also, the accuracy of the\nmodel in combination with the optimization models varied from 0.7801 to 0.8342."}
{"id": "2504.10458", "pdf": "https://arxiv.org/pdf/2504.10458", "abs": "https://arxiv.org/abs/2504.10458", "authors": ["Xiaobo Xia", "Run Luo"], "title": "GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents", "categories": ["cs.CV", "cs.CL", "cs.HC"], "comment": null, "summary": "Existing efforts in building Graphical User Interface (GUI) agents largely\nrely on the training paradigm of supervised fine-tuning on Large\nVision-Language Models (LVLMs). However, this approach not only demands\nextensive amounts of training data but also struggles to effectively understand\nGUI screenshots and generalize to unseen interfaces. The issue significantly\nlimits its application in real-world scenarios, especially for high-level\ntasks. Inspired by Reinforcement Fine-Tuning (RFT) in large reasoning models\n(e.g., DeepSeek-R1), which efficiently enhances the problem-solving\ncapabilities of large language models in real-world settings, we propose \\name,\nthe first reinforcement learning framework designed to enhance the GUI\ncapabilities of LVLMs in high-level real-world task scenarios, through unified\naction space rule modeling. By leveraging a small amount of carefully curated\nhigh-quality data across multiple platforms (including Windows, Linux, MacOS,\nAndroid, and Web) and employing policy optimization algorithms such as Group\nRelative Policy Optimization (GRPO) to update the model, \\name achieves\nsuperior performance using only 0.02\\% of the data (3K vs. 13M) compared to\nprevious state-of-the-art methods like OS-Atlas across eight benchmarks\nspanning three different platforms (mobile, desktop, and web). These results\ndemonstrate the immense potential of reinforcement learning based on unified\naction space rule modeling in improving the execution capabilities of LVLMs for\nreal-world GUI agent tasks."}
{"id": "2504.10462", "pdf": "https://arxiv.org/pdf/2504.10462", "abs": "https://arxiv.org/abs/2504.10462", "authors": ["Weixian Lei", "Jiacong Wang", "Haochen Wang", "Xiangtai Li", "Jun Hao Liew", "Jiashi Feng", "Zilong Huang"], "title": "The Scalability of Simplicity: Empirical Analysis of Vision-Language Learning with a Single Transformer", "categories": ["cs.CV"], "comment": null, "summary": "This paper introduces SAIL, a single transformer unified multimodal large\nlanguage model (MLLM) that integrates raw pixel encoding and language decoding\nwithin a singular architecture. Unlike existing modular MLLMs, which rely on a\npre-trained vision transformer (ViT), SAIL eliminates the need for a separate\nvision encoder, presenting a more minimalist architecture design. Instead of\nintroducing novel architectural components, SAIL adapts mix-attention\nmechanisms and multimodal positional encodings to better align with the\ndistinct characteristics of visual and textual modalities. We systematically\ncompare SAIL's properties-including scalability, cross-modal information flow\npatterns, and visual representation capabilities-with those of modular MLLMs.\nBy scaling both training data and model size, SAIL achieves performance\ncomparable to modular MLLMs. Notably, the removal of pretrained ViT components\nenhances SAIL's scalability and results in significantly different cross-modal\ninformation flow patterns. Moreover, SAIL demonstrates strong visual\nrepresentation capabilities, achieving results on par with ViT-22B in vision\ntasks such as semantic segmentation. Code and models are available at\nhttps://github.com/bytedance/SAIL."}
{"id": "2504.10465", "pdf": "https://arxiv.org/pdf/2504.10465", "abs": "https://arxiv.org/abs/2504.10465", "authors": ["Tao Zhang", "Xiangtai Li", "Zilong Huang", "Yanwei Li", "Weixian Lei", "Xueqing Deng", "Shihao Chen", "Shunping Ji", "Jiashi Feng"], "title": "Pixel-SAIL: Single Transformer For Pixel-Grounded Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) achieve remarkable performance for\nfine-grained pixel-level understanding tasks. However, all the works rely\nheavily on extra components, such as vision encoder (CLIP), segmentation\nexperts, leading to high system complexity and limiting model scaling. In this\nwork, our goal is to explore a highly simplified MLLM without introducing extra\ncomponents. Our work is motivated by the recent works on Single trAnsformer as\na unified vIsion-Language Model (SAIL) design, where these works jointly learn\nvision tokens and text tokens in transformers. We present Pixel-SAIL, a single\ntransformer for pixel-wise MLLM tasks. In particular, we present three\ntechnical improvements on the plain baseline. First, we design a learnable\nupsampling module to refine visual token features. Secondly, we propose a novel\nvisual prompt injection strategy to enable the single transformer to understand\nvisual prompt inputs and benefit from the early fusion of visual prompt\nembeddings and vision tokens. Thirdly, we introduce a vision expert\ndistillation strategy to efficiently enhance the single transformer's\nfine-grained feature extraction capability. In addition, we have collected a\ncomprehensive pixel understanding benchmark (PerBench), using a manual check.\nIt includes three tasks: detailed object description, visual prompt-based\nquestion answering, and visual-text referring segmentation. Extensive\nexperiments on four referring segmentation benchmarks, one visual prompt\nbenchmark, and our PerBench show that our Pixel-SAIL achieves comparable or\neven better results with a much simpler pipeline. Code and model will be\nreleased at https://github.com/magic-research/Sa2VA."}
{"id": "2504.10466", "pdf": "https://arxiv.org/pdf/2504.10466", "abs": "https://arxiv.org/abs/2504.10466", "authors": ["Xiaoyan Cong", "Jiayi Shen", "Zekun Li", "Rao Fu", "Tao Lu", "Srinath Sridhar"], "title": "Art3D: Training-Free 3D Generation from Flat-Colored Illustration", "categories": ["cs.CV"], "comment": "Technical Report. Course Project of Brown CSCI 1430 Computer Vision.\n  Project Page: https://joy-jy11.github.io/", "summary": "Large-scale pre-trained image-to-3D generative models have exhibited\nremarkable capabilities in diverse shape generations. However, most of them\nstruggle to synthesize plausible 3D assets when the reference image is\nflat-colored like hand drawings due to the lack of 3D illusion, which are often\nthe most user-friendly input modalities in art content creation. To this end,\nwe propose Art3D, a training-free method that can lift flat-colored 2D designs\ninto 3D. By leveraging structural and semantic features with pre- trained 2D\nimage generation models and a VLM-based realism evaluation, Art3D successfully\nenhances the three-dimensional illusion in reference images, thus simplifying\nthe process of generating 3D from 2D, and proves adaptable to a wide range of\npainting styles. To benchmark the generalization performance of existing\nimage-to-3D models on flat-colored images without 3D feeling, we collect a new\ndataset, Flat-2D, with over 100 samples. Experimental results demonstrate the\nperformance and robustness of Art3D, exhibiting superior generalizable capacity\nand promising practical applicability. Our source code and dataset will be\npublicly available on our project page: https://joy-jy11.github.io/ ."}
{"id": "2504.10471", "pdf": "https://arxiv.org/pdf/2504.10471", "abs": "https://arxiv.org/abs/2504.10471", "authors": ["Chenghao Xiao", "Isaac Chung", "Imene Kerboua", "Jamie Stirling", "Xin Zhang", "Márton Kardos", "Roman Solomatin", "Noura Al Moubayed", "Kenneth Enevoldsen", "Niklas Muennighoff"], "title": "MIEB: Massive Image Embedding Benchmark", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Image representations are often evaluated through disjointed, task-specific\nprotocols, leading to a fragmented understanding of model capabilities. For\ninstance, it is unclear whether an image embedding model adept at clustering\nimages is equally good at retrieving relevant images given a piece of text. We\nintroduce the Massive Image Embedding Benchmark (MIEB) to evaluate the\nperformance of image and image-text embedding models across the broadest\nspectrum to date. MIEB spans 38 languages across 130 individual tasks, which we\ngroup into 8 high-level categories. We benchmark 50 models across our\nbenchmark, finding that no single method dominates across all task categories.\nWe reveal hidden capabilities in advanced vision models such as their accurate\nvisual representation of texts, and their yet limited capabilities in\ninterleaved encodings and matching images and texts in the presence of\nconfounders. We also show that the performance of vision encoders on MIEB\ncorrelates highly with their performance when used in multimodal large language\nmodels. Our code, dataset, and leaderboard are publicly available at\nhttps://github.com/embeddings-benchmark/mteb."}
{"id": "2504.10479", "pdf": "https://arxiv.org/pdf/2504.10479", "abs": "https://arxiv.org/abs/2504.10479", "authors": ["Jinguo Zhu", "Weiyun Wang", "Zhe Chen", "Zhaoyang Liu", "Shenglong Ye", "Lixin Gu", "Yuchen Duan", "Hao Tian", "Weijie Su", "Jie Shao", "Zhangwei Gao", "Erfei Cui", "Yue Cao", "Yangzhou Liu", "Weiye Xu", "Hao Li", "Jiahao Wang", "Han Lv", "Dengnian Chen", "Songze Li", "Yinan He", "Tan Jiang", "Jiapeng Luo", "Yi Wang", "Conghui He", "Botian Shi", "Xingcheng Zhang", "Wenqi Shao", "Junjun He", "Yingtong Xiong", "Wenwen Qu", "Peng Sun", "Penglong Jiao", "Lijun Wu", "Kaipeng Zhang", "Huipeng Deng", "Jiaye Ge", "Kai Chen", "Limin Wang", "Min Dou", "Lewei Lu", "Xizhou Zhu", "Tong Lu", "Dahua Lin", "Yu Qiao", "Jifeng Dai", "Wenhai Wang"], "title": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "categories": ["cs.CV"], "comment": "Technical Report", "summary": "We introduce InternVL3, a significant advancement in the InternVL series\nfeaturing a native multimodal pre-training paradigm. Rather than adapting a\ntext-only large language model (LLM) into a multimodal large language model\n(MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and\nlinguistic capabilities from both diverse multimodal data and pure-text corpora\nduring a single pre-training stage. This unified training paradigm effectively\naddresses the complexities and alignment challenges commonly encountered in\nconventional post-hoc training pipelines for MLLMs. To further improve\nperformance and scalability, InternVL3 incorporates variable visual position\nencoding (V2PE) to support extended multimodal contexts, employs advanced\npost-training techniques such as supervised fine-tuning (SFT) and mixed\npreference optimization (MPO), and adopts test-time scaling strategies\nalongside an optimized training infrastructure. Extensive empirical evaluations\ndemonstrate that InternVL3 delivers superior performance across a wide range of\nmulti-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the\nMMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its\ncapabilities remain highly competitive with leading proprietary models,\nincluding ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also\nmaintaining strong pure-language proficiency. In pursuit of open-science\nprinciples, we will publicly release both the training data and model weights\nto foster further research and development in next-generation MLLMs."}
{"id": "2504.10483", "pdf": "https://arxiv.org/pdf/2504.10483", "abs": "https://arxiv.org/abs/2504.10483", "authors": ["Xingjian Leng", "Jaskirat Singh", "Yunzhong Hou", "Zhenchang Xing", "Saining Xie", "Liang Zheng"], "title": "REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion Transformers", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "In this paper we tackle a fundamental question: \"Can we train latent\ndiffusion models together with the variational auto-encoder (VAE) tokenizer in\nan end-to-end manner?\" Traditional deep-learning wisdom dictates that\nend-to-end training is often preferable when possible. However, for latent\ndiffusion transformers, it is observed that end-to-end training both VAE and\ndiffusion-model using standard diffusion-loss is ineffective, even causing a\ndegradation in final performance. We show that while diffusion loss is\nineffective, end-to-end training can be unlocked through the\nrepresentation-alignment (REPA) loss -- allowing both VAE and diffusion model\nto be jointly tuned during the training process. Despite its simplicity, the\nproposed training recipe (REPA-E) shows remarkable performance; speeding up\ndiffusion model training by over 17x and 45x over REPA and vanilla training\nrecipes, respectively. Interestingly, we observe that end-to-end tuning with\nREPA-E also improves the VAE itself; leading to improved latent space structure\nand downstream generation performance. In terms of final performance, our\napproach sets a new state-of-the-art; achieving FID of 1.26 and 1.83 with and\nwithout classifier-free guidance on ImageNet 256 x 256. Code is available at\nhttps://end2end-diffusion.github.io."}
{"id": "2504.10485", "pdf": "https://arxiv.org/pdf/2504.10485", "abs": "https://arxiv.org/abs/2504.10485", "authors": ["Yunsong Zhou", "Naisheng Ye", "William Ljungbergh", "Tianyu Li", "Jiazhi Yang", "Zetong Yang", "Hongzi Zhu", "Christoffer Petersson", "Hongyang Li"], "title": "Decoupled Diffusion Sparks Adaptive Scene Generation", "categories": ["cs.CV"], "comment": null, "summary": "Controllable scene generation could reduce the cost of diverse data\ncollection substantially for autonomous driving. Prior works formulate the\ntraffic layout generation as predictive progress, either by denoising entire\nsequences at once or by iteratively predicting the next frame. However, full\nsequence denoising hinders online reaction, while the latter's short-sighted\nnext-frame prediction lacks precise goal-state guidance. Further, the learned\nmodel struggles to generate complex or challenging scenarios due to a large\nnumber of safe and ordinal driving behaviors from open datasets. To overcome\nthese, we introduce Nexus, a decoupled scene generation framework that improves\nreactivity and goal conditioning by simulating both ordinal and challenging\nscenarios from fine-grained tokens with independent noise states. At the core\nof the decoupled pipeline is the integration of a partial noise-masking\ntraining strategy and a noise-aware schedule that ensures timely environmental\nupdates throughout the denoising process. To complement challenging scenario\ngeneration, we collect a dataset consisting of complex corner cases. It covers\n540 hours of simulated data, including high-risk interactions such as cut-in,\nsudden braking, and collision. Nexus achieves superior generation realism while\npreserving reactivity and goal orientation, with a 40% reduction in\ndisplacement error. We further demonstrate that Nexus improves closed-loop\nplanning by 20% through data augmentation and showcase its capability in\nsafety-critical data generation."}
{"id": "2504.10486", "pdf": "https://arxiv.org/pdf/2504.10486", "abs": "https://arxiv.org/abs/2504.10486", "authors": ["Zeren Jiang", "Shaofei Wang", "Siyu Tang"], "title": "DNF-Avatar: Distilling Neural Fields for Real-time Animatable Avatar Relighting", "categories": ["cs.CV"], "comment": "16 pages, 8 figures, Project pages:\n  https://jzr99.github.io/DNF-Avatar/", "summary": "Creating relightable and animatable human avatars from monocular videos is a\nrising research topic with a range of applications, e.g. virtual reality,\nsports, and video games. Previous works utilize neural fields together with\nphysically based rendering (PBR), to estimate geometry and disentangle\nappearance properties of human avatars. However, one drawback of these methods\nis the slow rendering speed due to the expensive Monte Carlo ray tracing. To\ntackle this problem, we proposed to distill the knowledge from implicit neural\nfields (teacher) to explicit 2D Gaussian splatting (student) representation to\ntake advantage of the fast rasterization property of Gaussian splatting. To\navoid ray-tracing, we employ the split-sum approximation for PBR appearance. We\nalso propose novel part-wise ambient occlusion probes for shadow computation.\nShadow prediction is achieved by querying these probes only once per pixel,\nwhich paves the way for real-time relighting of avatars. These techniques\ncombined give high-quality relighting results with realistic shadow effects.\nOur experiments demonstrate that the proposed student model achieves comparable\nor even better relighting results with our teacher model while being 370 times\nfaster at inference time, achieving a 67 FPS rendering speed."}
{"id": "2504.10487", "pdf": "https://arxiv.org/pdf/2504.10487", "abs": "https://arxiv.org/abs/2504.10487", "authors": ["Yasser Benigmim", "Mohammad Fahes", "Tuan-Hung Vu", "Andrei Bursuc", "Raoul de Charette"], "title": "FLOSS: Free Lunch in Open-vocabulary Semantic Segmentation", "categories": ["cs.CV", "cs.LG"], "comment": "Project Page: https://yasserben.github.io/FLOSS/", "summary": "Recent Open-Vocabulary Semantic Segmentation (OVSS) models extend the CLIP\nmodel to segmentation while maintaining the use of multiple templates (e.g., a\nphoto of <class>, a sketch of a <class>, etc.) for constructing class-wise\naveraged text embeddings, acting as a classifier. In this paper, we challenge\nthis status quo and investigate the impact of templates for OVSS. Empirically,\nwe observe that for each class, there exist single-template classifiers\nsignificantly outperforming the conventional averaged classifier. We refer to\nthem as class-experts. Given access to unlabeled images and without any\ntraining involved, we estimate these experts by leveraging the class-wise\nprediction entropy of single-template classifiers, selecting as class-wise\nexperts those which yield the lowest entropy. All experts, each specializing in\na specific class, collaborate in a newly proposed fusion method to generate\nmore accurate OVSS predictions. Our plug-and-play method, coined FLOSS, is\northogonal and complementary to existing OVSS methods, offering a ''free\nlunch'' to systematically improve OVSS without labels and additional training.\nExtensive experiments demonstrate that FLOSS consistently boosts\nstate-of-the-art methods on various OVSS benchmarks. Moreover, the selected\nexpert templates can generalize well from one dataset to others sharing the\nsame semantic categories, yet exhibiting distribution shifts. Additionally, we\nobtain satisfactory improvements under a low-data regime, where only a few\nunlabeled images are available. Our code is available at\nhttps://github.com/yasserben/FLOSS ."}
{"id": "2504.08823", "pdf": "https://arxiv.org/pdf/2504.08823", "abs": "https://arxiv.org/abs/2504.08823", "authors": ["Xiaobing Yu", "Jin Yang", "Xiao Wu", "Peijie Qiu", "Xiaofeng Liu"], "title": "FM-LoRA: Factorized Low-Rank Meta-Prompting for Continual Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "8 Pages, 4 figures", "summary": "How to adapt a pre-trained model continuously for sequential tasks with\ndifferent prediction class labels and domains and finally learn a generalizable\nmodel across diverse tasks is a long-lasting challenge. Continual learning (CL)\nhas emerged as a promising approach to leverage pre-trained models (e.g.,\nTransformers) for sequential tasks. While many existing CL methods\nincrementally store additional learned structures, such as Low-Rank Adaptation\n(LoRA) adapters or prompts and sometimes even preserve features from previous\nsamples to maintain performance. This leads to unsustainable parameter growth\nand escalating storage costs as the number of tasks increases. Moreover,\ncurrent approaches often lack task similarity awareness, which further hinders\nthe models ability to effectively adapt to new tasks without interfering with\npreviously acquired knowledge. To address these challenges, we propose FM-LoRA,\na novel and efficient low-rank adaptation method that integrates both a dynamic\nrank selector (DRS) and dynamic meta-prompting (DMP). This framework allocates\nmodel capacity more effectively across tasks by leveraging a shared low-rank\nsubspace critical for preserving knowledge, thereby avoiding continual\nparameter expansion. Extensive experiments on various CL benchmarks, including\nImageNet-R, CIFAR100, and CUB200 for class-incremental learning (CIL), and\nDomainNet for domain-incremental learning (DIL), with Transformers backbone\ndemonstrate that FM-LoRA effectively mitigates catastrophic forgetting while\ndelivering robust performance across a diverse range of tasks and domains."}
{"id": "2504.08824", "pdf": "https://arxiv.org/pdf/2504.08824", "abs": "https://arxiv.org/abs/2504.08824", "authors": ["Natalia Sikora", "Robert L. Manschke", "Alethea M. Tang", "Peter Dunstan", "Dean A. Harris", "Su Yang"], "title": "ColonScopeX: Leveraging Explainable Expert Systems with Multimodal Data for Improved Early Diagnosis of Colorectal Cancer", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.HC"], "comment": "Published to AAAI-25 Bridge Program", "summary": "Colorectal cancer (CRC) ranks as the second leading cause of cancer-related\ndeaths and the third most prevalent malignant tumour worldwide. Early detection\nof CRC remains problematic due to its non-specific and often embarrassing\nsymptoms, which patients frequently overlook or hesitate to report to\nclinicians. Crucially, the stage at which CRC is diagnosed significantly\nimpacts survivability, with a survival rate of 80-95\\% for Stage I and a stark\ndecline to 10\\% for Stage IV. Unfortunately, in the UK, only 14.4\\% of cases\nare diagnosed at the earliest stage (Stage I).\n  In this study, we propose ColonScopeX, a machine learning framework utilizing\nexplainable AI (XAI) methodologies to enhance the early detection of CRC and\npre-cancerous lesions. Our approach employs a multimodal model that integrates\nsignals from blood sample measurements, processed using the Savitzky-Golay\nalgorithm for fingerprint smoothing, alongside comprehensive patient metadata,\nincluding medication history, comorbidities, age, weight, and BMI. By\nleveraging XAI techniques, we aim to render the model's decision-making process\ntransparent and interpretable, thereby fostering greater trust and\nunderstanding in its predictions. The proposed framework could be utilised as a\ntriage tool or a screening tool of the general population.\n  This research highlights the potential of combining diverse patient data\nsources and explainable machine learning to tackle critical challenges in\nmedical diagnostics."}
{"id": "2504.08909", "pdf": "https://arxiv.org/pdf/2504.08909", "abs": "https://arxiv.org/abs/2504.08909", "authors": ["Islam Mansour", "Georg Fischer", "Ronny Haensch", "Irena Hajnsek"], "title": "Hybrid AI-Physical Modeling for Penetration Bias Correction in X-band InSAR DEMs: A Greenland Case Study", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": "8 pages", "summary": "Digital elevation models derived from Interferometric Synthetic Aperture\nRadar (InSAR) data over glacial and snow-covered regions often exhibit\nsystematic elevation errors, commonly termed \"penetration bias.\" We leverage\nexisting physics-based models and propose an integrated correction framework\nthat combines parametric physical modeling with machine learning. We evaluate\nthe approach across three distinct training scenarios - each defined by a\ndifferent set of acquisition parameters - to assess overall performance and the\nmodel's ability to generalize. Our experiments on Greenland's ice sheet using\nTanDEM-X data show that the proposed hybrid model corrections significantly\nreduce the mean and standard deviation of DEM errors compared to a purely\nphysical modeling baseline. The hybrid framework also achieves significantly\nimproved generalization than a pure ML approach when trained on data with\nlimited diversity in acquisition parameters."}
{"id": "2504.08937", "pdf": "https://arxiv.org/pdf/2504.08937", "abs": "https://arxiv.org/abs/2504.08937", "authors": ["Minjie Deng", "Yan Wei", "Hao Zhai", "An Wu", "Yuncan Ouyang", "Qianyao Peng"], "title": "Rethinking Few-Shot Fusion: Granular Ball Priors Enable General-Purpose Deep Image Fusion", "categories": ["cs.GR", "cs.CV", "cs.LG", "eess.IV", "stat.ML"], "comment": null, "summary": "In image fusion tasks, due to the lack of real fused images as priors, most\ndeep learning-based fusion methods obtain global weight features from original\nimages in large-scale data pairs to generate images that approximate real fused\nimages. However, unlike previous studies, this paper utilizes Granular Ball\nadaptation to extract features in the brightness space as priors for deep\nnetworks, enabling the fusion network to converge quickly and complete the\nfusion task. This leads to few-shot training for a general image fusion\nnetwork, and based on this, we propose the GBFF fusion method. According to the\ninformation expression division of pixel pairs in the original fused image, we\nclassify pixel pairs with significant performance as the positive domain and\nnon-significant pixel pairs as the boundary domain. We perform split inference\nin the brightness space using Granular Ball adaptation to compute weights for\npixels that express information to varying degrees, generating approximate\nsupervision images that provide priors for the neural network in the structural\nbrightness space. Additionally, the extracted global saliency features also\nadaptively provide priors for setting the loss function weights of each image\nin the network, guiding the network to converge quickly at both global and\npixel levels alongside the supervised images, thereby enhancing the\nexpressiveness of the fused images. Each modality only used 10 pairs of images\nas the training set, completing the fusion task with a limited number of\niterations. Experiments validate the effectiveness of the algorithm and theory,\nand qualitative and quantitative comparisons with SOTA methods show that this\napproach is highly competitive in terms of fusion time and image\nexpressiveness."}
{"id": "2504.08974", "pdf": "https://arxiv.org/pdf/2504.08974", "abs": "https://arxiv.org/abs/2504.08974", "authors": ["Pouya Pezeshkpour", "Moin Aminnaseri", "Estevam Hruschka"], "title": "Mixed Signals: Decoding VLMs' Reasoning and Underlying Bias in Vision-Language Conflict", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) have demonstrated impressive performance by\neffectively integrating visual and textual information to solve complex tasks.\nHowever, it is not clear how these models reason over the visual and textual\ndata together, nor how the flow of information between modalities is\nstructured. In this paper, we examine how VLMs reason by analyzing their biases\nwhen confronted with scenarios that present conflicting image and text cues, a\ncommon occurrence in real-world applications. To uncover the extent and nature\nof these biases, we build upon existing benchmarks to create five datasets\ncontaining mismatched image-text pairs, covering topics in mathematics,\nscience, and visual descriptions. Our analysis shows that VLMs favor text in\nsimpler queries but shift toward images as query complexity increases. This\nbias correlates with model scale, with the difference between the percentage of\nimage- and text-preferred responses ranging from +56.8% (image favored) to\n-74.4% (text favored), depending on the task and model. In addition, we explore\nthree mitigation strategies: simple prompt modifications, modifications that\nexplicitly instruct models on how to handle conflicting information (akin to\nchain-of-thought prompting), and a task decomposition strategy that analyzes\neach modality separately before combining their results. Our findings indicate\nthat the effectiveness of these strategies in identifying and mitigating bias\nvaries significantly and is closely linked to the model's overall performance\non the task and the specific modality in question."}
{"id": "2504.09088", "pdf": "https://arxiv.org/pdf/2504.09088", "abs": "https://arxiv.org/abs/2504.09088", "authors": ["Yonghao Huang", "Leiting Chen", "Chuan Zhou"], "title": "Multi-Modal Brain Tumor Segmentation via 3D Multi-Scale Self-attention and Cross-attention", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Due to the success of CNN-based and Transformer-based models in various\ncomputer vision tasks, recent works study the applicability of CNN-Transformer\nhybrid architecture models in 3D multi-modality medical segmentation tasks.\nIntroducing Transformer brings long-range dependent information modeling\nability in 3D medical images to hybrid models via the self-attention mechanism.\nHowever, these models usually employ fixed receptive fields of 3D volumetric\nfeatures within each self-attention layer, ignoring the multi-scale volumetric\nlesion features. To address this issue, we propose a CNN-Transformer hybrid 3D\nmedical image segmentation model, named TMA-TransBTS, based on an\nencoder-decoder structure. TMA-TransBTS realizes simultaneous extraction of\nmulti-scale 3D features and modeling of long-distance dependencies by\nmulti-scale division and aggregation of 3D tokens in a self-attention layer.\nFurthermore, TMA-TransBTS proposes a 3D multi-scale cross-attention module to\nestablish a link between the encoder and the decoder for extracting rich volume\nrepresentations by exploiting the mutual attention mechanism of cross-attention\nand multi-scale aggregation of 3D tokens. Extensive experimental results on\nthree public 3D medical segmentation datasets show that TMA-TransBTS achieves\nhigher averaged segmentation results than previous state-of-the-art CNN-based\n3D methods and CNN-Transform hybrid 3D methods for the segmentation of 3D\nmulti-modality brain tumors."}
{"id": "2504.09182", "pdf": "https://arxiv.org/pdf/2504.09182", "abs": "https://arxiv.org/abs/2504.09182", "authors": ["Zeyu Yang", "Zhilin Chen", "Yipeng Sun", "Anika Strittmatter", "Anish Raj", "Ahmad Allababidi", "Johann S. Rink", "Frank G. Zöllner"], "title": "seg2med: a segmentation-based medical image generation framework using denoising diffusion probabilistic models", "categories": ["eess.IV", "cs.CV"], "comment": "17 pages, 10 figures", "summary": "In this study, we present seg2med, an advanced medical image synthesis\nframework that uses Denoising Diffusion Probabilistic Models (DDPM) to generate\nhigh-quality synthetic medical images conditioned on anatomical masks from\nTotalSegmentator. The framework synthesizes CT and MR images from segmentation\nmasks derived from real patient data and XCAT digital phantoms, achieving a\nStructural Similarity Index Measure (SSIM) of 0.94 +/- 0.02 for CT and 0.89 +/-\n0.04 for MR images compared to ground-truth images of real patients. It also\nachieves a Feature Similarity Index Measure (FSIM) of 0.78 +/- 0.04 for CT\nimages from XCAT. The generative quality is further supported by a Fr\\'echet\nInception Distance (FID) of 3.62 for CT image generation.\n  Additionally, seg2med can generate paired CT and MR images with consistent\nanatomical structures and convert images between CT and MR modalities,\nachieving SSIM values of 0.91 +/- 0.03 for MR-to-CT and 0.77 +/- 0.04 for\nCT-to-MR conversion. Despite the limitations of incomplete anatomical details\nin segmentation masks, the framework shows strong performance in cross-modality\nsynthesis and multimodal imaging.\n  seg2med also demonstrates high anatomical fidelity in CT synthesis, achieving\na mean Dice coefficient greater than 0.90 for 11 abdominal organs and greater\nthan 0.80 for 34 organs out of 59 in 58 test cases. The highest Dice of 0.96\n+/- 0.01 was recorded for the right scapula. Leveraging the TotalSegmentator\ntoolkit, seg2med enables segmentation mask generation across diverse datasets,\nsupporting applications in clinical imaging, data augmentation, multimodal\nsynthesis, and diagnostic algorithm development."}
{"id": "2504.09209", "pdf": "https://arxiv.org/pdf/2504.09209", "abs": "https://arxiv.org/abs/2504.09209", "authors": ["Xiangyue Zhang", "Jianfang Li", "Jiaxu Zhang", "Jianqiang Ren", "Liefeng Bo", "Zhigang Tu"], "title": "EchoMask: Speech-Queried Attention-based Mask Modeling for Holistic Co-Speech Motion Generation", "categories": ["cs.GR", "cs.CV", "cs.SD"], "comment": "12 pages, 12 figures", "summary": "Masked modeling framework has shown promise in co-speech motion generation.\nHowever, it struggles to identify semantically significant frames for effective\nmotion masking. In this work, we propose a speech-queried attention-based mask\nmodeling framework for co-speech motion generation. Our key insight is to\nleverage motion-aligned speech features to guide the masked motion modeling\nprocess, selectively masking rhythm-related and semantically expressive motion\nframes. Specifically, we first propose a motion-audio alignment module (MAM) to\nconstruct a latent motion-audio joint space. In this space, both low-level and\nhigh-level speech features are projected, enabling motion-aligned speech\nrepresentation using learnable speech queries. Then, a speech-queried attention\nmechanism (SQA) is introduced to compute frame-level attention scores through\ninteractions between motion keys and speech queries, guiding selective masking\ntoward motion frames with high attention scores. Finally, the motion-aligned\nspeech features are also injected into the generation network to facilitate\nco-speech motion generation. Qualitative and quantitative evaluations confirm\nthat our method outperforms existing state-of-the-art approaches, successfully\nproducing high-quality co-speech motion."}
{"id": "2504.09265", "pdf": "https://arxiv.org/pdf/2504.09265", "abs": "https://arxiv.org/abs/2504.09265", "authors": ["Lei Kang", "Jia Li", "Mi Tian", "Hua Huang"], "title": "Mixture of Group Experts for Learning Invariant Representations", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Sparsely activated Mixture-of-Experts (MoE) models effectively increase the\nnumber of parameters while maintaining consistent computational costs per\ntoken. However, vanilla MoE models often suffer from limited diversity and\nspecialization among experts, constraining their performance and scalability,\nespecially as the number of experts increases. In this paper, we present a\nnovel perspective on vanilla MoE with top-$k$ routing inspired by sparse\nrepresentation. This allows us to bridge established theoretical insights from\nsparse representation into MoE models. Building on this foundation, we propose\na group sparse regularization approach for the input of top-$k$ routing, termed\nMixture of Group Experts (MoGE). MoGE indirectly regularizes experts by\nimposing structural constraints on the routing inputs, while preserving the\noriginal MoE architecture. Furthermore, we organize the routing input into a 2D\ntopographic map, spatially grouping neighboring elements. This structure\nenables MoGE to capture representations invariant to minor transformations,\nthereby significantly enhancing expert diversity and specialization.\nComprehensive evaluations across various Transformer models for image\nclassification and language modeling tasks demonstrate that MoGE substantially\noutperforms its MoE counterpart, with minimal additional memory and computation\noverhead. Our approach provides a simple yet effective solution to scale the\nnumber of experts and reduce redundancy among them. The source code is included\nin the supplementary material and will be publicly released."}
{"id": "2504.09341", "pdf": "https://arxiv.org/pdf/2504.09341", "abs": "https://arxiv.org/abs/2504.09341", "authors": ["Hsuan Wei Liao", "Christopher Klugmann", "Daniel Kondermann", "Rafid Mahmood"], "title": "Minority Reports: Balancing Cost and Quality in Ground Truth Data Annotation", "categories": ["cs.LG", "cs.CV", "cs.HC"], "comment": "39 pages", "summary": "High-quality data annotation is an essential but laborious and costly aspect\nof developing machine learning-based software. We explore the inherent tradeoff\nbetween annotation accuracy and cost by detecting and removing minority reports\n-- instances where annotators provide incorrect responses -- that indicate\nunnecessary redundancy in task assignments. We propose an approach to prune\npotentially redundant annotation task assignments before they are executed by\nestimating the likelihood of an annotator disagreeing with the majority vote\nfor a given task. Our approach is informed by an empirical analysis over\ncomputer vision datasets annotated by a professional data annotation platform,\nwhich reveals that the likelihood of a minority report event is dependent\nprimarily on image ambiguity, worker variability, and worker fatigue.\nSimulations over these datasets show that we can reduce the number of\nannotations required by over 60% with a small compromise in label quality,\nsaving approximately 6.6 days-equivalent of labor. Our approach provides\nannotation service platforms with a method to balance cost and dataset quality.\nMachine learning practitioners can tailor annotation accuracy levels according\nto specific application needs, thereby optimizing budget allocation while\nmaintaining the data quality necessary for critical settings like autonomous\ndriving technology."}
{"id": "2504.09352", "pdf": "https://arxiv.org/pdf/2504.09352", "abs": "https://arxiv.org/abs/2504.09352", "authors": ["Iason Chaimalas", "Arnas Vyšniauskas", "Gabriel Brostow"], "title": "Explorer: Robust Collection of Interactable GUI Elements", "categories": ["cs.HC", "cs.AI", "cs.CV"], "comment": "19 pages, 17 figures", "summary": "Automation of existing Graphical User Interfaces (GUIs) is important but hard\nto achieve. Upstream of making the GUI user-accessible or somehow scriptable,\neven the data-collection to understand the original interface poses significant\nchallenges. For example, large quantities of general UI data seem helpful for\ntraining general machine learning (ML) models, but accessibility for each\nperson can hinge on the ML's precision on a specific app. We therefore take the\nperspective that a given user needs confidence, that the relevant UI elements\nare being detected correctly throughout one app or digital environment. We\nmostly assume that the target application is known in advance, so that data\ncollection and ML-training can be personalized for the test-time target domain.\nThe proposed Explorer system focuses on detecting on-screen buttons and\ntext-entry fields, i.e. interactables, where the training process has access to\na live version of the application. The live application can run on almost any\npopular platform except iOS phones, and the collection is especially\nstreamlined for Android phones or for desktop Chrome browsers. Explorer also\nenables the recording of interactive user sessions, and subsequent mapping of\nhow such sessions overlap and sometimes loop back to similar states. We show\nhow having such a map enables a kind of path planning through the GUI, letting\na user issue audio commands to get to their destination. Critically, we are\nreleasing our code for Explorer openly at https://github.com/varnelis/Explorer."}
{"id": "2504.09430", "pdf": "https://arxiv.org/pdf/2504.09430", "abs": "https://arxiv.org/abs/2504.09430", "authors": ["Ruiwen Ding", "Lin Li", "Rajath Soans", "Tosha Shah", "Radha Krishnan", "Marc Alexander Sze", "Sasha Lukyanov", "Yash Deshpande", "Antong Chen"], "title": "Predicting ulcer in H&E images of inflammatory bowel disease using domain-knowledge-driven graph neural network", "categories": ["eess.IV", "cs.CV"], "comment": "Work accepted at ISBI 2025", "summary": "Inflammatory bowel disease (IBD) involves chronic inflammation of the\ndigestive tract, with treatment options often burdened by adverse effects.\nIdentifying biomarkers for personalized treatment is crucial. While immune\ncells play a key role in IBD, accurately identifying ulcer regions in whole\nslide images (WSIs) is essential for characterizing these cells and exploring\npotential therapeutics. Multiple instance learning (MIL) approaches have\nadvanced WSI analysis but they lack spatial context awareness. In this work, we\npropose a weakly-supervised model called DomainGCN that employs a graph\nconvolution neural network (GCN) and incorporates domain-specific knowledge of\nulcer features, specifically, the presence of epithelium, lymphocytes, and\ndebris for WSI-level ulcer prediction in IBD. We demonstrate that DomainGCN\noutperforms various state-of-the-art (SOTA) MIL methods and show the added\nvalue of domain knowledge."}
{"id": "2504.09456", "pdf": "https://arxiv.org/pdf/2504.09456", "abs": "https://arxiv.org/abs/2504.09456", "authors": ["Pengkun Jiao", "Bin Zhu", "Jingjing Chen", "Chong-Wah Ngo", "Yu-Gang Jiang"], "title": "Don't Deceive Me: Mitigating Gaslighting through Attention Reallocation in LMMs", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Large Multimodal Models (LMMs) have demonstrated remarkable capabilities\nacross a wide range of tasks. However, their vulnerability to user\ngaslighting-the deliberate use of misleading or contradictory inputs-raises\ncritical concerns about their reliability in real-world applications. In this\npaper, we address the novel and challenging issue of mitigating the negative\nimpact of negation-based gaslighting on LMMs, where deceptive user statements\nlead to significant drops in model accuracy. Specifically, we introduce\nGasEraser, a training-free approach that reallocates attention weights from\nmisleading textual tokens to semantically salient visual regions. By\nsuppressing the influence of \"attention sink\" tokens and enhancing focus on\nvisually grounded cues, GasEraser significantly improves LMM robustness without\nrequiring retraining or additional supervision. Extensive experimental results\ndemonstrate that GasEraser is effective across several leading open-source LMMs\non the GaslightingBench. Notably, for LLaVA-v1.5-7B, GasEraser reduces the\nmisguidance rate by 48.2%, demonstrating its potential for more trustworthy\nLMMs."}
{"id": "2504.09516", "pdf": "https://arxiv.org/pdf/2504.09516", "abs": "https://arxiv.org/abs/2504.09516", "authors": ["Yasar Abbas Ur Rehman", "Kin Wai Lau", "Yuyang Xie", "Ma Lan", "JiaJun Shen"], "title": "FSSUAVL: A Discriminative Framework using Vision Models for Federated Self-Supervised Audio and Image Understanding", "categories": ["cs.SD", "cs.CV", "eess.AS"], "comment": "8 pages", "summary": "Recent studies have demonstrated that vision models can effectively learn\nmultimodal audio-image representations when paired. However, the challenge of\nenabling deep models to learn representations from unpaired modalities remains\nunresolved. This issue is especially pertinent in scenarios like Federated\nLearning (FL), where data is often decentralized, heterogeneous, and lacks a\nreliable guarantee of paired data. Previous attempts tackled this issue through\nthe use of auxiliary pretrained encoders or generative models on local clients,\nwhich invariably raise computational cost with increasing number modalities.\nUnlike these approaches, in this paper, we aim to address the task of unpaired\naudio and image recognition using \\texttt{FSSUAVL}, a single deep model\npretrained in FL with self-supervised contrastive learning (SSL). Instead of\naligning the audio and image modalities, \\texttt{FSSUAVL} jointly discriminates\nthem by projecting them into a common embedding space using contrastive SSL.\nThis extends the utility of \\texttt{FSSUAVL} to paired and unpaired audio and\nimage recognition tasks. Our experiments with CNN and ViT demonstrate that\n\\texttt{FSSUAVL} significantly improves performance across various image- and\naudio-based downstream tasks compared to using separate deep models for each\nmodality. Additionally, \\texttt{FSSUAVL}'s capacity to learn multimodal feature\nrepresentations allows for integrating auxiliary information, if available, to\nenhance recognition accuracy."}
{"id": "2504.09544", "pdf": "https://arxiv.org/pdf/2504.09544", "abs": "https://arxiv.org/abs/2504.09544", "authors": ["Yemin Yu", "Neil Tenenholtz", "Lester Mackey", "Ying Wei", "David Alvarez-Melis", "Ava P. Amini", "Alex X. Lu"], "title": "Causal integration of chemical structures improves representations of microscopy images for morphological profiling", "categories": ["cs.LG", "cs.CE", "cs.CV"], "comment": "24 pages", "summary": "Recent advances in self-supervised deep learning have improved our ability to\nquantify cellular morphological changes in high-throughput microscopy screens,\na process known as morphological profiling. However, most current methods only\nlearn from images, despite many screens being inherently multimodal, as they\ninvolve both a chemical or genetic perturbation as well as an image-based\nreadout. We hypothesized that incorporating chemical compound structure during\nself-supervised pre-training could improve learned representations of images in\nhigh-throughput microscopy screens. We introduce a representation learning\nframework, MICON (Molecular-Image Contrastive Learning), that models chemical\ncompounds as treatments that induce counterfactual transformations of cell\nphenotypes. MICON significantly outperforms classical hand-crafted features\nsuch as CellProfiler and existing deep-learning-based representation learning\nmethods in challenging evaluation settings where models must identify\nreproducible effects of drugs across independent replicates and data-generating\ncenters. We demonstrate that incorporating chemical compound information into\nthe learning process provides consistent improvements in our evaluation setting\nand that modeling compounds specifically as treatments in a causal framework\noutperforms approaches that directly align images and compounds in a single\nrepresentation space. Our findings point to a new direction for representation\nlearning in morphological profiling, suggesting that methods should explicitly\naccount for the multimodal nature of microscopy screening data."}
{"id": "2504.09620", "pdf": "https://arxiv.org/pdf/2504.09620", "abs": "https://arxiv.org/abs/2504.09620", "authors": ["Yuta Matsui", "Ryosuke Yamaki", "Ryo Ueda", "Seitaro Shinagawa", "Tadahiro Taniguchi"], "title": "Metropolis-Hastings Captioning Game: Knowledge Fusion of Vision Language Models via Decentralized Bayesian Inference", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.MA"], "comment": null, "summary": "We propose the Metropolis-Hastings Captioning Game (MHCG), a method to fuse\nknowledge of multiple vision-language models (VLMs) by learning from each\nother. Although existing methods that combine multiple models suffer from\ninference costs and architectural constraints, MHCG avoids these problems by\nperforming decentralized Bayesian inference through a process resembling a\nlanguage game. The knowledge fusion process establishes communication between\ntwo VLM agents alternately captioning images and learning from each other. We\nconduct two image-captioning experiments with two VLMs, each pre-trained on a\ndifferent dataset. The first experiment demonstrates that MHCG achieves\nconsistent improvement in reference-free evaluation metrics. The second\nexperiment investigates how MHCG contributes to sharing VLMs' category-level\nvocabulary by observing the occurrence of the vocabulary in the generated\ncaptions."}
{"id": "2504.09648", "pdf": "https://arxiv.org/pdf/2504.09648", "abs": "https://arxiv.org/abs/2504.09648", "authors": ["Guixian Chen", "Jianhao Ma", "Salar Fattahi"], "title": "RANSAC Revisited: An Improved Algorithm for Robust Subspace Recovery under Adversarial and Noisy Corruptions", "categories": ["cs.LG", "cs.CV", "stat.CO"], "comment": null, "summary": "In this paper, we study the problem of robust subspace recovery (RSR) in the\npresence of both strong adversarial corruptions and Gaussian noise.\nSpecifically, given a limited number of noisy samples -- some of which are\ntampered by an adaptive and strong adversary -- we aim to recover a\nlow-dimensional subspace that approximately contains a significant fraction of\nthe uncorrupted samples, up to an error that scales with the Gaussian noise.\nExisting approaches to this problem often suffer from high computational costs\nor rely on restrictive distributional assumptions, limiting their applicability\nin truly adversarial settings. To address these challenges, we revisit the\nclassical random sample consensus (RANSAC) algorithm, which offers strong\nrobustness to adversarial outliers, but sacrifices efficiency and robustness\nagainst Gaussian noise and model misspecification in the process. We propose a\ntwo-stage algorithm, RANSAC+, that precisely pinpoints and remedies the failure\nmodes of standard RANSAC. Our method is provably robust to both Gaussian and\nadversarial corruptions, achieves near-optimal sample complexity without\nrequiring prior knowledge of the subspace dimension, and is more efficient than\nexisting RANSAC-type methods."}
{"id": "2504.09655", "pdf": "https://arxiv.org/pdf/2504.09655", "abs": "https://arxiv.org/abs/2504.09655", "authors": ["Justin Namuk Kim", "Yiqiao Liu", "Rajath Soans", "Keith Persson", "Sarah Halek", "Michal Tomaszewski", "Jianda Yuan", "Gregory Goldmacher", "Antong Chen"], "title": "OmniMamba4D: Spatio-temporal Mamba for longitudinal CT lesion segmentation", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted at IEEE International Symposium on Biomedical Imaging (ISBI)\n  2025", "summary": "Accurate segmentation of longitudinal CT scans is important for monitoring\ntumor progression and evaluating treatment responses. However, existing 3D\nsegmentation models solely focus on spatial information. To address this gap,\nwe propose OmniMamba4D, a novel segmentation model designed for 4D medical\nimages (3D images over time). OmniMamba4D utilizes a spatio-temporal\ntetra-orientated Mamba block to effectively capture both spatial and temporal\nfeatures. Unlike traditional 3D models, which analyze single-time points,\nOmniMamba4D processes 4D CT data, providing comprehensive spatio-temporal\ninformation on lesion progression. Evaluated on an internal dataset comprising\nof 3,252 CT scans, OmniMamba4D achieves a competitive Dice score of 0.682,\ncomparable to state-of-the-arts (SOTA) models, while maintaining computational\nefficiency and better detecting disappeared lesions. This work demonstrates a\nnew framework to leverage spatio-temporal information for longitudinal CT\nlesion segmentation."}
{"id": "2504.09712", "pdf": "https://arxiv.org/pdf/2504.09712", "abs": "https://arxiv.org/abs/2504.09712", "authors": ["Julius Broomfield", "Tom Gibbs", "Ethan Kosak-Hine", "George Ingebretsen", "Tia Nasir", "Jason Zhang", "Reihaneh Iranmanesh", "Sara Pieri", "Reihaneh Rabbany", "Kellin Pelrine"], "title": "The Structural Safety Generalization Problem", "categories": ["cs.CR", "cs.AI", "cs.CV"], "comment": null, "summary": "LLM jailbreaks are a widespread safety challenge. Given this problem has not\nyet been tractable, we suggest targeting a key failure mechanism: the failure\nof safety to generalize across semantically equivalent inputs. We further focus\nthe target by requiring desirable tractability properties of attacks to study:\nexplainability, transferability between models, and transferability between\ngoals. We perform red-teaming within this framework by uncovering new\nvulnerabilities to multi-turn, multi-image, and translation-based attacks.\nThese attacks are semantically equivalent by our design to their single-turn,\nsingle-image, or untranslated counterparts, enabling systematic comparisons; we\nshow that the different structures yield different safety outcomes. We then\ndemonstrate the potential for this framework to enable new defenses by\nproposing a Structure Rewriting Guardrail, which converts an input to a\nstructure more conducive to safety assessment. This guardrail significantly\nimproves refusal of harmful inputs, without over-refusing benign ones. Thus, by\nframing this intermediate challenge - more tractable than universal defenses\nbut essential for long-term safety - we highlight a critical milestone for AI\nsafety research."}
{"id": "2504.09795", "pdf": "https://arxiv.org/pdf/2504.09795", "abs": "https://arxiv.org/abs/2504.09795", "authors": ["Ryota Tanaka", "Taichi Iki", "Taku Hasegawa", "Kyosuke Nishida", "Kuniko Saito", "Jun Suzuki"], "title": "VDocRAG: Retrieval-Augmented Generation over Visually-Rich Documents", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR"], "comment": "Accepted by CVPR 2025; project page: https://vdocrag.github.io", "summary": "We aim to develop a retrieval-augmented generation (RAG) framework that\nanswers questions over a corpus of visually-rich documents presented in mixed\nmodalities (e.g., charts, tables) and diverse formats (e.g., PDF, PPTX). In\nthis paper, we introduce a new RAG framework, VDocRAG, which can directly\nunderstand varied documents and modalities in a unified image format to prevent\nmissing information that occurs by parsing documents to obtain text. To improve\nthe performance, we propose novel self-supervised pre-training tasks that adapt\nlarge vision-language models for retrieval by compressing visual information\ninto dense token representations while aligning them with textual content in\ndocuments. Furthermore, we introduce OpenDocVQA, the first unified collection\nof open-domain document visual question answering datasets, encompassing\ndiverse document types and formats. OpenDocVQA provides a comprehensive\nresource for training and evaluating retrieval and question answering models on\nvisually-rich documents in an open-domain setting. Experiments show that\nVDocRAG substantially outperforms conventional text-based RAG and has strong\ngeneralization capability, highlighting the potential of an effective RAG\nparadigm for real-world documents."}
{"id": "2504.09885", "pdf": "https://arxiv.org/pdf/2504.09885", "abs": "https://arxiv.org/abs/2504.09885", "authors": ["Zihao Liu", "Mingwen Ou", "Zunnan Xu", "Jiaqi Huang", "Haonan Han", "Ronghui Li", "Xiu Li"], "title": "Separate to Collaborate: Dual-Stream Diffusion Model for Coordinated Piano Hand Motion Synthesis", "categories": ["cs.SD", "cs.CV", "eess.AS"], "comment": "12 pages, 4 figures", "summary": "Automating the synthesis of coordinated bimanual piano performances poses\nsignificant challenges, particularly in capturing the intricate choreography\nbetween the hands while preserving their distinct kinematic signatures. In this\npaper, we propose a dual-stream neural framework designed to generate\nsynchronized hand gestures for piano playing from audio input, addressing the\ncritical challenge of modeling both hand independence and coordination. Our\nframework introduces two key innovations: (i) a decoupled diffusion-based\ngeneration framework that independently models each hand's motion via\ndual-noise initialization, sampling distinct latent noise for each while\nleveraging a shared positional condition, and (ii) a Hand-Coordinated\nAsymmetric Attention (HCAA) mechanism suppresses symmetric (common-mode) noise\nto highlight asymmetric hand-specific features, while adaptively enhancing\ninter-hand coordination during denoising. The system operates hierarchically:\nit first predicts 3D hand positions from audio features and then generates\njoint angles through position-aware diffusion models, where parallel denoising\nstreams interact via HCAA. Comprehensive evaluations demonstrate that our\nframework outperforms existing state-of-the-art methods across multiple\nmetrics."}
{"id": "2504.09949", "pdf": "https://arxiv.org/pdf/2504.09949", "abs": "https://arxiv.org/abs/2504.09949", "authors": ["Heming Xu", "Xiaohui Liu", "Zhilu Zhang", "Hongzhi Zhang", "Xiaohe Wu", "Wangmeng Zuo"], "title": "Pseudo-Label Guided Real-World Image De-weathering: A Learning Framework with Imperfect Supervision", "categories": ["cs.GR", "cs.CV"], "comment": "15 pages, 16 figures", "summary": "Real-world image de-weathering aims at removingvarious undesirable\nweather-related artifacts, e.g., rain, snow,and fog. To this end, acquiring\nideal training pairs is crucial.Existing real-world datasets are typically\nconstructed paired databy extracting clean and degraded images from live\nstreamsof landscape scene on the Internet. Despite the use of strictfiltering\nmechanisms during collection, training pairs inevitablyencounter inconsistency\nin terms of lighting, object position, scenedetails, etc, making de-weathering\nmodels possibly suffer fromdeformation artifacts under non-ideal supervision.\nIn this work,we propose a unified solution for real-world image\nde-weatheringwith non-ideal supervision, i.e., a pseudo-label guided\nlearningframework, to address various inconsistencies within the realworld\npaired dataset. Generally, it consists of a de-weatheringmodel (De-W) and a\nConsistent Label Constructor (CLC), bywhich restoration result can be\nadaptively supervised by originalground-truth image to recover sharp textures\nwhile maintainingconsistency with the degraded inputs in non-weather\ncontentthrough the supervision of pseudo-labels. Particularly, a Crossframe\nSimilarity Aggregation (CSA) module is deployed withinCLC to enhance the\nquality of pseudo-labels by exploring thepotential complementary information of\nmulti-frames throughgraph model. Moreover, we introduce an Information\nAllocationStrategy (IAS) to integrate the original ground-truth imagesand\npseudo-labels, thereby facilitating the joint supervision forthe training of\nde-weathering model. Extensive experimentsdemonstrate that our method exhibits\nsignificant advantageswhen trained on imperfectly aligned de-weathering\ndatasets incomparison with other approaches."}
{"id": "2504.09975", "pdf": "https://arxiv.org/pdf/2504.09975", "abs": "https://arxiv.org/abs/2504.09975", "authors": ["Si-Tong Wei", "Rui-Huan Wang", "Chuan-Zhi Zhou", "Baoquan Chen", "Peng-Shuai Wang"], "title": "OctGPT: Octree-based Multiscale Autoregressive Models for 3D Shape Generation", "categories": ["cs.GR", "cs.CV"], "comment": "SIGGRAPH 2025", "summary": "Autoregressive models have achieved remarkable success across various\ndomains, yet their performance in 3D shape generation lags significantly behind\nthat of diffusion models. In this paper, we introduce OctGPT, a novel\nmultiscale autoregressive model for 3D shape generation that dramatically\nimproves the efficiency and performance of prior 3D autoregressive approaches,\nwhile rivaling or surpassing state-of-the-art diffusion models. Our method\nemploys a serialized octree representation to efficiently capture the\nhierarchical and spatial structures of 3D shapes. Coarse geometry is encoded\nvia octree structures, while fine-grained details are represented by binary\ntokens generated using a vector quantized variational autoencoder (VQVAE),\ntransforming 3D shapes into compact \\emph{multiscale binary sequences} suitable\nfor autoregressive prediction. To address the computational challenges of\nhandling long sequences, we incorporate octree-based transformers enhanced with\n3D rotary positional encodings, scale-specific embeddings, and token-parallel\ngeneration schemes. These innovations reduce training time by 13 folds and\ngeneration time by 69 folds, enabling the efficient training of high-resolution\n3D shapes, e.g.,$1024^3$, on just four NVIDIA 4090 GPUs only within days.\nOctGPT showcases exceptional versatility across various tasks, including text-,\nsketch-, and image-conditioned generation, as well as scene-level synthesis\ninvolving multiple objects. Extensive experiments demonstrate that OctGPT\naccelerates convergence and improves generation quality over prior\nautoregressive methods, offering a new paradigm for high-quality, scalable 3D\ncontent creation."}
{"id": "2504.10000", "pdf": "https://arxiv.org/pdf/2504.10000", "abs": "https://arxiv.org/abs/2504.10000", "authors": ["Yanbo Wang", "Jiyang Guan", "Jian Liang", "Ran He"], "title": "Do We Really Need Curated Malicious Data for Safety Alignment in Multi-modal Large Language Models?", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "Accepted to CVPR 2025, codes in process", "summary": "Multi-modal large language models (MLLMs) have made significant progress, yet\ntheir safety alignment remains limited. Typically, current open-source MLLMs\nrely on the alignment inherited from their language module to avoid harmful\ngenerations. However, the lack of safety measures specifically designed for\nmulti-modal inputs creates an alignment gap, leaving MLLMs vulnerable to\nvision-domain attacks such as typographic manipulation. Current methods utilize\na carefully designed safety dataset to enhance model defense capability, while\nthe specific knowledge or patterns acquired from the high-quality dataset\nremain unclear. Through comparison experiments, we find that the alignment gap\nprimarily arises from data distribution biases, while image content, response\nquality, or the contrastive behavior of the dataset makes little contribution\nto boosting multi-modal safety. To further investigate this and identify the\nkey factors in improving MLLM safety, we propose finetuning MLLMs on a small\nset of benign instruct-following data with responses replaced by simple, clear\nrejection sentences. Experiments show that, without the need for\nlabor-intensive collection of high-quality malicious data, model safety can\nstill be significantly improved, as long as a specific fraction of rejection\ndata exists in the finetuning set, indicating the security alignment is not\nlost but rather obscured during multi-modal pretraining or instruction\nfinetuning. Simply correcting the underlying data bias could narrow the safety\ngap in the vision domain."}
{"id": "2504.10003", "pdf": "https://arxiv.org/pdf/2504.10003", "abs": "https://arxiv.org/abs/2504.10003", "authors": ["Yiming Zeng", "Hao Ren", "Shuhang Wang", "Junlong Huang", "Hui Cheng"], "title": "NaviDiffusor: Cost-Guided Diffusion Model for Visual Navigation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Visual navigation, a fundamental challenge in mobile robotics, demands\nversatile policies to handle diverse environments. Classical methods leverage\ngeometric solutions to minimize specific costs, offering adaptability to new\nscenarios but are prone to system errors due to their multi-modular design and\nreliance on hand-crafted rules. Learning-based methods, while achieving high\nplanning success rates, face difficulties in generalizing to unseen\nenvironments beyond the training data and often require extensive training. To\naddress these limitations, we propose a hybrid approach that combines the\nstrengths of learning-based methods and classical approaches for RGB-only\nvisual navigation. Our method first trains a conditional diffusion model on\ndiverse path-RGB observation pairs. During inference, it integrates the\ngradients of differentiable scene-specific and task-level costs, guiding the\ndiffusion model to generate valid paths that meet the constraints. This\napproach alleviates the need for retraining, offering a plug-and-play solution.\nExtensive experiments in both indoor and outdoor settings, across simulated and\nreal-world scenarios, demonstrate zero-shot transfer capability of our\napproach, achieving higher success rates and fewer collisions compared to\nbaseline methods. Code will be released at\nhttps://github.com/SYSU-RoboticsLab/NaviD."}
{"id": "2504.10007", "pdf": "https://arxiv.org/pdf/2504.10007", "abs": "https://arxiv.org/abs/2504.10007", "authors": ["Jiani Ni", "He Zhao", "Jintong Gao", "Dandan Guo", "Hongyuan Zha"], "title": "Balancing Two Classifiers via A Simplex ETF Structure for Model Calibration", "categories": ["cs.LG", "cs.CV"], "comment": "CVPR2025", "summary": "In recent years, deep neural networks (DNNs) have demonstrated\nstate-of-the-art performance across various domains. However, despite their\nsuccess, they often face calibration issues, particularly in safety-critical\napplications such as autonomous driving and healthcare, where unreliable\npredictions can have serious consequences. Recent research has started to\nimprove model calibration from the view of the classifier. However, the\nexploration of designing the classifier to solve the model calibration problem\nis insufficient. Let alone most of the existing methods ignore the calibration\nerrors arising from underconfidence. In this work, we propose a novel method by\nbalancing learnable and ETF classifiers to solve the overconfidence or\nunderconfidence problem for model Calibration named BalCAL. By introducing a\nconfidence-tunable module and a dynamic adjustment method, we ensure better\nalignment between model confidence and its true accuracy. Extensive\nexperimental validation shows that ours significantly improves model\ncalibration performance while maintaining high predictive accuracy,\noutperforming existing techniques. This provides a novel solution to the\ncalibration challenges commonly encountered in deep learning."}
{"id": "2504.10014", "pdf": "https://arxiv.org/pdf/2504.10014", "abs": "https://arxiv.org/abs/2504.10014", "authors": ["Hang Yin", "Yan-Ming Zhang", "Jian Xu", "Jian-Long Chang", "Yin Li", "Cheng-Lin Liu"], "title": "Air Quality Prediction with A Meteorology-Guided Modality-Decoupled Spatio-Temporal Network", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Air quality prediction plays a crucial role in public health and\nenvironmental protection. Accurate air quality prediction is a complex\nmultivariate spatiotemporal problem, that involves interactions across temporal\npatterns, pollutant correlations, spatial station dependencies, and\nparticularly meteorological influences that govern pollutant dispersion and\nchemical transformations. Existing works underestimate the critical role of\natmospheric conditions in air quality prediction and neglect comprehensive\nmeteorological data utilization, thereby impairing the modeling of dynamic\ninterdependencies between air quality and meteorological data. To overcome\nthis, we propose MDSTNet, an encoder-decoder framework that explicitly models\nair quality observations and atmospheric conditions as distinct modalities,\nintegrating multi-pressure-level meteorological data and weather forecasts to\ncapture atmosphere-pollution dependencies for prediction. Meantime, we\nconstruct ChinaAirNet, the first nationwide dataset combining air quality\nrecords with multi-pressure-level meteorological observations. Experimental\nresults on ChinaAirNet demonstrate MDSTNet's superiority, substantially\nreducing 48-hour prediction errors by 17.54\\% compared to the state-of-the-art\nmodel. The source code and dataset will be available on github."}
{"id": "2504.10020", "pdf": "https://arxiv.org/pdf/2504.10020", "abs": "https://arxiv.org/abs/2504.10020", "authors": ["Hao Yin", "Gunagzong Si", "Zilei Wang"], "title": "The Mirage of Performance Gains: Why Contrastive Decoding Fails to Address Multimodal Hallucination", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Contrastive decoding strategies are widely used to reduce hallucinations in\nmultimodal large language models (MLLMs). These methods work by constructing\ncontrastive samples to induce hallucinations and then suppressing them in the\noutput distribution. However, this paper demonstrates that such approaches fail\nto effectively mitigate the hallucination problem. The performance improvements\nobserved on POPE Benchmark are largely driven by two misleading factors: (1)\ncrude, unidirectional adjustments to the model's output distribution and (2)\nthe adaptive plausibility constraint, which reduces the sampling strategy to\ngreedy search. To further illustrate these issues, we introduce a series of\nspurious improvement methods and evaluate their performance against contrastive\ndecoding techniques. Experimental results reveal that the observed performance\ngains in contrastive decoding are entirely unrelated to its intended goal of\nmitigating hallucinations. Our findings challenge common assumptions about the\neffectiveness of contrastive decoding strategies and pave the way for\ndeveloping genuinely effective solutions to hallucinations in MLLMs."}
{"id": "2504.10025", "pdf": "https://arxiv.org/pdf/2504.10025", "abs": "https://arxiv.org/abs/2504.10025", "authors": ["Uyen Phan", "Ozer Can Devecioglu", "Serkan Kiranyaz", "Moncef Gabbouj"], "title": "Progressive Transfer Learning for Multi-Pass Fundus Image Restoration", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "13 pages, 12 figures including appendix", "summary": "Diabetic retinopathy is a leading cause of vision impairment, making its\nearly diagnosis through fundus imaging critical for effective treatment\nplanning. However, the presence of poor quality fundus images caused by factors\nsuch as inadequate illumination, noise, blurring and other motion artifacts\nyields a significant challenge for accurate DR screening. In this study, we\npropose progressive transfer learning for multi pass restoration to iteratively\nenhance the quality of degraded fundus images, ensuring more reliable DR\nscreening. Unlike previous methods that often focus on a single pass\nrestoration, multi pass restoration via PTL can achieve a superior blind\nrestoration performance that can even improve most of the good quality fundus\nimages in the dataset. Initially, a Cycle GAN model is trained to restore low\nquality images, followed by PTL induced restoration passes over the latest\nrestored outputs to improve overall quality in each pass. The proposed method\ncan learn blind restoration without requiring any paired data while surpassing\nits limitations by leveraging progressive learning and fine tuning strategies\nto minimize distortions and preserve critical retinal features. To evaluate\nPTL's effectiveness on multi pass restoration, we conducted experiments on\nDeepDRiD, a large scale fundus imaging dataset specifically curated for\ndiabetic retinopathy detection. Our result demonstrates state of the art\nperformance, showcasing PTL's potential as a superior approach to iterative\nimage quality restoration."}
{"id": "2504.10041", "pdf": "https://arxiv.org/pdf/2504.10041", "abs": "https://arxiv.org/abs/2504.10041", "authors": ["Hao Ren", "Yiming Zeng", "Zetong Bi", "Zhaoliang Wan", "Junlong Huang", "Hui Cheng"], "title": "Prior Does Matter: Visual Navigation via Denoising Diffusion Bridge Models", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Recent advancements in diffusion-based imitation learning, which show\nimpressive performance in modeling multimodal distributions and training\nstability, have led to substantial progress in various robot learning tasks. In\nvisual navigation, previous diffusion-based policies typically generate action\nsequences by initiating from denoising Gaussian noise. However, the target\naction distribution often diverges significantly from Gaussian noise, leading\nto redundant denoising steps and increased learning complexity. Additionally,\nthe sparsity of effective action distributions makes it challenging for the\npolicy to generate accurate actions without guidance. To address these issues,\nwe propose a novel, unified visual navigation framework leveraging the\ndenoising diffusion bridge models named NaviBridger. This approach enables\naction generation by initiating from any informative prior actions, enhancing\nguidance and efficiency in the denoising process. We explore how diffusion\nbridges can enhance imitation learning in visual navigation tasks and further\nexamine three source policies for generating prior actions. Extensive\nexperiments in both simulated and real-world indoor and outdoor scenarios\ndemonstrate that NaviBridger accelerates policy inference and outperforms the\nbaselines in generating target action sequences. Code is available at\nhttps://github.com/hren20/NaiviBridger."}
{"id": "2504.10127", "pdf": "https://arxiv.org/pdf/2504.10127", "abs": "https://arxiv.org/abs/2504.10127", "authors": ["Junlei Zhang", "Zichen Ding", "Chang Ma", "Zijie Chen", "Qiushi Sun", "Zhenzhong Lan", "Junxian He"], "title": "Breaking the Data Barrier -- Building GUI Agents Through Task Generalization", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "24 pages, 11 figures", "summary": "Graphical User Interface (GUI) agents offer cross-platform solutions for\nautomating complex digital tasks, with significant potential to transform\nproductivity workflows. However, their performance is often constrained by the\nscarcity of high-quality trajectory data. To address this limitation, we\npropose training Vision Language Models (VLMs) on data-rich,\nreasoning-intensive tasks during a dedicated mid-training stage, and then\nexamine how incorporating these tasks facilitates generalization to GUI\nplanning scenarios. Specifically, we explore a range of tasks with readily\navailable instruction-tuning data, including GUI perception, multimodal\nreasoning, and textual reasoning. Through extensive experiments across 11\nmid-training tasks, we demonstrate that: (1) Task generalization proves highly\neffective, yielding substantial improvements across most settings. For\ninstance, multimodal mathematical reasoning enhances performance on\nAndroidWorld by an absolute 6.3%. Remarkably, text-only mathematical data\nsignificantly boosts GUI web agent performance, achieving a 5.6% improvement on\nWebArena and 5.4% improvement on AndroidWorld, underscoring notable cross-modal\ngeneralization from text-based to visual domains; (2) Contrary to prior\nassumptions, GUI perception data - previously considered closely aligned with\nGUI agent tasks and widely utilized for training - has a comparatively limited\nimpact on final performance; (3) Building on these insights, we identify the\nmost effective mid-training tasks and curate optimized mixture datasets,\nresulting in absolute performance gains of 8.0% on WebArena and 12.2% on\nAndroidWorld. Our work provides valuable insights into cross-domain knowledge\ntransfer for GUI agents and offers a practical approach to addressing data\nscarcity challenges in this emerging field. The code, data and models will be\navailable at https://github.com/hkust-nlp/GUIMid."}
{"id": "2504.10143", "pdf": "https://arxiv.org/pdf/2504.10143", "abs": "https://arxiv.org/abs/2504.10143", "authors": ["Yichao Cai", "Yuhang Liu", "Erdun Gao", "Tianjiao Jiang", "Zhen Zhang", "Anton van den Hengel", "Javen Qinfeng Shi"], "title": "Negate or Embrace: On How Misalignment Shapes Multimodal Representation Learning", "categories": ["cs.LG", "cs.CV"], "comment": "38 pages", "summary": "Multimodal representation learning, exemplified by multimodal contrastive\nlearning (MMCL) using image-text pairs, aims to learn powerful representations\nby aligning cues across modalities. This approach relies on the core assumption\nthat the exemplar image-text pairs constitute two representations of an\nidentical concept. However, recent research has revealed that real-world\ndatasets often exhibit misalignment. There are two distinct viewpoints on how\nto address this issue: one suggests mitigating the misalignment, and the other\nleveraging it. We seek here to reconcile these seemingly opposing perspectives,\nand to provide a practical guide for practitioners. Using latent variable\nmodels we thus formalize misalignment by introducing two specific mechanisms:\nselection bias, where some semantic variables are missing, and perturbation\nbias, where semantic variables are distorted -- both affecting latent variables\nshared across modalities. Our theoretical analysis demonstrates that, under\nmild assumptions, the representations learned by MMCL capture exactly the\ninformation related to the subset of the semantic variables invariant to\nselection and perturbation biases. This provides a unified perspective for\nunderstanding misalignment. Based on this, we further offer actionable insights\ninto how misalignment should inform the design of real-world ML systems. We\nvalidate our theoretical findings through extensive empirical studies on both\nsynthetic data and real image-text datasets, shedding light on the nuanced\nimpact of misalignment on multimodal representation learning."}
{"id": "2504.10244", "pdf": "https://arxiv.org/pdf/2504.10244", "abs": "https://arxiv.org/abs/2504.10244", "authors": ["Ziyao Shang", "Misha Kaandorp", "Kelly Payette", "Marina Fernandez Garcia", "Roxane Licandro", "Georg Langs", "Jordina Aviles Verdera", "Jana Hutter", "Bjoern Menze", "Gregor Kasprian", "Meritxell Bach Cuadra", "Andras Jakab"], "title": "Towards contrast- and pathology-agnostic clinical fetal brain MRI segmentation using SynthSeg", "categories": ["eess.IV", "cs.CV"], "comment": "21 pages, 16 figures", "summary": "Magnetic resonance imaging (MRI) has played a crucial role in fetal\nneurodevelopmental research. Structural annotations of MR images are an\nimportant step for quantitative analysis of the developing human brain, with\nDeep learning providing an automated alternative for this otherwise tedious\nmanual process. However, segmentation performances of Convolutional Neural\nNetworks often suffer from domain shift, where the network fails when applied\nto subjects that deviate from the distribution with which it is trained on. In\nthis work, we aim to train networks capable of automatically segmenting fetal\nbrain MRIs with a wide range of domain shifts pertaining to differences in\nsubject physiology and acquisition environments, in particular shape-based\ndifferences commonly observed in pathological cases. We introduce a novel\ndata-driven train-time sampling strategy that seeks to fully exploit the\ndiversity of a given training dataset to enhance the domain generalizability of\nthe trained networks. We adapted our sampler, together with other existing data\naugmentation techniques, to the SynthSeg framework, a generator that utilizes\ndomain randomization to generate diverse training data, and ran thorough\nexperimentations and ablation studies on a wide range of training/testing data\nto test the validity of the approaches. Our networks achieved notable\nimprovements in the segmentation quality on testing subjects with intense\nanatomical abnormalities (p < 1e-4), though at the cost of a slighter decrease\nin performance in cases with fewer abnormalities. Our work also lays the\nfoundation for future works on creating and adapting data-driven sampling\nstrategies for other training pipelines."}
{"id": "2504.10281", "pdf": "https://arxiv.org/pdf/2504.10281", "abs": "https://arxiv.org/abs/2504.10281", "authors": ["Jingyun Yang", "Ruoyan Avery Yin", "Chi Jiang", "Yuepeng Hu", "Xiaokai Zhu", "Xingjian Hu", "Sutharsika Kumar", "Xiao Wang", "Xiaohua Zhai", "Keran Rong", "Yunyue Zhu", "Tianyi Zhang", "Zongyou Yin", "Jing Kong", "Neil Zhenqiang Gong", "Zhichu Ren", "Haozhe Wang"], "title": "Zero-shot Autonomous Microscopy for Scalable and Intelligent Characterization of 2D Materials", "categories": ["cond-mat.mtrl-sci", "cond-mat.mes-hall", "cs.AI", "cs.CV", "cs.LG"], "comment": "13 pages, 4 figures", "summary": "Characterization of atomic-scale materials traditionally requires human\nexperts with months to years of specialized training. Even for trained human\noperators, accurate and reliable characterization remains challenging when\nexamining newly discovered materials such as two-dimensional (2D) structures.\nThis bottleneck drives demand for fully autonomous experimentation systems\ncapable of comprehending research objectives without requiring large training\ndatasets. In this work, we present ATOMIC (Autonomous Technology for Optical\nMicroscopy & Intelligent Characterization), an end-to-end framework that\nintegrates foundation models to enable fully autonomous, zero-shot\ncharacterization of 2D materials. Our system integrates the vision foundation\nmodel (i.e., Segment Anything Model), large language models (i.e., ChatGPT),\nunsupervised clustering, and topological analysis to automate microscope\ncontrol, sample scanning, image segmentation, and intelligent analysis through\nprompt engineering, eliminating the need for additional training. When\nanalyzing typical MoS2 samples, our approach achieves 99.7% segmentation\naccuracy for single layer identification, which is equivalent to that of human\nexperts. In addition, the integrated model is able to detect grain boundary\nslits that are challenging to identify with human eyes. Furthermore, the system\nretains robust accuracy despite variable conditions including defocus, color\ntemperature fluctuations, and exposure variations. It is applicable to a broad\nspectrum of common 2D materials-including graphene, MoS2, WSe2, SnSe-regardless\nof whether they were fabricated via chemical vapor deposition or mechanical\nexfoliation. This work represents the implementation of foundation models to\nachieve autonomous analysis, establishing a scalable and data-efficient\ncharacterization paradigm that fundamentally transforms the approach to\nnanoscale materials research."}
{"id": "2504.10445", "pdf": "https://arxiv.org/pdf/2504.10445", "abs": "https://arxiv.org/abs/2504.10445", "authors": ["Suyu Ye", "Haojun Shi", "Darren Shih", "Hyokun Yun", "Tanya Roosta", "Tianmin Shu"], "title": "RealWebAssist: A Benchmark for Long-Horizon Web Assistance with Real-World Users", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "Project Website: https://scai.cs.jhu.edu/projects/RealWebAssist/\n  Code: https://github.com/SCAI-JHU/RealWebAssist", "summary": "To achieve successful assistance with long-horizon web-based tasks, AI agents\nmust be able to sequentially follow real-world user instructions over a long\nperiod. Unlike existing web-based agent benchmarks, sequential instruction\nfollowing in the real world poses significant challenges beyond performing a\nsingle, clearly defined task. For instance, real-world human instructions can\nbe ambiguous, require different levels of AI assistance, and may evolve over\ntime, reflecting changes in the user's mental state. To address this gap, we\nintroduce RealWebAssist, a novel benchmark designed to evaluate sequential\ninstruction-following in realistic scenarios involving long-horizon\ninteractions with the web, visual GUI grounding, and understanding ambiguous\nreal-world user instructions. RealWebAssist includes a dataset of sequential\ninstructions collected from real-world human users. Each user instructs a\nweb-based assistant to perform a series of tasks on multiple websites. A\nsuccessful agent must reason about the true intent behind each instruction,\nkeep track of the mental state of the user, understand user-specific routines,\nand ground the intended tasks to actions on the correct GUI elements. Our\nexperimental results show that state-of-the-art models struggle to understand\nand ground user instructions, posing critical challenges in following\nreal-world user instructions for long-horizon web assistance."}
