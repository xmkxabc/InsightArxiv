{"id": "2504.03739", "pdf": "https://arxiv.org/pdf/2504.03739", "abs": "https://arxiv.org/abs/2504.03739", "authors": ["Mingyan Liu"], "title": "A Unified Virtual Mixture-of-Experts Framework:Enhanced Inference and Hallucination Mitigation in Single-Model System", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Generative models, such as GPT and BERT, have significantly improved\nperformance in tasks like text generation and summarization. However,\nhallucinations \"where models generate non-factual or misleading content\" are\nespecially problematic in smaller-scale architectures, limiting their\nreal-world applicability.In this paper, we propose a unified Virtual\nMixture-of-Experts (MoE) fusion strategy that enhances inference performance\nand mitigates hallucinations in a single Qwen 1.5 0.5B model without increasing\nthe parameter count. Our method leverages multiple domain-specific expert\nprompts (with the number of experts being adjustable) to guide the model from\ndifferent perspectives. We apply a statistical outlier truncation strategy\nbased on the mean and standard deviation to filter out abnormally high\nprobability predictions, and we inject noise into the embedding space to\npromote output diversity. To clearly assess the contribution of each module, we\nadopt a fixed voting mechanism rather than a dynamic gating network, thereby\navoiding additional confounding factors. We provide detailed theoretical\nderivations from both statistical and ensemble learning perspectives to\ndemonstrate how our method reduces output variance and suppresses\nhallucinations. Extensive ablation experiments on dialogue generation tasks\nshow that our approach significantly improves inference accuracy and robustness\nin small models. Additionally, we discuss methods for evaluating the\northogonality of virtual experts and outline the potential for future work\ninvolving dynamic expert weight allocation using gating networks."}
{"id": "2504.03786", "pdf": "https://arxiv.org/pdf/2504.03786", "abs": "https://arxiv.org/abs/2504.03786", "authors": ["Sifan Li", "Yujun Cai", "Bryan Hooi", "Nanyun Peng", "Yiwei Wang"], "title": "Do \"New Snow Tablets\" Contain Snow? Large Language Models Over-Rely on Names to Identify Ingredients of Chinese Drugs", "categories": ["cs.CL"], "comment": null, "summary": "Traditional Chinese Medicine (TCM) has seen increasing adoption in\nhealthcare, with specialized Large Language Models (LLMs) emerging to support\nclinical applications. A fundamental requirement for these models is accurate\nidentification of TCM drug ingredients. In this paper, we evaluate how general\nand TCM-specialized LLMs perform when identifying ingredients of Chinese drugs.\nOur systematic analysis reveals consistent failure patterns: models often\ninterpret drug names literally, overuse common herbs regardless of relevance,\nand exhibit erratic behaviors when faced with unfamiliar formulations. LLMs\nalso fail to understand the verification task. These findings demonstrate that\ncurrent LLMs rely primarily on drug names rather than possessing systematic\npharmacological knowledge. To address these limitations, we propose a Retrieval\nAugmented Generation (RAG) approach focused on ingredient names. Experiments\nacross 220 TCM formulations show our method significantly improves accuracy\nfrom approximately 50% to 82% in ingredient verification tasks. Our work\nhighlights critical weaknesses in current TCM-specific LLMs and offers a\npractical solution for enhancing their clinical reliability."}
{"id": "2504.03790", "pdf": "https://arxiv.org/pdf/2504.03790", "abs": "https://arxiv.org/abs/2504.03790", "authors": ["Gon√ßalo Faria", "Noah A. Smith"], "title": "Sample, Don't Search: Rethinking Test-Time Alignment for Language Models", "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "Increasing test-time computation has emerged as a promising direction for\nimproving language model performance, particularly in scenarios where model\nfinetuning is impractical or impossible due to computational constraints or\nprivate model weights. However, existing test-time search methods using a\nreward model (RM) often degrade in quality as compute scales, due to the\nover-optimization of what are inherently imperfect reward proxies. We introduce\nQAlign, a new test-time alignment approach. As we scale test-time compute,\nQAlign converges to sampling from the optimal aligned distribution for each\nindividual prompt. By adopting recent advances in Markov chain Monte Carlo for\ntext generation, our method enables better-aligned outputs without modifying\nthe underlying model or even requiring logit access. We demonstrate the\neffectiveness of QAlign on mathematical reasoning benchmarks (GSM8K and\nGSM-Symbolic) using a task-specific RM, showing consistent improvements over\nexisting test-time compute methods like best-of-n and majority voting.\nFurthermore, when applied with more realistic RMs trained on the Tulu 3\npreference dataset, QAlign outperforms direct preference optimization (DPO),\nbest-of-n, majority voting, and weighted majority voting on a diverse range of\ndatasets (GSM8K, MATH500, IFEval, MMLU-Redux, and TruthfulQA). A practical\nsolution to aligning language models at test time using additional computation\nwithout degradation, our approach expands the limits of the capability that can\nbe obtained from off-the-shelf language models without further training."}
{"id": "2504.03794", "pdf": "https://arxiv.org/pdf/2504.03794", "abs": "https://arxiv.org/abs/2504.03794", "authors": ["Liangwei Yang", "Yuhui Xu", "Juntao Tan", "Doyen Sahoo", "Silvio Savarese", "Caiming Xiong", "Huan Wang", "Shelby Heinecke"], "title": "Entropy-Based Block Pruning for Efficient Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 8 figures", "summary": "As large language models continue to scale, their growing computational and\nstorage demands pose significant challenges for real-world deployment. In this\nwork, we investigate redundancy within Transformer-based models and propose an\nentropy-based pruning strategy to enhance efficiency while maintaining\nperformance. Empirical analysis reveals that the entropy of hidden\nrepresentations decreases in the early blocks but progressively increases\nacross most subsequent blocks. This trend suggests that entropy serves as a\nmore effective measure of information richness within computation blocks.\nUnlike cosine similarity, which primarily captures geometric relationships,\nentropy directly quantifies uncertainty and information content, making it a\nmore reliable criterion for pruning. Extensive experiments demonstrate that our\nentropy-based pruning approach surpasses cosine similarity-based methods in\nreducing model size while preserving accuracy, offering a promising direction\nfor efficient model deployment."}
{"id": "2504.03705", "pdf": "https://arxiv.org/pdf/2504.03705", "abs": "https://arxiv.org/abs/2504.03705", "authors": ["Luca Marini"], "title": "Semi-supervised learning for marine anomaly detection on board satellites", "categories": ["cs.CV"], "comment": "Master's project", "summary": "Aquatic bodies face numerous environmental threats caused by several marine\nanomalies. Marine debris can devastate habitats and endanger marine life\nthrough entanglement, while harmful algal blooms can produce toxins that\nnegatively affect marine ecosystems. Additionally, ships may discharge oil or\nengage in illegal and overfishing activities, causing further harm. These\nmarine anomalies can be identified by applying trained deep learning (DL)\nmodels on multispectral satellite imagery. Furthermore, the detection of other\nanomalies, such as clouds, could be beneficial in filtering out irrelevant\nimages. However, DL models often require a large volume of labeled data for\ntraining, which can be both costly and time-consuming, particularly for marine\nanomaly detection where expert annotation is needed. A potential solution is\nthe use of semi-supervised learning methods, which can also utilize unlabeled\ndata. In this project, we implement and study the performance of FixMatch for\nSemantic Segmentation, a semi-supervised algorithm for semantic segmentation.\nFirstly, we found that semi-supervised models perform best with a high\nconfidence threshold of 0.9 when there is a limited amount of labeled data.\nSecondly, we compare the performance of semi-supervised models with\nfully-supervised models under varying amounts of labeled data. Our findings\nsuggest that semi-supervised models outperform fully-supervised models with\nlimited labeled data, while fully-supervised models have a slightly better\nperformance with larger volumes of labeled data. We propose two hypotheses to\nexplain why fully-supervised models surpass semi-supervised ones when a high\nvolume of labeled data is used. All of our experiments were conducted using a\nU-Net model architecture with a limited number of parameters to ensure\ncompatibility with space-rated hardware."}
{"id": "2504.03803", "pdf": "https://arxiv.org/pdf/2504.03803", "abs": "https://arxiv.org/abs/2504.03803", "authors": ["Sander Noels", "Guillaume Bied", "Maarten Buyl", "Alexander Rogiers", "Yousra Fettach", "Jefrey Lijffijt", "Tijl De Bie"], "title": "What Large Language Models Do Not Talk About: An Empirical Study of Moderation and Censorship Practices", "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": "17 pages, 38 pages in total including appendix; 5 figures, 22 figures\n  in appendix", "summary": "Large Language Models (LLMs) are increasingly deployed as gateways to\ninformation, yet their content moderation practices remain underexplored. This\nwork investigates the extent to which LLMs refuse to answer or omit information\nwhen prompted on political topics. To do so, we distinguish between hard\ncensorship (i.e., generated refusals, error messages, or canned denial\nresponses) and soft censorship (i.e., selective omission or downplaying of key\nelements), which we identify in LLMs' responses when asked to provide\ninformation on a broad range of political figures. Our analysis covers 14\nstate-of-the-art models from Western countries, China, and Russia, prompted in\nall six official United Nations (UN) languages. Our analysis suggests that\nalthough censorship is observed across the board, it is predominantly tailored\nto an LLM provider's domestic audience and typically manifests as either hard\ncensorship or soft censorship (though rarely both concurrently). These findings\nunderscore the need for ideological and geographic diversity among publicly\navailable LLMs, and greater transparency in LLM moderation strategies to\nfacilitate informed user choices. All data are made freely available."}
{"id": "2504.03712", "pdf": "https://arxiv.org/pdf/2504.03712", "abs": "https://arxiv.org/abs/2504.03712", "authors": ["Jan Lewen", "Max Pargmann", "Jenia Jitsev", "Mehdi Cherti", "Robert Pitz-Paal", "Daniel Maldonado Quinto"], "title": "Scalable heliostat surface predictions from focal spots: Sim-to-Real transfer of inverse Deep Learning Raytracing", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.LG"], "comment": null, "summary": "Concentrating Solar Power (CSP) plants are a key technology in the transition\ntoward sustainable energy. A critical factor for their safe and efficient\noperation is the distribution of concentrated solar flux on the receiver.\nHowever, flux distributions from individual heliostats are sensitive to surface\nimperfections. Measuring these surfaces across many heliostats remains\nimpractical in real-world deployments. As a result, control systems often\nassume idealized heliostat surfaces, leading to suboptimal performance and\npotential safety risks. To address this, inverse Deep Learning Raytracing\n(iDLR) has been introduced as a novel method for inferring heliostat surface\nprofiles from target images recorded during standard calibration procedures. In\nthis work, we present the first successful Sim-to-Real transfer of iDLR,\nenabling accurate surface predictions directly from real-world target images.\nWe evaluate our method on 63 heliostats under real operational conditions. iDLR\nsurface predictions achieve a median mean absolute error (MAE) of 0.17 mm and\nshow good agreement with deflectometry ground truth in 84% of cases. When used\nin raytracing simulations, it enables flux density predictions with a mean\naccuracy of 90% compared to deflectometry over our dataset, and outperforms the\ncommonly used ideal heliostat surface assumption by 26%. We tested this\napproach in a challenging double-extrapolation scenario-involving unseen sun\npositions and receiver projection-and found that iDLR maintains high predictive\naccuracy, highlighting its generalization capabilities. Our results demonstrate\nthat iDLR is a scalable, automated, and cost-effective solution for integrating\nrealistic heliostat surface models into digital twins. This opens the door to\nimproved flux control, more precise performance modeling, and ultimately,\nenhanced efficiency and safety in future CSP plants."}
{"id": "2504.03846", "pdf": "https://arxiv.org/pdf/2504.03846", "abs": "https://arxiv.org/abs/2504.03846", "authors": ["Wei-Lin Chen", "Zhepei Wei", "Xinyu Zhu", "Shi Feng", "Yu Meng"], "title": "Do LLM Evaluators Prefer Themselves for a Reason?", "categories": ["cs.CL"], "comment": "Preprint. 31 pages", "summary": "Large language models (LLMs) are increasingly used as automatic evaluators in\napplications such as benchmarking, reward modeling, and self-refinement. Prior\nwork highlights a potential self-preference bias where LLMs favor their own\ngenerated responses, a tendency often intensifying with model size and\ncapability. This raises a critical question: Is self-preference detrimental, or\ndoes it simply reflect objectively superior outputs from more capable models?\nDisentangling these has been challenging due to the usage of subjective tasks\nin previous studies. To address this, we investigate self-preference using\nverifiable benchmarks (mathematical reasoning, factual knowledge, code\ngeneration) that allow objective ground-truth assessment. This enables us to\ndistinguish harmful self-preference (favoring objectively worse responses) from\nlegitimate self-preference (favoring genuinely superior ones). We conduct\nlarge-scale experiments under controlled evaluation conditions across diverse\nmodel families (e.g., Llama, Qwen, Gemma, Mistral, Phi, GPT, DeepSeek). Our\nfindings reveal three key insights: (1) Better generators are better judges --\nLLM evaluators' accuracy strongly correlates with their task performance, and\nmuch of the self-preference in capable models is legitimate. (2) Harmful\nself-preference persists, particularly when evaluator models perform poorly as\ngenerators on specific task instances. Stronger models exhibit more pronounced\nharmful bias when they err, though such incorrect generations are less\nfrequent. (3) Inference-time scaling strategies, such as generating a long\nChain-of-Thought before evaluation, effectively reduce the harmful\nself-preference. These results provide a more nuanced understanding of\nLLM-based evaluation and practical insights for improving its reliability."}
{"id": "2504.03724", "pdf": "https://arxiv.org/pdf/2504.03724", "abs": "https://arxiv.org/abs/2504.03724", "authors": ["Zhiqiang Wang", "Pengbin Feng", "Yanbin Lin", "Shuzhang Cai", "Zongao Bian", "Jinghua Yan", "Xingquan Zhu"], "title": "CrowdVLM-R1: Expanding R1 Ability to Vision Language Model for Crowd Counting using Fuzzy Group Relative Policy Reward", "categories": ["cs.CV", "cs.CL"], "comment": "11 pages, 6 figures and 4 tables", "summary": "We propose Fuzzy Group Relative Policy Reward (FGRPR), a novel framework that\nintegrates Group Relative Policy Optimization (GRPO) with a fuzzy reward\nfunction to enhance learning efficiency. Unlike the conventional binary 0/1\naccuracy reward, our fuzzy reward model provides nuanced incentives,\nencouraging more precise outputs. Experimental results demonstrate that GRPO\nwith a standard 0/1 accuracy reward underperforms compared to supervised\nfine-tuning (SFT). In contrast, FGRPR, applied to Qwen2.5-VL(3B and 7B),\nsurpasses all baseline models, including GPT4o, LLaMA2(90B), and SFT, across\nfive in-domain datasets. On an out-of-domain dataset, FGRPR achieves\nperformance comparable to SFT but excels when target values are larger, as its\nfuzzy reward function assigns higher rewards to closer approximations. This\napproach is broadly applicable to tasks where the precision of the answer is\ncritical. Code and data: https://github.com/yeyimilk/CrowdVLM-R1"}
{"id": "2504.03906", "pdf": "https://arxiv.org/pdf/2504.03906", "abs": "https://arxiv.org/abs/2504.03906", "authors": ["Abhilekh Borah", "Hasnat Md Abdullah", "Kangda Wei", "Ruihong Huang"], "title": "CliME: Evaluating Multimodal Climate Discourse on Social Media and the Climate Alignment Quotient (CAQ)", "categories": ["cs.CL"], "comment": "16 pages, 9 figures", "summary": "The rise of Large Language Models (LLMs) has raised questions about their\nability to understand climate-related contexts. Though climate change dominates\nsocial media, analyzing its multimodal expressions is understudied, and current\ntools have failed to determine whether LLMs amplify credible solutions or\nspread unsubstantiated claims. To address this, we introduce CliME (Climate\nChange Multimodal Evaluation), a first-of-its-kind multimodal dataset,\ncomprising 2579 Twitter and Reddit posts. The benchmark features a diverse\ncollection of humorous memes and skeptical posts, capturing how these formats\ndistill complex issues into viral narratives that shape public opinion and\npolicy discussions. To systematically evaluate LLM performance, we present the\nClimate Alignment Quotient (CAQ), a novel metric comprising five distinct\ndimensions: Articulation, Evidence, Resonance, Transition, and Specificity.\nAdditionally, we propose three analytical lenses: Actionability, Criticality,\nand Justice, to guide the assessment of LLM-generated climate discourse using\nCAQ. Our findings, based on the CAQ metric, indicate that while most evaluated\nLLMs perform relatively well in Criticality and Justice, they consistently\nunderperform on the Actionability axis. Among the models evaluated, Claude 3.7\nSonnet achieves the highest overall performance. We publicly release our CliME\ndataset and code to foster further research in this domain."}
{"id": "2504.03807", "pdf": "https://arxiv.org/pdf/2504.03807", "abs": "https://arxiv.org/abs/2504.03807", "authors": ["Maliheh Toozandehjani", "Ali Mousavi", "Reza Taheri"], "title": "From Keypoints to Realism: A Realistic and Accurate Virtual Try-on Network from 2D Images", "categories": ["cs.CV"], "comment": "in Persian language", "summary": "The aim of image-based virtual try-on is to generate realistic images of\nindividuals wearing target garments, ensuring that the pose, body shape and\ncharacteristics of the target garment are accurately preserved. Existing\nmethods often fail to reproduce the fine details of target garments effectively\nand lack generalizability to new scenarios. In the proposed method, the\nperson's initial garment is completely removed. Subsequently, a precise warping\nis performed using the predicted keypoints to fully align the target garment\nwith the body structure and pose of the individual. Based on the warped\ngarment, a body segmentation map is more accurately predicted. Then, using an\nalignment-aware segment normalization, the misaligned areas between the warped\ngarment and the predicted garment region in the segmentation map are removed.\nFinally, the generator produces the final image with high visual quality,\nreconstructing the precise characteristics of the target garment, including its\noverall shape and texture. This approach emphasizes preserving garment\ncharacteristics and improving adaptability to various poses, providing better\ngeneralization for diverse applications."}
{"id": "2504.03931", "pdf": "https://arxiv.org/pdf/2504.03931", "abs": "https://arxiv.org/abs/2504.03931", "authors": ["Zixuan Ke", "Yifei Ming", "Shafiq Joty"], "title": "Adaptation of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Tutorial Proposal for NAACL2025", "summary": "This tutorial on adaptation of LLMs is designed to address the growing demand\nfor models that go beyond the static capabilities of generic LLMs by providing\nan overview of dynamic, domain-specific, and task-adaptive LLM adaptation\ntechniques. While general LLMs have demonstrated strong generalization across a\nvariety of tasks, they often struggle to perform well in specialized domains\nsuch as finance, healthcare, and code generation for underrepresented\nlanguages. Additionally, their static nature limits their ability to evolve\nwith the changing world, and they are often extremely large in size, making\nthem impractical and costly to deploy at scale. As a result, the adaptation of\nLLMs has drawn much attention since the birth of LLMs and is of core\nimportance, both for industry, which focuses on serving its targeted users, and\nacademia, which can greatly benefit from small but powerful LLMs. To address\nthis gap, this tutorial aims to provide an overview of the LLM adaptation\ntechniques. We start with an introduction to LLM adaptation, from both the data\nperspective and the model perspective. We then emphasize how the evaluation\nmetrics and benchmarks are different from other techniques. After establishing\nthe problems, we explore various adaptation techniques. We categorize\nadaptation techniques into two main families. The first is parametric knowledge\nadaptation, which focuses on updating the parametric knowledge within LLMs.\nAdditionally, we will discuss real-time adaptation techniques, including model\nediting, which allows LLMs to be updated dynamically in production\nenvironments. The second kind of adaptation is semi-parametric knowledge\nadaptation, where the goal is to update LLM parameters to better leverage\nexternal knowledge or tools through techniques like retrieval-augmented\ngeneration (RAG) and agent-based systems."}
{"id": "2504.03821", "pdf": "https://arxiv.org/pdf/2504.03821", "abs": "https://arxiv.org/abs/2504.03821", "authors": ["Andrew Kiruluta", "Andreas Lemos"], "title": "A Hybrid Wavelet-Fourier Method for Next-Generation Conditional Diffusion Models", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "We present a novel generative modeling framework,Wavelet-Fourier-Diffusion,\nwhich adapts the diffusion paradigm to hybrid frequency representations in\norder to synthesize high-quality, high-fidelity images with improved spatial\nlocalization. In contrast to conventional diffusion models that rely\nexclusively on additive noise in pixel space, our approach leverages a\nmulti-transform that combines wavelet sub-band decomposition with partial\nFourier steps. This strategy progressively degrades and then reconstructs\nimages in a hybrid spectral domain during the forward and reverse diffusion\nprocesses. By supplementing traditional Fourier-based analysis with the spatial\nlocalization capabilities of wavelets, our model can capture both global\nstructures and fine-grained features more effectively. We further extend the\napproach to conditional image generation by integrating embeddings or\nconditional features via cross-attention. Experimental evaluations on CIFAR-10,\nCelebA-HQ, and a conditional ImageNet subset illustrate that our method\nachieves competitive or superior performance relative to baseline diffusion\nmodels and state-of-the-art GANs, as measured by Fr\\'echet Inception Distance\n(FID) and Inception Score (IS). We also show how the hybrid frequency-based\nrepresentation improves control over global coherence and fine texture\nsynthesis, paving the way for new directions in multi-scale generative\nmodeling."}
{"id": "2504.03932", "pdf": "https://arxiv.org/pdf/2504.03932", "abs": "https://arxiv.org/abs/2504.03932", "authors": ["Dongsuk Jang", "Alan Li", "Arman Cohan"], "title": "YaleNLP @ PerAnsSumm 2025: Multi-Perspective Integration via Mixture-of-Agents for Enhanced Healthcare QA Summarization", "categories": ["cs.CL"], "comment": "Paper accepted at CL4HEALTH @ NAACL 2025: Annual Conference of the\n  Nations of the Americas Chapter of the Association for Computational\n  Linguistics", "summary": "Automated summarization of healthcare community question-answering forums is\nchallenging due to diverse perspectives presented across multiple user\nresponses to each question. The PerAnsSumm Shared Task was therefore proposed\nto tackle this challenge by identifying perspectives from different answers and\nthen generating a comprehensive answer to the question. In this study, we\naddress the PerAnsSumm Shared Task using two complementary paradigms: (i) a\ntraining-based approach through QLoRA fine-tuning of LLaMA-3.3-70B-Instruct,\nand (ii) agentic approaches including zero- and few-shot prompting with\nfrontier LLMs (LLaMA-3.3-70B-Instruct and GPT-4o) and a Mixture-of-Agents (MoA)\nframework that leverages a diverse set of LLMs by combining outputs from\nmulti-layer feedback aggregation. For perspective span\nidentification/classification, GPT-4o zero-shot achieves an overall score of\n0.57, substantially outperforming the 0.40 score of the LLaMA baseline. With a\n2-layer MoA configuration, we were able to improve LLaMA performance up by 28\npercent to 0.51. For perspective-based summarization, GPT-4o zero-shot attains\nan overall score of 0.42 compared to 0.28 for the best LLaMA zero-shot, and our\n2-layer MoA approach boosts LLaMA performance by 32 percent to 0.37.\nFurthermore, in few-shot setting, our results show that the\nsentence-transformer embedding-based exemplar selection provides more gain than\nmanually selected exemplars on LLaMA models, although the few-shot prompting is\nnot always helpful for GPT-4o. The YaleNLP team's approach ranked the overall\nsecond place in the shared task."}
{"id": "2504.03850", "pdf": "https://arxiv.org/pdf/2504.03850", "abs": "https://arxiv.org/abs/2504.03850", "authors": ["Ved Umrajkar", "Aakash Kumar Singh"], "title": "Detection Limits and Statistical Separability of Tree Ring Watermarks in Rectified Flow-based Text-to-Image Generation Models", "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.LG", "stat.ML"], "comment": null, "summary": "Tree-Ring Watermarking is a significant technique for authenticating\nAI-generated images. However, its effectiveness in rectified flow-based models\nremains unexplored, particularly given the inherent challenges of these models\nwith noise latent inversion. Through extensive experimentation, we evaluated\nand compared the detection and separability of watermarks between SD 2.1 and\nFLUX.1-dev models. By analyzing various text guidance configurations and\naugmentation attacks, we demonstrate how inversion limitations affect both\nwatermark recovery and the statistical separation between watermarked and\nunwatermarked images. Our findings provide valuable insights into the current\nlimitations of Tree-Ring Watermarking in the current SOTA models and highlight\nthe critical need for improved inversion methods to achieve reliable watermark\ndetection and separability. The official implementation, dataset release and\nall experimental results are available at this\n\\href{https://github.com/dsgiitr/flux-watermarking}{\\textbf{link}}."}
{"id": "2504.03933", "pdf": "https://arxiv.org/pdf/2504.03933", "abs": "https://arxiv.org/abs/2504.03933", "authors": ["Samuele Marro", "Davide Evangelista", "X. Angelo Huang", "Emanuele La Malfa", "Michele Lombardi", "Michael Wooldridge"], "title": "Language Models Are Implicitly Continuous", "categories": ["cs.CL", "cs.LG", "I.2.7; I.2.6"], "comment": "Published at ICLR 2025", "summary": "Language is typically modelled with discrete sequences. However, the most\nsuccessful approaches to language modelling, namely neural networks, are\ncontinuous and smooth function approximators. In this work, we show that\nTransformer-based language models implicitly learn to represent sentences as\ncontinuous-time functions defined over a continuous input space. This\nphenomenon occurs in most state-of-the-art Large Language Models (LLMs),\nincluding Llama2, Llama3, Phi3, Gemma, Gemma2, and Mistral, and suggests that\nLLMs reason about language in ways that fundamentally differ from humans. Our\nwork formally extends Transformers to capture the nuances of time and space\ncontinuity in both input and output space. Our results challenge the\ntraditional interpretation of how LLMs understand language, with several\nlinguistic and engineering implications."}
{"id": "2504.03857", "pdf": "https://arxiv.org/pdf/2504.03857", "abs": "https://arxiv.org/abs/2504.03857", "authors": ["Keegan Harris"], "title": "Can ChatGPT Learn My Life From a Week of First-Person Video?", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Motivated by recent improvements in generative AI and wearable camera devices\n(e.g. smart glasses and AI-enabled pins), I investigate the ability of\nfoundation models to learn about the wearer's personal life through\nfirst-person camera data. To test this, I wore a camera headset for 54 hours\nover the course of a week, generated summaries of various lengths (e.g.\nminute-long, hour-long, and day-long summaries), and fine-tuned both GPT-4o and\nGPT-4o-mini on the resulting summary hierarchy. By querying the fine-tuned\nmodels, we are able to learn what the models learned about me. The results are\nmixed: Both models learned basic information about me (e.g. approximate age,\ngender). Moreover, GPT-4o correctly deduced that I live in Pittsburgh, am a PhD\nstudent at CMU, am right-handed, and have a pet cat. However, both models also\nsuffered from hallucination and would make up names for the individuals present\nin the video footage of my life."}
{"id": "2504.03964", "pdf": "https://arxiv.org/pdf/2504.03964", "abs": "https://arxiv.org/abs/2504.03964", "authors": ["Simon A. Lee", "Anthony Wu", "Jeffrey N. Chiang"], "title": "Clinical ModernBERT: An efficient and long context encoder for biomedical text", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Manuscript writeup corresponding to the Clinical ModernBERT\n  pre-trained encoder (https://huggingface.co/Simonlee711/Clinical_ModernBERT)", "summary": "We introduce Clinical ModernBERT, a transformer based encoder pretrained on\nlarge scale biomedical literature, clinical notes, and medical ontologies,\nincorporating PubMed abstracts, MIMIC IV clinical data, and medical codes with\ntheir textual descriptions. Building on ModernBERT the current state of the art\nnatural language text encoder featuring architectural upgrades such as rotary\npositional embeddings (RoPE), Flash Attention, and extended context length up\nto 8,192 tokens our model adapts these innovations specifically for biomedical\nand clinical domains. Clinical ModernBERT excels at producing semantically rich\nrepresentations tailored for long context tasks. We validate this both by\nanalyzing its pretrained weights and through empirical evaluation on a\ncomprehensive suite of clinical NLP benchmarks."}
{"id": "2504.03868", "pdf": "https://arxiv.org/pdf/2504.03868", "abs": "https://arxiv.org/abs/2504.03868", "authors": ["Ziming Liu", "Leichen Wang", "Ge Yang", "Xinrun Li", "Xingtao Hu", "Hao Sun", "Guangyu Gao"], "title": "Control Map Distribution using Map Query Bank for Online Map Generation", "categories": ["cs.CV"], "comment": null, "summary": "Reliable autonomous driving systems require high-definition (HD) map that\ncontains detailed map information for planning and navigation. However,\npre-build HD map requires a large cost. Visual-based Online Map Generation\n(OMG) has become an alternative low-cost solution to build a local HD map.\nQuery-based BEV Transformer has been a base model for this task. This model\nlearns HD map predictions from an initial map queries distribution which is\nobtained by offline optimization on training set. Besides the quality of BEV\nfeature, the performance of this model also highly relies on the capacity of\ninitial map query distribution. However, this distribution is limited because\nthe limited query number. To make map predictions optimal on each test sample,\nit is essential to generate a suitable initial distribution for each specific\nscenario. This paper proposes to decompose the whole HD map distribution into a\nset of point representations, namely map query bank (MQBank). To build specific\nmap query initial distributions of different scenarios, low-cost standard\ndefinition map (SD map) data is introduced as a kind of prior knowledge.\nMoreover, each layer of map decoder network learns instance-level map query\nfeatures, which will lose detailed information of each point. However, BEV\nfeature map is a point-level dense feature. It is important to keep point-level\ninformation in map queries when interacting with BEV feature map. This can also\nbe solved with map query bank method. Final experiments show a new insight on\nSD map prior and a new record on OpenLaneV2 benchmark with 40.5%, 45.7% mAP on\nvehicle lane and pedestrian area."}
{"id": "2504.03979", "pdf": "https://arxiv.org/pdf/2504.03979", "abs": "https://arxiv.org/abs/2504.03979", "authors": ["Amit K Verma", "Zhisong Zhang", "Junwon Seo", "Robin Kuo", "Runbo Jiang", "Emma Strubell", "Anthony D Rollett"], "title": "Structured Extraction of Process Structure Properties Relationships in Materials Science", "categories": ["cs.CL", "cond-mat.mtrl-sci", "cs.IR"], "comment": "16 pages, 3 figures, 13 table", "summary": "With the advent of large language models (LLMs), the vast unstructured text\nwithin millions of academic papers is increasingly accessible for materials\ndiscovery, although significant challenges remain. While LLMs offer promising\nfew- and zero-shot learning capabilities, particularly valuable in the\nmaterials domain where expert annotations are scarce, general-purpose LLMs\noften fail to address key materials-specific queries without further\nadaptation. To bridge this gap, fine-tuning LLMs on human-labeled data is\nessential for effective structured knowledge extraction. In this study, we\nintroduce a novel annotation schema designed to extract generic\nprocess-structure-properties relationships from scientific literature. We\ndemonstrate the utility of this approach using a dataset of 128 abstracts, with\nannotations drawn from two distinct domains: high-temperature materials (Domain\nI) and uncertainty quantification in simulating materials microstructure\n(Domain II). Initially, we developed a conditional random field (CRF) model\nbased on MatBERT, a domain-specific BERT variant, and evaluated its performance\non Domain I. Subsequently, we compared this model with a fine-tuned LLM (GPT-4o\nfrom OpenAI) under identical conditions. Our results indicate that fine-tuning\nLLMs can significantly improve entity extraction performance over the BERT-CRF\nbaseline on Domain I. However, when additional examples from Domain II were\nincorporated, the performance of the BERT-CRF model became comparable to that\nof the GPT-4o model. These findings underscore the potential of our schema for\nstructured knowledge extraction and highlight the complementary strengths of\nboth modeling approaches."}
{"id": "2504.03875", "pdf": "https://arxiv.org/pdf/2504.03875", "abs": "https://arxiv.org/abs/2504.03875", "authors": ["Wanhee Lee", "Klemen Kotar", "Rahul Mysore Venkatesh", "Jared Watrous", "Honglin Chen", "Khai Loong Aw", "Daniel L. K. Yamins"], "title": "3D Scene Understanding Through Local Random Access Sequence Modeling", "categories": ["cs.CV"], "comment": "Project webpage: https://neuroailab.github.io/projects/lras_3d/", "summary": "3D scene understanding from single images is a pivotal problem in computer\nvision with numerous downstream applications in graphics, augmented reality,\nand robotics. While diffusion-based modeling approaches have shown promise,\nthey often struggle to maintain object and scene consistency, especially in\ncomplex real-world scenarios. To address these limitations, we propose an\nautoregressive generative approach called Local Random Access Sequence (LRAS)\nmodeling, which uses local patch quantization and randomly ordered sequence\ngeneration. By utilizing optical flow as an intermediate representation for 3D\nscene editing, our experiments demonstrate that LRAS achieves state-of-the-art\nnovel view synthesis and 3D object manipulation capabilities. Furthermore, we\nshow that our framework naturally extends to self-supervised depth estimation\nthrough a simple modification of the sequence design. By achieving strong\nperformance on multiple 3D scene understanding tasks, LRAS provides a unified\nand effective framework for building the next generation of 3D vision models."}
{"id": "2504.03991", "pdf": "https://arxiv.org/pdf/2504.03991", "abs": "https://arxiv.org/abs/2504.03991", "authors": ["Siddharth Srikanth", "Varun Bhatt", "Boshen Zhang", "Werner Hager", "Charles Michael Lewis", "Katia P. Sycara", "Aaquib Tabrez", "Stefanos Nikolaidis"], "title": "Algorithmic Prompt Generation for Diverse Human-like Teaming and Communication with Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.MA"], "comment": null, "summary": "Understanding how humans collaborate and communicate in teams is essential\nfor improving human-agent teaming and AI-assisted decision-making. However,\nrelying solely on data from large-scale user studies is impractical due to\nlogistical, ethical, and practical constraints, necessitating synthetic models\nof multiple diverse human behaviors. Recently, agents powered by Large Language\nModels (LLMs) have been shown to emulate human-like behavior in social\nsettings. But, obtaining a large set of diverse behaviors requires manual\neffort in the form of designing prompts. On the other hand, Quality Diversity\n(QD) optimization has been shown to be capable of generating diverse\nReinforcement Learning (RL) agent behavior. In this work, we combine QD\noptimization with LLM-powered agents to iteratively search for prompts that\ngenerate diverse team behavior in a long-horizon, multi-step collaborative\nenvironment. We first show, through a human-subjects experiment (n=54\nparticipants), that humans exhibit diverse coordination and communication\nbehavior in this domain. We then show that our approach can effectively\nreplicate trends from human teaming data and also capture behaviors that are\nnot easily observed without collecting large amounts of data. Our findings\nhighlight the combination of QD and LLM-powered agents as an effective tool for\nstudying teaming and communication strategies in multi-agent collaboration."}
{"id": "2504.03886", "pdf": "https://arxiv.org/pdf/2504.03886", "abs": "https://arxiv.org/abs/2504.03886", "authors": ["Jianhao Zheng", "Zihan Zhu", "Valentin Bieri", "Marc Pollefeys", "Songyou Peng", "Iro Armeni"], "title": "WildGS-SLAM: Monocular Gaussian Splatting SLAM in Dynamic Environments", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "We present WildGS-SLAM, a robust and efficient monocular RGB SLAM system\ndesigned to handle dynamic environments by leveraging uncertainty-aware\ngeometric mapping. Unlike traditional SLAM systems, which assume static scenes,\nour approach integrates depth and uncertainty information to enhance tracking,\nmapping, and rendering performance in the presence of moving objects. We\nintroduce an uncertainty map, predicted by a shallow multi-layer perceptron and\nDINOv2 features, to guide dynamic object removal during both tracking and\nmapping. This uncertainty map enhances dense bundle adjustment and Gaussian map\noptimization, improving reconstruction accuracy. Our system is evaluated on\nmultiple datasets and demonstrates artifact-free view synthesis. Results\nshowcase WildGS-SLAM's superior performance in dynamic environments compared to\nstate-of-the-art methods."}
{"id": "2504.04022", "pdf": "https://arxiv.org/pdf/2504.04022", "abs": "https://arxiv.org/abs/2504.04022", "authors": ["Essential AI", ":", "Darsh J Shah", "Peter Rushton", "Somanshu Singla", "Mohit Parmar", "Kurt Smith", "Yash Vanjani", "Ashish Vaswani", "Adarsh Chaluvaraju", "Andrew Hojel", "Andrew Ma", "Anil Thomas", "Anthony Polloreno", "Ashish Tanwer", "Burhan Drak Sibai", "Divya S Mansingka", "Divya Shivaprasad", "Ishaan Shah", "Karl Stratos", "Khoi Nguyen", "Michael Callahan", "Michael Pust", "Mrinal Iyer", "Philip Monk", "Platon Mazarakis", "Ritvik Kapila", "Saurabh Srivastava", "Tim Romanski"], "title": "Rethinking Reflection in Pre-Training", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "A language model's ability to reflect on its own reasoning provides a key\nadvantage for solving complex problems. While most recent research has focused\non how this ability develops during reinforcement learning, we show that it\nactually begins to emerge much earlier - during the model's pre-training. To\nstudy this, we introduce deliberate errors into chains-of-thought and test\nwhether the model can still arrive at the correct answer by recognizing and\ncorrecting these mistakes. By tracking performance across different stages of\npre-training, we observe that this self-correcting ability appears early and\nimproves steadily over time. For instance, an OLMo2-7B model pre-trained on 4\ntrillion tokens displays self-correction on our six self-reflection tasks."}
{"id": "2504.03894", "pdf": "https://arxiv.org/pdf/2504.03894", "abs": "https://arxiv.org/abs/2504.03894", "authors": ["Haiqing Li", "Yuzhi Guo", "Feng Jiang", "Qifeng Zhou", "Hehuan Ma", "Junzhou Huang"], "title": "Leveraging Gait Patterns as Biomarkers: An attention-guided Deep Multiple Instance Learning Network for Scoliosis Classification", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages, 3 figures", "summary": "Scoliosis is a spinal curvature disorder that is difficult to detect early\nand can compress the chest cavity, impacting respiratory function and cardiac\nhealth. Especially for adolescents, delayed detection and treatment result in\nworsening compression. Traditional scoliosis detection methods heavily rely on\nclinical expertise, and X-ray imaging poses radiation risks, limiting\nlarge-scale early screening. We propose an Attention-Guided Deep Multi-Instance\nLearning method (Gait-MIL) to effectively capture discriminative features from\ngait patterns, which is inspired by ScoNet-MT's pioneering use of gait patterns\nfor scoliosis detection. We evaluate our method on the first large-scale\ndataset based on gait patterns for scoliosis classification. The results\ndemonstrate that our study improves the performance of using gait as a\nbiomarker for scoliosis detection, significantly enhances detection accuracy\nfor the particularly challenging Neutral cases, where subtle indicators are\noften overlooked. Our Gait-MIL also performs robustly in imbalanced scenarios,\nmaking it a promising tool for large-scale scoliosis screening."}
{"id": "2504.04038", "pdf": "https://arxiv.org/pdf/2504.04038", "abs": "https://arxiv.org/abs/2504.04038", "authors": ["Kaung Lwin Thant", "Kwankamol Nongpong", "Ye Kyaw Thu", "Thura Aung", "Khaing Hsu Wai", "Thazin Myint Oo"], "title": "myNER: Contextualized Burmese Named Entity Recognition with Bidirectional LSTM and fastText Embeddings via Joint Training with POS Tagging", "categories": ["cs.CL", "I.2.7"], "comment": "7 pages, 2 figures, 5 tables, to be published in the proceedings of\n  IEEE ICCI-2025", "summary": "Named Entity Recognition (NER) involves identifying and categorizing named\nentities within textual data. Despite its significance, NER research has often\noverlooked low-resource languages like Myanmar (Burmese), primarily due to the\nlack of publicly available annotated datasets. To address this, we introduce\nmyNER, a novel word-level NER corpus featuring a 7-tag annotation scheme,\nenriched with Part-of-Speech (POS) tagging to provide additional syntactic\ninformation. Alongside the corpus, we conduct a comprehensive evaluation of NER\nmodels, including Conditional Random Fields (CRF), Bidirectional LSTM\n(BiLSTM)-CRF, and their combinations with fastText embeddings in different\nsettings. Our experiments reveal the effectiveness of contextualized word\nembeddings and the impact of joint training with POS tagging, demonstrating\nsignificant performance improvements across models. The traditional CRF\njoint-task model with fastText embeddings as a feature achieved the best\nresult, with a 0.9818 accuracy and 0.9811 weighted F1 score with 0.7429 macro\nF1 score. BiLSTM-CRF with fine-tuned fastText embeddings gets the best result\nof 0.9791 accuracy and 0.9776 weighted F1 score with 0.7395 macro F1 score."}
{"id": "2504.03923", "pdf": "https://arxiv.org/pdf/2504.03923", "abs": "https://arxiv.org/abs/2504.03923", "authors": ["Tyler Ward", "Abdullah-Al-Zubaer Imran"], "title": "Improving Brain Disorder Diagnosis with Advanced Brain Function Representation and Kolmogorov-Arnold Networks", "categories": ["cs.CV"], "comment": "Paper accepted at MIDL 2025", "summary": "Quantifying functional connectivity (FC), a vital metric for the diagnosis of\nvarious brain disorders, traditionally relies on the use of a pre-defined brain\natlas. However, using such atlases can lead to issues regarding selection bias\nand lack of regard for specificity. Addressing this, we propose a novel\ntransformer-based classification network (AFBR-KAN) with effective brain\nfunction representation to aid in diagnosing autism spectrum disorder (ASD).\nAFBR-KAN leverages Kolmogorov-Arnold Network (KAN) blocks replacing traditional\nmulti-layer perceptron (MLP) components. Thorough experimentation reveals the\neffectiveness of AFBR-KAN in improving the diagnosis of ASD under various\nconfigurations of the model architecture. Our code is available at\nhttps://github.com/tbwa233/ABFR-KAN"}
{"id": "2504.04042", "pdf": "https://arxiv.org/pdf/2504.04042", "abs": "https://arxiv.org/abs/2504.04042", "authors": ["Kepu Zhang", "Weijie Yu", "Zhongxiang Sun", "Jun Xu"], "title": "SyLeR: A Framework for Explicit Syllogistic Legal Reasoning in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Syllogistic reasoning is a fundamental aspect of legal decision-making,\nenabling logical conclusions by connecting general legal principles with\nspecific case facts. Although existing large language models (LLMs) can\ngenerate responses to legal questions, they fail to perform explicit\nsyllogistic reasoning, often producing implicit and unstructured answers that\nlack explainability and trustworthiness. To address this limitation, we propose\nSyLeR, a novel framework that empowers LLMs to engage in explicit syllogistic\nlegal reasoning. SyLeR integrates a tree-structured hierarchical retrieval\nmechanism to effectively combine relevant legal statutes and precedent cases,\nforming comprehensive major premises. This is followed by a two-stage\nfine-tuning process: supervised fine-tuning warm-up establishes a foundational\nunderstanding of syllogistic reasoning, while reinforcement learning with a\nstructure-aware reward mechanism refines the ability of the model to generate\ndiverse logically sound and well-structured reasoning paths. We conducted\nextensive experiments across various dimensions, including in-domain and\ncross-domain user groups (legal laypersons and practitioners), multiple\nlanguages (Chinese and French), and different LLM backbones (legal-specific and\nopen-domain LLMs). The results show that SyLeR significantly improves response\naccuracy and consistently delivers explicit, explainable, and trustworthy legal\nreasoning."}
{"id": "2504.03948", "pdf": "https://arxiv.org/pdf/2504.03948", "abs": "https://arxiv.org/abs/2504.03948", "authors": ["Sanjoy Kundu", "Shanmukha Vellamchetti", "Sathyanarayanan N. Aakur"], "title": "ProbRes: Probabilistic Jump Diffusion for Open-World Egocentric Activity Recognition", "categories": ["cs.CV"], "comment": "17 pages, 6 figures, 3 tables. Under review", "summary": "Open-world egocentric activity recognition poses a fundamental challenge due\nto its unconstrained nature, requiring models to infer unseen activities from\nan expansive, partially observed search space. We introduce ProbRes, a\nProbabilistic Residual search framework based on jump-diffusion that\nefficiently navigates this space by balancing prior-guided exploration with\nlikelihood-driven exploitation. Our approach integrates structured commonsense\npriors to construct a semantically coherent search space, adaptively refines\npredictions using Vision-Language Models (VLMs) and employs a stochastic search\nmechanism to locate high-likelihood activity labels while minimizing exhaustive\nenumeration efficiently. We systematically evaluate ProbRes across multiple\nopenness levels (L0 - L3), demonstrating its adaptability to increasing search\nspace complexity. In addition to achieving state-of-the-art performance on\nbenchmark datasets (GTEA Gaze, GTEA Gaze+, EPIC-Kitchens, and Charades-Ego), we\nestablish a clear taxonomy for open-world recognition, delineating the\nchallenges and methodological advancements necessary for egocentric activity\nunderstanding. Our results highlight the importance of structured search\nstrategies, paving the way for scalable and efficient open-world activity\nrecognition."}
{"id": "2504.04050", "pdf": "https://arxiv.org/pdf/2504.04050", "abs": "https://arxiv.org/abs/2504.04050", "authors": ["Kang Xue", "Ming Dong", "Xinhui Tu", "Tingting He"], "title": "FISH-Tuning: Enhancing PEFT Methods with Fisher Information", "categories": ["cs.CL"], "comment": null, "summary": "The rapid growth in the parameter size of Large Language Models (LLMs) has\nled to the development of Parameter-Efficient Fine-Tuning (PEFT) methods to\nalleviate the computational costs of fine-tuning. Among these, Fisher Induced\nSparse uncHanging (FISH) Mask is a selection-based PEFT technique that\nidentifies a subset of pre-trained parameters for fine-tuning based on\napproximate Fisher information. However, the integration of FISH Mask with\nother PEFT methods, such as LoRA and Adapters, remains underexplored. In this\npaper, we propose FISH-Tuning, a novel approach that incorporates FISH Mask\ninto addition-based and reparameterization-based PEFT methods, including LoRA,\nAdapters, and their variants. By leveraging Fisher information to select\ncritical parameters within these methods, FISH-Tuning achieves superior\nperformance without additional memory overhead or inference latency.\nExperimental results across various datasets and pre-trained models demonstrate\nthat FISH-Tuning consistently outperforms the vanilla PEFT methods with the\nsame proportion of trainable parameters."}
{"id": "2504.03953", "pdf": "https://arxiv.org/pdf/2504.03953", "abs": "https://arxiv.org/abs/2504.03953", "authors": ["Arash Sajjadi", "Mark Eramian"], "title": "TGraphX: Tensor-Aware Graph Neural Network for Multi-Dimensional Feature Learning", "categories": ["cs.CV", "cs.AI", "68T07, 68T45, 68R10", "I.2.6; I.5.1; I.4.8"], "comment": "Submitted to arXiv. Code repository:\n  https://github.com/arashsajjadi/TGraphX |||\n  https://git.cs.usask.ca/arash/tgraphx", "summary": "TGraphX presents a novel paradigm in deep learning by unifying convolutional\nneural networks (CNNs) with graph neural networks (GNNs) to enhance visual\nreasoning tasks. Traditional CNNs excel at extracting rich spatial features\nfrom images but lack the inherent capability to model inter-object\nrelationships. Conversely, conventional GNNs typically rely on flattened node\nfeatures, thereby discarding vital spatial details. TGraphX overcomes these\nlimitations by employing CNNs to generate multi-dimensional node features\n(e.g., (3*128*128) tensors) that preserve local spatial semantics. These\nspatially aware nodes participate in a graph where message passing is performed\nusing 1*1 convolutions, which fuse adjacent features while maintaining their\nstructure. Furthermore, a deep CNN aggregator with residual connections is used\nto robustly refine the fused messages, ensuring stable gradient flow and\nend-to-end trainability. Our approach not only bridges the gap between spatial\nfeature extraction and relational reasoning but also demonstrates significant\nimprovements in object detection refinement and ensemble reasoning."}
{"id": "2504.04060", "pdf": "https://arxiv.org/pdf/2504.04060", "abs": "https://arxiv.org/abs/2504.04060", "authors": ["Yuhao Wang", "Heyang Liu", "Ziyang Cheng", "Ronghua Wu", "Qunshan Gu", "Yanfeng Wang", "Yu Wang"], "title": "VocalNet: Speech LLM with Multi-Token Prediction for Faster and High-Quality Generation", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Speech large language models (LLMs) have emerged as a prominent research\nfocus in speech processing. We propose VocalNet-1B and VocalNet-8B, a series of\nhigh-performance, low-latency speech LLMs enabled by a scalable and\nmodel-agnostic training framework for real-time voice interaction. Departing\nfrom the conventional next-token prediction (NTP), we introduce multi-token\nprediction (MTP), a novel approach optimized for speech LLMs that\nsimultaneously improves generation speed and quality. Experiments show that\nVocalNet outperforms mainstream Omni LLMs despite using significantly less\ntraining data, while also surpassing existing open-source speech LLMs by a\nsubstantial margin. To support reproducibility and community advancement, we\nwill open-source all model weights, inference code, training data, and\nframework implementations upon publication."}
{"id": "2504.03970", "pdf": "https://arxiv.org/pdf/2504.03970", "abs": "https://arxiv.org/abs/2504.03970", "authors": ["Dahun Kim", "AJ Piergiovanni", "Ganesh Mallya", "Anelia Angelova"], "title": "VideoComp: Advancing Fine-Grained Compositional and Temporal Alignment in Video-Text Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR"], "comment": "CVPR 2025, project page at\n  https://github.com/google-deepmind/video_comp", "summary": "We introduce VideoComp, a benchmark and learning framework for advancing\nvideo-text compositionality understanding, aimed at improving vision-language\nmodels (VLMs) in fine-grained temporal alignment. Unlike existing benchmarks\nfocused on static image-text compositionality or isolated single-event videos,\nour benchmark targets alignment in continuous multi-event videos. Leveraging\nvideo-text datasets with temporally localized event captions (e.g.\nActivityNet-Captions, YouCook2), we construct two compositional benchmarks,\nActivityNet-Comp and YouCook2-Comp. We create challenging negative samples with\nsubtle temporal disruptions such as reordering, action word replacement,\npartial captioning, and combined disruptions. These benchmarks comprehensively\ntest models' compositional sensitivity across extended, cohesive video-text\nsequences. To improve model performance, we propose a hierarchical pairwise\npreference loss that strengthens alignment with temporally accurate pairs and\ngradually penalizes increasingly disrupted ones, encouraging fine-grained\ncompositional learning. To mitigate the limited availability of densely\nannotated video data, we introduce a pretraining strategy that concatenates\nshort video-caption pairs to simulate multi-event sequences. We evaluate\nvideo-text foundational models and large multimodal models (LMMs) on our\nbenchmark, identifying both strengths and areas for improvement in\ncompositionality. Overall, our work provides a comprehensive framework for\nevaluating and enhancing model capabilities in achieving fine-grained,\ntemporally coherent video-text alignment."}
{"id": "2504.04076", "pdf": "https://arxiv.org/pdf/2504.04076", "abs": "https://arxiv.org/abs/2504.04076", "authors": ["Bing Wang", "Bingrui Zhao", "Ximing Li", "Changchun Li", "Wanfu Gao", "Shengsheng Wang"], "title": "Collaboration and Controversy Among Experts: Rumor Early Detection by Tuning a Comment Generator", "categories": ["cs.CL", "cs.SI"], "comment": "11 pages, 5 figures. Accepted by SIGIR 2025. Code:\n  https://github.com/wangbing1416/CAMERED", "summary": "Over the past decade, social media platforms have been key in spreading\nrumors, leading to significant negative impacts. To counter this, the community\nhas developed various Rumor Detection (RD) algorithms to automatically identify\nthem using user comments as evidence. However, these RD methods often fail in\nthe early stages of rumor propagation when only limited user comments are\navailable, leading the community to focus on a more challenging topic named\nRumor Early Detection (RED). Typically, existing RED methods learn from limited\nsemantics in early comments. However, our preliminary experiment reveals that\nthe RED models always perform best when the number of training and test\ncomments is consistent and extensive. This inspires us to address the RED issue\nby generating more human-like comments to support this hypothesis. To implement\nthis idea, we tune a comment generator by simulating expert collaboration and\ncontroversy and propose a new RED framework named CAMERED. Specifically, we\nintegrate a mixture-of-expert structure into a generative language model and\npresent a novel routing network for expert collaboration. Additionally, we\nsynthesize a knowledgeable dataset and design an adversarial learning strategy\nto align the style of generated comments with real-world comments. We further\nintegrate generated and original comments with a mutual controversy fusion\nmodule. Experimental results show that CAMERED outperforms state-of-the-art RED\nbaseline models and generation methods, demonstrating its effectiveness."}
{"id": "2504.04001", "pdf": "https://arxiv.org/pdf/2504.04001", "abs": "https://arxiv.org/abs/2504.04001", "authors": ["Chuang Yang", "Xu Han", "Tao Han", "Han Han", "Bingxuan Zhao", "Qi Wang"], "title": "Edge Approximation Text Detector", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Pursuing efficient text shape representations helps scene text detection\nmodels focus on compact foreground regions and optimize the contour\nreconstruction steps to simplify the whole detection pipeline. Current\napproaches either represent irregular shapes via box-to-polygon strategy or\ndecomposing a contour into pieces for fitting gradually, the deficiency of\ncoarse contours or complex pipelines always exists in these models. Considering\nthe above issues, we introduce EdgeText to fit text contours compactly while\nalleviating excessive contour rebuilding processes. Concretely, it is observed\nthat the two long edges of texts can be regarded as smooth curves. It allows us\nto build contours via continuous and smooth edges that cover text regions\ntightly instead of fitting piecewise, which helps avoid the two limitations in\ncurrent models. Inspired by this observation, EdgeText formulates the text\nrepresentation as the edge approximation problem via parameterized curve\nfitting functions. In the inference stage, our model starts with locating text\ncenters, and then creating curve functions for approximating text edges relying\non the points. Meanwhile, truncation points are determined based on the\nlocation features. In the end, extracting curve segments from curve functions\nby using the pixel coordinate information brought by truncation points to\nreconstruct text contours. Furthermore, considering the deep dependency of\nEdgeText on text edges, a bilateral enhanced perception (BEP) module is\ndesigned. It encourages our model to pay attention to the recognition of edge\nfeatures. Additionally, to accelerate the learning of the curve function\nparameters, we introduce a proportional integral loss (PI-loss) to force the\nproposed model to focus on the curve distribution and avoid being disturbed by\ntext scales."}
{"id": "2504.04083", "pdf": "https://arxiv.org/pdf/2504.04083", "abs": "https://arxiv.org/abs/2504.04083", "authors": ["Aviv Brokman", "Xuguang Ai", "Yuhang Jiang", "Shashank Gupta", "Ramakanth Kavuluru"], "title": "A Benchmark for End-to-End Zero-Shot Biomedical Relation Extraction with LLMs: Experiments with OpenAI Models", "categories": ["cs.CL"], "comment": null, "summary": "Objective: Zero-shot methodology promises to cut down on costs of dataset\nannotation and domain expertise needed to make use of NLP. Generative large\nlanguage models trained to align with human goals have achieved high zero-shot\nperformance across a wide variety of tasks. As of yet, it is unclear how well\nthese models perform on biomedical relation extraction (RE). To address this\nknowledge gap, we explore patterns in the performance of OpenAI LLMs across a\ndiverse sampling of RE tasks.\n  Methods: We use OpenAI GPT-4-turbo and their reasoning model o1 to conduct\nend-to-end RE experiments on seven datasets. We use the JSON generation\ncapabilities of GPT models to generate structured output in two ways: (1) by\ndefining an explicit schema describing the structure of relations, and (2)\nusing a setting that infers the structure from the prompt language.\n  Results: Our work is the first to study and compare the performance of the\nGPT-4 and o1 for the end-to-end zero-shot biomedical RE task across a broad\narray of datasets. We found the zero-shot performances to be proximal to that\nof fine-tuned methods. The limitations of this approach are that it performs\npoorly on instances containing many relations and errs on the boundaries of\ntextual mentions.\n  Conclusion: Recent large language models exhibit promising zero-shot\ncapabilities in complex biomedical RE tasks, offering competitive performance\nwith reduced dataset curation and NLP modeling needs at the cost of increased\ncomputing, potentially increasing medical community accessibility. Addressing\nthe limitations we identify could further boost reliability. The code, data,\nand prompts for all our experiments are publicly available:\nhttps://github.com/bionlproc/ZeroShotRE"}
{"id": "2504.04010", "pdf": "https://arxiv.org/pdf/2504.04010", "abs": "https://arxiv.org/abs/2504.04010", "authors": ["Maksim Siniukov", "Di Chang", "Minh Tran", "Hongkun Gong", "Ashutosh Chaubey", "Mohammad Soleymani"], "title": "DiTaiListener: Controllable High Fidelity Listener Video Generation with Diffusion", "categories": ["cs.CV", "cs.LG", "I.4.9"], "comment": "Project page: https://havent-invented.github.io/DiTaiListener", "summary": "Generating naturalistic and nuanced listener motions for extended\ninteractions remains an open problem. Existing methods often rely on\nlow-dimensional motion codes for facial behavior generation followed by\nphotorealistic rendering, limiting both visual fidelity and expressive\nrichness. To address these challenges, we introduce DiTaiListener, powered by a\nvideo diffusion model with multimodal conditions. Our approach first generates\nshort segments of listener responses conditioned on the speaker's speech and\nfacial motions with DiTaiListener-Gen. It then refines the transitional frames\nvia DiTaiListener-Edit for a seamless transition. Specifically,\nDiTaiListener-Gen adapts a Diffusion Transformer (DiT) for the task of listener\nhead portrait generation by introducing a Causal Temporal Multimodal Adapter\n(CTM-Adapter) to process speakers' auditory and visual cues. CTM-Adapter\nintegrates speakers' input in a causal manner into the video generation process\nto ensure temporally coherent listener responses. For long-form video\ngeneration, we introduce DiTaiListener-Edit, a transition refinement\nvideo-to-video diffusion model. The model fuses video segments into smooth and\ncontinuous videos, ensuring temporal consistency in facial expressions and\nimage quality when merging short video segments produced by DiTaiListener-Gen.\nQuantitatively, DiTaiListener achieves the state-of-the-art performance on\nbenchmark datasets in both photorealism (+73.8% in FID on RealTalk) and motion\nrepresentation (+6.1% in FD metric on VICO) spaces. User studies confirm the\nsuperior performance of DiTaiListener, with the model being the clear\npreference in terms of feedback, diversity, and smoothness, outperforming\ncompetitors by a significant margin."}
{"id": "2504.04131", "pdf": "https://arxiv.org/pdf/2504.04131", "abs": "https://arxiv.org/abs/2504.04131", "authors": ["Michael J Bommarito", "Daniel Martin Katz", "Jillian Bommarito"], "title": "Precise Legal Sentence Boundary Detection for Retrieval at Scale: NUPunkt and CharBoundary", "categories": ["cs.CL"], "comment": "12 pages, 5 figures, 6 tables", "summary": "We present NUPunkt and CharBoundary, two sentence boundary detection\nlibraries optimized for high-precision, high-throughput processing of legal\ntext in large-scale applications such as due diligence, e-discovery, and legal\nresearch. These libraries address the critical challenges posed by legal\ndocuments containing specialized citations, abbreviations, and complex sentence\nstructures that confound general-purpose sentence boundary detectors.\n  Our experimental evaluation on five diverse legal datasets comprising over\n25,000 documents and 197,000 annotated sentence boundaries demonstrates that\nNUPunkt achieves 91.1% precision while processing 10 million characters per\nsecond with modest memory requirements (432 MB). CharBoundary models offer\nbalanced and adjustable precision-recall tradeoffs, with the large model\nachieving the highest F1 score (0.782) among all tested methods.\n  Notably, NUPunkt provides a 29-32% precision improvement over general-purpose\ntools while maintaining exceptional throughput, processing multi-million\ndocument collections in minutes rather than hours. Both libraries run\nefficiently on standard CPU hardware without requiring specialized\naccelerators. NUPunkt is implemented in pure Python with zero external\ndependencies, while CharBoundary relies only on scikit-learn and optional ONNX\nruntime integration for optimized performance. Both libraries are available\nunder the MIT license, can be installed via PyPI, and can be interactively\ntested at https://sentences.aleainstitute.ai/.\n  These libraries address critical precision issues in retrieval-augmented\ngeneration systems by preserving coherent legal concepts across sentences,\nwhere each percentage improvement in precision yields exponentially greater\nreductions in context fragmentation, creating cascading benefits throughout\nretrieval pipelines and significantly enhancing downstream reasoning quality."}
{"id": "2504.04012", "pdf": "https://arxiv.org/pdf/2504.04012", "abs": "https://arxiv.org/abs/2504.04012", "authors": ["Houzhang Fang", "Xiaolin Wang", "Zengyang Li", "Lu Wang", "Qingshan Li", "Yi Chang", "Luxin Yan"], "title": "Detection-Friendly Nonuniformity Correction: A Union Framework for Infrared UAVTarget Detection", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted by CVPR2025", "summary": "Infrared unmanned aerial vehicle (UAV) images captured using thermal\ndetectors are often affected by temperature dependent low-frequency\nnonuniformity, which significantly reduces the contrast of the images.\nDetecting UAV targets under nonuniform conditions is crucial in UAV\nsurveillance applications. Existing methods typically treat infrared\nnonuniformity correction (NUC) as a preprocessing step for detection, which\nleads to suboptimal performance. Balancing the two tasks while enhancing\ndetection beneficial information remains challenging. In this paper, we present\na detection-friendly union framework, termed UniCD, that simultaneously\naddresses both infrared NUC and UAV target detection tasks in an end-to-end\nmanner. We first model NUC as a small number of parameter estimation problem\njointly driven by priors and data to generate detection-conducive images. Then,\nwe incorporate a new auxiliary loss with target mask supervision into the\nbackbone of the infrared UAV target detection network to strengthen target\nfeatures while suppressing the background. To better balance correction and\ndetection, we introduce a detection-guided self-supervised loss to reduce\nfeature discrepancies between the two tasks, thereby enhancing detection\nrobustness to varying nonuniformity levels. Additionally, we construct a new\nbenchmark composed of 50,000 infrared images in various nonuniformity types,\nmulti-scale UAV targets and rich backgrounds with target annotations, called\nIRBFD. Extensive experiments on IRBFD demonstrate that our UniCD is a robust\nunion framework for NUC and UAV target detection while achieving real-time\nprocessing capabilities. Dataset can be available at\nhttps://github.com/IVPLaboratory/UniCD."}
{"id": "2504.04141", "pdf": "https://arxiv.org/pdf/2504.04141", "abs": "https://arxiv.org/abs/2504.04141", "authors": ["Yougang Lyu", "Shijie Ren", "Yue Feng", "Zihan Wang", "Zhumin Chen", "Zhaochun Ren", "Maarten de Rijke"], "title": "Cognitive Debiasing Large Language Models for Decision-Making", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown potential in supporting\ndecision-making applications, particularly as personal conversational\nassistants in the financial, healthcare, and legal domains. While prompt\nengineering strategies have enhanced the capabilities of LLMs in\ndecision-making, cognitive biases inherent to LLMs present significant\nchallenges. Cognitive biases are systematic patterns of deviation from norms or\nrationality in decision-making that can lead to the production of inaccurate\noutputs. Existing cognitive bias mitigation strategies assume that input\nprompts contain (exactly) one type of cognitive bias and therefore fail to\nperform well in realistic settings where there maybe any number of biases.\n  To fill this gap, we propose a cognitive debiasing approach, called\nself-debiasing, that enhances the reliability of LLMs by iteratively refining\nprompts. Our method follows three sequential steps -- bias determination, bias\nanalysis, and cognitive debiasing -- to iteratively mitigate potential\ncognitive biases in prompts. Experimental results on finance, healthcare, and\nlegal decision-making tasks, using both closed-source and open-source LLMs,\ndemonstrate that the proposed self-debiasing method outperforms both advanced\nprompt engineering methods and existing cognitive debiasing techniques in\naverage accuracy under no-bias, single-bias, and multi-bias settings."}
{"id": "2504.04024", "pdf": "https://arxiv.org/pdf/2504.04024", "abs": "https://arxiv.org/abs/2504.04024", "authors": ["Yifan Li", "Wentao Bao", "Botao Ye", "Zhen Tan", "Tianlong Chen", "Huan Liu", "Yu Kong"], "title": "Window Token Concatenation for Efficient Visual Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "To effectively reduce the visual tokens in Visual Large Language Models\n(VLLMs), we propose a novel approach called Window Token Concatenation (WiCo).\nSpecifically, we employ a sliding window to concatenate spatially adjacent\nvisual tokens. However, directly concatenating these tokens may group diverse\ntokens into one, and thus obscure some fine details. To address this challenge,\nwe propose fine-tuning the last few layers of the vision encoder to adaptively\nadjust the visual tokens, encouraging that those within the same window exhibit\nsimilar features. To further enhance the performance on fine-grained visual\nunderstanding tasks, we introduce WiCo+, which decomposes the visual tokens in\nlater layers of the LLM. Such a design enjoys the merits of the large\nperception field of the LLM for fine-grained visual understanding while keeping\na small number of visual tokens for efficient inference. We perform extensive\nexperiments on both coarse- and fine-grained visual understanding tasks based\non LLaVA-1.5 and Shikra, showing better performance compared with existing\ntoken reduction projectors. The code is available:\nhttps://github.com/JackYFL/WiCo."}
{"id": "2504.04142", "pdf": "https://arxiv.org/pdf/2504.04142", "abs": "https://arxiv.org/abs/2504.04142", "authors": ["Kees van Deemter"], "title": "My Life in Artificial Intelligence: People, anecdotes, and some lessons learnt", "categories": ["cs.CL", "cs.AI"], "comment": "34 pages", "summary": "In this very personal workography, I relate my 40-year experiences as a\nresearcher and educator in and around Artificial Intelligence (AI), more\nspecifically Natural Language Processing. I describe how curiosity, and the\ncircumstances of the day, led me to work in both industry and academia, and in\nvarious countries, including The Netherlands (Amsterdam, Eindhoven, and\nUtrecht), the USA (Stanford), England (Brighton), Scotland (Aberdeen), and\nChina (Beijing and Harbin). People and anecdotes play a large role in my story;\nthe history of AI forms its backdrop. I focus on things that might be of\ninterest to (even) younger colleagues, given the choices they face in their own\nwork and life at a time when AI is finally emerging from the shadows."}
{"id": "2504.04025", "pdf": "https://arxiv.org/pdf/2504.04025", "abs": "https://arxiv.org/abs/2504.04025", "authors": ["Daniel Rivera", "Jacob Huddin", "Alexander Banerjee", "Rongzhen Zhang", "Brenda Mai", "Hanadi El Achi", "Jacob Armstrong", "Amer Wahed", "Andy Nguyen"], "title": "Artificial intelligence application in lymphoma diagnosis: from Convolutional Neural Network to Vision Transformer", "categories": ["cs.CV", "cs.LG"], "comment": "14 pages, 6 figures, 1 table", "summary": "Recently, vision transformers were shown to be capable of outperforming\nconvolutional neural networks when pretrained on sufficiently large datasets.\nVision transformer models show good accuracy on large scale datasets, with\nfeatures of multi-modal training. Due to their promising feature detection, we\naim to explore vision transformer models for diagnosis of anaplastic large cell\nlymphoma versus classical Hodgkin lymphoma using pathology whole slide images\nof HE slides. We compared the classification performance of the vision\ntransformer to our previously designed convolutional neural network on the same\ndataset. The dataset includes whole slide images of HE slides for 20 cases,\nincluding 10 cases in each diagnostic category. From each whole slide image, 60\nimage patches having size of 100 by 100 pixels and at magnification of 20 were\nobtained to yield 1200 image patches, from which 90 percent were used for\ntraining, 9 percent for validation, and 10 percent for testing. The test\nresults from the convolutional neural network model had previously shown an\nexcellent diagnostic accuracy of 100 percent. The test results from the vision\ntransformer model also showed a comparable accuracy at 100 percent. To the best\nof the authors' knowledge, this is the first direct comparison of predictive\nperformance between a vision transformer model and a convolutional neural\nnetwork model using the same dataset of lymphoma. Overall, convolutional neural\nnetwork has a more mature architecture than vision transformer and is usually\nthe best choice when large scale pretraining is not an available option.\nNevertheless, our current study shows comparable and excellent accuracy of\nvision transformer compared to that of convolutional neural network even with a\nrelatively small dataset of anaplastic large cell lymphoma and classical\nHodgkin lymphoma."}
{"id": "2504.04150", "pdf": "https://arxiv.org/pdf/2504.04150", "abs": "https://arxiv.org/abs/2504.04150", "authors": ["Yidong Wang"], "title": "Reasoning on Multiple Needles In A Haystack", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The Needle In A Haystack (NIAH) task has been widely used to evaluate the\nlong-context question-answering capabilities of Large Language Models (LLMs).\nHowever, its reliance on simple retrieval limits its effectiveness. To address\nthis limitation, recent studies have introduced the Multiple Needles In A\nHaystack Reasoning (MNIAH-R) task, which incorporates supporting documents\n(Multiple needles) of multi-hop reasoning tasks into a distracting context\n(Haystack}). Despite this advancement, existing approaches still fail to\naddress the issue of models providing direct answers from internal knowledge,\nand they do not explain or mitigate the decline in accuracy as context length\nincreases. In this paper, we tackle the memory-based answering problem by\nfiltering out direct-answer questions, and we reveal that performance\ndegradation is primarily driven by the reduction in the length of the thinking\nprocess as the input length increases. Building on this insight, we decompose\nthe thinking process into retrieval and reasoning stages and introduce a\nreflection mechanism for multi-round extension. We also train a model using the\ngenerated iterative thinking process, which helps mitigate the performance\ndegradation. Furthermore, we demonstrate the application of this\nretrieval-reflection capability in mathematical reasoning scenarios, improving\nGPT-4o's performance on AIME2024."}
{"id": "2504.04029", "pdf": "https://arxiv.org/pdf/2504.04029", "abs": "https://arxiv.org/abs/2504.04029", "authors": ["Shintaro Shiba", "Yoshimitsu Aoki", "Guillermo Gallego"], "title": "Simultaneous Motion And Noise Estimation with Event Cameras", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": "13 pages, 13 figures, 6 tables", "summary": "Event cameras are emerging vision sensors, whose noise is challenging to\ncharacterize. Existing denoising methods for event cameras consider other tasks\nsuch as motion estimation separately (i.e., sequentially after denoising).\nHowever, motion is an intrinsic part of event data, since scene edges cannot be\nsensed without motion. This work proposes, to the best of our knowledge, the\nfirst method that simultaneously estimates motion in its various forms (e.g.,\nego-motion, optical flow) and noise. The method is flexible, as it allows\nreplacing the 1-step motion estimation of the widely-used Contrast Maximization\nframework with any other motion estimator, such as deep neural networks. The\nexperiments show that the proposed method achieves state-of-the-art results on\nthe E-MLB denoising benchmark and competitive results on the DND21 benchmark,\nwhile showing its efficacy on motion estimation and intensity reconstruction\ntasks. We believe that the proposed approach contributes to strengthening the\ntheory of event-data denoising, as well as impacting practical denoising\nuse-cases, as we release the code upon acceptance. Project page:\nhttps://github.com/tub-rip/ESMD"}
{"id": "2504.04151", "pdf": "https://arxiv.org/pdf/2504.04151", "abs": "https://arxiv.org/abs/2504.04151", "authors": ["Kazuki Yano", "Takumi Ito", "Jun Suzuki"], "title": "STEP: Staged Parameter-Efficient Pre-training for Large Language Models", "categories": ["cs.CL"], "comment": "Accepted to NAACL 2025 Main", "summary": "Pre-training large language models (LLMs) faces significant memory challenges\ndue to the large size of model parameters. We introduce STaged\nparameter-Efficient Pre-training (STEP), which integrates parameter-efficient\ntuning techniques with model growth. We conduct experiments on pre-training\nLLMs of various sizes and demonstrate that STEP achieves up to a 53.9%\nreduction in maximum memory requirements compared to vanilla pre-training while\nmaintaining equivalent performance. Furthermore, we show that the model by STEP\nperforms comparably to vanilla pre-trained models on downstream tasks after\ninstruction tuning."}
{"id": "2504.04034", "pdf": "https://arxiv.org/pdf/2504.04034", "abs": "https://arxiv.org/abs/2504.04034", "authors": ["Dianshuo Li", "Li Chen", "Yunxiang Cao", "Kai Zhu", "Jun Cheng"], "title": "UCS: A Universal Model for Curvilinear Structure Segmentation", "categories": ["cs.CV"], "comment": "11 pages, 9 figures", "summary": "Curvilinear structure segmentation (CSS) is vital in various domains,\nincluding medical imaging, landscape analysis, industrial surface inspection,\nand plant analysis. While existing methods achieve high performance within\nspecific domains, their generalizability is limited. On the other hand,\nlarge-scale models such as Segment Anything Model (SAM) exhibit strong\ngeneralization but are not optimized for curvilinear structures. Existing\nadaptations of SAM primarily focus on general object segmentation and lack\nspecialized design for CSS tasks. To bridge this gap, we propose the Universal\nCurvilinear structure Segmentation (\\textit{UCS}) model, which adapts SAM to\nCSS tasks while enhancing its generalization. \\textit{UCS} features a novel\nencoder architecture integrating a pretrained SAM encoder with two innovations:\na Sparse Adapter, strategically inserted to inherit the pre-trained SAM\nencoder's generalization capability while minimizing the number of fine-tuning\nparameters, and a Prompt Generation module, which leverages Fast Fourier\nTransform with a high-pass filter to generate curve-specific prompts.\nFurthermore, the \\textit{UCS} incorporates a mask decoder that eliminates\nreliance on manual interaction through a dual-compression module: a\nHierarchical Feature Compression module, which aggregates the outputs of the\nsampled encoder to enhance detail preservation, and a Guidance Feature\nCompression module, which extracts and compresses image-driven guidance\nfeatures. Evaluated on a comprehensive multi-domain dataset, including an\nin-house dataset covering eight natural curvilinear structures, \\textit{UCS}\ndemonstrates state-of-the-art generalization and open-set segmentation\nperformance across medical, engineering, natural, and plant imagery,\nestablishing a new benchmark for universal CSS."}
{"id": "2504.04152", "pdf": "https://arxiv.org/pdf/2504.04152", "abs": "https://arxiv.org/abs/2504.04152", "authors": ["Zihao Li", "Shaoxiong Ji", "Hengyu Luo", "J√∂rg Tiedemann"], "title": "Rethinking Multilingual Continual Pretraining: Data Mixing for Adapting LLMs Across Languages and Resources", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) exhibit significant disparities in performance\nacross languages, primarily benefiting high-resource languages while\nmarginalizing underrepresented ones. Continual Pretraining (CPT) has emerged as\na promising approach to address this imbalance, although the relative\neffectiveness of monolingual, bilingual, and code-augmented data strategies\nremains unclear. This study systematically evaluates 36 CPT configurations\ninvolving three multilingual base models, across 30+ languages categorized as\naltruistic, selfish, and stagnant, spanning various resource levels. Our\nfindings reveal three major insights: (1) Bilingual CPT improves multilingual\nclassification but often causes language mixing issues during generation. (2)\nIncluding programming code data during CPT consistently enhances multilingual\nclassification accuracy, particularly benefiting low-resource languages, but\nintroduces a trade-off by slightly degrading generation quality. (3) Contrary\nto prior work, we observe substantial deviations from language classifications\naccording to their impact on cross-lingual transfer: Languages classified as\naltruistic often negatively affect related languages, selfish languages show\nconditional and configuration-dependent behavior, and stagnant languages\ndemonstrate surprising adaptability under certain CPT conditions. These nuanced\ninteractions emphasize the complexity of multilingual representation learning,\nunderscoring the importance of systematic studies on generalizable language\nclassification to inform future multilingual CPT strategies."}
{"id": "2504.04045", "pdf": "https://arxiv.org/pdf/2504.04045", "abs": "https://arxiv.org/abs/2504.04045", "authors": ["Conghao Xiong", "Hao Chen", "Joseph J. Y. Sung"], "title": "A Survey of Pathology Foundation Model: Progress and Future Directions", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Computational pathology, analyzing whole slide images for automated cancer\ndiagnosis, relies on the multiple instance learning framework where performance\nheavily depends on the feature extractor and aggregator. Recent Pathology\nFoundation Models (PFMs), pretrained on large-scale histopathology data, have\nsignificantly enhanced capabilities of extractors and aggregators but lack\nsystematic analysis frameworks. This survey presents a hierarchical taxonomy\norganizing PFMs through a top-down philosophy that can be utilized to analyze\nFMs in any domain: model scope, model pretraining, and model design.\nAdditionally, we systematically categorize PFM evaluation tasks into\nslide-level, patch-level, multimodal, and biological tasks, providing\ncomprehensive benchmarking criteria. Our analysis identifies critical\nchallenges in both PFM development (pathology-specific methodology, end-to-end\npretraining, data-model scalability) and utilization (effective adaptation,\nmodel maintenance), paving the way for future directions in this promising\nfield. Resources referenced in this survey are available at\nhttps://github.com/BearCleverProud/AwesomeWSI."}
{"id": "2504.04155", "pdf": "https://arxiv.org/pdf/2504.04155", "abs": "https://arxiv.org/abs/2504.04155", "authors": ["Hengyu Luo", "Zihao Li", "Joseph Attieh", "Sawal Devkota", "Ona de Gibert", "Shaoxiong Ji", "Peiqin Lin", "Bhavani Sai Praneeth Varma Mantina", "Ananda Sreenidhi", "Ra√∫l V√°zquez", "Mengjie Wang", "Samea Yusofi", "J√∂rg Tiedemann"], "title": "GlotEval: A Test Suite for Massively Multilingual Evaluation of Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are advancing at an unprecedented pace globally,\nwith regions increasingly adopting these models for applications in their\nprimary language. Evaluation of these models in diverse linguistic\nenvironments, especially in low-resource languages, has become a major\nchallenge for academia and industry. Existing evaluation frameworks are\ndisproportionately focused on English and a handful of high-resource languages,\nthereby overlooking the realistic performance of LLMs in multilingual and\nlower-resource scenarios. To address this gap, we introduce GlotEval, a\nlightweight framework designed for massively multilingual evaluation.\nSupporting seven key tasks (machine translation, text classification,\nsummarization, open-ended generation, reading comprehension, sequence labeling,\nand intrinsic evaluation), spanning over dozens to hundreds of languages,\nGlotEval highlights consistent multilingual benchmarking, language-specific\nprompt templates, and non-English-centric machine translation. This enables a\nprecise diagnosis of model strengths and weaknesses in diverse linguistic\ncontexts. A multilingual translation case study demonstrates GlotEval's\napplicability for multilingual and language-specific evaluations."}
{"id": "2504.04051", "pdf": "https://arxiv.org/pdf/2504.04051", "abs": "https://arxiv.org/abs/2504.04051", "authors": ["Xuyang Guo", "Zekai Huang", "Jiayan Huo", "Yingyu Liang", "Zhenmei Shi", "Zhao Song", "Jiahao Zhang"], "title": "Can You Count to Nine? A Human Evaluation Benchmark for Counting Limits in Modern Text-to-Video Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Generative models have driven significant progress in a variety of AI tasks,\nincluding text-to-video generation, where models like Video LDM and Stable\nVideo Diffusion can produce realistic, movie-level videos from textual\ninstructions. Despite these advances, current text-to-video models still face\nfundamental challenges in reliably following human commands, particularly in\nadhering to simple numerical constraints. In this work, we present\nT2VCountBench, a specialized benchmark aiming at evaluating the counting\ncapability of SOTA text-to-video models as of 2025. Our benchmark employs\nrigorous human evaluations to measure the number of generated objects and\ncovers a diverse range of generators, covering both open-source and commercial\nmodels. Extensive experiments reveal that all existing models struggle with\nbasic numerical tasks, almost always failing to generate videos with an object\ncount of 9 or fewer. Furthermore, our comprehensive ablation studies explore\nhow factors like video style, temporal dynamics, and multilingual inputs may\ninfluence counting performance. We also explore prompt refinement techniques\nand demonstrate that decomposing the task into smaller subtasks does not easily\nalleviate these limitations. Our findings highlight important challenges in\ncurrent text-to-video generation and provide insights for future research aimed\nat improving adherence to basic numerical constraints."}
{"id": "2504.04204", "pdf": "https://arxiv.org/pdf/2504.04204", "abs": "https://arxiv.org/abs/2504.04204", "authors": ["Jimmy Wang", "Thomas Zollo", "Richard Zemel", "Hongseok Namkoong"], "title": "Adaptive Elicitation of Latent Information Using Natural Language", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Eliciting information to reduce uncertainty about a latent entity is a\ncritical task in many application domains, e.g., assessing individual student\nlearning outcomes, diagnosing underlying diseases, or learning user\npreferences. Though natural language is a powerful medium for this purpose,\nlarge language models (LLMs) and existing fine-tuning algorithms lack\nmechanisms for strategically gathering information to refine their own\nunderstanding of the latent entity. To harness the generalization power and\nworld knowledge of LLMs in developing effective information-gathering\nstrategies, we propose an adaptive elicitation framework that actively reduces\nuncertainty on the latent entity. Since probabilistic modeling of an abstract\nlatent entity is difficult, our framework adopts a predictive view of\nuncertainty, using a meta-learned language model to simulate future\nobservations and enable scalable uncertainty quantification over complex\nnatural language. Through autoregressive forward simulation, our model\nquantifies how new questions reduce epistemic uncertainty, enabling the\ndevelopment of sophisticated information-gathering strategies to choose the\nmost informative next queries. In experiments on the 20 questions game, dynamic\nopinion polling, and adaptive student assessment, our method consistently\noutperforms baselines in identifying critical unknowns and improving downstream\npredictions, illustrating the promise of strategic information gathering in\nnatural language settings."}
{"id": "2504.04065", "pdf": "https://arxiv.org/pdf/2504.04065", "abs": "https://arxiv.org/abs/2504.04065", "authors": ["Jiaqi Deng", "Kaize Shi", "Zonghan Wu", "Huan Huo", "Dingxian Wang", "Guandong Xu"], "title": "UniRVQA: A Unified Framework for Retrieval-Augmented Vision Question Answering via Self-Reflective Joint Training", "categories": ["cs.CV", "cs.IR", "cs.MM"], "comment": "10 pages, 5 figures", "summary": "Knowledge-based Vision Question Answering (KB-VQA) systems address complex\nvisual-grounded questions requiring external knowledge, such as web-sourced\nencyclopedia articles. Existing methods often use sequential and separate\nframeworks for the retriever and the generator with limited parametric\nknowledge sharing. However, since both retrieval and generation tasks require\naccurate understanding of contextual and external information, such separation\ncan potentially lead to suboptimal system performance. Another key challenge is\nthe integration of multimodal information. General-purpose multimodal\npre-trained models, while adept at multimodal representation learning, struggle\nwith fine-grained retrieval required for knowledge-intensive visual questions.\nRecent specialized pre-trained models mitigate the issue, but are\ncomputationally expensive. To bridge the gap, we propose a Unified\nRetrieval-Augmented VQA framework (UniRVQA). UniRVQA adapts general multimodal\npre-trained models for fine-grained knowledge-intensive tasks within a unified\nframework, enabling cross-task parametric knowledge sharing and the extension\nof existing multimodal representation learning capability. We further introduce\na reflective-answering mechanism that allows the model to explicitly evaluate\nand refine its knowledge boundary. Additionally, we integrate late interaction\ninto the retrieval-augmented generation joint training process to enhance\nfine-grained understanding of queries and documents. Our approach achieves\ncompetitive performance against state-of-the-art models, delivering a\nsignificant 4.7% improvement in answering accuracy, and brings an average 7.5%\nboost in base MLLMs' VQA performance."}
{"id": "2504.04215", "pdf": "https://arxiv.org/pdf/2504.04215", "abs": "https://arxiv.org/abs/2504.04215", "authors": ["Vishnu Kabir Chhabra", "Mohammad Mahdi Khalili"], "title": "Towards Understanding and Improving Refusal in Compressed Models via Mechanistic Interpretability", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid growth of large language models has spurred significant interest in\nmodel compression as a means to enhance their accessibility and practicality.\nWhile extensive research has explored model compression through the lens of\nsafety, findings suggest that safety-aligned models often lose elements of\ntrustworthiness post-compression. Simultaneously, the field of mechanistic\ninterpretability has gained traction, with notable discoveries, such as the\nidentification of a single direction in the residual stream mediating refusal\nbehaviors across diverse model architectures. In this work, we investigate the\nsafety of compressed models by examining the mechanisms of refusal, adopting a\nnovel interpretability-driven perspective to evaluate model safety.\nFurthermore, leveraging insights from our interpretability analysis, we propose\na lightweight, computationally efficient method to enhance the safety of\ncompressed models without compromising their performance or utility."}
{"id": "2504.04085", "pdf": "https://arxiv.org/pdf/2504.04085", "abs": "https://arxiv.org/abs/2504.04085", "authors": ["Xiao-Hui Li", "Fei Yin", "Cheng-Lin Liu"], "title": "DocSAM: Unified Document Image Segmentation via Query Decomposition and Heterogeneous Mixed Learning", "categories": ["cs.CV", "cs.AI"], "comment": "This paper has been accepted by CVPR 2025", "summary": "Document image segmentation is crucial for document analysis and recognition\nbut remains challenging due to the diversity of document formats and\nsegmentation tasks. Existing methods often address these tasks separately,\nresulting in limited generalization and resource wastage. This paper introduces\nDocSAM, a transformer-based unified framework designed for various document\nimage segmentation tasks, such as document layout analysis, multi-granularity\ntext segmentation, and table structure recognition, by modelling these tasks as\na combination of instance and semantic segmentation. Specifically, DocSAM\nemploys Sentence-BERT to map category names from each dataset into semantic\nqueries that match the dimensionality of instance queries. These two sets of\nqueries interact through an attention mechanism and are cross-attended with\nimage features to predict instance and semantic segmentation masks. Instance\ncategories are predicted by computing the dot product between instance and\nsemantic queries, followed by softmax normalization of scores. Consequently,\nDocSAM can be jointly trained on heterogeneous datasets, enhancing robustness\nand generalization while reducing computational and storage resources.\nComprehensive evaluations show that DocSAM surpasses existing methods in\naccuracy, efficiency, and adaptability, highlighting its potential for\nadvancing document image understanding and segmentation across various\napplications. Codes are available at https://github.com/xhli-git/DocSAM."}
{"id": "2504.04216", "pdf": "https://arxiv.org/pdf/2504.04216", "abs": "https://arxiv.org/abs/2504.04216", "authors": ["Yuantao Zhang", "Zhankui Yang"], "title": "A Perplexity and Menger Curvature-Based Approach for Similarity Evaluation of Large Language Models", "categories": ["cs.CL"], "comment": "13 pages", "summary": "The rise of Large Language Models (LLMs) has brought about concerns regarding\ncopyright infringement and unethical practices in data and model usage. For\ninstance, slight modifications to existing LLMs may be used to falsely claim\nthe development of new models, leading to issues of model copying and\nviolations of ownership rights. This paper addresses these challenges by\nintroducing a novel metric for quantifying LLM similarity, which leverages\nperplexity curves and differences in Menger curvature. Comprehensive\nexperiments validate the performance of our methodology, demonstrating its\nsuperiority over baseline methods and its ability to generalize across diverse\nmodels and domains. Furthermore, we highlight the capability of our approach in\ndetecting model replication through simulations, emphasizing its potential to\npreserve the originality and integrity of LLMs. Code is available at\nhttps://github.com/zyttt-coder/LLM_similarity."}
{"id": "2504.04099", "pdf": "https://arxiv.org/pdf/2504.04099", "abs": "https://arxiv.org/abs/2504.04099", "authors": ["Chunzhao Xie", "Tongxuan Liu", "Lei Jiang", "Yuting Zeng", "jinrong Guo", "Yunheng Shen", "Weizhe Huang", "Jing Li", "Xiaohua Xu"], "title": "TARAC: Mitigating Hallucination in LVLMs via Temporal Attention Real-time Accumulative Connection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Vision-Language Models have demonstrated remarkable performance across\nvarious tasks; however, the challenge of hallucinations constrains their\npractical applications. The hallucination problem arises from multiple factors,\nincluding the inherent hallucinations in language models, the limitations of\nvisual encoders in perception, and biases introduced by multimodal data.\nExtensive research has explored ways to mitigate hallucinations. For instance,\nOPERA prevents the model from overly focusing on \"anchor tokens\", thereby\nreducing hallucinations, whereas VCD mitigates hallucinations by employing a\ncontrastive decoding approach. In this paper, we investigate the correlation\nbetween the decay of attention to image tokens and the occurrence of\nhallucinations. Based on this finding, we propose Temporal Attention Real-time\nAccumulative Connection (TARAC), a novel training-free method that dynamically\naccumulates and updates LVLMs' attention on image tokens during generation. By\nenhancing the model's attention to image tokens, TARAC mitigates hallucinations\ncaused by the decay of attention on image tokens. We validate the effectiveness\nof TARAC across multiple models and datasets, demonstrating that our approach\nsubstantially mitigates hallucinations. In particular, TARAC reduces $C_S$ by\n25.2 and $C_I$ by 8.7 compared to VCD on the CHAIR benchmark."}
{"id": "2504.04238", "pdf": "https://arxiv.org/pdf/2504.04238", "abs": "https://arxiv.org/abs/2504.04238", "authors": ["Yuheng Wu", "Wentao Guo", "Zirui Liu", "Heng Ji", "Zhaozhuo Xu", "Denghui Zhang"], "title": "Sensitivity Meets Sparsity: The Impact of Extremely Sparse Parameter Patterns on Theory-of-Mind of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper investigates the emergence of Theory-of-Mind (ToM) capabilities in\nlarge language models (LLMs) from a mechanistic perspective, focusing on the\nrole of extremely sparse parameter patterns. We introduce a novel method to\nidentify ToM-sensitive parameters and reveal that perturbing as little as\n0.001% of these parameters significantly degrades ToM performance while also\nimpairing contextual localization and language understanding. To understand\nthis effect, we analyze their interaction with core architectural components of\nLLMs. Our findings demonstrate that these sensitive parameters are closely\nlinked to the positional encoding module, particularly in models using Rotary\nPosition Embedding (RoPE), where perturbations disrupt dominant-frequency\nactivations critical for contextual processing. Furthermore, we show that\nperturbing ToM-sensitive parameters affects LLM's attention mechanism by\nmodulating the angle between queries and keys under positional encoding. These\ninsights provide a deeper understanding of how LLMs acquire social reasoning\nabilities, bridging AI interpretability with cognitive science. Our results\nhave implications for enhancing model alignment, mitigating biases, and\nimproving AI systems designed for human interaction."}
{"id": "2504.04124", "pdf": "https://arxiv.org/pdf/2504.04124", "abs": "https://arxiv.org/abs/2504.04124", "authors": ["Muhammad Ahmed Ullah Khan", "Abdul Hannan Khan", "Andreas Dengel"], "title": "EMF: Event Meta Formers for Event-based Real-time Traffic Object Detection", "categories": ["cs.CV"], "comment": "10 pages, 2 figures", "summary": "Event cameras have higher temporal resolution, and require less storage and\nbandwidth compared to traditional RGB cameras. However, due to relatively\nlagging performance of event-based approaches, event cameras have not yet\nreplace traditional cameras in performance-critical applications like\nautonomous driving. Recent approaches in event-based object detection try to\nbridge this gap by employing computationally expensive transformer-based\nsolutions. However, due to their resource-intensive components, these solutions\nfail to exploit the sparsity and higher temporal resolution of event cameras\nefficiently. Moreover, these solutions are adopted from the vision domain,\nlacking specificity to the event cameras. In this work, we explore efficient\nand performant alternatives to recurrent vision transformer models and propose\na novel event-based object detection backbone. The proposed backbone employs a\nnovel Event Progression Extractor module, tailored specifically for event data,\nand uses Metaformer concept with convolution-based efficient components. We\nevaluate the resultant model on well-established traffic object detection\nbenchmarks and conduct cross-dataset evaluation to test its ability to\ngeneralize. The proposed model outperforms the state-of-the-art on Prophesee\nGen1 dataset by 1.6 mAP while reducing inference time by 14%. Our proposed EMF\nbecomes the fastest DNN-based architecture in the domain by outperforming most\nefficient event-based object detectors. Moreover, the proposed model shows\nbetter ability to generalize to unseen data and scales better with the\nabundance of data."}
{"id": "2504.04264", "pdf": "https://arxiv.org/pdf/2504.04264", "abs": "https://arxiv.org/abs/2504.04264", "authors": ["Mingyang Wang", "Heike Adel", "Lukas Lange", "Yihong Liu", "Ercong Nie", "Jannik Str√∂tgen", "Hinrich Sch√ºtze"], "title": "Lost in Multilinguality: Dissecting Cross-lingual Factual Inconsistency in Transformer Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Multilingual language models (MLMs) store factual knowledge across languages\nbut often struggle to provide consistent responses to semantically equivalent\nprompts in different languages. While previous studies point out this\ncross-lingual inconsistency issue, the underlying causes remain unexplored. In\nthis work, we use mechanistic interpretability methods to investigate\ncross-lingual inconsistencies in MLMs. We find that MLMs encode knowledge in a\nlanguage-independent concept space through most layers, and only transition to\nlanguage-specific spaces in the final layers. Failures during the language\ntransition often result in incorrect predictions in the target language, even\nwhen the answers are correct in other languages. To mitigate this inconsistency\nissue, we propose a linear shortcut method that bypasses computations in the\nfinal layers, enhancing both prediction accuracy and cross-lingual consistency.\nOur findings shed light on the internal mechanisms of MLMs and provide a\nlightweight, effective strategy for producing more consistent factual outputs."}
{"id": "2504.04126", "pdf": "https://arxiv.org/pdf/2504.04126", "abs": "https://arxiv.org/abs/2504.04126", "authors": ["Zhenzhi Wang", "Yixuan Li", "Yanhong Zeng", "Yuwei Guo", "Dahua Lin", "Tianfan Xue", "Bo Dai"], "title": "Multi-identity Human Image Animation with Structural Video Diffusion", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages", "summary": "Generating human videos from a single image while ensuring high visual\nquality and precise control is a challenging task, especially in complex\nscenarios involving multiple individuals and interactions with objects.\nExisting methods, while effective for single-human cases, often fail to handle\nthe intricacies of multi-identity interactions because they struggle to\nassociate the correct pairs of human appearance and pose condition and model\nthe distribution of 3D-aware dynamics. To address these limitations, we present\nStructural Video Diffusion, a novel framework designed for generating realistic\nmulti-human videos. Our approach introduces two core innovations:\nidentity-specific embeddings to maintain consistent appearances across\nindividuals and a structural learning mechanism that incorporates depth and\nsurface-normal cues to model human-object interactions. Additionally, we expand\nexisting human video dataset with 25K new videos featuring diverse multi-human\nand object interaction scenarios, providing a robust foundation for training.\nExperimental results demonstrate that Structural Video Diffusion achieves\nsuperior performance in generating lifelike, coherent videos for multiple\nsubjects with dynamic and rich interactions, advancing the state of\nhuman-centric video generation."}
{"id": "2504.04275", "pdf": "https://arxiv.org/pdf/2504.04275", "abs": "https://arxiv.org/abs/2504.04275", "authors": ["T√∫lio Sousa de Gois", "Paloma Batista Cardoso"], "title": "negativas: a prototype for searching and classifying sentential negation in speech data", "categories": ["cs.CL"], "comment": null, "summary": "Negation is a universal feature of natural languages. In Brazilian\nPortuguese, the most commonly used negation particle is n\\~ao, which can scope\nover nouns or verbs. When it scopes over a verb, n\\~ao can occur in three\npositions: pre-verbal (NEG1), double negation (NEG2), or post-verbal (NEG3),\ne.g., n\\~ao gosto, n\\~ao gosto n\\~ao, gosto n\\~ao (\"I do not like it\"). From a\nvariationist perspective, these structures are different forms of expressing\nnegation. Pragmatically, they serve distinct communicative functions, such as\npoliteness and modal evaluation. Despite their grammatical acceptability, these\nforms differ in frequency. NEG1 dominates across Brazilian regions, while NEG2\nand NEG3 appear more rarely, suggesting its use is contextually restricted.\nThis low-frequency challenges research, often resulting in subjective,\nnon-generalizable interpretations of verbal negation with n\\~ao. To address\nthis, we developed negativas, a tool for automatically identifying NEG1, NEG2,\nand NEG3 in transcribed data. The tool's development involved four stages: i)\nanalyzing a dataset of 22 interviews from the Falares Sergipanos database,\nannotated by three linguists, ii) creating a code using natural language\nprocessing (NLP) techniques, iii) running the tool, iv) evaluating accuracy.\nInter-annotator consistency, measured using Fleiss' Kappa, was moderate (0.57).\nThe tool identified 3,338 instances of n\\~ao, classifying 2,085 as NEG1, NEG2,\nor NEG3, achieving a 93% success rate. However, negativas has limitations. NEG1\naccounted for 91.5% of identified structures, while NEG2 and NEG3 represented\n7.2% and 1.2%, respectively. The tool struggled with NEG2, sometimes\nmisclassifying instances as overlapping structures (NEG1/NEG2/NEG3)."}
{"id": "2504.04130", "pdf": "https://arxiv.org/pdf/2504.04130", "abs": "https://arxiv.org/abs/2504.04130", "authors": ["Andrei-Alexandru Preda", "Iulian-Marius TƒÉiatu", "Dumitru-Clementin Cercel"], "title": "Scaling Federated Learning Solutions with Kubernetes for Synthesizing Histopathology Images", "categories": ["cs.CV"], "comment": null, "summary": "In the field of deep learning, large architectures often obtain the best\nperformance for many tasks, but also require massive datasets. In the\nhistological domain, tissue images are expensive to obtain and constitute\nsensitive medical information, raising concerns about data scarcity and\nprivacy. Vision Transformers are state-of-the-art computer vision models that\nhave proven helpful in many tasks, including image classification. In this\nwork, we combine vision Transformers with generative adversarial networks to\ngenerate histopathological images related to colorectal cancer and test their\nquality by augmenting a training dataset, leading to improved classification\naccuracy. Then, we replicate this performance using the federated learning\ntechnique and a realistic Kubernetes setup with multiple nodes, simulating a\nscenario where the training dataset is split among several hospitals unable to\nshare their information directly due to privacy concerns."}
{"id": "2504.04279", "pdf": "https://arxiv.org/pdf/2504.04279", "abs": "https://arxiv.org/abs/2504.04279", "authors": ["Hongchao Fang", "Can Qin", "Ran Xu", "Feng Liu", "Yixin Liu", "Lichao Sun", "Dongwon Lee", "Lifu Huang", "Wenpeng Yin"], "title": "Could AI Trace and Explain the Origins of AI-Generated Images and Text?", "categories": ["cs.CL"], "comment": null, "summary": "AI-generated content is becoming increasingly prevalent in the real world,\nleading to serious ethical and societal concerns. For instance, adversaries\nmight exploit large multimodal models (LMMs) to create images that violate\nethical or legal standards, while paper reviewers may misuse large language\nmodels (LLMs) to generate reviews without genuine intellectual effort. While\nprior work has explored detecting AI-generated images and texts, and\noccasionally tracing their source models, there is a lack of a systematic and\nfine-grained comparative study. Important dimensions--such as AI-generated\nimages vs. text, fully vs. partially AI-generated images, and general vs.\nmalicious use cases--remain underexplored. Furthermore, whether AI systems like\nGPT-4o can explain why certain forged content is attributed to specific\ngenerative models is still an open question, with no existing benchmark\naddressing this. To fill this gap, we introduce AI-FAKER, a comprehensive\nmultimodal dataset with over 280,000 samples spanning multiple LLMs and LMMs,\ncovering both general and malicious use cases for AI-generated images and\ntexts. Our experiments reveal two key findings: (i) AI authorship detection\ndepends not only on the generated output but also on the model's original\ntraining intent; and (ii) GPT-4o provides highly consistent but less specific\nexplanations when analyzing content produced by OpenAI's own models, such as\nDALL-E and GPT-4o itself."}
{"id": "2504.04156", "pdf": "https://arxiv.org/pdf/2504.04156", "abs": "https://arxiv.org/abs/2504.04156", "authors": ["Kai Fang", "Anqi Zhang", "Guangyu Gao", "Jianbo Jiao", "Chi Harold Liu", "Yunchao Wei"], "title": "CoMBO: Conflict Mitigation via Branched Optimization for Class Incremental Segmentation", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Effective Class Incremental Segmentation (CIS) requires simultaneously\nmitigating catastrophic forgetting and ensuring sufficient plasticity to\nintegrate new classes. The inherent conflict above often leads to a\nback-and-forth, which turns the objective into finding the balance between the\nperformance of previous~(old) and incremental~(new) classes. To address this\nconflict, we introduce a novel approach, Conflict Mitigation via Branched\nOptimization~(CoMBO). Within this approach, we present the Query Conflict\nReduction module, designed to explicitly refine queries for new classes through\nlightweight, class-specific adapters. This module provides an additional branch\nfor the acquisition of new classes while preserving the original queries for\ndistillation. Moreover, we develop two strategies to further mitigate the\nconflict following the branched structure, \\textit{i.e.}, the Half-Learning\nHalf-Distillation~(HDHL) over classification probabilities, and the\nImportance-Based Knowledge Distillation~(IKD) over query features. HDHL\nselectively engages in learning for classification probabilities of queries\nthat match the ground truth of new classes, while aligning unmatched ones to\nthe corresponding old probabilities, thus ensuring retention of old knowledge\nwhile absorbing new classes via learning negative samples. Meanwhile, IKD\nassesses the importance of queries based on their matching degree to old\nclasses, prioritizing the distillation of important features and allowing less\ncritical features to evolve. Extensive experiments in Class Incremental\nPanoptic and Semantic Segmentation settings have demonstrated the superior\nperformance of CoMBO. Project page: https://guangyu-ryan.github.io/CoMBO."}
{"id": "2504.04292", "pdf": "https://arxiv.org/pdf/2504.04292", "abs": "https://arxiv.org/abs/2504.04292", "authors": ["Jie Yang", "Yiqiu Tang", "Yongjie Li", "Lihua Zhang", "Haoran Zhang"], "title": "Cross-Asset Risk Management: Integrating LLMs for Real-Time Monitoring of Equity, Fixed Income, and Currency Markets", "categories": ["cs.CL", "cs.CE"], "comment": "Accepted by IJCNN 2025", "summary": "Large language models (LLMs) have emerged as powerful tools in the field of\nfinance, particularly for risk management across different asset classes. In\nthis work, we introduce a Cross-Asset Risk Management framework that utilizes\nLLMs to facilitate real-time monitoring of equity, fixed income, and currency\nmarkets. This innovative approach enables dynamic risk assessment by\naggregating diverse data sources, ultimately enhancing decision-making\nprocesses. Our model effectively synthesizes and analyzes market signals to\nidentify potential risks and opportunities while providing a holistic view of\nasset classes. By employing advanced analytics, we leverage LLMs to interpret\nfinancial texts, news articles, and market reports, ensuring that risks are\ncontextualized within broader market narratives. Extensive backtesting and\nreal-time simulations validate the framework, showing increased accuracy in\npredicting market shifts compared to conventional methods. The focus on\nreal-time data integration enhances responsiveness, allowing financial\ninstitutions to manage risks adeptly under varying market conditions and\npromoting financial stability through the advanced application of LLMs in risk\nanalysis."}
{"id": "2504.04158", "pdf": "https://arxiv.org/pdf/2504.04158", "abs": "https://arxiv.org/abs/2504.04158", "authors": ["Yunlong Lin", "Zixu Lin", "Haoyu Chen", "Panwang Pan", "Chenxin Li", "Sixiang Chen", "Yeying Jin", "Wenbo Li", "Xinghao Ding"], "title": "JarvisIR: Elevating Autonomous Driving Perception with Intelligent Image Restoration", "categories": ["cs.CV"], "comment": "25 pages, 15 figures", "summary": "Vision-centric perception systems struggle with unpredictable and coupled\nweather degradations in the wild. Current solutions are often limited, as they\neither depend on specific degradation priors or suffer from significant domain\ngaps. To enable robust and autonomous operation in real-world conditions, we\npropose JarvisIR, a VLM-powered agent that leverages the VLM as a controller to\nmanage multiple expert restoration models. To further enhance system\nrobustness, reduce hallucinations, and improve generalizability in real-world\nadverse weather, JarvisIR employs a novel two-stage framework consisting of\nsupervised fine-tuning and human feedback alignment. Specifically, to address\nthe lack of paired data in real-world scenarios, the human feedback alignment\nenables the VLM to be fine-tuned effectively on large-scale real-world data in\nan unsupervised manner. To support the training and evaluation of JarvisIR, we\nintroduce CleanBench, a comprehensive dataset consisting of high-quality and\nlarge-scale instruction-responses pairs, including 150K synthetic entries and\n80K real entries. Extensive experiments demonstrate that JarvisIR exhibits\nsuperior decision-making and restoration capabilities. Compared with existing\nmethods, it achieves a 50% improvement in the average of all perception metrics\non CleanBench-Real. Project page: https://cvpr2025-jarvisir.github.io/."}
{"id": "2504.04295", "pdf": "https://arxiv.org/pdf/2504.04295", "abs": "https://arxiv.org/abs/2504.04295", "authors": ["Jie Yang", "Yiqiu Tang", "Yongjie Li", "Lihua Zhang", "Haoran Zhang"], "title": "Dynamic Hedging Strategies in Derivatives Markets with LLM-Driven Sentiment and News Analytics", "categories": ["cs.CL", "cs.CE"], "comment": "Accepted by IJCNN 2025", "summary": "Dynamic hedging strategies are essential for effective risk management in\nderivatives markets, where volatility and market sentiment can greatly impact\nperformance. This paper introduces a novel framework that leverages large\nlanguage models (LLMs) for sentiment analysis and news analytics to inform\nhedging decisions. By analyzing textual data from diverse sources like news\narticles, social media, and financial reports, our approach captures critical\nsentiment indicators that reflect current market conditions. The framework\nallows for real-time adjustments to hedging strategies, adapting positions\nbased on continuous sentiment signals. Backtesting results on historical\nderivatives data reveal that our dynamic hedging strategies achieve superior\nrisk-adjusted returns compared to conventional static approaches. The\nincorporation of LLM-driven sentiment analysis into hedging practices presents\na significant advancement in decision-making processes within derivatives\ntrading. This research showcases how sentiment-informed dynamic hedging can\nenhance portfolio management and effectively mitigate associated risks."}
{"id": "2504.04185", "pdf": "https://arxiv.org/pdf/2504.04185", "abs": "https://arxiv.org/abs/2504.04185", "authors": ["Dong Liu", "Yuanchao Wu", "Bowen Tong", "Jiansong Deng"], "title": "SDEIT: Semantic-Driven Electrical Impedance Tomography", "categories": ["cs.CV"], "comment": null, "summary": "Regularization methods using prior knowledge are essential in solving\nill-posed inverse problems such as Electrical Impedance Tomography (EIT).\nHowever, designing effective regularization and integrating prior information\ninto EIT remains challenging due to the complexity and variability of\nanatomical structures. In this work, we introduce SDEIT, a novel\nsemantic-driven framework that integrates Stable Diffusion 3.5 into EIT,\nmarking the first use of large-scale text-to-image generation models in EIT.\nSDEIT employs natural language prompts as semantic priors to guide the\nreconstruction process. By coupling an implicit neural representation (INR)\nnetwork with a plug-and-play optimization scheme that leverages SD-generated\nimages as generative priors, SDEIT improves structural consistency and recovers\nfine details. Importantly, this method does not rely on paired training\ndatasets, increasing its adaptability to varied EIT scenarios. Extensive\nexperiments on both simulated and experimental data demonstrate that SDEIT\noutperforms state-of-the-art techniques, offering superior accuracy and\nrobustness. This work opens a new pathway for integrating multimodal priors\ninto ill-posed inverse problems like EIT."}
{"id": "2504.04310", "pdf": "https://arxiv.org/pdf/2504.04310", "abs": "https://arxiv.org/abs/2504.04310", "authors": ["Weiwei Sun", "Shengyu Feng", "Shanda Li", "Yiming Yang"], "title": "CO-Bench: Benchmarking Language Model Agents in Algorithm Search for Combinatorial Optimization", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Although LLM-based agents have attracted significant attention in domains\nsuch as software engineering and machine learning research, their role in\nadvancing combinatorial optimization (CO) remains relatively underexplored.\nThis gap underscores the need for a deeper understanding of their potential in\ntackling structured, constraint-intensive problems-a pursuit currently limited\nby the absence of comprehensive benchmarks for systematic investigation. To\naddress this, we introduce CO-Bench, a benchmark suite featuring 36 real-world\nCO problems drawn from a broad range of domains and complexity levels. CO-Bench\nincludes structured problem formulations and curated data to support rigorous\ninvestigation of LLM agents. We evaluate multiple agent frameworks against\nestablished human-designed algorithms, revealing key strengths and limitations\nof current approaches and identifying promising directions for future research.\nCO-Bench is publicly available at https://github.com/sunnweiwei/CO-Bench."}
{"id": "2504.04190", "pdf": "https://arxiv.org/pdf/2504.04190", "abs": "https://arxiv.org/abs/2504.04190", "authors": ["Yuyang Zhang", "Baao Xie", "Hu Zhu", "Qi Wang", "Huanting Guo", "Xin Jin", "Wenjun Zeng"], "title": "Interpretable Single-View 3D Gaussian Splatting using Unsupervised Hierarchical Disentangled Representation Learning", "categories": ["cs.CV"], "comment": null, "summary": "Gaussian Splatting (GS) has recently marked a significant advancement in 3D\nreconstruction, delivering both rapid rendering and high-quality results.\nHowever, existing 3DGS methods pose challenges in understanding underlying 3D\nsemantics, which hinders model controllability and interpretability. To address\nit, we propose an interpretable single-view 3DGS framework, termed 3DisGS, to\ndiscover both coarse- and fine-grained 3D semantics via hierarchical\ndisentangled representation learning (DRL). Specifically, the model employs a\ndual-branch architecture, consisting of a point cloud initialization branch and\na triplane-Gaussian generation branch, to achieve coarse-grained\ndisentanglement by separating 3D geometry and visual appearance features.\nSubsequently, fine-grained semantic representations within each modality are\nfurther discovered through DRL-based encoder-adapters. To our knowledge, this\nis the first work to achieve unsupervised interpretable 3DGS. Evaluations\nindicate that our model achieves 3D disentanglement while preserving\nhigh-quality and rapid reconstruction."}
{"id": "2504.04314", "pdf": "https://arxiv.org/pdf/2504.04314", "abs": "https://arxiv.org/abs/2504.04314", "authors": ["Justin Miller", "Tristram Alexander"], "title": "Balancing Complexity and Informativeness in LLM-Based Clustering: Finding the Goldilocks Zone", "categories": ["cs.CL", "cs.AI", "math.ST", "stat.TH"], "comment": "12 pages, 4 figures, 2 tables", "summary": "The challenge of clustering short text data lies in balancing informativeness\nwith interpretability. Traditional evaluation metrics often overlook this\ntrade-off. Inspired by linguistic principles of communicative efficiency, this\npaper investigates the optimal number of clusters by quantifying the trade-off\nbetween informativeness and cognitive simplicity. We use large language models\n(LLMs) to generate cluster names and evaluate their effectiveness through\nsemantic density, information theory, and clustering accuracy. Our results show\nthat Gaussian Mixture Model (GMM) clustering on embeddings generated by a LLM,\nincreases semantic density compared to random assignment, effectively grouping\nsimilar bios. However, as clusters increase, interpretability declines, as\nmeasured by a generative LLM's ability to correctly assign bios based on\ncluster names. A logistic regression analysis confirms that classification\naccuracy depends on the semantic similarity between bios and their assigned\ncluster names, as well as their distinction from alternatives.\n  These findings reveal a \"Goldilocks zone\" where clusters remain distinct yet\ninterpretable. We identify an optimal range of 16-22 clusters, paralleling\nlinguistic efficiency in lexical categorization. These insights inform both\ntheoretical models and practical applications, guiding future research toward\noptimising cluster interpretability and usefulness."}
{"id": "2504.04191", "pdf": "https://arxiv.org/pdf/2504.04191", "abs": "https://arxiv.org/abs/2504.04191", "authors": ["Jieming Cui", "Tengyu Liu", "Ziyu Meng", "Jiale Yu", "Ran Song", "Wei Zhang", "Yixin Zhu", "Siyuan Huang"], "title": "GROVE: A Generalized Reward for Learning Open-Vocabulary Physical Skill", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Learning open-vocabulary physical skills for simulated agents presents a\nsignificant challenge in artificial intelligence. Current reinforcement\nlearning approaches face critical limitations: manually designed rewards lack\nscalability across diverse tasks, while demonstration-based methods struggle to\ngeneralize beyond their training distribution. We introduce GROVE, a\ngeneralized reward framework that enables open-vocabulary physical skill\nlearning without manual engineering or task-specific demonstrations. Our key\ninsight is that Large Language Models(LLMs) and Vision Language Models(VLMs)\nprovide complementary guidance -- LLMs generate precise physical constraints\ncapturing task requirements, while VLMs evaluate motion semantics and\nnaturalness. Through an iterative design process, VLM-based feedback\ncontinuously refines LLM-generated constraints, creating a self-improving\nreward system. To bridge the domain gap between simulation and natural images,\nwe develop Pose2CLIP, a lightweight mapper that efficiently projects agent\nposes directly into semantic feature space without computationally expensive\nrendering. Extensive experiments across diverse embodiments and learning\nparadigms demonstrate GROVE's effectiveness, achieving 22.2% higher motion\nnaturalness and 25.7% better task completion scores while training 8.4x faster\nthan previous methods. These results establish a new foundation for scalable\nphysical skill acquisition in simulated environments."}
{"id": "2504.04325", "pdf": "https://arxiv.org/pdf/2504.04325", "abs": "https://arxiv.org/abs/2504.04325", "authors": ["Juan Sosa", "Alejandro Urrego", "Cesar Prieto", "Emma J. Camargo-D√≠az"], "title": "Constructing the Truth: Text Mining and Linguistic Networks in Public Hearings of Case 03 of the Special Jurisdiction for Peace (JEP)", "categories": ["cs.CL", "stat.AP", "stat.ME"], "comment": "48 pages, in Spanish language, 11 tablas, 24 figures", "summary": "Case 03 of the Special Jurisdiction for Peace (JEP), focused on the so-called\nfalse positives in Colombia, represents one of the most harrowing episodes of\nthe Colombian armed conflict. This article proposes an innovative methodology\nbased on natural language analysis and semantic co-occurrence models to\nexplore, systematize, and visualize narrative patterns present in the public\nhearings of victims and appearing parties. By constructing skipgram networks\nand analyzing their modularity, the study identifies thematic clusters that\nreveal regional and procedural status differences, providing empirical evidence\non dynamics of victimization, responsibility, and acknowledgment in this case.\nThis computational approach contributes to the collective construction of both\njudicial and extrajudicial truth, offering replicable tools for other\ntransitional justice cases. The work is grounded in the pillars of truth,\njustice, reparation, and non-repetition, proposing a critical and in-depth\nreading of contested memories."}
{"id": "2504.04196", "pdf": "https://arxiv.org/pdf/2504.04196", "abs": "https://arxiv.org/abs/2504.04196", "authors": ["Hamza Riaz", "Alan F. Smeaton"], "title": "The Effects of Grouped Structural Global Pruning of Vision Transformers on Domain Generalisation", "categories": ["cs.CV", "cs.LG"], "comment": "9 pages", "summary": "With the growing sizes of AI models like large language models (LLMs) and\nvision transformers, deploying them on devices with limited computational\nresources is a significant challenge particularly when addressing domain\ngeneralisation (DG) tasks. This paper introduces a novel grouped structural\npruning method for pre-trained vision transformers (ViT, BeiT, and DeiT),\nevaluated on the PACS and Office-Home DG benchmarks. Our method uses dependency\ngraph analysis to identify and remove redundant groups of neurons, weights,\nfilters, or attention heads within transformers, using a range of selection\nmetrics. Grouped structural pruning is applied at pruning ratios of 50\\%, 75\\%\nand 95\\% and the models are then fine-tuned on selected distributions from DG\nbenchmarks to evaluate their overall performance in DG tasks. Results show\nsignificant improvements in inference speed and fine-tuning time with minimal\ntrade-offs in accuracy and DG task performance. For instance, on the PACS\nbenchmark, pruning ViT, BeiT, and DeiT models by 50\\% using the Hessian metric\nresulted in accuracy drops of only -2.94\\%, -1.42\\%, and -1.72\\%, respectively,\nwhile achieving speed boosts of 2.5x, 1.81x, and 2.15x. These findings\ndemonstrate the effectiveness of our approach in balancing model efficiency\nwith domain generalisation performance."}
{"id": "2504.04332", "pdf": "https://arxiv.org/pdf/2504.04332", "abs": "https://arxiv.org/abs/2504.04332", "authors": ["Quan Shi", "Carlos Jimenez", "Stephen Dong", "Brian Seo", "Caden Yao", "Adam Kelch", "Karthik Narasimhan"], "title": "IMPersona: Evaluating Individual Level LM Impersonation", "categories": ["cs.CL", "cs.AI"], "comment": "25 pages, 9 pages main", "summary": "As language models achieve increasingly human-like capabilities in\nconversational text generation, a critical question emerges: to what extent can\nthese systems simulate the characteristics of specific individuals? To evaluate\nthis, we introduce IMPersona, a framework for evaluating LMs at impersonating\nspecific individuals' writing style and personal knowledge. Using supervised\nfine-tuning and a hierarchical memory-inspired retrieval system, we demonstrate\nthat even modestly sized open-source models, such as Llama-3.1-8B-Instruct, can\nachieve impersonation abilities at concerning levels. In blind conversation\nexperiments, participants (mis)identified our fine-tuned models with memory\nintegration as human in 44.44% of interactions, compared to just 25.00% for the\nbest prompting-based approach. We analyze these results to propose detection\nmethods and defense strategies against such impersonation attempts. Our\nfindings raise important questions about both the potential applications and\nrisks of personalized language models, particularly regarding privacy,\nsecurity, and the ethical deployment of such technologies in real-world\ncontexts."}
{"id": "2504.04221", "pdf": "https://arxiv.org/pdf/2504.04221", "abs": "https://arxiv.org/abs/2504.04221", "authors": ["Rami Huu Nguyen", "Kenichi Maeda", "Mahsa Geshvadi", "Daniel Haehn"], "title": "Evaluating Graphical Perception with Multimodal LLMs", "categories": ["cs.CV"], "comment": "6 pages, 5 figures, 1 teaser, IEEE Pacific Visualization 2025\n  Conference", "summary": "Multimodal Large Language Models (MLLMs) have remarkably progressed in\nanalyzing and understanding images. Despite these advancements, accurately\nregressing values in charts remains an underexplored area for MLLMs. For\nvisualization, how do MLLMs perform when applied to graphical perception tasks?\nOur paper investigates this question by reproducing Cleveland and McGill's\nseminal 1984 experiment and comparing it against human task performance. Our\nstudy primarily evaluates fine-tuned and pretrained models and zero-shot\nprompting to determine if they closely match human graphical perception. Our\nfindings highlight that MLLMs outperform human task performance in some cases\nbut not in others. We highlight the results of all experiments to foster an\nunderstanding of where MLLMs succeed and fail when applied to data\nvisualization."}
{"id": "2504.04335", "pdf": "https://arxiv.org/pdf/2504.04335", "abs": "https://arxiv.org/abs/2504.04335", "authors": ["Yuya Ogasa", "Yuki Arase"], "title": "Hallucination Detection using Multi-View Attention Features", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This study tackles token-level hallucination detection in outputs of large\nlanguage models. Previous studies revealed that attention exhibits irregular\npatterns when hallucination occurs. Inspired by this, we extract features from\nthe attention matrix that provide complementary views of (a) the average\nattention each token receives, which helps identify whether certain tokens are\noverly influential or ignored, (b) the diversity of attention each token\nreceives, which reveals whether attention is biased toward specific subsets,\nand (c) the diversity of tokens a token attends to during generation, which\nindicates whether the model references a narrow or broad range of information.\nThese features are input to a Transformer-based classifier to conduct\ntoken-level classification to identify hallucinated spans. Experimental results\nindicate that the proposed method outperforms strong baselines on hallucination\ndetection with longer input contexts, i.e., data-to-text and summarization\ntasks."}
{"id": "2504.04225", "pdf": "https://arxiv.org/pdf/2504.04225", "abs": "https://arxiv.org/abs/2504.04225", "authors": ["Hamza Riaz", "Alan F. Smeaton"], "title": "Resilience of Vision Transformers for Domain Generalisation in the Presence of Out-of-Distribution Noisy Images", "categories": ["cs.CV"], "comment": "31 pages", "summary": "Modern AI models excel in controlled settings but often fail in real-world\nscenarios where data distributions shift unpredictably - a challenge known as\ndomain generalisation (DG). This paper tackles this limitation by rigorously\nevaluating vision tramsformers, specifically the BEIT architecture which is a\nmodel pre-trained with masked image modelling (MIM), against synthetic\nout-of-distribution (OOD) benchmarks designed to mimic real-world noise and\nocclusions. We introduce a novel framework to generate OOD test cases by\nstrategically masking object regions in images using grid patterns (25\\%, 50\\%,\n75\\% occlusion) and leveraging cutting-edge zero-shot segmentation via Segment\nAnything and Grounding DINO to ensure precise object localisation. Experiments\nacross three benchmarks (PACS, Office-Home, DomainNet) demonstrate BEIT's known\nrobustness while maintaining 94\\% accuracy on PACS and 87\\% on Office-Home,\ndespite significant occlusions, outperforming CNNs and other vision\ntransformers by margins of up to 37\\%. Analysis of self-attention distances\nreveals that the BEIT dependence on global features correlates with its\nresilience. Furthermore, our synthetic benchmarks expose critical failure\nmodes: performance degrades sharply when occlusions disrupt object shapes e.g.\n68\\% drop for external grid masking vs. 22\\% for internal masking. This work\nprovides two key advances (1) a scalable method to generate OOD benchmarks\nusing controllable noise, and (2) empirical evidence that MIM and\nself-attention mechanism in vision transformers enhance DG by learning\ninvariant features. These insights bridge the gap between lab-trained models\nand real-world deployment that offer a blueprint for building AI systems that\ngeneralise reliably under uncertainty."}
{"id": "2504.04336", "pdf": "https://arxiv.org/pdf/2504.04336", "abs": "https://arxiv.org/abs/2504.04336", "authors": ["Cong Sun", "Kurt Teichman", "Yiliang Zhou", "Brian Critelli", "David Nauheim", "Graham Keir", "Xindi Wang", "Judy Zhong", "Adam E Flanders", "George Shih", "Yifan Peng"], "title": "Generative Large Language Models Trained for Detecting Errors in Radiology Reports", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this retrospective study, a dataset was constructed with two parts. The\nfirst part included 1,656 synthetic chest radiology reports generated by GPT-4\nusing specified prompts, with 828 being error-free synthetic reports and 828\ncontaining errors. The second part included 614 reports: 307 error-free reports\nbetween 2011 and 2016 from the MIMIC-CXR database and 307 corresponding\nsynthetic reports with errors generated by GPT-4 on the basis of these\nMIMIC-CXR reports and specified prompts. All errors were categorized into four\ntypes: negation, left/right, interval change, and transcription errors. Then,\nseveral models, including Llama-3, GPT-4, and BiomedBERT, were refined using\nzero-shot prompting, few-shot prompting, or fine-tuning strategies. Finally,\nthe performance of these models was evaluated using the F1 score, 95\\%\nconfidence interval (CI) and paired-sample t-tests on our constructed dataset,\nwith the prediction results further assessed by radiologists. Using zero-shot\nprompting, the fine-tuned Llama-3-70B-Instruct model achieved the best\nperformance with the following F1 scores: 0.769 for negation errors, 0.772 for\nleft/right errors, 0.750 for interval change errors, 0.828 for transcription\nerrors, and 0.780 overall. In the real-world evaluation phase, two radiologists\nreviewed 200 randomly selected reports output by the model. Of these, 99 were\nconfirmed to contain errors detected by the models by both radiologists, and\n163 were confirmed to contain model-detected errors by at least one\nradiologist. Generative LLMs, fine-tuned on synthetic and MIMIC-CXR radiology\nreports, greatly enhanced error detection in radiology reports."}
{"id": "2504.04252", "pdf": "https://arxiv.org/pdf/2504.04252", "abs": "https://arxiv.org/abs/2504.04252", "authors": ["Muhammad Osama Zeeshan", "Marco Pedersoli", "Alessandro Lameiras Koerich", "Eric Grange"], "title": "Progressive Multi-Source Domain Adaptation for Personalized Facial Expression Recognition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Personalized facial expression recognition (FER) involves adapting a machine\nlearning model using samples from labeled sources and unlabeled target domains.\nGiven the challenges of recognizing subtle expressions with considerable\ninterpersonal variability, state-of-the-art unsupervised domain adaptation\n(UDA) methods focus on the multi-source UDA (MSDA) setting, where each domain\ncorresponds to a specific subject, and improve model accuracy and robustness.\nHowever, when adapting to a specific target, the diverse nature of multiple\nsource domains translates to a large shift between source and target data.\nState-of-the-art MSDA methods for FER address this domain shift by considering\nall the sources to adapt to the target representations. Nevertheless, adapting\nto a target subject presents significant challenges due to large distributional\ndifferences between source and target domains, often resulting in negative\ntransfer. In addition, integrating all sources simultaneously increases\ncomputational costs and causes misalignment with the target. To address these\nissues, we propose a progressive MSDA approach that gradually introduces\ninformation from subjects based on their similarity to the target subject. This\nwill ensure that only the most relevant sources from the target are selected,\nwhich helps avoid the negative transfer caused by dissimilar sources. We first\nexploit the closest sources to reduce the distribution shift with the target\nand then move towards the furthest while only considering the most relevant\nsources based on the predetermined threshold. Furthermore, to mitigate\ncatastrophic forgetting caused by the incremental introduction of source\nsubjects, we implemented a density-based memory mechanism that preserves the\nmost relevant historical source samples for adaptation. Our experiments show\nthe effectiveness of our proposed method on pain datasets: Biovid and\nUNBC-McMaster."}
{"id": "2504.04342", "pdf": "https://arxiv.org/pdf/2504.04342", "abs": "https://arxiv.org/abs/2504.04342", "authors": ["Ayan Sengupta", "Siddhant Chaudhary", "Tanmoy Chakraborty"], "title": "Compression Laws for Large Language Models", "categories": ["cs.CL"], "comment": "16 pages, 11 figures, 6 tables", "summary": "We introduce compression laws for language language models (LLMs). While\nrecent scaling laws have sought to understand how LLMs scale with respect to\nmodel size, pre-training data, and computational resources, we focus on\nunderstanding how model compression affects the performance of a pre-trained\nLLM on downstream tasks. We empirically examine the effects of structured model\ncompression on LLMs through over $1000$ experiments across eight models with\nsizes ranging from $0.5B$ to $14B$ parameters. Our findings indicate that the\ntest cross-entropy loss increases quadratically with the compression ratio,\nwhereas performance on downstream tasks declines only linearly. Our study\nemphasizes the importance of recovery fine-tuning in enhancing generation loss,\nshowing that the test loss of compressed LLMs can improve by up to 55% with\nrecovery fine-tuning. At higher compression ratios (up to 90%), compressed LLMs\ndemonstrate a speed increase of 60% during inference compared to their\nuncompressed counterparts, compensating for the performance degradation at this\nlevel. However, for smaller models ($\\le 7B$), the computational gains are\nlimited, peaking at just 35%. We conclude that model compression can be highly\nbeneficial for larger models, especially when a smaller model within the same\ncomputational budget is not available. These insights provide the practical\nguidelines for utilizing model compression techniques for adopting LLMs in\nreal-life applications in resource-constrained settings."}
{"id": "2504.04271", "pdf": "https://arxiv.org/pdf/2504.04271", "abs": "https://arxiv.org/abs/2504.04271", "authors": ["Mete Ahishali", "Anis Ur Rahman", "Einari Heinaro", "Samuli Junttila"], "title": "ADA-Net: Attention-Guided Domain Adaptation Network with Contrastive Learning for Standing Dead Tree Segmentation Using Aerial Imagery", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Information on standing dead trees is important for understanding forest\necosystem functioning and resilience but has been lacking over large geographic\nregions. Climate change has caused large-scale tree mortality events that can\nremain undetected due to limited data. In this study, we propose a novel method\nfor segmenting standing dead trees using aerial multispectral orthoimages.\nBecause access to annotated datasets has been a significant problem in forest\nremote sensing due to the need for forest expertise, we introduce a method for\ndomain transfer by leveraging domain adaptation to learn a transformation from\na source domain X to target domain Y. In this Image-to-Image translation task,\nwe aim to utilize available annotations in the target domain by pre-training a\nsegmentation network. When images from a new study site without annotations are\nintroduced (source domain X), these images are transformed into the target\ndomain. Then, transfer learning is applied by inferring the pre-trained network\non domain-adapted images. In addition to investigating the feasibility of\ncurrent domain adaptation approaches for this objective, we propose a novel\napproach called the Attention-guided Domain Adaptation Network (ADA-Net) with\nenhanced contrastive learning. Accordingly, the ADA-Net approach provides new\nstate-of-the-art domain adaptation performance levels outperforming existing\napproaches. We have evaluated the proposed approach using two datasets from\nFinland and the US. The USA images are converted to the Finland domain, and we\nshow that the synthetic USA2Finland dataset exhibits similar characteristics to\nthe Finland domain images. The software implementation is shared at\nhttps://github.com/meteahishali/ADA-Net. The data is publicly available at\nhttps://www.kaggle.com/datasets/meteahishali/aerial-imagery-for-standing-dead-tree-segmentation."}
{"id": "2504.04373", "pdf": "https://arxiv.org/pdf/2504.04373", "abs": "https://arxiv.org/abs/2504.04373", "authors": ["Shenyang Liu", "Yang Gao", "Shaoyan Zhai", "Liqiang Wang"], "title": "StyleRec: A Benchmark Dataset for Prompt Recovery in Writing Style Transformation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "2024 IEEE International Conference on Big Data (BigData)", "summary": "Prompt Recovery, reconstructing prompts from the outputs of large language\nmodels (LLMs), has grown in importance as LLMs become ubiquitous. Most users\naccess LLMs through APIs without internal model weights, relying only on\noutputs and logits, which complicates recovery. This paper explores a unique\nprompt recovery task focused on reconstructing prompts for style transfer and\nrephrasing, rather than typical question-answering. We introduce a dataset\ncreated with LLM assistance, ensuring quality through multiple techniques, and\ntest methods like zero-shot, few-shot, jailbreak, chain-of-thought,\nfine-tuning, and a novel canonical-prompt fallback for poor-performing cases.\nOur results show that one-shot and fine-tuning yield the best outcomes but\nhighlight flaws in traditional sentence similarity metrics for evaluating\nprompt recovery. Contributions include (1) a benchmark dataset, (2)\ncomprehensive experiments on prompt recovery strategies, and (3) identification\nof limitations in current evaluation metrics, all of which advance general\nprompt recovery research, where the structure of the input prompt is\nunrestricted."}
{"id": "2504.04294", "pdf": "https://arxiv.org/pdf/2504.04294", "abs": "https://arxiv.org/abs/2504.04294", "authors": ["Zhisheng Huang", "Peng Wang", "Jingdong Zhang", "Yuan Liu", "Xin Li", "Wenping Wang"], "title": "3R-GS: Best Practice in Optimizing Camera Poses Along with 3DGS", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has revolutionized neural rendering with its\nefficiency and quality, but like many novel view synthesis methods, it heavily\ndepends on accurate camera poses from Structure-from-Motion (SfM) systems.\nAlthough recent SfM pipelines have made impressive progress, questions remain\nabout how to further improve both their robust performance in challenging\nconditions (e.g., textureless scenes) and the precision of camera parameter\nestimation simultaneously. We present 3R-GS, a 3D Gaussian Splatting framework\nthat bridges this gap by jointly optimizing 3D Gaussians and camera parameters\nfrom large reconstruction priors MASt3R-SfM. We note that naively performing\njoint 3D Gaussian and camera optimization faces two challenges: the sensitivity\nto the quality of SfM initialization, and its limited capacity for global\noptimization, leading to suboptimal reconstruction results. Our 3R-GS,\novercomes these issues by incorporating optimized practices, enabling robust\nscene reconstruction even with imperfect camera registration. Extensive\nexperiments demonstrate that 3R-GS delivers high-quality novel view synthesis\nand precise camera pose estimation while remaining computationally efficient.\nProject page: https://zsh523.github.io/3R-GS/"}
{"id": "2504.04377", "pdf": "https://arxiv.org/pdf/2504.04377", "abs": "https://arxiv.org/abs/2504.04377", "authors": ["Priyanshu Kumar", "Devansh Jain", "Akhila Yerukola", "Liwei Jiang", "Himanshu Beniwal", "Thomas Hartvigsen", "Maarten Sap"], "title": "PolyGuard: A Multilingual Safety Moderation Tool for 17 Languages", "categories": ["cs.CL"], "comment": null, "summary": "Truly multilingual safety moderation efforts for Large Language Models (LLMs)\nhave been hindered by a narrow focus on a small set of languages (e.g.,\nEnglish, Chinese) as well as a limited scope of safety definition, resulting in\nsignificant gaps in moderation capabilities. To bridge these gaps, we release\nPOLYGUARD, a new state-of-the-art multilingual safety model for safeguarding\nLLM generations, and the corresponding training and evaluation datasets.\nPOLYGUARD is trained on POLYGUARDMIX, the largest multilingual safety training\ncorpus to date containing 1.91M samples across 17 languages (e.g., Chinese,\nCzech, English, Hindi). We also introduce POLYGUARDPROMPTS, a high quality\nmultilingual benchmark with 29K samples for the evaluation of safety\nguardrails. Created by combining naturally occurring multilingual human-LLM\ninteractions and human-verified machine translations of an English-only safety\ndataset (WildGuardMix; Han et al., 2024), our datasets contain prompt-output\npairs with labels of prompt harmfulness, response harmfulness, and response\nrefusal. Through extensive evaluations across multiple safety and toxicity\nbenchmarks, we demonstrate that POLYGUARD outperforms existing state-of-the-art\nopen-weight and commercial safety classifiers by 5.5%. Our contributions\nadvance efforts toward safer multilingual LLMs for all global users."}
{"id": "2504.04323", "pdf": "https://arxiv.org/pdf/2504.04323", "abs": "https://arxiv.org/abs/2504.04323", "authors": ["Yiming Shi", "Shaoshuai Yang", "Xun Zhu", "Haoyu Wang", "Miao Li", "Ji Wu"], "title": "MedM-VL: What Makes a Good Medical LVLM?", "categories": ["cs.CV"], "comment": null, "summary": "Medical image analysis is a fundamental component. As deep learning\nprogresses, the focus has shifted from single-task applications, such as\nclassification and segmentation, to more complex multimodal tasks, including\nmedical visual question answering and report generation. Traditional shallow\nand task-specific models are increasingly limited in addressing the complexity\nand scalability required in clinical practice. The emergence of large language\nmodels (LLMs) has driven the development of medical Large Vision-Language\nModels (LVLMs), offering a unified solution for diverse vision-language tasks.\nIn this study, we investigate various architectural designs for medical LVLMs\nbased on the widely adopted LLaVA framework, which follows an\nencoder-connector-LLM paradigm. We construct two distinct models targeting 2D\nand 3D modalities, respectively. These models are designed to support both\ngeneral-purpose medical tasks and domain-specific fine-tuning, thereby serving\nas effective foundation models. To facilitate reproducibility and further\nresearch, we develop a modular and extensible codebase, MedM-VL, and release\ntwo LVLM variants: MedM-VL-2D for 2D medical image analysis and\nMedM-VL-CT-Chest for 3D CT-based applications. The code and models are\navailable at: https://github.com/MSIIP/MedM-VL"}
{"id": "2504.04385", "pdf": "https://arxiv.org/pdf/2504.04385", "abs": "https://arxiv.org/abs/2504.04385", "authors": ["Xiaokai Wang", "Guiran Liu", "Binrong Zhu", "Jacky He", "Hongye Zheng", "Hanlu Zhang"], "title": "Pre-trained Language Models and Few-shot Learning for Medical Entity Extraction", "categories": ["cs.CL"], "comment": null, "summary": "This study proposes a medical entity extraction method based on Transformer\nto enhance the information extraction capability of medical literature.\nConsidering the professionalism and complexity of medical texts, we compare the\nperformance of different pre-trained language models (BERT, BioBERT,\nPubMedBERT, ClinicalBERT) in medical entity extraction tasks. Experimental\nresults show that PubMedBERT achieves the best performance (F1-score = 88.8%),\nindicating that a language model pre-trained on biomedical literature is more\neffective in the medical domain. In addition, we analyze the impact of\ndifferent entity extraction methods (CRF, Span-based, Seq2Seq) and find that\nthe Span-based approach performs best in medical entity extraction tasks\n(F1-score = 88.6%). It demonstrates superior accuracy in identifying entity\nboundaries. In low-resource scenarios, we further explore the application of\nFew-shot Learning in medical entity extraction. Experimental results show that\neven with only 10-shot training samples, the model achieves an F1-score of\n79.1%, verifying the effectiveness of Few-shot Learning under limited data\nconditions. This study confirms that the combination of pre-trained language\nmodels and Few-shot Learning can enhance the accuracy of medical entity\nextraction. Future research can integrate knowledge graphs and active learning\nstrategies to improve the model's generalization and stability, providing a\nmore effective solution for medical NLP research. Keywords- Natural Language\nProcessing, medical named entity recognition, pre-trained language model,\nFew-shot Learning, information extraction, deep learning"}
{"id": "2504.04339", "pdf": "https://arxiv.org/pdf/2504.04339", "abs": "https://arxiv.org/abs/2504.04339", "authors": ["Peng Gao", "Yujian Lee", "Zailong Chen", "Hui zhang", "Xubo Liu", "Yiyang Hu", "Guquang Jing"], "title": "NCL-CIR: Noise-aware Contrastive Learning for Composed Image Retrieval", "categories": ["cs.CV"], "comment": "Has been accepted by ICASSP2025", "summary": "Composed Image Retrieval (CIR) seeks to find a target image using a\nmulti-modal query, which combines an image with modification text to pinpoint\nthe target. While recent CIR methods have shown promise, they mainly focus on\nexploring relationships between the query pairs (image and text) through data\naugmentation or model design. These methods often assume perfect alignment\nbetween queries and target images, an idealized scenario rarely encountered in\npractice. In reality, pairs are often partially or completely mismatched due to\nissues like inaccurate modification texts, low-quality target images, and\nannotation errors. Ignoring these mismatches leads to numerous False Positive\nPair (FFPs) denoted as noise pairs in the dataset, causing the model to overfit\nand ultimately reducing its performance. To address this problem, we propose\nthe Noise-aware Contrastive Learning for CIR (NCL-CIR), comprising two key\ncomponents: the Weight Compensation Block (WCB) and the Noise-pair Filter Block\n(NFB). The WCB coupled with diverse weight maps can ensure more stable token\nrepresentations of multi-modal queries and target images. Meanwhile, the NFB,\nin conjunction with the Gaussian Mixture Model (GMM) predicts noise pairs by\nevaluating loss distributions, and generates soft labels correspondingly,\nallowing for the design of the soft-label based Noise Contrastive Estimation\n(NCE) loss function. Consequently, the overall architecture helps to mitigate\nthe influence of mismatched and partially matched samples, with experimental\nresults demonstrating that NCL-CIR achieves exceptional performance on the\nbenchmark datasets."}
{"id": "2504.04444", "pdf": "https://arxiv.org/pdf/2504.04444", "abs": "https://arxiv.org/abs/2504.04444", "authors": ["Daniel Bershatsky", "Ivan Oseledets"], "title": "On the Spatial Structure of Mixture-of-Experts in Transformers", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ICLR 2025 Workshop on Sparsity in LLMs (SLLM)", "summary": "A common assumption is that MoE routers primarily leverage semantic features\nfor expert selection. However, our study challenges this notion by\ndemonstrating that positional token information also plays a crucial role in\nrouting decisions. Through extensive empirical analysis, we provide evidence\nsupporting this hypothesis, develop a phenomenological explanation of the\nobserved behavior, and discuss practical implications for MoE-based\narchitectures."}
{"id": "2504.04340", "pdf": "https://arxiv.org/pdf/2504.04340", "abs": "https://arxiv.org/abs/2504.04340", "authors": ["Ying Zhao"], "title": "AnomalyHybrid: A Domain-agnostic Generative Framework for General Anomaly Detection", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025 workshop on Harnessing Generative Models for\n  Synthetic Visual Datasets (SyntaGen)", "summary": "Anomaly generation is an effective way to mitigate data scarcity for anomaly\ndetection task. Most existing works shine at industrial anomaly generation with\nmultiple specialists or large generative models, rarely generalizing to\nanomalies in other applications. In this paper, we present AnomalyHybrid, a\ndomain-agnostic framework designed to generate authentic and diverse anomalies\nsimply by combining the reference and target images. AnomalyHybrid is a\nGenerative Adversarial Network(GAN)-based framework having two decoders that\nintegrate the appearance of reference image into the depth and edge structures\nof target image respectively. With the help of depth decoders, AnomalyHybrid\nachieves authentic generation especially for the anomalies with depth values\nchanging, such a s protrusion and dent. More, it relaxes the fine granularity\nstructural control of the edge decoder and brings more diversity. Without using\nannotations, AnomalyHybrid is easily trained with sets of color, depth and edge\nof same images having different augmentations. Extensive experiments carried on\nHeliconiusButterfly, MVTecAD and MVTec3D datasets demonstrate that\nAnomalyHybrid surpasses the GAN-based state-of-the-art on anomaly generation\nand its downstream anomaly classification, detection and segmentation tasks. On\nMVTecAD dataset, AnomalyHybrid achieves 2.06/0.32 IS/LPIPS for anomaly\ngeneration, 52.6 Acc for anomaly classification with ResNet34, 97.3/72.9 AP for\nimage/pixel-level anomaly detection with a simple UNet."}
{"id": "2504.04462", "pdf": "https://arxiv.org/pdf/2504.04462", "abs": "https://arxiv.org/abs/2504.04462", "authors": ["David Herrera-Poyatos", "Carlos Pel√°ez-Gonz√°lez", "Cristina Zuheros", "Andr√©s Herrera-Poyatos", "Virilo Tejedor", "Francisco Herrera", "Rosana Montes"], "title": "An overview of model uncertainty and variability in LLM-based sentiment analysis. Challenges, mitigation strategies and the role of explainability", "categories": ["cs.CL", "cs.AI"], "comment": "25 pages and 3 figures", "summary": "Large Language Models (LLMs) have significantly advanced sentiment analysis,\nyet their inherent uncertainty and variability pose critical challenges to\nachieving reliable and consistent outcomes. This paper systematically explores\nthe Model Variability Problem (MVP) in LLM-based sentiment analysis,\ncharacterized by inconsistent sentiment classification, polarization, and\nuncertainty arising from stochastic inference mechanisms, prompt sensitivity,\nand biases in training data. We analyze the core causes of MVP, presenting\nillustrative examples and a case study to highlight its impact. In addition, we\ninvestigate key challenges and mitigation strategies, paying particular\nattention to the role of temperature as a driver of output randomness and\nemphasizing the crucial role of explainability in improving transparency and\nuser trust. By providing a structured perspective on stability,\nreproducibility, and trustworthiness, this study helps develop more reliable,\nexplainable, and robust sentiment analysis models, facilitating their\ndeployment in high-stakes domains such as finance, healthcare, and\npolicymaking, among others."}
{"id": "2504.04348", "pdf": "https://arxiv.org/pdf/2504.04348", "abs": "https://arxiv.org/abs/2504.04348", "authors": ["Shihao Wang", "Zhiding Yu", "Xiaohui Jiang", "Shiyi Lan", "Min Shi", "Nadine Chang", "Jan Kautz", "Ying Li", "Jose M. Alvarez"], "title": "OmniDrive: A Holistic Vision-Language Dataset for Autonomous Driving with Counterfactual Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "The advances in vision-language models (VLMs) have led to a growing interest\nin autonomous driving to leverage their strong reasoning capabilities. However,\nextending these capabilities from 2D to full 3D understanding is crucial for\nreal-world applications. To address this challenge, we propose OmniDrive, a\nholistic vision-language dataset that aligns agent models with 3D driving tasks\nthrough counterfactual reasoning. This approach enhances decision-making by\nevaluating potential scenarios and their outcomes, similar to human drivers\nconsidering alternative actions. Our counterfactual-based synthetic data\nannotation process generates large-scale, high-quality datasets, providing\ndenser supervision signals that bridge planning trajectories and language-based\nreasoning. Futher, we explore two advanced OmniDrive-Agent frameworks, namely\nOmni-L and Omni-Q, to assess the importance of vision-language alignment versus\n3D perception, revealing critical insights into designing effective LLM-agents.\nSignificant improvements on the DriveLM Q\\&A benchmark and nuScenes open-loop\nplanning demonstrate the effectiveness of our dataset and methods."}
{"id": "2504.04473", "pdf": "https://arxiv.org/pdf/2504.04473", "abs": "https://arxiv.org/abs/2504.04473", "authors": ["Archana Sahu", "Plaban Kumar Bhowmick"], "title": "Directed Graph-alignment Approach for Identification of Gaps in Short Answers", "categories": ["cs.CL", "cs.AI"], "comment": "30 pages, 11 figures", "summary": "In this paper, we have presented a method for identifying missing items known\nas gaps in the student answers by comparing them against the corresponding\nmodel answer/reference answers, automatically. The gaps can be identified at\nword, phrase or sentence level. The identified gaps are useful in providing\nfeedback to the students for formative assessment. The problem of gap\nidentification has been modelled as an alignment of a pair of directed graphs\nrepresenting a student answer and the corresponding model answer for a given\nquestion. To validate the proposed approach, the gap annotated student answers\nconsidering answers from three widely known datasets in the short answer\ngrading domain, namely, University of North Texas (UNT), SciEntsBank, and\nBeetle have been developed and this gap annotated student answers' dataset is\navailable at: https://github.com/sahuarchana7/gaps-answers-dataset. Evaluation\nmetrics used in the traditional machine learning tasks have been adopted to\nevaluate the task of gap identification. Though performance of the proposed\napproach varies across the datasets and the types of the answers, overall the\nperformance is observed to be promising."}
{"id": "2504.04423", "pdf": "https://arxiv.org/pdf/2504.04423", "abs": "https://arxiv.org/abs/2504.04423", "authors": ["Yang Jiao", "Haibo Qiu", "Zequn Jie", "Shaoxiang Chen", "Jingjing Chen", "Lin Ma", "Yu-Gang Jiang"], "title": "UniToken: Harmonizing Multimodal Understanding and Generation through Unified Visual Encoding", "categories": ["cs.CV", "cs.AI"], "comment": "Accpeted to CVPR 2025 workshop", "summary": "We introduce UniToken, an auto-regressive generation model that encodes\nvisual inputs through a combination of discrete and continuous representations,\nenabling seamless integration of unified visual understanding and image\ngeneration tasks. Unlike previous approaches that rely on unilateral visual\nrepresentations, our unified visual encoding framework captures both high-level\nsemantics and low-level details, delivering multidimensional information that\nempowers heterogeneous tasks to selectively assimilate domain-specific\nknowledge based on their inherent characteristics. Through in-depth\nexperiments, we uncover key principles for developing a unified model capable\nof both visual understanding and image generation. Extensive evaluations across\na diverse range of prominent benchmarks demonstrate that UniToken achieves\nstate-of-the-art performance, surpassing existing approaches. These results\nestablish UniToken as a robust foundation for future research in this domain.\nThe code and models are available at https://github.com/SxJyJay/UniToken."}
{"id": "2504.04514", "pdf": "https://arxiv.org/pdf/2504.04514", "abs": "https://arxiv.org/abs/2504.04514", "authors": ["Yao Tao", "Yehui Tang", "Yun Wang", "Mingjian Zhu", "Hailin Hu", "Yunhe Wang"], "title": "Saliency-driven Dynamic Token Pruning for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite the recent success of large language models (LLMs), LLMs are\nparticularly challenging in long-sequence inference scenarios due to the\nquadratic computational complexity of the attention mechanism. Inspired by the\ninterpretability theory of feature attribution in neural network models, we\nobserve that not all tokens have the same contribution. Based on this\nobservation, we propose a novel token pruning framework, namely Saliency-driven\nDynamic Token Pruning (SDTP), to gradually and dynamically prune redundant\ntokens based on the input context. Specifically, a lightweight saliency-driven\nprediction module is designed to estimate the importance score of each token\nwith its hidden state, which is added to different layers of the LLM to\nhierarchically prune redundant tokens. Furthermore, a ranking-based\noptimization strategy is proposed to minimize the ranking divergence of the\nsaliency score and the predicted importance score. Extensive experiments have\nshown that our framework is generalizable to various models and datasets. By\nhierarchically pruning 65\\% of the input tokens, our method greatly reduces\n33\\% $\\sim$ 47\\% FLOPs and achieves speedup up to 1.75$\\times$ during\ninference, while maintaining comparable performance. We further demonstrate\nthat SDTP can be combined with KV cache compression method for further\ncompression."}
{"id": "2504.04427", "pdf": "https://arxiv.org/pdf/2504.04427", "abs": "https://arxiv.org/abs/2504.04427", "authors": ["Shiyan Liu", "Rui Qu", "Yan Jin"], "title": "FluentLip: A Phonemes-Based Two-stage Approach for Audio-Driven Lip Synthesis with Optical Flow Consistency", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": null, "summary": "Generating consecutive images of lip movements that align with a given speech\nin audio-driven lip synthesis is a challenging task. While previous studies\nhave made strides in synchronization and visual quality, lip intelligibility\nand video fluency remain persistent challenges. This work proposes FluentLip, a\ntwo-stage approach for audio-driven lip synthesis, incorporating three featured\nstrategies. To improve lip synchronization and intelligibility, we integrate a\nphoneme extractor and encoder to generate a fusion of audio and phoneme\ninformation for multimodal learning. Additionally, we employ optical flow\nconsistency loss to ensure natural transitions between image frames.\nFurthermore, we incorporate a diffusion chain during the training of Generative\nAdversarial Networks (GANs) to improve both stability and efficiency. We\nevaluate our proposed FluentLip through extensive experiments, comparing it\nwith five state-of-the-art (SOTA) approaches across five metrics, including a\nproposed metric called Phoneme Error Rate (PER) that evaluates lip pose\nintelligibility and video fluency. The experimental results demonstrate that\nour FluentLip approach is highly competitive, achieving significant\nimprovements in smoothness and naturalness. In particular, it outperforms these\nSOTA approaches by approximately $\\textbf{16.3%}$ in Fr\\'echet Inception\nDistance (FID) and $\\textbf{35.2%}$ in PER."}
{"id": "2504.04534", "pdf": "https://arxiv.org/pdf/2504.04534", "abs": "https://arxiv.org/abs/2504.04534", "authors": ["Anantharaman Janakiraman", "Behnaz Ghoraani"], "title": "An Empirical Comparison of Text Summarization: A Multi-Dimensional Evaluation of Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Text summarization is crucial for mitigating information overload across\ndomains like journalism, medicine, and business. This research evaluates\nsummarization performance across 17 large language models (OpenAI, Google,\nAnthropic, open-source) using a novel multi-dimensional framework. We assessed\nmodels on seven diverse datasets (BigPatent, BillSum, CNN/DailyMail, PubMed,\nSAMSum, WikiHow, XSum) at three output lengths (50, 100, 150 tokens) using\nmetrics for factual consistency, semantic similarity, lexical overlap, and\nhuman-like quality, while also considering efficiency factors. Our findings\nreveal significant performance differences, with specific models excelling in\nfactual accuracy (deepseek-v3), human-like quality (claude-3-5-sonnet), and\nprocessing efficiency/cost-effectiveness (gemini-1.5-flash, gemini-2.0-flash).\nPerformance varies dramatically by dataset, with models struggling on technical\ndomains but performing well on conversational content. We identified a critical\ntension between factual consistency (best at 50 tokens) and perceived quality\n(best at 150 tokens). Our analysis provides evidence-based recommendations for\ndifferent use cases, from high-stakes applications requiring factual accuracy\nto resource-constrained environments needing efficient processing. This\ncomprehensive approach enhances evaluation methodology by integrating quality\nmetrics with operational considerations, incorporating trade-offs between\naccuracy, efficiency, and cost-effectiveness to guide model selection for\nspecific applications."}
{"id": "2504.04435", "pdf": "https://arxiv.org/pdf/2504.04435", "abs": "https://arxiv.org/abs/2504.04435", "authors": ["Tatiana Merkulova", "Bharani Jayakumar"], "title": "Evaluation framework for Image Segmentation Algorithms", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents a comprehensive evaluation framework for image\nsegmentation algorithms, encompassing naive methods, machine learning\napproaches, and deep learning techniques. We begin by introducing the\nfundamental concepts and importance of image segmentation, and the role of\ninteractive segmentation in enhancing accuracy. A detailed background theory\nsection explores various segmentation methods, including thresholding, edge\ndetection, region growing, feature extraction, random forests, support vector\nmachines, convolutional neural networks, U-Net, and Mask R-CNN. The\nimplementation and experimental setup are thoroughly described, highlighting\nthree primary approaches: algorithm assisting user, user assisting algorithm,\nand hybrid methods. Evaluation metrics such as Intersection over Union (IoU),\ncomputation time, and user interaction time are employed to measure\nperformance. A comparative analysis presents detailed results, emphasizing the\nstrengths, limitations, and trade-offs of each method. The paper concludes with\ninsights into the practical applicability of these approaches across various\nscenarios and outlines future work, focusing on expanding datasets, developing\nmore representative approaches, integrating real-time feedback, and exploring\nweakly supervised and self-supervised learning paradigms to enhance\nsegmentation accuracy and efficiency. Keywords: Image Segmentation, Interactive\nSegmentation, Machine Learning, Deep Learning, Computer Vision"}
{"id": "2504.04569", "pdf": "https://arxiv.org/pdf/2504.04569", "abs": "https://arxiv.org/abs/2504.04569", "authors": ["Chitranshu Harbola", "Anupam Purwar"], "title": "KnowsLM: A framework for evaluation of small language models for knowledge augmentation and humanised conversations", "categories": ["cs.CL"], "comment": null, "summary": "In the evolving landscape of conversational AI, generating concise,\ncontext-aware, and human-like dialogue using small and medium-sized language\nmodels (LLMs) remains a complex challenge. This study investigates the\ninfluence of LoRA rank, dataset scale, and prompt prefix design on both\nknowledge retention and stylistic alignment. While fine-tuning improves fluency\nand enables stylistic customization, its ability to integrate unseen knowledge\nis constrained -- particularly with smaller datasets. Conversely, RAG-augmented\nmodels, equipped to incorporate external documents at inference, demonstrated\nsuperior factual accuracy on out-of-distribution prompts, though they lacked\nthe stylistic consistency achieved by fine-tuning. Evaluations by LLM-based\njudges across knowledge accuracy, conversational quality, and conciseness\nsuggest that fine-tuning is best suited for tone adaptation, whereas RAG excels\nat real-time knowledge augmentation."}
{"id": "2504.04448", "pdf": "https://arxiv.org/pdf/2504.04448", "abs": "https://arxiv.org/abs/2504.04448", "authors": ["Etienne Chassaing", "Florent Forest", "Olga Fink", "Malcolm Mielle"], "title": "Thermoxels: a voxel-based method to generate simulation-ready 3D thermal models", "categories": ["cs.CV", "eess.IV"], "comment": "7 pages, 2 figures", "summary": "In the European Union, buildings account for 42% of energy use and 35% of\ngreenhouse gas emissions. Since most existing buildings will still be in use by\n2050, retrofitting is crucial for emissions reduction. However, current\nbuilding assessment methods rely mainly on qualitative thermal imaging, which\nlimits data-driven decisions for energy savings. On the other hand,\nquantitative assessments using finite element analysis (FEA) offer precise\ninsights but require manual CAD design, which is tedious and error-prone.\nRecent advances in 3D reconstruction, such as Neural Radiance Fields (NeRF) and\nGaussian Splatting, enable precise 3D modeling from sparse images but lack\nclearly defined volumes and the interfaces between them needed for FEA. We\npropose Thermoxels, a novel voxel-based method able to generate FEA-compatible\nmodels, including both geometry and temperature, from a sparse set of RGB and\nthermal images. Using pairs of RGB and thermal images as input, Thermoxels\nrepresents a scene's geometry as a set of voxels comprising color and\ntemperature information. After optimization, a simple process is used to\ntransform Thermoxels' models into tetrahedral meshes compatible with FEA. We\ndemonstrate Thermoxels' capability to generate RGB+Thermal meshes of 3D scenes,\nsurpassing other state-of-the-art methods. To showcase the practical\napplications of Thermoxels' models, we conduct a simple heat conduction\nsimulation using FEA, achieving convergence from an initial state defined by\nThermoxels' thermal reconstruction. Additionally, we compare Thermoxels' image\nsynthesis abilities with current state-of-the-art methods, showing competitive\nresults, and discuss the limitations of existing metrics in assessing mesh\nquality."}
{"id": "2504.04616", "pdf": "https://arxiv.org/pdf/2504.04616", "abs": "https://arxiv.org/abs/2504.04616", "authors": ["Qi Zhang", "Huitong Pan", "Zhijia Chen", "Longin Jan Latecki", "Cornelia Caragea", "Eduard Dragut"], "title": "DynClean: Training Dynamics-based Label Cleaning for Distantly-Supervised Named Entity Recognition", "categories": ["cs.CL"], "comment": "Accepted to NAACL2025-Findings", "summary": "Distantly Supervised Named Entity Recognition (DS-NER) has attracted\nattention due to its scalability and ability to automatically generate labeled\ndata. However, distant annotation introduces many mislabeled instances,\nlimiting its performance. Most of the existing work attempt to solve this\nproblem by developing intricate models to learn from the noisy labels. An\nalternative approach is to attempt to clean the labeled data, thus increasing\nthe quality of distant labels. This approach has received little attention for\nNER. In this paper, we propose a training dynamics-based label cleaning\napproach, which leverages the behavior of a model as training progresses to\ncharacterize the distantly annotated samples. We also introduce an automatic\nthreshold estimation strategy to locate the errors in distant labels. Extensive\nexperimental results demonstrate that: (1) models trained on our cleaned DS-NER\ndatasets, which were refined by directly removing identified erroneous\nannotations, achieve significant improvements in F1-score, ranging from 3.18%\nto 8.95%; and (2) our method outperforms numerous advanced DS-NER approaches\nacross four datasets."}
{"id": "2504.04454", "pdf": "https://arxiv.org/pdf/2504.04454", "abs": "https://arxiv.org/abs/2504.04454", "authors": ["Lei Cheng", "Mahdi Saleh", "Qing Cheng", "Lu Sang", "Hongli Xu", "Daniel Cremers", "Federico Tombari"], "title": "PRISM: Probabilistic Representation for Integrated Shape Modeling and Generation", "categories": ["cs.CV"], "comment": "Project page: https://starry-lei.github.io/prism_3d_shape", "summary": "Despite the advancements in 3D full-shape generation, accurately modeling\ncomplex geometries and semantics of shape parts remains a significant\nchallenge, particularly for shapes with varying numbers of parts. Current\nmethods struggle to effectively integrate the contextual and structural\ninformation of 3D shapes into their generative processes. We address these\nlimitations with PRISM, a novel compositional approach for 3D shape generation\nthat integrates categorical diffusion models with Statistical Shape Models\n(SSM) and Gaussian Mixture Models (GMM). Our method employs compositional SSMs\nto capture part-level geometric variations and uses GMM to represent part\nsemantics in a continuous space. This integration enables both high fidelity\nand diversity in generated shapes while preserving structural coherence.\nThrough extensive experiments on shape generation and manipulation tasks, we\ndemonstrate that our approach significantly outperforms previous methods in\nboth quality and controllability of part-level operations. Our code will be\nmade publicly available."}
{"id": "2504.04635", "pdf": "https://arxiv.org/pdf/2504.04635", "abs": "https://arxiv.org/abs/2504.04635", "authors": ["Patrick Queiroz Da Silva", "Hari Sethuraman", "Dheeraj Rajagopal", "Hannaneh Hajishirzi", "Sachin Kumar"], "title": "Steering off Course: Reliability Challenges in Steering Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Steering methods for language models (LMs) have gained traction as\nlightweight alternatives to fine-tuning, enabling targeted modifications to\nmodel activations. However, prior studies primarily report results on a few\nmodels, leaving critical gaps in understanding the robustness of these methods.\nIn this work, we systematically examine three prominent steering methods --\nDoLa, function vectors, and task vectors. In contrast to the original studies,\nwhich evaluated a handful of models, we test up to 36 models belonging to 14\nfamilies with sizes ranging from 1.5B to 70B parameters. Our experiments reveal\nsubstantial variability in the effectiveness of the steering approaches, with a\nlarge number of models showing no improvement and at times degradation in\nsteering performance. Our analysis demonstrate fundamental flaws in the\nassumptions underlying these methods, challenging their reliability as scalable\nsteering solutions."}
{"id": "2504.04457", "pdf": "https://arxiv.org/pdf/2504.04457", "abs": "https://arxiv.org/abs/2504.04457", "authors": ["Alejandro Fontan", "Tobias Fischer", "Javier Civera", "Michael Milford"], "title": "VSLAM-LAB: A Comprehensive Framework for Visual SLAM Methods and Datasets", "categories": ["cs.CV"], "comment": null, "summary": "Visual Simultaneous Localization and Mapping (VSLAM) research faces\nsignificant challenges due to fragmented toolchains, complex system\nconfigurations, and inconsistent evaluation methodologies. To address these\nissues, we present VSLAM-LAB, a unified framework designed to streamline the\ndevelopment, evaluation, and deployment of VSLAM systems. VSLAM-LAB simplifies\nthe entire workflow by enabling seamless compilation and configuration of VSLAM\nalgorithms, automated dataset downloading and preprocessing, and standardized\nexperiment design, execution, and evaluation--all accessible through a single\ncommand-line interface. The framework supports a wide range of VSLAM systems\nand datasets, offering broad compatibility and extendability while promoting\nreproducibility through consistent evaluation metrics and analysis tools. By\nreducing implementation complexity and minimizing configuration overhead,\nVSLAM-LAB empowers researchers to focus on advancing VSLAM methodologies and\naccelerates progress toward scalable, real-world solutions. We demonstrate the\nease with which user-relevant benchmarks can be created: here, we introduce\ndifficulty-level-based categories, but one could envision environment-specific\nor condition-specific categories."}
{"id": "2504.04640", "pdf": "https://arxiv.org/pdf/2504.04640", "abs": "https://arxiv.org/abs/2504.04640", "authors": ["Eylon Caplan", "Tania Chakraborty", "Dan Goldwasser"], "title": "Splits! A Flexible Dataset for Evaluating a Model's Demographic Social Inference", "categories": ["cs.CL", "cs.AI"], "comment": "Under review for COLM 2025", "summary": "Understanding how people of various demographics think, feel, and express\nthemselves (collectively called group expression) is essential for social\nscience and underlies the assessment of bias in Large Language Models (LLMs).\nWhile LLMs can effectively summarize group expression when provided with\nempirical examples, coming up with generalizable theories of how a group's\nexpression manifests in real-world text is challenging. In this paper, we\ndefine a new task called Group Theorization, in which a system must write\ntheories that differentiate expression across demographic groups. We make\navailable a large dataset on this task, Splits!, constructed by splitting\nReddit posts by neutral topics (e.g. sports, cooking, and movies) and by\ndemographics (e.g. occupation, religion, and race). Finally, we suggest a\nsimple evaluation framework for assessing how effectively a method can generate\n'better' theories about group expression, backed by human validation. We\npublicly release the raw corpora and evaluation scripts for Splits! to help\nresearchers assess how methods infer--and potentially misrepresent--group\ndifferences in expression. We make Splits! and our evaluation module available\nat https://github.com/eyloncaplan/splits."}
{"id": "2504.04463", "pdf": "https://arxiv.org/pdf/2504.04463", "abs": "https://arxiv.org/abs/2504.04463", "authors": ["Guandong Li", "Mengxia Ye"], "title": "Spatial-Geometry Enhanced 3D Dynamic Snake Convolutional Neural Network for Hyperspectral Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "Deep neural networks face several challenges in hyperspectral image\nclassification, including complex and sparse ground object distributions, small\nclustered structures, and elongated multi-branch features that often lead to\nmissing detections. To better adapt to ground object distributions and achieve\nadaptive dynamic feature responses while skipping redundant information, this\npaper proposes a Spatial-Geometry Enhanced 3D Dynamic Snake Network (SG-DSCNet)\nbased on an improved 3D-DenseNet model. The network employs Dynamic Snake\nConvolution (DSCConv), which introduces deformable offsets to enhance kernel\nflexibility through constrained self-learning, thereby improving regional\nperception of ground objects. Additionally, we propose a multi-view feature\nfusion strategy that generates multiple morphological kernel templates from\nDSCConv to observe target structures from different perspectives and achieve\nefficient feature fusion through summarizing key characteristics. This dynamic\napproach enables the model to focus more flexibly on critical spatial\nstructures when processing different regions, rather than relying on fixed\nreceptive fields of single static kernels. The DSC module enhances model\nrepresentation capability through dynamic kernel aggregation without increasing\nnetwork depth or width. Experimental results demonstrate superior performance\non the IN, UP, and KSC datasets, outperforming mainstream hyperspectral\nclassification methods."}
{"id": "2504.04698", "pdf": "https://arxiv.org/pdf/2504.04698", "abs": "https://arxiv.org/abs/2504.04698", "authors": ["Yuren Mao", "Yu Mi", "Peigen Liu", "Mengfei Zhang", "Hanqing Liu", "Yunjun Gao"], "title": "scAgent: Universal Single-Cell Annotation via a LLM Agent", "categories": ["cs.CL"], "comment": null, "summary": "Cell type annotation is critical for understanding cellular heterogeneity.\nBased on single-cell RNA-seq data and deep learning models, good progress has\nbeen made in annotating a fixed number of cell types within a specific tissue.\nHowever, universal cell annotation, which can generalize across tissues,\ndiscover novel cell types, and extend to novel cell types, remains less\nexplored. To fill this gap, this paper proposes scAgent, a universal cell\nannotation framework based on Large Language Models (LLMs). scAgent can\nidentify cell types and discover novel cell types in diverse tissues;\nfurthermore, it is data efficient to learn novel cell types. Experimental\nstudies in 160 cell types and 35 tissues demonstrate the superior performance\nof scAgent in general cell-type annotation, novel cell discovery, and\nextensibility to novel cell type."}
{"id": "2504.04470", "pdf": "https://arxiv.org/pdf/2504.04470", "abs": "https://arxiv.org/abs/2504.04470", "authors": ["Jiabao Guo", "Ajian Liu", "Yunfeng Diao", "Jin Zhang", "Hui Ma", "Bo Zhao", "Richang Hong", "Meng Wang"], "title": "Domain Generalization for Face Anti-spoofing via Content-aware Composite Prompt Engineering", "categories": ["cs.CV"], "comment": null, "summary": "The challenge of Domain Generalization (DG) in Face Anti-Spoofing (FAS) is\nthe significant interference of domain-specific signals on subtle spoofing\nclues. Recently, some CLIP-based algorithms have been developed to alleviate\nthis interference by adjusting the weights of visual classifiers. However, our\nanalysis of this class-wise prompt engineering suffers from two shortcomings\nfor DG FAS: (1) The categories of facial categories, such as real or spoof,\nhave no semantics for the CLIP model, making it difficult to learn accurate\ncategory descriptions. (2) A single form of prompt cannot portray the various\ntypes of spoofing. In this work, instead of class-wise prompts, we propose a\nnovel Content-aware Composite Prompt Engineering (CCPE) that generates\ninstance-wise composite prompts, including both fixed template and learnable\nprompts. Specifically, our CCPE constructs content-aware prompts from two\nbranches: (1) Inherent content prompt explicitly benefits from abundant\ntransferred knowledge from the instruction-based Large Language Model (LLM).\n(2) Learnable content prompts implicitly extract the most informative visual\ncontent via Q-Former. Moreover, we design a Cross-Modal Guidance Module (CGM)\nthat dynamically adjusts unimodal features for fusion to achieve better\ngeneralized FAS. Finally, our CCPE has been validated for its effectiveness in\nmultiple cross-domain experiments and achieves state-of-the-art (SOTA) results."}
{"id": "2504.04700", "pdf": "https://arxiv.org/pdf/2504.04700", "abs": "https://arxiv.org/abs/2504.04700", "authors": ["Hyunseo Shin", "Wonseok Hwang"], "title": "Causal Retrieval with Semantic Consideration", "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have significantly\nenhanced the performance of conversational AI systems. To extend their\ncapabilities to knowledge-intensive domains such as biomedical and legal\nfields, where the accuracy is critical, LLMs are often combined with\ninformation retrieval (IR) systems to generate responses based on retrieved\ndocuments. However, for IR systems to effectively support such applications,\nthey must go beyond simple semantic matching and accurately capture diverse\nquery intents, including causal relationships. Existing IR models primarily\nfocus on retrieving documents based on surface-level semantic similarity,\noverlooking deeper relational structures such as causality. To address this, we\npropose CAWAI, a retrieval model that is trained with dual objectives: semantic\nand causal relations. Our extensive experiments demonstrate that CAWAI\noutperforms various models on diverse causal retrieval tasks especially under\nlarge-scale retrieval settings. We also show that CAWAI exhibits strong\nzero-shot generalization across scientific domain QA tasks."}
{"id": "2504.04471", "pdf": "https://arxiv.org/pdf/2504.04471", "abs": "https://arxiv.org/abs/2504.04471", "authors": ["Zhuo Zhi", "Qiangqiang Wu", "Minghe shen", "Wenbo Li", "Yinchuan Li", "Kun Shao", "Kaiwen Zhou"], "title": "VideoAgent2: Enhancing the LLM-Based Agent System for Long-Form Video Understanding by Uncertainty-Aware CoT", "categories": ["cs.CV"], "comment": null, "summary": "Long video understanding has emerged as an increasingly important yet\nchallenging task in computer vision. Agent-based approaches are gaining\npopularity for processing long videos, as they can handle extended sequences\nand integrate various tools to capture fine-grained information. However,\nexisting methods still face several challenges: (1) they often rely solely on\nthe reasoning ability of large language models (LLMs) without dedicated\nmechanisms to enhance reasoning in long video scenarios; and (2) they remain\nvulnerable to errors or noise from external tools. To address these issues, we\npropose a specialized chain-of-thought (CoT) process tailored for long video\nanalysis. Our proposed CoT with plan-adjust mode enables the LLM to\nincrementally plan and adapt its information-gathering strategy. We further\nincorporate heuristic uncertainty estimation of both the LLM and external tools\nto guide the CoT process. This allows the LLM to assess the reliability of\nnewly collected information, refine its collection strategy, and make more\nrobust decisions when synthesizing final answers. Empirical experiments show\nthat our uncertainty-aware CoT effectively mitigates noise from external tools,\nleading to more reliable outputs. We implement our approach in a system called\nVideoAgent2, which also includes additional modules such as general context\nacquisition and specialized tool design. Evaluation on three dedicated long\nvideo benchmarks (and their subsets) demonstrates that VideoAgent2 outperforms\nthe previous state-of-the-art agent-based method, VideoAgent, by an average of\n13.1% and achieves leading performance among all zero-shot approaches"}
{"id": "2504.04713", "pdf": "https://arxiv.org/pdf/2504.04713", "abs": "https://arxiv.org/abs/2504.04713", "authors": ["Yifei Yu", "Qian-Wen Zhang", "Lingfeng Qiao", "Di Yin", "Fang Li", "Jie Wang", "Zengxi Chen", "Suncong Zheng", "Xiaolong Liang", "Xing Sun"], "title": "Sequential-NIAH: A Needle-In-A-Haystack Benchmark for Extracting Sequential Needles from Long Contexts", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Evaluating the ability of large language models (LLMs) to handle extended\ncontexts is critical, particularly for retrieving information relevant to\nspecific queries embedded within lengthy inputs. We introduce Sequential-NIAH,\na benchmark specifically designed to evaluate the capability of LLMs to extract\nsequential information items (known as needles) from long contexts. The\nbenchmark comprises three types of needle generation pipelines: synthetic,\nreal, and open-domain QA. It includes contexts ranging from 8K to 128K tokens\nin length, with a dataset of 14,000 samples (2,000 reserved for testing). To\nfacilitate evaluation on this benchmark, we trained a synthetic data-driven\nevaluation model capable of evaluating answer correctness based on\nchronological or logical order, achieving an accuracy of 99.49% on synthetic\ntest data. We conducted experiments on six well-known LLMs, revealing that even\nthe best-performing model achieved a maximum accuracy of only 63.15%. Further\nanalysis highlights the growing challenges posed by increasing context lengths\nand the number of needles, underscoring substantial room for improvement.\nAdditionally, noise robustness experiments validate the reliability of the\nbenchmark, making Sequential-NIAH an important reference for advancing research\non long text extraction capabilities of LLMs."}
{"id": "2504.04482", "pdf": "https://arxiv.org/pdf/2504.04482", "abs": "https://arxiv.org/abs/2504.04482", "authors": ["Mengxia Dai", "Wenqian Luo", "Tianyang Li"], "title": "Statistical Guarantees Of False Discovery Rate In Medical Instance Segmentation Tasks Based on Conformal Risk Control", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Instance segmentation plays a pivotal role in medical image analysis by\nenabling precise localization and delineation of lesions, tumors, and\nanatomical structures. Although deep learning models such as Mask R-CNN and\nBlendMask have achieved remarkable progress, their application in high-risk\nmedical scenarios remains constrained by confidence calibration issues, which\nmay lead to misdiagnosis. To address this challenge, we propose a robust\nquality control framework based on conformal prediction theory. This framework\ninnovatively constructs a risk-aware dynamic threshold mechanism that\nadaptively adjusts segmentation decision boundaries according to clinical\nrequirements.Specifically, we design a \\textbf{calibration-aware loss function}\nthat dynamically tunes the segmentation threshold based on a user-defined risk\nlevel $\\alpha$. Utilizing exchangeable calibration data, this method ensures\nthat the expected FNR or FDR on test data remains below $\\alpha$ with high\nprobability. The framework maintains compatibility with mainstream segmentation\nmodels (e.g., Mask R-CNN, BlendMask+ResNet-50-FPN) and datasets (PASCAL VOC\nformat) without requiring architectural modifications. Empirical results\ndemonstrate that we rigorously bound the FDR metric marginally over the test\nset via our developed calibration framework."}
{"id": "2504.04715", "pdf": "https://arxiv.org/pdf/2504.04715", "abs": "https://arxiv.org/abs/2504.04715", "authors": ["Will Cai", "Tianneng Shi", "Xuandong Zhao", "Dawn Song"], "title": "Are You Getting What You Pay For? Auditing Model Substitution in LLM APIs", "categories": ["cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "The proliferation of Large Language Models (LLMs) accessed via black-box APIs\nintroduces a significant trust challenge: users pay for services based on\nadvertised model capabilities (e.g., size, performance), but providers may\ncovertly substitute the specified model with a cheaper, lower-quality\nalternative to reduce operational costs. This lack of transparency undermines\nfairness, erodes trust, and complicates reliable benchmarking. Detecting such\nsubstitutions is difficult due to the black-box nature, typically limiting\ninteraction to input-output queries. This paper formalizes the problem of model\nsubstitution detection in LLM APIs. We systematically evaluate existing\nverification techniques, including output-based statistical tests, benchmark\nevaluations, and log probability analysis, under various realistic attack\nscenarios like model quantization, randomized substitution, and benchmark\nevasion. Our findings reveal the limitations of methods relying solely on text\noutputs, especially against subtle or adaptive attacks. While log probability\nanalysis offers stronger guarantees when available, its accessibility is often\nlimited. We conclude by discussing the potential of hardware-based solutions\nlike Trusted Execution Environments (TEEs) as a pathway towards provable model\nintegrity, highlighting the trade-offs between security, performance, and\nprovider adoption. Code is available at\nhttps://github.com/sunblaze-ucb/llm-api-audit"}
{"id": "2504.04485", "pdf": "https://arxiv.org/pdf/2504.04485", "abs": "https://arxiv.org/abs/2504.04485", "authors": ["Yapeng Mi", "Zhi Gao", "Xiaojian Ma", "Qing Li"], "title": "Building LLM Agents by Incorporating Insights from Computer Systems", "categories": ["cs.CV"], "comment": null, "summary": "LLM-driven autonomous agents have emerged as a promising direction in recent\nyears. However, many of these LLM agents are designed empirically or based on\nintuition, often lacking systematic design principles, which results in diverse\nagent structures with limited generality and scalability. In this paper, we\nadvocate for building LLM agents by incorporating insights from computer\nsystems. Inspired by the von Neumann architecture, we propose a structured\nframework for LLM agentic systems, emphasizing modular design and universal\nprinciples. Specifically, this paper first provides a comprehensive review of\nLLM agents from the computer system perspective, then identifies key challenges\nand future directions inspired by computer system design, and finally explores\nthe learning mechanisms for LLM agents beyond the computer system. The insights\ngained from this comparative analysis offer a foundation for systematic LLM\nagent design and advancement."}
{"id": "2504.04717", "pdf": "https://arxiv.org/pdf/2504.04717", "abs": "https://arxiv.org/abs/2504.04717", "authors": ["Yubo Li", "Xiaobin Shen", "Xinyu Yao", "Xueying Ding", "Yidi Miao", "Ramayya Krishnan", "Rema Padman"], "title": "Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "After 136 days of meticulous preparation, we're thrilled to finally\n  share our comprehensive survey on llm multi-turn interactions with the\n  community!", "summary": "Recent advancements in large language models (LLMs) have revolutionized their\nability to handle single-turn tasks, yet real-world applications demand\nsophisticated multi-turn interactions. This survey provides a comprehensive\nreview of recent advancements in evaluating and enhancing multi-turn\ninteractions in LLMs. Focusing on task-specific scenarios, from instruction\nfollowing in diverse domains such as math and coding to complex conversational\nengagements in roleplay, healthcare, education, and even adversarial jailbreak\nsettings, we systematically examine the challenges of maintaining context,\ncoherence, fairness, and responsiveness over prolonged dialogues. The paper\norganizes current benchmarks and datasets into coherent categories that reflect\nthe evolving landscape of multi-turn dialogue evaluation. In addition, we\nreview a range of enhancement methodologies under multi-turn settings,\nincluding model-centric strategies (contextual learning, supervised\nfine-tuning, reinforcement learning, and new architectures), external\nintegration approaches (memory-augmented, retrieval-based methods, and\nknowledge graph), and agent-based techniques for collaborative interactions.\nFinally, we discuss open challenges and propose future directions for research\nto further advance the robustness and effectiveness of multi-turn interactions\nin LLMs. Related resources and papers are available at\nhttps://github.com/yubol-cmu/Awesome-Multi-Turn-LLMs."}
{"id": "2504.04490", "pdf": "https://arxiv.org/pdf/2504.04490", "abs": "https://arxiv.org/abs/2504.04490", "authors": ["Kayato Nishitsunoi", "Yoshiyuki Ohmura", "Takayuki Komatsu", "Yasuo Kuniyoshi"], "title": "Learning Conditionally Independent Transformations using Normal Subgroups in Group Theory", "categories": ["cs.CV"], "comment": "8 pages, 10 figures, conference paper", "summary": "Humans develop certain cognitive abilities to recognize objects and their\ntransformations without explicit supervision, highlighting the importance of\nunsupervised representation learning. A fundamental challenge in unsupervised\nrepresentation learning is to separate different transformations in learned\nfeature representations. Although algebraic approaches have been explored, a\ncomprehensive theoretical framework remains underdeveloped. Existing methods\ndecompose transformations based on algebraic independence, but these methods\nprimarily focus on commutative transformations and do not extend to cases where\ntransformations are conditionally independent but noncommutative. To extend\ncurrent representation learning frameworks, we draw inspiration from Galois\ntheory, where the decomposition of groups through normal subgroups provides an\napproach for the analysis of structured transformations. Normal subgroups\nnaturally extend commutativity under certain conditions and offer a foundation\nfor the categorization of transformations, even when they do not commute. In\nthis paper, we propose a novel approach that leverages normal subgroups to\nenable the separation of conditionally independent transformations, even in the\nabsence of commutativity. Through experiments on geometric transformations in\nimages, we show that our method successfully categorizes conditionally\nindependent transformations, such as rotation and translation, in an\nunsupervised manner, suggesting a close link between group decomposition via\nnormal subgroups and transformation categorization in representation learning."}
{"id": "2504.04718", "pdf": "https://arxiv.org/pdf/2504.04718", "abs": "https://arxiv.org/abs/2504.04718", "authors": ["Minki Kang", "Jongwon Jeong", "Jaewoong Cho"], "title": "T1: Tool-integrated Self-verification for Test-time Compute Scaling in Small Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Recent studies have demonstrated that test-time compute scaling effectively\nimproves the performance of small language models (sLMs). However, prior\nresearch has mainly examined test-time compute scaling with an additional\nlarger model as a verifier, leaving self-verification by sLMs underexplored. In\nthis work, we investigate whether sLMs can reliably self-verify their outputs\nunder test-time scaling. We find that even with knowledge distillation from\nlarger verifiers, sLMs struggle with verification tasks requiring memorization,\nsuch as numerical calculations and fact-checking. To address this limitation,\nwe propose Tool-integrated self-verification (T1), which delegates\nmemorization-heavy verification steps to external tools, such as a code\ninterpreter. Our theoretical analysis shows that tool integration reduces\nmemorization demands and improves test-time scaling performance. Experiments on\nthe MATH benchmark demonstrate that, with T1, a Llama-3.2 1B model under\ntest-time scaling outperforms the significantly larger Llama-3.1 8B model.\nMoreover, T1 generalizes effectively to both mathematical (MATH500) and\nmulti-domain knowledge-intensive tasks (MMLU-Pro). Our findings highlight the\npotential of tool integration to substantially improve the self-verification\nabilities of sLMs."}
{"id": "2504.04494", "pdf": "https://arxiv.org/pdf/2504.04494", "abs": "https://arxiv.org/abs/2504.04494", "authors": ["Marin Benƒçeviƒá", "Robert ≈†ojo", "Irena Galiƒá"], "title": "Skin Color Measurement from Dermatoscopic Images: An Evaluation on a Synthetic Dataset", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents a comprehensive evaluation of skin color measurement\nmethods from dermatoscopic images using a synthetic dataset (S-SYNTH) with\ncontrolled ground-truth melanin content, lesion shapes, hair models, and 18\ndistinct lighting conditions. This allows for rigorous assessment of the\nrobustness and invariance to lighting conditions. We assess four classes of\nimage colorimetry approaches: segmentation-based, patch-based, color\nquantization, and neural networks. We use these methods to estimate the\nIndividual Typology Angle (ITA) and Fitzpatrick types from dermatoscopic\nimages. Our results show that segmentation-based and color quantization methods\nyield robust, lighting-invariant estimates, whereas patch-based approaches\nexhibit significant lighting-dependent biases that require calibration.\nFurthermore, neural network models, particularly when combined with heavy\nblurring to reduce overfitting, can provide light-invariant Fitzpatrick\npredictions, although their generalization to real-world images remains\nunverified. We conclude with practical recommendations for designing fair and\nreliable skin color estimation methods."}
{"id": "2504.04737", "pdf": "https://arxiv.org/pdf/2504.04737", "abs": "https://arxiv.org/abs/2504.04737", "authors": ["Shubham Kumar Nigam", "Balaramamahanthi Deepak Patnaik", "Shivam Mishra", "Noel Shallum", "Kripabandhu Ghosh", "Arnab Bhattacharya"], "title": "TathyaNyaya and FactLegalLlama: Advancing Factual Judgment Prediction and Explanation in the Indian Legal Context", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "In the landscape of Fact-based Judgment Prediction and Explanation (FJPE),\nreliance on factual data is essential for developing robust and realistic\nAI-driven decision-making tools. This paper introduces TathyaNyaya, the largest\nannotated dataset for FJPE tailored to the Indian legal context, encompassing\njudgments from the Supreme Court of India and various High Courts. Derived from\nthe Hindi terms \"Tathya\" (fact) and \"Nyaya\" (justice), the TathyaNyaya dataset\nis uniquely designed to focus on factual statements rather than complete legal\ntexts, reflecting real-world judicial processes where factual data drives\noutcomes. Complementing this dataset, we present FactLegalLlama, an\ninstruction-tuned variant of the LLaMa-3-8B Large Language Model (LLM),\noptimized for generating high-quality explanations in FJPE tasks. Finetuned on\nthe factual data in TathyaNyaya, FactLegalLlama integrates predictive accuracy\nwith coherent, contextually relevant explanations, addressing the critical need\nfor transparency and interpretability in AI-assisted legal systems. Our\nmethodology combines transformers for binary judgment prediction with\nFactLegalLlama for explanation generation, creating a robust framework for\nadvancing FJPE in the Indian legal domain. TathyaNyaya not only surpasses\nexisting datasets in scale and diversity but also establishes a benchmark for\nbuilding explainable AI systems in legal analysis. The findings underscore the\nimportance of factual precision and domain-specific tuning in enhancing\npredictive performance and interpretability, positioning TathyaNyaya and\nFactLegalLlama as foundational resources for AI-assisted legal decision-making."}
{"id": "2504.04495", "pdf": "https://arxiv.org/pdf/2504.04495", "abs": "https://arxiv.org/abs/2504.04495", "authors": ["Peng Wu", "Wanshun Su", "Guansong Pang", "Yujia Sun", "Qingsen Yan", "Peng Wang", "Yanning Zhang"], "title": "AVadCLIP: Audio-Visual Collaboration for Robust Video Anomaly Detection", "categories": ["cs.CV", "I.4.9; I.5.4"], "comment": "11 pages, 4 figures, 6 tables", "summary": "With the increasing adoption of video anomaly detection in intelligent\nsurveillance domains, conventional visual-based detection approaches often\nstruggle with information insufficiency and high false-positive rates in\ncomplex environments. To address these limitations, we present a novel weakly\nsupervised framework that leverages audio-visual collaboration for robust video\nanomaly detection. Capitalizing on the exceptional cross-modal representation\nlearning capabilities of Contrastive Language-Image Pretraining (CLIP) across\nvisual, audio, and textual domains, our framework introduces two major\ninnovations: an efficient audio-visual fusion that enables adaptive cross-modal\nintegration through lightweight parametric adaptation while maintaining the\nfrozen CLIP backbone, and a novel audio-visual prompt that dynamically enhances\ntext embeddings with key multimodal information based on the semantic\ncorrelation between audio-visual features and textual labels, significantly\nimproving CLIP's generalization for the video anomaly detection task. Moreover,\nto enhance robustness against modality deficiency during inference, we further\ndevelop an uncertainty-driven feature distillation module that synthesizes\naudio-visual representations from visual-only inputs. This module employs\nuncertainty modeling based on the diversity of audio-visual features to\ndynamically emphasize challenging features during the distillation process. Our\nframework demonstrates superior performance across multiple benchmarks, with\naudio integration significantly boosting anomaly detection accuracy in various\nscenarios. Notably, with unimodal data enhanced by uncertainty-driven\ndistillation, our approach consistently outperforms current unimodal VAD\nmethods."}
{"id": "2504.04745", "pdf": "https://arxiv.org/pdf/2504.04745", "abs": "https://arxiv.org/abs/2504.04745", "authors": ["Ankush Raut", "Xiaofeng Zhu", "Maria Leonor Pacheco"], "title": "Can LLMs Interpret and Leverage Structured Linguistic Representations? A Case Study with AMRs", "categories": ["cs.CL"], "comment": "13 pages, 23 figures. Submitted to XLLM @ ACL 2025", "summary": "This paper evaluates the ability of Large Language Models (LLMs) to leverage\ncontextual information in the form of structured linguistic representations.\nSpecifically, we examine the impact of encoding both short and long contexts\nusing Abstract Meaning Representation (AMR) structures across a diverse set of\nlanguage tasks. We perform our analysis using 8-bit quantized and\ninstruction-tuned versions of Llama 3.1 (8B), Phi-3, and Mistral 7B. Our\nresults indicate that, for tasks involving short contexts, augmenting the\nprompt with the AMR of the original language context often degrades the\nperformance of the underlying LLM. However, for tasks that involve long\ncontexts, such as dialogue summarization in the SAMSum dataset, this\nenhancement improves LLM performance, for example, by increasing the zero-shot\ncosine similarity score of Llama 3.1 from 66.2% to 76%. This improvement is\nmore evident in the newer and larger LLMs, but does not extend to the older or\nsmaller ones. In addition, we observe that LLMs can effectively reconstruct the\noriginal text from a linearized AMR, achieving a cosine similarity of 81.3% in\nthe best-case scenario."}
{"id": "2504.04510", "pdf": "https://arxiv.org/pdf/2504.04510", "abs": "https://arxiv.org/abs/2504.04510", "authors": ["Shijian Wang", "Linxin Song", "Ryotaro Shimizu", "Masayuki Goto", "Hanqian Wu"], "title": "Attributed Synthetic Data Generation for Zero-shot Domain-specific Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "Zero-shot domain-specific image classification is challenging in classifying\nreal images without ground-truth in-domain training examples. Recent research\ninvolved knowledge from texts with a text-to-image model to generate in-domain\ntraining images in zero-shot scenarios. However, existing methods heavily rely\non simple prompt strategies, limiting the diversity of synthetic training\nimages, thus leading to inferior performance compared to real images. In this\npaper, we propose AttrSyn, which leverages large language models to generate\nattributed prompts. These prompts allow for the generation of more diverse\nattributed synthetic images. Experiments for zero-shot domain-specific image\nclassification on two fine-grained datasets show that training with synthetic\nimages generated by AttrSyn significantly outperforms CLIP's zero-shot\nclassification under most situations and consistently surpasses simple prompt\nstrategies."}
{"id": "2504.04771", "pdf": "https://arxiv.org/pdf/2504.04771", "abs": "https://arxiv.org/abs/2504.04771", "authors": ["Leonardo Ranaldi", "Federico Ranaldi", "Fabio Massimo Zanzotto", "Barry Haddow", "Alexandra Birch"], "title": "Improving Multilingual Retrieval-Augmented Language Models through Dialectic Reasoning Argumentations", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-augmented generation (RAG) is key to enhancing large language\nmodels (LLMs) to systematically access richer factual knowledge. Yet, using RAG\nbrings intrinsic challenges, as LLMs must deal with potentially conflicting\nknowledge, especially in multilingual retrieval, where the heterogeneity of\nknowledge retrieved may deliver different outlooks. To make RAG more\nanalytical, critical and grounded, we introduce Dialectic-RAG (DRAG), a modular\napproach guided by Argumentative Explanations, i.e., structured reasoning\nprocess that systematically evaluates retrieved\n  information by comparing, contrasting, and resolving conflicting\nperspectives. Given a query and a set of multilingual related documents, DRAG\nselects and exemplifies relevant knowledge for delivering dialectic\nexplanations that, by critically weighing opposing arguments and filtering\nextraneous content, clearly determine the final response. Through a series of\nin-depth experiments, we show the impact of our framework both as an in-context\nlearning strategy and for constructing demonstrations to instruct smaller\nmodels. The final results demonstrate that DRAG significantly improves RAG\napproaches, requiring low-impact computational effort and providing robustness\nto knowledge perturbations."}
{"id": "2504.04517", "pdf": "https://arxiv.org/pdf/2504.04517", "abs": "https://arxiv.org/abs/2504.04517", "authors": ["Jiancheng Pan", "Yanxing Liu", "Xiao He", "Long Peng", "Jiahao Li", "Yuze Sun", "Xiaomeng Huang"], "title": "Enhance Then Search: An Augmentation-Search Strategy with Foundation Models for Cross-Domain Few-Shot Object Detection", "categories": ["cs.CV", "cs.AI"], "comment": "9 pages, 6 figures", "summary": "Foundation models pretrained on extensive datasets, such as GroundingDINO and\nLAE-DINO, have performed remarkably in the cross-domain few-shot object\ndetection (CD-FSOD) task. Through rigorous few-shot training, we found that the\nintegration of image-based data augmentation techniques and grid-based\nsub-domain search strategy significantly enhances the performance of these\nfoundation models. Building upon GroundingDINO, we employed several widely used\nimage augmentation methods and established optimization objectives to\neffectively navigate the expansive domain space in search of optimal\nsub-domains. This approach facilitates efficient few-shot object detection and\nintroduces an approach to solving the CD-FSOD problem by efficiently searching\nfor the optimal parameter configuration from the foundation model. Our findings\nsubstantially advance the practical deployment of vision-language models in\ndata-scarce environments, offering critical insights into optimizing their\ncross-domain generalization capabilities without labor-intensive retraining.\nCode is available at https://github.com/jaychempan/ETS."}
{"id": "2504.04782", "pdf": "https://arxiv.org/pdf/2504.04782", "abs": "https://arxiv.org/abs/2504.04782", "authors": ["Mia Jacobsen", "Ross Deans Kristensen-McLachlan"], "title": "I only read it for the plot! Maturity Ratings Affect Fanfiction Style and Community Engagement", "categories": ["cs.CL"], "comment": "Accepted to the 5th International Conference on Natural Language\n  Processing for Digital Humanities (NLP4DH 2025)", "summary": "We consider the textual profiles of different fanfiction maturity ratings,\nhow they vary across fan groups, and how this relates to reader engagement\nmetrics. Previous studies have shown that fanfiction writing is motivated by a\ncombination of admiration for and frustration with the fan object. These\nfindings emerge when looking at fanfiction as a whole, as well as when it is\ndivided into subgroups, also called fandoms. However, maturity ratings are used\nto indicate the intended audience of the fanfiction, as well as whether the\nstory includes mature themes and explicit scenes. Since these ratings can be\nused to filter readers and writers, they can also be seen as a proxy for\ndifferent reader/writer motivations and desires. We find that explicit\nfanfiction in particular has a distinct textual profile when compared to other\nmaturity ratings. These findings thus nuance our understanding of reader/writer\nmotivations in fanfiction communities, and also highlights the influence of the\ncommunity norms and fan behavior more generally on these cultural products."}
{"id": "2504.04519", "pdf": "https://arxiv.org/pdf/2504.04519", "abs": "https://arxiv.org/abs/2504.04519", "authors": ["Junjie Jiang", "Zelin Wang", "Manqi Zhao", "Yin Li", "DongSheng Jiang"], "title": "SAM2MOT: A Novel Paradigm of Multi-Object Tracking by Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Segment Anything 2 (SAM2) enables robust single-object tracking using\nsegmentation. To extend this to multi-object tracking (MOT), we propose\nSAM2MOT, introducing a novel Tracking by Segmentation paradigm. Unlike Tracking\nby Detection or Tracking by Query, SAM2MOT directly generates tracking boxes\nfrom segmentation masks, reducing reliance on detection accuracy. SAM2MOT has\ntwo key advantages: zero-shot generalization, allowing it to work across\ndatasets without fine-tuning, and strong object association, inherited from\nSAM2. To further improve performance, we integrate a trajectory manager system\nfor precise object addition and removal, and a cross-object interaction module\nto handle occlusions. Experiments on DanceTrack, UAVDT, and BDD100K show\nstate-of-the-art results. Notably, SAM2MOT outperforms existing methods on\nDanceTrack by +2.1 HOTA and +4.5 IDF1, highlighting its effectiveness in MOT."}
{"id": "2504.04823", "pdf": "https://arxiv.org/pdf/2504.04823", "abs": "https://arxiv.org/abs/2504.04823", "authors": ["Ruikang Liu", "Yuxuan Sun", "Manyi Zhang", "Haoli Bai", "Xianzhi Yu", "Tiezheng Yu", "Chun Yuan", "Lu Hou"], "title": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this study, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, and QwQ-32B. Our investigation covers weight, KV cache, and\nactivation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes will be open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models."}
{"id": "2504.04535", "pdf": "https://arxiv.org/pdf/2504.04535", "abs": "https://arxiv.org/abs/2504.04535", "authors": ["Weikai Lin", "Tianrui Ma", "Adith Boloor", "Yu Feng", "Ruofan Xing", "Xuan Zhang", "Yuhao Zhu"], "title": "SnapPix: Efficient-Coding--Inspired In-Sensor Compression for Edge Vision", "categories": ["cs.CV", "cs.AI", "I.2"], "comment": "7 pages, Accepted to Design Automation Conference (DAC), 2025", "summary": "Energy-efficient image acquisition on the edge is crucial for enabling remote\nsensing applications where the sensor node has weak compute capabilities and\nmust transmit data to a remote server/cloud for processing. To reduce the edge\nenergy consumption, this paper proposes a sensor-algorithm co-designed system\ncalled SnapPix, which compresses raw pixels in the analog domain inside the\nsensor. We use coded exposure (CE) as the in-sensor compression strategy as it\noffers the flexibility to sample, i.e., selectively expose pixels, both\nspatially and temporally. SNAPPIX has three contributions. First, we propose a\ntask-agnostic strategy to learn the sampling/exposure pattern based on the\nclassic theory of efficient coding. Second, we co-design the downstream vision\nmodel with the exposure pattern to address the pixel-level non-uniformity\nunique to CE-compressed images. Finally, we propose lightweight augmentations\nto the image sensor hardware to support our in-sensor CE compression.\nEvaluating on action recognition and video reconstruction, SnapPix outperforms\nstate-of-the-art video-based methods at the same speed while reducing the\nenergy by up to 15.4x. We have open-sourced the code at:\nhttps://github.com/horizon-research/SnapPix."}
{"id": "2504.04849", "pdf": "https://arxiv.org/pdf/2504.04849", "abs": "https://arxiv.org/abs/2504.04849", "authors": ["Sam Kirkham"], "title": "Discovering dynamical laws for speech gestures", "categories": ["cs.CL", "nlin.AO"], "comment": "Accepted for publication in 'Cognitive Science'", "summary": "A fundamental challenge in the cognitive sciences is discovering the dynamics\nthat govern behaviour. Take the example of spoken language, which is\ncharacterised by a highly variable and complex set of physical movements that\nmap onto the small set of cognitive units that comprise language. What are the\nfundamental dynamical principles behind the movements that structure speech\nproduction? In this study, we discover models in the form of symbolic equations\nthat govern articulatory gestures during speech. A sparse symbolic regression\nalgorithm is used to discover models from kinematic data on the tongue and\nlips. We explore these candidate models using analytical techniques and\nnumerical simulations, and find that a second-order linear model achieves high\nlevels of accuracy, but a nonlinear force is required to properly model\narticulatory dynamics in approximately one third of cases. This supports the\nproposal that an autonomous, nonlinear, second-order differential equation is a\nviable dynamical law for articulatory gestures in speech. We conclude by\nidentifying future opportunities and obstacles in data-driven model discovery\nand outline prospects for discovering the dynamical principles that govern\nlanguage, brain and behaviour."}
{"id": "2504.04540", "pdf": "https://arxiv.org/pdf/2504.04540", "abs": "https://arxiv.org/abs/2504.04540", "authors": ["Weichen Zhang", "Ruiying Peng", "Chen Gao", "Jianjie Fang", "Xin Zeng", "Kaiyuan Li", "Ziyou Wang", "Jinqiang Cui", "Xin Wang", "Xinlei Chen", "Yong Li"], "title": "The Point, the Vision and the Text: Does Point Cloud Boost Spatial Reasoning of Large Language Models?", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "3D Large Language Models (LLMs) leveraging spatial information in point\nclouds for 3D spatial reasoning attract great attention. Despite some promising\nresults, the role of point clouds in 3D spatial reasoning remains\nunder-explored. In this work, we comprehensively evaluate and analyze these\nmodels to answer the research question: \\textit{Does point cloud truly boost\nthe spatial reasoning capacities of 3D LLMs?} We first evaluate the spatial\nreasoning capacity of LLMs with different input modalities by replacing the\npoint cloud with the visual and text counterparts. We then propose a novel 3D\nQA (Question-answering) benchmark, ScanReQA, that comprehensively evaluates\nmodels' understanding of binary spatial relationships. Our findings reveal\nseveral critical insights: 1) LLMs without point input could even achieve\ncompetitive performance even in a zero-shot manner; 2) existing 3D LLMs\nstruggle to comprehend the binary spatial relationships; 3) 3D LLMs exhibit\nlimitations in exploiting the structural coordinates in point clouds for\nfine-grained spatial reasoning. We think these conclusions can help the next\nstep of 3D LLMs and also offer insights for foundation models in other\nmodalities. We release datasets and reproducible codes in the anonymous project\npage: https://3d-llm.xyz."}
{"id": "2504.04861", "pdf": "https://arxiv.org/pdf/2504.04861", "abs": "https://arxiv.org/abs/2504.04861", "authors": ["Hongtao Wang", "Renchi Yang", "Hewen Wang", "Haoran Zheng", "Jianliang Xu"], "title": "SAFT: Structure-aware Transformers for Textual Interaction Classification", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Textual interaction networks (TINs) are an omnipresent data structure used to\nmodel the interplay between users and items on e-commerce websites, social\nnetworks, etc., where each interaction is associated with a text description.\nClassifying such textual interactions (TIC) finds extensive use in detecting\nspam reviews in e-commerce, fraudulent transactions in finance, and so on.\nExisting TIC solutions either (i) fail to capture the rich text semantics due\nto the use of context-free text embeddings, and/or (ii) disregard the bipartite\nstructure and node heterogeneity of TINs, leading to compromised TIC\nperformance. In this work, we propose SAFT, a new architecture that integrates\nlanguage- and graph-based modules for the effective fusion of textual and\nstructural semantics in the representation learning of interactions. In\nparticular, line graph attention (LGA)/gated attention units (GAUs) and\npretrained language models (PLMs) are capitalized on to model the\ninteraction-level and token-level signals, which are further coupled via the\nproxy token in an iterative and contextualized fashion. Additionally, an\nefficient and theoretically-grounded approach is developed to encode the local\nand global topology information pertaining to interactions into structural\nembeddings. The resulting embeddings not only inject the structural features\nunderlying TINs into the textual interaction encoding but also facilitate the\ndesign of graph sampling strategies. Extensive empirical evaluations on\nmultiple real TIN datasets demonstrate the superiority of SAFT over the\nstate-of-the-art baselines in TIC accuracy."}
{"id": "2504.04549", "pdf": "https://arxiv.org/pdf/2504.04549", "abs": "https://arxiv.org/abs/2504.04549", "authors": ["Han Yuan", "Lican Kang", "Yong Li"], "title": "Opening the black box of deep learning: Validating the statistical association between explainable artificial intelligence (XAI) and clinical domain knowledge in fundus image-based glaucoma diagnosis", "categories": ["cs.CV"], "comment": null, "summary": "While deep learning has exhibited remarkable predictive capabilities in\nvarious medical image tasks, its inherent black-box nature has hindered its\nwidespread implementation in real-world healthcare settings. Our objective is\nto unveil the decision-making processes of deep learning models in the context\nof glaucoma classification by employing several Class Activation Map (CAM)\ntechniques to generate model focus regions and comparing them with clinical\ndomain knowledge of the anatomical area (optic cup, optic disk, and blood\nvessels). Four deep neural networks, including VGG-11, ResNet-18, DeiT-Tiny,\nand Swin Transformer-Tiny, were developed using binary diagnostic labels of\nglaucoma and five CAM methods (Grad-CAM, XGrad-CAM, Score-CAM, Eigen-CAM, and\nLayer-CAM) were employed to highlight the model focus area. We applied the\npaired-sample t-test to compare the percentage of anatomies in the model focus\narea to the proportion of anatomies in the entire image. After that, Pearson's\nand Spearman's correlation tests were implemented to examine the relationship\nbetween model predictive ability and the percentage of anatomical structures in\nthe model focus area. On five public glaucoma datasets, all deep learning\nmodels consistently displayed statistically significantly higher percentages of\nanatomical structures in the focus area than the proportions of anatomical\nstructures in the entire image. Also, we validated the positive relationship\nbetween the percentage of anatomical structures in the focus area and model\npredictive performance. Our study provides evidence of the convergence of\ndecision logic between deep neural networks and human clinicians through\nrigorous statistical tests. We anticipate that it can help alleviate\nclinicians' concerns regarding the trustworthiness of deep learning in\nhealthcare. For reproducibility, the code and dataset have been released at\nGitHub."}
{"id": "2504.04891", "pdf": "https://arxiv.org/pdf/2504.04891", "abs": "https://arxiv.org/abs/2504.04891", "authors": ["Longdi Xian", "Jianzhang Ni", "Mingzhu Wang"], "title": "Leveraging Large Language Models for Cost-Effective, Multilingual Depression Detection and Severity Assessment", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Depression is a prevalent mental health disorder that is difficult to detect\nearly due to subjective symptom assessments. Recent advancements in large\nlanguage models have offered efficient and cost-effective approaches for this\nobjective. In this study, we evaluated the performance of four LLMs in\ndepression detection using clinical interview data. We selected the best\nperforming model and further tested it in the severity evaluation scenario and\nknowledge enhanced scenario. The robustness was evaluated in complex diagnostic\nscenarios using a dataset comprising 51074 statements from six different mental\ndisorders. We found that DeepSeek V3 is the most reliable and cost-effective\nmodel for depression detection, performing well in both zero-shot and few-shot\nscenarios, with zero-shot being the most efficient choice. The evaluation of\nseverity showed low agreement with the human evaluator, particularly for mild\ndepression. The model maintains stably high AUCs for detecting depression in\ncomplex diagnostic scenarios. These findings highlight DeepSeek V3s strong\npotential for text-based depression detection in real-world clinical\napplications. However, they also underscore the need for further refinement in\nseverity assessment and the mitigation of potential biases to enhance clinical\nreliability."}
{"id": "2504.04550", "pdf": "https://arxiv.org/pdf/2504.04550", "abs": "https://arxiv.org/abs/2504.04550", "authors": ["Alkesh Patel", "Vibhav Chitalia", "Yinfei Yang"], "title": "Advancing Egocentric Video Question Answering with Multimodal Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "8 pages", "summary": "Egocentric Video Question Answering (QA) requires models to handle\nlong-horizon temporal reasoning, first-person perspectives, and specialized\nchallenges like frequent camera movement. This paper systematically evaluates\nboth proprietary and open-source Multimodal Large Language Models (MLLMs) on\nQaEgo4Dv2 - a refined dataset of egocentric videos derived from QaEgo4D. Four\npopular MLLMs (GPT-4o, Gemini-1.5-Pro, Video-LLaVa-7B and Qwen2-VL-7B-Instruct)\nare assessed using zero-shot and fine-tuned approaches for both OpenQA and\nCloseQA settings. We introduce QaEgo4Dv2 to mitigate annotation noise in\nQaEgo4D, enabling more reliable comparison. Our results show that fine-tuned\nVideo-LLaVa-7B and Qwen2-VL-7B-Instruct achieve new state-of-the-art\nperformance, surpassing previous benchmarks by up to +2.6% ROUGE/METEOR (for\nOpenQA) and +13% accuracy (for CloseQA). We also present a thorough error\nanalysis, indicating the model's difficulty in spatial reasoning and\nfine-grained object recognition - key areas for future improvement."}
{"id": "2504.04915", "pdf": "https://arxiv.org/pdf/2504.04915", "abs": "https://arxiv.org/abs/2504.04915", "authors": ["Ran Xu", "Wenqi Shi", "Yuchen Zhuang", "Yue Yu", "Joyce C. Ho", "Haoyu Wang", "Carl Yang"], "title": "Collab-RAG: Boosting Retrieval-Augmented Generation for Complex Question Answering via White-Box and Black-Box LLM Collaboration", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": "Work in progress. Code: https://github.com/ritaranx/Collab-RAG/", "summary": "Retrieval-Augmented Generation (RAG) systems often struggle to handle\nmulti-hop question-answering tasks accurately due to irrelevant context\nretrieval and limited complex reasoning capabilities. We introduce Collab-RAG,\na collaborative training framework that leverages mutual enhancement between a\nwhite-box small language model (SLM) and a blackbox large language model (LLM)\nfor RAG. Specifically, the SLM decomposes complex queries into simpler\nsub-questions, thus enhancing the accuracy of the retrieval and facilitating\nmore effective reasoning by the black-box LLM. Concurrently, the black-box LLM\nprovides feedback signals to improve the SLM's decomposition capability. We\nobserve that Collab-RAG relies solely on supervision from an affordable\nblack-box LLM without additional distillation from frontier LLMs, yet\ndemonstrates strong generalization across multiple black-box LLMs. Experimental\nevaluations across five multi-hop QA datasets demonstrate that Collab-RAG\nsubstantially outperforms existing black-box-only and SLM fine-tuning baselines\nby 1.8%-14.2% on average. In particular, our fine-tuned 3B SLM surpasses a\nfrozen 32B LLM in question decomposition, highlighting the efficiency of\nCollab-RAG in improving reasoning and retrieval for complex questions. The code\nof Collab-RAG is available on https://github.com/ritaranx/Collab-RAG/."}
{"id": "2504.04566", "pdf": "https://arxiv.org/pdf/2504.04566", "abs": "https://arxiv.org/abs/2504.04566", "authors": ["Maregu Assefa", "Muzammal Naseer", "Iyyakutti Iyappan Ganapathi", "Syed Sadaf Ali", "Mohamed L Seghier", "Naoufel Werghi"], "title": "DyCON: Dynamic Uncertainty-aware Consistency and Contrastive Learning for Semi-supervised Medical Image Segmentation", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Semi-supervised learning in medical image segmentation leverages unlabeled\ndata to reduce annotation burdens through consistency learning. However,\ncurrent methods struggle with class imbalance and high uncertainty from\npathology variations, leading to inaccurate segmentation in 3D medical images.\nTo address these challenges, we present DyCON, a Dynamic Uncertainty-aware\nConsistency and Contrastive Learning framework that enhances the generalization\nof consistency methods with two complementary losses: Uncertainty-aware\nConsistency Loss (UnCL) and Focal Entropy-aware Contrastive Loss (FeCL). UnCL\nenforces global consistency by dynamically weighting the contribution of each\nvoxel to the consistency loss based on its uncertainty, preserving\nhigh-uncertainty regions instead of filtering them out. Initially, UnCL\nprioritizes learning from uncertain voxels with lower penalties, encouraging\nthe model to explore challenging regions. As training progress, the penalty\nshift towards confident voxels to refine predictions and ensure global\nconsistency. Meanwhile, FeCL enhances local feature discrimination in\nimbalanced regions by introducing dual focal mechanisms and adaptive confidence\nadjustments into the contrastive principle. These mechanisms jointly\nprioritizes hard positives and negatives while focusing on uncertain sample\npairs, effectively capturing subtle lesion variations under class imbalance.\nExtensive evaluations on four diverse medical image segmentation datasets\n(ISLES'22, BraTS'19, LA, Pancreas) show DyCON's superior performance against\nSOTA methods."}
{"id": "2504.04953", "pdf": "https://arxiv.org/pdf/2504.04953", "abs": "https://arxiv.org/abs/2504.04953", "authors": ["Jos√© Pombal", "Dongkeun Yoon", "Patrick Fernandes", "Ian Wu", "Seungone Kim", "Ricardo Rei", "Graham Neubig", "Andr√© F. T. Martins"], "title": "M-Prometheus: A Suite of Open Multilingual LLM Judges", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The use of language models for automatically evaluating long-form text\n(LLM-as-a-judge) is becoming increasingly common, yet most LLM judges are\noptimized exclusively for English, with strategies for enhancing their\nmultilingual evaluation capabilities remaining largely unexplored in the\ncurrent literature. This has created a disparity in the quality of automatic\nevaluation methods for non-English languages, ultimately hindering the\ndevelopment of models with better multilingual capabilities. To bridge this\ngap, we introduce M-Prometheus, a suite of open-weight LLM judges ranging from\n3B to 14B parameters that can provide both direct assessment and pairwise\ncomparison feedback on multilingual outputs. M-Prometheus models outperform\nstate-of-the-art open LLM judges on multilingual reward benchmarks spanning\nmore than 20 languages, as well as on literary machine translation (MT)\nevaluation covering 4 language pairs. Furthermore, M-Prometheus models can be\nleveraged at decoding time to significantly improve generated outputs across\nall 3 tested languages, showcasing their utility for the development of better\nmultilingual models. Lastly, through extensive ablations, we identify the key\nfactors for obtaining an effective multilingual judge, including backbone model\nselection and training on natively multilingual feedback data instead of\ntranslated data. We release our models, training dataset, and code."}
{"id": "2504.04572", "pdf": "https://arxiv.org/pdf/2504.04572", "abs": "https://arxiv.org/abs/2504.04572", "authors": ["Mohamed Eltahir", "Osamah Sarraj", "Mohammed Bremoo", "Mohammed Khurd", "Abdulrahman Alfrihidi", "Taha Alshatiri", "Mohammad Almatrafi", "Tanveer Hussain"], "title": "Multimodal Lengthy Videos Retrieval Framework and Evaluation Metric", "categories": ["cs.CV"], "comment": null, "summary": "Precise video retrieval requires multi-modal correlations to handle unseen\nvocabulary and scenes, becoming more complex for lengthy videos where models\nmust perform effectively without prior training on a specific dataset. We\nintroduce a unified framework that combines a visual matching stream and an\naural matching stream with a unique subtitles-based video segmentation\napproach. Additionally, the aural stream includes a complementary audio-based\ntwo-stage retrieval mechanism that enhances performance on long-duration\nvideos. Considering the complex nature of retrieval from lengthy videos and its\ncorresponding evaluation, we introduce a new retrieval evaluation method\nspecifically designed for long-video retrieval to support further research. We\nconducted experiments on the YouCook2 benchmark, showing promising retrieval\nperformance."}
{"id": "2504.04963", "pdf": "https://arxiv.org/pdf/2504.04963", "abs": "https://arxiv.org/abs/2504.04963", "authors": ["Yuzhe Zhang", "Min Cen", "Hong Zhang"], "title": "Constraint Multi-class Positive and Unlabeled Learning for Distantly Supervised Named Entity Recognition", "categories": ["cs.CL"], "comment": "28pages, 3 figures. First submitted in Oct. 2023", "summary": "Distantly supervised named entity recognition (DS-NER) has been proposed to\nexploit the automatically labeled training data by external knowledge bases\ninstead of human annotations. However, it tends to suffer from a high false\nnegative rate due to the inherent incompleteness. To address this issue, we\npresent a novel approach called \\textbf{C}onstraint \\textbf{M}ulti-class\n\\textbf{P}ositive and \\textbf{U}nlabeled Learning (CMPU), which introduces a\nconstraint factor on the risk estimator of multiple positive classes. It\nsuggests that the constraint non-negative risk estimator is more robust against\noverfitting than previous PU learning methods with limited positive data. Solid\ntheoretical analysis on CMPU is provided to prove the validity of our approach.\nExtensive experiments on two benchmark datasets that were labeled using diverse\nexternal knowledge sources serve to demonstrate the superior performance of\nCMPU in comparison to existing DS-NER methods."}
{"id": "2504.04582", "pdf": "https://arxiv.org/pdf/2504.04582", "abs": "https://arxiv.org/abs/2504.04582", "authors": ["Nicolo Resmini", "Eugenio Lomurno", "Cristian Sbrolli", "Matteo Matteucci"], "title": "Your Image Generator Is Your New Private Dataset", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Generative diffusion models have emerged as powerful tools to synthetically\nproduce training data, offering potential solutions to data scarcity and\nreducing labelling costs for downstream supervised deep learning applications.\nHowever, effectively leveraging text-conditioned image generation for building\nclassifier training sets requires addressing key issues: constructing\ninformative textual prompts, adapting generative models to specific domains,\nand ensuring robust performance. This paper proposes the Text-Conditioned\nKnowledge Recycling (TCKR) pipeline to tackle these challenges. TCKR combines\ndynamic image captioning, parameter-efficient diffusion model fine-tuning, and\nGenerative Knowledge Distillation techniques to create synthetic datasets\ntailored for image classification. The pipeline is rigorously evaluated on ten\ndiverse image classification benchmarks. The results demonstrate that models\ntrained solely on TCKR-generated data achieve classification accuracies on par\nwith (and in several cases exceeding) models trained on real images.\nFurthermore, the evaluation reveals that these synthetic-data-trained models\nexhibit substantially enhanced privacy characteristics: their vulnerability to\nMembership Inference Attacks is significantly reduced, with the membership\ninference AUC lowered by 5.49 points on average compared to using real training\ndata, demonstrating a substantial improvement in the performance-privacy\ntrade-off. These findings indicate that high-fidelity synthetic data can\neffectively replace real data for training classifiers, yielding strong\nperformance whilst simultaneously providing improved privacy protection as a\nvaluable emergent property. The code and trained models are available in the\naccompanying open-source repository."}
{"id": "2504.04966", "pdf": "https://arxiv.org/pdf/2504.04966", "abs": "https://arxiv.org/abs/2504.04966", "authors": ["Shion Fukuhata", "Yoshinobu Kano"], "title": "Few Dimensions are Enough: Fine-tuning BERT with Selected Dimensions Revealed Its Redundant Nature", "categories": ["cs.CL"], "comment": "11 pages, 2 figures", "summary": "When fine-tuning BERT models for specific tasks, it is common to select part\nof the final layer's output and input it into a newly created fully connected\nlayer. However, it remains unclear which part of the final layer should be\nselected and what information each dimension of the layers holds. In this\nstudy, we comprehensively investigated the effectiveness and redundancy of\ntoken vectors, layers, and dimensions through BERT fine-tuning on GLUE tasks.\nThe results showed that outputs other than the CLS vector in the final layer\ncontain equivalent information, most tasks require only 2-3 dimensions, and\nwhile the contribution of lower layers decreases, there is little difference\namong higher layers. We also evaluated the impact of freezing pre-trained\nlayers and conducted cross-fine-tuning, where fine-tuning is applied\nsequentially to different tasks. The findings suggest that hidden layers may\nchange significantly during fine-tuning, BERT has considerable redundancy,\nenabling it to handle multiple tasks simultaneously, and its number of\ndimensions may be excessive."}
{"id": "2504.04597", "pdf": "https://arxiv.org/pdf/2504.04597", "abs": "https://arxiv.org/abs/2504.04597", "authors": ["Haebeom Jung", "Namtae Kim", "Jungwoo Kim", "Jaesik Park"], "title": "Targetless LiDAR-Camera Calibration with Anchored 3D Gaussians", "categories": ["cs.CV"], "comment": "Project page: https://zang09.github.io/tlc-calib-site", "summary": "We present a targetless LiDAR-camera calibration method that jointly\noptimizes sensor poses and scene geometry from arbitrary scenes, without\nrelying on traditional calibration targets such as checkerboards or spherical\nreflectors. Our approach leverages a 3D Gaussian-based scene representation. We\nfirst freeze reliable LiDAR points as anchors, then jointly optimize the poses\nand auxiliary Gaussian parameters in a fully differentiable manner using a\nphotometric loss. This joint optimization significantly reduces sensor\nmisalignment, resulting in higher rendering quality and consistently improved\nPSNR compared to the carefully calibrated poses provided in popular datasets.\nWe validate our method through extensive experiments on two real-world\nautonomous driving datasets, KITTI-360 and Waymo, each featuring distinct\nsensor configurations. Additionally, we demonstrate the robustness of our\napproach using a custom LiDAR-camera setup, confirming strong performance\nacross diverse hardware configurations."}
{"id": "2504.04976", "pdf": "https://arxiv.org/pdf/2504.04976", "abs": "https://arxiv.org/abs/2504.04976", "authors": ["Carlos Pel√°ez-Gonz√°lez", "Andr√©s Herrera-Poyatos", "Cristina Zuheros", "David Herrera-Poyatos", "Virilo Tejedor", "Francisco Herrera"], "title": "A Domain-Based Taxonomy of Jailbreak Vulnerabilities in Large Language Models", "categories": ["cs.CL", "I.2.7"], "comment": "21 pages, 5 figures", "summary": "The study of large language models (LLMs) is a key area in open-world machine\nlearning. Although LLMs demonstrate remarkable natural language processing\ncapabilities, they also face several challenges, including consistency issues,\nhallucinations, and jailbreak vulnerabilities. Jailbreaking refers to the\ncrafting of prompts that bypass alignment safeguards, leading to unsafe outputs\nthat compromise the integrity of LLMs. This work specifically focuses on the\nchallenge of jailbreak vulnerabilities and introduces a novel taxonomy of\njailbreak attacks grounded in the training domains of LLMs. It characterizes\nalignment failures through generalization, objectives, and robustness gaps. Our\nprimary contribution is a perspective on jailbreak, framed through the\ndifferent linguistic domains that emerge during LLM training and alignment.\nThis viewpoint highlights the limitations of existing approaches and enables us\nto classify jailbreak attacks on the basis of the underlying model deficiencies\nthey exploit. Unlike conventional classifications that categorize attacks based\non prompt construction methods (e.g., prompt templating), our approach provides\na deeper understanding of LLM behavior. We introduce a taxonomy with four\ncategories -- mismatched generalization, competing objectives, adversarial\nrobustness, and mixed attacks -- offering insights into the fundamental nature\nof jailbreak vulnerabilities. Finally, we present key lessons derived from this\ntaxonomic study."}
{"id": "2504.04631", "pdf": "https://arxiv.org/pdf/2504.04631", "abs": "https://arxiv.org/abs/2504.04631", "authors": ["Lei Wan", "Jianxin Zhao", "Andreas Wiedholz", "Manuel Bied", "Mateus Martinez de Lucena", "Abhishek Dinkar Jagtap", "Andreas Festag", "Ant√¥nio Augusto Fr√∂hlich", "Hannan Ejaz Keen", "Alexey Vinel"], "title": "Systematic Literature Review on Vehicular Collaborative Perception -- A Computer Vision Perspective", "categories": ["cs.CV"], "comment": "39 pages, 25 figures", "summary": "The effectiveness of autonomous vehicles relies on reliable perception\ncapabilities. Despite significant advancements in artificial intelligence and\nsensor fusion technologies, current single-vehicle perception systems continue\nto encounter limitations, notably visual occlusions and limited long-range\ndetection capabilities. Collaborative Perception (CP), enabled by\nVehicle-to-Vehicle (V2V) and Vehicle-to-Infrastructure (V2I) communication, has\nemerged as a promising solution to mitigate these issues and enhance the\nreliability of autonomous systems. Beyond advancements in communication, the\ncomputer vision community is increasingly focusing on improving vehicular\nperception through collaborative approaches. However, a systematic literature\nreview that thoroughly examines existing work and reduces subjective bias is\nstill lacking. Such a systematic approach helps identify research gaps,\nrecognize common trends across studies, and inform future research directions.\nIn response, this study follows the PRISMA 2020 guidelines and includes 106\npeer-reviewed articles. These publications are analyzed based on modalities,\ncollaboration schemes, and key perception tasks. Through a comparative\nanalysis, this review illustrates how different methods address practical\nissues such as pose errors, temporal latency, communication constraints, domain\nshifts, heterogeneity, and adversarial attacks. Furthermore, it critically\nexamines evaluation methodologies, highlighting a misalignment between current\nmetrics and CP's fundamental objectives. By delving into all relevant topics\nin-depth, this review offers valuable insights into challenges, opportunities,\nand risks, serving as a reference for advancing research in vehicular\ncollaborative perception."}
{"id": "2504.04994", "pdf": "https://arxiv.org/pdf/2504.04994", "abs": "https://arxiv.org/abs/2504.04994", "authors": ["Ling Hu", "Yuemei Xu", "Xiaoyang Gu", "Letao Han"], "title": "Following the Whispers of Values: Unraveling Neural Mechanisms Behind Value-Oriented Behaviors in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite the impressive performance of large language models (LLMs), they can\npresent unintended biases and harmful behaviors driven by encoded values,\nemphasizing the urgent need to understand the value mechanisms behind them.\nHowever, current research primarily evaluates these values through external\nresponses with a focus on AI safety, lacking interpretability and failing to\nassess social values in real-world contexts. In this paper, we propose a novel\nframework called ValueExploration, which aims to explore the behavior-driven\nmechanisms of National Social Values within LLMs at the neuron level. As a case\nstudy, we focus on Chinese Social Values and first construct C-voice, a\nlarge-scale bilingual benchmark for identifying and evaluating Chinese Social\nValues in LLMs. By leveraging C-voice, we then identify and locate the neurons\nresponsible for encoding these values according to activation difference.\nFinally, by deactivating these neurons, we analyze shifts in model behavior,\nuncovering the internal mechanism by which values influence LLM\ndecision-making. Extensive experiments on four representative LLMs validate the\nefficacy of our framework. The benchmark and code will be available."}
{"id": "2504.04633", "pdf": "https://arxiv.org/pdf/2504.04633", "abs": "https://arxiv.org/abs/2504.04633", "authors": ["Yanshu Li", "Hongyang He", "Yi Cao", "Qisen Cheng", "Xiang Fu", "Ruixiang Tang"], "title": "M2IV: Towards Efficient and Fine-grained Multimodal In-Context Learning in Large Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "Preprint, 28 pages, 10 figures, 15 tables", "summary": "Multimodal in-context learning (ICL) is a vital capability for Large\nVision-Language Models (LVLMs), allowing task adaptation via contextual prompts\nwithout parameter retraining. However, its application is hindered by the\ntoken-intensive nature of inputs and the high complexity of cross-modal\nfew-shot learning, which limits the expressive power of representation methods.\nTo tackle these challenges, we propose \\textbf{M2IV}, a method that substitutes\nexplicit demonstrations with learnable \\textbf{I}n-context \\textbf{V}ectors\ndirectly integrated into LVLMs. By exploiting the complementary strengths of\nmulti-head attention (\\textbf{M}HA) and multi-layer perceptrons (\\textbf{M}LP),\nM2IV achieves robust cross-modal fidelity and fine-grained semantic\ndistillation through training. This significantly enhances performance across\ndiverse LVLMs and tasks and scales efficiently to many-shot scenarios,\nbypassing the context window limitations. We also introduce \\textbf{VLibrary},\na repository for storing and retrieving M2IV, enabling flexible LVLM steering\nfor tasks like cross-modal alignment, customized generation and safety\nimprovement. Experiments across seven benchmarks and three LVLMs show that M2IV\nsurpasses Vanilla ICL and prior representation engineering approaches, with an\naverage accuracy gain of \\textbf{3.74\\%} over ICL with the same shot count,\nalongside substantial efficiency advantages."}
{"id": "2504.05008", "pdf": "https://arxiv.org/pdf/2504.05008", "abs": "https://arxiv.org/abs/2504.05008", "authors": ["Anastasiia Ivanova", "Natalia Fedorova", "Sergey Tilga", "Ekaterina Artemova"], "title": "Surveying Professional Writers on AI: Limitations, Expectations, and Fears", "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": null, "summary": "The rapid development of AI-driven tools, particularly large language models\n(LLMs), is reshaping professional writing. Still, key aspects of their adoption\nsuch as languages support, ethics, and long-term impact on writers voice and\ncreativity remain underexplored. In this work, we conducted a questionnaire (N\n= 301) and an interactive survey (N = 36) targeting professional writers\nregularly using AI. We examined LLM-assisted writing practices across 25+\nlanguages, ethical concerns, and user expectations. The findings of the survey\ndemonstrate important insights, reflecting upon the importance of: LLMs\nadoption for non-English speakers; the degree of misinformation, domain and\nstyle adaptation; usability and key features of LLMs. These insights can guide\nfurther development, benefiting both writers and a broader user base."}
{"id": "2504.04653", "pdf": "https://arxiv.org/pdf/2504.04653", "abs": "https://arxiv.org/abs/2504.04653", "authors": ["Yimu Wang", "Mozhgan Nasr Azadani", "Sean Sedwards", "Krzysztof Czarnecki"], "title": "LEO-MINI: An Efficient Multimodal Large Language Model using Conditional Token Reduction and Mixture of Multi-Modal Experts", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Redundancy of visual tokens in multi-modal large language models (MLLMs)\nsignificantly reduces their computational efficiency. Recent approaches, such\nas resamplers and summarizers, have sought to reduce the number of visual\ntokens, but at the cost of visual reasoning ability. To address this, we\npropose LEO-MINI, a novel MLLM that significantly reduces the number of visual\ntokens and simultaneously boosts visual reasoning capabilities. For efficiency,\nLEO-MINI incorporates CoTR, a novel token reduction module to consolidate a\nlarge number of visual tokens into a smaller set of tokens, using the\nsimilarity between visual tokens, text tokens, and a compact learnable query.\nFor effectiveness, to scale up the model's ability with minimal computational\noverhead, LEO-MINI employs MMoE, a novel mixture of multi-modal experts module.\nMMOE employs a set of LoRA experts with a novel router to switch between them\nbased on the input text and visual tokens instead of only using the input\nhidden state. MMoE also includes a general LoRA expert that is always activated\nto learn general knowledge for LLM reasoning. For extracting richer visual\nfeatures, MMOE employs a set of vision experts trained on diverse\ndomain-specific data. To demonstrate LEO-MINI's improved efficiency and\nperformance, we evaluate it against existing efficient MLLMs on various\nbenchmark vision-language tasks."}
{"id": "2504.05020", "pdf": "https://arxiv.org/pdf/2504.05020", "abs": "https://arxiv.org/abs/2504.05020", "authors": ["Charco Hui", "Yalu Wen"], "title": "Batch Aggregation: An Approach to Enhance Text Classification with Correlated Augmented Data", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Natural language processing models often face challenges due to limited\nlabeled data, especially in domain specific areas, e.g., clinical trials. To\novercome this, text augmentation techniques are commonly used to increases\nsample size by transforming the original input data into artificial ones with\nthe label preserved. However, traditional text classification methods ignores\nthe relationship between augmented texts and treats them as independent samples\nwhich may introduce classification error. Therefore, we propose a novel\napproach called 'Batch Aggregation' (BAGG) which explicitly models the\ndependence of text inputs generated through augmentation by incorporating an\nadditional layer that aggregates results from correlated texts. Through\nstudying multiple benchmark data sets across different domains, we found that\nBAGG can improve classification accuracy. We also found that the increase of\nperformance with BAGG is more obvious in domain specific data sets, with\naccuracy improvements of up to 10-29%. Through the analysis of benchmark data,\nthe proposed method addresses limitations of traditional techniques and\nimproves robustness in text classification tasks. Our result demonstrates that\nBAGG offers more robust results and outperforms traditional approaches when\ntraining data is limited."}
{"id": "2504.04658", "pdf": "https://arxiv.org/pdf/2504.04658", "abs": "https://arxiv.org/abs/2504.04658", "authors": ["Haisheng Fu", "Jie Liang", "Feng Liang", "Zhenman Fang", "Guohe Zhang", "Jingning Han"], "title": "3DM-WeConvene: Learned Image Compression with 3D Multi-Level Wavelet-Domain Convolution and Entropy Model", "categories": ["cs.CV", "stat.AP"], "comment": "13 pages", "summary": "Learned image compression (LIC) has recently made significant progress,\nsurpassing traditional methods. However, most LIC approaches operate mainly in\nthe spatial domain and lack mechanisms for reducing frequency-domain\ncorrelations. To address this, we propose a novel framework that integrates\nlow-complexity 3D multi-level Discrete Wavelet Transform (DWT) into\nconvolutional layers and entropy coding, reducing both spatial and channel\ncorrelations to improve frequency selectivity and rate-distortion (R-D)\nperformance.\n  Our proposed 3D multi-level wavelet-domain convolution (3DM-WeConv) layer\nfirst applies 3D multi-level DWT (e.g., 5/3 and 9/7 wavelets from JPEG 2000) to\ntransform data into the wavelet domain. Then, different-sized convolutions are\napplied to different frequency subbands, followed by inverse 3D DWT to restore\nthe spatial domain. The 3DM-WeConv layer can be flexibly used within existing\nCNN-based LIC models.\n  We also introduce a 3D wavelet-domain channel-wise autoregressive entropy\nmodel (3DWeChARM), which performs slice-based entropy coding in the 3D DWT\ndomain. Low-frequency (LF) slices are encoded first to provide priors for\nhigh-frequency (HF) slices.\n  A two-step training strategy is adopted: first balancing LF and HF rates,\nthen fine-tuning with separate weights.\n  Extensive experiments demonstrate that our framework consistently outperforms\nstate-of-the-art CNN-based LIC methods in R-D performance and computational\ncomplexity, with larger gains for high-resolution images. On the Kodak, Tecnick\n100, and CLIC test sets, our method achieves BD-Rate reductions of -12.24%,\n-15.51%, and -12.97%, respectively, compared to H.266/VVC."}
{"id": "2504.05050", "pdf": "https://arxiv.org/pdf/2504.05050", "abs": "https://arxiv.org/abs/2504.05050", "authors": ["Jiawei Lian", "Jianhong Pan", "Lefan Wang", "Yi Wang", "Shaohui Mei", "Lap-Pui Chau"], "title": "Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are foundational explorations to artificial\ngeneral intelligence, yet their alignment with human values via instruction\ntuning and preference learning achieves only superficial compliance. Here, we\ndemonstrate that harmful knowledge embedded during pretraining persists as\nindelible \"dark patterns\" in LLMs' parametric memory, evading alignment\nsafeguards and resurfacing under adversarial inducement at distributional\nshifts. In this study, we first theoretically analyze the intrinsic ethical\nvulnerability of aligned LLMs by proving that current alignment methods yield\nonly local \"safety regions\" in the knowledge manifold. In contrast, pretrained\nknowledge remains globally connected to harmful concepts via high-likelihood\nadversarial trajectories. Building on this theoretical insight, we empirically\nvalidate our findings by employing semantic coherence inducement under\ndistributional shifts--a method that systematically bypasses alignment\nconstraints through optimized adversarial prompts. This combined theoretical\nand empirical approach achieves a 100% attack success rate across 19 out of 23\nstate-of-the-art aligned LLMs, including DeepSeek-R1 and LLaMA-3, revealing\ntheir universal vulnerabilities."}
{"id": "2504.04676", "pdf": "https://arxiv.org/pdf/2504.04676", "abs": "https://arxiv.org/abs/2504.04676", "authors": ["Bo Li", "Jing Yun"], "title": "Dual Consistent Constraint via Disentangled Consistency and Complementarity for Multi-view Clustering", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multi-view clustering can explore common semantics from multiple views and\nhas received increasing attention in recent years. However, current methods\nfocus on learning consistency in representation, neglecting the contribution of\neach view's complementarity aspect in representation learning. This limit poses\na significant challenge in multi-view representation learning. This paper\nproposes a novel multi-view clustering framework that introduces a disentangled\nvariational autoencoder that separates multi-view into shared and private\ninformation, i.e., consistency and complementarity information. We first learn\ninformative and consistent representations by maximizing mutual information\nacross different views through contrastive learning. This process will ignore\ncomplementary information. Then, we employ consistency inference constraints to\nexplicitly utilize complementary information when attempting to seek the\nconsistency of shared information across all views. Specifically, we perform a\nwithin-reconstruction using the private and shared information of each view and\na cross-reconstruction using the shared information of all views. The dual\nconsistency constraints are not only effective in improving the representation\nquality of data but also easy to extend to other scenarios, especially in\ncomplex multi-view scenes. This could be the first attempt to employ dual\nconsistent constraint in a unified MVC theoretical framework. During the\ntraining procedure, the consistency and complementarity features are jointly\noptimized. Extensive experiments show that our method outperforms baseline\nmethods."}
{"id": "2504.05058", "pdf": "https://arxiv.org/pdf/2504.05058", "abs": "https://arxiv.org/abs/2504.05058", "authors": ["Aravind Krishnan", "Siva Reddy", "Marius Mosbach"], "title": "Not All Data Are Unlearned Equally", "categories": ["cs.CL"], "comment": null, "summary": "Machine unlearning is concerned with the task of removing knowledge learned\nfrom particular data points from a trained model. In the context of large\nlanguage models (LLMs), unlearning has recently received increased attention,\nparticularly for removing knowledge about named entities from models for\nprivacy purposes. While various approaches have been proposed to address the\nunlearning problem, most existing approaches treat all data points to be\nunlearned equally, i.e., unlearning that Montreal is a city in Canada is\ntreated exactly the same as unlearning the phone number of the first author of\nthis paper. In this work, we show that this all data is equal assumption does\nnot hold for LLM unlearning. We study how the success of unlearning depends on\nthe frequency of the knowledge we want to unlearn in the pre-training data of a\nmodel and find that frequency strongly affects unlearning, i.e., more frequent\nknowledge is harder to unlearn. Additionally, we uncover a misalignment between\nprobability and generation-based evaluations of unlearning and show that this\nproblem worsens as models become larger. Overall, our experiments highlight the\nneed for better evaluation practices and novel methods for LLM unlearning that\ntake the training data of models into account."}
{"id": "2504.04679", "pdf": "https://arxiv.org/pdf/2504.04679", "abs": "https://arxiv.org/abs/2504.04679", "authors": ["Wanzhou Liu", "Zhexiao Xiong", "Xinyu Li", "Nathan Jacobs"], "title": "DeclutterNeRF: Generative-Free 3D Scene Recovery for Occlusion Removal", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025 4th CV4Metaverse Workshop. 15 pages, 10\n  figures. Code and data at: https://github.com/wanzhouliu/declutter-nerf", "summary": "Recent novel view synthesis (NVS) techniques, including Neural Radiance\nFields (NeRF) and 3D Gaussian Splatting (3DGS) have greatly advanced 3D scene\nreconstruction with high-quality rendering and realistic detail recovery.\nEffectively removing occlusions while preserving scene details can further\nenhance the robustness and applicability of these techniques. However, existing\napproaches for object and occlusion removal predominantly rely on generative\npriors, which, despite filling the resulting holes, introduce new artifacts and\nblurriness. Moreover, existing benchmark datasets for evaluating occlusion\nremoval methods lack realistic complexity and viewpoint variations. To address\nthese issues, we introduce DeclutterSet, a novel dataset featuring diverse\nscenes with pronounced occlusions distributed across foreground, midground, and\nbackground, exhibiting substantial relative motion across viewpoints. We\nfurther introduce DeclutterNeRF, an occlusion removal method free from\ngenerative priors. DeclutterNeRF introduces joint multi-view optimization of\nlearnable camera parameters, occlusion annealing regularization, and employs an\nexplainable stochastic structural similarity loss, ensuring high-quality,\nartifact-free reconstructions from incomplete images. Experiments demonstrate\nthat DeclutterNeRF significantly outperforms state-of-the-art methods on our\nproposed DeclutterSet, establishing a strong baseline for future research."}
{"id": "2504.05074", "pdf": "https://arxiv.org/pdf/2504.05074", "abs": "https://arxiv.org/abs/2504.05074", "authors": ["Venkat Srinivasan", "Vishaal Jatav", "Anushka Chandrababu", "Geetika Sharma"], "title": "On the Performance of an Explainable Language Model on PubMedQA", "categories": ["cs.CL"], "comment": "Working Paper", "summary": "Large language models (LLMs) have shown significant abilities in retrieving\nmedical knowledge, reasoning over it and answering medical questions comparably\nto physicians. However, these models are not interpretable, hallucinate, are\ndifficult to maintain and require enormous compute resources for training and\ninference. In this paper, we report results from Gyan, an explainable language\nmodel based on an alternative architecture, on the PubmedQA data set. The Gyan\nLLM is a compositional language model and the model is decoupled from\nknowledge. Gyan is trustable, transparent, does not hallucinate and does not\nrequire significant training or compute resources. Gyan is easily transferable\nacross domains. Gyan-4.3 achieves SOTA results on PubmedQA with 87.1% accuracy\ncompared to 82% by MedPrompt based on GPT-4 and 81.8% by Med-PaLM 2 (Google and\nDeepMind). We will be reporting results for other medical data sets - MedQA,\nMedMCQA, MMLU - Medicine in the future."}
{"id": "2504.04687", "pdf": "https://arxiv.org/pdf/2504.04687", "abs": "https://arxiv.org/abs/2504.04687", "authors": ["Yicheng Leng", "Chaowei Fang", "Junye Chen", "Yixiang Fang", "Sheng Li", "Guanbin Li"], "title": "Bridging Knowledge Gap Between Image Inpainting and Large-Area Visible Watermark Removal", "categories": ["cs.CV", "cs.AI", "cs.MM", "eess.IV", "I.2.10; I.4.4; I.4.5"], "comment": "To be published in AAAI 2025", "summary": "Visible watermark removal which involves watermark cleaning and background\ncontent restoration is pivotal to evaluate the resilience of watermarks.\nExisting deep neural network (DNN)-based models still struggle with large-area\nwatermarks and are overly dependent on the quality of watermark mask\nprediction. To overcome these challenges, we introduce a novel feature adapting\nframework that leverages the representation modeling capacity of a pre-trained\nimage inpainting model. Our approach bridges the knowledge gap between image\ninpainting and watermark removal by fusing information of the residual\nbackground content beneath watermarks into the inpainting backbone model. We\nestablish a dual-branch system to capture and embed features from the residual\nbackground content, which are merged into intermediate features of the\ninpainting backbone model via gated feature fusion modules. Moreover, for\nrelieving the dependence on high-quality watermark masks, we introduce a new\ntraining paradigm by utilizing coarse watermark masks to guide the inference\nprocess. This contributes to a visible image removal model which is insensitive\nto the quality of watermark mask during testing. Extensive experiments on both\na large-scale synthesized dataset and a real-world dataset demonstrate that our\napproach significantly outperforms existing state-of-the-art methods. The\nsource code is available in the supplementary materials."}
{"id": "2504.05081", "pdf": "https://arxiv.org/pdf/2504.05081", "abs": "https://arxiv.org/abs/2504.05081", "authors": ["Tianshi Zheng", "Yixiang Chen", "Chengxi Li", "Chunyang Li", "Qing Zong", "Haochen Shi", "Baixuan Xu", "Yangqiu Song", "Ginny Y. Wong", "Simon See"], "title": "The Curse of CoT: On the Limitations of Chain-of-Thought in In-Context Learning", "categories": ["cs.CL"], "comment": "30 pages, 12 tables, 6 figures", "summary": "Chain-of-Thought (CoT) prompting has been widely recognized for its ability\nto enhance reasoning capabilities in large language models (LLMs) through the\ngeneration of explicit explanatory rationales. However, our study reveals a\nsurprising contradiction to this prevailing perspective. Through extensive\nexperiments involving 16 state-of-the-art LLMs and nine diverse pattern-based\nin-context learning (ICL) datasets, we demonstrate that CoT and its reasoning\nvariants consistently underperform direct answering across varying model scales\nand benchmark complexities. To systematically investigate this unexpected\nphenomenon, we designed extensive experiments to validate several hypothetical\nexplanations. Our analysis uncovers a fundamental explicit-implicit duality\ndriving CoT's performance in pattern-based ICL: while explicit reasoning\nfalters due to LLMs' struggles to infer underlying patterns from\ndemonstrations, implicit reasoning-disrupted by the increased contextual\ndistance of CoT rationales-often compensates, delivering correct answers\ndespite flawed rationales. This duality explains CoT's relative\nunderperformance, as noise from weak explicit inference undermines the process,\neven as implicit mechanisms partially salvage outcomes. Notably, even long-CoT\nreasoning models, which excel in abstract and symbolic reasoning, fail to fully\novercome these limitations despite higher computational costs. Our findings\nchallenge existing assumptions regarding the universal efficacy of CoT,\nyielding novel insights into its limitations and guiding future research toward\nmore nuanced and effective reasoning methodologies for LLMs."}
{"id": "2504.04701", "pdf": "https://arxiv.org/pdf/2504.04701", "abs": "https://arxiv.org/abs/2504.04701", "authors": ["Bo-Wen Yin", "Jiao-Long Cao", "Ming-Ming Cheng", "Qibin Hou"], "title": "DFormerv2: Geometry Self-Attention for RGBD Semantic Segmentation", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Recent advances in scene understanding benefit a lot from depth maps because\nof the 3D geometry information, especially in complex conditions (e.g., low\nlight and overexposed). Existing approaches encode depth maps along with RGB\nimages and perform feature fusion between them to enable more robust\npredictions. Taking into account that depth can be regarded as a geometry\nsupplement for RGB images, a straightforward question arises: Do we really need\nto explicitly encode depth information with neural networks as done for RGB\nimages? Based on this insight, in this paper, we investigate a new way to learn\nRGBD feature representations and present DFormerv2, a strong RGBD encoder that\nexplicitly uses depth maps as geometry priors rather than encoding depth\ninformation with neural networks. Our goal is to extract the geometry clues\nfrom the depth and spatial distances among all the image patch tokens, which\nwill then be used as geometry priors to allocate attention weights in\nself-attention. Extensive experiments demonstrate that DFormerv2 exhibits\nexceptional performance in various RGBD semantic segmentation benchmarks. Code\nis available at: https://github.com/VCIP-RGBD/DFormer."}
{"id": "2504.05097", "pdf": "https://arxiv.org/pdf/2504.05097", "abs": "https://arxiv.org/abs/2504.05097", "authors": ["Liu Xiao", "Li Zhiyuan", "Lin Yueyu"], "title": "State Tuning: State-based Test-Time Scaling on RWKV-7", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Test-time scaling has emerged as a prominent research direction in machine\nlearning, enabling models to enhance their expressive capabilities during\ninference.Transformers, renowned for striking a delicate balance between\nefficiency and expressiveness, have benefited from test-time scaling techniques\nthat leverage an expanding key-value (KV) cache to significantly improve\nperformance.In this paper, we introduce a novel state-based approach to\ntest-time scaling, which we term state tuning, tailored to the RNN-based RWKV-7\nmodel.By exploiting the unique strengths of RWKV-7, our method achieves\nstate-of-the-art performance on the target task without altering the model's\npre-trained weights. Our approach centers on three key innovations. First, we\ndevelop an observer framework that allows a smaller model to replicate and\nlearn the state dynamics of the RWKV-7 model. Second, we employ a kernel method\nto dynamically upscale the state size, enhancing the model's capacity to\ncapture intricate patterns. Third, we integrate Decorrelated Backpropagation\n(DBP) to optimize the upscaled state matrix, thereby improving convergence and\nexpressivity. By tuning only the state matrix, we demonstrate that a smaller\nmodel can outperform larger models on the given task. This method preserves the\nefficiency of the original RWKV-7 architecture while harnessing the power of\ntest-time scaling to deliver superior results. Our findings underscore the\npotential of state tuning as an effective strategy for advancing model\nperformance in resource-constrained settings. Our code is\nhttps://github.com/TorchRWKV/flash-linear-attention."}
{"id": "2504.04708", "pdf": "https://arxiv.org/pdf/2504.04708", "abs": "https://arxiv.org/abs/2504.04708", "authors": ["Minchul Kim", "Dingqiang Ye", "Yiyang Su", "Feng Liu", "Xiaoming Liu"], "title": "SapiensID: Foundation for Human Recognition", "categories": ["cs.CV"], "comment": "To appear in CVPR2025", "summary": "Existing human recognition systems often rely on separate, specialized models\nfor face and body analysis, limiting their effectiveness in real-world\nscenarios where pose, visibility, and context vary widely. This paper\nintroduces SapiensID, a unified model that bridges this gap, achieving robust\nperformance across diverse settings. SapiensID introduces (i) Retina Patch\n(RP), a dynamic patch generation scheme that adapts to subject scale and\nensures consistent tokenization of regions of interest, (ii) a masked\nrecognition model (MRM) that learns from variable token length, and (iii)\nSemantic Attention Head (SAH), an module that learns pose-invariant\nrepresentations by pooling features around key body parts. To facilitate\ntraining, we introduce WebBody4M, a large-scale dataset capturing diverse poses\nand scale variations. Extensive experiments demonstrate that SapiensID achieves\nstate-of-the-art results on various body ReID benchmarks, outperforming\nspecialized models in both short-term and long-term scenarios while remaining\ncompetitive with dedicated face recognition systems. Furthermore, SapiensID\nestablishes a strong baseline for the newly introduced challenge of Cross\nPose-Scale ReID, demonstrating its ability to generalize to complex, real-world\nconditions."}
{"id": "2504.05104", "pdf": "https://arxiv.org/pdf/2504.05104", "abs": "https://arxiv.org/abs/2504.05104", "authors": ["Saeid Ario Vaghefi", "Aymane Hachcham", "Veronica Grasso", "Jiska Manicus", "Nakiete Msemo", "Chiara Colesanti Senni", "Markus Leippold"], "title": "AI for Climate Finance: Agentic Retrieval and Multi-Step Reasoning for Early Warning System Investments", "categories": ["cs.CL"], "comment": null, "summary": "Tracking financial investments in climate adaptation is a complex and\nexpertise-intensive task, particularly for Early Warning Systems (EWS), which\nlack standardized financial reporting across multilateral development banks\n(MDBs) and funds. To address this challenge, we introduce an LLM-based agentic\nAI system that integrates contextual retrieval, fine-tuning, and multi-step\nreasoning to extract relevant financial data, classify investments, and ensure\ncompliance with funding guidelines. Our study focuses on a real-world\napplication: tracking EWS investments in the Climate Risk and Early Warning\nSystems (CREWS) Fund. We analyze 25 MDB project documents and evaluate multiple\nAI-driven classification methods, including zero-shot and few-shot learning,\nfine-tuned transformer-based classifiers, chain-of-thought (CoT) prompting, and\nan agent-based retrieval-augmented generation (RAG) approach. Our results show\nthat the agent-based RAG approach significantly outperforms other methods,\nachieving 87\\% accuracy, 89\\% precision, and 83\\% recall. Additionally, we\ncontribute a benchmark dataset and expert-annotated corpus, providing a\nvaluable resource for future research in AI-driven financial tracking and\nclimate finance transparency."}
{"id": "2504.04716", "pdf": "https://arxiv.org/pdf/2504.04716", "abs": "https://arxiv.org/abs/2504.04716", "authors": ["Haoren Zhao", "Tianyi Chen", "Zhen Wang"], "title": "On the Robustness of GUI Grounding Models Against Image Attacks", "categories": ["cs.CV"], "comment": null, "summary": "Graphical User Interface (GUI) grounding models are crucial for enabling\nintelligent agents to understand and interact with complex visual interfaces.\nHowever, these models face significant robustness challenges in real-world\nscenarios due to natural noise and adversarial perturbations, and their\nrobustness remains underexplored. In this study, we systematically evaluate the\nrobustness of state-of-the-art GUI grounding models, such as UGround, under\nthree conditions: natural noise, untargeted adversarial attacks, and targeted\nadversarial attacks. Our experiments, which were conducted across a wide range\nof GUI environments, including mobile, desktop, and web interfaces, have\nclearly demonstrated that GUI grounding models exhibit a high degree of\nsensitivity to adversarial perturbations and low-resolution conditions. These\nfindings provide valuable insights into the vulnerabilities of GUI grounding\nmodels and establish a strong benchmark for future research aimed at enhancing\ntheir robustness in practical applications. Our code is available at\nhttps://github.com/ZZZhr-1/Robust_GUI_Grounding."}
{"id": "2504.05122", "pdf": "https://arxiv.org/pdf/2504.05122", "abs": "https://arxiv.org/abs/2504.05122", "authors": ["Xinglin Lyu", "Wei Tang", "Yuang Li", "Xiaofeng Zhao", "Ming Zhu", "Junhui Li", "Yunfei Lu", "Min Zhang", "Daimeng Wei", "Hao Yang", "Min Zhang"], "title": "DoCIA: An Online Document-Level Context Incorporation Agent for Speech Translation", "categories": ["cs.CL"], "comment": null, "summary": "Document-level context is crucial for handling discourse challenges in\ntext-to-text document-level machine translation (MT). Despite the increased\ndiscourse challenges introduced by noise from automatic speech recognition\n(ASR), the integration of document-level context in speech translation (ST)\nremains insufficiently explored. In this paper, we develop DoCIA, an online\nframework that enhances ST performance by incorporating document-level context.\nDoCIA decomposes the ST pipeline into four stages. Document-level context is\nintegrated into the ASR refinement, MT, and MT refinement stages through\nauxiliary LLM (large language model)-based modules. Furthermore, DoCIA\nleverages document-level information in a multi-level manner while minimizing\ncomputational overhead. Additionally, a simple yet effective determination\nmechanism is introduced to prevent hallucinations from excessive refinement,\nensuring the reliability of the final results. Experimental results show that\nDoCIA significantly outperforms traditional ST baselines in both sentence and\ndiscourse metrics across four LLMs, demonstrating its effectiveness in\nimproving ST performance."}
{"id": "2504.04722", "pdf": "https://arxiv.org/pdf/2504.04722", "abs": "https://arxiv.org/abs/2504.04722", "authors": ["Adnan Khan", "Alireza Choubineh", "Mai A. Shaaban", "Abbas Akkasi", "Majid Komeili"], "title": "TactileNet: Bridging the Accessibility Gap with AI-Generated Tactile Graphics for Individuals with Vision Impairment", "categories": ["cs.CV"], "comment": null, "summary": "Tactile graphics are essential for providing access to visual information for\nthe 43 million people globally living with vision loss, as estimated by global\nprevalence data. However, traditional methods for creating these tactile\ngraphics are labor-intensive and struggle to meet demand. We introduce\nTactileNet, the first comprehensive dataset and AI-driven framework for\ngenerating tactile graphics using text-to-image Stable Diffusion (SD) models.\nBy integrating Low-Rank Adaptation (LoRA) and DreamBooth, our method fine-tunes\nSD models to produce high-fidelity, guideline-compliant tactile graphics while\nreducing computational costs. Evaluations involving tactile experts show that\ngenerated graphics achieve 92.86% adherence to tactile standards and 100%\nalignment with natural images in posture and features. Our framework also\ndemonstrates scalability, generating 32,000 images (7,050 filtered for quality)\nacross 66 classes, with prompt editing enabling customizable outputs (e.g.,\nadding/removing details). Our work empowers designers to focus on refinement,\nsignificantly accelerating accessibility efforts. It underscores the\ntransformative potential of AI for social good, offering a scalable solution to\nbridge the accessibility gap in education and beyond."}
{"id": "2504.05154", "pdf": "https://arxiv.org/pdf/2504.05154", "abs": "https://arxiv.org/abs/2504.05154", "authors": ["Geyang Guo", "Tarek Naous", "Hiromi Wakaki", "Yukiko Nishimura", "Yuki Mitsufuji", "Alan Ritter", "Wei Xu"], "title": "CARE: Aligning Language Models for Regional Cultural Awareness", "categories": ["cs.CL"], "comment": "24 pages", "summary": "Existing language models (LMs) often exhibit a Western-centric bias and\nstruggle to represent diverse cultural knowledge. Previous attempts to address\nthis rely on synthetic data and express cultural knowledge only in English. In\nthis work, we study whether a small amount of human-written, multilingual\ncultural preference data can improve LMs across various model families and\nsizes. We first introduce CARE, a multilingual resource of 24.1k responses with\nhuman preferences on 2,580 questions about Chinese and Arab cultures, all\ncarefully annotated by native speakers and offering more balanced coverage.\nUsing CARE, we demonstrate that cultural alignment improves existing LMs beyond\ngeneric resources without compromising general capabilities. Moreover, we\nevaluate the cultural awareness of LMs, native speakers, and retrieved web\ncontent when queried in different languages. Our experiment reveals regional\ndisparities among LMs, which may also be reflected in the documentation gap:\nnative speakers often take everyday cultural commonsense and social norms for\ngranted, while non-natives are more likely to actively seek out and document\nthem. CARE is publicly available at https://github.com/Guochry/CARE (we plan to\nadd Japanese data in the near future)."}
{"id": "2504.04728", "pdf": "https://arxiv.org/pdf/2504.04728", "abs": "https://arxiv.org/abs/2504.04728", "authors": ["Sheng Zheng", "Chaoning Zhang", "Dongshen Han", "Fachrina Dewi Puspitasari", "Xinhong Hao", "Yang Yang", "Heng Tao Shen"], "title": "Exploring Kernel Transformations for Implicit Neural Representations", "categories": ["cs.CV"], "comment": "Accepted at IEEE Transactions on Multimedia (TMM) on December 20,\n  2024 (To appear on IEEE Website soon)", "summary": "Implicit neural representations (INRs), which leverage neural networks to\nrepresent signals by mapping coordinates to their corresponding attributes,\nhave garnered significant attention. They are extensively utilized for image\nrepresentation, with pixel coordinates as input and pixel values as output. In\ncontrast to prior works focusing on investigating the effect of the model's\ninside components (activation function, for instance), this work pioneers the\nexploration of the effect of kernel transformation of input/output while\nkeeping the model itself unchanged. A byproduct of our findings is a simple yet\neffective method that combines scale and shift to significantly boost INR with\nnegligible computation overhead. Moreover, we present two perspectives, depth\nand normalization, to interpret the performance benefits caused by scale and\nshift transformation. Overall, our work provides a new avenue for future works\nto understand and improve INR through the lens of kernel transformation."}
{"id": "2504.05185", "pdf": "https://arxiv.org/pdf/2504.05185", "abs": "https://arxiv.org/abs/2504.05185", "authors": ["Mehdi Fatemi", "Banafsheh Rafiee", "Mingjie Tang", "Kartik Talamadupula"], "title": "Concise Reasoning via Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "Despite significant advancements in large language models (LLMs), a major\ndrawback of reasoning models is their enormous token usage, which increases\ncomputational cost, resource requirements, and response time. In this work, we\nrevisit the core principles of reinforcement learning (RL) and, through\nmathematical analysis, demonstrate that the tendency to generate lengthy\nresponses arises inherently from RL-based optimization during training. This\nfinding questions the prevailing assumption that longer responses inherently\nimprove reasoning accuracy. Instead, we uncover a natural correlation between\nconciseness and accuracy that has been largely overlooked. Moreover, we show\nthat introducing a secondary phase of RL post-training, using a small set of\nproblems and limited resources, can significantly reduce a model's chain of\nthought while maintaining or even enhancing accuracy. Finally, we validate our\nconclusions through extensive experimental results."}
{"id": "2504.04732", "pdf": "https://arxiv.org/pdf/2504.04732", "abs": "https://arxiv.org/abs/2504.04732", "authors": ["Zhenxing Ming", "Julie Stephany Berrio", "Mao Shan", "Stewart Worrall"], "title": "Inverse++: Vision-Centric 3D Semantic Occupancy Prediction Assisted with 3D Object Detection", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "3D semantic occupancy prediction aims to forecast detailed geometric and\nsemantic information of the surrounding environment for autonomous vehicles\n(AVs) using onboard surround-view cameras. Existing methods primarily focus on\nintricate inner structure module designs to improve model performance, such as\nefficient feature sampling and aggregation processes or intermediate feature\nrepresentation formats. In this paper, we explore multitask learning by\nintroducing an additional 3D supervision signal by incorporating an additional\n3D object detection auxiliary branch. This extra 3D supervision signal enhances\nthe model's overall performance by strengthening the capability of the\nintermediate features to capture small dynamic objects in the scene, and these\nsmall dynamic objects often include vulnerable road users, i.e. bicycles,\nmotorcycles, and pedestrians, whose detection is crucial for ensuring driving\nsafety in autonomous vehicles. Extensive experiments conducted on the nuScenes\ndatasets, including challenging rainy and nighttime scenarios, showcase that\nour approach attains state-of-the-art results, achieving an IoU score of 31.73%\nand a mIoU score of 20.91% and excels at detecting vulnerable road users (VRU).\nThe code will be made available at:https://github.com/DanielMing123/Inverse++"}
{"id": "2504.05211", "pdf": "https://arxiv.org/pdf/2504.05211", "abs": "https://arxiv.org/abs/2504.05211", "authors": ["Richard A. Blythe", "Casimir Fisch"], "title": "Exploiting individual differences to bootstrap communication", "categories": ["cs.CL", "physics.soc-ph", "q-bio.PE"], "comment": "13 pages including supplementary information, 3 figures", "summary": "Establishing a communication system is hard because the intended meaning of a\nsignal is unknown to its receiver when first produced, and the signaller also\nhas no idea how that signal will be interpreted. Most theoretical accounts of\nthe emergence of communication systems rely on feedback to reinforce behaviours\nthat have led to successful communication in the past. However, providing such\nfeedback requires already being able to communicate the meaning that was\nintended or interpreted. Therefore these accounts cannot explain how\ncommunication can be bootstrapped from non-communicative behaviours. Here we\npresent a model that shows how a communication system, capable of expressing an\nunbounded number of meanings, can emerge as a result of individual behavioural\ndifferences in a large population without any pre-existing means to determine\ncommunicative success. The two key cognitive capabilities responsible for this\noutcome are behaving predictably in a given situation, and an alignment of\npsychological states ahead of signal production that derives from shared\nintentionality. Since both capabilities can exist independently of\ncommunication, our results are compatible with theories in which large flexible\nsocially-learned communication systems like language are the product of a\ngeneral but well-developed capacity for social cognition."}
{"id": "2504.04740", "pdf": "https://arxiv.org/pdf/2504.04740", "abs": "https://arxiv.org/abs/2504.04740", "authors": ["Samarth Mishra", "Kate Saenko", "Venkatesh Saligrama"], "title": "Enhancing Compositional Reasoning in Vision-Language Models with Synthetic Preference Data", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Compositionality, or correctly recognizing scenes as compositions of atomic\nvisual concepts, remains difficult for multimodal large language models\n(MLLMs). Even state of the art MLLMs such as GPT-4o can make mistakes in\ndistinguishing compositions like \"dog chasing cat\" vs \"cat chasing dog\". While\non Winoground, a benchmark for measuring such reasoning, MLLMs have made\nsignificant progress, they are still far from a human's performance. We show\nthat compositional reasoning in these models can be improved by elucidating\nsuch concepts via data, where a model is trained to prefer the correct caption\nfor an image over a close but incorrect one. We introduce SCRAMBLe: Synthetic\nCompositional Reasoning Augmentation of MLLMs with Binary preference Learning,\nan approach for preference tuning open-weight MLLMs on synthetic preference\ndata generated in a fully automated manner from existing image-caption data.\nSCRAMBLe holistically improves these MLLMs' compositional reasoning\ncapabilities which we can see through significant improvements across multiple\nvision language compositionality benchmarks, as well as smaller but significant\nimprovements on general question answering tasks. As a sneak peek, SCRAMBLe\ntuned Molmo-7B model improves on Winoground from 49.5% to 54.8% (best reported\nto date), while improving by ~1% on more general visual question answering\ntasks. Code for SCRAMBLe along with tuned models and our synthetic training\ndataset is available at https://github.com/samarth4149/SCRAMBLe."}
{"id": "2504.05214", "pdf": "https://arxiv.org/pdf/2504.05214", "abs": "https://arxiv.org/abs/2504.05214", "authors": ["Sefika Efeoglu", "Adrian Paschke", "Sonja Schimmler"], "title": "Post-Training Language Models for Continual Relation Extraction", "categories": ["cs.CL"], "comment": "17 pages", "summary": "Real-world data, such as news articles, social media posts, and chatbot\nconversations, is inherently dynamic and non-stationary, presenting significant\nchallenges for constructing real-time structured representations through\nknowledge graphs (KGs). Relation Extraction (RE), a fundamental component of KG\ncreation, often struggles to adapt to evolving data when traditional models\nrely on static, outdated datasets. Continual Relation Extraction (CRE) methods\ntackle this issue by incrementally learning new relations while preserving\npreviously acquired knowledge. This study investigates the application of\npre-trained language models (PLMs), specifically large language models (LLMs),\nto CRE, with a focus on leveraging memory replay to address catastrophic\nforgetting. We evaluate decoder-only models (eg, Mistral-7B and Llama2-7B) and\nencoder-decoder models (eg, Flan-T5 Base) on the TACRED and FewRel datasets.\nTask-incremental fine-tuning of LLMs demonstrates superior performance over\nearlier approaches using encoder-only models like BERT on TACRED, excelling in\nseen-task accuracy and overall performance (measured by whole and average\naccuracy), particularly with the Mistral and Flan-T5 models. Results on FewRel\nare similarly promising, achieving second place in whole and average accuracy\nmetrics. This work underscores critical factors in knowledge transfer, language\nmodel architecture, and KG completeness, advancing CRE with LLMs and memory\nreplay for dynamic, real-time relation extraction."}
{"id": "2504.04743", "pdf": "https://arxiv.org/pdf/2504.04743", "abs": "https://arxiv.org/abs/2504.04743", "authors": ["Xiongbo Lu", "Yaxiong Chen", "Shengwu Xiong"], "title": "AnyArtisticGlyph: Multilingual Controllable Artistic Glyph Generation", "categories": ["cs.CV"], "comment": null, "summary": "Artistic Glyph Image Generation (AGIG) differs from current\ncreativity-focused generation models by offering finely controllable\ndeterministic generation. It transfers the style of a reference image to a\nsource while preserving its content. Although advanced and promising, current\nmethods may reveal flaws when scrutinizing synthesized image details, often\nproducing blurred or incorrect textures, posing a significant challenge. Hence,\nwe introduce AnyArtisticGlyph, a diffusion-based, multilingual controllable\nartistic glyph generation model. It includes a font fusion and embedding\nmodule, which generates latent features for detailed structure creation, and a\nvision-text fusion and embedding module that uses the CLIP model to encode\nreferences and blends them with transformation caption embeddings for seamless\nglobal image generation. Moreover, we incorporate a coarse-grained\nfeature-level loss to enhance generation accuracy. Experiments show that it\nproduces natural, detailed artistic glyph images with state-of-the-art\nperformance. Our project will be open-sourced on\nhttps://github.com/jiean001/AnyArtisticGlyph to advance text generation\ntechnology."}
{"id": "2504.05226", "pdf": "https://arxiv.org/pdf/2504.05226", "abs": "https://arxiv.org/abs/2504.05226", "authors": ["Jungyeul Park"], "title": "Proposing TAGbank as a Corpus of Tree-Adjoining Grammar Derivations", "categories": ["cs.CL"], "comment": null, "summary": "The development of lexicalized grammars, particularly Tree-Adjoining Grammar\n(TAG), has significantly advanced our understanding of syntax and semantics in\nnatural language processing (NLP). While existing syntactic resources like the\nPenn Treebank and Universal Dependencies offer extensive annotations for\nphrase-structure and dependency parsing, there is a lack of large-scale corpora\ngrounded in lexicalized grammar formalisms. To address this gap, we introduce\nTAGbank, a corpus of TAG derivations automatically extracted from existing\nsyntactic treebanks. This paper outlines a methodology for mapping\nphrase-structure annotations to TAG derivations, leveraging the generative\npower of TAG to support parsing, grammar induction, and semantic analysis. Our\napproach builds on the work of CCGbank, extending it to incorporate the unique\nstructural properties of TAG, including its transparent derivation trees and\nits ability to capture long-distance dependencies. We also discuss the\nchallenges involved in the extraction process, including ensuring consistency\nacross treebank schemes and dealing with language-specific syntactic\nidiosyncrasies. Finally, we propose the future extension of TAGbank to include\nmultilingual corpora, focusing on the Penn Korean and Penn Chinese Treebanks,\nto explore the cross-linguistic application of TAG's formalism. By providing a\nrobust, derivation-based resource, TAGbank aims to support a wide range of\ncomputational tasks and contribute to the theoretical understanding of TAG's\ngenerative capacity."}
{"id": "2504.04744", "pdf": "https://arxiv.org/pdf/2504.04744", "abs": "https://arxiv.org/abs/2504.04744", "authors": ["He Zhu", "Quyu Kong", "Kechun Xu", "Xunlong Xia", "Bing Deng", "Jieping Ye", "Rong Xiong", "Yue Wang"], "title": "Grounding 3D Object Affordance with Language Instructions, Visual Observations and Interactions", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "CVPR 2025", "summary": "Grounding 3D object affordance is a task that locates objects in 3D space\nwhere they can be manipulated, which links perception and action for embodied\nintelligence. For example, for an intelligent robot, it is necessary to\naccurately ground the affordance of an object and grasp it according to human\ninstructions. In this paper, we introduce a novel task that grounds 3D object\naffordance based on language instructions, visual observations and\ninteractions, which is inspired by cognitive science. We collect an Affordance\nGrounding dataset with Points, Images and Language instructions (AGPIL) to\nsupport the proposed task. In the 3D physical world, due to observation\norientation, object rotation, or spatial occlusion, we can only get a partial\nobservation of the object. So this dataset includes affordance estimations of\nobjects from full-view, partial-view, and rotation-view perspectives. To\naccomplish this task, we propose LMAffordance3D, the first multi-modal,\nlanguage-guided 3D affordance grounding network, which applies a\nvision-language model to fuse 2D and 3D spatial features with semantic\nfeatures. Comprehensive experiments on AGPIL demonstrate the effectiveness and\nsuperiority of our method on this task, even in unseen experimental settings.\nOur project is available at https://sites.google.com/view/lmaffordance3d."}
{"id": "2504.05228", "pdf": "https://arxiv.org/pdf/2504.05228", "abs": "https://arxiv.org/abs/2504.05228", "authors": ["Yiming Zhang", "Harshita Diddee", "Susan Holm", "Hanchen Liu", "Xinyue Liu", "Vinay Samuel", "Barry Wang", "Daphne Ippolito"], "title": "NoveltyBench: Evaluating Creativity and Diversity in Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Language models have demonstrated remarkable capabilities on standard\nbenchmarks, yet they struggle increasingly from mode collapse, the inability to\ngenerate diverse and novel outputs. Our work introduces NoveltyBench, a\nbenchmark specifically designed to evaluate the ability of language models to\nproduce multiple distinct and high-quality outputs. NoveltyBench utilizes\nprompts curated to elicit diverse answers and filtered real-world user queries.\nEvaluating 20 leading language models, we find that current state-of-the-art\nsystems generate significantly less diversity than human writers. Notably,\nlarger models within a family often exhibit less diversity than their smaller\ncounterparts, challenging the notion that capability on standard benchmarks\ntranslates directly to generative utility. While prompting strategies like\nin-context regeneration can elicit diversity, our findings highlight a\nfundamental lack of distributional diversity in current models, reducing their\nutility for users seeking varied responses and suggesting the need for new\ntraining and evaluation paradigms that prioritize creativity alongside quality."}
{"id": "2504.04747", "pdf": "https://arxiv.org/pdf/2504.04747", "abs": "https://arxiv.org/abs/2504.04747", "authors": ["Yoojin Jung", "Byung Cheol Song"], "title": "Two is Better than One: Efficient Ensemble Defense for Robust and Compact Models", "categories": ["cs.CV"], "comment": "Accepted to CVPR2025", "summary": "Deep learning-based computer vision systems adopt complex and large\narchitectures to improve performance, yet they face challenges in deployment on\nresource-constrained mobile and edge devices. To address this issue, model\ncompression techniques such as pruning, quantization, and matrix factorization\nhave been proposed; however, these compressed models are often highly\nvulnerable to adversarial attacks. We introduce the \\textbf{Efficient Ensemble\nDefense (EED)} technique, which diversifies the compression of a single base\nmodel based on different pruning importance scores and enhances ensemble\ndiversity to achieve high adversarial robustness and resource efficiency. EED\ndynamically determines the number of necessary sub-models during the inference\nstage, minimizing unnecessary computations while maintaining high robustness.\nOn the CIFAR-10 and SVHN datasets, EED demonstrated state-of-the-art robustness\nperformance compared to existing adversarial pruning techniques, along with an\ninference speed improvement of up to 1.86 times. This proves that EED is a\npowerful defense solution in resource-constrained environments."}
{"id": "2504.05239", "pdf": "https://arxiv.org/pdf/2504.05239", "abs": "https://arxiv.org/abs/2504.05239", "authors": ["Hang Li", "Yucheng Chu", "Kaiqi Yang", "Yasemin Copur-Gencturk", "Jiliang Tang"], "title": "LLM-based Automated Grading with Human-in-the-Loop", "categories": ["cs.CL"], "comment": null, "summary": "The rise of artificial intelligence (AI) technologies, particularly large\nlanguage models (LLMs), has brought significant advancements to the field of\neducation. Among various applications, automatic short answer grading (ASAG),\nwhich focuses on evaluating open-ended textual responses, has seen remarkable\nprogress with the introduction of LLMs. These models not only enhance grading\nperformance compared to traditional ASAG approaches but also move beyond simple\ncomparisons with predefined \"golden\" answers, enabling more sophisticated\ngrading scenarios, such as rubric-based evaluation. However, existing\nLLM-powered methods still face challenges in achieving human-level grading\nperformance in rubric-based assessments due to their reliance on fully\nautomated approaches. In this work, we explore the potential of LLMs in ASAG\ntasks by leveraging their interactive capabilities through a human-in-the-loop\n(HITL) approach. Our proposed framework, GradeHITL, utilizes the generative\nproperties of LLMs to pose questions to human experts, incorporating their\ninsights to refine grading rubrics dynamically. This adaptive process\nsignificantly improves grading accuracy, outperforming existing methods and\nbringing ASAG closer to human-level evaluation."}
{"id": "2504.04753", "pdf": "https://arxiv.org/pdf/2504.04753", "abs": "https://arxiv.org/abs/2504.04753", "authors": ["Cheng Chen", "Jiacheng Wei", "Tianrun Chen", "Chi Zhang", "Xiaofeng Yang", "Shangzhan Zhang", "Bingchen Yang", "Chuan-Sheng Foo", "Guosheng Lin", "Qixing Huang", "Fayao Liu"], "title": "CADCrafter: Generating Computer-Aided Design Models from Unconstrained Images", "categories": ["cs.CV"], "comment": "Accepted to CVPR2025", "summary": "Creating CAD digital twins from the physical world is crucial for\nmanufacturing, design, and simulation. However, current methods typically rely\non costly 3D scanning with labor-intensive post-processing. To provide a\nuser-friendly design process, we explore the problem of reverse engineering\nfrom unconstrained real-world CAD images that can be easily captured by users\nof all experiences. However, the scarcity of real-world CAD data poses\nchallenges in directly training such models. To tackle these challenges, we\npropose CADCrafter, an image-to-parametric CAD model generation framework that\ntrains solely on synthetic textureless CAD data while testing on real-world\nimages. To bridge the significant representation disparity between images and\nparametric CAD models, we introduce a geometry encoder to accurately capture\ndiverse geometric features. Moreover, the texture-invariant properties of the\ngeometric features can also facilitate the generalization to real-world\nscenarios. Since compiling CAD parameter sequences into explicit CAD models is\na non-differentiable process, the network training inherently lacks explicit\ngeometric supervision. To impose geometric validity constraints, we employ\ndirect preference optimization (DPO) to fine-tune our model with the automatic\ncode checker feedback on CAD sequence quality. Furthermore, we collected a\nreal-world dataset, comprised of multi-view images and corresponding CAD\ncommand sequence pairs, to evaluate our method. Experimental results\ndemonstrate that our approach can robustly handle real unconstrained CAD\nimages, and even generalize to unseen general objects."}
{"id": "2504.05262", "pdf": "https://arxiv.org/pdf/2504.05262", "abs": "https://arxiv.org/abs/2504.05262", "authors": ["Yang Yan", "Yu Lu", "Renjun Xu", "Zhenzhong Lan"], "title": "Do PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning vs. Memorization in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Despite high benchmark scores, Large Language Models (LLMs) often fail simple\nproblem, raising a critical question: Do LLMs learn mathematical principles or\nmerely memorize patterns? Rather than designing increasingly complex benchmarks\nlike recent works, we investigate this using elementary two-integer addition\n($0$ to $2^{64}$), probing two core properties: commutativity ($A+B=B+A$) and\ncompositional generalization (via isomorphic symbolic mappings, e.g., $7\n\\rightarrow y$). While state-of-the-art LLMs achieve 73.8-99.8\\% accuracy on\nnumerical addition, performance collapses to $\\leq$7.5\\% under symbolic\nmapping, indicating failure to generalize learned rules. Non-monotonic\nperformance scaling with digit count and frequent commutativity violations\n(over 1,700 cases of $A+B \\neq B+A$) further support this. Explicitly providing\naddition rules degrades performance by 81.2\\% on average, while\nself-explanation maintains baseline accuracy, suggesting LLM arithmetic\nprocessing is misaligned with human-defined principles. Our findings indicate\ncurrent LLMs rely on memory pattern over genuine rule learning, highlighting\narchitectural limitations and the need for new approaches to achieve true\nmathematical reasoning."}
{"id": "2504.04756", "pdf": "https://arxiv.org/pdf/2504.04756", "abs": "https://arxiv.org/abs/2504.04756", "authors": ["Inhwan Bae", "Junoh Lee", "Hae-Gon Jeon"], "title": "Continuous Locomotive Crowd Behavior Generation", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": "Accepted at CVPR 2025. Project page:\n  https://ihbae.com/publication/crowdes/", "summary": "Modeling and reproducing crowd behaviors are important in various domains\nincluding psychology, robotics, transport engineering and virtual environments.\nConventional methods have focused on synthesizing momentary scenes, which have\ndifficulty in replicating the continuous nature of real-world crowds. In this\npaper, we introduce a novel method for automatically generating continuous,\nrealistic crowd trajectories with heterogeneous behaviors and interactions\namong individuals. We first design a crowd emitter model. To do this, we obtain\nspatial layouts from single input images, including a segmentation map,\nappearance map, population density map and population probability, prior to\ncrowd generation. The emitter then continually places individuals on the\ntimeline by assigning independent behavior characteristics such as agents'\ntype, pace, and start/end positions using diffusion models. Next, our crowd\nsimulator produces their long-term locomotions. To simulate diverse actions, it\ncan augment their behaviors based on a Markov chain. As a result, our overall\nframework populates the scenes with heterogeneous crowd behaviors by\nalternating between the proposed emitter and simulator. Note that all the\ncomponents in the proposed framework are user-controllable. Lastly, we propose\na benchmark protocol to evaluate the realism and quality of the generated\ncrowds in terms of the scene-level population dynamics and the individual-level\ntrajectory accuracy. We demonstrate that our approach effectively models\ndiverse crowd behavior patterns and generalizes well across different\ngeographical environments. Code is publicly available at\nhttps://github.com/InhwanBae/CrowdES ."}
{"id": "2504.05276", "pdf": "https://arxiv.org/pdf/2504.05276", "abs": "https://arxiv.org/abs/2504.05276", "authors": ["Yucheng Chu", "Peng He", "Hang Li", "Haoyu Han", "Kaiqi Yang", "Yu Xue", "Tingting Li", "Joseph Krajcik", "Jiliang Tang"], "title": "Enhancing LLM-Based Short Answer Grading with Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "Short answer assessment is a vital component of science education, allowing\nevaluation of students' complex three-dimensional understanding. Large language\nmodels (LLMs) that possess human-like ability in linguistic tasks are\nincreasingly popular in assisting human graders to reduce their workload.\nHowever, LLMs' limitations in domain knowledge restrict their understanding in\ntask-specific requirements and hinder their ability to achieve satisfactory\nperformance. Retrieval-augmented generation (RAG) emerges as a promising\nsolution by enabling LLMs to access relevant domain-specific knowledge during\nassessment. In this work, we propose an adaptive RAG framework for automated\ngrading that dynamically retrieves and incorporates domain-specific knowledge\nbased on the question and student answer context. Our approach combines\nsemantic search and curated educational sources to retrieve valuable reference\nmaterials. Experimental results in a science education dataset demonstrate that\nour system achieves an improvement in grading accuracy compared to baseline LLM\napproaches. The findings suggest that RAG-enhanced grading systems can serve as\nreliable support with efficient performance gains."}
{"id": "2504.04764", "pdf": "https://arxiv.org/pdf/2504.04764", "abs": "https://arxiv.org/abs/2504.04764", "authors": ["Shyam Sundhar", "Riya Sharma", "Priyansh Maheshwari", "Suvidha Rupesh Kumar", "T. Sunil Kumar"], "title": "Enhancing Leaf Disease Classification Using GAT-GCN Hybrid Model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Agriculture plays a critical role in the global economy, providing\nlivelihoods and ensuring food security for billions. As innovative agricultural\npractices become more widespread, the risk of crop diseases has increased,\nhighlighting the urgent need for efficient, low-intervention disease\nidentification methods. This research presents a hybrid model combining Graph\nAttention Networks (GATs) and Graph Convolution Networks (GCNs) for leaf\ndisease classification. GCNs have been widely used for learning from\ngraph-structured data, and GATs enhance this by incorporating attention\nmechanisms to focus on the most important neighbors. The methodology integrates\nsuperpixel segmentation for efficient feature extraction, partitioning images\ninto meaningful, homogeneous regions that better capture localized features.\nThe authors have employed an edge augmentation technique to enhance the\nrobustness of the model. The edge augmentation technique has introduced a\nsignificant degree of generalization in the detection capabilities of the\nmodel. To further optimize training, weight initialization techniques are\napplied. The hybrid model is evaluated against the individual performance of\nthe GCN and GAT models and the hybrid model achieved a precision of 0.9822,\nrecall of 0.9818, and F1-score of 0.9818 in apple leaf disease classification,\na precision of 0.9746, recall of 0.9744, and F1-score of 0.9743 in potato leaf\ndisease classification, and a precision of 0.8801, recall of 0.8801, and\nF1-score of 0.8799 in sugarcane leaf disease classification. These results\ndemonstrate the robustness and performance of the model, suggesting its\npotential to support sustainable agricultural practices through precise and\neffective disease detection. This work is a small step towards reducing the\nloss of crops and hence supporting sustainable goals of zero hunger and life on\nland."}
{"id": "2504.05294", "pdf": "https://arxiv.org/pdf/2504.05294", "abs": "https://arxiv.org/abs/2504.05294", "authors": ["Pedro Ferreira", "Wilker Aziz", "Ivan Titov"], "title": "Truthful or Fabricated? Using Causal Attribution to Mitigate Reward Hacking in Explanations", "categories": ["cs.CL"], "comment": "22 pages, 10 figures, 5 tables", "summary": "Chain-of-thought explanations are widely used to inspect the decision process\nof large language models (LLMs) and to evaluate the trustworthiness of model\noutputs, making them important for effective collaboration between LLMs and\nhumans. We demonstrate that preference optimization - a key step in the\nalignment phase - can inadvertently reduce the faithfulness of these\nexplanations. This occurs because the reward model (RM), which guides\nalignment, is tasked with optimizing both the expected quality of the response\nand the appropriateness of the explanations (e.g., minimizing bias or adhering\nto safety standards), creating potential conflicts. The RM lacks a mechanism to\nassess the consistency between the model's internal decision process and the\ngenerated explanation. Consequently, the LLM may engage in \"reward hacking\" by\nproducing a final response that scores highly while giving an explanation\ntailored to maximize reward rather than accurately reflecting its reasoning. To\naddress this issue, we propose enriching the RM's input with a causal\nattribution of the prediction, allowing the RM to detect discrepancies between\nthe generated self-explanation and the model's decision process. In controlled\nsettings, we show that this approach reduces the tendency of the LLM to\ngenerate misleading explanations."}
{"id": "2504.04780", "pdf": "https://arxiv.org/pdf/2504.04780", "abs": "https://arxiv.org/abs/2504.04780", "authors": ["Chenxi Zhao", "Daochang Wang", "Siqian Zhang", "Gangyao Kuang"], "title": "Bottom-Up Scattering Information Perception Network for SAR target recognition", "categories": ["cs.CV"], "comment": null, "summary": "Deep learning methods based synthetic aperture radar (SAR) image target\nrecognition tasks have been widely studied currently. The existing deep methods\nare insufficient to perceive and mine the scattering information of SAR images,\nresulting in performance bottlenecks and poor robustness of the algorithms. To\nthis end, this paper proposes a novel bottom-up scattering information\nperception network for more interpretable target recognition by constructing\nthe proprietary interpretation network for SAR images. Firstly, the localized\nscattering perceptron is proposed to replace the backbone feature extractor\nbased on CNN networks to deeply mine the underlying scattering information of\nthe target. Then, an unsupervised scattering part feature extraction model is\nproposed to robustly characterize the target scattering part information and\nprovide fine-grained target representation. Finally, by aggregating the\nknowledge of target parts to form the complete target description, the\ninterpretability and discriminative ability of the model is improved. We\nperform experiments on the FAST-Vehicle dataset and the SAR-ACD dataset to\nvalidate the performance of the proposed method."}
{"id": "2504.03714", "pdf": "https://arxiv.org/pdf/2504.03714", "abs": "https://arxiv.org/abs/2504.03714", "authors": ["Runpeng Dai", "Run Yang", "Fan Zhou", "Hongtu Zhu"], "title": "Breach in the Shield: Unveiling the Vulnerabilities of Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Large Language Models (LLMs) and Vision-Language Models (VLMs) have become\nessential to general artificial intelligence, exhibiting remarkable\ncapabilities in task understanding and problem-solving. However, the real-world\nreliability of these models critically depends on their stability, which\nremains an underexplored area. Despite their widespread use, rigorous studies\nexamining the stability of LLMs under various perturbations are still lacking.\nIn this paper, we address this gap by proposing a novel stability measure for\nLLMs, inspired by statistical methods rooted in information geometry. Our\nmeasure possesses desirable invariance properties, making it well-suited for\nanalyzing model sensitivity to both parameter and input perturbations. To\nassess the effectiveness of our approach, we conduct extensive experiments on\nmodels ranging in size from 1.5B to 13B parameters. Our results demonstrate the\nutility of our measure in identifying salient parameters and detecting\nvulnerable regions in input images or critical dimensions in token embeddings.\nFurthermore, leveraging our stability framework, we enhance model robustness\nduring model merging, leading to improved performance."}
{"id": "2504.04781", "pdf": "https://arxiv.org/pdf/2504.04781", "abs": "https://arxiv.org/abs/2504.04781", "authors": ["Chaoyi Wang", "Baoqing Li", "Xinhan Di"], "title": "OCC-MLLM-CoT-Alpha: Towards Multi-stage Occlusion Recognition Based on Large Language Models via 3D-Aware Supervision and Chain-of-Thoughts Guidance", "categories": ["cs.CV", "I.2.10; I.4.8"], "comment": "This work has been accepted to the Multimodal Algorithmic Reasoning\n  (MAR) Workshop at CVPR 2025", "summary": "Comprehending occluded objects are not well studied in existing large-scale\nvisual-language multi-modal models. Current state-of-the-art multi-modal large\nmodels struggles to provide satisfactory results in understanding occluded\nobjects through universal visual encoders and supervised learning strategies.\nTherefore, we propose OCC-MLLM-CoT-Alpha, a multi-modal large vision language\nframework that integrates 3D-aware supervision and Chain-of-Thoughts guidance.\nParticularly, (1) we build a multi-modal large vision-language model framework\nwhich is consisted of a large multi-modal vision-language model and a 3D\nreconstruction expert model. (2) the corresponding multi-modal\nChain-of-Thoughts is learned through a combination of supervised and\nreinforcement training strategies, allowing the multi-modal vision-language\nmodel to enhance the recognition ability with learned multi-modal\nchain-of-thoughts guidance. (3) A large-scale multi-modal chain-of-thoughts\nreasoning dataset, consisting of $110k$ samples of occluded objects held in\nhand, is built. In the evaluation, the proposed methods demonstrate decision\nscore improvement of 15.75%,15.30%,16.98%,14.62%, and 4.42%,3.63%,6.94%,10.70%\nfor two settings of a variety of state-of-the-art models."}
{"id": "2504.03724", "pdf": "https://arxiv.org/pdf/2504.03724", "abs": "https://arxiv.org/abs/2504.03724", "authors": ["Zhiqiang Wang", "Pengbin Feng", "Yanbin Lin", "Shuzhang Cai", "Zongao Bian", "Jinghua Yan", "Xingquan Zhu"], "title": "CrowdVLM-R1: Expanding R1 Ability to Vision Language Model for Crowd Counting using Fuzzy Group Relative Policy Reward", "categories": ["cs.CV", "cs.CL"], "comment": "11 pages, 6 figures and 4 tables", "summary": "We propose Fuzzy Group Relative Policy Reward (FGRPR), a novel framework that\nintegrates Group Relative Policy Optimization (GRPO) with a fuzzy reward\nfunction to enhance learning efficiency. Unlike the conventional binary 0/1\naccuracy reward, our fuzzy reward model provides nuanced incentives,\nencouraging more precise outputs. Experimental results demonstrate that GRPO\nwith a standard 0/1 accuracy reward underperforms compared to supervised\nfine-tuning (SFT). In contrast, FGRPR, applied to Qwen2.5-VL(3B and 7B),\nsurpasses all baseline models, including GPT4o, LLaMA2(90B), and SFT, across\nfive in-domain datasets. On an out-of-domain dataset, FGRPR achieves\nperformance comparable to SFT but excels when target values are larger, as its\nfuzzy reward function assigns higher rewards to closer approximations. This\napproach is broadly applicable to tasks where the precision of the answer is\ncritical. Code and data: https://github.com/yeyimilk/CrowdVLM-R1"}
{"id": "2504.04784", "pdf": "https://arxiv.org/pdf/2504.04784", "abs": "https://arxiv.org/abs/2504.04784", "authors": ["Hui Liu", "Bin Zou", "Suiyun Zhang", "Kecheng Chen", "Rui Liu", "Haoliang Li"], "title": "Disentangling Instruction Influence in Diffusion Transformers for Parallel Multi-Instruction-Guided Image Editing", "categories": ["cs.CV"], "comment": "14 pages, 8 figures", "summary": "Instruction-guided image editing enables users to specify modifications using\nnatural language, offering more flexibility and control. Among existing\nframeworks, Diffusion Transformers (DiTs) outperform U-Net-based diffusion\nmodels in scalability and performance. However, while real-world scenarios\noften require concurrent execution of multiple instructions, step-by-step\nediting suffers from accumulated errors and degraded quality, and integrating\nmultiple instructions with a single prompt usually results in incomplete edits\ndue to instruction conflicts. We propose Instruction Influence Disentanglement\n(IID), a novel framework enabling parallel execution of multiple instructions\nin a single denoising process, designed for DiT-based models. By analyzing\nself-attention mechanisms in DiTs, we identify distinctive attention patterns\nin multi-instruction settings and derive instruction-specific attention masks\nto disentangle each instruction's influence. These masks guide the editing\nprocess to ensure localized modifications while preserving consistency in\nnon-edited regions. Extensive experiments on open-source and custom datasets\ndemonstrate that IID reduces diffusion steps while improving fidelity and\ninstruction completion compared to existing baselines. The codes will be\npublicly released upon the acceptance of the paper."}
{"id": "2504.03735", "pdf": "https://arxiv.org/pdf/2504.03735", "abs": "https://arxiv.org/abs/2504.03735", "authors": ["Erfan Shayegani", "G M Shahariar", "Sara Abdali", "Lei Yu", "Nael Abu-Ghazaleh", "Yue Dong"], "title": "Misaligned Roles, Misplaced Images: Structural Input Perturbations Expose Multimodal Alignment Blind Spots", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "Multimodal Language Models (MMLMs) typically undergo post-training alignment\nto prevent harmful content generation. However, these alignment stages focus\nprimarily on the assistant role, leaving the user role unaligned, and stick to\na fixed input prompt structure of special tokens, leaving the model vulnerable\nwhen inputs deviate from these expectations. We introduce Role-Modality Attacks\n(RMA), a novel class of adversarial attacks that exploit role confusion between\nthe user and assistant and alter the position of the image token to elicit\nharmful outputs. Unlike existing attacks that modify query content, RMAs\nmanipulate the input structure without altering the query itself. We\nsystematically evaluate these attacks across multiple Vision Language Models\n(VLMs) on eight distinct settings, showing that they can be composed to create\nstronger adversarial prompts, as also evidenced by their increased projection\nin the negative refusal direction in the residual stream, a property observed\nin prior successful attacks. Finally, for mitigation, we propose an adversarial\ntraining approach that makes the model robust against input prompt\nperturbations. By training the model on a range of harmful and benign prompts\nall perturbed with different RMA settings, it loses its sensitivity to Role\nConfusion and Modality Manipulation attacks and is trained to only pay\nattention to the content of the query in the input prompt structure,\neffectively reducing Attack Success Rate (ASR) while preserving the model's\ngeneral utility."}
{"id": "2504.04787", "pdf": "https://arxiv.org/pdf/2504.04787", "abs": "https://arxiv.org/abs/2504.04787", "authors": ["Mengxuan Wu", "Zekai Li", "Zhiyuan Liang", "Moyang Li", "Xuanlei Zhao", "Samir Khaki", "Zheng Zhu", "Xiaojiang Peng", "Konstantinos N. Plataniotis", "Kai Wang", "Wangbo Zhao", "Yang You"], "title": "Dynamic Vision Mamba", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Mamba-based vision models have gained extensive attention as a result of\nbeing computationally more efficient than attention-based models. However,\nspatial redundancy still exists in these models, represented by token and block\nredundancy. For token redundancy, we analytically find that early token pruning\nmethods will result in inconsistency between training and inference or\nintroduce extra computation for inference. Therefore, we customize token\npruning to fit the Mamba structure by rearranging the pruned sequence before\nfeeding it into the next Mamba block. For block redundancy, we allow each image\nto select SSM blocks dynamically based on an empirical observation that the\ninference speed of Mamba-based vision models is largely affected by the number\nof SSM blocks. Our proposed method, Dynamic Vision Mamba (DyVM), effectively\nreduces FLOPs with minor performance drops. We achieve a reduction of 35.2\\%\nFLOPs with only a loss of accuracy of 1.7\\% on Vim-S. It also generalizes well\nacross different Mamba vision model architectures and different vision tasks.\nOur code will be made public."}
{"id": "2504.03748", "pdf": "https://arxiv.org/pdf/2504.03748", "abs": "https://arxiv.org/abs/2504.03748", "authors": ["Kaiyuan Hou", "Minghui Zhao", "Lilin Xu", "Yuang Fan", "Xiaofan Jiang"], "title": "TDBench: Benchmarking Vision-Language Models in Understanding Top-Down Images", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The rapid emergence of Vision-Language Models (VLMs) has significantly\nadvanced multimodal understanding, enabling applications in scene comprehension\nand visual reasoning. While these models have been primarily evaluated and\ndeveloped for front-view image understanding, their capabilities in\ninterpreting top-down images have received limited attention, partly due to the\nscarcity of diverse top-down datasets and the challenges in collecting such\ndata. In contrast, top-down vision provides explicit spatial overviews and\nimproved contextual understanding of scenes, making it particularly valuable\nfor tasks like autonomous navigation, aerial imaging, and spatial planning. In\nthis work, we address this gap by introducing TDBench, a comprehensive\nbenchmark for VLMs in top-down image understanding. TDBench is constructed from\npublic top-down view datasets and high-quality simulated images, including\ndiverse real-world and synthetic scenarios. TDBench consists of visual\nquestion-answer pairs across ten evaluation dimensions of image understanding.\nMoreover, we conduct four case studies that commonly happen in real-world\nscenarios but are less explored. By revealing the strengths and limitations of\nexisting VLM through evaluation results, we hope TDBench to provide insights\nfor motivating future research. Project homepage:\nhttps://github.com/Columbia-ICSL/TDBench"}
{"id": "2504.04801", "pdf": "https://arxiv.org/pdf/2504.04801", "abs": "https://arxiv.org/abs/2504.04801", "authors": ["Jinhong Wang", "Shuo Tong", "Jian liu", "Dongqi Tang", "Weiqiang Wang", "Wentong Li", "Hongxia Xu", "Danny Chen", "Jintai Chen", "Jian Wu"], "title": "OrderChain: A General Prompting Paradigm to Improve Ordinal Understanding Ability of MLLM", "categories": ["cs.CV"], "comment": null, "summary": "Despite the remarkable progress of multimodal large language models (MLLMs),\nthey continue to face challenges in achieving competitive performance on\nordinal regression (OR; a.k.a. ordinal classification). To address this issue,\nthis paper presents OrderChain, a novel and general prompting paradigm that\nimproves the ordinal understanding ability of MLLMs by specificity and\ncommonality modeling. Specifically, our OrderChain consists of a set of\ntask-aware prompts to facilitate the specificity modeling of diverse OR tasks\nand a new range optimization Chain-of-Thought (RO-CoT), which learns a\ncommonality way of thinking about OR tasks by uniformly decomposing them into\nmultiple small-range optimization subtasks. Further, we propose a category\nrecursive division (CRD) method to generate instruction candidate category\nprompts to support RO-CoT automatic optimization. Comprehensive experiments\nshow that a Large Language and Vision Assistant (LLaVA) model with our\nOrderChain improves baseline LLaVA significantly on diverse OR datasets, e.g.,\nfrom 47.5% to 93.2% accuracy on the Adience dataset for age estimation, and\nfrom 30.0% to 85.7% accuracy on the Diabetic Retinopathy dataset. Notably,\nLLaVA with our OrderChain also remarkably outperforms state-of-the-art methods\nby 27% on accuracy and 0.24 on MAE on the Adience dataset. To our best\nknowledge, our OrderChain is the first work that augments MLLMs for OR tasks,\nand the effectiveness is witnessed across a spectrum of OR datasets."}
{"id": "2504.03775", "pdf": "https://arxiv.org/pdf/2504.03775", "abs": "https://arxiv.org/abs/2504.03775", "authors": ["Weiqing Li", "Guochao Jiang", "Xiangyong Ding", "Zhangcheng Tao", "Chuzhan Hao", "Chenfeng Xu", "Yuewei Zhang", "Hao Wang"], "title": "FlowKV: A Disaggregated Inference Framework with Low-Latency KV Cache Transfer and Load-Aware Scheduling", "categories": ["cs.DC", "cs.AI", "cs.CL"], "comment": null, "summary": "Disaggregated inference has become an essential framework that separates the\nprefill (P) and decode (D) stages in large language model inference to improve\nthroughput. However, the KV cache transfer faces significant delays between\nprefill and decode nodes. The block-wise calling method and discontinuous KV\ncache memory allocation increase the number of calls to the transmission\nkernel. Additionally, existing frameworks often fix the roles of P and D nodes,\nleading to computational imbalances. In this paper, we propose FlowKV, a novel\ndisaggregated inference framework, which reduces the average transmission\nlatency of KV cache by 96%, from 0.944s to 0.053s, almost eliminating the\ntransfer time relative to the total request latency by optimizing the KV cache\ntransfer. FlowKV introduces the Load-Aware Scheduler for balanced request\nscheduling and flexible PD node allocation. This design maximizes hardware\nresource utilization, achieving peak system throughput across various\nscenarios, including normal, computational imbalance, and extreme overload\nconditions. Experimental results demonstrate that FlowKV significantly\naccelerates inference by 15.2%-48.9% on LongBench dataset compared to the\nbaseline and supports applications with heterogeneous GPUs."}
{"id": "2504.04804", "pdf": "https://arxiv.org/pdf/2504.04804", "abs": "https://arxiv.org/abs/2504.04804", "authors": ["Yuanpei Liu", "Kai Han"], "title": "DebGCD: Debiased Learning with Distribution Guidance for Generalized Category Discovery", "categories": ["cs.CV"], "comment": "Accepted as a conference paper at ICLR 2025", "summary": "In this paper, we tackle the problem of Generalized Category Discovery (GCD).\nGiven a dataset containing both labelled and unlabelled images, the objective\nis to categorize all images in the unlabelled subset, irrespective of whether\nthey are from known or unknown classes. In GCD, an inherent label bias exists\nbetween known and unknown classes due to the lack of ground-truth labels for\nthe latter. State-of-the-art methods in GCD leverage parametric classifiers\ntrained through self-distillation with soft labels, leaving the bias issue\nunattended. Besides, they treat all unlabelled samples uniformly, neglecting\nvariations in certainty levels and resulting in suboptimal learning. Moreover,\nthe explicit identification of semantic distribution shifts between known and\nunknown classes, a vital aspect for effective GCD, has been neglected. To\naddress these challenges, we introduce DebGCD, a \\underline{Deb}iased learning\nwith distribution guidance framework for \\underline{GCD}. Initially, DebGCD\nco-trains an auxiliary debiased classifier in the same feature space as the GCD\nclassifier, progressively enhancing the GCD features. Moreover, we introduce a\nsemantic distribution detector in a separate feature space to implicitly boost\nthe learning efficacy of GCD. Additionally, we employ a curriculum learning\nstrategy based on semantic distribution certainty to steer the debiased\nlearning at an optimized pace. Thorough evaluations on GCD benchmarks\ndemonstrate the consistent state-of-the-art performance of our framework,\nhighlighting its superiority. Project page: https://visual-ai.github.io/debgcd/"}
{"id": "2504.03814", "pdf": "https://arxiv.org/pdf/2504.03814", "abs": "https://arxiv.org/abs/2504.03814", "authors": ["Grgur Kovaƒç", "J√©r√©my Perez", "R√©my Portelas", "Peter Ford Dominey", "Pierre-Yves Oudeyer"], "title": "Recursive Training Loops in LLMs: How training data properties modulate distribution shift in generated data?", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T50", "I.2.7"], "comment": null, "summary": "Large language models (LLMs) are increasingly contributing to the creation of\ncontent on the Internet. This creates a feedback loop as subsequent generations\nof models will be trained on this generated, synthetic data. This phenomenon is\nreceiving increasing interest, in particular because previous studies have\nshown that it may lead to distribution shift - models misrepresent and forget\nthe true underlying distributions of human data they are expected to\napproximate (e.g. resulting in a drastic loss of quality). In this study, we\nstudy the impact of human data properties on distribution shift dynamics in\niterated training loops. We first confirm that the distribution shift dynamics\ngreatly vary depending on the human data by comparing four datasets (two based\non Twitter and two on Reddit). We then test whether data quality may influence\nthe rate of this shift. We find that it does on the twitter, but not on the\nReddit datasets. We then focus on a Reddit dataset and conduct a more\nexhaustive evaluation of a large set of dataset properties. This experiment\nassociated lexical diversity with larger, and semantic diversity with smaller\ndetrimental shifts, suggesting that incorporating text with high lexical (but\nlimited semantic) diversity could exacerbate the degradation of generated text.\nWe then focus on the evolution of political bias, and find that the type of\nshift observed (bias reduction, amplification or inversion) depends on the\npolitical lean of the human (true) distribution. Overall, our work extends the\nexisting literature on the consequences of recursive fine-tuning by showing\nthat this phenomenon is highly dependent on features of the human data on which\ntraining occurs. This suggests that different parts of internet (e.g. GitHub,\nReddit) may undergo different types of shift depending on their properties."}
{"id": "2504.04818", "pdf": "https://arxiv.org/pdf/2504.04818", "abs": "https://arxiv.org/abs/2504.04818", "authors": ["Zuying Xie", "Changtao Miao", "Ajian Liu", "Jiabao Guo", "Feng Li", "Dan Guo", "Yunfeng Diao"], "title": "SUEDE:Shared Unified Experts for Physical-Digital Face Attack Detection Enhancement", "categories": ["cs.CV"], "comment": "Accepted in ICME 2025", "summary": "Face recognition systems are vulnerable to physical attacks (e.g., printed\nphotos) and digital threats (e.g., DeepFake), which are currently being studied\nas independent visual tasks, such as Face Anti-Spoofing and Forgery Detection.\nThe inherent differences among various attack types present significant\nchallenges in identifying a common feature space, making it difficult to\ndevelop a unified framework for detecting data from both attack modalities\nsimultaneously. Inspired by the efficacy of Mixture-of-Experts (MoE) in\nlearning across diverse domains, we explore utilizing multiple experts to learn\nthe distinct features of various attack types. However, the feature\ndistributions of physical and digital attacks overlap and differ. This suggests\nthat relying solely on distinct experts to learn the unique features of each\nattack type may overlook shared knowledge between them. To address these\nissues, we propose SUEDE, the Shared Unified Experts for Physical-Digital Face\nAttack Detection Enhancement. SUEDE combines a shared expert (always activated)\nto capture common features for both attack types and multiple routed experts\n(selectively activated) for specific attack types. Further, we integrate CLIP\nas the base network to ensure the shared expert benefits from prior visual\nknowledge and align visual-text representations in a unified space. Extensive\nresults demonstrate SUEDE achieves superior performance compared to\nstate-of-the-art unified detection methods."}
{"id": "2504.03947", "pdf": "https://arxiv.org/pdf/2504.03947", "abs": "https://arxiv.org/abs/2504.03947", "authors": ["Chris Samarinas", "Hamed Zamani"], "title": "Distillation and Refinement of Reasoning in Small Language Models for Document Re-ranking", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "We present a novel approach for training small language models for\nreasoning-intensive document ranking that combines knowledge distillation with\nreinforcement learning optimization. While existing methods often rely on\nexpensive human annotations or large black-box language models, our methodology\nleverages web data and a teacher LLM to automatically generate high-quality\ntraining examples with relevance explanations. By framing document ranking as a\nreinforcement learning problem and incentivizing explicit reasoning\ncapabilities, we train a compact 3B parameter language model that achieves\nstate-of-the-art performance on the BRIGHT benchmark. Our model ranks third on\nthe leaderboard while using substantially fewer parameters than other\napproaches, outperforming models that are over 20 times larger. Through\nextensive experiments, we demonstrate that generating explanations during\ninference, rather than directly predicting relevance scores, enables more\neffective reasoning with smaller language models. The self-supervised nature of\nour method offers a scalable and interpretable solution for modern information\nretrieval systems."}
{"id": "2504.04827", "pdf": "https://arxiv.org/pdf/2504.04827", "abs": "https://arxiv.org/abs/2504.04827", "authors": ["Long Ma", "Zhiyuan Yan", "Yize Chen", "Jin Xu", "Qinglang Guo", "Hu Huang", "Yong Liao", "Hui Lin"], "title": "From Specificity to Generality: Revisiting Generalizable Artifacts in Detecting Face Deepfakes", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Detecting deepfakes has been an increasingly important topic, especially\ngiven the rapid development of AI generation techniques. In this paper, we ask:\nHow can we build a universal detection framework that is effective for most\nfacial deepfakes? One significant challenge is the wide variety of deepfake\ngenerators available, resulting in varying forgery artifacts (e.g., lighting\ninconsistency, color mismatch, etc). But should we ``teach\" the detector to\nlearn all these artifacts separately? It is impossible and impractical to\nelaborate on them all. So the core idea is to pinpoint the more common and\ngeneral artifacts across different deepfakes. Accordingly, we categorize\ndeepfake artifacts into two distinct yet complementary types: Face\nInconsistency Artifacts (FIA) and Up-Sampling Artifacts (USA). FIA arise from\nthe challenge of generating all intricate details, inevitably causing\ninconsistencies between the complex facial features and relatively uniform\nsurrounding areas. USA, on the other hand, are the inevitable traces left by\nthe generator's decoder during the up-sampling process. This categorization\nstems from the observation that all existing deepfakes typically exhibit one or\nboth of these artifacts. To achieve this, we propose a new data-level\npseudo-fake creation framework that constructs fake samples with only the FIA\nand USA, without introducing extra less-general artifacts. Specifically, we\nemploy a super-resolution to simulate the USA, while design a Blender module\nthat uses image-level self-blending on diverse facial regions to create the\nFIA. We surprisingly found that, with this intuitive design, a standard image\nclassifier trained only with our pseudo-fake data can non-trivially generalize\nwell to unseen deepfakes."}
{"id": "2504.03970", "pdf": "https://arxiv.org/pdf/2504.03970", "abs": "https://arxiv.org/abs/2504.03970", "authors": ["Dahun Kim", "AJ Piergiovanni", "Ganesh Mallya", "Anelia Angelova"], "title": "VideoComp: Advancing Fine-Grained Compositional and Temporal Alignment in Video-Text Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR"], "comment": "CVPR 2025, project page at\n  https://github.com/google-deepmind/video_comp", "summary": "We introduce VideoComp, a benchmark and learning framework for advancing\nvideo-text compositionality understanding, aimed at improving vision-language\nmodels (VLMs) in fine-grained temporal alignment. Unlike existing benchmarks\nfocused on static image-text compositionality or isolated single-event videos,\nour benchmark targets alignment in continuous multi-event videos. Leveraging\nvideo-text datasets with temporally localized event captions (e.g.\nActivityNet-Captions, YouCook2), we construct two compositional benchmarks,\nActivityNet-Comp and YouCook2-Comp. We create challenging negative samples with\nsubtle temporal disruptions such as reordering, action word replacement,\npartial captioning, and combined disruptions. These benchmarks comprehensively\ntest models' compositional sensitivity across extended, cohesive video-text\nsequences. To improve model performance, we propose a hierarchical pairwise\npreference loss that strengthens alignment with temporally accurate pairs and\ngradually penalizes increasingly disrupted ones, encouraging fine-grained\ncompositional learning. To mitigate the limited availability of densely\nannotated video data, we introduce a pretraining strategy that concatenates\nshort video-caption pairs to simulate multi-event sequences. We evaluate\nvideo-text foundational models and large multimodal models (LMMs) on our\nbenchmark, identifying both strengths and areas for improvement in\ncompositionality. Overall, our work provides a comprehensive framework for\nevaluating and enhancing model capabilities in achieving fine-grained,\ntemporally coherent video-text alignment."}
{"id": "2504.04834", "pdf": "https://arxiv.org/pdf/2504.04834", "abs": "https://arxiv.org/abs/2504.04834", "authors": ["Pengju Sun", "Banglei Guan", "Zhenbao Yu", "Yang Shang", "Qifeng Yu", "Daniel Barath"], "title": "Learning Affine Correspondences by Integrating Geometric Constraints", "categories": ["cs.CV"], "comment": null, "summary": "Affine correspondences have received significant attention due to their\nbenefits in tasks like image matching and pose estimation. Existing methods for\nextracting affine correspondences still have many limitations in terms of\nperformance; thus, exploring a new paradigm is crucial. In this paper, we\npresent a new pipeline designed for extracting accurate affine correspondences\nby integrating dense matching and geometric constraints. Specifically, a novel\nextraction framework is introduced, with the aid of dense matching and a novel\nkeypoint scale and orientation estimator. For this purpose, we propose loss\nfunctions based on geometric constraints, which can effectively improve\naccuracy by supervising neural networks to learn feature geometry. The\nexperimental show that the accuracy and robustness of our method outperform the\nexisting ones in image matching tasks. To further demonstrate the effectiveness\nof the proposed method, we applied it to relative pose estimation. Affine\ncorrespondences extracted by our method lead to more accurate poses than the\nbaselines on a range of real-world datasets. The code is available at\nhttps://github.com/stilcrad/DenseAffine."}
{"id": "2504.04030", "pdf": "https://arxiv.org/pdf/2504.04030", "abs": "https://arxiv.org/abs/2504.04030", "authors": ["Wasi Uddin Ahmad", "Aleksander Ficek", "Mehrzad Samadi", "Jocelyn Huang", "Vahid Noroozi", "Somshubra Majumdar", "Boris Ginsburg"], "title": "OpenCodeInstruct: A Large-scale Instruction Tuning Dataset for Code LLMs", "categories": ["cs.SE", "cs.CL"], "comment": "Work in progress", "summary": "Large Language Models (LLMs) have transformed software development by\nenabling code generation, automated debugging, and complex reasoning. However,\ntheir continued advancement is constrained by the scarcity of high-quality,\npublicly available supervised fine-tuning (SFT) datasets tailored for coding\ntasks. To bridge this gap, we introduce OpenCodeInstruct, the largest\nopen-access instruction tuning dataset, comprising 5 million diverse samples.\nEach sample includes a programming question, solution, test cases, execution\nfeedback, and LLM-generated quality assessments. We fine-tune various base\nmodels, including LLaMA and Qwen, across multiple scales (1B+, 3B+, and 7B+)\nusing our dataset. Comprehensive evaluations on popular benchmarks (HumanEval,\nMBPP, LiveCodeBench, and BigCodeBench) demonstrate substantial performance\nimprovements achieved by SFT with OpenCodeInstruct. We also present a detailed\nmethodology encompassing seed data curation, synthetic instruction and solution\ngeneration, and filtering."}
{"id": "2504.04835", "pdf": "https://arxiv.org/pdf/2504.04835", "abs": "https://arxiv.org/abs/2504.04835", "authors": ["Shanshan Wang", "Haixiang Xu", "Hui Feng", "Xiaoqian Wang", "Pei Song", "Sijie Liu", "Jianhua He"], "title": "Inland Waterway Object Detection in Multi-environment: Dataset and Approach", "categories": ["cs.CV"], "comment": "37 pages,11 figures,5 tables", "summary": "The success of deep learning in intelligent ship visual perception relies\nheavily on rich image data. However, dedicated datasets for inland waterway\nvessels remain scarce, limiting the adaptability of visual perception systems\nin complex environments. Inland waterways, characterized by narrow channels,\nvariable weather, and urban interference, pose significant challenges to object\ndetection systems based on existing datasets. To address these issues, this\npaper introduces the Multi-environment Inland Waterway Vessel Dataset (MEIWVD),\ncomprising 32,478 high-quality images from diverse scenarios, including sunny,\nrainy, foggy, and artificial lighting conditions. MEIWVD covers common vessel\ntypes in the Yangtze River Basin, emphasizing diversity, sample independence,\nenvironmental complexity, and multi-scale characteristics, making it a robust\nbenchmark for vessel detection. Leveraging MEIWVD, this paper proposes a\nscene-guided image enhancement module to improve water surface images based on\nenvironmental conditions adaptively. Additionally, a parameter-limited dilated\nconvolution enhances the representation of vessel features, while a multi-scale\ndilated residual fusion method integrates multi-scale features for better\ndetection. Experiments show that MEIWVD provides a more rigorous benchmark for\nobject detection algorithms, and the proposed methods significantly improve\ndetector performance, especially in complex multi-environment scenarios."}
{"id": "2504.04110", "pdf": "https://arxiv.org/pdf/2504.04110", "abs": "https://arxiv.org/abs/2504.04110", "authors": ["Xin Quan", "Marco Valentino", "Danilo S. Carvalho", "Dhairya Dalal", "Andr√© Freitas"], "title": "PEIRCE: Unifying Material and Formal Reasoning via LLM-Driven Neuro-Symbolic Refinement", "categories": ["cs.AI", "cs.CL"], "comment": "Demo paper. Work in progress", "summary": "A persistent challenge in AI is the effective integration of material and\nformal inference - the former concerning the plausibility and contextual\nrelevance of arguments, while the latter focusing on their logical and\nstructural validity. Large Language Models (LLMs), by virtue of their extensive\npre-training on large textual corpora, exhibit strong capabilities in material\ninference. However, their reasoning often lacks formal rigour and\nverifiability. At the same time, LLMs' linguistic competence positions them as\na promising bridge between natural and formal languages, opening up new\nopportunities for combining these two modes of reasoning. In this paper, we\nintroduce PEIRCE, a neuro-symbolic framework designed to unify material and\nformal inference through an iterative conjecture-criticism process. Within this\nframework, LLMs play the central role of generating candidate solutions in\nnatural and formal languages, which are then evaluated and refined via\ninteraction with external critique models. These critiques include symbolic\nprovers, which assess formal validity, as well as soft evaluators that measure\nthe quality of the generated arguments along linguistic and epistemic\ndimensions such as plausibility, coherence, and parsimony. While PEIRCE is a\ngeneral-purpose framework, we demonstrate its capabilities in the domain of\nnatural language explanation generation - a setting that inherently demands\nboth material adequacy and formal correctness."}
{"id": "2504.04837", "pdf": "https://arxiv.org/pdf/2504.04837", "abs": "https://arxiv.org/abs/2504.04837", "authors": ["Zhi Zuo", "Chenyi Zhuang", "Zhiqiang Shen", "Pan Gao", "Jie Qin"], "title": "Uni4D: A Unified Self-Supervised Learning Framework for Point Cloud Videos", "categories": ["cs.CV"], "comment": "11 pages, 7 figures", "summary": "Point cloud video representation learning is primarily built upon the masking\nstrategy in a self-supervised manner. However, the progress is slow due to\nseveral significant challenges: (1) existing methods learn the motion\nparticularly with hand-crafted designs, leading to unsatisfactory motion\npatterns during pre-training which are non-transferable on fine-tuning\nscenarios. (2) previous Masked AutoEncoder (MAE) frameworks are limited in\nresolving the huge representation gap inherent in 4D data. In this study, we\nintroduce the first self-disentangled MAE for learning discriminative 4D\nrepresentations in the pre-training stage. To address the first challenge, we\npropose to model the motion representation in a latent space. The second issue\nis resolved by introducing the latent tokens along with the typical geometry\ntokens to disentangle high-level and low-level features during decoding.\nExtensive experiments on MSR-Action3D, NTU-RGBD, HOI4D, NvGesture, and SHREC'17\nverify this self-disentangled learning framework. We demonstrate that it can\nboost the fine-tuning performance on all 4D tasks, which we term Uni4D. Our\npre-trained model presents discriminative and meaningful 4D representations,\nparticularly benefits processing long videos, as Uni4D gets $+3.8\\%$\nsegmentation accuracy on HOI4D, significantly outperforming either\nself-supervised or fully-supervised methods after end-to-end fine-tuning."}
{"id": "2504.04277", "pdf": "https://arxiv.org/pdf/2504.04277", "abs": "https://arxiv.org/abs/2504.04277", "authors": ["Marios Kokkodis", "Richard Demsyn-Jones", "Vijay Raghavan"], "title": "Beyond the Hype: Embeddings vs. Prompting for Multiclass Classification Tasks", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.AP"], "comment": null, "summary": "Are traditional classification approaches irrelevant in this era of AI hype?\nWe show that there are multiclass classification problems where predictive\nmodels holistically outperform LLM prompt-based frameworks. Given text and\nimages from home-service project descriptions provided by Thumbtack customers,\nwe build embeddings-based softmax models that predict the professional category\n(e.g., handyman, bathroom remodeling) associated with each problem description.\nWe then compare against prompts that ask state-of-the-art LLM models to solve\nthe same problem. We find that the embeddings approach outperforms the best LLM\nprompts in terms of accuracy, calibration, latency, and financial cost. In\nparticular, the embeddings approach has 49.5% higher accuracy than the\nprompting approach, and its superiority is consistent across text-only,\nimage-only, and text-image problem descriptions. Furthermore, it yields\nwell-calibrated probabilities, which we later use as confidence signals to\nprovide contextualized user experience during deployment. On the contrary,\nprompting scores are overly uninformative. Finally, the embeddings approach is\n14 and 81 times faster than prompting in processing images and text\nrespectively, while under realistic deployment assumptions, it can be up to 10\ntimes cheaper. Based on these results, we deployed a variation of the\nembeddings approach, and through A/B testing we observed performance consistent\nwith our offline analysis. Our study shows that for multiclass classification\nproblems that can leverage proprietary datasets, an embeddings-based approach\nmay yield unequivocally better results. Hence, scientists, practitioners,\nengineers, and business leaders can use our study to go beyond the hype and\nconsider appropriate predictive models for their classification use cases."}
{"id": "2504.04841", "pdf": "https://arxiv.org/pdf/2504.04841", "abs": "https://arxiv.org/abs/2504.04841", "authors": ["Sebastian Schmidt", "Julius K√∂rner", "Dominik Fuchsgruber", "Stefano Gasperini", "Federico Tombari", "Stephan G√ºnnemann"], "title": "Prior2Former -- Evidential Modeling of Mask Transformers for Assumption-Free Open-World Panoptic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "In panoptic segmentation, individual instances must be separated within\nsemantic classes. As state-of-the-art methods rely on a pre-defined set of\nclasses, they struggle with novel categories and out-of-distribution (OOD)\ndata. This is particularly problematic in safety-critical applications, such as\nautonomous driving, where reliability in unseen scenarios is essential. We\naddress the gap between outstanding benchmark performance and reliability by\nproposing Prior2Former (P2F), the first approach for segmentation vision\ntransformers rooted in evidential learning. P2F extends the mask vision\ntransformer architecture by incorporating a Beta prior for computing model\nuncertainty in pixel-wise binary mask assignments. This design enables\nhigh-quality uncertainty estimation that effectively detects novel and OOD\nobjects enabling state-of-the-art anomaly instance segmentation and open-world\npanoptic segmentation. Unlike most segmentation models addressing unknown\nclasses, P2F operates without access to OOD data samples or contrastive\ntraining on void (i.e., unlabeled) classes, making it highly applicable in\nreal-world scenarios where such prior information is unavailable. Additionally,\nP2F can be flexibly applied to anomaly instance and panoptic segmentation.\nThrough comprehensive experiments on the Cityscapes, COCO, SegmentMeIfYouCan,\nand OoDIS datasets, we demonstrate the state-of-the-art performance of P2F. It\nachieves the highest ranking in the OoDIS anomaly instance benchmark among\nmethods not using OOD data in any way."}
{"id": "2504.04308", "pdf": "https://arxiv.org/pdf/2504.04308", "abs": "https://arxiv.org/abs/2504.04308", "authors": ["Yingcong Li", "Davoud Ataee Tarzanagh", "Ankit Singh Rawat", "Maryam Fazel", "Samet Oymak"], "title": "Gating is Weighting: Understanding Gated Linear Attention through In-context Learning", "categories": ["cs.LG", "cs.AI", "cs.CL", "math.OC"], "comment": null, "summary": "Linear attention methods offer a compelling alternative to softmax attention\ndue to their efficiency in recurrent decoding. Recent research has focused on\nenhancing standard linear attention by incorporating gating while retaining its\ncomputational benefits. Such Gated Linear Attention (GLA) architectures include\ncompetitive models such as Mamba and RWKV. In this work, we investigate the\nin-context learning capabilities of the GLA model and make the following\ncontributions. We show that a multilayer GLA can implement a general class of\nWeighted Preconditioned Gradient Descent (WPGD) algorithms with data-dependent\nweights. These weights are induced by the gating mechanism and the input,\nenabling the model to control the contribution of individual tokens to\nprediction. To further understand the mechanics of this weighting, we introduce\na novel data model with multitask prompts and characterize the optimization\nlandscape of learning a WPGD algorithm. Under mild conditions, we establish the\nexistence and uniqueness (up to scaling) of a global minimum, corresponding to\na unique WPGD solution. Finally, we translate these findings to explore the\noptimization landscape of GLA and shed light on how gating facilitates\ncontext-aware learning and when it is provably better than vanilla linear\nattention."}
{"id": "2504.04842", "pdf": "https://arxiv.org/pdf/2504.04842", "abs": "https://arxiv.org/abs/2504.04842", "authors": ["Mengchao Wang", "Qiang Wang", "Fan Jiang", "Yaqi Fan", "Yunpeng Zhang", "Yonggang Qi", "Kun Zhao", "Mu Xu"], "title": "FantasyTalking: Realistic Talking Portrait Generation via Coherent Motion Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Creating a realistic animatable avatar from a single static portrait remains\nchallenging. Existing approaches often struggle to capture subtle facial\nexpressions, the associated global body movements, and the dynamic background.\nTo address these limitations, we propose a novel framework that leverages a\npretrained video diffusion transformer model to generate high-fidelity,\ncoherent talking portraits with controllable motion dynamics. At the core of\nour work is a dual-stage audio-visual alignment strategy. In the first stage,\nwe employ a clip-level training scheme to establish coherent global motion by\naligning audio-driven dynamics across the entire scene, including the reference\nportrait, contextual objects, and background. In the second stage, we refine\nlip movements at the frame level using a lip-tracing mask, ensuring precise\nsynchronization with audio signals. To preserve identity without compromising\nmotion flexibility, we replace the commonly used reference network with a\nfacial-focused cross-attention module that effectively maintains facial\nconsistency throughout the video. Furthermore, we integrate a motion intensity\nmodulation module that explicitly controls expression and body motion\nintensity, enabling controllable manipulation of portrait movements beyond mere\nlip motion. Extensive experimental results show that our proposed approach\nachieves higher quality with better realism, coherence, motion intensity, and\nidentity preservation. Ours project page:\nhttps://fantasy-amap.github.io/fantasy-talking/."}
{"id": "2504.04351", "pdf": "https://arxiv.org/pdf/2504.04351", "abs": "https://arxiv.org/abs/2504.04351", "authors": ["Jinyang Li", "Sangwon Hyun", "M. Ali Babar"], "title": "DDPT: Diffusion-Driven Prompt Tuning for Large Language Model Code Generation", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": "ICSE CAIN 2025", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncode generation. However, the quality of the generated code is heavily\ndependent on the structure and composition of the prompts used. Crafting\nhigh-quality prompts is a challenging task that requires significant knowledge\nand skills of prompt engineering. To advance the automation support for the\nprompt engineering for LLM-based code generation, we propose a novel solution\nDiffusion-Driven Prompt Tuning (DDPT) that learns how to generate optimal\nprompt embedding from Gaussian Noise to automate the prompt engineering for\ncode generation. We evaluate the feasibility of diffusion-based optimization\nand abstract the optimal prompt embedding as a directional vector toward the\noptimal embedding. We use the code generation loss given by the LLMs to help\nthe diffusion model capture the distribution of optimal prompt embedding during\ntraining. The trained diffusion model can build a path from the noise\ndistribution to the optimal distribution at the sampling phrase, the evaluation\nresult demonstrates that DDPT helps improve the prompt optimization for code\ngeneration."}
{"id": "2504.04869", "pdf": "https://arxiv.org/pdf/2504.04869", "abs": "https://arxiv.org/abs/2504.04869", "authors": ["Gang Wu", "Junjun Jiang", "Kui Jiang", "Xianming Liu"], "title": "Content-Aware Transformer for All-in-one Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Image restoration has witnessed significant advancements with the development\nof deep learning models. Although Transformer architectures have progressed\nconsiderably in recent years, challenges remain, particularly the limited\nreceptive field in window-based self-attention. In this work, we propose\nDSwinIR, a Deformable Sliding window Transformer for Image Restoration. DSwinIR\nintroduces a novel deformable sliding window self-attention that adaptively\nadjusts receptive fields based on image content, enabling the attention\nmechanism to focus on important regions and enhance feature extraction aligned\nwith salient features. Additionally, we introduce a central ensemble pattern to\nreduce the inclusion of irrelevant content within attention windows. In this\nway, the proposed DSwinIR model integrates the deformable sliding window\nTransformer and central ensemble pattern to amplify the strengths of both CNNs\nand Transformers while mitigating their limitations. Extensive experiments on\nvarious image restoration tasks demonstrate that DSwinIR achieves\nstate-of-the-art performance. For example, in image deraining, compared to\nDRSformer on the SPA dataset, DSwinIR achieves a 0.66 dB PSNR improvement. In\nall-in-one image restoration, compared to PromptIR, DSwinIR achieves over a\n0.66 dB and 1.04 dB improvement on three-task and five-task settings,\nrespectively. Pretrained models and code are available at our project\nhttps://github.com/Aitical/DSwinIR."}
{"id": "2504.04383", "pdf": "https://arxiv.org/pdf/2504.04383", "abs": "https://arxiv.org/abs/2504.04383", "authors": ["Ximing Lu", "Seungju Han", "David Acuna", "Hyunwoo Kim", "Jaehun Jung", "Shrimai Prabhumoye", "Niklas Muennighoff", "Mostofa Patwary", "Mohammad Shoeybi", "Bryan Catanzaro", "Yejin Choi"], "title": "Retro-Search: Exploring Untaken Paths for Deeper and Efficient Reasoning", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Code and data will be publicly released upon internal approval", "summary": "Large reasoning models exhibit remarkable reasoning capabilities via long,\nelaborate reasoning trajectories. Supervised fine-tuning on such reasoning\ntraces, also known as distillation, can be a cost-effective way to boost\nreasoning capabilities of student models. However, empirical observations\nreveal that these reasoning trajectories are often suboptimal, switching\nexcessively between different lines of thought, resulting in under-thinking,\nover-thinking, and even degenerate responses. We introduce Retro-Search, an\nMCTS-inspired search algorithm, for distilling higher quality reasoning paths\nfrom large reasoning models. Retro-Search retrospectively revises reasoning\npaths to discover better, yet shorter traces, which can then lead to student\nmodels with enhanced reasoning capabilities with shorter, thus faster\ninference. Our approach can enable two use cases: self-improvement, where\nmodels are fine-tuned on their own Retro-Search-ed thought traces, and\nweak-to-strong improvement, where a weaker model revises stronger model's\nthought traces via Retro-Search. For self-improving, R1-distill-7B, fine-tuned\non its own Retro-Search-ed traces, reduces the average reasoning length by\n31.2% while improving performance by 7.7% across seven math benchmarks. For\nweak-to-strong improvement, we retrospectively revise R1-671B's traces from the\nOpenThoughts dataset using R1-distill-32B as the Retro-Search-er, a model 20x\nsmaller. Qwen2.5-32B, fine-tuned on this refined data, achieves performance\ncomparable to R1-distill-32B, yielding an 11.3% reduction in reasoning length\nand a 2.4% performance improvement compared to fine-tuning on the original\nOpenThoughts data. Our work counters recently emergent viewpoints that question\nthe relevance of search algorithms in the era of large reasoning models, by\ndemonstrating that there are still opportunities for algorithmic advancements,\neven for frontier models."}
{"id": "2504.04893", "pdf": "https://arxiv.org/pdf/2504.04893", "abs": "https://arxiv.org/abs/2504.04893", "authors": ["Justus Westerhoff", "Erblina Purellku", "Jakob Hackstein", "Leo Pinetzki", "Lorenz Hufe"], "title": "SCAM: A Real-World Typographic Robustness Evaluation for Multimodal Foundation Models", "categories": ["cs.CV", "cs.AI"], "comment": "Submitted to CVPR 2025 Workshop EVAL-FoMo-2", "summary": "Typographic attacks exploit the interplay between text and visual content in\nmultimodal foundation models, causing misclassifications when misleading text\nis embedded within images. However, existing datasets are limited in size and\ndiversity, making it difficult to study such vulnerabilities. In this paper, we\nintroduce SCAM, the largest and most diverse dataset of real-world typographic\nattack images to date, containing 1,162 images across hundreds of object\ncategories and attack words. Through extensive benchmarking of Vision-Language\nModels (VLMs) on SCAM, we demonstrate that typographic attacks significantly\ndegrade performance, and identify that training data and model architecture\ninfluence the susceptibility to these attacks. Our findings reveal that\ntypographic attacks persist in state-of-the-art Large Vision-Language Models\n(LVLMs) due to the choice of their vision encoder, though larger Large Language\nModels (LLMs) backbones help mitigate their vulnerability. Additionally, we\ndemonstrate that synthetic attacks closely resemble real-world (handwritten)\nattacks, validating their use in research. Our work provides a comprehensive\nresource and empirical insights to facilitate future research toward robust and\ntrustworthy multimodal AI systems. We publicly release the datasets introduced\nin this paper under https://huggingface.co/datasets/BLISS-e-V/SCAM, along with\nthe code for evaluations at https://github.com/Bliss-e-V/SCAM."}
{"id": "2504.04453", "pdf": "https://arxiv.org/pdf/2504.04453", "abs": "https://arxiv.org/abs/2504.04453", "authors": ["Mohammad Amaan Sayeed", "Engin Tekin", "Maryam Nadeem", "Nancy A. ElNaker", "Aahan Singh", "Natalia Vassilieva", "Boulbaba Ben Amor"], "title": "Prot42: a Novel Family of Protein Language Models for Target-aware Protein Binder Generation", "categories": ["q-bio.BM", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Unlocking the next generation of biotechnology and therapeutic innovation\ndemands overcoming the inherent complexity and resource-intensity of\nconventional protein engineering methods. Recent GenAI-powered computational\ntechniques often rely on the availability of the target protein's 3D structures\nand specific binding sites to generate high-affinity binders, constraints\nexhibited by models such as AlphaProteo and RFdiffusion. In this work, we\nexplore the use of Protein Language Models (pLMs) for high-affinity binder\ngeneration. We introduce Prot42, a novel family of Protein Language Models\n(pLMs) pretrained on vast amounts of unlabeled protein sequences. By capturing\ndeep evolutionary, structural, and functional insights through an advanced\nauto-regressive, decoder-only architecture inspired by breakthroughs in natural\nlanguage processing, Prot42 dramatically expands the capabilities of\ncomputational protein design based on language only. Remarkably, our models\nhandle sequences up to 8,192 amino acids, significantly surpassing standard\nlimitations and enabling precise modeling of large proteins and complex\nmulti-domain sequences. Demonstrating powerful practical applications, Prot42\nexcels in generating high-affinity protein binders and sequence-specific\nDNA-binding proteins. Our innovative models are publicly available, offering\nthe scientific community an efficient and precise computational toolkit for\nrapid protein engineering."}
{"id": "2504.04903", "pdf": "https://arxiv.org/pdf/2504.04903", "abs": "https://arxiv.org/abs/2504.04903", "authors": ["Yuandong Pu", "Le Zhuo", "Kaiwen Zhu", "Liangbin Xie", "Wenlong Zhang", "Xiangyu Chen", "Pneg Gao", "Yu Qiao", "Chao Dong", "Yihao Liu"], "title": "Lumina-OmniLV: A Unified Multimodal Framework for General Low-Level Vision", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present Lunima-OmniLV (abbreviated as OmniLV), a universal multimodal\nmulti-task framework for low-level vision that addresses over 100 sub-tasks\nacross four major categories: image restoration, image enhancement,\nweak-semantic dense prediction, and stylization. OmniLV leverages both textual\nand visual prompts to offer flexible and user-friendly interactions. Built on\nDiffusion Transformer (DiT)-based generative priors, our framework supports\narbitrary resolutions -- achieving optimal performance at 1K resolution --\nwhile preserving fine-grained details and high fidelity. Through extensive\nexperiments, we demonstrate that separately encoding text and visual\ninstructions, combined with co-training using shallow feature control, is\nessential to mitigate task ambiguity and enhance multi-task generalization. Our\nfindings also reveal that integrating high-level generative tasks into\nlow-level vision models can compromise detail-sensitive restoration. These\ninsights pave the way for more robust and generalizable low-level vision\nsystems."}
{"id": "2504.04520", "pdf": "https://arxiv.org/pdf/2504.04520", "abs": "https://arxiv.org/abs/2504.04520", "authors": ["Ivan Ilin"], "title": "Hessian of Perplexity for Large Language Models by PyTorch autograd (Open Source)", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T07, 65K10, 65Y05"], "comment": "15 pages, 3 figures, open source code on GitHub", "summary": "Computing the full Hessian matrix -- the matrix of second-order derivatives\nfor an entire Large Language Model (LLM) is infeasible due to its sheer size.\nIn this technical report, we aim to provide a comprehensive guide on how to\naccurately compute at least a small portion of the Hessian for LLMs using\nPyTorch autograd library. We also demonstrate how to compute the full diagonal\nof the Hessian matrix using multiple samples of vector-Hessian Products (HVPs).\nWe hope that both this guide and the accompanying GitHub code will be valuable\nresources for practitioners and researchers interested in better understanding\nthe behavior and structure of the Hessian in LLMs."}
{"id": "2504.04907", "pdf": "https://arxiv.org/pdf/2504.04907", "abs": "https://arxiv.org/abs/2504.04907", "authors": ["Hui Han", "Siyuan Li", "Jiaqi Chen", "Yiwen Yuan", "Yuling Wu", "Chak Tou Leong", "Hanwen Du", "Junchen Fu", "Youhua Li", "Jie Zhang", "Chi Zhang", "Li-jia Li", "Yongxin Ni"], "title": "Video-Bench: Human-Aligned Video Generation Benchmark", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by CVPR'25", "summary": "Video generation assessment is essential for ensuring that generative models\nproduce visually realistic, high-quality videos while aligning with human\nexpectations. Current video generation benchmarks fall into two main\ncategories: traditional benchmarks, which use metrics and embeddings to\nevaluate generated video quality across multiple dimensions but often lack\nalignment with human judgments; and large language model (LLM)-based\nbenchmarks, though capable of human-like reasoning, are constrained by a\nlimited understanding of video quality metrics and cross-modal consistency. To\naddress these challenges and establish a benchmark that better aligns with\nhuman preferences, this paper introduces Video-Bench, a comprehensive benchmark\nfeaturing a rich prompt suite and extensive evaluation dimensions. This\nbenchmark represents the first attempt to systematically leverage MLLMs across\nall dimensions relevant to video generation assessment in generative models. By\nincorporating few-shot scoring and chain-of-query techniques, Video-Bench\nprovides a structured, scalable approach to generated video evaluation.\nExperiments on advanced models including Sora demonstrate that Video-Bench\nachieves superior alignment with human preferences across all dimensions.\nMoreover, in instances where our framework's assessments diverge from human\nevaluations, it consistently offers more objective and accurate insights,\nsuggesting an even greater potential advantage over traditional human judgment."}
{"id": "2504.04596", "pdf": "https://arxiv.org/pdf/2504.04596", "abs": "https://arxiv.org/abs/2504.04596", "authors": ["Noga Ben Yoash", "Meni Brief", "Oded Ovadia", "Gil Shenderovitz", "Moshik Mishaeli", "Rachel Lemberg", "Eitam Sheetrit"], "title": "SECQUE: A Benchmark for Evaluating Real-World Financial Analysis Capabilities", "categories": ["cs.AI", "cs.CE", "cs.CL"], "comment": "Benchmark available at:\n  https://huggingface.co/datasets/nogabenyoash/SecQue", "summary": "We introduce SECQUE, a comprehensive benchmark for evaluating large language\nmodels (LLMs) in financial analysis tasks. SECQUE comprises 565 expert-written\nquestions covering SEC filings analysis across four key categories: comparison\nanalysis, ratio calculation, risk assessment, and financial insight generation.\nTo assess model performance, we develop SECQUE-Judge, an evaluation mechanism\nleveraging multiple LLM-based judges, which demonstrates strong alignment with\nhuman evaluations. Additionally, we provide an extensive analysis of various\nmodels' performance on our benchmark. By making SECQUE publicly available, we\naim to facilitate further research and advancements in financial AI."}
{"id": "2504.04911", "pdf": "https://arxiv.org/pdf/2504.04911", "abs": "https://arxiv.org/abs/2504.04911", "authors": ["Ziyun Liang", "Xiaoqing Guo", "Wentian Xu", "Yasin Ibrahim", "Natalie Voets", "Pieter M Pretorius", "J. Alison Noble", "Konstantinos Kamnitsas"], "title": "IterMask3D: Unsupervised Anomaly Detection and Segmentation with Test-Time Iterative Mask Refinement in 3D Brain MR", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Unsupervised anomaly detection and segmentation methods train a model to\nlearn the training distribution as 'normal'. In the testing phase, they\nidentify patterns that deviate from this normal distribution as 'anomalies'. To\nlearn the `normal' distribution, prevailing methods corrupt the images and\ntrain a model to reconstruct them. During testing, the model attempts to\nreconstruct corrupted inputs based on the learned 'normal' distribution.\nDeviations from this distribution lead to high reconstruction errors, which\nindicate potential anomalies. However, corrupting an input image inevitably\ncauses information loss even in normal regions, leading to suboptimal\nreconstruction and an increased risk of false positives. To alleviate this, we\npropose IterMask3D, an iterative spatial mask-refining strategy designed for 3D\nbrain MRI. We iteratively spatially mask areas of the image as corruption and\nreconstruct them, then shrink the mask based on reconstruction error. This\nprocess iteratively unmasks 'normal' areas to the model, whose information\nfurther guides reconstruction of 'normal' patterns under the mask to be\nreconstructed accurately, reducing false positives. In addition, to achieve\nbetter reconstruction performance, we also propose using high-frequency image\ncontent as additional structural information to guide the reconstruction of the\nmasked area. Extensive experiments on the detection of both synthetic and\nreal-world imaging artifacts, as well as segmentation of various pathological\nlesions across multiple MRI sequences, consistently demonstrate the\neffectiveness of our proposed method."}
{"id": "2504.04639", "pdf": "https://arxiv.org/pdf/2504.04639", "abs": "https://arxiv.org/abs/2504.04639", "authors": ["Alberto Larrauri"], "title": "Ineffectiveness for Search and Undecidability of PCSP Meta-Problems", "categories": ["cs.CC", "cs.CL", "cs.DS", "cs.LO", "68Q17, 68Q25"], "comment": null, "summary": "It is an open question whether the search and decision versions of promise\nCSPs are equivalent. Most known algorithms for PCSPs solve only their\n\\emph{decision} variant, and it is unknown whether they can be adapted to solve\n\\emph{search} as well. The main approaches, called BLP, AIP and BLP+AIP, handle\na PCSP by finding a solution to a relaxation of some integer program. We prove\nthat rounding those solutions to a proper search certificate can be as hard as\nany problem in the class TFNP. In other words, these algorithms are ineffective\nfor search. Building on the algebraic approach to PCSPs, we find sufficient\nconditions that imply ineffectiveness for search. Our tools are tailored to\nalgorithms that are characterized by minions in a suitable way, and can also be\nused to prove undecidability results for meta-problems. This way, we show that\nthe families of templates solvable via BLP, AIP, and BLP+AIP are undecidable.\n  Using the same techniques we also analyze several algebraic conditions that\nare known to guarantee the tractability of finite-template CSPs. We prove that\nseveral meta-problems related to cyclic polymorphims and WNUs are undecidable\nfor PCSPs. In particular, there is no algorithm deciding whether a finite PCSP\ntemplate (1) admits cyclic a polymorphism, (2) admits a WNU."}
{"id": "2504.04924", "pdf": "https://arxiv.org/pdf/2504.04924", "abs": "https://arxiv.org/abs/2504.04924", "authors": ["Changqing Su", "Yanqin Chen", "Zihan Lin", "Zhen Cheng", "You Zhou", "Bo Xiong", "Zhaofei Yu", "Tiejun Huang"], "title": "Inter-event Interval Microscopy for Event Cameras", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Event cameras, an innovative bio-inspired sensor, differ from traditional\ncameras by sensing changes in intensity rather than directly perceiving\nintensity and recording these variations as a continuous stream of \"events\".\nThe intensity reconstruction from these sparse events has long been a\nchallenging problem. Previous approaches mainly focused on transforming\nmotion-induced events into videos or achieving intensity imaging for static\nscenes by integrating modulation devices at the event camera acquisition end.\nIn this paper, for the first time, we achieve event-to-intensity conversion\nusing a static event camera for both static and dynamic scenes in fluorescence\nmicroscopy. Unlike conventional methods that primarily rely on event\nintegration, the proposed Inter-event Interval Microscopy (IEIM) quantifies the\ntime interval between consecutive events at each pixel. With a fixed threshold\nin the event camera, the time interval can precisely represent the intensity.\nAt the hardware level, the proposed IEIM integrates a pulse light modulation\ndevice within a microscope equipped with an event camera, termed Pulse\nModulation-based Event-driven Fluorescence Microscopy.mAdditionally, we have\ncollected IEIMat dataset under various scenes including high dynamic range and\nhigh-speed scenarios. Experimental results on the IEIMat dataset demonstrate\nthat the proposed IEIM achieves superior spatial and temporal resolution, as\nwell as a higher dynamic range, with lower bandwidth compared to other methods.\nThe code and the IEIMat dataset will be made publicly available."}
{"id": "2504.04653", "pdf": "https://arxiv.org/pdf/2504.04653", "abs": "https://arxiv.org/abs/2504.04653", "authors": ["Yimu Wang", "Mozhgan Nasr Azadani", "Sean Sedwards", "Krzysztof Czarnecki"], "title": "LEO-MINI: An Efficient Multimodal Large Language Model using Conditional Token Reduction and Mixture of Multi-Modal Experts", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Redundancy of visual tokens in multi-modal large language models (MLLMs)\nsignificantly reduces their computational efficiency. Recent approaches, such\nas resamplers and summarizers, have sought to reduce the number of visual\ntokens, but at the cost of visual reasoning ability. To address this, we\npropose LEO-MINI, a novel MLLM that significantly reduces the number of visual\ntokens and simultaneously boosts visual reasoning capabilities. For efficiency,\nLEO-MINI incorporates CoTR, a novel token reduction module to consolidate a\nlarge number of visual tokens into a smaller set of tokens, using the\nsimilarity between visual tokens, text tokens, and a compact learnable query.\nFor effectiveness, to scale up the model's ability with minimal computational\noverhead, LEO-MINI employs MMoE, a novel mixture of multi-modal experts module.\nMMOE employs a set of LoRA experts with a novel router to switch between them\nbased on the input text and visual tokens instead of only using the input\nhidden state. MMoE also includes a general LoRA expert that is always activated\nto learn general knowledge for LLM reasoning. For extracting richer visual\nfeatures, MMOE employs a set of vision experts trained on diverse\ndomain-specific data. To demonstrate LEO-MINI's improved efficiency and\nperformance, we evaluate it against existing efficient MLLMs on various\nbenchmark vision-language tasks."}
{"id": "2504.04935", "pdf": "https://arxiv.org/pdf/2504.04935", "abs": "https://arxiv.org/abs/2504.04935", "authors": ["Peng Liu", "Heng-Chao Li", "Sen Lei", "Nanqing Liu", "Bin Feng", "Xiao Wu"], "title": "RCCFormer: A Robust Crowd Counting Network Based on Transformer", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Crowd counting, which is a key computer vision task, has emerged as a\nfundamental technology in crowd analysis and public safety management. However,\nchallenges such as scale variations and complex backgrounds significantly\nimpact the accuracy of crowd counting. To mitigate these issues, this paper\nproposes a robust Transformer-based crowd counting network, termed RCCFormer,\nspecifically designed for background suppression and scale awareness. The\nproposed method incorporates a Multi-level Feature Fusion Module (MFFM), which\nmeticulously integrates features extracted at diverse stages of the backbone\narchitecture. It establishes a strong baseline capable of capturing intricate\nand comprehensive feature representations, surpassing traditional baselines.\nFurthermore, the introduced Detail-Embedded Attention Block (DEAB) captures\ncontextual information and local details through global self-attention and\nlocal attention along with a learnable manner for efficient fusion. This\nenhances the model's ability to focus on foreground regions while effectively\nmitigating background noise interference. Additionally, we develop an Adaptive\nScale-Aware Module (ASAM), with our novel Input-dependent Deformable\nConvolution (IDConv) as its fundamental building block. This module dynamically\nadapts to changes in head target shapes and scales, significantly improving the\nnetwork's capability to accommodate large-scale variations. The effectiveness\nof the proposed method is validated on the ShanghaiTech Part_A and Part_B,\nNWPU-Crowd, and QNRF datasets. The results demonstrate that our RCCFormer\nachieves excellent performance across all four datasets, showcasing\nstate-of-the-art outcomes."}
{"id": "2504.04699", "pdf": "https://arxiv.org/pdf/2504.04699", "abs": "https://arxiv.org/abs/2504.04699", "authors": ["Martin Weyssow", "Chengran Yang", "Junkai Chen", "Yikun Li", "Huihui Huang", "Ratnadira Widyasari", "Han Wei Ang", "Frank Liauw", "Eng Lieh Ouh", "Lwin Khin Shar", "David Lo"], "title": "R2Vul: Learning to Reason about Software Vulnerabilities with Reinforcement Learning and Structured Reasoning Distillation", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown promising performance in software\nvulnerability detection (SVD), yet their reasoning capabilities remain\nunreliable. Existing approaches relying on chain-of-thought (CoT) struggle to\nprovide relevant and actionable security assessments. Additionally, effective\nSVD requires not only generating coherent reasoning but also differentiating\nbetween well-founded and misleading yet plausible security assessments, an\naspect overlooked in prior work. To this end, we introduce R2Vul, a novel\napproach that distills structured reasoning into small LLMs using reinforcement\nlearning from AI feedback (RLAIF). Through RLAIF, R2Vul enables LLMs to produce\nstructured, security-aware reasoning that is actionable and reliable while\nexplicitly learning to distinguish valid assessments from misleading ones. We\nevaluate R2Vul across five languages against SAST tools, CoT, instruction\ntuning, and classification-based baselines. Our results show that R2Vul with\nstructured reasoning distillation enables a 1.5B student LLM to rival larger\nmodels while improving generalization to out-of-distribution vulnerabilities.\nBeyond model improvements, we contribute a large-scale, multilingual preference\ndataset featuring structured reasoning to support future research in SVD."}
{"id": "2504.04974", "pdf": "https://arxiv.org/pdf/2504.04974", "abs": "https://arxiv.org/abs/2504.04974", "authors": ["Ming Li", "Ruiyi Zhang", "Jian Chen", "Jiuxiang Gu", "Yufan Zhou", "Franck Dernoncourt", "Wanrong Zhu", "Tianyi Zhou", "Tong Sun"], "title": "Towards Visual Text Grounding of Multimodal Large Language Model", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Despite the existing evolution of Multimodal Large Language Models (MLLMs), a\nnon-neglectable limitation remains in their struggle with visual text\ngrounding, especially in text-rich images of documents. Document images, such\nas scanned forms and infographics, highlight critical challenges due to their\ncomplex layouts and textual content. However, current benchmarks do not fully\naddress these challenges, as they mostly focus on visual grounding on natural\nimages, rather than text-rich document images. Thus, to bridge this gap, we\nintroduce TRIG, a novel task with a newly designed instruction dataset for\nbenchmarking and improving the Text-Rich Image Grounding capabilities of MLLMs\nin document question-answering. Specifically, we propose an OCR-LLM-human\ninteraction pipeline to create 800 manually annotated question-answer pairs as\na benchmark and a large-scale training set of 90$ synthetic data based on four\ndiverse datasets. A comprehensive evaluation of various MLLMs on our proposed\nbenchmark exposes substantial limitations in their grounding capability on\ntext-rich images. In addition, we propose two simple and effective TRIG methods\nbased on general instruction tuning and plug-and-play efficient embedding,\nrespectively. By finetuning MLLMs on our synthetic dataset, they promisingly\nimprove spatial reasoning and grounding capabilities."}
{"id": "2504.04704", "pdf": "https://arxiv.org/pdf/2504.04704", "abs": "https://arxiv.org/abs/2504.04704", "authors": ["Manlai Liang", "JiaMing Zhang", "Xiong Li", "Jinlong Li"], "title": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are Important", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modifiation of the inference infrastructure and\nsignificant computation overhead. Base on the fact that the Large Lanuage\nmodels are autoregresssive models, we propose {\\it LagKV}, a KV allocation\nstrategy only relying on straight forward comparison among KV themself. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on LongBench and PasskeyRetrieval show that, our\napproach achieves nearly zero loss when the ratio is $2\\times$ and $\\approx\n90\\%$ of the original model performance for $8\\times$. Especially in the\n64-digit passkey retrieval task, our mehod outperforms the attention weight\nbased method $H_2O$ over $60\\%$ with same compression ratios. Our code is\navailable at \\url{https://github.com/AI-Lab-China-Merchants-Bank/LagKV}."}
{"id": "2504.04981", "pdf": "https://arxiv.org/pdf/2504.04981", "abs": "https://arxiv.org/abs/2504.04981", "authors": ["Sohyun Lee", "Nayeong Kim", "Juwon Kang", "Seong Joon Oh", "Suha Kwak"], "title": "DiCoTTA: Domain-invariant Learning for Continual Test-time Adaptation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper studies continual test-time adaptation (CTTA), the task of\nadapting a model to constantly changing unseen domains in testing while\npreserving previously learned knowledge. Existing CTTA methods mostly focus on\nadaptation to the current test domain only, overlooking generalization to\narbitrary test domains a model may face in the future. To tackle this\nlimitation, we present a novel online domain-invariant learning framework for\nCTTA, dubbed DiCoTTA. DiCoTTA aims to learn feature representation to be\ninvariant to both current and previous test domains on the fly during testing.\nTo this end, we propose a new model architecture and a test-time adaptation\nstrategy dedicated to learning domain-invariant features without corrupting\nsemantic contents, along with a new data structure and optimization algorithm\nfor effectively managing information from previous test domains. DiCoTTA\nachieved state-of-the-art performance on four public CTTA benchmarks. Moreover,\nit showed superior generalization to unseen test domains."}
{"id": "2504.04736", "pdf": "https://arxiv.org/pdf/2504.04736", "abs": "https://arxiv.org/abs/2504.04736", "authors": ["Anna Goldie", "Azalia Mirhoseini", "Hao Zhou", "Irene Cai", "Christopher D. Manning"], "title": "Synthetic Data Generation & Multi-Step RL for Reasoning & Tool Use", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Reinforcement learning has been shown to improve the performance of large\nlanguage models. However, traditional approaches like RLHF or RLAIF treat the\nproblem as single-step. As focus shifts toward more complex reasoning and\nagentic tasks, language models must take multiple steps of text generation,\nreasoning and environment interaction before generating a solution. We propose\na synthetic data generation and RL methodology targeting multi-step\noptimization scenarios. This approach, called Step-Wise Reinforcement Learning\n(SWiRL), iteratively generates multi-step reasoning and tool use data, and then\nlearns from that data. It employs a simple step-wise decomposition that breaks\neach multi-step trajectory into multiple sub-trajectories corresponding to each\naction by the original model. It then applies synthetic data filtering and RL\noptimization on these sub-trajectories. We evaluated SWiRL on a number of\nmulti-step tool use, question answering, and mathematical reasoning tasks. Our\nexperiments show that SWiRL outperforms baseline approaches by 21.5%, 12.3%,\n14.8%, 11.1%, and 15.3% in relative accuracy on GSM8K, HotPotQA, CofCA,\nMuSiQue, and BeerQA, respectively. Excitingly, the approach exhibits\ngeneralization across tasks: for example, training only on HotPotQA (text\nquestion-answering) improves zero-shot performance on GSM8K (a math dataset) by\na relative 16.9%."}
{"id": "2504.04988", "pdf": "https://arxiv.org/pdf/2504.04988", "abs": "https://arxiv.org/abs/2504.04988", "authors": ["Congcong Wen", "Yiting Lin", "Xiaokang Qu", "Nan Li", "Yong Liao", "Hui Lin", "Xiang Li"], "title": "RS-RAG: Bridging Remote Sensing Imagery and Comprehensive Knowledge with a Multi-Modal Dataset and Retrieval-Augmented Generation Model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent progress in VLMs has demonstrated impressive capabilities across a\nvariety of tasks in the natural image domain. Motivated by these advancements,\nthe remote sensing community has begun to adopt VLMs for remote sensing\nvision-language tasks, including scene understanding, image captioning, and\nvisual question answering. However, existing remote sensing VLMs typically rely\non closed-set scene understanding and focus on generic scene descriptions, yet\nlack the ability to incorporate external knowledge. This limitation hinders\ntheir capacity for semantic reasoning over complex or context-dependent queries\nthat involve domain-specific or world knowledge. To address these challenges,\nwe first introduced a multimodal Remote Sensing World Knowledge (RSWK) dataset,\nwhich comprises high-resolution satellite imagery and detailed textual\ndescriptions for 14,141 well-known landmarks from 175 countries, integrating\nboth remote sensing domain knowledge and broader world knowledge. Building upon\nthis dataset, we proposed a novel Remote Sensing Retrieval-Augmented Generation\n(RS-RAG) framework, which consists of two key components. The Multi-Modal\nKnowledge Vector Database Construction module encodes remote sensing imagery\nand associated textual knowledge into a unified vector space. The Knowledge\nRetrieval and Response Generation module retrieves and re-ranks relevant\nknowledge based on image and/or text queries, and incorporates the retrieved\ncontent into a knowledge-augmented prompt to guide the VLM in producing\ncontextually grounded responses. We validated the effectiveness of our approach\non three representative vision-language tasks, including image captioning,\nimage classification, and visual question answering, where RS-RAG significantly\noutperformed state-of-the-art baselines."}
{"id": "2504.04927", "pdf": "https://arxiv.org/pdf/2504.04927", "abs": "https://arxiv.org/abs/2504.04927", "authors": ["Danial Amin", "Joni Salminen", "Farhan Ahmed", "Sonja M. H. Tervola", "Sankalp Sethi", "Bernard J. Jansen"], "title": "How Is Generative AI Used for Persona Development?: A Systematic Review of 52 Research Articles", "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "Although Generative AI (GenAI) has the potential for persona development,\nmany challenges must be addressed. This research systematically reviews 52\narticles from 2022-2024, with important findings. First, closed commercial\nmodels are frequently used in persona development, creating a monoculture\nSecond, GenAI is used in various stages of persona development (data\ncollection, segmentation, enrichment, and evaluation). Third, similar to other\nquantitative persona development techniques, there are major gaps in persona\nevaluation for AI generated personas. Fourth, human-AI collaboration models are\nunderdeveloped, despite human oversight being crucial for maintaining ethical\nstandards. These findings imply that realizing the full potential of\nAI-generated personas will require substantial efforts across academia and\nindustry. To that end, we provide a list of research avenues to inspire future\nwork."}
{"id": "2504.05030", "pdf": "https://arxiv.org/pdf/2504.05030", "abs": "https://arxiv.org/abs/2504.05030", "authors": ["Wang Tang", "Fethiye Irmak Dogan", "Linbo Qing", "Hatice Gunes"], "title": "AsyReC: A Multimodal Graph-based Framework for Spatio-Temporal Asymmetric Dyadic Relationship Classification", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Dyadic social relationships, which refer to relationships between two\nindividuals who know each other through repeated interactions (or not), are\nshaped by shared spatial and temporal experiences. Current computational\nmethods for modeling these relationships face three major challenges: (1) the\nfailure to model asymmetric relationships, e.g., one individual may perceive\nthe other as a friend while the other perceives them as an acquaintance, (2)\nthe disruption of continuous interactions by discrete frame sampling, which\nsegments the temporal continuity of interaction in real-world scenarios, and\n(3) the limitation to consider periodic behavioral cues, such as rhythmic\nvocalizations or recurrent gestures, which are crucial for inferring the\nevolution of dyadic relationships. To address these challenges, we propose\nAsyReC, a multimodal graph-based framework for asymmetric dyadic relationship\nclassification, with three core innovations: (i) a triplet graph neural network\nwith node-edge dual attention that dynamically weights multimodal cues to\ncapture interaction asymmetries (addressing challenge 1); (ii) a clip-level\nrelationship learning architecture that preserves temporal continuity, enabling\nfine-grained modeling of real-world interaction dynamics (addressing challenge\n2); and (iii) a periodic temporal encoder that projects time indices onto\nsine/cosine waveforms to model recurrent behavioral patterns (addressing\nchallenge 3). Extensive experiments on two public datasets demonstrate\nstate-of-the-art performance, while ablation studies validate the critical role\nof asymmetric interaction modeling and periodic temporal encoding in improving\nthe robustness of dyadic relationship classification in real-world scenarios.\nOur code is publicly available at: https://github.com/tw-repository/AsyReC."}
{"id": "2504.04945", "pdf": "https://arxiv.org/pdf/2504.04945", "abs": "https://arxiv.org/abs/2504.04945", "authors": ["Rean Fernandes", "Andr√© Biedenkapp", "Frank Hutter", "Noor Awad"], "title": "A Llama walks into the 'Bar': Efficient Supervised Fine-Tuning for Legal Reasoning in the Multi-state Bar Exam", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.7; I.2.1"], "comment": "COLM 2025 preprint, 9 pages, 3 figures, 16 appendix pages", "summary": "Legal reasoning tasks present unique challenges for large language models\n(LLMs) due to the complexity of domain-specific knowledge and reasoning\nprocesses. This paper investigates how effectively smaller language models\n(Llama 2 7B and Llama 3 8B) can be fine-tuned with a limited dataset of 1,514\nMulti-state Bar Examination (MBE) questions to improve legal question answering\naccuracy. We evaluate these models on the 2022 MBE questions licensed from JD\nAdvising, the same dataset used in the 'GPT-4 passes the Bar exam' study. Our\nmethodology involves collecting approximately 200 questions per legal domain\nacross 7 domains. We distill the dataset using Llama 3 (70B) to transform\nexplanations into a structured IRAC (Issue, Rule, Application, Conclusion)\nformat as a guided reasoning process to see if it results in better performance\nover the non-distilled dataset. We compare the non-fine-tuned models against\ntheir supervised fine-tuned (SFT) counterparts, trained for different sample\nsizes per domain, to study the effect on accuracy and prompt adherence. We also\nanalyse option selection biases and their mitigation following SFT. In\naddition, we consolidate the performance across multiple variables: prompt type\n(few-shot vs zero-shot), answer ordering (chosen-option first vs\ngenerated-explanation first), response format (Numbered list vs Markdown vs\nJSON), and different decoding temperatures. Our findings show that\ndomain-specific SFT helps some model configurations achieve close to human\nbaseline performance, despite limited computational resources and a relatively\nsmall dataset. We release both the gathered SFT dataset and the family of\nSupervised Fine-tuned (SFT) adapters optimised for MBE performance. This\nestablishes a practical lower bound on resources needed towards achieving\neffective legal question answering in smaller LLMs."}
{"id": "2504.05040", "pdf": "https://arxiv.org/pdf/2504.05040", "abs": "https://arxiv.org/abs/2504.05040", "authors": ["Haiwan Wei", "Yitian Yuan", "Xiaohan Lan", "Wei Ke", "Lin Ma"], "title": "InstructionBench: An Instructional Video Understanding Benchmark", "categories": ["cs.CV"], "comment": null, "summary": "Despite progress in video large language models (Video-LLMs), research on\ninstructional video understanding, crucial for enhancing access to\ninstructional content, remains insufficient. To address this, we introduce\nInstructionBench, an Instructional video understanding Benchmark, which\nchallenges models' advanced temporal reasoning within instructional videos\ncharacterized by their strict step-by-step flow. Employing GPT-4, we formulate\nQ\\&A pairs in open-ended and multiple-choice formats to assess both\nCoarse-Grained event-level and Fine-Grained object-level reasoning. Our\nfiltering strategies exclude questions answerable purely by common-sense\nknowledge, focusing on visual perception and analysis when evaluating Video-LLM\nmodels. The benchmark finally contains 5k questions across over 700 videos. We\nevaluate the latest Video-LLMs on our InstructionBench, finding that\nclosed-source models outperform open-source ones. However, even the best model,\nGPT-4o, achieves only 53.42\\% accuracy, indicating significant gaps in temporal\nreasoning. To advance the field, we also develop a comprehensive instructional\nvideo dataset with over 19k Q\\&A pairs from nearly 2.5k videos, using an\nautomated data generation framework, thereby enriching the community's research\nresources."}
{"id": "2504.04974", "pdf": "https://arxiv.org/pdf/2504.04974", "abs": "https://arxiv.org/abs/2504.04974", "authors": ["Ming Li", "Ruiyi Zhang", "Jian Chen", "Jiuxiang Gu", "Yufan Zhou", "Franck Dernoncourt", "Wanrong Zhu", "Tianyi Zhou", "Tong Sun"], "title": "Towards Visual Text Grounding of Multimodal Large Language Model", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Despite the existing evolution of Multimodal Large Language Models (MLLMs), a\nnon-neglectable limitation remains in their struggle with visual text\ngrounding, especially in text-rich images of documents. Document images, such\nas scanned forms and infographics, highlight critical challenges due to their\ncomplex layouts and textual content. However, current benchmarks do not fully\naddress these challenges, as they mostly focus on visual grounding on natural\nimages, rather than text-rich document images. Thus, to bridge this gap, we\nintroduce TRIG, a novel task with a newly designed instruction dataset for\nbenchmarking and improving the Text-Rich Image Grounding capabilities of MLLMs\nin document question-answering. Specifically, we propose an OCR-LLM-human\ninteraction pipeline to create 800 manually annotated question-answer pairs as\na benchmark and a large-scale training set of 90$ synthetic data based on four\ndiverse datasets. A comprehensive evaluation of various MLLMs on our proposed\nbenchmark exposes substantial limitations in their grounding capability on\ntext-rich images. In addition, we propose two simple and effective TRIG methods\nbased on general instruction tuning and plug-and-play efficient embedding,\nrespectively. By finetuning MLLMs on our synthetic dataset, they promisingly\nimprove spatial reasoning and grounding capabilities."}
{"id": "2504.05046", "pdf": "https://arxiv.org/pdf/2504.05046", "abs": "https://arxiv.org/abs/2504.05046", "authors": ["Shenghao Ren", "Yi Lu", "Jiayi Huang", "Jiayi Zhao", "He Zhang", "Tao Yu", "Qiu Shen", "Xun Cao"], "title": "MotionPRO: Exploring the Role of Pressure in Human MoCap and Beyond", "categories": ["cs.CV"], "comment": null, "summary": "Existing human Motion Capture (MoCap) methods mostly focus on the visual\nsimilarity while neglecting the physical plausibility. As a result, downstream\ntasks such as driving virtual human in 3D scene or humanoid robots in real\nworld suffer from issues such as timing drift and jitter, spatial problems like\nsliding and penetration, and poor global trajectory accuracy. In this paper, we\nrevisit human MoCap from the perspective of interaction between human body and\nphysical world by exploring the role of pressure. Firstly, we construct a\nlarge-scale human Motion capture dataset with Pressure, RGB and Optical sensors\n(named MotionPRO), which comprises 70 volunteers performing 400 types of\nmotion, encompassing a total of 12.4M pose frames. Secondly, we examine both\nthe necessity and effectiveness of the pressure signal through two challenging\ntasks: (1) pose and trajectory estimation based solely on pressure: We propose\na network that incorporates a small kernel decoder and a long-short-term\nattention module, and proof that pressure could provide accurate global\ntrajectory and plausible lower body pose. (2) pose and trajectory estimation by\nfusing pressure and RGB: We impose constraints on orthographic similarity along\nthe camera axis and whole-body contact along the vertical axis to enhance the\ncross-attention strategy to fuse pressure and RGB feature maps. Experiments\ndemonstrate that fusing pressure with RGB features not only significantly\nimproves performance in terms of objective metrics, but also plausibly drives\nvirtual humans (SMPL) in 3D scene. Furthermore, we demonstrate that\nincorporating physical perception enables humanoid robots to perform more\nprecise and stable actions, which is highly beneficial for the development of\nembodied artificial intelligence. Project page is available at:\nhttps://nju-cite-mocaphumanoid.github.io/MotionPRO/"}
{"id": "2504.05019", "pdf": "https://arxiv.org/pdf/2504.05019", "abs": "https://arxiv.org/abs/2504.05019", "authors": ["Ngoc Bui", "Hieu Trung Nguyen", "Shantanu Kumar", "Julian Theodore", "Weikang Qiu", "Viet Anh Nguyen", "Rex Ying"], "title": "Mixture-of-Personas Language Models for Population Simulation", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Advances in Large Language Models (LLMs) paved the way for their emerging\napplications in various domains, such as human behavior simulations, where LLMs\ncould augment human-generated data in social science research and machine\nlearning model training. However, pretrained LLMs often fail to capture the\nbehavioral diversity of target populations due to the inherent variability\nacross individuals and groups. To address this, we propose \\textit{Mixture of\nPersonas} (MoP), a \\textit{probabilistic} prompting method that aligns the LLM\nresponses with the target population. MoP is a contextual mixture model, where\neach component is an LM agent characterized by a persona and an exemplar\nrepresenting subpopulation behaviors. The persona and exemplar are randomly\nchosen according to the learned mixing weights to elicit diverse LLM responses\nduring simulation. MoP is flexible, requires no model finetuning, and is\ntransferable across base models. Experiments for synthetic data generation show\nthat MoP outperforms competing methods in alignment and diversity metrics."}
{"id": "2504.05049", "pdf": "https://arxiv.org/pdf/2504.05049", "abs": "https://arxiv.org/abs/2504.05049", "authors": ["Shuai Chen", "Fanman Meng", "Haoran Wei", "Chenhao Wu", "Qingbo Wu", "Linfeng Xu", "Hongliang Li"], "title": "CMaP-SAM: Contraction Mapping Prior for SAM-driven Few-shot Segmentation", "categories": ["cs.CV"], "comment": "7 figures", "summary": "Few-shot segmentation (FSS) aims to segment new classes using few annotated\nimages. While recent FSS methods have shown considerable improvements by\nleveraging Segment Anything Model (SAM), they face two critical limitations:\ninsufficient utilization of structural correlations in query images, and\nsignificant information loss when converting continuous position priors to\ndiscrete point prompts. To address these challenges, we propose CMaP-SAM, a\nnovel framework that introduces contraction mapping theory to optimize position\npriors for SAM-driven few-shot segmentation. CMaP-SAM consists of three key\ncomponents: (1) a contraction mapping module that formulates position prior\noptimization as a Banach contraction mapping with convergence guarantees. This\nmodule iteratively refines position priors through pixel-wise structural\nsimilarity, generating a converged prior that preserves both semantic guidance\nfrom reference images and structural correlations in query images; (2) an\nadaptive distribution alignment module bridging continuous priors with SAM's\nbinary mask prompt encoder; and (3) a foreground-background decoupled\nrefinement architecture producing accurate final segmentation masks. Extensive\nexperiments demonstrate CMaP-SAM's effectiveness, achieving state-of-the-art\nperformance with 71.1 mIoU on PASCAL-$5^i$ and 56.1 on COCO-$20^i$ datasets."}
{"id": "2504.05216", "pdf": "https://arxiv.org/pdf/2504.05216", "abs": "https://arxiv.org/abs/2504.05216", "authors": ["Hengran Zhang", "Keping Bi", "Jiafeng Guo", "Xiaojie Sun", "Shihao Liu", "Daiting Shi", "Dawei Yin", "Xueqi Cheng"], "title": "Unleashing the Power of LLMs in Dense Retrieval with Query Likelihood Modeling", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "12 pages, 3 figures", "summary": "Dense retrieval is a crucial task in Information Retrieval (IR) and is the\nfoundation for downstream tasks such as re-ranking. Recently, large language\nmodels (LLMs) have shown compelling semantic understanding capabilities and are\nappealing to researchers studying dense retrieval. LLMs, as decoder-style\ngenerative models, are competent at language generation while falling short on\nmodeling global information due to the lack of attention to tokens afterward.\nInspired by the classical word-based language modeling approach for IR, i.e.,\nthe query likelihood (QL) model, we seek to sufficiently utilize LLMs'\ngenerative ability by QL maximization. However, instead of ranking documents\nwith QL estimation, we introduce an auxiliary task of QL maximization to yield\na better backbone for contrastively learning a discriminative retriever. We\nname our model as LLM-QL. To condense global document semantics to a single\nvector during QL modeling, LLM-QL has two major components, Attention Stop (AS)\nand Input Corruption (IC). AS stops the attention of predictive tokens to\nprevious tokens until the ending token of the document. IC masks a portion of\ntokens in the input documents during prediction. Experiments on MSMARCO show\nthat LLM-QL can achieve significantly better performance than other LLM-based\nretrievers and using QL estimated by LLM-QL for ranking outperforms word-based\nQL by a large margin."}
{"id": "2504.05062", "pdf": "https://arxiv.org/pdf/2504.05062", "abs": "https://arxiv.org/abs/2504.05062", "authors": ["Chenfeng Xu"], "title": "LDGNet: A Lightweight Difference Guiding Network for Remote Sensing Change Detection", "categories": ["cs.CV"], "comment": null, "summary": "With the rapid advancement of deep learning, the field of change detection\n(CD) in remote sensing imagery has achieved remarkable progress. Existing\nchange detection methods primarily focus on achieving higher accuracy with\nincreased computational costs and parameter sizes, leaving development of\nlightweight methods for rapid real-world processing an underexplored challenge.\nTo address this challenge, we propose a Lightweight Difference Guiding Network\n(LDGNet), leveraging absolute difference image to guide optical remote sensing\nchange detection. First, to enhance the feature representation capability of\nthe lightweight backbone network, we propose the Difference Guiding Module\n(DGM), which leverages multi-scale features extracted from the absolute\ndifference image to progressively influence the original image encoder at each\nlayer, thereby reinforcing feature extraction. Second, we propose the\nDifference-Aware Dynamic Fusion (DADF) module with Visual State Space Model\n(VSSM) for lightweight long-range dependency modeling. The module first uses\nfeature absolute differences to guide VSSM's global contextual modeling of\nchange regions, then employs difference attention to dynamically fuse these\nlong-range features with feature differences, enhancing change semantics while\nsuppressing noise and background. Extensive experiments on multiple datasets\ndemonstrate that our method achieves comparable or superior performance to\ncurrent state-of-the-art (SOTA) methods requiring several times more\ncomputation, while maintaining only 3.43M parameters and 1.12G FLOPs."}
{"id": "2504.05220", "pdf": "https://arxiv.org/pdf/2504.05220", "abs": "https://arxiv.org/abs/2504.05220", "authors": ["Hengran Zhang", "Minghao Tang", "Keping Bi", "Jiafeng Guo", "Shihao Liu", "Daiting Shi", "Dawei Yin", "Xueqi Cheng"], "title": "Leveraging LLMs for Utility-Focused Annotation: Reducing Manual Effort for Retrieval and RAG", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "12 pages, 4 figures", "summary": "Retrieval models typically rely on costly human-labeled query-document\nrelevance annotations for training and evaluation. To reduce this cost and\nleverage the potential of Large Language Models (LLMs) in relevance judgments,\nwe aim to explore whether LLM-generated annotations can effectively replace\nhuman annotations in training retrieval models. Retrieval usually emphasizes\nrelevance, which indicates \"topic-relatedness\" of a document to a query, while\nin RAG, the value of a document (or utility) depends on how it contributes to\nanswer generation. Recognizing this mismatch, some researchers use LLM\nperformance on downstream tasks with documents as labels, but this approach\nrequires manual answers for specific tasks, leading to high costs and limited\ngeneralization. In another line of work, prompting LLMs to select useful\ndocuments as RAG references eliminates the need for human annotation and is not\ntask-specific. If we leverage LLMs' utility judgments to annotate retrieval\ndata, we may retain cross-task generalization without human annotation in\nlarge-scale corpora. Therefore, we investigate utility-focused annotation via\nLLMs for large-scale retriever training data across both in-domain and\nout-of-domain settings on the retrieval and RAG tasks. To reduce the impact of\nlow-quality positives labeled by LLMs, we design a novel loss function, i.e.,\nDisj-InfoNCE. Our experiments reveal that: (1) Retrievers trained on\nutility-focused annotations significantly outperform those trained on human\nannotations in the out-of-domain setting on both tasks, demonstrating superior\ngeneralization capabilities. (2) LLM annotation does not replace human\nannotation in the in-domain setting. However, incorporating just 20%\nhuman-annotated data enables retrievers trained with utility-focused\nannotations to match the performance of models trained entirely with human\nannotations."}
{"id": "2504.05075", "pdf": "https://arxiv.org/pdf/2504.05075", "abs": "https://arxiv.org/abs/2504.05075", "authors": ["Jie Wang", "Tingfa Xu", "Lihe Ding", "Xinjie Zhang", "Long Bai", "Jianan Li"], "title": "PvNeXt: Rethinking Network Design and Temporal Motion for Point Cloud Video Recognition", "categories": ["cs.CV"], "comment": "Accepted by ICLR 2025", "summary": "Point cloud video perception has become an essential task for the realm of 3D\nvision. Current 4D representation learning techniques typically engage in\niterative processing coupled with dense query operations. Although effective in\ncapturing temporal features, this approach leads to substantial computational\nredundancy. In this work, we propose a framework, named as PvNeXt, for\neffective yet efficient point cloud video recognition, via personalized\none-shot query operation. Specially, PvNeXt consists of two key modules, the\nMotion Imitator and the Single-Step Motion Encoder. The former module, the\nMotion Imitator, is designed to capture the temporal dynamics inherent in\nsequences of point clouds, thus generating the virtual motion corresponding to\neach frame. The Single-Step Motion Encoder performs a one-step query operation,\nassociating point cloud of each frame with its corresponding virtual motion\nframe, thereby extracting motion cues from point cloud sequences and capturing\ntemporal dynamics across the entire sequence. Through the integration of these\ntwo modules, {PvNeXt} enables personalized one-shot queries for each frame,\neffectively eliminating the need for frame-specific looping and intensive query\nprocesses. Extensive experiments on multiple benchmarks demonstrate the\neffectiveness of our method."}
{"id": "2504.05258", "pdf": "https://arxiv.org/pdf/2504.05258", "abs": "https://arxiv.org/abs/2504.05258", "authors": ["Adri√°n Bazaga", "Rexhina Blloshmi", "Bill Byrne", "Adri√† de Gispert"], "title": "Learning to Reason Over Time: Timeline Self-Reflection for Improved Temporal Reasoning in Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have emerged as powerful tools for generating\ncoherent text, understanding context, and performing reasoning tasks. However,\nthey struggle with temporal reasoning, which requires processing time-related\ninformation such as event sequencing, durations, and inter-temporal\nrelationships. These capabilities are critical for applications including\nquestion answering, scheduling, and historical analysis. In this paper, we\nintroduce TISER, a novel framework that enhances the temporal reasoning\nabilities of LLMs through a multi-stage process that combines timeline\nconstruction with iterative self-reflection. Our approach leverages test-time\nscaling to extend the length of reasoning traces, enabling models to capture\ncomplex temporal dependencies more effectively. This strategy not only boosts\nreasoning accuracy but also improves the traceability of the inference process.\nExperimental results demonstrate state-of-the-art performance across multiple\nbenchmarks, including out-of-distribution test sets, and reveal that TISER\nenables smaller open-source models to surpass larger closed-weight models on\nchallenging temporal reasoning tasks."}
{"id": "2504.05076", "pdf": "https://arxiv.org/pdf/2504.05076", "abs": "https://arxiv.org/abs/2504.05076", "authors": ["Shuai Liu", "Qingyu Mao", "Chao Li", "Jiacong Chen", "Fanyang Meng", "Yonghong Tian", "Yongsheng Liang"], "title": "Content-Distortion High-Order Interaction for Blind Image Quality Assessment", "categories": ["cs.CV"], "comment": "19 pages (main text: 14 pages + appendix: 5 pages), 9 figures, 23\n  tables. In submission", "summary": "The content and distortion are widely recognized as the two primary factors\naffecting the visual quality of an image. While existing No-Reference Image\nQuality Assessment (NR-IQA) methods have modeled these factors, they fail to\ncapture the complex interactions between content and distortions. This\nshortfall impairs their ability to accurately perceive quality. To confront\nthis, we analyze the key properties required for interaction modeling and\npropose a robust NR-IQA approach termed CoDI-IQA (Content-Distortion high-order\nInteraction for NR-IQA), which aggregates local distortion and global content\nfeatures within a hierarchical interaction framework. Specifically, a\nProgressive Perception Interaction Module (PPIM) is proposed to explicitly\nsimulate how content and distortions independently and jointly influence image\nquality. By integrating internal interaction, coarse interaction, and fine\ninteraction, it achieves high-order interaction modeling that allows the model\nto properly represent the underlying interaction patterns. To ensure sufficient\ninteraction, multiple PPIMs are employed to hierarchically fuse multi-level\ncontent and distortion features at different granularities. We also tailor a\ntraining strategy suited for CoDI-IQA to maintain interaction stability.\nExtensive experiments demonstrate that the proposed method notably outperforms\nthe state-of-the-art methods in terms of prediction accuracy, data efficiency,\nand generalization ability."}
{"id": "2504.05288", "pdf": "https://arxiv.org/pdf/2504.05288", "abs": "https://arxiv.org/abs/2504.05288", "authors": ["Mingyang Fu", "Yuyang Peng", "Benlin Liu", "Yao Wan", "Dongping Chen"], "title": "LiveVQA: Live Visual Knowledge Seeking", "categories": ["cs.CV", "cs.CL"], "comment": "Work in progress", "summary": "We introduce LiveVQA, an automatically collected dataset of latest visual\nknowledge from the Internet with synthesized VQA problems. LiveVQA consists of\n3,602 single- and multi-hop visual questions from 6 news websites across 14\nnews categories, featuring high-quality image-text coherence and authentic\ninformation. Our evaluation across 15 MLLMs (e.g., GPT-4o, Gemma-3, and\nQwen-2.5-VL family) demonstrates that stronger models perform better overall,\nwith advanced visual reasoning capabilities proving crucial for complex\nmulti-hop questions. Despite excellent performance on textual problems, models\nwith tools like search engines still show significant gaps when addressing\nvisual questions requiring latest visual knowledge, highlighting important\nareas for future research."}
{"id": "2504.05089", "pdf": "https://arxiv.org/pdf/2504.05089", "abs": "https://arxiv.org/abs/2504.05089", "authors": ["Johannes Dollinger", "Damien Robert", "Elena Plekhanova", "Lukas Drees", "Jan Dirk Wegner"], "title": "Climplicit: Climatic Implicit Embeddings for Global Ecological Tasks", "categories": ["cs.CV"], "comment": "Published as a workshop paper at \"Tackling Climate Change with\n  Machine Learning\", ICLR 2025", "summary": "Deep learning on climatic data holds potential for macroecological\napplications. However, its adoption remains limited among scientists outside\nthe deep learning community due to storage, compute, and technical expertise\nbarriers. To address this, we introduce Climplicit, a spatio-temporal\ngeolocation encoder pretrained to generate implicit climatic representations\nanywhere on Earth. By bypassing the need to download raw climatic rasters and\ntrain feature extractors, our model uses x1000 fewer disk space and\nsignificantly reduces computational needs for downstream tasks. We evaluate our\nClimplicit embeddings on biomes classification, species distribution modeling,\nand plant trait regression. We find that linear probing our Climplicit\nembeddings consistently performs better or on par with training a model from\nscratch on downstream tasks and overall better than alternative geolocation\nencoding models."}
{"id": "2504.05112", "pdf": "https://arxiv.org/pdf/2504.05112", "abs": "https://arxiv.org/abs/2504.05112", "authors": ["Ronghui Zhang", "Dakang Lyu", "Tengfei Li", "Yunfan Wu", "Ujjal Manandhar", "Benfei Wang", "Junzhou Chen", "Bolin Gao", "Danwei Wang", "Yiqiu Tan"], "title": "ABCDWaveNet: Advancing Robust Road Ponding Detection in Fog through Dynamic Frequency-Spatial Synergy", "categories": ["cs.CV"], "comment": null, "summary": "Road ponding presents a significant threat to vehicle safety, particularly in\nadverse fog conditions, where reliable detection remains a persistent challenge\nfor Advanced Driver Assistance Systems (ADAS). To address this, we propose\nABCDWaveNet, a novel deep learning framework leveraging Dynamic\nFrequency-Spatial Synergy for robust ponding detection in fog. The core of\nABCDWaveNet achieves this synergy by integrating dynamic convolution for\nadaptive feature extraction across varying visibilities with a wavelet-based\nmodule for synergistic frequency-spatial feature enhancement, significantly\nimproving robustness against fog interference. Building on this foundation,\nABCDWaveNet captures multi-scale structural and contextual information,\nsubsequently employing an Adaptive Attention Coupling Gate (AACG) to adaptively\nfuse global and local features for enhanced accuracy. To facilitate realistic\nevaluations under combined adverse conditions, we introduce the Foggy Low-Light\nPuddle dataset. Extensive experiments demonstrate that ABCDWaveNet establishes\nnew state-of-the-art performance, achieving significant Intersection over Union\n(IoU) gains of 3.51%, 1.75%, and 1.03% on the Foggy-Puddle, Puddle-1000, and\nour Foggy Low-Light Puddle datasets, respectively. Furthermore, its processing\nspeed of 25.48 FPS on an NVIDIA Jetson AGX Orin confirms its suitability for\nADAS deployment. These findings underscore the effectiveness of the proposed\nDynamic Frequency-Spatial Synergy within ABCDWaveNet, offering valuable\ninsights for developing proactive road safety solutions capable of operating\nreliably in challenging weather conditions."}
{"id": "2504.05135", "pdf": "https://arxiv.org/pdf/2504.05135", "abs": "https://arxiv.org/abs/2504.05135", "authors": ["Jiamei Xiong", "Xuefeng Yan", "Yongzhen Wang", "Wei Zhao", "Xiao-Ping Zhang", "Mingqiang Wei"], "title": "DA2Diff: Exploring Degradation-aware Adaptive Diffusion Priors for All-in-One Weather Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Image restoration under adverse weather conditions is a critical task for\nmany vision-based applications. Recent all-in-one frameworks that handle\nmultiple weather degradations within a unified model have shown potential.\nHowever, the diversity of degradation patterns across different weather\nconditions, as well as the complex and varied nature of real-world\ndegradations, pose significant challenges for multiple weather removal. To\naddress these challenges, we propose an innovative diffusion paradigm with\ndegradation-aware adaptive priors for all-in-one weather restoration, termed\nDA2Diff. It is a new exploration that applies CLIP to perceive\ndegradation-aware properties for better multi-weather restoration.\nSpecifically, we deploy a set of learnable prompts to capture degradation-aware\nrepresentations by the prompt-image similarity constraints in the CLIP space.\nBy aligning the snowy/hazy/rainy images with snow/haze/rain prompts, each\nprompt contributes to different weather degradation characteristics. The\nlearned prompts are then integrated into the diffusion model via the designed\nweather specific prompt guidance module, making it possible to restore multiple\nweather types. To further improve the adaptiveness to complex weather\ndegradations, we propose a dynamic expert selection modulator that employs a\ndynamic weather-aware router to flexibly dispatch varying numbers of\nrestoration experts for each weather-distorted image, allowing the diffusion\nmodel to restore diverse degradations adaptively. Experimental results\nsubstantiate the favorable performance of DA2Diff over state-of-the-arts in\nquantitative and qualitative evaluation. Source code will be available after\nacceptance."}
{"id": "2504.05137", "pdf": "https://arxiv.org/pdf/2504.05137", "abs": "https://arxiv.org/abs/2504.05137", "authors": ["Jinxiang Lai", "Wenlong Wu", "Jiawei Zhan", "Jian Li", "Bin-Bin Gao", "Jun Liu", "Jie Zhang", "Song Guo"], "title": "BoxSeg: Quality-Aware and Peer-Assisted Learning for Box-supervised Instance Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Box-supervised instance segmentation methods aim to achieve instance\nsegmentation with only box annotations. Recent methods have demonstrated the\neffectiveness of acquiring high-quality pseudo masks under the teacher-student\nframework. Building upon this foundation, we propose a BoxSeg framework\ninvolving two novel and general modules named the Quality-Aware Module (QAM)\nand the Peer-assisted Copy-paste (PC). The QAM obtains high-quality pseudo\nmasks and better measures the mask quality to help reduce the effect of noisy\nmasks, by leveraging the quality-aware multi-mask complementation mechanism.\nThe PC imitates Peer-Assisted Learning to further improve the quality of the\nlow-quality masks with the guidance of the obtained high-quality pseudo masks.\nTheoretical and experimental analyses demonstrate the proposed QAM and PC are\neffective. Extensive experimental results show the superiority of our BoxSeg\nover the state-of-the-art methods, and illustrate the QAM and PC can be applied\nto improve other models."}
{"id": "2504.05141", "pdf": "https://arxiv.org/pdf/2504.05141", "abs": "https://arxiv.org/abs/2504.05141", "authors": ["Bingyang Wang", "Kaer Huang", "Bin Li", "Yiqiang Yan", "Lihe Zhang", "Huchuan Lu", "You He"], "title": "EffOWT: Transfer Visual Language Models to Open-World Tracking Efficiently and Effectively", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 5 figures", "summary": "Open-World Tracking (OWT) aims to track every object of any category, which\nrequires the model to have strong generalization capabilities. Trackers can\nimprove their generalization ability by leveraging Visual Language Models\n(VLMs). However, challenges arise with the fine-tuning strategies when VLMs are\ntransferred to OWT: full fine-tuning results in excessive parameter and memory\ncosts, while the zero-shot strategy leads to sub-optimal performance. To solve\nthe problem, EffOWT is proposed for efficiently transferring VLMs to OWT.\nSpecifically, we build a small and independent learnable side network outside\nthe VLM backbone. By freezing the backbone and only executing backpropagation\non the side network, the model's efficiency requirements can be met. In\naddition, EffOWT enhances the side network by proposing a hybrid structure of\nTransformer and CNN to improve the model's performance in the OWT field.\nFinally, we implement sparse interactions on the MLP, thus reducing parameter\nupdates and memory costs significantly. Thanks to the proposed methods, EffOWT\nachieves an absolute gain of 5.5% on the tracking metric OWTA for unknown\ncategories, while only updating 1.3% of the parameters compared to full\nfine-tuning, with a 36.4% memory saving. Other metrics also demonstrate obvious\nimprovement."}
{"id": "2504.05148", "pdf": "https://arxiv.org/pdf/2504.05148", "abs": "https://arxiv.org/abs/2504.05148", "authors": ["Yasuhiro Yao", "Ryoichi Ishikawa", "Takeshi Oishi"], "title": "Stereo-LiDAR Fusion by Semi-Global Matching With Discrete Disparity-Matching Cost and Semidensification", "categories": ["cs.CV", "cs.RO"], "comment": "8 pages, 8 figures, 7 tables", "summary": "We present a real-time, non-learning depth estimation method that fuses Light\nDetection and Ranging (LiDAR) data with stereo camera input. Our approach\ncomprises three key techniques: Semi-Global Matching (SGM) stereo with Discrete\nDisparity-matching Cost (DDC), semidensification of LiDAR disparity, and a\nconsistency check that combines stereo images and LiDAR data. Each of these\ncomponents is designed for parallelization on a GPU to realize real-time\nperformance. When it was evaluated on the KITTI dataset, the proposed method\nachieved an error rate of 2.79\\%, outperforming the previous state-of-the-art\nreal-time stereo-LiDAR fusion method, which had an error rate of 3.05\\%.\nFurthermore, we tested the proposed method in various scenarios, including\ndifferent LiDAR point densities, varying weather conditions, and indoor\nenvironments, to demonstrate its high adaptability. We believe that the\nreal-time and non-learning nature of our method makes it highly practical for\napplications in robotics and automation."}
{"id": "2504.05152", "pdf": "https://arxiv.org/pdf/2504.05152", "abs": "https://arxiv.org/abs/2504.05152", "authors": ["Zhexiao Xiong", "Zhang Chen", "Zhong Li", "Yi Xu", "Nathan Jacobs"], "title": "PanoDreamer: Consistent Text to 360-Degree Scene Generation", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025 Workshop on Computer Vision for Metaverse", "summary": "Automatically generating a complete 3D scene from a text description, a\nreference image, or both has significant applications in fields like virtual\nreality and gaming. However, current methods often generate low-quality\ntextures and inconsistent 3D structures. This is especially true when\nextrapolating significantly beyond the field of view of the reference image. To\naddress these challenges, we propose PanoDreamer, a novel framework for\nconsistent, 3D scene generation with flexible text and image control. Our\napproach employs a large language model and a warp-refine pipeline, first\ngenerating an initial set of images and then compositing them into a 360-degree\npanorama. This panorama is then lifted into 3D to form an initial point cloud.\nWe then use several approaches to generate additional images, from different\nviewpoints, that are consistent with the initial point cloud and expand/refine\nthe initial point cloud. Given the resulting set of images, we utilize 3D\nGaussian Splatting to create the final 3D scene, which can then be rendered\nfrom different viewpoints. Experiments demonstrate the effectiveness of\nPanoDreamer in generating high-quality, geometrically consistent 3D scenes."}
{"id": "2504.05164", "pdf": "https://arxiv.org/pdf/2504.05164", "abs": "https://arxiv.org/abs/2504.05164", "authors": ["Xingyu Hu", "Junjun Jiang", "Chenyang Wang", "Kui Jiang", "Xianming Liu", "Jiayi Ma"], "title": "Balancing Task-invariant Interaction and Task-specific Adaptation for Unified Image Fusion", "categories": ["cs.CV"], "comment": null, "summary": "Unified image fusion aims to integrate complementary information from\nmulti-source images, enhancing image quality through a unified framework\napplicable to diverse fusion tasks. While treating all fusion tasks as a\nunified problem facilitates task-invariant knowledge sharing, it often\noverlooks task-specific characteristics, thereby limiting the overall\nperformance. Existing general image fusion methods incorporate explicit task\nidentification to enable adaptation to different fusion tasks. However, this\ndependence during inference restricts the model's generalization to unseen\nfusion tasks. To address these issues, we propose a novel unified image fusion\nframework named \"TITA\", which dynamically balances both Task-invariant\nInteraction and Task-specific Adaptation. For task-invariant interaction, we\nintroduce the Interaction-enhanced Pixel Attention (IPA) module to enhance\npixel-wise interactions for better multi-source complementary information\nextraction. For task-specific adaptation, the Operation-based Adaptive Fusion\n(OAF) module dynamically adjusts operation weights based on task properties.\nAdditionally, we incorporate the Fast Adaptive Multitask Optimization (FAMO)\nstrategy to mitigate the impact of gradient conflicts across tasks during joint\ntraining. Extensive experiments demonstrate that TITA not only achieves\ncompetitive performance compared to specialized methods across three image\nfusion scenarios but also exhibits strong generalization to unseen fusion\ntasks."}
{"id": "2504.05170", "pdf": "https://arxiv.org/pdf/2504.05170", "abs": "https://arxiv.org/abs/2504.05170", "authors": ["Bonan Ding", "Jin Xie", "Jing Nie", "Jiale Cao"], "title": "SSLFusion: Scale & Space Aligned Latent Fusion Model for Multimodal 3D Object Detection", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by AAAI 2025", "summary": "Multimodal 3D object detection based on deep neural networks has indeed made\nsignificant progress. However, it still faces challenges due to the\nmisalignment of scale and spatial information between features extracted from\n2D images and those derived from 3D point clouds. Existing methods usually\naggregate multimodal features at a single stage. However, leveraging\nmulti-stage cross-modal features is crucial for detecting objects of various\nscales. Therefore, these methods often struggle to integrate features across\ndifferent scales and modalities effectively, thereby restricting the accuracy\nof detection. Additionally, the time-consuming Query-Key-Value-based\n(QKV-based) cross-attention operations often utilized in existing methods aid\nin reasoning the location and existence of objects by capturing non-local\ncontexts. However, this approach tends to increase computational complexity. To\naddress these challenges, we present SSLFusion, a novel Scale & Space Aligned\nLatent Fusion Model, consisting of a scale-aligned fusion strategy (SAF), a\n3D-to-2D space alignment module (SAM), and a latent cross-modal fusion module\n(LFM). SAF mitigates scale misalignment between modalities by aggregating\nfeatures from both images and point clouds across multiple levels. SAM is\ndesigned to reduce the inter-modal gap between features from images and point\nclouds by incorporating 3D coordinate information into 2D image features.\nAdditionally, LFM captures cross-modal non-local contexts in the latent space\nwithout utilizing the QKV-based attention operations, thus mitigating\ncomputational complexity. Experiments on the KITTI and DENSE datasets\ndemonstrate that our SSLFusion outperforms state-of-the-art methods. Our\napproach obtains an absolute gain of 2.15% in 3D AP, compared with the\nstate-of-art method GraphAlign on the moderate level of the KITTI test set."}
{"id": "2504.05178", "pdf": "https://arxiv.org/pdf/2504.05178", "abs": "https://arxiv.org/abs/2504.05178", "authors": ["Hao Fang", "Runmin Cong", "Xiankai Lu", "Zhiyang Chen", "Wei Zhang"], "title": "The 1st Solution for 4th PVUW MeViS Challenge: Unleashing the Potential of Large Multimodal Models for Referring Video Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Motion expression video segmentation is designed to segment objects in\naccordance with the input motion expressions. In contrast to the conventional\nReferring Video Object Segmentation (RVOS), it places emphasis on motion as\nwell as multi-object expressions, making it more arduous. Recently, Large\nMultimodal Models (LMMs) have begun to shine in RVOS due to their powerful\nvision-language perception capabilities. In this work, we propose a simple and\neffective inference optimization method to fully unleash the potential of LMMs\nin referring video segmentation. Firstly, we use Sa2VA as our baseline, which\nis a unified LMM for dense grounded understanding of both images and videos.\nSecondly, we uniformly sample the video frames during the inference process to\nenhance the model's understanding of the entire video. Finally, we integrate\nthe results of multiple expert models to mitigate the erroneous predictions of\na single model. Our solution achieved 61.98% J&F on the MeViS test set and\nranked 1st place in the 4th PVUW Challenge MeViS Track at CVPR 2025."}
{"id": "2504.05184", "pdf": "https://arxiv.org/pdf/2504.05184", "abs": "https://arxiv.org/abs/2504.05184", "authors": ["Rayan Merghani Ahmed", "Adnan Iltaf", "Bin Li", "Shoujun Zhou"], "title": "MSA-UNet3+: Multi-Scale Attention UNet3+ with New Supervised Prototypical Contrastive Loss for Coronary DSA Image Segmentation", "categories": ["cs.CV"], "comment": "Work in progress", "summary": "The accurate segmentation of coronary Digital Subtraction Angiography (DSA)\nimages is essential for diagnosing and treating coronary artery diseases.\nDespite advances in deep learning-based segmentation, challenges such as low\ncontrast, noise, overlapping structures, high intra-class variance, and class\nimbalance limit precise vessel delineation. To overcome these limitations, we\npropose the MSA-UNet3+: a Multi-Scale Attention enhanced UNet3+ architecture\nfor coronary DSA image segmentation. The framework combined Multi-Scale Dilated\nBottleneck (MSD-Bottleneck) with Contextual Attention Fusion Module (CAFM),\nwhich not only enhances multi-scale feature extraction but also preserve\nfine-grained details, and improve contextual understanding. Furthermore, we\npropose a new Supervised Prototypical Contrastive Loss (SPCL), which combines\nsupervised and prototypical contrastive learning to minimize class imbalance\nand high intra-class variance by focusing on hard-to-classified background\nsamples. Experiments carried out on a private coronary DSA dataset demonstrate\nthat MSA-UNet3+ outperforms state-of-the-art methods, achieving a Dice\ncoefficient of 87.73%, an F1-score of 87.78%, and significantly reduced Average\nSurface Distance (ASD) and Average Contour Distance (ACD). The developed\nframework provides clinicians with precise vessel segmentation, enabling\naccurate identification of coronary stenosis and supporting informed diagnostic\nand therapeutic decisions. The code will be released at the following GitHub\nprofile link https://github.com/rayanmerghani/MSA-UNet3plus."}
{"id": "2504.05186", "pdf": "https://arxiv.org/pdf/2504.05186", "abs": "https://arxiv.org/abs/2504.05186", "authors": ["Mikhail Karasikov", "Joost van Doorn", "Nicolas K√§nzig", "Melis Erdal Cesur", "Hugo Mark Horlings", "Robert Berke", "Fei Tang", "Sebastian Ot√°lora"], "title": "Training state-of-the-art pathology foundation models with orders of magnitude less data", "categories": ["cs.CV", "cs.LG"], "comment": "10 pages, 3 figures", "summary": "The field of computational pathology has recently seen rapid advances driven\nby the development of modern vision foundation models (FMs), typically trained\non vast collections of pathology images. Recent studies demonstrate that\nincreasing the training data set and model size and integrating domain-specific\nimage processing techniques can significantly enhance the model's performance\non downstream tasks. Building on these insights, our work incorporates several\nrecent modifications to the standard DINOv2 framework from the literature to\noptimize the training of pathology FMs. We also apply a post-training procedure\nfor fine-tuning models on higher-resolution images to further enrich the\ninformation encoded in the embeddings. We present three novel pathology FMs\ntrained on up to two orders of magnitude fewer WSIs than those used to train\nother state-of-the-art FMs while demonstrating a comparable or superior\nperformance on downstream tasks. Even the model trained on TCGA alone (12k\nWSIs) outperforms most existing FMs and, on average, matches Virchow2, the\nsecond-best FM published to date. This suggests that there still remains a\nsignificant potential for further improving the models and algorithms used to\ntrain pathology FMs to take full advantage of the vast data collections."}
{"id": "2504.05201", "pdf": "https://arxiv.org/pdf/2504.05201", "abs": "https://arxiv.org/abs/2504.05201", "authors": ["Jared Frazier", "Tejas Sudharshan Mathai", "Jianfei Liu", "Angshuman Paul", "Ronald M. Summers"], "title": "3D Universal Lesion Detection and Tagging in CT with Self-Training", "categories": ["cs.CV", "cs.AI"], "comment": "Published at SPIE Medical Imaging 2023", "summary": "Radiologists routinely perform the tedious task of lesion localization,\nclassification, and size measurement in computed tomography (CT) studies.\nUniversal lesion detection and tagging (ULDT) can simultaneously help alleviate\nthe cumbersome nature of lesion measurement and enable tumor burden assessment.\nPrevious ULDT approaches utilize the publicly available DeepLesion dataset,\nhowever it does not provide the full volumetric (3D) extent of lesions and also\ndisplays a severe class imbalance. In this work, we propose a self-training\npipeline to detect 3D lesions and tag them according to the body part they\noccur in. We used a significantly limited 30\\% subset of DeepLesion to train a\nVFNet model for 2D lesion detection and tagging. Next, the 2D lesion context\nwas expanded into 3D, and the mined 3D lesion proposals were integrated back\ninto the baseline training data in order to retrain the model over multiple\nrounds. Through the self-training procedure, our VFNet model learned from its\nown predictions, detected lesions in 3D, and tagged them. Our results indicated\nthat our VFNet model achieved an average sensitivity of 46.9\\% at [0.125:8]\nfalse positives (FP) with a limited 30\\% data subset in comparison to the\n46.8\\% of an existing approach that used the entire DeepLesion dataset. To our\nknowledge, we are the first to jointly detect lesions in 3D and tag them\naccording to the body part label."}
{"id": "2504.05207", "pdf": "https://arxiv.org/pdf/2504.05207", "abs": "https://arxiv.org/abs/2504.05207", "authors": ["Alexander Shieh", "Tejas Sudharshan Mathai", "Jianfei Liu", "Angshuman Paul", "Ronald M. Summers"], "title": "Correcting Class Imbalances with Self-Training for Improved Universal Lesion Detection and Tagging", "categories": ["cs.CV", "cs.AI"], "comment": "Published at SPIE Medical Imaging 2023", "summary": "Universal lesion detection and tagging (ULDT) in CT studies is critical for\ntumor burden assessment and tracking the progression of lesion status\n(growth/shrinkage) over time. However, a lack of fully annotated data hinders\nthe development of effective ULDT approaches. Prior work used the DeepLesion\ndataset (4,427 patients, 10,594 studies, 32,120 CT slices, 32,735 lesions, 8\nbody part labels) for algorithmic development, but this dataset is not\ncompletely annotated and contains class imbalances. To address these issues, in\nthis work, we developed a self-training pipeline for ULDT. A VFNet model was\ntrained on a limited 11.5\\% subset of DeepLesion (bounding boxes + tags) to\ndetect and classify lesions in CT studies. Then, it identified and incorporated\nnovel lesion candidates from a larger unseen data subset into its training set,\nand self-trained itself over multiple rounds. Multiple self-training\nexperiments were conducted with different threshold policies to select\npredicted lesions with higher quality and cover the class imbalances. We\ndiscovered that direct self-training improved the sensitivities of\nover-represented lesion classes at the expense of under-represented classes.\nHowever, upsampling the lesions mined during self-training along with a\nvariable threshold policy yielded a 6.5\\% increase in sensitivity at 4 FP in\ncontrast to self-training without class balancing (72\\% vs 78.5\\%) and a 11.7\\%\nincrease compared to the same self-training policy without upsampling (66.8\\%\nvs 78.5\\%). Furthermore, we show that our results either improved or maintained\nthe sensitivity at 4FP for all 8 lesion classes."}
{"id": "2504.05219", "pdf": "https://arxiv.org/pdf/2504.05219", "abs": "https://arxiv.org/abs/2504.05219", "authors": ["Abdurrahim Yilmaz", "Serra Atilla Aydin", "Deniz Temur", "Furkan Yuceyalcin", "Berkin Deniz Kahya", "Rahmetullah Varol", "Ozay Gokoz", "Gulsum Gencoglan", "Huseyin Uvet", "Gonca Elcin"], "title": "An ensemble deep learning approach to detect tumors on Mohs micrographic surgery slides", "categories": ["cs.CV", "eess.IV"], "comment": "14 pages, 2 figures", "summary": "Mohs micrographic surgery (MMS) is the gold standard technique for removing\nhigh risk nonmelanoma skin cancer however, intraoperative histopathological\nexamination demands significant time, effort, and professionality. The\nobjective of this study is to develop a deep learning model to detect basal\ncell carcinoma (BCC) and artifacts on Mohs slides. A total of 731 Mohs slides\nfrom 51 patients with BCCs were used in this study, with 91 containing tumor\nand 640 without tumor which was defined as non-tumor. The dataset was employed\nto train U-Net based models that segment tumor and non-tumor regions on the\nslides. The segmented patches were classified as tumor, or non-tumor to produce\npredictions for whole slide images (WSIs). For the segmentation phase, the deep\nlearning model success was measured using a Dice score with 0.70 and 0.67\nvalue, area under the curve (AUC) score with 0.98 and 0.96 for tumor and\nnon-tumor, respectively. For the tumor classification, an AUC of 0.98 for\npatch-based detection, and AUC of 0.91 for slide-based detection was obtained\non the test dataset. We present an AI system that can detect tumors and\nnon-tumors in Mohs slides with high success. Deep learning can aid Mohs\nsurgeons and dermatopathologists in making more accurate decisions."}
{"id": "2504.05224", "pdf": "https://arxiv.org/pdf/2504.05224", "abs": "https://arxiv.org/abs/2504.05224", "authors": ["Zeqin Yu", "Jiangqun Ni", "Jian Zhang", "Haoyi Deng", "Yuzhen Lin"], "title": "Reinforced Multi-teacher Knowledge Distillation for Efficient General Image Forgery Detection and Localization", "categories": ["cs.CV"], "comment": "Published to AAAI2025 (Oral)", "summary": "Image forgery detection and localization (IFDL) is of vital importance as\nforged images can spread misinformation that poses potential threats to our\ndaily lives. However, previous methods still struggled to effectively handle\nforged images processed with diverse forgery operations in real-world\nscenarios. In this paper, we propose a novel Reinforced Multi-teacher Knowledge\nDistillation (Re-MTKD) framework for the IFDL task, structured around an\nencoder-decoder \\textbf{C}onvNeXt-\\textbf{U}perNet along with\n\\textbf{E}dge-Aware Module, named Cue-Net. First, three Cue-Net models are\nseparately trained for the three main types of image forgeries, i.e.,\ncopy-move, splicing, and inpainting, which then serve as the multi-teacher\nmodels to train the target student model with Cue-Net through self-knowledge\ndistillation. A Reinforced Dynamic Teacher Selection (Re-DTS) strategy is\ndeveloped to dynamically assign weights to the involved teacher models, which\nfacilitates specific knowledge transfer and enables the student model to\neffectively learn both the common and specific natures of diverse tampering\ntraces. Extensive experiments demonstrate that, compared with other\nstate-of-the-art methods, the proposed method achieves superior performance on\nseveral recently emerged datasets comprised of various kinds of image\nforgeries."}
{"id": "2504.05227", "pdf": "https://arxiv.org/pdf/2504.05227", "abs": "https://arxiv.org/abs/2504.05227", "authors": ["Julio Silva-Rodr√≠guez", "Jose Dolz", "Ismail Ben Ayed"], "title": "A Reality Check of Vision-Language Pre-training in Radiology: Have We Progressed Using Text?", "categories": ["cs.CV"], "comment": "IPMI 2025. Code and weights: https://github.com/jusiro/DLILP", "summary": "Vision-language pre-training has recently gained popularity as it allows\nlearning rich feature representations using large-scale data sources. This\nparadigm has quickly made its way into the medical image analysis community. In\nparticular, there is an impressive amount of recent literature developing\nvision-language models for radiology. However, the available medical datasets\nwith image-text supervision are scarce, and medical concepts are fine-grained,\ninvolving expert knowledge that existing vision-language models struggle to\nencode. In this paper, we propose to take a prudent step back from the\nliterature and revisit supervised, unimodal pre-training, using fine-grained\nlabels instead. We conduct an extensive comparison demonstrating that unimodal\npre-training is highly competitive and better suited to integrating\nheterogeneous data sources. Our results also question the potential of recent\nvision-language models for open-vocabulary generalization, which have been\nevaluated using optimistic experimental settings. Finally, we study novel\nalternatives to better integrate fine-grained labels and noisy text\nsupervision."}
{"id": "2504.05238", "pdf": "https://arxiv.org/pdf/2504.05238", "abs": "https://arxiv.org/abs/2504.05238", "authors": ["Zhekai Zhou", "Guibo Luo", "Mingzhi Chen", "Zhenyu Weng", "Yuesheng Zhu"], "title": "Federated Learning for Medical Image Classification: A Comprehensive Benchmark", "categories": ["cs.CV", "cs.DC"], "comment": null, "summary": "The federated learning paradigm is wellsuited for the field of medical image\nanalysis, as it can effectively cope with machine learning on isolated\nmulticenter data while protecting the privacy of participating parties.\nHowever, current research on optimization algorithms in federated learning\noften focuses on limited datasets and scenarios, primarily centered around\nnatural images, with insufficient comparative experiments in medical contexts.\nIn this work, we conduct a comprehensive evaluation of several state-of-the-art\nfederated learning algorithms in the context of medical imaging. We conduct a\nfair comparison of classification models trained using various federated\nlearning algorithms across multiple medical imaging datasets. Additionally, we\nevaluate system performance metrics, such as communication cost and\ncomputational efficiency, while considering different federated learning\narchitectures. Our findings show that medical imaging datasets pose substantial\nchallenges for current federated learning optimization algorithms. No single\nalgorithm consistently delivers optimal performance across all medical\nfederated learning scenarios, and many optimization algorithms may underperform\nwhen applied to these datasets. Our experiments provide a benchmark and\nguidance for future research and application of federated learning in medical\nimaging contexts. Furthermore, we propose an efficient and robust method that\ncombines generative techniques using denoising diffusion probabilistic models\nwith label smoothing to augment datasets, widely enhancing the performance of\nfederated learning on classification tasks across various medical imaging\ndatasets. Our code will be released on GitHub, offering a reliable and\ncomprehensive benchmark for future federated learning studies in medical\nimaging."}
{"id": "2504.05249", "pdf": "https://arxiv.org/pdf/2504.05249", "abs": "https://arxiv.org/abs/2504.05249", "authors": ["Wenzhao Tang", "Weihang Li", "Xiucheng Liang", "Olaf Wysocki", "Filip Biljecki", "Christoph Holst", "Boris Jutzi"], "title": "Texture2LoD3: Enabling LoD3 Building Reconstruction With Panoramic Images", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted for CVPRW '25", "summary": "Despite recent advancements in surface reconstruction, Level of Detail (LoD)\n3 building reconstruction remains an unresolved challenge. The main issue\npertains to the object-oriented modelling paradigm, which requires\ngeoreferencing, watertight geometry, facade semantics, and low-poly\nrepresentation -- Contrasting unstructured mesh-oriented models. In\nTexture2LoD3, we introduce a novel method leveraging the ubiquity of 3D\nbuilding model priors and panoramic street-level images, enabling the\nreconstruction of LoD3 building models. We observe that prior low-detail\nbuilding models can serve as valid planar targets for ortho-rectifying\nstreet-level panoramic images. Moreover, deploying segmentation on accurately\ntextured low-level building surfaces supports maintaining essential\ngeoreferencing, watertight geometry, and low-poly representation for LoD3\nreconstruction. In the absence of LoD3 validation data, we additionally\nintroduce the ReLoD3 dataset, on which we experimentally demonstrate that our\nmethod leads to improved facade segmentation accuracy by 11% and can replace\ncostly manual projections. We believe that Texture2LoD3 can scale the adoption\nof LoD3 models, opening applications in estimating building solar potential or\nenhancing autonomous driving simulations. The project website, code, and data\nare available here: https://wenzhaotang.github.io/Texture2LoD3/."}
{"id": "2504.05253", "pdf": "https://arxiv.org/pdf/2504.05253", "abs": "https://arxiv.org/abs/2504.05253", "authors": ["Ben Lonnqvist", "Elsa Scialom", "Abdulkadir Gokce", "Zehra Merchant", "Michael H. Herzog", "Martin Schrimpf"], "title": "Contour Integration Underlies Human-Like Vision", "categories": ["cs.CV"], "comment": null, "summary": "Despite the tremendous success of deep learning in computer vision, models\nstill fall behind humans in generalizing to new input distributions. Existing\nbenchmarks do not investigate the specific failure points of models by\nanalyzing performance under many controlled conditions. Our study\nsystematically dissects where and why models struggle with contour integration\n-- a hallmark of human vision -- by designing an experiment that tests object\nrecognition under various levels of object fragmentation. Humans (n=50) perform\nat high accuracy, even with few object contours present. This is in contrast to\nmodels which exhibit substantially lower sensitivity to increasing object\ncontours, with most of the over 1,000 models we tested barely performing above\nchance. Only at very large scales ($\\sim5B$ training dataset size) do models\nbegin to approach human performance. Importantly, humans exhibit an integration\nbias -- a preference towards recognizing objects made up of directional\nfragments over directionless fragments. We find that not only do models that\nshare this property perform better at our task, but that this bias also\nincreases with model training dataset size, and training models to exhibit\ncontour integration leads to high shape bias. Taken together, our results\nsuggest that contour integration is a hallmark of object vision that underlies\nobject recognition performance, and may be a mechanism learned from data at\nscale."}
{"id": "2504.05254", "pdf": "https://arxiv.org/pdf/2504.05254", "abs": "https://arxiv.org/abs/2504.05254", "authors": ["Sara Pohland", "Claire Tomlin"], "title": "Explaining Low Perception Model Competency with High-Competency Counterfactuals", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "There exist many methods to explain how an image classification model\ngenerates its decision, but very little work has explored methods to explain\nwhy a classifier might lack confidence in its prediction. As there are various\nreasons the classifier might lose confidence, it would be valuable for this\nmodel to not only indicate its level of uncertainty but also explain why it is\nuncertain. Counterfactual images have been used to visualize changes that could\nbe made to an image to generate a different classification decision. In this\nwork, we explore the use of counterfactuals to offer an explanation for low\nmodel competency--a generalized form of predictive uncertainty that measures\nconfidence. Toward this end, we develop five novel methods to generate\nhigh-competency counterfactual images, namely Image Gradient Descent (IGD),\nFeature Gradient Descent (FGD), Autoencoder Reconstruction (Reco), Latent\nGradient Descent (LGD), and Latent Nearest Neighbors (LNN). We evaluate these\nmethods across two unique datasets containing images with six known causes for\nlow model competency and find Reco, LGD, and LNN to be the most promising\nmethods for counterfactual generation. We further evaluate how these three\nmethods can be utilized by pre-trained Multimodal Large Language Models (MLLMs)\nto generate language explanations for low model competency. We find that the\ninclusion of a counterfactual image in the language model query greatly\nincreases the ability of the model to generate an accurate explanation for the\ncause of low model competency, thus demonstrating the utility of counterfactual\nimages in explaining low perception model competency."}
{"id": "2504.05265", "pdf": "https://arxiv.org/pdf/2504.05265", "abs": "https://arxiv.org/abs/2504.05265", "authors": ["German Barquero", "Nadine Bertsch", "Manojkumar Marramreddy", "Carlos Chac√≥n", "Filippo Arcadu", "Ferran Rigual", "Nicky Sijia He", "Cristina Palmero", "Sergio Escalera", "Yuting Ye", "Robin Kips"], "title": "From Sparse Signal to Smooth Motion: Real-Time Motion Generation with Rolling Prediction Models", "categories": ["cs.CV"], "comment": "Published in CVPR'25. Webpage: https://barquerogerman.github.io/RPM/", "summary": "In extended reality (XR), generating full-body motion of the users is\nimportant to understand their actions, drive their virtual avatars for social\ninteraction, and convey a realistic sense of presence. While prior works\nfocused on spatially sparse and always-on input signals from motion\ncontrollers, many XR applications opt for vision-based hand tracking for\nreduced user friction and better immersion. Compared to controllers, hand\ntracking signals are less accurate and can even be missing for an extended\nperiod of time. To handle such unreliable inputs, we present Rolling Prediction\nModel (RPM), an online and real-time approach that generates smooth full-body\nmotion from temporally and spatially sparse input signals. Our model generates\n1) accurate motion that matches the inputs (i.e., tracking mode) and 2)\nplausible motion when inputs are missing (i.e., synthesis mode). More\nimportantly, RPM generates seamless transitions from tracking to synthesis, and\nvice versa. To demonstrate the practical importance of handling noisy and\nmissing inputs, we present GORP, the first dataset of realistic sparse inputs\nfrom a commercial virtual reality (VR) headset with paired high quality body\nmotion ground truth. GORP provides >14 hours of VR gameplay data from 28 people\nusing motion controllers (spatially sparse) and hand tracking (spatially and\ntemporally sparse). We benchmark RPM against the state of the art on both\nsynthetic data and GORP to highlight how we can bridge the gap for real-world\napplications with a realistic dataset and by handling unreliable input signals.\nOur code, pretrained models, and GORP dataset are available in the project\nwebpage."}
{"id": "2504.05271", "pdf": "https://arxiv.org/pdf/2504.05271", "abs": "https://arxiv.org/abs/2504.05271", "authors": ["Yusef Ahsini", "Marc Escoto", "J. Alberto Conejero"], "title": "AnomalousNet: A Hybrid Approach with Attention U-Nets and Change Point Detection for Accurate Characterization of Anomalous Diffusion in Video Data", "categories": ["cs.CV", "cs.LG"], "comment": "20 pages, 9 figures", "summary": "Anomalous diffusion occurs in a wide range of systems, including protein\ntransport within cells, animal movement in complex habitats, pollutant\ndispersion in groundwater, and nanoparticle motion in synthetic materials.\nAccurately estimating the anomalous diffusion exponent and the diffusion\ncoefficient from the particle trajectories is essential to distinguish between\nsub-diffusive, super-diffusive, or normal diffusion regimes. These estimates\nprovide a deeper insight into the underlying dynamics of the system,\nfacilitating the identification of particle behaviors and the detection of\nchanges in diffusion states. However, analyzing short and noisy video data,\nwhich often yield incomplete and heterogeneous trajectories, poses a\nsignificant challenge for traditional statistical approaches. We introduce a\ndata-driven method that integrates particle tracking, an attention\n  U-Net architecture, and a change-point detection algorithm to address these\nissues. This approach not only infers the anomalous diffusion parameters with\nhigh accuracy but also identifies temporal transitions between different\nstates, even in the presence of noise and limited temporal resolution. Our\nmethodology demonstrated strong performance in the 2nd Anomalous Diffusion\n(AnDi) Challenge benchmark within the top submissions for video tasks."}
{"id": "2504.05288", "pdf": "https://arxiv.org/pdf/2504.05288", "abs": "https://arxiv.org/abs/2504.05288", "authors": ["Mingyang Fu", "Yuyang Peng", "Benlin Liu", "Yao Wan", "Dongping Chen"], "title": "LiveVQA: Live Visual Knowledge Seeking", "categories": ["cs.CV", "cs.CL"], "comment": "Work in progress", "summary": "We introduce LiveVQA, an automatically collected dataset of latest visual\nknowledge from the Internet with synthesized VQA problems. LiveVQA consists of\n3,602 single- and multi-hop visual questions from 6 news websites across 14\nnews categories, featuring high-quality image-text coherence and authentic\ninformation. Our evaluation across 15 MLLMs (e.g., GPT-4o, Gemma-3, and\nQwen-2.5-VL family) demonstrates that stronger models perform better overall,\nwith advanced visual reasoning capabilities proving crucial for complex\nmulti-hop questions. Despite excellent performance on textual problems, models\nwith tools like search engines still show significant gaps when addressing\nvisual questions requiring latest visual knowledge, highlighting important\nareas for future research."}
{"id": "2504.05298", "pdf": "https://arxiv.org/pdf/2504.05298", "abs": "https://arxiv.org/abs/2504.05298", "authors": ["Karan Dalal", "Daniel Koceja", "Gashon Hussein", "Jiarui Xu", "Yue Zhao", "Youjin Song", "Shihao Han", "Ka Chun Cheung", "Jan Kautz", "Carlos Guestrin", "Tatsunori Hashimoto", "Sanmi Koyejo", "Yejin Choi", "Yu Sun", "Xiaolong Wang"], "title": "One-Minute Video Generation with Test-Time Training", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Transformers today still struggle to generate one-minute videos because\nself-attention layers are inefficient for long context. Alternatives such as\nMamba layers struggle with complex multi-scene stories because their hidden\nstates are less expressive. We experiment with Test-Time Training (TTT) layers,\nwhose hidden states themselves can be neural networks, therefore more\nexpressive. Adding TTT layers into a pre-trained Transformer enables it to\ngenerate one-minute videos from text storyboards. For proof of concept, we\ncurate a dataset based on Tom and Jerry cartoons. Compared to baselines such as\nMamba~2, Gated DeltaNet, and sliding-window attention layers, TTT layers\ngenerate much more coherent videos that tell complex stories, leading by 34 Elo\npoints in a human evaluation of 100 videos per method. Although promising,\nresults still contain artifacts, likely due to the limited capability of the\npre-trained 5B model. The efficiency of our implementation can also be\nimproved. We have only experimented with one-minute videos due to resource\nconstraints, but the approach can be extended to longer videos and more complex\nstories. Sample videos, code and annotations are available at:\nhttps://test-time-training.github.io/video-dit"}
{"id": "2504.05301", "pdf": "https://arxiv.org/pdf/2504.05301", "abs": "https://arxiv.org/abs/2504.05301", "authors": ["Heeji Yoon", "Heeseong Shin", "Eunbeen Hong", "Hyunwook Choi", "Hansang Cho", "Daun Jeong", "Seungryong Kim"], "title": "S^4M: Boosting Semi-Supervised Instance Segmentation with SAM", "categories": ["cs.CV"], "comment": null, "summary": "Semi-supervised instance segmentation poses challenges due to limited labeled\ndata, causing difficulties in accurately localizing distinct object instances.\nCurrent teacher-student frameworks still suffer from performance constraints\ndue to unreliable pseudo-label quality stemming from limited labeled data.\nWhile the Segment Anything Model (SAM) offers robust segmentation capabilities\nat various granularities, directly applying SAM to this task introduces\nchallenges such as class-agnostic predictions and potential over-segmentation.\nTo address these complexities, we carefully integrate SAM into the\nsemi-supervised instance segmentation framework, developing a novel\ndistillation method that effectively captures the precise localization\ncapabilities of SAM without compromising semantic recognition. Furthermore, we\nincorporate pseudo-label refinement as well as a specialized data augmentation\nwith the refined pseudo-labels, resulting in superior performance. We establish\nstate-of-the-art performance, and provide comprehensive experiments and\nablation studies to validate the effectiveness of our proposed approach."}
{"id": "2504.05303", "pdf": "https://arxiv.org/pdf/2504.05303", "abs": "https://arxiv.org/abs/2504.05303", "authors": ["Sai Kumar Dwivedi", "Dimitrije Antiƒá", "Shashank Tripathi", "Omid Taheri", "Cordelia Schmid", "Michael J. Black", "Dimitrios Tzionas"], "title": "InteractVLM: 3D Interaction Reasoning from 2D Foundational Models", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "We introduce InteractVLM, a novel method to estimate 3D contact points on\nhuman bodies and objects from single in-the-wild images, enabling accurate\nhuman-object joint reconstruction in 3D. This is challenging due to occlusions,\ndepth ambiguities, and widely varying object shapes. Existing methods rely on\n3D contact annotations collected via expensive motion-capture systems or\ntedious manual labeling, limiting scalability and generalization. To overcome\nthis, InteractVLM harnesses the broad visual knowledge of large Vision-Language\nModels (VLMs), fine-tuned with limited 3D contact data. However, directly\napplying these models is non-trivial, as they reason only in 2D, while\nhuman-object contact is inherently 3D. Thus we introduce a novel\nRender-Localize-Lift module that: (1) embeds 3D body and object surfaces in 2D\nspace via multi-view rendering, (2) trains a novel multi-view localization\nmodel (MV-Loc) to infer contacts in 2D, and (3) lifts these to 3D.\nAdditionally, we propose a new task called Semantic Human Contact estimation,\nwhere human contact predictions are conditioned explicitly on object semantics,\nenabling richer interaction modeling. InteractVLM outperforms existing work on\ncontact estimation and also facilitates 3D reconstruction from an in-the wild\nimage. Code and models are available at https://interactvlm.is.tue.mpg.de."}
{"id": "2504.05305", "pdf": "https://arxiv.org/pdf/2504.05305", "abs": "https://arxiv.org/abs/2504.05305", "authors": ["Sangbeom Lim", "Junwan Kim", "Heeji Yoon", "Jaewoo Jung", "Seungryong Kim"], "title": "URECA: Unique Region Caption Anything", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://cvlab-kaist.github.io/URECA Code:\n  https://github.com/cvlab-kaist/URECA", "summary": "Region-level captioning aims to generate natural language descriptions for\nspecific image regions while highlighting their distinguishing features.\nHowever, existing methods struggle to produce unique captions across\nmulti-granularity, limiting their real-world applicability. To address the need\nfor detailed region-level understanding, we introduce URECA dataset, a\nlarge-scale dataset tailored for multi-granularity region captioning. Unlike\nprior datasets that focus primarily on salient objects, URECA dataset ensures a\nunique and consistent mapping between regions and captions by incorporating a\ndiverse set of objects, parts, and background elements. Central to this is a\nstage-wise data curation pipeline, where each stage incrementally refines\nregion selection and caption generation. By leveraging Multimodal Large\nLanguage Models (MLLMs) at each stage, our pipeline produces distinctive and\ncontextually grounded captions with improved accuracy and semantic diversity.\nBuilding upon this dataset, we present URECA, a novel captioning model designed\nto effectively encode multi-granularity regions. URECA maintains essential\nspatial properties such as position and shape through simple yet impactful\nmodifications to existing MLLMs, enabling fine-grained and semantically rich\nregion descriptions. Our approach introduces dynamic mask modeling and a\nhigh-resolution mask encoder to enhance caption uniqueness. Experiments show\nthat URECA achieves state-of-the-art performance on URECA dataset and\ngeneralizes well to existing region-level captioning benchmarks."}
{"id": "2504.05306", "pdf": "https://arxiv.org/pdf/2504.05306", "abs": "https://arxiv.org/abs/2504.05306", "authors": ["Kavana Venkatesh", "Connor Dunlop", "Pinar Yanardag"], "title": "CREA: A Collaborative Multi-Agent Framework for Creative Content Generation with Diffusion Models", "categories": ["cs.CV"], "comment": "Project URL: https://crea-diffusion.github.io", "summary": "Creativity in AI imagery remains a fundamental challenge, requiring not only\nthe generation of visually compelling content but also the capacity to add\nnovel, expressive, and artistically rich transformations to images. Unlike\nconventional editing tasks that rely on direct prompt-based modifications,\ncreative image editing demands an autonomous, iterative approach that balances\noriginality, coherence, and artistic intent. To address this, we introduce\nCREA, a novel multi-agent collaborative framework that mimics the human\ncreative process. Our framework leverages a team of specialized AI agents who\ndynamically collaborate to conceptualize, generate, critique, and enhance\nimages. Through extensive qualitative and quantitative evaluations, we\ndemonstrate that CREA significantly outperforms state-of-the-art methods in\ndiversity, semantic alignment, and creative transformation. By structuring\ncreativity as a dynamic, agentic process, CREA redefines the intersection of AI\nand art, paving the way for autonomous AI-driven artistic exploration,\ngenerative design, and human-AI co-creation. To the best of our knowledge, this\nis the first work to introduce the task of creative editing."}
{"id": "2504.03654", "pdf": "https://arxiv.org/pdf/2504.03654", "abs": "https://arxiv.org/abs/2504.03654", "authors": ["Keondo Park", "You Rim Choi", "Inhoe Lee", "Hyung-Sin Kim"], "title": "PointSplit: Towards On-device 3D Object Detection with Heterogeneous Low-power Accelerators", "categories": ["cs.DC", "cs.AI", "cs.CV"], "comment": null, "summary": "Running deep learning models on resource-constrained edge devices has drawn\nsignificant attention due to its fast response, privacy preservation, and\nrobust operation regardless of Internet connectivity. While these devices\nalready cope with various intelligent tasks, the latest edge devices that are\nequipped with multiple types of low-power accelerators (i.e., both mobile GPU\nand NPU) can bring another opportunity; a task that used to be too heavy for an\nedge device in the single-accelerator world might become viable in the upcoming\nheterogeneous-accelerator world.To realize the potential in the context of 3D\nobject detection, we identify several technical challenges and propose\nPointSplit, a novel 3D object detection framework for multi-accelerator edge\ndevices that addresses the problems. Specifically, our PointSplit design\nincludes (1) 2D semantics-aware biased point sampling, (2) parallelized 3D\nfeature extraction, and (3) role-based group-wise quantization. We implement\nPointSplit on TensorFlow Lite and evaluate it on a customized hardware platform\ncomprising both mobile GPU and EdgeTPU. Experimental results on representative\nRGB-D datasets, SUN RGB-D and Scannet V2, demonstrate that PointSplit on a\nmulti-accelerator device is 24.7 times faster with similar accuracy compared to\nthe full-precision, 2D-3D fusion-based 3D detector on a GPU-only device."}
{"id": "2504.03687", "pdf": "https://arxiv.org/pdf/2504.03687", "abs": "https://arxiv.org/abs/2504.03687", "authors": ["Hanyu Liu", "Ying Yu", "Hang Xiao", "Siyao Li", "Xuze Li", "Jiarui Li", "Haotian Tang"], "title": "Process Optimization and Deployment for Sensor-Based Human Activity Recognition Based on Deep Learning", "categories": ["eess.SP", "cs.AI", "cs.CV"], "comment": null, "summary": "Sensor-based human activity recognition is a key technology for many\nhuman-centered intelligent applications. However, this research is still in its\ninfancy and faces many unresolved challenges. To address these, we propose a\ncomprehensive optimization process approach centered on multi-attention\ninteraction. We first utilize unsupervised statistical feature-guided diffusion\nmodels for highly adaptive data enhancement, and introduce a novel network\narchitecture-Multi-branch Spatiotemporal Interaction Network, which uses\nmulti-branch features at different levels to effectively Sequential ), which\nuses multi-branch features at different levels to effectively Sequential\nspatio-temporal interaction to enhance the ability to mine advanced latent\nfeatures. In addition, we adopt a multi-loss function fusion strategy in the\ntraining phase to dynamically adjust the fusion weights between batches to\noptimize the training results. Finally, we also conducted actual deployment on\nembedded devices to extensively test the practical feasibility of the proposed\nmethod in existing work. We conduct extensive testing on three public datasets,\nincluding ablation studies, comparisons of related work, and embedded\ndeployments."}
{"id": "2504.03703", "pdf": "https://arxiv.org/pdf/2504.03703", "abs": "https://arxiv.org/abs/2504.03703", "authors": ["Mario Padilla Rodriguez", "Mohamed Nafea"], "title": "Hierarchical Attention Network for Interpretable ECG-based Heart Disease Classification", "categories": ["eess.SP", "cs.CV", "cs.LG"], "comment": "Work in progress. 7 pages, 4 figures", "summary": "Cardiovascular disease remains one of the leading causes of mortality\nworldwide, underscoring the need for accurate as well as interpretable\ndiagnostic machine learning tools. In this work, we investigate heart disease\nclassification using electrocardiogram (ECG) data from two widely-utilized\ndatasets: The MIT-BIH Arrhythmia and the PTB-XL datasets. We adapt a\nhierarchical attention network (HAN), originally developed for text\nclassification, into an ECG-based heart-disease classification task. Our\nadapted HAN incorporates two attention layers that focus on ECG data segments\nof varying sizes. We conduct a comparative analysis between our adapted HAN and\na more sophisticated state-of-the-art architecture, featuring a network with\nconvolution, attention, and transformer layers (CAT-Net). Our empirical\nevaluation encompasses multiple aspects including test accuracy (quantified by\n0-1 loss); model complexity (measured by the number of model parameters); and\ninterpretability (through attention map visualization). Our adapted HAN\ndemonstrates comparable test accuracy with significant reductions in model\ncomplexity and enhanced interpretability analysis: For the MIT-BIH dataset, our\nadapted HAN achieves 98.55\\% test accuracy compared to 99.14\\% for CAT-Net,\nwhile reducing the number of model parameters by a factor of 15.6. For the\nPTB-XL dataset, our adapted HAN achieves a 19.3-fold reduction in model\ncomplexity compared to CAT-Net, with only a 5\\% lower test accuracy. From an\ninterpretability perspective, the significantly simpler architecture and the\nhierarchical nature of our adapted HAN model facilitate a more straightforward\ninterpretability analysis based on visualizing attention weights. Building on\nthis advantage, we conduct an interpretability analysis of our HAN that\nhighlights the regions of the ECG signal most relevant to the model's\ndecisions."}
{"id": "2504.03736", "pdf": "https://arxiv.org/pdf/2504.03736", "abs": "https://arxiv.org/abs/2504.03736", "authors": ["Teodor Chiaburu", "Felix Bie√ümann", "Frank Hau√üer"], "title": "Uncertainty Propagation in XAI: A Comparison of Analytical and Empirical Estimators", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "23 pages, 10 figures, accepted at WCXAI 2025 Istanbul", "summary": "Understanding uncertainty in Explainable AI (XAI) is crucial for building\ntrust and ensuring reliable decision-making in Machine Learning models. This\npaper introduces a unified framework for quantifying and interpreting\nUncertainty in XAI by defining a general explanation function $e_{\\theta}(x,\nf)$ that captures the propagation of uncertainty from key sources:\nperturbations in input data and model parameters. By using both analytical and\nempirical estimates of explanation variance, we provide a systematic means of\nassessing the impact uncertainty on explanations. We illustrate the approach\nusing a first-order uncertainty propagation as the analytical estimator. In a\ncomprehensive evaluation across heterogeneous datasets, we compare analytical\nand empirical estimates of uncertainty propagation and evaluate their\nrobustness. Extending previous work on inconsistencies in explanations, our\nexperiments identify XAI methods that do not reliably capture and propagate\nuncertainty. Our findings underscore the importance of uncertainty-aware\nexplanations in high-stakes applications and offer new insights into the\nlimitations of current XAI methods. The code for the experiments can be found\nin our repository at https://github.com/TeodorChiaburu/UXAI"}
{"id": "2504.03738", "pdf": "https://arxiv.org/pdf/2504.03738", "abs": "https://arxiv.org/abs/2504.03738", "authors": ["Litao Hua", "Fan Liu", "Jie Su", "Xingyu Miao", "Zizhou Ouyang", "Zeyu Wang", "Runze Hu", "Zhenyu Wen", "Bing Zhai", "Yang Long", "Haoran Duan", "Yuan Zhou"], "title": "Attention in Diffusion Model: A Survey", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Attention mechanisms have become a foundational component in diffusion\nmodels, significantly influencing their capacity across a wide range of\ngenerative and discriminative tasks. This paper presents a comprehensive survey\nof attention within diffusion models, systematically analysing its roles,\ndesign patterns, and operations across different modalities and tasks. We\npropose a unified taxonomy that categorises attention-related modifications\ninto parts according to the structural components they affect, offering a clear\nlens through which to understand their functional diversity. In addition to\nreviewing architectural innovations, we examine how attention mechanisms\ncontribute to performance improvements in diverse applications. We also\nidentify current limitations and underexplored areas, and outline potential\ndirections for future research. Our study provides valuable insights into the\nevolving landscape of diffusion models, with a particular focus on the\nintegrative and ubiquitous role of attention."}
{"id": "2504.03756", "pdf": "https://arxiv.org/pdf/2504.03756", "abs": "https://arxiv.org/abs/2504.03756", "authors": ["Yu-Lin Kuo", "Yu-Chee Tseng", "Ting-Hui Chiang", "Yan-Ann Chen"], "title": "Semi-Self Representation Learning for Crowdsourced WiFi Trajectories", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted by VTC2025-Spring", "summary": "WiFi fingerprint-based localization has been studied intensively. Point-based\nsolutions rely on position annotations of WiFi fingerprints. Trajectory-based\nsolutions, however, require end-position annotations of WiFi trajectories,\nwhere a WiFi trajectory is a multivariate time series of signal features. A\ntrajectory dataset is much larger than a pointwise dataset as the number of\npotential trajectories in a field may grow exponentially with respect to the\nsize of the field. This work presents a semi-self representation learning\nsolution, where a large dataset $C$ of crowdsourced unlabeled WiFi trajectories\ncan be automatically labeled by a much smaller dataset $\\tilde C$ of labeled\nWiFi trajectories. The size of $\\tilde C$ only needs to be proportional to the\nsize of the physical field, while the unlabeled $C$ could be much larger. This\nis made possible through a novel ``cut-and-flip'' augmentation scheme based on\nthe meet-in-the-middle paradigm. A two-stage learning consisting of trajectory\nembedding followed by endpoint embedding is proposed for the unlabeled $C$.\nThen the learned representations are labeled by $\\tilde C$ and connected to a\nneural-based localization network. The result, while delivering promising\naccuracy, significantly relieves the burden of human annotations for\ntrajectory-based localization."}
{"id": "2504.03758", "pdf": "https://arxiv.org/pdf/2504.03758", "abs": "https://arxiv.org/abs/2504.03758", "authors": ["Xuanwen Liang", "Jiayu Chen", "Eric Wai Ming Lee", "Wei Xie"], "title": "Improved visual-information-driven model for crowd simulation and its modular application", "categories": ["cs.CY", "cs.CV", "cs.GR"], "comment": null, "summary": "Data-driven crowd simulation models offer advantages in enhancing the\naccuracy and realism of simulations, and improving their generalizability is\nessential for promoting application. Current data-driven approaches are\nprimarily designed for a single scenario, with very few models validated across\nmore than two scenarios. It is still an open question to develop data-driven\ncrowd simulation models with strong generalizibility. We notice that the key to\naddressing this challenge lies in effectively and accurately capturing the core\ncommon influential features that govern pedestrians' navigation across diverse\nscenarios. Particularly, we believe that visual information is one of the most\ndominant influencing features. In light of this, this paper proposes a\ndata-driven model incorporating a refined visual information extraction method\nand exit cues to enhance generalizability. The proposed model is examined on\nfour common fundamental modules: bottleneck, corridor, corner and T-junction.\nThe evaluation results demonstrate that our model performs excellently across\nthese scenarios, aligning with pedestrian movement in real-world experiments,\nand significantly outperforms the classical knowledge-driven model.\nFurthermore, we introduce a modular approach to apply our proposed model in\ncomposite scenarios, and the results regarding trajectories and fundamental\ndiagrams indicate that our simulations closely match real-world patterns in the\ncomposite scenario. The research outcomes can provide inspiration for the\ndevelopment of data-driven crowd simulation models with high generalizability\nand advance the application of data-driven approaches."}
{"id": "2504.03782", "pdf": "https://arxiv.org/pdf/2504.03782", "abs": "https://arxiv.org/abs/2504.03782", "authors": ["Ramin Zarei Sabzevar", "Hamed Mohammadzadeh", "Tahmineh Tavakoli", "Ahad Harati"], "title": "A Study on Adversarial Robustness of Discriminative Prototypical Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Deep neural networks demonstrate significant vulnerability to adversarial\nperturbations, posing risks for critical applications. Current adversarial\ntraining methods predominantly focus on robustness against attacks without\nexplicitly leveraging geometric structures in the latent space, usually\nresulting in reduced accuracy on the original clean data. To address these\nissues, we propose a novel adversarial training framework named Adversarial\nDeep Positive-Negative Prototypes (Adv-DPNP), which integrates disriminative\nprototype-based learning with adversarial training. Adv-DPNP uses unified class\nprototypes serving dual roles as classifier weights and robust anchors,\nenhancing both intra-class compactness and inter-class separation in the latent\nspace. Moreover, a novel dual-branch training mechanism maintains stable\nprototypes by updating them exclusively with clean data; while the feature\nextractor layers are learned using both clean and adversarial data to remain\ninvariant against adversarial perturbations. In addition, our approach utilizes\na composite loss function combining positive prototype alignment, negative\nprototype repulsion, and consistency regularization to further enhance\ndiscrimination, adversarial robustness, and clean accuracy. Extensive\nexperiments conducted on standard benchmark datasets confirm the effectiveness\nof Adv-DPNP compared to state-of-the-art methods, achieving higher clean\naccuracy and competitive robustness under adversarial perturbations and common\ncorruptions. Our code is available at https://github.com/fum-rpl/adv-dpnp"}
{"id": "2504.03783", "pdf": "https://arxiv.org/pdf/2504.03783", "abs": "https://arxiv.org/abs/2504.03783", "authors": ["Haoyuan Li", "Jindong Wang", "Mathias Funk", "Aaqib Saeed"], "title": "FAST: Federated Active Learning with Foundation Models for Communication-efficient Sampling and Training", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DC"], "comment": null, "summary": "Federated Active Learning (FAL) has emerged as a promising framework to\nleverage large quantities of unlabeled data across distributed clients while\npreserving data privacy. However, real-world deployments remain limited by high\nannotation costs and communication-intensive sampling processes, particularly\nin a cross-silo setting, when clients possess substantial local datasets. This\npaper addresses the crucial question: What is the best practice to reduce\ncommunication costs in human-in-the-loop learning with minimal annotator\neffort? Existing FAL methods typically rely on iterative annotation processes\nthat separate active sampling from federated updates, leading to multiple\nrounds of expensive communication and annotation. In response, we introduce\nFAST, a two-pass FAL framework that harnesses foundation models for weak\nlabeling in a preliminary pass, followed by a refinement pass focused\nexclusively on the most uncertain samples. By leveraging representation\nknowledge from foundation models and integrating refinement steps into a\nstreamlined workflow, FAST substantially reduces the overhead incurred by\niterative active sampling. Extensive experiments on diverse medical and natural\nimage benchmarks demonstrate that FAST outperforms existing FAL methods by an\naverage of 4.36% while reducing communication rounds eightfold under a limited\n5% labeling budget."}
{"id": "2504.04000", "pdf": "https://arxiv.org/pdf/2504.04000", "abs": "https://arxiv.org/abs/2504.04000", "authors": ["James Noeckel", "Benjamin Jones", "Adriana Schulz", "Brian Curless"], "title": "View2CAD: Reconstructing View-Centric CAD Models from Single RGB-D Scans", "categories": ["cs.GR", "cs.CV", "I.3.5"], "comment": null, "summary": "Parametric CAD models, represented as Boundary Representations (B-reps), are\nfoundational to modern design and manufacturing workflows, offering the\nprecision and topological breakdown required for downstream tasks such as\nanalysis, editing, and fabrication. However, B-Reps are often inaccessible due\nto conversion to more standardized, less expressive geometry formats. Existing\nmethods to recover B-Reps from measured data require complete, noise-free 3D\ndata, which are laborious to obtain. We alleviate this difficulty by enabling\nthe precise reconstruction of CAD shapes from a single RGB-D image. We propose\na method that addresses the challenge of reconstructing only the observed\ngeometry from a single view. To allow for these partial observations, and to\navoid hallucinating incorrect geometry, we introduce a novel view-centric B-rep\n(VB-Rep) representation, which incorporates structures to handle visibility\nlimits and encode geometric uncertainty. We combine panoptic image segmentation\nwith iterative geometric optimization to refine and improve the reconstruction\nprocess. Our results demonstrate high-quality reconstruction on synthetic and\nreal RGB-D data, showing that our method can bridge the reality gap."}
{"id": "2504.04066", "pdf": "https://arxiv.org/pdf/2504.04066", "abs": "https://arxiv.org/abs/2504.04066", "authors": ["Mengyuan Liu", "Yixiao Chen", "Anning Tian", "Xinmeng Wu", "Mozhi Shen", "Tianchou Gong", "Jeongkyu Lee"], "title": "Performance Analysis of Deep Learning Models for Femur Segmentation in MRI Scan", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Convolutional neural networks like U-Net excel in medical image segmentation,\nwhile attention mechanisms and KAN enhance feature extraction. Meta's SAM 2\nuses Vision Transformers for prompt-based segmentation without fine-tuning.\nHowever, biases in these models impact generalization with limited data. In\nthis study, we systematically evaluate and compare the performance of three\nCNN-based models, i.e., U-Net, Attention U-Net, and U-KAN, and one\ntransformer-based model, i.e., SAM 2 for segmenting femur bone structures in\nMRI scan. The dataset comprises 11,164 MRI scans with detailed annotations of\nfemoral regions. Performance is assessed using the Dice Similarity Coefficient,\nwhich ranges from 0.932 to 0.954. Attention U-Net achieves the highest overall\nscores, while U-KAN demonstrated superior performance in anatomical regions\nwith a smaller region of interest, leveraging its enhanced learning capacity to\nimprove segmentation accuracy."}
{"id": "2504.04115", "pdf": "https://arxiv.org/pdf/2504.04115", "abs": "https://arxiv.org/abs/2504.04115", "authors": ["Yongchuan Cui", "Jinhe Zhang", "Peng Liu", "Weijing Song", "Yi Zeng"], "title": "Overcoming the Identity Mapping Problem in Self-Supervised Hyperspectral Anomaly Detection", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "The surge of deep learning has catalyzed considerable progress in\nself-supervised Hyperspectral Anomaly Detection (HAD). The core premise for\nself-supervised HAD is that anomalous pixels are inherently more challenging to\nreconstruct, resulting in larger errors compared to the background. However,\nowing to the powerful nonlinear fitting capabilities of neural networks,\nself-supervised models often suffer from the Identity Mapping Problem (IMP).\nThe IMP manifests as a tendency for the model to overfit to the entire image,\nparticularly with increasing network complexity or prolonged training\niterations. Consequently, the whole image can be precisely reconstructed, and\neven the anomalous pixels exhibit imperceptible errors, making them difficult\nto detect. Despite the proposal of several models aimed at addressing the\nIMP-related issues, a unified descriptive framework and validation of solutions\nfor IMP remain lacking. In this paper, we conduct an in-depth exploration to\nIMP, and summarize a unified framework that describes IMP from the perspective\nof network optimization, which encompasses three aspects: perturbation,\nreconstruction, and regularization. Correspondingly, we introduce three\nsolutions: superpixel pooling and uppooling for perturbation, error-adaptive\nconvolution for reconstruction, and online background pixel mining for\nregularization. With extensive experiments being conducted to validate the\neffectiveness, it is hoped that our work will provide valuable insights and\ninspire further research for self-supervised HAD. Code:\n\\url{https://github.com/yc-cui/Super-AD}."}
{"id": "2504.04153", "pdf": "https://arxiv.org/pdf/2504.04153", "abs": "https://arxiv.org/abs/2504.04153", "authors": ["Yikai Wang", "Guangce Liu", "Xinzhou Wang", "Zilong Chen", "Jiafang Li", "Xin Liang", "Fuchun Sun", "Jun Zhu"], "title": "Video4DGen: Enhancing Video and 4D Generation through Mutual Optimization", "categories": ["cs.GR", "cs.CV"], "comment": "Published in TPAMI 2025. Code: https://github.com/yikaiw/Vidu4D,\n  Project page: https://video4dgen.github.io", "summary": "The advancement of 4D (i.e., sequential 3D) generation opens up new\npossibilities for lifelike experiences in various applications, where users can\nexplore dynamic objects or characters from any viewpoint. Meanwhile, video\ngenerative models are receiving particular attention given their ability to\nproduce realistic and imaginative frames. These models are also observed to\nexhibit strong 3D consistency, indicating the potential to act as world\nsimulators. In this work, we present Video4DGen, a novel framework that excels\nin generating 4D representations from single or multiple generated videos as\nwell as generating 4D-guided videos. This framework is pivotal for creating\nhigh-fidelity virtual contents that maintain both spatial and temporal\ncoherence. The 4D outputs generated by Video4DGen are represented using our\nproposed Dynamic Gaussian Surfels (DGS), which optimizes time-varying warping\nfunctions to transform Gaussian surfels (surface elements) from a static state\nto a dynamically warped state. We design warped-state geometric regularization\nand refinements on Gaussian surfels, to preserve the structural integrity and\nfine-grained appearance details. To perform 4D generation from multiple videos\nand capture representation across spatial, temporal, and pose dimensions, we\ndesign multi-video alignment, root pose optimization, and pose-guided frame\nsampling strategies. The leveraging of continuous warping fields also enables a\nprecise depiction of pose, motion, and deformation over per-video frames.\nFurther, to improve the overall fidelity from the observation of all camera\nposes, Video4DGen performs novel-view video generation guided by the 4D\ncontent, with the proposed confidence-filtered DGS to enhance the quality of\ngenerated sequences. With the ability of 4D and video generation, Video4DGen\noffers a powerful tool for applications in virtual reality, animation, and\nbeyond."}
{"id": "2504.04170", "pdf": "https://arxiv.org/pdf/2504.04170", "abs": "https://arxiv.org/abs/2504.04170", "authors": ["Jianhua Sun", "Cewu Lu"], "title": "Learning about the Physical World through Analytic Concepts", "categories": ["cs.AI", "cs.CV", "cs.RO"], "comment": null, "summary": "Reviewing the progress in artificial intelligence over the past decade,\nvarious significant advances (e.g. object detection, image generation, large\nlanguage models) have enabled AI systems to produce more semantically\nmeaningful outputs and achieve widespread adoption in internet scenarios.\nNevertheless, AI systems still struggle when it comes to understanding and\ninteracting with the physical world. This reveals an important issue: relying\nsolely on semantic-level concepts learned from internet data (e.g. texts,\nimages) to understand the physical world is far from sufficient -- machine\nintelligence currently lacks an effective way to learn about the physical\nworld. This research introduces the idea of analytic concept -- representing\nthe concepts related to the physical world through programs of mathematical\nprocedures, providing machine intelligence a portal to perceive, reason about,\nand interact with the physical world. Except for detailing the design\nphilosophy and providing guidelines for the application of analytic concepts,\nthis research also introduce about the infrastructure that has been built\naround analytic concepts. I aim for my research to contribute to addressing\nthese questions: What is a proper abstraction of general concepts in the\nphysical world for machine intelligence? How to systematically integrate\nstructured priors with neural networks to constrain AI systems to comply with\nphysical laws?"}
{"id": "2504.04228", "pdf": "https://arxiv.org/pdf/2504.04228", "abs": "https://arxiv.org/abs/2504.04228", "authors": ["Brayan Monroy", "Kebin Contreras", "Jorge Bacca"], "title": "Autoregressive High-Order Finite Difference Modulo Imaging: High-Dynamic Range for Computer Vision Applications", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "High dynamic range (HDR) imaging is vital for capturing the full range of\nlight tones in scenes, essential for computer vision tasks such as autonomous\ndriving. Standard commercial imaging systems face limitations in capacity for\nwell depth, and quantization precision, hindering their HDR capabilities.\nModulo imaging, based on unlimited sampling (US) theory, addresses these\nlimitations by using a modulo analog-to-digital approach that resets signals\nupon saturation, enabling estimation of pixel resets through neighboring pixel\nintensities. Despite the effectiveness of (US) algorithms in one-dimensional\nsignals, their optimization problem for two-dimensional signals remains\nunclear. This work formulates the US framework as an autoregressive $\\ell_2$\nphase unwrapping problem, providing computationally efficient solutions in the\ndiscrete cosine domain jointly with a stride removal algorithm also based on\nspatial differences. By leveraging higher-order finite differences for\ntwo-dimensional images, our approach enhances HDR image reconstruction from\nmodulo images, demonstrating its efficacy in improving object detection in\nautonomous driving scenes without retraining."}
{"id": "2504.04242", "pdf": "https://arxiv.org/pdf/2504.04242", "abs": "https://arxiv.org/abs/2504.04242", "authors": ["Omar Elharrouss", "Yasir Mahmood", "Yassine Bechqito", "Mohamed Adel Serhani", "Elarbi Badidi", "Jamal Riffi", "Hamid Tairi"], "title": "Loss Functions in Deep Learning: A Comprehensive Review", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Loss functions are at the heart of deep learning, shaping how models learn\nand perform across diverse tasks. They are used to quantify the difference\nbetween predicted outputs and ground truth labels, guiding the optimization\nprocess to minimize errors. Selecting the right loss function is critical, as\nit directly impacts model convergence, generalization, and overall performance\nacross various applications, from computer vision to time series forecasting.\nThis paper presents a comprehensive review of loss functions, covering\nfundamental metrics like Mean Squared Error and Cross-Entropy to advanced\nfunctions such as Adversarial and Diffusion losses. We explore their\nmathematical foundations, impact on model training, and strategic selection for\nvarious applications, including computer vision (Discriminative and\ngenerative), tabular data prediction, and time series forecasting. For each of\nthese categories, we discuss the most used loss functions in the recent\nadvancements of deep learning techniques. Also, this review explore the\nhistorical evolution, computational efficiency, and ongoing challenges in loss\nfunction design, underlining the need for more adaptive and robust solutions.\nEmphasis is placed on complex scenarios involving multi-modal data, class\nimbalances, and real-world constraints. Finally, we identify key future\ndirections, advocating for loss functions that enhance interpretability,\nscalability, and generalization, leading to more effective and resilient deep\nlearning models."}
{"id": "2504.04260", "pdf": "https://arxiv.org/pdf/2504.04260", "abs": "https://arxiv.org/abs/2504.04260", "authors": ["Marimuthu Kalimuthu", "David Holzm√ºller", "Mathias Niepert"], "title": "LOGLO-FNO: Efficient Learning of Local and Global Features in Fourier Neural Operators", "categories": ["cs.LG", "cs.AI", "cs.CV", "physics.geo-ph"], "comment": "Accepted for Oral Presentation at the ICLR 2025 Workshop on Machine\n  Learning Multiscale Processes (MLMP), Singapura", "summary": "Modeling high-frequency information is a critical challenge in scientific\nmachine learning. For instance, fully turbulent flow simulations of\nNavier-Stokes equations at Reynolds numbers 3500 and above can generate\nhigh-frequency signals due to swirling fluid motions caused by eddies and\nvortices. Faithfully modeling such signals using neural networks depends on\naccurately reconstructing moderate to high frequencies. However, it has been\nwell known that deep neural nets exhibit the so-called spectral bias toward\nlearning low-frequency components. Meanwhile, Fourier Neural Operators (FNOs)\nhave emerged as a popular class of data-driven models in recent years for\nsolving Partial Differential Equations (PDEs) and for surrogate modeling in\ngeneral. Although impressive results have been achieved on several PDE\nbenchmark problems, FNOs often perform poorly in learning non-dominant\nfrequencies characterized by local features. This limitation stems from the\nspectral bias inherent in neural networks and the explicit exclusion of\nhigh-frequency modes in FNOs and their variants. Therefore, to mitigate these\nissues and improve FNO's spectral learning capabilities to represent a broad\nrange of frequency components, we propose two key architectural enhancements:\n(i) a parallel branch performing local spectral convolutions (ii) a\nhigh-frequency propagation module. Moreover, we propose a novel\nfrequency-sensitive loss term based on radially binned spectral errors. This\nintroduction of a parallel branch for local convolutions reduces number of\ntrainable parameters by up to 50% while achieving the accuracy of baseline FNO\nthat relies solely on global convolutions. Experiments on three challenging PDE\nproblems in fluid mechanics and biological pattern formation, and the\nqualitative and spectral analysis of predictions show the effectiveness of our\nmethod over the state-of-the-art neural operator baselines."}
{"id": "2504.04318", "pdf": "https://arxiv.org/pdf/2504.04318", "abs": "https://arxiv.org/abs/2504.04318", "authors": ["Mehmet Can Yavuz", "Berrin Yanikoglu"], "title": "Variational Self-Supervised Learning", "categories": ["cs.LG", "cs.CV"], "comment": "Submitted to NeurIPS 2025", "summary": "We present Variational Self-Supervised Learning (VSSL), a novel framework\nthat combines variational inference with self-supervised learning to enable\nefficient, decoder-free representation learning. Unlike traditional VAEs that\nrely on input reconstruction via a decoder, VSSL symmetrically couples two\nencoders with Gaussian outputs. A momentum-updated teacher network defines a\ndynamic, data-dependent prior, while the student encoder produces an\napproximate posterior from augmented views. The reconstruction term in the ELBO\nis replaced with a cross-view denoising objective, preserving the analytical\ntractability of Gaussian KL divergence. We further introduce cosine-based\nformulations of KL and log-likelihood terms to enhance semantic alignment in\nhigh-dimensional latent spaces. Experiments on CIFAR-10, CIFAR-100, and\nImageNet-100 show that VSSL achieves competitive or superior performance to\nleading self-supervised methods, including BYOL and MoCo V3. VSSL offers a\nscalable, probabilistically grounded approach to learning transferable\nrepresentations without generative reconstruction, bridging the gap between\nvariational modeling and modern self-supervised techniques."}
{"id": "2504.04338", "pdf": "https://arxiv.org/pdf/2504.04338", "abs": "https://arxiv.org/abs/2504.04338", "authors": ["Alexander Naumann", "Xunjiang Gu", "Tolga Dimlioglu", "Mariusz Bojarski", "Alperen Degirmenci", "Alexander Popov", "Devansh Bisla", "Marco Pavone", "Urs M√ºller", "Boris Ivanovic"], "title": "Data Scaling Laws for End-to-End Autonomous Driving", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "15 pages, 11 figures, 4 tables, CVPR 2025 Workshop on Autonomous\n  Driving", "summary": "Autonomous vehicle (AV) stacks have traditionally relied on decomposed\napproaches, with separate modules handling perception, prediction, and\nplanning. However, this design introduces information loss during inter-module\ncommunication, increases computational overhead, and can lead to compounding\nerrors. To address these challenges, recent works have proposed architectures\nthat integrate all components into an end-to-end differentiable model, enabling\nholistic system optimization. This shift emphasizes data engineering over\nsoftware integration, offering the potential to enhance system performance by\nsimply scaling up training resources. In this work, we evaluate the performance\nof a simple end-to-end driving architecture on internal driving datasets\nranging in size from 16 to 8192 hours with both open-loop metrics and\nclosed-loop simulations. Specifically, we investigate how much additional\ntraining data is needed to achieve a target performance gain, e.g., a 5%\nimprovement in motion prediction accuracy. By understanding the relationship\nbetween model performance and training dataset size, we aim to provide insights\nfor data-driven decision-making in autonomous driving development."}
{"id": "2504.04378", "pdf": "https://arxiv.org/pdf/2504.04378", "abs": "https://arxiv.org/abs/2504.04378", "authors": ["Taehoon Kim"], "title": "Future-Proof Yourself: An AI Era Survival Guide", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "10 chapters, 259 pages, Textbook for \"Data & AI\" and \"Artificial\n  Intelligence\" at Sogang University Graduate School of Metaverse", "summary": "Future-Proof Yourself is a practical guide that helps readers navigate the\nfast-changing world of artificial intelligence in everyday life. The book\nbegins by explaining how computers learn from data in simple, relatable terms,\nand gradually introduces the methods used in modern AI. It shows how basic\nideas in machine learning evolve into advanced systems that can recognize\nimages, understand language, and even make decisions. The guide also reviews\nthe history of AI and highlights the major breakthroughs that have shaped its\ngrowth. Looking ahead, the book explores emerging trends such as the\nintegration of AI with digital twins, wearable devices, and virtual\nenvironments. Designed for a general audience, the text avoids heavy technical\njargon and presents complex ideas in clear, straightforward language so that\nanyone can gain a solid understanding of the technology that is set to\ntransform our future."}
{"id": "2504.04411", "pdf": "https://arxiv.org/pdf/2504.04411", "abs": "https://arxiv.org/abs/2504.04411", "authors": ["Zehui Lin", "Chenxiao Hu", "Jinzhu Jia", "Sheng Li"], "title": "Hypothesis Testing for Progressive Kernel Estimation and VCM Framework", "categories": ["cs.GR", "cs.CV"], "comment": "This paper has been published in IEEE Transactions on Visualization\n  and Computer Graphics. This version is a preprint one", "summary": "Identifying an appropriate radius for unbiased kernel estimation is crucial\nfor the efficiency of radiance estimation. However, determining both the radius\nand unbiasedness still faces big challenges. In this paper, we first propose a\nstatistical model of photon samples and associated contributions for\nprogressive kernel estimation, under which the kernel estimation is unbiased if\nthe null hypothesis of this statistical model stands. Then, we present a method\nto decide whether to reject the null hypothesis about the statistical\npopulation (i.e., photon samples) by the F-test in the Analysis of Variance.\nHereby, we implement a progressive photon mapping (PPM) algorithm, wherein the\nkernel radius is determined by this hypothesis test for unbiased radiance\nestimation. Secondly, we propose VCM+, a reinforcement of Vertex Connection and\nMerging (VCM), and derive its theoretically unbiased formulation. VCM+ combines\nhypothesis testing-based PPM with bidirectional path tracing (BDPT) via\nmultiple importance sampling (MIS), wherein our kernel radius can leverage the\ncontributions from PPM and BDPT. We test our new algorithms, improved PPM and\nVCM+, on diverse scenarios with different lighting settings. The experimental\nresults demonstrate that our method can alleviate light leaks and visual blur\nartifacts of prior radiance estimate algorithms. We also evaluate the\nasymptotic performance of our approach and observe an overall improvement over\nthe baseline in all testing scenarios."}
{"id": "2504.04458", "pdf": "https://arxiv.org/pdf/2504.04458", "abs": "https://arxiv.org/abs/2504.04458", "authors": ["Bashir Alam", "Masa Cirkovic", "Mete Harun Akcay", "Md Kaf Shahrier", "Sebastien Lafond", "Hergys Rexha", "Kurt Benke", "Sepinoud Azimi", "Janan Arslan"], "title": "CALF: A Conditionally Adaptive Loss Function to Mitigate Class-Imbalanced Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Imbalanced datasets pose a considerable challenge in training deep learning\n(DL) models for medical diagnostics, particularly for segmentation tasks.\nImbalance may be associated with annotation quality limited annotated datasets,\nrare cases, or small-scale regions of interest (ROIs). These conditions\nadversely affect model training and performance, leading to segmentation\nboundaries which deviate from the true ROIs. Traditional loss functions, such\nas Binary Cross Entropy, replicate annotation biases and limit model\ngeneralization. We propose a novel, statistically driven, conditionally\nadaptive loss function (CALF) tailored to accommodate the conditions of\nimbalanced datasets in DL training. It employs a data-driven methodology by\nestimating imbalance severity using statistical methods of skewness and\nkurtosis, then applies an appropriate transformation to balance the training\ndataset while preserving data heterogeneity. This transformative approach\nintegrates a multifaceted process, encompassing preprocessing, dataset\nfiltering, and dynamic loss selection to achieve optimal outcomes. We benchmark\nour method against conventional loss functions using qualitative and\nquantitative evaluations. Experiments using large-scale open-source datasets\n(i.e., UPENN-GBM, UCSF, LGG, and BraTS) validate our approach, demonstrating\nsubstantial segmentation improvements. Code availability:\nhttps://anonymous.4open.science/r/MICCAI-Submission-43F9/."}
{"id": "2504.04506", "pdf": "https://arxiv.org/pdf/2504.04506", "abs": "https://arxiv.org/abs/2504.04506", "authors": ["Netta Shafir", "Guy Hacohen", "Daphna Weinshall"], "title": "Active Learning with a Noisy Annotator", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Active Learning (AL) aims to reduce annotation costs by strategically\nselecting the most informative samples for labeling. However, most active\nlearning methods struggle in the low-budget regime where only a few labeled\nexamples are available. This issue becomes even more pronounced when annotators\nprovide noisy labels. A common AL approach for the low- and mid-budget regimes\nfocuses on maximizing the coverage of the labeled set across the entire\ndataset. We propose a novel framework called Noise-Aware Active Sampling (NAS)\nthat extends existing greedy, coverage-based active learning strategies to\nhandle noisy annotations. NAS identifies regions that remain uncovered due to\nthe selection of noisy representatives and enables resampling from these areas.\nWe introduce a simple yet effective noise filtering approach suitable for the\nlow-budget regime, which leverages the inner mechanism of NAS and can be\napplied for noise filtering before model training. On multiple computer vision\nbenchmarks, including CIFAR100 and ImageNet subsets, NAS significantly improves\nperformance for standard active learning methods across different noise types\nand rates."}
{"id": "2504.04532", "pdf": "https://arxiv.org/pdf/2504.04532", "abs": "https://arxiv.org/abs/2504.04532", "authors": ["Moinak Bhattacharya", "Saumya Gupta", "Annie Singh", "Chao Chen", "Gagandeep Singh", "Prateek Prasanna"], "title": "BrainMRDiff: A Diffusion Model for Anatomically Consistent Brain MRI Synthesis", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Accurate brain tumor diagnosis relies on the assessment of multiple Magnetic\nResonance Imaging (MRI) sequences. However, in clinical practice, the\nacquisition of certain sequences may be affected by factors like motion\nartifacts or contrast agent contraindications, leading to suboptimal outcome,\nsuch as poor image quality. This can then affect image interpretation by\nradiologists. Synthesizing high quality MRI sequences has thus become a\ncritical research focus. Though recent advancements in controllable generative\nAI have facilitated the synthesis of diagnostic quality MRI, ensuring\nanatomical accuracy remains a significant challenge. Preserving critical\nstructural relationships between different anatomical regions is essential, as\neven minor structural or topological inconsistencies can compromise diagnostic\nvalidity. In this work, we propose BrainMRDiff, a novel topology-preserving,\nanatomy-guided diffusion model for synthesizing brain MRI, leveraging brain and\ntumor anatomies as conditioning inputs. To achieve this, we introduce two key\nmodules: Tumor+Structure Aggregation (TSA) and Topology-Guided Anatomy\nPreservation (TGAP). TSA integrates diverse anatomical structures with tumor\ninformation, forming a comprehensive conditioning mechanism for the diffusion\nprocess. TGAP enforces topological consistency during reverse denoising\ndiffusion process; both these modules ensure that the generated image respects\nanatomical integrity. Experimental results demonstrate that BrainMRDiff\nsurpasses existing baselines, achieving performance improvements of 23.33% on\nthe BraTS-AG dataset and 33.33% on the BraTS-Met dataset. Code will be made\npublicly available soon."}
{"id": "2504.04634", "pdf": "https://arxiv.org/pdf/2504.04634", "abs": "https://arxiv.org/abs/2504.04634", "authors": ["Foram Niravbhai Shah", "Parshwa Shah", "Muhammad Usama Saleem", "Ekkasit Pinyoanuntapong", "Pu Wang", "Hongfei Xue", "Ahmed Helmy"], "title": "DanceMosaic: High-Fidelity Dance Generation with Multimodal Editability", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Recent advances in dance generation have enabled automatic synthesis of 3D\ndance motions. However, existing methods still struggle to produce\nhigh-fidelity dance sequences that simultaneously deliver exceptional realism,\nprecise dance-music synchronization, high motion diversity, and physical\nplausibility. Moreover, existing methods lack the flexibility to edit dance\nsequences according to diverse guidance signals, such as musical prompts, pose\nconstraints, action labels, and genre descriptions, significantly restricting\ntheir creative utility and adaptability. Unlike the existing approaches,\nDanceMosaic enables fast and high-fidelity dance generation, while allowing\nmultimodal motion editing. Specifically, we propose a multimodal masked motion\nmodel that fuses the text-to-motion model with music and pose adapters to learn\nprobabilistic mapping from diverse guidance signals to high-quality dance\nmotion sequences via progressive generative masking training. To further\nenhance the motion generation quality, we propose multimodal classifier-free\nguidance and inference-time optimization mechanism that further enforce the\nalignment between the generated motions and the multimodal guidance. Extensive\nexperiments demonstrate that our method establishes a new state-of-the-art\nperformance in dance generation, significantly advancing the quality and\neditability achieved by existing approaches."}
{"id": "2504.04645", "pdf": "https://arxiv.org/pdf/2504.04645", "abs": "https://arxiv.org/abs/2504.04645", "authors": ["Tianyi Ren", "Juampablo Heras Rivera", "Hitender Oswal", "Yutong Pan", "Agamdeep Chopra", "Jacob Ruzevick", "Mehmet Kurt"], "title": "Here Comes the Explanation: A Shapley Perspective on Multi-contrast Medical Image Segmentation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Deep learning has been successfully applied to medical image segmentation,\nenabling accurate identification of regions of interest such as organs and\nlesions. This approach works effectively across diverse datasets, including\nthose with single-image contrast, multi-contrast, and multimodal imaging data.\nTo improve human understanding of these black-box models, there is a growing\nneed for Explainable AI (XAI) techniques for model transparency and\naccountability. Previous research has primarily focused on post hoc pixel-level\nexplanations, using methods gradient-based and perturbation-based apporaches.\nThese methods rely on gradients or perturbations to explain model predictions.\nHowever, these pixel-level explanations often struggle with the complexity\ninherent in multi-contrast magnetic resonance imaging (MRI) segmentation tasks,\nand the sparsely distributed explanations have limited clinical relevance. In\nthis study, we propose using contrast-level Shapley values to explain\nstate-of-the-art models trained on standard metrics used in brain tumor\nsegmentation. Our results demonstrate that Shapley analysis provides valuable\ninsights into different models' behavior used for tumor segmentation. We\ndemonstrated a bias for U-Net towards over-weighing T1-contrast and FLAIR,\nwhile Swin-UNETR provided a cross-contrast understanding with balanced Shapley\ndistribution."}
{"id": "2504.04664", "pdf": "https://arxiv.org/pdf/2504.04664", "abs": "https://arxiv.org/abs/2504.04664", "authors": ["Md Bayazid Hossain", "Md Anwarul Islam Himel", "Md Abdur Rahim", "Shabbir Mahmood", "Abu Saleh Musa Miah", "Jungpil Shin"], "title": "Classification of ADHD and Healthy Children Using EEG Based Multi-Band Spatial Features Enhancement", "categories": ["eess.SP", "cs.CV"], "comment": null, "summary": "Attention Deficit Hyperactivity Disorder (ADHD) is a common\nneurodevelopmental disorder in children, characterized by difficulties in\nattention, hyperactivity, and impulsivity. Early and accurate diagnosis of ADHD\nis critical for effective intervention and management. Electroencephalogram\n(EEG) signals have emerged as a non-invasive and efficient tool for ADHD\ndetection due to their high temporal resolution and ability to capture neural\ndynamics. In this study, we propose a method for classifying ADHD and healthy\nchildren using EEG data from the benchmark dataset. There were 61 children with\nADHD and 60 healthy children, both boys and girls, aged 7 to 12. The EEG\nsignals, recorded from 19 channels, were processed to extract Power Spectral\nDensity (PSD) and Spectral Entropy (SE) features across five frequency bands,\nresulting in a comprehensive 190-dimensional feature set. To evaluate the\nclassification performance, a Support Vector Machine (SVM) with the RBF kernel\ndemonstrated the best performance with a mean cross-validation accuracy of\n99.2\\% and a standard deviation of 0.0079, indicating high robustness and\nprecision. These results highlight the potential of spatial features in\nconjunction with machine learning for accurately classifying ADHD using EEG\ndata. This work contributes to developing non-invasive, data-driven tools for\nearly diagnosis and assessment of ADHD in children."}
{"id": "2504.04704", "pdf": "https://arxiv.org/pdf/2504.04704", "abs": "https://arxiv.org/abs/2504.04704", "authors": ["Manlai Liang", "JiaMing Zhang", "Xiong Li", "Jinlong Li"], "title": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are Important", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modifiation of the inference infrastructure and\nsignificant computation overhead. Base on the fact that the Large Lanuage\nmodels are autoregresssive models, we propose {\\it LagKV}, a KV allocation\nstrategy only relying on straight forward comparison among KV themself. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on LongBench and PasskeyRetrieval show that, our\napproach achieves nearly zero loss when the ratio is $2\\times$ and $\\approx\n90\\%$ of the original model performance for $8\\times$. Especially in the\n64-digit passkey retrieval task, our mehod outperforms the attention weight\nbased method $H_2O$ over $60\\%$ with same compression ratios. Our code is\navailable at \\url{https://github.com/AI-Lab-China-Merchants-Bank/LagKV}."}
{"id": "2504.04749", "pdf": "https://arxiv.org/pdf/2504.04749", "abs": "https://arxiv.org/abs/2504.04749", "authors": ["Ahmad Hussein", "Mukesh Prasad", "Ali Braytee"], "title": "Vision Transformers with Autoencoders and Explainable AI for Cancer Patient Risk Stratification Using Whole Slide Imaging", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "10 pages", "summary": "Cancer remains one of the leading causes of mortality worldwide,\nnecessitating accurate diagnosis and prognosis. Whole Slide Imaging (WSI) has\nbecome an integral part of clinical workflows with advancements in digital\npathology. While various studies have utilized WSIs, their extracted features\nmay not fully capture the most relevant pathological information, and their\nlack of interpretability limits clinical adoption.\n  In this paper, we propose PATH-X, a framework that integrates Vision\nTransformers (ViT) and Autoencoders with SHAP (Shapley Additive Explanations)\nto enhance model explainability for patient stratification and risk prediction\nusing WSIs from The Cancer Genome Atlas (TCGA). A representative image slice is\nselected from each WSI, and numerical feature embeddings are extracted using\nGoogle's pre-trained ViT. These features are then compressed via an autoencoder\nand used for unsupervised clustering and classification tasks. Kaplan-Meier\nsurvival analysis is applied to evaluate stratification into two and three risk\ngroups. SHAP is used to identify key contributing features, which are mapped\nonto histopathological slices to provide spatial context.\n  PATH-X demonstrates strong performance in breast and glioma cancers, where a\nsufficient number of WSIs enabled robust stratification. However, performance\nin lung cancer was limited due to data availability, emphasizing the need for\nlarger datasets to enhance model reliability and clinical applicability."}
{"id": "2504.04814", "pdf": "https://arxiv.org/pdf/2504.04814", "abs": "https://arxiv.org/abs/2504.04814", "authors": ["Nataliia Molchanova", "Pedro M. Gordaliza", "Alessandro Cagol", "Mario Ocampo--Pineda", "Po--Jui Lu", "Matthias Weigel", "Xinjie Chen", "Erin S. Beck", "Haris Tsagkas", "Daniel Reich", "Anna St√∂lting", "Pietro Maggi", "Delphine Ribes", "Adrien Depeursinge", "Cristina Granziera", "Henning M√ºller", "Meritxell Bach Cuadra"], "title": "Explainability of AI Uncertainty: Application to Multiple Sclerosis Lesion Segmentation on MRI", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Trustworthy artificial intelligence (AI) is essential in healthcare,\nparticularly for high-stakes tasks like medical image segmentation. Explainable\nAI and uncertainty quantification significantly enhance AI reliability by\naddressing key attributes such as robustness, usability, and explainability.\nDespite extensive technical advances in uncertainty quantification for medical\nimaging, understanding the clinical informativeness and interpretability of\nuncertainty remains limited. This study introduces a novel framework to explain\nthe potential sources of predictive uncertainty, specifically in cortical\nlesion segmentation in multiple sclerosis using deep ensembles. The proposed\nanalysis shifts the focus from the uncertainty-error relationship towards\nrelevant medical and engineering factors. Our findings reveal that\ninstance-wise uncertainty is strongly related to lesion size, shape, and\ncortical involvement. Expert rater feedback confirms that similar factors\nimpede annotator confidence. Evaluations conducted on two datasets (206\npatients, almost 2000 lesions) under both in-domain and distribution-shift\nconditions highlight the utility of the framework in different scenarios."}
{"id": "2504.04831", "pdf": "https://arxiv.org/pdf/2504.04831", "abs": "https://arxiv.org/abs/2504.04831", "authors": ["Sanjeev Muralikrishnan", "Niladri Shekhar Dutt", "Niloy J. Mitra"], "title": "SMF: Template-free and Rig-free Animation Transfer using Kinetic Codes", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Animation retargeting involves applying a sparse motion description (e.g.,\n2D/3D keypoint sequences) to a given character mesh to produce a semantically\nplausible and temporally coherent full-body motion. Existing approaches come\nwith a mix of restrictions - they require annotated training data, assume\naccess to template-based shape priors or artist-designed deformation rigs,\nsuffer from limited generalization to unseen motion and/or shapes, or exhibit\nmotion jitter. We propose Self-supervised Motion Fields (SMF) as a\nself-supervised framework that can be robustly trained with sparse motion\nrepresentations, without requiring dataset specific annotations, templates, or\nrigs. At the heart of our method are Kinetic Codes, a novel autoencoder-based\nsparse motion encoding, that exposes a semantically rich latent space\nsimplifying large-scale training. Our architecture comprises dedicated spatial\nand temporal gradient predictors, which are trained end-to-end. The resultant\nnetwork, regularized by the Kinetic Codes's latent space, has good\ngeneralization across shapes and motions. We evaluated our method on unseen\nmotion sampled from AMASS, D4D, Mixamo, and raw monocular video for animation\ntransfer on various characters with varying shapes and topology. We report a\nnew SoTA on the AMASS dataset in the context of generalization to unseen\nmotion. Project webpage at https://motionfields.github.io/"}
{"id": "2504.04844", "pdf": "https://arxiv.org/pdf/2504.04844", "abs": "https://arxiv.org/abs/2504.04844", "authors": ["Zhicong Sun", "Jacqueline Lo", "Jinxing Hu"], "title": "Embracing Dynamics: Dynamics-aware 4D Gaussian Splatting SLAM", "categories": ["cs.RO", "cs.CV"], "comment": "This paper is currently under reviewed for IROS 2025", "summary": "Simultaneous localization and mapping (SLAM) technology now has\nphotorealistic mapping capabilities thanks to the real-time high-fidelity\nrendering capability of 3D Gaussian splatting (3DGS). However, due to the\nstatic representation of scenes, current 3DGS-based SLAM encounters issues with\npose drift and failure to reconstruct accurate maps in dynamic environments. To\naddress this problem, we present D4DGS-SLAM, the first SLAM method based on\n4DGS map representation for dynamic environments. By incorporating the temporal\ndimension into scene representation, D4DGS-SLAM enables high-quality\nreconstruction of dynamic scenes. Utilizing the dynamics-aware InfoModule, we\ncan obtain the dynamics, visibility, and reliability of scene points, and\nfilter stable static points for tracking accordingly. When optimizing Gaussian\npoints, we apply different isotropic regularization terms to Gaussians with\nvarying dynamic characteristics. Experimental results on real-world dynamic\nscene datasets demonstrate that our method outperforms state-of-the-art\napproaches in both camera pose tracking and map quality."}
{"id": "2504.04939", "pdf": "https://arxiv.org/pdf/2504.04939", "abs": "https://arxiv.org/abs/2504.04939", "authors": ["Naoki Wake", "Atsushi Kanehira", "Kazuhiro Sasabuchi", "Jun Takamatsu", "Katsushi Ikeuchi"], "title": "A Taxonomy of Self-Handover", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "8 pages, 8 figures, 1 table, Last updated on April 7th, 2025", "summary": "Self-handover, transferring an object between one's own hands, is a common\nbut understudied bimanual action. While it facilitates seamless transitions in\ncomplex tasks, the strategies underlying its execution remain largely\nunexplored. Here, we introduce the first systematic taxonomy of self-handover,\nderived from manual annotation of over 12 hours of cooking activity performed\nby 21 participants. Our analysis reveals that self-handover is not merely a\npassive transition, but a highly coordinated action involving anticipatory\nadjustments by both hands. As a step toward automated analysis of human\nmanipulation, we further demonstrate the feasibility of classifying\nself-handover types using a state-of-the-art vision-language model. These\nfindings offer fresh insights into bimanual coordination, underscoring the role\nof self-handover in enabling smooth task transitions-an ability essential for\nadaptive dual-arm robotics."}
{"id": "2504.04956", "pdf": "https://arxiv.org/pdf/2504.04956", "abs": "https://arxiv.org/abs/2504.04956", "authors": ["Jihyun Lee", "Weipeng Xu", "Alexander Richard", "Shih-En Wei", "Shunsuke Saito", "Shaojie Bai", "Te-Li Wang", "Minhyuk Sung", "Tae-Kyun", "Kim", "Jason Saragih"], "title": "REWIND: Real-Time Egocentric Whole-Body Motion Diffusion with Exemplar-Based Identity Conditioning", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted to CVPR 2025, project page:\n  https://jyunlee.github.io/projects/rewind/", "summary": "We present REWIND (Real-Time Egocentric Whole-Body Motion Diffusion), a\none-step diffusion model for real-time, high-fidelity human motion estimation\nfrom egocentric image inputs. While an existing method for egocentric\nwhole-body (i.e., body and hands) motion estimation is non-real-time and\nacausal due to diffusion-based iterative motion refinement to capture\ncorrelations between body and hand poses, REWIND operates in a fully causal and\nreal-time manner. To enable real-time inference, we introduce (1) cascaded\nbody-hand denoising diffusion, which effectively models the correlation between\negocentric body and hand motions in a fast, feed-forward manner, and (2)\ndiffusion distillation, which enables high-quality motion estimation with a\nsingle denoising step. Our denoising diffusion model is based on a modified\nTransformer architecture, designed to causally model output motions while\nenhancing generalizability to unseen motion lengths. Additionally, REWIND\noptionally supports identity-conditioned motion estimation when identity prior\nis available. To this end, we propose a novel identity conditioning method\nbased on a small set of pose exemplars of the target identity, which further\nenhances motion estimation quality. Through extensive experiments, we\ndemonstrate that REWIND significantly outperforms the existing baselines both\nwith and without exemplar-based identity conditioning."}
{"id": "2504.05033", "pdf": "https://arxiv.org/pdf/2504.05033", "abs": "https://arxiv.org/abs/2504.05033", "authors": ["Jay Kamat", "J√∫lia Borr√†s", "Carme Torras"], "title": "CloSE: A Compact Shape- and Orientation-Agnostic Cloth State Representation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Cloth manipulation is a difficult problem mainly because of the non-rigid\nnature of cloth, which makes a good representation of deformation essential. We\npresent a new representation for the deformation-state of clothes. First, we\npropose the dGLI disk representation, based on topological indices computed for\nsegments on the edges of the cloth mesh border that are arranged on a circular\ngrid. The heat-map of the dGLI disk uncovers patterns that correspond to\nfeatures of the cloth state that are consistent for different shapes, sizes of\npositions of the cloth, like the corners and the fold locations. We then\nabstract these important features from the dGLI disk onto a circle, calling it\nthe Cloth StatE representation (CloSE). This representation is compact,\ncontinuous, and general for different shapes. Finally, we show the strengths of\nthis representation in two relevant applications: semantic labeling and high-\nand low-level planning. The code, the dataset and the video can be accessed\nfrom : https://jaykamat99.github.io/close-representation"}
{"id": "2504.05119", "pdf": "https://arxiv.org/pdf/2504.05119", "abs": "https://arxiv.org/abs/2504.05119", "authors": ["Jon Guti√©rrez Zaballa", "Koldo Basterretxea", "Javier Echanobe"], "title": "Balancing Robustness and Efficiency in Embedded DNNs Through Activation Function Selection", "categories": ["cs.LG", "cs.AI", "cs.AR", "cs.CV", "eess.IV"], "comment": null, "summary": "Machine learning-based embedded systems for safety-critical applications,\nsuch as aerospace and autonomous driving, must be robust to perturbations\ncaused by soft errors. As transistor geometries shrink and voltages decrease,\nmodern electronic devices become more susceptible to background radiation,\nincreasing the concern about failures produced by soft errors. The resilience\nof deep neural networks (DNNs) to these errors depends not only on target\ndevice technology but also on model structure and the numerical representation\nand arithmetic precision of their parameters. Compression techniques like\npruning and quantization, used to reduce memory footprint and computational\ncomplexity, alter both model structure and representation, affecting soft error\nrobustness. In this regard, although often overlooked, the choice of activation\nfunctions (AFs) impacts not only accuracy and trainability but also\ncompressibility and error resilience. This paper explores the use of bounded\nAFs to enhance robustness against parameter perturbations, while evaluating\ntheir effects on model accuracy, compressibility, and computational load with a\ntechnology-agnostic approach. We focus on encoder-decoder convolutional models\ndeveloped for semantic segmentation of hyperspectral images with application to\nautonomous driving systems. Experiments are conducted on an AMD-Xilinx's KV260\nSoM."}
{"id": "2504.05196", "pdf": "https://arxiv.org/pdf/2504.05196", "abs": "https://arxiv.org/abs/2504.05196", "authors": ["Tejas Sudharshan Mathai", "Sungwon Lee", "Thomas C. Shen", "Zhiyong Lu", "Ronald M. Summers"], "title": "Universal Lymph Node Detection in Multiparametric MRI with Selective Augmentation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Published at SPIE Medical Imaging 2023", "summary": "Robust localization of lymph nodes (LNs) in multiparametric MRI (mpMRI) is\ncritical for the assessment of lymphadenopathy. Radiologists routinely measure\nthe size of LN to distinguish benign from malignant nodes, which would require\nsubsequent cancer staging. Sizing is a cumbersome task compounded by the\ndiverse appearances of LNs in mpMRI, which renders their measurement difficult.\nFurthermore, smaller and potentially metastatic LNs could be missed during a\nbusy clinical day. To alleviate these imaging and workflow problems, we propose\na pipeline to universally detect both benign and metastatic nodes in the body\nfor their ensuing measurement. The recently proposed VFNet neural network was\nemployed to identify LN in T2 fat suppressed and diffusion weighted imaging\n(DWI) sequences acquired by various scanners with a variety of exam protocols.\nWe also use a selective augmentation technique known as Intra-Label LISA (ILL)\nto diversify the input data samples the model sees during training, such that\nit improves its robustness during the evaluation phase. We achieved a\nsensitivity of $\\sim$83\\% with ILL vs. $\\sim$80\\% without ILL at 4 FP/vol.\nCompared with current LN detection approaches evaluated on mpMRI, we show a\nsensitivity improvement of $\\sim$9\\% at 4 FP/vol."}
{"id": "2504.05231", "pdf": "https://arxiv.org/pdf/2504.05231", "abs": "https://arxiv.org/abs/2504.05231", "authors": ["C√©sar Leblanc", "Lukas Picek", "Benjamin Deneu", "Pierre Bonnet", "Maximilien Servajean", "R√©mi Palard", "Alexis Joly"], "title": "Mapping biodiversity at very-high resolution in Europe", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "This paper describes a cascading multimodal pipeline for high-resolution\nbiodiversity mapping across Europe, integrating species distribution modeling,\nbiodiversity indicators, and habitat classification. The proposed pipeline\nfirst predicts species compositions using a deep-SDM, a multimodal model\ntrained on remote sensing, climate time series, and species occurrence data at\n50x50m resolution. These predictions are then used to generate biodiversity\nindicator maps and classify habitats with Pl@ntBERT, a transformer-based LLM\ndesigned for species-to-habitat mapping. With this approach, continental-scale\nspecies distribution maps, biodiversity indicator maps, and habitat maps are\nproduced, providing fine-grained ecological insights. Unlike traditional\nmethods, this framework enables joint modeling of interspecies dependencies,\nbias-aware training with heterogeneous presence-absence data, and large-scale\ninference from multi-source remote sensing inputs."}
{"id": "2504.05296", "pdf": "https://arxiv.org/pdf/2504.05296", "abs": "https://arxiv.org/abs/2504.05296", "authors": ["Gal Fiebelman", "Hadar Averbuch-Elor", "Sagie Benaim"], "title": "Let it Snow! Animating Static Gaussian Scenes With Dynamic Weather Effects", "categories": ["cs.GR", "cs.CV"], "comment": "Project webpage: https://galfiebelman.github.io/let-it-snow/", "summary": "3D Gaussian Splatting has recently enabled fast and photorealistic\nreconstruction of static 3D scenes. However, introducing dynamic elements that\ninteract naturally with such static scenes remains challenging. Accordingly, we\npresent a novel hybrid framework that combines Gaussian-particle\nrepresentations for incorporating physically-based global weather effects into\nstatic 3D Gaussian Splatting scenes, correctly handling the interactions of\ndynamic elements with the static scene. We follow a three-stage process: we\nfirst map static 3D Gaussians to a particle-based representation. We then\nintroduce dynamic particles and simulate their motion using the Material Point\nMethod (MPM). Finally, we map the simulated particles back to the Gaussian\ndomain while introducing appearance parameters tailored for specific effects.\nTo correctly handle the interactions of dynamic elements with the static scene,\nwe introduce specialized collision handling techniques. Our approach supports a\nvariety of weather effects, including snowfall, rainfall, fog, and sandstorms,\nand can also support falling objects, all with physically plausible motion and\nappearance. Experiments demonstrate that our method significantly outperforms\nexisting approaches in both visual quality and physical realism."}
{"id": "2504.05299", "pdf": "https://arxiv.org/pdf/2504.05299", "abs": "https://arxiv.org/abs/2504.05299", "authors": ["Andr√©s Marafioti", "Orr Zohar", "Miquel Farr√©", "Merve Noyan", "Elie Bakouch", "Pedro Cuenca", "Cyril Zakka", "Loubna Ben Allal", "Anton Lozhkov", "Nouamane Tazi", "Vaibhav Srivastav", "Joshua Lochner", "Hugo Larcher", "Mathieu Morlon", "Lewis Tunstall", "Leandro von Werra", "Thomas Wolf"], "title": "SmolVLM: Redefining small and efficient multimodal models", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Large Vision-Language Models (VLMs) deliver exceptional performance but\nrequire significant computational resources, limiting their deployment on\nmobile and edge devices. Smaller VLMs typically mirror design choices of larger\nmodels, such as extensive image tokenization, leading to inefficient GPU memory\nusage and constrained practicality for on-device applications.\n  We introduce SmolVLM, a series of compact multimodal models specifically\nengineered for resource-efficient inference. We systematically explore\narchitectural configurations, tokenization strategies, and data curation\noptimized for low computational overhead. Through this, we identify key design\nchoices that yield substantial performance gains on image and video tasks with\nminimal memory footprints.\n  Our smallest model, SmolVLM-256M, uses less than 1GB GPU memory during\ninference and outperforms the 300-times larger Idefics-80B model, despite an\n18-month development gap. Our largest model, at 2.2B parameters, rivals\nstate-of-the-art VLMs consuming twice the GPU memory. SmolVLM models extend\nbeyond static images, demonstrating robust video comprehension capabilities.\n  Our results emphasize that strategic architectural optimizations, aggressive\nyet efficient tokenization, and carefully curated training data significantly\nenhance multimodal performance, facilitating practical, energy-efficient\ndeployments at significantly smaller scales."}
{"id": "2504.05304", "pdf": "https://arxiv.org/pdf/2504.05304", "abs": "https://arxiv.org/abs/2504.05304", "authors": ["Hansheng Chen", "Kai Zhang", "Hao Tan", "Zexiang Xu", "Fujun Luan", "Leonidas Guibas", "Gordon Wetzstein", "Sai Bi"], "title": "Gaussian Mixture Flow Matching Models", "categories": ["cs.LG", "cs.CV"], "comment": "Code: https://github.com/Lakonik/GMFlow", "summary": "Diffusion models approximate the denoising distribution as a Gaussian and\npredict its mean, whereas flow matching models reparameterize the Gaussian mean\nas flow velocity. However, they underperform in few-step sampling due to\ndiscretization error and tend to produce over-saturated colors under\nclassifier-free guidance (CFG). To address these limitations, we propose a\nnovel Gaussian mixture flow matching (GMFlow) model: instead of predicting the\nmean, GMFlow predicts dynamic Gaussian mixture (GM) parameters to capture a\nmulti-modal flow velocity distribution, which can be learned with a KL\ndivergence loss. We demonstrate that GMFlow generalizes previous diffusion and\nflow matching models where a single Gaussian is learned with an $L_2$ denoising\nloss. For inference, we derive GM-SDE/ODE solvers that leverage analytic\ndenoising distributions and velocity fields for precise few-step sampling.\nFurthermore, we introduce a novel probabilistic guidance scheme that mitigates\nthe over-saturation issues of CFG and improves image generation quality.\nExtensive experiments demonstrate that GMFlow consistently outperforms flow\nmatching baselines in generation quality, achieving a Precision of 0.942 with\nonly 6 sampling steps on ImageNet 256$\\times$256."}
