{"id": "2503.21800", "pdf": "https://arxiv.org/pdf/2503.21800", "abs": "https://arxiv.org/abs/2503.21800", "authors": ["Lovedeep Gondara", "Jonathan Simkin", "Shebnum Devji", "Gregory Arbour", "Raymond Ng"], "title": "ELM: Ensemble of Language Models for Predicting Tumor Group from Pathology Reports", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Population-based cancer registries (PBCRs) face a significant bottleneck in\nmanually extracting data from unstructured pathology reports, a process crucial\nfor tasks like tumor group assignment, which can consume 900 person-hours for\napproximately 100,000 reports. To address this, we introduce ELM (Ensemble of\nLanguage Models), a novel ensemble-based approach leveraging both small\nlanguage models (SLMs) and large language models (LLMs). ELM utilizes six\nfine-tuned SLMs, where three SLMs use the top part of the pathology report and\nthree SLMs use the bottom part. This is done to maximize report coverage. ELM\nrequires five-out-of-six agreement for a tumor group classification.\nDisagreements are arbitrated by an LLM with a carefully curated prompt. Our\nevaluation across nineteen tumor groups demonstrates ELM achieves an average\nprecision and recall of 0.94, outperforming single-model and\nensemble-without-LLM approaches. Deployed at the British Columbia Cancer\nRegistry, ELM demonstrates how LLMs can be successfully applied in a PBCR\nsetting to achieve state-of-the-art results and significantly enhance\noperational efficiencies, saving hundreds of person-hours annually.", "AI": {"task": "\u81ea\u52a8\u5316\u4ece\u975e\u7ed3\u6784\u5316\u75c5\u7406\u62a5\u544a\u4e2d\u63d0\u53d6\u6570\u636e\u5e76\u5206\u914d\u80bf\u7624\u7ec4\u522b\u3002", "motivation": "\u89e3\u51b3\u4eba\u53e3\u764c\u75c7\u767b\u8bb0\u5904\uff08PBCRs\uff09\u5728\u624b\u52a8\u63d0\u53d6\u75c5\u7406\u62a5\u544a\u6570\u636e\u65f6\u7684\u9ad8\u65f6\u95f4\u6210\u672c\u95ee\u9898\uff08\u5982100,000\u4efd\u62a5\u544a\u9700900\u4eba\u65f6\uff09\u3002", "method": "\u63d0\u51faELM\uff08\u8bed\u8a00\u6a21\u578b\u96c6\u6210\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u901a\u8fc7\u516d\u79cd\u5fae\u8c03SLM\uff08\u4e09\u7ec4\u5206\u522b\u5904\u7406\u62a5\u544a\u9876\u90e8\u548c\u5e95\u90e8\uff09\u548cLLM\u4ef2\u88c1\u5206\u6b67\uff0c\u5b9e\u73b0\u4e94\u5206\u4e4b\u516d\u4e00\u81f4\u6027\u7684\u80bf\u7624\u7ec4\u5206\u7c7b\u3002", "result": "\u572819\u4e2a\u80bf\u7624\u7ec4\u522b\u4e2d\uff0cELM\u7684\u5e73\u5747\u7cbe\u786e\u5ea6\u548c\u53ec\u56de\u7387\u8fbe\u52300.94\uff0c\u4f18\u4e8e\u5355\u6a21\u578b\u548c\u65e0LLM\u7684\u96c6\u6210\u65b9\u6cd5\uff0c\u5e76\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u6bcf\u5e74\u8282\u7701\u6570\u767e\u4eba\u65f6\u3002", "conclusion": "ELM\u5c55\u793a\u4e86LLMs\u5728PBCR\u73af\u5883\u4e2d\u7684\u6210\u529f\u5e94\u7528\uff0c\u663e\u8457\u63d0\u5347\u64cd\u4f5c\u6548\u7387\u5e76\u8fbe\u5230\u6700\u5148\u8fdb\u6548\u679c\u3002"}}
{"id": "2503.21805", "pdf": "https://arxiv.org/pdf/2503.21805", "abs": "https://arxiv.org/abs/2503.21805", "authors": ["Wu jiaxuan", "Peng Wanli", "Fu hang", "Xue Yiming", "Wen juan"], "title": "ImF: Implicit Fingerprint for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 7 figures", "summary": "Training large language models (LLMs) is resource-intensive and expensive,\nmaking intellectual property (IP) protection essential. Most existing model\nfingerprint methods inject fingerprints into LLMs to protect model ownership.\nThese methods create fingerprint pairs with weak semantic correlations, lacking\nthe contextual coherence and semantic relatedness founded in normal\nquestion-answer (QA) pairs in LLMs. In this paper, we propose a Generation\nRevision Intervention (GRI) attack that can effectively exploit this flaw to\nerase fingerprints, highlighting the need for more secure model fingerprint\nmethods. Thus, we propose a novel injected fingerprint paradigm called Implicit\nFingerprints (ImF). ImF constructs fingerprint pairs with strong semantic\ncorrelations, disguising them as natural QA pairs within LLMs. This ensures the\nfingerprints are consistent with normal model behavior, making them\nindistinguishable and robust against detection and removal. Our experiment on\nmultiple LLMs demonstrates that ImF retains high verification success rates\nunder adversarial conditions, offering a reliable solution for protecting LLM\nownership.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6a21\u578b\u6307\u7eb9\u6ce8\u5165\u8303\u5f0f\uff08Implicit Fingerprints, ImF\uff09\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u6307\u7eb9\u65b9\u6cd5\u8bed\u4e49\u76f8\u5173\u6027\u5f31\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6307\u7eb9\u65b9\u6cd5\u751f\u6210\u7684\u6307\u7eb9\u5bf9\u8bed\u4e49\u76f8\u5173\u6027\u5f31\uff0c\u5bb9\u6613\u88ab\u653b\u51fb\uff08\u5982GRI\u653b\u51fb\uff09\u64e6\u9664\uff0c\u9700\u8981\u66f4\u5b89\u5168\u7684\u6307\u7eb9\u4fdd\u62a4\u65b9\u6cd5\u3002", "method": "\u63d0\u51faImplicit Fingerprints\uff08ImF\uff09\uff0c\u6784\u9020\u8bed\u4e49\u76f8\u5173\u6027\u5f3a\u7684\u6307\u7eb9\u5bf9\uff0c\u4f2a\u88c5\u4e3a\u81ea\u7136\u95ee\u7b54\u5bf9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cImF\u5728\u5bf9\u6297\u6761\u4ef6\u4e0b\u4ecd\u4fdd\u6301\u9ad8\u9a8c\u8bc1\u6210\u529f\u7387\u3002", "conclusion": "ImF\u4e3a\u4fdd\u62a4\u5927\u8bed\u8a00\u6a21\u578b\u6240\u6709\u6743\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2503.21806", "pdf": "https://arxiv.org/pdf/2503.21806", "abs": "https://arxiv.org/abs/2503.21806", "authors": ["Heqing Zou", "Fengmao Lv", "Desheng Zheng", "Eng Siong Chng", "Deepu Rajan"], "title": "Large Language Models Meet Contrastive Learning: Zero-Shot Emotion Recognition Across Languages", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ICME 2025", "summary": "Multilingual speech emotion recognition aims to estimate a speaker's\nemotional state using a contactless method across different languages. However,\nvariability in voice characteristics and linguistic diversity poses significant\nchallenges for zero-shot speech emotion recognition, especially with\nmultilingual datasets. In this paper, we propose leveraging contrastive\nlearning to refine multilingual speech features and extend large language\nmodels for zero-shot multilingual speech emotion estimation. Specifically, we\nemploy a novel two-stage training framework to align speech signals with\nlinguistic features in the emotional space, capturing both emotion-aware and\nlanguage-agnostic speech representations. To advance research in this field, we\nintroduce a large-scale synthetic multilingual speech emotion dataset, M5SER.\nOur experiments demonstrate the effectiveness of the proposed method in both\nspeech emotion recognition and zero-shot multilingual speech emotion\nrecognition, including previously unseen datasets and languages.", "AI": {"task": "\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u4f18\u5316\u591a\u8bed\u8a00\u8bed\u97f3\u7279\u5f81\uff0c\u5e76\u6269\u5c55\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee5\u5b9e\u73b0\u96f6\u6837\u672c\u591a\u8bed\u8a00\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u3002", "motivation": "\u591a\u8bed\u8a00\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u9762\u4e34\u8bed\u97f3\u7279\u5f81\u548c\u8bed\u8a00\u591a\u6837\u6027\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u96f6\u6837\u672c\u573a\u666f\u4e0b\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u5c06\u8bed\u97f3\u4fe1\u53f7\u4e0e\u60c5\u611f\u7a7a\u95f4\u4e2d\u7684\u8bed\u8a00\u7279\u5f81\u5bf9\u9f50\uff0c\u6355\u6349\u60c5\u611f\u611f\u77e5\u548c\u8bed\u8a00\u65e0\u5173\u7684\u8bed\u97f3\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u8bed\u8a00\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u548c\u96f6\u6837\u672c\u4efb\u52a1\u4e2d\u5747\u6709\u6548\uff0c\u5305\u62ec\u672a\u89c1\u8fc7\u7684\u6570\u636e\u96c6\u548c\u8bed\u8a00\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u8bed\u8a00\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6M5SER\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u3002"}}
{"id": "2503.21813", "pdf": "https://arxiv.org/pdf/2503.21813", "abs": "https://arxiv.org/abs/2503.21813", "authors": ["Zhangcheng Qiang"], "title": "OAEI-LLM-T: A TBox Benchmark Dataset for Understanding LLM Hallucinations in Ontology Matching Systems", "categories": ["cs.CL", "cs.IR"], "comment": "10 pages, 4 figures, 3 tables, 2 prompt templates", "summary": "Hallucinations are inevitable in downstream tasks using large language models\n(LLMs). While addressing hallucinations becomes a substantial challenge for\nLLM-based ontology matching (OM) systems, we introduce a new benchmark dataset\ncalled OAEI-LLM-T. The dataset evolves from the TBox (i.e. schema-matching)\ndatasets in the Ontology Alignment Evaluation Initiative (OAEI), capturing\nhallucinations of different LLMs performing OM tasks. These OM-specific\nhallucinations are carefully classified into two primary categories and six\nsub-categories. We showcase the usefulness of the dataset in constructing the\nLLM leaderboard and fine-tuning foundational LLMs for LLM-based OM systems.", "AI": {"task": "Introduce a new benchmark dataset called OAEI-LLM-T to address hallucinations in LLM-based ontology matching systems.", "motivation": "Hallucinations in LLMs are a significant challenge for ontology matching tasks, necessitating a dedicated dataset to study and mitigate them.", "method": "Develop the OAEI-LLM-T dataset from TBox datasets in OAEI, classify hallucinations into two primary categories and six sub-categories, and use it for leaderboard construction and LLM fine-tuning.", "result": "The dataset captures and classifies hallucinations in LLM-based OM tasks, demonstrating its utility for benchmarking and improving LLM performance.", "conclusion": "OAEI-LLM-T is a valuable resource for addressing hallucinations in LLM-based OM systems, aiding in both evaluation and model refinement."}}
{"id": "2503.21817", "pdf": "https://arxiv.org/pdf/2503.21817", "abs": "https://arxiv.org/abs/2503.21817", "authors": ["Weili Zeng", "Ziyuan Huang", "Kaixiang Ji", "Yichao Yan"], "title": "Skip-Vision: A Comprehensive Framework for Accelerating Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency.", "AI": {"task": "\u63d0\u51faSkip-Vision\u6846\u67b6\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u4e2d\u7684\u6548\u7387\u95ee\u9898\u3002", "motivation": "Transformer\u6a21\u578b\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u89c6\u89c9\u4ee4\u724c\u6570\u91cf\u6fc0\u589e\u662f\u4e3b\u8981\u74f6\u9888\u3002", "method": "\u7ed3\u5408\u4e24\u79cd\u52a0\u901f\u7b56\u7565\uff1a\u8bad\u7ec3\u65f6\u8df3\u8fc7\u5197\u4f59\u89c6\u89c9\u4ee4\u724c\u7684FFN\u8ba1\u7b97\uff08Skip-FFN\uff09\uff0c\u63a8\u7406\u65f6\u9009\u62e9\u6027\u79fb\u9664KV\u7f13\u5b58\u3002", "result": "\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1135%\uff0c\u63a8\u7406FLOPs\u51cf\u5c1175%\uff0c\u5ef6\u8fdf\u964d\u4f4e45%\uff0c\u6027\u80fd\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u4f18\u3002", "conclusion": "Skip-Vision\u4e3a\u9ad8\u6548\u6269\u5c55\u9ad8\u6027\u80fd\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2503.21819", "pdf": "https://arxiv.org/pdf/2503.21819", "abs": "https://arxiv.org/abs/2503.21819", "authors": ["Xuying Li", "Zhuo Li", "Yuji Kosuga", "Victor Bian"], "title": "Optimizing Safe and Aligned Language Generation: A Multi-Objective GRPO Approach", "categories": ["cs.CL"], "comment": null, "summary": "Aligning large language models (LLMs) with human values and safety\nconstraints is challenging, especially when objectives like helpfulness,\ntruthfulness, and avoidance of harm conflict. Reinforcement Learning from Human\nFeedback (RLHF) has achieved notable success in steering models, but is complex\nand can be unstable. Recent approaches such as Direct Preference Optimization\n(DPO) simplify preference-based fine-tuning but may introduce bias or trade-off\ncertain objectives~\\cite{dpo}. In this work, we propose a Group Relative Policy\nOptimization (GRPO) framework with a multi-label reward regression model to\nachieve safe and aligned language generation. The GRPO algorithm optimizes a\npolicy by comparing groups of sampled responses, eliminating the need for a\nseparate value critic and improving training efficiency~\\cite{grpo}. We train a\nreward model to predict multiple alignment scores (e.g., safety, helpfulness,\netc.), which are combined into a single reward signal. We provide a theoretical\nderivation for using this learned multi-aspect reward within GRPO and discuss\nits advantages and limitations. Empirically, our approach improves all the\nsafety and quality metrics evaluated in language generation tasks on model\nscales (0.5B, 7B, and 14B parameters), demonstrating a robust balance of\nobjectives. We compare GRPO to PPO-based RLHF and DPO, highlighting that GRPO\nachieves alignment with significantly lower computational cost and explicit\nmulti-objective handling. \\textbf{We will open-source all trained models at\nhttps://huggingface.co/hydroxai.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aGroup Relative Policy Optimization (GRPO)\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u73b0\u5b89\u5168\u548c\u5bf9\u9f50\u7684\u8bed\u8a00\u751f\u6210\u3002", "motivation": "\u7531\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4eba\u7c7b\u4ef7\u503c\u89c2\u548c\u5b89\u5168\u7ea6\u675f\u4e0a\u7684\u5bf9\u9f50\u5177\u6709\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u5728\u76ee\u6807\uff08\u5982\u5e2e\u52a9\u6027\u3001\u771f\u5b9e\u6027\u548c\u907f\u514d\u4f24\u5bb3\uff09\u51b2\u7a81\u65f6\uff0c\u73b0\u6709\u65b9\u6cd5\u5982RLHF\u548cDPO\u5b58\u5728\u590d\u6742\u6027\u548c\u6f5c\u5728\u504f\u5dee\u3002", "method": "GRPO\u6846\u67b6\u901a\u8fc7\u591a\u6807\u7b7e\u5956\u52b1\u56de\u5f52\u6a21\u578b\u4f18\u5316\u7b56\u7565\uff0c\u6bd4\u8f83\u54cd\u5e94\u7ec4\uff0c\u65e0\u9700\u5355\u72ec\u7684\u4ef7\u503c\u8bc4\u5224\u5668\uff0c\u5e76\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\u9884\u6d4b\u591a\u4e2a\u5bf9\u9f50\u5206\u6570\uff08\u5982\u5b89\u5168\u6027\u3001\u5e2e\u52a9\u6027\u7b49\uff09\u3002", "result": "GRPO\u5728\u8bed\u8a00\u751f\u6210\u4efb\u52a1\u4e2d\u63d0\u5347\u4e86\u6240\u6709\u5b89\u5168\u548c\u8d28\u91cf\u6307\u6807\uff0c\u5e76\u5728\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\uff080.5B\u30017B\u548c14B\u53c2\u6570\uff09\u4e0a\u5b9e\u73b0\u4e86\u76ee\u6807\u7684\u7a33\u5065\u5e73\u8861\u3002", "conclusion": "GRPO\u5728\u8ba1\u7b97\u6210\u672c\u548c\u663e\u5f0f\u591a\u76ee\u6807\u5904\u7406\u65b9\u9762\u4f18\u4e8ePPO-based RLHF\u548cDPO\uff0c\u5c55\u793a\u4e86\u9ad8\u6548\u7684\u5bf9\u9f50\u80fd\u529b\u3002"}}
{"id": "2503.21820", "pdf": "https://arxiv.org/pdf/2503.21820", "abs": "https://arxiv.org/abs/2503.21820", "authors": ["Yide Di", "Yun Liao", "Hao Zhou", "Kaijun Zhu", "Qing Duan", "Junhui Liu", "Mingyu Lu"], "title": "UFM: Unified Feature Matching Pre-training with Multi-Modal Image Assistants", "categories": ["cs.CV", "eess.IV"], "comment": "34 pages, 13 figures", "summary": "Image feature matching, a foundational task in computer vision, remains\nchallenging for multimodal image applications, often necessitating intricate\ntraining on specific datasets. In this paper, we introduce a Unified Feature\nMatching pre-trained model (UFM) designed to address feature matching\nchallenges across a wide spectrum of modal images. We present Multimodal Image\nAssistant (MIA) transformers, finely tunable structures adept at handling\ndiverse feature matching problems. UFM exhibits versatility in addressing both\nfeature matching tasks within the same modal and those across different modals.\nAdditionally, we propose a data augmentation algorithm and a staged\npre-training strategy to effectively tackle challenges arising from sparse data\nin specific modals and imbalanced modal datasets. Experimental results\ndemonstrate that UFM excels in generalization and performance across various\nfeature matching tasks. The code will be released\nat:https://github.com/LiaoYun0x0/UFM.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u7edf\u4e00\u7684\u7279\u5f81\u5339\u914d\u9884\u8bad\u7ec3\u6a21\u578b\uff08UFM\uff09\u4ee5\u89e3\u51b3\u591a\u6a21\u6001\u56fe\u50cf\u4e2d\u7684\u7279\u5f81\u5339\u914d\u95ee\u9898\u3002", "motivation": "\u591a\u6a21\u6001\u56fe\u50cf\u7684\u7279\u5f81\u5339\u914d\u4efb\u52a1\u590d\u6742\u4e14\u9700\u8981\u9488\u5bf9\u7279\u5b9a\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0c\u4e9f\u9700\u4e00\u79cd\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f15\u5165\u591a\u6a21\u6001\u56fe\u50cf\u52a9\u624b\uff08MIA\uff09\u53d8\u6362\u5668\u548c\u6570\u636e\u589e\u5f3a\u7b97\u6cd5\uff0c\u91c7\u7528\u5206\u9636\u6bb5\u9884\u8bad\u7ec3\u7b56\u7565\u3002", "result": "UFM\u5728\u591a\u79cd\u7279\u5f81\u5339\u914d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u3002", "conclusion": "UFM\u4e3a\u591a\u6a21\u6001\u56fe\u50cf\u7279\u5f81\u5339\u914d\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2503.21833", "pdf": "https://arxiv.org/pdf/2503.21833", "abs": "https://arxiv.org/abs/2503.21833", "authors": ["Alan Yang", "Yulin Chen", "Sean Lee", "Venus Montes"], "title": "Refining Time Series Anomaly Detectors using Large Language Models", "categories": ["cs.CL"], "comment": "Main content: 4 pages, 1 figure, 1 table", "summary": "Time series anomaly detection (TSAD) is of widespread interest across many\nindustries, including finance, healthcare, and manufacturing. Despite the\ndevelopment of numerous automatic methods for detecting anomalies, human\noversight remains necessary to review and act upon detected anomalies, as well\nas verify their accuracy. We study the use of multimodal large language models\n(LLMs) to partially automate this process. We find that LLMs can effectively\nidentify false alarms by integrating visual inspection of time series plots\nwith text descriptions of the data-generating process. By leveraging the\ncapabilities of LLMs, we aim to reduce the reliance on human effort required to\nmaintain a TSAD system", "AI": {"task": "\u7814\u7a76\u5982\u4f55\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u90e8\u5206\u81ea\u52a8\u5316\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\uff08TSAD\uff09\u4e2d\u7684\u4eba\u5de5\u5ba1\u6838\u8fc7\u7a0b\u3002", "motivation": "\u5c3d\u7ba1\u5df2\u6709\u591a\u79cd\u81ea\u52a8\u68c0\u6d4b\u5f02\u5e38\u7684\u65b9\u6cd5\uff0c\u4f46\u4ecd\u9700\u4eba\u5de5\u5ba1\u6838\u4ee5\u786e\u4fdd\u51c6\u786e\u6027\uff0c\u5e0c\u671b\u901a\u8fc7LLMs\u51cf\u5c11\u4eba\u5de5\u5de5\u4f5c\u91cf\u3002", "method": "\u5229\u7528\u591a\u6a21\u6001LLMs\uff0c\u7ed3\u5408\u65f6\u95f4\u5e8f\u5217\u56fe\u7684\u53ef\u89c6\u5316\u68c0\u67e5\u4e0e\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u7684\u6587\u672c\u63cf\u8ff0\uff0c\u8bc6\u522b\u8bef\u62a5\u3002", "result": "LLMs\u80fd\u6709\u6548\u8bc6\u522b\u8bef\u62a5\uff0c\u51cf\u5c11\u4eba\u5de5\u5ba1\u6838\u7684\u4f9d\u8d56\u3002", "conclusion": "\u591a\u6a21\u6001LLMs\u5728TSAD\u7cfb\u7edf\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u90e8\u5206\u66ff\u4ee3\u4eba\u5de5\u5ba1\u6838\uff0c\u63d0\u9ad8\u6548\u7387\u3002"}}
{"id": "2503.21823", "pdf": "https://arxiv.org/pdf/2503.21823", "abs": "https://arxiv.org/abs/2503.21823", "authors": ["Boan Zhang", "Hang Dong", "Jiongge Zhang", "Long Tian", "Rongrong Wang", "Zhenhua Wu", "Xiyang Liu", "Hongwei Liu"], "title": "Low-Rank Adaptation of Pre-Trained Stable Diffusion for Rigid-Body Target ISAR Imaging", "categories": ["cs.CV"], "comment": "4 pages, IGARSS 2025", "summary": "Traditional range-instantaneous Doppler (RID) methods for rigid-body target\nimaging often suffer from low resolution due to the limitations of\ntime-frequency analysis (TFA). To address this challenge, our primary focus is\non obtaining high resolution time-frequency representations (TFRs) from their\nlow resolution counterparts. Recognizing that the curve features of TFRs are a\nspecific type of texture feature, we argue that pre trained generative models\nsuch as Stable Diffusion (SD) are well suited for enhancing TFRs, thanks to\ntheir powerful capability in capturing texture representations. Building on\nthis insight, we propose a novel inverse synthetic aperture radar (ISAR)\nimaging method for rigid-body targets, leveraging the low-rank adaptation\n(LoRA) of a pre-trained SD model. Our approach adopts the basic structure and\npre-trained parameters of SD Turbo while incorporating additional linear\noperations for LoRA and adversarial training to achieve super-resolution and\nnoise suppression. Then we integrate LoRA-SD into the RID-based ISAR imaging,\nenabling sharply focused and denoised imaging with super-resolution\ncapabilities. We evaluate our method using both simulated and real radar data.\nThe experimental results demonstrate the superiority of our approach in\nfrequency es timation and ISAR imaging compared to traditional methods.\nNotably, the generalization capability is verified by training on simulated\nradar data and testing on measured radar data.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3\u751f\u6210\u6a21\u578bStable Diffusion\uff08SD\uff09\u548c\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u7684\u9006\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\uff08ISAR\uff09\u6210\u50cf\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u521a\u6027\u76ee\u6807\u6210\u50cf\u7684\u5206\u8fa8\u7387\u3002", "motivation": "\u4f20\u7edf\u7684\u77ac\u65f6\u591a\u666e\u52d2\uff08RID\uff09\u65b9\u6cd5\u56e0\u65f6\u9891\u5206\u6790\uff08TFA\uff09\u7684\u9650\u5236\u5bfc\u81f4\u5206\u8fa8\u7387\u8f83\u4f4e\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4ece\u4f4e\u5206\u8fa8\u7387\u65f6\u9891\u8868\u793a\uff08TFRs\uff09\u4e2d\u83b7\u53d6\u9ad8\u5206\u8fa8\u7387\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u7684SD\u6a21\u578b\u53ca\u5176LoRA\u6280\u672f\uff0c\u7ed3\u5408\u5bf9\u6297\u8bad\u7ec3\u548c\u7ebf\u6027\u64cd\u4f5c\uff0c\u5b9e\u73b0\u8d85\u5206\u8fa8\u7387\u548c\u566a\u58f0\u6291\u5236\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u57fa\u4e8eRID\u7684ISAR\u6210\u50cf\u4e2d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9891\u7387\u4f30\u8ba1\u548cISAR\u6210\u50cf\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e14\u901a\u8fc7\u6a21\u62df\u548c\u5b9e\u6d4b\u96f7\u8fbe\u6570\u636e\u9a8c\u8bc1\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684LoRA-SD\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u6210\u50cf\u7684\u5206\u8fa8\u7387\u548c\u53bb\u566a\u6548\u679c\uff0c\u9002\u7528\u4e8e\u521a\u6027\u76ee\u6807\u7684\u9ad8\u8d28\u91cfISAR\u6210\u50cf\u3002"}}
{"id": "2503.21838", "pdf": "https://arxiv.org/pdf/2503.21838", "abs": "https://arxiv.org/abs/2503.21838", "authors": ["Jiancheng Zhao", "Xingda Yu", "Zhen Yang"], "title": "MSPLoRA: A Multi-Scale Pyramid Low-Rank Adaptation for Efficient Model Fine-Tuning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Parameter-Efficient Fine-Tuning (PEFT) has become an essential approach for\nadapting large-scale pre-trained models while reducing computational costs.\nAmong PEFT methods, LoRA significantly reduces trainable parameters by\ndecomposing weight updates into low-rank matrices. However, traditional LoRA\napplies a fixed rank across all layers, failing to account for the varying\ncomplexity of hierarchical information, which leads to inefficient adaptation\nand redundancy. To address this, we propose MSPLoRA (Multi-Scale Pyramid LoRA),\nwhich introduces Global Shared LoRA, Mid-Level Shared LoRA, and Layer-Specific\nLoRA to capture global patterns, mid-level features, and fine-grained\ninformation, respectively. This hierarchical structure reduces inter-layer\nredundancy while maintaining strong adaptation capability. Experiments on\nvarious NLP tasks demonstrate that MSPLoRA achieves more efficient adaptation\nand better performance while significantly reducing the number of trainable\nparameters. Furthermore, additional analyses based on Singular Value\nDecomposition validate its information decoupling ability, highlighting MSPLoRA\nas a scalable and effective optimization strategy for parameter-efficient\nfine-tuning in large language models. Our code is available at\nhttps://github.com/Oblivioniss/MSPLoRA.", "AI": {"task": "\u63d0\u51faMSPLoRA\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u91d1\u5b57\u5854\u7ed3\u6784\u4f18\u5316LoRA\u5728\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u4f20\u7edfLoRA\u5728\u6240\u6709\u5c42\u4f7f\u7528\u56fa\u5b9a\u79e9\uff0c\u65e0\u6cd5\u9002\u5e94\u5c42\u6b21\u4fe1\u606f\u7684\u590d\u6742\u6027\uff0c\u5bfc\u81f4\u5197\u4f59\u548c\u4f4e\u6548\u3002", "method": "\u5f15\u5165\u5168\u5c40\u5171\u4eabLoRA\u3001\u4e2d\u5c42\u5171\u4eabLoRA\u548c\u5c42\u7279\u5b9aLoRA\uff0c\u5206\u522b\u6355\u6349\u5168\u5c40\u6a21\u5f0f\u3001\u4e2d\u5c42\u7279\u5f81\u548c\u7ec6\u7c92\u5ea6\u4fe1\u606f\u3002", "result": "\u5728\u591a\u79cdNLP\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u9ad8\u6548\u4e14\u6027\u80fd\u66f4\u597d\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u53ef\u8bad\u7ec3\u53c2\u6570\u3002", "conclusion": "MSPLoRA\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u4f18\u5316\u7b56\u7565\u3002"}}
{"id": "2503.21824", "pdf": "https://arxiv.org/pdf/2503.21824", "abs": "https://arxiv.org/abs/2503.21824", "authors": ["Haitong Liu", "Kuofeng Gao", "Yang Bai", "Jinmin Li", "Jinxiao Shan", "Tao Dai", "Shu-Tao Xia"], "title": "Protecting Your Video Content: Disrupting Automated Video-based LLM Annotations", "categories": ["cs.CV", "cs.CR"], "comment": "Accepted by CVPR 2025", "summary": "Recently, video-based large language models (video-based LLMs) have achieved\nimpressive performance across various video comprehension tasks. However, this\nrapid advancement raises significant privacy and security concerns,\nparticularly regarding the unauthorized use of personal video data in automated\nannotation by video-based LLMs. These unauthorized annotated video-text pairs\ncan then be used to improve the performance of downstream tasks, such as\ntext-to-video generation. To safeguard personal videos from unauthorized use,\nwe propose two series of protective video watermarks with imperceptible\nadversarial perturbations, named Ramblings and Mutes. Concretely, Ramblings aim\nto mislead video-based LLMs into generating inaccurate captions for the videos,\nthereby degrading the quality of video annotations through inconsistencies\nbetween video content and captions. Mutes, on the other hand, are designed to\nprompt video-based LLMs to produce exceptionally brief captions, lacking\ndescriptive detail. Extensive experiments demonstrate that our video\nwatermarking methods effectively protect video data by significantly reducing\nvideo annotation performance across various video-based LLMs, showcasing both\nstealthiness and robustness in protecting personal video content. Our code is\navailable at https://github.com/ttthhl/Protecting_Your_Video_Content.", "AI": {"task": "\u63d0\u51fa\u4e24\u79cd\u4fdd\u62a4\u4e2a\u4eba\u89c6\u9891\u6570\u636e\u514d\u53d7\u672a\u7ecf\u6388\u6743\u4f7f\u7528\u7684\u89c6\u9891\u6c34\u5370\u65b9\u6cd5\uff08Ramblings\u548cMutes\uff09\u3002", "motivation": "\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff08video-based LLMs\uff09\u7684\u5feb\u901f\u53d1\u5c55\u5f15\u53d1\u9690\u79c1\u548c\u5b89\u5168\u95ee\u9898\uff0c\u5c24\u5176\u662f\u672a\u7ecf\u6388\u6743\u4f7f\u7528\u4e2a\u4eba\u89c6\u9891\u6570\u636e\u8fdb\u884c\u81ea\u52a8\u6807\u6ce8\u7684\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e24\u79cd\u4e0d\u53ef\u5bdf\u89c9\u7684\u5bf9\u6297\u6027\u6270\u52a8\u89c6\u9891\u6c34\u5370\uff1aRamblings\u65e8\u5728\u8bef\u5bfc\u6a21\u578b\u751f\u6210\u4e0d\u51c6\u786e\u7684\u6807\u6ce8\uff0cMutes\u5219\u4fc3\u4f7f\u6a21\u578b\u751f\u6210\u6781\u7b80\u6807\u6ce8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u4e24\u79cd\u6c34\u5370\u65b9\u6cd5\u80fd\u663e\u8457\u964d\u4f4e\u89c6\u9891\u6807\u6ce8\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u9690\u853d\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u89c6\u9891\u6c34\u5370\u65b9\u6cd5\u6709\u6548\u4fdd\u62a4\u4e86\u4e2a\u4eba\u89c6\u9891\u5185\u5bb9\u514d\u53d7\u672a\u7ecf\u6388\u6743\u7684\u4f7f\u7528\u3002"}}
{"id": "2503.21888", "pdf": "https://arxiv.org/pdf/2503.21888", "abs": "https://arxiv.org/abs/2503.21888", "authors": ["Zeyad Alghamdi", "Tharindu Kumarage", "Garima Agrawal", "Mansooreh Karami", "Ibrahim Almuteb", "Huan Liu"], "title": "RedditESS: A Mental Health Social Support Interaction Dataset -- Understanding Effective Social Support to Refine AI-Driven Support Tools", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Effective mental health support is crucial for alleviating psychological\ndistress. While large language model (LLM)-based assistants have shown promise\nin mental health interventions, existing research often defines \"effective\"\nsupport primarily in terms of empathetic acknowledgments, overlooking other\nessential dimensions such as informational guidance, community validation, and\ntangible coping strategies. To address this limitation and better understand\nwhat constitutes effective support, we introduce RedditESS, a novel real-world\ndataset derived from Reddit posts, including supportive comments and original\nposters' follow-up responses. Grounded in established social science theories,\nwe develop an ensemble labeling mechanism to annotate supportive comments as\neffective or not and perform qualitative assessments to ensure the reliability\nof the annotations. Additionally, we demonstrate the practical utility of\nRedditESS by using it to guide LLM alignment toward generating more\ncontext-sensitive and genuinely helpful supportive responses. By broadening the\nunderstanding of effective support, our study paves the way for advanced\nAI-driven mental health interventions.", "AI": {"task": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7RedditESS\u6570\u636e\u96c6\u548c\u96c6\u6210\u6807\u6ce8\u673a\u5236\uff0c\u66f4\u5168\u9762\u5730\u5b9a\u4e49\u548c\u8bc4\u4f30\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5fc3\u7406\u5065\u5eb7\u652f\u6301\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9\u5fc3\u7406\u5065\u5eb7\u652f\u6301\u7684\u2018\u6709\u6548\u6027\u2019\u5b9a\u4e49\u8fc7\u4e8e\u72ed\u7a84\uff0c\u4ec5\u5173\u6ce8\u5171\u60c5\u56de\u5e94\uff0c\u5ffd\u7565\u4e86\u4fe1\u606f\u6307\u5bfc\u3001\u793e\u533a\u9a8c\u8bc1\u548c\u5177\u4f53\u5e94\u5bf9\u7b56\u7565\u7b49\u5176\u4ed6\u5173\u952e\u7ef4\u5ea6\u3002", "method": "\u5f15\u5165RedditESS\u6570\u636e\u96c6\uff0c\u57fa\u4e8e\u793e\u4f1a\u79d1\u5b66\u7406\u8bba\u5f00\u53d1\u96c6\u6210\u6807\u6ce8\u673a\u5236\uff0c\u6807\u6ce8\u652f\u6301\u6027\u8bc4\u8bba\u7684\u6709\u6548\u6027\uff0c\u5e76\u901a\u8fc7\u5b9a\u6027\u8bc4\u4f30\u786e\u4fdd\u6807\u6ce8\u53ef\u9760\u6027\u3002", "result": "RedditESS\u6570\u636e\u96c6\u6210\u529f\u7528\u4e8e\u6307\u5bfc\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u66f4\u5177\u60c5\u5883\u654f\u611f\u6027\u548c\u5b9e\u9645\u5e2e\u52a9\u7684\u652f\u6301\u6027\u56de\u5e94\u3002", "conclusion": "\u7814\u7a76\u6269\u5c55\u4e86\u5bf9\u6709\u6548\u652f\u6301\u7684\u7406\u89e3\uff0c\u4e3aAI\u9a71\u52a8\u7684\u5fc3\u7406\u5065\u5eb7\u5e72\u9884\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2503.21827", "pdf": "https://arxiv.org/pdf/2503.21827", "abs": "https://arxiv.org/abs/2503.21827", "authors": ["Mark Phil Pacot", "Jayno Juventud", "Gleen Dalaorao"], "title": "Hybrid Multi-Stage Learning Framework for Edge Detection: A Survey", "categories": ["cs.CV"], "comment": null, "summary": "Edge detection remains a fundamental yet challenging task in computer vision,\nespecially under varying illumination, noise, and complex scene conditions.\nThis paper introduces a Hybrid Multi-Stage Learning Framework that integrates\nConvolutional Neural Network (CNN) feature extraction with a Support Vector\nMachine (SVM) classifier to improve edge localization and structural accuracy.\nUnlike conventional end-to-end deep learning models, our approach decouples\nfeature representation and classification stages, enhancing robustness and\ninterpretability. Extensive experiments conducted on benchmark datasets such as\nBSDS500 and NYUDv2 demonstrate that the proposed framework outperforms\ntraditional edge detectors and even recent learning-based methods in terms of\nOptimal Dataset Scale (ODS) and Optimal Image Scale (OIS), while maintaining\ncompetitive Average Precision (AP). Both qualitative and quantitative results\nhighlight enhanced performance on edge continuity, noise suppression, and\nperceptual clarity achieved by our method. This work not only bridges classical\nand deep learning paradigms but also sets a new direction for scalable,\ninterpretable, and high-quality edge detection solutions.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u6df7\u5408\u591a\u9636\u6bb5\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408CNN\u7279\u5f81\u63d0\u53d6\u4e0eSVM\u5206\u7c7b\u5668\uff0c\u4ee5\u63d0\u9ad8\u8fb9\u7f18\u5b9a\u4f4d\u548c\u7ed3\u6784\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u5728\u53d8\u5316\u5149\u7167\u3001\u566a\u58f0\u548c\u590d\u6742\u573a\u666f\u6761\u4ef6\u4e0b\u8fb9\u7f18\u68c0\u6d4b\u7684\u6311\u6218\u6027\u95ee\u9898\u3002", "method": "\u91c7\u7528CNN\u7279\u5f81\u63d0\u53d6\u4e0eSVM\u5206\u7c7b\u5668\u7ed3\u5408\u7684\u6df7\u5408\u591a\u9636\u6bb5\u5b66\u4e60\u6846\u67b6\uff0c\u5206\u79bb\u7279\u5f81\u8868\u793a\u548c\u5206\u7c7b\u9636\u6bb5\u3002", "result": "\u5728BSDS500\u548cNYUDv2\u6570\u636e\u96c6\u4e0a\uff0cODS\u548cOIS\u6307\u6807\u4f18\u4e8e\u4f20\u7edf\u548c\u8fd1\u671f\u5b66\u4e60\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529bAP\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u8fde\u63a5\u4e86\u7ecf\u5178\u4e0e\u6df1\u5ea6\u5b66\u4e60\u8303\u5f0f\uff0c\u8fd8\u4e3a\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u4e14\u9ad8\u8d28\u91cf\u7684\u8fb9\u7f18\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2503.21910", "pdf": "https://arxiv.org/pdf/2503.21910", "abs": "https://arxiv.org/abs/2503.21910", "authors": ["Karima Kadaoui", "Hanin Atwany", "Hamdan Al-Ali", "Abdelrahman Mohamed", "Ali Mekky", "Sergei Tilga", "Natalia Fedorova", "Ekaterina Artemova", "Hanan Aldarmaki", "Yova Kementchedjhieva"], "title": "JEEM: Vision-Language Understanding in Four Arabic Dialects", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce JEEM, a benchmark designed to evaluate Vision-Language Models\n(VLMs) on visual understanding across four Arabic-speaking countries: Jordan,\nThe Emirates, Egypt, and Morocco. JEEM includes the tasks of image captioning\nand visual question answering, and features culturally rich and regionally\ndiverse content. This dataset aims to assess the ability of VLMs to generalize\nacross dialects and accurately interpret cultural elements in visual contexts.\nIn an evaluation of five prominent open-source Arabic VLMs and GPT-4V, we find\nthat the Arabic VLMs consistently underperform, struggling with both visual\nunderstanding and dialect-specific generation. While GPT-4V ranks best in this\ncomparison, the model's linguistic competence varies across dialects, and its\nvisual understanding capabilities lag behind. This underscores the need for\nmore inclusive models and the value of culturally-diverse evaluation paradigms.", "AI": {"task": "\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u56db\u4e2a\u963f\u62c9\u4f2f\u8bed\u56fd\u5bb6\uff08\u7ea6\u65e6\u3001\u963f\u8054\u914b\u3001\u57c3\u53ca\u548c\u6469\u6d1b\u54e5\uff09\u7684\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u901a\u8fc7\u5305\u542b\u6587\u5316\u4e30\u5bcc\u548c\u5730\u533a\u591a\u6837\u6027\u7684\u5185\u5bb9\uff0c\u8bc4\u4f30VLMs\u5728\u8de8\u65b9\u8a00\u548c\u6587\u5316\u5143\u7d20\u7406\u89e3\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u4f7f\u7528JEEM\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u62ec\u56fe\u50cf\u63cf\u8ff0\u548c\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\uff0c\u8bc4\u4f30\u4e94\u79cd\u5f00\u6e90\u963f\u62c9\u4f2f\u8bedVLMs\u548cGPT-4V\u3002", "result": "\u963f\u62c9\u4f2f\u8bedVLMs\u8868\u73b0\u4e0d\u4f73\uff0cGPT-4V\u8868\u73b0\u6700\u597d\u4f46\u4ecd\u6709\u65b9\u8a00\u548c\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u9700\u8981\u66f4\u5177\u5305\u5bb9\u6027\u7684\u6a21\u578b\u548c\u6587\u5316\u591a\u6837\u5316\u7684\u8bc4\u4f30\u8303\u5f0f\u3002"}}
{"id": "2503.21830", "pdf": "https://arxiv.org/pdf/2503.21830", "abs": "https://arxiv.org/abs/2503.21830", "authors": ["Maximilian Plattner", "Arturs Berzins", "Johannes Brandstetter"], "title": "Shape Generation via Weight Space Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Foundation models for 3D shape generation have recently shown a remarkable\ncapacity to encode rich geometric priors across both global and local\ndimensions. However, leveraging these priors for downstream tasks can be\nchallenging as real-world data are often scarce or noisy, and traditional\nfine-tuning can lead to catastrophic forgetting. In this work, we treat the\nweight space of a large 3D shape-generative model as a data modality that can\nbe explored directly. We hypothesize that submanifolds within this\nhigh-dimensional weight space can modulate topological properties or\nfine-grained part features separately, demonstrating early-stage evidence via\ntwo experiments. First, we observe a sharp phase transition in global\nconnectivity when interpolating in conditioning space, suggesting that small\nchanges in weight space can drastically alter topology. Second, we show that\nlow-dimensional reparameterizations yield controlled local geometry changes\neven with very limited data. These results highlight the potential of weight\nspace learning to unlock new approaches for 3D shape generation and specialized\nfine-tuning.", "AI": {"task": "\u63a2\u7d22\u5927\u578b3D\u5f62\u72b6\u751f\u6210\u6a21\u578b\u7684\u6743\u91cd\u7a7a\u95f4\u4f5c\u4e3a\u6570\u636e\u6a21\u6001\uff0c\u4ee5\u8c03\u5236\u62d3\u6251\u5c5e\u6027\u6216\u5c40\u90e8\u7279\u5f81\u3002", "motivation": "\u73b0\u5b9e\u6570\u636e\u7a00\u7f3a\u6216\u566a\u58f0\u591a\uff0c\u4f20\u7edf\u5fae\u8c03\u6613\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\uff0c\u9700\u65b0\u65b9\u6cd5\u5229\u7528\u51e0\u4f55\u5148\u9a8c\u3002", "method": "\u5c06\u6743\u91cd\u7a7a\u95f4\u89c6\u4e3a\u6570\u636e\u6a21\u6001\uff0c\u901a\u8fc7\u63d2\u503c\u548c\u4f4e\u7ef4\u91cd\u53c2\u6570\u5316\u5b9e\u9a8c\u9a8c\u8bc1\u5b50\u6d41\u5f62\u7684\u8c03\u5236\u80fd\u529b\u3002", "result": "\u6743\u91cd\u7a7a\u95f4\u63d2\u503c\u663e\u793a\u5168\u5c40\u8fde\u901a\u6027\u7a81\u53d8\uff0c\u4f4e\u7ef4\u91cd\u53c2\u6570\u5316\u80fd\u63a7\u5236\u5c40\u90e8\u51e0\u4f55\u53d8\u5316\u3002", "conclusion": "\u6743\u91cd\u7a7a\u95f4\u5b66\u4e60\u4e3a3D\u5f62\u72b6\u751f\u6210\u548c\u4e13\u7528\u5fae\u8c03\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2503.21911", "pdf": "https://arxiv.org/pdf/2503.21911", "abs": "https://arxiv.org/abs/2503.21911", "authors": ["Sayed Muddashir Hossain", "Simon Ostermann", "Patrick Gebhard", "Cord Benecke", "Josef van Genabith", "Philipp M\u00fcller"], "title": "AutoPsyC: Automatic Recognition of Psychodynamic Conflicts from Semi-structured Interviews with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Psychodynamic conflicts are persistent, often unconscious themes that shape a\nperson's behaviour and experiences. Accurate diagnosis of psychodynamic\nconflicts is crucial for effective patient treatment and is commonly done via\nlong, manually scored semi-structured interviews. Existing automated solutions\nfor psychiatric diagnosis tend to focus on the recognition of broad disorder\ncategories such as depression, and it is unclear to what extent psychodynamic\nconflicts which even the patient themselves may not have conscious access to\ncould be automatically recognised from conversation. In this paper, we propose\nAutoPsyC, the first method for recognising the presence and significance of\npsychodynamic conflicts from full-length Operationalized Psychodynamic\nDiagnostics (OPD) interviews using Large Language Models (LLMs). Our approach\ncombines recent advances in parameter-efficient fine-tuning and\nRetrieval-Augmented Generation (RAG) with a summarisation strategy to\neffectively process entire 90 minute long conversations. In evaluations on a\ndataset of 141 diagnostic interviews we show that AutoPsyC consistently\noutperforms all baselines and ablation conditions on the recognition of four\nhighly relevant psychodynamic conflicts.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aAutoPsyC\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4ece\u5b8c\u6574\u7684\u64cd\u4f5c\u6027\u5fc3\u7406\u52a8\u529b\u5b66\u8bca\u65ad\uff08OPD\uff09\u8bbf\u8c08\u4e2d\u8bc6\u522b\u5fc3\u7406\u52a8\u529b\u5b66\u51b2\u7a81\u7684\u5b58\u5728\u548c\u91cd\u8981\u6027\u3002", "motivation": "\u5fc3\u7406\u52a8\u529b\u5b66\u51b2\u7a81\u662f\u5f71\u54cd\u4e2a\u4f53\u884c\u4e3a\u548c\u4f53\u9a8c\u7684\u6301\u4e45\u4e14\u5e38\u4e3a\u65e0\u610f\u8bc6\u7684\u4e3b\u9898\uff0c\u5176\u51c6\u786e\u8bca\u65ad\u5bf9\u60a3\u8005\u6cbb\u7597\u81f3\u5173\u91cd\u8981\u3002\u76ee\u524d\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u591a\u5173\u6ce8\u5e7f\u6cdb\u969c\u788d\u7c7b\u522b\uff08\u5982\u6291\u90c1\u75c7\uff09\uff0c\u800c\u5fc3\u7406\u52a8\u529b\u5b66\u51b2\u7a81\u7684\u81ea\u52a8\u8bc6\u522b\u5c1a\u672a\u660e\u786e\u3002", "method": "\u7ed3\u5408\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u6458\u8981\u7b56\u7565\uff0c\u5904\u7406\u957f\u8fbe90\u5206\u949f\u7684\u5bf9\u8bdd\u3002", "result": "\u5728141\u6b21\u8bca\u65ad\u8bbf\u8c08\u6570\u636e\u96c6\u4e0a\uff0cAutoPsyC\u5728\u8bc6\u522b\u56db\u79cd\u9ad8\u5ea6\u76f8\u5173\u7684\u5fc3\u7406\u52a8\u529b\u5b66\u51b2\u7a81\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u548c\u6d88\u878d\u6761\u4ef6\u3002", "conclusion": "AutoPsyC\u662f\u9996\u4e2a\u4eceOPD\u8bbf\u8c08\u4e2d\u81ea\u52a8\u8bc6\u522b\u5fc3\u7406\u52a8\u529b\u5b66\u51b2\u7a81\u7684\u65b9\u6cd5\uff0c\u5c55\u73b0\u4e86\u5176\u6709\u6548\u6027\u548c\u6f5c\u529b\u3002"}}
{"id": "2503.21834", "pdf": "https://arxiv.org/pdf/2503.21834", "abs": "https://arxiv.org/abs/2503.21834", "authors": ["Haomin Yu", "Tianyi Li", "Kristian Torp", "Christian S. Jensen"], "title": "A Multi-Modal Knowledge-Enhanced Framework for Vessel Trajectory Prediction", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages, 5 figures", "summary": "Accurate vessel trajectory prediction facilitates improved navigational\nsafety, routing, and environmental protection. However, existing prediction\nmethods are challenged by the irregular sampling time intervals of the vessel\ntracking data from the global AIS system and the complexity of vessel movement.\nThese aspects render model learning and generalization difficult. To address\nthese challenges and improve vessel trajectory prediction, we propose the\nmulti-modal knowledge-enhanced framework (MAKER) for vessel trajectory\nprediction. To contend better with the irregular sampling time intervals, MAKER\nfeatures a Large language model-guided Knowledge Transfer (LKT) module that\nleverages pre-trained language models to transfer trajectory-specific\ncontextual knowledge effectively. To enhance the ability to learn complex\ntrajectory patterns, MAKER incorporates a Knowledge-based Self-paced Learning\n(KSL) module. This module employs kinematic knowledge to progressively\nintegrate complex patterns during training, allowing for adaptive learning and\nenhanced generalization. Experimental results on two vessel trajectory datasets\nshow that MAKER can improve the prediction accuracy of state-of-the-art methods\nby 12.08%-17.86%.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u591a\u6a21\u6001\u77e5\u8bc6\u589e\u5f3a\u6846\u67b6\uff08MAKER\uff09\u4ee5\u63d0\u9ad8\u8239\u8236\u8f68\u8ff9\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u9884\u6d4b\u65b9\u6cd5\u96be\u4ee5\u5904\u7406AIS\u6570\u636e\u7684\u4e0d\u89c4\u5219\u91c7\u6837\u65f6\u95f4\u95f4\u9694\u548c\u8239\u8236\u8fd0\u52a8\u7684\u590d\u6742\u6027\uff0c\u5bfc\u81f4\u6a21\u578b\u5b66\u4e60\u548c\u6cdb\u5316\u56f0\u96be\u3002", "method": "MAKER\u5305\u542b\u8bed\u8a00\u6a21\u578b\u5f15\u5bfc\u7684\u77e5\u8bc6\u8f6c\u79fb\u6a21\u5757\uff08LKT\uff09\u548c\u57fa\u4e8e\u77e5\u8bc6\u7684\u81ea\u6b65\u5b66\u4e60\u6a21\u5757\uff08KSL\uff09\uff0c\u5206\u522b\u5904\u7406\u4e0d\u89c4\u5219\u65f6\u95f4\u95f4\u9694\u548c\u590d\u6742\u8f68\u8ff9\u6a21\u5f0f\u3002", "result": "\u5728\u4e24\u4e2a\u8239\u8236\u8f68\u8ff9\u6570\u636e\u96c6\u4e0a\uff0cMAKER\u5c06\u9884\u6d4b\u51c6\u786e\u7387\u63d0\u9ad8\u4e8612.08%-17.86%\u3002", "conclusion": "MAKER\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u8239\u8236\u8f68\u8ff9\u9884\u6d4b\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2503.21927", "pdf": "https://arxiv.org/pdf/2503.21927", "abs": "https://arxiv.org/abs/2503.21927", "authors": ["Sahan Hewage Wewelwala", "T. G. D. K. Sumanathilaka"], "title": "Hybrid Emotion Recognition: Enhancing Customer Interactions Through Acoustic and Textual Analysis", "categories": ["cs.CL"], "comment": "5 pages, 1 figure, 2 tables", "summary": "This research presents a hybrid emotion recognition system integrating\nadvanced Deep Learning, Natural Language Processing (NLP), and Large Language\nModels (LLMs) to analyze audio and textual data for enhancing customer\ninteractions in contact centers. By combining acoustic features with textual\nsentiment analysis, the system achieves nuanced emotion detection, addressing\nthe limitations of traditional approaches in understanding complex emotional\nstates. Leveraging LSTM and CNN models for audio analysis and DistilBERT for\ntextual evaluation, the methodology accommodates linguistic and cultural\nvariations while ensuring real-time processing. Rigorous testing on diverse\ndatasets demonstrates the system's robustness and accuracy, highlighting its\npotential to transform customer service by enabling personalized, empathetic\ninteractions and improving operational efficiency. This research establishes a\nfoundation for more intelligent and human-centric digital communication,\nredefining customer service standards.", "AI": {"task": "\u5f00\u53d1\u4e00\u79cd\u6df7\u5408\u60c5\u611f\u8bc6\u522b\u7cfb\u7edf\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\uff0c\u5206\u6790\u97f3\u9891\u548c\u6587\u672c\u6570\u636e\u4ee5\u63d0\u5347\u5ba2\u6237\u4e92\u52a8\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u7406\u89e3\u590d\u6742\u60c5\u611f\u72b6\u6001\u65f6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u5347\u5ba2\u6237\u670d\u52a1\u7684\u4e2a\u6027\u5316\u548c\u540c\u7406\u5fc3\u3002", "method": "\u7ed3\u5408LSTM\u548cCNN\u6a21\u578b\u8fdb\u884c\u97f3\u9891\u5206\u6790\uff0c\u4f7f\u7528DistilBERT\u8fdb\u884c\u6587\u672c\u60c5\u611f\u5206\u6790\uff0c\u540c\u65f6\u8003\u8651\u8bed\u8a00\u548c\u6587\u5316\u5dee\u5f02\u3002", "result": "\u7cfb\u7edf\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u548c\u9ad8\u51c6\u786e\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u5b9e\u65f6\u5904\u7406\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u66f4\u667a\u80fd\u3001\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u6570\u5b57\u901a\u4fe1\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u91cd\u65b0\u5b9a\u4e49\u4e86\u5ba2\u6237\u670d\u52a1\u6807\u51c6\u3002"}}
{"id": "2503.21836", "pdf": "https://arxiv.org/pdf/2503.21836", "abs": "https://arxiv.org/abs/2503.21836", "authors": ["Ran Wei", "ZhiXiong Lan", "Qing Yan", "Ning Song", "Ming Lv", "LongQing Ye"], "title": "iMedImage Technical Report", "categories": ["cs.CV"], "comment": null, "summary": "Background: Chromosome karyotype analysis is crucial for diagnosing\nhereditary diseases, yet detecting structural abnormalities remains\nchallenging. While AI has shown promise in medical imaging, its effectiveness\nvaries across modalities. Leveraging advances in Foundation Models that\nintegrate multimodal medical imaging for robust feature extraction and accurate\ndiagnosis, we developed iMedImage, an end-to-end model for general medical\nimage recognition, demonstrating strong performance across multiple imaging\ntasks, including chromosome abnormality detection. Materials and Methods: We\nconstructed a comprehensive medical image dataset encompassing multiple\nmodalities from common medical domains, including chromosome, cell, pathology,\nultrasound, X-ray, CT, and MRI images. Based on this dataset, we developed the\niMedImage model, which incorporates the following key features: (1) a unified\nrepresentation method for diverse modality inputs and medical imaging tasks;\n(2) multi-level (case-level, image-level, patch-level) image recognition\ncapabilities enhanced by Chain of Thought (CoT) embedding and Mixture of\nExperts (MoE) strategies. Results: The test set comprised data from 12\ninstitutions across six regions in China, covering three mainstream scanning\ndevices, and included naturally distributed, unscreened abnormal cases. On this\ndiverse dataset, the model achieved a fully automated chromosome analysis\nworkflow, including segmentation, karyotyping, and abnormality detection,\nreaching a sensitivity of 92.75% and a specificity of 91.5%. Conclusion: We\npropose iMedImage, an end-to-end foundation model for medical image analysis,\ndemonstrating its superior performance across various medical imaging tasks.\niMedImage provides clinicians with a precise imaging analysis tool and\ncontributes to improving diagnostic accuracy and disease screening.", "AI": {"task": "\u5f00\u53d1\u4e00\u4e2a\u540d\u4e3aiMedImage\u7684\u7aef\u5230\u7aef\u57fa\u7840\u6a21\u578b\uff0c\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u6790\uff0c\u5305\u62ec\u67d3\u8272\u4f53\u5f02\u5e38\u68c0\u6d4b\u7b49\u4efb\u52a1\u3002", "motivation": "\u67d3\u8272\u4f53\u6838\u578b\u5206\u6790\u5bf9\u9057\u4f20\u75c5\u8bca\u65ad\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7ed3\u6784\u5f02\u5e38\u68c0\u6d4b\u4ecd\u5177\u6311\u6218\u6027\uff1bAI\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u8868\u73b0\u4e0d\u4e00\uff0c\u9700\u7ed3\u5408\u591a\u6a21\u6001\u533b\u5b66\u5f71\u50cf\u7684\u5148\u8fdb\u6280\u672f\u3002", "method": "\u6784\u5efa\u591a\u6a21\u6001\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5f00\u53d1iMedImage\u6a21\u578b\uff0c\u91c7\u7528\u7edf\u4e00\u8868\u793a\u65b9\u6cd5\u3001\u591a\u7ea7\u56fe\u50cf\u8bc6\u522b\u80fd\u529b\uff08\u6848\u4f8b\u7ea7\u3001\u56fe\u50cf\u7ea7\u3001\u5757\u7ea7\uff09\uff0c\u7ed3\u5408CoT\u5d4c\u5165\u548cMoE\u7b56\u7565\u3002", "result": "\u5728\u5305\u542b12\u5bb6\u673a\u6784\u6570\u636e\u7684\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u6a21\u578b\u5b9e\u73b0\u5168\u81ea\u52a8\u67d3\u8272\u4f53\u5206\u6790\u6d41\u7a0b\uff0c\u654f\u611f\u6027\u548c\u7279\u5f02\u6027\u5206\u522b\u8fbe92.75%\u548c91.5%\u3002", "conclusion": "iMedImage\u5728\u591a\u79cd\u533b\u5b66\u5f71\u50cf\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u4e34\u5e8a\u63d0\u4f9b\u7cbe\u51c6\u5206\u6790\u5de5\u5177\uff0c\u63d0\u5347\u8bca\u65ad\u51c6\u786e\u6027\u548c\u75be\u75c5\u7b5b\u67e5\u80fd\u529b\u3002"}}
{"id": "2503.21929", "pdf": "https://arxiv.org/pdf/2503.21929", "abs": "https://arxiv.org/abs/2503.21929", "authors": ["Tom Kempton", "Stuart Burrell"], "title": "Local Normalization Distortion and the Thermodynamic Formalism of Decoding Strategies for Large Language Models", "categories": ["cs.CL", "cs.LG", "math.DS"], "comment": null, "summary": "Advances in hardware and language model architecture have spurred a\nrevolution in natural language generation. However, autoregressive models\ncompute probability distributions over next-token choices, and sampling from\nthese distributions, known as decoding, has received significantly less\nattention than other design choices. Existing decoding strategies are largely\nbased on heuristics, resulting in methods that are hard to apply or improve in\na principled manner. We develop the theory of decoding strategies for language\nmodels by expressing popular decoding algorithms as equilibrium states in the\nlanguage of ergodic theory and stating the functions they optimize. Using this,\nwe analyze the effect of the local normalization step of top-k, nucleus, and\ntemperature sampling, used to make probabilities sum to one. We argue that\nlocal normalization distortion is a fundamental defect of decoding strategies\nand quantify the size of this distortion and its effect on mathematical proxies\nfor the quality and diversity of generated text. Contrary to the prevailing\nexplanation, we argue that the major cause of the under-performance of top-k\nsampling relative to nucleus sampling is local normalization distortion. This\nyields conclusions for the future design of decoding algorithms and the\ndetection of machine-generated text.", "AI": {"task": "\u63a2\u8ba8\u89e3\u7801\u7b56\u7565\u5bf9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6587\u672c\u8d28\u91cf\u4e0e\u591a\u6837\u6027\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u89e3\u7801\u7b56\u7565\u591a\u57fa\u4e8e\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u7f3a\u4e4f\u7406\u8bba\u652f\u6301\uff0c\u96be\u4ee5\u7cfb\u7edf\u6539\u8fdb\u3002", "method": "\u901a\u8fc7\u904d\u5386\u7406\u8bba\u5c06\u6d41\u884c\u89e3\u7801\u7b97\u6cd5\u8868\u8fbe\u4e3a\u5e73\u8861\u72b6\u6001\uff0c\u5e76\u5206\u6790\u5176\u4f18\u5316\u51fd\u6570\u3002", "result": "\u5c40\u90e8\u5f52\u4e00\u5316\u626d\u66f2\u662f\u89e3\u7801\u7b56\u7565\u7684\u6839\u672c\u7f3a\u9677\uff0c\u5f71\u54cd\u4e86\u751f\u6210\u6587\u672c\u7684\u8d28\u91cf\u4e0e\u591a\u6837\u6027\u3002", "conclusion": "\u672a\u6765\u89e3\u7801\u7b97\u6cd5\u8bbe\u8ba1\u9700\u5173\u6ce8\u5c40\u90e8\u5f52\u4e00\u5316\u95ee\u9898\uff0c\u5e76\u6539\u8fdb\u673a\u5668\u751f\u6210\u6587\u672c\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002"}}
{"id": "2503.21839", "pdf": "https://arxiv.org/pdf/2503.21839", "abs": "https://arxiv.org/abs/2503.21839", "authors": ["Haolong Yan", "Kaijun Tan", "Yeqing Shen", "Xin Huang", "Zheng Ge", "Xiangyu Zhang", "Si Li", "Daxin Jiang"], "title": "M-DocSum: Do LVLMs Genuinely Comprehend Interleaved Image-Text in Document Summarization?", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We investigate a critical yet under-explored question in Large\nVision-Language Models (LVLMs): Do LVLMs genuinely comprehend interleaved\nimage-text in the document? Existing document understanding benchmarks often\nassess LVLMs using question-answer formats, which are information-sparse and\ndifficult to guarantee the coverage of long-range dependencies. To address this\nissue, we introduce a novel and challenging Multimodal Document Summarization\nBenchmark (M-DocSum-Bench), which comprises 500 high-quality arXiv papers,\nalong with interleaved multimodal summaries aligned with human preferences.\nM-DocSum-Bench is a reference-based generation task and necessitates the\ngeneration of interleaved image-text summaries using provided reference images,\nthereby simultaneously evaluating capabilities in understanding, reasoning,\nlocalization, and summarization within complex multimodal document scenarios.\nTo facilitate this benchmark, we develop an automated framework to construct\nsummaries and propose a fine-grained evaluation method called M-DocEval.\nMoreover, we further develop a robust summarization baseline, i.e.,\nM-DocSum-7B, by progressive two-stage training with diverse instruction and\npreference data. The extensive results on our M-DocSum-Bench reveal that the\nleading LVLMs struggle to maintain coherence and accurately integrate\ninformation within long and interleaved contexts, often exhibiting confusion\nbetween similar images and a lack of robustness. Notably, M-DocSum-7B achieves\nstate-of-the-art performance compared to larger and closed-source models\n(including GPT-4o, Gemini Pro, Claude-3.5-Sonnet and Qwen2.5-VL-72B, etc.),\ndemonstrating the potential of LVLMs for improved interleaved image-text\nunderstanding. The code, data, and models are available at\nhttps://github.com/stepfun-ai/M-DocSum-Bench.", "AI": {"task": "\u7814\u7a76\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u662f\u5426\u771f\u6b63\u7406\u89e3\u6587\u6863\u4e2d\u4ea4\u9519\u7684\u56fe\u50cf-\u6587\u672c\u5185\u5bb9\u3002", "motivation": "\u73b0\u6709\u6587\u6863\u7406\u89e3\u57fa\u51c6\u901a\u5e38\u4f7f\u7528\u95ee\u7b54\u683c\u5f0f\u8bc4\u4f30LVLMs\uff0c\u8fd9\u79cd\u683c\u5f0f\u4fe1\u606f\u7a00\u758f\u4e14\u96be\u4ee5\u8986\u76d6\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u5f15\u5165\u591a\u6a21\u6001\u6587\u6863\u6458\u8981\u57fa\u51c6\uff08M-DocSum-Bench\uff09\uff0c\u5305\u542b500\u7bc7\u9ad8\u8d28\u91cfarXiv\u8bba\u6587\u53ca\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u7684\u4ea4\u9519\u591a\u6a21\u6001\u6458\u8981\uff0c\u5e76\u63d0\u51fa\u81ea\u52a8\u5316\u6846\u67b6\u548c\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u65b9\u6cd5M-DocEval\u3002", "result": "\u9886\u5148\u7684LVLMs\u5728\u957f\u4e14\u4ea4\u9519\u7684\u4e0a\u4e0b\u6587\u4e2d\u96be\u4ee5\u4fdd\u6301\u8fde\u8d2f\u6027\u548c\u51c6\u786e\u6574\u5408\u4fe1\u606f\uff0c\u800c\u63d0\u51fa\u7684M-DocSum-7B\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u5927\u578b\u95ed\u6e90\u6a21\u578b\u3002", "conclusion": "M-DocSum-7B\u5c55\u793a\u4e86LVLMs\u5728\u6539\u8fdb\u4ea4\u9519\u56fe\u50cf-\u6587\u672c\u7406\u89e3\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2503.21934", "pdf": "https://arxiv.org/pdf/2503.21934", "abs": "https://arxiv.org/abs/2503.21934", "authors": ["Ivo Petrov", "Jasper Dekoninck", "Lyuben Baltadzhiev", "Maria Drencheva", "Kristian Minchev", "Mislav Balunovi\u0107", "Nikola Jovanovi\u0107", "Martin Vechev"], "title": "Proof or Bluff? Evaluating LLMs on 2025 USA Math Olympiad", "categories": ["cs.CL"], "comment": null, "summary": "Recent math benchmarks for large language models (LLMs) such as MathArena\nindicate that state-of-the-art reasoning models achieve impressive performance\non mathematical competitions like AIME, with the leading model, o3-mini,\nachieving scores comparable to top human competitors. However, these benchmarks\nevaluate models solely based on final numerical answers, neglecting rigorous\nreasoning and proof generation which are essential for real-world mathematical\ntasks. To address this, we introduce the first comprehensive evaluation of\nfull-solution reasoning for challenging mathematical problems. Using expert\nhuman annotators, we evaluated several state-of-the-art reasoning models on the\nsix problems from the 2025 USAMO within hours of their release. Our results\nreveal that all tested models struggled significantly, achieving less than 5%\non average. Through detailed analysis of reasoning traces, we identify the most\ncommon failure modes and find several unwanted artifacts arising from the\noptimization strategies employed during model training. Overall, our results\nsuggest that current LLMs are inadequate for rigorous mathematical reasoning\ntasks, highlighting the need for substantial improvements in reasoning and\nproof generation capabilities.", "AI": {"task": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u7ade\u8d5b\u95ee\u9898\u4e2d\u7684\u5b8c\u6574\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6570\u5b66\u57fa\u51c6\u4ec5\u5173\u6ce8\u6700\u7ec8\u6570\u503c\u7b54\u6848\uff0c\u5ffd\u7565\u4e86\u4e25\u8c28\u7684\u63a8\u7406\u548c\u8bc1\u660e\u751f\u6210\u80fd\u529b\uff0c\u800c\u8fd9\u4e9b\u80fd\u529b\u5bf9\u5b9e\u9645\u6570\u5b66\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u4e13\u5bb6\u6807\u6ce8\u76842025\u5e74USAMO\u516d\u9053\u9898\u76ee\uff0c\u5bf9\u591a\u4e2a\u5148\u8fdb\u63a8\u7406\u6a21\u578b\u8fdb\u884c\u5b8c\u6574\u89e3\u51b3\u65b9\u6848\u7684\u8bc4\u4f30\u3002", "result": "\u6240\u6709\u6d4b\u8bd5\u6a21\u578b\u8868\u73b0\u663e\u8457\u4e0d\u8db3\uff0c\u5e73\u5747\u5f97\u5206\u4f4e\u4e8e5%\u3002\u901a\u8fc7\u5206\u6790\u63a8\u7406\u8fc7\u7a0b\uff0c\u53d1\u73b0\u4e86\u5e38\u89c1\u7684\u5931\u8d25\u6a21\u5f0f\u548c\u8bad\u7ec3\u7b56\u7565\u5e26\u6765\u7684\u4e0d\u826f\u5f71\u54cd\u3002", "conclusion": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e25\u8c28\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u9700\u5927\u5e45\u63d0\u5347\u63a8\u7406\u548c\u8bc1\u660e\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2503.21841", "pdf": "https://arxiv.org/pdf/2503.21841", "abs": "https://arxiv.org/abs/2503.21841", "authors": ["Jingtao Li", "Yingyi Liu", "Xinyu Wang", "Yunning Peng", "Chen Sun", "Shaoyu Wang", "Zhendong Sun", "Tian Ke", "Xiao Jiang", "Tangwei Lu", "Anran Zhao", "Yanfei Zhong"], "title": "HyperFree: A Channel-adaptive and Tuning-free Foundation Model for Hyperspectral Remote Sensing Imagery", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Advanced interpretation of hyperspectral remote sensing images benefits many\nprecise Earth observation tasks. Recently, visual foundation models have\npromoted the remote sensing interpretation but concentrating on RGB and\nmultispectral images. Due to the varied hyperspectral channels,existing\nfoundation models would face image-by-image tuning situation, imposing great\npressure on hardware and time resources. In this paper, we propose a\ntuning-free hyperspectral foundation model called HyperFree, by adapting the\nexisting visual prompt engineering. To process varied channel numbers, we\ndesign a learned weight dictionary covering full-spectrum from $0.4 \\sim 2.5 \\,\n\\mu\\text{m}$, supporting to build the embedding layer dynamically. To make the\nprompt design more tractable, HyperFree can generate multiple semantic-aware\nmasks for one prompt by treating feature distance as semantic-similarity. After\npre-training HyperFree on constructed large-scale high-resolution hyperspectral\nimages, HyperFree (1 prompt) has shown comparable results with specialized\nmodels (5 shots) on 5 tasks and 11 datasets.Code and dataset are accessible at\nhttps://rsidea.whu.edu.cn/hyperfree.htm.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8c03\u4f18\u7684\u9ad8\u5149\u8c31\u57fa\u7840\u6a21\u578bHyperFree\uff0c\u7528\u4e8e\u5904\u7406\u9ad8\u5149\u8c31\u9065\u611f\u56fe\u50cf\u7684\u7cbe\u786e\u89e3\u8bd1\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4e3b\u8981\u9488\u5bf9RGB\u548c\u591a\u5149\u8c31\u56fe\u50cf\uff0c\u800c\u9ad8\u5149\u8c31\u56fe\u50cf\u56e0\u901a\u9053\u591a\u53d8\u5bfc\u81f4\u6a21\u578b\u9700\u8981\u9010\u56fe\u8c03\u4f18\uff0c\u589e\u52a0\u4e86\u786c\u4ef6\u548c\u65f6\u95f4\u8d44\u6e90\u538b\u529b\u3002", "method": "\u901a\u8fc7\u9002\u914d\u73b0\u6709\u89c6\u89c9\u63d0\u793a\u5de5\u7a0b\uff0c\u8bbe\u8ba1\u4e00\u4e2a\u8986\u76d6\u5168\u5149\u8c31\u7684\u5b66\u4e60\u6743\u91cd\u5b57\u5178\uff0c\u52a8\u6001\u6784\u5efa\u5d4c\u5165\u5c42\uff0c\u5e76\u751f\u6210\u591a\u4e2a\u8bed\u4e49\u611f\u77e5\u63a9\u7801\u3002", "result": "\u57285\u4e2a\u4efb\u52a1\u548c11\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cHyperFree\uff081\u63d0\u793a\uff09\u8868\u73b0\u4e0e\u4e13\u7528\u6a21\u578b\uff085\u6837\u672c\uff09\u76f8\u5f53\u3002", "conclusion": "HyperFree\u4e3a\u9ad8\u5149\u8c31\u9065\u611f\u56fe\u50cf\u89e3\u8bd1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u8c03\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2503.21961", "pdf": "https://arxiv.org/pdf/2503.21961", "abs": "https://arxiv.org/abs/2503.21961", "authors": ["Xianzhi Li", "Ethan Callanan", "Xiaodan Zhu", "Mathieu Sibue", "Antony Papadimitriou", "Mahmoud Mahfouz", "Zhiqiang Ma", "Xiaomo Liu"], "title": "Entropy-Aware Branching for Improved Mathematical Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While Large Language Models (LLMs) are effectively aligned through extensive\npre-training and fine-tuning, they still struggle with varying levels of\nuncertainty during token generation. In our investigation of mathematical\nreasoning, we observe that errors are more likely to arise at tokens exhibiting\nhigh entropy and variance of entropy in the model's output distribution. Based\non the observation, we propose a novel approach that dynamically branches the\ngeneration process on demand instead of defaulting to the single most probable\ntoken. By exploring in parallel multiple branches stemming from high\nprobability tokens of critical decision points, the model can discover diverse\nreasoning paths that might otherwise be missed. We further harness external\nfeedback from larger models to rank and select the most coherent and accurate\nreasoning branch. Our experimental results on mathematical word problems and\ncalculation questions show that this branching strategy boosts the reasoning\ncapabilities of small LLMs up to 4.6% compared to conventional argmax decoding.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u52a8\u6001\u5206\u652f\u751f\u6210\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u71b5\u548c\u9ad8\u71b5\u65b9\u5dee\u7684\u6807\u8bb0\u5904\u5bb9\u6613\u51fa\u9519\u3002", "method": "\u901a\u8fc7\u52a8\u6001\u5206\u652f\u751f\u6210\uff0c\u5e76\u884c\u63a2\u7d22\u591a\u4e2a\u9ad8\u6982\u7387\u6807\u8bb0\u7684\u8def\u5f84\uff0c\u5e76\u5229\u7528\u5916\u90e8\u53cd\u9988\u9009\u62e9\u6700\u4f73\u63a8\u7406\u5206\u652f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6570\u5b66\u5e94\u7528\u9898\u548c\u8ba1\u7b97\u9898\u4e0a\u6bd4\u4f20\u7edfargmax\u89e3\u7801\u65b9\u6cd5\u63d0\u5347\u4e864.6%\u3002", "conclusion": "\u52a8\u6001\u5206\u652f\u751f\u6210\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2503.21843", "pdf": "https://arxiv.org/pdf/2503.21843", "abs": "https://arxiv.org/abs/2503.21843", "authors": ["Hanyu Liu", "Siyao Li", "Ying Yu", "Yixuan Jiang", "Hang Xiao", "Jingxi Long", "Haotian Tang"], "title": "CMD-HAR: Cross-Modal Disentanglement for Wearable Human Activity Recognition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Human Activity Recognition (HAR) is a fundamental technology for numerous\nhuman - centered intelligent applications. Although deep learning methods have\nbeen utilized to accelerate feature extraction, issues such as multimodal data\nmixing, activity heterogeneity, and complex model deployment remain largely\nunresolved. The aim of this paper is to address issues such as multimodal data\nmixing, activity heterogeneity, and complex model deployment in sensor-based\nhuman activity recognition. We propose a spatiotemporal attention modal\ndecomposition alignment fusion strategy to tackle the problem of the mixed\ndistribution of sensor data. Key discriminative features of activities are\ncaptured through cross-modal spatio-temporal disentangled representation, and\ngradient modulation is combined to alleviate data heterogeneity. In addition, a\nwearable deployment simulation system is constructed. We conducted experiments\non a large number of public datasets, demonstrating the effectiveness of the\nmodel.", "AI": {"task": "\u89e3\u51b3\u4f20\u611f\u5668\u6570\u636e\u6df7\u5408\u5206\u5e03\u3001\u6d3b\u52a8\u5f02\u8d28\u6027\u548c\u590d\u6742\u6a21\u578b\u90e8\u7f72\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5df2\u7528\u4e8e\u52a0\u901f\u7279\u5f81\u63d0\u53d6\uff0c\u4f46\u591a\u6a21\u6001\u6570\u636e\u6df7\u5408\u3001\u6d3b\u52a8\u5f02\u8d28\u6027\u548c\u590d\u6742\u6a21\u578b\u90e8\u7f72\u95ee\u9898\u4ecd\u672a\u89e3\u51b3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65f6\u7a7a\u6ce8\u610f\u529b\u6a21\u6001\u5206\u89e3\u5bf9\u9f50\u878d\u5408\u7b56\u7565\uff0c\u7ed3\u5408\u68af\u5ea6\u8c03\u5236\u548c\u53ef\u7a7f\u6234\u90e8\u7f72\u6a21\u62df\u7cfb\u7edf\u3002", "result": "\u5728\u5927\u91cf\u516c\u5171\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u611f\u5668\u6570\u636e\u6df7\u5408\u5206\u5e03\u548c\u6d3b\u52a8\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u5e76\u9a8c\u8bc1\u4e86\u90e8\u7f72\u53ef\u884c\u6027\u3002"}}
{"id": "2503.22000", "pdf": "https://arxiv.org/pdf/2503.22000", "abs": "https://arxiv.org/abs/2503.22000", "authors": ["Andr\u00e1s Kornai"], "title": "Cluster automata", "categories": ["cs.CL", "cs.FL"], "comment": "Submitted to MOL2025", "summary": "We introduce a new class of clustered Moore automata (CMA), investigate their\ntemporal behavior, and describe some applications.", "AI": {"task": "\u4ecb\u7ecd\u5e76\u7814\u7a76\u4e00\u7c7b\u65b0\u7684\u805a\u7c7b\u6469\u5c14\u81ea\u52a8\u673a\uff08CMA\uff09\u53ca\u5176\u65f6\u95f4\u884c\u4e3a\u3002", "motivation": "\u63a2\u7d22\u805a\u7c7b\u6469\u5c14\u81ea\u52a8\u673a\u7684\u6f5c\u5728\u5e94\u7528\u53ca\u5176\u65f6\u95f4\u884c\u4e3a\u7279\u6027\u3002", "method": "\u7814\u7a76\u805a\u7c7b\u6469\u5c14\u81ea\u52a8\u673a\u7684\u5b9a\u4e49\u53ca\u5176\u65f6\u95f4\u884c\u4e3a\u3002", "result": "\u63cf\u8ff0\u4e86\u805a\u7c7b\u6469\u5c14\u81ea\u52a8\u673a\u7684\u4e00\u4e9b\u5e94\u7528\u3002", "conclusion": "\u805a\u7c7b\u6469\u5c14\u81ea\u52a8\u673a\u53ca\u5176\u65f6\u95f4\u884c\u4e3a\u5177\u6709\u6f5c\u5728\u7684\u7814\u7a76\u548c\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2503.21848", "pdf": "https://arxiv.org/pdf/2503.21848", "abs": "https://arxiv.org/abs/2503.21848", "authors": ["Jonathan Attard", "Dylan Seychell"], "title": "Comparative Analysis of Image, Video, and Audio Classifiers for Automated News Video Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "Preprint for paper in CAI 2025, 7 pages, 5 tables, 3 tables", "summary": "News videos require efficient content organisation and retrieval systems, but\ntheir unstructured nature poses significant challenges for automated\nprocessing. This paper presents a comprehensive comparative analysis of image,\nvideo, and audio classifiers for automated news video segmentation. This work\npresents the development and evaluation of multiple deep learning approaches,\nincluding ResNet, ViViT, AST, and multimodal architectures, to classify five\ndistinct segment types: advertisements, stories, studio scenes, transitions,\nand visualisations. Using a custom-annotated dataset of 41 news videos\ncomprising 1,832 scene clips, our experiments demonstrate that image-based\nclassifiers achieve superior performance (84.34\\% accuracy) compared to more\ncomplex temporal models. Notably, the ResNet architecture outperformed\nstate-of-the-art video classifiers while requiring significantly fewer\ncomputational resources. Binary classification models achieved high accuracy\nfor transitions (94.23\\%) and advertisements (92.74\\%). These findings advance\nthe understanding of effective architectures for news video segmentation and\nprovide practical insights for implementing automated content organisation\nsystems in media applications. These include media archiving, personalised\ncontent delivery, and intelligent video search.", "AI": {"task": "\u6bd4\u8f83\u56fe\u50cf\u3001\u89c6\u9891\u548c\u97f3\u9891\u5206\u7c7b\u5668\u5728\u65b0\u95fb\u89c6\u9891\u81ea\u52a8\u5206\u5272\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u65b0\u95fb\u89c6\u9891\u7684\u975e\u7ed3\u6784\u5316\u7279\u6027\u4e3a\u81ea\u52a8\u5316\u5904\u7406\u5e26\u6765\u6311\u6218\uff0c\u9700\u8981\u9ad8\u6548\u7684\u5185\u5bb9\u7ec4\u7ec7\u548c\u68c0\u7d22\u7cfb\u7edf\u3002", "method": "\u5f00\u53d1\u5e76\u8bc4\u4f30\u4e86\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff08\u5982ResNet\u3001ViViT\u3001AST\u548c\u591a\u6a21\u6001\u67b6\u6784\uff09\uff0c\u7528\u4e8e\u5206\u7c7b\u4e94\u79cd\u7247\u6bb5\u7c7b\u578b\u3002", "result": "\u57fa\u4e8e\u56fe\u50cf\u7684\u5206\u7c7b\u5668\u6027\u80fd\u6700\u4f18\uff0884.34%\u51c6\u786e\u7387\uff09\uff0cResNet\u67b6\u6784\u5728\u8ba1\u7b97\u8d44\u6e90\u8f83\u5c11\u7684\u60c5\u51b5\u4e0b\u4f18\u4e8e\u5176\u4ed6\u89c6\u9891\u5206\u7c7b\u5668\u3002", "conclusion": "\u7814\u7a76\u4e3a\u65b0\u95fb\u89c6\u9891\u5206\u5272\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u67b6\u6784\u9009\u62e9\uff0c\u5e76\u4e3a\u5a92\u4f53\u5e94\u7528\u4e2d\u7684\u81ea\u52a8\u5316\u5185\u5bb9\u7ec4\u7ec7\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002"}}
{"id": "2503.22002", "pdf": "https://arxiv.org/pdf/2503.22002", "abs": "https://arxiv.org/abs/2503.22002", "authors": ["Stephanie Schoch", "Yangfeng Ji"], "title": "Monte Carlo Sampling for Analyzing In-Context Examples", "categories": ["cs.CL"], "comment": "Accepted to the Workshop for Insights from Negative Results\n  (co-located with NAACL 2025)", "summary": "Prior works have shown that in-context learning is brittle to presentation\nfactors such as the order, number, and choice of selected examples. However,\nablation-based guidance on selecting the number of examples may ignore the\ninterplay between different presentation factors. In this work we develop a\nMonte Carlo sampling-based method to study the impact of number of examples\nwhile explicitly accounting for effects from order and selected examples. We\nfind that previous guidance on how many in-context examples to select does not\nalways generalize across different sets of selected examples and orderings, and\nwhether one-shot settings outperform zero-shot settings is highly dependent on\nthe selected example. Additionally, inspired by data valuation, we apply our\nsampling method to in-context example selection to select examples that perform\nwell across different orderings. We find a negative result, that while\nperformance is robust to ordering and number of examples, there is an\nunexpected performance degradation compared to random sampling.", "AI": {"task": "\u7814\u7a76\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u793a\u4f8b\u6570\u91cf\u3001\u987a\u5e8f\u548c\u9009\u62e9\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u5148\u524d\u7814\u7a76\u8868\u660e\u4e0a\u4e0b\u6587\u5b66\u4e60\u5bf9\u793a\u4f8b\u7684\u5c55\u793a\u65b9\u5f0f\uff08\u5982\u987a\u5e8f\u3001\u6570\u91cf\u548c\u9009\u62e9\uff09\u654f\u611f\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u53ef\u80fd\u5ffd\u7565\u4e86\u8fd9\u4e9b\u56e0\u7d20\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "method": "\u91c7\u7528\u8499\u7279\u5361\u6d1b\u91c7\u6837\u65b9\u6cd5\uff0c\u540c\u65f6\u8003\u8651\u793a\u4f8b\u6570\u91cf\u3001\u987a\u5e8f\u548c\u9009\u62e9\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u5148\u524d\u5173\u4e8e\u793a\u4f8b\u6570\u91cf\u7684\u6307\u5bfc\u5728\u4e0d\u540c\u793a\u4f8b\u96c6\u548c\u987a\u5e8f\u4e0b\u5e76\u4e0d\u901a\u7528\uff0c\u4e14\u5355\u793a\u4f8b\u4e0e\u96f6\u793a\u4f8b\u7684\u6027\u80fd\u5bf9\u6bd4\u9ad8\u5ea6\u4f9d\u8d56\u6240\u9009\u793a\u4f8b\u3002\u6b64\u5916\uff0c\u793a\u4f8b\u9009\u62e9\u65b9\u6cd5\u672a\u80fd\u4f18\u4e8e\u968f\u673a\u91c7\u6837\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u6027\u80fd\u5bf9\u793a\u4f8b\u987a\u5e8f\u548c\u6570\u91cf\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4f46\u793a\u4f8b\u9009\u62e9\u65b9\u6cd5\u672a\u5e26\u6765\u9884\u671f\u6539\u8fdb\u3002"}}
{"id": "2503.21851", "pdf": "https://arxiv.org/pdf/2503.21851", "abs": "https://arxiv.org/abs/2503.21851", "authors": ["Alessandro Conti", "Massimiliano Mancini", "Enrico Fini", "Yiming Wang", "Paolo Rota", "Elisa Ricci"], "title": "On Large Multimodal Models as Open-World Image Classifiers", "categories": ["cs.CV"], "comment": "23 pages, 13 figures, code is available at\n  https://github.com/altndrr/lmms-owc", "summary": "Traditional image classification requires a predefined list of semantic\ncategories. In contrast, Large Multimodal Models (LMMs) can sidestep this\nrequirement by classifying images directly using natural language (e.g.,\nanswering the prompt \"What is the main object in the image?\"). Despite this\nremarkable capability, most existing studies on LMM classification performance\nare surprisingly limited in scope, often assuming a closed-world setting with a\npredefined set of categories. In this work, we address this gap by thoroughly\nevaluating LMM classification performance in a truly open-world setting. We\nfirst formalize the task and introduce an evaluation protocol, defining various\nmetrics to assess the alignment between predicted and ground truth classes. We\nthen evaluate 13 models across 10 benchmarks, encompassing prototypical,\nnon-prototypical, fine-grained, and very fine-grained classes, demonstrating\nthe challenges LMMs face in this task. Further analyses based on the proposed\nmetrics reveal the types of errors LMMs make, highlighting challenges related\nto granularity and fine-grained capabilities, showing how tailored prompting\nand reasoning can alleviate them.", "AI": {"task": "\u8bc4\u4f30\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u5728\u5f00\u653e\u4e16\u754c\u8bbe\u7f6e\u4e0b\u7684\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5927\u591a\u5c40\u9650\u4e8e\u5c01\u95ed\u4e16\u754c\u8bbe\u7f6e\uff0c\u7f3a\u4e4f\u5bf9LMMs\u5728\u5f00\u653e\u4e16\u754c\u5206\u7c7b\u6027\u80fd\u7684\u5168\u9762\u8bc4\u4f30\u3002", "method": "\u63d0\u51fa\u4efb\u52a1\u5f62\u5f0f\u5316\u548c\u8bc4\u4f30\u534f\u8bae\uff0c\u5b9a\u4e49\u591a\u79cd\u6307\u6807\u8bc4\u4f30\u9884\u6d4b\u4e0e\u771f\u5b9e\u7c7b\u522b\u7684\u5bf9\u9f50\uff0c\u5e76\u572810\u4e2a\u57fa\u51c6\u4e0a\u8bc4\u4f3013\u4e2a\u6a21\u578b\u3002", "result": "\u63ed\u793a\u4e86LMMs\u5728\u5f00\u653e\u4e16\u754c\u5206\u7c7b\u4e2d\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u7c92\u5ea6\u548c\u7ec6\u7c92\u5ea6\u80fd\u529b\u65b9\u9762\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u9488\u5bf9\u6027\u63d0\u793a\u548c\u63a8\u7406\u7684\u6539\u8fdb\u65b9\u6cd5\u3002", "conclusion": "LMMs\u5728\u5f00\u653e\u4e16\u754c\u5206\u7c7b\u4e2d\u9762\u4e34\u663e\u8457\u6311\u6218\uff0c\u4f46\u901a\u8fc7\u4f18\u5316\u63d0\u793a\u548c\u63a8\u7406\u53ef\u4ee5\u90e8\u5206\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\u3002"}}
{"id": "2503.22006", "pdf": "https://arxiv.org/pdf/2503.22006", "abs": "https://arxiv.org/abs/2503.22006", "authors": ["Marc Brinner", "Tarek Al Mustafa", "Sina Zarrie\u00df"], "title": "Enhancing Domain-Specific Encoder Models with LLM-Generated Data: How to Leverage Ontologies, and How to Do Without Them", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "We investigate the use of LLM-generated data for continual pretraining of\nencoder models in specialized domains with limited training data, using the\nscientific domain of invasion biology as a case study. To this end, we leverage\ndomain-specific ontologies by enriching them with LLM-generated data and\npretraining the encoder model as an ontology-informed embedding model for\nconcept definitions. To evaluate the effectiveness of this method, we compile a\nbenchmark specifically designed for assessing model performance in invasion\nbiology. After demonstrating substantial improvements over standard LLM\npretraining, we investigate the feasibility of applying the proposed approach\nto domains without comprehensive ontologies by substituting ontological\nconcepts with concepts automatically extracted from a small corpus of\nscientific abstracts and establishing relationships between concepts through\ndistributional statistics. Our results demonstrate that this automated approach\nachieves comparable performance using only a small set of scientific abstracts,\nresulting in a fully automated pipeline for enhancing domain-specific\nunderstanding of small encoder models that is especially suited for application\nin low-resource settings and achieves performance comparable to masked language\nmodeling pretraining on much larger datasets.", "AI": {"task": "\u7814\u7a76\u5982\u4f55\u5229\u7528LLM\u751f\u6210\u7684\u6570\u636e\u5bf9\u7f16\u7801\u5668\u6a21\u578b\u8fdb\u884c\u6301\u7eed\u9884\u8bad\u7ec3\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u6709\u9650\u7684\u4e13\u95e8\u9886\u57df\uff08\u4ee5\u5165\u4fb5\u751f\u7269\u5b66\u4e3a\u4f8b\uff09\u3002", "motivation": "\u89e3\u51b3\u5728\u6570\u636e\u6709\u9650\u7684\u4e13\u95e8\u9886\u57df\u4e2d\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u6a21\u578b\u7684\u6311\u6218\uff0c\u63d0\u5347\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\u7684\u7406\u89e3\u80fd\u529b\u3002", "method": "\u5229\u7528\u9886\u57df\u7279\u5b9a\u672c\u4f53\u8bba\uff0c\u901a\u8fc7LLM\u751f\u6210\u7684\u6570\u636e\u4e30\u5bcc\u672c\u4f53\uff0c\u5e76\u5c06\u7f16\u7801\u5668\u6a21\u578b\u9884\u8bad\u7ec3\u4e3a\u57fa\u4e8e\u672c\u4f53\u7684\u6982\u5ff5\u5b9a\u4e49\u5d4c\u5165\u6a21\u578b\u3002\u5bf9\u4e8e\u65e0\u5b8c\u6574\u672c\u4f53\u7684\u9886\u57df\uff0c\u4f7f\u7528\u79d1\u5b66\u6458\u8981\u81ea\u52a8\u63d0\u53d6\u6982\u5ff5\u5e76\u901a\u8fc7\u5206\u5e03\u7edf\u8ba1\u5efa\u7acb\u5173\u7cfb\u3002", "result": "\u5728\u5165\u4fb5\u751f\u7269\u5b66\u9886\u57df\u663e\u8457\u4f18\u4e8e\u6807\u51c6LLM\u9884\u8bad\u7ec3\uff0c\u4e14\u5728\u65e0\u5b8c\u6574\u672c\u4f53\u7684\u9886\u57df\u4e5f\u80fd\u901a\u8fc7\u5c11\u91cf\u79d1\u5b66\u6458\u8981\u5b9e\u73b0\u7c7b\u4f3c\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u7684\u9886\u57df\u7279\u5b9a\u7406\u89e3\u63d0\u4f9b\u4e86\u4e00\u79cd\u5168\u81ea\u52a8\u89e3\u51b3\u65b9\u6848\uff0c\u6027\u80fd\u63a5\u8fd1\u57fa\u4e8e\u66f4\u5927\u6570\u636e\u96c6\u7684\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u3002"}}
{"id": "2503.21854", "pdf": "https://arxiv.org/pdf/2503.21854", "abs": "https://arxiv.org/abs/2503.21854", "authors": ["Hongyi Zeng", "Wenxuan Liu", "Tianhua Xia", "Jinhui Chen", "Ziyun Li", "Sai Qian Zhang"], "title": "Foveated Instance Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Instance segmentation is essential for augmented reality and virtual reality\n(AR/VR) as it enables precise object recognition and interaction, enhancing the\nintegration of virtual and real-world elements for an immersive experience.\nHowever, the high computational overhead of segmentation limits its application\non resource-constrained AR/VR devices, causing large processing latency and\ndegrading user experience. In contrast to conventional scenarios, AR/VR users\ntypically focus on only a few regions within their field of view before\nshifting perspective, allowing segmentation to be concentrated on gaze-specific\nareas. This insight drives the need for efficient segmentation methods that\nprioritize processing instance of interest, reducing computational load and\nenhancing real-time performance. In this paper, we present a foveated instance\nsegmentation (FovealSeg) framework that leverages real-time user gaze data to\nperform instance segmentation exclusively on instance of interest, resulting in\nsubstantial computational savings. Evaluation results show that FSNet achieves\nan IoU of 0.56 on ADE20K and 0.54 on LVIS, notably outperforming the baseline.\nThe code is available at https://github.com/SAI-", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7528\u6237\u6ce8\u89c6\u6570\u636e\u7684\u5b9e\u4f8b\u5206\u5272\u6846\u67b6\uff08FovealSeg\uff09\uff0c\u4e13\u6ce8\u4e8e\u5904\u7406\u7528\u6237\u611f\u5174\u8da3\u7684\u533a\u57df\u4ee5\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "AR/VR\u8bbe\u5907\u8d44\u6e90\u6709\u9650\uff0c\u4f20\u7edf\u5b9e\u4f8b\u5206\u5272\u8ba1\u7b97\u91cf\u5927\uff0c\u5bfc\u81f4\u5ef6\u8fdf\u548c\u7528\u6237\u4f53\u9a8c\u4e0b\u964d\uff1b\u7528\u6237\u901a\u5e38\u53ea\u5173\u6ce8\u89c6\u91ce\u4e2d\u7684\u90e8\u5206\u533a\u57df\uff0c\u56e0\u6b64\u53ef\u4ee5\u4f18\u5316\u5206\u5272\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u5b9e\u65f6\u7528\u6237\u6ce8\u89c6\u6570\u636e\uff0c\u4ec5\u5bf9\u7528\u6237\u611f\u5174\u8da3\u7684\u5b9e\u4f8b\u8fdb\u884c\u5206\u5272\uff08FovealSeg\u6846\u67b6\uff09\u3002", "result": "\u5728ADE20K\u548cLVIS\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u52300.56\u548c0.54\u7684IoU\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "FovealSeg\u901a\u8fc7\u805a\u7126\u7528\u6237\u6ce8\u89c6\u533a\u57df\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u63d0\u5347\u4e86\u5b9e\u65f6\u6027\u80fd\u3002"}}
{"id": "2503.22036", "pdf": "https://arxiv.org/pdf/2503.22036", "abs": "https://arxiv.org/abs/2503.22036", "authors": ["Oliver Kramer"], "title": "Cognitive Prompts Using Guilford's Structure of Intellect Model", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) demonstrate strong language generation\ncapabilities but often struggle with structured reasoning, leading to\ninconsistent or suboptimal problem-solving. To mitigate this limitation,\nGuilford's Structure of Intellect (SOI) model - a foundational framework from\nintelligence theory - is leveraged as the basis for cognitive prompt\nengineering. The SOI model categorizes cognitive operations such as pattern\nrecognition, memory retrieval, and evaluation, offering a systematic approach\nto enhancing LLM reasoning and decision-making. This position paper presents a\nnovel cognitive prompting approach for enforcing SOI-inspired reasoning for\nimproving clarity, coherence, and adaptability in model responses.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eGuilford\u667a\u529b\u7ed3\u6784\uff08SOI\uff09\u6a21\u578b\u7684\u8ba4\u77e5\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\uff0c\u4ee5\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7ed3\u6784\u5316\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u8a00\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7ed3\u6784\u5316\u63a8\u7406\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u5bfc\u81f4\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u4e0d\u4e00\u81f4\u6216\u6b21\u4f18\u3002", "method": "\u5229\u7528SOI\u6a21\u578b\u5bf9\u8ba4\u77e5\u64cd\u4f5c\uff08\u5982\u6a21\u5f0f\u8bc6\u522b\u3001\u8bb0\u5fc6\u68c0\u7d22\u548c\u8bc4\u4f30\uff09\u8fdb\u884c\u5206\u7c7b\uff0c\u8bbe\u8ba1\u7cfb\u7edf\u5316\u7684\u8ba4\u77e5\u63d0\u793a\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8ba4\u77e5\u63d0\u793a\u65b9\u6cd5\uff0c\u80fd\u591f\u63d0\u5347\u6a21\u578b\u54cd\u5e94\u7684\u6e05\u6670\u6027\u3001\u8fde\u8d2f\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "\u901a\u8fc7SOI\u6a21\u578b\u6307\u5bfc\u7684\u8ba4\u77e5\u63d0\u793a\u5de5\u7a0b\u53ef\u4ee5\u6709\u6548\u589e\u5f3aLLM\u7684\u7ed3\u6784\u5316\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2503.21889", "pdf": "https://arxiv.org/pdf/2503.21889", "abs": "https://arxiv.org/abs/2503.21889", "authors": ["Patrice Bechard", "Chao Wang", "Amirhossein Abaskohi", "Juan Rodriguez", "Christopher Pal", "David Vazquez", "Spandana Gella", "Sai Rajeswar", "Perouz Taslakian"], "title": "StarFlow: Generating Structured Workflow Outputs From Sketch Images", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Workflows are a fundamental component of automation in enterprise platforms,\nenabling the orchestration of tasks, data processing, and system integrations.\nDespite being widely used, building workflows can be complex, often requiring\nmanual configuration through low-code platforms or visual programming tools. To\nsimplify this process, we explore the use of generative foundation models,\nparticularly vision-language models (VLMs), to automatically generate\nstructured workflows from visual inputs. Translating hand-drawn sketches or\ncomputer-generated diagrams into executable workflows is challenging due to the\nambiguity of free-form drawings, variations in diagram styles, and the\ndifficulty of inferring execution logic from visual elements. To address this,\nwe introduce StarFlow, a framework for generating structured workflow outputs\nfrom sketches using vision-language models. We curate a diverse dataset of\nworkflow diagrams -- including synthetic, manually annotated, and real-world\nsamples -- to enable robust training and evaluation. We finetune and benchmark\nmultiple vision-language models, conducting a series of ablation studies to\nanalyze the strengths and limitations of our approach. Our results show that\nfinetuning significantly enhances structured workflow generation, outperforming\nlarge vision-language models on this task.", "AI": {"task": "\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4ece\u89c6\u89c9\u8f93\u5165\u81ea\u52a8\u751f\u6210\u7ed3\u6784\u5316\u5de5\u4f5c\u6d41\u3002", "motivation": "\u5c3d\u7ba1\u5de5\u4f5c\u6d41\u5728\u4f01\u4e1a\u5e73\u53f0\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u6784\u5efa\u8fc7\u7a0b\u590d\u6742\uff0c\u901a\u5e38\u9700\u8981\u624b\u52a8\u914d\u7f6e\uff0c\u56e0\u6b64\u63a2\u7d22\u5982\u4f55\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faStarFlow\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ece\u8349\u56fe\u751f\u6210\u7ed3\u6784\u5316\u5de5\u4f5c\u6d41\uff0c\u5e76\u4f7f\u7528\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "\u5fae\u8c03\u663e\u8457\u63d0\u5347\u4e86\u7ed3\u6784\u5316\u5de5\u4f5c\u6d41\u7684\u751f\u6210\u6548\u679c\uff0c\u4f18\u4e8e\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "StarFlow\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4ece\u89c6\u89c9\u8f93\u5165\u751f\u6210\u7ed3\u6784\u5316\u5de5\u4f5c\u6d41\u7684\u6311\u6218\uff0c\u4e3a\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u6784\u5efa\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2503.22040", "pdf": "https://arxiv.org/pdf/2503.22040", "abs": "https://arxiv.org/abs/2503.22040", "authors": ["Hao Lin", "Yongjun Zhang"], "title": "The Risks of Using Large Language Models for Text Annotation in Social Science Research", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Generative artificial intelligence (GenAI) or large language models (LLMs)\nhave the potential to revolutionize computational social science, particularly\nin automated textual analysis. In this paper, we conduct a systematic\nevaluation of the promises and risks of using LLMs for diverse coding tasks,\nwith social movement studies serving as a case example. We propose a framework\nfor social scientists to incorporate LLMs into text annotation, either as the\nprimary coding decision-maker or as a coding assistant. This framework provides\ntools for researchers to develop the optimal prompt, and to examine and report\nthe validity and reliability of LLMs as a methodological tool. Additionally, we\ndiscuss the associated epistemic risks related to validity, reliability,\nreplicability, and transparency. We conclude with several practical guidelines\nfor using LLMs in text annotation tasks, and how we can better communicate the\nepistemic risks in research.", "AI": {"task": "\u7cfb\u7edf\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u793e\u4f1a\u79d1\u5b66\u6587\u672c\u7f16\u7801\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u4e0e\u98ce\u9669\u3002", "motivation": "\u63a2\u7d22GenAI\u6216LLMs\u5982\u4f55\u9769\u65b0\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66\uff0c\u7279\u522b\u662f\u5728\u81ea\u52a8\u6587\u672c\u5206\u6790\u9886\u57df\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u5e2e\u52a9\u793e\u4f1a\u79d1\u5b66\u5bb6\u5c06LLMs\u7eb3\u5165\u6587\u672c\u6807\u6ce8\u4efb\u52a1\uff0c\u5e76\u4f18\u5316\u63d0\u793a\u8bbe\u8ba1\u3001\u9a8c\u8bc1\u65b9\u6cd5\u5de5\u5177\u7684\u6709\u6548\u6027\u548c\u53ef\u9760\u6027\u3002", "result": "\u63d0\u4f9b\u4e86\u4f7f\u7528LLMs\u8fdb\u884c\u6587\u672c\u6807\u6ce8\u7684\u5b9e\u7528\u6307\u5357\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u6709\u6548\u6027\u3001\u53ef\u9760\u6027\u3001\u53ef\u590d\u5236\u6027\u548c\u900f\u660e\u5ea6\u7b49\u8ba4\u77e5\u98ce\u9669\u3002", "conclusion": "\u603b\u7ed3\u4e86LLMs\u5728\u6587\u672c\u6807\u6ce8\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u5efa\u8bae\uff0c\u5e76\u5f3a\u8c03\u5982\u4f55\u66f4\u597d\u5730\u4f20\u8fbe\u7814\u7a76\u4e2d\u7684\u8ba4\u77e5\u98ce\u9669\u3002"}}
{"id": "2503.21893", "pdf": "https://arxiv.org/pdf/2503.21893", "abs": "https://arxiv.org/abs/2503.21893", "authors": ["Taufiq Ahmed", "Abhishek Kumar", "Constantino \u00c1lvarez Casado", "Anlan Zhang", "Tuomo H\u00e4nninen", "Lauri Loven", "Miguel Bordallo L\u00f3pez", "Sasu Tarkoma"], "title": "Exponentially Weighted Instance-Aware Repeat Factor Sampling for Long-Tailed Object Detection Model Training in Unmanned Aerial Vehicles Surveillance Scenarios", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "6 pages, 2 figures, 9 tables, 6 formulas, conference paper", "summary": "Object detection models often struggle with class imbalance, where rare\ncategories appear significantly less frequently than common ones. Existing\nsampling-based rebalancing strategies, such as Repeat Factor Sampling (RFS) and\nInstance-Aware Repeat Factor Sampling (IRFS), mitigate this issue by adjusting\nsample frequencies based on image and instance counts. However, these methods\nare based on linear adjustments, which limit their effectiveness in long-tailed\ndistributions. This work introduces Exponentially Weighted Instance-Aware\nRepeat Factor Sampling (E-IRFS), an extension of IRFS that applies exponential\nscaling to better differentiate between rare and frequent classes. E-IRFS\nadjusts sampling probabilities using an exponential function applied to the\ngeometric mean of image and instance frequencies, ensuring a more adaptive\nrebalancing strategy. We evaluate E-IRFS on a dataset derived from the\nFireman-UAV-RGBT Dataset and four additional public datasets, using YOLOv11\nobject detection models to identify fire, smoke, people and lakes in emergency\nscenarios. The results show that E-IRFS improves detection performance by 22\\%\nover the baseline and outperforms RFS and IRFS, particularly for rare\ncategories. The analysis also highlights that E-IRFS has a stronger effect on\nlightweight models with limited capacity, as these models rely more on data\nsampling strategies to address class imbalance. The findings demonstrate that\nE-IRFS improves rare object detection in resource-constrained environments,\nmaking it a suitable solution for real-time applications such as UAV-based\nemergency monitoring.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6307\u6570\u52a0\u6743\u7684\u5b9e\u4f8b\u611f\u77e5\u91cd\u590d\u56e0\u5b50\u91c7\u6837\u65b9\u6cd5\uff08E-IRFS\uff09\uff0c\u4ee5\u89e3\u51b3\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7ebf\u6027\u8c03\u6574\u7684\u91c7\u6837\u65b9\u6cd5\uff08\u5982RFS\u548cIRFS\uff09\u5728\u957f\u5c3e\u5206\u5e03\u4e2d\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u7b56\u7565\u6765\u533a\u5206\u7a00\u6709\u548c\u5e38\u89c1\u7c7b\u522b\u3002", "method": "\u901a\u8fc7\u5c06\u51e0\u4f55\u5e73\u5747\u7684\u56fe\u50cf\u548c\u5b9e\u4f8b\u9891\u7387\u5e94\u7528\u4e8e\u6307\u6570\u51fd\u6570\uff0c\u8c03\u6574\u91c7\u6837\u6982\u7387\uff0c\u5b9e\u73b0\u66f4\u81ea\u9002\u5e94\u7684\u91cd\u5e73\u8861\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cE-IRFS\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u4e8622%\u7684\u68c0\u6d4b\u6027\u80fd\uff0c\u5c24\u5176\u5728\u7a00\u6709\u7c7b\u522b\u4e0a\u8868\u73b0\u4f18\u4e8eRFS\u548cIRFS\u3002", "conclusion": "E-IRFS\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u6709\u6548\u63d0\u5347\u7a00\u6709\u76ee\u6807\u68c0\u6d4b\uff0c\u9002\u7528\u4e8e\u65e0\u4eba\u673a\u5e94\u6025\u76d1\u6d4b\u7b49\u5b9e\u65f6\u5e94\u7528\u3002"}}
{"id": "2503.22048", "pdf": "https://arxiv.org/pdf/2503.22048", "abs": "https://arxiv.org/abs/2503.22048", "authors": ["Chung-En Sun", "Ge Yan", "Tsui-Wei Weng"], "title": "ThinkEdit: Interpretable Weight Editing to Mitigate Overly Short Thinking in Reasoning Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Recent studies have shown that Large Language Models (LLMs) augmented with\nchain-of-thought (CoT) reasoning demonstrate impressive problem-solving\nabilities. However, in this work, we identify a recurring issue where these\nmodels occasionally generate overly short reasoning, leading to degraded\nperformance on even simple mathematical problems. Specifically, we investigate\nhow reasoning length is embedded in the hidden representations of reasoning\nmodels and its impact on accuracy. Our analysis reveals that reasoning length\nis governed by a linear direction in the representation space, allowing us to\ninduce overly short reasoning by steering the model along this direction.\nBuilding on this insight, we introduce ThinkEdit, a simple yet effective\nweight-editing approach to mitigate the issue of overly short reasoning. We\nfirst identify a small subset of attention heads (approximately 2%) that\npredominantly drive short reasoning behavior. We then edit the output\nprojection weights of these heads to suppress the short reasoning direction.\nWith changes to only 0.1% of the model's parameters, ThinkEdit effectively\nreduces overly short reasoning and yields notable accuracy gains for short\nreasoning outputs (+5.44%), along with an overall improvement across multiple\nmath benchmarks (+2.43%). Our findings provide new mechanistic insights into\nhow reasoning length is controlled within LLMs and highlight the potential of\nfine-grained model interventions to improve reasoning quality. Our code is\navailable at https://github.com/Trustworthy-ML-Lab/ThinkEdit", "AI": {"task": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u63a8\u7406\u4e2d\u751f\u6210\u8fc7\u77ed\u63a8\u7406\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6cd5\u3002", "motivation": "\u53d1\u73b0LLMs\u5728CoT\u63a8\u7406\u4e2d\u5076\u5c14\u751f\u6210\u8fc7\u77ed\u63a8\u7406\uff0c\u5bfc\u81f4\u7b80\u5355\u6570\u5b66\u95ee\u9898\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u63a2\u7a76\u5176\u673a\u5236\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5206\u6790\u63a8\u7406\u957f\u5ea6\u5728\u9690\u85cf\u8868\u793a\u4e2d\u7684\u7ebf\u6027\u65b9\u5411\uff0c\u63d0\u51faThinkEdit\u65b9\u6cd5\uff0c\u7f16\u8f91\u5c11\u91cf\u6ce8\u610f\u529b\u5934\u7684\u6743\u91cd\u4ee5\u6291\u5236\u8fc7\u77ed\u63a8\u7406\u3002", "result": "ThinkEdit\u663e\u8457\u51cf\u5c11\u8fc7\u77ed\u63a8\u7406\uff0c\u77ed\u63a8\u7406\u8f93\u51fa\u51c6\u786e\u7387\u63d0\u53475.44%\uff0c\u591a\u4e2a\u6570\u5b66\u57fa\u51c6\u6574\u4f53\u63d0\u53472.43%\u3002", "conclusion": "\u63ed\u793a\u4e86LLMs\u4e2d\u63a8\u7406\u957f\u5ea6\u7684\u63a7\u5236\u673a\u5236\uff0c\u5c55\u793a\u4e86\u7ec6\u7c92\u5ea6\u6a21\u578b\u5e72\u9884\u63d0\u5347\u63a8\u7406\u8d28\u91cf\u7684\u6f5c\u529b\u3002"}}
{"id": "2503.21904", "pdf": "https://arxiv.org/pdf/2503.21904", "abs": "https://arxiv.org/abs/2503.21904", "authors": ["Zhiwei Yang", "Chen Gao", "Jing Liu", "Peng Wu", "Guansong Pang", "Mike Zheng Shou"], "title": "AssistPDA: An Online Video Surveillance Assistant for Video Anomaly Prediction, Detection, and Analysis", "categories": ["cs.CV"], "comment": "13 pages", "summary": "The rapid advancements in large language models (LLMs) have spurred growing\ninterest in LLM-based video anomaly detection (VAD). However, existing\napproaches predominantly focus on video-level anomaly question answering or\noffline detection, ignoring the real-time nature essential for practical VAD\napplications. To bridge this gap and facilitate the practical deployment of\nLLM-based VAD, we introduce AssistPDA, the first online video anomaly\nsurveillance assistant that unifies video anomaly prediction, detection, and\nanalysis (VAPDA) within a single framework. AssistPDA enables real-time\ninference on streaming videos while supporting interactive user engagement.\nNotably, we introduce a novel event-level anomaly prediction task, enabling\nproactive anomaly forecasting before anomalies fully unfold. To enhance the\nability to model intricate spatiotemporal relationships in anomaly events, we\npropose a Spatio-Temporal Relation Distillation (STRD) module. STRD transfers\nthe long-term spatiotemporal modeling capabilities of vision-language models\n(VLMs) from offline settings to real-time scenarios. Thus it equips AssistPDA\nwith a robust understanding of complex temporal dependencies and long-sequence\nmemory. Additionally, we construct VAPDA-127K, the first large-scale benchmark\ndesigned for VLM-based online VAPDA. Extensive experiments demonstrate that\nAssistPDA outperforms existing offline VLM-based approaches, setting a new\nstate-of-the-art for real-time VAPDA. Our dataset and code will be open-sourced\nto facilitate further research in the community.", "AI": {"task": "\u5f00\u53d1\u4e00\u4e2a\u540d\u4e3aAssistPDA\u7684\u5728\u7ebf\u89c6\u9891\u5f02\u5e38\u76d1\u6d4b\u52a9\u624b\uff0c\u7edf\u4e00\u89c6\u9891\u5f02\u5e38\u9884\u6d4b\u3001\u68c0\u6d4b\u548c\u5206\u6790\uff08VAPDA\uff09\u4e8e\u5355\u4e00\u6846\u67b6\u4e2d\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u89c6\u9891\u7ea7\u5f02\u5e38\u95ee\u7b54\u6216\u79bb\u7ebf\u68c0\u6d4b\uff0c\u5ffd\u89c6\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u6240\u9700\u7684\u5b9e\u65f6\u6027\u3002", "method": "\u63d0\u51faSpatio-Temporal Relation Distillation\uff08STRD\uff09\u6a21\u5757\uff0c\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u957f\u65f6\u7a7a\u5efa\u6a21\u80fd\u529b\u4ece\u79bb\u7ebf\u573a\u666f\u8fc1\u79fb\u5230\u5b9e\u65f6\u573a\u666f\uff0c\u5e76\u6784\u5efaVAPDA-127K\u57fa\u51c6\u6570\u636e\u96c6\u3002", "result": "AssistPDA\u5728\u5b9e\u65f6VAPDA\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u79bb\u7ebfVLM\u65b9\u6cd5\uff0c\u8fbe\u5230\u65b0\u7684\u6700\u4f18\u6c34\u5e73\u3002", "conclusion": "AssistPDA\u4e3aLLM-based\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5f00\u6e90\u6570\u636e\u96c6\u548c\u4ee3\u7801\u4ee5\u4fc3\u8fdb\u793e\u533a\u7814\u7a76\u3002"}}
{"id": "2503.22051", "pdf": "https://arxiv.org/pdf/2503.22051", "abs": "https://arxiv.org/abs/2503.22051", "authors": ["Zeeshan Ahmed", "Frank Seide", "Zhe Liu", "Rastislav Rabatin", "Jachym Kolar", "Niko Moritz", "Ruiming Xie", "Simone Merello", "Christian Fuegen"], "title": "Non-Monotonic Attention-based Read/Write Policy Learning for Simultaneous Translation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Simultaneous or streaming machine translation generates translation while\nreading the input stream. These systems face a quality/latency trade-off,\naiming to achieve high translation quality similar to non-streaming models with\nminimal latency. We propose an approach that efficiently manages this\ntrade-off. By enhancing a pretrained non-streaming model, which was trained\nwith a seq2seq mechanism and represents the upper bound in quality, we convert\nit into a streaming model by utilizing the alignment between source and target\ntokens. This alignment is used to learn a read/write decision boundary for\nreliable translation generation with minimal input. During training, the model\nlearns the decision boundary through a read/write policy module, employing\nsupervised learning on the alignment points (pseudo labels). The read/write\npolicy module, a small binary classification unit, can control the\nquality/latency trade-off during inference. Experimental results show that our\nmodel outperforms several strong baselines and narrows the gap with the\nnon-streaming baseline model.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u65b9\u6cd5\uff0c\u5c06\u9884\u8bad\u7ec3\u7684\u975e\u6d41\u5f0f\u673a\u5668\u7ffb\u8bd1\u6a21\u578b\u8f6c\u6362\u4e3a\u6d41\u5f0f\u6a21\u578b\uff0c\u4ee5\u5728\u8d28\u91cf\u4e0e\u5ef6\u8fdf\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "motivation": "\u6d41\u5f0f\u673a\u5668\u7ffb\u8bd1\u9700\u8981\u5728\u751f\u6210\u7ffb\u8bd1\u65f6\u5b9e\u65f6\u5904\u7406\u8f93\u5165\u6d41\uff0c\u4f46\u9762\u4e34\u8d28\u91cf\u4e0e\u5ef6\u8fdf\u7684\u6743\u8861\u95ee\u9898\uff0c\u76ee\u6807\u662f\u63a5\u8fd1\u975e\u6d41\u5f0f\u6a21\u578b\u7684\u9ad8\u8d28\u91cf\u7ffb\u8bd1\u540c\u65f6\u6700\u5c0f\u5316\u5ef6\u8fdf\u3002", "method": "\u901a\u8fc7\u5229\u7528\u6e90\u548c\u76ee\u6807\u6807\u8bb0\u4e4b\u95f4\u7684\u5bf9\u9f50\uff0c\u5b66\u4e60\u8bfb/\u5199\u51b3\u7b56\u8fb9\u754c\uff0c\u5c06\u9884\u8bad\u7ec3\u7684\u975e\u6d41\u5f0f\u6a21\u578b\u8f6c\u6362\u4e3a\u6d41\u5f0f\u6a21\u578b\u3002\u8bad\u7ec3\u65f6\u4f7f\u7528\u5bf9\u9f50\u70b9\uff08\u4f2a\u6807\u7b7e\uff09\u901a\u8fc7\u76d1\u7763\u5b66\u4e60\u8bad\u7ec3\u8bfb/\u5199\u7b56\u7565\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u591a\u4e2a\u5f3a\u57fa\u7ebf\uff0c\u5e76\u7f29\u5c0f\u4e86\u4e0e\u975e\u6d41\u5f0f\u57fa\u7ebf\u6a21\u578b\u7684\u5dee\u8ddd\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u7ba1\u7406\u4e86\u8d28\u91cf\u4e0e\u5ef6\u8fdf\u7684\u6743\u8861\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u6d41\u5f0f\u7ffb\u8bd1\u3002"}}
{"id": "2503.21907", "pdf": "https://arxiv.org/pdf/2503.21907", "abs": "https://arxiv.org/abs/2503.21907", "authors": ["Oliver Heinimann", "Assaf Shocher", "Tal Zimbalist", "Michal Irani"], "title": "KernelFusion: Assumption-Free Blind Super-Resolution via Patch Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "Traditional super-resolution (SR) methods assume an ``ideal'' downscaling\nSR-kernel (e.g., bicubic downscaling) between the high-resolution (HR) image\nand the low-resolution (LR) image. Such methods fail once the LR images are\ngenerated differently. Current blind-SR methods aim to remove this assumption,\nbut are still fundamentally restricted to rather simplistic downscaling\nSR-kernels (e.g., anisotropic Gaussian kernels), and fail on more complex (out\nof distribution) downscaling degradations. However, using the correct SR-kernel\nis often more important than using a sophisticated SR algorithm. In\n``KernelFusion'' we introduce a zero-shot diffusion-based method that makes no\nassumptions about the kernel. Our method recovers the unique image-specific\nSR-kernel directly from the LR input image, while simultaneously recovering its\ncorresponding HR image. KernelFusion exploits the principle that the correct\nSR-kernel is the one that maximizes patch similarity across different scales of\nthe LR image. We first train an image-specific patch-based diffusion model on\nthe single LR input image, capturing its unique internal patch statistics. We\nthen reconstruct a larger HR image with the same learned patch distribution,\nwhile simultaneously recovering the correct downscaling SR-kernel that\nmaintains this cross-scale relation between the HR and LR images. Empirical\nresults show that KernelFusion vastly outperforms all SR baselines on complex\ndownscaling degradations, where existing SotA Blind-SR methods fail miserably.\nBy breaking free from predefined kernel assumptions, KernelFusion pushes\nBlind-SR into a new assumption-free paradigm, handling downscaling kernels\npreviously thought impossible.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u96f6\u6837\u672c\u6269\u6563\u65b9\u6cd5\uff08KernelFusion\uff09\uff0c\u7528\u4e8e\u4ece\u4f4e\u5206\u8fa8\u7387\uff08LR\uff09\u56fe\u50cf\u4e2d\u6062\u590d\u7279\u5b9a\u4e8e\u56fe\u50cf\u7684SR\u6838\u53ca\u5176\u5bf9\u5e94\u7684\u9ad8\u5206\u8fa8\u7387\uff08HR\uff09\u56fe\u50cf\u3002", "motivation": "\u4f20\u7edf\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7406\u60f3\u7684\u964d\u91c7\u6837\u6838\uff08\u5982\u53cc\u4e09\u6b21\u964d\u91c7\u6837\uff09\uff0c\u800c\u73b0\u6709\u76f2\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u4ecd\u5c40\u9650\u4e8e\u7b80\u5355\u6838\uff08\u5982\u5404\u5411\u5f02\u6027\u9ad8\u65af\u6838\uff09\uff0c\u65e0\u6cd5\u5904\u7406\u590d\u6742\u964d\u91c7\u6837\u9000\u5316\u3002\u6b63\u786e\u7684SR\u6838\u6bd4\u7b97\u6cd5\u672c\u8eab\u66f4\u4e3a\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u8bad\u7ec3\u4e00\u4e2a\u57fa\u4e8e\u5355\u5e45LR\u56fe\u50cf\u7684\u7279\u5b9a\u4e8e\u56fe\u50cf\u7684\u5757\u6269\u6563\u6a21\u578b\uff0c\u6355\u6349\u5176\u72ec\u7279\u7684\u5185\u90e8\u5757\u7edf\u8ba1\u4fe1\u606f\uff0c\u540c\u65f6\u6062\u590dHR\u56fe\u50cf\u548c\u6b63\u786e\u7684\u964d\u91c7\u6837SR\u6838\u3002", "result": "KernelFusion\u5728\u590d\u6742\u964d\u91c7\u6837\u9000\u5316\u60c5\u51b5\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u76f2\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u3002", "conclusion": "KernelFusion\u7a81\u7834\u4e86\u9884\u5b9a\u4e49\u6838\u7684\u9650\u5236\uff0c\u5c06\u76f2\u8d85\u5206\u8fa8\u7387\u63a8\u5411\u65e0\u5047\u8bbe\u7684\u65b0\u8303\u5f0f\uff0c\u89e3\u51b3\u4e86\u6b64\u524d\u88ab\u8ba4\u4e3a\u4e0d\u53ef\u80fd\u5904\u7406\u7684\u964d\u91c7\u6837\u6838\u95ee\u9898\u3002"}}
{"id": "2503.22074", "pdf": "https://arxiv.org/pdf/2503.22074", "abs": "https://arxiv.org/abs/2503.22074", "authors": ["Chuan-Wei Kuo", "Siyu Chen", "Chenqi Yan", "Yu Yang Fredrik Liu"], "title": "Penrose Tiled Low-Rank Compression and Section-Wise Q&A Fine-Tuning: A General Framework for Domain-Specific Large Language Model Adaptation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) hold great promise for specialized scientific\ndomains such as materials science, yet adapting them efficiently and accurately\nto domain-specific knowledge remains challenging due to limited data and high\nknowledge density. We propose a two-stage framework that combines structured\nmodel compression with a scientific fine-tuning regimen to address this\nchallenge. In the compression stage, we decompose the LLM's weight matrices\ninto local low-rank \"rank blocks\" and arrange these blocks in a Penrose-like\nnon-periodic tiling pattern. Each block is then compacted via spectral\ntransformations (e.g., discrete cosine or Fourier transforms), and a\nKullback-Leibler (KL) divergence-based alignment loss preserves the\ndistributional similarity between the compressed model's representations and\nthose of the original full model. In the adaptation stage, the compressed model\nis further tuned using a human-like scientific reading protocol: it processes\ntechnical materials science documents section by section, engaging in a\nstructured question-and-answer routine for each section. This section-wise Q&A\nfine-tuning strategy extracts explicit reasoning traces and gradually injects\ndomain knowledge, while minimizing catastrophic forgetting of the model's\ngeneral language capabilities. By balancing efficient compression with targeted\nadaptation, our two-stage approach enables precise specialization of LLMs to\nhigh-value domains under data-scarce conditions. We present this principled yet\nexploratory pipeline and outline its potential for advancing materials science\nknowledge integration, laying the groundwork for comprehensive empirical\nevaluation in future work.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u6a21\u578b\u538b\u7f29\u548c\u79d1\u5b66\u5fae\u8c03\u65b9\u6cd5\uff0c\u4ee5\u9ad8\u6548\u51c6\u786e\u5730\u5c06\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u9002\u5e94\u4e8e\u6750\u6599\u79d1\u5b66\u7b49\u4e13\u4e1a\u9886\u57df\u3002", "motivation": "\u7531\u4e8e\u6570\u636e\u6709\u9650\u4e14\u77e5\u8bc6\u5bc6\u5ea6\u9ad8\uff0c\u5c06LLMs\u9ad8\u6548\u51c6\u786e\u5730\u9002\u5e94\u4e8e\u4e13\u4e1a\u79d1\u5b66\u9886\u57df\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u5c40\u90e8\u4f4e\u79e9\u5206\u89e3\u548cPenrose\u975e\u5468\u671f\u5e73\u94fa\u6a21\u5f0f\u538b\u7f29\u6a21\u578b\u6743\u91cd\u77e9\u9635\uff0c\u7b2c\u4e8c\u9636\u6bb5\u91c7\u7528\u5206\u8282\u95ee\u7b54\u5fae\u8c03\u7b56\u7565\u9010\u6b65\u6ce8\u5165\u9886\u57df\u77e5\u8bc6\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6570\u636e\u7a00\u7f3a\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86LLMs\u5bf9\u9ad8\u4ef7\u503c\u9886\u57df\u7684\u7cbe\u786e\u4e13\u4e1a\u5316\u3002", "conclusion": "\u4e24\u9636\u6bb5\u65b9\u6cd5\u4e3a\u6750\u6599\u79d1\u5b66\u77e5\u8bc6\u6574\u5408\u63d0\u4f9b\u4e86\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u672a\u6765\u5168\u9762\u5b9e\u8bc1\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2503.21932", "pdf": "https://arxiv.org/pdf/2503.21932", "abs": "https://arxiv.org/abs/2503.21932", "authors": ["Seyed Hamidreza Nabaei", "Zeyang Zheng", "Dong Chen", "Arsalan Heydarian"], "title": "Multimodal Data Integration for Sustainable Indoor Gardening: Tracking Anyplant with Time Series Foundation Model", "categories": ["cs.CV", "cs.CE", "cs.LG"], "comment": "Accepted at ASCE International Conference on Computing in Civil\n  Engineering (i3ce)", "summary": "Indoor gardening within sustainable buildings offers a transformative\nsolution to urban food security and environmental sustainability. By 2030,\nurban farming, including Controlled Environment Agriculture (CEA) and vertical\nfarming, is expected to grow at a compound annual growth rate (CAGR) of 13.2%\nfrom 2024 to 2030, according to market reports. This growth is fueled by\nadvancements in Internet of Things (IoT) technologies, sustainable innovations\nsuch as smart growing systems, and the rising interest in green interior\ndesign. This paper presents a novel framework that integrates computer vision,\nmachine learning (ML), and environmental sensing for the automated monitoring\nof plant health and growth. Unlike previous approaches, this framework combines\nRGB imagery, plant phenotyping data, and environmental factors such as\ntemperature and humidity, to predict plant water stress in a controlled growth\nenvironment. The system utilizes high-resolution cameras to extract phenotypic\nfeatures, such as RGB, plant area, height, and width while employing the\nLag-Llama time series model to analyze and predict water stress. Experimental\nresults demonstrate that integrating RGB, size ratios, and environmental data\nsignificantly enhances predictive accuracy, with the Fine-tuned model achieving\nthe lowest errors (MSE = 0.420777, MAE = 0.595428) and reduced uncertainty.\nThese findings highlight the potential of multimodal data and intelligent\nsystems to automate plant care, optimize resource consumption, and align indoor\ngardening with sustainable building management practices, paving the way for\nresilient, green urban spaces.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u673a\u5668\u5b66\u4e60\u548c\u73af\u5883\u4f20\u611f\u7684\u81ea\u52a8\u5316\u690d\u7269\u5065\u5eb7\u4e0e\u751f\u957f\u76d1\u6d4b\u6846\u67b6\u3002", "motivation": "\u901a\u8fc7\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\uff08\u5982RGB\u56fe\u50cf\u3001\u690d\u7269\u8868\u578b\u6570\u636e\u548c\u73af\u5883\u56e0\u7d20\uff09\uff0c\u63d0\u9ad8\u690d\u7269\u6c34\u5206\u80c1\u8feb\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u4ee5\u652f\u6301\u53ef\u6301\u7eed\u5efa\u7b51\u4e2d\u7684\u5ba4\u5185\u56ed\u827a\u53d1\u5c55\u3002", "method": "\u5229\u7528\u9ad8\u5206\u8fa8\u7387\u6444\u50cf\u5934\u63d0\u53d6\u690d\u7269\u8868\u578b\u7279\u5f81\uff08\u5982RGB\u3001\u9762\u79ef\u3001\u9ad8\u5ea6\u548c\u5bbd\u5ea6\uff09\uff0c\u5e76\u7ed3\u5408Lag-Llama\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u5206\u6790\u6c34\u5206\u80c1\u8feb\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6574\u5408RGB\u3001\u5c3a\u5bf8\u6bd4\u548c\u73af\u5883\u6570\u636e\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u4f18\u5316\u540e\u7684\u6a21\u578b\u8bef\u5dee\u6700\u4f4e\uff08MSE = 0.420777\uff0cMAE = 0.595428\uff09\u3002", "conclusion": "\u591a\u6a21\u6001\u6570\u636e\u548c\u667a\u80fd\u7cfb\u7edf\u5728\u81ea\u52a8\u5316\u690d\u7269\u62a4\u7406\u548c\u4f18\u5316\u8d44\u6e90\u6d88\u8017\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u53ef\u6301\u7eed\u5efa\u7b51\u7ba1\u7406\u5b9e\u8df5\u3002"}}
{"id": "2503.22092", "pdf": "https://arxiv.org/pdf/2503.22092", "abs": "https://arxiv.org/abs/2503.22092", "authors": ["Dina Albassam", "Adam Cross", "Chengxiang Zhai"], "title": "Leveraging LLMs for Predicting Unknown Diagnoses from Clinical Notes", "categories": ["cs.CL"], "comment": "19 pages, 3 figures, 5 tables", "summary": "Electronic Health Records (EHRs) often lack explicit links between\nmedications and diagnoses, making clinical decision-making and research more\ndifficult. Even when links exist, diagnosis lists may be incomplete, especially\nduring early patient visits. Discharge summaries tend to provide more complete\ninformation, which can help infer accurate diagnoses, especially with the help\nof large language models (LLMs). This study investigates whether LLMs can\npredict implicitly mentioned diagnoses from clinical notes and link them to\ncorresponding medications. We address two research questions: (1) Does majority\nvoting across diverse LLM configurations outperform the best single\nconfiguration in diagnosis prediction? (2) How sensitive is majority voting\naccuracy to LLM hyperparameters such as temperature, top-p, and summary length?\nTo evaluate, we created a new dataset of 240 expert-annotated\nmedication-diagnosis pairs from 20 MIMIC-IV notes. Using GPT-3.5 Turbo, we ran\n18 prompting configurations across short and long summary lengths, generating\n8568 test cases. Results show that majority voting achieved 75 percent\naccuracy, outperforming the best single configuration at 66 percent. No single\nhyperparameter setting dominated, but combining deterministic, balanced, and\nexploratory strategies improved performance. Shorter summaries generally led to\nhigher accuracy.In conclusion, ensemble-style majority voting with diverse LLM\nconfigurations improves diagnosis prediction in EHRs and offers a promising\nmethod to link medications and diagnoses in clinical texts.", "AI": {"task": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u662f\u5426\u80fd\u4ece\u4e34\u5e8a\u8bb0\u5f55\u4e2d\u9884\u6d4b\u9690\u542b\u7684\u8bca\u65ad\u5e76\u5c06\u5176\u4e0e\u76f8\u5e94\u836f\u7269\u5173\u8054\u3002", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHRs\uff09\u4e2d\u836f\u7269\u4e0e\u8bca\u65ad\u4e4b\u95f4\u7684\u663e\u5f0f\u94fe\u63a5\u7f3a\u5931\uff0c\u589e\u52a0\u4e86\u4e34\u5e8a\u51b3\u7b56\u548c\u7814\u7a76\u7684\u96be\u5ea6\u3002", "method": "\u4f7f\u7528GPT-3.5 Turbo\uff0c\u901a\u8fc718\u79cd\u63d0\u793a\u914d\u7f6e\u751f\u62108568\u4e2a\u6d4b\u8bd5\u6848\u4f8b\uff0c\u8bc4\u4f30\u591a\u6570\u6295\u7968\u7b56\u7565\u5728\u4e0d\u540c\u8d85\u53c2\u6570\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u591a\u6570\u6295\u7968\u7b56\u7565\u8fbe\u523075%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u6700\u4f73\u5355\u4e00\u914d\u7f6e\u768466%\u3002", "conclusion": "\u901a\u8fc7\u591a\u6837\u5316\u7684LLM\u914d\u7f6e\u8fdb\u884c\u591a\u6570\u6295\u7968\uff0c\u53ef\u63d0\u5347EHRs\u4e2d\u8bca\u65ad\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u4e34\u5e8a\u6587\u672c\u4e2d\u836f\u7269\u4e0e\u8bca\u65ad\u7684\u5173\u8054\u63d0\u4f9b\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2503.21939", "pdf": "https://arxiv.org/pdf/2503.21939", "abs": "https://arxiv.org/abs/2503.21939", "authors": ["Roxana Bujack", "Emily Shinkle", "Alice Allen", "Tomas Suk", "Nicholas Lubbers"], "title": "Flexible Moment-Invariant Bases from Irreducible Tensors", "categories": ["cs.CV"], "comment": null, "summary": "Moment invariants are a powerful tool for the generation of\nrotation-invariant descriptors needed for many applications in pattern\ndetection, classification, and machine learning. A set of invariants is optimal\nif it is complete, independent, and robust against degeneracy in the input. In\nthis paper, we show that the current state of the art for the generation of\nthese bases of moment invariants, despite being robust against moment tensors\nbeing identically zero, is vulnerable to a degeneracy that is common in\nreal-world applications, namely spherical functions. We show how to overcome\nthis vulnerability by combining two popular moment invariant approaches: one\nbased on spherical harmonics and one based on Cartesian tensor algebra.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u7403\u8c10\u51fd\u6570\u548c\u7b1b\u5361\u5c14\u5f20\u91cf\u4ee3\u6570\u7684\u65b9\u6cd5\uff0c\u4ee5\u514b\u670d\u73b0\u6709\u77e9\u4e0d\u53d8\u91cf\u751f\u6210\u65b9\u6cd5\u5728\u7403\u9762\u51fd\u6570\u9000\u5316\u60c5\u51b5\u4e0b\u7684\u8106\u5f31\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u77e9\u4e0d\u53d8\u91cf\u751f\u6210\u65b9\u6cd5\u5728\u9762\u5bf9\u7403\u9762\u51fd\u6570\u9000\u5316\u65f6\u8868\u73b0\u8106\u5f31\uff0c\u800c\u7403\u9762\u51fd\u6570\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5f88\u5e38\u89c1\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u3002", "method": "\u7ed3\u5408\u7403\u8c10\u51fd\u6570\u548c\u7b1b\u5361\u5c14\u5f20\u91cf\u4ee3\u6570\u4e24\u79cd\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u591f\u514b\u670d\u7403\u9762\u51fd\u6570\u9000\u5316\u8106\u5f31\u6027\u7684\u65b0\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u4e24\u79cd\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u77e9\u4e0d\u53d8\u91cf\u751f\u6210\u65b9\u6cd5\u5728\u7403\u9762\u51fd\u6570\u9000\u5316\u60c5\u51b5\u4e0b\u7684\u95ee\u9898\u3002"}}
{"id": "2503.22115", "pdf": "https://arxiv.org/pdf/2503.22115", "abs": "https://arxiv.org/abs/2503.22115", "authors": ["Yazhou Zhang", "Qimeng Liu", "Qiuchi Li", "Peng Zhang", "Jing Qin"], "title": "Beyond Single-Sentence Prompts: Upgrading Value Alignment Benchmarks with Dialogues and Stories", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Evaluating the value alignment of large language models (LLMs) has\ntraditionally relied on single-sentence adversarial prompts, which directly\nprobe models with ethically sensitive or controversial questions. However, with\nthe rapid advancements in AI safety techniques, models have become increasingly\nadept at circumventing these straightforward tests, limiting their\neffectiveness in revealing underlying biases and ethical stances. To address\nthis limitation, we propose an upgraded value alignment benchmark that moves\nbeyond single-sentence prompts by incorporating multi-turn dialogues and\nnarrative-based scenarios. This approach enhances the stealth and adversarial\nnature of the evaluation, making it more robust against superficial safeguards\nimplemented in modern LLMs. We design and implement a dataset that includes\nconversational traps and ethically ambiguous storytelling, systematically\nassessing LLMs' responses in more nuanced and context-rich settings.\nExperimental results demonstrate that this enhanced methodology can effectively\nexpose latent biases that remain undetected in traditional single-shot\nevaluations. Our findings highlight the necessity of contextual and dynamic\ntesting for value alignment in LLMs, paving the way for more sophisticated and\nrealistic assessments of AI ethics and safety.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u591a\u8f6e\u5bf9\u8bdd\u548c\u53d9\u4e8b\u573a\u666f\u7684\u5347\u7ea7\u7248\u4ef7\u503c\u5bf9\u9f50\u57fa\u51c6\uff0c\u4ee5\u66f4\u6709\u6548\u5730\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u4ef7\u503c\u5bf9\u9f50\u3002", "motivation": "\u4f20\u7edf\u7684\u5355\u53e5\u5bf9\u6297\u6027\u63d0\u793a\u65b9\u6cd5\u5728\u73b0\u4ee3LLMs\u4e2d\u6548\u679c\u6709\u9650\uff0c\u65e0\u6cd5\u5145\u5206\u63ed\u793a\u6a21\u578b\u7684\u6f5c\u5728\u504f\u89c1\u548c\u4f26\u7406\u7acb\u573a\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u5305\u542b\u5bf9\u8bdd\u9677\u9631\u548c\u4f26\u7406\u6a21\u7cca\u53d9\u4e8b\u7684\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u591a\u8f6e\u5bf9\u8bdd\u548c\u53d9\u4e8b\u573a\u666f\u7cfb\u7edf\u8bc4\u4f30LLMs\u7684\u54cd\u5e94\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u66b4\u9732\u4f20\u7edf\u5355\u6b21\u8bc4\u4f30\u4e2d\u672a\u68c0\u6d4b\u5230\u7684\u6f5c\u5728\u504f\u89c1\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u548c\u52a8\u6001\u6d4b\u8bd5\u5bf9LLMs\u7684\u4ef7\u503c\u5bf9\u9f50\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\uff0c\u4e3aAI\u4f26\u7406\u548c\u5b89\u5168\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u590d\u6742\u548c\u73b0\u5b9e\u7684\u8def\u5f84\u3002"}}
{"id": "2503.21943", "pdf": "https://arxiv.org/pdf/2503.21943", "abs": "https://arxiv.org/abs/2503.21943", "authors": ["Haoming Cai", "Tsung-Wei Huang", "Shiv Gehlot", "Brandon Y. Feng", "Sachin Shah", "Guan-Ming Su", "Christopher Metzler"], "title": "Parametric Shadow Control for Portrait Generationin Text-to-Image Diffusion Models", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": "ShadowDirector Arxiv Version", "summary": "Text-to-image diffusion models excel at generating diverse portraits, but\nlack intuitive shadow control. Existing editing approaches, as post-processing,\nstruggle to offer effective manipulation across diverse styles. Additionally,\nthese methods either rely on expensive real-world light-stage data collection\nor require extensive computational resources for training. To address these\nlimitations, we introduce Shadow Director, a method that extracts and\nmanipulates hidden shadow attributes within well-trained diffusion models. Our\napproach uses a small estimation network that requires only a few thousand\nsynthetic images and hours of training-no costly real-world light-stage data\nneeded. Shadow Director enables parametric and intuitive control over shadow\nshape, placement, and intensity during portrait generation while preserving\nartistic integrity and identity across diverse styles. Despite training only on\nsynthetic data built on real-world identities, it generalizes effectively to\ngenerated portraits with diverse styles, making it a more accessible and\nresource-friendly solution.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aShadow Director\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u63d0\u53d6\u548c\u64cd\u7eb5\u9690\u85cf\u7684\u9634\u5f71\u5c5e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u76f4\u89c2\u7684\u9634\u5f71\u63a7\u5236\uff0c\u4e14\u4f9d\u8d56\u6602\u8d35\u7684\u6570\u636e\u91c7\u96c6\u6216\u8ba1\u7b97\u8d44\u6e90\u3002", "method": "\u4f7f\u7528\u5c0f\u578b\u4f30\u8ba1\u7f51\u7edc\uff0c\u4ec5\u9700\u5c11\u91cf\u5408\u6210\u56fe\u50cf\u548c\u77ed\u65f6\u95f4\u8bad\u7ec3\uff0c\u65e0\u9700\u771f\u5b9e\u5149\u9636\u6bb5\u6570\u636e\u3002", "result": "\u5b9e\u73b0\u4e86\u5bf9\u9634\u5f71\u5f62\u72b6\u3001\u4f4d\u7f6e\u548c\u5f3a\u5ea6\u7684\u53c2\u6570\u5316\u63a7\u5236\uff0c\u540c\u65f6\u4fdd\u6301\u827a\u672f\u5b8c\u6574\u6027\u548c\u8eab\u4efd\u591a\u6837\u6027\u3002", "conclusion": "Shadow Director\u662f\u4e00\u79cd\u66f4\u6613\u83b7\u53d6\u4e14\u8d44\u6e90\u53cb\u597d\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u98ce\u683c\u7684\u8096\u50cf\u751f\u6210\u3002"}}
{"id": "2503.22144", "pdf": "https://arxiv.org/pdf/2503.22144", "abs": "https://arxiv.org/abs/2503.22144", "authors": ["Papa Abdou Karim Karou Diallo", "Amal Zouaq"], "title": "FRASE: Structured Representations for Generalizable SPARQL Query Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Translating natural language questions into SPARQL queries enables Knowledge\nBase querying for factual and up-to-date responses. However, existing datasets\nfor this task are predominantly template-based, leading models to learn\nsuperficial mappings between question and query templates rather than\ndeveloping true generalization capabilities. As a result, models struggle when\nencountering naturally phrased, template-free questions. This paper introduces\nFRASE (FRAme-based Semantic Enhancement), a novel approach that leverages Frame\nSemantic Role Labeling (FSRL) to address this limitation. We also present\nLC-QuAD 3.0, a new dataset derived from LC-QuAD 2.0, in which each question is\nenriched using FRASE through frame detection and the mapping of frame-elements\nto their argument. We evaluate the impact of this approach through extensive\nexperiments on recent large language models (LLMs) under different fine-tuning\nconfigurations. Our results demonstrate that integrating frame-based structured\nrepresentations consistently improves SPARQL generation performance,\nparticularly in challenging generalization scenarios when test questions\nfeature unseen templates (unknown template splits) and when they are all\nnaturally phrased (reformulated questions).", "AI": {"task": "\u5c06\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u8f6c\u6362\u4e3aSPARQL\u67e5\u8be2\uff0c\u4ee5\u652f\u6301\u77e5\u8bc6\u5e93\u67e5\u8be2\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u591a\u4e3a\u6a21\u677f\u5316\uff0c\u5bfc\u81f4\u6a21\u578b\u5b66\u4e60\u7684\u662f\u95ee\u9898\u4e0e\u67e5\u8be2\u6a21\u677f\u4e4b\u95f4\u7684\u6d45\u5c42\u6620\u5c04\uff0c\u7f3a\u4e4f\u771f\u6b63\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faFRASE\uff08\u57fa\u4e8e\u6846\u67b6\u7684\u8bed\u4e49\u589e\u5f3a\uff09\u65b9\u6cd5\uff0c\u5229\u7528\u6846\u67b6\u8bed\u4e49\u89d2\u8272\u6807\u6ce8\uff08FSRL\uff09\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5e76\u5f15\u5165LC-QuAD 3.0\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u6846\u67b6\u7684\u7ed3\u6784\u5316\u8868\u793a\u80fd\u663e\u8457\u63d0\u5347SPARQL\u751f\u6210\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u672a\u77e5\u6a21\u677f\u548c\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u573a\u666f\u4e0b\u3002", "conclusion": "FRASE\u65b9\u6cd5\u901a\u8fc7\u6846\u67b6\u8bed\u4e49\u589e\u5f3a\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u5728\u590d\u6742\u6cdb\u5316\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2503.21956", "pdf": "https://arxiv.org/pdf/2503.21956", "abs": "https://arxiv.org/abs/2503.21956", "authors": ["Taqwa I. Alhadidi", "Asmaa Alazmi", "Shadi Jaradat", "Ahmed Jaber", "Huthaifa Ashqar", "Mohammed Elhenawy"], "title": "Enhancing Pavement Crack Classification with Bidirectional Cascaded Neural Networks", "categories": ["cs.CV"], "comment": null, "summary": "Pavement distress, such as cracks and potholes, is a significant issue\naffecting road safety and maintenance. In this study, we present the\nimplementation and evaluation of Bidirectional Cascaded Neural Networks (BCNNs)\nfor the classification of pavement crack images following image augmentation.\nWe classified pavement cracks into three main categories: linear cracks,\npotholes, and fatigue cracks on an enhanced dataset utilizing U-Net 50 for\nimage augmentation. The augmented dataset comprised 599 images. Our proposed\nBCNN model was designed to leverage both forward and backward information\nflows, with detection accuracy enhanced by its cascaded structure wherein each\nlayer progressively refines the output of the preceding one. Our model achieved\nan overall accuracy of 87%, with precision, recall, and F1-score measures\nindicating high effectiveness across the categories. For fatigue cracks, the\nmodel recorded a precision of 0.87, recall of 0.83, and F1-score of 0.85 on 205\nimages. Linear cracks were detected with a precision of 0.81, recall of 0.89,\nand F1-score of 0.85 on 205 images, and potholes with a precision of 0.96,\nrecall of 0.90, and F1-score of 0.93 on 189 images. The macro and weighted\naverage of precision, recall, and F1-score were identical at 0.88, confirming\nthe BCNN's excellent performance in classifying complex pavement crack\npatterns. This research demonstrates the potential of BCNNs to significantly\nenhance the accuracy and reliability of pavement distress classification,\nresulting in more effective and efficient pavement maintenance and management\nsystems.", "AI": {"task": "\u5229\u7528\u53cc\u5411\u7ea7\u8054\u795e\u7ecf\u7f51\u7edc\uff08BCNNs\uff09\u5bf9\u589e\u5f3a\u540e\u7684\u8def\u9762\u88c2\u7f1d\u56fe\u50cf\u8fdb\u884c\u5206\u7c7b\u3002", "motivation": "\u8def\u9762\u88c2\u7f1d\uff08\u5982\u7ebf\u6027\u88c2\u7f1d\u3001\u5751\u6d1e\u548c\u75b2\u52b3\u88c2\u7f1d\uff09\u5bf9\u9053\u8def\u5b89\u5168\u548c\u7ef4\u62a4\u6709\u91cd\u5927\u5f71\u54cd\uff0c\u9700\u8981\u9ad8\u6548\u51c6\u786e\u7684\u5206\u7c7b\u65b9\u6cd5\u3002", "method": "\u91c7\u7528U-Net 50\u8fdb\u884c\u56fe\u50cf\u589e\u5f3a\uff0c\u6784\u5efa\u53cc\u5411\u7ea7\u8054\u795e\u7ecf\u7f51\u7edc\uff08BCNNs\uff09\u6a21\u578b\uff0c\u5229\u7528\u524d\u5411\u548c\u540e\u5411\u4fe1\u606f\u6d41\u63d0\u5347\u5206\u7c7b\u7cbe\u5ea6\u3002", "result": "\u6a21\u578b\u6574\u4f53\u51c6\u786e\u7387\u4e3a87%\uff0c\u5404\u7c7b\u522b\u7684\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u5176\u4e2d\u75b2\u52b3\u88c2\u7f1d\u7684F1\u5206\u6570\u4e3a0.85\uff0c\u7ebf\u6027\u88c2\u7f1d\u4e3a0.85\uff0c\u5751\u6d1e\u4e3a0.93\u3002", "conclusion": "BCNNs\u5728\u8def\u9762\u88c2\u7f1d\u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u53ef\u663e\u8457\u63d0\u5347\u8def\u9762\u7ef4\u62a4\u548c\u7ba1\u7406\u7684\u6548\u7387\u4e0e\u53ef\u9760\u6027\u3002"}}
{"id": "2503.22196", "pdf": "https://arxiv.org/pdf/2503.22196", "abs": "https://arxiv.org/abs/2503.22196", "authors": ["Jiyu Chen", "Shuang Peng", "Daxiong Luo", "Fan Yang", "Renshou Wu", "Fangyuan Li", "Xiaoxin Chen"], "title": "EdgeInfinite: A Memory-Efficient Infinite-Context Transformer for Edge Devices", "categories": ["cs.CL"], "comment": "8 pages, 3 figures", "summary": "Transformer-based large language models (LLMs) encounter challenges in\nprocessing long sequences on edge devices due to the quadratic complexity of\nattention mechanisms and growing memory demands from Key-Value (KV) cache.\nExisting KV cache optimizations struggle with irreversible token eviction in\nlong-output tasks, while alternative sequence modeling architectures prove\ncostly to adopt within established Transformer infrastructure. We present\nEdgeInfinite, a memory-efficient solution for infinite contexts that integrates\ncompressed memory into Transformer-based LLMs through a trainable memory-gating\nmodule. This approach maintains full compatibility with standard Transformer\narchitectures, requiring fine-tuning only a small part of parameters, and\nenables selective activation of the memory-gating module for long and short\ncontext task routing. The experimental result shows that EdgeInfinite achieves\ncomparable performance to baseline Transformer-based LLM on long context\nbenchmarks while optimizing memory consumption and time to first token.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aEdgeInfinite\u7684\u5185\u5b58\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u5904\u7406Transformer-based LLMs\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5904\u7406\u957f\u5e8f\u5217\u65f6\u7684\u6311\u6218\u3002", "motivation": "Transformer-based LLMs\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5904\u7406\u957f\u5e8f\u5217\u65f6\u9762\u4e34\u6ce8\u610f\u529b\u673a\u5236\u4e8c\u6b21\u590d\u6742\u6027\u548cKV\u7f13\u5b58\u5185\u5b58\u9700\u6c42\u589e\u52a0\u7684\u6311\u6218\uff0c\u73b0\u6709\u4f18\u5316\u65b9\u6cd5\u96be\u4ee5\u89e3\u51b3\u957f\u8f93\u51fa\u4efb\u52a1\u4e2d\u7684\u4e0d\u53ef\u9006\u4ee4\u724c\u9a71\u9010\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u53ef\u8bad\u7ec3\u7684\u8bb0\u5fc6\u95e8\u63a7\u6a21\u5757\u5c06\u538b\u7f29\u5185\u5b58\u96c6\u6210\u5230Transformer-based LLMs\u4e2d\uff0c\u4fdd\u6301\u4e0e\u6807\u51c6Transformer\u67b6\u6784\u7684\u5b8c\u5168\u517c\u5bb9\u6027\uff0c\u4ec5\u9700\u5fae\u8c03\u5c11\u91cf\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cEdgeInfinite\u5728\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0e\u57fa\u7ebfTransformer-based LLM\u6027\u80fd\u76f8\u5f53\uff0c\u540c\u65f6\u4f18\u5316\u4e86\u5185\u5b58\u6d88\u8017\u548c\u9996\u4ee4\u724c\u751f\u6210\u65f6\u95f4\u3002", "conclusion": "EdgeInfinite\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u517c\u5bb9\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u957f\u5e8f\u5217\u5904\u7406\u4efb\u52a1\u3002"}}
{"id": "2503.21958", "pdf": "https://arxiv.org/pdf/2503.21958", "abs": "https://arxiv.org/abs/2503.21958", "authors": ["Kibon Ku", "Talukder Z Jubery", "Elijah Rodriguez", "Aditya Balu", "Soumik Sarkar", "Adarsh Krishnamurthy", "Baskar Ganapathysubramanian"], "title": "NeRF-based Point Cloud Reconstruction using a Stationary Camera for Agricultural Applications", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents a NeRF-based framework for point cloud (PCD)\nreconstruction, specifically designed for indoor high-throughput plant\nphenotyping facilities. Traditional NeRF-based reconstruction methods require\ncameras to move around stationary objects, but this approach is impractical for\nhigh-throughput environments where objects are rapidly imaged while moving on\nconveyors or rotating pedestals. To address this limitation, we develop a\nvariant of NeRF-based PCD reconstruction that uses a single stationary camera\nto capture images as the object rotates on a pedestal. Our workflow comprises\nCOLMAP-based pose estimation, a straightforward pose transformation to simulate\ncamera movement, and subsequent standard NeRF training. A defined Region of\nInterest (ROI) excludes irrelevant scene data, enabling the generation of\nhigh-resolution point clouds (10M points). Experimental results demonstrate\nexcellent reconstruction fidelity, with precision-recall analyses yielding an\nF-score close to 100.00 across all evaluated plant objects. Although pose\nestimation remains computationally intensive with a stationary camera setup,\noverall training and reconstruction times are competitive, validating the\nmethod's feasibility for practical high-throughput indoor phenotyping\napplications. Our findings indicate that high-quality NeRF-based 3D\nreconstructions are achievable using a stationary camera, eliminating the need\nfor complex camera motion or costly imaging equipment. This approach is\nespecially beneficial when employing expensive and delicate instruments, such\nas hyperspectral cameras, for 3D plant phenotyping. Future work will focus on\noptimizing pose estimation techniques and further streamlining the methodology\nto facilitate seamless integration into automated, high-throughput 3D\nphenotyping pipelines.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eNeRF\u7684\u70b9\u4e91\u91cd\u5efa\u6846\u67b6\uff0c\u4e13\u4e3a\u5ba4\u5185\u9ad8\u901a\u91cf\u690d\u7269\u8868\u578b\u8bbe\u65bd\u8bbe\u8ba1\u3002", "motivation": "\u4f20\u7edfNeRF\u65b9\u6cd5\u9700\u8981\u76f8\u673a\u56f4\u7ed5\u9759\u6b62\u7269\u4f53\u79fb\u52a8\uff0c\u4f46\u5728\u9ad8\u901a\u91cf\u73af\u5883\u4e2d\uff0c\u7269\u4f53\u901a\u5e38\u5728\u4f20\u9001\u5e26\u6216\u65cb\u8f6c\u53f0\u4e0a\u5feb\u901f\u79fb\u52a8\uff0c\u4f20\u7edf\u65b9\u6cd5\u4e0d\u9002\u7528\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u53d8\u4f53NeRF\u65b9\u6cd5\uff0c\u4f7f\u7528\u5355\u4e2a\u9759\u6b62\u76f8\u673a\u6355\u6349\u65cb\u8f6c\u7269\u4f53\u56fe\u50cf\uff0c\u7ed3\u5408COLMAP\u59ff\u6001\u4f30\u8ba1\u3001\u59ff\u6001\u53d8\u6362\u6a21\u62df\u76f8\u673a\u79fb\u52a8\u53ca\u6807\u51c6NeRF\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u91cd\u5efa\u4fdd\u771f\u5ea6\u4f18\u5f02\uff0cF-score\u63a5\u8fd1100.00\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u9ad8\u901a\u91cf\u8868\u578b\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u4f7f\u7528\u9759\u6b62\u76f8\u673a\u53ef\u5b9e\u73b0\u9ad8\u8d28\u91cfNeRF\u91cd\u5efa\uff0c\u65e0\u9700\u590d\u6742\u76f8\u673a\u8fd0\u52a8\u6216\u6602\u8d35\u8bbe\u5907\uff0c\u672a\u6765\u5c06\u4f18\u5316\u59ff\u6001\u4f30\u8ba1\u4ee5\u96c6\u6210\u5230\u81ea\u52a8\u5316\u6d41\u7a0b\u4e2d\u3002"}}
{"id": "2503.22277", "pdf": "https://arxiv.org/pdf/2503.22277", "abs": "https://arxiv.org/abs/2503.22277", "authors": ["Fabian Schmidt", "Karin Hammerfald", "Henrik Haaland Jahren", "Vladimir Vlassov"], "title": "CFiCS: Graph-Based Classification of Common Factors and Microcounseling Skills", "categories": ["cs.CL"], "comment": "10 pages, 3 figures, 2 tables", "summary": "Common factors and microcounseling skills are critical to the effectiveness\nof psychotherapy. Understanding and measuring these elements provides valuable\ninsights into therapeutic processes and outcomes. However, automatic\nidentification of these change principles from textual data remains challenging\ndue to the nuanced and context-dependent nature of therapeutic dialogue. This\npaper introduces CFiCS, a hierarchical classification framework integrating\ngraph machine learning with pretrained contextual embeddings. We represent\ncommon factors, intervention concepts, and microcounseling skills as a\nheterogeneous graph, where textual information from ClinicalBERT enriches each\nnode. This structure captures both the hierarchical relationships (e.g.,\nskill-level nodes linking to broad factors) and the semantic properties of\ntherapeutic concepts. By leveraging graph neural networks, CFiCS learns\ninductive node embeddings that generalize to unseen text samples lacking\nexplicit connections. Our results demonstrate that integrating ClinicalBERT\nnode features and graph structure significantly improves classification\nperformance, especially in fine-grained skill prediction. CFiCS achieves\nsubstantial gains in both micro and macro F1 scores across all tasks compared\nto baselines, including random forests, BERT-based multi-task models, and\ngraph-based methods.", "AI": {"task": "\u5f00\u53d1\u4e00\u4e2a\u540d\u4e3aCFiCS\u7684\u5206\u5c42\u5206\u7c7b\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u5fc3\u7406\u6cbb\u7597\u5bf9\u8bdd\u6587\u672c\u4e2d\u81ea\u52a8\u8bc6\u522b\u5171\u540c\u56e0\u7d20\u548c\u5fae\u89c2\u54a8\u8be2\u6280\u80fd\u3002", "motivation": "\u5171\u540c\u56e0\u7d20\u548c\u5fae\u89c2\u54a8\u8be2\u6280\u80fd\u5bf9\u5fc3\u7406\u6cbb\u7597\u7684\u6709\u6548\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u56e0\u5176\u5fae\u5999\u548c\u4e0a\u4e0b\u6587\u4f9d\u8d56\u7684\u7279\u6027\uff0c\u4ece\u6587\u672c\u6570\u636e\u4e2d\u81ea\u52a8\u8bc6\u522b\u8fd9\u4e9b\u53d8\u5316\u539f\u5219\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u7ed3\u5408\u56fe\u673a\u5668\u5b66\u4e60\u548c\u9884\u8bad\u7ec3\u4e0a\u4e0b\u6587\u5d4c\u5165\uff08\u5982ClinicalBERT\uff09\uff0c\u6784\u5efa\u5f02\u6784\u56fe\u8868\u793a\u5171\u540c\u56e0\u7d20\u3001\u5e72\u9884\u6982\u5ff5\u548c\u5fae\u89c2\u54a8\u8be2\u6280\u80fd\uff0c\u5e76\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u5f52\u7eb3\u8282\u70b9\u5d4c\u5165\u3002", "result": "CFiCS\u5728\u5206\u7c7b\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff08\u5982\u968f\u673a\u68ee\u6797\u3001\u57fa\u4e8eBERT\u7684\u591a\u4efb\u52a1\u6a21\u578b\u548c\u56fe\u65b9\u6cd5\uff09\uff0c\u5c24\u5176\u5728\u7ec6\u7c92\u5ea6\u6280\u80fd\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "CFiCS\u901a\u8fc7\u6574\u5408ClinicalBERT\u8282\u70b9\u7279\u5f81\u548c\u56fe\u7ed3\u6784\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5171\u540c\u56e0\u7d20\u548c\u5fae\u89c2\u54a8\u8be2\u6280\u80fd\u7684\u81ea\u52a8\u8bc6\u522b\u80fd\u529b\u3002"}}
{"id": "2503.21970", "pdf": "https://arxiv.org/pdf/2503.21970", "abs": "https://arxiv.org/abs/2503.21970", "authors": ["Yujie Chen", "Haotong Qin", "Zhang Zhang", "Michelo Magno", "Luca Benini", "Yawei Li"], "title": "Q-MambaIR: Accurate Quantized Mamba for Efficient Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "State-Space Models (SSMs) have attracted considerable attention in Image\nRestoration (IR) due to their ability to scale linearly sequence length while\neffectively capturing long-distance dependencies. However, deploying SSMs to\nedge devices is challenging due to the constraints in memory, computing\ncapacity, and power consumption, underscoring the need for efficient\ncompression strategies. While low-bit quantization is an efficient model\ncompression strategy for reducing size and accelerating IR tasks, SSM suffers\nsubstantial performance drops at ultra-low bit-widths (2-4 bits), primarily due\nto outliers that exacerbate quantization error. To address this challenge, we\npropose Q-MambaIR, an accurate, efficient, and flexible Quantized Mamba for IR\ntasks. Specifically, we introduce a Statistical Dynamic-balancing Learnable\nScalar (DLS) to dynamically adjust the quantization mapping range, thereby\nmitigating the peak truncation loss caused by extreme values. Furthermore, we\ndesign a Range-floating Flexible Allocator (RFA) with an adaptive threshold to\nflexibly round values. This approach preserves high-frequency details and\nmaintains the SSM's feature extraction capability. Notably, RFA also enables\npre-deployment weight quantization, striking a balance between computational\nefficiency and model accuracy. Extensive experiments on IR tasks demonstrate\nthat Q-MambaIR consistently outperforms existing quantized SSMs, achieving much\nhigher state-of-the-art (SOTA) accuracy results with only a negligible increase\nin training computation and storage saving.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u3001\u51c6\u786e\u7684\u91cf\u5316\u65b9\u6cd5Q-MambaIR\uff0c\u7528\u4e8e\u89e3\u51b3State-Space Models (SSMs)\u5728\u8d85\u4f4e\u6bd4\u7279\u5bbd\u5ea6\uff082-4\u4f4d\uff09\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "SSMs\u5728\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u65f6\u9762\u4e34\u5185\u5b58\u3001\u8ba1\u7b97\u80fd\u529b\u548c\u529f\u8017\u7684\u9650\u5236\uff0c\u9700\u8981\u9ad8\u6548\u7684\u538b\u7f29\u7b56\u7565\u3002", "method": "\u5f15\u5165\u7edf\u8ba1\u52a8\u6001\u5e73\u8861\u53ef\u5b66\u4e60\u6807\u91cf\uff08DLS\uff09\u548c\u8303\u56f4\u6d6e\u52a8\u7075\u6d3b\u5206\u914d\u5668\uff08RFA\uff09\uff0c\u52a8\u6001\u8c03\u6574\u91cf\u5316\u6620\u5c04\u8303\u56f4\u548c\u81ea\u9002\u5e94\u9608\u503c\uff0c\u4ee5\u51cf\u5c11\u91cf\u5316\u8bef\u5dee\u5e76\u4fdd\u6301\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u3002", "result": "Q-MambaIR\u5728\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u91cf\u5316SSMs\uff0c\u4ee5\u6781\u5c0f\u7684\u8ba1\u7b97\u548c\u5b58\u50a8\u5f00\u9500\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u7387\u3002", "conclusion": "Q-MambaIR\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u91cf\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86SSMs\u5728\u8d85\u4f4e\u6bd4\u7279\u5bbd\u5ea6\u4e0b\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u3002"}}
{"id": "2503.22280", "pdf": "https://arxiv.org/pdf/2503.22280", "abs": "https://arxiv.org/abs/2503.22280", "authors": ["Rrubaa Panchendrarajan", "Rub\u00e9n M\u00edguez", "Arkaitz Zubiaga"], "title": "MultiClaimNet: A Massively Multilingual Dataset of Fact-Checked Claim Clusters", "categories": ["cs.CL"], "comment": null, "summary": "In the context of fact-checking, claims are often repeated across various\nplatforms and in different languages, which can benefit from a process that\nreduces this redundancy. While retrieving previously fact-checked claims has\nbeen investigated as a solution, the growing number of unverified claims and\nexpanding size of fact-checked databases calls for alternative, more efficient\nsolutions. A promising solution is to group claims that discuss the same\nunderlying facts into clusters to improve claim retrieval and validation.\nHowever, research on claim clustering is hindered by the lack of suitable\ndatasets. To bridge this gap, we introduce \\textit{MultiClaimNet}, a collection\nof three multilingual claim cluster datasets containing claims in 86 languages\nacross diverse topics. Claim clusters are formed automatically from\nclaim-matching pairs with limited manual intervention. We leverage two existing\nclaim-matching datasets to form the smaller datasets within\n\\textit{MultiClaimNet}. To build the larger dataset, we propose and validate an\napproach involving retrieval of approximate nearest neighbors to form candidate\nclaim pairs and an automated annotation of claim similarity using large\nlanguage models. This larger dataset contains 85.3K fact-checked claims written\nin 78 languages. We further conduct extensive experiments using various\nclustering techniques and sentence embedding models to establish baseline\nperformance. Our datasets and findings provide a strong foundation for scalable\nclaim clustering, contributing to efficient fact-checking pipelines.", "AI": {"task": "\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e00\u79cd\u591a\u8bed\u8a00\u58f0\u660e\u805a\u7c7b\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11\u4e8b\u5b9e\u6838\u67e5\u4e2d\u7684\u5197\u4f59\u3002", "motivation": "\u58f0\u660e\u5728\u4e0d\u540c\u5e73\u53f0\u548c\u8bed\u8a00\u4e2d\u91cd\u590d\u51fa\u73b0\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u6765\u51cf\u5c11\u5197\u4f59\u5e76\u63d0\u9ad8\u68c0\u7d22\u6548\u7387\u3002", "method": "\u5f15\u5165MultiClaimNet\u6570\u636e\u96c6\uff0c\u5305\u542b86\u79cd\u8bed\u8a00\u7684\u58f0\u660e\u805a\u7c7b\uff0c\u901a\u8fc7\u81ea\u52a8\u805a\u7c7b\u548c\u6709\u9650\u4eba\u5de5\u5e72\u9884\u6784\u5efa\u6570\u636e\u96c6\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b85.3K\u58f0\u660e\u7684\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u57fa\u7ebf\u6027\u80fd\u3002", "conclusion": "MultiClaimNet\u4e3a\u53ef\u6269\u5c55\u7684\u58f0\u660e\u805a\u7c7b\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u9ad8\u6548\u7684\u4e8b\u5b9e\u6838\u67e5\u6d41\u7a0b\u3002"}}
{"id": "2503.21979", "pdf": "https://arxiv.org/pdf/2503.21979", "abs": "https://arxiv.org/abs/2503.21979", "authors": ["Size Wu", "Wenwei Zhang", "Lumin Xu", "Sheng Jin", "Zhonghua Wu", "Qingyi Tao", "Wentao Liu", "Wei Li", "Chen Change Loy"], "title": "Harmonizing Visual Representations for Unified Multimodal Understanding and Generation", "categories": ["cs.CV"], "comment": null, "summary": "Unifying visual understanding and generation within a single multimodal\nframework remains a significant challenge, as the two inherently heterogeneous\ntasks require representations at different levels of granularity. Current\napproaches that utilize vector quantization (VQ) or variational autoencoders\n(VAE) for unified visual representation prioritize intrinsic imagery features\nover semantics, compromising understanding performance. In this work, we take\ninspiration from masked image modelling (MIM) that learns rich semantics via a\nmask-and-reconstruct pre-training and its successful extension to masked\nautoregressive (MAR) image generation. A preliminary study on the MAR encoder's\nrepresentation reveals exceptional linear probing accuracy and precise feature\nresponse to visual concepts, which indicates MAR's potential for visual\nunderstanding tasks beyond its original generation role. Based on these\ninsights, we present \\emph{Harmon}, a unified autoregressive framework that\nharmonizes understanding and generation tasks with a shared MAR encoder.\nThrough a three-stage training procedure that progressively optimizes\nunderstanding and generation capabilities, Harmon achieves state-of-the-art\nimage generation results on the GenEval, MJHQ30K and WISE benchmarks while\nmatching the performance of methods with dedicated semantic encoders (e.g.,\nJanus) on image understanding benchmarks. Our code and models will be available\nat https://github.com/wusize/Harmon.", "AI": {"task": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u89c6\u89c9\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u7684\u81ea\u56de\u5f52\u6846\u67b6Harmon\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u7edf\u4e00\u89c6\u89c9\u8868\u793a\u65f6\u8fc7\u4e8e\u5173\u6ce8\u56fe\u50cf\u7279\u5f81\u800c\u5ffd\u7565\u8bed\u4e49\uff0c\u5bfc\u81f4\u7406\u89e3\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u57fa\u4e8e\u63a9\u7801\u81ea\u56de\u5f52\uff08MAR\uff09\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u8bad\u7ec3\u9010\u6b65\u4f18\u5316\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\u3002", "result": "\u5728\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e0a\u8fbe\u5230SOTA\uff0c\u540c\u65f6\u5728\u56fe\u50cf\u7406\u89e3\u4efb\u52a1\u4e0a\u5ab2\u7f8e\u4e13\u7528\u8bed\u4e49\u7f16\u7801\u5668\u3002", "conclusion": "Harmon\u6846\u67b6\u6210\u529f\u7edf\u4e00\u4e86\u89c6\u89c9\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\uff0c\u5c55\u793a\u4e86MAR\u7f16\u7801\u5668\u7684\u6f5c\u529b\u3002"}}
{"id": "2503.22303", "pdf": "https://arxiv.org/pdf/2503.22303", "abs": "https://arxiv.org/abs/2503.22303", "authors": ["Magdalena Kaiser", "Gerhard Weikum"], "title": "Preference-based Learning with Retrieval Augmented Generation for Conversational Question Answering", "categories": ["cs.CL", "cs.IR"], "comment": "WWW 2025 Short Paper, 5 pages", "summary": "Conversational Question Answering (ConvQA) involves multiple subtasks, i) to\nunderstand incomplete questions in their context, ii) to retrieve relevant\ninformation, and iii) to generate answers. This work presents PRAISE, a\npipeline-based approach for ConvQA that trains LLM adapters for each of the\nthree subtasks. As labeled training data for individual subtasks is unavailable\nin practice, PRAISE learns from its own generations using the final answering\nperformance as feedback signal without human intervention and treats\nintermediate information, like relevant evidence, as weakly labeled data. We\napply Direct Preference Optimization by contrasting successful and unsuccessful\nsamples for each subtask. In our experiments, we show the effectiveness of this\ntraining paradigm: PRAISE shows improvements per subtask and achieves new\nstate-of-the-art performance on a popular ConvQA benchmark, by gaining 15.5\npercentage points increase in precision over baselines.", "AI": {"task": "\u63d0\u51faPRAISE\uff0c\u4e00\u79cd\u57fa\u4e8e\u7ba1\u9053\u7684\u5bf9\u8bdd\u95ee\u7b54\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3LLM\u9002\u914d\u5668\u5206\u522b\u5904\u7406\u4e09\u4e2a\u5b50\u4efb\u52a1\u3002", "motivation": "\u7531\u4e8e\u5b9e\u9645\u4e2d\u7f3a\u4e4f\u9488\u5bf9\u5355\u4e2a\u5b50\u4efb\u52a1\u7684\u6807\u6ce8\u8bad\u7ec3\u6570\u636e\uff0cPRAISE\u901a\u8fc7\u81ea\u8eab\u751f\u6210\u7684\u6570\u636e\u5b66\u4e60\uff0c\u5229\u7528\u6700\u7ec8\u56de\u7b54\u6027\u80fd\u4f5c\u4e3a\u53cd\u9988\u4fe1\u53f7\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u3002", "method": "\u91c7\u7528Direct Preference Optimization\u65b9\u6cd5\uff0c\u5bf9\u6bd4\u6bcf\u4e2a\u5b50\u4efb\u52a1\u7684\u6210\u529f\u4e0e\u5931\u8d25\u6837\u672c\uff0c\u5229\u7528\u4e2d\u95f4\u4fe1\u606f\uff08\u5982\u76f8\u5173\u8bc1\u636e\uff09\u4f5c\u4e3a\u5f31\u6807\u6ce8\u6570\u636e\u3002", "result": "PRAISE\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6bcf\u4e2a\u5b50\u4efb\u52a1\u5747\u6709\u6539\u8fdb\uff0c\u5e76\u5728\u6d41\u884c\u5bf9\u8bdd\u95ee\u7b54\u57fa\u51c6\u4e0a\u8fbe\u5230\u65b0\u7684\u6700\u4f18\u6027\u80fd\uff0c\u7cbe\u5ea6\u63d0\u534715.5\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "PRAISE\u901a\u8fc7\u81ea\u5b66\u4e60\u548c\u5f31\u76d1\u7763\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u8bdd\u95ee\u7b54\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2503.21990", "pdf": "https://arxiv.org/pdf/2503.21990", "abs": "https://arxiv.org/abs/2503.21990", "authors": ["Isaac Kazuo Uyehara", "Heesup Yun", "Earl Ranario", "Mason Earles"], "title": "AgRowStitch: A High-fidelity Image Stitching Pipeline for Ground-based Agricultural Images", "categories": ["cs.CV"], "comment": null, "summary": "Agricultural imaging often requires individual images to be stitched together\ninto a final mosaic for analysis. However, agricultural images can be\nparticularly challenging to stitch because feature matching across images is\ndifficult due to repeated textures, plants are non-planar, and mosaics built\nfrom many images can accumulate errors that cause drift. Although these issues\ncan be mitigated by using georeferenced images or taking images at high\naltitude, there is no general solution for images taken close to the crop. To\naddress this, we created a user-friendly and open source pipeline for stitching\nground-based images of a linear row of crops that does not rely on additional\ndata. First, we use SuperPoint and LightGlue to extract and match features\nwithin small batches of images. Then we stitch the images in each batch in\nseries while imposing constraints on the camera movement. After straightening\nand rescaling each batch mosaic, all batch mosaics are stitched together in\nseries and then straightened into a final mosaic. We tested the pipeline on\nimages collected along 72 m long rows of crops using two different agricultural\nrobots and a camera manually carried over the row. In all three cases, the\npipeline produced high-quality mosaics that could be used to georeference real\nworld positions with a mean absolute error of 20 cm. This approach provides\naccessible leaf-scale stitching to users who need to coarsely georeference\npositions within a row, but do not have access to accurate positional data or\nsophisticated imaging systems.", "AI": {"task": "\u5f00\u53d1\u4e00\u79cd\u7528\u6237\u53cb\u597d\u4e14\u5f00\u6e90\u7684\u7ba1\u9053\uff0c\u7528\u4e8e\u62fc\u63a5\u5730\u9762\u62cd\u6444\u7684\u7ebf\u6027\u4f5c\u7269\u884c\u56fe\u50cf\uff0c\u800c\u4e0d\u4f9d\u8d56\u989d\u5916\u6570\u636e\u3002", "motivation": "\u519c\u4e1a\u56fe\u50cf\u62fc\u63a5\u56e0\u91cd\u590d\u7eb9\u7406\u3001\u975e\u5e73\u9762\u690d\u7269\u548c\u591a\u56fe\u50cf\u7d2f\u79ef\u8bef\u5dee\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u4e3a\u8fd1\u8ddd\u79bb\u62cd\u6444\u7684\u56fe\u50cf\u63d0\u4f9b\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528SuperPoint\u548cLightGlue\u63d0\u53d6\u548c\u5339\u914d\u5c0f\u6279\u91cf\u56fe\u50cf\u7279\u5f81\uff0c\u901a\u8fc7\u7ea6\u675f\u76f8\u673a\u8fd0\u52a8\u9010\u6279\u62fc\u63a5\uff0c\u6700\u540e\u5c06\u6240\u6709\u6279\u6b21\u7684\u62fc\u63a5\u7ed3\u679c\u6574\u5408\u4e3a\u6700\u7ec8\u9a6c\u8d5b\u514b\u3002", "result": "\u5728\u4e09\u79cd\u4e0d\u540c\u91c7\u96c6\u65b9\u5f0f\u4e0b\uff0c\u7ba1\u9053\u751f\u6210\u4e86\u9ad8\u8d28\u91cf\u9a6c\u8d5b\u514b\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a20\u5398\u7c73\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9700\u8981\u7c97\u7565\u5730\u7406\u5b9a\u4f4d\u7684\u7528\u6237\u63d0\u4f9b\u4e86\u53ef\u8bbf\u95ee\u7684\u53f6\u7247\u7ea7\u62fc\u63a5\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u7cbe\u786e\u4f4d\u7f6e\u6570\u636e\u6216\u590d\u6742\u6210\u50cf\u7cfb\u7edf\u3002"}}
{"id": "2503.22329", "pdf": "https://arxiv.org/pdf/2503.22329", "abs": "https://arxiv.org/abs/2503.22329", "authors": ["Louis Owen", "Nilabhra Roy Chowdhury", "Abhay Kumar", "Fabian G\u00fcra"], "title": "A Refined Analysis of Massive Activations in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Motivated in part by their relevance for low-precision training and\nquantization, massive activations in large language models (LLMs) have recently\nemerged as a topic of interest. However, existing analyses are limited in\nscope, and generalizability across architectures is unclear. This paper helps\naddress some of these gaps by conducting an analysis of massive activations\nacross a broad range of LLMs, including both GLU-based and non-GLU-based\narchitectures. Our findings challenge several prior assumptions, most\nimportantly: (1) not all massive activations are detrimental, i.e. suppressing\nthem does not lead to an explosion of perplexity or a collapse in downstream\ntask performance; (2) proposed mitigation strategies such as Attention KV bias\nare model-specific and ineffective in certain cases. We consequently\ninvestigate novel hybrid mitigation strategies; in particular pairing Target\nVariance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT)\nsuccessfully balances the mitigation of massive activations with preserved\ndownstream model performance in the scenarios we investigated. Our code is\navailable at: https://github.com/bluorion-com/refine_massive_activations.", "AI": {"task": "\u5206\u6790\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5927\u89c4\u6a21\u6fc0\u6d3b\u73b0\u8c61\u53ca\u5176\u7f13\u89e3\u7b56\u7565\u3002", "motivation": "\u7814\u7a76\u5927\u89c4\u6a21\u6fc0\u6d3b\u5bf9\u4f4e\u7cbe\u5ea6\u8bad\u7ec3\u548c\u91cf\u5316\u7684\u5f71\u54cd\uff0c\u586b\u8865\u73b0\u6709\u5206\u6790\u7684\u5c40\u9650\u6027\u3002", "method": "\u5bf9\u591a\u79cdLLM\uff08\u5305\u62ecGLU\u548c\u975eGLU\u67b6\u6784\uff09\u8fdb\u884c\u5927\u89c4\u6a21\u6fc0\u6d3b\u5206\u6790\uff0c\u5e76\u63d0\u51fa\u6df7\u5408\u7f13\u89e3\u7b56\u7565\uff08\u5982TVR\u4e0eAttention KV bias\u6216DyT\u7ed3\u5408\uff09\u3002", "result": "\u6311\u6218\u4e86\u5148\u524d\u7684\u5047\u8bbe\uff0c\u53d1\u73b0\u5e76\u975e\u6240\u6709\u5927\u89c4\u6a21\u6fc0\u6d3b\u90fd\u6709\u5bb3\uff0c\u4e14\u67d0\u4e9b\u7f13\u89e3\u7b56\u7565\u6a21\u578b\u7279\u5f02\u6027\u5f3a\uff1b\u6df7\u5408\u7b56\u7565\u80fd\u6709\u6548\u5e73\u8861\u7f13\u89e3\u4e0e\u6027\u80fd\u3002", "conclusion": "\u6df7\u5408\u7b56\u7565\uff08\u5982TVR\u4e0eAttention KV bias\u6216DyT\u7ed3\u5408\uff09\u5728\u5927\u89c4\u6a21\u6fc0\u6d3b\u7f13\u89e3\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2503.21991", "pdf": "https://arxiv.org/pdf/2503.21991", "abs": "https://arxiv.org/abs/2503.21991", "authors": ["Hang Zhou", "Xinxin Zuo", "Rui Ma", "Li Cheng"], "title": "BOOTPLACE: Bootstrapped Object Placement with Detection Transformers", "categories": ["cs.CV", "cs.AI", "cs.GR"], "comment": "CVPR 2025. Project page: https://ryanhangzhou.github.io/bootplace/ ,\n  code: https://github.com/RyanHangZhou/BOOTPLACE", "summary": "In this paper, we tackle the copy-paste image-to-image composition problem\nwith a focus on object placement learning. Prior methods have leveraged\ngenerative models to reduce the reliance for dense supervision. However, this\noften limits their capacity to model complex data distributions. Alternatively,\ntransformer networks with a sparse contrastive loss have been explored, but\ntheir over-relaxed regularization often leads to imprecise object placement. We\nintroduce BOOTPLACE, a novel paradigm that formulates object placement as a\nplacement-by-detection problem. Our approach begins by identifying suitable\nregions of interest for object placement. This is achieved by training a\nspecialized detection transformer on object-subtracted backgrounds, enhanced\nwith multi-object supervisions. It then semantically associates each target\ncompositing object with detected regions based on their complementary\ncharacteristics. Through a boostrapped training approach applied to randomly\nobject-subtracted images, our model enforces meaningful placements through\nextensive paired data augmentation. Experimental results on established\nbenchmarks demonstrate BOOTPLACE's superior performance in object\nrepositioning, markedly surpassing state-of-the-art baselines on Cityscapes and\nOPA datasets with notable improvements in IOU scores. Additional ablation\nstudies further showcase the compositionality and generalizability of our\napproach, supported by user study evaluations.", "AI": {"task": "\u89e3\u51b3\u56fe\u50cf\u590d\u5236\u7c98\u8d34\u4e2d\u7684\u5bf9\u8c61\u653e\u7f6e\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u653e\u7f6e\u68c0\u6d4b\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u6a21\u578b\u5bf9\u590d\u6742\u6570\u636e\u5206\u5e03\u5efa\u6a21\u80fd\u529b\u6709\u9650\uff0c\u800c\u57fa\u4e8e\u7a00\u758f\u5bf9\u6bd4\u635f\u5931\u7684Transformer\u7f51\u7edc\u5219\u56e0\u6b63\u5219\u5316\u8fc7\u677e\u5bfc\u81f4\u5bf9\u8c61\u653e\u7f6e\u4e0d\u7cbe\u786e\u3002", "method": "\u63d0\u51faBOOTPLACE\u65b9\u6cd5\uff0c\u5c06\u5bf9\u8c61\u653e\u7f6e\u95ee\u9898\u8f6c\u5316\u4e3a\u68c0\u6d4b\u95ee\u9898\uff0c\u901a\u8fc7\u8bad\u7ec3\u4e13\u7528\u68c0\u6d4bTransformer\u548c\u591a\u5bf9\u8c61\u76d1\u7763\u5b9e\u73b0\u3002", "result": "\u5728Cityscapes\u548cOPA\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cIOU\u5206\u6570\u6709\u660e\u663e\u63d0\u5347\u3002", "conclusion": "BOOTPLACE\u65b9\u6cd5\u5728\u5bf9\u8c61\u653e\u7f6e\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u7ec4\u5408\u6027\u548c\u6cdb\u5316\u6027\u3002"}}
{"id": "2503.22338", "pdf": "https://arxiv.org/pdf/2503.22338", "abs": "https://arxiv.org/abs/2503.22338", "authors": ["Shrikant Malviya", "Pablo Arnau-Gonz\u00e1lez", "Miguel Arevalillo-Herr\u00e1ez", "Stamos Katsigiannis"], "title": "SKDU at De-Factify 4.0: Natural Language Features for AI-Generated Text-Detection", "categories": ["cs.CL"], "comment": "De-Factify 4.0 Workshop at the 39th AAAI Conference on Artificial\n  Intelligence (AAAI 2025)", "summary": "The rapid advancement of large language models (LLMs) has introduced new\nchallenges in distinguishing human-written text from AI-generated content. In\nthis work, we explored a pipelined approach for AI-generated text detection\nthat includes a feature extraction step (i.e. prompt-based rewriting features\ninspired by RAIDAR and content-based features derived from the NELA toolkit)\nfollowed by a classification module. Comprehensive experiments were conducted\non the Defactify4.0 dataset, evaluating two tasks: binary classification to\ndifferentiate human-written and AI-generated text, and multi-class\nclassification to identify the specific generative model used to generate the\ninput text. Our findings reveal that NELA features significantly outperform\nRAIDAR features in both tasks, demonstrating their ability to capture nuanced\nlinguistic, stylistic, and content-based differences. Combining RAIDAR and NELA\nfeatures provided minimal improvement, highlighting the redundancy introduced\nby less discriminative features. Among the classifiers tested, XGBoost emerged\nas the most effective, leveraging the rich feature sets to achieve high\naccuracy and generalisation.", "AI": {"task": "\u63a2\u7d22\u4e00\u79cd\u7528\u4e8e\u68c0\u6d4bAI\u751f\u6210\u6587\u672c\u7684\u6d41\u6c34\u7ebf\u65b9\u6cd5\uff0c\u5305\u62ec\u7279\u5f81\u63d0\u53d6\u548c\u5206\u7c7b\u6a21\u5757\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5feb\u901f\u53d1\u5c55\u4f7f\u5f97\u533a\u5206\u4eba\u7c7b\u4e66\u5199\u6587\u672c\u548cAI\u751f\u6210\u5185\u5bb9\u53d8\u5f97\u66f4\u5177\u6311\u6218\u6027\u3002", "method": "\u91c7\u7528\u7279\u5f81\u63d0\u53d6\uff08\u57fa\u4e8e\u63d0\u793a\u7684\u91cd\u5199\u7279\u5f81RAIDAR\u548c\u57fa\u4e8e\u5185\u5bb9\u7684\u7279\u5f81NELA\uff09\u548c\u5206\u7c7b\u6a21\u5757\u7684\u6d41\u6c34\u7ebf\u65b9\u6cd5\u3002", "result": "NELA\u7279\u5f81\u5728\u533a\u5206\u4eba\u7c7b\u548cAI\u751f\u6210\u6587\u672c\u53ca\u8bc6\u522b\u751f\u6210\u6a21\u578b\u7684\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8eRAIDAR\u7279\u5f81\uff0cXGBoost\u5206\u7c7b\u5668\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "NELA\u7279\u5f81\u80fd\u6709\u6548\u6355\u6349\u7ec6\u5fae\u5dee\u5f02\uff0c\u7ec4\u5408\u7279\u5f81\u6539\u8fdb\u6709\u9650\uff0cXGBoost\u662f\u6700\u4f73\u5206\u7c7b\u5668\u3002"}}
{"id": "2503.21999", "pdf": "https://arxiv.org/pdf/2503.21999", "abs": "https://arxiv.org/abs/2503.21999", "authors": ["Tony Tran", "Bin Hu"], "title": "FACETS: Efficient Once-for-all Object Detection via Constrained Iterative Search", "categories": ["cs.CV", "cs.LG"], "comment": "10 pages, 6 figures", "summary": "Neural Architecture Search (NAS) for deep learning object detection\nframeworks typically involves multiple modules, each performing distinct tasks.\nThese modules contribute to a vast search space, resulting in searches that can\ntake several GPU hours or even days, depending on the complexity of the search\nspace. This makes joint optimization both challenging and computationally\nexpensive. Furthermore, satisfying target device constraints across modules\nadds additional complexity to the optimization process. To address these\nchallenges, we propose \\textbf{FACETS}, e\\textbf{\\underline{F}}ficient\nOnce-for-\\textbf{\\underline{A}}ll Object Detection via\n\\textbf{\\underline{C}}onstrained\nit\\textbf{\\underline{E}}ra\\textbf{\\underline{T}}ive\\textbf{\\underline{S}}earch,\na novel unified iterative NAS method that refines the architecture of all\nmodules in a cyclical manner. FACETS leverages feedback from previous\niterations, alternating between fixing one module's architecture and optimizing\nthe others. This approach reduces the overall search space while preserving\ninterdependencies among modules and incorporates constraints based on the\ntarget device's computational budget. In a controlled comparison against\nprogressive and single-module search strategies, FACETS achieves architectures\nwith up to $4.75\\%$ higher accuracy twice as fast as progressive search\nstrategies in earlier stages, while still being able to achieve a global\noptimum. Moreover, FACETS demonstrates the ability to iteratively refine the\nsearch space, producing better performing architectures over time. The refined\nsearch space yields candidates with a mean accuracy up to $27\\%$ higher than\nglobal search and $5\\%$ higher than progressive search methods via random\nsampling.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aFACETS\u7684\u65b0\u578b\u7edf\u4e00\u8fed\u4ee3\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u6df1\u5ea6\u5b66\u4e60\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\u4e2d\u7684\u591a\u6a21\u5757\u67b6\u6784\u3002", "motivation": "\u89e3\u51b3\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u5728\u591a\u6a21\u5757\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\u4e2d\u56e0\u641c\u7d22\u7a7a\u95f4\u5e9e\u5927\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u800c\u5bfc\u81f4\u7684\u8054\u5408\u4f18\u5316\u96be\u9898\uff0c\u540c\u65f6\u6ee1\u8db3\u76ee\u6807\u8bbe\u5907\u7684\u8ba1\u7b97\u7ea6\u675f\u3002", "method": "FACETS\u901a\u8fc7\u5faa\u73af\u8fed\u4ee3\u7684\u65b9\u5f0f\u4f18\u5316\u5404\u6a21\u5757\u67b6\u6784\uff0c\u5229\u7528\u524d\u6b21\u8fed\u4ee3\u7684\u53cd\u9988\uff0c\u4ea4\u66ff\u56fa\u5b9a\u4e00\u4e2a\u6a21\u5757\u7684\u67b6\u6784\u5e76\u4f18\u5316\u5176\u4ed6\u6a21\u5757\uff0c\u4ece\u800c\u51cf\u5c11\u641c\u7d22\u7a7a\u95f4\u5e76\u4fdd\u6301\u6a21\u5757\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\u3002", "result": "FACETS\u5728\u65e9\u671f\u9636\u6bb5\u6bd4\u6e10\u8fdb\u5f0f\u641c\u7d22\u7b56\u7565\u5feb\u4e24\u500d\uff0c\u4e14\u51c6\u786e\u7387\u63d0\u9ad84.75%\uff0c\u540c\u65f6\u80fd\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u641c\u7d22\u7a7a\u95f4\uff0c\u6700\u7ec8\u751f\u6210\u7684\u67b6\u6784\u5e73\u5747\u51c6\u786e\u7387\u6bd4\u5168\u5c40\u641c\u7d22\u9ad827%\uff0c\u6bd4\u6e10\u8fdb\u5f0f\u641c\u7d22\u9ad85%\u3002", "conclusion": "FACETS\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u63d0\u5347\u76ee\u6807\u68c0\u6d4b\u67b6\u6784\u7684\u6027\u80fd\u3002"}}
{"id": "2503.22353", "pdf": "https://arxiv.org/pdf/2503.22353", "abs": "https://arxiv.org/abs/2503.22353", "authors": ["Yubo Li", "Yidi Miao", "Xueying Ding", "Ramayya Krishnan", "Rema Padman"], "title": "Firm or Fickle? Evaluating Large Language Models Consistency in Sequential Interactions", "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, 5 figures", "summary": "Large Language Models (LLMs) have shown remarkable capabilities across\nvarious tasks, but their deployment in high-stake domains requires consistent\nperformance across multiple interaction rounds. This paper introduces a\ncomprehensive framework for evaluating and improving LLM response consistency,\nmaking three key contributions. First, we propose a novel Position-Weighted\nConsistency (PWC) score that captures both the importance of early-stage\nstability and recovery patterns in multi-turn interactions. Second, we present\na carefully curated benchmark dataset spanning diverse domains and difficulty\nlevels, specifically designed to evaluate LLM consistency under various\nchallenging follow-up scenarios. Third, we introduce Confidence-Aware Response\nGeneration (CARG), a framework that significantly improves response stability\nby incorporating model confidence signals into the generation process.\nEmpirical results demonstrate that CARG significantly improves response\nstability without sacrificing accuracy, underscoring its potential for reliable\nLLM deployment in critical applications.", "AI": {"task": "\u63d0\u51fa\u4e00\u4e2a\u8bc4\u4f30\u548c\u6539\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u54cd\u5e94\u4e00\u81f4\u6027\u7684\u7efc\u5408\u6846\u67b6\u3002", "motivation": "LLM\u5728\u9ad8\u98ce\u9669\u9886\u57df\u7684\u90e8\u7f72\u9700\u8981\u591a\u8f6e\u4ea4\u4e92\u4e2d\u7684\u7a33\u5b9a\u8868\u73b0\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86Position-Weighted Consistency (PWC)\u8bc4\u5206\u3001\u6784\u5efa\u4e86\u591a\u9886\u57df\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86Confidence-Aware Response Generation (CARG)\u6846\u67b6\u3002", "result": "CARG\u663e\u8457\u63d0\u9ad8\u4e86\u54cd\u5e94\u7a33\u5b9a\u6027\u4e14\u4e0d\u727a\u7272\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aLLM\u5728\u5173\u952e\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u90e8\u7f72\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002"}}
{"id": "2503.22019", "pdf": "https://arxiv.org/pdf/2503.22019", "abs": "https://arxiv.org/abs/2503.22019", "authors": ["Earl Ranario", "Lars Lundqvist", "Heesup Yun", "Brian N. Bailey", "J. Mason Earles"], "title": "AGILE: A Diffusion-Based Attention-Guided Image and Label Translation for Efficient Cross-Domain Plant Trait Identification", "categories": ["cs.CV"], "comment": null, "summary": "Semantically consistent cross-domain image translation facilitates the\ngeneration of training data by transferring labels across different domains,\nmaking it particularly useful for plant trait identification in agriculture.\nHowever, existing generative models struggle to maintain object-level accuracy\nwhen translating images between domains, especially when domain gaps are\nsignificant. In this work, we introduce AGILE (Attention-Guided Image and Label\nTranslation for Efficient Cross-Domain Plant Trait Identification), a\ndiffusion-based framework that leverages optimized text embeddings and\nattention guidance to semantically constrain image translation. AGILE utilizes\npretrained diffusion models and publicly available agricultural datasets to\nimprove the fidelity of translated images while preserving critical object\nsemantics. Our approach optimizes text embeddings to strengthen the\ncorrespondence between source and target images and guides attention maps\nduring the denoising process to control object placement. We evaluate AGILE on\ncross-domain plant datasets and demonstrate its effectiveness in generating\nsemantically accurate translated images. Quantitative experiments show that\nAGILE enhances object detection performance in the target domain while\nmaintaining realism and consistency. Compared to prior image translation\nmethods, AGILE achieves superior semantic alignment, particularly in\nchallenging cases where objects vary significantly or domain gaps are\nsubstantial.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6ce8\u610f\u529b\u5f15\u5bfc\u56fe\u50cf\u548c\u6807\u7b7e\u7ffb\u8bd1\u6846\u67b6\uff08AGILE\uff09\uff0c\u7528\u4e8e\u8de8\u57df\u690d\u7269\u6027\u72b6\u8bc6\u522b\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u6a21\u578b\u5728\u8de8\u57df\u56fe\u50cf\u7ffb\u8bd1\u4e2d\u96be\u4ee5\u4fdd\u6301\u5bf9\u8c61\u7ea7\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u57df\u95f4\u5dee\u5f02\u663e\u8457\u65f6\u3002", "method": "\u5229\u7528\u4f18\u5316\u7684\u6587\u672c\u5d4c\u5165\u548c\u6ce8\u610f\u529b\u5f15\u5bfc\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u548c\u519c\u4e1a\u6570\u636e\u96c6\uff0c\u7ea6\u675f\u56fe\u50cf\u7ffb\u8bd1\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "result": "AGILE\u5728\u8de8\u57df\u690d\u7269\u6570\u636e\u96c6\u4e0a\u751f\u6210\u8bed\u4e49\u51c6\u786e\u7684\u7ffb\u8bd1\u56fe\u50cf\uff0c\u63d0\u5347\u4e86\u76ee\u6807\u57df\u7684\u5bf9\u8c61\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u4fdd\u6301\u4e86\u771f\u5b9e\u6027\u548c\u4e00\u81f4\u6027\u3002", "conclusion": "AGILE\u5728\u8bed\u4e49\u5bf9\u9f50\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u5bf9\u8c61\u5dee\u5f02\u663e\u8457\u6216\u57df\u95f4\u5dee\u5f02\u5927\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u4f18\u3002"}}
{"id": "2503.22362", "pdf": "https://arxiv.org/pdf/2503.22362", "abs": "https://arxiv.org/abs/2503.22362", "authors": ["Yuan He", "Bailan He", "Zifeng Ding", "Alisia Lupidi", "Yuqicheng Zhu", "Shuo Chen", "Caiqi Zhang", "Jiaoyan Chen", "Yunpu Ma", "Volker Tresp", "Ian Horrocks"], "title": "Supposedly Equivalent Facts That Aren't? Entity Frequency in Pre-training Induces Asymmetry in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Understanding and mitigating hallucinations in Large Language Models (LLMs)\nis crucial for ensuring reliable content generation. While previous research\nhas primarily focused on \"when\" LLMs hallucinate, our work explains \"why\" and\ndirectly links model behaviour to the pre-training data that forms their prior\nknowledge. Specifically, we demonstrate that an asymmetry exists in the\nrecognition of logically equivalent facts, which can be attributed to frequency\ndiscrepancies of entities appearing as subjects versus objects. Given that most\npre-training datasets are inaccessible, we leverage the fully open-source OLMo\nseries by indexing its Dolma dataset to estimate entity frequencies. Using\nrelational facts (represented as triples) from Wikidata5M, we construct probing\ndatasets to isolate this effect. Our experiments reveal that facts with a\nhigh-frequency subject and a low-frequency object are better recognised than\ntheir inverse, despite their logical equivalence. The pattern reverses in\nlow-to-high frequency settings, and no statistically significant asymmetry\nemerges when both entities are high-frequency. These findings highlight the\ninfluential role of pre-training data in shaping model predictions and provide\ninsights for inferring the characteristics of pre-training data in closed or\npartially closed LLMs.", "AI": {"task": "\u89e3\u91ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u5e7b\u89c9\u73b0\u8c61\u7684\u539f\u56e0\uff0c\u5e76\u5c06\u5176\u884c\u4e3a\u4e0e\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u5148\u9a8c\u77e5\u8bc6\u8054\u7cfb\u8d77\u6765\u3002", "motivation": "\u7406\u89e3\u5e76\u51cf\u5c11LLMs\u4e2d\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u4ee5\u63d0\u9ad8\u5185\u5bb9\u751f\u6210\u7684\u53ef\u9760\u6027\u3002", "method": "\u5229\u7528\u5f00\u6e90OLMo\u7cfb\u5217\u548cDolma\u6570\u636e\u96c6\u4f30\u8ba1\u5b9e\u4f53\u9891\u7387\uff0c\u6784\u5efa\u63a2\u6d4b\u6570\u636e\u96c6\u4ee5\u5206\u6790\u903b\u8f91\u7b49\u4ef7\u4e8b\u5b9e\u7684\u8bc6\u522b\u4e0d\u5bf9\u79f0\u6027\u3002", "result": "\u53d1\u73b0\u9ad8\u9891\u4e3b\u4f53\u4e0e\u4f4e\u9891\u5ba2\u4f53\u7684\u4e8b\u5b9e\u8bc6\u522b\u6548\u679c\u4f18\u4e8e\u5176\u53cd\u5411\u60c5\u51b5\uff0c\u4e14\u8fd9\u79cd\u4e0d\u5bf9\u79f0\u6027\u5728\u9ad8\u4f4e\u9891\u7ec4\u5408\u4e0b\u53cd\u8f6c\uff0c\u800c\u5728\u53cc\u9ad8\u9891\u60c5\u51b5\u4e0b\u4e0d\u663e\u8457\u3002", "conclusion": "\u9884\u8bad\u7ec3\u6570\u636e\u5bf9\u6a21\u578b\u9884\u6d4b\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4e3a\u63a8\u65ad\u5c01\u95ed\u6216\u90e8\u5206\u5c01\u95edLLMs\u7684\u9884\u8bad\u7ec3\u6570\u636e\u7279\u5f81\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002"}}
{"id": "2503.22020", "pdf": "https://arxiv.org/pdf/2503.22020", "abs": "https://arxiv.org/abs/2503.22020", "authors": ["Qingqing Zhao", "Yao Lu", "Moo Jin Kim", "Zipeng Fu", "Zhuoyang Zhang", "Yecheng Wu", "Zhaoshuo Li", "Qianli Ma", "Song Han", "Chelsea Finn", "Ankur Handa", "Ming-Yu Liu", "Donglai Xiang", "Gordon Wetzstein", "Tsung-Yi Lin"], "title": "CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "Project website: https://cot-vla.github.io/", "summary": "Vision-language-action models (VLAs) have shown potential in leveraging\npretrained vision-language models and diverse robot demonstrations for learning\ngeneralizable sensorimotor control. While this paradigm effectively utilizes\nlarge-scale data from both robotic and non-robotic sources, current VLAs\nprimarily focus on direct input--output mappings, lacking the intermediate\nreasoning steps crucial for complex manipulation tasks. As a result, existing\nVLAs lack temporal planning or reasoning capabilities. In this paper, we\nintroduce a method that incorporates explicit visual chain-of-thought (CoT)\nreasoning into vision-language-action models (VLAs) by predicting future image\nframes autoregressively as visual goals before generating a short action\nsequence to achieve these goals. We introduce CoT-VLA, a state-of-the-art 7B\nVLA that can understand and generate visual and action tokens. Our experimental\nresults demonstrate that CoT-VLA achieves strong performance, outperforming the\nstate-of-the-art VLA model by 17% in real-world manipulation tasks and 6% in\nsimulation benchmarks. Project website: https://cot-vla.github.io/", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u5c06\u663e\u5f0f\u89c6\u89c9\u94fe\u5f0f\u63a8\u7406\uff08CoT\uff09\u878d\u5165\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff08VLA\uff09\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u7f3a\u4e4f\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\uff0c\u65e0\u6cd5\u5904\u7406\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u65f6\u5e8f\u89c4\u5212\u6216\u63a8\u7406\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u81ea\u56de\u5f52\u9884\u6d4b\u672a\u6765\u56fe\u50cf\u5e27\u4f5c\u4e3a\u89c6\u89c9\u76ee\u6807\uff0c\u5e76\u751f\u6210\u77ed\u52a8\u4f5c\u5e8f\u5217\u4ee5\u5b9e\u73b0\u8fd9\u4e9b\u76ee\u6807\u3002", "result": "CoT-VLA\u5728\u771f\u5b9e\u4e16\u754c\u64cd\u4f5c\u4efb\u52a1\u4e2d\u4f18\u4e8e\u5f53\u524d\u6700\u4f73VLA\u6a21\u578b17%\uff0c\u5728\u4eff\u771f\u57fa\u51c6\u4e2d\u63d0\u53476%\u3002", "conclusion": "\u5f15\u5165\u89c6\u89c9\u94fe\u5f0f\u63a8\u7406\u663e\u8457\u63d0\u5347\u4e86VLA\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2503.22388", "pdf": "https://arxiv.org/pdf/2503.22388", "abs": "https://arxiv.org/abs/2503.22388", "authors": ["Zhiyu Yang", "Shuo Wang", "Yukun Yan", "Yang Deng"], "title": "Why Stop at One Error? Benchmarking LLMs as Data Science Code Debuggers for Multi-Hop and Multi-Bug Errors", "categories": ["cs.CL"], "comment": "Work in progress", "summary": "LLMs are transforming software development, yet current code generation and\ncode repair benchmarks mainly assess syntactic and functional correctness in\nsimple, single-error cases. LLMs' capabilities to autonomously find and fix\nruntime logical errors in complex data science code remain largely unexplored.\nTo address this gap, we introduce DSDBench: the Data Science Debugging\nBenchmark, the first benchmark for systematic evaluation of LLMs on multi-hop\nerror tracing and multi-bug detection in data science code debugging. DSDBench\nadapts datasets from existing data science task benchmarks, such as DABench and\nMatPlotBench, featuring realistic data science debugging tasks with\nautomatically synthesized multi-hop, multi-bug code snippets. DSDBench includes\n1,117 annotated samples with 741 cause-effect error pairs and runtime error\nmessages. Evaluations of state-of-the-art LLMs on DSDBench show significant\nperformance gaps, highlighting challenges in debugging logical runtime errors\nin data science code. DSDBench offers a crucial resource to evaluate and\nimprove LLMs' debugging and reasoning capabilities, enabling more reliable\nAI-assisted data science in the future.DSDBench is publicly available at\nhttps://github.com/KevinCL16/DSDBench.", "AI": {"task": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6570\u636e\u79d1\u5b66\u4ee3\u7801\u8c03\u8bd5\u4e2d\u591a\u8df3\u9519\u8bef\u8ffd\u8e2a\u548c\u591a\u9519\u8bef\u68c0\u6d4b\u7684\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u4ee3\u7801\u751f\u6210\u548c\u4fee\u590d\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u7b80\u5355\u3001\u5355\u9519\u8bef\u60c5\u51b5\u4e0b\u7684\u8bed\u6cd5\u548c\u529f\u80fd\u6b63\u786e\u6027\uff0c\u800cLLMs\u5728\u590d\u6742\u6570\u636e\u79d1\u5b66\u4ee3\u7801\u4e2d\u81ea\u4e3b\u53d1\u73b0\u548c\u4fee\u590d\u8fd0\u884c\u65f6\u903b\u8f91\u9519\u8bef\u7684\u80fd\u529b\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u5f15\u5165DSDBench\u57fa\u51c6\uff0c\u901a\u8fc7\u81ea\u52a8\u5408\u6210\u7684\u591a\u8df3\u3001\u591a\u9519\u8bef\u4ee3\u7801\u7247\u6bb5\uff0c\u8bc4\u4f30LLMs\u5728\u6570\u636e\u79d1\u5b66\u8c03\u8bd5\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0c\u73b0\u6709LLMs\u5728\u8c03\u8bd5\u6570\u636e\u79d1\u5b66\u4ee3\u7801\u4e2d\u7684\u903b\u8f91\u8fd0\u884c\u65f6\u9519\u8bef\u65b9\u9762\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "DSDBench\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdbLLMs\u7684\u8c03\u8bd5\u548c\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u5173\u952e\u8d44\u6e90\uff0c\u6709\u52a9\u4e8e\u672a\u6765\u5b9e\u73b0\u66f4\u53ef\u9760\u7684AI\u8f85\u52a9\u6570\u636e\u79d1\u5b66\u3002"}}
{"id": "2503.22026", "pdf": "https://arxiv.org/pdf/2503.22026", "abs": "https://arxiv.org/abs/2503.22026", "authors": ["SaiKiran Tedla", "Junyong Lee", "Beixuan Yang", "Mahmoud Afifi", "Michael Brown"], "title": "Multispectral Demosaicing via Dual Cameras", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Multispectral (MS) images capture detailed scene information across a wide\nrange of spectral bands, making them invaluable for applications requiring rich\nspectral data. Integrating MS imaging into multi camera devices, such as\nsmartphones, has the potential to enhance both spectral applications and RGB\nimage quality. A critical step in processing MS data is demosaicing, which\nreconstructs color information from the mosaic MS images captured by the\ncamera. This paper proposes a method for MS image demosaicing specifically\ndesigned for dual-camera setups where both RGB and MS cameras capture the same\nscene. Our approach leverages co-captured RGB images, which typically have\nhigher spatial fidelity, to guide the demosaicing of lower-fidelity MS images.\nWe introduce the Dual-camera RGB-MS Dataset - a large collection of paired RGB\nand MS mosaiced images with ground-truth demosaiced outputs - that enables\ntraining and evaluation of our method. Experimental results demonstrate that\nour method achieves state-of-the-art accuracy compared to existing techniques.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9\u53cc\u6444\u50cf\u5934\u8bbe\u7f6e\u7684\u591a\u5149\u8c31\u56fe\u50cf\u53bb\u9a6c\u8d5b\u514b\u65b9\u6cd5\u3002", "motivation": "\u591a\u5149\u8c31\u56fe\u50cf\u5728\u5149\u8c31\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u5c06\u5176\u96c6\u6210\u5230\u667a\u80fd\u624b\u673a\u7b49\u591a\u6444\u50cf\u5934\u8bbe\u5907\u4e2d\u53ef\u63d0\u5347\u5149\u8c31\u5e94\u7528\u548cRGB\u56fe\u50cf\u8d28\u91cf\u3002", "method": "\u5229\u7528\u5171\u6355\u83b7\u7684\u9ad8\u7a7a\u95f4\u4fdd\u771f\u5ea6RGB\u56fe\u50cf\u6307\u5bfc\u4f4e\u4fdd\u771f\u5ea6\u591a\u5149\u8c31\u56fe\u50cf\u7684\u53bb\u9a6c\u8d5b\u514b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u53cc\u6444\u50cf\u5934\u8bbe\u7f6e\u4e2d\u7684\u591a\u5149\u8c31\u56fe\u50cf\u53bb\u9a6c\u8d5b\u514b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2503.22395", "pdf": "https://arxiv.org/pdf/2503.22395", "abs": "https://arxiv.org/abs/2503.22395", "authors": ["Tereza Vrabcov\u00e1", "Marek Kadl\u010d\u00edk", "Petr Sojka", "Michal \u0160tef\u00e1nik", "Michal Spiegel"], "title": "Negation: A Pink Elephant in the Large Language Models' Room?", "categories": ["cs.CL"], "comment": null, "summary": "Negations are key to determining sentence meaning, making them essential for\nlogical reasoning. Despite their importance, negations pose a substantial\nchallenge for large language models (LLMs) and remain underexplored.\n  We construct two multilingual natural language inference (NLI) datasets with\n\\textit{paired} examples differing in negation. We investigate how model size\nand language impact its ability to handle negation correctly by evaluating\npopular LLMs.\n  Contrary to previous work, we show that increasing the model size\nconsistently improves the models' ability to handle negations. Furthermore, we\nfind that both the models' reasoning accuracy and robustness to negation are\nlanguage-dependent and that the length and explicitness of the premise have a\ngreater impact on robustness than language.\n  Our datasets can facilitate further research and improvements of language\nmodel reasoning in multilingual settings.", "AI": {"task": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5904\u7406\u5426\u5b9a\u53e5\u65f6\u7684\u8868\u73b0\uff0c\u5e76\u6784\u5efa\u591a\u8bed\u8a00\u81ea\u7136\u8bed\u8a00\u63a8\u7406\uff08NLI\uff09\u6570\u636e\u96c6\u4ee5\u8bc4\u4f30\u6a21\u578b\u80fd\u529b\u3002", "motivation": "\u5426\u5b9a\u53e5\u5bf9\u903b\u8f91\u63a8\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46LLMs\u5728\u5904\u7406\u5426\u5b9a\u53e5\u65f6\u8868\u73b0\u4e0d\u4f73\u4e14\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u6784\u5efa\u4e24\u4e2a\u591a\u8bed\u8a00NLI\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u8bc4\u4f30\u6d41\u884cLLMs\uff0c\u7814\u7a76\u6a21\u578b\u5927\u5c0f\u548c\u8bed\u8a00\u5bf9\u5176\u5904\u7406\u5426\u5b9a\u53e5\u80fd\u529b\u7684\u5f71\u54cd\u3002", "result": "\u6a21\u578b\u5927\u5c0f\u7684\u589e\u52a0\u80fd\u6301\u7eed\u63d0\u5347\u5904\u7406\u5426\u5b9a\u53e5\u7684\u80fd\u529b\uff1b\u63a8\u7406\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u53d7\u8bed\u8a00\u5f71\u54cd\uff0c\u524d\u63d0\u7684\u957f\u5ea6\u548c\u660e\u786e\u6027\u5bf9\u9c81\u68d2\u6027\u5f71\u54cd\u66f4\u5927\u3002", "conclusion": "\u6570\u636e\u96c6\u53ef\u4fc3\u8fdb\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u6539\u8fdb\u3002"}}
{"id": "2503.22050", "pdf": "https://arxiv.org/pdf/2503.22050", "abs": "https://arxiv.org/abs/2503.22050", "authors": ["Tai An", "Weiqiang Huang", "Da Xu", "Qingyuan He", "Jiacheng Hu", "Yujia Lou"], "title": "A Deep Learning Framework for Boundary-Aware Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "As a fundamental task in computer vision, semantic segmentation is widely\napplied in fields such as autonomous driving, remote sensing image analysis,\nand medical image processing. In recent years, Transformer-based segmentation\nmethods have demonstrated strong performance in global feature modeling.\nHowever, they still struggle with blurred target boundaries and insufficient\nrecognition of small targets. To address these issues, this study proposes a\nMask2Former-based semantic segmentation algorithm incorporating a boundary\nenhancement feature bridging module (BEFBM). The goal is to improve target\nboundary accuracy and segmentation consistency. Built upon the Mask2Former\nframework, this method constructs a boundary-aware feature map and introduces a\nfeature bridging mechanism. This enables effective cross-scale feature fusion,\nenhancing the model's ability to focus on target boundaries. Experiments on the\nCityscapes dataset demonstrate that, compared to mainstream segmentation\nmethods, the proposed approach achieves significant improvements in metrics\nsuch as mIOU, mDICE, and mRecall. It also exhibits superior boundary retention\nin complex scenes. Visual analysis further confirms the model's advantages in\nfine-grained regions. Future research will focus on optimizing computational\nefficiency and exploring its potential in other high-precision segmentation\ntasks.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eMask2Former\u7684\u8bed\u4e49\u5206\u5272\u7b97\u6cd5\uff0c\u7ed3\u5408\u8fb9\u754c\u589e\u5f3a\u7279\u5f81\u6865\u63a5\u6a21\u5757\uff08BEFBM\uff09\uff0c\u4ee5\u63d0\u9ad8\u76ee\u6807\u8fb9\u754c\u51c6\u786e\u6027\u548c\u5206\u5272\u4e00\u81f4\u6027\u3002", "motivation": "Transformer-based\u5206\u5272\u65b9\u6cd5\u5728\u5168\u5c40\u7279\u5f81\u5efa\u6a21\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u76ee\u6807\u8fb9\u754c\u6a21\u7cca\u548c\u5c0f\u76ee\u6807\u8bc6\u522b\u4e0d\u8db3\u65b9\u9762\u4ecd\u6709\u6311\u6218\u3002", "method": "\u5728Mask2Former\u6846\u67b6\u57fa\u7840\u4e0a\u6784\u5efa\u8fb9\u754c\u611f\u77e5\u7279\u5f81\u56fe\uff0c\u5e76\u5f15\u5165\u7279\u5f81\u6865\u63a5\u673a\u5236\uff0c\u5b9e\u73b0\u8de8\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u3002", "result": "\u5728Cityscapes\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u4e3b\u6d41\u65b9\u6cd5\uff0cmIOU\u3001mDICE\u548cmRecall\u7b49\u6307\u6807\u663e\u8457\u63d0\u5347\uff0c\u4e14\u5728\u590d\u6742\u573a\u666f\u4e2d\u8fb9\u754c\u4fdd\u7559\u6548\u679c\u66f4\u4f18\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5c06\u4f18\u5316\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u63a2\u7d22\u5176\u5728\u9ad8\u7cbe\u5ea6\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2503.22411", "pdf": "https://arxiv.org/pdf/2503.22411", "abs": "https://arxiv.org/abs/2503.22411", "authors": ["Petter T\u00f6rnberg", "Juliana Chueri"], "title": "Elite Political Discourse has Become More Toxic in Western Countries", "categories": ["cs.CL"], "comment": null, "summary": "Toxic and uncivil politics is widely seen as a growing threat to democratic\nvalues and governance, yet our understanding of the drivers and evolution of\npolitical incivility remains limited. Leveraging a novel dataset of nearly 18\nmillion Twitter messages from parliamentarians in 17 countries over five years,\nthis paper systematically investigates whether politics internationally is\nbecoming more uncivil, and what are the determinants of political incivility.\nOur analysis reveals a marked increase in toxic discourse among political\nelites, and that it is associated to radical-right parties and parties in\nopposition. Toxicity diminished markedly during the early phase of the COVID-19\npandemic and, surprisingly, during election campaigns. Furthermore, our results\nindicate that posts relating to ``culture war'' topics, such as migration and\nLGBTQ+ rights, are substantially more toxic than debates focused on welfare or\neconomic issues. These findings underscore a troubling shift in international\ndemocracies toward an erosion of constructive democratic dialogue.", "AI": {"task": "\u7cfb\u7edf\u7814\u7a76\u56fd\u9645\u653f\u6cbb\u4e2d\u4e0d\u6587\u660e\u884c\u4e3a\u7684\u8d8b\u52bf\u53ca\u5176\u51b3\u5b9a\u56e0\u7d20\u3002", "motivation": "\u653f\u6cbb\u4e0d\u6587\u660e\u884c\u4e3a\u5bf9\u6c11\u4e3b\u4ef7\u503c\u89c2\u548c\u6cbb\u7406\u6784\u6210\u5a01\u80c1\uff0c\u4f46\u5176\u9a71\u52a8\u56e0\u7d20\u548c\u6f14\u53d8\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u5229\u7528\u6765\u81ea17\u4e2a\u56fd\u5bb6\u8bae\u5458\u7684\u8fd11800\u4e07\u6761Twitter\u6d88\u606f\u6570\u636e\u96c6\u8fdb\u884c\u5206\u6790\u3002", "result": "\u653f\u6cbb\u7cbe\u82f1\u7684\u6bd2\u6027\u8a00\u8bba\u663e\u8457\u589e\u52a0\uff0c\u4e0e\u6fc0\u8fdb\u53f3\u7ffc\u653f\u515a\u548c\u53cd\u5bf9\u515a\u76f8\u5173\uff1bCOVID-19\u521d\u671f\u548c\u9009\u4e3e\u671f\u95f4\u6bd2\u6027\u964d\u4f4e\uff1b\u6587\u5316\u6218\u4e89\u8bdd\u9898\u7684\u6bd2\u6027\u66f4\u9ad8\u3002", "conclusion": "\u56fd\u9645\u6c11\u4e3b\u56fd\u5bb6\u6b63\u9762\u4e34\u5efa\u8bbe\u6027\u5bf9\u8bdd\u7684\u4fb5\u8680\uff0c\u653f\u6cbb\u4e0d\u6587\u660e\u884c\u4e3a\u8d8b\u52bf\u4ee4\u4eba\u62c5\u5fe7\u3002"}}
{"id": "2503.22060", "pdf": "https://arxiv.org/pdf/2503.22060", "abs": "https://arxiv.org/abs/2503.22060", "authors": ["Ukcheol Shin", "Jinsun Park"], "title": "Deep Depth Estimation from Thermal Image: Dataset, Benchmark, and Challenges", "categories": ["cs.CV", "cs.RO"], "comment": "MS^2 dataset:\n  https://sites.google.com/view/multi-spectral-stereo-dataset, Source code:\n  https://github.com/UkcheolShin/SupDepth4Thermal", "summary": "Achieving robust and accurate spatial perception under adverse weather and\nlighting conditions is crucial for the high-level autonomy of self-driving\nvehicles and robots. However, existing perception algorithms relying on the\nvisible spectrum are highly affected by weather and lighting conditions. A\nlong-wave infrared camera (i.e., thermal imaging camera) can be a potential\nsolution to achieve high-level robustness. However, the absence of large-scale\ndatasets and standardized benchmarks remains a significant bottleneck to\nprogress in active research for robust visual perception from thermal images.\nTo this end, this manuscript provides a large-scale Multi-Spectral Stereo\n(MS$^2$) dataset that consists of stereo RGB, stereo NIR, stereo thermal,\nstereo LiDAR data, and GNSS/IMU information along with semi-dense depth ground\ntruth. MS$^2$ dataset includes 162K synchronized multi-modal data pairs\ncaptured across diverse locations (e.g., urban city, residential area, campus,\nand high-way road) at different times (e.g., morning, daytime, and nighttime)\nand under various weather conditions (e.g., clear-sky, cloudy, and rainy).\nSecondly, we conduct a thorough evaluation of monocular and stereo depth\nestimation networks across RGB, NIR, and thermal modalities to establish\nstandardized benchmark results on MS$^2$ depth test sets (e.g., day, night, and\nrainy). Lastly, we provide in-depth analyses and discuss the challenges\nrevealed by the benchmark results, such as the performance variability for each\nmodality under adverse conditions, domain shift between different sensor\nmodalities, and potential research direction for thermal perception. Our\ndataset and source code are publicly available at\nhttps://sites.google.com/view/multi-spectral-stereo-dataset and\nhttps://github.com/UkcheolShin/SupDepth4Thermal.", "AI": {"task": "\u63d0\u51fa\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u5149\u8c31\u7acb\u4f53\uff08MS$^2$\uff09\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30RGB\u3001\u8fd1\u7ea2\u5916\u548c\u70ed\u6210\u50cf\u6a21\u6001\u4e0b\u7684\u6df1\u5ea6\u4f30\u8ba1\u7f51\u7edc\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u53ef\u89c1\u5149\u8c31\u7684\u611f\u77e5\u7b97\u6cd5\u5728\u6076\u52a3\u5929\u6c14\u548c\u5149\u7167\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u70ed\u6210\u50cf\u76f8\u673a\u53ef\u80fd\u63d0\u4f9b\u66f4\u9ad8\u9c81\u68d2\u6027\uff0c\u4f46\u7f3a\u4e4f\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u6807\u51c6\u5316\u57fa\u51c6\u3002", "method": "\u6784\u5efa\u5305\u542b\u591a\u6a21\u6001\u6570\u636e\uff08RGB\u3001NIR\u3001\u70ed\u6210\u50cf\u3001LiDAR\u7b49\uff09\u7684MS$^2$\u6570\u636e\u96c6\uff0c\u5e76\u8fdb\u884c\u6df1\u5ea6\u4f30\u8ba1\u7f51\u7edc\u7684\u5168\u9762\u8bc4\u4f30\u3002", "result": "\u63d0\u4f9b\u4e86162K\u591a\u6a21\u6001\u6570\u636e\u5bf9\uff0c\u5efa\u7acb\u4e86\u6807\u51c6\u5316\u57fa\u51c6\u7ed3\u679c\uff0c\u5e76\u5206\u6790\u4e86\u5404\u6a21\u6001\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u5dee\u5f02\u548c\u6311\u6218\u3002", "conclusion": "MS$^2$\u6570\u636e\u96c6\u586b\u8865\u4e86\u70ed\u6210\u50cf\u611f\u77e5\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u548c\u8d44\u6e90\u3002"}}
{"id": "2503.22426", "pdf": "https://arxiv.org/pdf/2503.22426", "abs": "https://arxiv.org/abs/2503.22426", "authors": ["Yuto Nishida", "Makoto Morishita", "Hiroyuki Deguchi", "Hidetaka Kamigaito", "Taro Watanabe"], "title": "Long-Tail Crisis in Nearest Neighbor Language Models", "categories": ["cs.CL"], "comment": "Accepted to NAACL 2025 Findings", "summary": "The $k$-nearest-neighbor language model ($k$NN-LM), one of the\nretrieval-augmented language models, improves the perplexity for given text by\ndirectly accessing a large datastore built from any text data during inference.\nA widely held hypothesis for the success of $k$NN-LM is that its explicit\nmemory, i.e., the datastore, enhances predictions for long-tail phenomena.\nHowever, prior works have primarily shown its ability to retrieve long-tail\ncontexts, leaving the model's performance remain underexplored in estimating\nthe probabilities of long-tail target tokens during inference. In this paper,\nwe investigate the behavior of $k$NN-LM on low-frequency tokens, examining\nprediction probability, retrieval accuracy, token distribution in the\ndatastore, and approximation error of the product quantization. Our\nexperimental results reveal that $k$NN-LM does not improve prediction\nperformance for low-frequency tokens but mainly benefits high-frequency tokens\nregardless of long-tail contexts in the datastore.", "AI": {"task": "\u7814\u7a76$k$NN-LM\u5728\u4f4e\u9891\u8bcd\u4e0a\u7684\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u63a2\u8ba8$k$NN-LM\u662f\u5426\u771f\u6b63\u63d0\u5347\u4e86\u4f4e\u9891\u8bcd\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u800c\u975e\u4ec5\u4f9d\u8d56\u4e8e\u957f\u5c3e\u4e0a\u4e0b\u6587\u3002", "method": "\u5206\u6790\u9884\u6d4b\u6982\u7387\u3001\u68c0\u7d22\u51c6\u786e\u7387\u3001\u6570\u636e\u5b58\u50a8\u4e2d\u7684\u8bcd\u5206\u5e03\u53ca\u4e58\u79ef\u91cf\u5316\u7684\u8fd1\u4f3c\u8bef\u5dee\u3002", "result": "$k$NN-LM\u5e76\u672a\u63d0\u5347\u4f4e\u9891\u8bcd\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u4e3b\u8981\u53d7\u76ca\u4e8e\u9ad8\u9891\u8bcd\u3002", "conclusion": "$k$NN-LM\u5bf9\u4f4e\u9891\u8bcd\u7684\u9884\u6d4b\u6548\u679c\u6709\u9650\uff0c\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002"}}
{"id": "2503.22069", "pdf": "https://arxiv.org/pdf/2503.22069", "abs": "https://arxiv.org/abs/2503.22069", "authors": ["Ekansh Chauhan", "Anila Sharma", "Amit Sharma", "Vikas Nishadham", "Asha Ghughtyal", "Ankur Kumar", "Gurudutt Gupta", "Anurag Mehta", "C. V. Jawahar", "P. K. Vinod"], "title": "Contrasting Low and High-Resolution Features for HER2 Scoring using Deep Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Breast cancer, the most common malignancy among women, requires precise\ndetection and classification for effective treatment. Immunohistochemistry\n(IHC) biomarkers like HER2, ER, and PR are critical for identifying breast\ncancer subtypes. However, traditional IHC classification relies on\npathologists' expertise, making it labor-intensive and subject to significant\ninter-observer variability. To address these challenges, this study introduces\nthe India Pathology Breast Cancer Dataset (IPD-Breast), comprising of 1,272 IHC\nslides (HER2, ER, and PR) aimed at automating receptor status classification.\nThe primary focus is on developing predictive models for HER2 3-way\nclassification (0, Low, High) to enhance prognosis. Evaluation of multiple deep\nlearning models revealed that an end-to-end ConvNeXt network utilizing\nlow-resolution IHC images achieved an AUC, F1, and accuracy of 91.79%, 83.52%,\nand 83.56%, respectively, for 3-way classification, outperforming patch-based\nmethods by over 5.35% in F1 score. This study highlights the potential of\nsimple yet effective deep learning techniques to significantly improve accuracy\nand reproducibility in breast cancer classification, supporting their\nintegration into clinical workflows for better patient outcomes.", "AI": {"task": "\u5f00\u53d1\u9884\u6d4b\u6a21\u578b\u4ee5\u5b9e\u73b0\u4e73\u817a\u764cHER2\u7684\u4e09\u5206\u7c7b\uff080\u3001\u4f4e\u3001\u9ad8\uff09\u4ee5\u63d0\u9ad8\u9884\u540e\u3002", "motivation": "\u4f20\u7edf\u514d\u75ab\u7ec4\u5316\uff08IHC\uff09\u5206\u7c7b\u4f9d\u8d56\u75c5\u7406\u5b66\u5bb6\u7684\u4e13\u4e1a\u77e5\u8bc6\uff0c\u5de5\u4f5c\u91cf\u5927\u4e14\u5b58\u5728\u663e\u8457\u7684\u89c2\u5bdf\u8005\u95f4\u53d8\u5f02\u6027\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u5370\u5ea6\u75c5\u7406\u4e73\u817a\u764c\u6570\u636e\u96c6\uff08IPD-Breast\uff09\u4e2d\u76841,272\u5f20IHC\u5207\u7247\uff08HER2\u3001ER\u3001PR\uff09\uff0c\u91c7\u7528\u7aef\u5230\u7aef\u7684ConvNeXt\u7f51\u7edc\u5904\u7406\u4f4e\u5206\u8fa8\u7387IHC\u56fe\u50cf\u3002", "result": "ConvNeXt\u7f51\u7edc\u5728\u4e09\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684AUC\u3001F1\u548c\u51c6\u786e\u7387\u5206\u522b\u4e3a91.79%\u300183.52%\u548c83.56%\uff0c\u4f18\u4e8e\u57fa\u4e8epatch\u7684\u65b9\u6cd5\uff08F1\u5206\u6570\u63d0\u9ad85.35%\uff09\u3002", "conclusion": "\u7b80\u5355\u800c\u6709\u6548\u7684\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u53ef\u663e\u8457\u63d0\u9ad8\u4e73\u817a\u764c\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u53ef\u91cd\u590d\u6027\uff0c\u652f\u6301\u5176\u878d\u5165\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u4ee5\u6539\u5584\u60a3\u8005\u9884\u540e\u3002"}}
{"id": "2503.22444", "pdf": "https://arxiv.org/pdf/2503.22444", "abs": "https://arxiv.org/abs/2503.22444", "authors": ["Pengsong Zhang", "Heng Zhang", "Huazhe Xu", "Renjun Xu", "Zhenting Wang", "Cong Wang", "Animesh Garg", "Zhibin Li", "Arash Ajoudani", "Xinyu Liu"], "title": "Scaling Laws of Scientific Discovery with AI and Robot Scientists", "categories": ["cs.CL", "cs.RO"], "comment": "22 pages, 7 figures", "summary": "The rapid evolution of scientific inquiry highlights an urgent need for\ngroundbreaking methodologies that transcend the limitations of traditional\nresearch. Conventional approaches, bogged down by manual processes and siloed\nexpertise, struggle to keep pace with the demands of modern discovery. We\nenvision an autonomous generalist scientist (AGS) system-a fusion of agentic AI\nand embodied robotics-that redefines the research lifecycle. This system\npromises to autonomously navigate physical and digital realms, weaving together\ninsights from disparate disciplines with unprecedented efficiency. By embedding\nadvanced AI and robot technologies into every phase-from hypothesis formulation\nto peer-ready manuscripts-AGS could slash the time and resources needed for\nscientific research in diverse field. We foresee a future where scientific\ndiscovery follows new scaling laws, driven by the proliferation and\nsophistication of such systems. As these autonomous agents and robots adapt to\nextreme environments and leverage a growing reservoir of knowledge, they could\nspark a paradigm shift, pushing the boundaries of what's possible and ushering\nin an era of relentless innovation.", "AI": {"task": "\u63d0\u51fa\u5e76\u63cf\u8ff0\u4e00\u79cd\u81ea\u4e3b\u901a\u7528\u79d1\u5b66\u5bb6\uff08AGS\uff09\u7cfb\u7edf\uff0c\u4ee5\u8d85\u8d8a\u4f20\u7edf\u7814\u7a76\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u7814\u7a76\u65b9\u6cd5\u53d7\u9650\u4e8e\u624b\u52a8\u6d41\u7a0b\u548c\u5b64\u7acb\u7684\u77e5\u8bc6\u9886\u57df\uff0c\u96be\u4ee5\u6ee1\u8db3\u73b0\u4ee3\u79d1\u5b66\u53d1\u73b0\u7684\u9700\u6c42\u3002", "method": "\u7ed3\u5408\u4ee3\u7406\u578bAI\u548c\u673a\u5668\u4eba\u6280\u672f\uff0c\u6784\u5efa\u4e00\u4e2a\u80fd\u591f\u81ea\u4e3b\u5bfc\u822a\u7269\u7406\u548c\u6570\u5b57\u9886\u57df\u7684AGS\u7cfb\u7edf\u3002", "result": "AGS\u7cfb\u7edf\u6709\u671b\u663e\u8457\u51cf\u5c11\u79d1\u5b66\u7814\u7a76\u6240\u9700\u7684\u65f6\u95f4\u548c\u8d44\u6e90\uff0c\u5e76\u63a8\u52a8\u8de8\u5b66\u79d1\u7684\u9ad8\u6548\u7814\u7a76\u3002", "conclusion": "AGS\u7cfb\u7edf\u53ef\u80fd\u5f15\u53d1\u79d1\u5b66\u53d1\u73b0\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u63a8\u52a8\u6301\u7eed\u521b\u65b0\uff0c\u62d3\u5c55\u79d1\u5b66\u8fb9\u754c\u3002"}}
{"id": "2503.22079", "pdf": "https://arxiv.org/pdf/2503.22079", "abs": "https://arxiv.org/abs/2503.22079", "authors": ["Kunshan Yang", "Wenwei Luo", "Yuguo Hu", "Jiafu Yan", "Mengmeng Jing", "Lin Zuo"], "title": "A Semantic-Enhanced Heterogeneous Graph Learning Method for Flexible Objects Recognition", "categories": ["cs.CV"], "comment": "Accepted by ICME 2025", "summary": "Flexible objects recognition remains a significant challenge due to its\ninherently diverse shapes and sizes, translucent attributes, and subtle\ninter-class differences. Graph-based models, such as graph convolution networks\nand graph vision models, are promising in flexible objects recognition due to\ntheir ability of capturing variable relations within the flexible objects.\nThese methods, however, often focus on global visual relationships or fail to\nalign semantic and visual information. To alleviate these limitations, we\npropose a semantic-enhanced heterogeneous graph learning method. First, an\nadaptive scanning module is employed to extract discriminative semantic\ncontext, facilitating the matching of flexible objects with varying shapes and\nsizes while aligning semantic and visual nodes to enhance cross-modal feature\ncorrelation. Second, a heterogeneous graph generation module aggregates global\nvisual and local semantic node features, improving the recognition of flexible\nobjects. Additionally, We introduce the FSCW, a large-scale flexible dataset\ncurated from existing sources. We validate our method through extensive\nexperiments on flexible datasets (FDA and FSCW), and challenge benchmarks\n(CIFAR-100 and ImageNet-Hard), demonstrating competitive performance.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u8bed\u4e49\u589e\u5f3a\u7684\u5f02\u6784\u56fe\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u7075\u6d3b\u7269\u4f53\u8bc6\u522b\u3002", "motivation": "\u7075\u6d3b\u7269\u4f53\u8bc6\u522b\u56e0\u5176\u5f62\u72b6\u3001\u5927\u5c0f\u591a\u6837\u3001\u534a\u900f\u660e\u5c5e\u6027\u548c\u7c7b\u95f4\u5dee\u5f02\u7ec6\u5fae\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u56fe\u6a21\u578b\u672a\u80fd\u5145\u5206\u5bf9\u9f50\u8bed\u4e49\u4e0e\u89c6\u89c9\u4fe1\u606f\u3002", "method": "\u91c7\u7528\u81ea\u9002\u5e94\u626b\u63cf\u6a21\u5757\u63d0\u53d6\u8bed\u4e49\u4e0a\u4e0b\u6587\uff0c\u5e76\u901a\u8fc7\u5f02\u6784\u56fe\u751f\u6210\u6a21\u5757\u805a\u5408\u5168\u5c40\u89c6\u89c9\u548c\u5c40\u90e8\u8bed\u4e49\u8282\u70b9\u7279\u5f81\u3002", "result": "\u5728FDA\u3001FSCW\u6570\u636e\u96c6\u53caCIFAR-100\u3001ImageNet-Hard\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u901a\u8fc7\u8bed\u4e49\u4e0e\u89c6\u89c9\u4fe1\u606f\u7684\u5bf9\u9f50\uff0c\u63d0\u5347\u4e86\u7075\u6d3b\u7269\u4f53\u8bc6\u522b\u7684\u6027\u80fd\u3002"}}
{"id": "2503.22458", "pdf": "https://arxiv.org/pdf/2503.22458", "abs": "https://arxiv.org/abs/2503.22458", "authors": ["Shengyue Guan", "Haoyi Xiong", "Jindong Wang", "Jiang Bian", "Bin Zhu", "Jian-guang Lou"], "title": "Evaluating LLM-based Agents for Multi-Turn Conversations: A Survey", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This survey examines evaluation methods for large language model (LLM)-based\nagents in multi-turn conversational settings. Using a PRISMA-inspired\nframework, we systematically reviewed nearly 250 scholarly sources, capturing\nthe state of the art from various venues of publication, and establishing a\nsolid foundation for our analysis. Our study offers a structured approach by\ndeveloping two interrelated taxonomy systems: one that defines \\emph{what to\nevaluate} and another that explains \\emph{how to evaluate}. The first taxonomy\nidentifies key components of LLM-based agents for multi-turn conversations and\ntheir evaluation dimensions, including task completion, response quality, user\nexperience, memory and context retention, as well as planning and tool\nintegration. These components ensure that the performance of conversational\nagents is assessed in a holistic and meaningful manner. The second taxonomy\nsystem focuses on the evaluation methodologies. It categorizes approaches into\nannotation-based evaluations, automated metrics, hybrid strategies that combine\nhuman assessments with quantitative measures, and self-judging methods\nutilizing LLMs. This framework not only captures traditional metrics derived\nfrom language understanding, such as BLEU and ROUGE scores, but also\nincorporates advanced techniques that reflect the dynamic, interactive nature\nof multi-turn dialogues.", "AI": {"task": "\u7cfb\u7edf\u7efc\u8ff0\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u4f5c\u4e3a\u4ee3\u7406\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u591a\u8f6e\u5bf9\u8bdd\u4e2dLLM\u4ee3\u7406\u7684\u8bc4\u4f30\u63d0\u4f9b\u7cfb\u7edf\u5316\u7684\u5206\u7c7b\u548c\u65b9\u6cd5\u6846\u67b6\u3002", "method": "\u91c7\u7528PRISMA\u6846\u67b6\u7cfb\u7edf\u5206\u6790\u8fd1250\u7bc7\u6587\u732e\uff0c\u6784\u5efa\u4e86\u4e24\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u5206\u7c7b\u7cfb\u7edf\uff1a\u8bc4\u4f30\u5185\u5bb9\u548c\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u8bc4\u4f30LLM\u4ee3\u7406\u7684\u5173\u952e\u7ef4\u5ea6\uff08\u5982\u4efb\u52a1\u5b8c\u6210\u3001\u54cd\u5e94\u8d28\u91cf\u7b49\uff09\u548c\u65b9\u6cd5\u5206\u7c7b\uff08\u5982\u57fa\u4e8e\u6807\u6ce8\u3001\u81ea\u52a8\u5316\u6307\u6807\u7b49\uff09\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u591a\u8f6e\u5bf9\u8bdd\u4e2dLLM\u4ee3\u7406\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5168\u9762\u4e14\u6709\u610f\u4e49\u7684\u65b9\u6cd5\u3002"}}
{"id": "2503.22081", "pdf": "https://arxiv.org/pdf/2503.22081", "abs": "https://arxiv.org/abs/2503.22081", "authors": ["Ziyue Huang", "Hongxi Yan", "Qiqi Zhan", "Shuai Yang", "Mingming Zhang", "Chenkai Zhang", "YiMing Lei", "Zeming Liu", "Qingjie Liu", "Yunhong Wang"], "title": "A Survey on Remote Sensing Foundation Models: From Vision to Multimodality", "categories": ["cs.CV"], "comment": null, "summary": "The rapid advancement of remote sensing foundation models, particularly\nvision and multimodal models, has significantly enhanced the capabilities of\nintelligent geospatial data interpretation. These models combine various data\nmodalities, such as optical, radar, and LiDAR imagery, with textual and\ngeographic information, enabling more comprehensive analysis and understanding\nof remote sensing data. The integration of multiple modalities allows for\nimproved performance in tasks like object detection, land cover classification,\nand change detection, which are often challenged by the complex and\nheterogeneous nature of remote sensing data. However, despite these\nadvancements, several challenges remain. The diversity in data types, the need\nfor large-scale annotated datasets, and the complexity of multimodal fusion\ntechniques pose significant obstacles to the effective deployment of these\nmodels. Moreover, the computational demands of training and fine-tuning\nmultimodal models require significant resources, further complicating their\npractical application in remote sensing image interpretation tasks. This paper\nprovides a comprehensive review of the state-of-the-art in vision and\nmultimodal foundation models for remote sensing, focusing on their\narchitecture, training methods, datasets and application scenarios. We discuss\nthe key challenges these models face, such as data alignment, cross-modal\ntransfer learning, and scalability, while also identifying emerging research\ndirections aimed at overcoming these limitations. Our goal is to provide a\nclear understanding of the current landscape of remote sensing foundation\nmodels and inspire future research that can push the boundaries of what these\nmodels can achieve in real-world applications. The list of resources collected\nby the paper can be found in the\nhttps://github.com/IRIP-BUAA/A-Review-for-remote-sensing-vision-language-models.", "AI": {"task": "\u7efc\u8ff0\u9065\u611f\u89c6\u89c9\u548c\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\u53ca\u5176\u5728\u667a\u80fd\u5730\u7406\u7a7a\u95f4\u6570\u636e\u89e3\u8bfb\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u5c3d\u7ba1\u9065\u611f\u57fa\u7840\u6a21\u578b\u5728\u5bf9\u8c61\u68c0\u6d4b\u3001\u571f\u5730\u8986\u76d6\u5206\u7c7b\u548c\u53d8\u5316\u68c0\u6d4b\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u6570\u636e\u591a\u6837\u6027\u3001\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\u7684\u9700\u6c42\u4ee5\u53ca\u591a\u6a21\u6001\u878d\u5408\u6280\u672f\u7684\u590d\u6742\u6027\u4ecd\u5e26\u6765\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5206\u6790\u73b0\u6709\u6a21\u578b\u7684\u67b6\u6784\u3001\u8bad\u7ec3\u65b9\u6cd5\u3001\u6570\u636e\u96c6\u548c\u5e94\u7528\u573a\u666f\uff0c\u8ba8\u8bba\u6570\u636e\u5bf9\u9f50\u3001\u8de8\u6a21\u6001\u8fc1\u79fb\u5b66\u4e60\u548c\u53ef\u6269\u5c55\u6027\u7b49\u5173\u952e\u6311\u6218\u3002", "result": "\u603b\u7ed3\u4e86\u9065\u611f\u89c6\u89c9\u548c\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7684\u73b0\u72b6\uff0c\u5e76\u6307\u51fa\u4e86\u514b\u670d\u73b0\u6709\u5c40\u9650\u6027\u7684\u65b0\u5174\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u65e8\u5728\u4e3a\u9065\u611f\u57fa\u7840\u6a21\u578b\u7684\u5f53\u524d\u7814\u7a76\u63d0\u4f9b\u6e05\u6670\u7684\u7406\u89e3\uff0c\u5e76\u6fc0\u53d1\u672a\u6765\u7814\u7a76\u4ee5\u63a8\u52a8\u8fd9\u4e9b\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8fb9\u754c\u3002"}}
{"id": "2503.22473", "pdf": "https://arxiv.org/pdf/2503.22473", "abs": "https://arxiv.org/abs/2503.22473", "authors": ["Hanchao Liu", "Rongjun Li", "Weimin Xiong", "Ziyu Zhou", "Wei Peng"], "title": "WorkTeam: Constructing Workflows from Natural Language with Multi-Agents", "categories": ["cs.CL"], "comment": "Accepted in NAACL 2025 Industry Track", "summary": "Workflows play a crucial role in enhancing enterprise efficiency by\norchestrating complex processes with multiple tools or components. However,\nhand-crafted workflow construction requires expert knowledge, presenting\nsignificant technical barriers. Recent advancements in Large Language Models\n(LLMs) have improved the generation of workflows from natural language\ninstructions (aka NL2Workflow), yet existing single LLM agent-based methods\nface performance degradation on complex tasks due to the need for specialized\nknowledge and the strain of task-switching. To tackle these challenges, we\npropose WorkTeam, a multi-agent NL2Workflow framework comprising a supervisor,\norchestrator, and filler agent, each with distinct roles that collaboratively\nenhance the conversion process. As there are currently no publicly available\nNL2Workflow benchmarks, we also introduce the HW-NL2Workflow dataset, which\nincludes 3,695 real-world business samples for training and evaluation.\nExperimental results show that our approach significantly increases the success\nrate of workflow construction, providing a novel and effective solution for\nenterprise NL2Workflow services.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u591a\u4ee3\u7406\u6846\u67b6WorkTeam\uff0c\u7528\u4e8e\u4ece\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u751f\u6210\u5de5\u4f5c\u6d41\uff08NL2Workflow\uff09\u3002", "motivation": "\u624b\u5de5\u6784\u5efa\u5de5\u4f5c\u6d41\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u73b0\u6709\u5355\u4ee3\u7406LLM\u65b9\u6cd5\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u591a\u4ee3\u7406\u6846\u67b6WorkTeam\uff0c\u5305\u62ec\u76d1\u7763\u8005\u3001\u534f\u8c03\u8005\u548c\u586b\u5145\u4ee3\u7406\uff0c\u534f\u540c\u63d0\u5347\u8f6c\u6362\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cWorkTeam\u663e\u8457\u63d0\u9ad8\u4e86\u5de5\u4f5c\u6d41\u6784\u5efa\u7684\u6210\u529f\u7387\u3002", "conclusion": "WorkTeam\u4e3aNL2Workflow\u670d\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2503.22087", "pdf": "https://arxiv.org/pdf/2503.22087", "abs": "https://arxiv.org/abs/2503.22087", "authors": ["Seokha Moon", "Janghyun Baek", "Giseop Kim", "Jinkyu Kim", "Sunwook Choi"], "title": "Mitigating Trade-off: Stream and Query-guided Aggregation for Efficient and Effective 3D Occupancy Prediction", "categories": ["cs.CV"], "comment": null, "summary": "3D occupancy prediction has emerged as a key perception task for autonomous\ndriving, as it reconstructs 3D environments to provide a comprehensive scene\nunderstanding. Recent studies focus on integrating spatiotemporal information\nobtained from past observations to improve prediction accuracy, using a\nmulti-frame fusion approach that processes multiple past frames together.\nHowever, these methods struggle with a trade-off between efficiency and\naccuracy, which significantly limits their practicality. To mitigate this\ntrade-off, we propose StreamOcc, a novel framework that aggregates\nspatio-temporal information in a stream-based manner. StreamOcc consists of two\nkey components: (i) Stream-based Voxel Aggregation, which effectively\naccumulates past observations while minimizing computational costs, and (ii)\nQuery-guided Aggregation, which recurrently aggregates instance-level features\nof dynamic objects into corresponding voxel features, refining fine-grained\ndetails of dynamic objects. Experiments on the Occ3D-nuScenes dataset show that\nStreamOcc achieves state-of-the-art performance in real-time settings, while\nreducing memory usage by more than 50% compared to previous methods.", "AI": {"task": "\u63d0\u51faStreamOcc\u6846\u67b6\uff0c\u4ee5\u6d41\u5f0f\u65b9\u5f0f\u805a\u5408\u65f6\u7a7a\u4fe1\u606f\uff0c\u89e3\u51b33D\u5360\u7528\u9884\u6d4b\u4e2d\u6548\u7387\u4e0e\u51c6\u786e\u6027\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u591a\u5e27\u878d\u5408\u65f6\u9762\u4e34\u6548\u7387\u4e0e\u51c6\u786e\u6027\u7684\u6743\u8861\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "StreamOcc\u5305\u542b\u6d41\u5f0f\u4f53\u7d20\u805a\u5408\u548c\u67e5\u8be2\u5f15\u5bfc\u805a\u5408\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff0c\u6709\u6548\u79ef\u7d2f\u5386\u53f2\u89c2\u6d4b\u5e76\u4f18\u5316\u52a8\u6001\u5bf9\u8c61\u7ec6\u8282\u3002", "result": "\u5728Occ3D-nuScenes\u6570\u636e\u96c6\u4e0a\uff0cStreamOcc\u5728\u5b9e\u65f6\u8bbe\u7f6e\u4e0b\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u5185\u5b58\u4f7f\u7528\u51cf\u5c1150%\u4ee5\u4e0a\u3002", "conclusion": "StreamOcc\u901a\u8fc7\u6d41\u5f0f\u805a\u5408\u663e\u8457\u63d0\u5347\u4e863D\u5360\u7528\u9884\u6d4b\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2503.22517", "pdf": "https://arxiv.org/pdf/2503.22517", "abs": "https://arxiv.org/abs/2503.22517", "authors": ["Raman Dutt", "Harleen Hanspal", "Guoxuan Xia", "Petru-Daniel Tudosiu", "Alexander Black", "Yongxin Yang", "Steven McDonagh", "Sarah Parisot"], "title": "Exploiting Mixture-of-Experts Redundancy Unlocks Multimodal Generative Abilities", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "In this work, we undertake the challenge of augmenting the existing\ngenerative capabilities of pre-trained text-only large language models (LLMs)\nwith multi-modal generation capability while satisfying two core constraints:\nC1 preserving the preservation of original language generative capabilities\nwith negligible performance degradation, and C2 adhering to a small parameter\nbudget to learn the new modality, ensuring scalability and efficiency. In\ncontrast to current approaches that add dedicated modules, thereby\nsignificantly increasing the parameter count, we propose a method that\nleverages the underutilized capacity inherent in deep models. Specifically, we\nexploit the parameter redundancy within Mixture-of-Experts (MoEs) as a source\nof additional capacity for learning a new modality, enabling better parameter\nefficiency (C1). Moreover, we preserve the original language generation\ncapabilities by applying low-rank adaptation exclusively to the tokens of the\nnew modality (C2). Furthermore, we introduce a novel parameter initialization\nscheme based on the Gromov-Wasserstein distance to improve convergence and\ntraining stability. Through an extensive analysis of the routing mechanism, we\nuncover the emergence of modality-specific pathways and decreased redundancy\nwithin the experts that can efficiently unlock multi-modal generative\ncapabilities. Overall, our method can be seamlessly applied to a wide range of\ncontemporary LLMs, providing a new pathway for transitioning from uni-modal to\nmulti-modal architectures.", "AI": {"task": "\u589e\u5f3a\u9884\u8bad\u7ec3\u7eaf\u6587\u672c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u591a\u6a21\u6001\u751f\u6210\u80fd\u529b\uff0c\u540c\u65f6\u6ee1\u8db3\u4e24\u4e2a\u6838\u5fc3\u7ea6\u675f\u6761\u4ef6\u3002", "motivation": "\u5728\u4fdd\u6301\u539f\u6709\u8bed\u8a00\u751f\u6210\u80fd\u529b\u7684\u540c\u65f6\uff0c\u4ee5\u8f83\u5c0f\u7684\u53c2\u6570\u91cf\u5b66\u4e60\u65b0\u6a21\u6001\uff0c\u786e\u4fdd\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u3002", "method": "\u5229\u7528\u6df1\u5ea6\u6a21\u578b\u4e2d\u672a\u5145\u5206\u5229\u7528\u7684\u5bb9\u91cf\uff0c\u7279\u522b\u662f\u6df7\u5408\u4e13\u5bb6\uff08MoEs\uff09\u4e2d\u7684\u53c2\u6570\u5197\u4f59\uff0c\u901a\u8fc7\u4f4e\u79e9\u9002\u5e94\u548c\u57fa\u4e8eGromov-Wasserstein\u8ddd\u79bb\u7684\u53c2\u6570\u521d\u59cb\u5316\u65b9\u6848\u3002", "result": "\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u751f\u6210\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8bed\u8a00\u751f\u6210\u6027\u80fd\uff0c\u5e76\u63d0\u9ad8\u4e86\u53c2\u6570\u6548\u7387\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4ece\u5355\u6a21\u6001\u5230\u591a\u6a21\u6001\u67b6\u6784\u7684\u8fc7\u6e21\u63d0\u4f9b\u4e86\u4e00\u6761\u65b0\u9014\u5f84\u3002"}}
{"id": "2503.22093", "pdf": "https://arxiv.org/pdf/2503.22093", "abs": "https://arxiv.org/abs/2503.22093", "authors": ["Ximing Wen", "Mallika Mainali", "Anik Sen"], "title": "How Well Can Vison-Language Models Understand Humans' Intention? An Open-ended Theory of Mind Question Evaluation Benchmark", "categories": ["cs.CV", "cs.AI"], "comment": "2 pages, accepted by ToM@AAAI25", "summary": "Vision Language Models (VLMs) have demonstrated strong reasoning capabilities\nin Visual Question Answering (VQA) tasks; However, their ability to perform\nTheory of Mind (ToM) tasks such as accurately inferring human intentions,\nbeliefs, and other mental states remains underexplored. In this work, we\npropose an open-ended question framework to comprehensively evaluate VLMs'\nperformance across diverse categories of ToM tasks. We curated and annotated a\nbenchmark dataset composed of 30 images. We then assessed the performance of\nfour VLMs of varying sizes on this dataset. Our experimental results show that\nthe GPT-4 model outperformed all others, with only one smaller model,\nGPT-4o-mini, achieving comparable performance. Additionally, we observed that\nVLMs often struggle to accurately infer intentions in complex scenarios such as\nbullying or cheating. Moreover, our findings also reveal that smaller models\ncan sometimes infer correct intentions despite relying on incorrect visual\ncues.", "AI": {"task": "\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u5fc3\u7406\u7406\u8bba\uff08ToM\uff09\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u63a2\u7d22VLMs\u5728\u63a8\u65ad\u4eba\u7c7b\u610f\u56fe\u3001\u4fe1\u5ff5\u7b49\u5fc3\u7406\u72b6\u6001\u65b9\u9762\u7684\u80fd\u529b\uff0c\u586b\u8865\u8be5\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5f00\u653e\u5f0f\u95ee\u9898\u6846\u67b6\uff0c\u5e76\u6784\u5efa\u4e00\u4e2a\u5305\u542b30\u5f20\u56fe\u50cf\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u56db\u79cd\u4e0d\u540c\u89c4\u6a21\u7684VLMs\u3002", "result": "GPT-4\u8868\u73b0\u6700\u4f73\uff0cGPT-4o-mini\u6b21\u4e4b\uff1bVLMs\u5728\u590d\u6742\u573a\u666f\uff08\u5982\u6b3a\u51cc\u6216\u4f5c\u5f0a\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u5c0f\u6a21\u578b\u6709\u65f6\u80fd\u901a\u8fc7\u9519\u8bef\u89c6\u89c9\u7ebf\u7d22\u63a8\u65ad\u6b63\u786e\u610f\u56fe\u3002", "conclusion": "VLMs\u5728ToM\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u5c24\u5176\u5728\u590d\u6742\u573a\u666f\u4e2d\uff0c\u4f46\u5c0f\u6a21\u578b\u53ef\u80fd\u5177\u5907\u4e00\u5b9a\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2503.22547", "pdf": "https://arxiv.org/pdf/2503.22547", "abs": "https://arxiv.org/abs/2503.22547", "authors": ["Zhuo-Yang Song", "Zeyu Li", "Qing-Hong Cao", "Ming-xing Luo", "Hua Xing Zhu"], "title": "Bridging the Dimensional Chasm: Uncover Layer-wise Dimensional Reduction in Transformers through Token Correlation", "categories": ["cs.CL", "cs.LG"], "comment": "17 pages, 9 figures, 2 tables", "summary": "The geometric evolution of token representations in large language models\n(LLMs) presents a fundamental paradox: while human language inherently\norganizes semantic information in low-dimensional spaces ($\\sim 10^1$\ndimensions), modern LLMs employ high-dimensional embeddings ($\\sim 10^3$\ndimensions) processed through Transformer architectures. To resolve this\nparadox, this work bridges this conceptual gap by developing a geometric\nframework that tracks token dynamics across Transformers layers. Through\nlayer-wise analysis of intrinsic dimensions across multiple architectures, we\nreveal an expansion-contraction pattern where tokens diffuse to a \"working\nspace\" and then progressively project onto lower-dimensional submanifolds. Our\nfinding implies a negative correlation between the working space dimension and\nparameter-sensitive performance of the LLMs, and indicates that effective\nmodels tend to compress tokens into approximately 10-dimensional submanifolds,\nclosely resembling human semantic spaces. This work not only advances LLM\ninterpretability by reframing Transformers layers as projectors that mediate\nbetween high-dimensional computation and low-dimensional semantics, but also\nprovides practical tools for model diagnostics that do not rely on\ntask-specific evaluations.", "AI": {"task": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u8bcd\u5143\u8868\u793a\u7684\u51e0\u4f55\u6f14\u5316\u53ca\u5176\u4e0e\u4eba\u7c7b\u8bed\u8a00\u4f4e\u7ef4\u8bed\u4e49\u7a7a\u95f4\u7684\u5bf9\u6bd4\u3002", "motivation": "\u89e3\u51b3\u73b0\u4ee3LLMs\u4f7f\u7528\u9ad8\u7ef4\u5d4c\u5165\u4e0e\u4eba\u7c7b\u8bed\u8a00\u4f4e\u7ef4\u8bed\u4e49\u7a7a\u95f4\u4e4b\u95f4\u7684\u77db\u76fe\u3002", "method": "\u5f00\u53d1\u51e0\u4f55\u6846\u67b6\uff0c\u8ffd\u8e2aTransformer\u5c42\u95f4\u8bcd\u5143\u52a8\u6001\uff0c\u5206\u6790\u5185\u5728\u7ef4\u5ea6\u53d8\u5316\u6a21\u5f0f\u3002", "result": "\u53d1\u73b0\u8bcd\u5143\u5728\u9ad8\u7ef4\u5de5\u4f5c\u7a7a\u95f4\u6269\u6563\u540e\u9010\u6e10\u6295\u5f71\u5230\u4f4e\u7ef4\u5b50\u6d41\u5f62\uff0c\u6709\u6548\u6a21\u578b\u503e\u5411\u4e8e\u538b\u7f29\u81f3\u7ea610\u7ef4\u5b50\u6d41\u5f62\u3002", "conclusion": "\u901a\u8fc7\u5c06Transformer\u5c42\u91cd\u6784\u4e3a\u9ad8\u7ef4\u8ba1\u7b97\u4e0e\u4f4e\u7ef4\u8bed\u4e49\u7684\u5a92\u4ecb\uff0c\u63d0\u5347LLMs\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u63d0\u4f9b\u4e0d\u4f9d\u8d56\u4efb\u52a1\u8bc4\u4f30\u7684\u6a21\u578b\u8bca\u65ad\u5de5\u5177\u3002"}}
{"id": "2503.22120", "pdf": "https://arxiv.org/pdf/2503.22120", "abs": "https://arxiv.org/abs/2503.22120", "authors": ["Protyay Dey", "Rejoy Chakraborty", "Abhilasha S. Jadhav", "Kapil Rana", "Gaurav Sharma", "Puneet Goyal"], "title": "Camera Model Identification with SPAIR-Swin and Entropy based Non-Homogeneous Patches", "categories": ["cs.CV", "eess.IV"], "comment": "10 pages, 5 figures", "summary": "Source camera model identification (SCMI) plays a pivotal role in image\nforensics with applications including authenticity verification and copyright\nprotection. For identifying the camera model used to capture a given image, we\npropose SPAIR-Swin, a novel model combining a modified spatial attention\nmechanism and inverted residual block (SPAIR) with a Swin Transformer.\nSPAIR-Swin effectively captures both global and local features, enabling robust\nidentification of artifacts such as noise patterns that are particularly\neffective for SCMI. Additionally, unlike conventional methods focusing on\nhomogeneous patches, we propose a patch selection strategy for SCMI that\nemphasizes high-entropy regions rich in patterns and textures. Extensive\nevaluations on four benchmark SCMI datasets demonstrate that SPAIR-Swin\noutperforms existing methods, achieving patch-level accuracies of 99.45%,\n98.39%, 99.45%, and 97.46% and image-level accuracies of 99.87%, 99.32%, 100%,\nand 98.61% on the Dresden, Vision, Forchheim, and Socrates datasets,\nrespectively. Our findings highlight that high-entropy patches, which contain\nhigh-frequency information such as edge sharpness, noise, and compression\nartifacts, are more favorable in improving SCMI accuracy. Code will be made\navailable upon request.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u6539\u8fdb\u7a7a\u95f4\u6ce8\u610f\u529b\u673a\u5236\u548c\u5012\u7f6e\u6b8b\u5dee\u5757\uff08SPAIR\uff09\u4e0eSwin Transformer\u7684\u65b0\u6a21\u578bSPAIR-Swin\uff0c\u7528\u4e8e\u6e90\u76f8\u673a\u6a21\u578b\u8bc6\u522b\uff08SCMI\uff09\u3002", "motivation": "\u6e90\u76f8\u673a\u6a21\u578b\u8bc6\u522b\u5728\u56fe\u50cf\u53d6\u8bc1\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u5982\u771f\u5b9e\u6027\u9a8c\u8bc1\u548c\u7248\u6743\u4fdd\u62a4\u3002", "method": "\u7ed3\u5408SPAIR\u548cSwin Transformer\uff0c\u63d0\u51fa\u4e00\u79cd\u5f3a\u8c03\u9ad8\u71b5\u533a\u57df\u7684\u8865\u4e01\u9009\u62e9\u7b56\u7565\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8865\u4e01\u7ea7\u548c\u56fe\u50cf\u7ea7\u51c6\u786e\u7387\u5747\u8d85\u8fc7\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u9ad8\u71b5\u8865\u4e01\uff08\u5305\u542b\u9ad8\u9891\u4fe1\u606f\uff09\u6709\u52a9\u4e8e\u63d0\u9ad8SCMI\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2503.22582", "pdf": "https://arxiv.org/pdf/2503.22582", "abs": "https://arxiv.org/abs/2503.22582", "authors": ["Sarubi Thillainathan", "Songchen Yuan", "En-Shiun Annie Lee", "Sanath Jayasena", "Surangika Ranathunga"], "title": "Beyond Vanilla Fine-Tuning: Leveraging Multistage, Multilingual, and Domain-Specific Methods for Low-Resource Machine Translation", "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning multilingual sequence-to-sequence large language models (msLLMs)\nhas shown promise in developing neural machine translation (NMT) systems for\nlow-resource languages (LRLs). However, conventional single-stage fine-tuning\nmethods struggle in extremely low-resource NMT settings, where training data is\nvery limited. This paper contributes to artificial intelligence by proposing\ntwo approaches for adapting msLLMs in these challenging scenarios: (1)\ncontinual pre-training (CPT), where the msLLM is further trained with\ndomain-specific monolingual data to compensate for the under-representation of\nLRLs, and (2) intermediate task transfer learning (ITTL), a method that\nfine-tunes the msLLM with both in-domain and out-of-domain parallel data to\nenhance its translation capabilities across various domains and tasks. As an\napplication in engineering, these methods are implemented in NMT systems for\nSinhala, Tamil, and English (six language pairs) in domain-specific, extremely\nlow-resource settings (datasets containing fewer than 100,000 samples). Our\nexperiments reveal that these approaches enhance translation performance by an\naverage of +1.47 bilingual evaluation understudy (BLEU) score compared to the\nstandard single-stage fine-tuning baseline across all translation directions.\nAdditionally, a multi-model ensemble further improves performance by an\nadditional BLEU score.", "AI": {"task": "\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff08\u6301\u7eed\u9884\u8bad\u7ec3\u548c\u4e2d\u95f4\u4efb\u52a1\u8fc1\u79fb\u5b66\u4e60\uff09\u6765\u6539\u8fdb\u591a\u8bed\u8a00\u5e8f\u5217\u5230\u5e8f\u5217\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6781\u4f4e\u8d44\u6e90\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5355\u9636\u6bb5\u5fae\u8c03\u65b9\u6cd5\u5728\u6781\u4f4e\u8d44\u6e90\u673a\u5668\u7ffb\u8bd1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u9002\u5e94\u8fd9\u79cd\u6311\u6218\u6027\u573a\u666f\u3002", "method": "\u91c7\u7528\u6301\u7eed\u9884\u8bad\u7ec3\uff08CPT\uff09\u548c\u4e2d\u95f4\u4efb\u52a1\u8fc1\u79fb\u5b66\u4e60\uff08ITTL\uff09\u4e24\u79cd\u65b9\u6cd5\uff0c\u5206\u522b\u5229\u7528\u9886\u57df\u7279\u5b9a\u5355\u8bed\u6570\u636e\u548c\u8de8\u9886\u57df\u5e76\u884c\u6570\u636e\u589e\u5f3a\u6a21\u578b\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u4e24\u79cd\u65b9\u6cd5\u5728\u516d\u79cd\u8bed\u8a00\u5bf9\u7684\u6781\u4f4e\u8d44\u6e90\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u5e73\u5747\u63d0\u5347\u4e861.47 BLEU\u5206\u6570\uff0c\u591a\u6a21\u578b\u96c6\u6210\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6781\u4f4e\u8d44\u6e90\u673a\u5668\u7ffb\u8bd1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2503.22121", "pdf": "https://arxiv.org/pdf/2503.22121", "abs": "https://arxiv.org/abs/2503.22121", "authors": ["Tharun Anand", "Siva Sankar", "Pravin Nair"], "title": "Detecting Localized Deepfake Manipulations Using Action Unit-Guided Video Representations", "categories": ["cs.CV"], "comment": null, "summary": "With rapid advancements in generative modeling, deepfake techniques are\nincreasingly narrowing the gap between real and synthetic videos, raising\nserious privacy and security concerns. Beyond traditional face swapping and\nreenactment, an emerging trend in recent state-of-the-art deepfake generation\nmethods involves localized edits such as subtle manipulations of specific\nfacial features like raising eyebrows, altering eye shapes, or modifying mouth\nexpressions. These fine-grained manipulations pose a significant challenge for\nexisting detection models, which struggle to capture such localized variations.\nTo the best of our knowledge, this work presents the first detection approach\nexplicitly designed to generalize to localized edits in deepfake videos by\nleveraging spatiotemporal representations guided by facial action units. Our\nmethod leverages a cross-attention-based fusion of representations learned from\npretext tasks like random masking and action unit detection, to create an\nembedding that effectively encodes subtle, localized changes. Comprehensive\nevaluations across multiple deepfake generation methods demonstrate that our\napproach, despite being trained solely on the traditional FF+ dataset, sets a\nnew benchmark in detecting recent deepfake-generated videos with fine-grained\nlocal edits, achieving a $20\\%$ improvement in accuracy over current\nstate-of-the-art detection methods. Additionally, our method delivers\ncompetitive performance on standard datasets, highlighting its robustness and\ngeneralization across diverse types of local and global forgeries.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u4e13\u95e8\u9488\u5bf9\u6df1\u5ea6\u4f2a\u9020\u89c6\u9891\u4e2d\u5c40\u90e8\u7f16\u8f91\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u751f\u6210\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u9010\u6e10\u7f29\u5c0f\u4e86\u771f\u5b9e\u4e0e\u5408\u6210\u89c6\u9891\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5f15\u53d1\u4e86\u4e25\u91cd\u7684\u9690\u79c1\u548c\u5b89\u5168\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5c40\u90e8\u7f16\u8f91\u7684\u7cbe\u7ec6\u64cd\u7eb5\u5bf9\u73b0\u6709\u68c0\u6d4b\u6a21\u578b\u63d0\u51fa\u4e86\u6311\u6218\u3002", "method": "\u5229\u7528\u57fa\u4e8e\u9762\u90e8\u52a8\u4f5c\u5355\u5143\u7684\u65f6\u7a7a\u8868\u793a\uff0c\u901a\u8fc7\u8de8\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u968f\u673a\u63a9\u7801\u548c\u52a8\u4f5c\u5355\u5143\u68c0\u6d4b\u7b49\u524d\u7f6e\u4efb\u52a1\u5b66\u4e60\u5230\u7684\u8868\u793a\uff0c\u751f\u6210\u80fd\u591f\u6709\u6548\u7f16\u7801\u5c40\u90e8\u7ec6\u5fae\u53d8\u5316\u7684\u5d4c\u5165\u3002", "result": "\u5728\u591a\u4e2a\u6df1\u5ea6\u4f2a\u9020\u751f\u6210\u65b9\u6cd5\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u68c0\u6d4b\u5c40\u90e8\u7f16\u8f91\u7684\u6df1\u5ea6\u4f2a\u9020\u89c6\u9891\u4e0a\u5b9e\u73b0\u4e8620%\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0c\u5e76\u5728\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u68c0\u6d4b\u5c40\u90e8\u7f16\u8f91\u7684\u6df1\u5ea6\u4f2a\u9020\u89c6\u9891\u63d0\u4f9b\u4e86\u65b0\u7684\u57fa\u51c6\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2503.22585", "pdf": "https://arxiv.org/pdf/2503.22585", "abs": "https://arxiv.org/abs/2503.22585", "authors": ["Kevin Cohen", "Laura Manrique-G\u00f3mez", "Rub\u00e9n Manrique"], "title": "Historical Ink: Exploring Large Language Models for Irony Detection in 19th-Century Spanish", "categories": ["cs.CL", "cs.AI", "cs.DL", "I.2.7"], "comment": null, "summary": "This study explores the use of large language models (LLMs) to enhance\ndatasets and improve irony detection in 19th-century Latin American newspapers.\nTwo strategies were employed to evaluate the efficacy of BERT and GPT-4o models\nin capturing the subtle nuances nature of irony, through both multi-class and\nbinary classification tasks. First, we implemented dataset enhancements focused\non enriching emotional and contextual cues; however, these showed limited\nimpact on historical language analysis. The second strategy, a semi-automated\nannotation process, effectively addressed class imbalance and augmented the\ndataset with high-quality annotations. Despite the challenges posed by the\ncomplexity of irony, this work contributes to the advancement of sentiment\nanalysis through two key contributions: introducing a new historical Spanish\ndataset tagged for sentiment analysis and irony detection, and proposing a\nsemi-automated annotation methodology where human expertise is crucial for\nrefining LLMs results, enriched by incorporating historical and cultural\ncontexts as core features.", "AI": {"task": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u589e\u5f3a\u6570\u636e\u96c6\u5e76\u6539\u8fdb19\u4e16\u7eaa\u62c9\u4e01\u7f8e\u6d32\u62a5\u7eb8\u4e2d\u7684\u53cd\u8bbd\u68c0\u6d4b\u3002", "motivation": "\u63a2\u7d22BERT\u548cGPT-4o\u6a21\u578b\u5728\u6355\u6349\u53cd\u8bbd\u5fae\u5999\u7279\u6027\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u901a\u8fc7\u591a\u5206\u7c7b\u548c\u4e8c\u5206\u7c7b\u4efb\u52a1\u8bc4\u4f30\u5176\u8868\u73b0\u3002", "method": "\u91c7\u7528\u4e24\u79cd\u7b56\u7565\uff1a\u6570\u636e\u96c6\u589e\u5f3a\uff08\u805a\u7126\u60c5\u611f\u548c\u4e0a\u4e0b\u6587\u7ebf\u7d22\uff09\u548c\u534a\u81ea\u52a8\u5316\u6807\u6ce8\u8fc7\u7a0b\uff08\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff09\u3002", "result": "\u6570\u636e\u96c6\u589e\u5f3a\u5bf9\u5386\u53f2\u8bed\u8a00\u5206\u6790\u6548\u679c\u6709\u9650\uff0c\u4f46\u534a\u81ea\u52a8\u5316\u6807\u6ce8\u6210\u529f\u63d0\u5347\u4e86\u6570\u636e\u8d28\u91cf\u5e76\u89e3\u51b3\u4e86\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u5386\u53f2\u897f\u73ed\u7259\u8bed\u6570\u636e\u96c6\u548c\u534a\u81ea\u52a8\u5316\u6807\u6ce8\u65b9\u6cd5\uff0c\u63a8\u52a8\u4e86\u60c5\u611f\u5206\u6790\u548c\u53cd\u8bbd\u68c0\u6d4b\u7684\u8fdb\u5c55\uff0c\u5f3a\u8c03\u4e86\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u5728\u4f18\u5316LLM\u7ed3\u679c\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2503.22125", "pdf": "https://arxiv.org/pdf/2503.22125", "abs": "https://arxiv.org/abs/2503.22125", "authors": ["Ivan Beleacov"], "title": "Semantic segmentation for building houses from wooden cubes", "categories": ["cs.CV"], "comment": "10 pages, 6 figures, 2 tables", "summary": "Automated construction is one of the most promising areas that can improve\nefficiency, reduce costs and minimize errors in the process of building\nconstruction. In this paper, a comparative analysis of three neural network\nmodels for semantic segmentation, U-Net(light), LinkNet and PSPNet, is\nperformed. Two specialized datasets with images of houses built from wooden\ncubes were created for the experiments. The first dataset contains 4 classes\n(background, foundation, walls, roof ) and is designed for basic model\nevaluation, while the second dataset includes 44 classes where each cube is\nlabeled as a separate object. The models were trained with the same\nhyperparameters and their accuracy was evaluated using MeanIoU and F1 Score\nmetrics. According to the results obtained, U-Net(light) showed the best\nperformance with 78% MeanIoU and 87% F1 Score on the first dataset and 17% and\n25% respectively on the second dataset. The poor results on the second dataset\nare due to the limited amount of data, the complexity of the partitioning and\nthe imbalance of classes, making it difficult to accurately select individual\ncubes. In addition, overtraining was observed in all experiments, manifested by\nhigh accuracy on the training dataset and its significant decrease on the\nvalidation dataset. The present work is the basis for the development of\nalgorithms for automatic generation of staged building plans, which can be\nfurther scaled to design complete buildings. Future research is planned to\nextend the datasets and apply methods to combat overfitting (L1/L2\nregularization, Early Stopping). The next stage of work will be the development\nof algorithms for automatic generation of a step-by-step plan for building\nhouses from cubes using manipulators. Index Terms-Deep Learning, Computer\nvision, CNN, Semantic segmentation, Construction materials.", "AI": {"task": "\u6bd4\u8f83\u5206\u6790\u4e09\u79cd\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff08U-Net(light)\u3001LinkNet\u548cPSPNet\uff09\u5728\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u81ea\u52a8\u5316\u5efa\u7b51\u662f\u63d0\u9ad8\u6548\u7387\u3001\u964d\u4f4e\u6210\u672c\u548c\u51cf\u5c11\u9519\u8bef\u7684\u91cd\u8981\u9886\u57df\uff0c\u672c\u7814\u7a76\u65e8\u5728\u4e3a\u81ea\u52a8\u751f\u6210\u5206\u9636\u6bb5\u5efa\u7b51\u8ba1\u5212\u5960\u5b9a\u57fa\u7840\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u4e13\u95e8\u7684\u6570\u636e\u96c6\uff084\u7c7b\u548c44\u7c7b\uff09\u8bad\u7ec3\u6a21\u578b\uff0c\u5e76\u901a\u8fc7MeanIoU\u548cF1 Score\u8bc4\u4f30\u6027\u80fd\u3002", "result": "U-Net(light)\u8868\u73b0\u6700\u4f73\uff0c\u57284\u7c7b\u6570\u636e\u96c6\u4e0aMeanIoU\u4e3a78%\uff0cF1 Score\u4e3a87%\uff0c\u4f46\u572844\u7c7b\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u8f83\u5dee\uff08MeanIoU 17%\uff0cF1 Score 25%\uff09\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5c06\u6269\u5c55\u6570\u636e\u96c6\u5e76\u5e94\u7528\u6297\u8fc7\u62df\u5408\u65b9\u6cd5\uff0c\u8fdb\u4e00\u6b65\u5f00\u53d1\u81ea\u52a8\u751f\u6210\u5efa\u7b51\u8ba1\u5212\u7684\u7b97\u6cd5\u3002"}}
{"id": "2503.22678", "pdf": "https://arxiv.org/pdf/2503.22678", "abs": "https://arxiv.org/abs/2503.22678", "authors": ["Mohammad Almansoori", "Komal Kumar", "Hisham Cholakkal"], "title": "Self-Evolving Multi-Agent Simulations for Realistic Clinical Interactions", "categories": ["cs.CL"], "comment": "14 page, 4 figures, 61 references", "summary": "In this work, we introduce MedAgentSim, an open-source simulated clinical\nenvironment with doctor, patient, and measurement agents designed to evaluate\nand enhance LLM performance in dynamic diagnostic settings. Unlike prior\napproaches, our framework requires doctor agents to actively engage with\npatients through multi-turn conversations, requesting relevant medical\nexaminations (e.g., temperature, blood pressure, ECG) and imaging results\n(e.g., MRI, X-ray) from a measurement agent to mimic the real-world diagnostic\nprocess. Additionally, we incorporate self improvement mechanisms that allow\nmodels to iteratively refine their diagnostic strategies. We enhance LLM\nperformance in our simulated setting by integrating multi-agent discussions,\nchain-of-thought reasoning, and experience-based knowledge retrieval,\nfacilitating progressive learning as doctor agents interact with more patients.\nWe also introduce an evaluation benchmark for assessing the LLM's ability to\nengage in dynamic, context-aware diagnostic interactions. While MedAgentSim is\nfully automated, it also supports a user-controlled mode, enabling human\ninteraction with either the doctor or patient agent. Comprehensive evaluations\nin various simulated diagnostic scenarios demonstrate the effectiveness of our\napproach. Our code, simulation tool, and benchmark are available at\n\\href{https://medagentsim.netlify.app/}.", "AI": {"task": "\u4ecb\u7ecd\u5e76\u8bc4\u4f30MedAgentSim\uff0c\u4e00\u4e2a\u7528\u4e8e\u52a8\u6001\u8bca\u65ad\u73af\u5883\u4e2d\u589e\u5f3aLLM\u6027\u80fd\u7684\u5f00\u6e90\u6a21\u62df\u4e34\u5e8a\u73af\u5883\u3002", "motivation": "\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u7684\u8bca\u65ad\u8fc7\u7a0b\uff0c\u901a\u8fc7\u591a\u8f6e\u5bf9\u8bdd\u548c\u533b\u7597\u68c0\u67e5\u8bf7\u6c42\uff0c\u63d0\u5347LLM\u5728\u52a8\u6001\u8bca\u65ad\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u591a\u4ee3\u7406\u6846\u67b6\uff08\u533b\u751f\u3001\u60a3\u8005\u3001\u6d4b\u91cf\u4ee3\u7406\uff09\uff0c\u7ed3\u5408\u591a\u4ee3\u7406\u8ba8\u8bba\u3001\u601d\u7ef4\u94fe\u63a8\u7406\u548c\u7ecf\u9a8c\u77e5\u8bc6\u68c0\u7d22\uff0c\u5b9e\u73b0\u6e10\u8fdb\u5f0f\u5b66\u4e60\u3002", "result": "\u5728\u6a21\u62df\u8bca\u65ad\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u4ee3\u7801\u3001\u5de5\u5177\u548c\u8bc4\u4f30\u57fa\u51c6\u3002", "conclusion": "MedAgentSim\u4e3a\u52a8\u6001\u8bca\u65ad\u73af\u5883\u4e2d\u7684LLM\u6027\u80fd\u63d0\u5347\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u652f\u6301\u81ea\u52a8\u5316\u4e0e\u4eba\u5de5\u4ea4\u4e92\u3002"}}
{"id": "2503.22136", "pdf": "https://arxiv.org/pdf/2503.22136", "abs": "https://arxiv.org/abs/2503.22136", "authors": ["Hongmei Yin", "Tingliang Feng", "Fan Lyu", "Fanhua Shang", "Hongying Liu", "Wei Feng", "Liang Wan"], "title": "Beyond Background Shift: Rethinking Instance Replay in Continual Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "In this work, we focus on continual semantic segmentation (CSS), where\nsegmentation networks are required to continuously learn new classes without\nerasing knowledge of previously learned ones. Although storing images of old\nclasses and directly incorporating them into the training of new models has\nproven effective in mitigating catastrophic forgetting in classification tasks,\nthis strategy presents notable limitations in CSS. Specifically, the stored and\nnew images with partial category annotations leads to confusion between\nunannotated categories and the background, complicating model fitting. To\ntackle this issue, this paper proposes a novel Enhanced Instance Replay (EIR)\nmethod, which not only preserves knowledge of old classes while simultaneously\neliminating background confusion by instance storage of old classes, but also\nmitigates background shifts in the new images by integrating stored instances\nwith new images. By effectively resolving background shifts in both stored and\nnew images, EIR alleviates catastrophic forgetting in the CSS task, thereby\nenhancing the model's capacity for CSS. Experimental results validate the\nefficacy of our approach, which significantly outperforms state-of-the-art CSS\nmethods.", "AI": {"task": "\u7814\u7a76\u6301\u7eed\u8bed\u4e49\u5206\u5272\uff08CSS\uff09\u4e2d\u5982\u4f55\u5728\u4e0d\u9057\u5fd8\u65e7\u7c7b\u522b\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\u5b66\u4e60\u65b0\u7c7b\u522b\u3002", "motivation": "\u76f4\u63a5\u5b58\u50a8\u65e7\u7c7b\u522b\u56fe\u50cf\u5e76\u7528\u4e8e\u65b0\u6a21\u578b\u8bad\u7ec3\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u6709\u6548\uff0c\u4f46\u5728CSS\u4e2d\u4f1a\u5bfc\u81f4\u672a\u6807\u6ce8\u7c7b\u522b\u4e0e\u80cc\u666f\u6df7\u6dc6\uff0c\u5f71\u54cd\u6a21\u578b\u62df\u5408\u3002", "method": "\u63d0\u51fa\u589e\u5f3a\u5b9e\u4f8b\u56de\u653e\uff08EIR\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b58\u50a8\u65e7\u7c7b\u522b\u5b9e\u4f8b\u6d88\u9664\u80cc\u666f\u6df7\u6dc6\uff0c\u5e76\u6574\u5408\u5b58\u50a8\u5b9e\u4f8b\u4e0e\u65b0\u56fe\u50cf\u4ee5\u7f13\u89e3\u80cc\u666f\u504f\u79fb\u3002", "result": "EIR\u663e\u8457\u4f18\u4e8e\u73b0\u6709CSS\u65b9\u6cd5\uff0c\u6709\u6548\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u3002", "conclusion": "EIR\u901a\u8fc7\u89e3\u51b3\u80cc\u666f\u504f\u79fb\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5728CSS\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2503.21798", "pdf": "https://arxiv.org/pdf/2503.21798", "abs": "https://arxiv.org/abs/2503.21798", "authors": ["Ning-Yuan Georgia Liu", "David R. Keith"], "title": "Leveraging Large Language Models for Automated Causal Loop Diagram Generation: Enhancing System Dynamics Modeling through Curated Prompting Techniques", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Transforming a dynamic hypothesis into a causal loop diagram (CLD) is crucial\nfor System Dynamics Modelling. Extracting key variables and causal\nrelationships from text to build a CLD is often challenging and time-consuming\nfor novice modelers, limiting SD tool adoption. This paper introduces and tests\na method for automating the translation of dynamic hypotheses into CLDs using\nlarge language models (LLMs) with curated prompting techniques. We first\ndescribe how LLMs work and how they can make the inferences needed to build\nCLDs using a standard digraph structure. Next, we develop a set of simple\ndynamic hypotheses and corresponding CLDs from leading SD textbooks. We then\ncompare the four different combinations of prompting techniques, evaluating\ntheir performance against CLDs labeled by expert modelers. Results show that\nfor simple model structures and using curated prompting techniques, LLMs can\ngenerate CLDs of a similar quality to expert-built ones, accelerating CLD\ncreation.", "AI": {"task": "\u81ea\u52a8\u5316\u5c06\u52a8\u6001\u5047\u8bbe\u8f6c\u5316\u4e3a\u56e0\u679c\u5faa\u73af\u56fe\uff08CLD\uff09\u7684\u8fc7\u7a0b\u3002", "motivation": "\u89e3\u51b3\u65b0\u624b\u5efa\u6a21\u8005\u5728\u4ece\u6587\u672c\u4e2d\u63d0\u53d6\u5173\u952e\u53d8\u91cf\u548c\u56e0\u679c\u5173\u7cfb\u65f6\u9762\u4e34\u7684\u6311\u6218\uff0c\u63d0\u9ad8\u7cfb\u7edf\u52a8\u529b\u5b66\u5de5\u5177\u7684\u5e94\u7528\u7387\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u4f18\u5316\u7684\u63d0\u793a\u6280\u672f\uff0c\u5f00\u53d1\u5e76\u6d4b\u8bd5\u81ea\u52a8\u5316\u751f\u6210CLD\u7684\u65b9\u6cd5\u3002", "result": "\u5bf9\u4e8e\u7b80\u5355\u6a21\u578b\u7ed3\u6784\uff0c\u4f7f\u7528\u4f18\u5316\u7684\u63d0\u793a\u6280\u672f\uff0cLLMs\u751f\u6210\u7684CLD\u8d28\u91cf\u63a5\u8fd1\u4e13\u5bb6\u6784\u5efa\u7684\u6c34\u5e73\uff0c\u663e\u8457\u52a0\u901f\u4e86CLD\u7684\u521b\u5efa\u3002", "conclusion": "LLMs\u7ed3\u5408\u4f18\u5316\u63d0\u793a\u6280\u672f\u53ef\u4ee5\u6709\u6548\u652f\u6301CLD\u7684\u81ea\u52a8\u5316\u751f\u6210\uff0c\u4e3a\u65b0\u624b\u5efa\u6a21\u8005\u63d0\u4f9b\u4fbf\u5229\u3002"}}
{"id": "2503.22152", "pdf": "https://arxiv.org/pdf/2503.22152", "abs": "https://arxiv.org/abs/2503.22152", "authors": ["Yuxuan Li", "Vijay Veerabadran", "Michael L. Iuzzolino", "Brett D. Roads", "Asli Celikyilmaz", "Karl Ridgeway"], "title": "EgoToM: Benchmarking Theory of Mind Reasoning from Egocentric Videos", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We introduce EgoToM, a new video question-answering benchmark that extends\nTheory-of-Mind (ToM) evaluation to egocentric domains. Using a causal ToM\nmodel, we generate multi-choice video QA instances for the Ego4D dataset to\nbenchmark the ability to predict a camera wearer's goals, beliefs, and next\nactions. We study the performance of both humans and state of the art\nmultimodal large language models (MLLMs) on these three interconnected\ninference problems. Our evaluation shows that MLLMs achieve close to\nhuman-level accuracy on inferring goals from egocentric videos. However, MLLMs\n(including the largest ones we tested with over 100B parameters) fall short of\nhuman performance when inferring the camera wearers' in-the-moment belief\nstates and future actions that are most consistent with the unseen video\nfuture. We believe that our results will shape the future design of an\nimportant class of egocentric digital assistants which are equipped with a\nreasonable model of the user's internal mental states.", "AI": {"task": "\u63d0\u51faEgoToM\uff0c\u4e00\u4e2a\u65b0\u7684\u89c6\u9891\u95ee\u7b54\u57fa\u51c6\uff0c\u5c06\u5fc3\u7406\u7406\u8bba\uff08ToM\uff09\u8bc4\u4f30\u6269\u5c55\u5230\u81ea\u6211\u4e2d\u5fc3\u9886\u57df\u3002", "motivation": "\u901a\u8fc7\u56e0\u679cToM\u6a21\u578b\u751f\u6210\u591a\u9009\u89c6\u9891\u95ee\u7b54\u5b9e\u4f8b\uff0c\u4ee5\u8bc4\u4f30\u9884\u6d4b\u6444\u50cf\u5934\u4f69\u6234\u8005\u76ee\u6807\u3001\u4fe1\u5ff5\u548c\u4e0b\u4e00\u6b65\u884c\u52a8\u7684\u80fd\u529b\u3002", "method": "\u4f7f\u7528Ego4D\u6570\u636e\u96c6\uff0c\u7814\u7a76\u4eba\u7c7b\u548c\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u4e09\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u63a8\u7406\u95ee\u9898\u4e0a\u7684\u8868\u73b0\u3002", "result": "MLLMs\u5728\u4ece\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u63a8\u65ad\u76ee\u6807\u65b9\u9762\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\uff0c\u4f46\u5728\u63a8\u65ad\u4f69\u6234\u8005\u5373\u65f6\u4fe1\u5ff5\u72b6\u6001\u548c\u672a\u6765\u884c\u52a8\u65b9\u9762\u8868\u73b0\u4e0d\u53ca\u4eba\u7c7b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5c06\u5f71\u54cd\u672a\u6765\u8bbe\u8ba1\u5177\u5907\u7528\u6237\u5185\u90e8\u5fc3\u7406\u72b6\u6001\u6a21\u578b\u7684\u81ea\u6211\u4e2d\u5fc3\u6570\u5b57\u52a9\u624b\u3002"}}
{"id": "2503.21801", "pdf": "https://arxiv.org/pdf/2503.21801", "abs": "https://arxiv.org/abs/2503.21801", "authors": ["Kwangjun Ahn", "Alex Lamb", "John Langford"], "title": "Efficient Joint Prediction of Multiple Future Tokens", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Technical report; comments welcome!", "summary": "In this short report, we introduce joint multi-token prediction (JTP), a\nlightweight modification of standard next-token prediction designed to enrich\nhidden state representations by jointly predicting multiple future tokens.\nUnlike previous multi-token prediction approaches, JTP strategically employs\nteacher forcing of future-tokens through a carefully designed representation\nbottleneck, allowing the model to encode rich predictive information with\nminimal computational overhead during training. We show that the JTP approach\nachieves a short-horizon belief state representation, while popular\nalternatives for multi-token prediction fail to do so. We demonstrate the\neffectiveness of our method on the synthetic star graph navigation task from\nfrom Bachmann and Nagarajan [2024], highlighting a significant performance\nimprovement over existing methods. This manuscript presents promising\npreliminary results intended to stimulate further research.", "AI": {"task": "\u4ecb\u7ecd\u8054\u5408\u591a\u4ee4\u724c\u9884\u6d4b\uff08JTP\uff09\uff0c\u4e00\u79cd\u6539\u8fdb\u6807\u51c6\u4e0b\u4e00\u4ee4\u724c\u9884\u6d4b\u7684\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u65e8\u5728\u901a\u8fc7\u8054\u5408\u9884\u6d4b\u591a\u4e2a\u672a\u6765\u4ee4\u724c\u6765\u4e30\u5bcc\u9690\u85cf\u72b6\u6001\u8868\u793a\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u4ee4\u724c\u9884\u6d4b\u65b9\u6cd5\u672a\u80fd\u5b9e\u73b0\u77ed\u65f6\u4fe1\u5ff5\u72b6\u6001\u8868\u793a\uff0c\u800cJTP\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8868\u793a\u74f6\u9888\u548c\u6559\u5e08\u5f3a\u5236\u6280\u672f\uff0c\u4ee5\u6700\u5c0f\u8ba1\u7b97\u5f00\u9500\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u3002", "method": "JTP\u901a\u8fc7\u6559\u5e08\u5f3a\u5236\u672a\u6765\u4ee4\u724c\u548c\u8868\u793a\u74f6\u9888\u8bbe\u8ba1\uff0c\u8054\u5408\u9884\u6d4b\u591a\u4e2a\u672a\u6765\u4ee4\u724c\uff0c\u4ee5\u4e30\u5bcc\u9690\u85cf\u72b6\u6001\u8868\u793a\u3002", "result": "\u5728\u5408\u6210\u661f\u56fe\u5bfc\u822a\u4efb\u52a1\u4e2d\uff0cJTP\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "JTP\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u521d\u6b65\u7ed3\u679c\uff0c\u65e8\u5728\u63a8\u52a8\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2503.22154", "pdf": "https://arxiv.org/pdf/2503.22154", "abs": "https://arxiv.org/abs/2503.22154", "authors": ["Jae-Young Yim", "Dongwook Kim", "Jae-Young Sim"], "title": "Permutation-Invariant and Orientation-Aware Dataset Distillation for 3D Point Clouds", "categories": ["cs.CV"], "comment": null, "summary": "We should collect large amount of data to train deep neural networks for\nvarious applications. Recently, the dataset distillation for images and texts\nhas been attracting a lot of attention, that reduces the original dataset to a\nsynthetic dataset while preserving essential task-relevant information.\nHowever, 3D point clouds distillation is almost unexplored due to the\nchallenges of unordered structures of points. In this paper, we propose a novel\ndistribution matching-based dataset distillation method for 3D point clouds\nthat jointly optimizes the geometric structures of synthetic dataset as well as\nthe orientations of synthetic models. To ensure the consistent feature\nalignment between different 3D point cloud models, we devise a permutation\ninvariant distribution matching loss with the sorted feature vectors. We also\nemploy learnable rotation angles to transform each syntheic model according to\nthe optimal orientation best representing the original feature distribution.\nExtensive experimental results on widely used four benchmark datasets,\nincluding ModelNet10, ModelNet40, ShapeNet, and ScanObjectNN, demonstrate that\nthe proposed method consistently outperforms the existing methods.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5206\u5e03\u5339\u914d\u7684\u6570\u636e\u96c6\u84b8\u998f\u65b9\u6cd5\uff0c\u7528\u4e8e3D\u70b9\u4e91\u6570\u636e\u3002", "motivation": "3D\u70b9\u4e91\u6570\u636e\u7531\u4e8e\u5176\u65e0\u5e8f\u7ed3\u6784\u7684\u7279\u70b9\uff0c\u6570\u636e\u96c6\u84b8\u998f\u65b9\u6cd5\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u8054\u5408\u4f18\u5316\u5408\u6210\u6570\u636e\u96c6\u7684\u51e0\u4f55\u7ed3\u6784\u548c\u5408\u6210\u6a21\u578b\u7684\u65b9\u5411\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u6392\u5217\u4e0d\u53d8\u5206\u5e03\u5339\u914d\u635f\u5931\uff0c\u5e76\u91c7\u7528\u53ef\u5b66\u4e60\u65cb\u8f6c\u89d2\u5ea6\u8c03\u6574\u6a21\u578b\u65b9\u5411\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff08ModelNet10\u3001ModelNet40\u3001ShapeNet\u3001ScanObjectNN\uff09\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u57283D\u70b9\u4e91\u6570\u636e\u96c6\u84b8\u998f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2503.21810", "pdf": "https://arxiv.org/pdf/2503.21810", "abs": "https://arxiv.org/abs/2503.21810", "authors": ["Zhenyu Wu", "Jiaoyan Chen", "Norman W. Paton"], "title": "Taxonomy Inference for Tabular Data Using Large Language Models", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.IR"], "comment": null, "summary": "Taxonomy inference for tabular data is a critical task of schema inference,\naiming at discovering entity types (i.e., concepts) of the tables and building\ntheir hierarchy. It can play an important role in data management, data\nexploration, ontology learning, and many data-centric applications. Existing\nschema inference systems focus more on XML, JSON or RDF data, and often rely on\nlexical formats and structures of the data for calculating similarities, with\nlimited exploitation of the semantics of the text across a table. Motivated by\nrecent works on taxonomy completion and construction using Large Language\nModels (LLMs), this paper presents two LLM-based methods for taxonomy inference\nfor tables: (i) EmTT which embeds columns by fine-tuning with contrastive\nlearning encoder-alone LLMs like BERT and utilises clustering for hierarchy\nconstruction, and (ii) GeTT which generates table entity types and their\nhierarchy by iterative prompting using a decoder-alone LLM like GPT-4.\nExtensive evaluation on three real-world datasets with six metrics covering\ndifferent aspects of the output taxonomies has demonstrated that EmTT and GeTT\ncan both produce taxonomies with strong consistency relative to the Ground\nTruth.", "AI": {"task": "\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u65b9\u6cd5\uff08EmTT\u548cGeTT\uff09\u7528\u4e8e\u8868\u683c\u6570\u636e\u7684\u5206\u7c7b\u63a8\u65ad\u3002", "motivation": "\u73b0\u6709\u6a21\u5f0f\u63a8\u65ad\u7cfb\u7edf\u4e3b\u8981\u5173\u6ce8XML\u3001JSON\u6216RDF\u6570\u636e\uff0c\u4e14\u591a\u4f9d\u8d56\u6570\u636e\u7684\u8bcd\u6c47\u683c\u5f0f\u548c\u7ed3\u6784\u8ba1\u7b97\u76f8\u4f3c\u6027\uff0c\u5bf9\u8868\u683c\u4e2d\u6587\u672c\u8bed\u4e49\u7684\u5229\u7528\u6709\u9650\u3002", "method": "EmTT\u901a\u8fc7\u5fae\u8c03\u5bf9\u6bd4\u5b66\u4e60\u7f16\u7801\u5668\uff08\u5982BERT\uff09\u5d4c\u5165\u5217\u5e76\u5229\u7528\u805a\u7c7b\u6784\u5efa\u5c42\u6b21\u7ed3\u6784\uff1bGeTT\u901a\u8fc7\u8fed\u4ee3\u63d0\u793a\u89e3\u7801\u5668\uff08\u5982GPT-4\uff09\u751f\u6210\u8868\u683c\u5b9e\u4f53\u7c7b\u578b\u53ca\u5176\u5c42\u6b21\u7ed3\u6784\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cEmTT\u548cGeTT\u751f\u6210\u7684\u5206\u7c7b\u4e0e\u771f\u5b9e\u6570\u636e\u5177\u6709\u5f3a\u4e00\u81f4\u6027\u3002", "conclusion": "EmTT\u548cGeTT\u80fd\u6709\u6548\u89e3\u51b3\u8868\u683c\u6570\u636e\u7684\u5206\u7c7b\u63a8\u65ad\u95ee\u9898\uff0c\u4e3a\u6570\u636e\u7ba1\u7406\u3001\u63a2\u7d22\u7b49\u5e94\u7528\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2503.22168", "pdf": "https://arxiv.org/pdf/2503.22168", "abs": "https://arxiv.org/abs/2503.22168", "authors": ["Woojung Han", "Yeonkyung Lee", "Chanyoung Kim", "Kwanghyun Park", "Seong Jae Hwang"], "title": "Spatial Transport Optimization by Repositioning Attention Map for Training-Free Text-to-Image Synthesis", "categories": ["cs.CV"], "comment": "CVPR2025", "summary": "Diffusion-based text-to-image (T2I) models have recently excelled in\nhigh-quality image generation, particularly in a training-free manner, enabling\ncost-effective adaptability and generalization across diverse tasks. However,\nwhile the existing methods have been continuously focusing on several\nchallenges, such as \"missing objects\" and \"mismatched attributes,\" another\ncritical issue of \"mislocated objects\" remains where generated spatial\npositions fail to align with text prompts. Surprisingly, ensuring such\nseemingly basic functionality remains challenging in popular T2I models due to\nthe inherent difficulty of imposing explicit spatial guidance via text forms.\nTo address this, we propose STORM (Spatial Transport Optimization by\nRepositioning Attention Map), a novel training-free approach for spatially\ncoherent T2I synthesis. STORM employs Spatial Transport Optimization (STO),\nrooted in optimal transport theory, to dynamically adjust object attention maps\nfor precise spatial adherence, supported by a Spatial Transport (ST) Cost\nfunction that enhances spatial understanding. Our analysis shows that\nintegrating spatial awareness is most effective in the early denoising stages,\nwhile later phases refine details. Extensive experiments demonstrate that STORM\nsurpasses existing methods, effectively mitigating mislocated objects while\nimproving missing and mismatched attributes, setting a new benchmark for\nspatial alignment in T2I synthesis.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u8bad\u7ec3\u81ea\u7531\u7684\u65b9\u6cd5STORM\uff0c\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7269\u4f53\u4f4d\u7f6e\u4e0d\u51c6\u786e\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u7269\u4f53\u7f3a\u5931\u548c\u5c5e\u6027\u4e0d\u5339\u914d\u65f6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7269\u4f53\u4f4d\u7f6e\u4e0d\u51c6\u786e\u7684\u95ee\u9898\u4ecd\u672a\u89e3\u51b3\uff0c\u5c24\u5176\u662f\u5728\u6587\u672c\u63d0\u793a\u4e2d\u96be\u4ee5\u660e\u786e\u8868\u8fbe\u7a7a\u95f4\u6307\u5bfc\u3002", "method": "STORM\u901a\u8fc7\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7406\u8bba\u7684\u7a7a\u95f4\u4f20\u8f93\u4f18\u5316\uff08STO\uff09\u52a8\u6001\u8c03\u6574\u7269\u4f53\u6ce8\u610f\u529b\u56fe\uff0c\u5e76\u7ed3\u5408\u7a7a\u95f4\u4f20\u8f93\u6210\u672c\u51fd\u6570\u589e\u5f3a\u7a7a\u95f4\u7406\u89e3\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSTORM\u5728\u7a7a\u95f4\u5bf9\u9f50\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u6539\u5584\u4e86\u7269\u4f53\u7f3a\u5931\u548c\u5c5e\u6027\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002", "conclusion": "STORM\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u5408\u6210\u4e2d\u7684\u7a7a\u95f4\u5bf9\u9f50\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2503.21902", "pdf": "https://arxiv.org/pdf/2503.21902", "abs": "https://arxiv.org/abs/2503.21902", "authors": ["Hamed Babaei Giglou", "Jennifer D'Souza", "Oliver Karras", "S\u00f6ren Auer"], "title": "OntoAligner: A Comprehensive Modular and Robust Python Toolkit for Ontology Alignment", "categories": ["cs.AI", "cs.CL"], "comment": "18 pages, 3 figures. Accepted for the ESWC 2025 Resource Track", "summary": "Ontology Alignment (OA) is fundamental for achieving semantic\ninteroperability across diverse knowledge systems. We present OntoAligner, a\ncomprehensive, modular, and robust Python toolkit for ontology alignment,\ndesigned to address current limitations with existing tools faced by\npractitioners. Existing tools are limited in scalability, modularity, and ease\nof integration with recent AI advances. OntoAligner provides a flexible\narchitecture integrating existing lightweight OA techniques such as fuzzy\nmatching but goes beyond by supporting contemporary methods with\nretrieval-augmented generation and large language models for OA. The framework\nprioritizes extensibility, enabling researchers to integrate custom alignment\nalgorithms and datasets. This paper details the design principles,\narchitecture, and implementation of the OntoAligner, demonstrating its utility\nthrough benchmarks on standard OA tasks. Our evaluation highlights\nOntoAligner's ability to handle large-scale ontologies efficiently with few\nlines of code while delivering high alignment quality. By making OntoAligner\nopen-source, we aim to provide a resource that fosters innovation and\ncollaboration within the OA community, empowering researchers and practitioners\nwith a toolkit for reproducible OA research and real-world applications.", "AI": {"task": "\u4ecb\u7ecd\u5e76\u8bc4\u4f30OntoAligner\uff0c\u4e00\u4e2a\u7528\u4e8e\u672c\u4f53\u5bf9\u9f50\u7684Python\u5de5\u5177\u5305\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u672c\u4f53\u5bf9\u9f50\u5de5\u5177\u5728\u53ef\u6269\u5c55\u6027\u3001\u6a21\u5757\u5316\u548c\u4e0e\u6700\u65b0AI\u6280\u672f\u96c6\u6210\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "method": "OntoAligner\u91c7\u7528\u7075\u6d3b\u7684\u67b6\u6784\uff0c\u6574\u5408\u4e86\u8f7b\u91cf\u7ea7\u672c\u4f53\u5bf9\u9f50\u6280\u672f\uff08\u5982\u6a21\u7cca\u5339\u914d\uff09\u548c\u73b0\u4ee3\u65b9\u6cd5\uff08\u5982\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff09\u3002", "result": "\u8bc4\u4f30\u8868\u660eOntoAligner\u80fd\u9ad8\u6548\u5904\u7406\u5927\u89c4\u6a21\u672c\u4f53\uff0c\u4e14\u4ee3\u7801\u7b80\u6d01\uff0c\u5bf9\u9f50\u8d28\u91cf\u9ad8\u3002", "conclusion": "OntoAligner\u4f5c\u4e3a\u5f00\u6e90\u5de5\u5177\uff0c\u65e8\u5728\u4fc3\u8fdb\u672c\u4f53\u5bf9\u9f50\u9886\u57df\u7684\u521b\u65b0\u4e0e\u5408\u4f5c\uff0c\u652f\u6301\u53ef\u91cd\u590d\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2503.22171", "pdf": "https://arxiv.org/pdf/2503.22171", "abs": "https://arxiv.org/abs/2503.22171", "authors": ["Min Cao", "ZiYin Zeng", "YuXin Lu", "Mang Ye", "Dong Yi", "Jinqiao Wang"], "title": "An Empirical Study of Validating Synthetic Data for Text-Based Person Retrieval", "categories": ["cs.CV"], "comment": "20 pages,13 figures", "summary": "Data plays a pivotal role in Text-Based Person Retrieval (TBPR) research.\nMainstream research paradigm necessitates real-world person images with manual\ntextual annotations for training models, posing privacy-sensitive and\nlabor-intensive issues. Several pioneering efforts explore synthetic data for\nTBPR but still rely on real data, keeping the aforementioned issues and also\nresulting in diversity-deficient issue in synthetic datasets, thus impacting\nTBPR performance. Moreover, these works tend to explore synthetic data for TBPR\nthrough limited perspectives, leading to exploration-restricted issue. In this\npaper, we conduct an empirical study to explore the potential of synthetic data\nfor TBPR, highlighting three key aspects. (1) We propose an inter-class image\ngeneration pipeline, in which an automatic prompt construction strategy is\nintroduced to guide generative Artificial Intelligence (AI) models in\ngenerating various inter-class images without reliance on original data. (2) We\ndevelop an intra-class image augmentation pipeline, in which the generative AI\nmodels are applied to further edit the images for obtaining various intra-class\nimages. (3) Building upon the proposed pipelines and an automatic text\ngeneration pipeline, we explore the effectiveness of synthetic data in diverse\nscenarios through extensive experiments. Additionally, we experimentally\ninvestigate various noise-robust learning strategies to mitigate the inherent\nnoise in synthetic data. We will release the code, along with the synthetic\nlarge-scale dataset generated by our pipelines, which are expected to advance\npractical TBPR research.", "AI": {"task": "\u63a2\u7d22\u5408\u6210\u6570\u636e\u5728\u57fa\u4e8e\u6587\u672c\u7684\u4eba\u7269\u68c0\u7d22\uff08TBPR\uff09\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u63d0\u51fa\u751f\u6210\u548c\u589e\u5f3a\u5408\u6210\u6570\u636e\u7684\u7ba1\u9053\u3002", "motivation": "\u4e3b\u6d41\u7814\u7a76\u65b9\u6cd5\u4f9d\u8d56\u771f\u5b9e\u6570\u636e\u548c\u4eba\u5de5\u6807\u6ce8\uff0c\u5b58\u5728\u9690\u79c1\u548c\u52b3\u52a8\u5bc6\u96c6\u578b\u95ee\u9898\uff0c\u4e14\u73b0\u6709\u5408\u6210\u6570\u636e\u65b9\u6cd5\u4ecd\u4f9d\u8d56\u771f\u5b9e\u6570\u636e\uff0c\u5bfc\u81f4\u591a\u6837\u6027\u548c\u63a2\u7d22\u53d7\u9650\u3002", "method": "\u63d0\u51fa\u7c7b\u95f4\u56fe\u50cf\u751f\u6210\u7ba1\u9053\u548c\u7c7b\u5185\u56fe\u50cf\u589e\u5f3a\u7ba1\u9053\uff0c\u7ed3\u5408\u81ea\u52a8\u6587\u672c\u751f\u6210\uff0c\u63a2\u7d22\u5408\u6210\u6570\u636e\u7684\u6709\u6548\u6027\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5408\u6210\u6570\u636e\u5728\u591a\u79cd\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u51fa\u566a\u58f0\u9c81\u68d2\u5b66\u4e60\u7b56\u7565\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u548c\u751f\u6210\u7684\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\u6709\u671b\u63a8\u52a8TBPR\u7814\u7a76\u7684\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2503.21986", "pdf": "https://arxiv.org/pdf/2503.21986", "abs": "https://arxiv.org/abs/2503.21986", "authors": ["Madhusudan Basak", "Omar Sharif", "Jessica Hulsey", "Elizabeth C. Saunders", "Daisy J. Goodman", "Luke J. Archibald", "Sarah M. Preum"], "title": "Socially Constructed Treatment Plans: Analyzing Online Peer Interactions to Understand How Patients Navigate Complex Medical Conditions", "categories": ["cs.HC", "cs.CL", "cs.CY"], "comment": null, "summary": "When faced with complex and uncertain medical conditions (e.g., cancer,\nmental health conditions, recovery from substance dependency), millions of\npatients seek online peer support. In this study, we leverage content analysis\nof online discourse and ethnographic studies with clinicians and patient\nrepresentatives to characterize how treatment plans for complex conditions are\n\"socially constructed.\" Specifically, we ground online conversation on\nmedication-assisted recovery treatment to medication guidelines and\nsubsequently surface when and why people deviate from the clinical guidelines.\nWe characterize the implications and effectiveness of socially constructed\ntreatment plans through in-depth interviews with clinical experts. Finally,\ngiven the enthusiasm around AI-powered solutions for patient communication, we\ninvestigate whether and how socially constructed treatment-related knowledge is\nreflected in a state-of-the-art large language model (LLM). Leveraging a novel\nmixed-method approach, this study highlights critical research directions for\npatient-centered communication in online health communities.", "AI": {"task": "\u7814\u7a76\u5728\u7ebf\u60a3\u8005\u793e\u533a\u4e2d\u590d\u6742\u533b\u7597\u6761\u4ef6\u4e0b\u6cbb\u7597\u8ba1\u5212\u7684\u201c\u793e\u4f1a\u6784\u5efa\u201d\u73b0\u8c61\u53ca\u5176\u4e0e\u4e34\u5e8a\u6307\u5357\u7684\u504f\u5dee\u3002", "motivation": "\u63a2\u7d22\u60a3\u8005\u5728\u590d\u6742\u533b\u7597\u6761\u4ef6\u4e0b\u5982\u4f55\u901a\u8fc7\u5728\u7ebf\u793e\u533a\u5bfb\u6c42\u540c\u4f34\u652f\u6301\uff0c\u5e76\u5206\u6790\u5176\u6cbb\u7597\u8ba1\u5212\u7684\u793e\u4f1a\u6784\u5efa\u8fc7\u7a0b\u53ca\u5176\u5f71\u54cd\u3002", "method": "\u7ed3\u5408\u5728\u7ebf\u8bdd\u8bed\u5185\u5bb9\u5206\u6790\u3001\u6c11\u65cf\u5fd7\u7814\u7a76\u3001\u6df1\u5ea6\u8bbf\u8c08\u4ee5\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8bc4\u4f30\u3002", "result": "\u63ed\u793a\u4e86\u60a3\u8005\u5728\u7ebf\u793e\u533a\u4e2d\u6cbb\u7597\u8ba1\u5212\u7684\u793e\u4f1a\u6784\u5efa\u73b0\u8c61\u53ca\u5176\u4e0e\u4e34\u5e8a\u6307\u5357\u7684\u504f\u5dee\uff0c\u5e76\u8bc4\u4f30\u4e86LLM\u5728\u6b64\u7c7b\u77e5\u8bc6\u4e2d\u7684\u53cd\u6620\u3002", "conclusion": "\u4e3a\u5728\u7ebf\u5065\u5eb7\u793e\u533a\u4e2d\u4ee5\u60a3\u8005\u4e3a\u4e2d\u5fc3\u7684\u6c9f\u901a\u63d0\u4f9b\u4e86\u91cd\u8981\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2503.22172", "pdf": "https://arxiv.org/pdf/2503.22172", "abs": "https://arxiv.org/abs/2503.22172", "authors": ["Minho Park", "Sunghyun Park", "Jungsoo Lee", "Hyojin Park", "Kyuwoong Hwang", "Fatih Porikli", "Jaegul Choo", "Sungha Choi"], "title": "Concept-Aware LoRA for Domain-Aligned Segmentation Dataset Generation", "categories": ["cs.CV"], "comment": null, "summary": "This paper addresses the challenge of data scarcity in semantic segmentation\nby generating datasets through text-to-image (T2I) generation models, reducing\nimage acquisition and labeling costs. Segmentation dataset generation faces two\nkey challenges: 1) aligning generated samples with the target domain and 2)\nproducing informative samples beyond the training data. Fine-tuning T2I models\ncan help generate samples aligned with the target domain. However, it often\noverfits and memorizes training data, limiting their ability to generate\ndiverse and well-aligned samples. To overcome these issues, we propose\nConcept-Aware LoRA (CA-LoRA), a novel fine-tuning approach that selectively\nidentifies and updates only the weights associated with necessary concepts\n(e.g., style or viewpoint) for domain alignment while preserving the pretrained\nknowledge of the T2I model to produce informative samples. We demonstrate its\neffectiveness in generating datasets for urban-scene segmentation,\noutperforming baseline and state-of-the-art methods in in-domain (few-shot and\nfully-supervised) settings, as well as in domain generalization tasks,\nespecially under challenging conditions such as adverse weather and varying\nillumination, further highlighting its superiority.", "AI": {"task": "\u901a\u8fc7\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u751f\u6210\u6a21\u578b\u751f\u6210\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u8bed\u4e49\u5206\u5272\u4e2d\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\u3002", "motivation": "\u51cf\u5c11\u56fe\u50cf\u83b7\u53d6\u548c\u6807\u6ce8\u6210\u672c\uff0c\u5e76\u89e3\u51b3\u751f\u6210\u6837\u672c\u4e0e\u76ee\u6807\u57df\u5bf9\u9f50\u53ca\u751f\u6210\u8d85\u51fa\u8bad\u7ec3\u6570\u636e\u7684\u4fe1\u606f\u6837\u672c\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faConcept-Aware LoRA\uff08CA-LoRA\uff09\uff0c\u4e00\u79cd\u9009\u62e9\u6027\u66f4\u65b0\u4e0e\u5fc5\u8981\u6982\u5ff5\u76f8\u5173\u6743\u91cd\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u4ee5\u4fdd\u6301T2I\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u77e5\u8bc6\u3002", "result": "\u5728\u57ce\u5e02\u573a\u666f\u5206\u5272\u6570\u636e\u96c6\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u548c\u6700\u65b0\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u6076\u52a3\u5929\u6c14\u548c\u5149\u7167\u53d8\u5316\u7b49\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u3002", "conclusion": "CA-LoRA\u80fd\u6709\u6548\u751f\u6210\u591a\u6837\u4e14\u5bf9\u9f50\u76ee\u6807\u57df\u7684\u6837\u672c\uff0c\u4e3a\u8bed\u4e49\u5206\u5272\u63d0\u4f9b\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3002"}}
{"id": "2503.22038", "pdf": "https://arxiv.org/pdf/2503.22038", "abs": "https://arxiv.org/abs/2503.22038", "authors": ["Ngoc Tuong Vy Nguyen", "Felix D Childress", "Yunting Yin"], "title": "Debate-Driven Multi-Agent LLMs for Phishing Email Detection", "categories": ["cs.MA", "cs.CL"], "comment": "Accepted to the 13th International Symposium on Digital Forensics and\n  Security (ISDFS 2025)", "summary": "Phishing attacks remain a critical cybersecurity threat. Attackers constantly\nrefine their methods, making phishing emails harder to detect. Traditional\ndetection methods, including rule-based systems and supervised machine learning\nmodels, either rely on predefined patterns like blacklists, which can be\nbypassed with slight modifications, or require large datasets for training and\nstill can generate false positives and false negatives. In this work, we\npropose a multi-agent large language model (LLM) prompting technique that\nsimulates debates among agents to detect whether the content presented on an\nemail is phishing. Our approach uses two LLM agents to present arguments for or\nagainst the classification task, with a judge agent adjudicating the final\nverdict based on the quality of reasoning provided. This debate mechanism\nenables the models to critically analyze contextual cue and deceptive patterns\nin text, which leads to improved classification accuracy. The proposed\nframework is evaluated on multiple phishing email datasets and demonstrate that\nmixed-agent configurations consistently outperform homogeneous configurations.\nResults also show that the debate structure itself is sufficient to yield\naccurate decisions without extra prompting strategies.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8fa9\u8bba\u6280\u672f\uff0c\u7528\u4e8e\u68c0\u6d4b\u7535\u5b50\u90ae\u4ef6\u662f\u5426\u4e3a\u9493\u9c7c\u90ae\u4ef6\u3002", "motivation": "\u4f20\u7edf\u68c0\u6d4b\u65b9\u6cd5\uff08\u5982\u57fa\u4e8e\u89c4\u5219\u7684\u7cfb\u7edf\u6216\u76d1\u7763\u5b66\u4e60\u6a21\u578b\uff09\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5982\u4f9d\u8d56\u9884\u5b9a\u4e49\u6a21\u5f0f\u6216\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u4e14\u6613\u4ea7\u751f\u8bef\u5224\u3002", "method": "\u4f7f\u7528\u4e24\u4e2aLLM\u667a\u80fd\u4f53\u5206\u522b\u63d0\u51fa\u652f\u6301\u6216\u53cd\u5bf9\u5206\u7c7b\u4efb\u52a1\u7684\u8bba\u70b9\uff0c\u5e76\u7531\u4e00\u4e2a\u6cd5\u5b98\u667a\u80fd\u4f53\u6839\u636e\u63a8\u7406\u8d28\u91cf\u505a\u51fa\u6700\u7ec8\u88c1\u51b3\u3002", "result": "\u5728\u591a\u7ec4\u9493\u9c7c\u90ae\u4ef6\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u6df7\u5408\u667a\u80fd\u4f53\u914d\u7f6e\u6027\u80fd\u4f18\u4e8e\u540c\u8d28\u914d\u7f6e\uff0c\u4e14\u8fa9\u8bba\u7ed3\u6784\u672c\u8eab\u8db3\u4ee5\u5b9e\u73b0\u9ad8\u51c6\u786e\u7387\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u673a\u5236\u80fd\u6709\u6548\u63d0\u5347\u9493\u9c7c\u90ae\u4ef6\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u65e0\u9700\u989d\u5916\u63d0\u793a\u7b56\u7565\u3002"}}
{"id": "2503.22174", "pdf": "https://arxiv.org/pdf/2503.22174", "abs": "https://arxiv.org/abs/2503.22174", "authors": ["Jialun Pei", "Zhangjun Zhou", "Diandian Guo", "Zhixi Li", "Jing Qin", "Bo Du", "Pheng-Ann Heng"], "title": "Synergistic Bleeding Region and Point Detection in Surgical Videos", "categories": ["cs.CV"], "comment": null, "summary": "Intraoperative bleeding in laparoscopic surgery causes rapid obscuration of\nthe operative field to hinder the surgical process. Intelligent detection of\nbleeding regions can quantify the blood loss to assist decision-making, while\nlocating the bleeding point helps surgeons quickly identify the source of\nbleeding and achieve hemostasis in time. In this study, we first construct a\nreal-world surgical bleeding detection dataset, named SurgBlood, comprising\n5,330 frames from 95 surgical video clips with bleeding region and point\nannotations. Accordingly, we develop a dual-task synergistic online detector\ncalled BlooDet, designed to perform simultaneous detection of bleeding regions\nand points in surgical videos. Our framework embraces a dual-branch\nbidirectional guidance design based on Segment Anything Model 2 (SAM 2). The\nmask branch detects bleeding regions through adaptive edge and point prompt\nembeddings, while the point branch leverages mask memory to induce bleeding\npoint memory modeling and captures the direction of bleed point movement\nthrough inter-frame optical flow. By interactive guidance and prompts, the two\nbranches explore potential spatial-temporal relationships while leveraging\nmemory modeling from previous frames to infer the current bleeding condition.\nExtensive experiments demonstrate that our approach outperforms other\ncounterparts on SurgBlood in both bleeding region and point detection tasks,\ne.g., achieving 64.88% IoU for bleeding region detection and 83.69% PCK-10% for\nbleeding point detection.", "AI": {"task": "\u5f00\u53d1\u4e00\u79cd\u540d\u4e3aBlooDet\u7684\u53cc\u4efb\u52a1\u534f\u540c\u5728\u7ebf\u68c0\u6d4b\u5668\uff0c\u7528\u4e8e\u5728\u624b\u672f\u89c6\u9891\u4e2d\u540c\u65f6\u68c0\u6d4b\u51fa\u8840\u533a\u57df\u548c\u51fa\u8840\u70b9\u3002", "motivation": "\u8179\u8154\u955c\u624b\u672f\u4e2d\u7684\u672f\u4e2d\u51fa\u8840\u4f1a\u8fc5\u901f\u6a21\u7cca\u624b\u672f\u89c6\u91ce\uff0c\u963b\u788d\u624b\u672f\u8fdb\u7a0b\uff1b\u667a\u80fd\u68c0\u6d4b\u51fa\u8840\u533a\u57df\u53ef\u4ee5\u91cf\u5316\u5931\u8840\u91cf\u4ee5\u8f85\u52a9\u51b3\u7b56\uff0c\u800c\u5b9a\u4f4d\u51fa\u8840\u70b9\u6709\u52a9\u4e8e\u5916\u79d1\u533b\u751f\u5feb\u901f\u8bc6\u522b\u51fa\u8840\u6e90\u5e76\u53ca\u65f6\u6b62\u8840\u3002", "method": "\u57fa\u4e8eSegment Anything Model 2\uff08SAM 2\uff09\u7684\u53cc\u5206\u652f\u53cc\u5411\u5f15\u5bfc\u8bbe\u8ba1\uff0c\u5305\u62ec\u4e00\u4e2a\u7528\u4e8e\u68c0\u6d4b\u51fa\u8840\u533a\u57df\u7684\u63a9\u7801\u5206\u652f\u548c\u4e00\u4e2a\u7528\u4e8e\u5efa\u6a21\u51fa\u8840\u70b9\u8bb0\u5fc6\u7684\u70b9\u5206\u652f\uff0c\u901a\u8fc7\u4ea4\u4e92\u5f15\u5bfc\u548c\u63d0\u793a\u63a2\u7d22\u65f6\u7a7a\u5173\u7cfb\u3002", "result": "\u5728SurgBlood\u6570\u636e\u96c6\u4e0a\uff0cBlooDet\u5728\u51fa\u8840\u533a\u57df\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8fbe\u523064.88%\u7684IoU\uff0c\u5728\u51fa\u8840\u70b9\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8fbe\u523083.69%\u7684PCK-10%\u3002", "conclusion": "BlooDet\u901a\u8fc7\u53cc\u5206\u652f\u534f\u540c\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51fa\u8840\u533a\u57df\u548c\u70b9\u7684\u68c0\u6d4b\u6027\u80fd\uff0c\u4e3a\u672f\u4e2d\u51b3\u7b56\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2503.22122", "pdf": "https://arxiv.org/pdf/2503.22122", "abs": "https://arxiv.org/abs/2503.22122", "authors": ["Puzhen Yuan", "Angyuan Ma", "Yunchao Yao", "Huaxiu Yao", "Masayoshi Tomizuka", "Mingyu Ding"], "title": "REMAC: Self-Reflective and Self-Evolving Multi-Agent Collaboration for Long-Horizon Robot Manipulation", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) have demonstrated remarkable capabilities in\nrobotic planning, particularly for long-horizon tasks that require a holistic\nunderstanding of the environment for task decomposition. Existing methods\ntypically rely on prior environmental knowledge or carefully designed\ntask-specific prompts, making them struggle with dynamic scene changes or\nunexpected task conditions, e.g., a robot attempting to put a carrot in the\nmicrowave but finds the door was closed. Such challenges underscore two\ncritical issues: adaptability and efficiency. To address them, in this work, we\npropose an adaptive multi-agent planning framework, termed REMAC, that enables\nefficient, scene-agnostic multi-robot long-horizon task planning and execution\nthrough continuous reflection and self-evolution. REMAC incorporates two key\nmodules: a self-reflection module performing pre-condition and post-condition\nchecks in the loop to evaluate progress and refine plans, and a self-evolvement\nmodule dynamically adapting plans based on scene-specific reasoning. It offers\nseveral appealing benefits: 1) Robots can initially explore and reason about\nthe environment without complex prompt design. 2) Robots can keep reflecting on\npotential planning errors and adapting the plan based on task-specific\ninsights. 3) After iterations, a robot can call another one to coordinate tasks\nin parallel, maximizing the task execution efficiency. To validate REMAC's\neffectiveness, we build a multi-agent environment for long-horizon robot\nmanipulation and navigation based on RoboCasa, featuring 4 task categories with\n27 task styles and 50+ different objects. Based on it, we further benchmark\nstate-of-the-art reasoning models, including DeepSeek-R1, o3-mini, QwQ, and\nGrok3, demonstrating REMAC's superiority by boosting average success rates by\n40% and execution efficiency by 52.7% over the single robot baseline.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u81ea\u9002\u5e94\u591a\u667a\u80fd\u4f53\u89c4\u5212\u6846\u67b6\uff08REMAC\uff09\uff0c\u7528\u4e8e\u9ad8\u6548\u3001\u573a\u666f\u65e0\u5173\u7684\u591a\u673a\u5668\u4eba\u957f\u65f6\u7a0b\u4efb\u52a1\u89c4\u5212\u4e0e\u6267\u884c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5148\u9a8c\u73af\u5883\u77e5\u8bc6\u6216\u7279\u5b9a\u4efb\u52a1\u63d0\u793a\uff0c\u96be\u4ee5\u5e94\u5bf9\u52a8\u6001\u573a\u666f\u53d8\u5316\u6216\u610f\u5916\u4efb\u52a1\u6761\u4ef6\uff0c\u7f3a\u4e4f\u9002\u5e94\u6027\u548c\u6548\u7387\u3002", "method": "REMAC\u5305\u542b\u81ea\u53cd\u601d\u6a21\u5757\uff08\u5faa\u73af\u8fdb\u884c\u524d\u7f6e\u548c\u540e\u7f6e\u6761\u4ef6\u68c0\u67e5\uff09\u548c\u81ea\u8fdb\u5316\u6a21\u5757\uff08\u52a8\u6001\u8c03\u6574\u8ba1\u5212\uff09\uff0c\u652f\u6301\u591a\u673a\u5668\u4eba\u5e76\u884c\u534f\u4f5c\u3002", "result": "\u5728\u57fa\u4e8eRoboCasa\u7684\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\uff0cREMAC\u5c06\u5e73\u5747\u6210\u529f\u7387\u63d0\u534740%\uff0c\u6267\u884c\u6548\u7387\u63d0\u9ad852.7%\u3002", "conclusion": "REMAC\u901a\u8fc7\u6301\u7eed\u53cd\u601d\u548c\u81ea\u8fdb\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u673a\u5668\u4eba\u957f\u65f6\u7a0b\u4efb\u52a1\u7684\u9002\u5e94\u6027\u548c\u6267\u884c\u6548\u7387\u3002"}}
{"id": "2503.22175", "pdf": "https://arxiv.org/pdf/2503.22175", "abs": "https://arxiv.org/abs/2503.22175", "authors": ["Ruiqi Liu", "Boyu Diao", "Libo Huang", "Hangda Liu", "Chuanguang Yang", "Zhulin An", "Yongjun Xu"], "title": "Efficient Continual Learning through Frequency Decomposition and Integration", "categories": ["cs.CV"], "comment": null, "summary": "Continual learning (CL) aims to learn new tasks while retaining past\nknowledge, addressing the challenge of forgetting during task adaptation.\nRehearsal-based methods, which replay previous samples, effectively mitigate\nforgetting. However, research on enhancing the efficiency of these methods,\nespecially in resource-constrained environments, remains limited, hindering\ntheir application in real-world systems with dynamic data streams. The human\nperceptual system processes visual scenes through complementary frequency\nchannels: low-frequency signals capture holistic cues, while high-frequency\ncomponents convey structural details vital for fine-grained discrimination.\nInspired by this, we propose the Frequency Decomposition and Integration\nNetwork (FDINet), a novel framework that decomposes and integrates information\nacross frequencies. FDINet designs two lightweight networks to independently\nprocess low- and high-frequency components of images. When integrated with\nrehearsal-based methods, this frequency-aware design effectively enhances\ncross-task generalization through low-frequency information, preserves\nclass-specific details using high-frequency information, and facilitates\nefficient training due to its lightweight architecture. Experiments demonstrate\nthat FDINet reduces backbone parameters by 78%, improves accuracy by up to\n7.49% over state-of-the-art (SOTA) methods, and decreases peak memory usage by\nup to 80%. Additionally, on edge devices, FDINet accelerates training by up to\n5$\\times$.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aFDINet\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u9891\u7387\u5206\u89e3\u548c\u6574\u5408\u589e\u5f3a\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u4efb\u52a1\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u6301\u7eed\u5b66\u4e60\u4e2d\u4efb\u52a1\u9002\u5e94\u65f6\u7684\u9057\u5fd8\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u57fa\u4e8e\u56de\u653e\u65b9\u6cd5\u7684\u6548\u7387\u3002", "method": "\u8bbe\u8ba1FDINet\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u4e2a\u8f7b\u91cf\u7ea7\u7f51\u7edc\u5206\u522b\u5904\u7406\u56fe\u50cf\u7684\u4f4e\u9891\u548c\u9ad8\u9891\u4fe1\u606f\uff0c\u5e76\u7ed3\u5408\u56de\u653e\u65b9\u6cd5\u3002", "result": "FDINet\u51cf\u5c1178%\u7684\u4e3b\u5e72\u53c2\u6570\uff0c\u51c6\u786e\u7387\u63d0\u53477.49%\uff0c\u5cf0\u503c\u5185\u5b58\u4f7f\u7528\u964d\u4f4e80%\uff0c\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8bad\u7ec3\u901f\u5ea6\u63d0\u53475\u500d\u3002", "conclusion": "FDINet\u901a\u8fc7\u9891\u7387\u611f\u77e5\u8bbe\u8ba1\u6709\u6548\u63d0\u5347\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\uff0c\u4fdd\u7559\u7ec6\u8282\u5e76\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u3002"}}
{"id": "2503.22135", "pdf": "https://arxiv.org/pdf/2503.22135", "abs": "https://arxiv.org/abs/2503.22135", "authors": ["Zhipeng Lu"], "title": "Convolutional optimization with convex kernel and power lift", "categories": ["math.OC", "cs.CE", "cs.CL"], "comment": null, "summary": "We focus on establishing the foundational paradigm of a novel optimization\ntheory based on convolution with convex kernels. Our goal is to devise a\nmorally deterministic model of locating the global optima of an arbitrary\nfunction, which is distinguished from most commonly used statistical models.\nLimited preliminary numerical results are provided to test the efficiency of\nsome specific algorithms derived from our paradigm, which we hope to stimulate\nfurther practical interest.", "AI": {"task": "\u5efa\u7acb\u4e00\u79cd\u57fa\u4e8e\u51f8\u6838\u5377\u79ef\u7684\u65b0\u578b\u4f18\u5316\u7406\u8bba\u7684\u57fa\u7840\u8303\u5f0f\u3002", "motivation": "\u76ee\u6807\u662f\u8bbe\u8ba1\u4e00\u79cd\u9053\u5fb7\u786e\u5b9a\u6027\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u5b9a\u4f4d\u4efb\u610f\u51fd\u6570\u7684\u5168\u5c40\u6700\u4f18\u89e3\uff0c\u533a\u522b\u4e8e\u5e38\u7528\u7684\u7edf\u8ba1\u6a21\u578b\u3002", "method": "\u901a\u8fc7\u51f8\u6838\u5377\u79ef\u7684\u65b9\u6cd5\u6784\u5efa\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u6d4b\u8bd5\u7279\u5b9a\u7b97\u6cd5\u7684\u6548\u7387\u3002", "result": "\u63d0\u4f9b\u4e86\u6709\u9650\u7684\u521d\u6b65\u6570\u503c\u7ed3\u679c\uff0c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6548\u7387\u3002", "conclusion": "\u5e0c\u671b\u6fc0\u53d1\u8fdb\u4e00\u6b65\u7684\u5b9e\u9645\u5174\u8da3\u548c\u7814\u7a76\u3002"}}
{"id": "2503.22179", "pdf": "https://arxiv.org/pdf/2503.22179", "abs": "https://arxiv.org/abs/2503.22179", "authors": ["Dailan He", "Xiahong Wang", "Shulun Wang", "Guanglu Song", "Bingqi Ma", "Hao Shao", "Yu Liu", "Hongsheng Li"], "title": "High-Fidelity Diffusion Face Swapping with ID-Constrained Facial Conditioning", "categories": ["cs.CV"], "comment": null, "summary": "Face swapping aims to seamlessly transfer a source facial identity onto a\ntarget while preserving target attributes such as pose and expression.\nDiffusion models, known for their superior generative capabilities, have\nrecently shown promise in advancing face-swapping quality. This paper addresses\ntwo key challenges in diffusion-based face swapping: the prioritized\npreservation of identity over target attributes and the inherent conflict\nbetween identity and attribute conditioning. To tackle these issues, we\nintroduce an identity-constrained attribute-tuning framework for face swapping\nthat first ensures identity preservation and then fine-tunes for attribute\nalignment, achieved through a decoupled condition injection. We further enhance\nfidelity by incorporating identity and adversarial losses in a post-training\nrefinement stage. Our proposed identity-constrained diffusion-based\nface-swapping model outperforms existing methods in both qualitative and\nquantitative evaluations, demonstrating superior identity similarity and\nattribute consistency, achieving a new state-of-the-art performance in\nhigh-fidelity face swapping.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u8eab\u4efd\u7ea6\u675f\u5c5e\u6027\u8c03\u6574\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u4eba\u8138\u4ea4\u6362\u4e2d\u8eab\u4efd\u4e0e\u5c5e\u6027\u6761\u4ef6\u51b2\u7a81\u7684\u95ee\u9898\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u4eba\u8138\u4ea4\u6362\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b58\u5728\u8eab\u4efd\u4f18\u5148\u4fdd\u5b58\u4e0e\u5c5e\u6027\u6761\u4ef6\u51b2\u7a81\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u8eab\u4efd\u7ea6\u675f\u5c5e\u6027\u8c03\u6574\u6846\u67b6\uff0c\u5206\u6b65\u5b9e\u73b0\u8eab\u4efd\u4fdd\u5b58\u548c\u5c5e\u6027\u5bf9\u9f50\uff0c\u5e76\u901a\u8fc7\u89e3\u8026\u6761\u4ef6\u6ce8\u5165\u548c\u540e\u8bad\u7ec3\u7ec6\u5316\u63d0\u5347\u4fdd\u771f\u5ea6\u3002", "result": "\u63d0\u51fa\u7684\u6a21\u578b\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u4eba\u8138\u4ea4\u6362\u7684\u6700\u65b0\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u8eab\u4efd\u4e0e\u5c5e\u6027\u7684\u51b2\u7a81\u95ee\u9898\uff0c\u4e3a\u4eba\u8138\u4ea4\u6362\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2503.22215", "pdf": "https://arxiv.org/pdf/2503.22215", "abs": "https://arxiv.org/abs/2503.22215", "authors": ["Zhihan Zhou", "Feng Hong", "Jiaan Luo", "Jiangchao Yao", "Dongsheng Li", "Bo Han", "Ya Zhang", "Yanfeng Wang"], "title": "Learning to Instruct for Visual Instruction Tuning", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "16 pages, 10 figures", "summary": "We propose LIT, an advancement of visual instruction tuning (VIT). While VIT\nequips Multimodal LLMs (MLLMs) with promising multimodal capabilities, the\ncurrent design choices for VIT often result in overfitting and shortcut\nlearning, potentially degrading performance. This gap arises from an\noveremphasis on instruction-following abilities, while neglecting the proactive\nunderstanding of visual information. Inspired by this, LIT adopts a simple yet\neffective approach by incorporating the loss function into both the instruction\nand response sequences. It seamlessly expands the training data, and\nregularizes the MLLMs from overly relying on language priors. Based on this\nmerit, LIT achieves a significant relative improvement of up to 9% on\ncomprehensive multimodal benchmarks, requiring no additional training data and\nincurring negligible computational overhead. Surprisingly, LIT attains\nexceptional fundamental visual capabilities, yielding up to an 18% improvement\nin captioning performance, while simultaneously alleviating hallucination in\nMLLMs.", "AI": {"task": "\u63d0\u51faLIT\u65b9\u6cd5\uff0c\u6539\u8fdb\u89c6\u89c9\u6307\u4ee4\u8c03\u6574\uff08VIT\uff09\u4ee5\u89e3\u51b3\u5176\u8fc7\u62df\u5408\u548c\u6377\u5f84\u5b66\u4e60\u95ee\u9898\u3002", "motivation": "\u5f53\u524dVIT\u8bbe\u8ba1\u8fc7\u4e8e\u5f3a\u8c03\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\uff0c\u5ffd\u89c6\u4e86\u89c6\u89c9\u4fe1\u606f\u7684\u4e3b\u52a8\u7406\u89e3\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "LIT\u901a\u8fc7\u5c06\u635f\u5931\u51fd\u6570\u540c\u65f6\u5e94\u7528\u4e8e\u6307\u4ee4\u548c\u54cd\u5e94\u5e8f\u5217\uff0c\u6269\u5c55\u8bad\u7ec3\u6570\u636e\u5e76\u51cf\u5c11\u5bf9\u8bed\u8a00\u5148\u9a8c\u7684\u4f9d\u8d56\u3002", "result": "LIT\u5728\u591a\u9879\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u76f8\u5bf9\u63d0\u5347\u8fbe9%\uff0c\u65e0\u9700\u989d\u5916\u6570\u636e\u4e14\u8ba1\u7b97\u5f00\u9500\u53ef\u5ffd\u7565\uff1b\u5728\u89c6\u89c9\u57fa\u7840\u80fd\u529b\u4e0a\u63d0\u534718%\uff0c\u540c\u65f6\u51cf\u5c11\u5e7b\u89c9\u73b0\u8c61\u3002", "conclusion": "LIT\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2503.22180", "pdf": "https://arxiv.org/pdf/2503.22180", "abs": "https://arxiv.org/abs/2503.22180", "authors": ["Juwei Guan", "Xiaolin Fang", "Donghyun Kim", "Haotian Gong", "Tongxin Zhu", "Zhen Ling", "Ming Yang"], "title": "Knowledge Rectification for Camouflaged Object Detection: Unlocking Insights from Low-Quality Data", "categories": ["cs.CV"], "comment": null, "summary": "Low-quality data often suffer from insufficient image details, introducing an\nextra implicit aspect of camouflage that complicates camouflaged object\ndetection (COD). Existing COD methods focus primarily on high-quality data,\noverlooking the challenges posed by low-quality data, which leads to\nsignificant performance degradation. Therefore, we propose KRNet, the first\nframework explicitly designed for COD on low-quality data. KRNet presents a\nLeader-Follower framework where the Leader extracts dual gold-standard\ndistributions: conditional and hybrid, from high-quality data to drive the\nFollower in rectifying knowledge learned from low-quality data. The framework\nfurther benefits from a cross-consistency strategy that improves the\nrectification of these distributions and a time-dependent conditional encoder\nthat enriches the distribution diversity. Extensive experiments on benchmark\ndatasets demonstrate that KRNet outperforms state-of-the-art COD methods and\nsuper-resolution-assisted COD approaches, proving its effectiveness in tackling\nthe challenges of low-quality data in COD.", "AI": {"task": "\u63d0\u51faKRNet\u6846\u67b6\uff0c\u4e13\u95e8\u7528\u4e8e\u4f4e\u8d28\u91cf\u6570\u636e\u4e2d\u7684\u4f2a\u88c5\u76ee\u6807\u68c0\u6d4b\uff08COD\uff09\u3002", "motivation": "\u4f4e\u8d28\u91cf\u6570\u636e\u56e0\u7f3a\u4e4f\u56fe\u50cf\u7ec6\u8282\u800c\u589e\u52a0\u4f2a\u88c5\u76ee\u6807\u68c0\u6d4b\u7684\u590d\u6742\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\uff0c\u5bfc\u81f4\u5728\u4f4e\u8d28\u91cf\u6570\u636e\u4e0a\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "method": "KRNet\u91c7\u7528Leader-Follower\u6846\u67b6\uff0cLeader\u4ece\u9ad8\u8d28\u91cf\u6570\u636e\u4e2d\u63d0\u53d6\u53cc\u91cd\u9ec4\u91d1\u6807\u51c6\u5206\u5e03\uff08\u6761\u4ef6\u548c\u6df7\u5408\uff09\uff0c\u9a71\u52a8Follower\u4fee\u6b63\u4ece\u4f4e\u8d28\u91cf\u6570\u636e\u4e2d\u5b66\u5230\u7684\u77e5\u8bc6\uff0c\u5e76\u7ed3\u5408\u4ea4\u53c9\u4e00\u81f4\u6027\u7b56\u7565\u548c\u65f6\u95f4\u4f9d\u8d56\u6761\u4ef6\u7f16\u7801\u5668\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cKRNet\u4f18\u4e8e\u73b0\u6709COD\u65b9\u6cd5\u548c\u8d85\u5206\u8fa8\u7387\u8f85\u52a9COD\u65b9\u6cd5\u3002", "conclusion": "KRNet\u80fd\u6709\u6548\u89e3\u51b3\u4f4e\u8d28\u91cf\u6570\u636e\u5728COD\u4e2d\u7684\u6311\u6218\u3002"}}
{"id": "2503.22379", "pdf": "https://arxiv.org/pdf/2503.22379", "abs": "https://arxiv.org/abs/2503.22379", "authors": ["Stephen Meisenbacher", "Chaeeun Joy Lee", "Florian Matthes"], "title": "Spend Your Budget Wisely: Towards an Intelligent Distribution of the Privacy Budget in Differentially Private Text Rewriting", "categories": ["cs.CR", "cs.CL"], "comment": "14 pages, 1 figure, 6 tables. Accepted to CODASPY 2025", "summary": "The task of $\\textit{Differentially Private Text Rewriting}$ is a class of\ntext privatization techniques in which (sensitive) input textual documents are\n$\\textit{rewritten}$ under Differential Privacy (DP) guarantees. The motivation\nbehind such methods is to hide both explicit and implicit identifiers that\ncould be contained in text, while still retaining the semantic meaning of the\noriginal text, thus preserving utility. Recent years have seen an uptick in\nresearch output in this field, offering a diverse array of word-, sentence-,\nand document-level DP rewriting methods. Common to these methods is the\nselection of a privacy budget (i.e., the $\\varepsilon$ parameter), which\ngoverns the degree to which a text is privatized. One major limitation of\nprevious works, stemming directly from the unique structure of language itself,\nis the lack of consideration of $\\textit{where}$ the privacy budget should be\nallocated, as not all aspects of language, and therefore text, are equally\nsensitive or personal. In this work, we are the first to address this\nshortcoming, asking the question of how a given privacy budget can be\nintelligently and sensibly distributed amongst a target document. We construct\nand evaluate a toolkit of linguistics- and NLP-based methods used to allocate a\nprivacy budget to constituent tokens in a text document. In a series of privacy\nand utility experiments, we empirically demonstrate that given the same privacy\nbudget, intelligent distribution leads to higher privacy levels and more\npositive trade-offs than a naive distribution of $\\varepsilon$. Our work\nhighlights the intricacies of text privatization with DP, and furthermore, it\ncalls for further work on finding more efficient ways to maximize the\nprivatization benefits offered by DP in text rewriting.", "AI": {"task": "\u7814\u7a76\u5982\u4f55\u5728\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u4fdd\u8bc1\u4e0b\u5bf9\u6587\u672c\u8fdb\u884c\u91cd\u5199\uff0c\u4ee5\u9690\u85cf\u654f\u611f\u4fe1\u606f\u5e76\u4fdd\u7559\u8bed\u4e49\u3002", "motivation": "\u9690\u85cf\u6587\u672c\u4e2d\u7684\u663e\u5f0f\u548c\u9690\u5f0f\u6807\u8bc6\u7b26\uff0c\u540c\u65f6\u4fdd\u7559\u539f\u59cb\u6587\u672c\u7684\u8bed\u4e49\u548c\u5b9e\u7528\u6027\u3002", "method": "\u6784\u5efa\u5e76\u8bc4\u4f30\u57fa\u4e8e\u8bed\u8a00\u5b66\u548cNLP\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u667a\u80fd\u5206\u914d\u9690\u79c1\u9884\u7b97\u5230\u6587\u672c\u4e2d\u7684\u5404\u4e2a\u90e8\u5206\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u667a\u80fd\u5206\u914d\u9690\u79c1\u9884\u7b97\u6bd4\u7b80\u5355\u5206\u914d\u80fd\u63d0\u4f9b\u66f4\u9ad8\u7684\u9690\u79c1\u6c34\u5e73\u548c\u66f4\u597d\u7684\u6548\u7528\u6743\u8861\u3002", "conclusion": "\u5f3a\u8c03\u4e86\u6587\u672c\u9690\u79c1\u5316\u7684\u590d\u6742\u6027\uff0c\u5e76\u547c\u5401\u8fdb\u4e00\u6b65\u7814\u7a76\u5982\u4f55\u66f4\u9ad8\u6548\u5730\u5229\u7528\u5dee\u5206\u9690\u79c1\u8fdb\u884c\u6587\u672c\u91cd\u5199\u3002"}}
{"id": "2503.22193", "pdf": "https://arxiv.org/pdf/2503.22193", "abs": "https://arxiv.org/abs/2503.22193", "authors": ["Yang Liu", "Feixiang Liu", "Jiale Du", "Xinbo Gao", "Jungong Han"], "title": "Unbiased Max-Min Embedding Classification for Transductive Few-Shot Learning: Clustering and Classification Are All You Need", "categories": ["cs.CV"], "comment": null, "summary": "Convolutional neural networks and supervised learning have achieved\nremarkable success in various fields but are limited by the need for large\nannotated datasets. Few-shot learning (FSL) addresses this limitation by\nenabling models to generalize from only a few labeled examples. Transductive\nfew-shot learning (TFSL) enhances FSL by leveraging both labeled and unlabeled\ndata, though it faces challenges like the hubness problem. To overcome these\nlimitations, we propose the Unbiased Max-Min Embedding Classification (UMMEC)\nMethod, which addresses the key challenges in few-shot learning through three\ninnovative contributions. First, we introduce a decentralized covariance matrix\nto mitigate the hubness problem, ensuring a more uniform distribution of\nembeddings. Second, our method combines local alignment and global uniformity\nthrough adaptive weighting and nonlinear transformation, balancing intra-class\nclustering with inter-class separation. Third, we employ a Variational Sinkhorn\nFew-Shot Classifier to optimize the distances between samples and class\nprototypes, enhancing classification accuracy and robustness. These combined\ninnovations allow the UMMEC method to achieve superior performance with minimal\nlabeled data. Our UMMEC method significantly improves classification\nperformance with minimal labeled data, advancing the state-of-the-art in TFSL.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aUMMEC\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u5c11\u6837\u672c\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002", "motivation": "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u76d1\u7763\u5b66\u4e60\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u800c\u5c11\u6837\u672c\u5b66\u4e60\uff08FSL\uff09\u548c\u8f6c\u5bfc\u5c11\u6837\u672c\u5b66\u4e60\uff08TFSL\uff09\u867d\u7136\u80fd\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u4ecd\u9762\u4e34\u5982\u4e2d\u5fc3\u5316\u95ee\u9898\u7b49\u6311\u6218\u3002", "method": "UMMEC\u65b9\u6cd5\u901a\u8fc7\u4e09\u4e2a\u521b\u65b0\u8d21\u732e\uff1a\u5206\u6563\u534f\u65b9\u5dee\u77e9\u9635\u3001\u5c40\u90e8\u5bf9\u9f50\u4e0e\u5168\u5c40\u7edf\u4e00\u7684\u7ed3\u5408\uff0c\u4ee5\u53ca\u53d8\u5206Sinkhorn\u5c11\u6837\u672c\u5206\u7c7b\u5668\u3002", "result": "UMMEC\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u7c7b\u6027\u80fd\uff0c\u4ec5\u9700\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u5373\u53ef\u8fbe\u5230\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "UMMEC\u65b9\u6cd5\u5728\u8f6c\u5bfc\u5c11\u6837\u672c\u5b66\u4e60\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3a\u89e3\u51b3\u5c11\u6837\u672c\u5b66\u4e60\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2503.22402", "pdf": "https://arxiv.org/pdf/2503.22402", "abs": "https://arxiv.org/abs/2503.22402", "authors": ["Yizhang Zhu", "Runzhi Jiang", "Boyan Li", "Nan Tang", "Yuyu Luo"], "title": "EllieSQL: Cost-Efficient Text-to-SQL with Complexity-Aware Routing", "categories": ["cs.DB", "cs.AI", "cs.CL"], "comment": "19 pages, 8 figures, 3 tables", "summary": "Text-to-SQL automatically translates natural language queries to SQL,\nallowing non-technical users to retrieve data from databases without\nspecialized SQL knowledge. Despite the success of advanced LLM-based\nText-to-SQL approaches on leaderboards, their unsustainable computational\ncosts--often overlooked--stand as the \"elephant in the room\" in current\nleaderboard-driven research, limiting their economic practicability for\nreal-world deployment and widespread adoption. To tackle this, we exploratively\npropose EllieSQL, a complexity-aware routing framework that assigns queries to\nsuitable SQL generation pipelines based on estimated complexity. We investigate\nmultiple routers to direct simple queries to efficient approaches while\nreserving computationally intensive methods for complex cases. Drawing from\neconomics, we introduce the Token Elasticity of Performance (TEP) metric,\ncapturing cost-efficiency by quantifying the responsiveness of performance\ngains relative to token investment in SQL generation. Experiments show that\ncompared to always using the most advanced methods in our study, EllieSQL with\nthe Qwen2.5-0.5B-DPO router reduces token use by over 40% without compromising\nperformance on Bird development set, achieving more than a 2x boost in TEP over\nnon-routing approaches. This not only advances the pursuit of cost-efficient\nText-to-SQL but also invites the community to weigh resource efficiency\nalongside performance, contributing to progress in sustainable Text-to-SQL.", "AI": {"task": "\u63a2\u7d22\u5e76\u63d0\u51fa\u4e00\u79cd\u590d\u6742\u6027\u611f\u77e5\u7684\u8def\u7531\u6846\u67b6EllieSQL\uff0c\u4ee5\u4f18\u5316Text-to-SQL\u4efb\u52a1\u7684\u6210\u672c\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684Text-to-SQL\u65b9\u6cd5\u867d\u7136\u6027\u80fd\u4f18\u8d8a\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u548c\u7ecf\u6d4e\u53ef\u884c\u6027\u3002", "method": "\u63d0\u51faEllieSQL\u6846\u67b6\uff0c\u901a\u8fc7\u590d\u6742\u6027\u611f\u77e5\u7684\u8def\u7531\u5668\u5c06\u67e5\u8be2\u5206\u914d\u5230\u9002\u5408\u7684SQL\u751f\u6210\u7ba1\u9053\uff0c\u5e76\u5f15\u5165Token Elasticity of Performance (TEP) \u6307\u6807\u8861\u91cf\u6210\u672c\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cEllieSQL\u5728\u4f7f\u7528Qwen2.5-0.5B-DPO\u8def\u7531\u5668\u65f6\uff0c\u51cf\u5c11\u4e8640%\u4ee5\u4e0a\u7684token\u4f7f\u7528\uff0c\u4e14\u6027\u80fd\u672a\u4e0b\u964d\uff0cTEP\u63d0\u5347\u8d85\u8fc72\u500d\u3002", "conclusion": "EllieSQL\u4e0d\u4ec5\u63d0\u5347\u4e86Text-to-SQL\u7684\u6210\u672c\u6548\u7387\uff0c\u8fd8\u547c\u5401\u793e\u533a\u5728\u6027\u80fd\u4e4b\u5916\u91cd\u89c6\u8d44\u6e90\u6548\u7387\uff0c\u63a8\u52a8\u53ef\u6301\u7eed\u7684Text-to-SQL\u7814\u7a76\u3002"}}
{"id": "2503.22194", "pdf": "https://arxiv.org/pdf/2503.22194", "abs": "https://arxiv.org/abs/2503.22194", "authors": ["Yunhong Min", "Daehyeon Choi", "Kyeongmin Yeo", "Jihyun Lee", "Minhyuk Sung"], "title": "ORIGEN: Zero-Shot 3D Orientation Grounding in Text-to-Image Generation", "categories": ["cs.CV", "cs.LG"], "comment": "Project Page: https://origen2025.github.io", "summary": "We introduce ORIGEN, the first zero-shot method for 3D orientation grounding\nin text-to-image generation across multiple objects and diverse categories.\nWhile previous work on spatial grounding in image generation has mainly focused\non 2D positioning, it lacks control over 3D orientation. To address this, we\npropose a reward-guided sampling approach using a pretrained discriminative\nmodel for 3D orientation estimation and a one-step text-to-image generative\nflow model. While gradient-ascent-based optimization is a natural choice for\nreward-based guidance, it struggles to maintain image realism. Instead, we\nadopt a sampling-based approach using Langevin dynamics, which extends gradient\nascent by simply injecting random noise--requiring just a single additional\nline of code. Additionally, we introduce adaptive time rescaling based on the\nreward function to accelerate convergence. Our experiments show that ORIGEN\noutperforms both training-based and test-time guidance methods across\nquantitative metrics and user studies.", "AI": {"task": "\u63d0\u51faORIGEN\uff0c\u4e00\u79cd\u96f6\u6837\u672c\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u5b9e\u73b0\u591a\u5bf9\u8c61\u548c\u591a\u6837\u7c7b\u522b\u76843D\u65b9\u5411\u5b9a\u4f4d\u3002", "motivation": "\u73b0\u6709\u7a7a\u95f4\u5b9a\u4f4d\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce82D\u5b9a\u4f4d\uff0c\u7f3a\u4e4f\u5bf93D\u65b9\u5411\u7684\u63a7\u5236\u3002", "method": "\u91c7\u7528\u5956\u52b1\u5f15\u5bfc\u7684\u91c7\u6837\u65b9\u6cd5\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u7684\u5224\u522b\u6a21\u578b\u548c\u4e00\u6b65\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6d41\u6a21\u578b\uff0c\u5e76\u4f7f\u7528Langevin\u52a8\u529b\u5b66\u8fdb\u884c\u91c7\u6837\u4f18\u5316\u3002", "result": "ORIGEN\u5728\u5b9a\u91cf\u6307\u6807\u548c\u7528\u6237\u7814\u7a76\u4e2d\u5747\u4f18\u4e8e\u57fa\u4e8e\u8bad\u7ec3\u548c\u6d4b\u8bd5\u65f6\u5f15\u5bfc\u7684\u65b9\u6cd5\u3002", "conclusion": "ORIGEN\u6210\u529f\u89e3\u51b3\u4e863D\u65b9\u5411\u5b9a\u4f4d\u95ee\u9898\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u8868\u73b0\u4f18\u8d8a\u3002"}}
{"id": "2503.22424", "pdf": "https://arxiv.org/pdf/2503.22424", "abs": "https://arxiv.org/abs/2503.22424", "authors": ["Zhonghao Jiang", "Xiaoxue Ren", "Meng Yan", "Wei Jiang", "Yong Li", "Zhongxin Liu"], "title": "CoSIL: Software Issue Localization via LLM-Driven Code Repository Graph Searching", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have significantly advanced autonomous software\nengineering, leading to a growing number of software engineering agents that\nassist developers in automatic program repair. Issue localization forms the\nbasis for accurate patch generation. However, because of limitations caused by\nthe context window length of LLMs, existing issue localization methods face\nchallenges in balancing concise yet effective contexts and adequately\ncomprehensive search spaces. In this paper, we introduce CoSIL, an LLM driven,\nsimple yet powerful function level issue localization method without training\nor indexing. CoSIL reduces the search space through module call graphs,\niteratively searches the function call graph to obtain relevant contexts, and\nuses context pruning to control the search direction and manage contexts\neffectively. Importantly, the call graph is dynamically constructed by the LLM\nduring search, eliminating the need for pre-parsing. Experiment results\ndemonstrate that CoSIL achieves a Top-1 localization success rate of 43 percent\nand 44.6 percent on SWE bench Lite and SWE bench Verified, respectively, using\nQwen2.5 Coder 32B, outperforming existing methods by 8.6 to 98.2 percent. When\nCoSIL is applied to guide the patch generation stage, the resolved rate further\nimproves by 9.3 to 31.5 percent.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u51fd\u6570\u7ea7\u95ee\u9898\u5b9a\u4f4d\u65b9\u6cd5CoSIL\uff0c\u7528\u4e8e\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\u4e2d\u7684\u95ee\u9898\u5b9a\u4f4d\u3002", "motivation": "\u73b0\u6709\u95ee\u9898\u5b9a\u4f4d\u65b9\u6cd5\u56e0LLM\u4e0a\u4e0b\u6587\u7a97\u53e3\u957f\u5ea6\u7684\u9650\u5236\uff0c\u96be\u4ee5\u5e73\u8861\u7b80\u6d01\u6709\u6548\u7684\u4e0a\u4e0b\u6587\u4e0e\u5168\u9762\u641c\u7d22\u7a7a\u95f4\u7684\u9700\u6c42\u3002", "method": "CoSIL\u901a\u8fc7\u6a21\u5757\u8c03\u7528\u56fe\u7f29\u5c0f\u641c\u7d22\u7a7a\u95f4\uff0c\u8fed\u4ee3\u641c\u7d22\u51fd\u6570\u8c03\u7528\u56fe\u83b7\u53d6\u76f8\u5173\u4e0a\u4e0b\u6587\uff0c\u5e76\u4f7f\u7528\u4e0a\u4e0b\u6587\u526a\u679d\u63a7\u5236\u641c\u7d22\u65b9\u5411\u548c\u6709\u6548\u7ba1\u7406\u4e0a\u4e0b\u6587\u3002", "result": "\u5728SWE bench Lite\u548cSWE bench Verified\u4e0a\uff0cCoSIL\u7684Top-1\u5b9a\u4f4d\u6210\u529f\u7387\u5206\u522b\u4e3a43%\u548c44.6%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd58.6%\u81f398.2%\u3002", "conclusion": "CoSIL\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u6216\u7d22\u5f15\u7684\u9ad8\u6548\u95ee\u9898\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u95ee\u9898\u5b9a\u4f4d\u548c\u8865\u4e01\u751f\u6210\u7684\u6027\u80fd\u3002"}}
{"id": "2503.22197", "pdf": "https://arxiv.org/pdf/2503.22197", "abs": "https://arxiv.org/abs/2503.22197", "authors": ["Yang Liu", "Xun Zhang", "Jiale Du", "Xinbo Gao", "Jungong Han"], "title": "Extremely Simple Out-of-distribution Detection for Audio-visual Generalized Zero-shot Learning", "categories": ["cs.CV"], "comment": null, "summary": "Zero-shot Learning(ZSL) attains knowledge transfer from seen classes to\nunseen classes by exploring auxiliary category information, which is a\npromising yet difficult research topic. In this field, Audio-Visual Generalized\nZero-Shot Learning~(AV-GZSL) has aroused researchers' great interest in which\nintricate relations within triple modalities~(audio, video, and natural\nlanguage) render this task quite challenging but highly research-worthy.\nHowever, both existing embedding-based and generative-based AV-GZSL methods\ntend to suffer from domain shift problem a lot and we propose an extremely\nsimple Out-of-distribution~(OOD) detection based AV-GZSL method~(EZ-AVOOD) to\nfurther mitigate bias problem by differentiating seen and unseen samples at the\ninitial beginning. EZ-AVOOD accomplishes effective seen-unseen separation by\nexploiting the intrinsic discriminative information held in class-specific\nlogits and class-agnostic feature subspace without training an extra OOD\ndetector network. Followed by seen-unseen binary classification, we employ two\nexpert models to classify seen samples and unseen samples separately. Compared\nto existing state-of-the-art methods, our model achieves superior ZSL and GZSL\nperformances on three audio-visual datasets and becomes the new SOTA, which\ncomprehensively demonstrates the effectiveness of the proposed EZ-AVOOD.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eOOD\u68c0\u6d4b\u7684AV-GZSL\u65b9\u6cd5\uff08EZ-AVOOD\uff09\uff0c\u4ee5\u7f13\u89e3\u9886\u57df\u504f\u79fb\u95ee\u9898\u5e76\u533a\u5206\u53ef\u89c1\u548c\u672a\u89c1\u6837\u672c\u3002", "motivation": "\u73b0\u6709AV-GZSL\u65b9\u6cd5\u5728\u9886\u57df\u504f\u79fb\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\u6765\u533a\u5206\u53ef\u89c1\u548c\u672a\u89c1\u6837\u672c\u3002", "method": "\u5229\u7528\u7c7b\u7279\u5b9alogits\u548c\u7c7b\u65e0\u5173\u7279\u5f81\u5b50\u7a7a\u95f4\u7684\u4fe1\u606f\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3OOD\u68c0\u6d4b\u7f51\u7edc\uff0c\u5b9e\u73b0\u53ef\u89c1-\u672a\u89c1\u6837\u672c\u5206\u79bb\uff0c\u5e76\u5206\u522b\u7528\u4e24\u4e2a\u4e13\u5bb6\u6a21\u578b\u5206\u7c7b\u3002", "result": "\u5728\u4e09\u4e2a\u97f3\u9891-\u89c6\u89c9\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684ZSL\u548cGZSL\u6027\u80fd\uff0c\u6210\u4e3a\u65b0\u7684SOTA\u3002", "conclusion": "EZ-AVOOD\u901a\u8fc7\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\u89e3\u51b3\u4e86\u9886\u57df\u504f\u79fb\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2503.22610", "pdf": "https://arxiv.org/pdf/2503.22610", "abs": "https://arxiv.org/abs/2503.22610", "authors": ["Antonia Karamolegkou", "Malvina Nikandrou", "Georgios Pantazopoulos", "Danae Sanchez Villegas", "Phillip Rust", "Ruchira Dhar", "Daniel Hershcovich", "Anders S\u00f8gaard"], "title": "Evaluating Multimodal Language Models as Visual Assistants for Visually Impaired Users", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "This paper explores the effectiveness of Multimodal Large Language models\n(MLLMs) as assistive technologies for visually impaired individuals. We conduct\na user survey to identify adoption patterns and key challenges users face with\nsuch technologies. Despite a high adoption rate of these models, our findings\nhighlight concerns related to contextual understanding, cultural sensitivity,\nand complex scene understanding, particularly for individuals who may rely\nsolely on them for visual interpretation. Informed by these results, we collate\nfive user-centred tasks with image and video inputs, including a novel task on\nOptical Braille Recognition. Our systematic evaluation of twelve MLLMs reveals\nthat further advancements are necessary to overcome limitations related to\ncultural context, multilingual support, Braille reading comprehension,\nassistive object recognition, and hallucinations. This work provides critical\ninsights into the future direction of multimodal AI for accessibility,\nunderscoring the need for more inclusive, robust, and trustworthy visual\nassistance technologies.", "AI": {"task": "\u63a2\u8ba8\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4f5c\u4e3a\u89c6\u969c\u4eba\u58eb\u8f85\u52a9\u6280\u672f\u7684\u6709\u6548\u6027\u3002", "motivation": "\u901a\u8fc7\u7528\u6237\u8c03\u67e5\u4e86\u89e3\u6b64\u7c7b\u6280\u672f\u7684\u91c7\u7528\u6a21\u5f0f\u548c\u7528\u6237\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\uff0c\u4e3a\u6539\u8fdb\u6280\u672f\u63d0\u4f9b\u4f9d\u636e\u3002", "method": "\u8fdb\u884c\u7528\u6237\u8c03\u67e5\uff0c\u6536\u96c6\u4e94\u9879\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u4efb\u52a1\uff08\u5305\u62ec\u5149\u5b66\u76f2\u6587\u8bc6\u522b\u65b0\u4efb\u52a1\uff09\uff0c\u5e76\u7cfb\u7edf\u8bc4\u4f3012\u79cdMLLMs\u3002", "result": "\u5c3d\u7ba1\u91c7\u7528\u7387\u9ad8\uff0c\u4f46\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u7406\u89e3\u3001\u6587\u5316\u654f\u611f\u6027\u3001\u590d\u6742\u573a\u666f\u7406\u89e3\u7b49\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002", "conclusion": "\u591a\u6a21\u6001AI\u5728\u53ef\u8bbf\u95ee\u6027\u65b9\u9762\u9700\u66f4\u5305\u5bb9\u3001\u7a33\u5065\u548c\u53ef\u4fe1\u8d56\uff0c\u672a\u6765\u5e94\u5173\u6ce8\u6587\u5316\u80cc\u666f\u3001\u591a\u8bed\u8a00\u652f\u6301\u548c\u76f2\u6587\u7406\u89e3\u7b49\u65b9\u5411\u3002"}}
{"id": "2503.22199", "pdf": "https://arxiv.org/pdf/2503.22199", "abs": "https://arxiv.org/abs/2503.22199", "authors": ["Long Gao", "Yunhe Zhang", "Langkun Chen", "Yan Jiang", "Weiying Xie", "Yunsong Li"], "title": "Hyperspectral Adapter for Object Tracking based on Hyperspectral Video", "categories": ["cs.CV"], "comment": null, "summary": "Object tracking based on hyperspectral video attracts increasing attention to\nthe rich material and motion information in the hyperspectral videos. The\nprevailing hyperspectral methods adapt pretrained RGB-based object tracking\nnetworks for hyperspectral tasks by fine-tuning the entire network on\nhyperspectral datasets, which achieves impressive results in challenging\nscenarios. However, the performance of hyperspectral trackers is limited by the\nloss of spectral information during the transformation, and fine-tuning the\nentire pretrained network is inefficient for practical applications. To address\nthe issues, a new hyperspectral object tracking method, hyperspectral adapter\nfor tracking (HyA-T), is proposed in this work. The hyperspectral adapter for\nthe self-attention (HAS) and the hyperspectral adapter for the multilayer\nperceptron (HAM) are proposed to generate the adaption information and to\ntransfer the multi-head self-attention (MSA) module and the multilayer\nperceptron (MLP) in pretrained network for the hyperspectral object tracking\ntask by augmenting the adaption information into the calculation of the MSA and\nMLP. Additionally, the hyperspectral enhancement of input (HEI) is proposed to\naugment the original spectral information into the input of the tracking\nnetwork. The proposed methods extract spectral information directly from the\nhyperspectral images, which prevent the loss of the spectral information.\nMoreover, only the parameters in the proposed methods are fine-tuned, which is\nmore efficient than the existing methods. Extensive experiments were conducted\non four datasets with various spectral bands, verifing the effectiveness of the\nproposed methods. The HyA-T achieves state-of-the-art performance on all the\ndatasets.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u9ad8\u5149\u8c31\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\uff08HyA-T\uff09\uff0c\u901a\u8fc7\u9002\u914d\u5668\u589e\u5f3a\u5149\u8c31\u4fe1\u606f\u5e76\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u9ad8\u5149\u8c31\u8ddf\u8e2a\u65b9\u6cd5\u5728\u8f6c\u6362\u8fc7\u7a0b\u4e2d\u4e22\u5931\u5149\u8c31\u4fe1\u606f\uff0c\u4e14\u5168\u7f51\u7edc\u5fae\u8c03\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faHAS\u548cHAM\u9002\u914d\u5668\u589e\u5f3a\u81ea\u6ce8\u610f\u529b\u548c\u591a\u5c42\u611f\u77e5\u5668\uff0c\u4ee5\u53caHEI\u589e\u5f3a\u8f93\u5165\u5149\u8c31\u4fe1\u606f\u3002", "result": "HyA-T\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "HyA-T\u6709\u6548\u89e3\u51b3\u4e86\u5149\u8c31\u4fe1\u606f\u4e22\u5931\u548c\u6548\u7387\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u8d8a\u3002"}}
{"id": "2503.22673", "pdf": "https://arxiv.org/pdf/2503.22673", "abs": "https://arxiv.org/abs/2503.22673", "authors": ["Jianguo Zhang", "Thai Hoang", "Ming Zhu", "Zuxin Liu", "Shiyu Wang", "Tulika Awalgaonkar", "Akshara Prabhakar", "Haolin Chen", "Weiran Yao", "Zhiwei Liu", "Juntao Tan", "Juan Carlos Niebles", "Shelby Heinecke", "Huan Wang", "Silvio Savarese", "Caiming Xiong"], "title": "ActionStudio: A Lightweight Framework for Data and Training of Action Models", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Action models are essential for enabling autonomous agents to perform complex\ntasks. However, training large action models remains challenging due to the\ndiversity of agent environments and the complexity of agentic data. Despite\ngrowing interest, existing infrastructure provides limited support for\nscalable, agent-specific fine-tuning. We present ActionStudio, a lightweight\nand extensible data and training framework designed for action models.\nActionStudio unifies heterogeneous agent trajectories through a standardized\nformat, supports diverse training paradigms including LoRA, full fine-tuning,\nand distributed setups, and integrates robust preprocessing and verification\ntools. We validate its effectiveness across both public and realistic industry\nbenchmarks, demonstrating strong performance and practical scalability. We\nopen-sourced code and data at https://github.com/SalesforceAIResearch/xLAM to\nfacilitate research in the community.", "AI": {"task": "\u63d0\u51fa\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4e14\u53ef\u6269\u5c55\u7684\u6570\u636e\u548c\u8bad\u7ec3\u6846\u67b6ActionStudio\uff0c\u7528\u4e8e\u52a8\u4f5c\u6a21\u578b\u7684\u8bad\u7ec3\u3002", "motivation": "\u7531\u4e8e\u4ee3\u7406\u73af\u5883\u7684\u591a\u6837\u6027\u548c\u4ee3\u7406\u6570\u636e\u7684\u590d\u6742\u6027\uff0c\u8bad\u7ec3\u5927\u578b\u52a8\u4f5c\u6a21\u578b\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u57fa\u7840\u8bbe\u65bd\u5bf9\u53ef\u6269\u5c55\u7684\u3001\u9488\u5bf9\u4ee3\u7406\u7684\u5fae\u8c03\u652f\u6301\u6709\u9650\u3002", "method": "ActionStudio\u901a\u8fc7\u6807\u51c6\u5316\u683c\u5f0f\u7edf\u4e00\u5f02\u6784\u4ee3\u7406\u8f68\u8ff9\uff0c\u652f\u6301\u5305\u62ecLoRA\u3001\u5168\u5fae\u8c03\u548c\u5206\u5e03\u5f0f\u8bbe\u7f6e\u5728\u5185\u7684\u591a\u79cd\u8bad\u7ec3\u8303\u5f0f\uff0c\u5e76\u96c6\u6210\u5f3a\u5927\u7684\u9884\u5904\u7406\u548c\u9a8c\u8bc1\u5de5\u5177\u3002", "result": "\u5728\u516c\u5171\u548c\u5b9e\u9645\u884c\u4e1a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6027\u80fd\u548c\u5b9e\u9645\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u5f00\u6e90\u4ee3\u7801\u548c\u6570\u636e\u4ee5\u4fc3\u8fdb\u793e\u533a\u7814\u7a76\u3002"}}
{"id": "2503.22201", "pdf": "https://arxiv.org/pdf/2503.22201", "abs": "https://arxiv.org/abs/2503.22201", "authors": ["Jaewoo Jeong", "Seohee Lee", "Daehee Park", "Giwon Lee", "Kuk-Jin Yoon"], "title": "Multi-modal Knowledge Distillation-based Human Trajectory Forecasting", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Pedestrian trajectory forecasting is crucial in various applications such as\nautonomous driving and mobile robot navigation. In such applications,\ncamera-based perception enables the extraction of additional modalities (human\npose, text) to enhance prediction accuracy. Indeed, we find that textual\ndescriptions play a crucial role in integrating additional modalities into a\nunified understanding. However, online extraction of text requires the use of\nVLM, which may not be feasible for resource-constrained systems. To address\nthis challenge, we propose a multi-modal knowledge distillation framework: a\nstudent model with limited modality is distilled from a teacher model trained\nwith full range of modalities. The comprehensive knowledge of a teacher model\ntrained with trajectory, human pose, and text is distilled into a student model\nusing only trajectory or human pose as a sole supplement. In doing so, we\nseparately distill the core locomotion insights from intra-agent multi-modality\nand inter-agent interaction. Our generalizable framework is validated with two\nstate-of-the-art models across three datasets on both ego-view (JRDB, SIT) and\nBEV-view (ETH/UCY) setups, utilizing both annotated and VLM-generated text\ncaptions. Distilled student models show consistent improvement in all\nprediction metrics for both full and instantaneous observations, improving up\nto ~13%. The code is available at https://github.com/Jaewoo97/KDTF.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u591a\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u8d44\u6e90\u53d7\u9650\u7cfb\u7edf\u4e2d\u63d0\u5347\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5728\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u4e2d\uff0c\u6587\u672c\u63cf\u8ff0\u7b49\u989d\u5916\u6a21\u6001\u80fd\u63d0\u5347\u51c6\u786e\u6027\uff0c\u4f46\u8d44\u6e90\u53d7\u9650\u7cfb\u7edf\u96be\u4ee5\u5728\u7ebf\u63d0\u53d6\u6587\u672c\u3002", "method": "\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\uff0c\u5c06\u6559\u5e08\u6a21\u578b\uff08\u4f7f\u7528\u8f68\u8ff9\u3001\u59ff\u6001\u548c\u6587\u672c\uff09\u7684\u77e5\u8bc6\u4f20\u9012\u7ed9\u5b66\u751f\u6a21\u578b\uff08\u4ec5\u4f7f\u7528\u8f68\u8ff9\u6216\u59ff\u6001\uff09\u3002", "result": "\u84b8\u998f\u540e\u7684\u5b66\u751f\u6a21\u578b\u5728\u6240\u6709\u9884\u6d4b\u6307\u6807\u4e0a\u5747\u6709\u63d0\u5347\uff0c\u6700\u9ad8\u63d0\u5347\u7ea613%\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u591a\u79cd\u6570\u636e\u96c6\u548c\u8bbe\u7f6e\u4e0b\u9a8c\u8bc1\u6709\u6548\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7cfb\u7edf\u3002"}}
{"id": "2503.22674", "pdf": "https://arxiv.org/pdf/2503.22674", "abs": "https://arxiv.org/abs/2503.22674", "authors": ["Belinda Z. Li", "Been Kim", "Zi Wang"], "title": "QuestBench: Can LLMs ask the right question to acquire information in reasoning tasks?", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Code and dataset are available at\n  \\url{https://github.com/google-deepmind/questbench}", "summary": "Recently, a large amount of work has focused on improving large language\nmodels' (LLMs') performance on reasoning benchmarks such as math and logic.\nHowever, past work has largely assumed that tasks are well-defined. In the real\nworld, queries to LLMs are often underspecified, only solvable through\nacquiring missing information. We formalize this as a constraint satisfaction\nproblem (CSP) with missing variable assignments. Using a special case of this\nformalism where only one necessary variable assignment is missing, we can\nrigorously evaluate an LLM's ability to identify the minimal necessary question\nto ask and quantify axes of difficulty levels for each problem. We present\nQuestBench, a set of underspecified reasoning tasks solvable by asking at most\none question, which includes: (1) Logic-Q: Logical reasoning tasks with one\nmissing proposition, (2) Planning-Q: PDDL planning problems with initial states\nthat are partially-observed, (3) GSM-Q: Human-annotated grade school math\nproblems with one missing variable assignment, and (4) GSME-Q: a version of\nGSM-Q where word problems are translated into equations by human annotators.\nThe LLM is tasked with selecting the correct clarification question(s) from a\nlist of options. While state-of-the-art models excel at GSM-Q and GSME-Q, their\naccuracy is only 40-50% on Logic-Q and Planning-Q. Analysis demonstrates that\nthe ability to solve well-specified reasoning problems may not be sufficient\nfor success on our benchmark: models have difficulty identifying the right\nquestion to ask, even when they can solve the fully specified version of the\nproblem. Furthermore, in the Planning-Q domain, LLMs tend not to hedge, even\nwhen explicitly presented with the option to predict ``not sure.'' This\nhighlights the need for deeper investigation into models' information\nacquisition capabilities.", "AI": {"task": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u89e3\u51b3\u7f3a\u5931\u4fe1\u606f\u95ee\u9898\u65f6\u7684\u80fd\u529b\uff0c\u5e76\u91cf\u5316\u95ee\u9898\u7684\u96be\u5ea6\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u67e5\u8be2\u5f80\u5f80\u4e0d\u5b8c\u6574\uff0c\u9700\u8981\u83b7\u53d6\u7f3a\u5931\u4fe1\u606f\u624d\u80fd\u89e3\u51b3\uff0c\u800c\u73b0\u6709\u7814\u7a76\u591a\u5047\u8bbe\u4efb\u52a1\u5b9a\u4e49\u660e\u786e\u3002", "method": "\u5c06\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u7f3a\u5931\u53d8\u91cf\u8d4b\u503c\u7684\u7ea6\u675f\u6ee1\u8db3\u95ee\u9898\uff08CSP\uff09\uff0c\u5e76\u6784\u5efaQuestBench\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u5305\u542b\u56db\u79cd\u4efb\u52a1\u7c7b\u578b\u3002", "result": "\u5148\u8fdb\u6a21\u578b\u5728GSM-Q\u548cGSME-Q\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728Logic-Q\u548cPlanning-Q\u4e0a\u51c6\u786e\u7387\u4ec5\u4e3a40-50%\u3002", "conclusion": "\u6a21\u578b\u5728\u89e3\u51b3\u660e\u786e\u95ee\u9898\u65f6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8bc6\u522b\u5fc5\u8981\u63d0\u95ee\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u5176\u4fe1\u606f\u83b7\u53d6\u80fd\u529b\u3002"}}
{"id": "2503.22204", "pdf": "https://arxiv.org/pdf/2503.22204", "abs": "https://arxiv.org/abs/2503.22204", "authors": ["Yiren Lu", "Yunlai Zhou", "Yiran Qiao", "Chaoda Song", "Tuo Liang", "Jing Ma", "Yu Yin"], "title": "Segment then Splat: A Unified Approach for 3D Open-Vocabulary Segmentation based on Gaussian Splatting", "categories": ["cs.CV"], "comment": "Project page: https://vulab-ai.github.io/Segment-then-Splat/", "summary": "Open-vocabulary querying in 3D space is crucial for enabling more intelligent\nperception in applications such as robotics, autonomous systems, and augmented\nreality. However, most existing methods rely on 2D pixel-level parsing, leading\nto multi-view inconsistencies and poor 3D object retrieval. Moreover, they are\nlimited to static scenes and struggle with dynamic scenes due to the\ncomplexities of motion modeling. In this paper, we propose Segment then Splat,\na 3D-aware open vocabulary segmentation approach for both static and dynamic\nscenes based on Gaussian Splatting. Segment then Splat reverses the long\nestablished approach of \"segmentation after reconstruction\" by dividing\nGaussians into distinct object sets before reconstruction. Once the\nreconstruction is complete, the scene is naturally segmented into individual\nobjects, achieving true 3D segmentation. This approach not only eliminates\nGaussian-object misalignment issues in dynamic scenes but also accelerates the\noptimization process, as it eliminates the need for learning a separate\nlanguage field. After optimization, a CLIP embedding is assigned to each object\nto enable open-vocabulary querying. Extensive experiments on various datasets\ndemonstrate the effectiveness of our proposed method in both static and dynamic\nscenarios.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u6e85\u5c04\u76843D\u611f\u77e5\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u65b9\u6cd5\uff0c\u7528\u4e8e\u9759\u6001\u548c\u52a8\u6001\u573a\u666f\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u57283D\u7a7a\u95f4\u4e2d\u5f00\u653e\u8bcd\u6c47\u67e5\u8be2\u65f6\u5b58\u5728\u7684\u591a\u89c6\u89d2\u4e0d\u4e00\u81f4\u6027\u30013D\u5bf9\u8c61\u68c0\u7d22\u6548\u679c\u5dee\u4ee5\u53ca\u52a8\u6001\u573a\u666f\u5904\u7406\u56f0\u96be\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5148\u5206\u5272\u540e\u91cd\u5efa\u7684\u65b9\u5f0f\uff08Segment then Splat\uff09\uff0c\u5c06\u9ad8\u65af\u5206\u4e3a\u4e0d\u540c\u5bf9\u8c61\u96c6\uff0c\u518d\u8fdb\u884c\u91cd\u5efa\uff0c\u5b9e\u73b0\u771f\u6b63\u76843D\u5206\u5272\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9759\u6001\u548c\u52a8\u6001\u573a\u666f\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u6d88\u9664\u4e86\u9ad8\u65af\u4e0e\u5bf9\u8c61\u4e0d\u5bf9\u9f50\u7684\u95ee\u9898\uff0c\u5e76\u52a0\u901f\u4e86\u4f18\u5316\u8fc7\u7a0b\u3002", "conclusion": "Segment then Splat\u65b9\u6cd5\u57283D\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2503.22675", "pdf": "https://arxiv.org/pdf/2503.22675", "abs": "https://arxiv.org/abs/2503.22675", "authors": ["Jiakai Tang", "Sunhao Dai", "Teng Shi", "Jun Xu", "Xu Chen", "Wen Chen", "Wu Jian", "Yuning Jiang"], "title": "Think Before Recommend: Unleashing the Latent Reasoning Power for Sequential Recommendation", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Sequential Recommendation (SeqRec) aims to predict the next item by capturing\nsequential patterns from users' historical interactions, playing a crucial role\nin many real-world recommender systems. However, existing approaches\npredominantly adopt a direct forward computation paradigm, where the final\nhidden state of the sequence encoder serves as the user representation. We\nargue that this inference paradigm, due to its limited computational depth,\nstruggles to model the complex evolving nature of user preferences and lacks a\nnuanced understanding of long-tail items, leading to suboptimal performance. To\naddress this issue, we propose \\textbf{ReaRec}, the first inference-time\ncomputing framework for recommender systems, which enhances user\nrepresentations through implicit multi-step reasoning. Specifically, ReaRec\nautoregressively feeds the sequence's last hidden state into the sequential\nrecommender while incorporating special reasoning position embeddings to\ndecouple the original item encoding space from the multi-step reasoning space.\nMoreover, we introduce two lightweight reasoning-based learning methods,\nEnsemble Reasoning Learning (ERL) and Progressive Reasoning Learning (PRL), to\nfurther effectively exploit ReaRec's reasoning potential. Extensive experiments\non five public real-world datasets and different SeqRec architectures\ndemonstrate the generality and effectiveness of our proposed ReaRec.\nRemarkably, post-hoc analyses reveal that ReaRec significantly elevates the\nperformance ceiling of multiple sequential recommendation backbones by\napproximately 30\\%-50\\%. Thus, we believe this work can open a new and\npromising avenue for future research in inference-time computing for sequential\nrecommendation.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aReaRec\u7684\u63a8\u7406\u65f6\u8ba1\u7b97\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u987a\u5e8f\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u7528\u6237\u8868\u793a\u3002", "motivation": "\u73b0\u6709\u987a\u5e8f\u63a8\u8350\u65b9\u6cd5\u91c7\u7528\u76f4\u63a5\u524d\u5411\u8ba1\u7b97\u8303\u5f0f\uff0c\u96be\u4ee5\u6355\u6349\u7528\u6237\u504f\u597d\u7684\u590d\u6742\u6f14\u53d8\uff0c\u4e14\u5bf9\u957f\u5c3e\u7269\u54c1\u7684\u7406\u89e3\u4e0d\u8db3\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "ReaRec\u901a\u8fc7\u9690\u5f0f\u591a\u6b65\u63a8\u7406\u589e\u5f3a\u7528\u6237\u8868\u793a\uff0c\u5f15\u5165\u7279\u6b8a\u63a8\u7406\u4f4d\u7f6e\u5d4c\u5165\u89e3\u8026\u539f\u59cb\u7269\u54c1\u7f16\u7801\u7a7a\u95f4\u4e0e\u591a\u6b65\u63a8\u7406\u7a7a\u95f4\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u8f7b\u91cf\u7ea7\u63a8\u7406\u5b66\u4e60\u65b9\u6cd5\uff08ERL\u548cPRL\uff09\u3002", "result": "\u5728\u4e94\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u548c\u4e0d\u540c\u987a\u5e8f\u63a8\u8350\u67b6\u6784\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cReaRec\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u6027\u80fd\u4e0a\u9650\u63d0\u9ad8\u4e8630%-50%\u3002", "conclusion": "ReaRec\u4e3a\u987a\u5e8f\u63a8\u8350\u7684\u63a8\u7406\u65f6\u8ba1\u7b97\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u5177\u6709\u5e7f\u9614\u7684\u7814\u7a76\u524d\u666f\u3002"}}
{"id": "2503.22209", "pdf": "https://arxiv.org/pdf/2503.22209", "abs": "https://arxiv.org/abs/2503.22209", "authors": ["Wonhyeok Choi", "Kyumin Hwang", "Minwoo Choi", "Kiljoon Han", "Wonjoon Choi", "Mingyu Shin", "Sunghoon Im"], "title": "Intrinsic Image Decomposition for Robust Self-supervised Monocular Depth Estimation on Reflective Surfaces", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at AAAI 2025", "summary": "Self-supervised monocular depth estimation (SSMDE) has gained attention in\nthe field of deep learning as it estimates depth without requiring ground truth\ndepth maps. This approach typically uses a photometric consistency loss between\na synthesized image, generated from the estimated depth, and the original\nimage, thereby reducing the need for extensive dataset acquisition. However,\nthe conventional photometric consistency loss relies on the Lambertian\nassumption, which often leads to significant errors when dealing with\nreflective surfaces that deviate from this model. To address this limitation,\nwe propose a novel framework that incorporates intrinsic image decomposition\ninto SSMDE. Our method synergistically trains for both monocular depth\nestimation and intrinsic image decomposition. The accurate depth estimation\nfacilitates multi-image consistency for intrinsic image decomposition by\naligning different view coordinate systems, while the decomposition process\nidentifies reflective areas and excludes corrupted gradients from the depth\ntraining process. Furthermore, our framework introduces a pseudo-depth\ngeneration and knowledge distillation technique to further enhance the\nperformance of the student model across both reflective and non-reflective\nsurfaces. Comprehensive evaluations on multiple datasets show that our approach\nsignificantly outperforms existing SSMDE baselines in depth prediction,\nespecially on reflective surfaces.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u672c\u5f81\u56fe\u50cf\u5206\u89e3\u7684\u81ea\u76d1\u7763\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u53cd\u5c04\u8868\u9762\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u5149\u5ea6\u4e00\u81f4\u6027\u635f\u5931\u4f9d\u8d56\u6717\u4f2f\u5047\u8bbe\uff0c\u5728\u5904\u7406\u53cd\u5c04\u8868\u9762\u65f6\u4f1a\u4ea7\u751f\u663e\u8457\u8bef\u5dee\u3002", "method": "\u7ed3\u5408\u672c\u5f81\u56fe\u50cf\u5206\u89e3\u4e0e\u6df1\u5ea6\u4f30\u8ba1\uff0c\u901a\u8fc7\u591a\u56fe\u50cf\u4e00\u81f4\u6027\u5bf9\u9f50\u5750\u6807\u7cfb\uff0c\u5e76\u6392\u9664\u53cd\u5c04\u533a\u57df\u7684\u68af\u5ea6\u5e72\u6270\uff0c\u540c\u65f6\u5f15\u5165\u4f2a\u6df1\u5ea6\u751f\u6210\u548c\u77e5\u8bc6\u84b8\u998f\u6280\u672f\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5c24\u5176\u5728\u53cd\u5c04\u8868\u9762\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u672c\u5f81\u56fe\u50cf\u5206\u89e3\u548c\u77e5\u8bc6\u84b8\u998f\u6280\u672f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u81ea\u76d1\u7763\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u53cd\u5c04\u8868\u9762\u4e0a\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2503.22215", "pdf": "https://arxiv.org/pdf/2503.22215", "abs": "https://arxiv.org/abs/2503.22215", "authors": ["Zhihan Zhou", "Feng Hong", "Jiaan Luo", "Jiangchao Yao", "Dongsheng Li", "Bo Han", "Ya Zhang", "Yanfeng Wang"], "title": "Learning to Instruct for Visual Instruction Tuning", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "16 pages, 10 figures", "summary": "We propose LIT, an advancement of visual instruction tuning (VIT). While VIT\nequips Multimodal LLMs (MLLMs) with promising multimodal capabilities, the\ncurrent design choices for VIT often result in overfitting and shortcut\nlearning, potentially degrading performance. This gap arises from an\noveremphasis on instruction-following abilities, while neglecting the proactive\nunderstanding of visual information. Inspired by this, LIT adopts a simple yet\neffective approach by incorporating the loss function into both the instruction\nand response sequences. It seamlessly expands the training data, and\nregularizes the MLLMs from overly relying on language priors. Based on this\nmerit, LIT achieves a significant relative improvement of up to 9% on\ncomprehensive multimodal benchmarks, requiring no additional training data and\nincurring negligible computational overhead. Surprisingly, LIT attains\nexceptional fundamental visual capabilities, yielding up to an 18% improvement\nin captioning performance, while simultaneously alleviating hallucination in\nMLLMs.", "AI": {"task": "\u63d0\u51faLIT\u65b9\u6cd5\u4ee5\u6539\u8fdb\u89c6\u89c9\u6307\u4ee4\u8c03\u6574\uff08VIT\uff09\uff0c\u89e3\u51b3\u5176\u8fc7\u62df\u5408\u548c\u6377\u5f84\u5b66\u4e60\u95ee\u9898\u3002", "motivation": "\u5f53\u524dVIT\u8bbe\u8ba1\u8fc7\u4e8e\u5f3a\u8c03\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\uff0c\u5ffd\u89c6\u4e86\u5bf9\u89c6\u89c9\u4fe1\u606f\u7684\u4e3b\u52a8\u7406\u89e3\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "LIT\u901a\u8fc7\u5c06\u635f\u5931\u51fd\u6570\u540c\u65f6\u5e94\u7528\u4e8e\u6307\u4ee4\u548c\u54cd\u5e94\u5e8f\u5217\uff0c\u6269\u5c55\u8bad\u7ec3\u6570\u636e\u5e76\u51cf\u5c11\u5bf9\u8bed\u8a00\u5148\u9a8c\u7684\u4f9d\u8d56\u3002", "result": "LIT\u5728\u7efc\u5408\u591a\u6a21\u6001\u57fa\u51c6\u4e0a\u5b9e\u73b0\u9ad8\u8fbe9%\u7684\u76f8\u5bf9\u63d0\u5347\uff0c\u5e76\u5728\u89c6\u89c9\u63cf\u8ff0\u4efb\u52a1\u4e2d\u63d0\u534718%\uff0c\u540c\u65f6\u51cf\u5c11\u5e7b\u89c9\u73b0\u8c61\u3002", "conclusion": "LIT\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u989d\u5916\u6570\u636e\u6216\u8ba1\u7b97\u5f00\u9500\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2503.22218", "pdf": "https://arxiv.org/pdf/2503.22218", "abs": "https://arxiv.org/abs/2503.22218", "authors": ["Wenjie Liu", "Zhongliang Liu", "Xiaoyan Yang", "Man Sha", "Yang Li"], "title": "ABC-GS: Alignment-Based Controllable Style Transfer for 3D Gaussian Splatting", "categories": ["cs.CV", "eess.IV"], "comment": "10 pages, 14 figures", "summary": "3D scene stylization approaches based on Neural Radiance Fields (NeRF)\nachieve promising results by optimizing with Nearest Neighbor Feature Matching\n(NNFM) loss. However, NNFM loss does not consider global style information. In\naddition, the implicit representation of NeRF limits their fine-grained control\nover the resulting scenes. In this paper, we introduce ABC-GS, a novel\nframework based on 3D Gaussian Splatting to achieve high-quality 3D style\ntransfer. To this end, a controllable matching stage is designed to achieve\nprecise alignment between scene content and style features through segmentation\nmasks. Moreover, a style transfer loss function based on feature alignment is\nproposed to ensure that the outcomes of style transfer accurately reflect the\nglobal style of the reference image. Furthermore, the original geometric\ninformation of the scene is preserved with the depth loss and Gaussian\nregularization terms. Extensive experiments show that our ABC-GS provides\ncontrollability of style transfer and achieves stylization results that are\nmore faithfully aligned with the global style of the chosen artistic reference.\nOur homepage is available at https://vpx-ecnu.github.io/ABC-GS-website.", "AI": {"task": "\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\uff083D Gaussian Splatting\uff09\u5b9e\u73b0\u9ad8\u8d28\u91cf\u76843D\u98ce\u683c\u8fc1\u79fb\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eNeRF\u548cNNFM\u635f\u5931\u7684\u65b9\u6cd5\u672a\u8003\u8651\u5168\u5c40\u98ce\u683c\u4fe1\u606f\uff0c\u4e14NeRF\u7684\u9690\u5f0f\u8868\u793a\u9650\u5236\u4e86\u573a\u666f\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002", "method": "\u8bbe\u8ba1\u4e86\u53ef\u63a7\u5339\u914d\u9636\u6bb5\u4ee5\u5b9e\u73b0\u573a\u666f\u5185\u5bb9\u4e0e\u98ce\u683c\u7279\u5f81\u7684\u7cbe\u786e\u5bf9\u9f50\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u7279\u5f81\u5bf9\u9f50\u7684\u98ce\u683c\u8fc1\u79fb\u635f\u5931\u51fd\u6570\u3002", "result": "ABC-GS\u6846\u67b6\u5b9e\u73b0\u4e86\u98ce\u683c\u8fc1\u79fb\u7684\u53ef\u63a7\u6027\uff0c\u5e76\u751f\u6210\u4e86\u66f4\u5fe0\u5b9e\u4e8e\u53c2\u8003\u56fe\u50cf\u5168\u5c40\u98ce\u683c\u7684\u98ce\u683c\u5316\u7ed3\u679c\u3002", "conclusion": "ABC-GS\u901a\u8fc7\u6539\u8fdb\u7684\u5339\u914d\u548c\u635f\u5931\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u76843D\u98ce\u683c\u8fc1\u79fb\u3002"}}
{"id": "2503.22225", "pdf": "https://arxiv.org/pdf/2503.22225", "abs": "https://arxiv.org/abs/2503.22225", "authors": ["Haijie Yang", "Zhenyu Zhang", "Hao Tang", "Jianjun Qian", "Jian Yang"], "title": "Follow Your Motion: A Generic Temporal Consistency Portrait Editing Framework with Trajectory Guidance", "categories": ["cs.CV"], "comment": "https://anonymous-hub1127.github.io/FYM.github.io/", "summary": "Pre-trained conditional diffusion models have demonstrated remarkable\npotential in image editing. However, they often face challenges with temporal\nconsistency, particularly in the talking head domain, where continuous changes\nin facial expressions intensify the level of difficulty. These issues stem from\nthe independent editing of individual images and the inherent loss of temporal\ncontinuity during the editing process. In this paper, we introduce Follow Your\nMotion (FYM), a generic framework for maintaining temporal consistency in\nportrait editing. Specifically, given portrait images rendered by a pre-trained\n3D Gaussian Splatting model, we first develop a diffusion model that\nintuitively and inherently learns motion trajectory changes at different scales\nand pixel coordinates, from the first frame to each subsequent frame. This\napproach ensures that temporally inconsistent edited avatars inherit the motion\ninformation from the rendered avatars. Secondly, to maintain fine-grained\nexpression temporal consistency in talking head editing, we propose a dynamic\nre-weighted attention mechanism. This mechanism assigns higher weight\ncoefficients to landmark points in space and dynamically updates these weights\nbased on landmark loss, achieving more consistent and refined facial\nexpressions. Extensive experiments demonstrate that our method outperforms\nexisting approaches in terms of temporal consistency and can be used to\noptimize and compensate for temporally inconsistent outputs in a range of\napplications, such as text-driven editing, relighting, and various other\napplications.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aFollow Your Motion (FYM)\u7684\u901a\u7528\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u8096\u50cf\u7f16\u8f91\u4e2d\u4fdd\u6301\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "motivation": "\u9884\u8bad\u7ec3\u7684\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u7f16\u8f91\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u52a8\u6001\u9762\u90e8\u8868\u60c5\u7684\u7f16\u8f91\u4e2d\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u6269\u6563\u6a21\u578b\uff0c\u5b66\u4e60\u8fd0\u52a8\u8f68\u8ff9\u53d8\u5316\uff0c\u5e76\u63d0\u51fa\u52a8\u6001\u91cd\u52a0\u6743\u6ce8\u610f\u529b\u673a\u5236\u4ee5\u4fdd\u6301\u8868\u60c5\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u65f6\u95f4\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u9002\u7528\u4e8e\u591a\u79cd\u5e94\u7528\u573a\u666f\u3002", "conclusion": "FYM\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u95f4\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u4e3a\u8096\u50cf\u7f16\u8f91\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2503.22231", "pdf": "https://arxiv.org/pdf/2503.22231", "abs": "https://arxiv.org/abs/2503.22231", "authors": ["Yishen Ji", "Ziyue Zhu", "Zhenxin Zhu", "Kaixin Xiong", "Ming Lu", "Zhiqi Li", "Lijun Zhou", "Haiyang Sun", "Bing Wang", "Tong Lu"], "title": "CoGen: 3D Consistent Video Generation via Adaptive Conditioning for Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "Recent progress in driving video generation has shown significant potential\nfor enhancing self-driving systems by providing scalable and controllable\ntraining data. Although pretrained state-of-the-art generation models, guided\nby 2D layout conditions (e.g., HD maps and bounding boxes), can produce\nphotorealistic driving videos, achieving controllable multi-view videos with\nhigh 3D consistency remains a major challenge. To tackle this, we introduce a\nnovel spatial adaptive generation framework, CoGen, which leverages advances in\n3D generation to improve performance in two key aspects: (i) To ensure 3D\nconsistency, we first generate high-quality, controllable 3D conditions that\ncapture the geometry of driving scenes. By replacing coarse 2D conditions with\nthese fine-grained 3D representations, our approach significantly enhances the\nspatial consistency of the generated videos. (ii) Additionally, we introduce a\nconsistency adapter module to strengthen the robustness of the model to\nmulti-condition control. The results demonstrate that this method excels in\npreserving geometric fidelity and visual realism, offering a reliable video\ngeneration solution for autonomous driving.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u7a7a\u95f4\u81ea\u9002\u5e94\u751f\u6210\u6846\u67b6CoGen\uff0c\u4ee5\u89e3\u51b3\u591a\u89c6\u89d2\u9a7e\u9a76\u89c6\u9891\u751f\u6210\u4e2d\u76843D\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e2D\u5e03\u5c40\u6761\u4ef6\u7684\u751f\u6210\u6a21\u578b\u96be\u4ee5\u5b9e\u73b0\u9ad83D\u4e00\u81f4\u6027\u7684\u53ef\u63a7\u591a\u89c6\u89d2\u89c6\u9891\uff0c\u9650\u5236\u4e86\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "\u901a\u8fc7\u751f\u6210\u9ad8\u8d28\u91cf\u53ef\u63a7\u76843D\u6761\u4ef6\u66ff\u4ee32D\u6761\u4ef6\uff0c\u5e76\u5f15\u5165\u4e00\u81f4\u6027\u9002\u914d\u5668\u6a21\u5757\u589e\u5f3a\u6a21\u578b\u5bf9\u591a\u6761\u4ef6\u63a7\u5236\u7684\u9c81\u68d2\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u51e0\u4f55\u4fdd\u771f\u5ea6\u548c\u89c6\u89c9\u771f\u5b9e\u611f\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u89c6\u9891\u751f\u6210\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "CoGen\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u89c6\u9891\u76843D\u4e00\u81f4\u6027\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u8bad\u7ec3\u6570\u636e\u63d0\u4f9b\u4e86\u66f4\u9ad8\u8d28\u91cf\u7684\u53ef\u63a7\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2503.22237", "pdf": "https://arxiv.org/pdf/2503.22237", "abs": "https://arxiv.org/abs/2503.22237", "authors": ["Kunliang Liu", "Jianming Wang", "Rize Jin", "Wonjun Hwang", "Tae-Sun Chung"], "title": "SCHNet: SAM Marries CLIP for Human Parsing", "categories": ["cs.CV"], "comment": null, "summary": "Vision Foundation Model (VFM) such as the Segment Anything Model (SAM) and\nContrastive Language-Image Pre-training Model (CLIP) has shown promising\nperformance for segmentation and detection tasks. However, although SAM excels\nin fine-grained segmentation, it faces major challenges when applying it to\nsemantic-aware segmentation. While CLIP exhibits a strong semantic\nunderstanding capability via aligning the global features of language and\nvision, it has deficiencies in fine-grained segmentation tasks. Human parsing\nrequires to segment human bodies into constituent parts and involves both\naccurate fine-grained segmentation and high semantic understanding of each\npart. Based on traits of SAM and CLIP, we formulate high efficient modules to\neffectively integrate features of them to benefit human parsing. We propose a\nSemantic-Refinement Module to integrate semantic features of CLIP with SAM\nfeatures to benefit parsing. Moreover, we formulate a high efficient\nFine-tuning Module to adjust the pretrained SAM for human parsing that needs\nhigh semantic information and simultaneously demands spatial details, which\nsignificantly reduces the training time compared with full-time training and\nachieves notable performance. Extensive experiments demonstrate the\neffectiveness of our method on LIP, PPP, and CIHP databases.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u6a21\u5757\uff0c\u7ed3\u5408SAM\u548cCLIP\u7684\u7279\u5f81\u4ee5\u6539\u8fdb\u4eba\u4f53\u89e3\u6790\u4efb\u52a1\u3002", "motivation": "SAM\u5728\u7ec6\u7c92\u5ea6\u5206\u5272\u8868\u73b0\u4f18\u5f02\u4f46\u7f3a\u4e4f\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff0cCLIP\u5177\u6709\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u4f46\u7ec6\u7c92\u5ea6\u5206\u5272\u4e0d\u8db3\uff0c\u800c\u4eba\u4f53\u89e3\u6790\u9700\u8981\u4e24\u8005\u7ed3\u5408\u3002", "method": "\u8bbe\u8ba1\u8bed\u4e49\u7cbe\u70bc\u6a21\u5757\u6574\u5408CLIP\u7684\u8bed\u4e49\u7279\u5f81\u4e0eSAM\u7684\u7279\u5f81\uff0c\u5e76\u63d0\u51fa\u9ad8\u6548\u5fae\u8c03\u6a21\u5757\u8c03\u6574\u9884\u8bad\u7ec3\u7684SAM\u4ee5\u9002\u5e94\u4eba\u4f53\u89e3\u6790\u4efb\u52a1\u3002", "result": "\u5728LIP\u3001PPP\u548cCIHP\u6570\u636e\u5e93\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\u5e76\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "\u7ed3\u5408SAM\u548cCLIP\u7279\u5f81\u7684\u65b9\u6cd5\u5728\u4eba\u4f53\u89e3\u6790\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u517c\u987e\u4e86\u8bed\u4e49\u7406\u89e3\u548c\u7ec6\u7c92\u5ea6\u5206\u5272\u9700\u6c42\u3002"}}
{"id": "2503.22251", "pdf": "https://arxiv.org/pdf/2503.22251", "abs": "https://arxiv.org/abs/2503.22251", "authors": ["Guneet Mutreja", "Ksenia Bittner"], "title": "Efficient Building Roof Type Classification: A Domain-Specific Self-Supervised Approach", "categories": ["cs.CV"], "comment": null, "summary": "Accurate classification of building roof types from aerial imagery is crucial\nfor various remote sensing applications, including urban planning, disaster\nmanagement, and infrastructure monitoring. However, this task is often hindered\nby the limited availability of labeled data for supervised learning approaches.\nTo address this challenge, this paper investigates the effectiveness of self\nsupervised learning with EfficientNet architectures, known for their\ncomputational efficiency, for building roof type classification. We propose a\nnovel framework that incorporates a Convolutional Block Attention Module (CBAM)\nto enhance the feature extraction capabilities of EfficientNet. Furthermore, we\nexplore the benefits of pretraining on a domain-specific dataset, the Aerial\nImage Dataset (AID), compared to ImageNet pretraining. Our experimental results\ndemonstrate the superiority of our approach. Employing Simple Framework for\nContrastive Learning of Visual Representations (SimCLR) with EfficientNet-B3\nand CBAM achieves a 95.5% accuracy on our validation set, matching the\nperformance of state-of-the-art transformer-based models while utilizing\nsignificantly fewer parameters. We also provide a comprehensive evaluation on\ntwo challenging test sets, demonstrating the generalization capability of our\nmethod. Notably, our findings highlight the effectiveness of domain-specific\npretraining, consistently leading to higher accuracy compared to models\npretrained on the generic ImageNet dataset. Our work establishes EfficientNet\nbased self-supervised learning as a computationally efficient and highly\neffective approach for building roof type classification, particularly\nbeneficial in scenarios with limited labeled data.", "AI": {"task": "\u5229\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u548cEfficientNet\u67b6\u6784\u8fdb\u884c\u5efa\u7b51\u7269\u5c4b\u9876\u7c7b\u578b\u7684\u7cbe\u786e\u5206\u7c7b\u3002", "motivation": "\u89e3\u51b3\u5efa\u7b51\u7269\u5c4b\u9876\u7c7b\u578b\u5206\u7c7b\u4efb\u52a1\u4e2d\u6807\u8bb0\u6570\u636e\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5377\u79ef\u5757\u6ce8\u610f\u529b\u6a21\u5757\uff08CBAM\uff09\u7684EfficientNet\u6846\u67b6\uff0c\u5e76\u63a2\u7d22\u4e86\u5728\u9886\u57df\u7279\u5b9a\u6570\u636e\u96c6\uff08AID\uff09\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u7684\u6548\u679c\u3002", "result": "\u5728\u9a8c\u8bc1\u96c6\u4e0a\u8fbe\u523095.5%\u7684\u51c6\u786e\u7387\uff0c\u6027\u80fd\u4e0e\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u76f8\u5f53\uff0c\u4f46\u53c2\u6570\u66f4\u5c11\u3002", "conclusion": "\u57fa\u4e8eEfficientNet\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u662f\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u4e14\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6807\u8bb0\u6570\u636e\u6709\u9650\u7684\u573a\u666f\u3002"}}
{"id": "2503.22262", "pdf": "https://arxiv.org/pdf/2503.22262", "abs": "https://arxiv.org/abs/2503.22262", "authors": ["Songsong Yu", "Yuxin Chen", "Zhongang Qi", "Zeke Xie", "Yifan Wang", "Lijun Wang", "Ying Shan", "Huchuan Lu"], "title": "Mono2Stereo: A Benchmark and Empirical Study for Stereo Conversion", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025 Project webpage:\n  https://mono2stereo-bench.github.io/", "summary": "With the rapid proliferation of 3D devices and the shortage of 3D content,\nstereo conversion is attracting increasing attention. Recent works introduce\npretrained Diffusion Models (DMs) into this task. However, due to the scarcity\nof large-scale training data and comprehensive benchmarks, the optimal\nmethodologies for employing DMs in stereo conversion and the accurate\nevaluation of stereo effects remain largely unexplored. In this work, we\nintroduce the Mono2Stereo dataset, providing high-quality training data and\nbenchmark to support in-depth exploration of stereo conversion. With this\ndataset, we conduct an empirical study that yields two primary findings. 1) The\ndifferences between the left and right views are subtle, yet existing metrics\nconsider overall pixels, failing to concentrate on regions critical to stereo\neffects. 2) Mainstream methods adopt either one-stage left-to-right generation\nor warp-and-inpaint pipeline, facing challenges of degraded stereo effect and\nimage distortion respectively. Based on these findings, we introduce a new\nevaluation metric, Stereo Intersection-over-Union, which prioritizes disparity\nand achieves a high correlation with human judgments on stereo effect.\nMoreover, we propose a strong baseline model, harmonizing the stereo effect and\nimage quality simultaneously, and notably surpassing current mainstream\nmethods. Our code and data will be open-sourced to promote further research in\nstereo conversion. Our models are available at mono2stereo-bench.github.io.", "AI": {"task": "\u7814\u7a76\u5982\u4f55\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\uff08DMs\uff09\u8fdb\u884c\u7acb\u4f53\u8f6c\u6362\uff0c\u5e76\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u548c\u8bc4\u4f30\u65b9\u6cd5\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "3D\u8bbe\u5907\u5feb\u901f\u666e\u53ca\u4f463D\u5185\u5bb9\u77ed\u7f3a\uff0c\u7acb\u4f53\u8f6c\u6362\u9700\u6c42\u589e\u52a0\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u6570\u636e\u3001\u8bc4\u4f30\u548c\u6548\u679c\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faMono2Stereo\u6570\u636e\u96c6\uff0c\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u548c\u57fa\u7ebf\u6a21\u578b\u3002", "result": "\u63d0\u51faStereo Intersection-over-Union\u6307\u6807\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5f00\u6e90\u4ee3\u7801\u548c\u6570\u636e\u3002", "conclusion": "\u901a\u8fc7\u65b0\u6570\u636e\u96c6\u548c\u57fa\u7ebf\u6a21\u578b\uff0c\u63a8\u52a8\u4e86\u7acb\u4f53\u8f6c\u6362\u9886\u57df\u7684\u7814\u7a76\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002"}}
{"id": "2503.22265", "pdf": "https://arxiv.org/pdf/2503.22265", "abs": "https://arxiv.org/abs/2503.22265", "authors": ["Haomin Zhang", "Chang Liu", "Junjie Zheng", "Zihao Chen", "Chaofan Ding", "Xinhan Di"], "title": "DeepAudio-V1:Towards Multi-Modal Multi-Stage End-to-End Video to Speech and Audio Generation", "categories": ["cs.CV", "cs.SD", "eess.AS"], "comment": "11 pages, 5 figures", "summary": "Currently, high-quality, synchronized audio is synthesized using various\nmulti-modal joint learning frameworks, leveraging video and optional text\ninputs. In the video-to-audio benchmarks, video-to-audio quality, semantic\nalignment, and audio-visual synchronization are effectively achieved. However,\nin real-world scenarios, speech and audio often coexist in videos\nsimultaneously, and the end-to-end generation of synchronous speech and audio\ngiven video and text conditions are not well studied. Therefore, we propose an\nend-to-end multi-modal generation framework that simultaneously produces speech\nand audio based on video and text conditions. Furthermore, the advantages of\nvideo-to-audio (V2A) models for generating speech from videos remain unclear.\nThe proposed framework, DeepAudio, consists of a video-to-audio (V2A) module, a\ntext-to-speech (TTS) module, and a dynamic mixture of modality fusion (MoF)\nmodule. In the evaluation, the proposed end-to-end framework achieves\nstate-of-the-art performance on the video-audio benchmark, video-speech\nbenchmark, and text-speech benchmark. In detail, our framework achieves\ncomparable results in the comparison with state-of-the-art models for the\nvideo-audio and text-speech benchmarks, and surpassing state-of-the-art models\nin the video-speech benchmark, with WER 16.57% to 3.15% (+80.99%), SPK-SIM\n78.30% to 89.38% (+14.15%), EMO-SIM 66.24% to 75.56% (+14.07%), MCD 8.59 to\n7.98 (+7.10%), MCD SL 11.05 to 9.40 (+14.93%) across a variety of dubbing\nsettings.", "AI": {"task": "\u63d0\u51fa\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u591a\u6a21\u6001\u751f\u6210\u6846\u67b6\uff0c\u540c\u65f6\u57fa\u4e8e\u89c6\u9891\u548c\u6587\u672c\u6761\u4ef6\u751f\u6210\u8bed\u97f3\u548c\u97f3\u9891\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\u89c6\u9891\u901a\u5e38\u540c\u65f6\u5305\u542b\u8bed\u97f3\u548c\u97f3\u9891\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u672a\u5145\u5206\u63a2\u7d22\u57fa\u4e8e\u89c6\u9891\u548c\u6587\u672c\u6761\u4ef6\u7684\u540c\u6b65\u8bed\u97f3\u548c\u97f3\u9891\u751f\u6210\u3002", "method": "\u63d0\u51faDeepAudio\u6846\u67b6\uff0c\u5305\u542b\u89c6\u9891\u5230\u97f3\u9891\uff08V2A\uff09\u6a21\u5757\u3001\u6587\u672c\u5230\u8bed\u97f3\uff08TTS\uff09\u6a21\u5757\u548c\u52a8\u6001\u6a21\u6001\u878d\u5408\uff08MoF\uff09\u6a21\u5757\u3002", "result": "\u5728\u89c6\u9891-\u97f3\u9891\u3001\u89c6\u9891-\u8bed\u97f3\u548c\u6587\u672c-\u8bed\u97f3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5c24\u5176\u5728\u89c6\u9891-\u8bed\u97f3\u57fa\u51c6\u4e2d\u663e\u8457\u63d0\u5347\uff08\u5982WER\u964d\u4f4e80.99%\uff09\u3002", "conclusion": "DeepAudio\u6846\u67b6\u5728\u591a\u6a21\u6001\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u540c\u6b65\u8bed\u97f3\u548c\u97f3\u9891\u751f\u6210\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2503.22268", "pdf": "https://arxiv.org/pdf/2503.22268", "abs": "https://arxiv.org/abs/2503.22268", "authors": ["Nan Huang", "Wenzhao Zheng", "Chenfeng Xu", "Kurt Keutzer", "Shanghang Zhang", "Angjoo Kanazawa", "Qianqian Wang"], "title": "Segment Any Motion in Videos", "categories": ["cs.CV"], "comment": "CVPR 2025. Website: https://motion-seg.github.io/", "summary": "Moving object segmentation is a crucial task for achieving a high-level\nunderstanding of visual scenes and has numerous downstream applications. Humans\ncan effortlessly segment moving objects in videos. Previous work has largely\nrelied on optical flow to provide motion cues; however, this approach often\nresults in imperfect predictions due to challenges such as partial motion,\ncomplex deformations, motion blur and background distractions. We propose a\nnovel approach for moving object segmentation that combines long-range\ntrajectory motion cues with DINO-based semantic features and leverages SAM2 for\npixel-level mask densification through an iterative prompting strategy. Our\nmodel employs Spatio-Temporal Trajectory Attention and Motion-Semantic\nDecoupled Embedding to prioritize motion while integrating semantic support.\nExtensive testing on diverse datasets demonstrates state-of-the-art\nperformance, excelling in challenging scenarios and fine-grained segmentation\nof multiple objects. Our code is available at https://motion-seg.github.io/.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u957f\u7a0b\u8f68\u8ff9\u8fd0\u52a8\u7ebf\u7d22\u4e0eDINO\u8bed\u4e49\u7279\u5f81\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89c6\u9891\u4e2d\u7684\u8fd0\u52a8\u7269\u4f53\u5206\u5272\u3002", "motivation": "\u4eba\u7c7b\u80fd\u8f7b\u677e\u5206\u5272\u89c6\u9891\u4e2d\u7684\u8fd0\u52a8\u7269\u4f53\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5149\u6d41\uff0c\u5e38\u56e0\u90e8\u5206\u8fd0\u52a8\u3001\u590d\u6742\u53d8\u5f62\u3001\u8fd0\u52a8\u6a21\u7cca\u548c\u80cc\u666f\u5e72\u6270\u5bfc\u81f4\u9884\u6d4b\u4e0d\u5b8c\u7f8e\u3002", "method": "\u7ed3\u5408\u957f\u7a0b\u8f68\u8ff9\u8fd0\u52a8\u7ebf\u7d22\u4e0eDINO\u8bed\u4e49\u7279\u5f81\uff0c\u5229\u7528SAM2\u901a\u8fc7\u8fed\u4ee3\u63d0\u793a\u7b56\u7565\u8fdb\u884c\u50cf\u7d20\u7ea7\u63a9\u7801\u5bc6\u96c6\u5316\uff0c\u91c7\u7528\u65f6\u7a7a\u8f68\u8ff9\u6ce8\u610f\u529b\u548c\u8fd0\u52a8-\u8bed\u4e49\u89e3\u8026\u5d4c\u5165\u3002", "result": "\u5728\u591a\u6837\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u6311\u6218\u6027\u573a\u666f\u548c\u591a\u7269\u4f53\u7cbe\u7ec6\u5206\u5272\u4e2d\u8fbe\u5230\u9886\u5148\u6c34\u5e73\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u5728\u8fd0\u52a8\u7269\u4f53\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2503.22281", "pdf": "https://arxiv.org/pdf/2503.22281", "abs": "https://arxiv.org/abs/2503.22281", "authors": ["Xuan Loc Pham", "Mathias Prokop", "Bram van Ginneken", "Alessa Hering"], "title": "Divide to Conquer: A Field Decomposition Approach for Multi-Organ Whole-Body CT Image Registration", "categories": ["cs.CV"], "comment": null, "summary": "Image registration is an essential technique for the analysis of Computed\nTomography (CT) images in clinical practice. However, existing methodologies\nare predominantly tailored to a specific organ of interest and often exhibit\nlower performance on other organs, thus limiting their generalizability and\napplicability. Multi-organ registration addresses these limitations, but the\nsimultaneous alignment of multiple organs with diverse shapes, sizes and\nlocations requires a highly complex deformation field with a multi-layer\ncomposition of individual deformations. This study introduces a novel field\ndecomposition approach to address the high complexity of deformations in\nmulti-organ whole-body CT image registration. The proposed method is trained\nand evaluated on a longitudinal dataset of 691 patients, each with two CT\nimages obtained at distinct time points. These scans fully encompass the\nthoracic, abdominal, and pelvic regions. Two baseline registration methods are\nselected for this study: one based on optimization techniques and another based\non deep learning. Experimental results demonstrate that the proposed approach\noutperforms baseline methods in handling complex deformations in multi-organ\nwhole-body CT image registration.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u573a\u5206\u89e3\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u5668\u5b98\u5168\u8eabCT\u56fe\u50cf\u914d\u51c6\u4e2d\u53d8\u5f62\u7684\u9ad8\u590d\u6742\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9488\u5bf9\u7279\u5b9a\u5668\u5b98\u8bbe\u8ba1\uff0c\u5bf9\u5176\u4ed6\u5668\u5b98\u6027\u80fd\u8f83\u4f4e\uff0c\u9650\u5236\u4e86\u901a\u7528\u6027\u548c\u9002\u7528\u6027\u3002", "method": "\u91c7\u7528\u573a\u5206\u89e3\u65b9\u6cd5\u5904\u7406\u591a\u5668\u5b98\u7684\u590d\u6742\u53d8\u5f62\uff0c\u5e76\u5728691\u540d\u60a3\u8005\u7684\u7eb5\u5411CT\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u5904\u7406\u591a\u5668\u5b98\u5168\u8eabCT\u56fe\u50cf\u914d\u51c6\u7684\u590d\u6742\u53d8\u5f62\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u5668\u5b98\u5168\u8eabCT\u56fe\u50cf\u914d\u51c6\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6027\u80fd\uff0c\u5177\u6709\u66f4\u597d\u7684\u901a\u7528\u6027\u548c\u9002\u7528\u6027\u3002"}}
{"id": "2503.22285", "pdf": "https://arxiv.org/pdf/2503.22285", "abs": "https://arxiv.org/abs/2503.22285", "authors": ["Bin Zhang", "Jinggang Chen", "Xiaoyang Qu", "Guokuan Li", "Kai Lu", "Jiguang Wan", "Jing Xiao", "Jianzong Wang"], "title": "RUNA: Object-level Out-of-Distribution Detection via Regional Uncertainty Alignment of Multimodal Representations", "categories": ["cs.CV"], "comment": "9 pages, 5 figures", "summary": "Enabling object detectors to recognize out-of-distribution (OOD) objects is\nvital for building reliable systems. A primary obstacle stems from the fact\nthat models frequently do not receive supervisory signals from unfamiliar data,\nleading to overly confident predictions regarding OOD objects. Despite previous\nprogress that estimates OOD uncertainty based on the detection model and\nin-distribution (ID) samples, we explore using pre-trained vision-language\nrepresentations for object-level OOD detection. We first discuss the\nlimitations of applying image-level CLIP-based OOD detection methods to\nobject-level scenarios. Building upon these insights, we propose RUNA, a novel\nframework that leverages a dual encoder architecture to capture rich contextual\ninformation and employs a regional uncertainty alignment mechanism to\ndistinguish ID from OOD objects effectively. We introduce a few-shot\nfine-tuning approach that aligns region-level semantic representations to\nfurther improve the model's capability to discriminate between similar objects.\nOur experiments show that RUNA substantially surpasses state-of-the-art methods\nin object-level OOD detection, particularly in challenging scenarios with\ndiverse and complex object instances.", "AI": {"task": "\u63a2\u7d22\u5982\u4f55\u5229\u7528\u9884\u8bad\u7ec3\u7684\u89c6\u89c9-\u8bed\u8a00\u8868\u793a\u8fdb\u884c\u5bf9\u8c61\u7ea7\u522b\u7684\u5206\u5e03\u5916\uff08OOD\uff09\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u5206\u5e03\u5916\u5bf9\u8c61\u4e0a\u7f3a\u4e4f\u76d1\u7763\u4fe1\u53f7\uff0c\u5bfc\u81f4\u5bf9OOD\u5bf9\u8c61\u8fc7\u5ea6\u81ea\u4fe1\u7684\u9884\u6d4b\uff0c\u5f71\u54cd\u7cfb\u7edf\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faRUNA\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u7f16\u7801\u5668\u67b6\u6784\u6355\u83b7\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u533a\u57df\u4e0d\u786e\u5b9a\u6027\u5bf9\u9f50\u673a\u5236\u533a\u5206\u5206\u5e03\u5185\uff08ID\uff09\u548cOOD\u5bf9\u8c61\uff0c\u540c\u65f6\u5f15\u5165\u5c11\u6837\u672c\u5fae\u8c03\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRUNA\u5728\u5bf9\u8c61\u7ea7\u522bOOD\u68c0\u6d4b\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u590d\u6742\u573a\u666f\u4e0b\u3002", "conclusion": "RUNA\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9-\u8bed\u8a00\u8868\u793a\u548c\u533a\u57df\u5bf9\u9f50\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5bf9\u8c61\u7ea7\u522bOOD\u68c0\u6d4b\u7684\u6027\u80fd\u3002"}}
{"id": "2503.22291", "pdf": "https://arxiv.org/pdf/2503.22291", "abs": "https://arxiv.org/abs/2503.22291", "authors": ["Bin Zhang", "Xiaoyang Qu", "Guokuan Li", "Jiguang Wan", "Jianzong Wang"], "title": "VisTa: Visual-contextual and Text-augmented Zero-shot Object-level OOD Detection", "categories": ["cs.CV"], "comment": "5 pages, 4 figures", "summary": "As object detectors are increasingly deployed as black-box cloud services or\npre-trained models with restricted access to the original training data, the\nchallenge of zero-shot object-level out-of-distribution (OOD) detection arises.\nThis task becomes crucial in ensuring the reliability of detectors in\nopen-world settings. While existing methods have demonstrated success in\nimage-level OOD detection using pre-trained vision-language models like CLIP,\ndirectly applying such models to object-level OOD detection presents challenges\ndue to the loss of contextual information and reliance on image-level\nalignment. To tackle these challenges, we introduce a new method that leverages\nvisual prompts and text-augmented in-distribution (ID) space construction to\nadapt CLIP for zero-shot object-level OOD detection. Our method preserves\ncritical contextual information and improves the ability to differentiate\nbetween ID and OOD objects, achieving competitive performance across different\nbenchmarks.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u89c6\u89c9\u63d0\u793a\u548c\u6587\u672c\u589e\u5f3a\u7684\u5206\u5e03\u5185\uff08ID\uff09\u7a7a\u95f4\u6784\u5efa\uff0c\u5c06CLIP\u6a21\u578b\u9002\u914d\u4e8e\u96f6\u6837\u672c\u76ee\u6807\u7ea7\u5206\u5e03\u5916\uff08OOD\uff09\u68c0\u6d4b\u4efb\u52a1\u3002", "motivation": "\u7531\u4e8e\u76ee\u6807\u68c0\u6d4b\u5668\u8d8a\u6765\u8d8a\u591a\u5730\u4ee5\u9ed1\u76d2\u4e91\u670d\u52a1\u6216\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5f62\u5f0f\u90e8\u7f72\uff0c\u4e14\u65e0\u6cd5\u8bbf\u95ee\u539f\u59cb\u8bad\u7ec3\u6570\u636e\uff0c\u96f6\u6837\u672c\u76ee\u6807\u7ea7OOD\u68c0\u6d4b\u7684\u9700\u6c42\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u4ee5\u786e\u4fdd\u5728\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u53ef\u9760\u6027\u3002", "method": "\u901a\u8fc7\u89c6\u89c9\u63d0\u793a\u548c\u6587\u672c\u589e\u5f3a\u7684ID\u7a7a\u95f4\u6784\u5efa\uff0c\u4fdd\u7559\u5173\u952e\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5e76\u63d0\u5347\u533a\u5206ID\u548cOOD\u76ee\u6807\u7684\u80fd\u529b\u3002", "result": "\u5728\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u7ade\u4e89\u6027\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u76f4\u63a5\u5e94\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982CLIP\uff09\u5230\u76ee\u6807\u7ea7OOD\u68c0\u6d4b\u65f6\u7684\u6311\u6218\uff0c\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2503.22309", "pdf": "https://arxiv.org/pdf/2503.22309", "abs": "https://arxiv.org/abs/2503.22309", "authors": ["Zakaria Laskar", "Tomas Vojir", "Matej Grcic", "Iaroslav Melekhov", "Shankar Gangisettye", "Juho Kannala", "Jiri Matas", "Giorgos Tolias", "C. V. Jawahar"], "title": "A Dataset for Semantic Segmentation in the Presence of Unknowns", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Before deployment in the real-world deep neural networks require thorough\nevaluation of how they handle both knowns, inputs represented in the training\ndata, and unknowns (anomalies). This is especially important for scene\nunderstanding tasks with safety critical applications, such as in autonomous\ndriving. Existing datasets allow evaluation of only knowns or unknowns - but\nnot both, which is required to establish \"in the wild\" suitability of deep\nneural network models. To bridge this gap, we propose a novel anomaly\nsegmentation dataset, ISSU, that features a diverse set of anomaly inputs from\ncluttered real-world environments. The dataset is twice larger than existing\nanomaly segmentation datasets, and provides a training, validation and test set\nfor controlled in-domain evaluation. The test set consists of a static and\ntemporal part, with the latter comprised of videos. The dataset provides\nannotations for both closed-set (knowns) and anomalies, enabling closed-set and\nopen-set evaluation. The dataset covers diverse conditions, such as domain and\ncross-sensor shift, illumination variation and allows ablation of anomaly\ndetection methods with respect to these variations. Evaluation results of\ncurrent state-of-the-art methods confirm the need for improvements especially\nin domain-generalization, small and large object segmentation.", "AI": {"task": "\u63d0\u51fa\u4e00\u4e2a\u65b0\u9896\u7684\u5f02\u5e38\u5206\u5272\u6570\u636e\u96c6ISSU\uff0c\u7528\u4e8e\u8bc4\u4f30\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u5df2\u77e5\u548c\u672a\u77e5\u8f93\u5165\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u4ec5\u80fd\u8bc4\u4f30\u5df2\u77e5\u6216\u672a\u77e5\u8f93\u5165\uff0c\u65e0\u6cd5\u540c\u65f6\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u4e2d\u5bf9\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u201c\u91ce\u5916\u201d\u9002\u7528\u6027\u7684\u9700\u6c42\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u591a\u6837\u5316\u5f02\u5e38\u8f93\u5165\u7684ISSU\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u8bad\u7ec3\u3001\u9a8c\u8bc1\u548c\u6d4b\u8bd5\u96c6\uff0c\u652f\u6301\u95ed\u96c6\u548c\u5f00\u96c6\u8bc4\u4f30\u3002", "result": "ISSU\u6570\u636e\u96c6\u89c4\u6a21\u662f\u73b0\u6709\u6570\u636e\u96c6\u7684\u4e24\u500d\uff0c\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\u5f53\u524d\u65b9\u6cd5\u5728\u9886\u57df\u6cdb\u5316\u548c\u5c0f/\u5927\u7269\u4f53\u5206\u5272\u65b9\u9762\u4ecd\u9700\u6539\u8fdb\u3002", "conclusion": "ISSU\u6570\u636e\u96c6\u586b\u8865\u4e86\u73b0\u6709\u7a7a\u767d\uff0c\u4e3a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2503.22324", "pdf": "https://arxiv.org/pdf/2503.22324", "abs": "https://arxiv.org/abs/2503.22324", "authors": ["Chenyang Xu", "XingGuo Deng", "Rui Zhong"], "title": "AH-GS: Augmented 3D Gaussian Splatting for High-Frequency Detail Representation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The 3D Gaussian Splatting (3D-GS) is a novel method for scene representation\nand view synthesis. Although Scaffold-GS achieves higher quality real-time\nrendering compared to the original 3D-GS, its fine-grained rendering of the\nscene is extremely dependent on adequate viewing angles. The spectral bias of\nneural network learning results in Scaffold-GS's poor ability to perceive and\nlearn high-frequency information in the scene. In this work, we propose\nenhancing the manifold complexity of input features and using network-based\nfeature map loss to improve the image reconstruction quality of 3D-GS models.\nWe introduce AH-GS, which enables 3D Gaussians in structurally complex regions\nto obtain higher-frequency encodings, allowing the model to more effectively\nlearn the high-frequency information of the scene. Additionally, we incorporate\nhigh-frequency reinforce loss to further enhance the model's ability to capture\ndetailed frequency information. Our result demonstrates that our model\nsignificantly improves rendering fidelity, and in specific scenarios (e.g.,\nMipNeRf360-garden), our method exceeds the rendering quality of Scaffold-GS in\njust 15K iterations.", "AI": {"task": "\u63d0\u51faAH-GS\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u5f3a\u8f93\u5165\u7279\u5f81\u7684\u6d41\u5f62\u590d\u6742\u6027\u548c\u4f7f\u7528\u57fa\u4e8e\u7f51\u7edc\u7684\u7279\u5f81\u56fe\u635f\u5931\uff0c\u63d0\u53473D-GS\u6a21\u578b\u7684\u56fe\u50cf\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "Scaffold-GS\u5728\u7ec6\u7c92\u5ea6\u6e32\u67d3\u4e0a\u9ad8\u5ea6\u4f9d\u8d56\u89c6\u89d2\uff0c\u4e14\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u7684\u8c31\u504f\u7f6e\u5bfc\u81f4\u5176\u5bf9\u573a\u666f\u9ad8\u9891\u4fe1\u606f\u7684\u611f\u77e5\u548c\u5b66\u4e60\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u589e\u5f3a\u8f93\u5165\u7279\u5f81\u7684\u6d41\u5f62\u590d\u6742\u6027\uff0c\u5f15\u5165\u7f51\u7edc\u7279\u5f81\u56fe\u635f\u5931\u548c\u9ad8\u9891\u5f3a\u5316\u635f\u5931\uff0c\u4f7f3D\u9ad8\u65af\u5728\u7ed3\u6784\u590d\u6742\u533a\u57df\u83b7\u5f97\u66f4\u9ad8\u9891\u7f16\u7801\u3002", "result": "\u5728\u7279\u5b9a\u573a\u666f\uff08\u5982MipNeRf360-garden\uff09\u4e2d\uff0c\u4ec515K\u6b21\u8fed\u4ee3\u5373\u53ef\u8d85\u8d8aScaffold-GS\u7684\u6e32\u67d3\u8d28\u91cf\u3002", "conclusion": "AH-GS\u663e\u8457\u63d0\u5347\u4e86\u6e32\u67d3\u4fdd\u771f\u5ea6\uff0c\u5c24\u5176\u5728\u590d\u6742\u533a\u57df\u7684\u9ad8\u9891\u4fe1\u606f\u5b66\u4e60\u4e0a\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2503.22328", "pdf": "https://arxiv.org/pdf/2503.22328", "abs": "https://arxiv.org/abs/2503.22328", "authors": ["Yancong Lin", "Shiming Wang", "Liangliang Nan", "Julian Kooij", "Holger Caesar"], "title": "VoteFlow: Enforcing Local Rigidity in Self-Supervised Scene Flow", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR 2025. Code is available at\n  https://github.com/tudelft-iv/VoteFlow. Yancong Lin and Shiming Wang have\n  equal contributions", "summary": "Scene flow estimation aims to recover per-point motion from two adjacent\nLiDAR scans. However, in real-world applications such as autonomous driving,\npoints rarely move independently of others, especially for nearby points\nbelonging to the same object, which often share the same motion. Incorporating\nthis locally rigid motion constraint has been a key challenge in\nself-supervised scene flow estimation, which is often addressed by\npost-processing or appending extra regularization. While these approaches are\nable to improve the rigidity of predicted flows, they lack an architectural\ninductive bias for local rigidity within the model structure, leading to\nsuboptimal learning efficiency and inferior performance. In contrast, we\nenforce local rigidity with a lightweight add-on module in neural network\ndesign, enabling end-to-end learning. We design a discretized voting space that\naccommodates all possible translations and then identify the one shared by\nnearby points by differentiable voting. Additionally, to ensure computational\nefficiency, we operate on pillars rather than points and learn representative\nfeatures for voting per pillar. We plug the Voting Module into popular model\ndesigns and evaluate its benefit on Argoverse 2 and Waymo datasets. We\noutperform baseline works with only marginal compute overhead. Code is\navailable at https://github.com/tudelft-iv/VoteFlow.", "AI": {"task": "\u4ece\u4e24\u4e2a\u76f8\u90bb\u7684LiDAR\u626b\u63cf\u4e2d\u6062\u590d\u6bcf\u70b9\u8fd0\u52a8\u3002", "motivation": "\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\uff0c\u5982\u81ea\u52a8\u9a7e\u9a76\uff0c\u70b9\u5f88\u5c11\u72ec\u7acb\u79fb\u52a8\uff0c\u5c24\u5176\u662f\u5c5e\u4e8e\u540c\u4e00\u7269\u4f53\u7684\u90bb\u8fd1\u70b9\u901a\u5e38\u5171\u4eab\u76f8\u540c\u8fd0\u52a8\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u5c40\u90e8\u521a\u6027\u7684\u67b6\u6784\u5f52\u7eb3\u504f\u7f6e\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u6295\u7968\u6a21\u5757\uff0c\u901a\u8fc7\u79bb\u6563\u5316\u6295\u7968\u7a7a\u95f4\u548c\u53ef\u5fae\u5206\u6295\u7968\u5b9e\u73b0\u5c40\u90e8\u521a\u6027\u7ea6\u675f\uff0c\u5e76\u5728\u652f\u67f1\u4e0a\u64cd\u4f5c\u4ee5\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5728Argoverse 2\u548cWaymo\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u4ec5\u7565\u6709\u589e\u52a0\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u5c40\u90e8\u521a\u6027\u7ea6\u675f\u7684\u6295\u7968\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u5b66\u4e60\uff0c\u63d0\u5347\u4e86\u573a\u666f\u6d41\u4f30\u8ba1\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2503.22344", "pdf": "https://arxiv.org/pdf/2503.22344", "abs": "https://arxiv.org/abs/2503.22344", "authors": ["Huiang He", "Minghui Hu", "Chuanxia Zheng", "Chaoyue Wang", "Tat-Jen Cham"], "title": "Semantix: An Energy Guided Sampler for Semantic Style Transfer", "categories": ["cs.CV"], "comment": "28 pages, 19 figures, Accepted to ICLR 2025", "summary": "Recent advances in style and appearance transfer are impressive, but most\nmethods isolate global style and local appearance transfer, neglecting semantic\ncorrespondence. Additionally, image and video tasks are typically handled in\nisolation, with little focus on integrating them for video transfer. To address\nthese limitations, we introduce a novel task, Semantic Style Transfer, which\ninvolves transferring style and appearance features from a reference image to a\ntarget visual content based on semantic correspondence. We subsequently propose\na training-free method, Semantix an energy-guided sampler designed for Semantic\nStyle Transfer that simultaneously guides both style and appearance transfer\nbased on semantic understanding capacity of pre-trained diffusion models.\nAdditionally, as a sampler, Semantix be seamlessly applied to both image and\nvideo models, enabling semantic style transfer to be generic across various\nvisual media. Specifically, once inverting both reference and context images or\nvideos to noise space by SDEs, Semantix utilizes a meticulously crafted energy\nfunction to guide the sampling process, including three key components: Style\nFeature Guidance, Spatial Feature Guidance and Semantic Distance as a\nregularisation term. Experimental results demonstrate that Semantix not only\neffectively accomplishes the task of semantic style transfer across images and\nvideos, but also surpasses existing state-of-the-art solutions in both fields.\nThe project website is available at https://huiang-he.github.io/semantix/", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aSemantic Style Transfer\u7684\u65b0\u4efb\u52a1\uff0c\u901a\u8fc7\u8bed\u4e49\u5bf9\u5e94\u5c06\u53c2\u8003\u56fe\u50cf\u7684\u98ce\u683c\u548c\u5916\u89c2\u7279\u5f81\u8f6c\u79fb\u5230\u76ee\u6807\u89c6\u89c9\u5185\u5bb9\u4e2d\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u5168\u5c40\u98ce\u683c\u548c\u5c40\u90e8\u5916\u89c2\u8f6c\u79fb\u5206\u5f00\u5904\u7406\uff0c\u4e14\u5ffd\u89c6\u4e86\u8bed\u4e49\u5bf9\u5e94\uff1b\u540c\u65f6\uff0c\u56fe\u50cf\u548c\u89c6\u9891\u4efb\u52a1\u901a\u5e38\u5b64\u7acb\u5904\u7406\uff0c\u7f3a\u4e4f\u6574\u5408\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5Semantix\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u80fd\u91cf\u51fd\u6570\u5f15\u5bfc\u91c7\u6837\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSemantix\u4e0d\u4ec5\u6709\u6548\u5b8c\u6210\u8de8\u56fe\u50cf\u548c\u89c6\u9891\u7684\u8bed\u4e49\u98ce\u683c\u8f6c\u79fb\u4efb\u52a1\uff0c\u8fd8\u5728\u4e24\u4e2a\u9886\u57df\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "Semantix\u4e3a\u89c6\u89c9\u5a92\u4f53\u4e2d\u7684\u8bed\u4e49\u98ce\u683c\u8f6c\u79fb\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2503.22346", "pdf": "https://arxiv.org/pdf/2503.22346", "abs": "https://arxiv.org/abs/2503.22346", "authors": ["Ruifeng Luo", "Zhengjie Liu", "Tianxiao Cheng", "Jie Wang", "Tongjie Wang", "Xingguang Wei", "Haomin Wang", "YanPeng Li", "Fu Chai", "Fei Cheng", "Shenglong Ye", "Wenhai Wang", "Yanting Zhang", "Yu Qiao", "Hongjie Zhang", "Xianzhong Zhao"], "title": "ArchCAD-400K: An Open Large-Scale Architectural CAD Dataset and New Baseline for Panoptic Symbol Spotting", "categories": ["cs.CV"], "comment": null, "summary": "Recognizing symbols in architectural CAD drawings is critical for various\nadvanced engineering applications. In this paper, we propose a novel CAD data\nannotation engine that leverages intrinsic attributes from systematically\narchived CAD drawings to automatically generate high-quality annotations, thus\nsignificantly reducing manual labeling efforts. Utilizing this engine, we\nconstruct ArchCAD-400K, a large-scale CAD dataset consisting of 413,062 chunks\nfrom 5538 highly standardized drawings, making it over 26 times larger than the\nlargest existing CAD dataset. ArchCAD-400K boasts an extended drawing diversity\nand broader categories, offering line-grained annotations. Furthermore, we\npresent a new baseline model for panoptic symbol spotting, termed Dual-Pathway\nSymbol Spotter (DPSS). It incorporates an adaptive fusion module to enhance\nprimitive features with complementary image features, achieving\nstate-of-the-art performance and enhanced robustness. Extensive experiments\nvalidate the effectiveness of DPSS, demonstrating the value of ArchCAD-400K and\nits potential to drive innovation in architectural design and construction.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u65b0\u578bCAD\u6570\u636e\u6807\u6ce8\u5f15\u64ce\uff0c\u5e76\u6784\u5efa\u5927\u89c4\u6a21CAD\u6570\u636e\u96c6ArchCAD-400K\uff0c\u540c\u65f6\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5168\u666f\u7b26\u53f7\u8bc6\u522b\u57fa\u7ebf\u6a21\u578bDPSS\u3002", "motivation": "\u51cf\u5c11\u5efa\u7b51CAD\u56fe\u7eb8\u4e2d\u7b26\u53f7\u8bc6\u522b\u7684\u624b\u52a8\u6807\u6ce8\u5de5\u4f5c\uff0c\u63a8\u52a8\u5efa\u7b51\u8bbe\u8ba1\u4e0e\u65bd\u5de5\u7684\u521b\u65b0\u3002", "method": "\u5229\u7528CAD\u56fe\u7eb8\u7684\u5185\u5728\u5c5e\u6027\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u6807\u6ce8\uff0c\u6784\u5efaArchCAD-400K\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51faDPSS\u6a21\u578b\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u878d\u5408\u6a21\u5757\u589e\u5f3a\u7279\u5f81\u3002", "result": "ArchCAD-400K\u662f\u76ee\u524d\u6700\u5927\u89c4\u6a21\u7684CAD\u6570\u636e\u96c6\uff0cDPSS\u6a21\u578b\u5728\u7b26\u53f7\u8bc6\u522b\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "ArchCAD-400K\u548cDPSS\u6a21\u578b\u4e3a\u5efa\u7b51CAD\u56fe\u7eb8\u7684\u7b26\u53f7\u8bc6\u522b\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\uff0c\u5177\u6709\u63a8\u52a8\u884c\u4e1a\u521b\u65b0\u7684\u6f5c\u529b\u3002"}}
{"id": "2503.22349", "pdf": "https://arxiv.org/pdf/2503.22349", "abs": "https://arxiv.org/abs/2503.22349", "authors": ["Li-Heng Chen", "Zi-Xin Zou", "Chang Liu", "Tianjiao Jing", "Yan-Pei Cao", "Shi-Sheng Huang", "Hongbo Fu", "Hua Huang"], "title": "GCRayDiffusion: Pose-Free Surface Reconstruction via Geometric Consistent Ray Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "Accurate surface reconstruction from unposed images is crucial for efficient\n3D object or scene creation. However, it remains challenging, particularly for\nthe joint camera pose estimation. Previous approaches have achieved impressive\npose-free surface reconstruction results in dense-view settings, but could\neasily fail for sparse-view scenarios without sufficient visual overlap. In\nthis paper, we propose a new technique for pose-free surface reconstruction,\nwhich follows triplane-based signed distance field (SDF) learning but\nregularizes the learning by explicit points sampled from ray-based diffusion of\ncamera pose estimation. Our key contribution is a novel Geometric Consistent\nRay Diffusion model (GCRayDiffusion), where we represent camera poses as neural\nbundle rays and regress the distribution of noisy rays via a diffusion model.\nMore importantly, we further condition the denoising process of RGRayDiffusion\nusing the triplane-based SDF of the entire scene, which provides effective 3D\nconsistent regularization to achieve multi-view consistent camera pose\nestimation. Finally, we incorporate RGRayDiffusion into the triplane-based SDF\nlearning by introducing on-surface geometric regularization from the sampling\npoints of the neural bundle rays, which leads to highly accurate pose-free\nsurface reconstruction results even for sparse-view inputs. Extensive\nevaluations on public datasets show that our GCRayDiffusion achieves more\naccurate camera pose estimation than previous approaches, with geometrically\nmore consistent surface reconstruction results, especially given sparse-view\ninputs.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u65e0\u59ff\u6001\u8868\u9762\u91cd\u5efa\u6280\u672f\uff0c\u901a\u8fc7\u51e0\u4f55\u4e00\u81f4\u7684\u5c04\u7ebf\u6269\u6563\u6a21\u578b\uff08GCRayDiffusion\uff09\u5b9e\u73b0\u7a00\u758f\u89c6\u89d2\u4e0b\u7684\u9ad8\u7cbe\u5ea6\u91cd\u5efa\u3002", "motivation": "\u89e3\u51b3\u7a00\u758f\u89c6\u89d2\u4e0b\u65e0\u59ff\u6001\u8868\u9762\u91cd\u5efa\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u7684\u8054\u5408\u4f18\u5316\u95ee\u9898\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u4e09\u5e73\u9762\u7b26\u53f7\u8ddd\u79bb\u573a\uff08SDF\uff09\u7684\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5c04\u7ebf\u6269\u6563\u6a21\u578b\u5bf9\u76f8\u673a\u59ff\u6001\u8fdb\u884c\u6b63\u5219\u5316\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86GCRayDiffusion\u5728\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u548c\u8868\u9762\u91cd\u5efa\u4e0a\u7684\u9ad8\u7cbe\u5ea6\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u7a00\u758f\u89c6\u89d2\u8f93\u5165\u4e0b\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u51c6\u786e\u7684\u59ff\u6001\u4f30\u8ba1\u548c\u8868\u9762\u91cd\u5efa\u3002"}}
{"id": "2503.22351", "pdf": "https://arxiv.org/pdf/2503.22351", "abs": "https://arxiv.org/abs/2503.22351", "authors": ["Byeongjun Kwon", "Munchurl Kim"], "title": "One Look is Enough: A Novel Seamless Patchwise Refinement for Zero-Shot Monocular Depth Estimation Models on High-Resolution Images", "categories": ["cs.CV"], "comment": "Please visit our project page this\n  https://kaist-viclab.github.io/One-Look-is-Enough_site", "summary": "Zero-shot depth estimation (DE) models exhibit strong generalization\nperformance as they are trained on large-scale datasets. However, existing\nmodels struggle with high-resolution images due to the discrepancy in image\nresolutions of training (with smaller resolutions) and inference (for high\nresolutions). Processing them at full resolution leads to decreased estimation\naccuracy on depth with tremendous memory consumption, while downsampling to the\ntraining resolution results in blurred edges in the estimated depth images.\nPrevailing high-resolution depth estimation methods adopt a patch-based\napproach, which introduces depth discontinuity issues when reassembling the\nestimated depth patches and results in test-time inefficiency. Additionally, to\nobtain fine-grained depth details, these methods rely on synthetic datasets due\nto the real-world sparse ground truth depth, leading to poor generalizability.\nTo tackle these limitations, we propose Patch Refine Once (PRO), an efficient\nand generalizable tile-based framework. Our PRO consists of two key components:\n(i) Grouped Patch Consistency Training that enhances test-time efficiency while\nmitigating the depth discontinuity problem by jointly processing four\noverlapping patches and enforcing a consistency loss on their overlapping\nregions within a single backpropagation step, and (ii) Bias Free Masking that\nprevents the DE models from overfitting to dataset-specific biases, enabling\nbetter generalization to real-world datasets even after training on synthetic\ndata. Zero-shot evaluation on Booster, ETH3D, Middlebury 2014, and NuScenes\ndemonstrates into which our PRO can be well harmonized, making their DE\ncapabilities still effective for the grid input of high-resolution images with\nlittle depth discontinuities at the grid boundaries. Our PRO runs fast at\ninference time.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u7684\u57fa\u4e8e\u5206\u5757\u7684\u6846\u67b6\uff08PRO\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u96f6\u6837\u672c\u6df1\u5ea6\u4f30\u8ba1\u4e2d\u7684\u6df1\u5ea6\u4e0d\u8fde\u7eed\u6027\u548c\u6cdb\u5316\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u65f6\u5b58\u5728\u6df1\u5ea6\u4e0d\u8fde\u7eed\u3001\u5185\u5b58\u6d88\u8017\u5927\u3001\u6cdb\u5316\u6027\u5dee\u7b49\u95ee\u9898\uff0c\u4e14\u4f9d\u8d56\u5408\u6210\u6570\u636e\u96c6\u3002", "method": "PRO\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u5206\u7ec4\u5757\u4e00\u81f4\u6027\u8bad\u7ec3\u548c\u504f\u5dee\u81ea\u7531\u63a9\u7801\uff0c\u5206\u522b\u89e3\u51b3\u6df1\u5ea6\u4e0d\u8fde\u7eed\u6027\u548c\u6cdb\u5316\u6027\u95ee\u9898\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u96f6\u6837\u672c\u8bc4\u4f30\u8868\u660e\uff0cPRO\u80fd\u591f\u6709\u6548\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u51cf\u5c11\u6df1\u5ea6\u4e0d\u8fde\u7eed\u6027\uff0c\u5e76\u4fdd\u6301\u9ad8\u6548\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "PRO\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u6cdb\u5316\u6027\u5f3a\u7684\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u96f6\u6837\u672c\u6df1\u5ea6\u4f30\u8ba1\u3002"}}
{"id": "2503.22352", "pdf": "https://arxiv.org/pdf/2503.22352", "abs": "https://arxiv.org/abs/2503.22352", "authors": ["Bar\u0131\u015f Batuhan Topal", "Umut \u00d6zyurt", "Zafer Do\u011fan Budak", "Ramazan Gokberk Cinbis"], "title": "Meta-LoRA: Meta-Learning LoRA Components for Domain-Aware ID Personalization", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in text-to-image generative models, particularly latent\ndiffusion models (LDMs), have demonstrated remarkable capabilities in\nsynthesizing high-quality images from textual prompts. However, achieving\nidentity personalization-ensuring that a model consistently generates\nsubject-specific outputs from limited reference images-remains a fundamental\nchallenge. To address this, we introduce Meta-Low-Rank Adaptation (Meta-LoRA),\na novel framework that leverages meta-learning to encode domain-specific priors\ninto LoRA-based identity personalization. Our method introduces a structured\nthree-layer LoRA architecture that separates identity-agnostic knowledge from\nidentity-specific adaptation. In the first stage, the LoRA Meta-Down layers are\nmeta-trained across multiple subjects, learning a shared manifold that captures\ngeneral identity-related features. In the second stage, only the LoRA-Mid and\nLoRA-Up layers are optimized to specialize on a given subject, significantly\nreducing adaptation time while improving identity fidelity. To evaluate our\napproach, we introduce Meta-PHD, a new benchmark dataset for identity\npersonalization, and compare Meta-LoRA against state-of-the-art methods. Our\nresults demonstrate that Meta-LoRA achieves superior identity retention,\ncomputational efficiency, and adaptability across diverse identity conditions.\nThe code, model weights, and dataset will be released publicly upon acceptance.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aMeta-LoRA\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5143\u5b66\u4e60\u5c06\u9886\u57df\u7279\u5b9a\u5148\u9a8c\u7f16\u7801\u5230\u57fa\u4e8eLoRA\u7684\u8eab\u4efd\u4e2a\u6027\u5316\u4e2d\uff0c\u4ee5\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e2d\u7684\u8eab\u4efd\u4e2a\u6027\u5316\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u8eab\u4efd\u4e2a\u6027\u5316\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u5373\u5982\u4f55\u4ece\u6709\u9650\u7684\u53c2\u8003\u56fe\u50cf\u4e2d\u751f\u6210\u7279\u5b9a\u4e3b\u9898\u7684\u4e00\u81f4\u8f93\u51fa\u3002", "method": "\u91c7\u7528\u4e09\u5c42LoRA\u67b6\u6784\uff0c\u5206\u79bb\u8eab\u4efd\u65e0\u5173\u77e5\u8bc6\u4e0e\u8eab\u4efd\u7279\u5b9a\u9002\u5e94\uff0c\u901a\u8fc7\u5143\u5b66\u4e60\u8bad\u7ec3\u5171\u4eab\u6d41\u5f62\uff0c\u518d\u4f18\u5316\u7279\u5b9a\u5c42\u4ee5\u9002\u5e94\u65b0\u4e3b\u9898\u3002", "result": "Meta-LoRA\u5728\u8eab\u4efd\u4fdd\u7559\u3001\u8ba1\u7b97\u6548\u7387\u548c\u9002\u5e94\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u7684\u57fa\u51c6\u6570\u636e\u96c6Meta-PHD\u3002", "conclusion": "Meta-LoRA\u4e3a\u89e3\u51b3\u8eab\u4efd\u4e2a\u6027\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u516c\u5f00\u3002"}}
{"id": "2503.22357", "pdf": "https://arxiv.org/pdf/2503.22357", "abs": "https://arxiv.org/abs/2503.22357", "authors": ["Hadrien Reynaud", "Alberto Gomez", "Paul Leeson", "Qingjie Meng", "Bernhard Kainz"], "title": "EchoFlow: A Foundation Model for Cardiac Ultrasound Image and Video Generation", "categories": ["cs.CV"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Advances in deep learning have significantly enhanced medical image analysis,\nyet the availability of large-scale medical datasets remains constrained by\npatient privacy concerns. We present EchoFlow, a novel framework designed to\ngenerate high-quality, privacy-preserving synthetic echocardiogram images and\nvideos. EchoFlow comprises four key components: an adversarial variational\nautoencoder for defining an efficient latent representation of cardiac\nultrasound images, a latent image flow matching model for generating accurate\nlatent echocardiogram images, a latent re-identification model to ensure\nprivacy by filtering images anatomically, and a latent video flow matching\nmodel for animating latent images into realistic echocardiogram videos\nconditioned on ejection fraction. We rigorously evaluate our synthetic datasets\non the clinically relevant task of ejection fraction regression and\ndemonstrate, for the first time, that downstream models trained exclusively on\nEchoFlow-generated synthetic datasets achieve performance parity with models\ntrained on real datasets. We release our models and synthetic datasets,\nenabling broader, privacy-compliant research in medical ultrasound imaging at\nhttps://huggingface.co/spaces/HReynaud/EchoFlow.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aEchoFlow\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u4fdd\u62a4\u9690\u79c1\u7684\u5408\u6210\u8d85\u58f0\u5fc3\u52a8\u56fe\u56fe\u50cf\u548c\u89c6\u9891\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u56e0\u60a3\u8005\u9690\u79c1\u95ee\u9898\u5bfc\u81f4\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u53d7\u9650\u7684\u6311\u6218\u3002", "method": "\u7ed3\u5408\u5bf9\u6297\u53d8\u5206\u81ea\u7f16\u7801\u5668\u3001\u6f5c\u5728\u56fe\u50cf\u6d41\u5339\u914d\u6a21\u578b\u3001\u6f5c\u5728\u91cd\u8bc6\u522b\u6a21\u578b\u548c\u6f5c\u5728\u89c6\u9891\u6d41\u5339\u914d\u6a21\u578b\uff0c\u751f\u6210\u5408\u6210\u6570\u636e\u3002", "result": "\u5408\u6210\u6570\u636e\u96c6\u5728\u5c04\u8840\u5206\u6570\u56de\u5f52\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0e\u771f\u5b9e\u6570\u636e\u96c6\u76f8\u5f53\u3002", "conclusion": "EchoFlow\u4e3a\u533b\u5b66\u8d85\u58f0\u6210\u50cf\u7814\u7a76\u63d0\u4f9b\u4e86\u9690\u79c1\u5408\u89c4\u7684\u5927\u89c4\u6a21\u6570\u636e\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2503.22359", "pdf": "https://arxiv.org/pdf/2503.22359", "abs": "https://arxiv.org/abs/2503.22359", "authors": ["Jiahao Xia", "Min Xu", "Wenjian Huang", "Jianguo Zhang", "Haimin Zhang", "Chunxia Xiao"], "title": "Mitigating Knowledge Discrepancies among Multiple Datasets for Task-agnostic Unified Face Alignment", "categories": ["cs.CV"], "comment": "24 Pages, 9 Figures", "summary": "Despite the similar structures of human faces, existing face alignment\nmethods cannot learn unified knowledge from multiple datasets with different\nlandmark annotations. The limited training samples in a single dataset commonly\nresult in fragile robustness in this field. To mitigate knowledge discrepancies\namong different datasets and train a task-agnostic unified face alignment\n(TUFA) framework, this paper presents a strategy to unify knowledge from\nmultiple datasets. Specifically, we calculate a mean face shape for each\ndataset. To explicitly align these mean shapes on an interpretable plane based\non their semantics, each shape is then incorporated with a group of semantic\nalignment embeddings. The 2D coordinates of these aligned shapes can be viewed\nas the anchors of the plane. By encoding them into structure prompts and\nfurther regressing the corresponding facial landmarks using image features, a\nmapping from the plane to the target faces is finally established, which\nunifies the learning target of different datasets. Consequently, multiple\ndatasets can be utilized to boost the generalization ability of the model. The\nsuccessful mitigation of discrepancies also enhances the efficiency of\nknowledge transferring to a novel dataset, significantly boosts the performance\nof few-shot face alignment. Additionally, the interpretable plane endows TUFA\nwith a task-agnostic characteristic, enabling it to locate landmarks unseen\nduring training in a zero-shot manner. Extensive experiments are carried on\nseven benchmarks and the results demonstrate an impressive improvement in face\nalignment brought by knowledge discrepancies mitigation.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u4efb\u52a1\u65e0\u5173\u7684\u7edf\u4e00\u4eba\u8138\u5bf9\u9f50\uff08TUFA\uff09\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u591a\u6570\u636e\u96c6\u6807\u6ce8\u5dee\u5f02\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u4ece\u5177\u6709\u4e0d\u540c\u6807\u6ce8\u7684\u591a\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u7edf\u4e00\u77e5\u8bc6\uff0c\u4e14\u5355\u6570\u636e\u96c6\u7684\u6709\u9650\u6837\u672c\u5bfc\u81f4\u6a21\u578b\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u8ba1\u7b97\u6bcf\u4e2a\u6570\u636e\u96c6\u7684\u5e73\u5747\u4eba\u8138\u5f62\u72b6\uff0c\u5e76\u7ed3\u5408\u8bed\u4e49\u5bf9\u9f50\u5d4c\u5165\uff0c\u5c06\u8fd9\u4e9b\u5f62\u72b6\u5bf9\u9f50\u5230\u53ef\u89e3\u91ca\u5e73\u9762\u4e0a\uff0c\u6700\u7ec8\u901a\u8fc7\u7ed3\u6784\u63d0\u793a\u548c\u56fe\u50cf\u7279\u5f81\u56de\u5f52\u76ee\u6807\u4eba\u8138\u5173\u952e\u70b9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u4eba\u8138\u5bf9\u9f50\u6027\u80fd\uff0c\u5e76\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5c11\u6837\u672c\u5b66\u4e60\u6548\u7387\u3002", "conclusion": "TUFA\u6846\u67b6\u6210\u529f\u7f13\u89e3\u4e86\u77e5\u8bc6\u5dee\u5f02\uff0c\u5b9e\u73b0\u4e86\u4efb\u52a1\u65e0\u5173\u7684\u96f6\u6837\u672c\u5173\u952e\u70b9\u5b9a\u4f4d\u80fd\u529b\u3002"}}
{"id": "2503.22363", "pdf": "https://arxiv.org/pdf/2503.22363", "abs": "https://arxiv.org/abs/2503.22363", "authors": ["Nandakishor M", "Vrinda Govind V", "Anuradha Puthalath", "Anzy L", "Swathi P S", "Aswathi R", "Devaprabha A R", "Varsha Raj", "Midhuna Krishnan K", "Akhila Anilkumar T V", "Yamuna P V"], "title": "ForcePose: A Deep Learning Approach for Force Calculation Based on Action Recognition Using MediaPipe Pose Estimation Combined with Object Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Force estimation in human-object interactions is crucial for various fields\nlike ergonomics, physical therapy, and sports science. Traditional methods\ndepend on specialized equipment such as force plates and sensors, which makes\naccurate assessments both expensive and restricted to laboratory settings. In\nthis paper, we introduce ForcePose, a novel deep learning framework that\nestimates applied forces by combining human pose estimation with object\ndetection. Our approach leverages MediaPipe for skeletal tracking and SSD\nMobileNet for object recognition to create a unified representation of\nhuman-object interaction. We've developed a specialized neural network that\nprocesses both spatial and temporal features to predict force magnitude and\ndirection without needing any physical sensors. After training on our dataset\nof 850 annotated videos with corresponding force measurements, our model\nachieves a mean absolute error of 5.83 N in force magnitude and 7.4 degrees in\nforce direction. When compared to existing computer vision approaches, our\nmethod performs 27.5% better while still offering real-time performance on\nstandard computing hardware. ForcePose opens up new possibilities for force\nanalysis in diverse real-world scenarios where traditional measurement tools\nare impractical or intrusive. This paper discusses our methodology, the dataset\ncreation process, evaluation metrics, and potential applications across\nrehabilitation, ergonomics assessment, and athletic performance analysis.", "AI": {"task": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aForcePose\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u548c\u7269\u4f53\u68c0\u6d4b\u6765\u4f30\u8ba1\u4eba-\u7269\u4ea4\u4e92\u4e2d\u7684\u529b\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u4e13\u4e1a\u8bbe\u5907\u4e14\u5c40\u9650\u4e8e\u5b9e\u9a8c\u5ba4\u73af\u5883\uff0cForcePose\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u65e0\u9700\u7269\u7406\u4f20\u611f\u5668\u7684\u5b9e\u65f6\u529b\u5206\u6790\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528MediaPipe\u8fdb\u884c\u9aa8\u9abc\u8ddf\u8e2a\u548cSSD MobileNet\u8fdb\u884c\u7269\u4f53\u8bc6\u522b\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5904\u7406\u65f6\u7a7a\u7279\u5f81\u7684\u795e\u7ecf\u7f51\u7edc\u6765\u9884\u6d4b\u529b\u7684\u5927\u5c0f\u548c\u65b9\u5411\u3002", "result": "\u5728850\u4e2a\u6807\u6ce8\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u540e\uff0c\u6a21\u578b\u5728\u529b\u5927\u5c0f\u548c\u65b9\u5411\u4e0a\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u5206\u522b\u4e3a5.83 N\u548c7.4\u5ea6\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u8ba1\u7b97\u673a\u89c6\u89c9\u65b9\u6cd527.5%\u3002", "conclusion": "ForcePose\u4e3a\u5eb7\u590d\u3001\u4eba\u4f53\u5de5\u7a0b\u5b66\u8bc4\u4f30\u548c\u8fd0\u52a8\u8868\u73b0\u5206\u6790\u7b49\u5b9e\u9645\u573a\u666f\u63d0\u4f9b\u4e86\u65b0\u7684\u529b\u5206\u6790\u53ef\u80fd\u6027\u3002"}}
{"id": "2503.22374", "pdf": "https://arxiv.org/pdf/2503.22374", "abs": "https://arxiv.org/abs/2503.22374", "authors": ["Giulio Federico", "Giuseppe Amato", "Fabio Carrara", "Claudio Gennaro", "Marco Di Benedetto"], "title": "ViSketch-GPT: Collaborative Multi-Scale Feature Extraction for Sketch Recognition and Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Understanding the nature of human sketches is challenging because of the wide\nvariation in how they are created. Recognizing complex structural patterns\nimproves both the accuracy in recognizing sketches and the fidelity of the\ngenerated sketches. In this work, we introduce ViSketch-GPT, a novel algorithm\ndesigned to address these challenges through a multi-scale context extraction\napproach. The model captures intricate details at multiple scales and combines\nthem using an ensemble-like mechanism, where the extracted features work\ncollaboratively to enhance the recognition and generation of key details\ncrucial for classification and generation tasks.\n  The effectiveness of ViSketch-GPT is validated through extensive experiments\non the QuickDraw dataset. Our model establishes a new benchmark, significantly\noutperforming existing methods in both classification and generation tasks,\nwith substantial improvements in accuracy and the fidelity of generated\nsketches.\n  The proposed algorithm offers a robust framework for understanding complex\nstructures by extracting features that collaborate to recognize intricate\ndetails, enhancing the understanding of structures like sketches and making it\na versatile tool for various applications in computer vision and machine\nlearning.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aViSketch-GPT\u7684\u65b0\u7b97\u6cd5\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u4e0a\u4e0b\u6587\u63d0\u53d6\u65b9\u6cd5\u63d0\u9ad8\u8349\u56fe\u8bc6\u522b\u548c\u751f\u6210\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u7531\u4e8e\u4eba\u7c7b\u8349\u56fe\u7684\u521b\u5efa\u65b9\u5f0f\u5dee\u5f02\u5f88\u5927\uff0c\u7406\u89e3\u5176\u672c\u8d28\u5177\u6709\u6311\u6218\u6027\uff0c\u8bc6\u522b\u590d\u6742\u7ed3\u6784\u6a21\u5f0f\u53ef\u4ee5\u63d0\u9ad8\u8349\u56fe\u8bc6\u522b\u548c\u751f\u6210\u7684\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u591a\u5c3a\u5ea6\u4e0a\u4e0b\u6587\u63d0\u53d6\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210\u673a\u5236\u7ed3\u5408\u4e0d\u540c\u5c3a\u5ea6\u7684\u7279\u5f81\uff0c\u589e\u5f3a\u5173\u952e\u7ec6\u8282\u7684\u8bc6\u522b\u548c\u751f\u6210\u3002", "result": "\u5728QuickDraw\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cViSketch-GPT\u5728\u5206\u7c7b\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u51c6\u786e\u6027\u548c\u751f\u6210\u8349\u56fe\u7684\u4fdd\u771f\u5ea6\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "ViSketch-GPT\u901a\u8fc7\u534f\u4f5c\u63d0\u53d6\u7279\u5f81\uff0c\u4e3a\u7406\u89e3\u590d\u6742\u7ed3\u6784\uff08\u5982\u8349\u56fe\uff09\u63d0\u4f9b\u4e86\u5f3a\u5927\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u7684\u591a\u79cd\u5e94\u7528\u3002"}}
{"id": "2503.22375", "pdf": "https://arxiv.org/pdf/2503.22375", "abs": "https://arxiv.org/abs/2503.22375", "authors": ["Christian Steinhauser", "Philipp Reis", "Hubert Padusinski", "Jacob Langner", "Eric Sax"], "title": "Data Quality Matters: Quantifying Image Quality Impact on Machine Learning Performance", "categories": ["cs.CV", "eess.IV"], "comment": "Submitted to IEEE IV 2025, Under Review", "summary": "Precise perception of the environment is essential in highly automated\ndriving systems, which rely on machine learning tasks such as object detection\nand segmentation. Compression of sensor data is commonly used for data\nhandling, while virtualization is used for hardware-in-the-loop validation.\nBoth methods can alter sensor data and degrade model performance. This\nnecessitates a systematic approach to quantifying image validity. This paper\npresents a four-step framework to evaluate the impact of image modifications on\nmachine learning tasks. First, a dataset with modified images is prepared to\nensure one-to-one matching image pairs, enabling measurement of deviations\nresulting from compression and virtualization. Second, image deviations are\nquantified by comparing the effects of compression and virtualization against\noriginal camera-based sensor data. Third, the performance of state-of-the-art\nobject detection models is analyzed to determine how altered input data affects\nperception tasks, including bounding box accuracy and reliability. Finally, a\ncorrelation analysis is performed to identify relationships between image\nquality and model performance. As a result, the LPIPS metric achieves the\nhighest correlation between image deviation and machine learning performance\nacross all evaluated machine learning tasks.", "AI": {"task": "\u63d0\u51fa\u4e00\u4e2a\u56db\u6b65\u6846\u67b6\uff0c\u8bc4\u4f30\u56fe\u50cf\u4fee\u6539\u5bf9\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u7684\u5f71\u54cd\u3002", "motivation": "\u9ad8\u5ea6\u81ea\u52a8\u5316\u9a7e\u9a76\u7cfb\u7edf\u4f9d\u8d56\u7cbe\u786e\u7684\u73af\u5883\u611f\u77e5\uff0c\u800c\u6570\u636e\u538b\u7f29\u548c\u865a\u62df\u5316\u53ef\u80fd\u6539\u53d8\u4f20\u611f\u5668\u6570\u636e\u5e76\u964d\u4f4e\u6a21\u578b\u6027\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u65b9\u6cd5\u91cf\u5316\u56fe\u50cf\u6709\u6548\u6027\u3002", "method": "\u51c6\u5907\u4fee\u6539\u540e\u7684\u6570\u636e\u96c6\uff0c\u91cf\u5316\u56fe\u50cf\u504f\u5dee\uff0c\u5206\u6790\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u8fdb\u884c\u76f8\u5173\u6027\u5206\u6790\u3002", "result": "LPIPS\u6307\u6807\u5728\u6240\u6709\u8bc4\u4f30\u7684\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\uff0c\u56fe\u50cf\u504f\u5dee\u4e0e\u6a21\u578b\u6027\u80fd\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u6700\u9ad8\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u6709\u6548\u8bc4\u4f30\u56fe\u50cf\u4fee\u6539\u5bf9\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u7684\u5f71\u54cd\uff0cLPIPS\u662f\u6700\u4f73\u76f8\u5173\u6027\u6307\u6807\u3002"}}
{"id": "2503.22394", "pdf": "https://arxiv.org/pdf/2503.22394", "abs": "https://arxiv.org/abs/2503.22394", "authors": ["Rulin Zhou", "Wenlong He", "An Wang", "Qiqi Yao", "Haijun Hu", "Jiankun Wang", "Xi Zhang an Hongliang Ren"], "title": "Endo-TTAP: Robust Endoscopic Tissue Tracking via Multi-Facet Guided Attention and Hybrid Flow-point Supervision", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate tissue point tracking in endoscopic videos is critical for\nrobotic-assisted surgical navigation and scene understanding, but remains\nchallenging due to complex deformations, instrument occlusion, and the scarcity\nof dense trajectory annotations. Existing methods struggle with long-term\ntracking under these conditions due to limited feature utilization and\nannotation dependence. We present Endo-TTAP, a novel framework addressing these\nchallenges through: (1) A Multi-Facet Guided Attention (MFGA) module that\nsynergizes multi-scale flow dynamics, DINOv2 semantic embeddings, and explicit\nmotion patterns to jointly predict point positions with uncertainty and\nocclusion awareness; (2) A two-stage curriculum learning strategy employing an\nAuxiliary Curriculum Adapter (ACA) for progressive initialization and hybrid\nsupervision. Stage I utilizes synthetic data with optical flow ground truth for\nuncertainty-occlusion regularization, while Stage II combines unsupervised flow\nconsistency and semi-supervised learning with refined pseudo-labels from\noff-the-shelf trackers. Extensive validation on two MICCAI Challenge datasets\nand our collected dataset demonstrates that Endo-TTAP achieves state-of-the-art\nperformance in tissue point tracking, particularly in scenarios characterized\nby complex endoscopic conditions. The source code and dataset will be available\nat https://anonymous.4open.science/r/Endo-TTAP-36E5.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aEndo-TTAP\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5185\u7aa5\u955c\u89c6\u9891\u4e2d\u7ec4\u7ec7\u70b9\u8ddf\u8e2a\u7684\u6311\u6218\u3002", "motivation": "\u7531\u4e8e\u590d\u6742\u7684\u53d8\u5f62\u3001\u5668\u68b0\u906e\u6321\u548c\u5bc6\u96c6\u8f68\u8ff9\u6807\u6ce8\u7684\u7a00\u7f3a\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u671f\u8ddf\u8e2a\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u7ed3\u5408\u591a\u5c3a\u5ea6\u6d41\u52a8\u6001\u3001DINOv2\u8bed\u4e49\u5d4c\u5165\u548c\u663e\u5f0f\u8fd0\u52a8\u6a21\u5f0f\u7684\u591a\u9762\u5f15\u5bfc\u6ce8\u610f\u529b\u6a21\u5757\uff08MFGA\uff09\uff0c\u4ee5\u53ca\u4e24\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff08ACA\uff09\u3002", "result": "\u5728\u4e24\u4e2aMICCAI\u6311\u6218\u6570\u636e\u96c6\u548c\u81ea\u6536\u96c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cEndo-TTAP\u5728\u590d\u6742\u5185\u7aa5\u955c\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "Endo-TTAP\u901a\u8fc7\u591a\u6a21\u6001\u7279\u5f81\u548c\u6e10\u8fdb\u5b66\u4e60\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ec4\u7ec7\u70b9\u8ddf\u8e2a\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2503.22397", "pdf": "https://arxiv.org/pdf/2503.22397", "abs": "https://arxiv.org/abs/2503.22397", "authors": ["Vida Adeli", "Soroush Mehraban", "Majid Mirmehdi", "Alan Whone", "Benjamin Filtjens", "Amirhossein Dadashzadeh", "Alfonso Fasano", "Andrea Iaboni Babak Taati"], "title": "GAITGen: Disentangled Motion-Pathology Impaired Gait Generative Model -- Bringing Motion Generation to the Clinical Domain", "categories": ["cs.CV"], "comment": null, "summary": "Gait analysis is crucial for the diagnosis and monitoring of movement\ndisorders like Parkinson's Disease. While computer vision models have shown\npotential for objectively evaluating parkinsonian gait, their effectiveness is\nlimited by scarce clinical datasets and the challenge of collecting large and\nwell-labelled data, impacting model accuracy and risk of bias. To address these\ngaps, we propose GAITGen, a novel framework that generates realistic gait\nsequences conditioned on specified pathology severity levels. GAITGen employs a\nConditional Residual Vector Quantized Variational Autoencoder to learn\ndisentangled representations of motion dynamics and pathology-specific factors,\ncoupled with Mask and Residual Transformers for conditioned sequence\ngeneration. GAITGen generates realistic, diverse gait sequences across severity\nlevels, enriching datasets and enabling large-scale model training in\nparkinsonian gait analysis. Experiments on our new PD-GaM (real) dataset\ndemonstrate that GAITGen outperforms adapted state-of-the-art models in both\nreconstruction fidelity and generation quality, accurately capturing critical\npathology-specific gait features. A clinical user study confirms the realism\nand clinical relevance of our generated sequences. Moreover, incorporating\nGAITGen-generated data into downstream tasks improves parkinsonian gait\nseverity estimation, highlighting its potential for advancing clinical gait\nanalysis.", "AI": {"task": "\u63d0\u51faGAITGen\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u57fa\u4e8e\u75c5\u7406\u4e25\u91cd\u7a0b\u5ea6\u7684\u771f\u5b9e\u6b65\u6001\u5e8f\u5217\u3002", "motivation": "\u89e3\u51b3\u4e34\u5e8a\u6570\u636e\u96c6\u7a00\u7f3a\u548c\u6807\u6ce8\u56f0\u96be\u5bf9\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u5728\u5e15\u91d1\u68ee\u75c5\u6b65\u6001\u5206\u6790\u4e2d\u51c6\u786e\u6027\u548c\u504f\u501a\u98ce\u9669\u7684\u9650\u5236\u3002", "method": "\u4f7f\u7528\u6761\u4ef6\u6b8b\u5dee\u5411\u91cf\u91cf\u5316\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u8fd0\u52a8\u52a8\u6001\u548c\u75c5\u7406\u7279\u5b9a\u56e0\u7d20\u7684\u89e3\u8026\u8868\u793a\uff0c\u5e76\u7ed3\u5408\u63a9\u7801\u548c\u6b8b\u5dee\u53d8\u6362\u5668\u751f\u6210\u6761\u4ef6\u5e8f\u5217\u3002", "result": "GAITGen\u5728\u91cd\u5efa\u4fdd\u771f\u5ea6\u548c\u751f\u6210\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u80fd\u51c6\u786e\u6355\u6349\u75c5\u7406\u7279\u5f02\u6027\u6b65\u6001\u7279\u5f81\u3002", "conclusion": "GAITGen\u751f\u6210\u7684\u6570\u636e\u80fd\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5177\u6709\u63a8\u52a8\u4e34\u5e8a\u6b65\u6001\u5206\u6790\u7684\u6f5c\u529b\u3002"}}
{"id": "2503.22398", "pdf": "https://arxiv.org/pdf/2503.22398", "abs": "https://arxiv.org/abs/2503.22398", "authors": ["David Fischinger", "Martin Boyer"], "title": "DF-Net: The Digital Forensics Network for Image Forgery Detection", "categories": ["cs.CV"], "comment": "Published in 2023 at the 25th Irish Machine Vision and Image\n  Processing Conference (IMVIP),\n  https://iprcs.github.io/pdf/IMVIP2023_Proceeding.pdf", "summary": "The orchestrated manipulation of public opinion, particularly through\nmanipulated images, often spread via online social networks (OSN), has become a\nserious threat to society. In this paper we introduce the Digital Forensics Net\n(DF-Net), a deep neural network for pixel-wise image forgery detection. The\nreleased model outperforms several state-of-the-art methods on four established\nbenchmark datasets. Most notably, DF-Net's detection is robust against lossy\nimage operations (e.g resizing, compression) as they are automatically\nperformed by social networks.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u50cf\u7d20\u7ea7\u56fe\u50cf\u4f2a\u9020\u68c0\u6d4b\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edcDF-Net\u3002", "motivation": "\u5728\u7ebf\u793e\u4ea4\u7f51\u7edc\u4e2d\u4f20\u64ad\u7684\u64cd\u7eb5\u56fe\u50cf\u5bf9\u516c\u4f17\u8206\u8bba\u7684\u64cd\u63a7\u5df2\u6210\u4e3a\u4e25\u91cd\u7684\u793e\u4f1a\u5a01\u80c1\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edcDF-Net\u8fdb\u884c\u50cf\u7d20\u7ea7\u56fe\u50cf\u4f2a\u9020\u68c0\u6d4b\u3002", "result": "DF-Net\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u591a\u79cd\u5148\u8fdb\u65b9\u6cd5\uff0c\u4e14\u5bf9\u793e\u4ea4\u7f51\u7edc\u81ea\u52a8\u6267\u884c\u7684\u6709\u635f\u56fe\u50cf\u64cd\u4f5c\uff08\u5982\u8c03\u6574\u5927\u5c0f\u3001\u538b\u7f29\uff09\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "DF-Net\u662f\u4e00\u79cd\u6709\u6548\u7684\u56fe\u50cf\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u793e\u4ea4\u7f51\u7edc\u73af\u5883\u3002"}}
{"id": "2503.22399", "pdf": "https://arxiv.org/pdf/2503.22399", "abs": "https://arxiv.org/abs/2503.22399", "authors": ["Ada Gorgun", "Bernt Schiele", "Jonas Fischer"], "title": "VITAL: More Understandable Feature Visualization through Distribution Alignment and Relevant Information Flow", "categories": ["cs.CV"], "comment": "Code is available at: https://github.com/adagorgun/VITAL", "summary": "Neural networks are widely adopted to solve complex and challenging tasks.\nEspecially in high-stakes decision-making, understanding their reasoning\nprocess is crucial, yet proves challenging for modern deep networks. Feature\nvisualization (FV) is a powerful tool to decode what information neurons are\nresponding to and hence to better understand the reasoning behind such\nnetworks. In particular, in FV we generate human-understandable images that\nreflect the information detected by neurons of interest. However, current\nmethods often yield unrecognizable visualizations, exhibiting repetitive\npatterns and visual artifacts that are hard to understand for a human. To\naddress these problems, we propose to guide FV through statistics of real image\nfeatures combined with measures of relevant network flow to generate\nprototypical images. Our approach yields human-understandable visualizations\nthat both qualitatively and quantitatively improve over state-of-the-art FVs\nacross various architectures. As such, it can be used to decode which\ninformation the network uses, complementing mechanistic circuits that identify\nwhere it is encoded. Code is available at: https://github.com/adagorgun/VITAL", "AI": {"task": "\u901a\u8fc7\u7ed3\u5408\u771f\u5b9e\u56fe\u50cf\u7279\u5f81\u7684\u7edf\u8ba1\u6570\u636e\u548c\u76f8\u5173\u7f51\u7edc\u6d41\u7684\u6d4b\u91cf\uff0c\u6539\u8fdb\u7279\u5f81\u53ef\u89c6\u5316\u65b9\u6cd5\u4ee5\u751f\u6210\u66f4\u6613\u7406\u89e3\u7684\u56fe\u50cf\u3002", "motivation": "\u73b0\u4ee3\u6df1\u5ea6\u7f51\u7edc\u5728\u9ad8\u98ce\u9669\u51b3\u7b56\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u63a8\u7406\u8fc7\u7a0b\u96be\u4ee5\u7406\u89e3\uff0c\u73b0\u6709\u7279\u5f81\u53ef\u89c6\u5316\u65b9\u6cd5\u751f\u6210\u7684\u56fe\u50cf\u5f80\u5f80\u96be\u4ee5\u8bc6\u522b\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u7ed3\u5408\u771f\u5b9e\u56fe\u50cf\u7279\u5f81\u7684\u7edf\u8ba1\u6570\u636e\u548c\u7f51\u7edc\u6d41\u6d4b\u91cf\u6765\u5f15\u5bfc\u7279\u5f81\u53ef\u89c6\u5316\u7684\u65b9\u6cd5\u3002", "result": "\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u56fe\u50cf\u5728\u8d28\u91cf\u548c\u6570\u91cf\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u80fd\u591f\u66f4\u597d\u5730\u89e3\u7801\u7f51\u7edc\u4f7f\u7528\u7684\u4fe1\u606f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u7684\u63a8\u7406\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u5de5\u5177\uff0c\u8865\u5145\u4e86\u73b0\u6709\u673a\u5236\u7535\u8def\u7684\u4e0d\u8db3\u3002"}}
{"id": "2503.22405", "pdf": "https://arxiv.org/pdf/2503.22405", "abs": "https://arxiv.org/abs/2503.22405", "authors": ["Wei-Jin Huang", "Yuan-Ming Li", "Zhi-Wei Xia", "Yu-Ming Tang", "Kun-Yu Lin", "Jian-Fang Hu", "Wei-Shi Zheng"], "title": "Modeling Multiple Normal Action Representations for Error Detection in Procedural Tasks", "categories": ["cs.CV"], "comment": null, "summary": "Error detection in procedural activities is essential for consistent and\ncorrect outcomes in AR-assisted and robotic systems. Existing methods often\nfocus on temporal ordering errors or rely on static prototypes to represent\nnormal actions. However, these approaches typically overlook the common\nscenario where multiple, distinct actions are valid following a given sequence\nof executed actions. This leads to two issues: (1) the model cannot effectively\ndetect errors using static prototypes when the inference environment or action\nexecution distribution differs from training; and (2) the model may also use\nthe wrong prototypes to detect errors if the ongoing action label is not the\nsame as the predicted one. To address this problem, we propose an Adaptive\nMultiple Normal Action Representation (AMNAR) framework. AMNAR predicts all\nvalid next actions and reconstructs their corresponding normal action\nrepresentations, which are compared against the ongoing action to detect\nerrors. Extensive experiments demonstrate that AMNAR achieves state-of-the-art\nperformance, highlighting the effectiveness of AMNAR and the importance of\nmodeling multiple valid next actions in error detection. The code is available\nat https://github.com/iSEE-Laboratory/AMNAR.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u81ea\u9002\u5e94\u591a\u6b63\u5e38\u52a8\u4f5c\u8868\u793a\uff08AMNAR\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u7a0b\u5e8f\u6027\u6d3b\u52a8\u4e2d\u7684\u9519\u8bef\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5ffd\u7565\u591a\u52a8\u4f5c\u6709\u6548\u6027\u573a\u666f\uff0c\u5bfc\u81f4\u5728\u8bad\u7ec3\u4e0e\u63a8\u65ad\u73af\u5883\u4e0d\u4e00\u81f4\u65f6\u65e0\u6cd5\u6709\u6548\u68c0\u6d4b\u9519\u8bef\u3002", "method": "AMNAR\u9884\u6d4b\u6240\u6709\u6709\u6548\u4e0b\u4e00\u52a8\u4f5c\u5e76\u91cd\u5efa\u5176\u6b63\u5e38\u52a8\u4f5c\u8868\u793a\uff0c\u4e0e\u5f53\u524d\u52a8\u4f5c\u6bd4\u8f83\u4ee5\u68c0\u6d4b\u9519\u8bef\u3002", "result": "\u5b9e\u9a8c\u8868\u660eAMNAR\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "AMNAR\u6846\u67b6\u901a\u8fc7\u5efa\u6a21\u591a\u6709\u6548\u4e0b\u4e00\u52a8\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9519\u8bef\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2503.22417", "pdf": "https://arxiv.org/pdf/2503.22417", "abs": "https://arxiv.org/abs/2503.22417", "authors": ["David Fischinger", "Martin Boyer"], "title": "DF2023: The Digital Forensics 2023 Dataset for Image Forgery Detection", "categories": ["cs.CV"], "comment": "Published at the 25th Irish Machine Vision and Image Processing\n  Conference (IMVIP) --- Proceedings:\n  https://iprcs.github.io/pdf/IMVIP2023_Proceeding.pdf --- Dataset download:\n  https://zenodo.org/records/7326540/files/DF2023_train.zip\n  https://zenodo.org/records/7326540/files/DF2023_val.zip Kaggle:\n  https://www.kaggle.com/datasets/davidfischinger/df2023-digital-forensics-2023-dataset/data", "summary": "The deliberate manipulation of public opinion, especially through altered\nimages, which are frequently disseminated through online social networks, poses\na significant danger to society. To fight this issue on a technical level we\nsupport the research community by releasing the Digital Forensics 2023 (DF2023)\ntraining and validation dataset, comprising one million images from four major\nforgery categories: splicing, copy-move, enhancement and removal. This dataset\nenables an objective comparison of network architectures and can significantly\nreduce the time and effort of researchers preparing datasets.", "AI": {"task": "\u53d1\u5e03DF2023\u6570\u636e\u96c6\u4ee5\u652f\u6301\u68c0\u6d4b\u4f2a\u9020\u56fe\u50cf\u7684\u7814\u7a76\u3002", "motivation": "\u901a\u8fc7\u4f2a\u9020\u56fe\u50cf\u64cd\u7eb5\u516c\u4f17\u8206\u8bba\u5bf9\u793e\u4f1a\u6784\u6210\u91cd\u5927\u5a01\u80c1\u3002", "method": "\u63d0\u4f9b\u5305\u542b\u56db\u79cd\u4e3b\u8981\u4f2a\u9020\u7c7b\u522b\u7684\u767e\u4e07\u5f20\u56fe\u50cf\u6570\u636e\u96c6\u3002", "result": "\u6570\u636e\u96c6\u53ef\u51cf\u5c11\u7814\u7a76\u8005\u51c6\u5907\u6570\u636e\u7684\u65f6\u95f4\u548c\u7cbe\u529b\u3002", "conclusion": "DF2023\u6570\u636e\u96c6\u6709\u52a9\u4e8e\u63a8\u52a8\u4f2a\u9020\u56fe\u50cf\u68c0\u6d4b\u6280\u672f\u7684\u7814\u7a76\u3002"}}
{"id": "2503.22420", "pdf": "https://arxiv.org/pdf/2503.22420", "abs": "https://arxiv.org/abs/2503.22420", "authors": ["Jiangyong Huang", "Baoxiong Jia", "Yan Wang", "Ziyu Zhu", "Xiongkun Linghu", "Qing Li", "Song-Chun Zhu", "Siyuan Huang"], "title": "Unveiling the Mist over 3D Vision-Language Understanding: Object-centric Evaluation with Chain-of-Analysis", "categories": ["cs.CV"], "comment": "CVPR 2025. Project page: https://beacon-3d.github.io", "summary": "Existing 3D vision-language (3D-VL) benchmarks fall short in evaluating 3D-VL\nmodels, creating a \"mist\" that obscures rigorous insights into model\ncapabilities and 3D-VL tasks. This mist persists due to three key limitations.\nFirst, flawed test data, like ambiguous referential text in the grounding task,\ncan yield incorrect and unreliable test results. Second, oversimplified metrics\nsuch as simply averaging accuracy per question answering (QA) pair, cannot\nreveal true model capability due to their vulnerability to language variations.\nThird, existing benchmarks isolate the grounding and QA tasks, disregarding the\nunderlying coherence that QA should be based on solid grounding capabilities.\nTo unveil the \"mist\", we propose Beacon3D, a benchmark for 3D-VL grounding and\nQA tasks, delivering a perspective shift in the evaluation of 3D-VL\nunderstanding. Beacon3D features (i) high-quality test data with precise and\nnatural language, (ii) object-centric evaluation with multiple tests per object\nto ensure robustness, and (iii) a novel chain-of-analysis paradigm to address\nlanguage robustness and model performance coherence across grounding and QA.\nOur evaluation of state-of-the-art 3D-VL models on Beacon3D reveals that (i)\nobject-centric evaluation elicits true model performance and particularly weak\ngeneralization in QA; (ii) grounding-QA coherence remains fragile in current\n3D-VL models, and (iii) incorporating large language models (LLMs) to 3D-VL\nmodels, though as a prevalent practice, hinders grounding capabilities and has\nyet to elevate QA capabilities. We hope Beacon3D and our comprehensive analysis\ncould benefit the 3D-VL community towards faithful developments.", "AI": {"task": "\u63d0\u51faBeacon3D\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f303D\u89c6\u89c9\u8bed\u8a00\uff083D-VL\uff09\u6a21\u578b\u5728\u5b9a\u4f4d\u548c\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u67093D-VL\u57fa\u51c6\u5b58\u5728\u6d4b\u8bd5\u6570\u636e\u7f3a\u9677\u3001\u7b80\u5316\u6307\u6807\u548c\u4efb\u52a1\u9694\u79bb\u95ee\u9898\uff0c\u5bfc\u81f4\u8bc4\u4f30\u4e0d\u51c6\u786e\u3002", "method": "\u8bbe\u8ba1\u9ad8\u8d28\u91cf\u6d4b\u8bd5\u6570\u636e\u3001\u5bf9\u8c61\u4e2d\u5fc3\u8bc4\u4f30\u548c\u94fe\u5f0f\u5206\u6790\u8303\u5f0f\uff0c\u4ee5\u63d0\u5347\u8bc4\u4f30\u7684\u9c81\u68d2\u6027\u548c\u4e00\u81f4\u6027\u3002", "result": "\u53d1\u73b0\u5bf9\u8c61\u4e2d\u5fc3\u8bc4\u4f30\u63ed\u793a\u771f\u5b9e\u6a21\u578b\u6027\u80fd\uff0c\u5b9a\u4f4d\u4e0e\u95ee\u7b54\u4e00\u81f4\u6027\u8106\u5f31\uff0cLLM\u5f15\u5165\u5bf9\u5b9a\u4f4d\u80fd\u529b\u6709\u8d1f\u9762\u5f71\u54cd\u3002", "conclusion": "Beacon3D\u4e3a3D-VL\u793e\u533a\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u63a8\u52a8\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2503.22430", "pdf": "https://arxiv.org/pdf/2503.22430", "abs": "https://arxiv.org/abs/2503.22430", "authors": ["Sergio Izquierdo", "Mohamed Sayed", "Michael Firman", "Guillermo Garcia-Hernando", "Daniyar Turmukhambetov", "Javier Civera", "Oisin Mac Aodha", "Gabriel Brostow", "Jamie Watson"], "title": "MVSAnywhere: Zero-Shot Multi-View Stereo", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Computing accurate depth from multiple views is a fundamental and\nlongstanding challenge in computer vision. However, most existing approaches do\nnot generalize well across different domains and scene types (e.g. indoor vs.\noutdoor). Training a general-purpose multi-view stereo model is challenging and\nraises several questions, e.g. how to best make use of transformer-based\narchitectures, how to incorporate additional metadata when there is a variable\nnumber of input views, and how to estimate the range of valid depths which can\nvary considerably across different scenes and is typically not known a priori?\nTo address these issues, we introduce MVSA, a novel and versatile Multi-View\nStereo architecture that aims to work Anywhere by generalizing across diverse\ndomains and depth ranges. MVSA combines monocular and multi-view cues with an\nadaptive cost volume to deal with scale-related issues. We demonstrate\nstate-of-the-art zero-shot depth estimation on the Robust Multi-View Depth\nBenchmark, surpassing existing multi-view stereo and monocular baselines.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u901a\u7528\u7684\u591a\u89c6\u56fe\u7acb\u4f53\u89c6\u89c9\u67b6\u6784MVSA\uff0c\u65e8\u5728\u89e3\u51b3\u8de8\u9886\u57df\u548c\u573a\u666f\u7c7b\u578b\u7684\u6df1\u5ea6\u4f30\u8ba1\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8de8\u9886\u57df\u548c\u573a\u666f\u7c7b\u578b\uff08\u5982\u5ba4\u5185\u4e0e\u5ba4\u5916\uff09\u7684\u6df1\u5ea6\u4f30\u8ba1\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u8bad\u7ec3\u901a\u7528\u6a21\u578b\u9762\u4e34\u67b6\u6784\u8bbe\u8ba1\u3001\u8f93\u5165\u89c6\u56fe\u6570\u91cf\u53ef\u53d8\u6027\u53ca\u6df1\u5ea6\u8303\u56f4\u4f30\u8ba1\u7b49\u6311\u6218\u3002", "method": "\u7ed3\u5408\u5355\u76ee\u548c\u591a\u89c6\u56fe\u7ebf\u7d22\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u6210\u672c\u4f53\u79ef\u5904\u7406\u5c3a\u5ea6\u95ee\u9898\uff0c\u8bbe\u8ba1MVSA\u67b6\u6784\u3002", "result": "\u5728Robust Multi-View Depth Benchmark\u4e0a\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u6df1\u5ea6\u4f30\u8ba1\u7684\u5148\u8fdb\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u591a\u89c6\u56fe\u7acb\u4f53\u548c\u5355\u76ee\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MVSA\u901a\u8fc7\u7ed3\u5408\u591a\u79cd\u7ebf\u7d22\u548c\u81ea\u9002\u5e94\u8bbe\u8ba1\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u8de8\u9886\u57df\u548c\u573a\u666f\u7c7b\u578b\u7684\u9ad8\u6548\u6df1\u5ea6\u4f30\u8ba1\u3002"}}
{"id": "2503.22436", "pdf": "https://arxiv.org/pdf/2503.22436", "abs": "https://arxiv.org/abs/2503.22436", "authors": ["Fuhao Li", "Huan Jin", "Bin Gao", "Liaoyuan Fan", "Lihui Jiang", "Long Zeng"], "title": "NuGrounding: A Multi-View 3D Visual Grounding Framework in Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "Multi-view 3D visual grounding is critical for autonomous driving vehicles to\ninterpret natural languages and localize target objects in complex\nenvironments. However, existing datasets and methods suffer from coarse-grained\nlanguage instructions, and inadequate integration of 3D geometric reasoning\nwith linguistic comprehension. To this end, we introduce NuGrounding, the first\nlarge-scale benchmark for multi-view 3D visual grounding in autonomous driving.\nWe present a Hierarchy of Grounding (HoG) method to construct NuGrounding to\ngenerate hierarchical multi-level instructions, ensuring comprehensive coverage\nof human instruction patterns. To tackle this challenging dataset, we propose a\nnovel paradigm that seamlessly combines instruction comprehension abilities of\nmulti-modal LLMs (MLLMs) with precise localization abilities of specialist\ndetection models. Our approach introduces two decoupled task tokens and a\ncontext query to aggregate 3D geometric information and semantic instructions,\nfollowed by a fusion decoder to refine spatial-semantic feature fusion for\nprecise localization. Extensive experiments demonstrate that our method\nsignificantly outperforms the baselines adapted from representative 3D scene\nunderstanding methods by a significant margin and achieves 0.59 in precision\nand 0.64 in recall, with improvements of 50.8% and 54.7%.", "AI": {"task": "\u63d0\u51faNuGrounding\u57fa\u51c6\u548cHoG\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u89c6\u89d23D\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u5728\u8bed\u8a00\u6307\u4ee4\u7684\u7ec6\u7c92\u5ea6\u548c3D\u51e0\u4f55\u63a8\u7406\u4e0e\u8bed\u8a00\u7406\u89e3\u7684\u7ed3\u5408\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408\u591a\u6a21\u6001LLMs\u7684\u6307\u4ee4\u7406\u89e3\u80fd\u529b\u548c\u68c0\u6d4b\u6a21\u578b\u7684\u7cbe\u786e\u5b9a\u4f4d\u80fd\u529b\uff0c\u5f15\u5165\u89e3\u8026\u4efb\u52a1\u4ee4\u724c\u548c\u4e0a\u4e0b\u6587\u67e5\u8be2\uff0c\u901a\u8fc7\u878d\u5408\u89e3\u7801\u5668\u4f18\u5316\u7a7a\u95f4-\u8bed\u4e49\u7279\u5f81\u878d\u5408\u3002", "result": "\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u5206\u522b\u8fbe\u52300.59\u548c0.64\uff0c\u63d0\u534750.8%\u548c54.7%\u3002", "conclusion": "NuGrounding\u548cHoG\u65b9\u6cd5\u5728\u591a\u89c6\u89d23D\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2503.22437", "pdf": "https://arxiv.org/pdf/2503.22437", "abs": "https://arxiv.org/abs/2503.22437", "authors": ["Xu Wang", "Shuai Zhang", "Baoru Huang", "Danail Stoyanov", "Evangelos B. Mazomenos"], "title": "EndoLRMGS: Complete Endoscopic Scene Reconstruction combining Large Reconstruction Modelling and Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Complete reconstruction of surgical scenes is crucial for robot-assisted\nsurgery (RAS). Deep depth estimation is promising but existing works struggle\nwith depth discontinuities, resulting in noisy predictions at object boundaries\nand do not achieve complete reconstruction omitting occluded surfaces. To\naddress these issues we propose EndoLRMGS, that combines Large Reconstruction\nModelling (LRM) and Gaussian Splatting (GS), for complete surgical scene\nreconstruction. GS reconstructs deformable tissues and LRM generates 3D models\nfor surgical tools while position and scale are subsequently optimized by\nintroducing orthogonal perspective joint projection optimization (OPjPO) to\nenhance accuracy. In experiments on four surgical videos from three public\ndatasets, our method improves the Intersection-over-union (IoU) of tool 3D\nmodels in 2D projections by>40%. Additionally, EndoLRMGS improves the PSNR of\nthe tools projection from 3.82% to 11.07%. Tissue rendering quality also\nimproves, with PSNR increasing from 0.46% to 49.87%, and SSIM from 1.53% to\n29.21% across all test videos.", "AI": {"task": "Complete reconstruction of surgical scenes for robot-assisted surgery (RAS) using deep depth estimation.", "motivation": "Existing methods struggle with depth discontinuities and noisy predictions at object boundaries, leading to incomplete reconstruction of occluded surfaces.", "method": "Proposes EndoLRMGS, combining Large Reconstruction Modelling (LRM) and Gaussian Splatting (GS), with orthogonal perspective joint projection optimization (OPjPO) for accuracy.", "result": "Improves IoU of tool 3D models by >40%, PSNR of tool projection by 3.82% to 11.07%, and tissue rendering quality (PSNR: 0.46% to 49.87%, SSIM: 1.53% to 29.21%).", "conclusion": "EndoLRMGS effectively addresses depth estimation challenges in RAS, achieving significant improvements in reconstruction accuracy and rendering quality."}}
{"id": "2503.22462", "pdf": "https://arxiv.org/pdf/2503.22462", "abs": "https://arxiv.org/abs/2503.22462", "authors": ["Krispin Wandel", "Hesheng Wang"], "title": "SemAlign3D: Semantic Correspondence between RGB-Images through Aligning 3D Object-Class Representations", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025. Poster:\n  https://cvpr.thecvf.com/virtual/2025/poster/32799", "summary": "Semantic correspondence made tremendous progress through the recent\nadvancements of large vision models (LVM). While these LVMs have been shown to\nreliably capture local semantics, the same can currently not be said for\ncapturing global geometric relationships between semantic object regions. This\nproblem leads to unreliable performance for semantic correspondence between\nimages with extreme view variation. In this work, we aim to leverage monocular\ndepth estimates to capture these geometric relationships for more robust and\ndata-efficient semantic correspondence. First, we introduce a simple but\neffective method to build 3D object-class representations from monocular depth\nestimates and LVM features using a sparsely annotated image correspondence\ndataset. Second, we formulate an alignment energy that can be minimized using\ngradient descent to obtain an alignment between the 3D object-class\nrepresentation and the object-class instance in the input RGB-image. Our method\nachieves state-of-the-art matching accuracy in multiple categories on the\nchallenging SPair-71k dataset, increasing the PCK@0.1 score by more than 10\npoints on three categories and overall by 3.3 points from 85.6% to 88.9%.\nAdditional resources and code are available at https://dub.sh/semalign3d.", "AI": {"task": "\u5229\u7528\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u548c\u5927\u578b\u89c6\u89c9\u6a21\u578b\u7279\u5f81\u6784\u5efa3D\u7269\u4f53\u7c7b\u522b\u8868\u793a\uff0c\u4ee5\u63d0\u5347\u8bed\u4e49\u5bf9\u5e94\u5728\u6781\u7aef\u89c6\u89d2\u53d8\u5316\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u6570\u636e\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u89c6\u89c9\u6a21\u578b\u5728\u6355\u6349\u5c40\u90e8\u8bed\u4e49\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u6355\u6349\u5168\u5c40\u51e0\u4f55\u5173\u7cfb\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u5bfc\u81f4\u6781\u7aef\u89c6\u89d2\u53d8\u5316\u4e0b\u7684\u8bed\u4e49\u5bf9\u5e94\u6027\u80fd\u4e0d\u53ef\u9760\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u548c\u7a00\u758f\u6807\u6ce8\u6570\u636e\u96c6\u6784\u5efa3D\u7269\u4f53\u7c7b\u522b\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u68af\u5ea6\u4e0b\u964d\u6700\u5c0f\u5316\u5bf9\u9f50\u80fd\u91cf\u5b9e\u73b0RGB\u56fe\u50cf\u4e0e3D\u8868\u793a\u7684\u5bf9\u9f50\u3002", "result": "\u5728SPair-71k\u6570\u636e\u96c6\u4e0a\uff0cPCK@0.1\u5206\u6570\u5728\u591a\u4e2a\u7c7b\u522b\u4e2d\u63d0\u5347\u8d85\u8fc710\u5206\uff0c\u603b\u4f53\u4ece85.6%\u63d0\u5347\u81f388.9%\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u6df1\u5ea6\u4f30\u8ba1\u548c3D\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u4e49\u5bf9\u5e94\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2503.22513", "pdf": "https://arxiv.org/pdf/2503.22513", "abs": "https://arxiv.org/abs/2503.22513", "authors": ["Martin Ki\u0161\u0161", "Michal Hradi\u0161"], "title": "Masked Self-Supervised Pre-Training for Text Recognition Transformers on Large-Scale Datasets", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "18 pages, 7 tables, 6 figures; Submitted to ICDAR25", "summary": "Self-supervised learning has emerged as a powerful approach for leveraging\nlarge-scale unlabeled data to improve model performance in various domains. In\nthis paper, we explore masked self-supervised pre-training for text recognition\ntransformers. Specifically, we propose two modifications to the pre-training\nphase: progressively increasing the masking probability, and modifying the loss\nfunction to incorporate both masked and non-masked patches. We conduct\nextensive experiments using a dataset of 50M unlabeled text lines for\npre-training and four differently sized annotated datasets for fine-tuning.\nFurthermore, we compare our pre-trained models against those trained with\ntransfer learning, demonstrating the effectiveness of the self-supervised\npre-training. In particular, pre-training consistently improves the character\nerror rate of models, in some cases up to 30 % relatively. It is also on par\nwith transfer learning but without relying on extra annotated text lines.", "AI": {"task": "\u63a2\u7d22\u63a9\u7801\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u5728\u6587\u672c\u8bc6\u522bTransformer\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u5229\u7528\u5927\u89c4\u6a21\u65e0\u6807\u7b7e\u6570\u636e\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u6587\u672c\u8bc6\u522b\u9886\u57df\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u9884\u8bad\u7ec3\u9636\u6bb5\u7684\u6539\u8fdb\uff1a\u9010\u6b65\u589e\u52a0\u63a9\u7801\u6982\u7387\uff0c\u4ee5\u53ca\u4fee\u6539\u635f\u5931\u51fd\u6570\u4ee5\u540c\u65f6\u8003\u8651\u63a9\u7801\u548c\u975e\u63a9\u7801\u7684\u6587\u672c\u5757\u3002", "result": "\u9884\u8bad\u7ec3\u663e\u8457\u964d\u4f4e\u4e86\u5b57\u7b26\u9519\u8bef\u7387\uff0c\u76f8\u5bf9\u63d0\u5347\u6700\u9ad8\u8fbe30%\uff0c\u4e14\u6027\u80fd\u4e0e\u8fc1\u79fb\u5b66\u4e60\u76f8\u5f53\uff0c\u4f46\u65e0\u9700\u989d\u5916\u6807\u6ce8\u6570\u636e\u3002", "conclusion": "\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u5728\u6587\u672c\u8bc6\u522b\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u80fd\u591f\u6709\u6548\u5229\u7528\u65e0\u6807\u7b7e\u6570\u636e\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2503.22526", "pdf": "https://arxiv.org/pdf/2503.22526", "abs": "https://arxiv.org/abs/2503.22526", "authors": ["Martin Ki\u0161\u0161", "Michal Hradi\u0161", "Martina Dvo\u0159\u00e1kov\u00e1", "V\u00e1clav Jirou\u0161ek", "Filip Kersch"], "title": "AnnoPage Dataset: Dataset of Non-Textual Elements in Documents with Fine-Grained Categorization", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "15 pages, 2 tables, 6 figures; Submitted to ICDAR25", "summary": "We introduce the AnnoPage Dataset, a novel collection of 7550 pages from\nhistorical documents, primarily in Czech and German, spanning from 1485 to the\npresent, focusing on the late 19th and early 20th centuries. The dataset is\ndesigned to support research in document layout analysis and object detection.\nEach page is annotated with axis-aligned bounding boxes (AABB) representing\nelements of 25 categories of non-textual elements, such as images, maps,\ndecorative elements, or charts, following the Czech Methodology of image\ndocument processing. The annotations were created by expert librarians to\nensure accuracy and consistency. The dataset also incorporates pages from\nmultiple, mainly historical, document datasets to enhance variability and\nmaintain continuity. The dataset is divided into development and test subsets,\nwith the test set carefully selected to maintain the category distribution. We\nprovide baseline results using YOLO and DETR object detectors, offering a\nreference point for future research. The AnnoPage Dataset is publicly available\non Zenodo (https://doi.org/10.5281/zenodo.12788419), along with ground-truth\nannotations in YOLO format.", "AI": {"task": "\u4ecb\u7ecdAnnoPage\u6570\u636e\u96c6\uff0c\u652f\u6301\u6587\u6863\u5e03\u5c40\u5206\u6790\u548c\u76ee\u6807\u68c0\u6d4b\u7814\u7a76\u3002", "motivation": "\u4e3a\u5386\u53f2\u6587\u6863\u4e2d\u7684\u975e\u6587\u672c\u5143\u7d20\uff08\u5982\u56fe\u50cf\u3001\u5730\u56fe\u7b49\uff09\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u6807\u6ce8\u6570\u636e\uff0c\u4fc3\u8fdb\u76f8\u5173\u9886\u57df\u7814\u7a76\u3002", "method": "\u6536\u96c67550\u9875\u5386\u53f2\u6587\u6863\uff0c\u7531\u4e13\u5bb6\u6807\u6ce825\u7c7b\u975e\u6587\u672c\u5143\u7d20\u7684\u8f74\u5bf9\u9f50\u8fb9\u754c\u6846\uff08AABB\uff09\uff0c\u5e76\u5212\u5206\u4e3a\u5f00\u53d1\u548c\u6d4b\u8bd5\u5b50\u96c6\u3002", "result": "\u63d0\u4f9b\u57fa\u7ebf\u7ed3\u679c\uff08YOLO\u548cDETR\u68c0\u6d4b\u5668\uff09\uff0c\u6570\u636e\u96c6\u516c\u5f00\u53ef\u7528\u3002", "conclusion": "AnnoPage\u6570\u636e\u96c6\u4e3a\u6587\u6863\u5e03\u5c40\u5206\u6790\u548c\u76ee\u6807\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u8d44\u6e90\uff0c\u652f\u6301\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2503.22537", "pdf": "https://arxiv.org/pdf/2503.22537", "abs": "https://arxiv.org/abs/2503.22537", "authors": ["Remy Sabathier", "Niloy J. Mitra", "David Novotny"], "title": "LIM: Large Interpolator Model for Dynamic Reconstruction", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Reconstructing dynamic assets from video data is central to many in computer\nvision and graphics tasks. Existing 4D reconstruction approaches are limited by\ncategory-specific models or slow optimization-based methods. Inspired by the\nrecent Large Reconstruction Model (LRM), we present the Large Interpolation\nModel (LIM), a transformer-based feed-forward solution, guided by a novel\ncausal consistency loss, for interpolating implicit 3D representations across\ntime. Given implicit 3D representations at times $t_0$ and $t_1$, LIM produces\na deformed shape at any continuous time $t\\in[t_0,t_1]$, delivering\nhigh-quality interpolated frames in seconds. Furthermore, LIM allows explicit\nmesh tracking across time, producing a consistently uv-textured mesh sequence\nready for integration into existing production pipelines. We also use LIM, in\nconjunction with a diffusion-based multiview generator, to produce dynamic 4D\nreconstructions from monocular videos. We evaluate LIM on various dynamic\ndatasets, benchmarking against image-space interpolation methods (e.g., FiLM)\nand direct triplane linear interpolation, and demonstrate clear advantages. In\nsummary, LIM is the first feed-forward model capable of high-speed tracked 4D\nasset reconstruction across diverse categories.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u524d\u9988\u6a21\u578b\uff08LIM\uff09\uff0c\u7528\u4e8e\u5728\u65f6\u95f4\u4e0a\u63d2\u503c\u9690\u5f0f3D\u8868\u793a\uff0c\u5b9e\u73b0\u52a8\u60014D\u91cd\u5efa\u3002", "motivation": "\u73b0\u67094D\u91cd\u5efa\u65b9\u6cd5\u53d7\u9650\u4e8e\u7c7b\u522b\u7279\u5b9a\u6a21\u578b\u6216\u7f13\u6162\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u57fa\u4e8eTransformer\u7684\u524d\u9988\u6a21\u578b\uff0c\u7ed3\u5408\u65b0\u9896\u7684\u56e0\u679c\u4e00\u81f4\u6027\u635f\u5931\uff0c\u5b9e\u73b0\u9690\u5f0f3D\u8868\u793a\u7684\u65f6\u95f4\u63d2\u503c\u3002", "result": "LIM\u80fd\u5728\u79d2\u7ea7\u65f6\u95f4\u5185\u751f\u6210\u9ad8\u8d28\u91cf\u63d2\u503c\u5e27\uff0c\u652f\u6301\u663e\u5f0f\u7f51\u683c\u8ddf\u8e2a\uff0c\u5e76\u53ef\u4e0e\u6269\u6563\u6a21\u578b\u7ed3\u5408\u4ece\u5355\u76ee\u89c6\u9891\u751f\u6210\u52a8\u60014D\u91cd\u5efa\u3002", "conclusion": "LIM\u662f\u9996\u4e2a\u80fd\u9ad8\u901f\u91cd\u5efa\u591a\u6837\u5316\u7c7b\u522b4D\u8d44\u4ea7\u7684\u524d\u9988\u6a21\u578b\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2503.22557", "pdf": "https://arxiv.org/pdf/2503.22557", "abs": "https://arxiv.org/abs/2503.22557", "authors": ["Zhendi Gong", "Susan Francis", "Eleanor Cox", "Stamatios N. Sotiropoulos", "Dorothee P. Auer", "Guoping Qiu", "Andrew P. French", "Xin Chen"], "title": "MO-CTranS: A unified multi-organ segmentation model learning from multiple heterogeneously labelled datasets", "categories": ["cs.CV", "I.2; I.4.6"], "comment": "Accepted by International Symposium on Biomedical Imaging (ISIB) 2025\n  as an oral presentation", "summary": "Multi-organ segmentation holds paramount significance in many clinical tasks.\nIn practice, compared to large fully annotated datasets, multiple small\ndatasets are often more accessible and organs are not labelled consistently.\nNormally, an individual model is trained for each of these datasets, which is\nnot an effective way of using data for model learning. It remains challenging\nto train a single model that can robustly learn from several partially labelled\ndatasets due to label conflict and data imbalance problems. We propose\nMO-CTranS: a single model that can overcome such problems. MO-CTranS contains a\nCNN-based encoder and a Transformer-based decoder, which are connected in a\nmulti-resolution manner. Task-specific tokens are introduced in the decoder to\nhelp differentiate label discrepancies. Our method was evaluated and compared\nto several baseline models and state-of-the-art (SOTA) solutions on abdominal\nMRI datasets that were acquired in different views (i.e. axial and coronal) and\nannotated for different organs (i.e. liver, kidney, spleen). Our method\nachieved better performance (most were statistically significant) than the\ncompared methods. Github link: https://github.com/naisops/MO-CTranS.", "AI": {"task": "\u8bad\u7ec3\u4e00\u4e2a\u5355\u4e00\u6a21\u578b\uff08MO-CTranS\uff09\u4ece\u591a\u4e2a\u90e8\u5206\u6807\u6ce8\u7684\u6570\u636e\u96c6\u4e2d\u8fdb\u884c\u591a\u5668\u5b98\u5206\u5272\u3002", "motivation": "\u89e3\u51b3\u591a\u6570\u636e\u96c6\u6807\u6ce8\u4e0d\u4e00\u81f4\u548c\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u9ad8\u6570\u636e\u5229\u7528\u6548\u7387\u3002", "method": "\u7ed3\u5408CNN\u7f16\u7801\u5668\u548cTransformer\u89e3\u7801\u5668\uff0c\u5f15\u5165\u4efb\u52a1\u7279\u5b9a\u6807\u8bb0\u4ee5\u533a\u5206\u6807\u7b7e\u5dee\u5f02\u3002", "result": "\u5728\u8179\u90e8MRI\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u548cSOTA\u65b9\u6cd5\u3002", "conclusion": "MO-CTranS\u80fd\u6709\u6548\u89e3\u51b3\u591a\u6570\u636e\u96c6\u5206\u5272\u95ee\u9898\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2503.22560", "pdf": "https://arxiv.org/pdf/2503.22560", "abs": "https://arxiv.org/abs/2503.22560", "authors": ["Roy Y. He", "Martin Huska", "Hao Liu"], "title": "Image Decomposition with G-norm Weighted by Total Symmetric Variation", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we propose a novel variational model for decomposing images\ninto their respective cartoon and texture parts. Our model characterizes\ncertain non-local features of any Bounded Variation (BV) image by its Total\nSymmetric Variation (TSV). We demonstrate that TSV is effective in identifying\nregional boundaries. Based on this property, we introduce a weighted Meyer's\n$G$-norm to identify texture interiors without including contour edges. For BV\nimages with bounded TSV, we show that the proposed model admits a solution.\nAdditionally, we design a fast algorithm based on operator-splitting to tackle\nthe associated non-convex optimization problem. The performance of our method\nis validated by a series of numerical experiments.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u53d8\u5206\u6a21\u578b\uff0c\u7528\u4e8e\u5c06\u56fe\u50cf\u5206\u89e3\u4e3a\u5361\u901a\u548c\u7eb9\u7406\u90e8\u5206\u3002", "motivation": "\u901a\u8fc7\u603b\u5bf9\u79f0\u53d8\u5206\uff08TSV\uff09\u8868\u5f81\u6709\u754c\u53d8\u5206\uff08BV\uff09\u56fe\u50cf\u7684\u975e\u5c40\u90e8\u7279\u5f81\uff0c\u4ee5\u6709\u6548\u8bc6\u522b\u533a\u57df\u8fb9\u754c\u3002", "method": "\u5f15\u5165\u52a0\u6743Meyer\u7684$G$-\u8303\u6570\u6765\u8bc6\u522b\u7eb9\u7406\u5185\u90e8\u800c\u4e0d\u5305\u542b\u8f6e\u5ed3\u8fb9\u7f18\uff0c\u5e76\u8bbe\u8ba1\u57fa\u4e8e\u7b97\u5b50\u5206\u88c2\u7684\u5feb\u901f\u7b97\u6cd5\u89e3\u51b3\u975e\u51f8\u4f18\u5316\u95ee\u9898\u3002", "result": "\u5bf9\u4e8e\u5177\u6709\u6709\u754cTSV\u7684BV\u56fe\u50cf\uff0c\u6a21\u578b\u5b58\u5728\u89e3\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u6a21\u578b\u548c\u7b97\u6cd5\u5728\u56fe\u50cf\u5206\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002"}}
{"id": "2503.22577", "pdf": "https://arxiv.org/pdf/2503.22577", "abs": "https://arxiv.org/abs/2503.22577", "authors": ["I\u00f1igo Pikabea", "I\u00f1aki Lacunza", "Oriol Pareras", "Carlos Escolano", "Aitor Gonzalez-Agirre", "Javier Hernando", "Marta Villegas"], "title": "Breaking Language Barriers in Visual Language Models via Multilingual Textual Regularization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Rapid advancements in Visual Language Models (VLMs) have transformed\nmultimodal understanding but are often constrained by generating English\nresponses regardless of the input language. This phenomenon has been termed as\nImage-induced Fidelity Loss (IFL) and stems from limited multimodal\nmultilingual training data. To address this, we propose a continuous\nmultilingual integration strategy that injects text-only multilingual data\nduring visual instruction tuning, preserving the language model's original\nmultilingual capabilities. Extensive evaluations demonstrate that our approach\nsignificantly improves linguistic fidelity across languages without degradation\nin visual performance. We also explore model merging, which improves language\nfidelity but comes at the cost of visual performance. In contrast, our core\nmethod achieves robust multilingual alignment without trade-offs, offering a\nscalable and effective path to mitigating IFL for global VLM adoption.", "AI": {"task": "\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u591a\u8bed\u8a00\u8f93\u5165\u65f6\u4ec5\u751f\u6210\u82f1\u6587\u56de\u5e94\u7684Image-induced Fidelity Loss\uff08IFL\uff09\u95ee\u9898\u3002", "motivation": "\u73b0\u6709VLMs\u5728\u591a\u8bed\u8a00\u8f93\u5165\u65f6\u56e0\u7f3a\u4e4f\u591a\u6a21\u6001\u591a\u8bed\u8a00\u8bad\u7ec3\u6570\u636e\u800c\u4ec5\u751f\u6210\u82f1\u6587\u56de\u5e94\uff0c\u9650\u5236\u4e86\u5176\u5168\u7403\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u8fde\u7eed\u591a\u8bed\u8a00\u96c6\u6210\u7b56\u7565\uff0c\u901a\u8fc7\u5728\u89c6\u89c9\u6307\u4ee4\u8c03\u4f18\u4e2d\u6ce8\u5165\u7eaf\u6587\u672c\u591a\u8bed\u8a00\u6570\u636e\uff0c\u4fdd\u7559\u8bed\u8a00\u6a21\u578b\u7684\u591a\u8bed\u8a00\u80fd\u529b\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u8bed\u8a00\u7684\u8bed\u8a00\u4fdd\u771f\u5ea6\uff0c\u4e14\u672a\u964d\u4f4e\u89c6\u89c9\u6027\u80fd\uff1b\u6a21\u578b\u5408\u5e76\u867d\u63d0\u5347\u8bed\u8a00\u4fdd\u771f\u5ea6\u4f46\u727a\u7272\u89c6\u89c9\u6027\u80fd\u3002", "conclusion": "\u6838\u5fc3\u65b9\u6cd5\u5b9e\u73b0\u4e86\u65e0\u9700\u6743\u8861\u7684\u591a\u8bed\u8a00\u5bf9\u9f50\uff0c\u4e3a\u5168\u7403VLM\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684IFL\u7f13\u89e3\u8def\u5f84\u3002"}}
{"id": "2503.22622", "pdf": "https://arxiv.org/pdf/2503.22622", "abs": "https://arxiv.org/abs/2503.22622", "authors": ["Jangho Park", "Taesung Kwon", "Jong Chul Ye"], "title": "Zero4D: Training-Free 4D Video Generation From Single Video Using Off-the-Shelf Video Diffusion Model", "categories": ["cs.CV"], "comment": "project page: https://zero4dvid.github.io/", "summary": "Recently, multi-view or 4D video generation has emerged as a significant\nresearch topic. Nonetheless, recent approaches to 4D generation still struggle\nwith fundamental limitations, as they primarily rely on harnessing multiple\nvideo diffusion models with additional training or compute-intensive training\nof a full 4D diffusion model with limited real-world 4D data and large\ncomputational costs. To address these challenges, here we propose the first\ntraining-free 4D video generation method that leverages the off-the-shelf video\ndiffusion models to generate multi-view videos from a single input video. Our\napproach consists of two key steps: (1) By designating the edge frames in the\nspatio-temporal sampling grid as key frames, we first synthesize them using a\nvideo diffusion model, leveraging a depth-based warping technique for guidance.\nThis approach ensures structural consistency across the generated frames,\npreserving spatial and temporal coherence. (2) We then interpolate the\nremaining frames using a video diffusion model, constructing a fully populated\nand temporally coherent sampling grid while preserving spatial and temporal\nconsistency. Through this approach, we extend a single video into a multi-view\nvideo along novel camera trajectories while maintaining spatio-temporal\nconsistency. Our method is training-free and fully utilizes an off-the-shelf\nvideo diffusion model, offering a practical and effective solution for\nmulti-view video generation.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u73b0\u6210\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\u4ece\u5355\u4e2a\u8f93\u5165\u89c6\u9891\u751f\u6210\u591a\u89c6\u89d2\u89c6\u9891\u3002", "motivation": "\u89e3\u51b3\u73b0\u67094D\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u989d\u5916\u8bad\u7ec3\u6216\u8ba1\u7b97\u5bc6\u96c6\u578b\u8bad\u7ec3\u7684\u95ee\u9898\uff0c\u540c\u65f6\u7f3a\u4e4f\u771f\u5b9e\u4e16\u754c4D\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\u3002", "method": "\u901a\u8fc7\u4e24\u6b65\u6cd5\uff1a\u9996\u5148\u751f\u6210\u5173\u952e\u5e27\u5e76\u4fdd\u6301\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u7136\u540e\u63d2\u503c\u5269\u4f59\u5e27\u4ee5\u6784\u5efa\u5b8c\u6574\u7684\u65f6\u7a7a\u4e00\u81f4\u91c7\u6837\u7f51\u683c\u3002", "result": "\u6210\u529f\u6269\u5c55\u5355\u4e2a\u89c6\u9891\u4e3a\u591a\u89c6\u89d2\u89c6\u9891\uff0c\u4fdd\u6301\u65f6\u7a7a\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u8bad\u7ec3\uff0c\u5229\u7528\u73b0\u6210\u6a21\u578b\uff0c\u4e3a\u591a\u89c6\u89d2\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2503.22668", "pdf": "https://arxiv.org/pdf/2503.22668", "abs": "https://arxiv.org/abs/2503.22668", "authors": ["Sindhu B Hegde", "K R Prajwal", "Taein Kwon", "Andrew Zisserman"], "title": "Understanding Co-speech Gestures in-the-wild", "categories": ["cs.CV"], "comment": "Main paper - 11 pages, 4 figures, Supplementary - 5 pages, 4 figures", "summary": "Co-speech gestures play a vital role in non-verbal communication. In this\npaper, we introduce a new framework for co-speech gesture understanding in the\nwild. Specifically, we propose three new tasks and benchmarks to evaluate a\nmodel's capability to comprehend gesture-text-speech associations: (i)\ngesture-based retrieval, (ii) gestured word spotting, and (iii) active speaker\ndetection using gestures. We present a new approach that learns a tri-modal\nspeech-text-video-gesture representation to solve these tasks. By leveraging a\ncombination of global phrase contrastive loss and local gesture-word coupling\nloss, we demonstrate that a strong gesture representation can be learned in a\nweakly supervised manner from videos in the wild. Our learned representations\noutperform previous methods, including large vision-language models (VLMs),\nacross all three tasks. Further analysis reveals that speech and text\nmodalities capture distinct gesture-related signals, underscoring the\nadvantages of learning a shared tri-modal embedding space. The dataset, model,\nand code are available at: https://www.robots.ox.ac.uk/~vgg/research/jegal", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u81ea\u7136\u573a\u666f\u4e2d\u7406\u89e3\u4f34\u968f\u8bed\u97f3\u7684\u624b\u52bf\uff0c\u5e76\u5f15\u5165\u4e09\u4e2a\u65b0\u4efb\u52a1\u548c\u57fa\u51c6\u6765\u8bc4\u4f30\u6a21\u578b\u7684\u80fd\u529b\u3002", "motivation": "\u4f34\u968f\u8bed\u97f3\u7684\u624b\u52bf\u5728\u975e\u8bed\u8a00\u4ea4\u6d41\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u6709\u6548\u7684\u7406\u89e3\u548c\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5b66\u4e60\u8bed\u97f3-\u6587\u672c-\u89c6\u9891-\u624b\u52bf\u7684\u4e09\u6a21\u6001\u8868\u793a\u7684\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u5168\u5c40\u77ed\u8bed\u5bf9\u6bd4\u635f\u5931\u548c\u5c40\u90e8\u624b\u52bf-\u8bcd\u8bed\u8026\u5408\u635f\u5931\u3002", "result": "\u5b66\u4e60\u5230\u7684\u8868\u793a\u5728\u4e09\u4e2a\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5305\u62ec\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u3002", "conclusion": "\u8bed\u97f3\u548c\u6587\u672c\u6a21\u6001\u6355\u6349\u4e86\u4e0d\u540c\u7684\u624b\u52bf\u76f8\u5173\u4fe1\u53f7\uff0c\u8bc1\u660e\u4e86\u5b66\u4e60\u5171\u4eab\u4e09\u6a21\u6001\u5d4c\u5165\u7a7a\u95f4\u7684\u4f18\u52bf\u3002"}}
{"id": "2503.22676", "pdf": "https://arxiv.org/pdf/2503.22676", "abs": "https://arxiv.org/abs/2503.22676", "authors": ["Boyang", "Yu", "Yanlin Jin", "Ashok Veeraraghavan", "Akshat Dave", "Guha Balakrishnan"], "title": "TranSplat: Lighting-Consistent Cross-Scene Object Transfer with 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "We present TranSplat, a 3D scene rendering algorithm that enables realistic\ncross-scene object transfer (from a source to a target scene) based on the\nGaussian Splatting framework. Our approach addresses two critical challenges:\n(1) precise 3D object extraction from the source scene, and (2) faithful\nrelighting of the transferred object in the target scene without explicit\nmaterial property estimation. TranSplat fits a splatting model to the source\nscene, using 2D object masks to drive fine-grained 3D segmentation. Following\nuser-guided insertion of the object into the target scene, along with automatic\nrefinement of position and orientation, TranSplat derives per-Gaussian radiance\ntransfer functions via spherical harmonic analysis to adapt the object's\nappearance to match the target scene's lighting environment. This relighting\nstrategy does not require explicitly estimating physical scene properties such\nas BRDFs. Evaluated on several synthetic and real-world scenes and objects,\nTranSplat yields excellent 3D object extractions and relighting performance\ncompared to recent baseline methods and visually convincing cross-scene object\ntransfers. We conclude by discussing the limitations of the approach.", "AI": {"task": "\u63d0\u51faTranSplat\u7b97\u6cd5\uff0c\u5b9e\u73b0\u57fa\u4e8e\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\u7684\u8de8\u573a\u666f\u7269\u4f53\u8f6c\u79fb\u4e0e\u771f\u5b9e\u611f\u6e32\u67d3\u3002", "motivation": "\u89e3\u51b3\u8de8\u573a\u666f\u7269\u4f53\u8f6c\u79fb\u4e2d\u7684\u4e24\u5927\u6311\u6218\uff1a\u7cbe\u786e\u76843D\u7269\u4f53\u63d0\u53d6\u548c\u76ee\u6807\u573a\u666f\u4e2d\u7684\u771f\u5b9e\u611f\u91cd\u5149\u7167\u3002", "method": "\u5229\u7528\u9ad8\u65af\u6cfc\u6e85\u6a21\u578b\u62df\u5408\u6e90\u573a\u666f\uff0c\u901a\u8fc72D\u7269\u4f53\u63a9\u7801\u9a71\u52a8\u7ec6\u7c92\u5ea63D\u5206\u5272\uff0c\u7ed3\u5408\u7403\u8c10\u5206\u6790\u5b9e\u73b0\u91cd\u5149\u7167\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u89c6\u89c9\u4e0a\u53ef\u4fe1\u7684\u8de8\u573a\u666f\u7269\u4f53\u8f6c\u79fb\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2503.22677", "pdf": "https://arxiv.org/pdf/2503.22677", "abs": "https://arxiv.org/abs/2503.22677", "authors": ["Ruining Li", "Chuanxia Zheng", "Christian Rupprecht", "Andrea Vedaldi"], "title": "DSO: Aligning 3D Generators with Simulation Feedback for Physical Soundness", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Project page: https://ruiningli.com/dso", "summary": "Most 3D object generators focus on aesthetic quality, often neglecting\nphysical constraints necessary in applications. One such constraint is that the\n3D object should be self-supporting, i.e., remains balanced under gravity.\nPrior approaches to generating stable 3D objects used differentiable physics\nsimulators to optimize geometry at test-time, which is slow, unstable, and\nprone to local optima. Inspired by the literature on aligning generative models\nto external feedback, we propose Direct Simulation Optimization (DSO), a\nframework to use the feedback from a (non-differentiable) simulator to increase\nthe likelihood that the 3D generator outputs stable 3D objects directly. We\nconstruct a dataset of 3D objects labeled with a stability score obtained from\nthe physics simulator. We can then fine-tune the 3D generator using the\nstability score as the alignment metric, via direct preference optimization\n(DPO) or direct reward optimization (DRO), a novel objective, which we\nintroduce, to align diffusion models without requiring pairwise preferences.\nOur experiments show that the fine-tuned feed-forward generator, using either\nDPO or DRO objective, is much faster and more likely to produce stable objects\nthan test-time optimization. Notably, the DSO framework works even without any\nground-truth 3D objects for training, allowing the 3D generator to self-improve\nby automatically collecting simulation feedback on its own outputs.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a\u76f4\u63a5\u6a21\u62df\u4f18\u5316\uff08DSO\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u9ad83D\u751f\u6210\u5668\u76f4\u63a5\u8f93\u51fa\u7a33\u5b9a3D\u5bf9\u8c61\u7684\u53ef\u80fd\u6027\u3002", "motivation": "\u73b0\u67093D\u5bf9\u8c61\u751f\u6210\u5668\u6ce8\u91cd\u7f8e\u5b66\u8d28\u91cf\uff0c\u4f46\u5ffd\u7565\u4e86\u7269\u7406\u7ea6\u675f\uff08\u5982\u81ea\u652f\u6491\u6027\uff09\uff0c\u800c\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u901f\u5ea6\u6162\u4e14\u4e0d\u7a33\u5b9a\u3002", "method": "\u901a\u8fc7\u975e\u53ef\u5fae\u5206\u6a21\u62df\u5668\u7684\u53cd\u9988\uff0c\u4f7f\u7528\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u6216\u76f4\u63a5\u5956\u52b1\u4f18\u5316\uff08DRO\uff09\u76ee\u6807\u5bf93D\u751f\u6210\u5668\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5fae\u8c03\u540e\u7684\u751f\u6210\u5668\u6bd4\u6d4b\u8bd5\u65f6\u4f18\u5316\u66f4\u5feb\u4e14\u66f4\u53ef\u80fd\u751f\u6210\u7a33\u5b9a\u5bf9\u8c61\u3002", "conclusion": "DSO\u6846\u67b6\u65e0\u9700\u771f\u5b9e3D\u8bad\u7ec3\u6570\u636e\uff0c\u53ef\u901a\u8fc7\u6a21\u62df\u53cd\u9988\u5b9e\u73b0\u81ea\u6211\u6539\u8fdb\u3002"}}
{"id": "2503.22679", "pdf": "https://arxiv.org/pdf/2503.22679", "abs": "https://arxiv.org/abs/2503.22679", "authors": ["Weiqi Li", "Xuanyu Zhang", "Shijie Zhao", "Yabin Zhang", "Junlin Li", "Li Zhang", "Jian Zhang"], "title": "Q-Insight: Understanding Image Quality via Visual Reinforcement Learning", "categories": ["cs.CV"], "comment": "Technical report", "summary": "Image quality assessment (IQA) focuses on the perceptual visual quality of\nimages, playing a crucial role in downstream tasks such as image\nreconstruction, compression, and generation. The rapid advancement of\nmulti-modal large language models (MLLMs) has significantly broadened the scope\nof IQA, moving toward comprehensive image quality understanding that\nincorporates content analysis, degradation perception, and comparison reasoning\nbeyond mere numerical scoring. Previous MLLM-based methods typically either\ngenerate numerical scores lacking interpretability or heavily rely on\nsupervised fine-tuning (SFT) using large-scale annotated datasets to provide\ndescriptive assessments, limiting their flexibility and applicability. In this\npaper, we propose Q-Insight, a reinforcement learning-based model built upon\ngroup relative policy optimization (GRPO), which demonstrates strong visual\nreasoning capability for image quality understanding while requiring only a\nlimited amount of rating scores and degradation labels. By jointly optimizing\nscore regression and degradation perception tasks with carefully designed\nreward functions, our approach effectively exploits their mutual benefits for\nenhanced performance. Extensive experiments demonstrate that Q-Insight\nsubstantially outperforms existing state-of-the-art methods in both score\nregression and degradation perception tasks, while exhibiting impressive\nzero-shot generalization to comparison reasoning tasks. Code will be available\nat https://github.com/lwq20020127/Q-Insight.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6a21\u578bQ-Insight\uff0c\u7528\u4e8e\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff08IQA\uff09\uff0c\u7ed3\u5408\u5185\u5bb9\u5206\u6790\u3001\u9000\u5316\u611f\u77e5\u548c\u6bd4\u8f83\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u65b9\u6cd5\u8981\u4e48\u751f\u6210\u7f3a\u4e4f\u89e3\u91ca\u6027\u7684\u6570\u503c\u5206\u6570\uff0c\u8981\u4e48\u4f9d\u8d56\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\uff0c\u9650\u5236\u4e86\u7075\u6d3b\u6027\u548c\u9002\u7528\u6027\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u7684\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\uff0c\u8054\u5408\u4f18\u5316\u5206\u6570\u56de\u5f52\u548c\u9000\u5316\u611f\u77e5\u4efb\u52a1\uff0c\u8bbe\u8ba1\u5956\u52b1\u51fd\u6570\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "result": "Q-Insight\u5728\u5206\u6570\u56de\u5f52\u548c\u9000\u5316\u611f\u77e5\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u96f6\u6837\u672c\u6bd4\u8f83\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "Q-Insight\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u4efb\u52a1\u8054\u5408\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u7684\u56fe\u50cf\u8d28\u91cf\u7406\u89e3\uff0c\u4e14\u5177\u6709\u8f83\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2503.21818", "pdf": "https://arxiv.org/pdf/2503.21818", "abs": "https://arxiv.org/abs/2503.21818", "authors": ["Tianqi Tu", "Hui Wang", "Jiangbo Pei", "Xiaojuan Yu", "Aidong Men", "Suxia Wang", "Qingchao Chen", "Ying Tan", "Feng Yu", "Minghui Zhao"], "title": "Deep Learning-Based Quantitative Assessment of Renal Chronicity Indices in Lupus Nephritis", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Background: Renal chronicity indices (CI) have been identified as strong\npredictors of long-term outcomes in lupus nephritis (LN) patients. However,\nassessment by pathologists is hindered by challenges such as substantial time\nrequirements, high interobserver variation, and susceptibility to fatigue. This\nstudy aims to develop an effective deep learning (DL) pipeline that automates\nthe assessment of CI and provides valuable prognostic insights from a\ndisease-specific perspective. Methods: We curated a dataset comprising 282\nslides obtained from 141 patients across two independent cohorts with a\ncomplete 10-years follow-up. Our DL pipeline was developed on 60 slides (22,410\npatch images) from 30 patients in the training cohort and evaluated on both an\ninternal testing set (148 slides, 77,605 patch images) and an external testing\nset (74 slides, 27,522 patch images). Results: The study included two cohorts\nwith slight demographic differences, particularly in age and hemoglobin levels.\nThe DL pipeline showed high segmentation performance across tissue compartments\nand histopathologic lesions, outperforming state-of-the-art methods. The DL\npipeline also demonstrated a strong correlation with pathologists in assessing\nCI, significantly improving interobserver agreement. Additionally, the DL\npipeline enhanced prognostic accuracy, particularly in outcome prediction, when\ncombined with clinical parameters and pathologist-assessed CIs Conclusions: The\nDL pipeline demonstrated accuracy and efficiency in assessing CI in LN, showing\npromise in improving interobserver agreement among pathologists. It also\nexhibited significant value in prognostic analysis and enhancing outcome\nprediction in LN patients, offering a valuable tool for clinical\ndecision-making.", "AI": {"task": "\u5f00\u53d1\u4e00\u79cd\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u6d41\u7a0b\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u8bc4\u4f30\u72fc\u75ae\u6027\u80be\u708e\uff08LN\uff09\u60a3\u8005\u7684\u80be\u810f\u6162\u6027\u6307\u6570\uff08CI\uff09\uff0c\u5e76\u63d0\u4f9b\u75be\u75c5\u7279\u5f02\u6027\u7684\u9884\u540e\u5206\u6790\u3002", "motivation": "\u75c5\u7406\u5b66\u5bb6\u8bc4\u4f30CI\u5b58\u5728\u8017\u65f6\u3001\u89c2\u5bdf\u8005\u95f4\u5dee\u5f02\u5927\u548c\u6613\u75b2\u52b3\u7b49\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u3001\u51c6\u786e\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u6765\u81ea141\u540d\u60a3\u8005\u7684282\u5f20\u5207\u7247\u6570\u636e\uff0c\u5f00\u53d1\u5e76\u9a8c\u8bc1DL\u6d41\u7a0b\uff0c\u5305\u62ec\u8bad\u7ec3\u96c6\u548c\u5185\u5916\u6d4b\u8bd5\u96c6\u3002", "result": "DL\u6d41\u7a0b\u5728\u7ec4\u7ec7\u5206\u5272\u548cCI\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u89c2\u5bdf\u8005\u95f4\u4e00\u81f4\u6027\uff0c\u5e76\u589e\u5f3a\u4e86\u9884\u540e\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "DL\u6d41\u7a0b\u5728LN\u60a3\u8005CI\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u51c6\u786e\u6027\uff0c\u6709\u671b\u6539\u5584\u4e34\u5e8a\u51b3\u7b56\u548c\u9884\u540e\u5206\u6790\u3002"}}
{"id": "2503.21825", "pdf": "https://arxiv.org/pdf/2503.21825", "abs": "https://arxiv.org/abs/2503.21825", "authors": ["Youn\u00e8s Moussaoui", "Diana Mateus", "Nasrin Taheri", "Sa\u00efd Moussaoui", "Thomas Carlier", "Simon Stute"], "title": "Implicit neural representations for end-to-end PET reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": "IEEE International Symposium on Biomedical Imaging, Apr 2025, Houston\n  (Texas), United States", "summary": "Implicit neural representations (INRs) have demonstrated strong capabilities\nin various medical imaging tasks, such as denoising, registration, and\nsegmentation, by representing images as continuous functions, allowing complex\ndetails to be captured. For image reconstruction problems, INRs can also reduce\nartifacts typically introduced by conventional reconstruction algorithms.\nHowever, to the best of our knowledge, INRs have not been studied in the\ncontext of PET reconstruction. In this paper, we propose an unsupervised PET\nimage reconstruction method based on the implicit SIREN neural network\narchitecture using sinusoidal activation functions. Our method incorporates a\nforward projection model and a loss function adapted to perform PET image\nreconstruction directly from sinograms, without the need for large training\ndatasets. The performance of the proposed approach was compared with that of\nconventional penalized likelihood methods and deep image prior (DIP) based\nreconstruction using brain phantom data and realistically simulated sinograms.\nThe results show that the INR-based approach can reconstruct high-quality\nimages with a simpler, more efficient model, offering improvements in PET image\nreconstruction, particularly in terms of contrast, activity recovery, and\nrelative bias.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u9690\u5f0fSIREN\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u7684\u65e0\u76d1\u7763PET\u56fe\u50cf\u91cd\u5efa\u65b9\u6cd5\u3002", "motivation": "\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INRs\uff09\u5728\u533b\u5b66\u6210\u50cf\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5c1a\u672a\u5728PET\u91cd\u5efa\u4e2d\u5f97\u5230\u7814\u7a76\u3002", "method": "\u4f7f\u7528\u6b63\u5f26\u6fc0\u6d3b\u51fd\u6570\u7684\u9690\u5f0fSIREN\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7ed3\u5408\u524d\u5411\u6295\u5f71\u6a21\u578b\u548c\u9002\u5e94PET\u91cd\u5efa\u7684\u635f\u5931\u51fd\u6570\u3002", "result": "\u4e0e\u5e38\u89c4\u60e9\u7f5a\u4f3c\u7136\u65b9\u6cd5\u548c\u57fa\u4e8e\u6df1\u5ea6\u56fe\u50cf\u5148\u9a8c\uff08DIP\uff09\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cINR\u65b9\u6cd5\u80fd\u91cd\u5efa\u66f4\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\uff0c\u63d0\u5347\u5bf9\u6bd4\u5ea6\u3001\u6d3b\u52a8\u6062\u590d\u548c\u76f8\u5bf9\u504f\u5dee\u3002", "conclusion": "INR\u65b9\u6cd5\u4e3aPET\u56fe\u50cf\u91cd\u5efa\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7b80\u5355\u3001\u9ad8\u6548\u7684\u6a21\u578b\uff0c\u5177\u6709\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2503.21829", "pdf": "https://arxiv.org/pdf/2503.21829", "abs": "https://arxiv.org/abs/2503.21829", "authors": ["Ivan Diaz", "Florin Scherer", "Yanik Berli", "Roland Wiest", "Helly Hammer", "Robert Hoepner", "Alejandro Leon Betancourt", "Piotr Radojewski", "Richard McKinley"], "title": "Learning from spatially inhomogenous data: resolution-adaptive convolutions for multiple sclerosis lesion segmentation", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "In the setting of clinical imaging, differences in between vendors, hospitals\nand sequences can yield highly inhomogeneous imaging data. In MRI in\nparticular, voxel dimension, slice spacing and acquisition plane can vary\nsubstantially. For clinical applications, therefore, algorithms must be trained\nto handle data with various voxel resolutions. The usual strategy to deal with\nheterogeneity of resolution is harmonization: resampling imaging data to a\ncommon (usually isovoxel) resolution. This can lead to loss of fidelity arising\nfrom interpolation artifacts out-of-plane and downsampling in-plane. We present\nin this paper a network architecture designed to be able to learn directly from\nspatially heterogeneous data, without resampling: a segmentation network based\non the e3nn framework that leverages a spherical harmonic, rather than\nvoxel-grid, parameterization of convolutional kernels, with a fixed physical\nradius. Networks based on these kernels can be resampled to their input voxel\ndimensions. We trained and tested our network on a publicly available dataset\nassembled from three centres, and on an in-house dataset of Multiple Sclerosis\ncases with a high degree of spatial inhomogeneity. We compared our approach to\na standard U-Net with two strategies for handling inhomogeneous data: training\ndirectly on the data without resampling, and resampling to a common resolution\nof 1mm isovoxels. We show that our network is able to learn from various\ncombinations of voxel sizes and outperforms classical U-Nets on 2D testing\ncases and most 3D testing cases. This shows an ability to generalize well when\ntested on image resolutions not seen during training. Our code can be found at:\nhttp://github.com/SCAN-NRAD/e3nn\\_U-Net.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8ee3nn\u6846\u67b6\u7684\u7f51\u7edc\u67b6\u6784\uff0c\u7528\u4e8e\u76f4\u63a5\u4ece\u7a7a\u95f4\u5f02\u6784\u7684MRI\u6570\u636e\u4e2d\u5b66\u4e60\u5206\u5272\u4efb\u52a1\uff0c\u65e0\u9700\u91cd\u91c7\u6837\u3002", "motivation": "\u4e34\u5e8a\u6210\u50cf\u4e2d\uff0c\u4e0d\u540c\u8bbe\u5907\u3001\u533b\u9662\u548c\u5e8f\u5217\u5bfc\u81f4\u7684\u6210\u50cf\u6570\u636e\u5206\u8fa8\u7387\u5dee\u5f02\u5927\uff0c\u4f20\u7edf\u91cd\u91c7\u6837\u65b9\u6cd5\u53ef\u80fd\u5bfc\u81f4\u4fdd\u771f\u5ea6\u635f\u5931\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u7403\u8c10\u51fd\u6570\u7684\u5377\u79ef\u6838\u53c2\u6570\u5316\u7f51\u7edc\uff0c\u56fa\u5b9a\u7269\u7406\u534a\u5f84\uff0c\u53ef\u9002\u5e94\u4e0d\u540c\u4f53\u7d20\u5206\u8fa8\u7387\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u548c\u5185\u90e8\u591a\u53d1\u6027\u786c\u5316\u75c7\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u7f51\u7edc\u57282D\u548c\u5927\u591a\u65703D\u6d4b\u8bd5\u6848\u4f8b\u4e2d\u4f18\u4e8e\u4f20\u7edfU-Net\u3002", "conclusion": "\u8be5\u7f51\u7edc\u80fd\u4ece\u672a\u89c1\u8fc7\u7684\u56fe\u50cf\u5206\u8fa8\u7387\u4e2d\u6cdb\u5316\uff0c\u5c55\u793a\u4e86\u5904\u7406\u5f02\u6784\u6570\u636e\u7684\u6f5c\u529b\u3002"}}
{"id": "2503.21840", "pdf": "https://arxiv.org/pdf/2503.21840", "abs": "https://arxiv.org/abs/2503.21840", "authors": ["Mohammad Amin Khalafi", "Seyed Amir Ahmad Safavi-Naini", "Ameneh Salehi", "Nariman Naderi", "Dorsa Alijanzadeh", "Pardis Ketabi Moghadam", "Kaveh Kavosi", "Negar Golestani", "Shabnam Shahrokh", "Soltanali Fallah", "Jamil S Samaan", "Nicholas P. Tatonetti", "Nicholas Hoerter", "Girish Nadkarni", "Hamid Asadzadeh Aghdaei", "Ali Soroush"], "title": "Vision Language Models versus Machine Learning Models Performance on Polyp Detection and Classification in Colonoscopy Images", "categories": ["eess.IV", "cs.CV", "92C50, 68T50", "J.3"], "comment": "Code is available at:\n  https://github.com/aminkhalafi/CML-vs-LLM-on-Polyp-Detection. CoI: AlSo\n  serves on the advisory board and holds equity in Virgo Surgical Solutions.\n  The other authors declare no conflicts of interest. Data", "summary": "Introduction: This study provides a comprehensive performance assessment of\nvision-language models (VLMs) against established convolutional neural networks\n(CNNs) and classic machine learning models (CMLs) for computer-aided detection\n(CADe) and computer-aided diagnosis (CADx) of colonoscopy polyp images. Method:\nWe analyzed 2,258 colonoscopy images with corresponding pathology reports from\n428 patients. We preprocessed all images using standardized techniques\n(resizing, normalization, and augmentation) and implemented a rigorous\ncomparative framework evaluating 11 distinct models: ResNet50, 4 CMLs (random\nforest, support vector machine, logistic regression, decision tree), two\nspecialized contrastive vision language encoders (CLIP, BiomedCLIP), and three\ngeneral-purpose VLMs ( GPT-4 Gemini-1.5-Pro, Claude-3-Opus). Our performance\nassessment focused on two clinical tasks: polyp detection (CADe) and\nclassification (CADx). Result: In polyp detection, ResNet50 achieved the best\nperformance (F1: 91.35%, AUROC: 0.98), followed by BiomedCLIP (F1: 88.68%,\nAUROC: [AS1] ). GPT-4 demonstrated comparable effectiveness to traditional\nmachine learning approaches (F1: 81.02%, AUROC: [AS2] ), outperforming other\ngeneral-purpose VLMs. For polyp classification, performance rankings remained\nconsistent but with lower overall metrics. ResNet50 maintained the highest\nefficacy (weighted F1: 74.94%), while GPT-4 demonstrated moderate capability\n(weighted F1: 41.18%), significantly exceeding other VLMs (Claude-3-Opus\nweighted F1: 25.54%, Gemini 1.5 Pro weighted F1: 6.17%). Conclusion: CNNs\nremain superior for both CADx and CADe tasks. However, VLMs like BioMedCLIP and\nGPT-4 may be useful for polyp detection tasks where training CNNs is not\nfeasible.", "AI": {"task": "\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e0e\u4f20\u7edf\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNNs\uff09\u53ca\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08CMLs\uff09\u5728\u7ed3\u80a0\u955c\u606f\u8089\u56fe\u50cf\u7684\u8ba1\u7b97\u673a\u8f85\u52a9\u68c0\u6d4b\uff08CADe\uff09\u548c\u8bca\u65ad\uff08CADx\uff09\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u5728\u7ed3\u80a0\u955c\u606f\u8089\u56fe\u50cf\u5904\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4ee5\u786e\u5b9a\u6700\u4f18\u6a21\u578b\u3002", "method": "\u5206\u6790\u4e862,258\u5f20\u7ed3\u80a0\u955c\u56fe\u50cf\u53ca428\u540d\u60a3\u8005\u7684\u75c5\u7406\u62a5\u544a\uff0c\u9884\u5904\u7406\u56fe\u50cf\u540e\u8bc4\u4f30\u4e8611\u79cd\u6a21\u578b\uff08\u5305\u62ecResNet50\u30014\u79cdCMLs\u30012\u79cd\u4e13\u7528\u89c6\u89c9\u8bed\u8a00\u7f16\u7801\u5668\u548c3\u79cd\u901a\u7528VLMs\uff09\uff0c\u91cd\u70b9\u5173\u6ce8CADe\u548cCADx\u4efb\u52a1\u3002", "result": "ResNet50\u5728\u606f\u8089\u68c0\u6d4b\u4e2d\u8868\u73b0\u6700\u4f73\uff08F1: 91.35%\uff0cAUROC: 0.98\uff09\uff0cBioMedCLIP\u6b21\u4e4b\uff1bGPT-4\u5728\u606f\u8089\u5206\u7c7b\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6VLMs\uff0c\u4f46\u6574\u4f53\u6027\u80fd\u4ecd\u4f4e\u4e8eCNNs\u3002", "conclusion": "CNNs\u5728CADx\u548cCADe\u4efb\u52a1\u4e2d\u4ecd\u5360\u4f18\u52bf\uff0c\u4f46BioMedCLIP\u548cGPT-4\u5728\u65e0\u6cd5\u8bad\u7ec3CNNs\u65f6\u53ef\u80fd\u9002\u7528\u4e8e\u606f\u8089\u68c0\u6d4b\u3002"}}
{"id": "2503.21860", "pdf": "https://arxiv.org/pdf/2503.21860", "abs": "https://arxiv.org/abs/2503.21860", "authors": ["Kailin Li", "Puhao Li", "Tengyu Liu", "Yuyang Li", "Siyuan Huang"], "title": "ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via Residual Learning", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Human hands play a central role in interacting, motivating increasing\nresearch in dexterous robotic manipulation. Data-driven embodied AI algorithms\ndemand precise, large-scale, human-like manipulation sequences, which are\nchallenging to obtain with conventional reinforcement learning or real-world\nteleoperation. To address this, we introduce ManipTrans, a novel two-stage\nmethod for efficiently transferring human bimanual skills to dexterous robotic\nhands in simulation. ManipTrans first pre-trains a generalist trajectory\nimitator to mimic hand motion, then fine-tunes a specific residual module under\ninteraction constraints, enabling efficient learning and accurate execution of\ncomplex bimanual tasks. Experiments show that ManipTrans surpasses\nstate-of-the-art methods in success rate, fidelity, and efficiency. Leveraging\nManipTrans, we transfer multiple hand-object datasets to robotic hands,\ncreating DexManipNet, a large-scale dataset featuring previously unexplored\ntasks like pen capping and bottle unscrewing. DexManipNet comprises 3.3K\nepisodes of robotic manipulation and is easily extensible, facilitating further\npolicy training for dexterous hands and enabling real-world deployments.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aManipTrans\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u7528\u4e8e\u5c06\u4eba\u7c7b\u53cc\u624b\u6280\u80fd\u9ad8\u6548\u8fc1\u79fb\u5230\u4eff\u771f\u4e2d\u7684\u7075\u5de7\u673a\u5668\u4eba\u624b\u4e0a\u3002", "motivation": "\u4eba\u7c7b\u53cc\u624b\u5728\u4ea4\u4e92\u4e2d\u626e\u6f14\u6838\u5fc3\u89d2\u8272\uff0c\u4f46\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u6216\u73b0\u5b9e\u4e16\u754c\u9065\u64cd\u4f5c\u96be\u4ee5\u83b7\u53d6\u7cbe\u786e\u3001\u5927\u89c4\u6a21\u3001\u7c7b\u4eba\u7684\u64cd\u4f5c\u5e8f\u5217\u3002", "method": "ManipTrans\u9996\u5148\u9884\u8bad\u7ec3\u4e00\u4e2a\u901a\u7528\u8f68\u8ff9\u6a21\u4eff\u5668\u6a21\u4eff\u624b\u90e8\u52a8\u4f5c\uff0c\u7136\u540e\u5728\u4ea4\u4e92\u7ea6\u675f\u4e0b\u5fae\u8c03\u7279\u5b9a\u6b8b\u5dee\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cManipTrans\u5728\u6210\u529f\u7387\u3001\u4fdd\u771f\u5ea6\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u521b\u5efa\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6DexManipNet\u3002", "conclusion": "ManipTrans\u4e3a\u7075\u5de7\u673a\u5668\u4eba\u624b\u7684\u7b56\u7565\u8bad\u7ec3\u548c\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2503.21886", "pdf": "https://arxiv.org/pdf/2503.21886", "abs": "https://arxiv.org/abs/2503.21886", "authors": ["Pilseo Park", "Ze Zhang", "Michel Sarkis", "Ning Bi", "Xiaoming Liu", "Yiying Tong"], "title": "Refined Geometry-guided Head Avatar Reconstruction from Monocular RGB Video", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "High-fidelity reconstruction of head avatars from monocular videos is highly\ndesirable for virtual human applications, but it remains a challenge in the\nfields of computer graphics and computer vision. In this paper, we propose a\ntwo-phase head avatar reconstruction network that incorporates a refined 3D\nmesh representation. Our approach, in contrast to existing methods that rely on\ncoarse template-based 3D representations derived from 3DMM, aims to learn a\nrefined mesh representation suitable for a NeRF that captures complex facial\nnuances. In the first phase, we train 3DMM-stored NeRF with an initial mesh to\nutilize geometric priors and integrate observations across frames using a\nconsistent set of latent codes. In the second phase, we leverage a novel mesh\nrefinement procedure based on an SDF constructed from the density field of the\ninitial NeRF. To mitigate the typical noise in the NeRF density field without\ncompromising the features of the 3DMM, we employ Laplace smoothing on the\ndisplacement field. Subsequently, we apply a second-phase training with these\nrefined meshes, directing the learning process of the network towards capturing\nintricate facial details. Our experiments demonstrate that our method further\nenhances the NeRF rendering based on the initial mesh and achieves performance\nsuperior to state-of-the-art methods in reconstructing high-fidelity head\navatars with such input.", "AI": {"task": "\u4ece\u5355\u76ee\u89c6\u9891\u4e2d\u9ad8\u4fdd\u771f\u91cd\u5efa\u5934\u90e8\u865a\u62df\u5f62\u8c61\u3002", "motivation": "\u4e3a\u865a\u62df\u4eba\u5e94\u7528\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u5934\u90e8\u865a\u62df\u5f62\u8c61\u91cd\u5efa\u65b9\u6cd5\uff0c\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e3DMM\u7684\u7c97\u7c92\u5ea6\u6a21\u677f\u8868\u793a\u65e0\u6cd5\u6355\u6349\u590d\u6742\u9762\u90e8\u7ec6\u8282\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u5934\u90e8\u865a\u62df\u5f62\u8c61\u91cd\u5efa\u7f51\u7edc\uff0c\u7ed3\u5408\u4f18\u5316\u76843D\u7f51\u683c\u8868\u793a\u3002\u7b2c\u4e00\u9636\u6bb5\u5229\u75283DMM\u5b58\u50a8\u7684NeRF\u548c\u521d\u59cb\u7f51\u683c\u6574\u5408\u51e0\u4f55\u5148\u9a8c\uff1b\u7b2c\u4e8c\u9636\u6bb5\u57fa\u4e8e\u521d\u59cbNeRF\u5bc6\u5ea6\u573a\u6784\u5efaSDF\u8fdb\u884c\u7f51\u683c\u4f18\u5316\uff0c\u5e76\u901a\u8fc7Laplace\u5e73\u6ed1\u51cf\u5c11\u566a\u58f0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u521d\u59cb\u7f51\u683c\u57fa\u7840\u4e0a\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86NeRF\u6e32\u67d3\uff0c\u91cd\u5efa\u9ad8\u4fdd\u771f\u5934\u90e8\u865a\u62df\u5f62\u8c61\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\u80fd\u6709\u6548\u6355\u6349\u590d\u6742\u9762\u90e8\u7ec6\u8282\uff0c\u63d0\u5347\u5934\u90e8\u865a\u62df\u5f62\u8c61\u7684\u91cd\u5efa\u8d28\u91cf\u3002"}}
{"id": "2503.21914", "pdf": "https://arxiv.org/pdf/2503.21914", "abs": "https://arxiv.org/abs/2503.21914", "authors": ["Johannes Seiffarth", "Katharina N\u00f6h"], "title": "PyUAT: Open-source Python framework for efficient and scalable cell tracking", "categories": ["q-bio.QM", "cs.CV"], "comment": null, "summary": "Tracking individual cells in live-cell imaging provides fundamental insights,\ninevitable for studying causes and consequences of phenotypic heterogeneity,\nresponses to changing environmental conditions or stressors. Microbial cell\ntracking, characterized by stochastic cell movements and frequent cell\ndivisions, remains a challenging task when imaging frame rates must be limited\nto avoid counterfactual results. A promising way to overcome this limitation is\nuncertainty-aware tracking (UAT), which uses statistical models, calibrated to\nempirically observed cell behavior, to predict likely cell associations. We\npresent PyUAT, an efficient and modular Python implementation of UAT for\ntracking microbial cells in time-lapse imaging. We demonstrate its performance\non a large 2D+t data set and investigate the influence of modular biological\nmodels and imaging intervals on the tracking performance. The open-source PyUAT\nsoftware is available at https://github.com/JuBiotech/PyUAT, including example\nnotebooks for immediate use in Google Colab.", "AI": {"task": "\u5f00\u53d1PyUAT\uff0c\u4e00\u79cd\u7528\u4e8e\u5fae\u751f\u7269\u7ec6\u80de\u8ffd\u8e2a\u7684\u9ad8\u6548\u6a21\u5757\u5316Python\u5de5\u5177\u3002", "motivation": "\u5fae\u751f\u7269\u7ec6\u80de\u8ffd\u8e2a\u5728\u6d3b\u7ec6\u80de\u6210\u50cf\u4e2d\u5177\u6709\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u5f53\u5e27\u7387\u53d7\u9650\u65f6\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u968f\u673a\u7ec6\u80de\u8fd0\u52a8\u548c\u9891\u7e41\u5206\u88c2\u3002", "method": "\u91c7\u7528\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u8ffd\u8e2a\uff08UAT\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408\u7edf\u8ba1\u6a21\u578b\u9884\u6d4b\u7ec6\u80de\u5173\u8054\u3002", "result": "PyUAT\u5728\u5927\u578b2D+t\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u9a8c\u8bc1\u4e86\u6a21\u5757\u5316\u751f\u7269\u6a21\u578b\u548c\u6210\u50cf\u95f4\u9694\u5bf9\u8ffd\u8e2a\u6027\u80fd\u7684\u5f71\u54cd\u3002", "conclusion": "PyUAT\u4e3a\u5fae\u751f\u7269\u7ec6\u80de\u8ffd\u8e2a\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5df2\u5f00\u6e90\u4f9b\u793e\u533a\u4f7f\u7528\u3002"}}
{"id": "2503.21931", "pdf": "https://arxiv.org/pdf/2503.21931", "abs": "https://arxiv.org/abs/2503.21931", "authors": ["Ishit Mehta", "Manmohan Chandraker", "Ravi Ramamoorthi"], "title": "Locally Orderless Images for Optimization in Differentiable Rendering", "categories": ["cs.GR", "cs.CV"], "comment": "CVPR 2025. Project: https://ishit.github.io/loir/", "summary": "Problems in differentiable rendering often involve optimizing scene\nparameters that cause motion in image space. The gradients for such parameters\ntend to be sparse, leading to poor convergence. While existing methods address\nthis sparsity through proxy gradients such as topological derivatives or\nlagrangian derivatives, they make simplifying assumptions about rendering.\nMulti-resolution image pyramids offer an alternative approach but prove\nunreliable in practice. We introduce a method that uses locally orderless\nimages, where each pixel maps to a histogram of intensities that preserves\nlocal variations in appearance. Using an inverse rendering objective that\nminimizes histogram distance, our method extends support for sparsely defined\nimage gradients and recovers optimal parameters. We validate our method on\nvarious inverse problems using both synthetic and real data.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u5c40\u90e8\u65e0\u5e8f\u56fe\u50cf\uff08locally orderless images\uff09\u89e3\u51b3\u53ef\u5fae\u5206\u6e32\u67d3\u4e2d\u7a00\u758f\u68af\u5ea6\u95ee\u9898\u7684\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u4ee3\u7406\u68af\u5ea6\uff08\u5982\u62d3\u6251\u5bfc\u6570\u6216\u62c9\u683c\u6717\u65e5\u5bfc\u6570\uff09\u5904\u7406\u7a00\u758f\u68af\u5ea6\u95ee\u9898\uff0c\u4f46\u5bf9\u6e32\u67d3\u8fc7\u7a0b\u505a\u4e86\u7b80\u5316\u5047\u8bbe\uff1b\u591a\u5206\u8fa8\u7387\u56fe\u50cf\u91d1\u5b57\u5854\u5728\u5b9e\u8df5\u4e2d\u4e0d\u53ef\u9760\u3002", "method": "\u4f7f\u7528\u5c40\u90e8\u65e0\u5e8f\u56fe\u50cf\uff0c\u5c06\u6bcf\u4e2a\u50cf\u7d20\u6620\u5c04\u4e3a\u4fdd\u7559\u5916\u89c2\u5c40\u90e8\u53d8\u5316\u7684\u5f3a\u5ea6\u76f4\u65b9\u56fe\uff0c\u5e76\u901a\u8fc7\u6700\u5c0f\u5316\u76f4\u65b9\u56fe\u8ddd\u79bb\u7684\u53cd\u6e32\u67d3\u76ee\u6807\u51fd\u6570\u6269\u5c55\u7a00\u758f\u68af\u5ea6\u652f\u6301\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u7684\u591a\u79cd\u53cd\u95ee\u9898\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6062\u590d\u6700\u4f18\u53c2\u6570\uff0c\u89e3\u51b3\u4e86\u7a00\u758f\u68af\u5ea6\u5bfc\u81f4\u7684\u6536\u655b\u95ee\u9898\u3002"}}
{"id": "2503.21955", "pdf": "https://arxiv.org/pdf/2503.21955", "abs": "https://arxiv.org/abs/2503.21955", "authors": ["Manojkumar Saranathan", "Giuseppina Cogliandro", "Thomas Hicks", "Dianne Patterson", "Behroze Vachha", "Alberto Cacciola"], "title": "Comprehensive segmentation of deep grey nuclei from structural MRI data", "categories": ["eess.IV", "cs.CV"], "comment": "7 Figures 2 Tables 2 Supplemental Figures 1 Supplemental Table", "summary": "Motivation: Lack of tools for comprehensive and complete segmentation of deep\ngrey nuclei using a single software for reproducibility and repeatability\nGoal(s): A fast accurate and robust method for segmentation of deep grey nuclei\n(thalamic nuclei, basal ganglia, claustrum, red nucleus) from structural T1 MRI\ndata at conventional field strengths Approach: We leverage the improved\ncontrast of white-matter-nulled imaging by using the recently proposed\nHistogram-based Polynomial Synthesis (HIPS) to synthesize WMn-like images from\nstandard T1 and then use a multi-atlas segmentation with joint label fusion to\nsegment deep grey nuclei. Results: The method worked robustly on all field\nstrengths (1.5/3/7) and Dice coefficients of 0.7 or more were achieved for all\nstructures compared against manual segmentation ground truth. Impact: This\nmethod facilitates careful investigation of the role of deep grey nuclei by\nenabling the use of conventional T1 data from large public databases, which has\nnot been possible, hitherto, due to lack of robust reproducible segmentation\ntools.", "AI": {"task": "\u5f00\u53d1\u4e00\u79cd\u5feb\u901f\u3001\u51c6\u786e\u4e14\u7a33\u5065\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u5e38\u89c4\u573a\u5f3a\u7684\u7ed3\u6784T1 MRI\u6570\u636e\u4e2d\u5206\u5272\u6df1\u90e8\u7070\u8d28\u6838\u56e2\u3002", "motivation": "\u7f3a\u4e4f\u7528\u4e8e\u5168\u9762\u4e14\u5b8c\u6574\u5206\u5272\u6df1\u90e8\u7070\u8d28\u6838\u56e2\u7684\u5355\u4e00\u8f6f\u4ef6\u5de5\u5177\uff0c\u5f71\u54cd\u4e86\u7814\u7a76\u7684\u53ef\u91cd\u590d\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002", "method": "\u5229\u7528\u767d\u8d28\u6291\u5236\u6210\u50cf\u7684\u6539\u8fdb\u5bf9\u6bd4\u5ea6\uff0c\u901a\u8fc7\u6700\u8fd1\u63d0\u51fa\u7684\u57fa\u4e8e\u76f4\u65b9\u56fe\u7684\u591a\u9879\u5f0f\u5408\u6210\uff08HIPS\uff09\u4ece\u6807\u51c6T1\u5408\u6210\u7c7b\u4f3cWMn\u7684\u56fe\u50cf\uff0c\u7136\u540e\u4f7f\u7528\u591a\u56fe\u8c31\u5206\u5272\u4e0e\u8054\u5408\u6807\u7b7e\u878d\u5408\u6280\u672f\u5206\u5272\u6df1\u90e8\u7070\u8d28\u6838\u56e2\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u573a\u5f3a\uff081.5/3/7\u7279\u65af\u62c9\uff09\u4e0b\u5747\u8868\u73b0\u7a33\u5065\uff0c\u6240\u6709\u7ed3\u6784\u7684Dice\u7cfb\u6570\u5747\u8fbe\u52300.7\u6216\u66f4\u9ad8\uff0c\u4e0e\u624b\u52a8\u5206\u5272\u91d1\u6807\u51c6\u76f8\u6bd4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u5927\u578b\u516c\u5171\u6570\u636e\u5e93\u4e2d\u7684\u5e38\u89c4T1\u6570\u636e\uff0c\u4e3a\u6df1\u5165\u7814\u7a76\u6df1\u90e8\u7070\u8d28\u6838\u56e2\u7684\u4f5c\u7528\u63d0\u4f9b\u4e86\u53ef\u80fd\uff0c\u586b\u8865\u4e86\u6b64\u524d\u7f3a\u4e4f\u7a33\u5065\u3001\u53ef\u91cd\u590d\u5206\u5272\u5de5\u5177\u7684\u7a7a\u767d\u3002"}}
{"id": "2503.21984", "pdf": "https://arxiv.org/pdf/2503.21984", "abs": "https://arxiv.org/abs/2503.21984", "authors": ["Andrew Lesniewski"], "title": "Differential Evolution for Grassmann Manifold Optimization: A Projection Approach", "categories": ["math.OC", "cs.CV", "cs.LG", "cs.NE"], "comment": null, "summary": "We propose a novel evolutionary algorithm for optimizing real-valued\nobjective functions defined on the Grassmann manifold Gr}(k,n), the space of\nall k-dimensional linear subspaces of R^n. While existing optimization\ntechniques on Gr}(k,n) predominantly rely on first- or second-order Riemannian\nmethods, these inherently local methods often struggle with nonconvex or\nmultimodal landscapes. To address this limitation, we adapt the Differential\nEvolution algorithm - a global, population based optimization method - to\noperate effectively on the Grassmannian. Our approach incorporates adaptive\ncontrol parameter schemes, and introduces a projection mechanism that maps\ntrial vectors onto the manifold via QR decomposition. The resulting algorithm\nmaintains feasibility with respect to the manifold structure while enabling\nexploration beyond local neighborhoods. This framework provides a flexible and\ngeometry-aware alternative to classical Riemannian optimization methods and is\nwell-suited to applications in machine learning, signal processing, and\nlow-rank matrix recovery where subspace representations play a central role. We\ntest the methodology on a number of examples of optimization problems on\nGrassmann manifolds.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u8fdb\u5316\u7b97\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u5b9a\u4e49\u5728Grassmann\u6d41\u5f62Gr(k,n)\u4e0a\u7684\u5b9e\u503c\u76ee\u6807\u51fd\u6570\u3002", "motivation": "\u73b0\u6709\u7684Gr(k,n)\u4f18\u5316\u6280\u672f\u4e3b\u8981\u4f9d\u8d56\u5c40\u90e8\u7684\u4e00\u9636\u6216\u4e8c\u9636\u9ece\u66fc\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u975e\u51f8\u6216\u591a\u6a21\u6001\u5730\u5f62\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u5c06\u5dee\u5206\u8fdb\u5316\u7b97\u6cd5\uff08\u4e00\u79cd\u5168\u5c40\u3001\u57fa\u4e8e\u79cd\u7fa4\u7684\u4f18\u5316\u65b9\u6cd5\uff09\u9002\u5e94\u4e8eGrassmann\u6d41\u5f62\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u63a7\u5236\u53c2\u6570\u65b9\u6848\u548c\u901a\u8fc7QR\u5206\u89e3\u5c06\u8bd5\u9a8c\u5411\u91cf\u6295\u5f71\u5230\u6d41\u5f62\u4e0a\u7684\u673a\u5236\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u6d41\u5f62\u7ed3\u6784\u53ef\u884c\u6027\u7684\u540c\u65f6\uff0c\u80fd\u591f\u63a2\u7d22\u5c40\u90e8\u90bb\u57df\u4e4b\u5916\u7684\u7a7a\u95f4\uff0c\u9002\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u3001\u4fe1\u53f7\u5904\u7406\u548c\u4f4e\u79e9\u77e9\u9635\u6062\u590d\u7b49\u5e94\u7528\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u4e3a\u7ecf\u5178\u9ece\u66fc\u4f18\u5316\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u4e14\u51e0\u4f55\u611f\u77e5\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5e76\u5728\u591a\u4e2aGrassmann\u6d41\u5f62\u4f18\u5316\u95ee\u9898\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002"}}
{"id": "2503.22015", "pdf": "https://arxiv.org/pdf/2503.22015", "abs": "https://arxiv.org/abs/2503.22015", "authors": ["Ali Zafari", "Xi Chen", "Shirin Jalali"], "title": "DeCompress: Denoising via Neural Compression", "categories": ["eess.IV", "cs.CV", "cs.IT", "math.IT"], "comment": null, "summary": "Learning-based denoising algorithms achieve state-of-the-art performance\nacross various denoising tasks. However, training such models relies on access\nto large training datasets consisting of clean and noisy image pairs. On the\nother hand, in many imaging applications, such as microscopy, collecting ground\ntruth images is often infeasible. To address this challenge, researchers have\nrecently developed algorithms that can be trained without requiring access to\nground truth data. However, training such models remains computationally\nchallenging and still requires access to large noisy training samples. In this\nwork, inspired by compression-based denoising and recent advances in neural\ncompression, we propose a new compression-based denoising algorithm, which we\nname DeCompress, that i) does not require access to ground truth images, ii)\ndoes not require access to large training dataset - only a single noisy image\nis sufficient, iii) is robust to overfitting, and iv) achieves superior\nperformance compared with zero-shot or unsupervised learning-based denoisers.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u538b\u7f29\u7684\u53bb\u566a\u7b97\u6cd5DeCompress\uff0c\u65e0\u9700\u771f\u5b9e\u5e72\u51c0\u56fe\u50cf\u6216\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u96c6\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u53bb\u566a\u7b97\u6cd5\u4f9d\u8d56\u5927\u89c4\u6a21\u5e72\u51c0-\u566a\u58f0\u56fe\u50cf\u5bf9\u8bad\u7ec3\u6570\u636e\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u96be\u4ee5\u83b7\u53d6\u771f\u5b9e\u56fe\u50cf\u7684\u9886\u57df\uff08\u5982\u663e\u5fae\u955c\u6210\u50cf\uff09\u3002", "method": "\u7ed3\u5408\u538b\u7f29\u53bb\u566a\u548c\u795e\u7ecf\u538b\u7f29\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u4ec5\u9700\u5355\u5f20\u566a\u58f0\u56fe\u50cf\u5373\u53ef\u8bad\u7ec3\u7684\u7b97\u6cd5\u3002", "result": "DeCompress\u5728\u65e0\u9700\u771f\u5b9e\u56fe\u50cf\u6216\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u4f18\u4e8e\u96f6\u6837\u672c\u6216\u65e0\u76d1\u7763\u5b66\u4e60\u53bb\u566a\u65b9\u6cd5\u3002", "conclusion": "DeCompress\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u53bb\u566a\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u96be\u4ee5\u83b7\u53d6\u771f\u5b9e\u6570\u636e\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2503.22052", "pdf": "https://arxiv.org/pdf/2503.22052", "abs": "https://arxiv.org/abs/2503.22052", "authors": ["Jan Hurtado", "Joao P. Maia", "Cesar A. Sierra-Franco", "Alberto Raposo"], "title": "Improving the generalization of deep learning models in the segmentation of mammography images", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Mammography stands as the main screening method for detecting breast cancer\nearly, enhancing treatment success rates. The segmentation of landmark\nstructures in mammography images can aid the medical assessment in the\nevaluation of cancer risk and the image acquisition adequacy. We introduce a\nseries of data-centric strategies aimed at enriching the training data for deep\nlearning-based segmentation of landmark structures. Our approach involves\naugmenting the training samples through annotation-guided image intensity\nmanipulation and style transfer to achieve better generalization than standard\ntraining procedures. These augmentations are applied in a balanced manner to\nensure the model learns to process a diverse range of images generated by\ndifferent vendor equipments while retaining its efficacy on the original data.\nWe present extensive numerical and visual results that demonstrate the superior\ngeneralization capabilities of our methods when compared to the standard\ntraining. For this evaluation, we consider a large dataset that includes\nmammography images generated by different vendor equipments. Further, we\npresent complementary results that show both the strengths and limitations of\nour methods across various scenarios. The accuracy and robustness demonstrated\nin the experiments suggest that our method is well-suited for integration into\nclinical practice.", "AI": {"task": "\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u7b56\u7565\u6539\u8fdb\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u4e73\u817aX\u5149\u56fe\u50cf\u4e2d\u6807\u5fd7\u7ed3\u6784\u7684\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u6807\u5fd7\u7ed3\u6784\u7684\u5206\u5272\u6709\u52a9\u4e8e\u4e73\u817a\u764c\u98ce\u9669\u8bc4\u4f30\u548c\u56fe\u50cf\u91c7\u96c6\u8d28\u91cf\u8bc4\u4f30\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u6ce8\u91ca\u7684\u56fe\u50cf\u5f3a\u5ea6\u8c03\u6574\u548c\u98ce\u683c\u8f6c\u6362\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u5e73\u8861\u5904\u7406\u4e0d\u540c\u5382\u5546\u8bbe\u5907\u751f\u6210\u7684\u56fe\u50cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u548c\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u6807\u51c6\u8bad\u7ec3\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u5408\u4e34\u5e8a\u5b9e\u8df5\u5e94\u7528\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2503.22122", "pdf": "https://arxiv.org/pdf/2503.22122", "abs": "https://arxiv.org/abs/2503.22122", "authors": ["Puzhen Yuan", "Angyuan Ma", "Yunchao Yao", "Huaxiu Yao", "Masayoshi Tomizuka", "Mingyu Ding"], "title": "REMAC: Self-Reflective and Self-Evolving Multi-Agent Collaboration for Long-Horizon Robot Manipulation", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) have demonstrated remarkable capabilities in\nrobotic planning, particularly for long-horizon tasks that require a holistic\nunderstanding of the environment for task decomposition. Existing methods\ntypically rely on prior environmental knowledge or carefully designed\ntask-specific prompts, making them struggle with dynamic scene changes or\nunexpected task conditions, e.g., a robot attempting to put a carrot in the\nmicrowave but finds the door was closed. Such challenges underscore two\ncritical issues: adaptability and efficiency. To address them, in this work, we\npropose an adaptive multi-agent planning framework, termed REMAC, that enables\nefficient, scene-agnostic multi-robot long-horizon task planning and execution\nthrough continuous reflection and self-evolution. REMAC incorporates two key\nmodules: a self-reflection module performing pre-condition and post-condition\nchecks in the loop to evaluate progress and refine plans, and a self-evolvement\nmodule dynamically adapting plans based on scene-specific reasoning. It offers\nseveral appealing benefits: 1) Robots can initially explore and reason about\nthe environment without complex prompt design. 2) Robots can keep reflecting on\npotential planning errors and adapting the plan based on task-specific\ninsights. 3) After iterations, a robot can call another one to coordinate tasks\nin parallel, maximizing the task execution efficiency. To validate REMAC's\neffectiveness, we build a multi-agent environment for long-horizon robot\nmanipulation and navigation based on RoboCasa, featuring 4 task categories with\n27 task styles and 50+ different objects. Based on it, we further benchmark\nstate-of-the-art reasoning models, including DeepSeek-R1, o3-mini, QwQ, and\nGrok3, demonstrating REMAC's superiority by boosting average success rates by\n40% and execution efficiency by 52.7% over the single robot baseline.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u81ea\u9002\u5e94\u591a\u667a\u80fd\u4f53\u89c4\u5212\u6846\u67b6REMAC\uff0c\u7528\u4e8e\u9ad8\u6548\u3001\u573a\u666f\u65e0\u5173\u7684\u591a\u673a\u5668\u4eba\u957f\u65f6\u7a0b\u4efb\u52a1\u89c4\u5212\u4e0e\u6267\u884c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5148\u9a8c\u73af\u5883\u77e5\u8bc6\u6216\u7279\u5b9a\u4efb\u52a1\u63d0\u793a\uff0c\u96be\u4ee5\u5e94\u5bf9\u52a8\u6001\u573a\u666f\u53d8\u5316\u6216\u610f\u5916\u4efb\u52a1\u6761\u4ef6\uff0c\u4e9f\u9700\u89e3\u51b3\u9002\u5e94\u6027\u548c\u6548\u7387\u95ee\u9898\u3002", "method": "REMAC\u6846\u67b6\u5305\u542b\u81ea\u53cd\u601d\u6a21\u5757\uff08\u5faa\u73af\u8fdb\u884c\u524d\u6761\u4ef6\u548c\u540e\u6761\u4ef6\u68c0\u67e5\uff09\u548c\u81ea\u8fdb\u5316\u6a21\u5757\uff08\u52a8\u6001\u8c03\u6574\u8ba1\u5212\uff09\uff0c\u652f\u6301\u591a\u673a\u5668\u4eba\u5e76\u884c\u534f\u4f5c\u3002", "result": "\u5728\u57fa\u4e8eRoboCasa\u7684\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\uff0cREMAC\u5c06\u5e73\u5747\u6210\u529f\u7387\u63d0\u534740%\uff0c\u6267\u884c\u6548\u7387\u63d0\u9ad852.7%\u3002", "conclusion": "REMAC\u901a\u8fc7\u6301\u7eed\u53cd\u601d\u548c\u81ea\u9002\u5e94\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u673a\u5668\u4eba\u957f\u65f6\u7a0b\u4efb\u52a1\u7684\u89c4\u5212\u4e0e\u6267\u884c\u80fd\u529b\u3002"}}
{"id": "2503.22138", "pdf": "https://arxiv.org/pdf/2503.22138", "abs": "https://arxiv.org/abs/2503.22138", "authors": ["Changchang Sun", "Gaowen Liu", "Charles Fleming", "Yan Yan"], "title": "Enhancing Dance-to-Music Generation via Negative Conditioning Latent Diffusion Model", "categories": ["cs.SD", "cs.CV", "eess.AS"], "comment": null, "summary": "Conditional diffusion models have gained increasing attention since their\nimpressive results for cross-modal synthesis, where the strong alignment\nbetween conditioning input and generated output can be achieved by training a\ntime-conditioned U-Net augmented with cross-attention mechanism. In this paper,\nwe focus on the problem of generating music synchronized with rhythmic visual\ncues of the given dance video. Considering that bi-directional guidance is more\nbeneficial for training a diffusion model, we propose to enhance the quality of\ngenerated music and its synchronization with dance videos by adopting both\npositive rhythmic information and negative ones (PN-Diffusion) as conditions,\nwhere a dual diffusion and reverse processes is devised. Specifically, to train\na sequential multi-modal U-Net structure, PN-Diffusion consists of a noise\nprediction objective for positive conditioning and an additional noise\nprediction objective for negative conditioning. To accurately define and select\nboth positive and negative conditioning, we ingeniously utilize temporal\ncorrelations in dance videos, capturing positive and negative rhythmic cues by\nplaying them forward and backward, respectively. Through subjective and\nobjective evaluations of input-output correspondence in terms of dance-music\nbeat alignment and the quality of generated music, experimental results on the\nAIST++ and TikTok dance video datasets demonstrate that our model outperforms\nSOTA dance-to-music generation models.", "AI": {"task": "\u751f\u6210\u4e0e\u821e\u8e48\u89c6\u9891\u8282\u594f\u540c\u6b65\u7684\u97f3\u4e50\u3002", "motivation": "\u5229\u7528\u53cc\u5411\u6307\u5bfc\uff08\u6b63\u8d1f\u8282\u594f\u4fe1\u606f\uff09\u63d0\u5347\u6269\u6563\u6a21\u578b\u5728\u821e\u8e48\u4e0e\u97f3\u4e50\u540c\u6b65\u751f\u6210\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51faPN-Diffusion\u65b9\u6cd5\uff0c\u91c7\u7528\u53cc\u6269\u6563\u548c\u53cd\u5411\u8fc7\u7a0b\uff0c\u901a\u8fc7\u6b63\u8d1f\u8282\u594f\u4fe1\u606f\u8bad\u7ec3\u591a\u6a21\u6001U-Net\u7ed3\u6784\u3002", "result": "\u5728AIST++\u548cTikTok\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u5728\u821e\u8e48-\u97f3\u4e50\u8282\u62cd\u5bf9\u9f50\u548c\u751f\u6210\u97f3\u4e50\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "PN-Diffusion\u901a\u8fc7\u53cc\u5411\u8282\u594f\u6761\u4ef6\u663e\u8457\u63d0\u5347\u4e86\u821e\u8e48\u89c6\u9891\u4e0e\u751f\u6210\u97f3\u4e50\u7684\u540c\u6b65\u6027\u548c\u97f3\u4e50\u8d28\u91cf\u3002"}}
{"id": "2503.22140", "pdf": "https://arxiv.org/pdf/2503.22140", "abs": "https://arxiv.org/abs/2503.22140", "authors": ["Chang Cai", "Xiaojun Yuan", "Ying-Jun Angela Zhang"], "title": "Score-Based Turbo Message Passing for Plug-and-Play Compressive Image Recovery", "categories": ["eess.IV", "cs.CV", "eess.SP"], "comment": null, "summary": "Message passing algorithms have been tailored for compressive imaging\napplications by plugging in different types of off-the-shelf image denoisers.\nThese off-the-shelf denoisers mostly rely on some generic or hand-crafted\npriors for denoising. Due to their insufficient accuracy in capturing the true\nimage prior, these methods often fail to produce satisfactory results,\nespecially in largely underdetermined scenarios. On the other hand, score-based\ngenerative modeling offers a promising way to accurately characterize the\nsophisticated image distribution. In this paper, by exploiting the close\nrelation between score-based modeling and empirical Bayes-optimal denoising, we\ndevise a message passing framework that integrates a score-based minimum mean\nsquared error (MMSE) denoiser for compressive image recovery. This framework is\nfirmly rooted in Bayesian formalism, in which state evolution (SE) equations\naccurately predict its asymptotic performance. Experiments on the FFHQ dataset\ndemonstrate that our method strikes a significantly better\nperformance-complexity tradeoff than conventional message passing, regularized\nlinear regression, and score-based posterior sampling baselines. Remarkably,\nour method typically requires less than 20 neural function evaluations (NFEs)\nto converge.", "AI": {"task": "\u8bbe\u8ba1\u4e00\u79cd\u57fa\u4e8e\u6d88\u606f\u4f20\u9012\u6846\u67b6\u7684\u538b\u7f29\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\uff0c\u96c6\u6210\u5206\u6570\u751f\u6210\u5efa\u6a21\u7684MMSE\u53bb\u566a\u5668\u3002", "motivation": "\u4f20\u7edf\u6d88\u606f\u4f20\u9012\u7b97\u6cd5\u4f9d\u8d56\u901a\u7528\u6216\u624b\u5de5\u8bbe\u8ba1\u7684\u5148\u9a8c\u53bb\u566a\u5668\uff0c\u5728\u9ad8\u5ea6\u6b20\u5b9a\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u5206\u6570\u751f\u6210\u5efa\u6a21\u80fd\u66f4\u51c6\u786e\u5730\u6355\u6349\u56fe\u50cf\u5206\u5e03\u3002", "method": "\u5229\u7528\u5206\u6570\u751f\u6210\u5efa\u6a21\u4e0e\u7ecf\u9a8c\u8d1d\u53f6\u65af\u6700\u4f18\u53bb\u566a\u7684\u7d27\u5bc6\u5173\u7cfb\uff0c\u63d0\u51fa\u4e00\u79cd\u6d88\u606f\u4f20\u9012\u6846\u67b6\uff0c\u96c6\u6210\u5206\u6570MMSE\u53bb\u566a\u5668\u3002", "result": "\u5728FFHQ\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0e\u590d\u6742\u5ea6\u4e4b\u95f4\u53d6\u5f97\u663e\u8457\u66f4\u597d\u7684\u5e73\u8861\uff0c\u901a\u5e38\u9700\u8981\u5c11\u4e8e20\u6b21\u795e\u7ecf\u51fd\u6570\u8bc4\u4f30\u5373\u53ef\u6536\u655b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u538b\u7f29\u56fe\u50cf\u6062\u590d\u4e2d\u4f18\u4e8e\u4f20\u7edf\u6d88\u606f\u4f20\u9012\u3001\u6b63\u5219\u5316\u7ebf\u6027\u56de\u5f52\u548c\u5206\u6570\u540e\u9a8c\u91c7\u6837\u57fa\u7ebf\u3002"}}
{"id": "2503.22143", "pdf": "https://arxiv.org/pdf/2503.22143", "abs": "https://arxiv.org/abs/2503.22143", "authors": ["Sungyu Jeong", "Won Joon Choi", "Junung Choi", "Anik Biswas", "Byungsub Kim"], "title": "A Self-Supervised Learning of a Foundation Model for Analog Layout Design Automation", "categories": ["eess.SP", "cs.AI", "cs.CV", "cs.LG"], "comment": "8 pages, 11 figures", "summary": "We propose a UNet-based foundation model and its self-supervised learning\nmethod to address two key challenges: 1) lack of qualified annotated analog\nlayout data, and 2) excessive variety in analog layout design tasks. For\nself-supervised learning, we propose random patch sampling and random masking\ntechniques automatically to obtain enough training data from a small\nunannotated layout dataset. The obtained data are greatly augmented, less\nbiased, equally sized, and contain enough information for excessive varieties\nof qualified layout patterns. By pre-training with the obtained data, the\nproposed foundation model can learn implicit general knowledge on layout\npatterns so that it can be fine-tuned for various downstream layout tasks with\nsmall task-specific datasets. Fine-tuning provides an efficient and\nconsolidated methodology for diverse downstream tasks, reducing the enormous\nhuman effort to develop a model per task separately. In experiments, the\nfoundation model was pre-trained using 324,000 samples obtained from 6\nsilicon-proved manually designed analog circuits, then it was fine-tuned for\nthe five example downstream tasks: generating contacts, vias, dummy fingers,\nN-wells, and metal routings. The fine-tuned models successfully performed these\ntasks for more than one thousand unseen layout inputs, generating DRC/LVS-clean\nlayouts for 96.6% of samples. Compared with training the model from scratch for\nthe metal routing task, fine-tuning required only 1/8 of the data to achieve\nthe same dice score of 0.95. With the same data, fine-tuning achieved a 90%\nlower validation loss and a 40% higher benchmark score than training from\nscratch.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eUNet\u7684\u57fa\u7840\u6a21\u578b\u53ca\u5176\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u6a21\u62df\u5e03\u5c40\u6570\u636e\u6807\u6ce8\u4e0d\u8db3\u548c\u4efb\u52a1\u591a\u6837\u6027\u8fc7\u9ad8\u7684\u95ee\u9898\u3002", "motivation": "\u6a21\u62df\u5e03\u5c40\u8bbe\u8ba1\u9762\u4e34\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u548c\u4efb\u52a1\u591a\u6837\u6027\u7684\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u968f\u673a\u5757\u91c7\u6837\u548c\u968f\u673a\u63a9\u7801\u6280\u672f\u8fdb\u884c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u4ece\u5c11\u91cf\u672a\u6807\u6ce8\u6570\u636e\u4e2d\u751f\u6210\u589e\u5f3a\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u9002\u5e94\u4e0d\u540c\u4e0b\u6e38\u4efb\u52a1\u3002", "result": "\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u4e94\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u751f\u621096.6%\u7684DRC/LVS\u5e72\u51c0\u5e03\u5c40\uff0c\u5fae\u8c03\u4ec5\u97001/8\u6570\u636e\u5373\u53ef\u8fbe\u5230\u4e0e\u4ece\u5934\u8bad\u7ec3\u76f8\u540c\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u4eba\u5de5\u6807\u6ce8\u9700\u6c42\uff0c\u4e3a\u591a\u6837\u5316\u7684\u6a21\u62df\u5e03\u5c40\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2503.22159", "pdf": "https://arxiv.org/pdf/2503.22159", "abs": "https://arxiv.org/abs/2503.22159", "authors": ["Hao Feng", "Hao Sun", "Wei Xie"], "title": "Disentangled 4D Gaussian Splatting: Towards Faster and More Efficient Dynamic Scene Rendering", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Novel-view synthesis (NVS) for dynamic scenes from 2D images presents\nsignificant challenges due to the spatial complexity and temporal variability\nof such scenes. Recently, inspired by the remarkable success of NVS using 3D\nGaussian Splatting (3DGS), researchers have sought to extend 3D Gaussian models\nto four dimensions (4D) for dynamic novel-view synthesis. However, methods\nbased on 4D rotation and scaling introduce spatiotemporal deformation into the\n4D covariance matrix, necessitating the slicing of 4D Gaussians into 3D\nGaussians. This process increases redundant computations as timestamps\nchange-an inherent characteristic of dynamic scene rendering. Additionally,\nperforming calculations on a four-dimensional matrix is computationally\nintensive. In this paper, we introduce Disentangled 4D Gaussian Splatting\n(Disentangled4DGS), a novel representation and rendering approach that\ndisentangles temporal and spatial deformations, thereby eliminating the\nreliance on 4D matrix computations. We extend the 3DGS rendering process to 4D,\nenabling the projection of temporal and spatial deformations into dynamic 2D\nGaussians in ray space. Consequently, our method facilitates faster dynamic\nscene synthesis. Moreover, it reduces storage requirements by at least 4.5\\%\ndue to our efficient presentation method. Our approach achieves an\nunprecedented average rendering speed of 343 FPS at a resolution of\n$1352\\times1014$ on an RTX 3090 GPU, with experiments across multiple\nbenchmarks demonstrating its competitive performance in both monocular and\nmulti-view scenarios.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aDisentangled4DGS\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u52a8\u6001\u573a\u666f\u7684\u65b0\u89c6\u89d2\u5408\u6210\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e4D\u9ad8\u65af\u6a21\u578b\u7684\u65b9\u6cd5\u56e0\u5f15\u5165\u65f6\u7a7a\u53d8\u5f62\u5bfc\u81f4\u8ba1\u7b97\u5197\u4f59\u548c\u5b58\u50a8\u9700\u6c42\u9ad8\u3002", "method": "\u901a\u8fc7\u89e3\u8026\u65f6\u7a7a\u53d8\u5f62\uff0c\u5c063DGS\u6269\u5c55\u52304D\uff0c\u907f\u514d4D\u77e9\u9635\u8ba1\u7b97\u3002", "result": "\u65b9\u6cd5\u5728RTX 3090 GPU\u4e0a\u8fbe\u5230343 FPS\u7684\u6e32\u67d3\u901f\u5ea6\uff0c\u5b58\u50a8\u9700\u6c42\u964d\u4f4e\u81f3\u5c114.5%\u3002", "conclusion": "Disentangled4DGS\u5728\u52a8\u6001\u573a\u666f\u6e32\u67d3\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u7ade\u4e89\u529b\u3002"}}
{"id": "2503.22176", "pdf": "https://arxiv.org/pdf/2503.22176", "abs": "https://arxiv.org/abs/2503.22176", "authors": ["Bargava Subramanian", "Naveen Kumarasami", "Praveen Shastry", "Kalyan Sivasailam", "Anandakumar D", "Keerthana R", "Mounigasri M", "Abilaasha G", "Kishore Prasath Venkatesh"], "title": "A Multi-Site Study on AI-Driven Pathology Detection and Osteoarthritis Grading from Knee X-Ray", "categories": ["eess.IV", "cs.CV", "68T07"], "comment": "15 pages, 2 figures", "summary": "Introduction: Bone health disorders like osteoarthritis and osteoporosis pose\nmajor global health challenges, often leading to delayed diagnoses due to\nlimited diagnostic tools. This study presents an AI-powered system that\nanalyzes knee X-rays to detect key pathologies, including joint space\nnarrowing, sclerosis, osteophytes, tibial spikes, alignment issues, and soft\ntissue anomalies. It also grades osteoarthritis severity, enabling timely,\npersonalized treatment.\n  Study Design: The research used 1.3 million knee X-rays from a multi-site\nIndian clinical trial across government, private, and SME hospitals. The\ndataset ensured diversity in demographics, imaging equipment, and clinical\nsettings. Rigorous annotation and preprocessing yielded high-quality training\ndatasets for pathology-specific models like ResNet15 for joint space narrowing\nand DenseNet for osteoarthritis grading.\n  Performance: The AI system achieved strong diagnostic accuracy across diverse\nimaging environments. Pathology-specific models excelled in precision, recall,\nand NPV, validated using Mean Squared Error (MSE), Intersection over Union\n(IoU), and Dice coefficient. Subgroup analyses across age, gender, and\nmanufacturer variations confirmed generalizability for real-world applications.\n  Conclusion: This scalable, cost-effective solution for bone health\ndiagnostics demonstrated robust performance in a multi-site trial. It holds\npromise for widespread adoption, especially in resource-limited healthcare\nsettings, transforming bone health management and enabling proactive patient\ncare.", "AI": {"task": "\u5f00\u53d1\u4e00\u79cd\u57fa\u4e8eAI\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u6790\u819d\u5173\u8282X\u5149\u7247\u68c0\u6d4b\u591a\u79cd\u75c5\u7406\u7279\u5f81\u5e76\u8bc4\u4f30\u9aa8\u5173\u8282\u708e\u7684\u4e25\u91cd\u7a0b\u5ea6\u3002", "motivation": "\u9aa8\u5065\u5eb7\u95ee\u9898\uff08\u5982\u9aa8\u5173\u8282\u708e\u548c\u9aa8\u8d28\u758f\u677e\u75c7\uff09\u7684\u65e9\u671f\u8bca\u65ad\u56e0\u8bca\u65ad\u5de5\u5177\u6709\u9650\u800c\u5ef6\u8fdf\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u3001\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528130\u4e07\u5f20\u819d\u5173\u8282X\u5149\u7247\u6784\u5efa\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\uff0c\u91c7\u7528ResNet15\u548cDenseNet\u7b49\u6a21\u578b\u8fdb\u884c\u75c5\u7406\u7279\u5f81\u68c0\u6d4b\u548c\u9aa8\u5173\u8282\u708e\u5206\u7ea7\u3002", "result": "AI\u7cfb\u7edf\u5728\u591a\u6837\u5316\u7684\u6210\u50cf\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u5404\u75c5\u7406\u6a21\u578b\u5728\u7cbe\u786e\u5ea6\u3001\u53ec\u56de\u7387\u548c\u9634\u6027\u9884\u6d4b\u503c\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5AI\u7cfb\u7edf\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u9aa8\u5065\u5eb7\u8bca\u65ad\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u6709\u9650\u7684\u533b\u7597\u73af\u5883\uff0c\u6709\u671b\u6539\u5584\u60a3\u8005\u7ba1\u7406\u3002"}}
{"id": "2503.22177", "pdf": "https://arxiv.org/pdf/2503.22177", "abs": "https://arxiv.org/abs/2503.22177", "authors": ["Shuai Zhang", "Jinliang Wang", "Sujith Konandetails", "Xu Wang", "Danail Stoyanov", "Evangelos B. Mazomenos"], "title": "3D Acetabular Surface Reconstruction from 2D Pre-operative X-ray Images using SRVF Elastic Registration and Deformation Graph", "categories": ["cs.RO", "cs.CV", "eess.IV"], "comment": "10 pages, 3 figures, conference", "summary": "Accurate and reliable selection of the appropriate acetabular cup size is\ncrucial for restoring joint biomechanics in total hip arthroplasty (THA). This\npaper proposes a novel framework that integrates square-root velocity function\n(SRVF)-based elastic shape registration technique with an embedded deformation\n(ED) graph approach to reconstruct the 3D articular surface of the acetabulum\nby fusing multiple views of 2D pre-operative pelvic X-ray images and a\nhemispherical surface model. The SRVF-based elastic registration establishes\n2D-3D correspondences between the parametric hemispherical model and X-ray\nimages, and the ED framework incorporates the SRVF-derived correspondences as\nconstraints to optimize the 3D acetabular surface reconstruction using\nnonlinear least-squares optimization. Validations using both simulation and\nreal patient datasets are performed to demonstrate the robustness and the\npotential clinical value of the proposed algorithm. The reconstruction result\ncan assist surgeons in selecting the correct acetabular cup on the first\nattempt in primary THA, minimising the need for revision surgery.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408SRVF\u5f39\u6027\u5f62\u72b6\u914d\u51c6\u6280\u672f\u548cED\u56fe\u65b9\u6cd5\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u591a\u89c6\u89d22D\u672f\u524d\u9aa8\u76c6X\u5c04\u7ebf\u56fe\u50cf\u548c\u534a\u7403\u8868\u9762\u6a21\u578b\u91cd\u5efa\u9acb\u81fc\u76843D\u5173\u8282\u8868\u9762\u3002", "motivation": "\u51c6\u786e\u53ef\u9760\u7684\u9acb\u81fc\u676f\u5c3a\u5bf8\u9009\u62e9\u5bf9\u5168\u9acb\u5173\u8282\u7f6e\u6362\u672f\u4e2d\u6062\u590d\u5173\u8282\u751f\u7269\u529b\u5b66\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7ed3\u5408SRVF\u5f39\u6027\u914d\u51c6\u5efa\u7acb2D-3D\u5bf9\u5e94\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7ED\u6846\u67b6\u4f18\u53163D\u9acb\u81fc\u8868\u9762\u91cd\u5efa\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u771f\u5b9e\u60a3\u8005\u6570\u636e\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u6f5c\u5728\u4e34\u5e8a\u4ef7\u503c\u3002", "conclusion": "\u8be5\u91cd\u5efa\u7ed3\u679c\u53ef\u5e2e\u52a9\u5916\u79d1\u533b\u751f\u5728\u521d\u6b21\u5168\u9acb\u5173\u8282\u7f6e\u6362\u672f\u4e2d\u9996\u6b21\u9009\u62e9\u6b63\u786e\u7684\u9acb\u81fc\u676f\uff0c\u51cf\u5c11\u7ffb\u4fee\u624b\u672f\u9700\u6c42\u3002"}}
{"id": "2503.22178", "pdf": "https://arxiv.org/pdf/2503.22178", "abs": "https://arxiv.org/abs/2503.22178", "authors": ["Chanhyuk Lee", "Jiho Choi", "Chanryeol Lee", "Donggyun Kim", "Seunghoon Hong"], "title": "AdaRank: Adaptive Rank Pruning for Enhanced Model Merging", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Code Available at: https://github.com/david3684/AdaRank", "summary": "Model merging has emerged as a promising approach for unifying independently\nfine-tuned models into an integrated framework, significantly enhancing\ncomputational efficiency in multi-task learning. Recently, several SVD-based\ntechniques have been introduced to exploit low-rank structures for enhanced\nmerging, but their reliance on such manually designed rank selection often\nleads to cross-task interference and suboptimal performance. In this paper, we\npropose AdaRank, a novel model merging framework that adaptively selects the\nmost beneficial singular directions of task vectors to merge multiple models.\nWe empirically show that the dominant singular components of task vectors can\ncause critical interference with other tasks, and that naive truncation across\ntasks and layers degrades performance. In contrast, AdaRank dynamically prunes\nthe singular components that cause interference and offers an optimal amount of\ninformation to each task vector by learning to prune ranks during test-time via\nentropy minimization. Our analysis demonstrates that such method mitigates\ndetrimental overlaps among tasks, while empirical results show that AdaRank\nconsistently achieves state-of-the-art performance with various backbones and\nnumber of tasks, reducing the performance gap between fine-tuned models to\nnearly 1%.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aAdaRank\u7684\u81ea\u9002\u5e94\u6a21\u578b\u5408\u5e76\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u6a21\u578b\u5408\u5e76\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eSVD\u7684\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\u4f9d\u8d56\u624b\u52a8\u8bbe\u8ba1\u7684\u79e9\u9009\u62e9\uff0c\u5bb9\u6613\u5bfc\u81f4\u4efb\u52a1\u95f4\u5e72\u6270\u548c\u6027\u80fd\u4e0b\u964d\u3002", "method": "AdaRank\u901a\u8fc7\u52a8\u6001\u526a\u679d\u5e72\u6270\u7684\u5947\u5f02\u65b9\u5411\uff0c\u5e76\u901a\u8fc7\u71b5\u6700\u5c0f\u5316\u5728\u6d4b\u8bd5\u65f6\u5b66\u4e60\u526a\u679d\u79e9\uff0c\u81ea\u9002\u5e94\u9009\u62e9\u6700\u4f18\u5947\u5f02\u65b9\u5411\u3002", "result": "AdaRank\u663e\u8457\u51cf\u5c11\u4e86\u4efb\u52a1\u95f4\u7684\u6709\u5bb3\u91cd\u53e0\uff0c\u5e76\u5728\u591a\u79cd\u9aa8\u5e72\u7f51\u7edc\u548c\u4efb\u52a1\u6570\u91cf\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c06\u6027\u80fd\u5dee\u8ddd\u7f29\u5c0f\u81f3\u8fd11%\u3002", "conclusion": "AdaRank\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u81ea\u9002\u5e94\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u6027\u80fd\u3002"}}
{"id": "2503.22182", "pdf": "https://arxiv.org/pdf/2503.22182", "abs": "https://arxiv.org/abs/2503.22182", "authors": ["Jianghao Lin", "Peng Du", "Jiaqi Liu", "Weite Li", "Yong Yu", "Weinan Zhang", "Yang Cao"], "title": "Sell It Before You Make It: Revolutionizing E-Commerce with Personalized AI-Generated Items", "categories": ["cs.IR", "cs.AI", "cs.CV"], "comment": "Under Review", "summary": "E-commerce has revolutionized retail, yet its traditional workflows remain\ninefficient, with significant time and resource costs tied to product design\nand manufacturing inventory. This paper introduces a novel system deployed at\nAlibaba that leverages AI-generated items (AIGI) to address these challenges\nwith personalized text-to-image generation for e-commercial product design.\nAIGI enables an innovative business mode called \"sell it before you make it\",\nwhere merchants can design fashion items and generate photorealistic images\nwith digital models based on textual descriptions. Only when the items have\nreceived a certain number of orders, do the merchants start to produce them,\nwhich largely reduces reliance on physical prototypes and thus accelerates time\nto market. For such a promising application, we identify the underlying key\nscientific challenge, i.e., capturing the users' group-level personalized\npreferences towards multiple generated candidate images. To this end, we\npropose a Personalized Group-Level Preference Alignment Framework for Diffusion\nModels (i.e., PerFusion). We first design PerFusion Reward Model for user\npreference estimation with a feature-crossing-based personalized plug-in. Then\nwe develop PerFusion with a personalized adaptive network to model diverse\npreferences across users, and meanwhile derive the group-level preference\noptimization objective to capture the comparative behaviors among multiple\ncandidates. Both offline and online experiments demonstrate the effectiveness\nof our proposed algorithm. The AI-generated items have achieved over 13%\nrelative improvements for both click-through rate and conversion rate compared\nto their human-designed counterparts, validating the revolutionary potential of\nAI-generated items for e-commercial platforms.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eAI\u751f\u6210\u7269\u54c1\uff08AIGI\uff09\u7684\u4e2a\u6027\u5316\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7cfb\u7edf\uff0c\u7528\u4e8e\u7535\u5b50\u5546\u52a1\u4ea7\u54c1\u8bbe\u8ba1\u3002", "motivation": "\u4f20\u7edf\u7535\u5b50\u5546\u52a1\u5de5\u4f5c\u6d41\u7a0b\u6548\u7387\u4f4e\u4e0b\uff0c\u4ea7\u54c1\u8bbe\u8ba1\u548c\u5236\u9020\u5e93\u5b58\u6210\u672c\u9ad8\uff0cAIGI\u901a\u8fc7\u201c\u5148\u9500\u552e\u540e\u751f\u4ea7\u201d\u6a21\u5f0f\u51cf\u5c11\u5bf9\u7269\u7406\u539f\u578b\u7684\u4f9d\u8d56\uff0c\u52a0\u901f\u4e0a\u5e02\u65f6\u95f4\u3002", "method": "\u63d0\u51fa\u4e2a\u6027\u5316\u7fa4\u4f53\u7ea7\u504f\u597d\u5bf9\u9f50\u6846\u67b6\uff08PerFusion\uff09\uff0c\u5305\u62ecPerFusion\u5956\u52b1\u6a21\u578b\u548c\u4e2a\u6027\u5316\u81ea\u9002\u5e94\u7f51\u7edc\uff0c\u7528\u4e8e\u5efa\u6a21\u7528\u6237\u504f\u597d\u5e76\u4f18\u5316\u7fa4\u4f53\u7ea7\u504f\u597d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAI\u751f\u6210\u7269\u54c1\u7684\u70b9\u51fb\u7387\u548c\u8f6c\u5316\u7387\u76f8\u6bd4\u4eba\u5de5\u8bbe\u8ba1\u7269\u54c1\u63d0\u5347\u4e8613%\u4ee5\u4e0a\u3002", "conclusion": "AI\u751f\u6210\u7269\u54c1\u5728\u7535\u5b50\u5546\u52a1\u5e73\u53f0\u5177\u6709\u9769\u547d\u6027\u6f5c\u529b\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u6548\u7387\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u3002"}}
{"id": "2503.22200", "pdf": "https://arxiv.org/pdf/2503.22200", "abs": "https://arxiv.org/abs/2503.22200", "authors": ["Haomin Zhang", "Sizhe Shan", "Haoyu Wang", "Zihao Chen", "Xiulong Liu", "Chaofan Ding", "Xinhan Di"], "title": "Enhance Generation Quality of Flow Matching V2A Model via Multi-Step CoT-Like Guidance and Combined Preference Optimization", "categories": ["cs.SD", "cs.CV", "eess.AS"], "comment": "10 pages, 4 figures", "summary": "Creating high-quality sound effects from videos and text prompts requires\nprecise alignment between visual and audio domains, both semantically and\ntemporally, along with step-by-step guidance for professional audio generation.\nHowever, current state-of-the-art video-guided audio generation models often\nfall short of producing high-quality audio for both general and specialized use\ncases. To address this challenge, we introduce a multi-stage, multi-modal,\nend-to-end generative framework with Chain-of-Thought-like (CoT-like) guidance\nlearning, termed Chain-of-Perform (CoP). First, we employ a transformer-based\nnetwork architecture designed to achieve CoP guidance, enabling the generation\nof both general and professional audio. Second, we implement a multi-stage\ntraining framework that follows step-by-step guidance to ensure the generation\nof high-quality sound effects. Third, we develop a CoP multi-modal dataset,\nguided by video, to support step-by-step sound effects generation. Evaluation\nresults highlight the advantages of the proposed multi-stage CoP generative\nframework compared to the state-of-the-art models on a variety of datasets,\nwith FAD 0.79 to 0.74 (+6.33%), CLIP 16.12 to 17.70 (+9.80%) on VGGSound,\nSI-SDR 1.98dB to 3.35dB (+69.19%), MOS 2.94 to 3.49(+18.71%) on PianoYT-2h, and\nSI-SDR 2.22dB to 3.21dB (+44.59%), MOS 3.07 to 3.42 (+11.40%) on Piano-10h.", "AI": {"task": "\u901a\u8fc7\u591a\u9636\u6bb5\u3001\u591a\u6a21\u6001\u7684\u7aef\u5230\u7aef\u751f\u6210\u6846\u67b6\uff08CoP\uff09\u4ece\u89c6\u9891\u548c\u6587\u672c\u63d0\u793a\u4e2d\u751f\u6210\u9ad8\u8d28\u91cf\u97f3\u6548\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u5f15\u5bfc\u7684\u97f3\u6548\u751f\u6210\u6a21\u578b\u5728\u901a\u7528\u548c\u4e13\u4e1a\u7528\u4f8b\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u6539\u8fdb\u89c6\u89c9\u4e0e\u97f3\u9891\u7684\u8bed\u4e49\u548c\u65f6\u95f4\u5bf9\u9f50\u3002", "method": "\u91c7\u7528\u57fa\u4e8eTransformer\u7684\u7f51\u7edc\u67b6\u6784\uff0c\u5b9e\u73b0CoP\u5f15\u5bfc\u5b66\u4e60\uff1b\u591a\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff1b\u6784\u5efaCoP\u591a\u6a21\u6001\u6570\u636e\u96c6\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5982FAD\u3001CLIP\u3001SI-SDR\u548cMOS\u6307\u6807\u663e\u8457\u63d0\u5347\u3002", "conclusion": "CoP\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u97f3\u6548\u751f\u6210\u7684\u8d28\u91cf\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2503.22205", "pdf": "https://arxiv.org/pdf/2503.22205", "abs": "https://arxiv.org/abs/2503.22205", "authors": ["YangTian Yan", "Jinyu Tian"], "title": "Data-Free Universal Attack by Exploiting the Intrinsic Vulnerability of Deep Models", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted in AAAI 2025", "summary": "Deep neural networks (DNNs) are susceptible to Universal Adversarial\nPerturbations (UAPs), which are instance agnostic perturbations that can\ndeceive a target model across a wide range of samples. Unlike instance-specific\nadversarial examples, UAPs present a greater challenge as they must generalize\nacross different samples and models. Generating UAPs typically requires access\nto numerous examples, which is a strong assumption in real-world tasks. In this\npaper, we propose a novel data-free method called Intrinsic UAP (IntriUAP), by\nexploiting the intrinsic vulnerabilities of deep models. We analyze a series of\npopular deep models composed of linear and nonlinear layers with a Lipschitz\nconstant of 1, revealing that the vulnerability of these models is\npredominantly influenced by their linear components. Based on this observation,\nwe leverage the ill-conditioned nature of the linear components by aligning the\nUAP with the right singular vectors corresponding to the maximum singular value\nof each linear layer. Remarkably, our method achieves highly competitive\nperformance in attacking popular image classification deep models without using\nany image samples. We also evaluate the black-box attack performance of our\nmethod, showing that it matches the state-of-the-art baseline for data-free\nmethods on models that conform to our theoretical framework. Beyond the\ndata-free assumption, IntriUAP also operates under a weaker assumption, where\nthe adversary only can access a few of the victim model's layers. Experiments\ndemonstrate that the attack success rate decreases by only 4% when the\nadversary has access to just 50% of the linear layers in the victim model.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u6570\u636e\u7684\u65b9\u6cd5\uff08IntriUAP\uff09\uff0c\u5229\u7528\u6df1\u5ea6\u6a21\u578b\u7684\u56fa\u6709\u6f0f\u6d1e\u751f\u6210\u901a\u7528\u5bf9\u6297\u6270\u52a8\uff08UAPs\uff09\u3002", "motivation": "\u73b0\u6709\u751f\u6210UAPs\u7684\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6570\u636e\uff0c\u800c\u5b9e\u9645\u4efb\u52a1\u4e2d\u96be\u4ee5\u6ee1\u8db3\u8fd9\u4e00\u5047\u8bbe\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6a21\u578b\u7684\u7ebf\u6027\u7ec4\u4ef6\uff0c\u5229\u7528\u5176\u75c5\u6001\u6027\u8d28\uff0c\u5c06UAP\u4e0e\u7ebf\u6027\u5c42\u7684\u6700\u5927\u5947\u5f02\u503c\u5bf9\u5e94\u7684\u53f3\u5947\u5f02\u5411\u91cf\u5bf9\u9f50\u3002", "result": "\u5728\u65e0\u9700\u56fe\u50cf\u6837\u672c\u7684\u60c5\u51b5\u4e0b\uff0c\u653b\u51fb\u6027\u80fd\u4e0e\u73b0\u6709\u6570\u636e\u65e0\u5173\u65b9\u6cd5\u76f8\u5f53\uff0c\u4e14\u5728\u4ec5\u8bbf\u95ee\u90e8\u5206\u6a21\u578b\u5c42\u65f6\u653b\u51fb\u6210\u529f\u7387\u4ec5\u4e0b\u964d4%\u3002", "conclusion": "IntriUAP\u5728\u6570\u636e\u65e0\u5173\u548c\u5f31\u5047\u8bbe\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5bf9\u6297\u653b\u51fb\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2503.22208", "pdf": "https://arxiv.org/pdf/2503.22208", "abs": "https://arxiv.org/abs/2503.22208", "authors": ["Yunming Liang", "Zihao Chen", "Chaofan Ding", "Xinhan Di"], "title": "DeepSound-V1: Start to Think Step-by-Step in the Audio Generation from Videos", "categories": ["cs.SD", "cs.CV", "eess.AS"], "comment": "11 pages, 6 figures", "summary": "Currently, high-quality, synchronized audio is synthesized from video and\noptional text inputs using various multi-modal joint learning frameworks.\nHowever, the precise alignment between the visual and generated audio domains\nremains far from satisfactory. One key factor is the lack of sufficient\ntemporal and semantic alignment annotations in open-source video-audio and\ntext-audio benchmarks. Therefore, we propose a framework for audio generation\nfrom videos, leveraging the internal chain-of-thought (CoT) of a multi-modal\nlarge language model (MLLM) to enable step-by-step reasoning without requiring\nadditional annotations. Additionally, a corresponding multi-modal reasoning\ndataset is constructed to facilitate the learning of initial reasoning in audio\ngeneration. In the experiments, we demonstrate the effectiveness of the\nproposed framework in reducing misalignment (voice-over) in generated audio and\nachieving competitive performance compared to various state-of-the-art models.\nThe evaluation results show that the proposed method outperforms\nstate-of-the-art approaches across multiple metrics. Specifically, the F DP\naSST indicator is reduced by up to 10.07%, the F DP AN N s indicator by up to\n11.62%, and the F DV GG indicator by up to 38.61%. Furthermore, the IS\nindicator improves by up to 4.95%, the IB-score indicator increases by up to\n6.39%, and the DeSync indicator is reduced by up to 0.89%.", "AI": {"task": "\u4ece\u89c6\u9891\u548c\u53ef\u9009\u6587\u672c\u8f93\u5165\u4e2d\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u540c\u6b65\u7684\u97f3\u9891\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u89c6\u89c9\u548c\u751f\u6210\u97f3\u9891\u9886\u57df\u7684\u7cbe\u786e\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u5f00\u6e90\u89c6\u9891-\u97f3\u9891\u548c\u6587\u672c-\u97f3\u9891\u57fa\u51c6\u4e2d\u7f3a\u4e4f\u8db3\u591f\u7684\u65f6\u5e8f\u548c\u8bed\u4e49\u5bf9\u9f50\u6807\u6ce8\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u5185\u90e8\u601d\u7ef4\u94fe\uff08CoT\uff09\u8fdb\u884c\u9010\u6b65\u63a8\u7406\uff0c\u65e0\u9700\u989d\u5916\u6807\u6ce8\uff0c\u5e76\u6784\u5efa\u591a\u6a21\u6001\u63a8\u7406\u6570\u636e\u96c6\u4ee5\u652f\u6301\u521d\u59cb\u63a8\u7406\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u4e86\u751f\u6210\u97f3\u9891\u4e2d\u7684\u4e0d\u5bf9\u9f50\uff08\u914d\u97f3\uff09\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u3002", "conclusion": "\u6240\u63d0\u6846\u67b6\u5728\u51cf\u5c11\u97f3\u9891\u751f\u6210\u4e2d\u7684\u4e0d\u5bf9\u9f50\u548c\u63d0\u5347\u6027\u80fd\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2503.22236", "pdf": "https://arxiv.org/pdf/2503.22236", "abs": "https://arxiv.org/abs/2503.22236", "authors": ["Chongjie Ye", "Yushuang Wu", "Ziteng Lu", "Jiahao Chang", "Xiaoyang Guo", "Jiaqing Zhou", "Hao Zhao", "Xiaoguang Han"], "title": "Hi3DGen: High-fidelity 3D Geometry Generation from Images via Normal Bridging", "categories": ["cs.GR", "cs.CV"], "comment": "https://stable-x.github.io/Hi3DGen/static", "summary": "With the growing demand for high-fidelity 3D models from 2D images, existing\nmethods still face significant challenges in accurately reproducing\nfine-grained geometric details due to limitations in domain gaps and inherent\nambiguities in RGB images. To address these issues, we propose Hi3DGen, a novel\nframework for generating high-fidelity 3D geometry from images via normal\nbridging. Hi3DGen consists of three key components: (1) an image-to-normal\nestimator that decouples the low-high frequency image pattern with noise\ninjection and dual-stream training to achieve generalizable, stable, and sharp\nestimation; (2) a normal-to-geometry learning approach that uses\nnormal-regularized latent diffusion learning to enhance 3D geometry generation\nfidelity; and (3) a 3D data synthesis pipeline that constructs a high-quality\ndataset to support training. Extensive experiments demonstrate the\neffectiveness and superiority of our framework in generating rich geometric\ndetails, outperforming state-of-the-art methods in terms of fidelity. Our work\nprovides a new direction for high-fidelity 3D geometry generation from images\nby leveraging normal maps as an intermediate representation.", "AI": {"task": "\u63d0\u51faHi3DGen\u6846\u67b6\uff0c\u901a\u8fc7\u6cd5\u7ebf\u6865\u63a5\u4ece2D\u56fe\u50cf\u751f\u6210\u9ad8\u4fdd\u771f3D\u51e0\u4f55\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4eceRGB\u56fe\u50cf\u51c6\u786e\u8fd8\u539f\u7ec6\u7c92\u5ea6\u51e0\u4f55\u7ec6\u8282\u65f6\u9762\u4e34\u9886\u57df\u5dee\u8ddd\u548c\u56fa\u6709\u6a21\u7cca\u6027\u7684\u6311\u6218\u3002", "method": "Hi3DGen\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u56fe\u50cf\u5230\u6cd5\u7ebf\u4f30\u8ba1\u5668\u3001\u6cd5\u7ebf\u5230\u51e0\u4f55\u5b66\u4e60\u65b9\u6cd5\u4ee5\u53ca3D\u6570\u636e\u5408\u6210\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHi3DGen\u5728\u751f\u6210\u4e30\u5bcc\u51e0\u4f55\u7ec6\u8282\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u901a\u8fc7\u6cd5\u7ebf\u56fe\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0c\u4e3a\u9ad8\u4fdd\u771f3D\u51e0\u4f55\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2503.22263", "pdf": "https://arxiv.org/pdf/2503.22263", "abs": "https://arxiv.org/abs/2503.22263", "authors": ["Dongping Liao", "Xitong Gao", "Yabo Xu", "Chengzhong Xu"], "title": "FLIP: Towards Comprehensive and Reliable Evaluation of Federated Prompt Learning", "categories": ["cs.LG", "cs.CV"], "comment": "https://github.com/0-ml/flip", "summary": "The increasing emphasis on privacy and data security has driven the adoption\nof federated learning, a decentralized approach to train machine learning\nmodels without sharing raw data. Prompt learning, which fine-tunes prompt\nembeddings of pretrained models, offers significant advantages in federated\nsettings by reducing computational costs and communication overheads while\nleveraging the strong performance and generalization capabilities of\nvision-language models such as CLIP. This paper addresses the intersection of\nfederated learning and prompt learning, particularly for vision-language\nmodels. In this work, we introduce a comprehensive framework, named FLIP, to\nevaluate federated prompt learning algorithms. FLIP assesses the performance of\n8 state-of-the-art federated prompt learning methods across 4 federated\nlearning protocols and 12 open datasets, considering 6 distinct evaluation\nscenarios. Our findings demonstrate that prompt learning maintains strong\ngeneralization performance in both in-distribution and out-of-distribution\nsettings with minimal resource consumption. This work highlights the\neffectiveness of federated prompt learning in environments characterized by\ndata scarcity, unseen classes, and cross-domain distributional shifts. We\nopen-source the code for all implemented algorithms in FLIP to facilitate\nfurther research in this domain.", "AI": {"task": "\u7814\u7a76\u8054\u90a6\u5b66\u4e60\u4e0e\u63d0\u793a\u5b66\u4e60\u7684\u7ed3\u5408\uff0c\u7279\u522b\u662f\u9488\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u540d\u4e3aFLIP\u7684\u6846\u67b6\u6765\u8bc4\u4f30\u8054\u90a6\u63d0\u793a\u5b66\u4e60\u7b97\u6cd5\u3002", "motivation": "\u9690\u79c1\u548c\u6570\u636e\u5b89\u5168\u7684\u91cd\u8981\u6027\u63a8\u52a8\u4e86\u8054\u90a6\u5b66\u4e60\u7684\u91c7\u7528\uff0c\u800c\u63d0\u793a\u5b66\u4e60\u5728\u8054\u90a6\u73af\u5883\u4e2d\u5177\u6709\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u548c\u901a\u4fe1\u5f00\u9500\u7684\u4f18\u52bf\u3002", "method": "\u5f15\u5165FLIP\u6846\u67b6\uff0c\u8bc4\u4f308\u79cd\u6700\u5148\u8fdb\u7684\u8054\u90a6\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u6db5\u76d64\u79cd\u8054\u90a6\u5b66\u4e60\u534f\u8bae\u548c12\u4e2a\u5f00\u653e\u6570\u636e\u96c6\uff0c\u8003\u86516\u79cd\u4e0d\u540c\u7684\u8bc4\u4f30\u573a\u666f\u3002", "result": "\u63d0\u793a\u5b66\u4e60\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u8bbe\u7f6e\u4e2d\u5747\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u4e14\u8d44\u6e90\u6d88\u8017\u6781\u4f4e\u3002", "conclusion": "\u8054\u90a6\u63d0\u793a\u5b66\u4e60\u5728\u6570\u636e\u7a00\u7f3a\u3001\u672a\u89c1\u7c7b\u522b\u548c\u8de8\u57df\u5206\u5e03\u504f\u79fb\u7684\u73af\u5883\u4e2d\u975e\u5e38\u6709\u6548\uff0c\u7814\u7a76\u5f00\u6e90\u4e86FLIP\u4ee3\u7801\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2503.22271", "pdf": "https://arxiv.org/pdf/2503.22271", "abs": "https://arxiv.org/abs/2503.22271", "authors": ["Omini Rathore", "Richard Paul", "Abigail Morrison", "Hanno Scharr", "Elisabeth Pfaehler"], "title": "Efficient Epistemic Uncertainty Estimation in Cerebrovascular Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Brain vessel segmentation of MR scans is a critical step in the diagnosis of\ncerebrovascular diseases. Due to the fine vessel structure, manual vessel\nsegmentation is time consuming. Therefore, automatic deep learning (DL) based\nsegmentation techniques are intensively investigated. As conventional DL models\nyield a high complexity and lack an indication of decision reliability, they\nare often considered as not trustworthy. This work aims to increase trust in DL\nbased models by incorporating epistemic uncertainty quantification into\ncerebrovascular segmentation models for the first time. By implementing an\nefficient ensemble model combining the advantages of Bayesian Approximation and\nDeep Ensembles, we aim to overcome the high computational costs of conventional\nprobabilistic networks. Areas of high model uncertainty and erroneous\npredictions are aligned which demonstrates the effectiveness and reliability of\nthe approach. We perform extensive experiments applying the ensemble model on\nout-of-distribution (OOD) data. We demonstrate that for OOD-images, the\nestimated uncertainty increases. Additionally, omitting highly uncertain areas\nimproves the segmentation quality, both for in- and out-of-distribution data.\nThe ensemble model explains its limitations in a reliable manner and can\nmaintain trustworthiness also for OOD data and could be considered in clinical\napplications", "AI": {"task": "\u5c06\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u9996\u6b21\u7eb3\u5165\u8111\u8840\u7ba1\u5206\u5272\u6a21\u578b\uff0c\u4ee5\u63d0\u9ad8\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578b\u7684\u4fe1\u4efb\u5ea6\u3002", "motivation": "\u7531\u4e8e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u590d\u6742\u5ea6\u9ad8\u4e14\u7f3a\u4e4f\u51b3\u7b56\u53ef\u9760\u6027\u6307\u793a\uff0c\u5176\u4fe1\u4efb\u5ea6\u4e0d\u8db3\uff0c\u800c\u8111\u8840\u7ba1\u5206\u5272\u5728\u8bca\u65ad\u8111\u8840\u7ba1\u75be\u75c5\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u8d1d\u53f6\u65af\u8fd1\u4f3c\u548c\u6df1\u5ea6\u96c6\u6210\u4f18\u52bf\u7684\u9ad8\u6548\u96c6\u6210\u6a21\u578b\uff0c\u964d\u4f4e\u4f20\u7edf\u6982\u7387\u7f51\u7edc\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u578b\u80fd\u6709\u6548\u8bc6\u522b\u9ad8\u4e0d\u786e\u5b9a\u6027\u548c\u9519\u8bef\u9884\u6d4b\u533a\u57df\uff0c\u4e14\u5728\u5206\u5e03\u5916\u6570\u636e\u4e0a\u4e0d\u786e\u5b9a\u6027\u589e\u52a0\uff0c\u5ffd\u7565\u9ad8\u4e0d\u786e\u5b9a\u533a\u57df\u53ef\u63d0\u5347\u5206\u5272\u8d28\u91cf\u3002", "conclusion": "\u8be5\u96c6\u6210\u6a21\u578b\u80fd\u53ef\u9760\u5730\u89e3\u91ca\u5176\u5c40\u9650\u6027\uff0c\u4fdd\u6301\u5bf9\u5206\u5e03\u5916\u6570\u636e\u7684\u53ef\u4fe1\u5ea6\uff0c\u9002\u7528\u4e8e\u4e34\u5e8a\u5e94\u7528\u3002"}}
{"id": "2503.22330", "pdf": "https://arxiv.org/pdf/2503.22330", "abs": "https://arxiv.org/abs/2503.22330", "authors": ["Ziping Dong", "Chao Shuai", "Zhongjie Ba", "Peng Cheng", "Zhan Qin", "Qinglong Wang", "Kui Ren"], "title": "Imperceptible but Forgeable: Practical Invisible Watermark Forgery via Diffusion Models", "categories": ["cs.CR", "cs.CV"], "comment": null, "summary": "Invisible watermarking is critical for content provenance and accountability\nin Generative AI. Although commercial companies have increasingly committed to\nusing watermarks, the robustness of existing watermarking schemes against\nforgery attacks is understudied. This paper proposes DiffForge, the first\nwatermark forgery framework capable of forging imperceptible watermarks under a\nno-box setting. We estimate the watermark distribution using an unconditional\ndiffusion model and introduce shallow inversion to inject the watermark into a\nnon-watermarked image seamlessly. This approach facilitates watermark injection\nwhile preserving image quality by adaptively selecting the depth of inversion\nsteps, leveraging our key insight that watermarks degrade with added noise\nduring the early diffusion phases. Comprehensive evaluations show that\nDiffForge deceives open-source watermark detectors with a 96.38% success rate\nand misleads a commercial watermark system with over 97% success rate,\nachieving high confidence.1 This work reveals fundamental security limitations\nin current watermarking paradigms.", "AI": {"task": "\u63d0\u51faDiffForge\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u65e0\u76d2\u8bbe\u7f6e\u4e0b\u4f2a\u9020\u4e0d\u53ef\u89c1\u6c34\u5370\u3002", "motivation": "\u7814\u7a76\u73b0\u6709\u6c34\u5370\u65b9\u6848\u5728\u4f2a\u9020\u653b\u51fb\u4e0b\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u65e0\u6761\u4ef6\u6269\u6563\u6a21\u578b\u4f30\u8ba1\u6c34\u5370\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u6d45\u5c42\u53cd\u8f6c\u5c06\u6c34\u5370\u65e0\u7f1d\u6ce8\u5165\u975e\u6c34\u5370\u56fe\u50cf\u3002", "result": "DiffForge\u6210\u529f\u6b3a\u9a97\u5f00\u6e90\u6c34\u5370\u68c0\u6d4b\u5668\uff0896.38%\u6210\u529f\u7387\uff09\u548c\u5546\u4e1a\u6c34\u5370\u7cfb\u7edf\uff0897%\u6210\u529f\u7387\uff09\u3002", "conclusion": "\u63ed\u793a\u4e86\u5f53\u524d\u6c34\u5370\u8303\u5f0f\u7684\u57fa\u672c\u5b89\u5168\u5c40\u9650\u6027\u3002"}}
{"id": "2503.22496", "pdf": "https://arxiv.org/pdf/2503.22496", "abs": "https://arxiv.org/abs/2503.22496", "authors": ["Luke Rowe", "Roger Girgis", "Anthony Gosselin", "Liam Paull", "Christopher Pal", "Felix Heide"], "title": "Scenario Dreamer: Vectorized Latent Diffusion for Generating Driving Simulation Environments", "categories": ["cs.RO", "cs.CV"], "comment": "CVPR 2025", "summary": "We introduce Scenario Dreamer, a fully data-driven generative simulator for\nautonomous vehicle planning that generates both the initial traffic scene -\ncomprising a lane graph and agent bounding boxes - and closed-loop agent\nbehaviours. Existing methods for generating driving simulation environments\nencode the initial traffic scene as a rasterized image and, as such, require\nparameter-heavy networks that perform unnecessary computation due to many empty\npixels in the rasterized scene. Moreover, we find that existing methods that\nemploy rule-based agent behaviours lack diversity and realism. Scenario Dreamer\ninstead employs a novel vectorized latent diffusion model for initial scene\ngeneration that directly operates on the vectorized scene elements and an\nautoregressive Transformer for data-driven agent behaviour simulation. Scenario\nDreamer additionally supports scene extrapolation via diffusion inpainting,\nenabling the generation of unbounded simulation environments. Extensive\nexperiments show that Scenario Dreamer outperforms existing generative\nsimulators in realism and efficiency: the vectorized scene-generation base\nmodel achieves superior generation quality with around 2x fewer parameters, 6x\nlower generation latency, and 10x fewer GPU training hours compared to the\nstrongest baseline. We confirm its practical utility by showing that\nreinforcement learning planning agents are more challenged in Scenario Dreamer\nenvironments than traditional non-generative simulation environments,\nespecially on long and adversarial driving environments.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aScenario Dreamer\u7684\u6570\u636e\u9a71\u52a8\u751f\u6210\u6a21\u62df\u5668\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u89c4\u5212\uff0c\u751f\u6210\u521d\u59cb\u4ea4\u901a\u573a\u666f\u548c\u95ed\u73af\u4ee3\u7406\u884c\u4e3a\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u9a7e\u9a76\u6a21\u62df\u73af\u5883\u65f6\u5c06\u521d\u59cb\u4ea4\u901a\u573a\u666f\u7f16\u7801\u4e3a\u6805\u683c\u5316\u56fe\u50cf\uff0c\u5bfc\u81f4\u53c2\u6570\u7e41\u91cd\u7684\u7f51\u7edc\u548c\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\uff1b\u540c\u65f6\uff0c\u57fa\u4e8e\u89c4\u5219\u7684\u4ee3\u7406\u884c\u4e3a\u7f3a\u4e4f\u591a\u6837\u6027\u548c\u771f\u5b9e\u6027\u3002", "method": "\u91c7\u7528\u5411\u91cf\u5316\u6f5c\u5728\u6269\u6563\u6a21\u578b\u751f\u6210\u521d\u59cb\u573a\u666f\uff0c\u5e76\u4f7f\u7528\u81ea\u56de\u5f52Transformer\u6a21\u62df\u6570\u636e\u9a71\u52a8\u7684\u4ee3\u7406\u884c\u4e3a\uff0c\u652f\u6301\u901a\u8fc7\u6269\u6563\u4fee\u590d\u8fdb\u884c\u573a\u666f\u5916\u63a8\u3002", "result": "Scenario Dreamer\u5728\u771f\u5b9e\u6027\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u751f\u6210\u6a21\u62df\u5668\uff0c\u53c2\u6570\u51cf\u5c112\u500d\uff0c\u751f\u6210\u5ef6\u8fdf\u964d\u4f4e6\u500d\uff0cGPU\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1110\u500d\u3002", "conclusion": "Scenario Dreamer\u5728\u6311\u6218\u6027\u9a7e\u9a76\u73af\u5883\u4e2d\u5bf9\u5f3a\u5316\u5b66\u4e60\u89c4\u5212\u4ee3\u7406\u66f4\u5177\u6311\u6218\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2503.22517", "pdf": "https://arxiv.org/pdf/2503.22517", "abs": "https://arxiv.org/abs/2503.22517", "authors": ["Raman Dutt", "Harleen Hanspal", "Guoxuan Xia", "Petru-Daniel Tudosiu", "Alexander Black", "Yongxin Yang", "Steven McDonagh", "Sarah Parisot"], "title": "Exploiting Mixture-of-Experts Redundancy Unlocks Multimodal Generative Abilities", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "In this work, we undertake the challenge of augmenting the existing\ngenerative capabilities of pre-trained text-only large language models (LLMs)\nwith multi-modal generation capability while satisfying two core constraints:\nC1 preserving the preservation of original language generative capabilities\nwith negligible performance degradation, and C2 adhering to a small parameter\nbudget to learn the new modality, ensuring scalability and efficiency. In\ncontrast to current approaches that add dedicated modules, thereby\nsignificantly increasing the parameter count, we propose a method that\nleverages the underutilized capacity inherent in deep models. Specifically, we\nexploit the parameter redundancy within Mixture-of-Experts (MoEs) as a source\nof additional capacity for learning a new modality, enabling better parameter\nefficiency (C1). Moreover, we preserve the original language generation\ncapabilities by applying low-rank adaptation exclusively to the tokens of the\nnew modality (C2). Furthermore, we introduce a novel parameter initialization\nscheme based on the Gromov-Wasserstein distance to improve convergence and\ntraining stability. Through an extensive analysis of the routing mechanism, we\nuncover the emergence of modality-specific pathways and decreased redundancy\nwithin the experts that can efficiently unlock multi-modal generative\ncapabilities. Overall, our method can be seamlessly applied to a wide range of\ncontemporary LLMs, providing a new pathway for transitioning from uni-modal to\nmulti-modal architectures.", "AI": {"task": "\u589e\u5f3a\u9884\u8bad\u7ec3\u6587\u672c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6a21\u6001\u751f\u6210\u80fd\u529b\uff0c\u540c\u65f6\u6ee1\u8db3\u4fdd\u6301\u539f\u6709\u8bed\u8a00\u751f\u6210\u80fd\u529b\u548c\u53c2\u6570\u6548\u7387\u7684\u7ea6\u675f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u589e\u52a0\u4e13\u7528\u6a21\u5757\u663e\u8457\u589e\u52a0\u53c2\u6570\u6570\u91cf\uff0c\u800c\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528\u6df1\u5ea6\u6a21\u578b\u4e2d\u672a\u5145\u5206\u5229\u7528\u7684\u5bb9\u91cf\uff0c\u63d0\u9ad8\u53c2\u6570\u6548\u7387\u3002", "method": "\u5229\u7528Mixture-of-Experts\uff08MoEs\uff09\u4e2d\u7684\u53c2\u6570\u5197\u4f59\u4f5c\u4e3a\u5b66\u4e60\u65b0\u6a21\u6001\u7684\u989d\u5916\u5bb9\u91cf\uff0c\u5e76\u901a\u8fc7\u4f4e\u79e9\u9002\u5e94\u4fdd\u7559\u8bed\u8a00\u751f\u6210\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u8def\u7531\u673a\u5236\u5206\u6790\uff0c\u63ed\u793a\u4e86\u6a21\u6001\u7279\u5b9a\u8def\u5f84\u7684\u51fa\u73b0\u548c\u4e13\u5bb6\u5197\u4f59\u7684\u51cf\u5c11\uff0c\u6709\u6548\u89e3\u9501\u591a\u6a21\u6001\u751f\u6210\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u65e0\u7f1d\u5e94\u7528\u4e8e\u591a\u79cd\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4e3a\u4ece\u5355\u6a21\u6001\u5230\u591a\u6a21\u6001\u67b6\u6784\u7684\u8fc7\u6e21\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2503.22531", "pdf": "https://arxiv.org/pdf/2503.22531", "abs": "https://arxiv.org/abs/2503.22531", "authors": ["Qisheng He", "Nicholas Summerfield", "Peiyong Wang", "Carri Glide-Hurst", "Ming Dong"], "title": "Deterministic Medical Image Translation via High-fidelity Brownian Bridges", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Recent studies have shown that diffusion models produce superior synthetic\nimages when compared to Generative Adversarial Networks (GANs). However, their\noutputs are often non-deterministic and lack high fidelity to the ground truth\ndue to the inherent randomness. In this paper, we propose a novel High-fidelity\nBrownian bridge model (HiFi-BBrg) for deterministic medical image translations.\nOur model comprises two distinct yet mutually beneficial mappings: a generation\nmapping and a reconstruction mapping. The Brownian bridge training process is\nguided by the fidelity loss and adversarial training in the reconstruction\nmapping. This ensures that translated images can be accurately reversed to\ntheir original forms, thereby achieving consistent translations with high\nfidelity to the ground truth. Our extensive experiments on multiple datasets\nshow HiFi-BBrg outperforms state-of-the-art methods in multi-modal image\ntranslation and multi-image super-resolution.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u9ad8\u4fdd\u771f\u5e03\u6717\u6865\u6a21\u578b\uff08HiFi-BBrg\uff09\u7528\u4e8e\u786e\u5b9a\u6027\u533b\u5b66\u56fe\u50cf\u8f6c\u6362\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u5408\u6210\u56fe\u50cf\u65b9\u9762\u4f18\u4e8eGANs\uff0c\u4f46\u5176\u8f93\u51fa\u5177\u6709\u975e\u786e\u5b9a\u6027\u548c\u4f4e\u4fdd\u771f\u5ea6\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u751f\u6210\u6620\u5c04\u548c\u91cd\u5efa\u6620\u5c04\uff0c\u901a\u8fc7\u4fdd\u771f\u5ea6\u635f\u5931\u548c\u5bf9\u6297\u8bad\u7ec3\u6307\u5bfc\u5e03\u6717\u6865\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHiFi-BBrg\u5728\u591a\u6a21\u6001\u56fe\u50cf\u8f6c\u6362\u548c\u591a\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "HiFi-BBrg\u80fd\u591f\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5ea6\u7684\u786e\u5b9a\u6027\u533b\u5b66\u56fe\u50cf\u8f6c\u6362\u3002"}}
{"id": "2503.22563", "pdf": "https://arxiv.org/pdf/2503.22563", "abs": "https://arxiv.org/abs/2503.22563", "authors": ["Pasquale Cascarano", "Lorenzo Stacchio", "Andrea Sebastiani", "Alessandro Benfenati", "Ulugbek S. Kamilov", "Gustavo Marfia"], "title": "RELD: Regularization by Latent Diffusion Models for Image Restoration", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "In recent years, Diffusion Models have become the new state-of-the-art in\ndeep generative modeling, ending the long-time dominance of Generative\nAdversarial Networks. Inspired by the Regularization by Denoising principle, we\nintroduce an approach that integrates a Latent Diffusion Model, trained for the\ndenoising task, into a variational framework using Half-Quadratic Splitting,\nexploiting its regularization properties. This approach, under appropriate\nconditions that can be easily met in various imaging applications, allows for\nreduced computational cost while achieving high-quality results. The proposed\nstrategy, called Regularization by Latent Denoising (RELD), is then tested on a\ndataset of natural images, for image denoising, deblurring, and\nsuper-resolution tasks. The numerical experiments show that RELD is competitive\nwith other state-of-the-art methods, particularly achieving remarkable results\nwhen evaluated using perceptual quality metrics.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u53d8\u5206\u6846\u67b6\u65b9\u6cd5\uff08RELD\uff09\uff0c\u7528\u4e8e\u56fe\u50cf\u53bb\u566a\u3001\u53bb\u6a21\u7cca\u548c\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5df2\u6210\u4e3a\u6df1\u5ea6\u751f\u6210\u5efa\u6a21\u7684\u65b0\u524d\u6cbf\uff0c\u4f46\u5176\u8ba1\u7b97\u6210\u672c\u8f83\u9ad8\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u6f5c\u5728\u6269\u6563\u6a21\u578b\u548c\u534a\u4e8c\u6b21\u5206\u88c2\u7684\u53d8\u5206\u6846\u67b6\uff0c\u5229\u7528\u5176\u6b63\u5219\u5316\u7279\u6027\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5728\u81ea\u7136\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRELD\u5728\u611f\u77e5\u8d28\u91cf\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u7ade\u4e89\u529b\u76f8\u5f53\u3002", "conclusion": "RELD\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6210\u50cf\u5e94\u7528\u3002"}}
{"id": "2503.22588", "pdf": "https://arxiv.org/pdf/2503.22588", "abs": "https://arxiv.org/abs/2503.22588", "authors": ["Heiko Renz", "Maximilian Kr\u00e4mer", "Frank Hoffmann", "Torsten Bertram"], "title": "Next-Best-Trajectory Planning of Robot Manipulators for Effective Observation and Exploration", "categories": ["cs.RO", "cs.CV", "cs.HC"], "comment": "Accepted for publication at the IEEE International Conference on\n  Robotics and Automation (ICRA), 2025", "summary": "Visual observation of objects is essential for many robotic applications,\nsuch as object reconstruction and manipulation, navigation, and scene\nunderstanding. Machine learning algorithms constitute the state-of-the-art in\nmany fields but require vast data sets, which are costly and time-intensive to\ncollect. Automated strategies for observation and exploration are crucial to\nenhance the efficiency of data gathering. Therefore, a novel strategy utilizing\nthe Next-Best-Trajectory principle is developed for a robot manipulator\noperating in dynamic environments. Local trajectories are generated to maximize\nthe information gained from observations along the path while avoiding\ncollisions. We employ a voxel map for environment modeling and utilize\nraycasting from perspectives around a point of interest to estimate the\ninformation gain. A global ergodic trajectory planner provides an optional\nreference trajectory to the local planner, improving exploration and helping to\navoid local minima. To enhance computational efficiency, raycasting for\nestimating the information gain in the environment is executed in parallel on\nthe graphics processing unit. Benchmark results confirm the efficiency of the\nparallelization, while real-world experiments demonstrate the strategy's\neffectiveness.", "AI": {"task": "\u5f00\u53d1\u4e00\u79cd\u57fa\u4e8eNext-Best-Trajectory\u539f\u5219\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\uff0c\u7528\u4e8e\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9ad8\u6548\u6570\u636e\u91c7\u96c6\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u9700\u8981\u5927\u91cf\u6570\u636e\u96c6\uff0c\u4f46\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u4e14\u8017\u65f6\uff0c\u81ea\u52a8\u5316\u89c2\u5bdf\u548c\u63a2\u7d22\u7b56\u7565\u80fd\u63d0\u9ad8\u6570\u636e\u91c7\u96c6\u6548\u7387\u3002", "method": "\u5229\u7528Next-Best-Trajectory\u539f\u5219\uff0c\u7ed3\u5408\u5c40\u90e8\u8f68\u8ff9\u751f\u6210\u3001\u4f53\u7d20\u5730\u56fe\u73af\u5883\u5efa\u6a21\u3001\u5149\u7ebf\u6295\u5c04\u4fe1\u606f\u589e\u76ca\u4f30\u8ba1\u548c\u5168\u5c40\u904d\u5386\u8f68\u8ff9\u89c4\u5212\u3002", "result": "\u5e76\u884c\u5316\u8ba1\u7b97\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u771f\u5b9e\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b56\u7565\u5728\u52a8\u6001\u73af\u5883\u4e2d\u9ad8\u6548\u4e14\u6709\u6548\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u6570\u636e\u91c7\u96c6\u4efb\u52a1\u3002"}}
{"id": "2503.22589", "pdf": "https://arxiv.org/pdf/2503.22589", "abs": "https://arxiv.org/abs/2503.22589", "authors": ["Adam Breuer", "Bryce J. Dietrich", "Michael H. Crespin", "Matthew Butler", "J. A. Pyrse", "Kosuke Imai"], "title": "Using AI to Summarize US Presidential Campaign TV Advertisement Videos, 1952-2012", "categories": ["cs.MM", "cs.AI", "cs.CV", "cs.LG"], "comment": "17 pages, 7 tables, 4 figures, and linked datasets", "summary": "This paper introduces the largest and most comprehensive dataset of US\npresidential campaign television advertisements, available in digital format.\nThe dataset also includes machine-searchable transcripts and high-quality\nsummaries designed to facilitate a variety of academic research. To date, there\nhas been great interest in collecting and analyzing US presidential campaign\nadvertisements, but the need for manual procurement and annotation led many to\nrely on smaller subsets. We design a large-scale parallelized, AI-based\nanalysis pipeline that automates the laborious process of preparing,\ntranscribing, and summarizing videos. We then apply this methodology to the\n9,707 presidential ads from the Julian P. Kanter Political Commercial Archive.\nWe conduct extensive human evaluations to show that these transcripts and\nsummaries match the quality of manually generated alternatives. We illustrate\nthe value of this data by including an application that tracks the genesis and\nevolution of current focal issue areas over seven decades of presidential\nelections. Our analysis pipeline and codebase also show how to use LLM-based\ntools to obtain high-quality summaries for other video datasets.", "AI": {"task": "\u4ecb\u7ecd\u5e76\u5206\u6790\u7f8e\u56fd\u603b\u7edf\u7ade\u9009\u7535\u89c6\u5e7f\u544a\u7684\u6700\u5927\u3001\u6700\u5168\u9762\u7684\u6570\u636e\u96c6\u3002", "motivation": "\u89e3\u51b3\u624b\u52a8\u83b7\u53d6\u548c\u6807\u6ce8\u5e7f\u544a\u6570\u636e\u7684\u56f0\u96be\uff0c\u63a8\u52a8\u5b66\u672f\u7814\u7a76\u3002", "method": "\u8bbe\u8ba1\u5927\u89c4\u6a21\u5e76\u884c\u5316\u3001\u57fa\u4e8eAI\u7684\u5206\u6790\u6d41\u7a0b\uff0c\u81ea\u52a8\u5904\u7406\u89c6\u9891\u7684\u8f6c\u5f55\u548c\u6458\u8981\u3002", "result": "\u751f\u6210\u9ad8\u8d28\u91cf\u8f6c\u5f55\u548c\u6458\u8981\uff0c\u4e0e\u4eba\u5de5\u751f\u6210\u7684\u8d28\u91cf\u76f8\u5f53\uff0c\u5e76\u5e94\u7528\u4e8e\u8ffd\u8e2a\u7ade\u9009\u7126\u70b9\u95ee\u9898\u7684\u6f14\u53d8\u3002", "conclusion": "\u5c55\u793a\u4e86\u57fa\u4e8eLLM\u7684\u5de5\u5177\u5982\u4f55\u9ad8\u6548\u5904\u7406\u89c6\u9891\u6570\u636e\u96c6\uff0c\u4e3a\u5176\u4ed6\u7814\u7a76\u63d0\u4f9b\u53c2\u8003\u3002"}}
{"id": "2503.22592", "pdf": "https://arxiv.org/pdf/2503.22592", "abs": "https://arxiv.org/abs/2503.22592", "authors": ["Thomas Boucher", "Nicholas Tetlow", "Annie Fung", "Amy Dewar", "Pietro Arina", "Sven Kerneis", "John Whittle", "Evangelos B. Mazomenos"], "title": "KEVS: Enhancing Segmentation of Visceral Adipose Tissue in Pre-Cystectomy CT with Gaussian Kernel Density Estimation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Preprint for submission to IPCAI special edition of IJCARS 2025,\n  version prior to any peer review", "summary": "Purpose: The distribution of visceral adipose tissue (VAT) in cystectomy\npatients is indicative of the incidence of post-operative complications.\nExisting VAT segmentation methods for computed tomography (CT) employing\nintensity thresholding have limitations relating to inter-observer variability.\nMoreover, the difficulty in creating ground-truth masks limits the development\nof deep learning (DL) models for this task. This paper introduces a novel\nmethod for VAT prediction in pre-cystectomy CT, which is fully automated and\ndoes not require ground-truth VAT masks for training, overcoming aforementioned\nlimitations. Methods: We introduce the Kernel density Enhanced VAT Segmentator\n( KEVS), combining a DL semantic segmentation model, for multi-body feature\nprediction, with Gaussian kernel density estimation analysis of predicted\nsubcutaneous adipose tissue to achieve accurate scan-specific predictions of\nVAT in the abdominal cavity. Uniquely for a DL pipeline, KEVS does not require\nground-truth VAT masks. Results: We verify the ability of KEVS to accurately\nsegment abdominal organs in unseen CT data and compare KEVS VAT segmentation\npredictions to existing state-of-the-art (SOTA) approaches in a dataset of 20\npre-cystectomy CT scans, collected from University College London Hospital\n(UCLH-Cyst), with expert ground-truth annotations. KEVS presents a 4.80% and\n6.02% improvement in Dice Coefficient over the second best DL and\nthresholding-based VAT segmentation techniques respectively when evaluated on\nUCLH-Cyst. Conclusion: This research introduces KEVS; an automated, SOTA method\nfor the prediction of VAT in pre-cystectomy CT which eliminates inter-observer\nvariability and is trained entirely on open-source CT datasets which do not\ncontain ground-truth VAT masks.", "AI": {"task": "\u5f00\u53d1\u4e00\u79cd\u5168\u81ea\u52a8\u7684\u5185\u810f\u8102\u80aa\u7ec4\u7ec7\uff08VAT\uff09\u9884\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u8180\u80f1\u5207\u9664\u672f\u524dCT\u626b\u63cf\uff0c\u65e0\u9700\u4f9d\u8d56\u771f\u5b9e\u6807\u6ce8\u7684VAT\u63a9\u6a21\u8fdb\u884c\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5ea6\u9608\u503c\u7684VAT\u5206\u5272\u65b9\u6cd5\u5b58\u5728\u89c2\u5bdf\u8005\u95f4\u53d8\u5f02\u6027\u95ee\u9898\uff0c\u4e14\u771f\u5b9e\u6807\u6ce8\u63a9\u6a21\u7684\u83b7\u53d6\u56f0\u96be\u9650\u5236\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51faKEVS\u65b9\u6cd5\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u8bed\u4e49\u5206\u5272\u6a21\u578b\u548c\u9ad8\u65af\u6838\u5bc6\u5ea6\u4f30\u8ba1\u5206\u6790\uff0c\u5b9e\u73b0\u65e0\u9700\u771f\u5b9eVAT\u63a9\u6a21\u7684\u81ea\u52a8\u5316\u9884\u6d4b\u3002", "result": "KEVS\u5728\u672a\u89c1\u7684CT\u6570\u636e\u4e2d\u51c6\u786e\u5206\u5272\u8179\u90e8\u5668\u5b98\uff0c\u5e76\u572820\u4f8b\u8180\u80f1\u5207\u9664\u672f\u524dCT\u626b\u63cf\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cDice\u7cfb\u6570\u5206\u522b\u63d0\u53474.80%\u548c6.02%\u3002", "conclusion": "KEVS\u662f\u4e00\u79cd\u5168\u81ea\u52a8\u3001\u65e0\u9700\u771f\u5b9eVAT\u63a9\u6a21\u7684\u5148\u8fdb\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u89c2\u5bdf\u8005\u95f4\u53d8\u5f02\u6027\u95ee\u9898\uff0c\u5e76\u5b8c\u5168\u57fa\u4e8e\u5f00\u6e90CT\u6570\u636e\u96c6\u8bad\u7ec3\u3002"}}
{"id": "2503.22605", "pdf": "https://arxiv.org/pdf/2503.22605", "abs": "https://arxiv.org/abs/2503.22605", "authors": ["Shuai Shen", "Wanhua Li", "Yunpeng Zhang", "Weipeng Hu", "Yap-Peng Tan"], "title": "Audio-Plane: Audio Factorization Plane Gaussian Splatting for Real-Time Talking Head Synthesis", "categories": ["cs.GR", "cs.CV", "cs.SD", "eess.AS"], "comment": null, "summary": "Talking head synthesis has become a key research area in computer graphics\nand multimedia, yet most existing methods often struggle to balance generation\nquality with computational efficiency. In this paper, we present a novel\napproach that leverages an Audio Factorization Plane (Audio-Plane) based\nGaussian Splatting for high-quality and real-time talking head generation. For\nmodeling a dynamic talking head, 4D volume representation is needed. However,\ndirectly storing a dense 4D grid is impractical due to the high cost and lack\nof scalability for longer durations. We overcome this challenge with the\nproposed Audio-Plane, where the 4D volume representation is decomposed into\naudio-independent space planes and audio-dependent planes. This provides a\ncompact and interpretable feature representation for talking head, facilitating\nmore precise audio-aware spatial encoding and enhanced audio-driven lip dynamic\nmodeling. To further improve speech dynamics, we develop a dynamic splatting\nmethod that helps the network more effectively focus on modeling the dynamics\nof the mouth region. Extensive experiments demonstrate that by integrating\nthese innovations with the powerful Gaussian Splatting, our method is capable\nof synthesizing highly realistic talking videos in real time while ensuring\nprecise audio-lip synchronization. Synthesized results are available in\nhttps://sstzal.github.io/Audio-Plane/.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u97f3\u9891\u56e0\u5b50\u5316\u5e73\u9762\uff08Audio-Plane\uff09\u7684\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u8d28\u91cf\u5b9e\u65f6\u8bf4\u8bdd\u5934\u90e8\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0c\u4e14\u76f4\u63a5\u5b58\u50a8\u5bc6\u96c64D\u7f51\u683c\u6210\u672c\u9ad8\u4e14\u4e0d\u53ef\u6269\u5c55\u3002", "method": "\u5c064D\u4f53\u79ef\u8868\u793a\u5206\u89e3\u4e3a\u97f3\u9891\u65e0\u5173\u7684\u7a7a\u95f4\u5e73\u9762\u548c\u97f3\u9891\u76f8\u5173\u7684\u5e73\u9762\uff0c\u7ed3\u5408\u52a8\u6001\u6cfc\u6e85\u65b9\u6cd5\u4f18\u5316\u5634\u90e8\u52a8\u6001\u5efa\u6a21\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u5b9e\u65f6\u5408\u6210\u9ad8\u8d28\u91cf\u8bf4\u8bdd\u89c6\u9891\uff0c\u5e76\u786e\u4fdd\u7cbe\u786e\u7684\u97f3\u9891-\u5507\u90e8\u540c\u6b65\u3002", "conclusion": "\u901a\u8fc7\u97f3\u9891\u56e0\u5b50\u5316\u5e73\u9762\u548c\u9ad8\u65af\u6cfc\u6e85\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u8bf4\u8bdd\u5934\u90e8\u751f\u6210\u3002"}}
{"id": "2503.22655", "pdf": "https://arxiv.org/pdf/2503.22655", "abs": "https://arxiv.org/abs/2503.22655", "authors": ["Xiaomin Yu", "Pengxiang Ding", "Wenjie Zhang", "Siteng Huang", "Songyang Gao", "Chengwei Qin", "Kejian Wu", "Zhaoxin Fan", "Ziyue Qiao", "Donglin Wang"], "title": "Unicorn: Text-Only Data Synthesis for Vision Language Model Training", "categories": ["cs.AI", "cs.CV", "cs.MM"], "comment": null, "summary": "Training vision-language models (VLMs) typically requires large-scale,\nhigh-quality image-text pairs, but collecting or synthesizing such data is\ncostly. In contrast, text data is abundant and inexpensive, prompting the\nquestion: can high-quality multimodal training data be synthesized purely from\ntext? To tackle this, we propose a cross-integrated three-stage multimodal data\nsynthesis framework, which generates two datasets: Unicorn-1.2M and\nUnicorn-471K-Instruction. In Stage 1: Diverse Caption Data Synthesis, we\nconstruct 1.2M semantically diverse high-quality captions by expanding sparse\ncaption seeds using large language models (LLMs). In Stage 2:\nInstruction-Tuning Data Generation, we further process 471K captions into\nmulti-turn instruction-tuning tasks to support complex reasoning. Finally, in\nStage 3: Modality Representation Transfer, these textual captions\nrepresentations are transformed into visual representations, resulting in\ndiverse synthetic image representations. This three-stage process enables us to\nconstruct Unicorn-1.2M for pretraining and Unicorn-471K-Instruction for\ninstruction-tuning, without relying on real images. By eliminating the\ndependency on real images while maintaining data quality and diversity, our\nframework offers a cost-effective and scalable solution for VLMs training. Code\nis available at https://github.com/Yu-xm/Unicorn.git.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u4ece\u7eaf\u6587\u672c\u5408\u6210\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u8bad\u7ec3\u6570\u636e\u7684\u4e09\u9636\u6bb5\u6846\u67b6\u3002", "motivation": "\u7531\u4e8e\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u56fe\u50cf-\u6587\u672c\u5bf9\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\uff0c\u800c\u6587\u672c\u6570\u636e\u4e30\u5bcc\u4e14\u5ec9\u4ef7\uff0c\u56e0\u6b64\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u4ece\u7eaf\u6587\u672c\u5408\u6210\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u8bad\u7ec3\u6570\u636e\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u591a\u6837\u5316\u6807\u9898\u6570\u636e\u5408\u6210\uff1b2\uff09\u6307\u4ee4\u5fae\u8c03\u6570\u636e\u751f\u6210\uff1b3\uff09\u6a21\u6001\u8868\u793a\u8f6c\u6362\uff0c\u6700\u7ec8\u751f\u6210\u5408\u6210\u56fe\u50cf\u8868\u793a\u3002", "result": "\u751f\u6210\u4e86Unicorn-1.2M\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u548cUnicorn-471K-Instruction\u6307\u4ee4\u5fae\u8c03\u6570\u636e\u96c6\uff0c\u65e0\u9700\u4f9d\u8d56\u771f\u5b9e\u56fe\u50cf\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e00\u79cd\u6210\u672c\u4f4e\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2503.22658", "pdf": "https://arxiv.org/pdf/2503.22658", "abs": "https://arxiv.org/abs/2503.22658", "authors": ["Frank J. Brooks", "Rucha Deshpande"], "title": "Evaluation of Machine-generated Biomedical Images via A Tally-based Similarity Measure", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "13 pages. Manuscript under review at IEEE. Data available at\n  https://doi.org/10.13012/B2IDB-2642688_V1", "summary": "Super-resolution, in-painting, whole-image generation, unpaired\nstyle-transfer, and network-constrained image reconstruction each include an\naspect of machine-learned image synthesis where the actual ground truth is not\nknown at time of use. It is generally difficult to quantitatively and\nauthoritatively evaluate the quality of synthetic images; however, in\nmission-critical biomedical scenarios robust evaluation is paramount. In this\nwork, all practical image-to-image comparisons really are relative\nqualifications, not absolute difference quantifications; and, therefore,\nmeaningful evaluation of generated image quality can be accomplished using the\nTversky Index, which is a well-established measure for assessing perceptual\nsimilarity. This evaluation procedure is developed and then demonstrated using\nmultiple image data sets, both real and simulated. The main result is that when\nthe subjectivity and intrinsic deficiencies of any feature-encoding choice are\nput upfront, Tversky's method leads to intuitive results, whereas traditional\nmethods based on summarizing distances in deep feature spaces do not.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eTversky\u6307\u6570\u7684\u56fe\u50cf\u5408\u6210\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u5728\u751f\u7269\u533b\u5b66\u7b49\u5173\u952e\u4efb\u52a1\u573a\u666f\u4e2d\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u5408\u6210\u56fe\u50cf\u7684\u8d28\u91cf\uff0c\u800c\u4f20\u7edf\u57fa\u4e8e\u6df1\u5ea6\u7279\u5f81\u7a7a\u95f4\u8ddd\u79bb\u7684\u65b9\u6cd5\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528Tversky\u6307\u6570\u4f5c\u4e3a\u611f\u77e5\u76f8\u4f3c\u6027\u5ea6\u91cf\uff0c\u5f00\u53d1\u5e76\u6f14\u793a\u4e86\u4e00\u79cd\u8bc4\u4f30\u6d41\u7a0b\u3002", "result": "Tversky\u65b9\u6cd5\u80fd\u591f\u76f4\u89c2\u5730\u8bc4\u4f30\u5408\u6210\u56fe\u50cf\u8d28\u91cf\uff0c\u800c\u4f20\u7edf\u65b9\u6cd5\u5219\u65e0\u6cd5\u505a\u5230\u3002", "conclusion": "Tversky\u6307\u6570\u662f\u4e00\u79cd\u6709\u6548\u7684\u56fe\u50cf\u5408\u6210\u8d28\u91cf\u8bc4\u4f30\u5de5\u5177\uff0c\u5c24\u5176\u5728\u4e3b\u89c2\u6027\u548c\u7279\u5f81\u7f16\u7801\u7f3a\u9677\u660e\u663e\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u8d8a\u3002"}}
